An Unsupervised Model for Statistically Determining CoordinatePhrase AttachmentMir iam Go ldbergCentral High School &Dept.
of Computer and Information Science200 South 33rd StreetPhiladelphia, PA 19104-6389University of PennsylvaniamiriamgOunagi, cis.
upenn, eduAbst ractThis paper examines the use of an unsuper-vised statistical model for determining the at-tachment of ambiguous coordinate phrases (CP)of the form nl  p n2 cc n3.
The model pre-sented here is based on JAR98\], an unsupervisedmodel for determining prepositional phrase at-tachment.
After training on unannotated 1988Wall Street Journal text, the model performsat 72% accuracy on a development set fromsections 14 through 19 of the WSJ TreeBank\[MSM93\].1 In t roduct ionThe coordinate phrase (CP) is a source of struc-tural ambiguity in natural anguage.
For exam-ple, take the phrase:box of chocolates and roses'Roses' attaches either high to 'box' or low to'chocolates'.
In this case, attachment is high,yielding:H-attach: ((box (of chocolates)) (and roses))Consider, then, the phrase:salad of lettuce and tomatoes'Lettuce' attaches low to 'tomatoes', giving:L-attach: (salad (of ((lettuce) and(tomatoes)))\[AR98\] models.
In addition to these, a corpus-based model for PP-attachment \[SN97\] has beenreported that uses information from a semanticdictionary.Sparse data can be a major concern in corpus-based disambiguation.
Supervised models arelimited by the amount of annotated ata avail-able for training.
Such a model is useful onlyfor languages in which annotated corpora areavailable.
Because an unsupervised model doesnot rely on such corpora it may be modified foruse in multiple languages as in \[AR98\].The unsupervised model presented heretrains from an unannotated version of the 1988Wall Street Journal.
After tagging and chunk-ing the text, a rough heuristic is then employedto pick out training examples.
This results ina training set that is less accurate, but muchlarger, than currently existing annotated cor-pora.
It is the goal, then, of unsupervised train-ing data to be abundant in order to offset itsnoisiness.2 BackgroundThe statistical model must determine the prob-ability of a given CP attaching either high (H)or low (L), p( attachment I phrase).
Resultsshown come from a development corpus of 500phrases of extracted head word tuples from theWSJ TreeBank \[MSM93\].
64% of these phrasesattach low and 36% attach high.
After furtherdevelopment, final testing will be done on a sep-arate corpus.The phrase:Previous work has used corpus-based ap-proaches to solve the similar problem of prepo-sitional phrase attachment.
These have in-cluded backed-off \[CB 95\], maximum entropy\[RRR94\], rule-based \[HR94\], and unsupervised(busloads (of ((executives) and (their wives)))gives the 6-tuple:L busloads of executives and wives610where, a = L, n l  = busloads, p = of, n2 =executives, cc = and, n3 = wives.
The CP at-tachment model must determine a for all (nlp n2 cc n3) sets.
The attachment decision iscorrect if it is the same as the correspondingdecision in the TreeBank set.The probabil ity of a CP attaching high isconditional on the 5-tuple.
The algorithm pre-sented in this paper estimates the probability:regular expressions that replace noun and quan-tifier phrases with their head words.
These headwords were then passed through a set of heuris-tics to extract the unambiguous phrases.
Theheuristics to find an unambiguous CP are:?
wn is a coordinating conjunction (cc) if itis tagged cc.?
w,~_~ is the leftmost noun (nl) i f :I5 = (a l n l ,p ,  n2, cc, n3)The parts of the CP are analogous to thoseof the prepositional phrase (PP) such that{n l ,n2}  - {n ,v}  and n3 - p. JAR98\] de-termines the probabil ity p(v ,n ,p ,a ) .
To beconsistent, here we determine the probabil ityp(n l ,  n2, n3, a).3 Tra in ing  Data  Ext rac t ionA statistical learning model must train from un-ambiguous data.
In annotated corpora ambigu-ous data are made unambiguous through classi-fications made by human annotators.
In unan-notated corpora the data themselves must beunambiguous.
Therefore, while this model dis-ambiguates CPs of the form (nl p n2 cc n3), ittrains from implicitly unambiguous CPs of theform (n ccn) .
For example:- Wn-x is the first noun to occur within4 words to the left of cc.-no  preposition occurs between thisnoun and cc.- no preposition occurs within 4 wordsto the left of this noun.?
wn+x is the rightmost noun (n2) if:- it is the first noun to occur within 4words to the right of cc.- No preposition occurs between cc andthis noun.The first noun to occur within 4 words to theright of cc is always extracted.
This is ncc.
Suchnouns are also used in the statistical model.
Forexample, the we process the sentence below asfollows:dog and catBecause there are only two nouns in the un-ambiguous CP, we must redefine its compo-nents.
The first noun will be referred to as nl.It is analogous to n l  and n2 in the ambiguousCP.
The second, terminal noun will be referredto as n3.
It is analogous to the third noun inthe ambiguous CP.
Hence n l  -- dog, cc --- and,n3 = cat.
In addition to the unambiguous CPs,the model also uses any noun that follows acc.Such nouns are classified, ncc.We extracted 119629 unambiguous CPs and325261 nccs from the unannotated 1988 WallStreet Journal.
First the raw text was fed intothe part-of-speech tagger described in \[AR96\] 1.This was then passed to a simple chunker asused in \[AR98\], implemented with two smallIBecause this tagger trained on annotated ata, onemay argue that the model presented here is not purelyunsupervised.Several firms have also launched busi-ness subsidiaries and consulting armsspecializing in trade, lobbying andother areas.First it is annotated with parts of speech:Several_JJ firms__NNS have_VBPalso_RB launched_VBN business.aNNsubsidiaries_NNS and_CC consult-ing_VBG armsANNS specializing_VBGin_IN tradeANN ,_, lobbying_NNand_CC other_JJ areas_NNS ._.From there, it is passed to the chunker yield-ing:firmsANNS have_VBP also_RBlaunched_VBN subsidiaries_NNSand_CC consulting_VBG armsANNSspecializing_VBG in_IN tradeANN ,_,Iobbying_.NN and_CC areas_NNS ._.611Noun phrase heads of ambiguous and unam-biguous CPs are then extracted according to theheuristic, giving:subsidiaries and armsand areaswhere the extracted unambiguous CP is{nl = subsidiaries, cc = and, n3 = arms} andareas is extracted as a ncc because, althoughit is not part of an unambiguous CP, it occurswithin four words after a conjunction.4 The  Stat i s t i ca l  Mode lFirst, we can factor p(a, nl, n2, n3) as follows:p(a, nl,n2, n3) = p(nl)p(n2), p (a ln l  ,n2), p(n3 I a, nl,n2)The terms p(nl) and p(n2) are independentof the attachment and need not be computed.The other two terms are more problematic.
Be-cause the training phrases are unambiguous andof the form (nl cc n2), nl  and n2 of the CPin question ever appear together in the train-ing data.
To compensate we use the followingheuristic as in JAR98\].
Let the random variable?
range over (true, false} and let it denote thepresence or absence of any n3 that unambigu-ously attaches to the nl  or n2 in question.
If?
= true when any n3 unambiguously attachesto nl,  then p(?
= true \[ nl) is the conditionalprobability that a particular nl  occurs with anunambiguously attached n3.
Now p(a I nl,n2)can be approximated as:p(a = H ln l ,  n2) p(true l nl) Z(nl,n2)p(a = L \ [n l ,n2)  ~ p(true In2)" Z(nl, n2)where the normalization factor, Z(nl,n2) =p(true I nl) + p(true I n2).
The reasoning be-hind this approximation is that the tendency ofa CP to attach high (low) is related to the ten-dency of the nl (n2) in question to appear inan unambiguous CP in the training data.We approximate p(n3la, nl, n2) as follows:p(n3 I a = H, nl, n2) ~ p(n3 I true, nl)p(n3 I a = L, nl, n2) ~ p(n3 I true, n2)The reasoning behind this approximation isthat when generating n3 given high (low) at-tachment, the only counts from the trainingdata that matter are those which unambigu-ously attach to nl  (n2), i.e., ?
= true.
Wordstatistics from the extracted CPs are used toformulate these probabilities.4.1 Generate  ?The conditional probabilities p( t rue ln l )  andp(true I n2) denote the probability of whethera noun will appear attached unambiguously tosome n3.
These probabilities are estimated as:{ $(.~1,true) i f f (nl ,true) >0 f(nl) p(truelnl)  = .5 otherwise{ /(n2,~r~,e) if f(n2, true)> 0 /(n2) p(true\[n2) = .5 otherwisewhere f(n2, true) is the number of times n2appears in an unambiguously attached CP inthe training data and f(n2) is the number oftimes this noun has appeared as either nl,  n3,or ncc in the training data.4.2 Generate  n3The terms p(n3 I nl, true) and p(n3 I n2, true)denote the probabilies that the noun n3 appearsattached unambiguously to nl  and n2 respec-tively.
Bigram counts axe used to compute theseas follows:f(nl,n3,true)p(n3 \[ true, nl) = l\](nl, TM) if I(nl,n3,true)>Ootherwisef(n2,n3,true) p(n3 l true, n2) = 11(n2, TM) i f  f(n2,n3,true)>Ootherwisewhere N is the set of all n3s and nets thatoccur in the training data.5 Resu l tsDecisions were deemed correct if they agreedwith the decision in the corresponding Tree-Bank data.
The correct attachment was chosen61272% of the time on the 500-phrase developmentcorpus from the WSJ TreeBank.
Because it isa forced binary decision, there are no measure-ments for recall or precision.
If low attachmentis always chosen, the accuracy is 64%.
After fur-ther development the model will be tested on atesting corpus.When evaluating the effectiveness of an un-supervised model, it is helpful to compare itsperformance to that of an analogous upervisedmodel.
The smaller the error reduction whengoing from unsupervised to supervised models,the more comparable the unsupervised modelis to its supervised counterpart.
To our knowl-edge there has been very little if any work in thearea of ambiguous CPs.
In addition to develop-ing an unsupervised CP disambiguation model,In \[MG, in prep\] we have developed two super-vised models (one backed-off and one maximumentropy) for determining CP attachment.
Thebacked-off model, closely based on \[CB95\] per-forms at 75.6% accuracy.
The reduction errorfrom the unsupervised model presented here tothe backed-off model is 13%.
This is compa-rable to the 14.3% error reduction found whengoing from JAR98\] to \[CB95\].It is interesting to note that after reducingthe volume of training data by half there wasno drop in accuracy.
In fact, accuracy remainedexactly the same as the volume of data was in-creased from half to full.
The backed-off modelin \[MG, in prep\] trained on only 1380 train-ing phrases.
The training corpus used in thestudy presented here consisted of 119629 train-ing phrases.
Reducing this figure by half is notoverly significant.6 Discuss ionIn an effort to make the heuristic concise andportable, we may have oversimplified it therebynegatively affecting the performance of themodel.
For example, when the heuristic cameupon a noun phrase consisting of more than oneconsecutive noun the noun closest o the cc wasextracted.
In a phrase like coffee and rhubarbapple pie the heuristic would chose rhubarb asthe n3 when clearly pie should have been cho-sen. Also, the heuristic did not check if a prepo-sition occurred between either n l  and cc or ccand n3.
Such cases make the CP ambiguousthereby invalidating it as an unambiguous train-ing example.
By including annotated trainingdata from the TreeBank set, this model couldbe modified to become a partially-unsupervisedclassifier.Because the model presented here is basicallya straight reimplementation of \[AR98\] it fails totake into account attributes that are specific tothe CP.
For example, whereas (nl  ce n3) -- (n3cc nl),  (v p n) ~ (n p v).
In other words, thereis no reason to make the distinction between"dog and cat" and "cat and dog."
Modifyingthe model accordingly may greatly increase theusefulness of the training data.7 AcknowledgementsWe thank Mitch Marcus and Dennis Erlick formaking this research possible, Mike Col\]in.~ forhis guidance, and Adwait Ratnaparkhi and Ja-son Eisner for their helpful insights.Re ferences~\[CB95\] M. Collins, J. Brooks.
1995.
Preposi-tional Phrase Attachment through a Backed-Off Model, A CL 3rd Workshop on Very LargeCorpora, Pages 27-38, Cambridge, Mas-sachusetts, June.\[MG, in prep\] M. Goldberg.
in preparation.Three Models for Statistically DeterminingCoordinate Phrase Attachment.\[HR93\] D. Hindle, M. Rooth.
1993.
StructuralAmbiguity and Lexical Relations.
Computa-tional Linguistics, 19(1):103-120.\[MSM93\] M. Marcus, B. Santorini and M.Marcinkiewicz.
1993.
Building a Large Anno-tated Corpus of English: the Penn Treebank,Computational Linguistics, 19(2):313-330.\[RRR94\] A. Ratnaparkhi, J. Reynar and S.Roukos.
1994.
A Maximum Entropy Modelfor Prepositional Phrase Attachment, In Pro-ceedings of the ARPA Workshop on HumanLanguage Technology, 1994.\[AR96\] A. Ratnaparkhi.
1996.
A Maximum En-tropy Part-Of-Speech Tagger, In Proceedingsof the Empirical Methods in Natural Lan-guage Processing Conference, May 17-18.\[AR98\] A. Ratnaparkhi.
1998.
UnsupervisedStatistical Models for Prepositional PhraseAttachment, In Proceedings of the Seven-teenth International Conference on Compu-tational Linguistics, Aug. 10-14, Montreal,Canada.613\[SN97\] J. Stetina, M. Nagao.
1997.
CorpusBased PP Attachment Ambiguity Resolutionwith a Semantic Dictionary.
In Jou Shou andKenneth Church, editors, Proceedings o\] theFifth Workshop on Very Large Corpora, pages66-80, Beijing and Hong Kong, Aug. 18-20.614
