Zock/Rapp/Huang (eds.
): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 68?74,Dublin, Ireland, August 23, 2014.WordFinderCatalin MititeluStefanini / 6A Dimitrie Pompei Bd,Bucharest, Romaniacatalinmititelu@yahoo.comVerginica Barbu MititeluRACAI / 13 Calea 13 Septembrie,Bucharest, Romaniavergi@racai.roAbstractThis paper presents our relations-oriented approach to the shared task on lexical access in lan-guage production, as well as the results we obtained.
We relied mainly on the semantic andlexical relations between words as they are recorded in the Princeton WordNet, although alsoconsidering co-occurrence in the Google n-gram corpus.
After the end of the shared task we con-tinued working on the system and the further adjustments (involving part of speech informationand position of the candidate in the synset) and those results are presented as well.1 IntroductionIn this paper we present our experience in the shared task on lexical access in language production,organized as part of the CogALex workshop.
Given a list of five words (let us call them seeds), thesystem should return a word (we will call it target) which is assumed to be the most closely associatedto all the seeds.
Two remarks are worth being made here: on the one hand, what we call word is in facta word form, as inflected forms are both among the seeds and among the expected targets in the trainingand the test sets.
On the other hand, the closeness of association remains understated by the organizers.It can be understood at several levels, given our analysis of the training data: the meaning and/or theform, the syntagmatic associations, i.e.
associations of words in texts.
However, our system dealt mainlywith the semantic level.
The form level is involved only to the extent to which lexical relations (usuallyderivational relations and antonymy) in Princeton WordNet (PWN) are used.
The syntagmatic relationswe use are the co-occurrences in the Google n-gram corpus2 Our understanding of the lexical access taskHaving already established what meaning we, as speakers, want to render, the lexical choice is influencedby several factors: the person we talk to, the circumstances (place, other participants) of our discussion,the social (or even other types of) relations between the participants to the discussion.
The shared taskfocuses on the tip of the tongue (TOT) phenomenon, as rightly described in the shared task presentation:we do not remember the word ?mocha?, but we want to express the idea (i.e., the meaning) ?superiordark coffee made of beans from Arabia?.
In a real life conversation, dealing with TOT is much simpler:the speaker (the one affected by TOT) has the ability of defining the word s/he is looking for or ofenumerating some words AND specifying the relation(s) they establish with the looked for word.
Thus,we consider that the task here, consisting of being able to find the target when receiving five seeds,does not mimic the real life situation.
In fact, we deprive the system of vital information that, we, asspeakers, possess, to our great advantage reflected in our success in dealing with the TOT problem, afterall.
Moreover, given the information provided by the organizers once the results were send, the seeds thatwe received are derived from the Edinburgh Associative Thesaurus, so they are, in fact, the associationsintroduced by the users to a seed.
So, the organizers implicitly considered the association of two wordsis the same, irrespective of which of them is the seed and which is the target, which is definitely not thesame, especially if the association is a syntagmatic one.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/683 Related workIn a recent experiment (Zock and Shcwab, 2013), a set of seeds (called stimuli therein) is presented to asystem and, relying on information available in the eXtended WordNet (Mihalcea, 2001) and in DBpedia,a list of words is returned.
The authors explain the bad results by the small dimensions of the eXtendedWordNet and by the small number of syntagmatic relations it contains.
Although they emphasize thenecessity of using big corpora, with heterogenous data, to help solve the TOT problem, the conclusionsspeculate about various elements that can lead to, but do not guarantee the success:?
the big size of the corpus, the heterogeneity of the texts it contains;?
high density of relations in a network;?
the quality of the search;?
all these together.4 Our approach4.1 The dataThe training set contains a list of 2000 pairs of five seeds and the target.
They look quite heterogeneous:there are content and functional words alike, lemmas and inflected forms (see ?occurs ?
happens happenoften sometimes now?
), capitalized (sometimes unnecessarily, for example ?Nevertheless?
in the pair?however ?
but never Nevertheless when although?)
and uncapitalized words.Interestingly, two different inflected forms are targets of (partially) different sets of seeds: compare:occur ?
happen event often perfume todaywithoccurs ?
happens happen often sometimes now.This means that not only semantic relations are established between the seed and the target, but alsogrammatical ones.4.2 AssumptionsIn order to construct our system we made the assumption, supported by the manual analysis of thetraining set, that the seeds and the target are related to each other by different kinds of relations:?
semantic relations;?
co-occurrence, in either order;?
syntactic relations;?
gloss-like relations, i.e.
the target may be defined using one or more seeds;?
domain relations, i.e.
the target and at least some seeds may belong to the same domain;?
form relation, i.e.
the target and one or more seeds may display a partial identity of form (andsometimes even of the acoustic form of words);?
inflection as a relation among forms of the same word;?
etc.Given these, we were aware of the impossibility of dealing with cases involving inflected forms, someof them occurring as seeds, while one occurs as target, such as:am ?
I not is me are.In this case, an inflectional relation can be found between ?is?
and ?am?
and between ?are?
and?am?, whereas the relations between ?am?
and ?I?
and between ?am?
and ?not?
are syntagmatic (co-occurrences).
No relation can we identify between ?am?
and ?me?.694.3 ResourcesAs a consequence of the assumptions made, the language resources we used for the competition werethe Princeton WordNet (PWN) (Fellbaum, 1998) and Google n-grams corpus (Brants and Franz, 2006).The implied limitations of our approach are:?
the impossibility of dealing with pairs involving only inflected words (as in the previous example)or only functional words (as in the case: ?at ?
home by here in on?);?
no contribution made by some of the seeds in the process of finding the target;?
the partial dealing with inflected forms such as plurals, third person singular of verbs, gerunds, asthey cannot be found in PWN; the only source of information about them is the n-grams corpus;?
some combinations (although quite frequent, according to our intuitions obout the language) cannotbe found in the Google n-gram corpus.For all (2000x5) pairs seed-target in the training set we extracted from PWN the shortest relations chains,as a kind of lexical chains (Moldovan and Novischi, 2002), existing between them, disregarding the partof speech of the words.
These chains are made up of both semantic and lexical relations (as they are de-fined in the wordnet literature, i.e.
lexical relations are established between word forms, while semanticrelations are established between word meanings).
The most frequent relations chains are presented inTable 1.
Straightforwardly, the most frequent association between the seeds and the targets (occurringLexical chain Number of occurrencessynonym 548hypernym hyponym 332hyponym 328hypernym 182antonym 143similar to 128derivat 119hypernym hyponym hyponym 115hypernym hypernym hyponym 100hyponym hyponym 81hypernym hypernym hyponym hyponym 75similar to similar to 59derivat derivat 59part meronym 49hyponym derivat 46hypernym derivat 42derivat hyponym 40hypernym hyponym derivat 37domain TOPIC domain member TOPIC 36derivat hypernym hyponym 35also see 35Table 1: The most frequent relations chains between a seed and the target.548 times) is of the kind synonymy.
However, various combinations of hyponymy and hypernymy ac-count for a significant number of pairs: 1213.
Almost half of these cases (510) are solved by only one ofthe two relations (328 by hyponymy alone and 182 by hypernymy alone).
Moreover, these relations con-tribute also in chains involving the derivat relation.
So, we can consider them the most useful ones.
(Ourfinding is similar to the weight associated to these relations by Moldovan and Novischi (Moldovan and70Training setCandidate criteriaCandidatesFeatures extractorModelMaxent create modelFeaturesseeds[seeds, candidate]Figure 1: The training flowchart.Novischi, 2002), who top rank them in finding paths between related concepts for a Question Answeringsystem.)
However, they introduce a lot of noise, too, especially when the last relation in the chain ishyponymy and the node from which it starts is one with very many hyponyms.4.4 The system in the shared task competitionWe reformulated this as a classification problem.
Assuming that having a list of seeds and the list oftheir possible candidates, the problem will be solved by considering the most probable candidate as theclosest to all seeds.
We chose valid and invalid as classification categories.The system uses the machine learning technique called Maximum Entropy Modeling (MaxEnt forshort) and the features needed by MaxEnt are extracted from the kinds of relations presented above, insubsection 4.2.
In other words, we mapped each kind of relation to a feature.
The entire process has twodistinct phases: training and prediction.The training mechanism is presented in Figure 1.
For each training set entry (i.e.
the list of 5 seeds andthe expected target) a list of possible candidates is generated using the PWN relations chains presentedabove.
We called this process Candidate Criteria.
Combining each set of seeds with their candidates weextracted the list of features needed to enter into the MaxEnt process to create the model.
For instance,giving the sequence of seeds away fonder illness leave presence and two possible candi-dates absence and being we obtained the following lists of features ending with the correspondingclassification category:domain=s factotum domain=t factotum src=1 wn=an wn=he he ho hownshort=he ho validdomain=s factotum domain=t factotum src=1 wn=he ho d d invalidThe following list of features were used:71?
wn=chain: chain represents the relations chain found between any seed and the current candidate.We used short forms to label relations: for example, an stands for antonymy, he for hypernymy,ho for hyponymy, d for derivational relation;?
form=first upper when at least one seed and the candidate begin with a capital letter; we didnot allow for candidates with initial capital letter unless at least one seed had an initial capital letter;?
src=n marks the number n of seeds that reached the candidate using the PWN chains.
In the caseof the seed presence and candidate absence there are two chains linking the two words: anand he he ho ho and only presence contributes to them;?
gloss=n marks the number n of seeds that occur in the target gloss;?
n2gram=high used when any seed occurs in any Google 2-grams with the candidate;?
domain=s domain used to mark the seed domain(s);?
domain=t domain used to mark the candidate domain(s);?
wnshort=short chain here the short chain represents a reduced version of the PWN chain.For example, the chain he he ho ho can be reduced to he ho (or to a co-hyponym relation, inan extended meaning).
The reason is to create an invariant chain that can hold irrespectively of thenumber of similar consecutive relations.
This is useful in hierarchies involving many scientific orartificial nodes which are not known or simply disregarded by common speakers.
For example, thechain between hippopotamus and animal is 7 hyponyms long in PWN, whereas for a speakerthey are in a direct relation.The selection of candidates is done using exclusively the PWN relations chains with a maximun lengthof 5 relations in a chain and only the first literal from the target synset is taken into account (on theassumption that literals PWN synsets are in reverse order of their frequency of occurrence in corpora,with the first as the most frequent).
To reduce the number of possible candidates some filtering criteriaare applied before pairing them with their corresponding seeds to extract the features described above.These criteria are:?
the candidates that appear among seeds are eliminated;?
the compound terms (recognized by the use of underscore among elements) are excluded;?
the candidates should appear together with any seed among Google 5-grams with a minimum fre-quency of 5000 (occurrences).The prediction phase takes the test set and, using the model created in the training phase, produces foreach candidate a percent for each category (valid / invalid).
The candidate selection and featuresextraction are done similarly to the training phase.
The prediction phase is presented in Figure 2.
Theresult of this phase is a list of candidates (sorted in reverse order) for each set of 5 seeds in the test set.The list of results presented to the shared task organizers contains, for each set of seeds, the best rankedcandidate.4.5 Modifications after the competitionAfter the end of the competition we tried several mechanisms that could improve our results.
They were:?
adding two new features that dealt with the part of speech of the words:?
pos= s pos: the part-of-speech of the seed(s) corresponding to PWN chain that relates tothe candidate;?
pos= t pos: similar for candidate/target;?
considering more literals from synsets when creating the list of candidates.72Test set Candidate criteriaCandidatesFeatures extractorModelFeaturesMaxent classifierValid / Invalidseeds[seeds, candidate]Figure 2: The prediction flowchart.735 Results5.1 Results within the competitionOut of the total number of items (2000) only 30 of our targets matched the ones expected by the organiz-ers, so we obtained 1.50% accuracy.5.2 Improved results after the competitionAfter considering the part of speech of the words, we were able to match 51 targets, thus increasing theaccuracy to 2.55%.After considering two literals from a synset in the candidates list, the number of matches was 59, soan accuracy of 2.95%.Furthermore, if we consider the top five candidates in our list, we noticed that 140 targets could befound.Considering three or even four literals in the synsets did not improve the results (either for the bestranked candidate or for the top 5 ones).6 ConclusionsWe presented here the way we dealt with the challenging task proposed by the organizers.
Althoughinitially we intended to consider using a large corpus (ukWAC) as well for finding candidates, we foundourselves in the technical impossibility of doing so, because of the costly (timewise especially) resourcesrequired by its processing.
What is left to be checked is to what extent the lexical and syntactic patternsthat can be extracted from a corpus help us improve the results.We cannot boast good results of our approach mainly because we used only a dictionary (in the formof the PWN).
Although it was created on psychological principles about the way words are structured inthe speakers?
mind, it cannot ensure satisfying results.
At least within our approach, the contribution ofthe relations encoded in PWN is very low.
An evaluation of the type n top-ranked candidates could havea higher accuracy for our type of approach.
We could dare say that our approach was a further proof ofthe statement tested by (Zock and Shcwab, 2013): ?Words storage does not guarantee their access?.ReferencesThorsten Brants, and Alex Franz.
2006.
Web 1T 5-gram Version 1 LDC2006T13.
Philadelphia: Linguistic DataConsortium.Gemma Bel Enguix, Reinhard Rapp, and Michael Zock.
2014.
How Well Can a Corpus-Derived Co-OccurrenceNetwork Simulate Human Associative Behavior?
Proceedings of the 5th workshop on Cognitive Aspects ofComputational Language Learning (CogACLL 2014), pp.
43-48.Christiane Fellbaum.
1998.
WordNet: An Electronic Lexical Database.
MIT Press.Rada Mihalcea, and Dan Moldovan.
2001. eXtended WordNet: Progress Report.
In Proceedings of NAACLWorkshop on WordNet and Other Lexical Resources.Dan Moldovan, and Adrian Novischi.
2002.
Lexical Chains for Question Answering.
Proceedings of COLING2002.Reinhard Rapp.
2008.
The Computation of Associative Responses to Multiword Stimuli.
Proceedings of theworkshop on Cognitive Aspects of the Lexicon (CogALex 2008), pp.
102-109.Michael Zock, and Didier Schwab.
2013.
L?index, une ressource vitale pour guider les auteurs `a trouver le motbloqu?e sur le bout de la langue.
In Ressources Lexicales : contenu, construction, utilisation,?evaluation, N.Gala et M. Zock (eds.).
John Benjamins.74
