2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162?171,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsHyTER: Meaning-Equivalent Semantics for Translation EvaluationMarkus DreyerSDL Language Weaver6060 Center Drive, Suite 150Los Angeles, CA 90045, USAmdreyer@sdl.comDaniel MarcuSDL Language Weaver6060 Center Drive, Suite 150Los Angeles, CA 90045, USAdmarcu@sdl.comAbstractIt is common knowledge that translation isan ambiguous, 1-to-n mapping process, butto date, our community has produced no em-pirical estimates of this ambiguity.
We havedeveloped an annotation tool that enables usto create representations that compactly en-code an exponential number of correct trans-lations for a sentence.
Our findings show thatnaturally occurring sentences have billions oftranslations.
Having access to such large setsof meaning-equivalent translations enables usto develop a new metric, HyTER, for transla-tion accuracy.
We show that our metric pro-vides better estimates of machine and humantranslation accuracy than alternative evalua-tion metrics.1 MotivationDuring the last decade, automatic evaluation met-rics (Papineni et al, 2002; Snover et al, 2006; Lavieand Denkowski, 2009) have helped researchers ac-celerate the pace at which they improve machinetranslation (MT) systems.
And human-assisted met-rics (Snover et al, 2006) have enabled and sup-ported large-scale U.S. government sponsored pro-grams, such as DARPA GALE (Olive et al, 2011).However, these metrics have started to show signs ofwear and tear.Automatic metrics are often criticized for provid-ing non-intuitive scores ?
few researchers can ex-plain to casual users what a BLEU score of 27.9means.
And researchers have grown increasinglyconcerned that automatic metrics have a strong biastowards preferring statistical translation outputs; theNIST (2008, 2010), MATR (Gao et al, 2010) andWMT (Callison-Burch et al, 2011) evaluations heldduring the last five years have provided ample ev-idence that automatic metrics yield results that areinconsistent with human evaluations when compar-ing statistical, rule-based, and human outputs.In contrast, human-informed metrics have otherdeficiencies: they have large variance across humanjudges (Bojar et al, 2011) and produce unstable re-sults from one evaluation to another (Przybocki etal., 2011).
Because evaluation scores are not com-puted automatically, systems developers cannot au-tomatically tune to human-based metrics.Table 1 summarizes the dimensions along whichevaluation metrics should do well and the strengthsand weaknesses of the automatic and human-informed metrics proposed to date.
Our goal isto develop metrics that do well along all these di-mensions.
The fundamental insight on which ourresearch relies is that the failures of current auto-matic metrics are not algorithmic: BLEU, Meteor,TER (Translation Edit Rate), and other metrics ef-ficiently and correctly compute informative distancefunctions between a translation and one or more hu-man references.
We believe that these metrics failsimply because they have access to sets of humanreferences that are too small.
If we had access tothe set of all correct translations of a given sentence,we could measure the minimum distance between atranslation and the set.
When a translation is perfect,it can be found in the set, so it requires no editing toproduce a perfect translation.
Therefore, its scoreshould be zero.
If the translation has errors, we can162Desiderata Auto.
Manu.
HyTERMetric is intuitive N Y YMetric is computed automatically Y N YMetric is stable and reproducible from one evaluation to another Y N YMetric works equally well when comparing human and automatic outputsand when comparing rule-based, statistical-based, and hybrid enginesN Y YSystem developers can tune to the metric Y N YMetric helps developers identify deficiencies of MT engines N N YTable 1: Desiderata of evaluation metrics: Current automatic and human metrics, proposed metric.efficiently compute the minimum number of edits(substitutions, deletions, insertions, moves) neededto rewrite the translation into the ?closest?
referencein the set.
Current automatic evaluation metrics donot assign their best scores to most perfect transla-tions because the set of references they use is toosmall; their scores can therefore be perceived as lessintuitive.Following these considerations, we developed anannotation tool that enables one to efficiently createan exponential number of correct translations for agiven sentence, and present a new evaluation met-ric, HyTER, which efficiently exploits these mas-sive reference networks.
In the rest of the paper, wefirst describe our annotation environment, process,and meaning-equivalent representations that we cre-ate (Section 2).
We then present the HyTER met-ric (Section 3).
We show that this new metric pro-vides better support than current metrics for machinetranslation evaluation (Section 4) and human trans-lation proficiency assessment (Section 5).2 Annotating sentences with exponentialnumbers of meaning equivalents2.1 Annotation toolWe have developed a web-based annotation toolthat can be used to create a representation encodingan exponential number of meaning equivalentsfor a given sentence.
The meaning equivalentsare constructed in a bottom-up fashion by typingtranslation equivalents for larger and larger phrases.For example, when building the meaning equiv-alents for the Spanish phrase ?el primer ministroitaliano Silvio Berlusconi?, the annotator first typesin the meaning equivalents for ?primer ministro??
?prime-minister; PM; prime minister; head ofgovernment; premier; etc.?
; ?italiano?
?
?Italian?
;and ?Silvio Berlusconi?
?
?Silvio Berlusconi;Berlusconi?.
The tool creates a card that storesall the alternative meanings for a phrase as adeterminized FSA and gives it a name in the targetlanguage that is representative of the underly-ing meaning-equivalent set: [PRIME-MINISTER],[ITALIAN], and [SILVIO-BERLUSCONI].
Each basecard can be thought of expressing a semantic con-cept.
A combination of existing cards and additionalwords can be subsequently used to create largermeaning equivalents that cover increasingly largersource sentence segments.
For example, to createthe meaning equivalents for ?el primer ministro ital-iano?
one can drag-and-drop existing cards or typein new words: ?the [ITALIAN] [PRIME-MINISTER];the [PRIME-MINISTER] of Italy?
; to create themeaning equivalents for ?el primer ministro italianoSilvio Berlusconi?, one can drag-and-drop and type:?
[SILVIO-BERLUSCONI] , [THE-ITALIAN-PRIME-MINISTER]; [THE-ITALIAN-PRIME-MINISTER] ,[SILVIO-BERLUSCONI]; [THE-ITALIAN-PRIME-MINISTER] [SILVIO-BERLUSCONI] ?.
All meaningequivalents associated with a given card are ex-panded and used when that card is re-used to createlarger meaning-equivalent sets.The annotation tool supports, but does not en-force, re-use of annotations created by other anno-tators.
The resulting meaning equivalents are storedas recursive transition networks (RTNs), where eachcard is a subnetwork; if needed, these non-cyclicRTNs can be automatically expanded into finite-state acceptors (FSAs, see Section 3).2.2 Data and Annotation ProtocolsUsing the annotation tool, we have created meaning-equivalent annotations for 102 Arabic and 102 Chi-nese sentences ?
a subset of the ?progress set?
usedin the 2010 Open MT NIST evaluation (the average163sentence length was 24 words).
For each sentence,we had access to four human reference translationsproduced by LDC and five MT system outputs,which were selected by NIST to cover a variety ofsystem architectures (statistical, rule-based, hybrid)and performances.
For each MT output, we also hadaccess to sentence-level HTER scores (Snover et al,2006), which were produced by experienced LDCannotators.We have experimented with three annotation pro-tocols:?
Ara-A2E and Chi-C2E: Foreign language nativesbuilt English networks starting from foreign lan-guage sentences.?
Eng-A2E and Eng-C2E: English natives built En-glish networks starting from ?the best translation?of a foreign language sentence, as identified byNIST.?
Eng*-A2E and Eng*-C2E: English natives builtEnglish networks starting from ?the best transla-tion?, but had access to three additional, indepen-dently produced human translations to boost theircreativity.Each protocol was implemented independently byat least three annotators.
In general, annotators needto be fluent in the target language, familiar with theannotation tool we provide and careful not to gen-erate incorrect paths, but they do not need to be lin-guists.2.3 Exploiting multiple annotationsFor each sentence, we combine all networks thatwere created by the different annotators.
We eval-uate two different combination methods, each ofwhich combines networks N1 and N2 of two anno-tators (see an example in Figure 1):(a) Standard union U(N1, N2): The standardfinite-state union operation combines N1 and N2on the whole-network level.
When traversingU(N1, N2), one can follow a path that comes fromeither N1 or N2.
(b) Source-phrase-level union SPU(N1, N2): Asan alternative, we introduce SPU, a more fine-grained union which operates on sub-sentence seg-ments.
Here we exploit the fact that each annotatorexplicitly aligned each of her various subnetworksN1the level of approvalwasclose tozerothe approval ratepracticallythe approval levelwasclose tozerothe approval rateabout equal to(a)waszerothe approval ratethe level of approvalthe approval levelclose topracticallyabout equal to(b)N2Figure 1: (a) Finite-state union versus (b) source-phrase-level union (SPU).
The former does not contain the path?the approval level was practically zero?.for a given sentence to a source span of that sen-tence.
Now for each pair of subnetworks (S1, S2)from N1 and N2, we build their union if they arecompatible; two subnetworks S1, S2 are defined tobe compatible if they are aligned to the same sourcespan and have at least one path in common.The purpose of SPU is to create new paths by mix-ing paths from N1 and N2.
In Figure 1, for example,the path ?the approval level was practically zero?
iscontained in the SPU, but not in the standard union.We build SPUs using a dynamic programming al-gorithm that builds subnetworks bottom-up, build-ing unions of intermediate results.
Two larger sub-networks can be compatible only if their recursivesmaller subnetworks are compatible.
Each SPU con-tains at least all paths from the standard union.2.4 Empirical findingsNow that we have described how we created partic-ular networks for a given dataset, we describe someempirical findings that characterize our annotationprocess and the created networks.Meaning-equivalent productivity.
When wecompare the productivity of the three annotationprotocols in terms of the number of reference trans-lations that they enable, we observe that target lan-guage natives that have access to multiple humanreferences produce the largest networks.
The me-dian number of paths produced by one annotator un-der the three protocols varies from 7.7 ?
105 pathsfor Ara-A2E, to 1.4 ?
108 paths for Eng-A2E, to5.9 ?
108 paths for Eng*-A2E; in Chinese, the me-164dians vary from 1.0?
105 for Chi-C2E, to 1.7?
108for Eng-C2E, to 7.8?
109 for Eng*-C2E.Protocol productivity.
When we compare theannotator time required by the three protocols, wefind that foreign language natives work faster ?
theyneed about 2 hours per sentence ?
while target lan-guage natives need 2.5 hours per sentence.
Giventhat target language natives build significantly largernetworks and that bilingual speakers are in shortersupply than monolingual ones, we conclude that us-ing target language annotators is more cost-effectiveoverall.Exploiting multiple annotations.
Overall, the me-dian number of paths produced by a single annota-tor for A2E is 1.5 ?
106, two annotators (randomlypicked per sentence) produce a median number of4.7 ?
107 (Union), for all annotators together it is2.1?
1010 (Union) and 2.1?
1011 (SPU).
For C2E,these numbers are 5.2?
106 (one), 1.1?
108 (two),and 2.6?1010 (all, Union) and 8.5?1011 (all, SPU).Number of annotators and annotation time.
Wecompute the minimum number of edits and length-normalized distance scores required to rewrite ma-chine and human translations into translations foundin the networks produced by one, two, and threeannotators.
We find that the length-normalized dis-tances do not vary by more than 1% when adding themeaning equivalents produced by a third annotator.We conclude that 2-3 annotators per sentence pro-duce a sufficiently large set of alternative meaningequivalents, which takes 4-7.5 hours.
We are cur-rently investigating alternative ways to create net-works more efficiently.Grammaticality.
For each of the four human trans-lation references and each of the five machine trans-lation outputs (see Section 2.2), we algorithmicallyfind the closest path in the annotated networks ofmeaning equivalents (see Section 3).
We presentedthe resulting 1836 closest paths extracted from thenetworks (2 language pairs ?102 sentences ?9 hu-man/machine translations) to three independent En-glish speakers.
We asked each English path to belabeled as grammatical, grammatical-but-slightly-odd, or non-grammatical.
The metric is harsh: pathssuch as ?he said that withdrawing US force with-out promoting security would be cataclysmic?
arejudged as non-grammatical by all three judges al-though a simple rewrite of ?force?
into ?forces?would make this path grammatical.
We found that90% of the paths are judged as grammatical and96% as grammatical or grammatical-but-slightly-odd, by at least one annotator.
We interpret theseresults as positive: the annotation process leads tosome ungrammatical paths being created, but mostof the closest paths to human and machine outputs,those that matter from an evaluation perspective, arejudged as correct by at least one judge.Coverage.
We found it somewhat disappoint-ing that networks that encode billions of meaning-equivalent translations for a given sentence do notcontain every independently produced human ref-erence translation.
The average length-normalizededit distance (computed as described in Section 3)between an independently produced human refer-ence and the corresponding network is 19% forArabic-English and 34% for Chinese-English acrossthe entire corpus.
Our analysis shows that abouthalf of the edits are explained by several non-content words (?the?, ?of?, ?for?, ?their?, ?,?)
be-ing optional in certain contexts; several ?obvious?equivalents not being part of the networks (?that???this?
; ?so???accordingly?
); and spelling alterna-tives/errors (?rockstrom???rockstroem?).
We hy-pothesize that most of these ommissions/edits can bedetected automatically and dealt with in an appropri-ate fashion.
The rest of the edits would require moresophisticated machinery, to figure out, for example,that in a particular context pairs like ?with??
?and?or ?that???therefore?
are interchangeable.Given that Chinese is significantly more under-specified compared to Arabic and English, it is con-sistent with our intuition to see that the average mini-mal distance is higher between Chinese-English ref-erences and their respective networks (34%) thanbetween Arabic-English references and their respec-tive networks (19%).3 Measuring translation quality with largenetworks of meaning equivalentsIn this section, we present HyTER (Hybrid Trans-lation Edit Rate), a novel metric that makes use oflarge reference networks.165				 		   <ts> 			   <root>Reference RTNReorderedhypothesis	?
????????
?Levenshteinlazy composition?xLSH(x,Y)Y  	 HypothesisxconvertFigure 2: Defining the search space H(x,Y) through (lazy) composition.
x is a translation hypothesis ?where trainstation is?, Y contains all correct translations.
?x may be defined in various ways, here local reordering (k = 3) isused.HyTER is an automatically computed version ofHTER (Snover et al, 2006); HyTER computes theminimum number of edits between a translation xand an exponentially sized reference set Y , which isencoded as a Recursive Transition Network (RTN).Perfect translations have a HyTER score of 0.General Setup.
The unnormalized HyTER scoreis defined as in equation (1) where ?x is a set ofpermutations of the hypothesis x, d(x, x?)
is the dis-tance between x and a permutation x?
?typicallymeasured as the number of reordering moves be-tween the two?and LS(x?, y) is the standard Lev-enshtein distance (Levenshtein, 1966) between x?and y, defined as the minimum number of insertions,deletion, and substitutions.
We normalize uhyter bythe number of words in the found closest path.uhyter(x,Y) def= minx??
?x,y?Yd(x, x?)
+ LS(x?, y) (1)We treat this minimization problem as graph-based search.
The search space over which we mini-mize is implicitly represented as the Recursive Tran-sition Network H (see equation (2)), where ?x isencoded as a weighted FSA that represents the setof permutations of x with their associated distancecosts, and LS is the one-state Levenshtein trans-ducer whose output weight for a string pair (x,y) isthe Levenshtein distance between x, and y, and thesymbol ?
denotes composition.
The model is de-picted in Figure 2.H(x,Y) def= ?x ?
LS ?
Y (2)Permutations.
We define an FSA ?x that allowspermutations according to certain constraints.
Al-lowing all permutations of the hypothesis x wouldincrease the search space to factorial size and makeinference NP-complete (Cormode and Muthukrish-nan, 2007).
We use local-window constraints (see,e.g., Kanthak et al (2005)), where words may movewithin a fixed window of size k; these constraintsare of size O(n) with a constant factor k, where n isthe length of the translation hypothesis x.Lazy Evaluation.
For efficiency, we use lazy evalu-ation when defining the search space H(x,Y).
Thismeans we never explicitly compose ?x, LS, and Y .Parts of the composition that our inference algorithmdoes not explore are not constructed, saving compu-tation time and memory.
Permutation paths in ?xare constructed on demand.
Similarly, the referenceset Y is expanded on demand, and large parts mayremain unexpanded.1Exact Inference.
To compute uhyter(x,Y), we de-fine the composition H(x,Y) and can apply anyshortest-path search algorithm (Mohri, 2002).
Wefound that using the A* algorithm (Hart et al, 1972)was the most efficient; we devised an A* heuristicsimilar to Karakos et al (2008).Runtime.
Computing the HyTER score takes 30ms per sentence on networks by single annotators(combined all-annotator networks: 285 ms) if no1These on-demand operations are supported by the OpenFstlibrary (Allauzen et al, 2007); specifically, to expand the RTNsinto FSAs we use the Replace operation.166Arabic-English Chinese-EnglishMetric Human mean Machine mean m/h Human mean Machine mean m/h[100-0]-BLEU, 1 ref 59.90 69.14 1.15 71.86 84.34 1.17[100-0]-BLEU, 3 refs 41.49 57.44 1.38 54.25 75.22 1.39[100-0]-Meteor, 1 ref 60.13 65.70 1.09 66.81 73.66 1.10[100-0]-Meteor, 3 refs 55.98 62.91 1.12 62.95 70.68 1.12[100-0]-TERp, 1 ref 35.87 46.48 1.30 53.58 71.70 1.34[100-0]-TERp, 3 refs 27.08 39.52 1.46 41.79 60.61 1.45HyTER U 18.42 34.94 1.90 27.98 52.08 1.86HyTER SPU 17.85 34.39 1.93 27.57 51.73 1.88[100-0]-Likert 5.26 50.37 9.57 4.35 48.37 11.12Table 2: Scores assigned to human versus machine translations, under various metrics.
Each score is normalized torange from 100 (worst) to 0 (perfect translation).reordering is used.
These numbers increase to 143ms (1.5 secs) for local reordering with window size3, and 533 ms (8 secs) for window size 5.
Manyspeedups for computing the score with reorderingsare possible, but we will see below that using re-ordering does not give consistent improvements (Ta-ble 3).Output.
As a by-product of computing the HyTERscore, one can obtain the closest path itself, for er-ror analysis.
It can be useful to separately count thenumbers of insertions, deletions, etc., and inspectthe types of error.
For example, one may find thata particular system output tends to be missing the fi-nite verb of the sentence or that certain word choiceswere incorrect.4 Using meaning-equivalent networks formachine translation evaluationWe now present experiments designed to measurehow well HyTER performs, compared to other eval-uation metrics.
For these experiments, we sample 82of the 102 available sentences; 20 sentences are heldout for future use in optimizing our metric.4.1 Differentiating human from machinetranslation outputsWe score the set of human translations and machinetranslations separately, using several popular met-rics, with the goal of determining which metric per-forms better at separating machine translations fromhuman translations.
To ease comparisons across dif-ferent metrics, we normalize all scores to a numberbetween 0 (best) and 100 (worst).
Table 2 showsthe normalized mean scores for the machine trans-lations and human translations under multiple au-tomatic and one human evaluation metric (Likert).The quotient of interest, m/h, is the mean score formachine translations divided by the mean score forthe human translations: the higher this number, thebetter a metric separates machine from human pro-duced outputs.Under HyTER, m/h is about 1.9, which showsthat the HyTER scores for machine translations are,on average, almost twice as high as for human trans-lations.
Under Likert ?
a score assigned by hu-man annotators who compare pairs of sentences ata time?, the quotient is higher, suggesting that hu-man raters make stronger distinctions between hu-man and machine translations.
The quotient is lowerunder the automatic metrics Meteor (Version 1.3,(Denkowski and Lavie, 2011)), BLEU and TERp(Snover et al, 2009).
These results show thatHyTER separates machine from human translationsbetter than alternative metrics.4.2 Ranking MT systems by qualityWe rank the five machine translation systems ac-cording to several widely used metrics (see Fig-ure 3).
Our results show that BLEU, Meteor andTERp do not rank the systems in the same wayas HTER and humans do, while the HyTER met-ric yields the correct ranking.
Also, separation be-tween the quality of the five systems is higher un-der HyTER, HTER, and Likert than under alterna-tive metrics.4.3 Correlations with HTERWe know that current metrics (e.g., BLEU, Meteor,TER) correlate well with HTER and human judg-167Arabic-EnglishSize Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)1 .653 .529 .541 .512 .675 .452 .547 .643 (.661) .647 (.655)2 .645 .614 .636 .544 .706 .599 .649 .733 (.741) .735 (.732)4 .739 .782 .804 .710 .803 .782 .803 .827 (.840) .831 (.838)8 .741 .809 .822 .757 .818 .796 .833 .827 (.828) .830 (.825)16 .868 .840 .885 .815 .887 .824 .862 .888 (.890) .893 (.894)32 .938 .945 .957 .920 .948 .930 .947 .938 (.935) .940 (.936)64 .970 .973 .979 .964 .973 .966 .968 .964 (.960) .966 (.961)Chinese-EnglishSize Likert Meteor 1 Meteor 4 BLEU 1 BLEU 4 TERp 1 TERp 4 HyTER U (r5) HyTER SPU (r5)1 .713 .495 .557 .464 .608 .569 .594 .708 (.721) .668 (.681)2 .706 .623 .673 .569 .655 .639 .651 .713 (.716) .702 (.701)4 .800 .628 .750 .593 .734 .651 .726 .822 (.825) .820 (.814)8 .810 .745 .778 .783 .808 .754 .754 .852 (.856) .854 (.845)16 .881 .821 .887 .811 .884 .826 .844 .912 (.914) .914 (.908)32 .915 .873 .918 .911 .930 .851 .911 .943 (.942) .941 (.937)64 .950 .971 .976 .979 .973 .952 .970 .962 (.958) .958 (.957)Table 3: Document-level correlations of various scores to HTER.
Meteor, BLEU and TERp are shown with 1 and 4references each, HyTER is shown with the two combination methods (U and SPU), and with reordering (r5).!
"#$% !!#!%!&#'%&"#'%()#*%'&%!
"%!&%&"%&&%("%(&%)% *% '% !% &%!"#$%&'!"#$%!&#'%(!#)%((#*%"'#&%!+%!
'%(+%('%"+%"'%'+%,% !% (% "% '%!"#$%!"#$%!&#'%&"#$%&"#(%$"#(%!
)%!&%&)%&&%$)%$&%')%*% +% "% !% &%!"#$%&'&!"#$%!!#&%'$#(%'"#)%)!#*%!+%!
'%'+%''%)+%)'%,+%*% $% "% !% '%!"#$%&'()&!"#$%"&#'%"$#(%"$#)%*&#(%!
'%!+%"'%"+%*'%*+%)''%)% &% ,% $% +%!"#$%&'&(#)&!"#$%!$#&%'&#(%')#)%$&#)%!(%!
*%'(%'*%$(%$*%+(%,% "% )% &% *%!"#$%&'&(#)*&!!"!#!$"%#%&"'#%&"'#%("'#!&#!
)#%&#%)#$&#$)#*&#+# '# ,# (# )#!"#"$%&'('%")*'!"#$%!
!#!% !$#&% !$#'%&(#!%)"%)!%!"%!
!%&"%&!%("%*% +% '% )% !%!
"#$%&'&()*+&Figure 3: Five MT systems (Chinese-English), scored by8 different metrics.
The x-axis shows the five systems, they-axis shows the [100-0] normalized scores, with 0 cor-responding to a perfect translation.
(Note that the scaleis similar in all eight graphs.)
HTER and HyTER show asimilar pattern and similar ranking of the systems.ments on large test corpora (Papineni et al, 2002;Snover et al, 2006; Lavie and Denkowski, 2009).We believe, however, that the field of MT will bebetter served if researchers have access to metricsthat provide high correlation at the sentence level aswell.
To this end, we estimate how well various met-rics correlate with the Human TER (HTER) metricfor corpora of increasingly larger sizes.Table 3 shows Pearson correlations betweenHTER and various metrics for scoring documentsof size s =1, 2, 4, .
.
.
, and 64 sentences.
To getmore reliable results, we create 50 documents persize s, where each document is created by select-ing s sentences at random from the available 82 sen-tences.
For each document, there are 5 translationsfrom different systems, so we have 250 translateddocuments per size.
For each language and size, wescore the 250 documents under HTER and the othermetrics and report the Pearson correlation.
Our re-sults show that for large documents, all metrics cor-relate well with HTER.
However, as the sizes of thedocuments decrease, and especially at the sentencelevel, HyTER provides especially high correlationwith HTER as compared to the other metrics.
Asa side note, we can see that using reordering whencomputing the HyTER score does not give consis-tently better results ?
see the (r5) numbers, whichsearched over hypothesis permutations within a lo-cal window of size 5; this shows that most reorder-ings are already captured in the networks.
In all ex-periments, we use BLEU with plus-one smoothing(Lin and Och, 2004).5 Using meaning-equivalent networks forhuman translation evaluationIn this section, we present a use case for the HyTERmetric outside of machine translation.1685.1 Setup and problemLanguage Testing units assess the translation profi-ciency of thousands of applicants interested in per-forming language translation work for the US Gov-ernment.
Job candidates typically take a written testin which they are asked to translate four passages(i.e., paragraphs) of increasing difficulty into En-glish.
The passages are at difficulty levels 2, 2+, 3,and 4 on the Interagency Language Roundable (ILR)scale.2 The translations produced by each candidateare manually reviewed to identify mistranslation,word choice, omission, addition, spelling, grammar,register/tone, and meaning distortion errors.
Eachpassage is then assigned one of five labels: Success-fully Matches the definition of a successful transla-tion (SM); Mostly Matches the definition (MM); In-termittently Matches (IM); Hardly Matches (HM);Not Translated (NT) for anything where less than50% of a passage is translated.We have access to a set of more than 100 rules thatagencies practically use to assign each candidate anILR translation proficiency level: 0, 0+, 1, 1+, 2, 2+,3, and 3+.
For example, a candidate who producespassages labeled as SM, SM, MM, IM for difficultylevels 2, 2+, 3, and 4, respectively, is assigned anILR level of 2+.We investigate whether the assessment processdescribed above can be automated.
To this end, weobtained the exam results of 195 candidates, whereeach exam result consists of three passages trans-lated into English by a candidate, as well as themanual rating for each passage translation (i.e., thegold labels SM, MM, IM, HM, or NT).
49 exam re-sults are from a Chinese exam, 71 from a Russianexam and 75 from a Spanish exam.
The three pas-sages in each exam are of difficulty levels 2, 2+, and3; level 4 is not available in our data set.
In eachexam result, the translations produced by each can-didate are sentence-aligned to their respective for-eign sentences.
We applied the passage-to-ILR map-ping rules described above to automatically create agold overall ILR assessment for each exam submis-sion.
Since the languages used here have only 3 pas-sages each, some rules map to several different ILRratings.
Table 4 shows the label distribution at the2See http://www.govtilr.org/skills/AdoptedILRTranslationGuidelines.htm.Lang.
0 0+ 1 1+ 2 2+ 3 3+Chi.
0.0 8.2 40.8 65.3 59.2 10.2 4.1 0.0Rus.
0.0 2.8 12.7 42.3 60.6 46.5 25.4 5.6Spa.
0.0 1.3 33.3 66.7 88.0 24.0 4.0 0.0All 0.0 3.6 27.7 57.4 70.8 28.7 11.8 2.1Table 4: Percentage of exams with ILR levels 0, 0+, .
.
.
,3+ as gold labels.
Multiple levels per exam possible.ILR assessment level across all languages.5.2 ExperimentsWe automatically assess the proficiency of candi-dates who take a translation exam.
We treat this as aclassification task where, for each translation of thethree passages, we predict the three passage assess-ment labels as well as one overall ILR rating.In support of our goal, we asked annotators to cre-ate an English HyTER network for each foreign sen-tence in the exams.
These HyTER networks thenserve as English references for the candidate transla-tions.
The median number of paths in these HyTERnetworks is 1.6?
106 paths/network.In training, we observe a set of submitted examtranslations, each of which is annotated with threepassage-level ratings and one overall ILR rating.We develop features (Section 5.3) that describe eachpassage translation in its relation to the HyTER net-works for the passage.
We then train a classifier topredict passage-level ratings given the passage-levelfeatures that describe the candidate translation.
Asclassifier, we use a multi-class support-vector ma-chine (SVM, Krammer and Singer (2001)).
In de-coding, we observe a set of exams without their rat-ings, derive the features and use the trained SVM topredict ratings of the passage translations.
We thenderive an overall ILR rating based on the predictedpassage-level ratings.
Since our dataset is small werun 10-fold cross-validation.5.3 FeaturesWe define features describing a candidate?s transla-tion with respect to the corresponding HyTER refer-ence networks.
Each of the feature values is com-puted based on a passage translation as a whole,rather than sentence-by-sentence.
As features, weuse the HyTER score, as well as the number of in-sertions, deletions, substitutions, and insertions-or-deletions.
We use these numbers normalized by the169Level Measure Baseline HyTER-enabledAll Accuracy 72.31 90.772 or betterPrecision 85.62 82.11Recall 84.93 98.63F1 85.27 89.62Table 5: Predicting final ILR ratings for candidate exams.length of the passage, as well as unnormalized.
Wealso use n-gram precisions (for n=1,.
.
.
,20) as fea-tures.5.4 ResultsWe report the accuracy on predicting the overall ILRrating of the 195 exams (Table 5).
The results in 2or better show how well we predict a performancelevel of 2, 2+, 3 or 3+.
It is important to retrievesuch relatively good exams with high recall, so thata manual review QA process can confirm the choiceswhile avoid discarding qualified candidates.
The re-sults show that high recall is reached while preserv-ing good precision.
Since we have several possiblegold labels per exam, precision and recall are com-puted similar to precision and recall in the NLP taskof word alignment.
F1(P,R) = 2PRP+R is the har-monic mean of precision and recall.
The row Allshows the accuracy in predicting ILR performancelabels overall.
As a baseline method we assign themost frequent label per language; these are 1+ forChinese, and 2 for Russian and Spanish.
The resultsin Table 5 strongly suggest that the process of as-signing a proficiency level to human translators canbe automated.6 DiscussionWe have introduced an annotation tool and processthat can be used to create meaning-equivalent net-works that encode an exponential number of trans-lations for a given sentence.
We have shown thatthese networks can be used as foundation for devel-oping improved machine translation evaluation met-rics and automating the evaluation of human trans-lation proficiency.
We plan to release the OpenMTHyTER networks to the community after the 2012NIST Open MT evaluation.
We believe that ourmeaning-equivalent networks can be used to supportinteresting research programs in semantics, para-phrase generation, natural language understanding,generation, and machine translation.AcknowledgmentsWe thank our colleagues and the anonymous review-ers for their valuable feedback.
This work was par-tially sponsored by DARPA contract HR0011-11-C-0150 and TSWG contract N41756-08-C-3020 toLanguage Weaver Inc.ReferencesC.
Allauzen, M. Riley, J. Schalkwyk, W. Skut, andM.
Mohri.
2007.
OpenFst: a general and efficientweighted finite-state transducer library.
In Proceed-ings of the 12th international conference on Implemen-tation and application of automata, page 1123.O.
Bojar, M.
Ercegovc?evic?, M. Popel, and O. Zaidan.2011.
A grain of salt for the wmt manual evaluation.In Proceedings of the Sixth Workshop on StatisticalMachine Translation, pages 1?11, Edinburgh, Scot-land, July.
Association for Computational Linguistics.C.
Callison-Burch, Ph.
Koehn, Ch.
Monz, and O. Zaidan.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 22?64,Edinburgh, Scotland, July.
Association for Computa-tional Linguistics.G.
Cormode and S. Muthukrishnan.
2007.
The stringedit distance matching problem with moves.
ACMTransactions on Algorithms (TALG), 3(1):1?19.M.
Denkowski and A. Lavie.
2011.
Meteor 1.3: Au-tomatic Metric for Reliable Optimization and Evalua-tion of Machine Translation Systems.
In Proceedingsof the EMNLP 2011 Workshop on Statistical MachineTranslation.Q.
Gao, N. Bach, S. Vogel, B. Chen, G. Foster, R. Kuhn,C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,et al, 2010.
Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metrics(MATR), pages 121?126.P.E.
Hart, N.J. Nilsson, and B. Raphael.
1972.
Cor-rection to ?A formal basis for the heuristic determina-tion of minimum cost paths?.
ACM SIGART Bulletin,pages 28?29, December.
ACM ID: 1056779.S.
Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.2005.
Novel reordering approaches in phrase-basedstatistical machine translation.
In Proceedings of theACL Workshop on Building and Using Parallel Texts,pages 167?174.D.
Karakos, J. Eisner, S. Khudanpur, and M. Dreyer.2008.
Machine translation system combination using170ITG-based alignments.
Proc.
ACL-08: HLT, Short Pa-pers (Companion Volume), page 8184.K.
Krammer and Y.
Singer.
2001.
On the algorithmic im-plementation of multi-class SVMs.
In Proc.
of JMLR.A.
Lavie and M. Denkowski.
2009.
The meteor metricfor automatic evaluation of machine translation.
Ma-chine Translation, 23(2-3):105?115.L.
I. Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions and reversals.
Soviet PhysicsDoklady, 10:707?710.C.Y.
Lin and F.J. Och.
2004.
Orange: a method for eval-uating automatic evaluation metrics for machine trans-lation.
In Proceedings of the 20th international con-ference on Computational Linguistics, page 501.
As-sociation for Computational Linguistics.M.
Mohri.
2002.
Semiring frameworks and algorithmsfor shortest-distance problems.
Journal of Automata,Languages and Combinatorics, 7(3):321?350.J.
Olive, C. Christianson, and J. McCary, editors.
2011.Handbook of Natural Language Processing and Ma-chine Translation.
DARPA Global Autonomous Lan-guage Exploitation.
Springer.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages311?318, Philadelphia, Pennsylvania, USA, July.
As-sociation for Computational Linguistics.M.
Przybocki, A.
Le, G. Sanders, S. Bronsart, S. Strassel,and M. Glenn, 2011.
GALE Machine TranslationMetrology: Definition, Implementation, and Calcula-tion, chapter 5.4, pages 583?811.
Springer.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings of theAssociation for Machine Translation in the Americas,pages 223?231.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009.Fluency, adequacy, or HTER?
Exploring different hu-man judgments with a tunable MT metric.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation at the 12th Meeting of the EACL.171
