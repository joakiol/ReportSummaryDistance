Experimenting with the Interaction between Aggregation andText StructuringHua ChengDivision of InformaticsUniversity of Edinburgh80 South Bridge, Edinburgh EH1 1HN, UKEmail: huacOdai ,  ed.
ac.
ukAbst rac tIn natural anguage generation, different gener-ation tasks often interact with each other in acomplex way, which is hard to capture in thepipeline architecture described by Reiter (Re-iter, 1994).
This paper focuses on the interac-tion between a specific type of aggregation andtext planning, in particular, maintaining localcoherence, and tries to explore what preferencesexist among the factors related to the two tasks.The evaluation result shows that it is these pref-erences that decide the quality of the generatedtext and capturing them properly in a genera-tion system could lead to coherent text.1 In t roduct ionIn automatic natural language generation(NLG), various versions of the pipeline archi-tecture specified by Reiter and Dale ((Reiter,1994) and (Reiter and Dale, 1997)) are usuallyadopted.
They successfully modularise the gen-eration problem, but fail to capture the complexinteractions between different modules.
Takeaggregation as an example.
It combines implerepresentations to form a complex one, whichin the mean time leads to a shorter text as awhole.
There is no consensus as to where aggre-gation should happen and how it is related toother generation processes ((Wilkinson, 1995)and (Reape and Mellish, 1999)).We think that the effect of aggregationspreads from text planning to sentence reali-sation.
The task of text planning is to se-lect the relevant information to be expressedin the text and organise it into a hierarchi-cal structure which captures certain discoursepreferences such as preferences for global co-herence (e.g.
the use of RST relations (Mannand Thompson, 1987)) and local coherence (e.g.center transitions as defined in Centering The-ory (Grosz et al, 1995)).
Aggregation affectstext planning by taking away facts from a se-quence featuring preferred center movements forsubordination.
As a result, the preferred cen-ter transitions in the sequence are cut off.
Forexample, comparing the two descriptions of anecklace in Figure 1, 2 is less coherent han 1because of the shifting from the description ofthe necklace to that of the designer.
To avoidthis side effect, aggregation should be consid-ered in text planning, which might produce adifferent planning sequence.Aggregation is also closely related to the taskof referring expression generation.
A referringexpression is used not only for identifying a ref-erent, but also for providing additional infor-mation about the referent and expressing thespeaker's emotional attitude toward the refer-ent (Appelt, 1985).
The syntactic form of a re-ferring expression affects how much additionalinformation can be expressed, but it can only bedetermined after sentence planning, when theordering between sentences and sentence com-ponents has been decided.
This demands thatthe factors relevant o referring expression gen-eration and aggregation be considered at thesame time rather than sequentially to generatereferring expressions capable of serving multiplegoals.In this paper, we are concerned with a specifictype of aggregation called embedding, whichshifts one clause to become a component withinthe structure of an NP in another clause.
Wefocus on the interaction between maintaininglocal coherence and embedding, and describehow to capture this interaction as preferencesamong related factors.
We believe that if thesepreferences are used properly, we would be ableto generate more flexible texts without sacri-ficing quality.
We implemented the preferencesI.
This necklace is in the Arts and Crafts style.
Arts and Crafts style jewels usually havean elaborate design.
They tend to have floral motifs.
For instance, this necklace has floralmotifs.
It was designed by Jessie King.
King once lived in Scotland.2.
This necklace, which was designed by Jessie King, is in the Arts and Crafts style.
Artsand Crafts style jewels usually have an elaborate design.
They tend to have floral motifs.For instance, this necklace has floral motifs.
King once lived in Scotland.Figure 1: An aggregation examplein an experimental generation system based ona Genetic Algorithm to produce museum de-scriptions, which describe museum objects ondisplay.
The result shows that the system cangenerate a number texts of similar qualities tohuman written texts.2 Embedd ing  in a GA Text  P lannerTo experiment with the interaction betweenmaintaining local coherence and embedding, weadopt the text planner based on a Genetic Al-gorithm (GA) as described in (Mellish et al,1998).
The task is, given a set of facts and aset of relations between facts, to produce a le-gal rhetorical structure tree using all the factsand some relations.
A fragment of the possibleinput is given in Figure 2.A genetic algorithm is suitable for such aproblem because the number of possible com-binations is huge, the search space is not per-fectly smooth and unimodal, and the genera-tion task does not require a global optimumto be found.
The algorithm of (Mellish et al,1998) is basically a repeated two step process -first sequences of facts are generated by apply-ing GA operators (crossover and mutation) andthen the RS trees built from these sequences areevaluated.
This provides a mechanism to inte-grate various planning factors in the evaluationfunction and search for the best combinationsof them.To explore the whole space of embedding, wedid not perform embedding on structured factsor on adjacent facts in a linear sequence be-cause these might restrict the possibilities andeven miss out good candidates.
Instead, we de-fined an operator called embedding mutation.It randomly selects two units (say Ui and Uk)mentioning a common entity from a sequence\[U1,U2,...,Ui,...,Uk,...,Uu\] to form a list \[Ui,Uk\]representing an embedding.
The list substitutesthe original unit Ui to produce a new sequence\[U1,U2,...,\[Ui,Uk\],...,Un\], which is then evalu-ated and ordered in the population.3 Captur ing  the  In teract ions  asPre ferencesA key requirement of the GA approach is theability to evaluate the quality of a possible so-lution.
We claim that it is the relative prefer-ences among factors rather than each individ-ual factor that play the crucial role in decidingthe quality.
Therefore, if we can capture thesepreferences in a generation system properly, wewould be able to produce coherent text.
In thissection, we first discuss the preferences amongfactors related to text planning, based on whichthose for embedding can be introduced.3.1 P re ferences  for global coherenceFollowing the assumption of RST, a text is glob-ally coherent if a hierarchical structure like anRST tree can be constructed from the text.
Inaddition to the semantic relations and the Jointrelation 1 used in (Mellish et al, 1998), we as-sume a Conjunct or Disjunct relation betweentwo facts with at least two identical compo-nents, so that semantic parataxis can be treatedas a combining operation on two subtrees con-nected by the relation.Embedding a Conjunct relation inside an-other semantic relation is not preferred becausethis could convey wrong information, for exam-ple, in Figure 3, 2 cannot be used to substitute1.
Also a semantic relation is preferred to beused whenever possible.
Here is the preferencesconcerning the use of relations, where "A>B"means that A is preferred over B:1In (Mellish et al, 1998), a Joint relat ion is used toconnect every two text spans that  do not have a normalsemant ic  relat ion in between.2fact (choker, is, broad, fact_no de- 1 ).fact('Queen Alexandra',wore,choker,fact_node-2).fact (choker,'can cover',scar,fact_node-3).fact(band,'might be made of',plaques,fact_node-4).fact(band/might be made of',panels,fact_node-5).fact(scar,is/on her neck',fact_node-6).rel(in_that_reln,fact_node-2,fact_node-3, ~).rel(conjunct,fact_node-4,fact_node-5,\[\]).Figure 2: A fragment of the input to the GA text planner1.
The necklace is set with jewels in that it features cabuchon stones.
Indeed, an Arts andCrafts style jewel usually uses cabuchon stones.
An Arts and Crafts style jewel usually usesoval stones.2.
The necklace is set with jewels in that it features cabuchon stones.
Indeed, an Arts andCrafts style jewel usually uses cabuchon stones and oval stones.Figure 3: Conjunct and semantic relationsHeur i s t i c  1 Preferences among features forglobal coherence:a semantic relation > Conjunct > Joint >parataxis in a semantic relation3.2 Preferences for local coherenceIn Centering Theory, Rule 2 specifies prefer-ences among center transitions in a locally co-herent discourse segment: sequences of continu-ation are preferred over sequences of retaining,which are then preferred over sequences of shift-ing.
Instead of claiming that this is the bestmodel, we use it simply as an example of a lin-guistic model being used for evaluating factorsfor text planning.Another type of center transition that ap-pears frequently in museum descriptions i asso-ciate shifting, where the description starts withan object and then moves to a closely associatedobject or perspectives of that object.
Our ob-servation from museum descriptions shows thatassociate shifting is preferred by human writ-ers to all other types of movements except forcenter continuation.Oberlander et al (1999) define yet anothertype of transition called resuming, where an ut-terance mentions an entity not in the immedi-ate previous utterance, but in the previous dis-course.
The following is the preferences amongfeatures for local coherence:Heur i s t i c  2 Preferences among center transi-tions and semantic relations:Continuation > Associate shifting > Retain-ing > Shifting > Resuminga semantic relation > Joint + Continuation3.3 Preferences for embeddingFor a randomly produced embedding, we mustbe able to judge its quality.
We distinguish be-tween a good, normal and bad embedding basedon the features it bears 2.
A good embedding isone satisfying all following conditions:1.
The referring expression is an indefinite,a demonstrative or a bridging description (asdefined in (Poesio et al, 1997)).2.
The embedded part can be realised as anadjective or a prepositional phrase (Scott andde Souza, 1990) 3.3.
The embedded part does not lie betweentext spans connected by semantic parataxis orhypotaxis (Cheng, 1998).4.
There is an available syntactic slot to holdthe embedded part.2We do not claim that the set of features is complete.In a different context, more criteria might have to beconsidered.3We assume that syntactic onstraints have been in-serted before in text planning, using Meteer's Text Struc-ture (Meteer, 1992) for example.3A good embedding is highly preferred andshould be performed whenever possible.
A nor-mal embedding is one satisfying condition 1,3 and 4 and the embedded part is a relativeclause.
A bad embedding consists of all thoseleft.To decide the preferences among embeddingsand center transitions, let's look at the para-graphs in Figure 1 again.
The only differencebetween them is the position of the sentence"This necklace was designed by Jessie King",which can be represented in terms of features oflocal coherence and embedding as follows:the last three sentences in 1: Joint +Continuation + Joint + Shiftingthe last two sentences plus embeddingin 2: Joint + Resuming + NormalembeddingSince 1 is preferred over 2, we have the fol-lowing heuristics:Heur i s t i c  3 Preferences among features forembedding and center transition:Continuation + Shifting + Joint > Resuming+ Normal embeddingGood embedding > Normal embedding >Joint > Bad embeddingGood embedding > Continuation + Joint4 Jus t i fy ing  the  Eva luat ion  Funct ionWe have illustrated the linguistic theories thatcan be used to evaluate a text.
However, theyonly give evidence in qualitative terms.
For aGA-based planner to work, we have to comeup with actual numbers that can be used toevaluate an RS tree.We extended the existing scoring scheme of(Mellish et al, 1998) to account for featuresfor local coherence, embedding and semanticparataxis.
This resulted in the rater 1 in Ta-ble 14 , which satisfied all the heuristics intro-duced in Section 3.We manually broke down four human writtenmuseum descriptions into individual facts andrelations and reconstructed sequences of factswith the same orderings and aggregations as in4The table only shows the features we are concernedwith in this paper.the original texts.
We then used the evaluationfunction of the GA planner to score the RS treesbuilt from these sequences.
In the meantime,we ran the GA algorithm for 5000 iterations onthe facts and relations for 10 times.
All humantexts were scored among the highest and ma-chine generated texts can get scores very closeto human ones sometimes (see Table 2 for theactual scores of the four texts).
Since the fourhuman texts were written and revised by mu-seum experts, they can be treated as "nearlybest texts".
The result shows that the evalu-ation function based on our heuristics can findgood combinations.To justify our claim that it is the preferencesamong generation factors that decide the co-herence of a text, we fed the heuristics into aconstraint-based program, which produced a lotof raters satisfying the heuristics.
One of themis given in Table 1 as the rater 2.
We then gen-erated all possible combinations, including em-bedding, of seven facts from a human text andused the two raters to score each of them.
Thetwo distributions are shown in Figure 4.The qualities of the generated texts are nor-mally distributed according to both raters.
Thetwo raters assign different scores to a text asthe means of the two distributions are quite dif-ferent.
There is also slight difference in stan-dard deviations, where the deviation of Rater2 is bigger and therefore it has more distin-guishing power.
Despite these differences, thebehaviours of the two raters are indeed verysimilar as the two histograms are of roughlythe same shape, including the two right halveswhich tell us how many good texts there are andif they can be distinguished from the rest.
Thedifference in standard deviations is not signifi-cant at all.
So the distributions of the scoresfrom the two raters show that they behave verysimilarly in distinguishing the qualities of textsfrom the same population.As to what extent the two raters agree witheach other, we drew the scatterplot of thescores, which showed a strong positive linearcorrelation between the variables representingthe two scores.
That is, the higher the scorefrom rater 1 for a given text of the population,the higher the score from rater 2 tends to be.We also calculated the Pearson correlation co-efficient between the two raters and the corre-4Features/FactorsI Values11 2Semantic relationsa Joint -20 -46a Conjunct or Disjunct 10 11a relation other than Joint, Conjunct or Disjunct 21 69a Conjunct inside another semantic relation -50 -63a precondition ot satisfied -30 -61Center transitionsa Continuation 20 7an Associate shifting 16 1a Shifting 14 -3resuming a previous center 6 -43Embeddinga Good embeddinga Normal embeddinga Bad embeddingOtherstopic not mentioned in the first sentence6 33 0-30 -64-10 -12Table 1: Two different raters satisfying the same constraintsscores of the human textsl~ighest scores of the generated textsaverage scores of the generated textstext 1 text2 text3 text4170 22 33 24167 24 31 25125.7 18.9 26.1 9.3Table 2: The scores oflation was .9567.
So we can claim that for thisdata, the scores from rater 1 and rater 2 corre-late, and we have fairly good chance to believeour hypothesis that the two raters, randomlyproduced in a sense, agree with each other onevaluating the text and they measure basicallythe same thing.Since the two raters are derived from theheuristics in Section 3, the above result partiallyvalidates our claim that it is the relevant pref-erences among factors that decide the quality ofthe generated text.5 Summary  and Future  workThis paper focuses on the complex interac-tions between embedding and planning local co-herence, and tries to capture the interactionsas preferences among related features.
Theseinteractions cannot be easily modelled in apipeline architecture, but the GA-based archi-tecture offers a mechanism to coordinate themin the planning of a coherent ext.
The resultshows to some extent that capturing the inter-four human written textsactions properly in an NLG system is importantto the generation of coherence text.Our experiment could be extended in manyaspects, for example, validating the evaluationfunction through empirical analysis of humanassessments of the generated texts, and experi-menting with the interaction between aggrega-tion and referring expression generation.
Thearchitecture based on the Genetic Algorithmcan also be used for testing interactions betweenor within other text generation modules.
Togeneralise our claim, a larger scale experimentis needed.Acknowledgement  This research is sup-ported by a University of Edinburgh Stu-dentship.Re ferencesDouglas Appelt.
1985.
Planning english refer-ring expressions.
Artificial Intelligence, 26:1-33.Hua Cheng.
1998.
Embedding new informa-tion into referring expressions.
In Proceedings5Figure 4: Histogram ofthe scores from rater 1 (top)and rater 2 (bottom)of COLING-ACL'98, pages 1478-1480, Mon-treal, Canada.Barbara Grosz, Aravind Joshi, and Scott We-instein.
1995.
Centering: A framework formodelling the local coherence of discourse.Computational Linguistics, 21 (2):203-226.William Mann and Sandra Thompson.
1987.Rhetorical Structure Theory: A Theoryof Text Organization.
Technical ReportISI/RR-87-190, Information Sciences Insti-tute, University of Southern California.Chris Mellish, Alistair Knott, Jon Oberlander,and Mick O'Donnell.
1998.
Experiments us-ing stochastic search for text planning.
InProceedings o\] the 9th International Work-shop on Natural Language Generation, On-tario, Canada.Marie Meteer.
1992.
Expressibility and TheProblem of Efficient Text Planning.
Commu-nication in Artificial Intelligence.
Pinter Pub-lishers Limited, London.Jon Oberlander, Alistair Knott, Mick O'Don-nell, and Chris Mellish.
1999.
Beyond elabo-ration: Generating descriptive t xts contain-ing it-clefts.
In T Sanders, J Schilperoord,and W Spooren, editors, Text Representation:Linguistic and Psycholinguistic Aspects.
Ben-jamins, Amsterdam.Massimo Poesio, Renata Vieira, and SimoneTeufel.
1997.
Resolving bridging references inunrestricted text.
Research paper hcrc-rp87,Centre for Cognitive Science, University ofEdinburgh.Michael Reape and Chris Mellish.
1999.
Justwhat is aggregation a yway?
In Proceedingsof the 7th European Workshop on NaturalLanguage Generation, pages 20-29, Toulouse,France.Ehud Reiter and Robert Dale.
1997.
Buildingapplied natural language generation systems.Natural Language Engineering, 3(1):57-87.Ehud Reiter.
1994.
Has a consensus nl gen-eration architecture appeared, and is it psy-cholinguistically plausible?
In Proceedings ofthe 7th International Workshop on NaturalLanguage Generation.Donia Scott and Clarisse Sieckenius de Souza.1990.
Getting the Message Across in RST-based Text Generation.
In R. Dale, C. Mel-lish, and M. Zock, editors, Current Researchin Natural Language Generation, pages 47-73.
Academic Press.John Wilkinson.
1995.
Aggregation i NaturalLanguage Generation: Another Look.
Tech-nical report, Computer Science Department,University of Waterloo.G
