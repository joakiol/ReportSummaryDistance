Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 241?244,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsValidation of a Dialog System for Language LearnersAlicia Sagae, W. Lewis Johnson, Stephen BodnarAlelo, Inc.Los Angeles, CA{asagae,ljohnson,sbodnar}@alelo.comAbstractIn this paper we present experiments related tothe validation of spoken language understand-ing capabilities in a language and culture train-ing system.
In this application, word-levelrecognition rates are insufficient to character-ize how well the system serves its users.
Wepresent the results of an annotation exercisethat distinguishes instances of non-recognitiondue to learner error from instances due to poorsystem coverage.
These statistics give a moreaccurate and interesting description of systemperformance, showing how the system couldbe improved without sacrificing the instruc-tional value of rejecting learner utteranceswhen they are poorly formed.1 IntroductionConversational practice in real-time dialogs withvirtual humans is a compelling element of train-ing systems for communicative competency,helping learners acquire procedural skills in addi-tion to declarative knowledge (Johnson, Rickel etal.
2000).
Alelo's language and culture trainingsystems allow language learners to engage insuch dialogs in a serious game environment,where they practice task-based missions in newlinguistic and cultural settings (Barrett andJohnson 2010).
To support this capability, Aleloproducts apply a variety of spoken dialog tech-nologies, including automatic speech recognition(ASR) and agent-based models of dialog thatcapture theories of politeness (Wang andJohnson 2008), and cultural expectations (John-son, 2010; (Sagae, Wetzel et al 2009).To properly assess these dialog systems, wemust take several issues into account.
First, us-ers who interact with these systems are languagelearners, who can be expected occasionally toproduce invalid speech, and who may benefitfrom the corrective signal of recognizer rejec-tion.
Second, word recognition is one step in asocial simulation pipeline that allows virtual hu-mans to respond to learner input (Samtani,Valente et al 2008).
Consequently, the systemgoals extend beyond word-level decoding intomeaning interpretation and response planning.As a result, Word Error Rate (WER) and re-lated metrics, such as those described by Hunt(1990) for evaluating ASR performance, are in-sufficient to characterize how well the speechunderstanding component of the dialog systemperforms.
We need a meaningful way to accountfor the performance of the dialog system as awhole, which can distinguish acceptable interpre-tation failures from unacceptable ones.We present a validation process for assessingspeech understanding in dialog systems for lan-guage training applications.
The process in-volves annotation of historical user data acquiredfrom learner interaction with the Tactical Lan-guage and Culture Training System (Johnson andValente 2009).
The results indicate that learnermistakes make up the majority of non-recognitions, confirming the hypothesis that?recognition failures?
are a complex category ofevents that are only partly explained by lack ofcoverage in speech understanding componentssuch as ASR.2 Metrics for Dialog System AssessmentSpeech recognition errors in the dialog systemresult in at least two sub-types of error: non-understandings, where the system cannot find aninterpretation for user input, and misunderstand-ings, where the system finds an interpretationthat does not match the learner?s intent (McRoyand Hirst 1995).These classes generalize beyond speech rec-ognition to speech understanding.
This is shownin Figure 1, where "act" refers to a message241modeled along the lines of Traum & Hinkleman(1992).
In the context of speech-enabled dialogsystems, the understanding task is more critical,since it more closely models the overall successof the communication between the human userand the virtual human interlocutor.Figure 1.
Speech understanding pipeline.As a result, a variety of metrics have been sug-gested that assess performance at the level ofintent recognition, rather than word recognition.Examples include PARADISE (Walker, Litmanet al 1998) and the work of Suendermann, Lis-combe, et al(2009).We propose an assessment procedure that usesexpert annotation to compare speaker-intendedacts to the acts recognized by the speech-understanding component of the dialog system.Like the metrics mentioned above, it evaluatesthe system's ability to recognize intent as well aswords.
However we focus our attention on adap-tations that characterize interactions with lan-guage learners, who are a special type of user.As a result, we can distinguish system non-understandings and mis-understandings that aredue to system error from those that are caused bylearner mistakes.Our goal is to use this information to reducemis-understandings due to system errors; suchmis-understandings can yield confusing dialogbehavior, causing learners to lose confidence inthe accuracy of the speech recognizer.
Non-understandings may be less serious, since theyoccur in real life between learners and nativespeakers.
Non-understandings due to learner er-ror may be beneficial if the additional practicethat results from non-understandings leads to anincrease in language accuracy.3 ProcedureTo assess performance, we recruited two annota-tors to provide judgments on historical log dataregarding the accuracy of the system interpreta-tions at multiple levels, including word-levelrecognition and act recognition.3.1 Annotation team and data collectionThe annotators are Alelo team members withexpertise in General Linguistics, French andSpanish Linguistics, Translation, and TeachingEnglish as a Foreign Language (TEFL).
Theircombined experience in content authoring forAlelo courses covers more than 10 languages.The data was collected in the fall of 2009 aspart of a field test for Alelo courses teaching Ira-qi Arabic and Sub-Saharan French.
Naval per-sonnel at several sites around the United Statesvolunteered to complete the courses in self-study.
The training systems generated user logs,capturing recordings of learner turns and systemrecognition results for each turn.
From theselogs, samples of beginner-level and intermediate-level dialogs were selected and anonymized forannotation.3.2 Speech understanding accuracyThe point of this exercise is to explore how of-ten the system fails to understand what a learneris trying to say during spoken dialog.Annotation was performed on a total of 345learner turns.
To determine the act-level accura-cy of the speech understanding system, annota-tors listened to the recording of each turn andselected the act they heard from a drop-down list.The results were compared with the system-perceived act result recovered from the log.Speech understanding rejections, where the sys-tem determined that no meaningful act could beperceived from the learner turn, were labeledwith the act name "garbage".
Human annotatorscould also select the garbage act for recordingswhere no meaningful interpretation could bemade.4 ResultsTo analyze the results, we measure system ac-curacy at two levels.
First, we determine accura-cy on distinguishing meaningful utterances (ut-terances that annotators labeled with an act) fromnon-meaningful speech attempts (labeled as gar-bage by annotators).
The results are shown inTable 1.
Inter-annotator agreement as measuredby Cohen's Kappa on the first task is 0.8, indicat-ing good agreement between our two experts.Next, we examine the utterances classified asmeaningful by both the system and the annota-242tors, to assess correctness at a finer level of gra-nularity: given that the system identified the ut-terance as meaningful, did the meaning that itassigned match our annotators?
judgments?
Ifnot, mis-understandings occur.
These results areshown in Table 2.
System mis-understandingsover all meaningful utterances.
Inter-annotatoragreement on the non-understanding classifica-tion task was 0.73, suggesting that there is sub-stantial agreement between our raters.4.1 Correct interpretationsNumbers in the bottom-right cells of Table 1and the first row of Table 2 represent correct sys-tem interpretations, according to an annotator.
Inthese instances, the annotator assigned an act tothe turn that matched the system interpretationfor that turn (in Table 2), or both the annotatorand the system assigned the label "garbage" (inTable 1).
On average these examples account for62% of the total turns.An important result from this procedure is thatit reveals the class of appropriate rejections bythe speech understanding component.
These"garbage-in, garbage-out" instances are instruc-tive cases where the system indicates to thelearner that he or she should re-try the utterance.4.2 Mis-understandingsIn Table 2, the row labeled "Incorrect" con-tains mis-understandings, where the system madean interpretation but failed to match the expertannotation.
Mis-understandings account foraround 3.5% of the turns in our data set, on aver-age.
The low rate of mis-understandings is anencouraging result for the overall quality of theunderstanding component.
Prior to the introduc-tion of the garbage model into the speech recog-nizer the mis-understanding rate had been rela-tively high, and these results indicate a signifi-cant improvement.Annotator 1Act GarbageSystem Act 175 3Garbage 94 73Annotator 2Act GarbageSystem Act 176 2Garbage 134 33Table 1.
Distinguishing meaningful utterances(corresponding to an Act) from non-meaningfulattempts (Garbage).System Annotator 1 Annotator 2Correct 167 160Incorrect 8 16Table 2.
System mis-understandings over all mea-ningful utterances.4.3 Non-understandingsInstances from the data set where the annota-tor was able to interpret an act, but the systemreturned "garbage," are shown in the lower-leftcells of Table 1.
These are system non-understandings, since the speech understandingcomponent was not able to map the learner inputto a meaningful act, even though the annotatorswere.
Non-understandings account for 33% ofturns in our data set, on average.To understand the impact of these non-understandings on dialog system quality, wemust consider the specialized case of languagelearners.
Several components of the speech un-derstanding pipeline are tuned with languagelearners in mind.
For example, acoustic modelsused in the automatic speech recognizer aretrained on a mixture of native and non-nativedata.
The goal is for the system to be as tolerantas possible of pronunciation variability, whilestill catching learner mistakes.We expect learner speech attempts to occuron a continuum, ranging from fully correct tominor mistakes to unrecoverable errors.
In thefirst procedure, the annotators were instructed tolabel a recording with a meaningful act in allcases where they could do so, using garbage onlyfor unintelligible attempts.
As a result, we con-sciously placed the annotator tolerance at the farend of this spectrum.Since the system is less forgiving, we hypo-thesize that the non-understandings we foundmask two different sub-classes: instances wherethe system truly failed to interpret a well-formedutterance, and instances where the system was(perhaps appropriately) rejecting a learner mis-take: an intelligible but malformed utterance.In a follow-up procedure, the annotators revi-sited instances labeled as non-understandings.
Inthis second round, they distinguished instanceswhere the learner successfully performed an actthat was simply outside the coverage of thespeech understanding system from instanceswhere they perceived a learner error, either inpronunciation or grammar.
The results are sum-marized in Table 3.We found that most of the cases of non-recognition were actually due to learner error,rather than system error.243Annotator 1Error Type CountLearner Grammar 0Learner Pronunciation 58 (62%)System Error 36Total 94Annotator 2Error Type Count ?Learner Grammar 2 0Learner Pronunciation 85 (63%) 0.65System Error 47 0.65Total 134 0.73Table 3.
Classification of non-understandings.Inter-annotator agreement (?)
is substantialover all classes.5 Conclusions and Future WorkBy applying a method for assessment that goesbeyond word recognition rate, we have producedan analysis of the speech understanding compo-nents in a dialog system for language learners.Expert annotators found that most system-understood speech attempts were interpreted cor-rectly, with mis-understandings occurring only3% of the time.
While non-understandings oc-curred much more frequently, a follow-up exer-cise showed that learner pronunciation error wasthe most frequent cause; these cases are legiti-mate candidates for system rejection, leaving12% of all instances as non-understandingswhere the system was at fault.
These instancesrepresent the most beneficial errors to correctwhen making refinements to the speech under-standing module.In this exercise, one could interpret the hu-man-assigned acts as a model of recognition byan extremely sympathetic hearer.
Although thismodel may be too lenient to provide learnerswith realistic communication practice, it could beuseful for the dialog engine to recognize somepoorly-formed utterances, for the purpose ofproviding feedback.
For example, a learner whorepeatedly attempts the same utterance with un-acceptable but intelligible pronunciation couldtrigger a tutoring-style intervention (?Are youtrying to say bonjour?
Try it more like this...?
).The assessment methods and analysis pre-sented in this paper are a first step toward thistype of system improvement, one that meets theneeds of language learners as a unique type ofdialog-system user.AcknowledgmentsThe authors thank Rebecca Row and MickeyRosenberg for their contributions to the experi-ments described here, and three anonymous re-viewers for comments that improved the clarityof the paper.
This work was sponsored by PMTRASYS, Voice of America, the Office of NavalResearch, and DARPA.
Opinions expressed hereare those of the author and not of the sponsors orthe US Government.ReferencesBarrett, K. A. and W. L. Johnson (2010).
Developingserious games for learning language-in-culture.
Inter-disciplinary Models and Tools for Serious Games:Emerging Concepts and Future Directions.
R. V. Eck.Hershey, PA, IGI Global.Hunt, M. J.
(1990).
"Figures of Merit for AssessingConnected Word Recognisers."
Speech Communica-tion 9: 239-336.Johnson, W. L., J. Rickel, et al (2000).
"AnimatedPedagogical Agents: Face-to-Face Interaction in In-teractive Learning Environments."
Journal of Artifi-cial Intelligence in Education 11: 47--78.Johnson, W. L. and A. Valente (2009).
"Tactical Lan-guage and Culture Training Systems: Using AI toTeach Foreign Languages and Cultures."
AI Maga-zine 30(2).McRoy, S. W. and G. Hirst (1995).
"The repair ofspeech act misunderstandings by abductive infe-rence."
Computational Linguistics 21(4): 435--478.Sagae, A., B. Wetzel, et al (2009).
Culture-DrivenResponse Strategies for Virtual Human Behavior inTraining Systems.
SLaTE-2009, Warwickshire, Eng-land.Samtani, P., A. Valente, et al (2008).
Applying theSAIBA framework to the Tactical Language and Cul-ture Training System.
AAMAS 2008 Workshop onFunctional Markup Language (FML).Suendermann, D., J. Liscombe, et al (2009).
A hand-some set of metrics to measure utterance classificationperformance in spoken dialog systems.
SigDial 2009.Walker, M. A., D. J. Litman, et al (1998).
"Evaluat-ing spoken dialogue agents with PARADISE: Twocase studies."
Computer Speech & Language 12(4):317-347.Wang, N. and W. L. Johnson (2008).
The PolitenessEffect in an Intelligent Foreign Language TutoringSystem.
ITS 2008.244
