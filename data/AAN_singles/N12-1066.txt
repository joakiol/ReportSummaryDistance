2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 563?567,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsBehavioral Factors in Interactive Training of Text ClassifiersBurr SettlesMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh PA 15213, USAbsettles@cs.cmu.eduXiaojin ZhuComputer Sciences DepartmentUniversity of WisconsinMadison WI 53715, USAjerryzhu@cs.wisc.eduAbstractThis paper describes a user study where hu-mans interactively train automatic text clas-sifiers.
We attempt to replicate previous re-sults using multiple ?average?
Internet usersinstead of a few domain experts as annotators.We also analyze user annotation behaviors tofind that certain labeling actions have an im-pact on classifier accuracy, drawing attentionto the important role these behavioral factorsplay in interactive learning systems.1 IntroductionThere is growing interest in methods that incorpo-rate human domain knowledge in machine learningalgorithms, either as priors on model parameters oras constraints in an objective function.
Such ap-proaches lend themselves well to natural languagetasks, where input features are often discrete vari-ables that carry semantic meaning (e.g., words).
Afeature label is a simple but expressive form of do-main knowledge that has received considerable at-tention recently (Druck et al, 2008; Melville et al,2009).
For example, a single feature (word) can beused to indicate a particular label or set of labels,such as ?excellent??
positive or ?terrible??
neg-ative, which might be useful word-label rules for asentiment analysis task.Contemporary work has also focused on mak-ing such learning algorithms active, by enablingthem to pose ?queries?
in the form of feature-basedrules to be labeled by annotators in addition to ?and sometimes lieu of ?
data instances such asdocuments (Attenberg et al, 2010; Druck et al,2009).
These concepts were recently implementedin a practical system for interactive training of textclassifiers called DUALIST1.
Settles (2011) reportsthat, in user experiments with real annotators, hu-mans were able to train near state of the art classi-fiers with only a few minutes of effort.
However,there were only five subjects, who were all com-puter science researchers.
It is possible that thesepositive results can be attributed to the subjects?
im-plicit familiarity with machine learning and naturallanguage processing algorithms.This short paper sheds more light on previous ex-periments by replicating them with many more hu-man subjects, and of a different type: non-expertsrecruited through the Amazon Mechanical Turk ser-vice2.
We also analyze the impact of annotator be-havior on the resulting classifiers, and suggest rela-tionships to recent work in curriculum learning.2 DUALISTFigure 1 shows a screenshot of DUALIST, an inter-active machine learning system for quickly build-ing text classifiers.
The annotator is allowed totake three kinds of actions: ?
label query docu-ments (instances) by clicking class-label buttons inthe left panel, ?
label query words (features) byselecting them from the class-label columns in theright panel, or ?
?volunteer?
domain knowledge bytyping labeled words into a text box at the top ofeach class column.
The underlying classifier is ana?
?ve Bayes variant combining informative priors,1http://code.google.com/p/dualist/2http://mturk.com563123Figure 1: The DUALIST user interface.maximum likelihood estimation, and the EM algo-rithm for fast semi-supervised training.
When auser performs action ?
or ?, she labels queries thatshould help minimize the classifier?s uncertainty onunlabeled documents (according to active learningheuristics).
For action ?, the user is free to volun-teer any relevant word, whether or not it appears ina document or word column.
For example, the usermight volunteer the labeled word ?oscar?
?
posi-tive in a sentiment analysis task for movie reviews(leveraging her knowledge of domain), even if theword ?oscar?
does not appear anywhere in the in-terface.
This flexibility goes beyond traditional ac-tive learning, which restricts the user to feedback onitems queried by the learner (i.e., actions ?
and ?
).After a few labeling actions, the user submits herfeedback and receives the next set of queries in realtime.
For more details, see Settles (2011).3 Experimental SetupWe recruited annotators through the crowdsourcingmarketplace Mechanical Turk.
Subjects were showna tutorial page with a brief description of the clas-sification task, as well as a cartoon of the interfacesimilar to Figure 1 explaining the various annotationoptions.
When they decided they were ready, usersfollowed a link to a web server running a customizedversion of DUALIST, which is an open source web-based application.
At the end of each trial, subjectswere given a confirmation code to receive payment.We conducted experiments using two corporafrom the original DUALIST study: Science (a subsetof the 20 Newsgroups benchmark: cryptography,electronics, medicine, and space) and Movie Re-views (a sentiment analysis collection).
These arenot specialized domains, i.e., we could expect av-erage Internet users to be knowledgable enough toperform the annotations.
While both are generallyaccessible, these corpora represent different types oftasks and vary both in number of categories (fourvs.
two) and difficulty (Movie Reviews is known tobe harder for learning algorithms).
We replicatedthe same experimental conditions as previous work:DUALIST (the full interface in Figure 1), active-doc(the left-hand ?
document panel only), and passive-doc (the ?
document panel only, but with texts se-lected at random and not queried by active learning).For each condition, we recruited 25 users for theScience corpus (75 total) and 35 users for Movie Re-views (105 total).
We were careful to publish taskson MTurk in a way that no one user annotated morethan one condition.
Some users experienced techni-cal difficulties that nullified their work, and four ap-peared to be spammers3.
After removing these sub-jects from the analysis, we were left with 23 usersfor the Science DUALIST condition, 25 each for thetwo document-only conditions (73 total), 32 usersfor the Movie Reviews DUALIST condition, and33 each for the document-only conditions (98 total).DUALIST automatically logged data about user ac-tions and model accuracies as training progressed,although users could not see these statistics.
Trialslasted 6 minutes for the Science corpus and 10 min-utes for Movie Reviews.
We did advertise a ?bonus?for the user who trained the best classifier to encour-age correctness, but otherwise offered no guidanceon how subjects should prioritize their time.4 ResultsFigure 2(a) shows learning curves aggregated acrossall users in each experimental condition.
Curves areLOESS fits to classifier accuracy over time: locally-weighted polynomial regressions (Cleveland et al,1992) ?1 standard error, with the actual user datapoints omitted for clarity.
For the Science task (top),DUALIST users trained significantly better classi-fiers after about four minutes of annotation time.Document-only active learning also outperformed3A spammer was ruled to be one whose document error rate(vs. the gold standard) was more than double the chance error,and whose feature labels appeared to be arbitrary clicks.5640.200.300.400.500.600.700  60  120  180  240  300  360ScienceDUALISTactive-docpassive-doc0.490.520.550.580.610.640  120  240  360  480  600MovieReviewsannotation time (sec)DUALISTactive-docpassive-doc(a) learning curvesDUALIST active-doc passive-doc0.30.50.7DUALIST active-doc passive-doc0.500.600.70(b) final classifier accuracies0.200.300.400.500.600.700  60  120  180  240  300  360ScienceDV++ (5)DV+ (9)DV- (9)0.490.520.550.580.610.640  120  240  360  480  600MovieReviewsannotation time (sec)DV++ (8)DV+ (13)DV- (11)(c) behavioral subgroup curvesFigure 2: (a) Learning curves plotting accuracy vs. actual annotation time for the three conditions.
Curves are LOESSfits (?1 SE) to all classifier accuracies at that point in time.
(b) Box plots showing the distribution of final accuraciesunder each condition.
(c) Learning curves for three behavioral subgroups found in the DUALIST condition.
TheDV++ group volunteered many labeled words (action ?
), DV+ volunteered some, and DV- volunteered none.standard passive learning, which is consistent withprevious work.
However, for Movie Reviews (bot-tom), there is little difference among the three set-tings, and in fact models trained with DUALIST ap-pear to lag behind active learning with documents.Figure 2(b) shows the distribution of final classi-fier accuracies in each condition.
For Science, theDUALIST users are significantly better than eitherof the baselines (two-sided KS test, p < 0.005).While the differences in DUALIST accuracies arenot significantly different, we can see that the topquartile does much better than the two baselines.Clearly some DUALIST users are making better useof the interface and training better classifiers.
How?It is important to note that users in the active-doc and passive-doc conditions can only choose ac-tion ?
(label documents), whereas those in the DU-ALIST condition must allocate their time amongthree kinds of actions.
It turns out that the anno-tators exhibited very non-uniform behavior in thisrespect.
In particular, activity of action ?
(volunteerlabeled words) follows a power law, and many sub-jects volunteered no features at all.
By inspectingthe distribution of these actions for natural break-points, we identified three subgroups of DUALISTusers: DV++ (many volunteered words), DV+ (somewords), and DV- (none; labeled queries only).
NoteMovie Reviews ScienceGroup # Words Users # Words UsersDV++ 21?62 8 24?42 5DV+ 1?15 13 2?19 9DV- 0 11 0 9Table 1: The range of volunteered words and number ofusers in each behavioral subgroup of DUALIST subjects.that DV- is not functionally equivalent to the active-doc condition, as users in the DV- group could stillview and label word queries.
The three behavioralsubgroups are summarized in Table 1.Figure 2(c) shows learning curves for these threegroups.
We can see that the DV++ and DV+ groupsultimately train better classifiers than the DV- group,and DV++ also dominates both the active and pas-sive baselines from Figure 2(a).
The DV++ group isparticularly effective on the Movie Reviews corpus.This suggests that a user?s choice to volunteer morelabeled features ?
by occasionally side-stepping thequeries posed by the active learner and directly in-jecting their domain knowledge ?
is a good predic-tor of classifier accuracy on this task.To tease apart the relative impact of other behav-iors, we conducted an ordinary least-squares regres-sion to predict classifier accuracy at the end of a trial.We included the number of user events for each ac-565tion as independent variables, plus two controls: thesubject?s document error rate in [0,1] with respect tothe gold standard, and class entropy in [0, logC] ofall labeled words (whereC is the number of classes).The entropy variable is meant to capture how ?bal-anced?
a user?s word-labeling activity was for ac-tions?
and?, with the intuition that a skewed set ofwords could confuse the learner, by biasing it awayfrom categories with fewer labeled words.Table 2 summarizes these results.
Surprisingly,query-labeling actions (?
and ?)
have a relativelysmall impact on accuracy.
The number of volun-teered words and entropy among word labels appearto be the only two factors that are somewhat signif-icant: the former is strongest in the Movie Reviewscorpus, the latter in Science4.
Interestingly, there is astrong positive correlation between these two factorsin the Movie Reviews corpus (Spearman?s ?
= 0.51,p = 0.02) but not in Science (?
= 0.03).
When weconsider change in word label entropy over time, theScience DA++ group is balanced early on and be-comes steadily more so on average , whereasDA+ goes for several minutes before catching up(and briefly overtaking) .
This may accountfor DA+?s early dip in accuracy in Figure 2(c).
ForMovie Reviews, DA++ is more balanced than DA+throughout the trial.
DA++ labeled many words thatwere also class-balanced, which may explain whyit is the best consistently-performing group.
As iscommon in behavior modeling with small samples,the data are noisy and the regressions in Table 2 onlyexplain 33%?46% of the variance in accuracy.5 DiscussionWe were able to partially replicate the results fromSettles (2011).
That is, for two of the same data sets,some of the subjects using DUALIST significantlyoutperformed those using traditional document-onlyinterfaces.
However, our results show that thegains come not merely from the interface itself, butfrom which labeling actions the users chose to per-form.
As interactive learning systems continue toexpand the palette of interactive options (e.g., la-4Science has four labels and a larger entropy range, whichmight explain the importance of the entropy factor here.
Also,labels are more related to natural clusterings in this corpus(Nigam et al, 2000), so class-balanced priors might be key forDUALIST?s semi-supervised EM procedure to work well.Movie Reviews ScienceAction ?
SE ?
SE(intercept) 0.505 0.038 *** 0.473 0.147 **?
label query docs 0.001 0.001 0.005 0.005?
label query words -0.001 0.001 0.000 0.001?
volunteer words 0.002 0.001 * 0.000 0.002human error rate -0.036 0.109 -0.328 0.230word label entropy 0.053 0.051 0.201 0.102 .R2 = 0.4608 ** R2 = 0.3342*** p < 0.001 ** p < 0.01 * p < 0.05 .
p < 0.1Table 2: Linear regressions estimating the accuracy of aclassifier as a function of annotator actions and behaviors.beling and/or volunteering features), understandinghow these options impact learning becomes moreimportant.
In particular, training a good classifierin our experiments appears to be linked to (1) vol-unteering more labeled words, and (2) maintaininga class balance among them.
Users who exhibitedboth of these behaviors ?
which are possibly arti-facts of their good intuitions ?
performed the best.We posit that there is a conceptual connection be-tween these insights and curriculum learning (Ben-gio et al, 2009), the commonsense notion that learn-ers perform better if they begin with clear and unam-biguous examples before graduating to more com-plex training data.
A recent study found that somehumans use a curriculum strategy when teaching a1D classification task to a robot (Khan et al, 2012).About half of those subjects alternated between ex-treme positive and negative instances in a relativelyclass-balanced way.
This behavior was explained byshowing that it is optimal under an assumption that,in reality, the learning task has many input featuresfor which only one is relevant to the task.Text classification exhibits similar properties:there are many features (words), of which only a feware relevant.
We argue that labeling features can beseen as a kind of training by curriculum.
By volun-teering labeled words in a class-balanced way (espe-cially early on), a user provides clear, unambiguoustraining signals that effectively perform feature se-lection while biasing the classifier toward the user?shypothesis.
Future research on mixed-initiative userinterfaces might try to detect and encourage thesekinds of annotator behaviors, and potentially im-prove interactive machine learning outcomes.566AcknowledgmentsThis work was funded in part by DARPA, theNational Science Foundation (under grants IIS-0953219 and IIS-0968487), and Google.ReferencesJ.
Attenberg, P. Melville, and F. Provost.
2010.
A uni-fied approach to active dual supervision for labelingfeatures and examples.
In Proceedings of the Euro-pean Conference on Machine Learning and Principlesand Practice of Knowledge Discovery in Databases(ECML PKDD), pages 40?55.
Springer.Y.
Bengio, J. Louradour, R. Collobert, and J. Weston.2009.
Curriculum learning.
In Proceedings of the In-ternational Conference on Machine Learning (ICML),pages 119?126.
Omnipress.W.S.
Cleveland, E. Grosse, and W.M.
Shyu.
1992.
Lo-cal regression models.
In J.M.
Chambers and T.J.Hastie, editors, Statistical Models in S. Wadsworth &Brooks/Cole.G.
Druck, G. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In Proceedings of the ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 595?602.
ACM Press.G.
Druck, B.
Settles, and A. McCallum.
2009.
Ac-tive learning by labeling features.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 81?90.
ACL Press.F.
Khan, X. Zhu, and B. Mutlu.
2012.
How do humansteach: On curriculum learning and teaching dimen-sion.
In Advances in Neural Information ProcessingSystems (NIPS), volume 24, pages 1449?1457.
Mor-gan Kaufmann.P.
Melville, W. Gryc, and R.D.
Lawrence.
2009.
Sen-timent analysis of blogs by combining lexical knowl-edge with text classification.
In Proceedings of the In-ternational Conference on Knowledge Discovery andData Mining (KDD), pages 1275?1284.
ACM Press.K.
Nigam, A.K.
Mccallum, S. Thrun, and T. Mitchell.2000.
Text classification from labeled and unlabeleddocuments using em.
Machine Learning, 39:103?134.B.
Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1467?1478.
ACL Press.567
