Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 88?95,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsUdS at the CoNLL 2013 Shared TaskDesmond Darma Putra, Lili Szabo?Saarland UniversityFaculty of Computational Linguistics and Phonetics{ddputra, lilis}@coli.uni-saarland.deAbstractThis paper describes our submission for theCoNLL 2013 Shared Task, which aims to toimprove the detection and correction of thefive most common grammatical error types inEnglish text written by non-native speakers.Our system concentrates only on two of them;it employs machine learning classifiers for theArtOrDet-, and a fully deterministic rule basedworkflow for the SVA error type.1 IntroductionGrammatical error correction is not a new task in Natu-ral Language Processing field.
Many previous researchwas done to solve the problem.
Most of these worksfocus on article and preposition correction.In this paper we present our implementation of oursystem that participated in the CoNLL 2013 SharedTask for grammatical error correction.
Out of the 28annotated error types in the training data, this year?stask focuses on 5 error types: article or determiner (Ar-tOrDet), preposition (Prep), noun number (Nn), verbform (Vform) and subject-verb agreement (SVA).
Thiserror proportion can be seen in Table 1.From these error types we focused on ArtOrDet andError type CountsArtOrDet 6658Nn 3779Prep 2404Vform 1453SVA 1527Table 1: Error types in NUCLE corpusSVA mistakes only.The remaining part of this paper is organized asfollows.
Chapters refcorp and 3 describe the dataand system architecture.
Chapter 4.2 explains the Ar-tOrDet classification task.
Our experimental setup forArtOrDet error is presented in Section 4.3.
Chapter 4.4describes the results from our experiments and someanalysis regarding the results.
Chapters 5.1 and 5.1.1describe the task and issues respectively, Chapter 5.2explains the how the subject-verb pairs are extracted,Chapter 5.3 is about the evaluation of the pairs.
Lastly,Chapter 8 will conclude our work.2 Corpora and ToolsThe training corpus (Dahlmeier, 2013) consists of ap-prox.
1400, 40-sentence long essays (summing up tooverall 1161567 tokens), written by non-native speak-ers, and annotated by professional English language in-structors for error tags and corrections.The tokenized, POS-tagged and dependency andconstituency parsed version of the corpus was alsoprovided, along with the tools (tokenization - NLTK,POS-tagging and parsing - Stanford parser (Marie-Catherine de Marneffe, 2011)).The other NLP-tools used in our implementation(described in the relevant sections) are the LIBLIN-EAR classifier and NodeBox.For evaluation of the system results the M2 Scorer(Dahlmeier, 2012) was used.3 System and PipelineOur system consists of two independent subsystems,which are combined serially.
The parsed version of theinput text first goes through the ArtOrDet subsystemwhose output is re-parsed, and serves as the input forthe SVA subsystem:1.
Article and determiner correction2.
Re-parsing of the data3.
Subject-verb agreement correctionIn the following 2 Chapters we present the workflowsfor the ArtOrDet and SVA mistake types separately.4 ArtOrDet Correction4.1 ArtOrDet Mistake TypeThe ArtOrDet error type is the most common mistake.We pose this ArtOrDet error correction as a multi-classclassification task.
The output from the classificationtask will be used to correct the data.Both sentences ?girls like flowers?
and ?the girls likeflowers?
can be accepted as correct, depending on thecontext - whether the noun refers to a specific group orit is a general statement.
Another example like ?he ate88the cake?
and ?he ate a cake?
are also grammaticallycorrect depending on the context whether the cake hasbeen introduced before or not.4.2 ArtOrDet ClassificationAn article or a determiner is followed by an NP.
Thisarticle often refers to a definite or indefinite elementof a class or pointing to something specific or general.There are many examples article/determiner that fol-lows an NP, for example, the, a, some, any, this, these,that, those, etc.
According to (Huddleston, 1984), oneNP can hold up to three determiners e.g.
all her manyideas.
Moreover, each NP has a head which is nountype class.
This noun consists of three subclasses in-cluding common noun (e.g.
book, car, dog), propernoun (e.g.
Larry, Sarah, Germany) and pronoun (e.g.you, we, they, them, it).
Since we are working withArtOrDet errors, then there is no point of checking NPwhich contains pronoun subclass because an article cannever be followed by pronoun.We classify these ArtOrDet errors into several typeswhich are described in Table 2.
The most common er-ror is caused by missing the (around 39%).
Addition-ally, unnecessary use of the contributes 26% of error.Furthermore, confusion between using the or or a/anbring 4.3% error.
We classified around 15% as unde-fined error due to several reason.
First, the error doesnot appear in front of the NP itself, sometimes it ap-pears in the middle of the NP.
Second, the error appearsin other type phrase like adjective phrase, this makesthe problem is more difficult to trace.
For example, aclause ?...such invention helps to prevent elderly fromfalling down.?
The word elderly is recognized as adjec-tive phrase and the correction happens in front of thatword (adding article the).
Third, the correction involvesother articles for example this, that, and many more.Besides the above error, there is another error whichwe have to handle such as confusion between a oran.
This problem can be solved using a rule-basedapproach which will be discussed in the next section.To simplify this, we normalize article a and an into a.Later on, after the classification is done, we will usethis rule-based to return the correct article.4.3 Experimental setupAfter defining the error types, we split the corpus intotraining and testing dataset.
We select 50 documentsfrom the corpus as a held-out test data and the rest isused for the training data.
For the training part, weextract the NP (which is not headed by pronoun) usingthe information from constituent parse tree and POStags.
Each NP that is extracted represents one trainingexample.
Thus, if an NP is incorrect then we label itto one of the label from Table 2.
We consider this taskas a multi-class classification task, that one NP finds amapping f : x?
{c1, c2, .
.
.
, c8} that maps x ?
NPsinto one of the 8 labels.For the first experiment, we select two well knownClassification label TrainingCorrect NP 97.91%Missing the 0.92%Missing a/an 0.30%Unnecessary the 0.07%Unnecessary a/an 0.61%Use the instead of a/an 0.03%Use a/an instead of the 0.06%Undefined 0.11%Table 3: Training dataclassification methods such as LIBLINEAR (Fan et al2008) and Naive Bayes (McCallum and Nigam, 1998).Both of these methods are trained using the same train-ing data and features which we are going to discuss inSubsection 4.3.5.
In the testing part, our classifier willpredict a label for each NP.
If the classifier predicts thatthe observed NP is already correct or it needs to addarticle a then we apply a rule-based approach to makesure it puts the right article (a/an).
This rule-based willutilize CMU pronouncing dictionary from NLTK to dothe checking and put conditional constraints such aschecking whether this NP is an acronym or not.The second and third experiments are inspired by(Dahlmeier et al 2012; Rozovskaya et al 2012).
Werealize that the proportion of observed NP without ar-ticle error outnumbers the observed NP with an articleerror (see Table 3).
Therefore, this huge proportion ofcorrect NP may affect the classifier accuracy.
To justifythis claim, we will utilize error inflation method for thesecond experiment and do re-sampling and undersam-pling NP as the third experiment.4.3.1 Naive BayesNaive Bayes is a famous classification method whichapplies Bayes theorem?s with naive assumptions.
Thisassumptions believe that all features that are use to de-scribe the data are independent (McCallum and Nigam,1998).
The advantages of this method are fast andeasy to implement.
This method has shown to be agood classification tool in NLP field (e.g.
spam filter-ing, news classification, etc.).
To classify an instanceD = ?f1, f2, .
.
.
, fn?
according to one of the classescj ?
C, we calculate the maximum likelihood estima-tion of a prior probability cj times the product of everyfeaturesf1,...,n given class cj times as described below:c = argmaxcj?CP (cj)?iP (fi|cj) (1)For this task, we utilize naive bayes package fromNLTK.
This method is trained using the features whichare already described in Table 4.4.3.2 LIBLINEARLIBLINEAR provides a large-scale classification li-brary to handle sparse data that contains a large num-bers of instances and features (Fan et al 2008).89ArtOrDet errors Proportion Example(s)Missing the 38.9% Working class Singaporean would be motivated to work hard as they know the governmentwould contribute...Missing a/an 12.8% If China can come up with an effective policy to change its education system and stimulateinnovationUnnecessary the 26% The innovators, who are normally work under Research and Development department, haveto recognize...Unnecessary a/an 2.7% It would no longer be able to a have constant economic growth which places a detrimentaleffect on the countryUse the instead of a/an 2.9% The government budgets should be diverted to other areas of the a country?s developmentsince resources are limitedUse a/an instead of the 1.4% As a result of a the growing aging population...Undefined 15.3% ...such invention helps to prevent the elderly from falling down.Of course, it this is not possible.This caused problem like the appearance of slums which most of the time is not safe due tothe their unhealthy environmentTable 2: ArtOrDet errors distribution from NUCLE corpusIt supports two binary linear classifiers such as L2-regularized logistic regression (LR), L1-loss and L2-loss linear SVM.
Given a pair training set instance(xi, yi), where i = 1, .
.
.
, l, xi ?
Rn and y ?{+1,?1}l.
This data will be considered as optimiza-tion problem:minw12wTw + Cl?i=1?isubject to yi(wT?
(xi) + b) ?
1?
?i(2)where C > 0 as a penalty parameter.LIBLINEAR not only supports binary class prob-lems but also multi-class problems via one-vs-the-reststrategy.
For our purpose, we will use this LIBLINEARpackage with C = 0.125.
This penalty value is comefrom the grid search which is provided in the packageto find the best parameter C.Both of these classification methods are evaluated bycalculating the number of corrects prediction compareto the annotation label which is defined as:Accuracy =# of correct predictions# of predictions(3)4.3.3 Error Inflation MethodSince the ArtOrDet errors that we have is sparse andincrease the errors proportion in the training data canhelp the classifier to perform better then we apply thiserror inflation method (Rozovskaya et al 2012).
Weselect some positive constant (less than 1.0) to reducethe proportion of the correct example and adding thisproportion to the other error types by generating theartificial error.
We found that probability among thecorrections are still similar.4.3.4 Re-sampling and UndersamplingBesides error inflation method, we are also interestedin re-sampling NP with ArtOrDet error and undersam-pling without ArtOrDet error.
Some combination willbe selected to see whether it can help the classifier indetecting and correcting the ArtOrDet errors.
we selectsome constant number to re-sample the NP which con-tains ArtOrDet error and some threshold to undersam-pling the NP which is correct.
The results from thesetwo approaches are discussed in the next section.4.3.5 Feature ExtractionWe adopt some features from (Dahlmeier et al 2012;Rozovskaya et al 2012) which are described in Ta-ble 4.
Most of the features are coming from lexical andPOS.
If the NP contains an article, then we will sepa-rate it and consider as as additional feature.wNb and wNa in Table 4 represent word at posi-tion N before the NP and word at position N after thearticle position.
If there is no article in the beginningof NP then first word in the NP is recognize as w1a.pNb and pNa describe the POS of wNb and wNa.NC is a noun compound and this compound is gener-ated by the last two words inside the NP which havenoun POS.
head of the NP is identified with headWordfeature and it is determined using the information fromdependency tree.
NP is a noun phrase which is ex-tracted from the constituent parse tree.
posX is a POSfeature of X where x ?
{NC,NP, headWord}.
verbfeature and prep are determined from the POS informa-tion.
wordAfterNP is activated if there is another wordafter the NP.4.4 Results & DiscussionThe result from the first experiment can be seen in Ta-ble 6.
We compare the baseline with Naive Bayes andLIBLINEAR classifier.
The baseline that we choose forthis task has similar definition with (Rozovskaya andRoth, 2010) which is ?do nothing?.
The reason behindof this is because the proportion of NP using correctarticle is more than 90% and this is better than state-of-the-art classifier for article selection (with article selec-tion, usually the baseline is set by majority class whichis zero article).
The result shows that LIBLINEAR pro-duces a minor improvement than the baseline.
This in-crease is influenced by the rule based approach that wedevelop to correct the use of a and an.
Naive Bayesdoesn?t perform well due to the dependent features that90Feature Type DescriptionObserved article articleWord n-grams w1b, w2b, w3b, w2b w1b, w3b w2b w1b, w1a, w2a, w3a, w1a w2a, w1a w2a w3a, w1b w1a, w2b w1b w1a,w1b w1a w2a, w2b w1b w1a w2a, w3b w2b w1b w1a, w1b w1a w2a w3aPOS features p1b, p2b, p3b, p2b p1b, p3b p2b p1b, p1a, p2a, p3a, p1a p2a, p1a p2a p3a, p1b p1a, p2b p1b p1a, p1b p1a p2a,p2b p1b p1a p2a, p3b p2b p1b p1a, p1b p1a p2a p3a, p1b w1b, p1b w1a, p2b w2b, p2b w2aNP NC, posNC, headWord, posHeadWord, headWord posHeadWord, w1b posNP posHeadWord, w1b headWord,w1b headWord wordAfterNPVerb verb, verb headWord, verb NC, verb NP, verb posNP headWord, verb posNP NCPreposition prep, prep headWord, prep NC, prep NP, prep posNP headWord, prep posNP NCTable 4: Features set1 0.9 0.8 0.7 0.6 0.5acc.
98.64% 98.63% 98.14% 97.12% 95.10% 92.36%Table 5: ArtOrDet accuracy using error inflationMethod AccuracyBaseline 98.5%Naive Bayes 82 %LIBLINEAR 98.67 %Table 6: Classifier performance on correcting Ar-tOrDet errorswe employs.Our second experiment tests the use of error inflationmethod on LIBLINEAR classifier.
This test is appliedto LIBLINEAR classifier with since it has a higher ac-curacy than Naive Bayes.
The results from this experi-ment is described in Table 5.
The smaller the constantnumber will result in larger article errors.
Nonetheless,if we introduce too many error it will reduce the accu-racy.The last experiment test the effect of re-samplingNP with ArtOrDet several error times and reducing thenumber of observed NP that is already correct can beseen in Table ??.
The re-sampling parameter is puton the first column (5, 10, 15, 20 and 25 times) deter-mine how many duplicates are made for each NP.
Onthe row side we use a threshold to reduce the propor-tion of the observed NP which is already correct.
Sofor each correct NP, we generate a random number andif it is higher than the threshold, then it is included inthe training dataset.
Table ??
reveals that re-samplingsome NP that has ArtOrDet error does not increase theaccuracy.
On the other hand, reducing the thresholdimprove the accuracy.If we look deeper, we found that increasing thethreshold and re-sampling may have a positive corre-lation with correcting the error.
However, the numberof false positives also increased.4.5 Further analysisInspired by (Gamon et al 2008) to make two classi-fiers for detecting and correcting article errors.
If weconsider that our classifier can detect correctly the er-ror, then we only need to train another classifier tomake the correction by using the same features as de-Classification label # AccuracyMissing the 45 96%Missing a/an 26 38%Unnecessary the 47 100%Unnecessary a/an 4 100%Use the instead of a/an 4 0%Use a/an instead of the 1 0%Undefined 5 0%TOTAL 132 79%Table 7: Error Correction distributionscribed in Table 4.
The training for this classifier comesfrom all NP with ArtOrDet error.
Our result proves that79% of the ArtOrDet can be corrected (see Table 7)On one hand, our classifier does a good job in a senseof detecting missing article and removing unnecessaryarticle.
On the other hand, it is hard to predict eitherchoosing between a/an or the.
We found that our clas-sifier labels this confusion as unnecessary the or a/an.This means that we have to remove the article for bothof these confusions.This may be caused by lack of training data for par-ticular errors such as confusion between the & a/an.We realize that this mistake occurs often when the ar-ticle would appear in front of an adjective - and in ourfeature sets there is no explicit adjective feature.5 SVA Correction5.1 SVA Mistake TypeSubject-verb agreement is the fourth most commonmistake type in texts written by English languagelearners.
It is also the highest done by machinetranslation systems, yet still an unsolved problem.The English verb inflection paradigm is relativelysimple, and only the misuse of third person singularand finite form of the verb (the form coinciding withthe infinitive form) are of interest for the SVA errorcorrection:*John and Mary goes to work every day.
*Mary go to work every day.Nevertheless, it is not a straightforward task, mainlybecause of the difficulties of linking the corresponding91subjects and verbs together.
The detection of the dis-agreement is relatively simple, compared to the task ofrecognizing the number of the subject and verb.This mistake type is different in nature form the errortypes (e.g.
determiner and preposition) as the scope ofthe analysis cannot be determined as easily, therefore ithas to be the whole sentence.
The verb and its corre-sponding subject can be quite distant from one anotherin the sentence, and by no means have predictable po-sitions.In English the verbs and their subjects have no fixedpositions; in indicative sentences the verb most of thetimes (not immediately) but follows the subject, al-though not necessarily, e.g.
in sentences with exple-tives the subject follows the verb:However, there/EXPL are/VERB still many prob-lems/SUBJ hampering engineering design process forinnovations.5.1.1 Issues on the Syntactic LevelThere are two types of syntactic phenomena that makethe recognition and agreement evaluation of subject-verb difficult.These issues are explained on dependency parsing ex-amples, but can be generalized to any kind of grammar.5.1.2 Multiple SubjectsWhen there are multiple subjects in the sentence, onlythe first one is labeled as a subject, the ones followingit get the conj label.
Even if all of them are in singularform, the verb has to be in its plural form, as multi-ple subjects mean plural number in English.
If thesetype of sentences are not taken care of, that can lead tomany missed corrections and to even more faulty ones.Figure 5.1.2 visualizes the problem.5.1.3 Subject CoreferenceIf a sentence contains a wh-subordinate clause, theverb in the subordinate clause has to agree with theantecedent of the subject, but the subject is a WH-determiner (that, what, which, who, etc.)
that can referto both singular and plural antecedents.The referent (ref) of the head of an NP is the rela-tive word introducing the relative clause modifying theNP is an existing label in dependency parsing, but notavailable with the parser used here.There are multiple ways to resolve the coreference, theone simplistic method1 applied here is based on the as-sumption that the antecedent of the wh-subject is theclosest preceding noun or pronoun to it.Another competing method is to use the head of theverb in the subordinate clause, which is exactly the an-tecedent of the wh-subject (see in Figure 5.1.3).
Thisrelation is labeled as rcmod, the relative clause modi-fier.When the verb is an auxiliary, its head can be a verb1In sentences, where the wh-subject is a clausal subject,like What engineers should do is to invent new machines.
arehandled separately.
(which have shaped/VBN), an adjective (which is ef-fective/JJ) or a noun (which is a competitive fundingscheme/NN), whose head is the antecedent of the rela-tive clause.The second method, apart from being challenging toimplement, yields to significantly worse results thanthe first one, most probably because of the dependencyannotation mistakes in the corpus.
The other problemwith it is, that it requires the subjects and verbs to bepaired before they the pairing is done in the pipeline.5.2 Subject-Verb Pair ExtractionIn order to being able to evaluate their agreement, thefirst task in finding SVA errors is identifying matchingsubjects and verbs.
This is done in two steps:1. extracting all predicate verbs and subjects fromthe sentence,2.
identifying which subject(s) belongs to whichverb(s).For recognizing inflected verb forms in 1. the POS-tagsare used; all inflected verb forms (VBZ, VBP, VBD,MD) are extracted from the sentence.
As for the sub-jects, the dependency labels nsubj, nsubjpass, csubjare used to recognize them.This is also the place where the multiple subject identi-fication and coreference resolution is done.
Pronoun-and determiner subjects are classified as singular orplural subjects, based on a finite list.
Noun subjectsare classified based on their POS-tags: NN and NNP assingular, NNS and NNPS as plural.Once all subjects and verbs were extracted from thesentence, they have to be paired.In 2., depending on how many subjects and verbs wereextracted, POS templates were used to pair them.It has to be noted here that in dependency parsing thesubjects are not always dependent on the predicate verbitself, but rather on the main verb in the sentence, suchas in Figure 5.2, so the head of the subject informationcouldn?t be used.There is no straightforward solution in the constituencyparse trees either; it is not sufficient to take the head ofthe NP under the ROOT as the subject, as this solutionwouldn?t handle relative clauses properly.5.2.1 PatternsOnly patterns, which can be almost exhaustivelycorrectly classify subject-verb pairs are used.Each verb is paired with the subject that is assigned anidentical index.
The following patterns are used:Subject1 Verb1Verb1 Subject1Subject1/2 Verb1 Verb2Subject1 Verb1 Subject2Subject1 Verb1 Subject2 Verb2Subject1 Verb1 Verb2 Subject292Good environment , efficient technology support and proper use of food *is required .ROOTnsubjpassconjconjFigure 1: Dependency relations in a sentence with conjunct subjects.
Only the relevant dependencies are marked.There is an original SVA mistake (made by the author) in the sentence due to the missed identification of theconjunct subjects.Innovations that are radically different face even greater problems .ROOTnsubjnsubjcoprcmodFigure 2: Sentence with subordinate clause.
Only relevant dependencies are marked.
The subject of the subordinatesentence is headed by the adjective, which is headed by the subject of the main clause.Wind and wave can all be used for generating power .ROOTnsubjpassccconjadvmodauxpassprep prepamodpobjFigure 3: Sentence with labeled dependency relations.
The first subject is not headed by the finite verb of thesentence can, but rather by the verb in the participial form used.All other patterns (with 5 and more subjects or verbsin the sentence) were discarded from the evaluation,due to the far too many pairing possibilities.
Theselong sentences generally contain a lot of modifiers, andmake up 34% of the development data.5.3 SV-Agreement Evaluation: Rule-basedSystemAfter the pairing is complete, only the pairs which in-clude VBP2/VBZ3 tags for the verbs, or verb forms inthe past tense of the copula (was/were) are retained forthe agreement evaluation.If the number of the subject and verb don?t agree, theverb form gets corrected.2plural verb form3third person singular verb form5.3.1 CorrectionThe correction is done by using NodeBox, which is atool that generates the morphologically correct singularor plural form of a given English verb.5.4 SVA ResultsOn development set, only SVA-corrections, with othererror types not being corrected we get a precision of0.18.25% and a recall of 22.20%.5.4.1 System Error AnalysisThe following patterns emerged.
False negatives(missed corrections) are mostly, but not exclusively dueto non-accurate POS-tags, non-accurate parse trees (in-cluding many titles of the documents), dependency onother mistake types: especially on the noun number93type mistakes, mistake annotation errors and other spe-cific cases.6 Integrating the SystemsThe systems, handling separately the mistake types, arecombined in a sequential order.The SVA mistake type heavily depends on the correc-tion of the other mistake types, most prominently onthe noun number (Nn) mistakes, as the example sen-tence below shows.
*This will , if not already , caused/Vform problems asthere are/SVA very limited spaces/Nn for us .This will , if not already , cause problems as there isvery limited space for us .Although we don?t deal with Nn-mistakes, the SVA-system is still the last in the row.
After each iteration,the test data is re-parsed, to become the input for thenext system.7 Joint Results on Blind DataOur final results (run on the M2 scorer) are as shown inTable 7.Precision 0.2769Recall 0.110F1 0.0211Table 8: System results on blind data8 ConclusionCorrecting ArtOrDet errors for this task is not an easyjob especially the number of NP using correct articleis really high (more than 95%).
However our LIBLIN-EAR classifier performance is slightly better than thebaseline and Naive Bayes.
Besides comparing betweenNaive Bayes and LIBLINEAR classifiers for this taskwe also adapt two approaches from (Dahlmeier et al2012) and (Rozovskaya et al 2012).
Our result ex-plains that neither re-sampling method nor error infla-tion method contribute to the increase of accuracy.There are several directions that can be pursuedto improve the classifier accuracy.
Adding languagemodel feature which is mentioned by (Gamon et al2008; Dahlmeier et al 2012) might be useful to filterthe result.
However using language model like GoogleN-gram corpus would need some extra treatment sincethe data is really big and need a lot of computation timeto build the language model.The hardest part of the SVA-correction task is to ex-tract the matching subject-verb pairs; with sufficientamount of data annotated for that purpose (there is oneout there, for Swedish), the rule-based approach couldbe turned into a statistical learning one, which mightimprove the recall of the system.
I have found no pre-vious research pointing to this direction.
Long andcomplex sentences, with more than one subject-verbpairs, are frequent in corpora specific to life sciencesand technology literature, such as the corpus used inthis shared task.
The system definitely works better onshorter sentences.ReferencesDan Roth Alla Rozovskaya.
2010.
Training paradigmsfor correcting errors in grammar and usage.
HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the ACL,pages 154?162.Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun FengNg.
2012.
Nus at the hoo 2012 shared task.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 216?224,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Ng Hwee Tou Dahlmeier, Daniel.
2012.
Better evalu-ation for grammatical error correction.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, (NAACL2012), pages 568?572, Montreal, Canada.Ng Hwee Tou Wu Siew Mei Dahlmeier, Daniel.
2013.Building a large annotated corpus of learner english:The nus corpus of learner english.
In To appear inProceedings of the 8th Workshop on Innovative Useof NLP for Building Educational Applications (BEA2013), Atlanta, Georgia, USA.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
Hoo 2012: A report on the preposition anddeterminer error correction shared task.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 54?62,Montre?al, Canada, June.
Association for Computa-tional Linguistics.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the Fourth ACL-SIGSEM Workshopon Prepositions, SigSem ?07, pages 45?50, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Rachele De Felice and Stephen G. Pulman.
2008.
Aclassifier-based approach to preposition and deter-miner error correction in l2 english.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics - Volume 1, COLING ?08, pages169?176, Stroudsburg, PA, USA.
Association forComputational Linguistics.Xuelei Miao Yan Song Dongfeng Cai, Yonghua Hu.2009.
Dependency grammar based english subject-verb agreement evaluation.
23rd Pacific Asia Con-ference on Language, Information and Computation,pages 63?71.94Mark Dredze and Koby Crammer.
2008.
Confidence-weighted linear classification.
In In ICML ?08: Pro-ceedings of the 25th international conference on Ma-chine learning, pages 264?271.
ACM.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Michael Gamon, Jianfeng Gao, Chris Brockett,Alexandre Klementiev, William B. Dolan, DmitriyBelenko, and Lucy Vanderwende.
2008.
Using con-textual speller techniques and language modeling foresl error correction.
In IJCNLP, pages 449?456.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in english article usage bynon-native speakers.
Nat.
Lang.
Eng., 12(2):115?129, June.Rodney D. Huddleston.
1984.
Introductionto the grammar of English / Rodney Huddle-ston.
Cambridge University Press Cambridge [Cam-bridgeshire] ; New York.Stephanie Seneff John Lee.
2008.
Correcting mis-use of verb forms.
Proceedings of ACL-08: HLT,12:174?182.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL, pages 423?430.Kevin Knight and Ishwar Ch.
1994.
Automated poste-diting of documents.
In In Proceedings of AAAI.Gerard Lynch, Erwan Moreau, and Carl Vogel.
2012.A naive bayes classifier for automatic correction ofpreposition and determiner errors in esl text.
In Pro-ceedings of the Seventh Workshop on Building Ed-ucational Applications Using NLP, pages 257?262,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Christopher D. Manning Marie-Catherine de Marneffe.2011.
Stanford typed dependencies manual.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for naive bayes text classi-fication.
In IN AAAI-98 WORKSHOP ON LEARN-ING FOR TEXT CATEGORIZATION, pages 41?48.AAAI Press.Alla Rozovskaya and Dan Roth.
2010.
Trainingparadigms for correcting errors in grammar and us-age.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,HLT ?10, pages 154?162, Stroudsburg, PA, USA.Association for Computational Linguistics.Alla Rozovskaya, Mark Sammons, and Dan Roth.2012.
The ui system in the hoo 2012 shared taskon error correction.
In Proceedings of the SeventhWorkshop on Building Educational Applications Us-ing NLP, pages 272?280, Stroudsburg, PA, USA.Association for Computational Linguistics.95
