Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 534?541,Sydney, July 2006. c?2006 Association for Computational LinguisticsQuality Assessment of Large Scale Knowledge ResourcesMontse CuadrosIXA NLP GroupEHU/UPVDonostia, Basque Countrymcuadros001@ikasle.ehu.esGerman RigauIXA NLP GroupEHU/UPVDonostia, Basque Countrygerman.rigau@ehu.esAbstractThis paper presents an empirical eval-uation of the quality of publicly avail-able large-scale knowledge resources.
Thestudy includes a wide range of manu-ally and automatically derived large-scaleknowledge resources.
In order to establisha fair and neutral comparison, the qual-ity of each knowledge resource is indi-rectly evaluated using the same method ona Word Sense Disambiguation task.
Theevaluation framework selected has beenthe Senseval-3 English Lexical SampleTask.
The study empirically demonstratesthat automatically acquired knowledge re-sources surpass both in terms of preci-sion and recall the knowledge resourcesderived manually, and that the combina-tion of the knowledge contained in theseresources is very close to the most frequentsense classifier.
As far as we know, this isthe first time that such a quality assessmenthas been performed showing a clear pic-ture of the current state-of-the-art of pub-licly available wide coverage semantic re-sources.1 IntroductionUsing large-scale semantic knowledge bases, suchas WordNet (Fellbaum, 1998), has become ausual, often necessary, practice for most currentNatural Language Processing systems.
Even now,building large and rich enough knowledge basesfor broad?coverage semantic processing takes agreat deal of expensive manual effort involvinglarge research groups during long periods of de-velopment.
This fact has severely hampered thestate-of-the-art of current Natural Language Pro-cessing (NLP) applications.
For example, dozensof person-years have been invested in the develop-ment of wordnets for various languages (Vossen,1998), but the data in these resources seems not tobe rich enough to support advanced concept-basedNLP applications directly.
It seems that applica-tions will not scale up to working in open domainswithout more detailed and rich general-purpose(and also domain-specific) linguistic knowledgebuilt by automatic means.For instance, in more than eight years of man-ual construction (from version 1.5 to 2.0), Word-Net passed from 103,445 semantic relations to204,074 semantic relations1.
That is, aroundtwelve thousand semantic relations per year.
How-ever, during the last years the research commu-nity has devised a large set of innovative processesand tools for large-scale automatic acquisition oflexical knowledge from structured or unstructuredcorpora.
Among others we can mention eX-tended WordNet (Mihalcea and Moldovan, 2001),large collections of semantic preferences acquiredfrom SemCor (Agirre and Martinez, 2001; Agirreand Martinez, 2002) or acquired from British Na-tional Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for each synset acquiredfrom the web (Agirre and de la Calle, 2004) oracquired from the BNC (Cuadros et al, 2005).Obviously, all these semantic resources havebeen acquired using a very different set of meth-ods, tools and corpora, resulting on a different setof new semantic relations between synsets.
In fact,each resource has different volume and accuracyfigures.
Although isolated evaluations have beenperformed by their developers in different experi-1Symmetric relations are counted only once.534mental settings, to date no comparable evaluationhas been carried out in a common and controlledframework.This work tries to establish the relative qual-ity of these semantic resources in a neutral envi-ronment.
The quality of each large-scale knowl-edge resource is indirectly evaluated on a WordSense Disambiguation (WSD) task.
In particular,we use a well defined WSD evaluation benchmark(Senseval-3 English Lexical Sample task) to eval-uate the quality of each resource.Furthermore, this work studies how these re-sources complement each other.
That is, to whichextent each knowledge base provides new knowl-edge not provided by the others.This paper is organized as follows: after thisintroduction, section 2 describes the large-scaleknowledge resources studied in this work.
Section3 describes the evaluation framework.
Section 4presents the evaluation results of the different se-mantic resources considered.
Section 5 provides aqualitative assessment of this empirical study andfinally, the conclusions and future work are pre-sented in section 6.2 Large Scale Knowledge ResourcesThis study covers a wide range of large-scaleknowledge resources: WordNet (WN) (Fell-baum, 1998), eXtended WordNet (Mihalcea andMoldovan, 2001), large collections of semanticpreferences acquired from SemCor (Agirre andMartinez, 2001; Agirre and Martinez, 2002) oracquired from the BNC (McCarthy, 2001), large-scale Topic Signatures for each synset acquiredfrom the web (Agirre and de la Calle, 2004) oracquired from the BNC (Cuadros et al, 2005).However, although these resources have beenderived using different WN versions, the researchcommunity has the technology for the automaticalignment of wordnets (Daude?
et al, 2003).
Thistechnology provides a mapping among synsets ofdifferent WN versions, maintaining the compati-bility to all the knowledge resources which usea particular WN version as a sense repository.Furthermore, this technology allows to port theknowledge associated to a particular WN versionto the rest of WN versions already connected.Using this technology, most of these resourcesare integrated into a common resource called Mul-tilingual Central Repository (MCR) (Atserias etal., 2004).
In particular, all WordNet versions, eX-tended WordNet, and the semantic preferences ac-quired from SemCor and BNC.2.1 Multilingual Central RepositoryThe Multilingual Central Repository (MCR)2 fol-lows the model proposed by the EuroWordNetproject.
EuroWordNet (Vossen, 1998) is a multi-lingual lexical database with wordnets for severalEuropean languages, which are structured as thePrinceton WordNet.
The Princeton WordNet con-tains information about nouns, verbs, adjectivesand adverbs in English and is organized around thenotion of a synset.
A synset is a set of words withthe same part-of-speech that can be interchangedin a certain context.
For example, <party, po-litical party> form a synset because they can beused to refer to the same concept.
A synset isoften further described by a gloss, in this case:?an organization to gain political power?.
Finally,synsets can be related to each other by semanticrelations, such as hyponymy (between specific andmore general concepts), meronymy (between partsand wholes), cause, etc.The current version of the MCR (Atserias et al,2004) is a result of the 5th Framework MEANINGproject.
The MCR integrates into the same Eu-roWordNet framework wordnets from five differ-ent languages (together with four English Word-Net versions).
The MCR also integrates WordNetDomains (Magnini and Cavaglia`, 2000) and newversions of the Base Concepts and Top ConceptOntology.
The final version of the MCR contains1,642,389 semantic relations between synsets,most of them acquired by automatic means.
Thisrepresents almost one order of magnitude largerthan the Princeton WordNet (204,074 unique se-mantic relations in WordNet 2.0).
Table 1 summa-rizes the main sources for semantic relations inte-grated into the MCR.Table 2 shows the number of semantic relationsbetween synsets pairs in the MCR and its overlap-pings.
Note that, most of the relations in the MCRbetween synsets-pairs are unique.Hereinafter we will refer to each semantic re-source as follows:?
WN (Fellbaum, 1998): This knowledge re-source uses the direct relations encoded inWordNet 1.6 or 2.0.
We also tested WN-2(using relations at distance 1 and 2) and WN-3 (using relations at distance 1, 2 and 3).2http://nipadio.lsi.upc.es/?nlp/meaning535Source #relationsPrinceton WN1.6 138,091Selectional Preferences from SemCor 203,546Selectional Preferences from the BNC 707,618New relations from Princeton WN2.0 42,212Gold relations from eXtended WN 17,185Silver relations from eXtended WN 239,249Normal relations from eXtended WN 294,488Total 1,642,389Table 1: Main sources of semantic relationsType of Relations #relationsTotal Relations 1,642,389Different Relations 1,531,380Unique Relations 1,390,181Non-unique relations (>1) 70,425Non-unique relations (>2) 341Non-unique relations (>3) 8Table 2: Overlapping relations in the MCR?
XWN (Mihalcea and Moldovan, 2001): Thisknowledge resource uses the direct relationsencoded in eXtended WordNet.?
XWN+WN: This knowledge resource usesthe direct relations included in WN andXWN.?
spBNC (McCarthy, 2001): This knowledgeresource contains the selectional preferencesacquired from the BNC.?
spSemCor (Agirre and Martinez, 2001;Agirre and Martinez, 2002): This knowledgeresource contains the selectional preferencesacquired from SemCor.?
spBNC+spSemCor: This knowledge re-source uses the selectional preferences ac-quired from the BNC and SemCor.?
MCR (Atserias et al, 2004): This knowledgeresource uses the direct relations included inMCR.2.2 Automatically retrieved Topic SignaturesTopic Signatures (TS) are word vectors related toa particular topic (Lin and Hovy, 2000).
TopicSignatures are built by retrieving context wordsof a target topic from large volumes of text.
Inour case, we consider word senses as topics.
Ba-sically, the acquisition of TS consists of A) ac-quiring the best possible corpus examples for aparticular word sense (usually characterizing eachword sense as a query and performing a search onthe corpus for those examples that best match thequeries), and then, B) building the TS by deriv-ing the context words that best represent the wordsense from the selected corpora.For this study, we use the large-scale Topic Sig-natures acquired from the web (Agirre and de laCalle, 2004) and those acquired from the BNC(Cuadros et al, 2005).?
TSWEB3: Inspired by the work of (Lea-cock et al, 1998), these Topic Signatureswere constructed using monosemous rela-tives from WordNet (synonyms, hypernyms,direct and indirect hyponyms, and siblings),querying Google and retrieving up to onethousand snippets per query (that is, a wordsense).
In particular, the method was as fol-lows:?
Organizing the retrieved examples fromthe web in collections, one collectionper word sense.?
Extracting the words and their frequen-cies for each collection.?
Comparing these frequencies with thosepertaining to other word senses usingTFIDF (see formula 1).?
Gathering in an ordered list, the wordswith distinctive frequency for one of thecollections, which constitutes the TopicSignature for the respective word sense.This constitutes the largest available seman-tic resource with around 100 million relations(between synsets and words).?
TSBNC: These Topic Signatures have beenconstructed using ExRetriever4, a flexibletool to perform sense queries on large cor-pora.?
This tool characterizes each sense of aword as a specific query using a declar-ative language.?
This is automatically done by using aparticular query construction strategy,defined a priori, and using informationfrom a knowledge base.In this study, ExRetriever has been evaluatedusing the BNC, WN as a knowledge base and3http://ixa.si.ehu.es/Ixa/resources/sensecorpus4http://www.lsi.upc.es/?nlp/meaning/downloads.html536TFIDF (as shown in formula 1) (Agirre andde la Calle, 2004)5.TFIDF (w,C) = wfwmaxwwfw ?
logNCfw (1)Where w stands for word context, wf for theword frecuency, C for Collection (all the cor-pus gathered for a particular word sense), andCf stands for the Collection frecuency.In this study we consider two different querystrategies:?
Monosemous A (queryA): (ORmonosemous-words).
That is, the unionset of all synonym, hyponym and hyper-onym words of a WordNet synset which aremonosemous nouns (these words can haveother senses as verbs, adjectives or adverbs).?
Monosemous W (queryW): (ORmonosemous-words).
That is, the unionset of all words appearing as synonyms,direct hyponyms, hypernyms indirect hy-ponyms (distance 2 and 3) and siblings.
Inthis case, the nouns collected are monose-mous having no other senses as verbs,adjectives or adverbs.While TSWEB use the query constructionqueryW, ExRetriever use both.3 Indirect Evaluation on Word SenseDisambiguationIn order to measure the quality of the knowl-edge resources described in the previous section,we performed an indirect evaluation by using allthese resources as Topic Signatures (TS).
That is,word vectors with weights associated to a partic-ular synset which are obtained by collecting thoseword senses appearing in the synsets directly re-lated to them 6.
This simple representation tries tobe as neutral as possible with respect to the evalu-ation framework.All knowledge resources are indirectly evalu-ated on a WSD task.
In particular, the noun-set5Although other measures have been tested, such as Mu-tual Information or Association Ratio, the best results havebeen obtained using TFIDF formula.6A weight of 1 is given when the resource do not has as-sociated weight.of Senseval-3 English Lexical Sample task whichconsists of 20 nouns.
All performances are evalu-ated on the test data using the fine-grained scoringsystem provided by the organizers.Furthermore, trying to be as neutral as possi-ble with respect to the semantic resources studied,we applied systematically the same disambigua-tion method to all of them.
Recall that our maingoal is to establish a fair comparison of the knowl-edge resources rather than providing the best dis-ambiguation technique for a particular semanticknowledge base.A common WSD method has been applied toall knowledge resources.
A simple word over-lapping counting (or weighting) is performed be-tween the Topic Signature and the test example7.Thus, the occurrence evaluation measure countsthe amount of overlapped words and the weightevaluation measure adds up the weights of theoverlapped words.
The synset having higher over-lapping word counts (or weights) is selected for aparticular test example.
However, for TSWEB andTSBNC the better results have been obtained us-ing occurrences (the weights are only used to or-der the words of the vector).
Finally, we shouldremark that the results are not skewed (for in-stance, for resolving ties) by the most frequentsense in WN or any other statistically predictedknowledge.Figure 3 presents an example of Topic Signa-ture from TSWEB using queryW and the web andfrom TSBNC using queryA and the BNC for thefirst sense of the noun party.
Although both auto-matically acquired TS seem to be closely related tothe first sense of the noun party, they do not havewords in common.As an example, table 4 shows a test example ofSenseval-3 corresponding to the first sense of thenoun party.
In bold there are the words that ap-pear in TSBNC-queryA.
There are several impor-tant words that appear in the text that also appearin the TS.4 Evaluating the quality of knowledgeresourcesIn order to establish a clear picture of the currentstate-of-the-art of publicly available wide cover-age knowledge resources we also consider a num-ber of basic baselines.7We also consider multiword terms.537democratic 0.0126 socialist 0.0062tammany 0.0124 organization 0.0060alinement 0.0122 conservative 0.0059federalist 0.0115 populist 0.0053missionary 0.0103 dixiecrats 0.0051whig 0.0099 know-nothing 0.0049greenback 0.0089 constitutional 0.0045anti-masonic 0.0083 pecking 0.0043nazi 0.0081 democratic-republican 0.0040republican 0.0074 republicans 0.0039alcoholics 0.0073 labor 0.0039bull 0.0070 salvation 0.0038party 4.9350 trade 1.5295political 3.7722 parties 1.4083government 2.4129 politics 1.2703election 2.2265 campaign 1.2551policy 2.0795 leadership 1.2277support 1.8537 movement 1.2156leader 1.8280 general 1.2034years 1.7128 public 1.1874people 1.7044 member 1.1855local 1.6899 opposition 1.1751conference 1.6702 unions 1.1563power 1.6105 national 1.1474Table 3: Topic Signatures for party#n#1 using TSWEB (24 out of 15881 total words) and TS-BNC(queryA) with TFIDF (24 out of 9069 total words)<instance id=?party.n.bnc.00008131?
docsrc=?BNC?> <context> Up to the late 1960s , catholic nationalists were split betweentwo main political groupings .
There was the Nationalist Party , a weak organization for which local priests had to providesome kind of legitimation .
As a <head>party</head> , it really only exercised a modicum of power in relation to the Stormontadministration .
Then there were the republican parties who focused their attention on Westminster elections .
The disorganizednature of catholic nationalist politics was only turned round with the emergence of the civil rights movement of 1968 and thesubsequent forming of the SDLP in 1970 .
</context> </instance>Table 4: Example of test num.
00008131 for party#n which its correct sense is 14.1 BaselinesWe have designed several baselines in order to es-tablish a relative comparison of the performanceof each semantic resource:?
RANDOM: For each target word, thismethod selects a random sense.
This baselinecan be considered as a lower-bound.?
WordNet MFS (WN-MFS): This methodselects the most frequent sense (the first sensein WordNet) of the target word.?
TRAIN-MFS: This method selects the mostfrequent sense in the training corpus of thetarget word.?
Train Topic Signatures (TRAIN): Thisbaseline uses the training corpus to directlybuild a Topic Signature using TFIDF measurefor each word sense.
Note that in this case,this baseline can be considered as an upper-bound of our evaluation framework.Table 5 presents the F1 measure (harmonicmean of recall and precision) of the different base-lines.
In this table, TRAIN has been calculatedwith a fixed vector size of 450 words.
As ex-pected, RANDOM baseline obtains the poorestresult while the most frequent sense of Word-Net (WN-MFS) is very close to the most frequentsense of the training corpus (TRAIN-MFS), butBaselines F1TRAIN 65.1TRAIN-MFS 54.5WN-MFS 53.0RANDOM 19.1Table 5: Baselinesboth are far below to the Topic Signatures acquiredusing the training corpus (TRAIN).4.2 Performance of the knowledge resourcesTable 6 presents the performance of each knowl-edge resource uploaded into the MCR and the av-erage size of its vectors.
In bold appear the bestresults for precision, recall and F1 measures.
Thelowest result is obtained by the knowledge directlygathered from WN mainly because of its poor cov-erage (Recall of 17.6 and F1 of 25.6).
Its perfor-mance is improved using words at distance 1 and2 (F1 of 33.3), but it decreases using words at dis-tance 1, 2 and 3 (F1 of 30.4).
The best precision isobtained by WN (46.7), but the best performanceis achieved by the combined knowledge of MCR-spBNC8 (Recall of 42.9 and F1 of 44.1).
This rep-resents a recall 18.5 points higher than WN.
Thatis, the knowledge integrated into the MCR (Word-Net, eXtended WordNet and the selectional prefer-ences acquired from SemCor) although partly de-rived by automatic means performs much better8MCR without Selectional Preferences from BNC538KB P R F1 Av.
SizeMCR-spBNC 45.4 42.9 44.1 115MCR 41.8 40.4 41.1 235spSemCor 43.1 38.7 40.8 56spBNC+spSemCor 41.4 30.1 40.7 184WN+XWN 45.5 28.1 34.7 68WN-2 38.0 29.7 33.3 72XWN 45.0 25.6 32.6 55WN-3 31.6 29.3 30.4 297spBNC 36.3 25.4 29.9 128WN 46.7 17.6 25.6 13Table 6: P, R and F1 fine-grained results for theresources integrated into the MCR.in terms of recall and F1 measures than using theknowledge currently present in WN alone (with asmall decrease in precision).
It also seems that theknowledge from spBNC always degrades the per-formance of their combinations9.Regarding the baselines, all knowledge re-sources integrated into the MCR surpass RAN-DOM, but none achieves neither WN-MFS,TRAIN-MFS nor TRAIN.Figure 1 plots F1 results of the fine-grainedevaluation on the nominal part of the English lex-ical sample of Senseval-3 of the baselines (in-cluding upper and lower-bounds), the knowledgebases integrated into the MCR, the best perform-ing Topic Signatures acquired from the web andthe BNC evaluated individually and in combina-tion with others.
The figure presents F1 (Y-axis)in terms of the size of the word vectors (X-axis)10.In order to evaluate more deeply the quality ofeach knowledge resource, we also provide someevaluations of the combined outcomes of severalknowledge resources.
The combinations are per-formed following a very simple voting method:first, for each knowledge resource, the scoring re-sults obtained for each word sense are normal-ized, and then, for each word sense, the normal-ized scores are added up selecting the word sensewith higher score.Regarding Topic Signatures, as expected, ingeneral the knowledge gathered from the web(TSWEB) is superior to the one acquired from theBNC either using queryA or queryW (TSBNC-queryA and TSBNC-queryW).
Interestingly, theperformance of TSBNC-queryA when using the9All selectional preferences acquired from SemCor or theBNC have been considered including those with very lowconfidence score.10Only varying the size of TS for TSWEB and TSBNC.first two hundred words of the TS is slightly bet-ter than using queryW (both using the web or theBNC).Although TSBNC-queryA and TSBNC-queryW perform very similar, both knowledgeresources contain different knowledge.
This isshown when combining the outcomes of thesetwo different knowledge resources with TSWEB.While no improvement is obtained when com-bining the knowledge acquired from the weband the BNC when using the same acquisitionmethod (queryW), the combination of TSWEBand TSBNC-queryA (TSWEB+ExRetA) obtainsbetter F1 results than TSWEB (TSBNC-queryAhave some knowledge not included into TSWEB).Surprisingly, the knowledge integrated into theMCR (MCR-spBNC) surpass the knowledge fromTopic Signatures acquired from the web or theBNC, using queryA, queryW or their combina-tions.Furthermore, the combination of TSWEB andMCR-spBNC (TSWEB+MCR-spBNC) outper-forms both resources individually indicating thatboth knowledge bases contain complementary in-formation.
The maximum is achieved with TSvectors of at most 700 words (with 49.3% preci-sion, 49.2% recall and 49.2% F1).
In fact, theresulting combination is very close to the mostfrequent sense baselines.
This fact indicates thatthe resulting large-scale knowledge base almostencodes the knowledge necessary to behave as amost frequent sense tagger.4.3 Senseval-3 system performancesFor sake of comparison, tables 7 and 8 present theF1 measure of the fine-grained results for nounsof the Senseval-3 lexical sample task for the bestand worst unsupervised and supervised systems,respectively.
We also include in these tables someof the baselines and the best performing combina-tion of knowledge resources (including TSWEBand MCR-spBNC)11.
Regarding the knowledgeresources evaluated in this study, the best com-bination (including TSWEB and MCR-spBNC)achieves an F1 measure much better than some su-pervised and unsupervised systems and it is closeto the most frequent sense of WordNet (WN-MFS)and to the most frequent sense of the training cor-pora (TRAIN-MFS).11Although we maintain the classification of the organiz-ers, system s3 wsdiit used the train data.539Figure 1: Fine-grained evaluation results for the knowledge resourcess3 systems F1s3 wsdiit 68.0WN-MFS 53.0Comb TSWEB MCR-spBNC 49.2s3 DLSI 17.8Table 7: Senseval-3 Unsupervised Systemss3 systems F1htsa3 U.Bucharest (Grozea) 74.2TRAIN 65.1TRAIN-MFS 54.5DLSI-UA-LS-SU U.Alicante (Vazquez) 41.0Table 8: Senseval-3 Supervised SystemsWe must recall that the main goal of this re-search is to establish a clear and neutral view of therelative quality of available knowledge resources,not to provide the best WSD algorithm using theseresources.
Obviously, much more sophisticatedWSD systems using these resources could be de-vised.5 Quality AssessmentSummarizing, this study provides empirical evi-dence for the relative quality of publicly avail-able large-scale knowledge resources.
The rela-tive quality has been measured indirectly in termsof precision and recall on a WSD task.The study empirically demonstrates that auto-matically acquired knowledge bases clearly sur-pass both in terms of precision and recall theknowledge manually encoded from WordNet (us-ing relations expanded to one, two or three levels).Surprisingly, the knowledge contained into theMCR (WordNet, eXtended WordNet, SelectionalPreferences acquired automatically from SemCor)is of a better quality than the automatically ac-quired Topic Signatures.
In fact, the knowledgeresulting from the combination of all these large-scale resources outperforms each resource indi-vidually indicating that these knowledge basescontain complementary information.
Finally, weshould remark that the resulting combination isvery close to the most frequent sense classifiers.Regarding the automatic acquisition of large-scale Topic Signatures it seems that those ac-quired from the web are slightly better than thoseacquired from smaller corpora (for instance, theBNC).
It also seems that queryW performs betterthan queryA but that both methods (queryA and540queryW) also produce complementary knowledge.Finally, it seems that the weights are not useful formeasuring the strength of a vote (they are only use-ful for ordering the words in the Topic Signature).6 Conclusions and future workDuring the last years the research community hasderived a large set of semantic resources using avery different set of methods, tools and corpus, re-sulting on a different set of new semantic relationsbetween synsets.
In fact, each resource has dif-ferent volume and accuracy figures.
Although iso-lated evaluations have been performed by their de-velopers in different experimental settings, to dateno complete evaluation has been carried out in acommon framework.In order to establish a fair comparison, the qual-ity of each resource has been indirectly evaluatedin the same way on a WSD task.
The evaluationframework selected has been the Senseval-3 En-glish Lexical Sample Task.
The study empiricallydemonstrates that automatically acquired knowl-edge bases surpass both in terms of precision andrecall to the knowledge bases derived manually,and that the combination of the knowledge con-tained in these resources is very close to the mostfrequent sense classifier.Once empirically demonstrated that the knowl-edge resulting from MCR and Topic Signatures ac-quired from the web is complementary and closeto the most frequent sense classifier, we plan tointegrate the Topic Signatures acquired from theweb (of about 100 million relations) into the MCR.This process will be performed by disambiguat-ing the Topic Signatures.
That is, trying to obtainword sense vectors instead of word vectors.
Thiswill allow to enlarge the existing knowledge basesin several orders of magnitude by fully automaticmethods.
Other evaluation frameworks such as PPattachment will be also considered.7 AcknowledgementsThis work is being funded by the IXA NLPgroup from the Basque Country Univer-sity, EHU/UPV-CLASS project and BasqueGovernment-ADIMEN project.
We would liketo thank also the three anonymous reviewers fortheir valuable comments.ReferencesE.
Agirre and O. Lopez de la Calle.
2004.
Publiclyavailable topic signatures for all wordnet nominalsenses.
In Proceedings of LREC, Lisbon, Portugal.E.
Agirre and D. Martinez.
2001.
Learning class-to-class selectional preferences.
In Proceedings ofCoNLL, Toulouse, France.E.
Agirre and D. Martinez.
2002.
Integrating selec-tional preferences in wordnet.
In Proceedings ofGWC, Mysore, India.J.
Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,B.
Magnini, and Piek Vossen.
2004.
The meaningmultilingual central repository.
In Proceedings ofGWC, Brno, Czech Republic.M.
Cuadros, L.
Padro?, and G. Rigau.
2005.
Compar-ing methods for automatic acquisition of topic sig-natures.
In Proceedings of RANLP, Borovets, Bul-garia.J.
Daude?, L.
Padro?, and G. Rigau.
2003.
Validation andTuning of Wordnet Mapping Techniques.
In Pro-ceedings of RANLP, Borovets, Bulgaria.C.
Fellbaum, editor.
1998.
WordNet.
An ElectronicLexical Database.
The MIT Press.C.
Leacock, M. Chodorow, and G. Miller.
1998.
Us-ing Corpus Statistics and WordNet Relations forSense Identification.
Computational Linguistics,24(1):147?166.C.
Lin and E. Hovy.
2000.
The automated acquisitionof topic signatures for text summarization.
In Pro-ceedings of COLING.
Strasbourg, France.B.
Magnini and G. Cavaglia`.
2000.
Integrating subjectfield codes into wordnet.
In Proceedings of LREC,Athens.
Greece.D.
McCarthy.
2001.
Lexical Acquisition at the Syntax-Semantics Interface: Diathesis Aternations, Sub-categorization Frames and Selectional Preferences.Ph.D.
thesis, University of Sussex.R.
Mihalcea and D. Moldovan.
2001. extended word-net: Progress report.
In Proceedings of NAACLWorkshop on WordNet and Other Lexical Resources,Pittsburgh, PA.P.
Vossen, editor.
1998.
EuroWordNet: A MultilingualDatabase with Lexical Semantic Networks .
KluwerAcademic Publishers .541
