Proceedings of NAACL HLT 2009: Short Papers, pages 281?284,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTightly coupling Speech Recognition and SearchTaniya MishraAT&T Labs-Research180 Park AveFlorham Park, NJ 07932taniya@research.att.comSrinivas BangaloreAT&T Labs-Research180 Park AveFlorham Park, NJ 07932srini@research.att.comAbstractIn this paper, we discuss the benefits of tightlycoupling speech recognition and search com-ponents in the context of a speech-drivensearch application.
We demonstrate that by in-corporating constraints from the informationrepository that is being searched not only im-proves the speech recognition accuracy butalso results in higher search accuracy.1 IntroductionWith the exponential growth in the use of mobile de-vices in recent years, the need for speech-driven in-terfaces is becoming apparent.
The limited screenspace and soft keyboards of mobile devices make itcumbersome to type in text input.
Furthermore, bythe mobile nature of these devices, users often wouldlike to use them in hands-busy environments, rulingout the possibility of typing text.In this paper, we focus on the problem of speech-driven search to access information repositories us-ing mobile devices.
Such an application typicallyuses a speech recognizer (ASR) for transforming theuser?s speech input to text and a search componentthat uses the resulting text as a query to retrievethe relevant documents from the information reposi-tory.
For the purposes of this paper, we use the busi-ness listings containing the name, address and phonenumber of businesses as the information repository.Most of the literature on speech-driven search ap-plications that are available in the consumer mar-ket (Acero et al, 2008; Bacchiani et al, 2008;VLingo FIND, 2009) have quite rightly emphasizedthe importance of the robustness of the ASR lan-guage model and the data needed to build such a ro-bust language model.
We acknowledge that this is asignificant issue for building such systems, and weprovide our approach to creating a language model.However, in contrast to most of these systems thattreat speech-driven search to be largely an ASRproblem followed by a Search problem, in this pa-per, we show the benefits of tightly coupling ASRand Search tasks and illustrate techniques to im-prove the accuracy of both components by exploit-ing the co-constraints between the two components.The outline of the paper is as follows.
In Sec-tion 2, we discuss the set up of our speech-drivenapplication.
In Section 3, we discuss our method tointegrating the speech and search components.
Wepresent the results of the experiments in Section 4and conclude in Section 5.2 Speech-driven SearchWe describe the speech-driven search application inthis section.
The user of this application providesa speech utterance to a mobile device intending tosearch for the address and phone number of a busi-ness.
The speech utterance typically contains a busi-ness name, optionally followed by a city and stateto indicate the location of the business (e.g.
pizzahut near urbana illinois.).
User input with a busi-ness category (laundromats in madison) and withoutlocation information (hospitals) are some variantssupported by this application.
The result of ASR isused to search a business listing database of over 10million entries to retrieve the entries pertinent to theuser query.The ASR used to recognize these utterances in-corporates an acoustic model adapted to speech col-lected from mobile devices and a trigram languagemodel that is built from over 10 million text querylogs obtained from the web-based text-driven ver-sion of this application.
The 1-best speech recogni-tion output is used to retrieve the relevant businesslisting entries.2813 Tightly coupling ASR and SearchAs mentioned earlier, most of the speech-drivensearch systems use the the 1-best output from theASR as the query for the search component.
Giventhat ASR 1-best output is likely to be erroneous,this serialization of the ASR and search componentsmight result in sub-optimal search accuracy.
As willbe shown in our experiments, the oracle word/phraseaccuracy using n-best hypotheses is far greater thanthe 1-best output.
However, using each of the n-best hypothesis as a query to the search compo-nent is computationally sub-optimal since the stringsin the n-best hypotheses usually share large subse-quences with each other.
A lattice representationof the ASR output, in particular, a word-confusionnetwork (WCN) transformation of the lattice, com-pactly encodes the n-best hypothesis with the flexi-bility of pruning alternatives at each word position.An example of a WCN is shown in Figure 1.
In or-der to obtain a measure of the ambiguity per wordposition in the WCN, we define the (average) arcdensity of a WCN as the ratio of the total numberof arcs to the number of states in the WCN.
As canbe seen, with very small increase in arc density, thenumber of paths that are encoded in the WCN canbe increased exponentially.
In Figure 2, we showthe improvement in oracle-path word and phrase ac-curacies as a function of the arc density for our dataset.
Oracle-path is a path in the WCN that has theleast edit-distance (Levenshtein, 1966) to the refer-ence string.
It is interesting to note that the oracleaccuracies can be improved by almost 10% absoluteover the 1-best accuracy with small increase in thearc density.01ballys/0.317audi/2.126ballew/4.704bally/3.625ellies/4.037elliot/4.372elliott/4.5132/1automobiles/6.735Figure 1: A sample word confusion network3.1 Representing Search Index as an FSTIn order to exploit WCNs for Search, we have im-plemented our own search engine instead of using an1 1.17 1.26 1.37 1.53 1.72 1.935456586062646668707274Arc DensitiesAccuracy (in %)Word accuracyPhrase accuracyFigure 2: Oracle accuracy graph for the WCNs at differ-ent arc densities0audi:audi_repair/c1audi:audi_automobile_dealers/c2automobile:automobile_salvage/c3automobile:audi_automobile_dealers/c4ballys:ballys_hotel/c5ballys:ballys_fitness/c6ballys:ballys_fitness/c6Figure 3: An example of an FST representing the searchindexoff-the-shelf search engine such as Lucene (Hatcherand Gospodnetic., 2004).
We index each businesslisting (d) in our data that we intend to search usingthe words (wd) in that listing.
The pair (wd, d) isassigned a weight (c(wd,d)) using different metrics,including the standard tf ?
idf , as explained below.This index is represented as a weighted finite-statetransducer (SearchFST) as shown in Figure 3 wherewd is the input symbol, d is the output symbol andc(wd,d) is the weight of that arc.3.2 Relevance MetricsIn this section, we describe six different weightingmetrics used to determine the relevance of a docu-ment for a given query word that we have experi-mented with in this paper.idfw: idfw refers to the inverse document fre-quency of the word, w, which is computed asln(D/dw), where D refers to the total numberof documents in the collection, and dw refers tothe total number of documents in the collectionthat contain the word, w (Robertson and Jones,1997; Robertson, 2004).atfw: atfw refers to average term frequency, whichis computed as cfw/dw (Pirkola et al, 2002).cfw ?
idfw: Here cfw refers to the collection fre-quency, which is simply the total number of oc-currences of the word, w in the collection.282atfw ?
idfw: (Each term as described above).?
fw,d|dw| ?
idfw: Here fw,d refers to the frequency ofthe word, w, in the document, d, whereas |dw|is the length of the document, d, in which theword, w, occurs.cfw?|dw| ?
idfw: (Each term as described above).3.3 SearchBy composing a query (Qfst) (either a 1-beststring represented as a finite-state acceptor, or aWCN), with the SearchFST, we obtain all the arcs(wq, dwq , c(wq ,dwq )) where wq is a query word, dwqis a listing with the query word and, c(wq ,dwq ) is theweight associated with that pair.
Using this informa-tion, we aggregate the weight for a listing (dq) acrossall query words and rank the retrieved listings in thedescending order of this aggregated weight.
We se-lect the top N listings from this ranked list.
Thequery composition, listing weight aggregation andselection of top N listings are computed with finite-state transducer operations.In Figure 4, we illustrate the result of rerankingthe WCN shown in Figure 1 using the search rele-vance weights of each word in the WCN.
It must benoted that the least cost path1 for the WCN in Fig-ure 1 is ballys automobiles while the reranked 1-bestoutput in Figure 4 is audi automobiles.
Given thatthe user voice query was audi automobiles, the list-ings retrieved from the 1-best output after rerankingare much more relevant than those retrieved beforereranking, as shown in Table 1.01audi/2.100ballys/2.2762/4automobiles/0.251Figure 4: A WCN rescored using word-level search rele-vance weights.4 Experiments and ResultsWe took 852 speech queries collected from users us-ing a mobile device based speech search application.We ran the speech recognizer on these queries us-ing the language model described in Section 2 andcreated word-confusion networks such as those il-lustrated in Figure 1.
These 852 utterances weredivided into 300 utterances for the development setand 552 for the test set.1We transform the scores into costs and search for minimumcost paths.Before rescoring After rescoringballys intl auburn audi repairlos angeles ca auburn waballys las vegas audi bellevue repairlas vegas nv bellevue waballys las health spa university audi seattle walas vegas nvballys cleaners beverly hills audipalm desert ca los angeles caballys brothers audi independent repairsyorba linda ca by eurotech livermore caTable 1: Listings retrieved for query audi automobilesbefore and after ASR WCNs were rescored using searchrelevance weights.4.1 ASR ExperimentsThe baseline ASR word and sentence (completestring) accuracies on the development set are 63.1%and 57.0% while those on the test set are 65.1% and55.3% respectively.Metric Word Sent.
Scaling ADAcc.
Acc.
Factoridfw 63.1 57.0 10?3 allcfw ?
idfw 63.5 58.3 15 ?
10?4 1.37atfw 63.6 57.3 1 allatfw ?
idf 63.1 57.0 10?3 all?
fw,d|dfw| ?
idf 63.9 58.3 15 ?
10?4 1.25cfw?|dfw|?
idfw 63.5 57.3 1 allTable 2: Performance of the metrics used for rescoringthe WCNs output by ASR.
(AD refers to arc density.
)In Table 2, we summarize the improvements ob-tained by rescoring the ASRWCNs based on the dif-ferent metrics used for computing the word scoresaccording to the search criteria.
The largest im-provement in word and sentence accuracies is ob-tained by using the rescoring metric: ?
fw,d|dfw| ?
idf .The word-level accuracy improved from the baselineaccuracy of 63.1% to 63.9% after rescoring whilethe sentence-level accuracy improved from 57.0%to 58.3%.
Thus, this rescoring metric, and the cor-responding pruning AD and the scaling factor wasused to rerank the 552 WCNs in the test set.
Afterrescoring, on the test set, the word-level accuracyimproved from 65.1% to 65.9% and sentence-levelaccuracy improved from 55.3% to 56.2%.283Number of Scores Baseline RerankeddocumentsAllPrecision 0.708 0.728DocumentsRecall 0.728 0.742F-Score 0.718 0.735Table 3: Table showing the relevancy of the search resultsobtained by the baseline ASR output compared to thoseobtained by the reranked ASR output.4.2 Search ExperimentsTo analyze the Search accuracy of the baseline ASRoutput in comparison to the ASR output, rerankedusing the ?
fw,d|dfw| ?
idf reranking metric, we usedeach of the two sets of ASR outputs (i.e., base-line and reranked) as queries to our search engine,SearchFST (described in Section 3).
For the searchresults produced by each set of queries, we com-puted the precision, recall, and F-score values of thelistings retrieved with respect to the listings retrievedby the set of human transcribed queries (Reference).The precision, recall, and F-scores for the baselineASR output and the reranked ASR output, averagedacross each set, is presented in Table 3.
For the pur-poses of this experiment, we assume that the set re-turned by our SearchFST for the human transcribedset of queries is the reference search set.
This ishowever an approximation for a human annotatedsearch set.In Table 3, by comparing the search accuracyscores corresponding to the baseline ASR output tothose corresponding to the reranked ASR output, wesee that reranking the ASR output using the informa-tion repository produces a substantial improvementin the accuracy of the search results.It is interesting to note that even though thereranking of the ASR as shown in Table 2 is of theorder of 1%, the improvement in Search accuracy issubstantially higher.
This indicates to the fact thatexploiting constraints from both components resultsin improving the recognition accuracy of that subsetof words that are more relevant for Search.5 ConclusionIn this paper, we have presented techniques fortightly coupling ASR and Search.
The central ideabehind these techniques is to rerank the ASR out-put using the constraints (encoded as relevance met-rics) from the Search task.
The relevance metric thatbest improved accuracy is ?
fw,d|dw| ?
idfw, as deter-mined on our development set.
Using this metricto rerank the ASR output of our test set, we im-proved ASR accuracy from 65.1% to 65.9% at theword-level and from 55.3% to 56.2% at the phraselevel.
This reranking also improved the F-score ofthe search component from 0.718 to 0.735.
Theseresults bear out our expectation that tightly couplingASR and Search can improve the accuracy of bothcomponents.Encouraged by the results of our experiments, weplan to explore other relevance metrics that can en-code more sophisticated constraints such as the rel-ative coherence of the terms within a query.AcknowledgmentsThe data used in this work is partly derived from theSpeak4It voice search prototype.
We wish to thankevery member of that team for having deployed thatvoice search system.ReferencesA.
Acero, N. Bernstein, R.Chambers, Y. Ju, X. Li,J.
Odell, O. Scholtz P. Nguyen, and G. Zweig.
2008.Live search for mobile: Web services by voice on thecellphone.
In Proceedings of ICASSP 2008, Las Ve-gas.M.
Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,and B. Strope.
2008.
Deploying GOOG-411: Earlylesstons in data, measurement and testing.
In Proceed-ings of ICASSP 2008, Las Vegas.E.
Hatcher and O. Gospodnetic.
2004.
Lucene in Action(In Action series).
Manning Publications Co., Green-wich, CT, USA.V.I.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertion and reversals.
Soviet PhysicsDoklady, 10:707?710.A.
Pirkola, E. Lepaa?nen, and K. Ja?rvelin.
2002.
The?ratf?
formula (kwok?s formula): exploiting averageterm frequency in cross-language retrieval.
Informa-tion Research, 7(2).S.
E. Robertson and K. Sparck Jones.
1997.
Simpleproven approaches to text retrieval.
Technical report,Cambridge University.Stephen Robertson.
2004.
Understanding inverse doc-ument frequency: On theoretical arguments for idf.Journal of Documentation, 60.VLingo FIND, 2009.http://www.vlingomobile.com/downloads.html.284
