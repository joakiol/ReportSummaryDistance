Proceedings of the SIGDIAL 2013 Conference, pages 423?432,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsA Simple and Generic Belief Tracking Mechanism for the Dialog StateTracking Challenge: On the believability of observed informationZhuoran Wang and Oliver LemonInteraction Lab, MACS, Heriot-Watt UniversityEdinburgh, EH14 4AS, United Kingdom{zhuoran.wang; o.lemon}@hw.ac.ukAbstractThis paper presents a generic dialoguestate tracker that maintains beliefs overuser goals based on a few simple domain-independent rules, using basic probabilityoperations.
The rules apply to observedsystem actions and partially observableuser acts, without using any knowledgeobtained from external resources (i.e.without requiring training data).
The coreinsight is to maximise the amount of in-formation directly gainable from an error-prone dialogue itself, so as to better lower-bound one?s expectations on the perfor-mance of more advanced statistical tech-niques for the task.
The proposed methodis evaluated in the Dialog State Track-ing Challenge, where it achieves compara-ble performance in hypothesis accuracy tomachine learning based systems.
Conse-quently, with respect to different scenariosfor the belief tracking problem, the poten-tial superiority and weakness of machinelearning approaches in general are investi-gated.1 IntroductionSpoken dialogue system (SDS) can be modelledas a decision process, in which one of the mainproblems researchers try to overcome is the un-certainty in tracking dialogue states due to error-prone outputs from automatic speech recognition(ASR) and spoken language understanding (SLU)components (Williams, 2012).
Recent advancesin SDS have demonstrated that maintaining a dis-tribution over a set of possible (hidden) dialoguestates and optimising dialogue policies with re-spect to long term expected rewards can signifi-cantly improve the interaction performance (Royet al 2000; Williams and Young, 2007a).
Suchmethods are usually developed under a partiallyobservable Markov decision process (POMDP)framework (Young et al 2010; Thomson andYoung, 2010; Williams, 2010), where the distribu-tion over dialogue states is called a ?belief?
and ismodelled as a posterior updated every turn givenan observation.
Furthermore, instead of simplytaking the most probable (or highest confidencescore) hypothesis of the user act as in ?traditional?handcrafted systems, the observation here mayconsist of an n-best list of the SLU hypotheses (di-alogue acts) with (normalised) confidence scores.See (Henderson and Lemon, 2008; Williams andYoung, 2007b; Thomson et al 2010; Young et al2013) for more details of POMDP-based SDS.It is understandable that beliefs more accuratelyestimating the true dialogue states will ease thetuning of dialogue policies, and hence can resultin better overall system performance.
The accu-racy of belief tracking has been studied in depthby Williams (2012) based on two SDS in publicuse.
Here the effects of several mechanisms areanalysed, which can alter the ?most-believed?
dia-logue state hypothesis (computed using a genera-tive POMDP model) from the one derived directlyfrom an observed top SLU hypothesis.
Williams?swork comprehensively explores how and why amachine learning approach (more specifically thegenerative model proposed in (Williams, 2010))functions in comparison with a naive baseline.However, we target a missing intermediate anal-ysis in this work: how much information onecan gain purely from the SLU n-best lists (andthe corresponding confidence scores), without anyprior knowledge either being externally learned(using data-driven methods) or designed (based ondomain-specific strategies), but beyond only con-sidering the top SLU hypotheses.
We explain thisidea in greater detail as follows.Firstly, we can view the belief update procedurein previous models as re-constructing the hidden423dialogue states (or user goals) based on the previ-ous belief, a current observation (normally an SLUn-best list), and some prior knowledge.
The priorknowledge can be observation probabilities givena hidden state, the previous system action and/ordialogue histories (Young et al 2010; Thom-son and Young, 2010; Williams, 2010), or prob-abilistic domain-specific ontologies (Mehta et al2010), where the probabilities can be either trainedon a collection of dialogue examples or manuallyassigned by human experts.
In such models, acommon strategy is to use the confidence scores inthe observed n-best list as immediate informationsubstituted into the model for belief computation,which implies that the performance of such belieftracking methods to a large extent depends on thereliability of the confidence scores.
On the otherhand, since the confidence scores may reflect theprobabilities of the occurrences of correspondinguser acts (SLU hypotheses), a belief can also bemaintained based on basic probability operationson those events (as introduced in this paper).
Sucha belief will advance the estimation obtained fromtop SLU hypotheses only, and can serve as a base-line to justify how much further improvement isactually contributed by the use of prior knowledge.Note that the fundamental method in this paper re-lies on the assumption that confidence scores carrysome useful information, and their informative-ness will affect the performance of the proposedmethod as will be seen in our experiments (Sec-tion 5).Therefore, this paper presents a generic belieftracker that maintains beliefs over user goals onlyusing information directly observable from the di-alogue itself, including SLU n-best list confidencescores and user and system behaviours, such asa user not disconfirming an implicit confirma-tion of the system, or the system explicitly re-jecting a query (since no matching item exists),etc.
The belief update is based on simple proba-bility operations and a few very general domain-independent rules.
The proposed method wasevaluated in the Dialog State Tracking Challenge(DSTC) (Williams et al 2013).
A systematicanalysis is then conducted to investigate the ex-tent to which machine learning can advance thisnaive strategy.
Moreover, the results show the per-formance of the proposed method to be compara-ble to other machine learning based approaches,which, in consideration of the simplicity of its im-plementation, suggests that another practical useof the proposed method could be as a modulein an initial system installation to collect trainingdata for machine learning techniques, in additionto functioning as a baseline for further analysingthem.The remainder of this paper is organised as fol-lows.
Section 2 reviews some basic mathematicalbackground, based on which Section 3 introducesthe proposed belief tracker.
Section 4 briefly de-scribes the DSTC task.
The evaluation results anddetailed analysis are illustrated in Section 5.
Fi-nally, we further discuss in Section 6 and concludein Section 7.2 Basic MathematicsWe first review some basic mathematics, whichprovide the fundamental principles for our be-lief tracker.
Let P (X) denote the probability ofthe occurrence of an event X , then the proba-bility of X not occurring is simply P (?X) =1 ?
P (X).
Accordingly, if X occurs at a timewith probability P1(X), and at a second time, itoccurs with probability P2(X) independently ofthe first time, then the overall probability of itsoccurrence is P (X) = 1 ?
P1(?X)P2(?X) =1 ?
(1 ?
P1(X))(1 ?
P2(X)).
To generalise,we can say that in a sequence of k independentevents, if the probability of X occurring at the ithtime is Pi(X), the overall probability of X hav-ing occurred at least once among the k chancesis P (X) = 1 ?
?ki=1 Pi(?X) = 1 ?
?ki=1(1 ?Pi(X)).
This quantity can also be computed re-cursively as:P t(X) = 1?
(1?
P t?1(X))(1?
Pt(X)) (1)where P t(X) denotes the value of P (X) after tevent occurring chances, and we let P 0(X) = 0.Now we consider another situation.
Let A bea binary random variable.
Suppose that we knowthe prior probability of A being true is Pr(A).
Ifthere is a chance where with probability P (B) wewill observe an event B independent of A, and weassume that if B happens, we must set A to false,then after this, the probability of A still being truewill become P (A = true) = Pr(A) ?
P (?B) =Pr(A)(1?
P (B)).3 A Generic Belief TrackerIn this section, we will take the semantics definedin the bus information systems of DSTC as424examples to explain our belief tracker.
Withoutlosing generality, the principle applies to otherdomains and/or semantic representations.
TheSDS we are interested in here is a turn-basedslot-filling task.
In each turn, the system executesan action and receives an observation.
Theobservation is an SLU n-best list, in which eachelement could be either a dialogue act withouttaking any slot-value arguments (e.g.
affirm()or negate()) or an act presenting one or moreslot-value pairs (e.g.
deny(route=64a) orinform(date.day=today, time.ampm=am)), and normalised confidence scores areassigned to those dialogue act hypotheses.
Inaddition, we follow a commonly used assumptionthat the user?s goal does not change during adialogue unless an explicit restart action isperformed.3.1 Tracking Marginal BeliefsSince a confidence score reflects the probabilityof the corresponding dialogue act occurring in thecurrent turn, we can apply the probability opera-tions described in Section 2 plus some ?commonsense?
rules to track the marginal probability of acertain goal being stated by the user during a di-alogue trajectory, which is then used to constructour beliefs over user goals.
Concretely, we startfrom an initial belief b0 with zero probabilities forall the slot-value hypotheses and track the beliefsover individual slot-value pairs as follows.3.1.1 Splitting-Merging HypothesesFirstly, in each turn, we split those dialogue actswith more than one slot-value pairs into singleslot-value statements and merge those identicalstatements among the n-best list by summing overtheir confidence scores, to yield marginal confi-dence scores for individual slot-value representa-tions.
For example, an n-best list observation:inform(date.day=today, time.ampm=am) 0.7inform(date.day=today) 0.3after the splitting-merging procedure will become:inform(date.day=today) 1inform(time.ampm=am) 0.73.1.2 Applying RulesLet Pt(u, s, v) denote the marginal confidencescore for a user dialogue act u(s = v) at turnt.
Then the belief bt(s, v) for the slot-value pair(s, v) is updated as:?
Rule 1: If u = inform, then bt(s, v) =1?
(1?
bt?1(s, v))(1?
Pt(u, s, v)).?
Rule 2: If u = deny, then bt(s, v) =bt?1(s, v)(1?
Pt(u, s, v)).In addition, motivated by some strategies com-monly used in rule-based systems (Bohus andRudnicky, 2005), we consider the effects of cer-tain system actions on the beliefs as well.
Let a(h)be one of the system actions performed in turn t,where h stands for a set of n slot-value argumentstaken by a, i.e.
h = {(s1, v1), .
.
.
, (sn, vn)}.
Wecheck:?
Rule 3: If a is an implicit or explicit confir-mation action (denoted by impl-conf andexpl-conf, respectively) and an affirmor negate user act u is observed with con-fidence score Pt(u):?
Rule 3.1: If u = affirm, thenbt(si, vi) = 1 ?
(1 ?
bt?1(si, vi))(1 ?Pt(u)), ?
(si, vi) ?
h.?
Rule 3.2: If u = negate, thenbt(si, vi) = bt?1(si, vi)(1 ?
Pt(u)),?
(si, vi) ?
h.?
Rule 4: Otherwise, if a is an impl-confaction, and there are no affirm/negateuser acts observed, and no information pre-sented in a is re-informed or denied in thecurrent turn, then we take all (si, vi) ?
h asbeing affirmed by the user with probability 1.However, note that, the marginal probabilitiesb(s, v) computed using the above rules do not nec-essarily yield valid beliefs, because sometimes wemay have?v b(s, v) > 1 for a given slot s. Whenthis occurs, a reasonable solution is to seek amultinomial vector b?
(s, ?)
that minimises the sym-metrised Kullback-Leibler (KL) divergence be-tween b(s, ?)
and itself.
It can be checked thatsolving such an optimisation problem is actuallyequivalent to simply normalising b(s, ?
), for whichthe proof is omitted here but can be found in Ap-pendix B.Finally, we consider an extra fact that normallya user will not insist on a goal if he/she has beennotified by the system that it is impossible to sat-isfy.
(In the DSTC case, such notifications cor-respond to those canthelp.
* system actions.
)Therefore, we have:425?
Rule 5: If the system has explicitly disableda hypothesis h, we will block the generationof any hypotheses containing h in the be-lief tracking procedure, until the dialogue fin-ishes.Note here, if h is a marginal hypothesis, elimi-nating it from our marginal belief will result injoint hypotheses (see Section 3.2) containing halso being blocked, but if h is a joint representa-tion, we will only block the generation of thosejoint hypothesis containing h, without affectingany marginal belief.3.2 Constructing Joint RepresentationsBeliefs over joint hypotheses can then be con-structed by probabilistic disjunctions of thosemarginal representations.
For example, given twomarginal hypotheses (s1, v1) and (s2, v2) (s1 6=s2) with beliefs b(s1, v1) and b(s2, v2) respec-tively, one can compute the beliefs of their jointrepresentations as:bjoint(s1 = v1, s2 = v2) = b(s1, v1)b(s2, v2)bjoint(s1 = v1, s2 = null) = b(s1, v1)b(s2,null)bjoint(s1 = null, s2 = v2) = b(s1,null)b(s2, v2)where null represents that none of the currenthypotheses for the corresponding slot is correct,i.e.
b(s,null) stands for the belief that the in-formation for slot s has never been presented bythe user, and can be computed as b(s,null) =1?
?v b(s, v).3.3 LimitationsThe insight of the proposed approach is to explorethe upper limit of the observability one can ex-pect from an error-prone dialogue itself.
Never-theless, this method has two obvious deficiencies.Firstly, the dialogue acts in an SLU n-best listare assumed to be independent events, hence er-ror correlations cannot be handled in this method(which is also a common drawback of most ex-isting models as discussed by Williams (2012)).Modelling error correlations requires statistics ona certain amount of data, which implies a poten-tial space of improvement left for machine learn-ing techniques.
Secondly, the model is designedto be biased on the accuracy of marginal be-liefs rather than that of joint beliefs.
The be-liefs for joint hypotheses in this method can onlylower-bound the true probability, as the observ-able dependencies among some slot-value pairsare eliminated by the splitting-merging and re-joining procedures described above.
For exam-ple, in the worst case, a multi-slot SLU hypoth-esis inform(s1 = v1, s2 = v2) with a confi-dence score p < 1 may yield two marginal be-liefs b(s1, v1) = p and b(s2, v2) = p,1 then there-constructed joint hypothesis will have its beliefbjoint(s1 = v1, s2 = v2) = p2, which is exponen-tially reduced compared to the originally observedconfidence score.
However, the priority betweenthe marginal hypotheses and the joint representa-tions to a greater extent depends on the action se-lection strategy employed by the system.4 Description of DSTCDSTC (Williams et al 2013) is a public eval-uation of belief tracking (a.k.a.
dialogue statetracking) models based on the data collectedfrom different dialogue systems that provide bustimetables for Pittsburgh, Pennsylvania, USA.The dialogue systems here were fielded by threeanonymised groups (denoted as Group A, B, andC).There are 4 training sets (train1a,train1b, train2 and train3) and 4test sets (test1.
.
.4) provided, where all thedata logs are transcribed and labelled, excepttrain1b which is transcribed but not labelled(and contains a much larger number of dialoguesthan others).
It is known in advance to partici-pants that test1 was collected using the samedialogue system from Group A as train1* andtrain2, test2 was collected using a differentversion of Group A?s dialogue manager but isto a certain extent similar to the previous ones,train3 and test3 were collected using thesame dialogue system from Group B (but thetraining set for this scenario is relatively smallerthan that for test1), and test4 was collectedusing Group C?s system totally different from anyof the training sets.The evaluation is based on several different met-rics2, but considering the nature of our system, wewill mainly focus on the hypothesis accuracy, i.e.1The worst case happens when (s1, v1) and (s2, v2) arestated for the first time in the dialogue and cannot merge withany other marginal hypotheses in the current turn, as theirmarginal beliefs will remain p without being either propa-gated by the belief update rules, or increased by the mergingprocedure.2Detailed descriptions of these metrics can be found in theDSTC handbook at http://research.microsoft.com/en-us/events/dstc/426all joint all joint all joint00.10.20.30.40.50.60.70.80.91SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 1all joint all joint all joint00.10.20.30.40.50.60.70.80.91SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 2all joint all joint all joint00.10.20.30.40.50.60.70.80.91SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 3all joint all joint all joint00.10.20.30.40.50.60.70.80.9SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 40 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 212345678910Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9Figure 1: Hypothesis accuracy on the four test sets: the columns in each schedule, from left to right,stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4where the last three groups are merged into the right-hand side column.percentage of turns in which the tracker?s 1-besthypothesis is correct, but with the receiver operat-ing characteristic (ROC) performance briefly dis-cussed as well.
In addition, there are 3 ?sched-ules?
for determining which turns to include whenmeasuring a metric: schedule 1 ?
includingall turns, schedule 2 ?
including a turn for agiven concept only if that concept either appearson the SLU n-best list in that turn, or if the sys-tem action references that concept in that turn, andschedule 3 ?
including only the turn before therestart system action (if there is one), and thelast turn of the dialogue.5 Evaluation and AnalysisThe method proposed in this paper corresponds toTeam 2, Entry 1 in the DSTC submissions.
Inthe following analysis, we will compare it withthe 26 machine learning models submitted by theother 8 anonymised participant teams plus a base-line system (Team 0, Entry 1) that only con-siders the top SLU result.Each team can submit up to 5 systems, whilstthe systems from a same team may differ fromeach other in either the statistical model or thetraining data selection (or both of them).
There isa brief description of each system available afterthe challenge.
For the convenience of analysis andillustration, on each test set we categorise thesesystems into the following groups: in-domain ?systems trained only using the data sets whichare similar (including the ?to-some-extent-similar?ones) to the particular test set, out-of-domain ?systems trained on the data sets which are to-tally different from the particular test set, mixed-domain ?
systems trained on a mixture of the in-domain and out-of-domain data, and ensemble ?systems combining multiple models to generatetheir final output.
(The ensemble systems here areall trained on the mixed-domain data.)
Note that,4270 0.2 0.4 0.6 0.8 10.10.20.30.40.50.60.7 TEST 1CorrectIncorrect0 0.2 0.4 0.6 0.8 10.10.20.30.40.50.60.7 TEST 2CorrectIncorrect0 0.2 0.4 0.6 0.8 10.10.20.30.40.50.60.70.80.9 TEST 3CorrectIncorrect0 0.2 0.4 0.6 0.8 10.10.20.30.40.50.60.7 TEST 4CorrectIncorrectFigure 2: Distributions of SLU confidence scores on the four test sets: The x-axis stands for the confi-dence score interval, and the y-axis stands for the occurrence rate.for test4 there are no in-domain data available,so all those non-ensemble systems are merged intoone group.
Detailed system categorisation on eachtest set can be found in Appendix A.5.1 Hypothesis AccuracyWe plot the hypothesis accuracy of our method(red dashed line) on the 4 test sets in compari-son with the baseline system (blue dotted line) andother systems in Figure 1, where different mark-ers are used to identify the systems from differentteams.
Here we use the overall accuracy of themarginal hypotheses (all) and the accuracy ofthe joint hypotheses (joint) to sketch the gen-eral performance of the systems, without lookinginto the result for each individual slot.It can be seen that the proposed method pro-duces more accurate marginal and joint hypothe-ses than the baseline on all the test sets and inall the schedules.
Moreover, generally speak-ing, further improvement can be achieved by prop-erly designed machine learning techniques.
Forexample, some systems from Team 6, especiallytheir in-domain and ensemble ones, almost consis-tently outperform our approach (as well as most ofthe models from the other teams) in all the abovetasks.
In addition, the following detailed trendscan be found.Firstly, and surprisingly, our method tendsto be more competitive when measured usingschedule 1 and schedule 3 than usingschedule 2.
As schedule 2 is supposed tomeasure system performance on the concepts thatare in focus, and to prevent a belief tracker receiv-ing credit for new guesses about those conceptsnot in focus, the results disagree with our origi-nal expectation of the proposed method.
A possi-ble explanation here is that some machine learningmodels tend to give a better belief estimation whena concept is in focus, however their correct top hy-potheses might more easily be replaced by otherincorrect ones when the focus on the concepts inthose correct hypotheses are lost (possibly due toimproperly assigned correlations among the con-cepts).
In this sense, our method is more robust,as the beliefs will not change if their correspond-ing concepts are not in focus.428all joint all joint all joint00.10.20.30.40.50.60.70.80.9SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 1all joint all joint all joint00.10.20.30.40.50.60.70.80.9SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 2all joint all joint all joint00.10.20.30.40.50.60.70.80.91SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 3all joint all joint all joint00.10.20.30.40.50.60.70.80.9SCHEDULE 1 SCHEDULE 2 SCHEDULE 3TEST 40 0.2 0.4 0.6 0.8 1 1.2 .4 1.6 1.8 212345678910Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9Figure 3: ROC equal error rate on the four test sets: The columns in each schedule, from left to right,stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4where the last three groups are merged into the right-hand side column.Secondly, the proposed method had been sup-posed to be more preferable when there are no (ornot sufficient amount of) in-domain training dataavailable for those statistical methods.
Initial evi-dence to support this point of view can be observedfrom the results on test1, test2 and test3.More concretely, when the test data distributionbecomes less identical to the training data distri-bution on test2, out system outperforms mostof the other systems except those from Team 6(and a few others in the schedule 2/all taskonly), compared to its middle-level performanceon test1.
Similarly, on test3 when the amountof available in-domain training data is small, ourapproach gives more accurate beliefs than most ofthe others with only a few exceptions in each sce-nario, even if extra out-of-domain data are used toenlarge the training set for many systems.
How-ever, the results on test4 entirely contradicts theprevious trend, where a significant number of ma-chine learning techniques perform better than ourdomain-independent rules without using any in-domain training data at all.
We analyse such re-sults in detail as follows.To explain the unexpected outcome on test4,our first concern is the influence of Rule 4, whichis relatively ?stronger?
and more artificial thanthe other rules.
Hence, for the four test sets,we compute the percentage of dialogues where aimpl-conf system action occurs.
The statisticsshow that the occurrence rates of the implicit con-firmation system actions in test1.
.
.4 are 0.01,0, 0.94 and 0.67, respectively.
This means thatthe two very extreme cases happen in test3 andtest2 (the situation in test1 is very similar totest2), and the result for test4 is roughly rightin the middle of them, which suggests that Rule4 will not be the main factor to affect our per-formance on test4.
Therefore, we further lookinto the distributions of the SLU confidence scoresacross these different test sets.
A normalised his-togram of the confidence scores for correct andincorrect SLU hypotheses observed in each testset is plotted in Figure 2.
Here we only consider429the SLU hypotheses that will actually contributeduring our belief tracking processes, i.e.
only theinform, deny, affirm and negate user dia-logue acts.
It can be found that the dialogue sys-tem used to collect the data in test4 tends toproduce significantly more ?very confident?
SLUhypotheses (those with confidence scores greaterthan 0.8) than the dialogue systems used for col-lecting the other test sets, where, however, a con-siderable proportion of its highly confident hy-potheses are incorrect.
In such a case, our systemwould be less capable in revising those incorrecthypotheses with high confidence scores than manymachine learning techniques, since it to a greaterextent relies on the confidence scores to update thebeliefs.
This finding indicates that statistical ap-proaches will be helpful when observed informa-tion is less reliable.5.2 Discussions on the ROC PerformanceBesides the hypothesis accuracy, another impor-tant issue will be the ability of the beliefs to dis-criminate between correct and incorrect hypothe-ses.
Williams (2012) suggests that a metric tomeasure such performance of a system is the ROCcurve.
Note that, in the DSTC task, most of thesystems from the other teams are based on dis-criminative models (except two systems, a simplegenerative model from Team 3 and a deep neuralnetwork method from Team 1), which are opti-mised specifically for discrimination.
Unsurpris-ingly, our approach becomes much less competi-tive when evaluated based on the ROC curve met-rics, as illustrated in Figure 3 using the ROC equalerror rate (EER) for the all and joint scenar-ios.
(ERR stands for the intersection of the ROCcurve with the diagonal, i.e.
where the false ac-cept rate equals the false reject rate.
The smallerthe ERR value, the better a system?s performanceis.)
However, our argument on this point is thatsince an optimised POMDP policy is not a linearclassifier but has a manifold decision surface (Cas-sandra, 1998), the ROC curves may not be able toaccurately reflect the influence of beliefs on a sys-tem?s decision quality, for which further investiga-tions will be needed in our future work.6 Further DiscussionsIn this paper, we made the rules for our belieftracker as generic as possible, in order to ensurethe generality of the proposed mechanism.
How-ever, in practice, it is extendable by using moredetailed rules to address additional phenomena ifthose phenomena are deterministically identifiablein a particular system.
For example, when the sys-tem confirms a joint hypothesis (s1 = v1, s2 =v2) and the user negates it and only re-informs oneof the two slot-values (e.g.
inform(s1 = v?1)),one may consider that it is more reasonable to onlydegrade the belief on s1 = v1 instead of reducingthe beliefs on both s1 = v1 and s2 = v2 syn-chronously as we currently do in Rule 3.2.
How-ever, the applicability of this strategy will dependon whether it is possible to effectively determinesuch a compact user intention from an observedSLU n-best list without ambiguities.7 ConclusionsThis paper introduces a simple rule-based belieftracker for dialogue systems, which can maintainbeliefs over both marginal and joint representa-tions of user goals using only the information ob-served within the dialogue itself (i.e.
without need-ing training data).
Based on its performance inthe DSTC task, potential advantages and disad-vantages of machine learning techniques are anal-ysed.
The analysis here is more focused on generalperformance of those statistical approaches, whereour concerns include the similarity of distributionsbetween the training and test data, the adequacy ofavailable training corpus, as well as the SLU confi-dence score distributions.
Model-specific featuresfor different machine learning systems are not ad-dressed at this stage.
Considering its competitive-ness and simplicity of implementation, we suggestthat the proposed method can serve either as a rea-sonable baseline for future research on dialoguestate tracking problems, or a module in an ini-tial system installation to collect training data forthose machine learning techniques.AcknowledgmentsThe research leading to these results was sup-ported by the EC FP7 projects JAMES (ref.270435) and Spacebook (ref.
270019).
We thankJason D. Williams for fruitful comments on an ear-lier version of this paper.
We also acknowledgehelpful discussions with Simon Keizer and Herib-erto Cuaya?huitl.430ReferencesDan Bohus and Alexander I. Rudnicky.
2005.
Con-structing accurate beliefs in spoken dialog systems.In Proceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 272?277.Anthony R. Cassandra.
1998.
Exact and Approxi-mate Algorithms for Partially Observable MarkovDecision Processes.
Ph.D. thesis, Brown University,Providence, RI, USA.James Henderson and Oliver Lemon.
2008.
Mixturemodel POMDPs for efficient handling of uncertaintyin dialogue management.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics on Human Language Technolo-gies: Short Papers, pages 73?76.Neville Mehta, Rakesh Gupta, Antoine Raux, DeepakRamachandran, and Stefan Krawczyk.
2010.
Prob-abilistic ontology trees for belief tracking in dialogsystems.
In Proceedings of the 11th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 37?46.Nicholas Roy, Joelle Pineau, and Sebastian Thrun.2000.
Spoken dialogue management using proba-bilistic reasoning.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics, pages 93?100.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.Blaise Thomson, Filip Jurc?ic?ek, Milica Gas?ic?, SimonKeizer, Francois Mairesse, Kai Yu, and Steve Young.2010.
Parameter learning for POMDP spoken dia-logue models.
In Proceedings of IEEE Workshop onSpoken Language Technology.Jason D. Williams and Steve Young.
2007a.
Partiallyobservable Markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason D. Williams and Steve Young.
2007b.
Scal-ing POMDPs for spoken dialog management.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 15(7):2116?2129.Jason D. Williams, Antoine Raux, Deepak Ramachan-dran, and Alan W. Black.
2013.
The Dialog StateTracking Challenge.
In Proceedings of the 14th An-nual Meeting of the Special Interest Group on Dis-course and Dialogue.Jason D. Williams.
2010.
Incremental partition re-combination for efficient tracking of multiple dia-log states.
In Proceedings of the IEEE InternationalConference on Acoustics, Speech, and Signal Pro-cessing, pages 5382?5385.Jason D. Williams.
2012.
Challenges and oppor-tunities for state tracking in statistical spoken dia-log systems: Results from two public deployments.IEEE Journal of Selected Topics in Signal Process-ing, 6(8):959?970.Steve Young, Milica Gas?ic?, Simon Keizer, FrancoisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The Hidden Information State model:a practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174.Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-son D. Williams.
2013.
POMDP-based statisticalspoken dialog systems: A review.
Proceedings ofthe IEEE, 101(5):1160?1179.A System CategorisationTable 1 shows detailed categorisation of the sys-tems submitted to DSTC, where TiEj stands forTeam i, Entry j.ensembleT6E3, T6E4, T9E1, T9E2, T9E3T9E4, T9E5mixed-domain non-ensemblefortest4T1E1, T3E1, T3E2, T3E3, T4E1T5E2, T5E4, T5E5, T8E4, T8E5in-domaintest1 T5E1test2 T6E1, T8E1, T8E2 T5E3test3 T6E2, T6E5, T8E3 T7E1out-of-domaintest1test2 T6E2, T6E5, T8E3test3 T6E1, T8E1, T8E2Table 1: Categorisation of the systems submittedto DSTC.B Symmetrised KL-divergenceMinimisationWe prove the following proposition to support ourdiscussions in the end of Section 3.1.Proposition 1 Let p ?
RN be an arbitrary N -dimensional non-negative vector (i.e.
p ?
0).Let p?
= p?p?1 , where ?
?
?1 stands for the `1-norm of a vector.
Then p?
is the solution of theoptimisation problem.
minq?0,?q?1=1 DSKL(p?q),where DSKL(p?q) denotes the symmetrised KL-divergence between p and q, defined as:DSKL(p?q) = DKL(p?q) + DKL(q?p) (2)=?ipi logpiqi+?iqi logqipi431and pi and qi denote the ith element in p and qrespectively.Proof Let q?
= arg minq?0,?q?1=1 DSKL(p?q).Firstly, using the facts that limx?0 x log xy ?
0and limx?0 y log yx ?
+?, ?y > 0, one can eas-ily prove that if pi = 0 then q?i = 0, and pi 6= 0then q?i 6= 0, because otherwise the objective valueof Eq.
(2) will become unbounded.Therefore, we only consider the case p > 0 andq > 0.
By substituting pi = p?i?p?1 into Eq.
(2),we obtain:DSKL(p?q) = ?p?1?ip?i log?p?1p?iqi+?iqi logqi?p?1p?i= ?p?1(?ip?i logp?iqi+?ip?i log ?p?1)+?iqi logqip?i?
?iqi log ?p?1= ?p?1?ip?i logp?iqi+?iqi logqip?i+(?p?1 ?
1) log ?p?1= ?p?1DKL(p?
?q) + DKL(q?p?
)+(?p?1 ?
1) log ?p?1?
(?p?1 ?
1) log ?p?1where we use the facts that?i p?i = 1,?i qi = 1,DKL(p?
?q) ?
0 and DKL(q?p?)
?
0, since p?
andq are valid distributions.
It can be found that theminimum (?p?1 ?
1) log ?p?1 is only achievablewhen DKL(p?
?q) = 0 and DKL(q?p?)
= 0, i.e.
q =p?, which proves Proposition 1.432
