Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 188?196,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsUsing Other Learner Corpora in the 2013 NLI Shared TaskJulian BrookeDepartment of Computer ScienceUniversity of Torontojbrooke@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractOur efforts in the 2013 NLI shared task fo-cused on the potential benefits of external cor-pora.
We show that including training datafrom multiple corpora is highly effective at ro-bust, cross-corpus NLI (i.e.
open-training task1), particularly when some form of domainadaptation is also applied.
This method canalso be used to boost performance even whentraining data from the same corpus is available(i.e.
open-training task 2).
However, in theclosed-training task, despite testing a numberof new features, we did not see much improve-ment on a simple model based on earlier work.1 IntroductionOur participation in the 2013 NLI shared task(Tetreault et al 2013) follows on our recent workexploring cross-corpus evaluation, i.e.
using dis-tinct corpora for training and testing (Brooke andHirst, 2011; Brooke and Hirst, 2012a; Brooke andHirst, 2012b), an approach that is now becomingfairly standard alternative in relevant work (Bykhand Meurers, 2012; Tetreault et al 2012; Swan-son and Charniak, 2013).
Our promotion of cross-corpus evaluation in NLI was partially motivated byserious issues with the most popular corpus for na-tive language identification work up to now, the In-ternational Corpus of Learner English (Granger etal., 2009).
The new TOEFL-11 (Blanchard et al2013) used for this NLI shared task addresses someof the problems with the ICLE (most glaringly, thefact that some topics in the ICLE appeared only insome L1 backgrounds), but, from the perspective oftopic, proficiency, and particularly genre, it is nec-essarily limited in scope (perhaps even more so thanthe ICLE); in short, it addresses only a small por-tion of the space of learner texts.
Our interest, then,continues to be in robust models for NLI that are notrestricted to utility in a particular corpus, and in ourparticipation in this task we have focused our effortson the open-training tasks which allow the use ofcorpora beyond the TOEFL-11.
Since participationin these tasks was low relative to the closed-trainingtask, fewer papers will address them, making ouremphasis here all the more relevant.The models built for all of three of the tasks areextensions of the model used in our recent work(Brooke and Hirst, 2012b); we will discuss the as-pects of this model common to all tasks in Section2.
Section 3 is a brief review of our methodologyand results in the closed-training task, which was fo-cused exclusively on testing features (both new andold); we found almost nothing that improved on ourbest feature set from previous work, and most fea-tures actually hurt performance.
In Section 4, wediscuss the corpora we used for the open-trainingtasks, some of which we collected and/or have notbeen applied to NLI before.
Our approach to theopen-training task 2 using these corpora is presentedin Section 5.
In Section 6, we discuss how we useddomain adaption methods and our various externalcorpora to create the (winning) model for the open-training task 1, which did not permit usage of theTOEFL-11; we also present some post hoc testing(now that TOEFL-11 is no longer off limits).
In Sec-tion 7 we offer conclusions.1882 Basic ModelIn our recent work on cross-corpus NLI (Brooke andHirst, 2012b), we tested a number of classifier andfeature options, and most of our choices there arecarried over to this work.
In particular, we use theLiblinear SVM 1va (one versus all) classifier (Fan etal., 2008).
Using the TOEFL-11 corpus, we brieflytested the other options explored in that paper (in-cluding SVM 1v1) as well as the logistic regressionclassifier included in Liblinear, and found that theSVM 1va classifier was still preferred (with our bestfeature set, see below), though the differences in-volved were marginal.
Although small variations inthe choice of C parameter within the SVM modeldid occasionally produce benefits (here and in ourprevious work), these were not consistent, whereasthe default value of 1 showed consistently near opti-mal results.
We used a binary feature representation,and then feature vectors were normalized to the unitcircle.
With respect to feature selection, our earlierwork used a frequency cutoff of 5 for all features; wecontinue to use frequency cutoffs here; other com-mon feature selection methods (e.g.
use of informa-tion gain) were ineffective in our previous work, sowe did not explore them in detail here.With regards to the features themselves, our ear-lier work tested a fairly standard collection of distri-butional features, including function words, word n-grams (up to bigram), POS n-grams (up to trigram),character n-grams (up to trigram), dependencies,context-free productions, and ?mixed?
POS/functionn-grams (up to trigram), i.e.
n-grams with all lex-ical words replaced with part of speech.
Most ofthese had appeared in previous NLI work (Koppelet al 2005; Wong and Dras, 2011; Wong et al2012), though until recently word n-grams had beenavoided because of ICLE topic bias.
Our best modelused only two of these features, word n-grams andthe mixed POS/function n-grams.
This was ourstarting point for the present work.
The Stanfordparser (Klein and Manning, 2003) was used for POStagging and parsing.Obviously, the training set used varies through-out the paper, and other differences in specific mod-els built for each task will be mentioned as theybecome relevant.
For evaluation here, we primar-ily use the test set for NLI shared task, though weTable 1: Feature testing for closed-training task, previ-ously investigated features; best result is in bold.Feature Set Accuracy (%)Word+mixed 76.8Word+mixed+characters 72.0Word+mixed+POS 76.6Word+mixed+productions 77.9Word+mixed+dependencies 78.9Word+mixed+dep+prod 78.4employ some other evaluation corpora, as appropri-ate.
During the preparation for the shared task, wemade our decisions regarding models for two taskswith TOEFL-11 training according to the results intwo training/test sets (800 per language for training,100 per language for testing) sampled from the re-leased training data.
Since our research was focusedon cross-corpus evaluation, we never created mecha-nisms for cross-validation in our system, and in factit creates practical difficulties for the open-trainingtask 2, so we do not include cross-validated resultshere.3 Closed-training TaskOur approach to the closed-training task primarilyinvolved feature testing.
Table 1 contains the re-sults of testing our previously investigated featuresfrom Brooke and Hirst (2012b) in the TOEFL-11,pivoted around the best set (word n-grams + mixedPOS/Function n-grams) from that earlier work.Some of the features we rejected in our previouswork also underperform here, in particular charac-ter and POS n-grams.
In fact, character n-grams hada much more negative effect on performance herethan they had previously.
Dependencies are clearly auseful feature in the TOEFL-11, this is fully consis-tent with out initial testing.
CFG productions offer asmall benefit on top of our base feature set, but arenot useful when dependencies are also included, sowe discarded them.
Thus, our feature set going for-ward consists of word n-grams, mixed POS/functionn-grams, and dependencies.Next, we evaluate our feature frequency cutoff us-ing this feature set (Table 2).
We used the rather highcutoff of 5 (for all features) in the previous work be-cause of our much larger training set.
We looked at189Table 2: Feature frequency cutoff testing for closed-training task; best result is in bold.Cutoff Accuracy (%)At least 5 occurrences 78.9At least 3 occurrences 79.5At least 2 occurrences 79.7All features 80.2higher values there, but for this task we focused ontesting lower values.Lowering our frequency cutoff is indeed benefi-cial, and we got our best result in the test set whenwe had no feature selection at all.
This was not con-sistent with our preparatory testing, which showedsome benefit to removing hapax legomena, thoughthe difference was marginal.
However, we did in-clude a run with this option in our final submis-sion, and so this last result represents our best per-formance on the closed-training task.We tested several other feature options that wereadded to our system for this task.
Inspired by Bykhand Meurers (2012), we first considered n-grams(up to trigrams) where at least one lexical word isabstracted to its POS, and at least one isn?t (par-tial abstraction).
Since dependencies were found tobe a positive feature, we tried adding dependencychains, which combine two dependencies, i.e.
threelexical words linked by two grammatical relations.We tested productions with wild cards, e.g.
S?
NPVP * matches any sentence production which startswith NP VP.
Tree Substitution grammar fragmentshave been shown to be superior to CFG produc-tions (Swanson and Charniak, 2012); we used rawTree Substitution Grammar (TSG) fragments for theTOEFL-111 and tested a subset of those fragmentswhich involved at least two levels of the grammar(i.e.
those not already covered by n-grams or CFGproductions).Our final feature option requires slightly moreexplanation.
Crossley and McNamara (2012) re-port that metrics associated with word concreteness,imagability, meaningfulness, and familiarity are use-ful for NLI; the metrics they use are derived from theMRC Psycholinguistic database (Coltheart, 1980),1We thank Ben Swanson for letting us use his TSG frag-ments.Table 3: Feature testing for closed-training task, new fea-tures; best result is in bold.Feature Set Accuracy (%)Best 80.2Best+partial abstraction 79.7Best+dependency chains 78.6Best+wild card productions 78.8Best+TSG fragments 78.1Best+MRC lexicon 54.2which assign values for each dimension to individ-ual words.
We used the scores in the MRC to getan average score for each dimension for each text,further normalized to the range 0?1; texts with nowords in the dictionaries were assigned the averageacross the training set.Table 3 indicates that all of these new featureswere, to varying degrees, a drag on our model.
Thestrongly negative effect of the MRC lexicons is par-ticularly surprising.
We speculate that this mightmight be due partially to problems with combininga large number of binary features with a small num-ber of continuous metrics directly in a single SVM.A meta-classifier might solve this problem, but wedid not explore meta-classification for features here.Finally, since that information was available tous, we tested creating sub-models segregated bytopic and proficiency.
The topic-segregated modelconsisted of 8 SVMs, one for each topic; accu-racy of this model was quite low, only 67.3%.
Theproficiency-segregated model used two groups, highand low/medium (there were few low texts, so wedid not think they would be sufficient by themselvesfor a viable model).
Results were higher, 74.9%, butstill well below the best unsegregated model.4 External CorporaIn this section we review corpora which will be usedfor the open-training tasks in the next two sections.Including the TOEFL-11, there are at least six pub-licly available multi-L1 learner text corpora for NLI,with many of these corpora becoming available rel-atively recently.
Below, we introduce each corpus indetail; a summary of the number of tokens from eachL1 background for each of the corpora is in Table 4.190Table 4: Number of tokens (in thousands) in external learner corpora, by L1.L1CorpusLang-8 (new) ICLE FCE ICCI ICNALEJapanese 11694k 227k 33k 232k 199kChinese 7044k 552k 30k 243k 366kKorean 5174k 0k 37k 0k 151kFrench 536k 256k 61k 0k 0kSpanish 861k 225k 83k 49k 0kItalian 450k 251k 31k 0k 0kGerman 331k 258k 29k 91k 0kTurkish 51k 222k 22k 0k 0kArabic 218k 0k 0k 0k 0kHindi 11k 0k 0k 0k 0kTelugu 2k 0k 0k 0k 0kLang-8 Lang-8 is a website where language learn-ers write journal entries in their L2 to be correctedby native speakers.
We collected a large set of theseentries, which we?ve shown to be useful for NLI(Brooke and Hirst, 2012b), despite the noisiness ofthe corpus (for instance, some entries directly mixL1 and L2).
For this task we added more entrieswritten since the first version was collected (58kon top of the existing 154k entries).2 The corpuscontains entries from all the L1 backgrounds in theTOEFL-11, though the amounts for Hindi and par-ticularly Telugu are small.
Since many of the entriesare very short, as in our previous work we add en-tries of the same L1 together to reach a minimumsize of 250 tokens.ICLE Before 2011, nearly all work on NLI wasdone in the International Corpus of Learner Englishor ICLE (Granger et al 2009), a collection of col-lege student essays from 15 L1 backgrounds, 8 ofwhich overlap with the 11 L1s in the TOEFL-11.Despite known issues that might cause problems(Brooke and Hirst, 2011), it is probably the closestmatch in terms of genre and writer proficiency to theTOEFL-11.FCE What we call the FCE corpus is a smallsample of the First Certificate in English portionof the Cambridge Learner Corpus, which was re-2We do not have permission to distribute the corpus directly;however, we can offer a list of URLs together with softwarewhich can be used to recreate the corpus.leased for the purposes of essay scoring evaluation(Yannakoudakis et al 2011); 16 different L1 back-grounds are represented, 9 of which overlap with theTOEFL-11.
Each of the texts consists of two shortanswers in the form of a letter, a report, an article,or a short story.
Relative to the other corpora, theactual amount of text in the FCE is small.ICCI Like the ICLE and TOEFL-11, the Inter-national Corpus of Crosslinguistic Interlanguage(Tono et al 2012) is also an essay corpus, thoughin contrast with other corpora it is focused on younglearners, i.e.
those in grade school.
It includes bothdescriptive and argumentative essays on a number oftopics.
Only 4 of its L1s overlap with the TOEFL-11.ICANLE The International Corpus Network ofAsian Learners of English or ICANLE (Ishikawa,2011) is a collection of essays from college studentsin 10 Asian countries; 3 of the L1s overlap with theTOEFL-11.3 Even more so than the TOEFL-11, thiscorpus is strictly controlled for topic, it has only 2topics (part-time jobs and smoking in restaurants).One obvious problem with using the above cor-pora to classify L1s in the TOEFL-11 is the lackof Hindi and Telugu text, which we found werethe two most easily confused L1s in the closed-3The ICANLE also contains 103K of Urdu text.
Since Urduand Hindi are mutually intelligible, this could be a good substi-tute for Hindi; we overlooked this possibility during our prepa-ration for the task, unfortunately.191Table 5: Number of tokens (in thousands) in Indian cor-pora, by expected L1.L1Indian CorpusNews Twitter BlogHindi 996k 146k 2089kTelugu 998k 133k 76ktraining task.
We explored a few methods to getdata to fill this gap.
First, we downloaded twocollections of English language Indian news arti-cles, one from a Hindi newspaper, the Hindus-tan Times, and one from a Telugu newspaper, theAndhra Jyothy.4 Second, we extracted a collectionof English tweets from the WORLD twitter corpus(Han et al 2012) that were geolocated in the Hindiand Telugu speaking areas; as with the Lang-8, thesewere combined to create texts of at least 250 tokens.5Our third Indian corpus consists of translations (byGoogle Translate) of Hindi and Telugu blogs fromthe ICWSM 2009 Spinn3r Dataset (Burton et al2009), which we used in other work on using L1text for NLI (Brooke and Hirst, 2012a).
The numberof tokens in each of these corpora are given in Table5.5 Open-training Task 2Our approach to open-training task 2 is based on theassumption that in many ways it is a direct extensionof the closed-training task.
For example, we directlyuse the best feature set from that task, with no furthertesting.
Based on the results in our initial testing,we used a feature frequency cutoff of 2 during ourtesting for open-training task 2; for consistency, wecontinue with that cutoff in this section.We first attempted to integrate information fromother corpora by using a meta-classifier, as was suc-cessfully used for features by Tetreault et al(2012).Briefly, classifiers were trained on each major exter-nal corpus (including only the L1s in the TOEFL-11), and then tested on the TOEFL-11 training set;4As with the Lang-8, we cannot distribute the corpus di-rectly but would be happy to provide URLs and scraping soft-ware for those would like to build it themselves.5We extracted India regions 07 and 36 for Hindi, and 02 and25 for Telegu; We can provide a list of tweet ids for reconstruct-ing the corpus if desired.
Our thanks to Bo Han and Paul Cookfor helping us get these tweets.TOEFL-11 training was accomplished using 10-foldcrossvalidation (by modifying the code for Liblin-ear crossvalidation to output margins).
With theTOEFL-11 as the training set, the SVM marginsfrom each 1va classifier (across all L1s and all cor-pora) were used as the feature input to the meta-classifier (also an SVM).
In addition to Liblinear,we also outputted this meta-classification problem toWEKA format (Witten and Frank, 2005), and testeda number of other classifier options not availablein Liblinear (e.g.
Na?
?ve Bayes, decision trees, ran-dom forests).
In addition to (continuous) margins,we also tested using the classification directly.
Ul-timately, we came to the conclusion were that anyuse of a meta-classifer came with a cost (a mini-mum 2?3% drop in performance) that could not befully overcome with the additional information fromour external corpora.
The result using SVM classi-fiers, margin features, and an SVM meta-classifierwas 78.5%, well below the TOEFL-11?only base-line.The other approach to using these external cor-pora is to add the data directly to the TOEFL-11 dataand train a single classifier.
This is very straightfor-ward; really the only variable is which corpora willbe included.
However, we need to introduce, at thispoint, a domain-adaptation technique from our mostrecent work (Brooke and Hirst, 2012b), bias adap-tion, which we used to greatly improve the accu-racy of cross-corpus classification.
Without gettinginto the algorithmic details, bias adaption involveschanging the bias (constant) factor of a model untilthe output of the model in some dataset is balancedacross classes (or otherwise fits the expected distri-bution); it partially addresses skewed results due todifferences between training and testing corpora.
Inthe previous work, we used a separate developmentset, but here we rely on the test set itself; since thetechnique is unsupervised, we do not need to knowthe classes.
Table 6 shows model performance afteradding various corpora to the training set (TOEFL-11 is always included), with and without bias adap-tion (BA).Many of the differences in Table 6 are modest,but there are are few points to be made.
First,there is a small improvement using either the Lang-8 or the ICLE as additional data.
The ICCI, on theother hand, has a clearly negative effect, perhaps be-192Table 6: Corpus testing for open-training task; best resultis in bold.Training SetAccuracy (%)no BA with BATOEFL-11 only 79.7 79.2+Lang-8 79.5 80.5+ICLE 80.2 80.2+FCE 79.6 79.3+ICCI 77.3 76.7+ICANLE 79.7 79.3+Lang-8+ICLE 80.4 80.4+all but ICCI 80.0 80.4cause of the age or proficiency of the contributors tothat corpus.
Bias adaption seems to help when the(messy and highly unbalanced) Lang-8 is involved(consistent with our previous work), but it does notseem useful applied to other corpora, at least not inthis setting.Our second adaptation technique involves trainingdata selection, which has been used, for instance incross-domain parsing (Plank and van Noord, 2011).The method used here is very simple: we count thenumber of times each word appears in a document inour test data, rank the texts in our training data ac-cording to the sum of counts (in the test data) eachword that appears in a training texts, and throw awaya certain numbers of low-ranked texts.
For example,if a training text consists solely of the two words Iagree6 and I appears in 1053 texts in the test set,and agree appears in 325, then the value for that textis 1378.
This method simultaneously penalizes shorttexts, those texts with low lexical diversity, and textsthat do not use the same words as our test set.
Weuse a fixed cutoff, r, which refers to the proportionof training data that is thrown away for each L1 (al-lowing this to work independent of L1 was not ef-fective).
We tested this on this method in tandemwith bias adaption on two corpus sets: The TOEFL-11 and the Lang-8, and all corpora except the ICCI.The results are in Table 7.
The number in italics isthe best run that we submitted.Again, it is difficult to come to any firm con-clusions when the differences are this small, but6This is not a made-up example; there is actually a text inthe TOEFL-11 corpus like this.Table 7: Training set selection testing for open-trainingtask 2; best result is in bold, best submitted run is in ital-ics.Training SetAccuracy (%)no BA with BATOEFL-11 only 79.7 79.2+Lang-8 79.5 80.5+Lang-8 r = 0.1 81.4 81.6+Lang-8 r = 0.2 80.6 81.5+Lang-8 r = 0.3 81.0 80.6+all but ICCI 80.0 80.4+all but ICCI r = 0.1 81.5 82.5+all but ICCI r = 0.2 81.0 81.6+all but ICCI r = 0.3 80.9 81.3our best results involve all of the corpora (exceptthe ICCI) and both adaptation techniques.
Unfor-tunately, our initial testing suggested r = 0.2 wasthe better choice, so our official best result in thistask (81.6%) is not the best result in this table.
Per-formance clearly drops for r > 0.2.
Nevertheless,nearly all the results in the table show clear improve-ment on our closed-training task model.6 Open-training Task 1The central challenge of open-training task 1 wasthat the TOEFL-11 was completely off-limits, evenfor testing.
Therefore, a discussion of how we pre-pared for this task is very distinct from a post hocanalysis of the best method once we allowed our-selves access to the TOEFL-11; we separate the twohere.
We did use the feature set (and frequency cut-off) from the closed-training (and open-training 2)task; it was close enough to the feature set from ourearlier work (using the Lang-8, ICLE, and FCE) thatit did not seem like cheating to preserve it.6.1 MethodGiven our failure to create a meta-classifier in open-training task 2, we did not pursue that option here,focusing purely on adding corpora directly to amixed training set.
The central question was whichcorpora to add, and whether to use our domain-adaptation methods.
Our experience with the ICCIin the open-training task 2 suggested that it might beworth leaving it (or perhaps other corpora) out, but193Table 8: ICLE testing for Open-training task 1; best resultis in bold.Training SetAccuracy (%)no BA with BALang-8 47.0 57.1Lang-8+FCE 47.9 58.2Lang-8+ICCI 46.4 54.8Lang-8+ICNALE 46.9 57.5Lang-8+ICNALE+FCE 47.7 58.8Lang-8+ICNALE+FCE r = 0.1 46.6 58.2could we come to that conclusion independently?Our approach involved considering each externalcorpus as a test set, and seeing which other corporawere useful when included in the training set; cor-pora which were consistently useful would be in-cluded in the final set.
Our original exploration in-volved looking at all of the corpora (as test sets),but it was haphazard; here, we present results justwith the ICLE and the ICANLE, which are arguablythe two closest corpora to the TOEFL-11 in termsof proficiency and genre.
For this, we used a dif-ferent selection of L1s, 12 for the ICLE, 7 for theICANLE; all of these languages appeared in at leastthe Lang-8, and 2 of them (Chinese and Japanese)appeared in all corpora.
Both sets were balanced byL1.
Again, we report results with and without biasadaption.
The results for the ICLE are in Table 8.The clearest result in Table 8 is the consistentlypositive effect of bias adaption, at least 10 percent-age points, which is line with our previous work.Adding both ICLE and ICNALE to the Lang-8 cor-pus gave a small boost in performance, but the effectof the ICCI was once again negative, as was the ef-fect of our training set selection.The ICNALE results in Table 9 support many ofthe conclusions that we reached in the ICLE (andother sets like the FCE and ICCI, which are not in-cluded here but gave similar results); the effect ofbias adaption is even more pronounced.
Two dif-ferences: the slightly positive effect of training dataselection and the positive effect of the ICCI, the lat-ter of which we saw nowhere else.
We speculatethat this might be due to that fact that although theICNALE is a college-level corpus, it is a corpus ofTable 9: ICNALE testing for open-training task 1; bestresult is in bold.Training SetAccuracyno BA with BALang-8 37.2 59.6Lang-8+FCE 37.9 61.3Lang-8+ICCI 35.7 61.4Lang-8+ICLE 37.3 61.4Lang-8+ICLE+FCE 37.6 61.7Lang-8+ICLE+FCE r = 0.1 37.7 61.9Asian-language native speakers.
Our theory is thatEuropeans are, on average, more proficient usersof English (this is supported by, for instance, thetesting from Granger et al(2009)), and that there-fore the European component of the low-proficiencyICCI actually interferes with using high proficiencyas a way of distinguishing European L1s, a problemwhich would obviously not extend to an Asian-L1-only corpus.
This is an interesting result, but we willnot explore it further here.
In any case, it would leadus to predict that including ICCI data would be a badidea for TOEFL-11 testing.Since we did not have any way to evaluate ourIndian corpora (i.e.
the news, twitter, and translatedblogs from Section 4) without using the TOEFL-11,we instead took advantage of the option to submitmultiple runs, submitting runs which use each of thecorpora, and combining the blogs and news.6.2 Post Hoc AnalysisWith the TOEFL-11 data now visible to us, we firstask whether our specially collected Indian corporacan distinguish texts in the ICCI.
The test set usedin Table 10 contains only Hindi and Telugu texts.The results are quite modest (the guessing baselineis 50%), but suggest that all three corpora containsome information that distinguish Hindi and Telugu,particularly if bias adaption is used.The results for a selection of models on the fullset of TOEFL-11 languages is presented in Table11.
Since ours was the best-performing model inthis task, we include results for both the TOEFL-11 training (including development set) and test set,to facilitate future comparison.
Again, there is littledoubt that bias adaption is of huge benefit, thoughin fact our results in the Lang-8 alone, without bias194Table 11: 11-language testing on TOEFL-11 sets for open-training task 1; best result is in bold, best submitted run isin italics.Training SetAccuracy (%)TOEFL-11 test TOEFL-11 trainingno BA with BA no BA with BALang-8 39.5 53.2 37.2 48.2Lang-8+ICCI 36.9 51.0 34.9 46.3Lang-8+FCE+ICLE+ICNALE 44.5 55.8 44.9 53.1Lang-8+FCE+ICLE+ICNALE+Indian news 45.2 56.5 45.5 54.9Lang-8+FCE+ICLE+ICNALE+Indian tweets 44.9 56.4 45.1 53.4Lang-8+FCE+ICLE+ICNALE+Indian translated blog 45.4 50.1 45.7 49.9Lang-8+FCE+ICLE+ICNALE+News+Tweets 45.2 57.5 45.5 55.2Lang-8+FCE+ICLE+ICNALE+News+Tweets r = 0.1 44.9 58.2 45.0 58.2Table 10: Indian corpus testing for Open-training task 1;best result is in bold.Training SetAccuracy (%)no BA with BAIndian news 50.0 54.0Indian tweets 54.0 56.0Indian blogs 51.5 56.0adaption, would have been enough to take first placein this task.
Adding other corpora, including the In-dian corpora but not the ICCI, did consistently im-prove performance, as suggested by our testing inother corpora.
Although the translated blog data wasuseful in distinguishing Hindi from Telugu alone, ithad an unpredictable effect in the main task, lower-ing bias-adapted performance.
Training set selectiondoes seem to have a small positive effect, though wedid not see this consistently in our original testing.7 ConclusionOur efforts in the 2013 NLI shared task focused onthe potential benefits of external corpora.
We haveshown here that including training data from multi-ple corpora is effective at creating good cross-corpusNLI systems, particularly when domain adaptation,i.e.
bias adaption or training set selection, is alsoapplied; we were the highest-performing group inopen-training task 1 by a large margin.
This ap-proach can also be applied to improve performanceeven when training data from the same corpus isavailable, as in open-training task 2.
However, inthe closed-training task, despite testing a numberof new features, we did not see much improvementon our simple model based on earlier work.
Otherteams clearly did find some ways to improve onthis straightforward approach, and we hope to seeto what extent those improvements are generalizableacross different NLI corpora.AcknowledgementsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada.ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: Acorpus of non-native English.
Technical report, Edu-cational Testing Service.Julian Brooke and Graeme Hirst.
2011.
Native lan-guage detection with ?cheap?
learner corpora.
Pre-sented at the 2011 Learner Corpus Research Con-ference.
Published in Sylviane Granger, Gae?tanelleGilquin and Fanny Meunier, editors, (2013) TwentyYears of Learner Corpus Research: Looking back,Moving ahead.
Corpora and Language in Use - Pro-ceedings 1, Louvain-la-Neuve: Presses universitairesde Louvain.Julian Brooke and Graeme Hirst.
2012a.
Measuring in-terlanguage: Native language identification with L1-influence metrics.
In Proceedings of the Eighth In-ternational Conference on Language Resources andEvaluation (LREC ?12), pages 779?784, Istanbul,Turkey.195Julian Brooke and Graeme Hirst.
2012b.
Robust, lexical-ized native language identification.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (COLING ?12).Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia (ICWSM 2009), San Jose, CA.Serhiy Bykh and Detmar Meurers.
2012.
Native lan-guage identification using recurring n-grams ?
in-vestigating abstraction and domain dependence.
InProceedings of the 24th International Conference onComputational Linguistics (COLING ?12).Max Coltheart.
1980.
MRC Psycholinguistic DatabaseUser Manual: Version 1.
Birkbeck College.Scott A. Crossley and Danielle S. McNamara.
2012.
De-tecting the first language of second language writersusing automated indicies of cohesion, lexical sophis-tication, syntactic complexity and conceptual knowl-edge.
In Scott Jarvis and Scott A. Crossley, editors,Approaching Language Transfer through Text Clas-sification: Explorations in the Detection-based Ap-proach.
Multilingual Matters.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English (Version 2).
Presses Universitaires deLouvain, Louvain-la-Neuve.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Geolo-cation prediction in social media data by finding loca-tion indicative words.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics(COLING ?12).Shin?ichiro Ishikawa, 2011.
A new horizon in learnercorpus studies: The aim of the ICNALE project, pages3?11.
University of Strathclyde Press, Glasgow, UK.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics, pages 423?430.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005.Determining an author?s native language by min-ing a text for errors.
In Proceedings of the 11thACM SIGKDD International Conference on Knowl-edge Discovery in Data Mining (KDD ?05), pages624?628, Chicago, Illinois, USA.Barbara Plank and Gertjan van Noord.
2011.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 1566?1576, Portland, Oregon, USA,June.Ben Swanson and Eugene Charniak.
2012.
Native lan-guage detection with tree substitution grammars.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?12), pages193?197, Jeju, Korea.Ben Swanson and Eugene Charniak.
2013.
Extractingthe native language signal for second language acqui-sition.
In Proceedings of the 2013 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL HLT ?13).Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native tongues, lost and found:Resources and empirical evaluations in native lan-guage identification.
In Proceedings of the 24th In-ternational Conference on Computational Linguistics(COLING ?12).Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.Summary report on the first shared task on native lan-guage identification.
In Proceedings of the EighthWorkshop on Building Educational Applications Us-ing NLP, Atlanta, GA, USA, June.
Association forComputational Linguistics.Yukio Tono, Yuji Kawaguchi, and Makoto Minegishi,editors.
2012.
Developmental and Cross-linguisticPerspectives in Learner Corpus Research.
John Ben-jamins, Amsterdam/Philadelphia.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco.Sze-Meng Jojo Wong and Mark Dras.
2011.
Exploit-ing parse structures for native language identification.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP?11), pages 1600?1610, Edinburgh, Scotland, UK.Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.2012.
Exploring adaptor grammars for native lan-guage identification.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL ?12), Jeju, Korea.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 180?189, Portland, Oregon.196
