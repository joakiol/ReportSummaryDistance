Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 253?256,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsDetection of time-pressure induced stress in speech via acoustic indicators ?Matthew Frampton, Sandeep Sripada, Ricardo Augusto Hoffmann Bion and Stanley PetersCenter for the Study of Language and InformationStanford University, Stanford, CA, 94305 USA{frampton@,ssandeep@,ricardoh@,peters@csli.
}stanford.eduAbstractWe use automatically extracted acousticfeatures to detect speech which is gener-ated under stress, achieving 76.24% accu-racy with a binary logistic regression.
Ourdata are task-oriented human-human dia-logues in which a time-limit is unexpect-edly introduced partway through.
Anal-ysis suggests that we can detect approxi-mately when this event occurs.
We alsoconsider the importance of normalizingthe acoustic features by speaker, and de-tecting stress in new speakers.1 IntroductionThe term stressed speech can refer to speechgenerated under psychological stress (Sigmund etal., 2007).
Stress alters an individual?s mentaland physiological state, which then affects theirspeech.
The ability to identify stressed speechwould be very valuable to Spoken Dialogue Sys-tems (SDSs), especially in ?stressful?
applicationssuch as search-and-rescue robots.
Speech recog-nizers are usually trained on normal speech, andso can struggle badly on other speech.
Tech-niques exist for making ASR robust to noise/stress(Hansen and Patil, 2007), but knowing when to ap-ply them will in general require the ability to de-tect stressed speech.
This ability is clearly alsoneeded when the user?s stress level should affecthow the SDS responds.
An SDS should some-times generate stressed speech itself?for exam-ple, to impart a sense of urgency on the user.This paper investigates spectral-based acousticindicators of stress in human-human, task-oriented?The research reported in this paper was sponsored by theDepartment of the Navy, Office of Naval Research, undergrant number N00014-017-1-1049.
Any opinions, findingsand conclusions or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflect theviews of the Office of Naval Research.dialogues in which stress is induced in the lat-ter stages by the unexpected introduction of time-pressure.
Unlike previous studies, we detect stressin whole utterances in the raw audio, which ismore realistic for applications.
We also considerthe importance of normalizing the features, anddetection of both the introduction of the stressor,and stress in new speakers.2 Related workStressors and clip sizes: The stressors in pre-vious studies include logical problems, images ofhuman bodies with skin diseases/severe accidentinjuries (Tolkmitt and Scherer, 1986), loss of con-trol of a helicopter (Protopapas and Liberman,2001), university examinations (Sigmund et al,2007), and an increasingly difficult air controllersimulation and verbal quiz (Scherer et al, 2008).Sigmund et al (2007) detect stress in approxi-mately 2000 voiced segments of 5 vowels.
Tolk-mitt and Scherer (1986), Protopapas and Liberman(2001) and Scherer et al (2008) detect stress inwhole utterances, but these are respectively, readfrom a card, quiz answers, and with verbal contentremoved.
Studies on the Speech under Simulatedand Actual Stress (SUSAS) corpus (Hansen andBou-Ghazale, 1997) detect stress in words.
Theseinclude (Hansen, 1996; Zhou, 1999; Hansen andWomack, 1996; Zhou, 2001; Casale et al, 2007).The SUSAS corpus contains aircraft communica-tion words from a common highly confusable vo-cabulary set of 35, and they are divided into differ-ent speaking styles.Acoustic cues: The most widely investigatedacoustic cues relate to fundamental frequency (F0,also called pitch), formant frequencies and spec-tral composition e.g.
(Tolkmitt and Scherer, 1986;Hansen, 1996; Zhou, 1999; Protopapas and Liber-man, 2001; Sigmund et al, 2007; Scherer etal., 2008).
Mel-Frequency Cepstral Coefficients253Category ExamplesF0-related Median, mean, minimum, time of minimum as % thr?
clip, max, time of max as % thr?
clip,range (max-min), standard deviation, mean absolute slope,mean slope without octave jumps, number of voiced frames.Intensity-related Median, mean, minimum, time of minimum as % thr?
clip, max, time of max as % thr?
clip,range (max-min), standard deviation.Formant-related Mean, minimum, time of minimum as % through clip, max, time of max as % through clip,(for F1-F3) range (max-min).Spectral tilt-related Mean, minimum, maximum, range (max-min).Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).
(MFCCs)1 and Teager Energy Operator (TEO)2(Kaiser, 1990) based features have also been con-sidered e.g.
(Hansen and Womack, 1996; Zhou,2001; Casale et al, 2007).Features of all these types have proved usefulin detecting stressed speech.
The classificationmethods employed are various, including a tradi-tional binary hypothesis detection-theory method(Zhou, 1999) and neural networks (Hansen andWomack, 1996; Scherer et al, 2008), while Casaleet al (2007) used genetic algorithms for featureselection.
Of the two more recent studies whichdetected stress in whole utterances, Protopapasand Lieberman found that mean and maximum F0within an utterance correlate highly with subjectstress ratings, and Scherer et al?s neural networkoutperformed a human baseline.
Note that find-ings/results in these and other previous studies arenot directly comparable with our own, because wedetect stress in whole utterances in raw audio.3 DataThe original data (Eberhard et al, 2010) are 4task-oriented dialogues between 2 native English-speaking participants.
Hence there are 8 speakersin total (7 male, 1 female), and the dialogues con-tain 263, 172, 228 and 210 utterances respectively.During a dialogue, the participants (the direc-tor and member) are on a floor with corridors androoms that contain various colored boxes.
The di-rector stays in one room, and gives task instruc-tions via walkie-talkie to the member, providingdirections with a map which is partially completeand accurate for box locations.
The tasks are lo-cating boxes which are unmarked on the map, andtransferring blocks between and retrieving speci-fied boxes.
Initial instructions do not mention a1MFCCs model the human auditory system?s nonlinearfiltering in measuring spectral band energies.2The TEO is a nonlinear operator which uses mechanicaland physical considerations to extract the signal energy.time limit, but at the end of the 7th minute, the di-rector is given a timer and told there are 3 minutesto complete the current tasks, plus one new task.We use the Nuance speech recognizer (V. 9.0)to end-point each dialogue?s audio signal, and theresulting clips are mostly 1 to 3 seconds.
Inpreliminary experiments (not reported), denoisingseemed to remove acoustic information which isindicative of stress.
Hence we use raw audio.Stressed speech: For present purposes, we as-sume that all speech after the introduction of thetime limit is stressed.
Hence 448 of the 663 au-dio clips in our experimental data are unstressed,and 215 are stressed.
In future we plan to usethe Amazon Mechanical Turk to obtain perceivedstress ratings on a scale with more gradations.4 ExperimentsAcoustic features: We use Praat (Boersma andWeenink, 2010) to compute F0, intensity, formantand spectral tilt-related features for each clip (Ta-ble 1).
F0 (pitch) corresponds to the rate of vocalcord vibration in Hertz (Hz), and Intensity, to thesound?s loudness in decibels (dB), (derived fromthe amplitude or increase in air pressure).
A for-mant is a concentration of acoustic energy arounda particular frequency in the speech wave.
Thereare several, each corresponding to a resonance inthe vocal tract, and we consider the lowest three(F1-F3).
Spectral tilt measures the difference inenergy between the 1st and 2nd formants, and soestimates the degree to which energy at the funda-mental dominates in the glottal source waveform.Comparing different normalization methods:We evaluate binary logistic regression models with10-fold cross-validation, and try the following 4methods for normalizing each clip?s acoustic fea-tures according to its speaker.?
Maximum normalization: Due to the possi-bility of outliers, we divide each feature value254Normalization % Accuracy US %correct S %correct MCBMaximum normalization 74.4 (74.25) 86.67 (85.05) 48.5 (54.3) 67.8 (67.1)Z-score 73.5 (73.78) 84.89 (84.12) 49.53 (52.7) 67.8 (67.1)US Average 75.61 (76.24) 86.63 (86.2) 53.5 (55.9) 67.8 (67.1)S Average 75.31 (75) 84.67 (84.375) 55.6 (55.9) 67.8 (67.1)No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1)Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores withinbrackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline.by the 95th percentile value for that feature,rather than the maximum.?
Z-score: Using the mean and standard devi-ation for each feature, the feature vector isconverted to Z-scores3.?
Unstressed (US) average: Each feature isnormalized by its mean value in the un-stressed region.?
Stressed (S) average: Each feature is normal-ized by its mean value in the stressed region.Table 2 shows the results.
All those gener-ated with feature normalization are significantlybetter (p < 0.005) than the majority class base-line (MCB), (i.e.
classifying all utterances as un-stressed).
Without normalization, the overall accu-racy drops about 5?6%, and the stressed speechclass about 11?18%.
Different normalizationmethods do not produce very different results,but US average gives the best overall accuracy(75.61%).
When we remove the female speaker,this increases to 76.24%, and feature normaliza-tion remains important.We also tested our assumption that the speechbefore and after the introduction of time-pressureis unstressed and stressed respectively, by check-ing that they really are different.
As before, weconsidered 7 minutes unstressed, and 3 stressed,and used US average normalization.
However wenow assigned different minutes to the unstressedand stressed categories: first we swapped the 6thand 8th, then also the 5th and 9th, and then also the7th and 10th.
As a result, classification accuracydropped, (to 75%, then 68.71%, then 67.66%),which supports our assumption.Feature contribution analysis: Table 3 showsthe US average normalized features with informa-tion gain greater than zero.
Intensity and pitchfeatures are ranked most predictive (i.e.
maximum3A Z-score indicates the number of standard deviationsbetween an observation and the mean.and mean intensity, and mean and median pitch),but Spectral tilt mean and a couple of formant fea-tures are also predictive.
In general, higher valuesfor the most predictive pitch and intensity features(e.g.
Intensity max and Pitch mean) seem to indi-cate stress.
An interaction term for Intensity maxand Pitch mean caused a significant improvementin the fit of the model?the ?2 value (or change inthe -2 Log likelihood) was 4.952 (p < 0.05).Feature Info.
GainIntensity max .101Pitch mean .099Intensity mean .099Pitch median .088Pitch max .059Intensity min .046Spectral tilt mean .042Pitch min .041F1 min .038Intensity range .034Intensity std.
dev.
.033F3 range .033Intensity median .031Table 3: Unstressed average normalized features ranked byinformation gain.Detecting the introduction of the stressor:Figure 1 shows the percentage of audio clips ineach minute that were classified as stressed.
As wewould hope, there is a dramatic increase from the7th to the 8th minute (around 20% to over 50%).Such an increase could be used to detect the intro-duction of the stressor, time-pressure.Detecting stress in new speakers: To detectstressed speech in new speakers, we evaluate thelogistic regression with an 8-fold cross-validation,in each fold training on 7 speakers, and testing onthe other.
We apply US average normalization, ini-tially with the average values for the new speaker?sunstressed speech, and then with the average val-ues in unstressed speech across all ?seen?
speakers(speakers in the training set).
Evaluation scores(Table 4) are now lower, especially for the lat-ter approach, but the former remains significantly255Figure 1: The percentage of clips in each minute of thedialogues which our classifier marks as stressed, (note thattime-pressure is introduced at the end of minute 7).better than the MCB.
Since the female speaker?sstress class F-score is 0, we tried normalizing the7 male speakers based on only seen male data,and then average accuracy for a male rose from67.09% to 68.02% (not statistically significant).Spkr % Accuracy F-unstress F-stress1 62.5 (62.2) .77 (.74) .07 (0.34)2 75 (75) .84 (.84) .09 (0.44)3 57.6 (72.9) .62 (.81) .49 (0.5)4 71.73 (74) .84 (.83) 0 (0.43)5 71.62 (73.0) .77 (.77) .62 (0.68)6 77.6 (80.4) .86 (.88) .51 (0.52)7 64.36 (65.6) .74 (.77) .4 (0.35)8 60.97 (71.7) .67 (.8) .49 (0.46)Av.
67.67 (71.9) .76 (.80) .33 (0.46)Table 4: Predicting stress in new speakers: New speakerfeatures are normalized based on unstressed speech for allspeakers in training set (unbracketed) and on their own un-stressed speech (bracketed).
Speaker 4 is the female.5 ConclusionFor detecting stressed speech, we demonstratedthe importance of normalizing acoustic features byspeaker, and achieved 76.24% classification accu-racy with a binary logistic regression model.
Themost indicative features were maximum and meanintensity within an utterance, and mean and me-dian pitch.
After the introduction of time-pressure,the percentage of clips classified as stressed in-creased dramatically, showing that it is possibleto detect approximately when this event occurs.We also attempted to detect stressed speech in newspeakers, and as expected, results were poorer.In future work we plan to expand our data-setwith more dialogues, and test accuracy for detect-ing the introduction of the stressor.
We want to useMFCCs and TEO features, and also non-acousticfeatures such as disfluency features.
As mentionedpreviously, we also hope to move beyond binaryclassification, by acquiring perceived stress ratingson a scale with more gradations.ReferencesP.
Boersma and D. Weenink.
2010.
Praat: doing pho-netics by computer (version 5.1.29).
Available fromhttp://www.praat.org/.
[Computer program].S.
Casale, A. Russo, and S. Serrano.
2007.
Multistyle classi-fication of speech under stress using feature subset selec-tion based on genetic algorithms.
Speech Communication,49:801?810.K.
Eberhard, H. Nicholson, S. Kubler, S. Gundersen, and M.Scheutz.
2010.
The Indiana ?Cooperative Remote SearchTask?
(CReST) Corpus.
In Proc.
of LREC.J.H.L.
Hansen and S. Bou-Ghazale.
1997.
Getting startedwith SUSAS: a Speech Under Simulated and Actual Stressdatabase.
In Eurospeech-97: International Conference onSpeech Communication and Technology.J.
Hansen and S. Patil, 2007.
Speaker Classification I: Fun-damentals, Features, and Methods, chapter Speech UnderStress: Analysis, Modeling and Recognition, pages 108?137.
Springer-Verlag, Berlin, Heidelberg.J.
Hansen and B.Womack.
1996.
Feature analysis and neuralnetwork based classification of speech under stress.
IEEETransactions on Speech & Audio Processing, 4(4):307?313.J.
Hansen.
1996.
Analysis and compensation of speechunder stress and noise for environmental robustness inspeech recognition.
Speech Communications, Special Is-sue on Speech Under Stress, 20(2):151?170.J.F.
Kaiser.
1990.
On a simple algorithm to calculate theenergy of a signal.
In Proc.
of ICASSP.A.
Protopapas and P. Liberman.
2001.
Fundamental fre-quency of phonation and perceived emotional stress.
Jour-nal of the Acoustical Society of America, 101(4):2267?2277.S.
Scherer, H. Hofmann, M. Lampmann, M. Pfeil, S. Rhinow,F.
Schwenker, and G. Palm.
2008.
Emotion recognitionfrom speech: Stress experiment.
In Proc.
of LREC.M.
Sigmund, A. Prokes, and Z. Brabec.
2007.
Statisticalanalysis of glottal pulses in speech under psychologicalstress.
In Proc.
of the 16th European Signal ProcessingConference.F.
J. Tolkmitt and K. R. Scherer.
1986.
Effect of experi-mentally induced stress on vocal parameters.
Journal ofExperimental Psychology, 12(3):302?313.G.
Zhou.
1999.
Nonlinear speech analysis and acousticmodel adaptation with applications to stress classificationand speech recognition.
Ph.D. thesis, Duke University.G.
Zhou.
2001.
Nonlinear feature based classification ofspeech under stress.
IEEE Transactions on Speech & Au-dio Processing, 9:201?216.256
