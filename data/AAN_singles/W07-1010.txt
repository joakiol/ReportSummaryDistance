BioNLP 2007: Biological, translational, and clinical language processing, pages 73?80,Prague, June 2007. c?2007 Association for Computational LinguisticsExploring the Efficacy ofCaption Search for Bioscience Journal Search InterfacesMarti A. Hearst, Anna Divoli, Jerry YeSchool of Information, UC BerkeleyBerkeley, CA 94720{hearst,divoli,jerryye}@ischool.berkeley.eduMichael A. WooldridgeCalifornia Digital LibraryOakland, CA 94612mikew@ucop.eduAbstractThis paper presents the results of a pilot us-ability study of a novel approach to searchuser interfaces for bioscience journal arti-cles.
The main idea is to support search overfigure captions explicitly, and show the cor-responding figures directly within the searchresults.
Participants in a pilot study ex-pressed surprise at the idea, noting that theyhad never thought of search in this way.They also reported strong positive reactionsto the idea: 7 out of 8 said they would use asearch system with this kind of feature, sug-gesting that this is a promising idea for jour-nal article search.1 IntroductionFor at least two decades, the standard way to searchfor bioscience journal articles has been to use theNational Library of Medicine?s PubMed system tosearch the MEDLINE collection of journal articles.PubMed has innovated search in many ways, but todate search in PubMed is restricted to the title, ab-stract, and several kinds of metadata about the doc-ument, including authors, Medical Subject Heading(MeSH) labels, publication year, and so on.On the Web, searching within the full text of doc-uments has been standard for more than a decade,and much progress has been made on how to dothis well.
However, until recently, full text searchof bioscience journal articles was not possible dueto two major constraints: (1) the full text was notwidely available online, and (2) publishers restrictresearchers from downloading these articles in bulk.Recently, online full text of bioscience journal ar-ticles has become ubiquitous, eliminating one bar-rier.
The intellectual property restriction is underattack, and we are optimistic that it will be nearlyentirely diffused in a few years.
In the meantime,the PubMedCentral Open Access collection of jour-nals provides an unrestricted resource for scientiststo experiment with for providing full text search.1Full text availability requires a re-thinking of howsearch should be done on bioscience journal arti-cles.
One opportunity is to do information extrac-tion (text mining) to extract facts and relations fromthe body of the text, as well as from the title andabstract as done by much of the early text miningwork.
(The Biocreative competition includes tasksthat allow for extraction within full text (Yeh et al,2003; Hirschman et al, 2005).)
The results of textextraction can then be exposed in search interfaces,as done in systems like iHOP (Hoffmann and Va-lencia, 2004) and ChiliBot (Chen and Sharp, 2004)(although both of these search only over abstracts).Another issue is how to adjust search ranking al-gorithms when using full text journal articles.
Forexample, there is evidence that ranking algorithmsshould consider which section of an article the queryterms are found in, and assign different weights todifferent sections for different query types (Shah etal., 2003), as seen in the TREC 2006 GenomicsTrack (Hersh et al, 2006).Recently Google Scholar has provided search1The license terms for use for BioMed Central can befound at: http://www.biomedcentral.com/info/authors/licenseand the license for PubMedCentral can be found at:http://www.pubmedcentral.gov/about/openftlist.html73over the full text of journal articles from a widerange of fields, but with no special considerationfor the needs of bioscience researchers2.
GoogleScholar?s distinguishing characteristic is its abilityto show the number of papers that cite a given arti-cle, and rank papers by this citation count.
We be-lieve this is an excellent starting point for full textsearch, and any future journal article search systemshould use citation count as a metric.
Unfortunately,citation count requires access to the entire collectionof articles; something that is currently only avail-able to a search system that has entered into con-tracts with all of the journal publishers.In this article, we focus on another new opportu-nity: the ability to search over figure captions anddisplay the associated figures.
This idea is basedon the observation, noted by our own group as wellas many others, that when reading bioscience arti-cles, researchers tend to start by looking at the title,abstract, figures, and captions.
Figure captions canbe especially useful for locating information aboutexperimental results.
A prominent example of thiswas seen in the 2002 KDD competition, the goalof which was to find articles that contained exper-imental evidence for gene products, where the top-performing team focused its analysis on the figurecaptions (Yeh et al, 2003).In the Biotext project, we are exploring how toincorporate figures and captions into journal articlesearch explicitly, as part of a larger effort to providehigh-quality article search interfaces.
This paper re-ports on the results of a pilot study of the captionsearch idea.
Participants found the idea novel, stim-ulating, and most expressed a desire to use a searchinterface that supports caption search and figure dis-play.32 Related Work2.1 Automated Caption AnalysisSeveral research projects have examined the auto-mated analysis of text from captions.
Srihari (1991;1995) did early work on linking information be-tween photographs and their captions, to determine,for example, which person?s face in a newspaper2http://scholar.google.com3The current version of the interface can be seen athttp://biosearch.berkeley.eduphotograph corresponded to which name in the cap-tion.
Shatkay et al (2006) combined informationfrom images as well as captions to enhance a textcategorization algorithm.Cohen, Murphy, et al have explored several dif-ferent aspects of biological text caption analysis.
Inone piece of work (Cohen et al, 2003) they devisedand tested algorithms for parsing the structure of im-age captions, which are often quite complex, espe-cially when referring to a figure that has multipleimages within it.
In another effort, they developedtools to extract information relating to subcellularlocalization by automatically analyzing fluorescencemicroscope images of cells (Murphy et al, 2003).They later developed methods to extract facts fromthe captions referring to these images (Cohen et al,2003).Liu et al (2004) collected a set of figures andclassified them according to whether or not they de-picted schematic representations of protein interac-tions.
They then allowed users to search for a genename within the figure caption, returning only thosefigures that fit within the one class (protein interac-tion schematics) and contained the gene name.Yu et al (2006) created a bioscience image tax-onomy (consisting of Gel-Image, Graph, Image-of-Thing, Mix, Model, and Table) and used SupportVector Machines to classify the figures, using prop-erties of both the textual captions and the images.2.2 Figures in Bioscience Article SearchSome bioscience journal publishers provide a ser-vice called ?SummaryPlus?
that allows for displayof figures and captions in the description of a partic-ular article, but the interface does not apply to searchresults listings.4A medical image retrieval and image annotationtask have been part of the ImageCLEF competitionsince 2005 (Muller et al, 2006).5 The datasets forthis competition are clinical images, and the task isto retrieve images relevant to a query such as ?Showblood smears that include polymorphonuclear neu-4Recently a commercial offering by a company called CSAIllustrata was brought to our attention; it claims to use figuresand tables in search in some manner, but detailed information isnot freely available.5CLEF stands for Cross-language Evaluation Forum; it orig-inally evaluated multi-lingual information retrieval, but hassince broadened its mission.74trophils.?
Thus, the emphasis is on identifying thecontent of the images themselves.Yu and Lee (2006) hypothesized that the infor-mation found in the figures of a bioscience articleare summarized by sentences from that article?s ab-stract.
They succeeded in having 119 scientists markup the abstract of one of their own articles, indicat-ing which sentence corresponded to each figure inthe article.
They then developed algorithms to linksentences from the abstract to the figure caption con-tent.
They also developed and assessed a user inter-face called BioEx that makes use of this linking in-formation.
The interface shows a set of very smallimage thumbnails beneath each abstract.
When thesearcher?s mouse hovers over the thumbnail, the cor-responding sentence from the abstract is highlighteddynamically.To evaluate BioEx, Yu and Lee (2006) sent a ques-tionnaire to the 119 biologists who had done thehand-labeling linking abstract sentences to images,asking them to assess three different article displaydesigns.
The first design looked like the PubMedabstract view.
The second augmented the first viewwith very small thumbnails of figures extracted fromthe article.
The third was the second view aug-mented with color highlighting of the abstract?s sen-tences.
It is unclear if the biologists were asked todo searches over a collection or were just shown asample of each view and asked to rate it.
35% of thebiologists responded to the survey, and of these, 36out of 41 (88%) preferred the linked abstract viewover the other views.
(It should be noted that theeffort invested in annotating the abstracts may haveaffected the scientists?
view of the design.
)It is not clear, however, whether biologists wouldprefer to see the caption text itself rather than theassociated information from the abstract.
The sys-tem described did not allow for searching over textcorresponding to the figure caption.
The system alsodid not focus on how to design a full text and captionsearch system in general.3 Interface Design and ImplementationThe Biotext search engine indexes all Open Accessarticles available at PubMedCentral.
This collectionconsists of more than 150 journals, 20,000 articlesand 80,000 figures.
The figures are stored locally,and at different scales, in order to be able to presentthumbnails quickly.
The Lucene search engine6 isused to index, retrieve, and rank the text (default sta-tistical ranking).
The interface is web-based and isimplemented in Python and PHP.
Logs and other in-formation are stored and queried using MySQL.Figure 1a shows the results of searching over thecaption text in the Caption Figure view.
Figure1b shows the same search in the Caption Figurewith additional Thumbnails (CFT) view.
Figure 2a-b shows two examples of the Grid view, in whichthe query terms are searched for in the captions, andthe resulting figures are shown in a grid, along withmetadata information.7 The Grid view may be espe-cially useful for seeing commonalities among topics,such as all the phylogenetic trees that include a givengene, or seeing all images of embryo development ofsome species.The next section describes the study participants?reaction to these designs.4 Pilot Usability StudyThe design of search user interfaces is difficult; theevidence suggests that most searchers are reluctantto switch away from something that is familiar.
Asearch interface needs to offer something qualita-tively better than what is currently available in orderto be acceptable to a large user base (Hearst, 2006).Because text search requires the display of text,results listings can quickly obtain an undesirablycluttered look, and so careful attention to detail isrequired in the elements of layout and graphic de-sign.
Small details that users find objectionable canrender an interface objectionable, or too difficult touse.
Thus, when introducing a new search interfaceidea, great care must be taken to get the details right.The practice of user-centered design teaches how toachieve this goal: first prototype, then test the resultswith potential users, then refine the design based ontheir responses, and repeat (Hix and Hartson, 1993;Shneiderman and Plaisant, 2004).Before embarking on a major usability study todetermine if a new search interface idea is a goodone, it is advantageous to run a series of pilot stud-ies to determine which aspects of the design work,6http://lucene.apache.org7These screenshots represent the system as it was evaluated.The design has subsequently evolved and changed.75(a)(b)Figure 1: Search results on a query of zebrafish over the captions within the articles with (a) CF view, and(b) CFT view.
The thumbnail is shown to the left of a blue box containing the bibliographic informationabove a yellow box containing the caption text.
The full-size view of the figure can be overlaid over thecurrent page or in a new browser window.
In (b) the first few figures are shown as mini-thumbnails in a rowbelow the caption text with a link to view all the figures and captions.76(a)(b)Figure 2: Grid views of the first sets of figures returned as the result of queries for (a) mutagenesis and for(b) pathways over captions in the Open Access collection.77ID status sex lit search area(s) of specialization1 undergrad F monthly organic chemistry2 graduate F weekly genetics / molecular bio.3 other F rarely medical diagnostics4 postdoc M weekly neurobiology, evolution5 graduate F daily evolutionary bio., entomology6 undergrad F weekly molecular bio., biochemistry7 undergrad F monthly cell developmental bio.8 postdoc M daily molecular / developmental bio.Table 1: Participant Demographics.
Participant 3 isan unemployed former lab worker.which do not, make adjustments, and test somemore.
Once the design has stabilized and is re-ceiving nearly uniform positive feedback from pilotstudy participants, then a formal study can be runthat compares the novel idea to the state-of-the-art,and evaluates hypotheses about which features workwell for which kinds of tasks.The primary goal of this pilot study was to deter-mine if biological researchers would find the idea ofcaption search and figure display to be useful or not.The secondary goal was to determine, should cap-tion search and figure display be useful, how bestto support these features in the interface.
We wantto retain those aspects of search interfaces that areboth familiar and useful, and to introduce new ele-ments in such a way as to further enhance the searchexperience without degrading it.4.1 MethodWe recruited participants who work in our campus?main biology buildings to participate in the study.None of the participants were known to us in ad-vance.
To help avoid positive bias, we told partici-pants that we were evaluating a search system, butdid not mention that our group was the one whowas designing the system.
The participants all hadstrong interests in biosciences; their demographicsare shown in Table 1.Each participant?s session lasted approximatelyone hour.
First, they were told the purpose of thestudy, and then filled out an informed consent formand a background questionnaire.
Next, they used thesearch interfaces (the order of presentation was var-ied).
Before the use of each search interface, weexplained the idea behind the design.
The partici-pant then used the interface to search on their ownFigure 3: Likert scores on the CF view.
X-axis:participant ID, y-axis: Likert scores: 1 = stronglydisagree, 7 = strongly agree.
(Scale reversed forquestionnaire-posed cluttered and overwhelming.
)queries for about 10 minutes, and then filled out aquestionnaire describing their reaction to that de-sign.
After viewing all of the designs, they filledout a post-study questionnaire where they indicatedwhether or not they would like to use any of thedesigns in their work, and compared the design toPubMed-type search.Along with these standardized questions, we hadopen discussions with participants about their reac-tions to each view in terms of design and content.Throughout the study, we asked participants to as-sume that the new designs would eventually searchover the entire contents of PubMed and not just theOpen Access collection.We showed all 8 participants the Caption withFigure (CF) view (see Figure 1a), and Caption withFigure and additional Thumbnails (CFT) (see Figure1b), as we didn?t know if participants would want tosee additional figures from the caption?s paper.8 Wedid not show the first few participants the Grid view,as we did not know how the figure/caption searchwould be received, and were worried about over-whelming participants with new ideas.
(Usabilitystudy participants can become frustrated if exposedto too many options that they find distasteful or con-fusing.)
Because the figure search did receive pos-8We also experimented with showing full text search to thefirst five participants, but as that view was problematic, we dis-continued it and substituted a title/abstract search for the re-maining three participants.
These are not the focus of this studyand are not discussed further here.78itive reactions from 3 of the first 4 participants, wedecided to show the Grid view to the next 4.4.2 ResultsThe idea of caption search and figure display wasvery positively perceived by all but one participant.7 out of 8 said they would want to use either CFor CFT in their bioscience journal article searches.Figure 3 shows Likert scores for CF view.The one participant (number 2) who did not likeCF nor CFT thought that the captions/figures wouldnot be useful for their tasks, and preferred seeingthe articles?
abstracts.
Many participants noted thatcaption search would be better for some tasks thanothers, where a more standard title & abstract or full-text search would be preferable.
Some participantssaid that different views serve different roles, andthey would use more than one view depending onthe goal of their search.
Several suggested combin-ing abstract and figure captions in the search and/orthe display.
(Because this could lead to search re-sults that require a lot of scrolling, it would probablybe best to use modern Web interface technologiesto dynamically expand long abstracts and captions.
)When asked for their preference versus PubMed, 5out of 8 rated at least one of the figure searchesabove PubMed?s interface.
(In some cases this maybe due to a preference for the layout in our design asopposed to entirely a preference for caption search.
)Two of the participants preferred CFT to CF; therest thought CFT was too busy.
It became clearthrough the course of this study that it would bebest to show all the thumbnails that correspond to agiven article as the result of a full-text or abstract-text search interface, and to show only the figurethat corresponds to the caption in the caption searchview, with a link to view all figures from this articlein a new page.All four participants who saw the Grid view likedit, but noted that the metadata shown was insuffi-cient; if it were changed to include title and otherbibliographic data, 2 of the 4 who saw Grid said theywould prefer that view over the CF view.
Severalparticipants commented that they have used GoogleImages to search for images but they rarely find whatthey are looking for.
They reacted very positivelyto the idea of a Google Image-type system special-ized to biomedical images.
One participant went sofar as to open up Google Image search and comparethe results directly, finding the caption search to bepreferable.All participants favored the ability to browse allfigures from a paper once they find the abstract orone of the figures relevant to their query.
Two partic-ipants commented that if they were looking for gen-eral concepts, abstract search would be more suit-able but for a specific method, caption view wouldbe better.4.3 Suggestions for RedesignAll participants found the design of the new viewsto be simple and clear.
They told us that they gen-erally want information displayed in a simple man-ner, with as few clicks needed as possible, and withas few distracting links as possible.
Only a few ad-ditional types of information were suggested fromsome participants: display, or show links to, relatedpapers and provide a link to the full text PDF directlyin the search results, as opposed to having to accessthe paper via PubMed.Participants also made clear that they would of-ten want to start from search results based on titleand abstract, and then move to figures and captions,and from there to the full article, unless they are do-ing figure search explicitly.
In that case, they wantto start with CF or Grid view, depending on howmuch information they want about the figure at firstglance.They also wished to have the ability to sort the re-sults along different criteria, including year of pub-lication, alphabetically by either journal or authorname, and by relevance ranking.
This result hasbeen seen in studies of other kinds of search inter-faces as well (Reiterer et al, 2005; Dumais et al,2003).
We have also received several requests for ta-ble caption search along with figure caption search.5 Conclusions and Future WorkThe results of this pilot study suggest that captionsearch and figure display is a very promising direc-tion for bioscience journal article search, especiallypaired with title/abstract search and potentially withother forms of full-text search.
A much larger-scalestudy must be performed to firmly establish this re-sult, but this pilot study provides insight about how79to design a search interface that will be positively re-ceived in such a study.
Our results also suggest thatweb search systems like Google Scholar and GoogleImages could be improved by showing images fromthe articles along lines of specialization.The Grid view should be able to show imagesgrouped by category type that is of interest to biolo-gists, such as heat maps and phylogenetic trees.
Oneparticipant searched on pancreas and was surprisedwhen the top-ranked figure was an image of a ma-chine.
This idea underscores the need for BioNLPresearch in the study of automated caption classifi-cation.
NLP is needed both to classify images andperhaps also to automatically determine which im-ages are most ?interesting?
for a given article.To this end, we are in the process of building aclassifier for the figure captions, in order to allowfor grouping by type.
We have developed an im-age annotation interface and are soliciting help withhand-labeling from the research community, to builda training set for an automated caption classifier.In future, we plan to integrate table captionsearch, to index the text that refers to the cap-tion, along with the caption, and to provide inter-face features that allow searchers to organize andfilter search results according to metadata such asyear published, and topical information such asgenes/proteins mentioned.
We also plan to conductformal interface evaluation studies, including com-paring to PubMed-style presentations.Acknowledgements: This work was supported inpart by NSF DBI-0317510.
We thank the study par-ticipants for their invaluable help.ReferencesH.
Chen and B.M.
Sharp.
2004.
Content-rich biological net-work constructed by mining PubMed abstracts.
BMC Bioin-formatics, 5(147).W.W.
Cohen, R. Wang, and R.F.
Murphy.
2003.
Understand-ing captions in biomedical publications.
Proceedings of theninth ACM SIGKDD international conference on Knowledgediscovery and data mining, pages 499?504.S.
Dumais, E. Cutrell, J.J. Cadiz, G. Jancke, R. Sarin, and D.C.Robbins.
2003.
Stuff I?ve seen: a system for personal in-formation retrieval and re-use.
Proceedings of SIGIR 2003,pages 72?79.M.
Hearst.
2006.
Design recommendations for hierarchi-cal faceted search interfaces.
In ACM SIGIR Workshop onFaceted Search, Seattle, WA.W.
Hersh, A. Cohen, P. Roberts, and Rekapalli H. K. 2006.TREC 2006 genomics track overview.
The Fifteenth TextRetrieval Conference.L.
Hirschman, A. Yeh, C. Blaschke, and A. Valencia.
2005.Overview of BioCreAtIvE: critical assessment of informa-tion extraction for biology.
BMC Bioinformatics, 6:1.D.
Hix and H.R.
Hartson.
1993.
Developing user interfaces:ensuring usability through product & process.
John Wiley& Sons, Inc. New York, NY, USA.R.
Hoffmann and A. Valencia.
2004.
A gene network for navi-gating the literature.
Nature Genetics, 36(664).F.
Liu, T-K. Jenssen, V. Nygaard, J. Sack, and E. Hovig.
2004.FigSearch: a figure legend indexing and classification sys-tem.
Bioinformatics, 20(16):2880?2882.H.
Muller, T. Deselaers, T. Lehmann, P. Clough, E. Kim, andW.
Hersh.
2006.
Overview of the ImageCLEF 2006 MedicalImage Retrieval Tasks.
In Working Notes for the CLEF 2006Workshop.R.F.
Murphy, M. Velliste, and G. Porreca.
2003.
Robust Nu-merical Features for Description and Classification of Sub-cellular Location Patterns in Fluorescence Microscope Im-ages.
The Journal of VLSI Signal Processing, 35(3):311?321.B.
Rafkind, M. Lee, S.F.
Chang, and H. Yu.
2006.
Exploringtext and image features to classify images in bioscience lit-erature.
Proceedings of the BioNLP Workshop on LinkingNatural Language Processing and Biology at HLT-NAACL,6:73?80.H.
Reiterer, G. Tullius, and T. M. Mann.
2005.
Insyder:a content-based visual-information-seeking system for theweb.
International Journal on Digital Libraries, 5(1):25?41, Mar.P.K.
Shah, C. Perez-Iratxeta, P. Bork, and M.A.
Andrade.2003.
Information extraction from full text scientific arti-cles: where are the keywords?
BMC Bioinformatics, 4(20).H.
Shatkay, N. Chen, and D. Blostein.
2006.
Integrating im-age data into biomedical text categorization.
Bioinformatics,22(14):e446.B.
Shneiderman and C. Plaisant.
2004.
Designing the user in-terface: strategies for effective human-computer interaction,4/E.
Addison Wesley.R.K.
Srihari.
1991.
PICTION: A System that Uses Captionsto Label Human Faces in Newspaper Photographs.
Proceed-ings AAAI-91, pages 80?85.RK Srihari.
1995.
Automatic indexing and content-based re-trieval of captioned images.
Computer, 28(9):49?56.A.S.
Yeh, L. Hirschman, and A.A. Morgan.
2003.
Evaluationof text data mining for database curation: lessons learnedfrom the KDD Challenge Cup.
Bioinformatics, 19(1):i331?i339.H.
Yu and M. Lee.
2006.
Accessing bioscience images fromabstract sentences.
Bioinformatics, 22(14):e547.80
