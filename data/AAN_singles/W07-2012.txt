Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64?69,Prague, June 2007. c?2007 Association for Computational LinguisticsThe SemEval-2007 WePS Evaluation: Establishing a benchmark for theWeb People Search TaskJavier ArtilesUNED NLP & IR groupMadrid, Spainjavart@bec.uned.esnlp.uned.es/?javierJulio GonzaloUNED NLP & IR groupMadrid, Spainjulio@lsi.uned.esnlp.uned.es/?julioSatoshi SekineComputer Science DepartmentNew York University, USAsekine@cs.nyu.edunlp.cs.nyu.edu/sekineAbstractThis paper presents the task definition, re-sources, participation, and comparative re-sults for the Web People Search task, whichwas organized as part of the SemEval-2007evaluation exercise.
This task consists ofclustering a set of documents that mentionan ambiguous person name according to theactual entities referred to using that name.1 IntroductionFinding information about people in the World WideWeb is one of the most common activities of Internetusers.
Person names, however, are highly ambigu-ous.
In most cases, the results for a person namesearch are a mix of pages about different peoplesharing the same name.
The user is then forced ei-ther to add terms to the query (probably losing recalland focusing on one single aspect of the person), orto browse every document in order to filter the infor-mation about the person he is actually looking for.In an ideal system the user would simply type aperson name, and receive search results clustered ac-cording to the different people sharing that name.And this is, in essence, the WePS (Web PeopleSearch) task we have proposed to SemEval-2007participants: systems receive a set of web pages(which are the result of a web search for a per-son name), and they have to cluster them in asmany sets as entities sharing the name.
This taskhas close links with Word Sense Disambiguation(WSD), which is generally formulated as the taskof deciding which sense a word has in a given con-text.
In both cases, the problem addressed is the res-olution of the ambiguity in a natural language ex-pression.
A couple of differences make our prob-lem different.
WSD is usually focused on open-class words (common nouns, adjectives, verbs andadverbs).
The first difference is that boundaries be-tween word senses in a dictionary are often subtleor even conflicting, making binary decisions harderand sometimes even useless depending on the ap-plication.
In contrast, distinctions between peopleshould be easier to establish.
The second differenceis that WSD usually operates with a dictionary con-taining a relatively small number of senses that canbe assigned to each word.
Our task is rather a caseof Word Sense Discrimination, because the numberof ?senses?
(actual people) is unknown a priori, andit is in average much higher than in the WSD task(there are 90,000 different names shared by 100 mil-lion people according to the U.S. Census Bureau).There is also a strong relation of our proposedtask with the Co-reference Resolution problem, fo-cused on linking mentions (including pronouns) ina text.
Our task can be seen as a co-reference reso-lution problem where the focus is on solving inter-document co-reference, disregarding the linking ofall the mentions of an entity inside each document.An early work in name disambiguation (Baggaand Baldwin, 1998) uses the similarity between doc-uments in a Vector Space using a ?bag of words?representation.
An alternative approach by Mannand Yarowsky (2003) is based on a rich feature spaceof automatically extracted biographic information.Fleischman and Hovy (2004) propose a MaximumEntropy model trained to give the probability that64two names refer to the same individual 1.The paper is organized as follows.
Section 2 pro-vides a description of the experimental methodol-ogy, the training and test data provided to the par-ticipants, the evaluation measures, baseline systemsand the campaign design.
Section 3 gives a descrip-tion of the participant systems and provides the eval-uation results.
Finally, Section 4 presents some con-clusions.2 Experimental Methodology2.1 DataFollowing the general SemEval guidelines, we haveprepared trial, training and test data sets for the task,which are described below.2.1.1 Trial dataFor this evaluation campaign we initially deliv-ered a trial corpus for the potential participants.
Thetrial data consisted of an adapted version of theWePS corpus described in (Artiles et al, 2006).
Thepredominant feature of this corpus is a high numberof entities in each document set, due to the fact thatthe ambiguous names were extracted from the mostcommon names in the US Census.
This corpus didnot completely match task specifications because itdid not consider documents with internal ambiguity,nor it did consider non-person entities; but it was,however, a cost-effective way of releasing data toplay around with.
During the first weeks after releas-ing this trial data to potential participants, some an-notation mistakes were noticed.
We preferred, how-ever, to leave the corpus ?as is?
and concentrate ourefforts in producing clean training and test datasets,rather than investing time in improving trial data.2.1.2 Training dataIn order to provide different ambiguity scenarios,we selected person names from different sources:US Census.
We reused the Web03 corpus (Mann,2006), which contains 32 names randomly pickedfrom the US Census, and was well suited for thetask.Wikipedia.
Another seven names were sampledfrom a list of ambiguous person names in the En-glish Wikipedia.
These were expected to have a1For a comprehensive bibliography on person name disam-biguation refer to http://nlp.uned.es/wepsfew predominant entities (popular or historical), andtherefore a lower ambiguity than the previous set.ECDL.
Finally, ten additional names were ran-domly selected from the Program Committee listingof a Computer Science conference (ECDL 2006).This set offers a scenario of potentially low am-biguity (computer science scholars usually have astronger Internet presence than other professionalfields) with the added value of the a priori knowl-edge of a domain specific type of entity (scholar)present in the data.All datasets consist of collections of web pagesobtained from the 100 top results for a person namequery to an Internet search engine 2.
Note that 100is an upper bound, because in some occasions theURL returned by the search engine no longer exists.The second and third datasets (developed explic-itly for our task) consist of 17 person names and1685 associated documents in total (99 documentsper name in average).
Each web page was down-loaded and stored for off-line processing.
We alsostored the basic metadata associated to each searchresult, including the original URL, title, position inthe results ranking and the corresponding snippetgenerated by the search engine.In the process of generating the corpus, the se-lection of the names plays an important role, poten-tially conditioning the degree of ambiguity that willbe found later in the Web search results.
The reasonsfor this variability in the ambiguity of names are di-verse and do not always correlate with the straight-forward census frequency.
A much more decisivefeature is, for instance, the presence of famous en-tities sharing the ambiguous name with less popularpeople.
As we are considering top search results,these can easily be monopolized by a single entitythat is popular in the Internet.After the annotation of this data (see section2.1.4.)
we found our predictions about the averageambiguity of each dataset not to be completely ac-curate.
In Table 1 we see that the ECDL-06 averageambiguity is indeed relatively low (except for thedocuments for ?Thomas Baker?
standing as the mostambiguous name in the whole training).
Wikipedianames have an average ambiguity of 23,14 entities2We used the Yahoo!
API from Yahoo!
Search Web Ser-vices (http://developer.yahoo.com/search/web/).65Name entities documents discardedWikipedia namesJohn Kennedy 27 99 6George Clinton 27 99 6Michael Howard 32 99 8Paul Collins 37 98 6Tony Abbott 7 98 9Alexander Macomb 21 100 14David Lodge 11 100 9Average 23,14 99,00 8,29ECDL-06 NamesEdward Fox 16 100 36Allan Hanbury 2 100 32Donna Harman 7 98 6Andrew Powell 19 98 48Gregory Crane 4 99 17Jane Hunter 15 99 59Paul Clough 14 100 35Thomas Baker 60 100 31Christine Borgman 7 99 11Anita Coleman 9 99 28Average 15,30 99,20 30,30WEB03 CorpusTim Whisler 10 33 8Roy Tamashiro 5 23 6Cynthia Voigt 1 405 314Miranda Bollinger 2 2 0Guy Dunbar 4 51 34Todd Platts 2 239 144Stacey Doughty 1 2 0Young Dawkins 4 61 35Luke Choi 13 20 6Gregory Brennan 32 96 38Ione Westover 1 4 0Patrick Karlsson 10 24 8Celeste Paquette 2 17 2Elmo Hardy 3 55 15Louis Sidoti 2 6 3Alexander Markham 9 32 16Helen Cawthorne 3 46 13Dan Rhone 2 4 2Maile Doyle 1 13 1Alice Gilbreath 8 74 30Sidney Shorter 3 4 0Alfred Schroeder 35 112 58Cathie Ely 1 2 0Martin Nagel 14 55 31Abby Watkins 13 124 35Mary Lemanski 2 152 78Gillian Symons 3 30 6Pam Tetu 1 4 2Guy Crider 2 2 0Armando Valencia 16 79 20Hannah Bassham 2 3 0Charlotte Bergeron 5 21 8Average 5,90 47,20 18,00Global average 10,76 71,02 26,00Table 1: Training Dataper name, which is higher than for the ECDL set.The WEB03 Corpus has the lowest ambiguity (5,9entities per name), for two reasons: first, randomlypicked names belong predominantly to the long tailof unfrequent person names which, per se, have lowambiguity.
Being rare names implies that in averagethere are fewer documents returned by the search en-gine (47,20 per name), which also reduces the pos-sibilities to find ambiguity.2.1.3 Test dataFor the test data we followed the same processdescribed for the training.
In the name selection wetried to maintain a similar distribution of ambigu-ity degrees and scenario.
For that reason we ran-domly extracted 10 person names from the EnglishWikipedia and another 10 names from participantsin the ACL-06 conference.
In the case of the US cen-sus names, we decided to focus on relatively com-mon names, to avoid the problems explained above.Unfortunately, after the annotation was finished(once the submission deadline had expired), wefound a major increase in the ambiguity degrees (Ta-ble 2) of all data sets.
While we expected a raise inthe case of the US census names, the other two casesjust show that there is a high (and unpredictable)variability, which would require much larger datasets to have reliable population samples.This has made the task particularly challengingfor participants, because naive learning strategies(such as empirical adjustment of distance thresholdsto optimize standard clustering algorithms) might bemisleaded by the training set.2.1.4 AnnotationThe annotation of the data was performed sepa-rately in each set of documents related to an ambigu-ous name.
Given this set of approximately 100 doc-uments that mention the ambiguous name, the an-notation consisted in the manual clustering of eachdocument according to the actual entity that is re-ferred on it.When non person entities were found (for in-stance, organization or places named after a person)the annotation was performed without any specialrule.
Generally, the annotator browses documentsfollowing the original ranking in the search results;after reading a document he will decide whether thementions of the ambiguous name refer to a new en-tity or to a entity previously identified.
We askedthe annotators to concentrate first on mentions thatstrictly contained the search string, and then to payattention to the co-referent variations of the name.For instance ?John Edward Fox?
or ?Edward FoxSmith?
would be valid mentions.
?Edward J.
Fox?,however, breaks the original search string, and wedo not get into name variation detection, so it willbe considered valid only if it is co-referent to a valid66Name entities documents discardedWikipedia namesArthur Morgan 19 100 52James Morehead 48 100 11James Davidson 59 98 16Patrick Killen 25 96 4William Dickson 91 100 8George Foster 42 99 11James Hamilton 81 100 15John Nelson 55 100 25Thomas Fraser 73 100 13Thomas Kirk 72 100 20Average 56,50 99,30 17,50ACL06 NamesDekang Lin 1 99 0Chris Brockett 19 98 5James Curran 63 99 9Mark Johnson 70 99 7Jerry Hobbs 15 99 7Frank Keller 28 100 20Leon Barrett 33 98 9Robert Moore 38 98 28Sharon Goldwater 2 97 4Stephen Clark 41 97 39Average 31,00 98,40 12,80US Census NamesAlvin Cooper 43 99 9Harry Hughes 39 98 9Jonathan Brooks 83 97 8Jude Brown 32 100 39Karen Peterson 64 100 16Marcy Jackson 51 100 5Martha Edwards 82 100 9Neil Clark 21 99 7Stephan Johnson 36 100 20Violet Howard 52 98 27Average 50,30 99,10 14,90Global average 45,93 98,93 15,07Table 2: Test Datamention.In order to perform the clustering, the annotatorwas asked to pay attention to objective facts (bi-ographical dates, related names, occupations, etc.
)and to be conservative when making decisions.
Thefinal result is a complete clustering of the docu-ments, where each cluster contains the documentsthat refer to a particular entity.
Following the pre-vious example, in documents for the name ?EdwardFox?
the annotator found 16 different entities withthat name.
Note that there is no a priori knowledgeabout the number of entities that will be discoveredin a document set.
This makes the task speciallydifficult when there are many different entities anda high volume of scattered biographical informationto take into account.In cases where the document does not offerenough information to decide whether it belongs toa cluster or is a new entity, it is discarded from theevaluation process (not from the dataset).
Anothercommon reason for discarding documents was theabsence of the person name in the document, usu-ally due to a mismatch between the search enginecache and the downloaded URL.We found that, in many cases, different entitieswere mentioned using the ambiguous name within asingle document.
This was the case when a doc-ument mentions relatives with names that containthe ambiguous string (for instance ?Edward Fox?and ?Edward Fox Jr.?).
Another common case ofintra-document ambiguity is that of pages contain-ing database search results, such as book lists fromAmazon, actors from IMDB, etc.
A similar case isthat of pages that explicitly analyze the ambiguity ofa person name (Wikipedia ?disambiguation?
pages).The way this situation was handled, in terms of theannotation, was to assign each document to as manyclusters as entities were referred to on it with theambiguous name.2.2 Evaluation measuresEvaluation was performed in each document set(web pages mentioning an ambiguous person name)of the data distributed as test.
The human annotationwas used as the gold standard for the evaluation.Each system was evaluated using the standard pu-rity and inverse purity clustering measures Purity isrelated to the precision measure, well known in In-formation Retrieval.
This measure focuses on thefrequency of the most common category in eachcluster, and rewards the clustering solutions that in-troduce less noise in each cluster.
Being C the setof clusters to be evaluated, L the set of categories(manually annotated) and n the number of clusteredelements, purity is computed by taking the weightedaverage of maximal precision values:Purity =?i|Ci|nmax Precision(Ci, Lj)where the precision of a cluster Ci for a given cat-egory Lj is defined as:Precision(Ci, Lj) =|Ci?Lj ||Ci|Inverse Purity focuses on the cluster with maxi-mum recall for each category, rewarding the clus-tering solutions that gathers more elements of eachcategory in a corresponding single cluster.
InversePurity is defined as:67Inverse Purity =?i|Li|nmax Precision(Li, Cj)For the final ranking of systems we used the har-monic mean of purity and inverse purity F?=0,5 .
TheF measure is defined as follows:F =1?
1Purity + (1?
?
)1Inverse PurityF?=0,2 is included as an additional measure giv-ing more importance to the inverse purity aspect.The rationale is that, for a search engine user, itshould be easier to discard a few incorrect webpages in a cluster containing all the informationneeded, than having to collect the relevant infor-mation across many different clusters.
Therefore,achieving a high inverse purity should be rewardedmore than having high purity.2.3 BaselinesTwo simple baseline approaches were applied to thetest data.
The ALL-IN-ONE baseline provides aclustering solution where all the documents are as-signed to a single cluster.
This has the effect of al-ways achieving the highest score in the inverse pu-rity measure, because all classes have their docu-ments in a single cluster.
On the other hand, thepurity measure will be equal to the precision of thepredominant class in that single cluster.
The ONE-IN-ONE baseline gives another extreme clusteringsolution, where every document is assigned to a dif-ferent cluster.
In this case purity always gives itsmaximum value, while inverse purity will decreasewith larger classes.2.4 Campaign designThe schedule for the evaluation campaign was set bythe SemEval organisation as follows: (i) release taskdescription and trial data set; (ii) release of trainingand test; (iii) participants send their answers to thetask organizers; (iv) the task organizers evaluate theanswers and send the results.The task description and the initial trial data setwere publicly released before the start of the officialevaluation.The official evaluation period started with the si-multaneous release of both training and test data, to-gether with a scoring script with the main evaluationmeasures to be used.
This period spanned five weeksin which teams were allowed to register and down-load the data.
During that period, results for a giventask had to be submitted no later than 21 days af-ter downloading the training data and no later than 7days after downloading the test data.
Only one sub-mission per team was allowed.Training data included the downloaded webpages, their associated metadata and the human clus-tering of each document set, providing a develop-ment test-bed for the participant?s systems.
We alsospecified the source of each ambiguous name in thetraining data (Wikipedia, ECDL conference and USCensus).
Test data only included the downloadedweb pages and their metadata.
This section of thecorpus was used for the systems evaluation.
Partici-pants were required to send a clustering for each testdocument set.Finally, after the evaluation period was finishedand all the participants sent their data, the task orga-nizers sent the evaluation for the test data.3 Results of the evaluation campaign29 teams expressed their interest in the task; thisnumber exceeded our expectations for this pilot ex-perience, and confirms the potential interest of theresearch community in this highly practical prob-lem.
Out of them, 16 teams submitted results withinthe deadline; their results are reported below.3.1 Results and discussionTable 3 presents the macro-averaged results ob-tained by the sixteen systems plus the two baselineson the test data.
We found macro-average 3 prefer-able to micro-average 4 because it has a clear inter-pretation: if the evaluation measure is F, then weshould calculate F for every test case (person name)and then average over all trials.
The interpretationof micro-average F is less clear.The systems are ranked according to the scoresobtained with the harmonic mean measure F?=0,5 of3Macro-average F consists of computing F for every test set(person name) and then averaging over all test sets.4Micro-average F consists of computing the average P andIP (over all test sets) and then calculating F with these figures.68Macro-averaged ScoresF-measuresrank team-id ?
=,5 ?
=,2 Pur Inv Pur1 CU COMSEM ,78 ,83 ,72 ,882 IRST-BP ,75 ,77 ,75 ,803 PSNUS ,75 ,78 ,73 ,824 UVA ,67 ,62 ,81 ,605 SHEF ,66 ,73 ,60 ,826 FICO ,64 ,76 ,53 ,907 UNN ,62 ,67 ,60 ,738 ONE-IN-ONE ,61 ,52 1,00 ,479 AUG ,60 ,73 ,50 ,8810 SWAT-IV ,58 ,64 ,55 ,7111 UA-ZSA ,58 ,60 ,58 ,6412 TITPI ,57 ,71 ,45 ,8913 JHU1-13 ,53 ,65 ,45 ,8214 DFKI2 ,50 ,63 ,39 ,8315 WIT ,49 ,66 ,36 ,9316 UC3M 13 ,48 ,66 ,35 ,9517 UBC-AS ,40 ,55 ,30 ,9118 ALL-IN-ONE ,40 ,58 ,29 1,00Table 3: Team rankingpurity and inverse purity.
Considering only the par-ticipant systems, the average value for the rankingmeasure was 0, 60 and its standard deviation 0, 11.Results with F?=0,2 are not substantially different(except for the two baselines, which roughly swappositions).
There are some ranking swaps, but gen-erally only within close pairs.The good performance of the ONE-IN-ONE base-line system is indicative of the abundance of single-ton entities (entities represented by only one doc-ument).
This situation increases the inverse purityscore for this system giving a harmonic measurehigher than the expected.4 ConclusionsThe WEPS task ended with considerable success interms of participation, and we believe that a carefulanalysis of the contributions made by participants(which is not possible at the time of writing this re-port) will be an interesting reference for future re-search.
In addition, all the collected and annotateddataset will be publicly available 5 as a benchmarkfor Web People Search systems.At the same time, it is clear that building a re-liable test-bed for the task is not simple.
First ofall, the variability across test cases is large and un-predictable, and a system that works well with the5http://nlp.uned.es/wepsnames in our test bed may not be reliable in practi-cal, open search situations.
Partly because of that,our test-bed happened to be unintentionally chal-lenging for systems, with a large difference be-tween the average ambiguity in the training and testdatasets.
Secondly, it is probably necessary to thinkabout specific evaluation measures beyond standardclustering metrics such as purity and inverse purity,which are not tailored to the task and do not be-have well when multiple classification is allowed.We hope to address these problems in a forthcom-ing edition of the WEPS task.5 AcknowledgementsThis research was supported in part by the NationalScience Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish gov-ernment under project Text-Mess (TIN2006-15265-C06).
This paper does not necessarily reflect the po-sition of the U.S. Government.ReferencesJavier Artiles, Julio Gonzalo, and Felisa Verdejo.
2005.A Testbed for People Searching Strategies in theWWW In Proceedings of the 28th annual Interna-tional ACM SIGIR conference on Research and De-velopment in Information Retrieval (SIGIR?05), pages569-570.Amit Bagga and Breck Baldwin.
1998.
Entity-Based Cross-Document Coreferencing Using the Vec-tor Space Model In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and the 17th International Conference on Compu-tational Linguistics (COLING-ACL?98), pages 79-85.Michael B. Fleischman and Eduard Hovy 2004.
Multi-document person name resolution.
In Proceedings ofACL-42, Reference Resolution Workshop.Gideon S. Mann.
2006.
Multi-Document Statistical FactExtraction and Fusion Ph.D. Thesis.Gideon S. Mann and David Yarowsky 2003.
Unsuper-vised Personal Name Disambiguation In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL, pages 33-40.69
