Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11?17,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTemporal Restricted Boltzmann Machines for Dependency ParsingNikhil GargDepartment of Computer ScienceUniversity of GenevaSwitzerlandnikhil.garg@unige.chJames HendersonDepartment of Computer ScienceUniversity of GenevaSwitzerlandjames.henderson@unige.chAbstractWe propose a generative model based onTemporal Restricted Boltzmann Machines fortransition based dependency parsing.
Theparse tree is built incrementally using a shift-reduce parse and an RBM is used to modeleach decision step.
The RBM at the currenttime step induces latent features with the helpof temporal connections to the relevant previ-ous steps which provide context information.Our parser achieves labeled and unlabeled at-tachment scores of 88.72% and 91.65% re-spectively, which compare well with similarprevious models and the state-of-the-art.1 IntroductionThere has been significant interest recently in ma-chine learning methods that induce generative mod-els with high-dimensional hidden representations,including neural networks (Bengio et al, 2003; Col-lobert and Weston, 2008), Bayesian networks (Titovand Henderson, 2007a), and Deep Belief Networks(Hinton et al, 2006).
In this paper, we investi-gate how these models can be applied to dependencyparsing.
We focus on Shift-Reduce transition-basedparsing proposed by Nivre et al (2004).
In this classof algorithms, at any given step, the parser has tochoose among a set of possible actions, each repre-senting an incremental modification to the partiallybuilt tree.
To assign probabilities to these actions,previous work has proposed memory-based classi-fiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b),and Incremental Sigmoid Belief Networks (ISBN)(Titov and Henderson, 2007b).
In a related earlierwork, Ratnaparkhi (1999) proposed a maximum en-tropy model for transition-based constituency pars-ing.
Of these approaches, only ISBNs induce high-dimensional latent representations to encode parsehistory, but suffer from either very approximate orslow inference procedures.We propose to address the problem of inferencein a high-dimensional latent space by using an undi-rected graphical model, Restricted Boltzmann Ma-chines (RBMs), to model the individual parsingdecisions.
Unlike the Sigmoid Belief Networks(SBNs) used in ISBNs, RBMs have tractable infer-ence procedures for both forward and backward rea-soning, which allows us to efficiently infer both theprobability of the decision given the latent variablesand vice versa.
The key structural difference be-tween the two models is that the directed connec-tions between latent and decision vectors in SBNsbecome undirected in RBMs.
A complete parsingmodel consists of a sequence of RBMs interlinkedvia directed edges, which gives us a form of Tempo-ral Restricted Boltzmann Machines (TRBM) (Tay-lor et al, 2007), but with the incrementally speci-fied model structure required by parsing.
In this pa-per, we analyze and contrast ISBNs with TRBMsand show that the latter provide an accurate andtheoretically sound model for parsing with high-dimensional latent variables.2 An ISBN Parsing ModelOur TRBM parser uses the same history-based probability model as the ISBNparser of Titov and Henderson (2007b):P (tree) = ?tP (vt|v1, ..., vt?1), where each11Figure 1: An ISBN network.
Shaded nodes representdecision variables and ?H?
represents a vector of latentvariables.
W (c)HH denotes the weight matrix for directedconnection of type c between two latent vectors.vt is a parser decision of the type Left-Arc,Right-Arc, Reduce or Shift.
These decisions are fur-ther decomposed into sub-decisions, as for exampleP (Left-Arc|v1, ..., vt?1)P (Label|Left-Arc, v1, ..., vt?1).The TRBMs and ISBNs model these probabilities.In the ISBN model shown in Figure 1, the de-cisions are shown as boxes and the sub-decisionsas shaded circles.
At each decision step, the ISBNmodel also includes a vector of latent variables, de-noted by ?H?, which act as latent features of theparse history.
As explained in (Titov and Hender-son, 2007b), the temporal connections between la-tent variables are constructed to take into account thestructural locality in the partial dependency struc-ture.
The model parameters are learned by back-propagating likelihood gradients.Because decision probabilities are conditioned onthe history, once a decision is made the correspond-ing variable becomes observed, or visible.
In anISBN, the directed edges to these visible variablesand the large numbers of heavily inter-connected la-tent variables make exact inference of decision prob-abilities intractable.
Titov and Henderson (2007a)proposed two approximation procedures for infer-ence.
The first was a feed forward approximationwhere latent variables were allowed to depend onlyon their parent variables, and hence did not take intoaccount the current or future observations.
Due tothis limitation, the authors proposed to make latentvariables conditionally dependent also on a set ofexplicit features derived from the parsing history,specifically, the base features defined in (Nivre et al,2006b).
As shown in our experiments, this additionresults in a big improvement for the parsing task.The second approximate inference procedure,called the incremental mean field approximation, ex-tended the feed-forward approximation by updatingthe current time step?s latent variables after eachsub-decision.
Although this approximation is moreaccurate than the feed-forward one, there is no ana-lytical way to maximize likelihood w.r.t.
the meansof the latent variables, which requires an iterativenumerical method and thus makes inference veryslow, restricting the model to only shorter sentences.3 Temporal Restricted BoltzmannMachinesIn the proposed TRBM model, RBMs provide an an-alytical way to do exact inference within each timestep.
Although information passing between timesteps is still approximated, TRBM inference is moreaccurate than the ISBN approximations.3.1 Restricted Boltzmann Machines (RBM)An RBM is an undirected graphical model with aset of binary visible variables v, a set of binary la-tent variables h, and a weight matrix W for bipar-tite connections between v and h. The probabilityof an RBM configuration is given by: p(v,h) =(1/Z)e?E(v,h) where Z is the partition function andE is the energy function defined as:E(v,h) = ?
?iaivi ?
?jbjhj ?
?i,jvihjwijwhere ai and bj are biases for corresponding visi-ble and latent variables respectively, and wij is thesymmetric weight between vi and hj .
Given the vis-ible variables, the latent variables are conditionallyindependent of each other, and vice versa:p(hj = 1|v) = ?
(bj +?iviwij) (1)p(vi = 1|h) = ?
(ai +?jhjwij) (2)where ?
(x) = 1/(1 + e?x) (the logistic sigmoid).RBM based models have been successfully usedin image and video processing, such as Deep BeliefNetworks (DBNs) for recognition of hand-writtendigits (Hinton et al, 2006) and TRBMs for mod-eling motion capture data (Taylor et al, 2007).
De-spite their success, RBMs have seen limited use inthe NLP community.
Previous work includes RBMsfor topic modeling in text documents (Salakhutdinovand Hinton, 2009), and Temporal Factored RBM forlanguage modeling (Mnih and Hinton, 2007).3.2 Proposed TRBM Model StructureTRBMs (Taylor et al, 2007) can be used to modelsequences where the decision at each step requiressome context information from the past.
Figure 212Figure 2: Proposed TRBM Model.
Edges with no arrowsrepresent undirected RBM connections.
The directedtemporal connections between time steps contribute abias to the latent layer inference in the current step.shows our proposed TRBM model with latent tolatent connections between time steps.
Each stephas an RBM with weights WRBM composed ofsmaller weight matrices corresponding to differentsub-decisions.
For instance, for the action Left-Arc,WRBM consists of RBM weights between the la-tent vector and the sub-decisions: ?Left-Arc?
and?Label?.
Similarly, for the action Shift, the sub-decisions are ?Shift?, ?Part-of-Speech?
and ?Word?.The probability distribution of a TRBM is:p(vT1 ,hT1 ) = ?Tt=1p(vt,ht|h(1), ...,h(C))where vT1 denotes the set of visible vectors from timesteps 1 to T i.e.
v1 to vT .
The notation for latentvectors h is similar.
h(c) denotes the latent vectorin the past time step that is connected to the currentlatent vector through a connection of type c. To sim-plify notation, we will denote the past connections{h(1), ...,h(C)} by historyt.
The conditional distri-bution of the RBM at each time step is given by:p(vt,ht|historyt) = (1/Z)exp(?iaivti +?i,jvtihtjwij+?j(bj +?c,lw(c)HHljh(c)l )htj)where vti and htj denote the ith visible and jth latentvariable respectively at time step t. h(c)l denotes alatent variable in the past time step, and w(c)HHlj de-notes the weight of the corresponding connection.3.3 TRBM Likelihood and InferenceSection 3.1 describes an RBM where visible vari-ables can take binary values.
In our model, similar to(Salakhutdinov et al, 2007), we have multi-valuedvisible variables which we represent as one-hot bi-nary vectors and model via a softmax distribution:p(vtk = 1|ht) =exp(ak +?j htjwkj)?i exp(ai +?j htjwij)(3)Latent variable inference is similar to equation 1with an additional bias due to the temporal connec-tions.
?tj = p(htj = 1|vt, historyt)= ??
(bj +?c,lw(c)HHljh(c)l +?ivtiwij)??
?
(b?j +?ivtiwij), (4)b?j = bj +?c,lw(c)HHlj?
(c)l .Here, ?
denotes the mean of the corresponding la-tent variable.
To keep inference tractable, we do notdo any backward reasoning across directed connec-tions to update ?(c).
Thus, the inference procedurefor latent variables takes into account both the parsehistory and the current observation, but no future ob-servations.The limited set of possible values for the visi-ble layer makes it possible to marginalize out latentvariables in linear time to compute the exact likeli-hood.
Let vt(k) denote a vector with vtk = 1 andvti(i 6=k) = 0.
The conditional probability of a sub-decision is:p(vt(k)|historyt) = (1/Z)?hte?E(vt(k),ht) (5)= (1/Z)eak?j(1 + eb?j+wkj),where Z = ?i?visibleeai?j?latent(1 + eb?j+wij ).We actually perform this calculation once foreach sub-decision, ignoring the future sub-decisionsin that time step.
This is a slight approximation,but avoids having to compute the partition functionover all possible combinations of values for all sub-decisions.1The complete probability of a derivation is:p(vT1 ) = p(v1).p(v2|history2)...p(vT |historyT )3.4 TRBM TrainingThe gradient of an RBM is given by:?
log p(v)/?wij = ?vihj?data ?
?vihj?model (6)where ?
?d denotes the expectation under distribu-tion d. In general, computing the exact gradientis intractable and previous work proposed a Con-trastive Divergence (CD) based learning procedurethat approximates the above gradient using only onestep reconstruction (Hinton, 2002).
Fortunately, ourmodel has only a limited set of possible visible val-ues, which allows us to use a better approximationby taking the derivative of equation 5:1In cases where computing the partition function is still notfeasible (for instance, because of a large vocabulary), samplingmethods could be used.
However, we did not find this to benecessary.13?
log p(vt(k)|historyt)?wij=(?ki ?
p(vt(i)|historyt)) ?
(b?j + wij)(7)Further, the weights on the temporal connectionsare learned by back-propagating the likelihood gra-dients through the directed links between steps.The back-proped gradient from future time steps isalso used to train the current RBM weights.
Thisback-propagation is similar to the Recurrent TRBMmodel of Sutskever et al (2008).
However, unliketheir model, we do not use CD at each step to com-pute gradients.3.5 PredictionWe use the same beam-search decoding strategy asused in (Titov and Henderson, 2007b).
Given aderivation prefix, its partial parse tree and associ-ated TRBM, the decoder adds a step to the TRBMfor calculating the probabilities of hypothesized nextdecisions using equation 5.
If the decoder selects adecision for addition to the candidate list, then thecurrent step?s latent variable means are inferred us-ing equation 4, given that the chosen decision is nowvisible.
These means are then stored with the newcandidate for use in subsequent TRBM calculations.4 Experiments & ResultsWe used syntactic dependencies from the Englishsection of the CoNLL 2009 shared task dataset(Hajic?
et al, 2009).
Standard splits of training, de-velopment and test sets were used.
To handle wordsparsity, we replaced all the (POS, word) pairs withfrequency less than 20 in the training set with (POS,UNKNOWN), giving us only 4530 tag-word pairs.Since our model can work only with projective trees,we used MaltParser (Nivre et al, 2006a) to projec-tivize/deprojectivize the training input/test output.4.1 ResultsTable 1 lists the labeled (LAS) and unlabeled (UAS)attachment scores.
Row a shows that a simple ISBNmodel without features, using feed forward infer-ence procedure, does not work well.
As explainedin section 2, this is expected since in the absence ofexplicit features, the latent variables in a given layerdo not take into account the observations in the pre-vious layers.
The huge improvement in performanceModel LAS UASa.
ISBN w/o features 38.38 54.52b.
ISBN w/ features 88.65 91.44c.
TRBM w/o features 86.01 89.78d.
TRBM w/ features 88.72 91.65e.
MST (McDonald et al, 2005) 87.07 89.95f .
Malt?
?AE (Hall et al, 2007) 85.96 88.64g.
MSTMalt (Nivre and McDonald, 2008) 87.45 90.22h.
CoNLL 2008 #1 (Johansson and Nugues, 2008) 90.13 92.45i.
ensemble3100% (Surdeanu and Manning, 2010) 88.83 91.47j.
CoNLL 2009 #1 (Bohnet, 2009) 89.88 unknownTable 1: LAS and UAS for different models.on adding the features (row b) shows that the feedforward inference procedure for ISBNs relies heav-ily on these feature connections to compensate forthe lack of backward inference.The TRBM model avoids this problem as the in-ference procedure takes into account the current ob-servation, which makes the latent variables muchmore informed.
However, as row c shows, theTRBM model without features falls a bit short ofthe ISBN performance, indicating that features areindeed a powerful substitute for backward inferencein sequential latent variable models.
TRBM mod-els would still be preferred in cases where such fea-ture engineering is difficult or expensive, or wherethe objective is to compute the latent features them-selves.
For a fair comparison, we add the same setof features to the TRBM model (row d) and the per-formance improves by about 2% to reach the samelevel (non-significantly better) as ISBN with fea-tures.
The improved inference in TRBM does how-ever come at the cost of increased training and test-ing time.
Keeping the same likelihood convergencecriteria, we could train the ISBN in about 2 days andTRBM in about 5 days on a 3.3 GHz Xeon proces-sor.
With the same beam search parameters, the testtime was about 1.5 hours for ISBN and about 4.5hours for TRBM.
Although more code optimizationis possible, this trend is likely to remain.We also tried a Contrastive Divergence basedtraining procedure for TRBM instead of equation7, but that resulted in about an absolute 10% lowerLAS.
Further, we also tried a very simple modelwithout latent variables where temporal connectionsare between decision variables themselves.
This14model gave an LAS of only 60.46%, which indi-cates that without latent variables, it is very difficultto capture the parse history.For comparison, we also include the performancenumbers for some state-of-the-art dependency pars-ing systems.
Surdeanu and Manning (2010) com-pare different parsing models using CoNLL 2008shared task dataset (Surdeanu et al, 2008), whichis the same as our dataset.
Rows e?
i show the per-formance numbers of some systems as mentioned intheir paper.
Row j shows the best syntactic modelin CoNLL 2009 shared task.
The TRBM model hasonly 1.4% lower LAS and 0.8% lower UAS com-pared to the best performing model.4.2 Latent Layer AnalysisWe analyzed the latent layers in our models to see ifthey captured semantic patterns.
A latent layer is avector of 100 latent variables.
Every Shift operationgives a latent representation for the correspondingword.
We took all the verbs in the development set2and partitioned their representations into 50 clus-ters using the k-means algorithm.
Table 2 showssome partitions for the TRBM model.
The partitionslook semantically meaningful but to get a quantita-tive analysis, we computed pairwise semantic simi-larity between all word pairs in a given cluster andaggregated this number over all the clusters.
The se-mantic similarity was calculated using two differentsimilarity measures on the wordnet corpus (Milleret al, 1990): path and lin.
path similarity is a scorebetween 0 and 1, equal to the inverse of the shortestpath length between the two word senses.
lin simi-larity (Lin, 1998) is a score between 0 and 1 basedon the Information Content of the two word sensesand of the Least Common Subsumer.
Table 3 showsthe similarity scores.3 We observe that TRBM la-tent representations give a slightly better clusteringthan ISBN models.
Again, this is because of the factthat the inference procedure in TRBMs takes into ac-count the current observation.
However, at the sametime, the similarity numbers for ISBN with features2Verbs are words corresponding to POS tags: VB, VBD,VBG, VBN, VBP, VBZ.
We selected verbs as they have goodcoverage in Wordnet.3To account for randomness in k-means clustering, the clus-tering was performed 10 times with random initializations, sim-ilarity scores were computed for each run and a mean was taken.Cluster 1 Cluster 2 Cluster 3 Cluster 4says needed pressing renewingcontends expected bridging causeadds encouraged curing repeatinsists allowed skirting brokenremarked thought tightening extendedTable 2: K-means clustering of words according to theirTRBM latent representations.
Duplicate words in thesame cluster are not shown.Model path linISBN w/o features 0.228 0.381ISBN w/features 0.366 0.466TRBM w/o features 0.386 0.487TRBM w/ features 0.390 0.489Table 3: Wordnet similarity scores for clusters given bydifferent models.are not very low, which shows that features are apowerful way to compensate for the lack of back-ward inference.
This is in agreement with their goodperformance on the parsing task.5 Conclusions & Future WorkWe have presented a Temporal Restricted Boltz-mann Machines based model for dependency pars-ing.
The model shows how undirected graphicalmodels can be used to generate latent representa-tions of local parsing actions, which can then beused as features for later decisions.The TRBM model for dependency parsing couldbe extended to a Deep Belief Network by addingone more latent layer on top of the existing one(Hinton et al, 2006).
Furthermore, as done forunlabeled images (Hinton et al, 2006), one couldlearn high-dimensional features from unlabeled text,which could then be used to aid parsing.
Parser la-tent representations could also help other tasks suchas Semantic Role Labeling (Henderson et al, 2008).A free distribution of our implementation is avail-able at http://cui.unige.ch/?garg.AcknowledgmentsThis work was partly funded by Swiss NSF grant200021 125137 and European Community FP7grant 216594 (CLASSiC, www.classic-project.org).15ReferencesY.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.
2003.A neural probabilistic language model.
The Journal ofMachine Learning Research, 3:1137?1155.B.
Bohnet.
2009.
Efficient parsing of syntactic andsemantic dependency structures.
In Proceedings ofthe Thirteenth Conference on Computational NaturalLanguage Learning: Shared Task, CoNLL ?09, pages67?72.
Association for Computational Linguistics.R.
Collobert and J. Weston.
2008.
A unified architecturefor natural language processing: Deep neural networkswith multitask learning.
In Proceedings of the 25thinternational conference on Machine learning, pages160?167.
ACM.J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,J.
?Ste?pa?nek, et al 2009.
The CoNLL-2009 sharedtask: Syntactic and semantic dependencies in multiplelanguages.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 1?18.
Association for Computa-tional Linguistics.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,M.
Nilsson, and M. Saers.
2007.
Single malt orblended?
A study in multilingual parser optimiza-tion.
In Proceedings of the CoNLL Shared Task Ses-sion of EMNLP-CoNLL 2007, pages 933?939.
Associ-ation for Computational Linguistics.J.
Henderson, P. Merlo, G. Musillo, and I. Titov.
2008.A latent variable model of synchronous parsing forsyntactic and semantic dependencies.
In Proceedingsof the Twelfth Conference on Computational NaturalLanguage Learning, pages 178?182.
Association forComputational Linguistics.G.E.
Hinton, S. Osindero, and Y.W.
Teh.
2006.
A fastlearning algorithm for deep belief nets.
Neural com-putation, 18(7):1527?1554.G.E.
Hinton.
2002.
Training products of experts by min-imizing contrastive divergence.
Neural Computation,14(8):1771?1800.R.
Johansson and P. Nugues.
2008.
Dependency-based syntactic-semantic analysis with PropBank andNomBank.
In Proceedings of the Twelfth Conferenceon Computational Natural Language Learning, pages183?187.
Association for Computational Linguistics.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, volume 1, pages296?304.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, pages 523?530.
As-sociation for Computational Linguistics.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.J.
Miller.
1990.
Introduction to wordnet: An on-line lexical database.
International Journal of lexicog-raphy, 3(4):235.A.
Mnih and G. Hinton.
2007.
Three new graphical mod-els for statistical language modelling.
In Proceedingsof the 24th international conference on Machine learn-ing, pages 641?648.
ACM.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
Pro-ceedings of ACL-08: HLT, pages 950?958.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of CoNLL, pages49?56.J.
Nivre, J.
Hall, and J. Nilsson.
2006a.
MaltParser: Adata-driven parser-generator for dependency parsing.In Proceedings of LREC, volume 6.J.
Nivre, J.
Hall, J. Nilsson, G. Eryiit, and S. Marinov.2006b.
Labeled pseudo-projective dependency pars-ing with support vector machines.
In Proceedingsof the Tenth Conference on Computational NaturalLanguage Learning, pages 221?225.
Association forComputational Linguistics.A.
Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34(1):151?175.R.
Salakhutdinov and G. Hinton.
2009.
Replicated soft-max: an undirected topic model.
Advances in NeuralInformation Processing Systems, 22.R.
Salakhutdinov, A. Mnih, and G. Hinton.
2007.
Re-stricted Boltzmann machines for collaborative filter-ing.
In Proceedings of the 24th international confer-ence on Machine learning, page 798.
ACM.M.
Surdeanu and C.D.
Manning.
2010.
Ensemble mod-els for dependency parsing: cheap and good?
In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 649?652.Association for Computational Linguistics.M.
Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task onjoint parsing of syntactic and semantic dependencies.In Proceedings of the Twelfth Conference on Compu-tational Natural Language Learning, pages 159?177.Association for Computational Linguistics.I.
Sutskever, G. Hinton, and G. Taylor.
2008.
The recur-rent temporal restricted boltzmann machine.
In NIPS,volume 21, page 2008.G.W.
Taylor, G.E.
Hinton, and S.T.
Roweis.
2007.Modeling human motion using binary latent variables.Advances in neural information processing systems,19:1345.16I.
Titov and J. Henderson.
2007a.
Constituent parsingwith incremental sigmoid belief networks.
In Pro-ceedings of the 45th Annual Meeting on Associationfor Computational Linguistics, volume 45, page 632.I.
Titov and J. Henderson.
2007b.
Fast and robust mul-tilingual dependency parsing with a generative latentvariable model.
In Proceedings of the CoNLL SharedTask Session of EMNLP-CoNLL, pages 947?951.17
