Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 225?228,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPA Beam-Search Extraction Algorithm for Comparable DataChristoph TillmannIBM T.J. Watson Research CenterYorktown Heights, N.Y. 10598ctill@us.ibm.comAbstractThis paper extends previous work on ex-tracting parallel sentence pairs from com-parable data (Munteanu and Marcu, 2005).For a given source sentence S, a max-imum entropy (ME) classifier is appliedto a large set of candidate target transla-tions .
A beam-search algorithm is usedto abandon target sentences as non-parallelearly on during classification if they falloutside the beam.
This way, our novelalgorithm avoids any document-level pre-filtering step.
The algorithm increases thenumber of extracted parallel sentence pairssignificantly, which leads to a BLEU im-provement of about 1 % on our Spanish-English data.1 IntroductionThe paper presents a novel algorithm for ex-tracting parallel sentence pairs from comparablemonolingual news data.
We select source-targetsentence pairs (S, T ) based on a ME classifier(Munteanu and Marcu, 2005).
Because the set oftarget sentences T considered can be huge, pre-vious work (Fung and Cheung, 2004; Resnik andSmith, 2003; Snover et al, 2008; Munteanu andMarcu, 2005) pre-selects target sentences T at thedocument level .
We have re-implemented a par-ticular filtering scheme based on BM25 (Quirk etal., 2007; Utiyama and Isahara, 2003; Robertsonet al, 1995).
In this paper, we demonstrate a dif-ferent strategy .
We compute the ME score in-crementally at the word level and apply a beam-search algorithm to a large number of sentences.We abandon target sentences early on during clas-sification if they fall outside the beam.
For com-parison purposes, we run our novel extraction al-gorithm with and without the document-level pre-filtering step.
The results in Section 4 show thatthe number of extracted sentence pairs is morethan doubled which also leads to an increase inBLEU by about 1 % on the Spanish-English data.The classification probability is defined as fol-lows:p(c|S, T ) =exp( wT?
f(c, S, T ) )Z(S, T ), (1)where S = sJ1is a source sentence of length J andT = tI1is a target sentence of length I .
c ?
{0, 1}is a binary variable .
p(c|S, T ) ?
[0, 1] is a proba-bility where a value p(c = 1|S, T ) close to 1.0 in-dicates that S and T are translations of each other.w ?
Rn is a weight vector obtained during train-ing.
f(c, S, T ) is a feature vector where the fea-tures are co-indexed with respect to the alignmentvariable c. Finally, Z(S, T ) is an appropriatelychosen normalization constant.Section 2 summarizes the use of the binary clas-sifier.
Section 3 presents the beam-search algo-rithm.
In Section 4, we show experimental results.Finally, Section 5 discusses the novel algorithm.2 Classifier TrainingThe classifier in Eq.
1 is based on several real-valued feature functions fi.
Their computationis based on the so-called IBM Model-1 (Brown etal., 1993).
The Model-1 is trained on some paral-lel data available for a language pair, i.e.
the dataused to train the baseline systems in Section 4.p(s|T ) is the Model-1 probability assigned to asource word s given the target sentence T , p(t|S)is defined accordingly.
p(s|t) and p(t|s) are wordtranslation probabilities obtained by two parallelModel-1 training steps on the same data, but swap-ping the role of source and target language.
Tocompute these values efficiently, the implementa-tion techniques in (Tillmann and Xu, 2009) areused.
Coverage and fertility features are definedbased on the Model-1 Viterbi alignment: a source225word s is said to be covered if there is a targetword t ?
T such that its probability is above athreshold ?
: p(s|t) > ?
.
We define the fertilityof a source word s as the number of target wordst ?
T for which p(s|t) > ?.
Target word cover-age and fertility are defined accordingly.
A largenumber of ?uncovered?
source and target positionsas well as a large number of high fertility wordsindicate non-parallelism.
We use the followingN = 7 features: 1,2) lexical Model-1 weight-ing:?s?log( p(s|T ) ) and?t?log( p(t|S) ),3,4) number of uncovered source and target po-sitions, 5,6) sum of source and target fertilities,7) number of covered source and target positions.
These features are defined in a way that theycan be computed incrementally at the word level.Some thresholding is applied, e.g.
a sequence ofuncovered positions has to be at least 3 positionslong to generate a non-zero feature value .
In thefeature vector f(c, S, T ), each feature fioccurspotentially twice, once for each class c ?
{0, 1}.For the feature vector f(c = 1, S, T ), all the fea-ture values corresponding to class c = 0 are setto 0, and vice versa.
This particular way of defin-ing the feature vector is needed for the search inSection 3: the contribution of the ?negative?
fea-tures for c = 0 is only computed when Eq.
1 isevaluated for the highest scoring final hypothesisin the beam.
To train the classifier, we have manu-ally annotated a collection of 524 sentence pairs .A sentence pair is considered parallel if at least75 % of source and target words have a corre-sponding translation in the other sentence, other-wise it is labeled as non-parallel.
A weight vectorw ?
R2?N is trained with respect to classificationaccuracy using the on-line maxent training algo-rithm in (Tillmann and Zhang, 2007).3 Beam Search AlgorithmWe process the comparable data at the sentencelevel: sentences are indexed based on their publi-cation date.
For each source sentence S, a match-ing score is computed over all the target sentencesTm?
?
that have a publication date which differsless than 7 days from the publication date of thesource sentence 1.
We are aiming at finding the ?Twith the highest probability p(c = 1|S, ?T ), but wecannot compute that probability for all sentence1In addition, the sentence length filter in (Munteanu andMarcu, 2005) is used: the length ratio max(J, I)/min(J, I)of source and target sentence has to be smaller than 2.pairs (S, Tm) since |?| can be in tens of thousandsof sentences .
Instead, we use a beam-search algo-rithm to search for the sentence pair (S, ?T ) withthe highest matching score wT ?
f(1, S, ?T ) 2.
The?light-weight?
features defined in Section 2 aresuch that the matching score can be computed in-crementally while processing the source and targetsentence positions in some order.
To that end, wemaintain a stack of matching hypotheses for eachsource position j.
Each hypothesis is assigned apartial matching score based on the source and tar-get positions processed so far.
Whenever a partialmatching score is low compared to partial match-ing scores of other target sentence candidates, thattranslation pair can be discarded by carrying outa beam-search pruning step.
The search is orga-nized in a single left-to-right run over the sourcepositions 1?
j ?
J and all active partial hypothe-ses match the same portion of that source sentence.There is at most a single active hypothesis for eachdifferent target sentence Ti, and search states aredefined as follows:[ m , j , uj, ui; d ] .Here, m ?
{1, ?
?
?
, |?|} is a target sentence in-dex.
j is a position in the source sentence, ujanduiare the number of uncovered source and targetpositions to the left of source position j and tar-get position i (coverage computation is explainedabove), and d is the partial matching score .
Thetarget position i corresponding to the source posi-tion j is computed deterministically as follows:i = ?I ?jJ?
, (2)where the sentence lengths I and J are knownfor a sentence pair (S, T ).
Covering an additionalsource position leads to covering additional targetpositions as well, and source and target featuresare computed accordingly.
The search is initial-ized by adding a single hypothesis for each targetsentence Tm?
?
to the stack for j = 1:[ m , j = 1 , uj= 0 , ui= 0 ; 0 ] .During the left-to-right search , state transitions ofthe following type occur:[ m , j , uj, ui; d ] ?
[ m , j + 1 , u?j, u?i; d?]
,2This is similar to standard phrase-based SMT decoding,where a set of real-valued features is used and any sentence-level normalization is ignored during decoding.
We assumethe effect of this approximation to be small.226where the partial score is updated as: d?
= d +wT?
f(1, j, i) .
Here, f(1, j, i) is a partial fea-ture vector computed for all the additional sourceand target positions processed in the last extensionstep.
The number of uncovered source and targetpositions u?
is updated as well.
The beam-searchalgorithm is carried out until all source positions jhave been processed.
We extract the highest scor-ing partial hypothesis from the final stack j = J.
For that hypothesis, we compute a global featurevector f(1, S, T ) by adding all the local f(1, j, i)?scomponent-wise.
The ?negative?
feature vectorf(0, S, T ) is computed from f(1, S, T ) by copy-ing its feature values.
We then use Eq.
1 to com-pute the probability p(1|S, T ) and apply a thresh-old of ?
= 0.75 to extract parallel sentence pairs.We have adjusted beam-search pruning techniquestaken from regular SMT decoding (Tillmann et al,1997; Koehn, 2004) to reduce the number of hy-potheses after each extension step.
Currently, onlyhistogram pruning is employed to reduce the num-ber of hypotheses in each stack.The resulting beam-search algorithm is similarto a monotone decoder for SMT: rather then in-crementally generating a target translation, the de-coder is used to select entire target sentences out ofa pre-defined list.
That way, our beam search algo-rithm is similar to algorithms in large-scale speechrecognition (Ney, 1984; Vintsyuk, 1971), wherean acoustic signal is matched to a pre-assigned listof words in the recognizer vocabulary.4 ExperimentsThe parallel sentence extraction algorithm pre-sented in this paper is tested in detail on all of thelarge-scale Spanish-English Gigaword data (Graff,2006; Graff, 2007) as well as on some smallerPortuguese-English news data .
For the Spanish-English data , matching sentence pairs come fromthe same news feed.
Table 1 shows the size ofthe comparable data, and Table 2 shows the ef-fect of including the additional sentence pairs intothe training of a phrase-based SMT system.
Here,both languages use a test set with a single ref-erence.
The test data comes from Spanish andPortuguese news web pages that have been trans-lated into English.
Including about 1.35 millionsentence pairs extracted from the Gigaword data,we obtain a statistically significant improvementfrom 42.3 to 45.7 in BLEU.
The baseline systemhas been trained on about 1.8 million sentenceTable 1: Corpus statistics for comparable data.Spanish EnglishSentences 19.4 million 47.9 millionWords 601.5 million 1.36 billionPortuguese EnglishSentences 366.0 thousand 5.3 millionWords 11.6 million 171.1 millionpairs from Europarl and FBIS parallel data.
Wealso present results for a Portuguese-English sys-tem: the baseline has been trained on Europarl andJRC data.
Parallel sentence pairs are extractedfrom comparable news data published in 2006.For this data, no document-level information wasavailable.
To gauge the effect of the document-level pre-filtering step, we have re-implementedan IR technique based on BM25 (Robertson et al,1995).
This type of pre-filtering has also been usedin (Quirk et al, 2007; Utiyama and Isahara, 2003).We split the Spanish data into documents.
EachSpanish document is translated into a bag of En-glish words using Model-1 lexicon probabilitiestrained on the baseline data.
Each of these Englishbag-of-words is then issued as a query against allthe English documents that have been publishedwithin a 7 day window of the source document.We select the 20 highest scoring English docu-ments for each source document .
These 20 docu-ments provide a restricted set of target sentencecandidates.
The sentence-level beam-search al-gorithm without the document-level filtering stepsearches through close to 1 trillion sentence pairs.For the data obtained by the BM25-based filteringstep, we still use the same beam-search algorithmbut on a much smaller candidate set of only 25.4billion sentence pairs.
The probability selectionthreshold ?
is determined on some developmentset in terms of precision and recall (based on thedefinitions in (Munteanu and Marcu, 2005)).
Theclassifier obtains an F-measure classifications per-formance of about 85 %.
The BM25 filtering stepleads to a significantly more complex processingpipeline since sentences have to be indexed withrespect to document boundaries and publicationdate.
The document-level pre-filtering reduces theoverall processing time by about 40 % (from 4 to2.5 days on a 100-CPU cluster).
However, the ex-haustive sentence-level search improves the BLEUscore by about 1 % on the Spanish-English data.227Table 2: Spanish-English and Portuguese-Englishextraction results.
Extraction threshold is ?
=0.75 for both language pairs.
# cands reports thesize of the overall search space in terms of sen-tence pairs processed .Data Source # cands # pairs BleuBaseline - 1.826 M 42.3+ Giga 999.3 B 1.357 M 45.7+ Giga (BM25) 25.4 B 0.609 M 44.8Baseline - 2.222 M 45.3+ News Data 2006 77.8 B 56 K 47.25 Future Work and DiscussionIn this paper, we have presented a novel beam-search algorithm to extract sentence pairs fromcomparable data .
It can avoid any pre-filteringat the document level (Resnik and Smith, 2003;Snover et al, 2008; Utiyama and Isahara, 2003;Munteanu and Marcu, 2005; Fung and Cheung,2004).
The novel algorithm is successfully eval-uated on news data for two language pairs.
Arelated approach that also avoids any document-level pre-filtering has been presented in (Tillmannand Xu, 2009).
The efficient implementation tech-niques in that paper are extended for the ME clas-sifier and beam search algorithm in the current pa-per, i.e.
feature function values are cached alongwith Model-1 probabilities.The search-driven extraction algorithm presentedin this paper might also be applicable to otherNLP extraction task, e.g.
named entity extraction.Rather then employing a cascade of filtering steps,a one-stage search with a specially adopted featureset and search space organization might be carriedout .
Such a search-driven approach makes lessassumptions about the data and may increase thenumber of extracted entities, i.e.
increase recall.AcknowledgmentsWe would like to thanks the anonymous reviewersfor their valuable remarks.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
CL, 19(2):263?311.Pascale Fung and Percy Cheung.
2004.
Mining Very-Non-Parallel Corpora: Parallel Sentence and Lexi-con Extraction via Bootstrapping and EM.
In Proc,of EMNLP 2004, pages 57?63, Barcelona, Spain,July.Dave Graff.
2006.
LDC2006T12: Spanish GigawordCorpus First Edition.
LDC.Dave Graff.
2007.
LDC2007T07: English GigawordCorpus Third Edition.
LDC.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proceedings of AMTA?04, Washing-ton DC, September-October.Dragos S. Munteanu and Daniel Marcu.
2005.
Im-proving Machine Translation Performance by Ex-ploiting Non-Parallel Corpora.
CL, 31(4):477?504.H.
Ney.
1984.
The Use of a One-stage Dynamic Pro-gramming Algorithm for Connected Word Recogni-tion.
IEEE Transactions on Acoustics, Speech, andSignal Processing, 32(2):263?271.Chris Quirk, Raghavendra Udupa, and Arul Menezes.2007.
Generative Models of Noisy Translationswith Applications to Parallel Fragment Extraction.In Proc.
of the MT Summit XI, pages 321?327,Copenhagen,Demark, September.Philip Resnik and Noah Smith.
2003.
The Web asParallel Corpus.
CL, 29(3):349?380.S E Robertson, S Walker, M M Beaulieu, and M Gat-ford.
1995.
Okapi at TREC-4.
In Proc.
of the 4thText Retrieval Conference (TREC-4), pages 73?96.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and Translation Model Adaptationusing Comparable Corpora.
In Proc.
of EMNLP08,pages 856?865, Honolulu, Hawaii, October.Christoph Tillmann and Jian-Ming Xu.
2009.
A Sim-ple Sentence-Level Extraction Algorithm for Com-parable Data.
In Companion Vol.
of NAACL HLT09, pages 93?96, Boulder, Colorado, June.Christoph Tillmann and Tong Zhang.
2007.
A BlockBigram Prediction Model for Statistical MachineTranslation.
ACM-TSLP, 4(6):1?31, July.Christoph Tillmann, Stefan Vogel, Hermann Ney, andAlex Zubiaga.
1997.
A DP-based Search UsingMonotone Alignments in Statistical Translation.
InProc.
of ACL 97, pages 289?296, Madrid,Spain,July.Masao Utiyama and Hitoshi Isahara.
2003.
ReliableMeasures for Aligning Japanese-English News Arti-cles and Sentences.
In Proc.
of ACL03, pages 72?79,Sapporo, Japan, July.T.K.
Vintsyuk.
1971.
Element-Wise Recognition ofContinuous Speech Consisting of Words From aSpecified Vocabulary.
Cybernetics (Kibernetica),(2):133?143, March-April.228
