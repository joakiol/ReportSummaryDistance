Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 42?49,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsPredicting Concept Types in User Corrections in DialogSvetlana Stoyanchev and Amanda StentDepartment of Computer ScienceStony Brook UniversityStony Brook, NY 11794-4400, USAsvetlana.stoyanchev@gmail.com, amanda.stent@stonybrook.eduAbstractMost dialog systems explicitly confirmuser-provided task-relevant concepts.User responses to these system confirma-tions (e.g.
corrections, topic changes) maybe misrecognized because they containunrequested task-related concepts.
In thispaper, we propose a concept-specific lan-guage model adaptation strategy wherethe language model (LM) is adapted tothe concept type(s) actually present inthe user?s post-confirmation utterance.We evaluate concept type classificationand LM adaptation for post-confirmationutterances in the Let?s Go!
dialog system.We achieve 93% accuracy on concept typeclassification using acoustic, lexical anddialog history features.
We also show thatthe use of concept type classification forLM adaptation can lead to improvementsin speech recognition performance.1 IntroductionIn most dialog systems, the system explicitly con-firms user-provided task-relevant concepts.
Theuser?s response to a confirmation prompt such as?leaving from Waterfront??
may consist of a sim-ple confirmation (e.g.
?yes?
), a simple rejection(e.g.
?no?
), a correction (e.g.
?no, Oakland?)
or atopic change (e.g.
?no, leave at 7?
or ?yes, and goto Oakland?).
Each type of utterance has implica-tions for further processing.
In particular, correc-tions and topic changes are likely to contain un-requested task-relevant concepts that are not wellrepresented in the recognizer?s post-confirmationlanguage model (LM)1.
This means that they are1The word error rate on post-confirmation Let?s Go!
utter-ances containing a concept is 10% higher than on utteranceslikely to be misrecognized, frustrating the user andleading to cascading errors.
Correct determina-tion of the content of post-confirmation utterancescan lead to improved speech recognition, fewerand shorter sequences of speech recognition er-rors, and improved dialog system performance.In this paper, we look at user responses to sys-tem confirmation prompts CMU?s deployed Let?sGo!
dialog system.
We adopt a two-pass recogni-tion architecture (Young, 1994).
In the first pass,the input utterance is processed using a general-purpose LM (e.g.
specific to the domain, or spe-cific to the dialog state).
Recognition may failon concept words such as ?Oakland?
or ?61C?
,but is likely to succeed on closed-class words (e.g.
?yes?, ?no?, ?and?, ?but?, ?leaving?).
If the ut-terance follows a system confirmation prompt, wethen use acoustic, lexical and dialog history fea-tures to determine the task-related concept type(s)likely to be present in the utterance.
In the secondrecognition pass, any utterance containing a con-cept type is re-processed using a concept-specificLM.
We show that: (1) it is possible to achievehigh accuracy in determining presence or absenceof particular concept types in a post-confirmationutterance; and (2) 2-pass speech recognition withconcept type classification and language modeladaptation can lead to improved speech recogni-tion performance for post-confirmation utterances.The rest of this paper is structured as follows: InSection 2 we discuss related work.
In Section 3 wedescribe our data.
In Section 4 we present our con-cept type classification experiment.
In Section 5we present our LM adaptation experiment.
In Sec-tion 6 we conclude and discuss future work.without a concept.422 Related WorkWhen a dialog system requests a confirmation,the user?s subsequent corrections and topic changeutterances are particularly likely to be misrecog-nized.
Considerable research has now been doneon the automatic detection of spoken corrections.Linguistic cues to corrections include the num-ber of words in the post-confirmation utteranceand the use of marked word order (Krahmer etal., 2001).
Prosodic cues include F0 max, RMSmax, RMS mean, duration, speech tempo, andpercentage of silent frames(Litman et al, 2006;Hirschberg et al, 2004; Levow, 1998).
Discoursecues include the removal, repetition, addition ormodification of a concept, the system?s dialog acttype, and information about error rates in the dia-log so far (Krahmer et al, 2001; et al, 2002; Lit-man et al, 2006; Walker et al, 2000).
In our ex-periments, we use most of these features as well asadditional lexical features.We can use knowledge of the type or contentof a user utterance to modify system behavior.For example, in this paper we use the concepttype(s) in the user?s utterance to adapt the recog-nizer?s LM.
It is now common practice to adaptthe recognizer to the type, context or style of in-put speech (Bellegarda, 2004).
LM adaptationhas been used to improve automatic speech recog-nition performance in automated meeting tran-scription (Tur and Stolcke, 2007), speech-drivenquestion answering (Stenchikova et al, 2008),broadcast news recognition (Gildea and Hofmann,1999), and spoken dialog systems (Tur, 2005).LMs in dialog systems can be adapted to the di-alog state (e.g.
(Riccardi and Gorin, 2000; Esteveet al, 2001)), the topic (Iyer and Ostendorf, 1999;Gildea and Hofmann, 1999), or the speaker (Tur,2007).3 DataIn this experiment we use annotated dialog tran-scripts and speech from the Let?s Go!
sys-tem, a telephone-based spoken dialog system thatprovides information about bus routes in Pitts-burgh (Raux et al, 2005).
The data we used comesfrom the first two months of Let?s Go!
systemoperation in 2005 (2411 dialogs), and one monthin 2006 (1430 dialogs).
This data has been tran-scribed, and annotated by hand for concept types.In order to provide the user with route in-formation, Let?s Go!
must elicit a departure1 Sys Welcome to the CMU Let?s Go bus informa-tion system.
What can I do for you?2 User I need to go from Oakland:pASR I need to go .from.
can?t3 Sys Where do you wanna leave from?4 User to Waterfront:pASR told.
.me.
Waterfront5 Sys Leaving from WATERFRONT .
Did I getthat right?6 User OAKLAND:pASR OAKLAND7 Sys Leaving from OAKLAND .
Did I get thatright?Figure 1: Dialog extract from Let?s Go!
data.
Userutterances are annotated with concept types (e.g.
:p for place)location, a destination, a departure time, andoptionally a bus route number.
Each conceptvalue provided by the user is explicitly con-firmed by the system (see Figure 1).
In theannotated transcripts, the following concepts arelabeled: neighborhood, place, time,hour, minute, time-of-day, and bus.For our experiments we collapsed these conceptsinto three concept types: time , place and bus.Let?s Go!
has five dialog states correspondingto the type of user utterance it expects: first-query,next-query, yes-no, place and time.
Its speechrecognizer uses dialog state-specific n-gram LMstrained on user utterances from the 2005 data.We focus on user utterances in response to sys-tem confirmation prompts (the yes-no state).
Ta-ble 1 shows statistics about yes-no state utterancesin Let?s Go!.
Table 2 shows a confusion matrixfor confirmation prompt concept type and post-confirmation utterance concept type.
This tableindicates the potential for misrecognition of post-confirmation utterances.
For example, in the 2006dataset after a system confirmation prompt for abus, a bus concept is used in only 64% of concept-containing user utterances.In our experiments, we used the 2006 data totrain concept type classifiers and for testing.
Weused the 2005 data to build LMs for our speechrecognition experiment.4 Concept Classification4.1 MethodOur goal is to classify each post-confirmation userutterance by the concept type(s) it contains (place,time, bus or none) for later language-model adap-tation (see Section 5).
From the post-confirmationuser utterances in the 2006 dataset described in43Event 2005 2006num % num %Total dialogs 2411 1430Total yes-no confirms 9098 100 9028 100Yes-no confirms witha concept2194 24 1635 18.1Dialog StateTotal confirm placeutts5548 61 5347 59.2Total confirm bus utts 1763 19.4 1589 17.6Total confirm timeutts1787 19.6 2011 22.3Concept Type FeaturesYes-no utts with place 1416 15.6 1007 11.2Yes-no utts with time 296 3.2 305 3.4Yes-no utts with bus 584 6.4 323 3.6Lexical FeaturesYes-no utts with ?yes?
4395 48.3 3693 40.9Yes-no utts with ?no?
2076 22.8 1564 17.3Yes-no utts with ?I?
203 2.2 129 1.4Yes-no utts with?from?114 1.3 185 2.1Yes-no utts with ?to?
204 2.2 237 2.6Acoustic Featuresfeature mean stdev mean stdevDuration (seconds) 1.341 1.097 1.365 1.242RMS mean .037 .033 .055 .049F0 mean 183.0 60.86 185.7 58.63F0 max 289.8 148.5 296.9 146.5Table 1: Statistics on post-confirmation utterancesplace bus time2005 datasetconfirm place 0.86 0.13 0.01confirm bus 0.18 0.81 0.01confirm time 0.07 0.01 0.922006 datasetconfirm place 0.87 0.10 0.03confirm bus 0.34 0.64 0.02confirm time 0.15 0.13 0.71Table 2: Confirmation state vs. user concept typeSection 3, we extracted the features described inSection 4.2 below.
To identify the correct concepttype(s) for each utterance, we used the human an-notations provided with the data.We performed a series of 10-fold cross-validation experiments to examine the impact ofdifferent types of feature on concept type classifi-cation.
We trained three binary classifiers for eachexperiment, one for each concept type, i.e.
we sep-arately classified each post-confirmation utteranceas place + or place -, time + or time -, and bus + orbus -.
We used Weka?s implementation of the J48decision tree classifier (Witten and Frank, 2005)2.For each experiment, we report precision (pre+)and recall (rec+) for determining presence of eachconcept type, and overall classification accuracy2J48 gave the highest classification accuracy compared toother machine learning algorithms we tried on this data.for each concept type (place, bus and time)3.
Wealso report overall pre+, rec+, f-measure (f+), andclassification accuracy across the three concepttypes.
Finally, we report the percentage of switch+errors and switch errors.
Switch+ errors are utter-ances containing bus classified as time/place, timeas bus/place, and place as bus/time; these are theerrors most likely to cause decreases in speechrecognition accuracy after language model adap-tation.
Switch errors include utterances with noconcept classified as place, bus or time.Only utterances classified as containing one ofthe three concept types are subject to second-pass recognition using a concept-specific languagemodel.
Therefore, these are the only utterances onwhich speech recognition performance may im-prove.
This means that we want to maximize rec+(proportion of utterances containing a concept thatare classified correctly).
On the other hand, utter-ances that are incorrectly classified as containing aparticular concept type will be subject to second-pass recognition using a poorly-chosen languagemodel.
This may cause speech recognition per-formance to suffer.
This means that we want tominimize switch+ errors.4.2 FeaturesWe used the features summarized in Table 3.
Allof these features are available at run-time and somay be used in a live system.
Below we give ad-ditional information about the RAW and LEX fea-tures; the other feature sets are self-explanatory.4.2.1 Acoustic and Dialog History FeaturesThe acoustic/prosodic and dialog history featuresare adapted from those identified in previous workon detecting speech recognition errors (particu-larly (Litman et al, 2006)).
We anticipated thatthese features would help us distinguish correc-tions and rejections from confirmations.4.2.2 Lexical FeaturesWe used lexical features from the user?s current ut-terance.
Words in the output of first-pass ASR arehighly indicative both of concept presence or ab-sence, and of the presence of particular concepttypes; for example, going to suggests the pres-ence of a place.
We selected the most salient lexi-3We do not report precision or recall for determining ab-sence of each concept type.
In our data set 82.2% of the ut-terances do not contain any concepts (see Table 1).
Conse-quently, precision and recall for determining absence of eachconcept type are above .9 in each of the experiments.44Feature type Feature source FeaturesSystem confirmation type(DIA)system log System?s confirmation prompt concept type (confirm time,confirm place, or confirm bus)Acoustic (RAW) raw speech F0 max; RMS max; RMS mean; Duration; Difference be-tween F0 max in first half and in second halfLexical (LEX) transcripts/ASR output Presence of specific lexical items; Number of tokens in utter-ance; [transcribed speech only] String edit distance betweencurrent and previous user utterancesDialog history (DH1, DH3) 1-3 previous utterances System?s dialog states of previous utterances(place, bus,time, confirm time, confirm place, or confirm bus); [tran-scribed speech only] Concept(s) that occurred in user?s ut-terances (YES/NO for each of the concepts place, bus, time)ASR confidence score (ASR) ASR output Speech recognizer confidence scoreConcept type match (CTM) transcripts/ASR output Presence of concept-specific lexical itemsTable 3: Features for concept type classifierscal features (unigrams and bigrams) for each con-cept type by computing the mutual information be-tween potential features and concept types (Man-ning et al, 2008).
For each lexical feature t andeach concept type class c ?
{ place +, place -,time +, time -, bus +, bus -}, we computed I:I = NtcN ?
log2N ?
NtcNt.
?
N.c+N0cN ?
log2N ?
N0cN0.
?
N.c+Nt0N ?
log2N ?
Nt0Nt.
?
N.0+N00N ?
log2N ?
N00N0.
?
N.0where Ntc= number of utterances where t co-occurs with c, N0c= number of utterances with cbut without t, Nt0= number of utterances where toccurs without c, N00= number of utterances withneither t nor c, Nt.= total number of utterancescontaining t, N.c= total number of utterances con-taining c, and N = total number of utterances.To identify the most relevant lexical features,we extracted from the data all the transcribed userutterances.
We removed all words that realize con-cepts (e.g.
?61C?, ?Squirrel Hill?
), as these arelikely to be misrecognized in a post-confirmationutterance.
We then extracted all word unigramsand bigrams.
We computed the mutual informa-tion between each potential lexical feature andconcept type.
We then selected the 30 featureswith the highest mutual information which oc-curred at least 20 times in the training data4.For transcribed speech only, we also computethe string edit distance between the current andprevious user utterances.
This gives some indica-tion of whether the current utterance is a correc-tion or topic change (vs. a confirmation).
How-4We aimed to select equal number of features for eachclass with information measure in the top 25%.
30 was anempirically derived threshold for the number of lexical fea-tures to satisfy the desired condition.ever, for recognized speech recognition errors re-duce the effectiveness of this feature (and of theconcept features in the dialog history feature set).4.3 BaselineA simple baseline for this task, No-Concept, al-ways predicts none in post-confirmation utter-ances.
This baseline achieves overall classifica-tion accuracy of 82% but rec+ of 0.
At the otherextreme, the Confirmation State baseline assignsto each utterance the dialog system?s confirmationprompt type (using the DIA feature).
This base-line achieves rec+ of .79, but overall classificationaccuracy of only 14%.
In all of the models used inour experiments, we include the current confirma-tion prompt type (DIA) feature.4.4 Experiment ResultsIn this section we report the results of experimentson concept type classification in which we exam-ine the impact of the feature sets presented in Ta-ble 3.
We report performance separately for recog-nized speech, which is available at runtime (Table5); and for transcribed speech, which gives us anidea of best possible performance (Table 4).4.4.1 Features from the Current UtteranceWe first look at lexical (LEX) and prosodic (RAW)features from the current utterance.
For both rec-ognized and transcribed speech, the LEX modelachieves significantly higher rec+ and overall ac-curacy than the RAW model (p < .001).
Forrecognized speech, however, the LEX model hassignificantly more switch+ errors than the RAWmodel (p < .001).
This is not surprising since themajority of errors made by the RAW model arelabeling an utterance with a concept as none.
Ut-terances misclassified in this way are not subject tosecond-pass recognition and do not increase WER.45Features Place Time Bus Overallpre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switchNo Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03LEX 0.81 0.88 0.96 0.77 0.48 0.98 0.83 0.59 0.98 0.87 0.72 0.79 0.93 7.32 3.22LEX RAW 0.83 0.84 0.96 0.75 0.54 0.98 0.76 0.59 0.98 0.88 0.70 0.78 0.93 7.39 3.00DH1 LEX 0.85 0.91 0.97 0.72 0.63 0.98 0.89 0.83 0.99 0.88 0.81 0.84 0.95 5.48 2.85DH3 LEX 0.85 0.87 0.97 0.72 0.59 0.98 0.92 0.82 0.99 0.89 0.78 0.83 0.94 5.22 2.62Table 4: Concept type classification results: transcribed speech (all models include feature DIA).
Bestoverall values in each group are highlighted in bold.Features Place Time Bus Overallpre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switchNo Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03LEX 0.70 0.70 0.93 0.67 0.15 0.97 0.65 0.62 0.98 0.75 0.56 0.64 0.89 9.94 4.93LEX RAW 0.70 0.72 0.93 0.66 0.38 0.97 0.68 0.57 0.98 0.76 0.60 0.67 0.90 10.32 5.10DH1 LEX RAW 0.71 0.68 0.93 0.68 0.38 0.97 0.78 0.63 0.98 0.77 0.60 0.67 0.90 8.15 4.55DH3 LEX RAW 0.71 0.70 0.93 0.67 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.57ASR DH3 LEXRAW0.71 0.70 0.93 0.69 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.54CTM DH3 LEXRAW0.82 0.82 0.96 0.86 0.71 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 3.89 2.94CTM ASR DH3LEX RAW0.82 0.81 0.96 0.86 0.69 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 4.27 3.01Table 5: Concept type classification results: recognized speech (all models include feature DIA).
Bestoverall values in each group are highlighted in bold.For transcribed speech, the LEX RAW modeldoes not perform significantly differently from theLEX model in terms of overall accuracy, rec+, orswitch+ errors.
However, for recognized speech,LEX RAW achieves significantly higher rec+ andoverall accuracy than LEX (p < .001).
Lexicalcontent from transcribed speech is a very good in-dicator of concept type.
However, lexical contentfrom recognized speech is noisy, so concept typeclassification from ASR output can be improvedby using acoustic/prosodic features.We note that models containing only featuresfrom the current utterance perform significantlyworse than the confirmation state baseline in termsof rec+ (p < .001).
However, they have signif-icantly better overall accuracy and fewer switch+errors (p < .001) .4.4.2 Features from the Dialog HistoryNext, we add features from the dialog historyto our best-performing models so far.
For tran-scribed speech, DH1 LEX performs significantlybetter than LEX in terms of overall accuracy, rec+,and switch+ errors (p < .001).
DH3 LEX per-forms significantly worse than DH1 LEX in termsof rec+ (p < 0.05).
For recognized speech,neither DH1 LEX RAW nor DH3 LEX RAW issignificantly different from LEX RAW in termsof rec+ or overall accuracy.
However, bothDH1 LEX RAW and DH3 LEX RAW do per-form significantly better than LEX RAW in termsof switch+ errors (p < .05).
There areno significant performance differences betweenDH1 LEX RAW and DH3 LEX RAW.4.4.3 Features Specific to Recognized SpeechFinally, we add the ASR and CTM features tomodels trained on recognized speech.We hypothesized that the classifier can use therecognizer?s confidence score to decide whetheran utterance is likely to have been misrecognized.However, ASR DH3 LEX RAW is not signifi-cantly different from DH3 LEX RAW in terms ofrec+, overall accuracy or switch+ errors.We hypothesized that the CTM feature will im-prove cases where a part of (but not the whole)concept instance is recognized in first-pass recog-nition5.
The generic language model used in first-pass recognition recognizes some concept-relatedwords.
So, if in the utterance Madison avenue,avenue (but not Madison), is recognized in thefirst-pass recognition, the CTM feature can flagthe utterance with a partial match for place, help-ing the classifier to correctly assign the place5We do not try the CTM feature on transcribed speech be-cause there is a one-to-one correspondence between presenceof the concept and the CTM feature, so it perfectly indicatespresence of a concept.46type to the utterance.
Then, in the second-passrecognition the utterance will be decoded witha place concept-specific language model, poten-tially improving speech recognition performance.Adding the CTM feature to DH3 LEX RAW andASR DH3 LEX RAW leads to a large statisticallysignificant improvement in all measures: a 12%absolute increase in rec+, a 3% absolute increasein overall accuracy, and decreases in switch+ er-rors (p < .001).
There are no statistically signifi-cant differences between these two models.4.4.4 Summary and DiscussionIn this section we evaluated different models forconcept type classification.
The best perform-ing transcribed speech model, DH1 LEX, signif-icantly outperforms the Confirmation State base-line on overall accuracy and on switch+ and switcherrors (p < .001), and is not significantly differenton rec+.
The best performing recognized speechmodel, CTM DH3 LEX RAW, significantly out-performs the Confirmation State baseline onoverall accuracy and on switch+ and switch er-rors, but is significantly worse on rec+ (p < .001).The best transcribed speech model achieves signif-icantly higher rec+ and overall accuracy than thebest recognized speech model (p < .01).5 Speech Recognition ExperimentIn this section we report the impact of concept typeprediction on recognition of post-confirmation ut-terances in Let?s Go!
system data.
We hypothe-sized that speech recognition performance for ut-terances containing a concept can be improvedwith the use of concept-specific LMs.
We (1) com-pare the existing dialog state-specific LM adap-tation approach used in Let?s Go!
with our pro-posed concept-specific adaptation; (2) comparetwo approaches to concept-specific adaptation (us-ing the system?s confirmation prompt type and us-ing our concept type classifiers); and (3) evaluatethe impact of different concept type classifiers onconcept-specific LM adaptation.5.1 MethodWe used the PocketSphinx speech recognition en-gine (et al, 2006) with gender-specific telephone-quality acoustic models built for Communica-tor (et al, 2000).
We trained trigram LMs us-ing 0.5 ratio discounting with the CMU languagemodeling toolkit (Xu and Rudnicky, 2000)6.
Webuilt state- and concept-specific hierarchical LMsfrom the Let?s Go!
2005 data.
The LMs are builtwith [place], [time] and [bus] submodels.We evaluate speech recognition performanceon the post-confirmation user utterances from the2006 testing dataset.
Each experiment varies in 1)the LM used for the final recognition pass and 2)the method of selecting a LM for use in decoding.5.1.1 Language modelsWe built seven LMs for these experiments.
Thestate-specific LM contains all utterances in thetraining data that were produced in the yes-no di-alog state.
The confirm-place, confirm-bus andconfirm-time LMs contain all utterances producedin the yes-no dialog state following confirm place,confirm bus and confirm time system confirma-tion prompts respectively.
Finally, the concept-place, concept-bus and concept-time LMs containall utterances produced in the yes-no dialog statethat contain a mention of a place, bus or time.5.1.2 DecodersIn the baseline, 1-pass general condition, weuse the state-specific LM to recognize all post-confirmation utterances.
In the 1-pass state ex-perimental condition we use the confirm-place,confirm-bus and confirm-time LMs to recog-nize testing utterances produced following a con-firm place, confirm bus and confirm time promptrespectively7 .
In the 1-pass concept experimen-tal condition we use the concept-place, concept-bus and concept-time LMs to recognize testing ut-terances produced following a confirm place, con-firm bus and confirm time prompt respectively.In the 2-pass conditions we perform first-passrecognition using the general LM.
Then, we clas-sify the output of the first pass using a concepttype classifier.
Finally, we perform second-passrecognition using the concept-place, concept-busor concept-time LMs if the utterance was classi-fied as place, bus or time respectively8 .
We usedthe three classification models with highest overallrec+: DH3 LEX RAW, ASR DH3 LEX RAW,6We chose the same speech recognizer, acoustic models,language modeling toolkit, and LM building parameters thatare used in the live Let?s Go!
system (Raux et al, 2005).7As we showed in Table 2, most, but not all, utterances ina confirmation state contain the corresponding concept.8We treat utterances classified as containing more thanconcept type as none.
In the 2006 data, only 5.6% of ut-terances with a concept contain more than one concept type.47Recognizer Concept type Language Overall Concept utterancesclassifier model WER WER Concept recall1-pass general state-specific 38.49% 49.12% 50.75%1-pass confirm state confirm-{place,bus,time} 38.83% 48.96% 51.36%1-pass confirm state concept-{place,bus,time},state-specific46.47% ?
50.73% ?
52.9% ?2-pass DH3 LEX RAW concept-{place,bus,time},state-specific38.48% 47.56% ?
53.2% ?2-pass ASR DH3 LEXRAWconcept-{place,bus,time},state-specific38.51% 47.99% ?
52.7%2-pass CTM ASR DH3LEX RAWconcept-{place,bus,time},state-specific38.42% 47.86% ?
52.6%2-pass oracle concept-{place,bus,time},state-specific37.85% ?
45.94% ?
54.91% ?Table 6: Speech recognition results.
?
indicates significant difference (p<.01).
?
indicates significantdifference (p<.05).
* indicates near-significant trend in difference (p<.07).
Significance for WER iscomputed as a paired t-test.
Significance for concept recall is an inference on proportion.and CTM ASR DH3 LEX RAW.
To get an ideaof ?best possible?
performance, we also report 2-pass oracle recognition results, assuming an oracleclassifier that always outputs the correct concepttype for an utterance.5.2 ResultsIn Table 6 we report average per-utterance worderror rate (WER) on post-confirmation utterances,average per-utterance WER on post-confirmationutterances containing a concept, and average con-cept recall rate (percentage of correctly recog-nized concepts) on post-confirmation utterancescontaining a concept.
In slot-filling dialog sys-tems like Let?s Go!, the concept recall rate largelydetermines the potential of the system to under-stand user-provided information and continue thedialog successfully.
Our goal is to maximize con-cept recall and minimize concept utterance WER,without causing overall WER to decline.As Table 6 shows, the 1-pass state and 1-passconcept recognizers perform better than the 1-pass general recognizer in terms of concept recall,but worse in terms of overall WER.
Most of thesedifferences are not statistically significant.
How-ever, the 1-pass concept recognizer has signifi-cantly worse overall and concept utterance WERthan the 1-pass general recognizer (p < .01).All of the 2-pass recognizers that use au-tomatic concept prediction achieve significantlylower concept utterance WER than the 1-passgeneral recognizer (p < .05).
Differences be-tween these recognizers in overall WER and con-cept recall are not significant.The 2-pass oracle recognizer achieves signif-icantly higher concept recall and significantlylower overall and concept utterance WER thanthe 1-pass general recognizer (p < .01).
Italso achieves significantly lower concept utteranceWER than any of the 2-pass recognizers that useautomatic concept prediction (p < .01).Our 2-pass concept results show that it is possi-ble to use knowledge of the concepts in a user?s ut-terance to improve speech recognition.
Our 1-passconcept results show that this cannot be effec-tively done by assuming that the user will alwaysaddress the system?s question; instead, one mustconsider the user?s actual utterance and the dis-course history (as in our DH3 LEX RAW model).6 Conclusions and Future WorkIn this paper, we examined user responses to sys-tem confirmation prompts in task-oriented spokendialog.
We showed that these post-confirmationutterances may contain unrequested task-relevantconcepts that are likely to be misrecognized.
Us-ing acoustic, lexical, dialog state and dialog his-tory features, we were able to classify task-relevant concepts in the ASR output for post-confirmation utterances with 90% accuracy.
Weshowed that use of a concept type classifier canlead to improvements in speech recognition per-formance in terms of WER and concept recall.Of course, any possible improvements in speechrecognition performance are dependent on (1) theperformance of concept type classification; (2)the accuracy of the first-pass speech recognition;and (3) the accuracy of the second-pass speechrecognition.
For example, with our general lan-guage model, we get a fairly high overall WERof 38.49%.
In future work, we will systematicallyvary the WER of both the first- and second-pass48speech recognizers to further explore the interac-tion between speech recognition performance andconcept type classification.The improvements our two-pass recognizersachieve have quite small local effects (up to 3.18%absolute improvement in WER on utterances con-taining a concept, and less than 1% on post-confirmation utterances overall) but may havelarger impact on dialog completion times and taskcompletion rates, as they reduce the number ofcascading recognition errors in the dialog (et al,2002).
Furthermore, we could also use knowledgeof the concept type(s) contained in a user utteranceto improve dialog management and response plan-ning (Bohus, 2007).
In future work, we will lookat (1) extending the use of our concept-type clas-sifiers to utterances following any system prompt;and (2) the impact of these interventions on overallmetrics of dialog success.7 AcknowledgementsWe would like to thank the researchers at CMUfor providing the Let?s Go!
data and additionalresources.ReferencesJ.
R. Bellegarda.
2004.
Statistical language modeladaptation: Review and perspectives.
Speech Com-munication Special Issue on Adaptation Methods forSpeech Recognition, 42:93?108.D.
Bohus.
2007.
Error awareness and recovery intask-oriented spoken dialog systems.
Ph.D. thesis,Carnegie Mellon University.Y.
Esteve, F. Bechet, A. Nasr, and R. Mori.
2001.Stochastic finite state automata language model trig-gered by dialogue states.
In Proceedings of Eu-rospeech.A.
Rudnicky et al 2000.
Task and domain specificmodelling in the Carnegie Mellon Communicatorsystem.
In Proceedings of ICSLP.J.
Shin et al 2002.
Analysis of user behavior undererror conditions in spoken dialogs.
In Proceedingsof ICSLP.D.
Huggins-Daines et al 2006.
Sphinx: A free, real-time continuous speech recognition system for hand-held devices.
In Proceedings of ICASSP.D.
Gildea and T. Hofmann.
1999.
Topic-based lan-guage models using EM.
In Proceedings of Eu-rospeech.J.
Hirschberg, D. Litman, and M. Swerts.
2004.Prosodic and other cues to speech recognition fail-ures.
Speech Communication, 43:155?175.R.
Iyer and M. Ostendorf.
1999.
Modeling long dis-tance dependencies in language: Topic mixtures ver-sus dynamic cache model.
IEEE Transactions onSpeech and Audio Processing, 7(1):30?39.E.
Krahmer, M. Swerts, M. Theune, and M. Weegels.2001.
Error detection in spoken human-machine in-teraction.
International Journal of Speech Technol-ogy, 4(1).G.-A.
Levow.
1998.
Characterizing and recognizingspoken corrections in human-computer dialogue.
InProceedings of COLING-ACL.D.
Litman, J.Hirschberg, and M. Swerts.
2006.
Char-acterizing and predicting corrections in spoken dia-logue systems.
Computational Linguistics, 32:417?438.C.
D. Manning, P. Raghavan, and H. Schu?tze.
2008.Introduction to Information Retrieval.
CambridgeUniversity Press.A.
Raux, B. Langner, A.
Black, and M Eskenazi.
2005.Let?s Go Public!
Taking a spoken dialog system tothe real world.
In Proceedings of Eurospeech.G.
Riccardi and A. L. Gorin.
2000.
Stochastic lan-guage adaptation over time and state in a natural spo-ken dialog system.
IEEE Transactions on Speechand Audio Processing, 8(1):3?9.S.
Stenchikova, D. Hakkani-Tu?r, and G. Tur.
2008.Name-aware speech recognition for interactivequestion answering.
In Proceedings of ICASSP.G.
Tur and A. Stolcke.
2007.
Unsupervised languagemodel adaptation for meeting recognition.
In Pro-ceedings of ICASSP.G.
Tur.
2005.
Model adaptation for spoken languageunderstanding.
In Proceedings of ICASSP.G.
Tur.
2007.
Extending boosting for large scalespoken language understanding.
Machine Learning,69(1):55?74.M.
Walker, J. Wright, and I. Langkilde.
2000.
Usingnatural language processing and discourse featuresto identify understanding errors in a spoken dialoguesystem.
In Proceedings of ICML.I.
Witten and E. Frank.
2005.
Data Mining: Practi-cal machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.W.
Xu and A. Rudnicky.
2000.
Language modelingfor dialog system.
In Proceedings of ICSLP.S.
Young.
1994.
Detecting misrecognitions and out-of-vocabulary words.
In Proceedings of ICASSP.49
