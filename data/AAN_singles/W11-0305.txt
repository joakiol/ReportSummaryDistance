Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 39?47,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsWord Segmentation as General ChunkingDaniel Hewlett and Paul CohenDepartment of Computer ScienceUniversity of ArizonaTucson, AZ 85721{dhewlett,cohen}@cs.arizona.eduAbstractDuring language acquisition, children learnto segment speech into phonemes, syllables,morphemes, and words.
We examine wordsegmentation specifically, and explore thepossibility that children might have general-purpose chunking mechanisms to performword segmentation.
The Voting Experts (VE)and Bootstrapped Voting Experts (BVE) algo-rithms serve as computational models of thischunking ability.
VE finds chunks by search-ing for a particular information-theoretic sig-nature: low internal entropy and high bound-ary entropy.
BVE adds to VE the abil-ity to incorporate information about wordboundaries previously found by the algorithminto future segmentations.
We evaluate thegeneral chunking model on phonemically-encoded corpora of child-directed speech, andshow that it is consistent with empirical resultsin the developmental literature.
We argue thatit offers a parsimonious alternative to special-purpose linguistic models.1 IntroductionThe ability to extract words from fluent speech ap-pears as early as the seventh month in human de-velopment (Jusczyk et al, 1999).
Models of thisability have emerged from such diverse fields as lin-guistics, psychology and computer science.
Manyof these models make unrealistic assumptions aboutchild language learning, or rely on supervision, orare specific to speech or language.
Here we presentan alternative: a general unsupervised model ofchunking that performs very well on word segmen-tation tasks.
We will examine the Voting Experts,Bootstrapped Voting Experts, and Phoneme to Mor-pheme algorithms in Section 2.
Each searches for ageneral, information-theoretic signature of chunks.Each can operate in either a fully unsupervised set-ting, where the input is a single continuous se-quence of phonemes, or a semi-supervised setting,where the input is a sequence of sentences.
In Sec-tion 4, we evaluate these general chunking methodson phonetically-encoded corpora of child-directedspeech, and compare them to a representative set ofcomputational models of early word segmentation.Section 4.4 presents evidence that words optimizethe information-theoretic signature of chunks.
Sec-tion 5 discusses segmentation methods in light ofwhat is known about the segmentation abilities ofchildren.2 General ChunkingThe Voting Experts algorithm (Cohen and Adams,2001) defines the chunk operationally as a sequencewith the property that elements within the sequencepredict one another but do not predict elements out-side the sequence.
In information-theoretic terms,chunks have low entropy internally and high entropyat their boundaries.
Voting Experts (VE) is a lo-cal, greedy algorithm that works by sliding a rel-atively small window along a relatively long inputsequence, calculating the internal and boundary en-tropies of sequences within the window.The name Voting Experts refers to the two ?ex-perts?
that vote on possible boundary locations:One expert votes to place boundaries after se-quences that have low internal entropy (also calledsurprisal), given by HI(seq) = ?
logP (seq).The other places votes after sequences that have39high branching entropy, given by HB(seq) =?
?c?S P (c|seq) logP (c|seq), where S is the setof successors to seq.
In a modified version of VE,a third expert ?looks backward?
and computes thebranching entropy at locations before, rather than af-ter, seq.The statistics required to calculate HI and HB arestored efficiently using an n-gram trie, which is typ-ically constructed in a single pass over the corpus.The trie depth is 1 greater than the size of the slid-ing window.
Importantly, all statistics in the trie arenormalized so as to be expressed in standard devia-tion units.
This allows statistics from sequences ofdifferent lengths to be compared.The sliding window is then passed over the cor-pus, and each expert votes once per window for theboundary location that best matches that expert?s cri-teria.
After voting is complete, the algorithm yieldsan array of vote counts, each element of which isthe number of times some expert voted to segmentat that location.
The result of voting on the stringthisisacat could be represented in the follow-ing way, where the number between each letter isthe number of votes that location received, as int0h0i1s3i1s4a4c1a0t.With the final vote totals in place, the boundariesare placed at locations where the number of votesexceeds a chosen threshold.
For further details of theVoting Experts algorithm see Cohen et al (2007),and also Miller and Stoytchev (2008).2.1 Generality of the Chunk SignatureThe information-theoretic properties of chunks uponwhich VE depends are present in every non-randomsequence, of which sequences of speech sounds areonly one example.
Cohen et al (2007) exploredword segmentation in a variety of languages, as wellas segmenting sequences of robot actions.
Hewlettand Cohen (2010) demonstrated high performancefor a version of VE that segmented sequences of ut-terances between a human teacher and an AI stu-dent.
Miller and Stoytchev (2008) applied VE in akind of bootstrapping procedure to perform a visiontask similar to OCR: first to chunk columns of pix-els into letters, then to chunk sequences of these dis-covered letters into words.
Of particular relevance tothe present discussion are the results of Miller et al(2009), who showed that VE was able to segment acontinuous audio speech stream into phonemes.
Theinput in that experiment was generated to mimic theinput presented to infants by Saffran et al (1996),and was discretized for VE with a Self-OrganizingMap (Kohonen, 1988).2.2 Similar Chunk SignaturesHarris (1955) noticed that if one proceeds incremen-tally through a sequence of letters and asks speakersof the language to list all the letters that could ap-pear next in the sequence (today called the succes-sor count), the points where the number increasesoften correspond to morpheme boundaries.
Tanaka-Ishii and Jin (2006) correctly recognized that thisidea was an early version of branching entropy, oneof the experts in VE, and they developed an algo-rithm called Phoneme to Morpheme (PtM) around it.PtM calculates branching entropy in both directions,but it does not use internal entropy, as VE does.
Itdetects change-points in the absolute branching en-tropy rather than local maxima in the standardizedentropy.
PtM achieved scores similar to those of VEon word segmentation in phonetically-encoded En-glish and Chinese.Within the morphology domain, Johnson andMartin?s HubMorph algorithm (2003) constructs atrie from a set of words, and then converts it intoa DFA by the process of minimization.
HubMorphsearches for stretched hubs in this DFA, which aresequences of states in the DFA that have a lowbranching factor internally, and high branching fac-tor at the edges (shown in Figure 1).
This is a nearlyidentical chunk signature to that of VE, only withsuccessor/predecessor count approximating branch-ing entropy.
The generality of this idea was not loston Johnson and Martin, either: Speaking with re-spect to the morphology problem, Johnson and Mar-tin close by saying ?We believe that hub-automatawill be the basis of a general solution for Indo-European languages as well as for Inuktitut.?
12.3 Chunking and BootstrappingBootstrapped Voting Experts (BVE) is an exten-sion to VE that incorporates knowledge gained fromprior segmentation attempts when segmenting newinput, a process known as bootstrapping.
This1Inuktitut is a polysynthetic Inuit language known for itshighly complex morphology.40Figure 1: The DFA signature of a hub (top) and stretchedhub in the HubMorph algorithm.
Figure from Johnsonand Martin (2003).knowledge does not consist in the memorization ofwhole words (chunks), but rather in statistics de-scribing the beginnings and endings of chunks.
Inthe word segmentation domain, these statistics ef-fectively correspond to phonotactic constraints thatare inferred from hypothesized segmentations.
In-ferred boundaries are stored in a data structure calleda knowledge trie (shown in Figure 2), which is es-sentially a generalized prefix or suffix trie.a3t3t3h2s1o1root.
.
.a3t3t3h2s1o1root#3#3o1n1#1e3n1Figure 2: A portion of the knowledge trie built from#the#cat#sat#on#the#mat#.
Numbers withineach node are frequency counts.BVE was tested on a phonemically-encoded cor-pus of child-directed speech and achieved a higherlevel of performance than any other unsupervised al-gorithm (Hewlett and Cohen, 2009).
We reproducethese results in Section 4.3 Computational Models of WordSegmentationWhile many algorithms exist for solving the wordsegmentation problem, few have been proposedspecifically as computational models of word seg-mentation in language acquisition.
One of the mostwidely cited is MBDP-1 (Model-Based DynamicProgramming) by Brent (1999).
Brent describesthree features that an algorithm should have to qual-ify as an algorithm that ?children could use for seg-mentation and word discovery during language ac-quisition.?
Algorithms should learn in a completelyunsupervised fashion, should segment incrementally(i.e., segment each utterance before considering thenext one), and should not have any built-in knowl-edge about specific natural languages (Brent, 1999).However, the word segmentation paradigm Brentdescribes as ?completely unsupervised?
is actuallysemi-supervised, because the boundaries at the be-ginning and end of each utterance are known tobe true boundaries.
A fully unsupervised paradigmwould include no boundary information at all, mean-ing that the input is, or is treated as, a continuous se-quences of phonemes.
The MBDP-1 algorithm wasnot designed for operation in this continuous condi-tion, as it relies on having at least some true bound-ary information to generalize.MBDP-1 achieves a robust form of bootstrappingthrough the use of Bayesian maximum-likelihoodestimation of the parameters of a language model.More recent algorithms in the same tradition, includ-ing the refined MBDP-1 of Venkataraman (2001),the WordEnds algorithm of Fleck (2008), and theHierarchical Dirichlet Process (HDP) algorithm ofGoldwater (2007), share this limitation.
However,infants are able to discover words in a single streamof continuous speech, as shown by the seminal seriesof studies by Saffran et al (1996; 1998; 2003).
Inthese studies, Saffran et al show that both adults and8-month-old infants quickly learn to extract wordsof a simple artificial language from a continuousspeech stream containing no pauses.The general chunking algorithms VE, BVE, andPtM work in either condition.
The unsupervised,continuous condition is the norm (Cohen et al,2007; Hewlett and Cohen, 2009; Tanaka-Ishii andJin, 2006) but these algorithms are easily adaptedto the semi-supervised, incremental condition.
Re-call that these methods make one pass over the entirecorpus to gather statistics, and then make a secondpass to segment the corpus, thus violating Brent?s re-quirement of incremental segmentation.
To adhereto the incremental requirement, the algorithms sim-ply must segment each sentence as it is seen, andthen update their trie(s) with statistics from that sen-tence.
While VE and PtM have no natural way tostore true boundary information, and so cannot ben-41efit from the supervision inherent in the incrementalparadigm, BVE has the knowledge trie which servesexactly this purpose.
In the incremental paradigm,BVE simply adds each segmented sentence to theknowledge trie, which will inform the segmentationof future sentences.
This way it learns from its owndecisions as well as the ground truth boundaries sur-rounding each utterance, much like MBDP-1 does.BVE and VE were first tested in the incrementalparadigm by Hewlett and Cohen (2009), though onlyon sentences from a literary corpus, George Orwell?s1984.4 Evaluation of Computational ModelsIn this section, we evaluate the general chunking al-gorithms VE, BVE, and PtM in both the continu-ous, unsupervised paradigm of Saffran et al (1996)and the incremental, semi-supervised paradigm as-sumed by bootstrapping algorithms like MBDP-1.We briefly describe the artificial input used by Saf-fran et al, and then turn to the broader problemof word segmentation in natural languages by eval-uating against corpora drawn from the CHILDESdatabase (MacWhinney and Snow, 1985).We evaluate segmentation quality at two levels:boundaries and words.
At the boundary level, wecompute the Boundary Precision (BP), which is sim-ply the percentage of induced boundaries that werecorrect, and Boundary Recall (BR), which is thepercentage of true boundaries that were recoveredby the algorithm.
These measures are commonlycombined into a single metric, the Boundary F-score (BF), which is the harmonic mean of BP andBR: BF = (2 ?
BP ?
BR)/(BP + BR).
Gener-ally, higher BF scores correlate with finding cor-rect chunks more frequently, but for completenesswe also compute the Word Precision (WP), which isthe percentage of induced words that were correct,and the Word Recall (WR), which is the percent-age of true words that were recovered exactly by thealgorithm.
These measures can naturally be com-bined into a single F-score, the Word F-score (WF):WF = (2?WP?WR)/(WP + WR).4.1 Artificial Language ResultsTo simulate the input children heard during Saf-fran et al?s 1996 experiment, we generated a corpusof 400 words, each chosen from the four artificialwords from that experiment (dapiku, tilado,burobi, and pagotu).
As in the original study,the only condition imposed on the random sequencewas that no word would appear twice in succession.VE, BVE, and PtM all achieve a boundary F-scoreof 1.0 whether the input is syllabified or consideredsimply as a stream of phonemes, suggesting that achild equipped with a chunking ability similar to VEcould succeed even without syllabification.4.2 CHILDES: PhonemesTo evaluate these algorithms on data that is closerto the language children hear, we used corporaof child-directed speech taken from the CHILDESdatabase (MacWhinney and Snow, 1985).
Two cor-pora have been examined repeatedly in prior stud-ies: the Bernstein Ratner corpus (Bernstein Rat-ner, 1987), abbreviated BR87, used by Brent (1999),Venkataraman (2001), Fleck (2008), and Goldwateret al (2009), and the Brown corpus (Brown, 1973),used by Gambell and Yang (2006).Before segmentation, all corpora were encodedinto a phonemic representation, to better simulatethe segmentation problem facing children.
TheBR87 corpus has a traditional phonemic encodingcreated by Brent (1999), which facilitates compar-ison with other published results.
Otherwise, thecorpora are translated into a phonemic representa-tion using the CMU Pronouncing Dictionary, withunknown words discarded.The BR87 corpus consists of speech from ninedifferent mothers to their children, who had an av-erage age of 18 months (Brent, 1999).
BR87 con-sists of 9790 utterances, with a total of 36441 words,yielding an average of 3.72 words per utterance.
Weevaluate word segmentation models against BR87 intwo different paradigms, the incremental paradigmdiscussed above and an unconstrained paradigm.Many of the results in the literature do not constrainthe number of times algorithms can process the cor-pus, meaning that algorithms generally process theentire corpus once to gather statistics, and then atleast one more time to actually segment it.
Resultsof VE and other algorithms in this unconstrained set-ting are presented below in Table 1.
In this test, thegeneral chunking algorithms were given one contin-uous corpus with no boundaries, while the results for42bootstrapping algorithms were reported in a semi-supervised condition.Algorithm BP BR BF WP WR WFPtM 0.861 0.897 0.879 0.676 0.704 0.690VE 0.875 0.803 0.838 0.614 0.563 0.587BVE 0.949 0.879 0.913 0.793 0.734 0.762MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682HDP 0.903 0.808 0.852 0.752 0.696 0.723WordEnds 0.946 0.737 0.829 NR NR 0.707Table 1: Results for the BR87 corpus with unconstrainedprocessing of the corpus.
Algorithms in italics are semi-supervised.In the incremental setting, the corpus is treated asa series of utterances and the algorithm must seg-ment each one before moving on to the next.
This isdesigned to better simulate the learning process, as achild would normally listen to a series of utterancesproduced by adults, analyzing each one in turn.
Toperform this test, we used the incremental versionsof PtM, VE, and BVE described in Section 3, andcompared them with MBDP-1 on the BR87 corpus.Each point in Figure 3 shows the boundary F-scoreof each algorithm on the last 500 utterances.
Notethat VE and PtM do not benefit from the informa-tion about boundaries at the beginnings and endingsof utterances, yet they achieve levels of performancenot very inferior to MBDP-1 and BVE, which doleverage true boundary information.0.500.550.600.650.700.750.800.850.900.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5Boundary F-Score(BF)Thousands of UtterancesVEMBDP-1BVEPtMFigure 3: Results for three chunking algorithms andMBDP-1 on BR87 in the incremental paradigm.We also produced a phonemic encoding of theBR87 and Bloom73 (Bloom, 1973) corpora fromCHILDES with the CMU pronouncing dictionary,which encodes stress information (primary, sec-ondary, or unstressed) on phonemes that serve assyllable nuclei.
Stress information is known to bea useful factor in word segmentation, and infantsappear to be sensitive to stress patterns by as earlyas 8 months of age (Jusczyk et al, 1999).
Resultswith these corpora are shown below in Figures 4 and5.
For each of the general chunking algorithms, awindow size of 4 was used, meaning decisions weremade in a highly local manner.
Even so, BVE out-performs MBDP-1 in this arguably more realisticsetting, while VE and PtM rival it or even surpassit.
Note that the quite different results shown in Fig-ure 3 and Figure 4 are for the same corpus, undertwo different phonemic encodings, illustrating theimportance of accurately representing the input chil-dren receive.0.500.550.600.650.700.750.800.850.900.950.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5Boundary F-Score(BF)Thousands of UtterancesVEMBDP-1BVEPtMFigure 4: Results for chunking algorithms and MBDP-1on BR87 (CMU) in the incremental paradigm.0.600.650.700.750.800.850.900.950.5 1 1.5 2 2.5Boundary F-Score(BF)Thousands of UtterancesVEMBDP-1BVEPtMFigure 5: Results for chunking algorithms and MBDP-1on Bloom73 (CMU) in the incremental paradigm.4.3 CHILDES: SyllablesIn many empirical studies of word segmentation inchildren, especially after Saffran et al (1996), theproblem is treated as though syllables were the ba-sic units of the stream to be segmented, rather thanphonemes.
If we assume children can syllabify their43phonemic representation, and that word boundariesonly occur at syllable boundaries, then word seg-mentation becomes a very different, and potentiallymuch easier, problem.
This must be the case, as theprocess of syllabification removes a high percent-age of the potential boundary locations, and all ofthe locations it removes would be incorrect choices.Table 2 supports this argument.
In the CHILDEScorpora examined here, over 85% of the words di-rected to the child are monosyllabic.
This means thatthe trivial All-Locations baseline, which segmentsat every possible location, achieves an F-measure of0.913 when working with syllabic input, comparedto only 0.524 for phonemic input.Gambell and Yang (2006) present an algorithmfor word segmentation that achieves a boundary F-score of 0.946 on correctly syllabified input.
In or-der to achieve this level of performance, Gambelland Yang use a form of bootstrapping combinedwith a rule called the ?Unique Stress Constraint,?or USC, which simply requires that each word con-tain exactly one stressed syllable.
Gambell and Yangdeveloped this algorithm partially as a response toa hypothesis put forward by Saffran et al (1996)to explain their own experimental results.
Saffranet al concluded that young infants can attend tothe transitional probabilities between syllables, andposit word boundaries where transitional probability(TP) is low.
The TP from syllable X to syllable Y issimply given by:P (Y |X) = frequency of XY/frequency of X (1)While TP is sufficient to explain the results of Saf-fran et al?s 1996 study, it performs very poorly onactual child-directed speech, regardless of whetherthe probabilities are calculated between phonemes(Brent, 1999) or syllables.
Because of the dramaticperformance gains shown by the addition of USCin testing, as well as the poor performance of TP,Gambell and Yang conclude that the USC is requiredfor word segmentation and thus is a likely candidatefor inclusion in Universal Grammar (Gambell andYang, 2006).However, as the results in Table 2 show, VE iscapable of slightly superior performance on syllableinput, without assuming any prior constraints on syl-lable stress distribution.
Moreover, the performanceof both algorithms is also only a few points aboveAlgorithm BP BR BFTP 0.416 0.233 0.298TP + USC 0.735 0.712 0.723Bootstrapping + USC 0.959 0.934 0.946Voting Experts 0.918 0.992 0.953All Points 0.839 1.000 0.913Table 2: Performance of various algorithms on the Browncorpus from CHILDES.
Other than VE and All Points,values are taken from (Gambell and Yang, 2006).the baseline of segmenting at every possible bound-ary location (i.e., at every syllable).
These resultsshow the limitations of simple statistics like TP, butalso show that segmenting a sequence of syllables isa simple problem for more powerful statistical algo-rithms like VE.
The fact that a very high percentageof the words found by VE have one stressed syllablesuggest that a rule like the USC could be emergentrather than innate.4.4 Optimality of the VE Chunk SignatureIt is one thing to find chunks in sequences, anotherto have a theory or model of chunks.
The questionaddressed in this section is whether the chunk sig-nature ?
low internal entropy and high boundary en-tropy ?
is merely a good detector of chunk bound-aries, or whether it characterizes chunks, them-selves.
Is the chunk signature merely a good detec-tor of word boundaries, or are words those objectsthat maximize the signal from the signature?
Oneway to answer the question is to define a ?chunki-ness score?
and show that words maximize the scorewhile other objects do not.The chunkiness score is:Ch(s) =Hf (s) +Hb(s)2?
logPr(s) (2)It is just the average of the forward and backwardboundary entropies, which our theory says shouldbe high at true boundaries, minus the internal en-tropy between the boundaries, which should be low.Ch(s) can be calculated for any segment of any se-quence for which we can build a trie.Our prediction is that words have higher chunk-iness scores than other objects.
Given a sequence,such as the letters in this sentence, we can generateother objects by segmenting the sequence in every44possible way (there are 2n?1 of these for a sequenceof length n).
Every segmentation will produce somechunks, each of which will have a chunkiness score.For each 5-word sequence (usually between 18and 27 characters long) in the Bloom73 corpus fromCHILDES, we generated all possible chunks andranked them by their chunkiness.
The average rankof true words was the 98.7th percentile of the distri-bution of chunkiness.
It appears that syntax is theprimary reason that true chunks do not rank higher:When the word-order in the training corpus is scram-bled, the rank of true words is the 99.6th percentileof the chunkiness distribution.
These early results,based on a corpus of child-directed speech, stronglysuggest that words are objects that maximize chunk-iness.
Keep in mind that the chunkiness score knowsnothing of words: The probabilities and entropies onwhich it is based are estimated from continuous se-quences that contain no boundaries.
It is thereforenot obvious or necessary that the objects that maxi-mize chunkiness scores should be words.
It might bethat letters, or phones, or morphemes, or syllables,or something altogether novel maximize chunkinessscores.
However, empirically, the chunkiest objectsin the corpus are words.5 DiscussionWhether segmentation is performed on phonemic orsyllabic sequences, and whether it is unsupervised orprovided information such as utterance boundariesand pauses, information-theoretic algorithms suchas VE, PtM and especially BVE perform segmen-tation very well.
The performance of VE on BR87is on par with other state-of-the-art semi-supervisedsegmentation algorithms such as WordEnds (Fleck,2008) and HDP (Goldwater et al, 2009).
Theperformance of BVE on corpora of child-directedspeech is unmatched in the unconstrained case, tothe best of our knowledge.These results suggest that BVE provides a sin-gle, general chunking ability that that accounts forword segmentation in both scenarios, and potentiallya wide variety of other cognitive tasks as well.
Wenow consider other properties of BVE that are es-pecially relevant to natural language learning.
Overtime, BVE?s knowledge trie comes to represent thedistribution of phoneme sequences that begin andend words it has found.
We now discuss how thisknowledge trie models phonotactic constraints, andultimately becomes an emergent lexicon.5.1 Phonotactic ConstraintsEvery language has a set of constraints on howphonemes can combine together into syllables,called phonotactic constraints.
These constraints af-fect the distribution of phonemes found at the be-ginnings and ends of words.
For example, wordsin English never begin with /ts/, because it is not avalid syllable onset in English.
Knowledge of theseconstraints allows a language learner to simplify thesegmentation problem by eliminating many possi-ble segmentations, as demonstrated in Section 4.3.This approach has inspired algorithms in the litera-ture, such as WordEnds (Fleck, 2008), which buildsa statistical model of phoneme distributions at thebeginnings and ends of words.
BVE also learns amodel of phonotactics at word boundaries by keep-ing similar statistics in its knowledge trie, but cando so in a fully unsupervised setting by inferring itsown set of high-precision word boundaries with thechunk signature.5.2 An Emergent LexiconVE does not represent explicitly a ?lexicon?
ofchunks that it has discovered.
VE produces chunkswhen applied to a sequence, but its internal datastructures do not represent the chunks it has dis-covered explicitly.
By contrast, BVE stores bound-ary information in the knowledge trie and refines itover time.
Simply by storing the beginnings andendings of segments, the knowledge trie comes tostore sequences like #cat#, where # represents aword boundary.
The set of such bounded sequencesconstitutes an emergent lexicon.
After segmentinga corpus of child-directed speech, the ten most fre-quent words of this lexicon are you, the, that, what,is, it, this, what?s, to, and look.
Of the 100 mostfrequent words, 93 are correct.
The 7 errors includesplitting off morphemes such as ing, and mergingfrequently co-occurring word pairs such as do you.6 Implications for Cognitive ScienceRecently, researchers have begun to empirically as-sess the degree to which segmentation algorithmsaccurately model human performance.
In particular,45Frank et al (2010) compared the segmentation pre-dictions made by TP and a Bayesian Lexical modelagainst the segmentation performance of adults, andfound that the predictions of the Bayesian modelwere a better match for the human data.
As men-tioned in Section 4.3, computational evaluation hasdemonstrated repeatedly that TP provides a poormodel of segmentation ability in natural language.Any of the entropic chunking methods investigatedhere can explain the artificial language results moti-vating TP, as well as the segmentation of natural lan-guage, which argues for their inclusion in future em-pirical investigations of human segmentation ability.6.1 Innate KnowledgeThe word segmentation problem provides a reveal-ing case study of the relationship between nativismand statistical learning.
The initial statistical pro-posals, such as TP, were too simple to explain thephenomenon.
However, robust statistical methodswere eventually developed that perform the linguis-tic task successfully.
With statistical learning mod-els in place that perform as well as (or better than)models based on innate knowledge, the argument foran impoverished stimulus becomes difficult to main-tain, and thus the need for a nativist explanation isremoved.Importantly, it should be noted that the successof a statistical learning method is not an argumentthat nothing is innate in the domain of word segmen-tation, but simply that it is the learning procedure,rather than any specific linguistic knowledge, that isinnate.
The position that a statistical segmentationability is innate is bolstered by speech segmentationexperiments with cotton-top tamarins (Hauser et al,2001) that have yielded similar results to Saffran?sexperiments with human infants, suggesting that theability may be present in the common ancestor ofhumans and cotton-top tamarins.Further evidence for a domain-general chunkingability can be found in experiments where humansubjects proved capable of discovering chunks ina single continuous sequence of non-linguistic in-puts.
Saffran et al (1999) found that adults and 8-month-old infants were able to segment sequencesof tones at the level of performance previously estab-lished for syllable sequences (Saffran et al, 1996).Hunt and Aslin (1998) measured the reaction timeof adults when responding to a single continuoussequence of light patterns, and found that subjectsquickly learned to exploit predictive subsequenceswith quicker reactions, while delaying reaction atsubsequence boundaries where prediction was un-certain.
In both of these results, as well as the wordsegmentation experiments of Saffran et al, humanslearned to segment the sequences quickly, usuallywithin minutes, just as general chunking algorithmsquickly reach high levels of performance.7 ConclusionWe have shown that a domain-independent theory ofchunking can be applied effectively to the problemof word segmentation, and can explain the ability ofchildren to segment a continuous sequence, whichother computational models examined here do notattempt to explain.
The human ability to segmentcontinuous sequences extends to non-linguistic do-mains as well, which further strengthens the gen-eral chunking account, as these chunking algorithmshave been successfully applied to a diverse array ofnon-linguistic sequences.
In particular, BVE com-bines the power of the information-theoretic chunksignature with a bootstrapping capability to achievehigh levels of performance in both the continuousand incremental paradigms.8 Future WorkWithin the CHILDES corpus, our results have onlybeen demonstrated for English, which leaves openthe possibility that other languages may presenta more serious segmentation problem.
In En-glish, where many words in child-directed speechare mono-morphemic, the difference between find-ing words and finding morphs is small.
In somelanguages, ignoring the word/morph distinction islikely to be a more costly assumption, especiallyfor highly agglutinative or even polysynthetic lan-guages.
One possibility that merits further explo-ration is that, in such languages, morphs rather thanwords are the units that optimize chunkiness.AcknowledgementsThis work was supported by the Office of Naval Re-search under contract ONR N00141010117.
Any viewsexpressed in this publication are solely those of the au-thors and do not necessarily reflect the views of the ONR.46ReferencesRichard N. Aslin, Jenny R. Saffran, and Elissa L. New-port.
1998.
Computation of Conditional ProbabilityStatistics by 8-Month-Old Infants.
Psychological Sci-ence, 9(4):321?324.Nan Bernstein Ratner, 1987.
The phonology of parent-child speech, pages 159?174.
Erlbaum, Hillsdale, NJ.Lois Bloom.
1973.
One Word at a Time.
Mouton, Paris.Michael R. Brent.
1999.
An Efficient, ProbabilisticallySound Algorithm for Segmentation and Word Discov-ery.
Machine Learning, (34):71?105.Roger Brown.
1973.
A first language: The early stages.Harvard University, Cambridge, MA.Paul Cohen and Niall Adams.
2001.
An algorithmfor segmenting categorical time series into meaning-ful episodes.
In Proceedings of the Fourth Symposiumon Intelligent Data Analysis.Paul Cohen, Niall Adams, and Brent Heeringa.
2007.Voting Experts: An Unsupervised Algorithm forSegmenting Sequences.
Intelligent Data Analysis,11(6):607?625.Margaret M. Fleck.
2008.
Lexicalized phonotactic wordsegmentation.
In Proceedings of The 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 130?138,Columbus, Ohio, USA.
Association for ComputationalLinguistics.Michael C Frank, Harry Tily, Inbal Arnon, and SharonGoldwater.
2010.
Beyond Transitional Probabilities :Human Learners Impose a Parsimony Bias in Statisti-cal Word Segmentation.
In Proceedings of the 32ndAnnual Meeting of the Cognitive Science Society.Timothy Gambell and Charles Yang.
2006.
StatisticsLearning and Universal Grammar: Modeling WordSegmentation.
In Workshop on Psycho-computationalModels of Human Language.Sharon Goldwater, Thomas L Griffiths, and Mark John-son.
2009.
A Bayesian Framework for Word Segmen-tation: Exploring the Effects of Context.
Cognition,112(1):21?54.Sharon Goldwater.
2007.
Nonparametric Bayesian mod-els of lexical acquisition.
Ph.D. dissertation, BrownUniversity.Zellig S. Harris.
1955.
From Phoneme to Morpheme.Language, 31(2):190?222.Marc D. Hauser, Elissa L. Newport, and Richard N.Aslin.
2001.
Segmentation of the speech stream in anon-human primate: statistical learning in cotton-toptamarins.
Cognition, 78(3):B53?64.Daniel Hewlett and Paul Cohen.
2009.
Bootstrap VotingExperts.
In Proceedings of the Twenty-first Interna-tional Joint Conference on Artificial Intelligence.Daniel Hewlett and Paul Cohen.
2010.
Artificial GeneralSegmentation.
In The Third Conference on ArtificialGeneral Intelligence.Ruskin H. Hunt and Richard N. Aslin.
1998.
Statisti-cal learning of visuomotor sequences: Implicit acqui-sition of sub-patterns.
In Proceedings of the TwentiethAnnual Conference of the Cognitive Science Society,Mahwah, NJ.
Lawrence Erlbaum Associates.Howard Johnson and Joel Martin.
2003.
Unsupervisedlearning of morphology for English and Inuktitut.
Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL 2003), pages 43?45.Peter W. Jusczyk, Derek M. Houston, and Mary New-some.
1999.
The Beginnings of Word Segmentationin English-Learning Infants.
Cognitive Psychology,39(3-4):159?207.Teuvo Kohonen.
1988.
Self-organized formation of topo-logically correct feature maps.Brian MacWhinney and Catherine E Snow.
1985.
Thechild language data exchange system (CHILDES).Journal of Child Language.Matthew Miller and Alexander Stoytchev.
2008.
Hierar-chical Voting Experts: An Unsupervised Algorithm forHierarchical Sequence Segmentation.
In Proceedingsof the 7th IEEE International Conference on Develop-ment and Learning, pages 186?191.Matthew Miller, Peter Wong, and Alexander Stoytchev.2009.
Unsupervised Segmentation of Audio SpeechUsing the Voting Experts Algorithm.
Proceedings ofthe 2nd Conference on Artificial General Intelligence(AGI 2009).Jenny R. Saffran and Erik D. Thiessen.
2003.
Patterninduction by infant language learners.
DevelopmentalPsychology, 39(3):484?494.Jenny R. Saffran, Richard N. Aslin, and Elissa L. New-port.
1996.
Statistical Learning by 8-Month-Old In-fants.
Science, 274(December):926?928.Jenny R. Saffran, Elizabeth K Johnson, Richard N. Aslin,and Elissa L. Newport.
1999.
Statistical learning oftone sequences by human infants and adults.
Cogni-tion, 70(1):27?52.Kumiko Tanaka-Ishii and Zhihui Jin.
2006.
FromPhoneme to Morpheme: Another Verification Usinga Corpus.
In Proceedings of the 21st InternationalConference on Computer Processing of Oriental Lan-guages, pages 234?244.Anand Venkataraman.
2001.
A procedure for unsuper-vised lexicon learning.
In Proceedings of the Eigh-teenth International Conference on Machine Learning.47
