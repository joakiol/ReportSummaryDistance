Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 778?787,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUsing External Resources and Joint Learning for Bigram Weightingin ILP-Based Multi-Document SummarizationChen Li1, Yang Liu1, Lin Zhao21Computer Science Department, The University of Texas at DallasRichardson, Texas 75080, USA2Research and Technology Center, Robert Bosch LLCPalo Alto, California 94304, USA{chenli,yangl@hlt.utdallas.edu}{lin.zhao@us.bosch.com}AbstractSome state-of-the-art summarization systemsuse integer linear programming (ILP) basedmethods that aim to maximize the importantconcepts covered in the summary.
These con-cepts are often obtained by selecting bigramsfrom the documents.
In this paper, we improvesuch bigram based ILP summarization meth-ods from different aspects.
First we use syn-tactic information to select more important bi-grams.
Second, to estimate the importance ofthe bigrams, in addition to the internal featuresbased on the test documents (e.g., documentfrequency, bigram positions), we propose toextract features by leveraging multiple exter-nal resources (such as word embedding fromadditional corpus, Wikipedia, Dbpedia, Word-Net, SentiWordNet).
The bigram weights arethen trained discriminatively in a joint learn-ing model that predicts the bigram weightsand selects the summary sentences in the ILPframework at the same time.
We demonstratethat our system consistently outperforms theprior ILP method on different TAC data sets,and performs competitively compared to otherpreviously reported best results.
We also con-ducted various analyses to show the contribu-tions of different components.1 IntroductionExtractive summarization is a sentence selectionproblem: identifying important summary sentencesfrom one or multiple documents.
Many methodshave been developed for this problem, including su-pervised approaches that use a classifier to predictwhether or not a sentence is in the summary, or un-supervised methods such as graph-based approachesto rank the sentences.
Recently global optimiza-tion methods such as integer linear programming(ILP) have been shown to be quite powerful for thistask.
For example, Gillick et al (2009) used ILPto achieve the best result in the TAC 09 summa-rization task.
The core idea of this summarizationmethod is to select the summary sentences by maxi-mizing the sum of the weights of the language con-cepts that appear in the summary.
Bigrams are oftenused as these language concepts because Gillick etal.
(2009) stated that the bigrams gave consistentlybetter performance than unigrams or trigrams for avariety of ROUGE measures.
The association be-tween the language concepts and sentences servesas the constraints.
This ILP method is formally rep-resented as below (see (Gillick et al, 2009) for moredetails):max?iwici(1)s.t.
sjOccij?
ci(2)?jsjOccij?
ci(3)?jljsj?
L (4)ci?
{0, 1} ?i (5)sj?
{0, 1} ?j (6)ciand sjare binary variables that indicate the pres-ence of a concept and a sentence respectively.
ljis the sentence length and L is maximum length ofthe generated summary.
wiis a concept?s weightand Occijmeans the occurrence of concept i in sen-tence j. Inequalities (2)(3) associate the sentences778and concepts.
They ensure that selecting a sen-tence leads to the selection of all the concepts it con-tains, and selecting a concept only happens when itis present in at least one of the selected sentences.In such ILP-based summarization methods, howto determine the concepts and measure their weightsis the key factor impacting the system performance.Intuitively, if we can successfully identify the im-portant key bigrams to use in the ILP system, or as-sign large weights to those important bigrams, thesystem generated summary sentences will contain asmany important bigrams as possible.
The oracle ex-periment in (Gillick et al, 2008) showed that if theyjust use the bigrams extracted from human generatedsummaries as the input of the ILP system, much bet-ter ROUGE scores can be obtained than using theautomatically selected bigrams.In this paper, we adopt the ILP summarizationframework, but make improvement from three as-pects.
First, we use the part-of-speech tag andconstituent parse information to identify importantbigram candidates: bigrams from base NP (nounphrases) and bigrams containing verbs or adjectives.This bigram selection method allows us to keep theimportant bigrams and filter useless ones.
Second, toestimate the bigrams?
weights, in addition to usinginformation from the test documents, such as doc-ument frequency, syntactic role in a sentence, etc.,we utilize a variety of external resources, includinga corpus of news articles with human generated sum-maries, Wiki documents, description of name en-tities from DBpedia, WordNet, and SentiWordNet.Discriminative features are computed based on theseexternal resources with the goal to better representthe importance of a bigram and its semantic similar-ity with the given query.
Finally, we propose to usea joint bigram weighting and sentence selection pro-cess to train the feature weights.
Our experimentalresults on multiple TAC data sets show the competi-tiveness of our proposed methods.2 Related WorkOptimization methods have been widely used inextractive summarization lately.
McDonald (2007)first introduced the sentence level ILP for summa-rization.
Later Gillick et al (2009) revised it toconcept-based ILP, which is similar to the Bud-geted Maximal Coverage problem in (Khuller et al,1999).
The concept-based ILP system performedvery well in the TAC 2008 and 2009 summariza-tion task (Gillick et al, 2008; Gillick et al, 2009).After that, the global optimization strategy attractedincreasing attention in the summarization task.
Linand Bilmes (2010) treated the summarization task asa maximization problem of submodular functions.Davis et al (2012) proposed an optimal combina-torial covering algorithm combined with LSA tomeasure the term weight for extractive summariza-tion.
Takamura and Okumura (2009) also definedthe summarization problem as a maximum cover-age problem and used a branch-and-bound methodto search for the optimal solution.
Li et al (2013b)used the same ILP framework as (Gillick et al,2009), but incorporated a supervised model to es-timate the bigram frequency in the final summary.Similar optimization methods are also widelyused in the abstractive summarization task.
Martinsand Smith (2009) leveraged ILP technique to jointlyselect and compress sentences for multi-documentsummarization.
A novel summary guided sentencecompression was proposed by (Li et al, 2013a) andit successfully improved the summarization perfor-mance.
Woodsend and Lapata (2012) and Li etal.
(2014) both leveraged constituent parser trees tohelp sentence compression, which is also modeledin the optimization framework.
But these kinds ofwork involve using complex linguistic information,often based on syntactic analysis.Since the language concepts (or bigrams) can beconsidered as key phrases of the documents, theother line related to our work is how to extract andmeasure the importance of key phrases from doc-uments.
In particular, our work is related to keyphrase extraction by using external resources.
Asurvey by (Hasan and Ng, 2014) showed that us-ing external resources to extract and measure keyphrases is very effective.
In (Medelyan et al, 2009),Wikipedia-based key phrases are determined basedon a candidate?s document frequency multiplied bythe ratio of the number of Wikipedia articles con-taining the candidate as a link to the number of ar-ticles containing the candidate.
Query logs werealso used as another external resource by (Yih etal., 2006) to exploit the observation that a candidateis potentially important if it was used as a search779query.
Similarly terminological databases have beenexploited to encode the salience of candidate keyphrases in scientific papers (Lopez and Romary,2010).
In summarization, external information hasalso been used to measure word salience.
SomeTAC systems like (Kumar et al, 2010; Jia et al,2010) used Wiki as an important external resourceto measure the words?
importance, which helped im-prove the summarization results.
Hong and Nenkova(2014) introduced a supervised model for predictingword importance that incorporated a rich set of fea-tures.
Tweets information is leveraged by (Wei andGao, 2014) to help generate news highlights.In this paper our focus is on choosing useful bi-grams and estimating accurate weights to use in theconcept-based ILP methods.
We explore many ex-ternal resources to extract features for bigram candi-dates, and more importantly, propose to estimate thefeature weights in a joint process via structured per-ceptron learning that optimizes summary sentenceselection.3 Summarization SystemIn this study we use the ILP-based summarizationframework (Formulas 1-6) that tries to maximize theweights of the selected concepts (bigrams) under thesummary length constraint.
Our focus is on betterselection of the bigrams and estimation of the bi-gram weights.
We use syntax tree and POS of tokensto help filter some useless bigrams.
Then supervisedmethods are applied to predict the bigram weights.The rich set of features we use is introduced in Sec-tion 4.
In the following we describe how to selectimportant bigrams and how the feature weights aretrained.3.1 Bigram SelectionIn (Gillick et al, 2009), bigrams whose docu-ment frequency is higher than a predefined thresh-old (df=3 in previous work) are used as the conceptsin the ILP model.
The weight for these bigrams inthe ILP optimization objective function (Formula 1)is simply set as their document frequency.
Althoughthis setting has been demonstrated to be quite effec-tive, its gap with the oracle experiment (using bi-grams that appear in the human summaries) is stillvery large, suggesting potential gains by using betterbigrams/concepts in the ILP optimization method.Details are described in (Gillick et al, 2009).In this paper, rather than considering all the bi-grams, we propose to utilize syntactic information tohelp select important bigrams.
Intuitively bigramscontaining content words carry more topic relatedinformation.
As proven in (Klavans and Kan, 1998),nouns, verbs, and adjectives were indeed beneficialin document analysis.
Therefore we focus on choos-ing bigrams containing these words.
First, we usea bottom-up strategy to go through the constituentparse tree and identify the ?NP?
nodes in the low-est level of the tree.
Then all the bigrams in thesebase NPs are kept as candidates.
Second, we findthe verbs and adjectives from the sentence based onthe POS tags, and construct bigrams by concatenat-ing the previous or the next word of that verb or ad-jective.
If these bigrams are not included in thosealready found from the base NPs, they are added tothe bigram candidates.
After the above filtering, wefurther drop bigrams if both words are stop words,as previous work in (Gillick et al, 2009).3.2 Weight TrainingWe propose to train the feature weights in a jointlearning fashion.
In the ILP summarization frame-work, we use the following new objective function:max?i(?
?
f(bi))ci(7)We replace the wiin Formula 1 with a vector innerproduct of bigram features and their correspondingweights.
Constraints remain the same as those inFormula 2 to 6.To train the model (feature weights), we leveragestructured perceptron strategy (Collins, 2002) to up-date the feature weights whenever the hypothesis of-fered by the ILP decoding process is incorrect.
Bi-nary class labels are used for bigrams in the learningprocess, that is, we only consider whether a bigramis in the system generated summary or human sum-maries, not their term or document frequency.
Dur-ing perceptron training, a fixed learning rate is usedand parameters are averaged to prevent overfitting.4 Features for BigramsWe use a rich set of features to represent each bi-gram candidate, including internal features based on780the test documents, and features extracted from ex-ternal resources.
The goal is to better predict theimportance of a bigram, which we expect will helpthe ILP module better determine whether to includethe bigram in the summary.4.1 Internal FeaturesThese features are generated from the provided testdocuments (note our task is multi-document summa-rization, and there is a given query topic.
See Sec-tion 5 for the description of tasks and data).?
Frequency of the bigram in the entire set.?
Frequency of the bigram in related sentences.1?
Document frequency of the bigram in the entireset.?
Is this bigram in the first 1/2/3 sentence??
Is this bigram in the last 1/2/3 sentence??
Similarity with the topic title, calculated by thenumber of common tokens in these two strings,divided by the length of the longer string.4.2 Importance Score based on LanguageModelsThe idea is to train two language models (LMs), onefrom the original documents, and the other one fromthe summaries, and compare the likelihood of a bi-gram generated by these two LMs, which can indi-cate how often a bigram is used in a summary.
Sim-ilar to previous work in (Hong and Nenkova, 2014),we leveraged The New York Times Annotated Cor-pus (LDC Catalog No: LDC2008T19), which hasthe original news articles and human generated ab-stracts.
We build two language models, from thenews articles and the corresponding summaries re-spectively.
We used about 160K abstract-originalpairs.
The KL scores for a bigram are defined asfollows:KL(LMA|LMO)(b) = PrA(b) ?
lnPrA(b)PrO(b)(8)KL(LMO|LMA)(b) = PrO(b) ?
lnPrO(b)PrA(b)(9)1Note that we do not use all the sentences in the ILP module.The ?relevant?
sentences are those that have at least one bigramwith document frequency larger than or equal to three.where (LMA) and (LMO) are the LMs from theabstracts and the original news articles.
Note thatone difference from (Hong and Nenkova, 2014) isthat we calculate these scores for a bigram, not aword.
As (Hong and Nenkova, 2014) showed, ahigher value from the score in Formula 8 meansthe words are favored in the summaries, and viceverse in Formula 9.
In addition to the above fea-tures, we also include the likelihood PrA(b) andPrO(b) based on the two LMs, and the absoluteand relative difference between them: PrA(b) ?PrO(b), P rA(b)/PrO(b).4.3 Similarity based on Word EmbeddingRepresentationGiven the recent success of the continuous represen-tation for words, we propose to use an unsupervisedmethod to induce dense real-valued low dimensionalword embedding, and then use the inner product as ameasure of semantic similarity between two strings.In the word embedding model, every word can berepresented by a vector ~w.
We define the similar-ity between two sequences S1 = x1, x2, ...xkandsequence S2 = y1, y2, ...ylas the average pairwisesimilarity between any two words in them:Sim(S1, S2) =?ki=1?lj=1~xi?
~yjk ?
l(10)Based on such word embedding models, we de-rive two similarity features: (1) similarity betweena bigram and the topic query, and (2) similarity be-tween a bigram and top-k most frequent unigrams inthis topic.
We trained two word embedding mod-els, from the abstract and news article collectionsin the New York Times Annotated Corpus, and thushave two sets of the above similarity features.
Weuse the continuous bag-of-words model introducedby (Mikolov et al, 2013), and the tool word2vec2toobtain the word embeddings.4.4 Similarity based on WordNet3Similar to the above method, here we still focuson measuring the similarity between a bigram andthe topic query, but based on WordNet.
We useWordNet to identify the synonyms of nouns, verbs,2https://code.google.com/p/word2vec/3http://wordnet.princeton.edu/781and adjectives from each bigram and the query ofthe topic.
Then every bigram and sentence can berepresented as a bag of synonyms of the originalwords.
Finally based on these synonyms we lever-age the following four similarity measurements: LinSimilarity (Lin, 1998), Wu-Palmer Similarity (Wuand Palmer, 1994), Jiang-Conrath Similarity (Jiangand Conrath, 1997), and Resnik Similarity (Resnik,1995).
These four similarity measurements are allimplemented in the NLTK toolkit4.
We expect thatthese features would improve the estimation accu-racy because they can overcome the ambiguity andthe diversity of the vocabulary.4.5 Importance based on WikipediaWikipedia is a very popular resource used in manydifferent tasks.
In order to obtain more precise ex-ternal information from Wikipedia for our task, wecollect the articles from Wikipedia by two steps.
Ifthe query is already the title of a wiki page, wewill not further gather other wiki pages for thistopic.
Otherwise, we first search for the wiki pagesfor the given topic query and description (if avail-able) using Google advanced search function to findpages from http://en.wikipedia.org/.
For each re-turned wiki page, we further calculate its similaritybetween its abstract and the test documents?
top kfrequent words.
We select 3 most similar pages asthe external Wiki resource for this topic.
For thesewikipages, we split into two parts: abstract and con-tent.5The features are the following: For each bi-gram, we collect its tf*idf score from the abstractand content part respectively, and the average tf*idfvalue of the unigrams in the bigram candidate.
In ad-dition, we design two boolean features that representwhether a bigram is the top-k most frequent ones inthe abstract or the content part of the Wikepages.4.6 DBpedia6for Extending Name EntityDBpedia is a crowd-sourced community effort to ex-tract structured information from Wikipedia and itsSpotlight Service7is an entity linking tool to connect4http://www.nltk.org/5Every Wikipage has a table of contents.
The part beforethat is considered as abstract and the part after that is the contentof that page.6http://dbpedia.org/About7http://blog.dbpedia.org/2014/07/21/dbpedia-spotlight-v07-released/free text to DBpedia through the recognition and dis-ambiguation of entities and concepts from the DB-pedia Knowledge Base.
We use this service to ex-tract the entity from each sentence, and if the recog-nized entity is also identified as a named entity byStanford CoreNLP8, we use this entity?s DBpediaabstract content to extend the bigrams.
For exam-ple, in the bigram ?Kashmir area?, the word ?Kash-mir?
is recognized as an entity by both (StanfordCoreNLP and DBpedia Spotlight service), then weuse the description for ?Kashmir?
from DBpedia9toextend this bigram, and calculate the cosine similar-ity between this description and the topic query andtop-k most frequent unigrams in the documents.4.7 Sentiment Feature from SentiWordNet10SentiWordNet (Baccianella et al, 2010) is an exten-sion on WordNet and it further assigns to each synsetof WordNet three sentiment scores: positivity, neg-ativity, objectivity.
The sentiment score of a bigramis the average score of the two words in the bigram.To sum up, the features we use include the in-ternal features, and external ones derived from vari-ous resources: news article corpus with summaries,Wikipeida, DBpedia, WordNet and SentiWordNet.Some external features represent the inherent im-portance of bigrams.
For example, features ex-tracted from the news article corpus and wikipediaare used to represent how often bigrams are used insummary/abstract compared to the entire document.Some external features are used to better computesemantic similarity, for example, features from theword embedding methods, DBpedia, and WordNet.5 Experiments5.1 Data and Experiment SetupWe evaluate our methods using several recent TACdata sets, from 2008 to 2011.
The TAC summa-rization task is to generate at most 100 words sum-maries from 10 documents for a given topic query8http://nlp.stanford.edu/software/corenlp.shtml9The Indian subcontinent is a southerly region of Asia,mostly situated on the Indian Plate and projecting southwardinto the Indian Ocean.
Definitions of the extent of the Indiansubcontinent differ but it usually includes the core lands of In-dia, Pakistan, and Bangladesh10http://sentiwordnet.isti.cnr.it/782consisting of a title and more detailed description(this is unavailable in 2010 and 2011 data).
Whenevaluating on one TAC data set, we use the datafrom the other three years as the training set.
Allthe summaries are evaluated using ROUGE (Lin,2004; Owczarzak et al, 2012).
In all of our ex-periments, we use Stanford CoreNLP toolkit to to-kenize the sentences, extract name entities and POStags.
Berkeley Parser (Petrov et al, 2006) is used toget the constituent parse tree for every sentence.
Anacademic free solver11does all the ILP decoding.5.2 Results and Analysis5.2.1 Summarization ResultsTable 1 shows the ROUGE-2 results of our pro-posed joint system, the ICSI system (which usesdocument frequency threshold to select bigram con-cepts and uses df as weights), the best performingsystem in the NIST TAC evaluation, and the state ofthe art performance we could find.
The result of ourproposed method is statistically significantly betterthan that of ICSI ILP (p < 0.05 based on paired t-test).
It is also statistically significantly (p < 0.05)better than that of TAC Rank1 except 2011, andprevious best in 2008 and 2010.
The 2011 previ-ous best results from (Ng et al, 2012) involve somerule-based sentence compression, which improvesthe ROUGE value.
If we apply the same or similarrule-based sentence compression on our results, andthe ROUGE-2 of our proposed method improves to14.38.2008 2009 2010 2011ICSI ILP 10.23 11.60 10.03 12.71TAC Rank1 10.38 12.16 9.57 13.44Previous Best 10.76?
12.46?
10.8?
13.93?Proposed Method 11.84 12.77 11.78 13.97Table 1: ROUGE-2 summarization results.?
is from (Liet al, 2013b), ?
is from (Davis et al, 2012), and ?
is from(Ng et al, 2012).5.2.2 The Effect of Bigram SelectionIn our experiments, the document frequencythreshold used to filter the bigrams is 3, the same asthat in (Gillick et al, 2009), in order to make a bet-ter comparison with previous work.
Figure 1 shows11http://www.gurobi.comthe percentage of the correct bigrams (those in thehuman reference summaries) by our proposed selec-tion method and the original ICSI system which justused document frequency based selection.
We cansee that our selection method yields a higher per-cent of the correctly chosen bigrams.
Since our pro-posed method is slightly aggressive when filteringbigrams, the absolute number of the correct bigramsdecreased.
However, our filtering method success-fully removes more useless bigrams, resulting in ahigher percentage of the correct bigrams.Table 2 shows the summarization results when us-ing different bigrams: the method used in the ICSIILP system, that is, document frequency based se-lection/filtering and our selection method.
Both ofthem use document frequency as the bigram weightin the ILP summarization module.
The results showthat just by changing the input bigrams, our methodhas already outperformed the ICSI system, whichmeans the selection of bigram indeed has an impacton summarization results.20222426283032342008 2009 2010 2011ICSI SystemPercentage(%)Percentage(%)ICSI SystemICSI System Proposed SystemFigure 1: Percentage of correct bigrams in the selectedbigrams from ICSI and our proposed system.2008 2009 2010 2011ICSI ILP 10.23 11.60 10.03 12.71Ours 10.26 11.65 10.25 12.75Table 2: ROUGE-2 summarization results when usingdifferent bigrams, both using document frequencies asweights.7835.2.3 The Effect of FeaturesNext we evaluate the contributions of differentfeatures.
We show results for four experiments: (i)use just one type of features; (ii) combine the in-ternal features with features from just one externalresource; (iii) incrementally add external resourcesone by one; (iv) leave out each feature type.Table 3 shows the ROUGE-2 results when weonly apply one type of features.
First, we can seethat the system with the internal features has alreadyoutperformed the baseline which used document fre-quency as the weight.
It shows that the other cho-sen internal features (beyond document frequency)are useful.
Second, when we use the features fromonly one external resource, the results from someresources are competitive compared to that from thesystem using internal features.
In particular, whenusing the LM scores, Wiki or Word Embedding fea-tures, the results are slightly better than the inter-nal features.
Using DBpedia or SentiWordNet hasworse results than the internal features.
This isbecause the SentiWordNet features themselves arenot very discriminative.
For DBpedia, since it onlyhas feature values for the bigrams containing nameentities, it will only assign weights for those bi-grams.
Therefore, only considering DBpedia fea-tures means that the ILP decoder would prefer tochoose bigrams that are name entities with positiveweights.2008 2009 2010 2011Internal 10.40 11.76 10.42 12.91LM 10.58 11.86 10.48 12.94Word Embedding 10.67 11.96 10.58 13.02Wikipedia 10.61 11.90 10.52 13.00DBpedia 8.35 9.85 9.46 11.00WordNet 10.39 11.76 10.40 12.86SentiwordNet 9.90 10.80 10.08 12.50Table 3: ROUGE-2 results using one feature type.Table 4 shows the results when combining the in-ternal features with features from one external re-source.
We can see that the features from Word Em-bedding model outperform others, suggesting the ef-fectiveness of this semantic similarity measure.
Fea-tures from the LM scores and Wiki are also quiteuseful.
Wiki pages are extracted for the test topicitself, therefore they provide topic relevant back-ground information.
The LM score features are ex-tracted from large amounts of news article data, andare good representation of the general importanceof bigrams for the test domain.
In contrast, Word-Net information is collected from a more general as-pect, which may not be a very good choice for thistask.
Also notice that even though the features fromDBpedia and sentiwordnet do not perform well bythemselves, after the combination with internal fea-tures, there is significant improvement.
This provesthat the features from DBpedia and sentiwordnetprovide additional information not captured by theinternal features from the documents.2008 2009 2010 2011Internal 10.40 11.76 10.42 12.91+LM 10.76 12.03 10.80 13.11+Word Embedding 10.92 12.12 10.85 13.24+Wikipedia 10.81 12.08 10.76 13.17+WordNet 10.68 11.96 10.71 12.99+SentiwordNet 10.60 11.96 10.63 12.96+DBpedia 10.69 12.00 10.70 13.07Table 4: ROUGE-2 results using internal features com-bined with features from just one external resource.Table 5 shows the results when adding featuresone by one.
The order is based on its individual im-pact when combined with internal features.
The re-sults show that Wiki, LM and DBpedia features givemore improvement than WordNet and SentiWord-Net features.
This shows the different impact of theexternal resources.
We can see there is consistentimprovement when more features are added.2008 2009 2010 20111: Internal10.92 12.12 10.85 13.24+Word Embedding2: 1+Wiki 11.22 12.25 11.15 13.473: 2+LM 11.41 12.41 11.37 13.684: 3+DBpedia 11.65 12.60 11.61 13.775: 4+WordNet 11.75 12.67 11.70 13.906: 5+SentiWordNet 11.84 12.77 11.78 13.97Table 5: ROUGE-2 results using features incrementallycombined.Table 6 shows the feature ablation results, that is,each row means that the corresponding features are784excluded and the system uses all the other features.This set of experiments again shows that the externalfeatures like Word Embedding model based on largecorpus and Wiki resource are very useful.
Withoutusing them, the system has the biggest performancedegradation compared to the best result.2008 2009 2010 2011-Internal 11.34 12.41 11.42 13.71-Word Embedding 11.29 12.25 11.36 13.55-Wiki 11.35 12.38 11.38 13.58-LM 11.40 12.39 11.42 13.61-DBpedia 11.50 12.47 11.47 13.71-WordNet 11.67 12.64 11.64 13.80-SentiWordNet 11.75 12.67 11.70 13.90Table 6: ROUGE-2 results when leaving out each featuretype.5.2.4 Distribution of Correct Bigrams AfterFeature WeightingIn the next experiment we analyze the distribu-tion of the correct bigrams from the ranked bigramsusing different features in order to better evaluatetheir impact on bigram weighting.
We rank all thebigrams in descending order according to the esti-mated weight, then calculate the number of correctbigrams (i.e., the bigrams in human generated sum-mary) in Top10, 30, 50 and 80.
The more correctbigrams appear on the top of the list, the better ourfeatures estimate the importance of the bigrams.
Weconducted this experiment using four systems: thesystem only with internal features, only with WordEmbedding features, with combination of internaland Word Embedding features, and with all the fea-tures.
Figure 2 shows the results of this experi-ment on TAC 2008 data.
The pattern is similar onthe other three years?
data.
The results show thatsystems with better ROUGE-2 value indeed can as-sign higher weights to correct bigrams, allowing theILP decoding process to select these bigrams, whichleads to a better sentence selection.5.2.5 Joint Learning ResultsFinally we evaluate the effectiveness of our pro-posed joint learning approach.
For comparison, weimplement a pipeline method, where we use the bi-gram?s document frequency as the target value totrain a regression model, and during testing use the0510152025303540Top10 Top30 Top50 Top80Internal FeaturesWord Embedding FeaturesInternal and Word Embedding FeaturesFull FeaturesBigramCountFigure 2: Distribution of correct bigrams in Top-nweighted bigrams from four systems.model?s predicted value as the weight in the ILPframework.
Table 7 compares the results using thejoint learning method and this pipeline approach.We only show the results using the system with allthe features due to limited space.
We can see that ourjoint method outperforms the pipeline system basedon ROUGE-2 measurement, indicating that weightsare better learned in the joint process that takes intoaccount both bigram and sentence selection.System 2008 2009 2010 2011Pipeline System 11.60 12.64 11.56 13.65Joint Model 11.84 12.77 11.78 13.97Table 7: ROUGE-2 results using different training strate-gies.6 ConclusionsIn this paper, we adopt the ILP based summariza-tion framework, and propose methods to improvebigram concept selection and weighting.
We usesyntactic information to filter and select bigrams,various external resources to extract features, anda joint learning process for weight training.
Ourexperiments in the TAC data sets demonstrate thatour proposed methods outperform other state-of-the-art results.
Through the analysis, we foundthe external resources are helpful to estimate thebigram importance and thus improve the summa-rization performance.
While in summarization re-search, optimization-based methods have already ri-valed other approaches in performance, the task is785far from being solved.
Our analysis revealed thatthere are at least three points worth mentioning.First, using external resources contributes to the im-proved performance of our method compared to oth-ers that only use internal features.
Second, em-ploying and designing sophisticated features, espe-cially those that encode background knowledge orsemantic relationship like the word embedding fea-tures from a large corpus we used, will enable lan-guage concepts to be distinguished more easily inthe presence of a large number of candidates.
Third,one limitation of the use of the external resourcesis that they are not always available, such as thepairwise news articles along with the human gener-ated summaries, and the relevant Wiki pages.
Whilemuch recent work has focused on algorithmic de-velopment, the summarization task needs to have adeeper ?understanding?
of a document in order toreach the next level of performance.
Such an un-derstanding can be facilitated by the incorporationof background knowledge, which can lead to signif-icant summarization performance improvement, asdemonstrated in this study.AcknowledgmentsWe thank the anonymous reviewers for their detailedand insightful comments on earlier drafts of this pa-per.
The work is partially supported by NSF awardIIS-0845484 and DARPA Contract No.
FA8750-13-2-0041.
Any opinions, findings, and conclusions orrecommendations expressed are those of the authorsand do not necessarily reflect the views of the fund-ing agencies.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.
InProceedings of LREC.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.Sashka T. Davis, John M. Conroy, and Judith D.Schlesinger.
2012.
Occams - an optimal combina-torial covering algorithm for multi-document summa-rization.
In Proceedings of ICDM.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008.The ICSI summarization system at tac 2008.
In Pro-ceedings of TAC.Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
TheICSI/UTD summarization system at tac 2009.
In Pro-ceedings of TAC.Kazi Saidul Hasan and Vincent Ng.
2014.
Automatickeyphrase extraction: A survey of the state of the art.In Proceedings of ACL.Kai Hong and Ani Nenkova.
2014.
Improving the esti-mation of word importance for news multi-documentsummarization.
In Proceedings of EACL.Houping Jia, Xiaojiang Huang, Tengfei Ma, XiaojunWan, and Jianguo Xiao.
2010.
Pkutm participation attac 2010 rte and summarization track.
In Proceedingsof TAC.Jay J Jiang and David W Conrath.
1997.
Semantic simi-larity based on corpus statistics and lexical taxonomy.arXiv preprint cmp-lg/9709008.Samir Khuller, Anna Moss, and Joseph Seffi Naor.
1999.The budgeted maximum coverage problem.
Informa-tion Processing Letters.Judith L. Klavans and Min-Yen Kan. 1998.
Role of verbsin document analysis.
In Proceedings of the ACL.Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.2010.
An effective approach for aesop and guidedsummarization task.
In Proceedings of TAC.Chen Li, Fei Liu, Fuliang Weng, and Yang Liu.
2013a.Document summarization via guided sentence com-pression.
In Proceedings of the EMNLP.Chen Li, Xian Qian, and Yang Liu.
2013b.
Using super-vised bigram-based ilp for extractive summarization.In Proceedings of ACL.Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng.2014.
Improving multi-documents summarization bysentence compression based on expanded constituentparse trees.
In Proceedings of EMNLP.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submodularfunctions.
In Proceedings of NAACL.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML.Chin-Yew Lin.
2004.
Rouge: a package for automaticevaluation of summaries.
In Proceedings of ACL.Patrice Lopez and Laurent Romary.
2010.
Humb: Au-tomatic key term extraction from scientific articles ingrobid.
In Proceedings of the international workshopon semantic evaluation.Andre F. T. Martins and Noah A. Smith.
2009.
Summa-rization with a joint model for sentence extraction andcompression.
In Proceedings of the ACL Workshopon Integer Linear Programming for Natural LanguageProcessing.786Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Pro-ceedings of ECIR.Olena Medelyan, Eibe Frank, and Ian H Witten.2009.
Human-competitive tagging using automatickeyphrase extraction.
In Proceedings of the EMNLP.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
Proceedings of Workshop atICLR.Jun-Ping Ng, Praveen Bysani, Ziheng Lin, Min-Yen Kan,and Chew-Lim Tan.
2012.
Exploiting category-specific information for multi-document summariza-tion.
In Proceedings of COLING.Karolina Owczarzak, John M. Conroy, Hoa Trang Dang,and Ani Nenkova.
2012.
An assessment of the ac-curacy of automatic evaluation in summarization.
InProceedings of Workshop on Evaluation Metrics andSystem Comparison for Automatic Summarization.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
arXiv preprintcmp-lg/9511007.Hiroya Takamura and Manabu Okumura.
2009.
Textsummarization model based on maximum coverageproblem and its variant.
In Proceedings of EACL.Zhongyu Wei and Wei Gao.
2014.
Utilizing microblogsfor automatic news highlights extraction.
In Proceed-ings of COLING.Kristian Woodsend and Mirella Lapata.
2012.
Multipleaspect summarization using integer linear program-ming.
In Proceedings of EMNLP-CoNLL.Zhibiao Wu and Martha Palmer.
1994.
Verbs semanticsand lexical selection.
In Proceedings of ACL.Wen-Tau Yih, Joshua Goodman, and Vitor R Carvalho.2006.
Finding advertising keywords on web pages.In Proceedings of international conference on WorldWide Web.787
