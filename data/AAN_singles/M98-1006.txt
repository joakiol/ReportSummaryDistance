Using Collocation Statistics in Information ExtractionDekang LinDepartment of Computer ScienceUniversity of ManitobaWinnipeg, Manitoba, Canada R3T 2N2lindek@cs.umanitoba.caandNalante, Inc.7 Blackwood Bay, Winnipeg, Manitoba, Canadalindek@nalante.comINTRODUCTIONOur main objective in participating MUC-7 is to investigate and experiment with the use of col-location statistics in information extraction.
A collocation is a habitual word combination, suchas \weather a storm", \le a lawsuit", and \the falling yen".
Collocation statistics refers to thefrequency counts of the collocational relations extracted from a parsed corpus.
For example, out of6577 instances of \addition" in a corpus, 5190 was used as the object of \in".
Out of 3214 instancesof \hire", 12 of them take \alien" as the object.We participated in two tasks: Named Entity and Coreference.
In both tasks, the input textis processed in two passes.
During the rst pass we use the parse trees of input texts, combinedwith collocation statistics obtained from a large corpus, to automatically acquire or enrich lexicalentries which are then used in the second pass.COLLOCATION DATABASEWe dene a collocation to be a dependency triple that consists of three elds:(word, relation, relative)where the word eld is a word in a sentence, the relative eld can either be the modiee or amodier of word, and the relation eld species the type of the relationship between word andrelative as well as their parts of speech.For example, the dependency triples extracted from the sentence \I have a brown dog" are:(have V:subj:N I) (I N:r-subj:V have)(have V:comp1:N dog) (dog N:r-comp1:V have)(dog N:jnab:A brown) (brown A:r-jnab:N dog)(dog N:det:D a) (a D:r-det:N dog)1The identiers for the dependency types are explained in Table 1.Table 1: Dependency typesLabel Relationship between:N:det:D a noun and its determinerN:jnab:A a noun and its adjectival modierN:nn:N a noun and its nominal modierV:comp1:N a verb and its noun objectV:subj:N a verb and its subjectV:jvab:A a verb and its adverbial modierWe used MINIPAR, a descendent of PRINCIPAR [2], to parse a text corpus that is made upof 55-million-word Wall Street Journal and 45-million-word San Jose Mercury.
Two steps weretaken to reduce the number of errors in the parsed corpus.
Firstly, only sentences with no morethan 25 words are fed into the parser.
Secondly, only complete parses are included in the parsedcorpus.
The 100 million word text corpus is parsed in about 72 hours on a Pentium 200 with 80MBmemory.
There are about 22 million words in the parse trees.Figure 1 shows an example entry in the resulting collocation database.
Each entry contains ofall the dependency triples that have the same word eld.
The dependency triples in an entry aresorted rst in the order of the part of speech of their word elds, then the relation eld, and thenthe relative eld.The symbols used in Figure (1) are explained as follows.
Let X be a multiset.
The symbol kXkstands for the number of elements in X and jXj stands for the number of distinct elements in X. Forexample,a.
k(review, V:comp1:N, acquisition)k is the number of times \acquisition" is used as theobject of the verb \review".b.
k(review, *, *)k is the number of dependency triples in which the word eld is \review"(which can be a noun or a verb).c.
k(review, V:jvab:A, *)k is the number of times [vreview] is pre-modied by an adverb.d.
j(review, V:jvab:A, *)j is the number of distinct adverbs that were used as a pre-modierof [vreview].e.
k(*, *, *)k is the total number of dependency triples, which is twice the number of depen-dency relationships in the parsed corpus.f.
k(review, N)k is the number of times the word \review" is used as a noun.g.
k(*, N)k is the total number of occurrences of nouns.h.
j(*, N)j is the total number of distinct nouns that2review 8514V 1424V:subj:N 789 179administration 5appeals court 2... ...V:jvab:A 101 39briefly 2carefully 7formally 2... ...V:comp1:N 1353 384account 7acquisition 3action 5activity 2... ...N 1576N:r-subj:V 239 118affect 8become 2... ...N:r-comp1:V 525 157approve 3avoid 5await 3... ...N:nn:N 241 85admission 2bank 2book 4... ...N:jnp:P 76 9by 26for 28... ...N:jnab:A 518 182administrative 5annual 12antitrust 10... ...part of speech||(review, V:comp1:N, acquisition)||||(review, V:jvab:A, *)|||(review, V:jvab:A, *)|||(review, * ,*)||word-fieldrelation-fieldrelative-field"review" has been used as the objectsof these verbs in the corpusthese nouns were used as a prenominalmodifier of "review"||(review, N)||Figure 1: An example entry in the Collocation Database3i.
k(review, *)k is the total number of occurrences of the word \review" (used as any category)in the parsed corpus.NAMED ENTITY RECOGNITIONOur named entity recognizer is a nite-state pattern matcher, which was developed as part Univer-sity of Manitoba MUC-6 eort.
The pattern matcher has access to both lexical items and surfacestrings in the input text.
In MUC-7, we extended the earlier system in two ways: We extracted recognition rules automatically from the collocation database to augment themanually coded pattern rules. We treated the collocational context of words in the input texts as features and used aNaive-Bayes classier to categorized unknown proper names, which are then inserted into thesystems lexicon.A collocational context of a proper name is often a good indicator of its classication.
Forexample, in the 22-million-word corpus, there are 33 instances where a proper noun is used asa prenominal modier of \managing director".
In 26 of the 33 instances, the proper name wasclassied as an organization.
In the remaining 7 instances, the proper name was not classied.Therefore, if an unknown proper name is a prenominal modier of \managing director", it is likelyto refer to an organization.
We extracted 3623 such contexts in which the frequency of one typeof proper names is much greater (as dened by a rather arbitrary threshold) than the frequenciesof other types of proper names.
If a proper name occurs in one of these contexts, we can thenclassify it accordingly.
This use of the collocation database is equivalent to automatic generation ofclassication rules.
In fact, some of the collocational contexts are equivalent to pattern-matchingrules that were manually coded in the system.There are only a small number of collocational contexts in which the classication of a propername can be reliably determined.
In most cases, a clear decision cannot be reached based on a singlecollocational context.
For example, among 1504 objects of \convince", 49 of them were classiedas organizations, and 457 of them were classied as persons.
This suggests that if a proper nameis used as the object of \convince", it is likely that the name refers to a person.
However, there isalso signicant probability that the name refers to an organization.
Instead of making the decisionbased on this single piece of evidence, we collect from the input texts all the collocational contextsin which an unknown proper names occurred.
We then classify the the proper name with a naiveBayes classier, using the the set of collocation contexts as features.The naive Bayes classier uses a table to store the frequencies of proper name classes in col-locational contexts.
Sample entries of the frequency table are shown in Table 2.
Each row in thetable represents a collocation feature.
The rst column is a collocation feature.
Words with thisfeature have been observed to occur at position X in the second column.
The third to fth columnscontain the frequencies of dierent proper name classes.Let C be a class of proper name (C is one of LOC, ORG, or PER).
Let Fibe a collocationfeature.
Classication decision is made by nd the class C that maximizesQki=1P (FijC)P (C),4Table 2: Frequency of Collocation FeaturesCollocation Context Frequency CountsFeature Pattern LOC ORG PERcontrol|N:r-comp1:V to control X 9 87 39control|N:r-gen:N X's control 14 14 54control|N:r-nn:N the X control 6 0 0control|N:r-subj:V X to control 10 99 307control|N:subj:N X is the control 0 3 0convene|N:r-comp1:V to convene X 0 5 0convene|N:r-subj:V X to convene 0 10 18convention|N:r-gen:N X's convention 0 4 0convention|N:r-nn:N the X convention 5 23 5where F1; F2; : : : Fkare the features of an unknown proper name.
The probability P (FijC) isestimated by m-estimates [5], with m = 1 and p =1jCF jas the parameters, where CF is the set ofcollocation features:Pm(FijC) =kFi; Ck +1jCF jPf2CFkf; Ck + 1where kFi; Ck denotes the frequency of words that belong to C in the context represented by f .Example: The walkthrough article contains several occurrences of the word \Xichang" which isnot found in our lexicon.
The parser extracted the following set of collocation contexts from theformal testing corpus:1.
\the Xichang base", where Xichang is used as the prenominal modier of \base" (base|N:nn:N);2.
\the Xichang site", where Xichang is used as the prenominal modier of \site" (site|N:nn:N);3.
\the site in Xichang", from which two features are extracted: the object of \in" (in|P:pcomp:N); indirect modier of \site" via the preposition \in" (site|N:pnp-in:N).The frequencies of the features are shown in Table 3.
These features allowed the naive Bayesclassier to correctly classify \Xichang" as a locale.Automatically acquiring lexical information on they is an double edged sword.
On the onehand, it allows classication of proper names that would otherwise be unclassied.
On the otherhand, since there is no human conrmation, the correctness of the automatically acquired lexicalitems cannot be guaranteed.
When incorrect information is entered into the lexicon, a single errormay propagate to many places.
For example, during the development of our system, a combination5Table 3: Frequencies of features of \Xichang"Collocation Frequency CountsFeature LOC ORG PERbase|N:nn:N 77 19 0site|N:nn:N 26 16 34in|P:pcomp:N 35641 15630 0site|N:pnp-in:N 7 0 0of parser errors and the naive Bayes classication caused the word \I" to be added into the lexiconas a personal name.
During the second pass, 143 spurious personal names were generated.Our NE evaluation results are shown in Table 4.
The \pass1" results are obtained by manuallycoded patterns in conjunction with the classication rules automatically extracted from the collo-cation database.
With the naive Bayes classication, the recall is boosted by 6 percent while theprecision is decreased by 2% with an overall increase of F-measure by 2.67.Table 4: Evaluation results of the named entity taskPrecision Recall F-measurepass1 89% 79% 83.70ocial 87% 85% 86.37COREFERENCEOur coreference recognition subsystem used the same constraint-based model as our MUC-6 system.This model consists of an integrator and a set of independent modules, such as syntactic patterns(e.g., copula construction and appositive), string matching, binding theory, and centering heuristics.Each module proposes weighted assertions to the integrator.
There are two types of assertions.
Anequality assertion states that two noun phrases have the same referent.
An inequality assertionstates that two noun phrases must not have the same referent.
The modules are allowed to freelycontradict one another, or even themselves.
The integrator use the weights associated with theassertions to resolve the conicts.
A discourse model is constructed incrementally by the sequenceof assertions that are sorted in descending order of their weights.
When an assertion is consistentwith the current model, the model is modied accordingly.
Otherwise, the assertion is ignored andthe model remains the same.One of the important factors to determine whether or not two noun phrases may refer to thesame entity is their semantic compatibility.
A personal pronoun must refer to a person.
Forexample, the pronoun \it" may refer to an organization, an artifact, but not a person.
A \plane"may refer to an aircraft.
A \disaster" may refer to a crash.
In MUC-6, we used the WordNet to6determine the semantic compatibility and similarity between two noun phrases.
However, withoutthe ability to determine the intended sense of a word in the input text, we had to say that allsenses are possible.1The problem with this approach is that the WordNet, like any other generalpurpose lexical resource, aims at providing broad-coverage.
Consequently, it includes many usagesof words that are very rare in our domain of interest.
For example, one of the 8 potential sensesof \company" in WordNet 1.5 is a \visitor/visitant", which is a hyponym of \person".
This usageof the word practically never happens in newspaper articles.
However, its existence prevents us tomake assertions that personal pronouns like \she" cannot co-refer with \company".In MUC-7, we developed a word sense disambiguation (WSD) module, which removes some ofthe implausible senses from the list of potential senses.
It does not necessarily narrows down thepossible senses of a word instance to a single one, however.Given a polysemous word w in the input text, we take the following steps to narrow down thepossibilities for its intended meaning:1.
Retrieve collocational contexts of w from the parse trees of the input text.2.
For each collocational context of w, retrieve its set of collocates, i.e., the set of words thatoccurred in the same collocational context.
Take the union of all the sets of collocates of w.3.
Take the intersection of the union and the set of similar words of w which are extractedautomatically with the collocational database [4].
We call the words in the intersectionselectors.4.
Score the set of potential senses of w by computing the similarities between senses of w andsenses of the selectors in the WordNet [3].
Remove the senses of w that received a score lessthan 75% of the highest score.Example: consider the word \ghter" in the following context in the walkthrough article:... in the multibillion-dollar deals for ghter jets.WordNet lists three senses of \ghter": combatant, battler, disrupter champion, hero, defender, protector ghter aircraft, attack aircraftThe disambiguation of this word takes the following steps:1.
The parser recognized that \ghter" was used as the prenominal modier of \jet".7Table 5: Collocates of \ghter" as prenominal modier of \jet"Word Freq LogL Word Freq LogLghter 80 449.56 NUM 212 160.15ORG 187 59.56 air force 13 56.28passenger 17 51.93 Airbus 10 44.18Lear 6 37.79 Harrier 5 33.62PROD 14 30.08 -bound 3 22.68Concorde 4 22.22 Mirage 4 20.02Avianca 3 15.93 widebody 3 15.66stealth 4 10.43 turbofan 2 10.35MiG 2 10.35 KAL 2 9.23series 5 8.69 cargo 4 8.30Aeroot 2 8.16 four-engine 1 7.55Delta 3 7.53 steering 2 7.09CANADIENS 2 6.34 water 6 6.23NUM-passenger 1 6.17 Dragonair 1 6.17BLACKHAWKS 2 5.98 Skyhawk 1 5.65Egyptair 1 5.65 transport 3 5.63trainer 2 5.50 Coast guard 3 5.43Advanced Tactical Fighter 1 5.31 reconnaissance 2 5.12Qantas 1 5.05 Pan American 1 5.05training 3 4.97 United Express 1 4.85Gulfstream 1 4.85 Swissair 1 4.69PSA 1 4.69 ANA 1 4.69ground attack 1 4.54 NUM-seat 1 4.21Alitalia 1 4.12 Lufthansa 1 3.96PAL 1 3.89 KLM 1 3.89NUM Syrian 1 3.76 whirlpool 1 3.0382.
Retrieve words from the collocation database that were also used as the prenominal modierof \jet" (shown in Table 5).
Freq is the frequency of the word in the context, LogL is the loglikelihood ratio between the word and the context [1].3.
Retrieve the similar words of \ghter" from an automatically generated thesaurus:jet 0.15; guerrilla 0.14; aircraft 0.12; rebel 0.11; bomber 0.11; soldier 0.11; troop0.10; plane 0.10; missile 0.09; force 0.09; militia 0.09; helicopter 0.09; leader 0.08;civilian 0.07; faction 0.07; pilot 0.07; airplane 0.07; insurgent 0.07; commander 0.06;tank 0.06; airliner 0.05; militant 0.05; marine 0.05; transport 0.05; reconnaissance0.05; prisoner 0.05; artillery 0.05; army 0.05; stealth 0.05; victim 0.05; terrorist 0.05;weapon 0.04; rocket 0.04; resistance 0.04; rioter 0.04; gunboat 0.04; collaborator0.04; assailant 0.04; thousand 0.04; gunman 0.04; sympathizer 0.04; radio 0.04;submarine 0.04; attacker 0.04; youth 0.04; camp 0.04; refugee 0.04; dependent 0.04;combat 0.04; mechanic 0.04; demonstrator 0.04; personnel 0.04; movement 0.04;gunner 0.04; territory 0.04The number after a word is the similarity between the word and \ghter".
The intersectionof the similar word list and the above table consists of:combat 0.04; reconnaissance 0.05; stealth 0.05; transport 0.05;4.
Find a sense of \ghter" in WordNet that is most similar to senses of \combat", \reconnais-sance", \stealth" or \transport".
The \ghter aircraft" sense of \ghter" was selected.We submitted two sets of results in MUC-7: the \nowsd" result in which the senses of a word are chosen simply by choosing its rst twosenses in the WordNet. the ocial result that employs the above word sense disambiguation algorithm.The results are summarized in Table 6.
Although the dierence between the use of WSD and thebaseline is quite small, it turns out to be statistically signicant.
In some of the 20 input texts thatwere scored in coreference evaluation, the WSD module did not make any dierence.
However,whenever there was a dierence it was always an improvement.
It is also worth noting that, withWSD, both the recall and precision are increased.Table 6: Coreference recognition resultsPrecision Recall F-measurenowsd 62.7% 57.5% 60.0ocial 64.2% 58.2% 61.11In hindsight, we probably should have just used the rst sense listed in the WordNet for each word.9CONCLUSIONSThe use of collocational statistics greatly improved the performance of our named entity recognitionsystem.
Although collocation-based Word Sense Disambiguation lead only to a small improvementin coreference recognition, the dierence is nonetheless statistically signicant.ACKNOWLEDGEMENTSThis research is supported by NSERC Research Grant OGP121338 and a research contract awardedto Nalante Inc. by Communications Security Establishment.REFERENCES[1] Ted Dunning.
Accurate methods for the statistics of surprise and coincidence.
ComputationalLinguistics, 19(1):61{74, March 1993.
[2] Dekang Lin.
Principle-based parsing without overgeneration.
In Proceedings of ACL{93, pages112{120, Columbus, Ohio, 1993.
[3] Dekang Lin.
Using syntactic dependency as local context to resolve word sense ambiguity.
InProceedings of ACL/EACL-97, pages 64{71, Madrid, Spain, July 1997.
[4] Dekang Lin.
Automatic retrieval and clustering of similar words.
In Proceedings of COLING-ACL '98, pages 768{774, Montreal, Canada, August 1998.
[5] Tom M. Mitchell.
Machine Learning.
McGraw-Hill, 1997.10
