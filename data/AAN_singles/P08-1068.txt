Proceedings of ACL-08: HLT, pages 595?603,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSimple Semi-supervised Dependency ParsingTerry Koo, Xavier Carreras, and Michael CollinsMIT CSAIL, Cambridge, MA 02139, USA{maestro,carreras,mcollins}@csail.mit.eduAbstractWe present a simple and effective semi-supervised method for training dependencyparsers.
We focus on the problem of lex-ical representation, introducing features thatincorporate word clusters derived from a largeunannotated corpus.
We demonstrate the ef-fectiveness of the approach in a series of de-pendency parsing experiments on the PennTreebank and Prague Dependency Treebank,and we show that the cluster-based featuresyield substantial gains in performance acrossa wide range of conditions.
For example, inthe case of English unlabeled second-orderparsing, we improve from a baseline accu-racy of 92.02% to 93.16%, and in the caseof Czech unlabeled second-order parsing, weimprove from a baseline accuracy of 86.13%to 87.13%.
In addition, we demonstrate thatour method also improves performance whensmall amounts of training data are available,and can roughly halve the amount of super-vised data required to reach a desired level ofperformance.1 IntroductionIn natural language parsing, lexical information isseen as crucial to resolving ambiguous relationships,yet lexicalized statistics are sparse and difficult to es-timate directly.
It is therefore attractive to considerintermediate entities which exist at a coarser levelthan the words themselves, yet capture the informa-tion necessary to resolve the relevant ambiguities.In this paper, we introduce lexical intermediariesvia a simple two-stage semi-supervised approach.First, we use a large unannotated corpus to defineword clusters, and then we use that clustering toconstruct a new cluster-based feature mapping fora discriminative learner.
We are thus relying on theability of discriminative learning methods to identifyand exploit informative features while remaining ag-nostic as to the origin of such features.
To demon-strate the effectiveness of our approach, we conductexperiments in dependency parsing, which has beenthe focus of much recent research?e.g., see workin the CoNLL shared tasks on dependency parsing(Buchholz and Marsi, 2006; Nivre et al, 2007).The idea of combining word clusters with dis-criminative learning has been previously exploredby Miller et al (2004), in the context of named-entity recognition, and their work directly inspiredour research.
However, our target task of depen-dency parsing involves more complex structured re-lationships than named-entity tagging; moreover, itis not at all clear that word clusters should have anyrelevance to syntactic structure.
Nevertheless, ourexperiments demonstrate that word clusters can bequite effective in dependency parsing applications.In general, semi-supervised learning can be mo-tivated by two concerns: first, given a fixed amountof supervised data, we might wish to leverage ad-ditional unlabeled data to facilitate the utilization ofthe supervised corpus, increasing the performance ofthe model in absolute terms.
Second, given a fixedtarget performance level, we might wish to use un-labeled data to reduce the amount of annotated datanecessary to reach this target.We show that our semi-supervised approachyields improvements for fixed datasets by perform-ing parsing experiments on the Penn Treebank (Mar-cus et al, 1993) and Prague Dependency Treebank(Hajic?, 1998; Hajic?
et al, 2001) (see Sections 4.1and 4.3).
By conducting experiments on datasets ofvarying sizes, we demonstrate that for fixed levels ofperformance, the cluster-based approach can reducethe need for supervised data by roughly half, whichis a substantial savings in data-annotation costs (seeSections 4.2 and 4.4).The remainder of this paper is divided as follows:595Ms.
Haag plays Elianti .
*objprootnmod sbjFigure 1: An example of a labeled dependency tree.
Thetree contains a special token ?*?
which is always the rootof the tree.
Each arc is directed from head to modifier andhas a label describing the function of the attachment.Section 2 gives background on dependency parsingand clustering, Section 3 describes the cluster-basedfeatures, Section 4 presents our experimental results,Section 5 discusses related work, and Section 6 con-cludes with ideas for future research.2 Background2.1 Dependency parsingRecent work (Buchholz and Marsi, 2006; Nivreet al, 2007) has focused on dependency parsing.Dependency syntax represents syntactic informa-tion as a network of head-modifier dependency arcs,typically restricted to be a directed tree (see Fig-ure 1 for an example).
Dependency parsing dependscritically on predicting head-modifier relationships,which can be difficult due to the statistical sparsityof these word-to-word interactions.
Bilexical depen-dencies are thus ideal candidates for the applicationof coarse word proxies such as word clusters.In this paper, we take a part-factored structuredclassification approach to dependency parsing.
For agiven sentence x, let Y(x) denote the set of possibledependency structures spanning x, where each y ?Y(x) decomposes into a set of ?parts?
r ?
y.
In thesimplest case, these parts are the dependency arcsthemselves, yielding a first-order or ?edge-factored?dependency parsing model.
In higher-order parsingmodels, the parts can consist of interactions betweenmore than two words.
For example, the parser ofMcDonald and Pereira (2006) defines parts for sib-ling interactions, such as the trio ?plays?, ?Elianti?,and ?.?
in Figure 1.
The Carreras (2007) parserhas parts for both sibling interactions and grandpar-ent interactions, such as the trio ?
*?, ?plays?, and?Haag?
in Figure 1.
These kinds of higher-orderfactorizations allow dependency parsers to obtain alimited form of context-sensitivity.Given a factorization of dependency structuresinto parts, we restate dependency parsing as the fol-apple pear Apple IBM bought run of in01100 101 110 111000 001 010 01100010111Figure 2: An example of a Brown word-cluster hierarchy.Each node in the tree is labeled with a bit-string indicat-ing the path from the root node to that node, where 0indicates a left branch and 1 indicates a right branch.lowing maximization:PARSE(x;w) = argmaxy?Y(x)?r?yw ?
f(x, r)Above, we have assumed that each part is scoredby a linear model with parameters w and feature-mapping f(?).
For many different part factoriza-tions and structure domains Y(?
), it is possible tosolve the above maximization efficiently, and severalrecent efforts have concentrated on designing newmaximization algorithms with increased context-sensitivity (Eisner, 2000; McDonald et al, 2005b;McDonald and Pereira, 2006; Carreras, 2007).2.2 Brown clustering algorithmIn order to provide word clusters for our exper-iments, we used the Brown clustering algorithm(Brown et al, 1992).
We chose to work with theBrown algorithm due to its simplicity and prior suc-cess in other NLP applications (Miller et al, 2004;Liang, 2005).
However, we expect that our approachcan function with other clustering algorithms (as in,e.g., Li and McCallum (2005)).
We briefly describethe Brown algorithm below.The input to the algorithm is a vocabulary ofwords to be clustered and a corpus of text containingthese words.
Initially, each word in the vocabularyis considered to be in its own distinct cluster.
The al-gorithm then repeatedly merges the pair of clusterswhich causes the smallest decrease in the likelihoodof the text corpus, according to a class-based bigramlanguage model defined on the word clusters.
Bytracing the pairwise merge operations, one obtainsa hierarchical clustering of the words, which can berepresented as a binary tree as in Figure 2.Within this tree, each word is uniquely identifiedby its path from the root, and this path can be com-pactly represented with a bit string, as in Figure 2.In order to obtain a clustering of the words, we se-lect all nodes at a certain depth from the root of the596hierarchy.
For example, in Figure 2 we might selectthe four nodes at depth 2 from the root, yielding theclusters {apple,pear}, {Apple,IBM}, {bought,run},and {of,in}.
Note that the same clustering can be ob-tained by truncating each word?s bit-string to a 2-bitprefix.
By using prefixes of various lengths, we canproduce clusterings of different granularities (Milleret al, 2004).For all of the experiments in this paper, we usedthe Liang (2005) implementation of the Brown algo-rithm to obtain the necessary word clusters.3 Feature designKey to the success of our approach is the use of fea-tures which allow word-cluster-based information toassist the parser.
The feature sets we used are simi-lar to other feature sets in the literature (McDonaldet al, 2005a; Carreras, 2007), so we will not attemptto give a exhaustive description of the features inthis section.
Rather, we describe our features at ahigh level and concentrate on our methodology andmotivations.
In our experiments, we employed twodifferent feature sets: a baseline feature set whichdraws upon ?normal?
information sources such asword forms and parts of speech, and a cluster-basedfeature set that also uses information derived fromthe Brown cluster hierarchy.3.1 Baseline featuresOur first-order baseline feature set is similar to thefeature set of McDonald et al (2005a), and consistsof indicator functions for combinations of words andparts of speech for the head and modifier of eachdependency, as well as certain contextual tokens.1Our second-order baseline features are the same asthose of Carreras (2007) and include indicators fortriples of part of speech tags for sibling interactionsand grandparent interactions, as well as additionalbigram features based on pairs of words involvedthese higher-order interactions.
Examples of base-line features are provided in Table 1.1We augment the McDonald et al (2005a) feature set withbacked-off versions of the ?Surrounding Word POS Features?that include only one neighboring POS tag.
We also add binneddistance features which indicate whether the number of tokensbetween the head and modifier of a dependency is greater than2, 5, 10, 20, 30, or 40 tokens.Baseline Cluster-basedht,mt hc4,mc4hw,mw hc6,mc6hw,ht,mt hc*,mc*hw,ht,mw hc4,mtht,mw,mt ht,mc4hw,mw,mt hc6,mthw,ht,mw,mt ht,mc6?
?
?
hc4,mwhw,mc4?
?
?ht,mt,st hc4,mc4,sc4ht,mt,gt hc6,mc6,sc6?
?
?
ht,mc4,sc4hc4,mc4,gc4?
?
?Table 1: Examples of baseline and cluster-based featuretemplates.
Each entry represents a class of indicators fortuples of information.
For example, ?ht,mt?
representsa class of indicator features with one feature for each pos-sible combination of head POS-tag and modifier POS-tag.
Abbreviations: ht = head POS, hw = head word,hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,hc* = full bit string of head; mt,mw,mc4,mc6,mc* =likewise for modifier; st,gt,sc4,gc4,.
.
.
= likewisefor sibling and grandchild.3.2 Cluster-based featuresThe first- and second-order cluster-based feature setsare supersets of the baseline feature sets: they in-clude all of the baseline feature templates, and addan additional layer of features that incorporate wordclusters.
Following Miller et al (2004), we use pre-fixes of the Brown cluster hierarchy to produce clus-terings of varying granularity.
We found that it wasnontrivial to select the proper prefix lengths for thedependency parsing task; in particular, the prefixlengths used in the Miller et al (2004) work (be-tween 12 and 20 bits) performed poorly in depen-dency parsing.2 After experimenting with many dif-ferent feature configurations, we eventually settledon a simple but effective methodology.First, we found that it was helpful to employ twodifferent types of word clusters:1.
Short bit-string prefixes (e.g., 4?6 bits), whichwe used as replacements for parts of speech.2One possible explanation is that the kinds of distinctionsrequired in a named-entity recognition task (e.g., ?Alice?
versus?Intel?)
are much finer-grained than the kinds of distinctionsrelevant to syntax (e.g., ?apple?
versus ?eat?).5972.
Full bit strings,3 which we used as substitutesfor word forms.Using these two types of clusters, we generated newfeatures by mimicking the template structure of theoriginal baseline features.
For example, the baselinefeature set includes indicators for word-to-word andtag-to-tag interactions between the head and mod-ifier of a dependency.
In the cluster-based featureset, we correspondingly introduce new indicators forinteractions between pairs of short bit-string pre-fixes and pairs of full bit strings.
Some examplesof cluster-based features are given in Table 1.Second, we found it useful to concentrate on?hybrid?
features involving, e.g., one bit-string andone part of speech.
In our initial attempts, we fo-cused on features that used cluster information ex-clusively.
While these cluster-only features providedsome benefit, we found that adding hybrid featuresresulted in even greater improvements.
One possibleexplanation is that the clusterings generated by theBrown algorithm can be noisy or only weakly rele-vant to syntax; thus, the clusters are best exploitedwhen ?anchored?
to words or parts of speech.Finally, we found it useful to impose a form ofvocabulary restriction on the cluster-based features.Specifically, for any feature that is predicated on aword form, we eliminate this feature if the wordin question is not one of the top-N most frequentwords in the corpus.
When N is between roughly100 and 1,000, there is little effect on the perfor-mance of the cluster-based feature sets.4 In addition,the vocabulary restriction reduces the size of the fea-ture sets to managable proportions.4 ExperimentsIn order to evaluate the effectiveness of the cluster-based feature sets, we conducted dependency pars-ing experiments in English and Czech.
We test thefeatures in a wide range of parsing configurations,including first-order and second-order parsers, andlabeled and unlabeled parsers.53As in Brown et al (1992), we limit the clustering algorithmso that it recovers at most 1,000 distinct bit-strings; thus full bitstrings are not equivalent to word forms.4We used N = 800 for all experiments in this paper.5In an ?unlabeled?
parser, we simply ignore dependency la-bel information, which is a common simplification.The English experiments were performed on thePenn Treebank (Marcus et al, 1993), using a stan-dard set of head-selection rules (Yamada and Mat-sumoto, 2003) to convert the phrase structure syn-tax of the Treebank to a dependency tree represen-tation.6 We split the Treebank into a training set(Sections 2?21), a development set (Section 22), andseveral test sets (Sections 0,7 1, 23, and 24).
Thedata partition and head rules were chosen to matchprevious work (Yamada and Matsumoto, 2003; Mc-Donald et al, 2005a; McDonald and Pereira, 2006).The part of speech tags for the development and testdata were automatically assigned by MXPOST (Rat-naparkhi, 1996), where the tagger was trained onthe entire training corpus; to generate part of speechtags for the training data, we used 10-way jackknif-ing.8 English word clusters were derived from theBLLIP corpus (Charniak et al, 2000), which con-tains roughly 43 million words of Wall Street Jour-nal text.9The Czech experiments were performed on thePrague Dependency Treebank 1.0 (Hajic?, 1998;Hajic?
et al, 2001), which is directly annotatedwith dependency structures.
To facilitate compar-isons with previous work (McDonald et al, 2005b;McDonald and Pereira, 2006), we used the train-ing/development/test partition defined in the corpusand we also used the automatically-assigned part ofspeech tags provided in the corpus.10 Czech wordclusters were derived from the raw text section ofthe PDT 1.0, which contains about 39 million wordsof newswire text.11We trained the parsers using the averaged percep-tron (Freund and Schapire, 1999; Collins, 2002),which represents a balance between strong perfor-mance and fast training times.
To select the number6We used Joakim Nivre?s ?Penn2Malt?
conversion tool(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html).
Depen-dency labels were obtained via the ?Malt?
hard-coded setting.7For computational reasons, we removed a single 249-wordsentence from Section 0.8That is, we tagged each fold with the tagger trained on theother 9 folds.9We ensured that the sentences of the Penn Treebank wereexcluded from the text used for the clustering.10Following Collins et al (1999), we used a coarsened ver-sion of the Czech part of speech tags; this choice also matchesthe conditions of previous work (McDonald et al, 2005b; Mc-Donald and Pereira, 2006).11This text was disjoint from the training and test corpora.598Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L00 90.48 91.57 (+1.09) ?
91.76 92.77 (+1.01) ?
90.29 91.03 (+0.74) 91.33 92.09 (+0.76)01 91.31 92.43 (+1.12) ?
92.46 93.34 (+0.88) ?
90.84 91.73 (+0.89) 91.94 92.65 (+0.71)23 90.84 92.23 (+1.39) 90.9 92.02 93.16 (+1.14) 91.5 90.32 91.24 (+0.92) 91.38 92.14 (+0.76)24 89.67 91.30 (+1.63) ?
90.92 91.85 (+0.93) ?
89.55 90.06 (+0.51) 90.42 91.18 (+0.76)Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24.
Abbreviations: dep1/dep1c = first-order parser withbaseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-Donald et al (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser.
Unlabeled parsers are scoredusing unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions.
Improvements ofcluster-based features over baseline features are shown in parentheses.of iterations of perceptron training, we performed upto 30 iterations and chose the iteration which opti-mized accuracy on the development set.
Our featuremappings are quite high-dimensional, so we elimi-nated all features which occur only once in the train-ing data.
The resulting models still had very highdimensionality, ranging from tens of millions to asmany as a billion features.12All results presented in this section are givenin terms of parent-prediction accuracy, which mea-sures the percentage of tokens that are attached tothe correct head token.
For labeled dependencystructures, both the head token and dependency labelmust be correctly predicted.
In addition, in Englishparsing we ignore the parent-predictions of punc-tuation tokens,13 and in Czech parsing we retainthe punctuation tokens; this matches previous work(Yamada and Matsumoto, 2003; McDonald et al,2005a; McDonald and Pereira, 2006).4.1 English main resultsIn our English experiments, we tested eight differ-ent parsing configurations, representing all possi-ble choices between baseline or cluster-based fea-ture sets, first-order (Eisner, 2000) or second-order(Carreras, 2007) factorizations, and labeled or unla-beled parsing.Table 2 compiles our final test results and alsoincludes two results from previous work by Mc-Donald et al (2005a) and McDonald and Pereira(2006), for the purposes of comparison.
We notea few small differences between our parsers and the12Due to the sparsity of the perceptron updates, however,only a small fraction of the possible features were active in ourtrained models.13A punctuation token is any token whose gold-standard partof speech tag is one of {??
??
: , .
}.parsers evaluated in this previous work.
First, theMD1 and MD2 parsers were trained via the MIRAalgorithm (Crammer and Singer, 2003; Crammer etal., 2004), while we use the averaged perceptron.
Inaddition, the MD2 model uses only sibling interac-tions, whereas the dep2/dep2c parsers include bothsibling and grandparent interactions.There are some clear trends in the results of Ta-ble 2.
First, performance increases with the order ofthe parser: edge-factored models (dep1 and MD1)have the lowest performance, adding sibling rela-tionships (MD2) increases performance, and addinggrandparent relationships (dep2) yields even betteraccuracies.
Similar observations regarding the ef-fect of model order have also been made by Carreras(2007).Second, note that the parsers using cluster-basedfeature sets consistently outperform the models us-ing the baseline features, regardless of model orderor label usage.
Some of these improvements can bequite large; for example, a first-order model usingcluster-based features generally performs as well asa second-order model using baseline features.
More-over, the benefits of cluster-based feature sets com-bine additively with the gains of increasing modelorder.
For example, consider the unlabeled parsersin Table 2: on Section 23, increasing the model or-der from dep1 to dep2 results in a relative reductionin error of roughly 13%, while introducing cluster-based features from dep2 to dep2c yields an addi-tional relative error reduction of roughly 14%.
As afinal note, all 16 comparisons between cluster-basedfeatures and baseline features shown in Table 2 arestatistically significant.1414We used the sign test at the sentence level.
The comparisonbetween dep1-L and dep1c-L is significant at p < 0.05, and allother comparisons are significant at p < 0.0005.599Tagger always trained on full Treebank Tagger trained on reduced datasetSize dep1 dep1c ?
dep2 dep2c ?1k 84.54 85.90 1.36 86.29 87.47 1.182k 86.20 87.65 1.45 87.67 88.88 1.214k 87.79 89.15 1.36 89.22 90.46 1.248k 88.92 90.22 1.30 90.62 91.55 0.9316k 90.00 91.27 1.27 91.27 92.39 1.1232k 90.74 92.18 1.44 92.05 93.36 1.31All 90.89 92.33 1.44 92.42 93.30 0.88Size dep1 dep1c ?
dep2 dep2c ?1k 80.49 84.06 3.57 81.95 85.33 3.382k 83.47 86.04 2.57 85.02 87.54 2.524k 86.53 88.39 1.86 87.88 89.67 1.798k 88.25 89.94 1.69 89.71 91.37 1.6616k 89.66 91.03 1.37 91.14 92.22 1.0832k 90.78 92.12 1.34 92.09 93.21 1.12All 90.89 92.33 1.44 92.42 93.30 0.88Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22.
Abbreviations: Size = #sentences intraining corpus; ?
= difference between cluster-based and baseline features; other abbreviations are as in Table 2.4.2 English learning curvesWe performed additional experiments to evaluate theeffect of the cluster-based features as the amountof training data is varied.
Note that the depen-dency parsers we use require the input to be taggedwith parts of speech; thus the quality of the part-of-speech tagger can have a strong effect on the per-formance of the parser.
In these experiments, weconsider two possible scenarios:1.
The tagger has a large training corpus, whilethe parser has a smaller training corpus.
Thisscenario can arise when tagged data is cheaperto obtain than syntactically-annotated data.2.
The same amount of labeled data is availablefor training both tagger and parser.Table 3 displays the accuracy of first- and second-order models when trained on smaller portions ofthe Treebank, in both scenarios described above.Note that the cluster-based features obtain consistentgains regardless of the size of the training set.
Whenthe tagger is trained on the reduced-size datasets,the gains of cluster-based features are more pro-nounced, but substantial improvements are obtainedeven when the tagger is accurate.It is interesting to consider the amount by whichcluster-based features reduce the need for superviseddata, given a desired level of accuracy.
Based onTable 3, we can extrapolate that cluster-based fea-tures reduce the need for supervised data by roughlya factor of 2.
For example, the performance of thedep1c and dep2c models trained on 1k sentences isroughly the same as the performance of the dep1and dep2 models, respectively, trained on 2k sen-tences.
This approximate data-halving effect can beobserved throughout the results in Table 3.When combining the effects of model order andcluster-based features, the reductions in the amountof supervised data required are even larger.
For ex-ample, in scenario 1 the dep2c model trained on 1ksentences is close in performance to the dep1 modeltrained on 4k sentences, and the dep2c model trainedon 4k sentences is close to the dep1 model trained onthe entire training set (roughly 40k sentences).4.3 Czech main resultsIn our Czech experiments, we considered only unla-beled parsing,15 leaving four different parsing con-figurations: baseline or cluster-based features andfirst-order or second-order parsing.
Note that ourfeature sets were originally tuned for English pars-ing, and except for the use of Czech clusters, wemade no attempt to retune our features for Czech.Czech dependency structures may contain non-projective edges, so we employ a maximum directedspanning tree algorithm (Chu and Liu, 1965; Ed-monds, 1967; McDonald et al, 2005b) as our first-order parser for Czech.
For the second-order pars-ing experiments, we used the Carreras (2007) parser.Since this parser only considers projective depen-dency structures, we ?projectivized?
the PDT 1.0training set by finding, for each sentence, the pro-jective tree which retains the most correct dependen-cies; our second-order parsers were then trained withrespect to these projective trees.
The developmentand test sets were not projectivized, so our second-order parser is guaranteed to make errors in test sen-tences containing non-projective dependencies.
Toovercome this, McDonald and Pereira (2006) use a15We leave labeled parsing experiments to future work.600dep1 dep1c dep2 dep2c84.49 86.07 (+1.58) 86.13 87.13 (+1.00)Table 4: Parent-prediction accuracies of unlabeled Czechparsers on the PDT 1.0 test set, for baseline features andcluster-based features.
Abbreviations are as in Table 2.Parser AccuracyNivre and Nilsson (2005) 80.1McDonald et al (2005b) 84.4Hall and Nova?k (2005) 85.1McDonald and Pereira (2006) 85.2dep1c 86.07dep2c 87.13Table 5: Unlabeled parent-prediction accuracies of Czechparsers on the PDT 1.0 test set, for our models and forprevious work.Size dep1 dep1c ?
dep2 dep2c ?1k 72.79 73.66 0.87 74.35 74.63 0.282k 74.92 76.23 1.31 76.63 77.60 0.974k 76.87 78.14 1.27 78.34 79.34 1.008k 78.17 79.83 1.66 79.82 80.98 1.1616k 80.60 82.44 1.84 82.53 83.69 1.1632k 82.85 84.65 1.80 84.66 85.81 1.1564k 84.20 85.98 1.78 86.01 87.11 1.10All 84.36 86.09 1.73 86.09 87.26 1.17Table 6: Parent-prediction accuracies of unlabeled Czechparsers on the PDT 1.0 development set.
Abbreviationsare as in Table 3.two-stage approximate decoding process in whichthe output of their second-order parser is ?deprojec-tivized?
via greedy search.
For simplicity, we didnot implement a deprojectivization stage on top ofour second-order parser, but we conjecture that suchtechniques may yield some additional performancegains; we leave this to future work.Table 4 gives accuracy results on the PDT 1.0test set for our unlabeled parsers.
As in the En-glish experiments, there are clear trends in the re-sults: parsers using cluster-based features outper-form parsers using baseline features, and second-order parsers outperform first-order parsers.
Both ofthe comparisons between cluster-based and baselinefeatures in Table 4 are statistically significant.16 Ta-ble 5 compares accuracy results on the PDT 1.0 testset for our parsers and several other recent papers.16We used the sign test at the sentence level; both compar-isons are significant at p < 0.0005.N dep1 dep1c dep2 dep2c100 89.19 92.25 90.61 93.14200 90.03 92.26 91.35 93.18400 90.31 92.32 91.72 93.20800 90.62 92.33 91.89 93.301600 90.87 ?
92.20 ?All 90.89 ?
92.42 ?Table 7: Parent-prediction accuracies of unlabeled En-glish parsers on Section 22.
Abbreviations: N = thresh-old value; other abbreviations are as in Table 2.
Wedid not train cluster-based parsers using threshold valueslarger than 800 due to computational limitations.dep1-P dep1c-P dep1 dep2-P dep2c-P dep277.19 90.69 90.89 86.73 91.84 92.42Table 8: Parent-prediction accuracies of unlabeled En-glish parsers on Section 22.
Abbreviations: suffix -P =model without POS; other abbreviations are as in Table 2.4.4 Czech learning curvesAs in our English experiments, we performed addi-tional experiments on reduced sections of the PDT;the results are shown in Table 6.
For simplicity, wedid not retrain a tagger for each reduced dataset,so we always use the (automatically-assigned) partof speech tags provided in the corpus.
Note thatthe cluster-based features obtain improvements at alltraining set sizes, with data-reduction factors simi-lar to those observed in English.
For example, thedep1c model trained on 4k sentences is roughly asgood as the dep1 model trained on 8k sentences.4.5 Additional resultsHere, we present two additional results which fur-ther explore the behavior of the cluster-based fea-ture sets.
In Table 7, we show the development-setperformance of second-order parsers as the thresh-old for lexical feature elimination (see Section 3.2)is varied.
Note that the performance of cluster-basedfeatures is fairly insensitive to the threshold value,whereas the performance of baseline features clearlydegrades as the vocabulary size is reduced.In Table 8, we show the development-set perfor-mance of the first- and second-order parsers whenfeatures containing part-of-speech-based informa-tion are eliminated.
Note that the performance ob-tained by using clusters without parts of speech isclose to the performance of the baseline features.6015 Related WorkAs mentioned earlier, our approach was inspired bythe success of Miller et al (2004), who demon-strated the effectiveness of using word clusters asfeatures in a discriminative learning approach.
Ourresearch, however, applies this technique to depen-dency parsing rather than named-entity recognition.In this paper, we have focused on developing newrepresentations for lexical information.
Previous re-search in this area includes several models which in-corporate hidden variables (Matsuzaki et al, 2005;Koo and Collins, 2005; Petrov et al, 2006; Titovand Henderson, 2007).
These approaches have theadvantage that the model is able to learn differentusages for the hidden variables, depending on thetarget problem at hand.
Crucially, however, thesemethods do not exploit unlabeled data when learn-ing their representations.Wang et al (2005) used distributional similarityscores to smooth a generative probability model fordependency parsing and obtained improvements ina Chinese parsing task.
Our approach is similar totheirs in that the Brown algorithm produces clustersbased on distributional similarity, and the cluster-based features can be viewed as being a kind of?backed-off?
version of the baseline features.
How-ever, our work is focused on discriminative learningas opposed to generative models.Semi-supervised phrase structure parsing hasbeen previously explored by McClosky et al (2006),who applied a reranked parser to a large unsuper-vised corpus in order to obtain additional train-ing data for the parser; this self-training appraochwas shown to be quite effective in practice.
How-ever, their approach depends on the usage of ahigh-quality parse reranker, whereas the method de-scribed here simply augments the features of an ex-isting parser.
Note that our two approaches are com-patible in that we could also design a reranker andapply self-training techniques on top of the cluster-based features.6 ConclusionsIn this paper, we have presented a simple but effec-tive semi-supervised learning approach and demon-strated that it achieves substantial improvement overa competitive baseline in two broad-coverage depen-dency parsing tasks.
Despite this success, there areseveral ways in which our approach might be im-proved.To begin, recall that the Brown clustering algo-rithm is based on a bigram language model.
Intu-itively, there is a ?mismatch?
between the kind oflexical information that is captured by the Brownclusters and the kind of lexical information that ismodeled in dependency parsing.
A natural avenuefor further research would be the development ofclustering algorithms that reflect the syntactic be-havior of words; e.g., an algorithm that attempts tomaximize the likelihood of a treebank, according toa probabilistic dependency model.
Alternately, onecould design clustering algorithms that cluster entirehead-modifier arcs rather than individual words.Another idea would be to integrate the cluster-ing algorithm into the training algorithm in a limitedfashion.
For example, after training an initial parser,one could parse a large amount of unlabeled text anduse those parses to improve the quality of the clus-ters.
These improved clusters can then be used toretrain an improved parser, resulting in an overallalgorithm similar to that of McClosky et al (2006).Setting aside the development of new clusteringalgorithms, a final area for future work is the exten-sion of our method to new domains, such as con-versational text or other languages, and new NLPproblems, such as machine translation.AcknowledgmentsThe authors thank the anonymous reviewers fortheir insightful comments.
Many thanks also toPercy Liang for providing his implementation ofthe Brown algorithm, and Ryan McDonald for hisassistance with the experimental setup.
The au-thors gratefully acknowledge the following sourcesof support.
Terry Koo was funded by NSF grantDMS-0434222 and a grant from NTT, Agmt.
Dtd.6/21/1998.
Xavier Carreras was supported by theCatalan Ministry of Innovation, Universities andEnterprise, and a grant from NTT, Agmt.
Dtd.6/21/1998.
Michael Collins was funded by NSFgrants 0347631 and DMS-0434222.602ReferencesP.F.
Brown, V.J.
Della Pietra, P.V.
deSouza, J.C. Lai,and R.L.
Mercer.
1992.
Class-Based n-gram Mod-els of Natural Language.
Computational Linguistics,18(4):467?479.S.
Buchholz and E. Marsi.
2006.
CoNLL-X Shared Taskon Multilingual Dependency Parsing.
In Proceedingsof CoNLL, pages 149?164.X.
Carreras.
2007.
Experiments with a Higher-OrderProjective Dependency Parser.
In Proceedings ofEMNLP-CoNLL, pages 957?961.E.
Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.2000.
BLLIP 1987?89 WSJ Corpus Release 1, LDCNo.
LDC2000T43.
Linguistic Data Consortium.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.M.
Collins, J.
Hajic?, L. Ramshaw, and C. Tillmann.
1999.A Statistical Parser for Czech.
In Proceedings of ACL,pages 505?512.M.
Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proceedings ofEMNLP, pages 1?8.K.
Crammer and Y.
Singer.
2003.
Ultraconservative On-line Algorithms for Multiclass Problems.
Journal ofMachine Learning Research, 3:951?991.K.
Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.2004.
Online Passive-Aggressive Algorithms.
InS.
Thrun, L. Saul, and B. Scho?lkopf, editors, NIPS 16,pages 1229?1236.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.J.
Eisner.
2000.
Bilexical Grammars and Their Cubic-Time Parsing Algorithms.
In H. Bunt and A. Nijholt,editors, Advances in Probabilistic and Other ParsingTechnologies, pages 29?62.
Kluwer Academic Pub-lishers.Y.
Freund and R. Schapire.
1999.
Large Margin Clas-sification Using the Perceptron Algorithm.
MachineLearning, 37(3):277?296.J.
Hajic?, E.
Hajic?ova?, P. Pajas, J. Panevova, and P. Sgall.2001.
The Prague Dependency Treebank 1.0, LDCNo.
LDC2001T10.
Linguistics Data Consortium.J.
Hajic?.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
InE.
Hajic?ova?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevova?, pages 12?19.K.
Hall and V. Nova?k.
2005.
Corrective Modeling forNon-Projective Dependency Parsing.
In Proceedingsof IWPT, pages 42?52.T.
Koo and M. Collins.
2005.
Hidden-Variable Modelsfor Discriminative Reranking.
In Proceedings of HLT-EMNLP, pages 507?514.W.
Li and A. McCallum.
2005.
Semi-Supervised Se-quence Modeling with Syntactic Topic Models.
InProceedings of AAAI, pages 813?818.P.
Liang.
2005.
Semi-Supervised Learning for NaturalLanguage.
Master?s thesis, Massachusetts Institute ofTechnology.M.P.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with Latent Annotations.
In Proceedings ofACL, pages 75?82.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective Self-Training for Parsing.
In Proceedings ofHLT-NAACL, pages 152?159.R.
McDonald and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.
InProceedings of EACL, pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line Large-Margin Training of Dependency Parsers.
InProceedings of ACL, pages 91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b.Non-Projective Dependency Parsing using SpanningTree Algorithms.
In Proceedings of HLT-EMNLP,pages 523?530.S.
Miller, J. Guinness, and A. Zamanian.
2004.
NameTagging with Word Clusters and Discriminative Train-ing.
In Proceedings of HLT-NAACL, pages 337?342.J.
Nivre and J. Nilsson.
2005.
Pseudo-Projective Depen-dency Parsing.
In Proceedings of ACL, pages 99?106.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007Shared Task on Dependency Parsing.
In Proceedingsof EMNLP-CoNLL 2007, pages 915?932.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning Accurate, Compact, and Interpretable TreeAnnotation.
In Proceedings of COLING-ACL, pages433?440.A.
Ratnaparkhi.
1996.
A Maximum Entropy Model forPart-Of-Speech Tagging.
In Proceedings of EMNLP,pages 133?142.I.
Titov and J. Henderson.
2007.
Constituent Parsingwith Incremental Sigmoid Belief Networks.
In Pro-ceedings of ACL, pages 632?639.Q.I.
Wang, D. Schuurmans, and D. Lin.
2005.
StrictlyLexical Dependency Parsing.
In Proceedings of IWPT,pages 152?159.H.
Yamada and Y. Matsumoto.
2003.
Statistical De-pendency Analysis With Support Vector Machines.
InProceedings of IWPT, pages 195?206.603
