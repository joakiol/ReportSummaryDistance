Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 162?168,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAn Open Web Platform for Rule-Based Speech-to-Sign TranslationManny Rayner, Pierrette Bouillon, Johanna Gerlach, Irene Strasly, Nikos TsourakisUniversity of Geneva, FTI/TIM, Switzerland{Emmanuel.Rayner,Pierrette.Bouillon,Johanna.Gerlach}@unige.ch{Irene.Strasly,Nikolaos.Tsourakis}@unige.chSarah EblingUniversity of Zurich, Institute of Computational Linguistics, Switzerlandebling@ifi.uzh.chAbstractWe present an open web platform for de-veloping, compiling, and running rule-based speech to sign language translationapplications.
Speech recognition is per-formed using the Nuance Recognizer 10.2toolkit, and signed output, including bothmanual and non-manual components, isrendered using the JASigning avatar sys-tem.
The platform is designed to makethe component technologies readily acces-sible to sign language experts who are notnecessarily computer scientists.
Transla-tion grammars are written in a versionof Synchronous Context-Free Grammaradapted to the peculiarities of sign lan-guage.
All processing is carried out on aremote server, with content uploaded andaccessed through a web interface.
Ini-tial experiences show that simple transla-tion grammars can be implemented on atime-scale of a few hours to a few daysand produce signed output readily com-prehensible to Deaf informants.
Overall,the platform drastically lowers the barrierto entry for researchers interested in build-ing applications that generate high-qualitysigned language.1 IntroductionWhile a considerable amount of linguistic researchhas been carried out on sign languages to date,work in automatic sign language processing is stillin its infancy.
Automatic sign language process-ing comprises applications such as sign languagerecognition, sign language synthesis, and sign lan-guage translation (S?f?r and Glauert, 2012).
Forall of these applications, drawing on the expertiseof native signers, sign language linguists and signlanguage interpreters is crucial.
These differenttypes of sign language experts may exhibit varyingdegrees of computer literacy.
In the past, their con-tribution to the development of systems that au-tomatically translate into sign language has beenrestricted mostly to the provision of transcribedand/or annotated sign language data.In this paper, we report on the development andevaluation of a platform that allows sign languageexperts with modest computational skills to play amore active role in sign language machine trans-lation.
The platform enables these users to inde-pendently develop and run applications translatingspeech into synthesized sign language through aweb interface.
Synthesized sign language is pre-sented by means of a signing avatar.
To the best ofour knowledge, our platform is the first to facilitatelow-threshold speech-to-sign translation, openingup various possible use cases, e.g.
that of com-municating with a Deaf customer in a public ser-vice setting like a hospital, train station or bank.1By pursuing a rule-based translation approach, theplatform also offers new possibilities for empiri-cal investigation of sign language linguistics: thelinguist can concretely implement a fragment of ahypothesized sign language grammar, sign a rangeof generated utterances through the avatar, and ob-tain judgements from Deaf informants.The remainder of this paper is structured as fol-lows.
Section 2 presents background and relatedwork.
Section 3 describes the architecture of thespeech-to-sign platform.
Section 4 reports on apreliminary evaluation of the usability of the plat-form and of translations produced by the platform.Section 5 offers a conclusion and an outlook on fu-ture research questions.1We follow the widely recognized convention of using theupper-cased word Deaf to describe members of the linguis-tic community of sign language users and, in contrast, thelower-cased word deaf to describe the audiological state of ahearing loss (Morgan and Woll, 2002).1622 Background and related workThere has been surprisingly little work to date onspeech to sign language translation.
The best-performing system reported in the literature stillappears to be TESSA (Cox et al, 2002), whichtranslated English speech into British Sign Lan-guage (BSL) in a tightly constrained post officecounter service domain, using coverage capturedin 370 English phrasal patterns with associatedBSL translations.
The system was evaluated ina realistic setting in a British post office, withthree post office clerks on the hearing side of thedialogues and six Deaf subjects playing the roleof customers, and performed creditably.
Anothersubstantial project is the one described by San-Segundo et al (2008), which translated Spanishspeech into Spanish Sign Language; this, however,does not appear to have reached the stage of be-ing able to achieve reasonable coverage even of asmall domain, and the evaluation described in thepaper is restricted to comprehensibility of signsfrom the manual alphabet.2It is reasonable to ask why so little attention hasbeen devoted to what many people would agree isan important and interesting problem, especiallygiven the early success of TESSA.
Our own ex-periences, and those of other researchers we havetalked to, suggest that the critical problem is thehigh barrier to entry: in order to build a speech-to-sign system, it is necessary to be able to com-bine components for speech recognition, transla-tion and sign language animation.
The first twotechnologies are now well-understood, and goodplatforms are readily available.
Sign languageanimation is still, however, a niche subject, andthe practical problems involved in obtaining us-able sign language animation components are non-trivial.
The fact that San-Segundo et al (2008)chose to develop their own animation componentspeaks eloquently about the difficulties involved.There are three approaches to sign language an-imation: hand-crafted animation, motion captur-ing and synthesis from form notation (Glauert,2013).
Hand-crafted animation consists of manu-ally modeling and posing an avatar character.
Thisprocedure typically yields high-quality results butis very labor-intensive.
A signing avatar may also2Sign languages make use of a communication formknown as the manual alphabet (or, finger alphabet), in whichthe letters of a spoken language word are fingerspelled, i.e.,dedicated signs are used for each letter of the word.be animated based on information obtained frommotion capturing, which involves recording a hu-man?s signing.
Although sign language anima-tions obtained through motion capturing also tendto be of good quality, the major drawback of thisapproach is the long calibration time and extensivepostprocessing required.Synthesis from form notation permits construc-tion of a fully-fledged animation system that al-lows synthesis of any signed form that can be de-scribed through the associated notation.
Avatarsigning synthesized from form notation is the mostflexible in that it is able to render dynamic content,e.g.
display the sign language output of a machinetranslation system, present the contents of a signlanguage wiki or an e-learning application, visual-ize lexicon entries or present public transportationinformation (Efthimiou et al, 2012; Kipp et al,2011).
At the same time, this approach to signlanguage animation typically results in the lowestquality: controlling the appearance of all possiblesign forms that may be produced from a given no-tation is virtually impossible.The most comprehensive existing sign languageanimation system based on synthesis from formnotation is undoubtedly JASigning (Elliott et al,2008; Jennings et al, 2010), a distant descen-dant of the avatar system used in TESSA whichwas further developed over the course of the eS-IGN and DictaSign European Framework projects.JASigning performs synthesis from SiGML (El-liott et al, 2000), an XML-based representationof the physical form of signs based on the well-understood Hamburg Notation System for SignLanguages (HamNoSys) (Prillwitz et al, 1989).HamNoSys can be converted into SiGML in astraightforward fashion.
Unfortunately, despite itsmany good and indeed unique properties, JASign-ing is a piece of research software that in practicehas posed an insurmountable challenge to mostlinguists without a computer science background.The basic purpose of the Lite Speech2Signproject can now be summarised in a sentence:we wished to package JASigning together witha state-of-the-art commercial speech recognitionplatform and a basic machine translation frame-work in a way that makes the combination easilyusable by sign language linguists who are not soft-ware engineers.
In the rest of the paper, we de-scribe the result.1633 The Lite Speech2Sign platformThe fact that the Lite Speech2Sign platform is in-tended primarily for use by sign language expertswho may only have modest skills in computer sci-ence has dictated several key design decisions.
Inparticular, 1) the formalism used is simple andminimal and 2) no software need be installed onthe local machine: all processing (compilation, de-ployment, testing) is performed on a remote serveraccessed through the web interface.3.1 Runtime functionality and formalismAt runtime, the basic processing flow is speech ?source language text ?
?sign table?
?
SiGML?
signed animation.
Input speech, source lan-guage text and signed animation have their ob-vious meanings, and we have already introducedSiGML in the preceding section.
At the inputend of the pipeline, speech recognition is carriedout using the Nuance Recognizer 10.2 platform,equipped with domain-specific language modelscompiled from the grammar.
At the output end,SiGML is converted into signed animation formusing the JASigning avatar system.The ?sign table?, the level which joins all thesepieces together, is an intermediate representa-tion modelled on the diagrams typically used intheoretical sign language linguistics to representsigned utterances.
A sign table is, concretely, amatrix whose rows represent the different paral-lel channels of signed language output (manualactivities, gaze, head movements, mouth move-ments, etc).
The only obligatory row is the one formanual activities, which consists of a sequence of?glosses?, each gloss referring to one manual ac-tivity.
There is one column for each gloss/manualactivity in the signed utterance.The usefulness of this representation is depen-dent on the appropriateness of the assumption thatsign language is timed so that each non-manualactivity can be assumed synchronous with somemanual activity.
This has been shown to be true fornon-manual activities that serve linguistic func-tions.
Non-manual activities that serve purely af-fective purposes, e.g., expressing anger or disgust,are known to start slightly earlier than the sur-rounding manual activities (Reilly and Anderson,2002; Wilbur, 2000).
A restriction imposed bythe low-level SiGML representation is that non-manual activities cannot be extended across sev-eral manual activities in a straightforward way;include lsf_ch.csvinclude visicast.txtDomainName toy1Client speech2sign_clientSourceLanguage frenchTargetLanguages gloss head gaze \eyebrows aperture mouthingEndDomainUtteranceSource je m?appelle $$nameGloss MOI S_APPELER $$nameHead Nod Neutral NeutralGaze Neutral Neutral NeutralEyebrows Up Up UpAperture Wide Wide WideMouthing mwe appel $$nameEndUtteranceTrPhrase $$nameSource claudeGloss C L A U D EMouthing C L a u: d eEndTrPhraseTrPhrase $$nameSource marieGloss M A R I EMouthing L23 a R i eEndTrPhraseFigure 1: Toy speech2sign application definition.however, workarounds have been introduced forthis (Ebling and Glauert, 2015).
Experience withSiGML has shown that it is capable of support-ing signed animation of satisfactory quality (Smithand Nolan, 2015).The core translation formalism is a versionof Synchronous Context Free Grammar (SCFG;(Aho and Ullman, 1969; Chiang, 2005)) adaptedto the peculiarities of sign language translation.A complete toy application definition is shown inFigure 1.
The top-level Utterance rule trans-lates French expressions of the form Je m?appelle?NAME?
(?I am called ?NAME??)
to Swiss FrenchSign Language (LSF-CH) expressions of a form164that can be glossed as MOI S_APPELER ?NAME?together with accompanying non-manual com-ponents; for example, the manual activity MOI(signed by pointing at one?s chest) is here per-formed together with a head nod, raised eyebrows,widened eyes, and a series of mouth movementsapproximating the shapes used to say ?mwe?.The two TrPhrase rules translate the names?Claude?
and ?Marie?
into fingerspelled formswith accompanying mouthings.The mapping between the sign table andSiGML levels is specified using three other typesof declarations, defined in the resource lexicalisted in the initial include lines.
1) Glossesare associated with strings of HamNoSys sym-bols; in this case, the resource lexicon usedis lsf_ch.csv, a CSV spreadsheet whosecolumns are glosses and HNS strings for LSF-CHsigns.
2) Symbols in the non-manual rows (Head,Gaze, etc) are mapped into the set of SiGML tagssupported by the avatar, according to the decla-rations in the sign-language-independent resourcefile visicast.txt.
3) The Mouthing line istreated specially.
Two types of mouthings are sup-ported: ?mouth pictures?, approximate mouthingsof phonemes, are written as SAMPA (Wells, 1997)strings (e.g.
mwe is a SAMPA string).
It is alsopossible to use the repertoire of ?mouth gestures?
(mouth movements not related to spoken languagewords, produced with teeth, jaw, lips, cheeks,or tongue) supported by the avatar, again usingdefinitions taken from the visicast.txt re-source file.
For example, L23 denotes pursed lips(Hanke, 2001).The Domain unit at the top defines the name ofthe translation app, the source language3and signlanguage channels, and the type of web client usedto display it.3.2 Compile- and deploy-time functionalityThe compilation process takes application de-scriptions like the one above as input and trans-forms them first into SCFG grammars, then intoGrXML grammars4, and finally into runnable Nu-ance recognition grammars.
The compiler alsoproduces tables of metadata listing associations3Any recognition language supported by Nuance Recog-nizer 10.2 can potentially be used as a source language; thecurrent version of the platform is loaded with language packsfor English, French, German, Italian, Japanese and Slove-nian.4GrXML is an open standard for writing speech recogni-tion grammars.between symbols and HamNoSys, SAMPA, andSiGML constants.Two main challenges needed to be addressedwhen designing the compile-time functionality.The first was to make the process of developing,uploading, compiling, and deploying web-basedspeech applications simple to invoke, so that theseoperations could be performed without detailedunderstanding of the underlying technology.
Thesecond was to support development on a sharedserver; here, it is critical to ensure that a developerwho uploads bad content is not able to break thesystem for other users.At an abstract level, the architecture is as fol-lows.
Content is divided into separate ?names-paces?, with each developer controlling one ormore namespaces; a namespace in turn containsone or more translation apps.
At the source level,each namespace is a self-contained directory, andeach app a self-contained subdirectory.From the developer?s point of view, the wholeupload/compile/deploy cycle reduces to a simpleprogression across a dashboard with four tabs la-beled ?Select?, ?Compile?, ?Test?, and ?Release?.The developer starts the upload/compile/deploycycle by uploading one or more namespace direc-tories over an FTP client and choosing one of themfrom the ?Select?
tab.The platform contains three separate servers,respectively called compilation, staging, and de-ployment.
After selecting the app on the firsttab, the developer moves to the second one andpresses the ?Compile?
button to invoke the com-pilation server.
Successful compilation resultsin a Nuance grammar recognition module and aset of namespace-specific table entries; a separateNuance recognition grammar is created for eachnamespace.
As part of the compilation process,a set of files is also created which list undefinedconstants.
These can be downloaded over the FTPconnection and are structured so as to make it easyfor the developer to fill in missing entries and addthe new content to the resource files.When the app has compiled, the developer pro-ceeds to the third, ?Staging?
tab, and presses the?Test?
button.
This initiates a process whichcopies the compiled recognition grammar, tableentries and metadata to appropriate places on thestaging server and registers the grammar as avail-able for use by the recognition engine, after whichthe developer can interactively test the application165through the web interface.
It is important that onlycopying actions are performed by the ?Staging?server; experience shows that recompiling appli-cations can often lead to problems if the compilerchanges after an application is uploaded.When the developer is satisfied with the appli-cation, they move to the fourth tab and press the?Release?
button.
This carries out a second set ofcopying operations which transfer the applicationto the deployment server.4 Initial experiences with the platformThe Lite Speech2Sign platform is undergoing ini-tial testing; during this process, we have con-structed half a dozen toy apps for the transla-tion directions French ?
LSF-CH and German?
Swiss German Sign Language, and one mod-erately substantial app for French ?
LSF-CH.Grammars written so far all have a flat structure.Our central claims regarding the platform arethat it greatly simplifies the process of building aspeech-to-sign application and allows rapid con-struction of apps which produce signed languageof adequate quality.
To give some substance tothese statements, we tracked the construction ofa small French ?
LSF-CH medical questionnaireapp and performed a short evaluation.
The appwas built by a sign language expert whose mainqualifications are in sign language interpretation.The expert began by discussing the corpus withDeaf native signers, to obtain video-recorded ma-terial on which to base development.
They thenimplemented rules and HNS entries, uploaded, de-bugged, and deployed the content, and used thedeployed system to perform the evaluation.Rule-writing typically required on the order often to fifteen minutes per rule, using a method ofrepeatedly playing the recorded video and enter-ing first the gloss line and then the accompany-ing non-manual lines.
Uploading, debugging, anddeployment of the app was completely straight-forward and took approximately one hour.
Themost time-consuming part of the process was im-plementing HNS entries for signs missing fromthe current LSF-CH HNS lexicon.
The time re-quired per entry varied a great deal depending onthe sign?s complexity, but was typically on the or-der of half an hour to two hours.
This part of thetask will of course become less important as theHNS lexicon resource becomes more complete.The evaluation was carried out with five Deafsubjects and based on recommendations for signlanguage animation evaluation studies by Kacorriet al (2015).
Each subject was first given a shortdemographic questionnaire.
Subjects were thenasked to watch seven outputs from the app andecho them back, either in signed or mouthed form,to check the comprensibility of the app?s signedoutput.
They then answered a second short ques-tionnaire which asked for their overall impres-sions.
The result was encouraging: although noneof the subjects felt the signing was truly fluent andhuman-like (a frequent comment was ?artificial?
),they all considered it grammatically correct andperfectly comprehensible.5 Conclusions and further directionsAlthough the Lite Speech2Sign platform is de-signed to appear very simple and most of its run-time processing is carried out by the third-partyJASigning and Nuance components, it representsa non-trivial engineering effort.
The value it addsis that it allows sign language linguists who mayhave only modest computational skills to buildtranslation applications that produce synthesizedsigned language, using a tool whose basic func-tioning can be mastered in two or three weeks.
Byincluding speech recognition, these applicationscan potentially be useful in real situations.In a research context, the platform opens up newpossibilities for investigation of the grammar ofsigned languages.
If the linguist wishes to inves-tigate the productivity of a hypothesized syntac-tic rule, they can quickly implement a grammarfragment and produce a set of related signed utter-ances, all signed uniformly using the avatar.
Ourinitial experiences, as described in Section 4, sug-gest that rendering quality is sufficient to obtainuseful signer judgements.Full documentation for Lite Speech2Sign isavailable (Rayner, 2016).
The platform is cur-rently in alpha testing; we plan to open it up forgeneral use during Q3 2016.
People interested inobtaining an account may do so by mailing one ofthe authors of this paper.AcknowledgementsWe would like to thank John Glauert of the Schoolof Computing Sciences, UEA, for his invaluablehelp with JASigning, and Nuance Inc for gener-ously making their software available to us for re-search purposes.166ReferencesAlfred V. Aho and Jeffrey D. Ullman.
Propertiesof syntax directed translations.
Journal of Com-puter and System Sciences, 3(3):319?334, 1969.David Chiang.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceed-ings of the 43rd Annual Meeting on Associa-tion for Computational Linguistics, pages 263?270.
Association for Computational Linguis-tics, 2005.Stephen Cox, Michael Lincoln, Judy Tryggvason,Melanie Nakisa, Mark Wells, Marcus Tutt, andSanja Abbott.
Tessa, a system to aid communi-cation with deaf people.
In Proceedings of thefifth international ACM conference on Assistivetechnologies, pages 205?212.
ACM, 2002.Sarah Ebling and John Glauert.
Building aSwiss German Sign Language avatar withJASigning and evaluating it among the Deafcommunity.
Universal Access in the In-formation Society, pages 1?11, 2015.
Re-trieved from http://dx.doi.org/10.1007/s10209-015-0408-1 (last accessedNovember 20, 2015).Eleni Efthimiou, Stavroula-Evita Fotinea, ThomasHanke, John Glauert, Richard Bowden, An-nelies Braffort, Christophe Collet, Petros Mara-gos, and Fran?ois Lefebvre-Albaret.
The Dicta-Sign Wiki: Enabling web communication forthe Deaf.
In Proceedings of the 13th Interna-tional Conference on Computers Helping Peo-ple with Special Needs (ICCHP), pages 205?212, Linz, Austria, 2012.Ralph Elliott, John RW Glauert, JR Kennaway,and Ian Marshall.
The development of lan-guage processing support for the ViSiCASTproject.
In Proceedings of the fourth interna-tional ACM conference on Assistive technolo-gies, pages 101?108.
ACM, 2000.Ralph Elliott, John RW Glauert, JR Kennaway,Ian Marshall, and Eva Safar.
Linguistic mod-elling and language-processing technologies foravatar-based sign language presentation.
Uni-versal Access in the Information Society, 6(4):375?391, 2008.John Glauert.
Animating sign language for Deafpeople.
Lecture held at the University of Zurich,October 9, 2013 (unpublished), 2013.Thomas Hanke.
ViSiCAST Deliverable D5-1: In-terface definitions.
Technical report, ViSiCASTproject, 2001.
Retrieved from http://www.visicast.cmp.uea.ac.uk/Papers/ViSiCAST_D5-1v017rev2.pdf (lastaccessed November 20, 2015).Vince Jennings, Ralph Elliott, Richard Kennaway,and John Glauert.
Requirements for a signingavatar.
In Proceedings of the 4th LREC Work-shop on the Representation and Processing ofSign Languages, pages 133?136, La Valetta,Malta, 2010.Hernisa Kacorri, Matt Huenerfauth, Sarah Ebling,Kasmira Patel, and Mackenzie Willard.
Demo-graphic and experiential factors influencing ac-ceptance of sign language animation by Deafusers.
In Proceedings of the 17th InternationalACM SIGACCESS Conference on Computers &Accessibility, pages 147?154.
ACM, 2015.Michael Kipp, Alexis Heloir, and Quan Nguyen.Sign language avatars: Animation and com-prehensibility.
In Proceedings of the 11th In-ternational Conference on Intelligent VirtualAgents (IVA), pages 113?126, Reykjav?k, Ice-land, 2011.Gary Morgan and Bencie Woll.
The developmentof complex sentences in British Sign Language.In Gary Morgan and Bencie Woll, editors, Di-rections in Sign Language Acquisition: Trendsin Language Acquisition Research, pages 255?276.
John Benjamins, Amsterdam, Netherlands,2002.Siegmund Prillwitz, Regina Leven, Heiko Zienert,Thomas Hanke, and Jan Henning.
HamNoSys:Version 2.0: An Introductory Guide.
Signum,Hamburg, Germany, 1989.Manny Rayner.
Using the Regulus LiteSpeech2Sign Platform.
http://www.issco.unige.ch/en/research/projects/Speech2SignDoc/build/html/index.html, 2016.
Online documen-tation.J.
Reilly and D. Anderson.
FACES: The ac-quisition of non-manual morphology in ASL.In G. Morgan and B. Woll, editors, Direc-tions in Sign Language Acquisition, pages 159?181.
John Benjamins, Amsterdam, Netherlands,2002.Eva S?f?r and John Glauert.
Computer modelling.In Roland Pfau, Markus Steinbach, and Bencie167Woll, editors, Sign Language: An InternationalHandbook, pages 1075?1101.
De Gruyter Mou-ton, Berlin, Germany, 2012.Rub?n San-Segundo, Juan Manuel Montero,Javier Mac?as-Guarasa, R C?rdoba, Javier Fer-reiros, and Jos?
Manuel Pardo.
Proposing aspeech to gesture translation architecture forSpanish deaf people.
Journal of Visual Lan-guages & Computing, 19(5):523?538, 2008.Robert Smith and Brian Nolan.
Emotional fa-cial expressions in synthesised sign languageavatars: A manual evaluation.
Universal Accessin the Information Society, pages 1?10, 2015.Retrieved from http://dx.doi.org/10.1007/s10209-015-0410-7 (last accessedNovember 20, 2015).J.C.
Wells.
SAMPA computer readable pho-netic alphabet.
In D. Gibbon, R. Moore, andR.
Winski, editors, Handbook of Standards andResources for Spoken Language Systems.
DeGruyter Mouton, Berlin, Germany, 1997.Ronnie B. Wilbur.
Phonological and prosodic lay-ering of nonmanuals in American Sign Lan-guage.
In Karen Emmorey and Harlan Lane,editors, The Signs of Language Revisited, pages215?244.
Erlbaum, Mahwah, NJ, 2000.168
