Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152?159,New York, June 2006. c?2006 Association for Computational LinguisticsEffective Self-Training for ParsingDavid McClosky, Eugene Charniak, and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{dmcc|ec|mj}@cs.brown.eduAbstractWe present a simple, but surprisingly ef-fective, method of self-training a two-phase parser-reranker system using read-ily available unlabeled data.
We showthat this type of bootstrapping is possiblefor parsing when the bootstrapped parsesare processed by a discriminative reranker.Our improved model achieves an f -scoreof 92.1%, an absolute 1.1% improvement(12% error reduction) over the previousbest result for Wall Street Journal parsing.Finally, we provide some analysis to bet-ter understand the phenomenon.1 IntroductionIn parsing, we attempt to uncover the syntactic struc-ture from a string of words.
Much of the challengeof this lies in extracting the appropriate parsingdecisions from textual examples.
Given sufficientlabelled data, there are several ?supervised?
tech-niques of training high-performance parsers (Char-niak and Johnson, 2005; Collins, 2000; Henderson,2004).
Other methods are ?semi-supervised?
wherethey use some labelled data to annotate unlabeleddata.
Examples of this include self-training (Char-niak, 1997) and co-training (Blum and Mitchell,1998; Steedman et al, 2003).
Finally, there are ?un-supervised?
strategies where no data is labeled andall annotations (including the grammar itself) mustbe discovered (Klein and Manning, 2002).Semi-supervised and unsupervised methods areimportant because good labeled data is expensive,whereas there is no shortage of unlabeled data.While some domain-language pairs have quite a bitof labelled data (e.g.
news text in English), manyother categories are not as fortunate.
Less unsuper-vised methods are more likely to be portable to thesenew domains, since they do not rely as much on ex-isting annotations.2 Previous workA simple method of incorporating unlabeled datainto a new model is self-training.
In self-training,the existing model first labels unlabeled data.
Thenewly labeled data is then treated as truth and com-bined with the actual labeled data to train a newmodel.
This process can be iterated over differentsets of unlabeled data if desired.
It is not surprisingthat self-training is not normally effective: Charniak(1997) and Steedman et al (2003) report either mi-nor improvements or significant damage from usingself-training for parsing.
Clark et al (2003) appliesself-training to POS-tagging and reports the sameoutcomes.
One would assume that errors in the orig-inal model would be amplified in the new model.Parser adaptation can be framed as a semi-supervised or unsupervised learning problem.
Inparser adaptation, one is given annotated trainingdata from a source domain and unannotated datafrom a target.
In some cases, some annotated datafrom the target domain is available as well.
The goalis to use the various data sets to produce a modelthat accurately parses the target domain data despiteseeing little or no annotated data from that domain.Gildea (2001) and Bacchiani et al (2006) show thatout-of-domain training data can improve parsing ac-152curacy.
The unsupervised adaptation experiment byBacchiani et al (2006) is the only successful in-stance of parsing self-training that we have found.Our work differs in that all our data is in-domainwhile Bacchiani et al uses the Brown corpus as la-belled data.
These correspond to different scenarios.Additionally, we explore the use of a reranker.Co-training is another way to train models fromunlabeled data (Blum and Mitchell, 1998).
Unlikeself-training, co-training requires multiple learners,each with a different ?view?
of the data.
When onelearner is confident of its predictions about the data,we apply the predicted label of the data to the train-ing set of the other learners.
A variation suggestedby Dasgupta et al (2001) is to add data to the train-ing set when multiple learners agree on the label.
Ifthis is the case, we can be more confident that thedata was labelled correctly than if only one learnerhad labelled it.Sarkar (2001) and Steedman et al (2003) inves-tigated using co-training for parsing.
These studiessuggest that this type of co-training is most effec-tive when small amounts of labelled training data isavailable.
Additionally, co-training for parsing canbe helpful for parser adaptation.3 Experimental SetupOur parsing model consists of two phases.
First, weuse a generative parser to produce a list of the top nparses.
Next, a discriminative reranker reorders then-best list.
These components constitute two viewsof the data, though the reranker?s view is restrictedto the parses suggested by the first-stage parser.
Thereranker is not able to suggest new parses and, more-over, uses the probability of each parse tree accord-ing to the parser as a feature to perform the rerank-ing.
Nevertheless, the reranker?s value comes fromits ability to make use of more powerful features.3.1 The first-stage 50-best parserThe first stage of our parser is the lexicalized proba-bilistic context-free parser described in (Charniak,2000) and (Charniak and Johnson, 2005).
Theparser?s grammar is a smoothed third-order Markovgrammar, enhanced with lexical heads, their partsof speech, and parent and grandparent informa-tion.
The parser uses five probability distributions,one each for heads, their parts-of-speech, head-constituent, left-of-head constituents, and right-of-head constituents.
As all distributions are condi-tioned with five or more features, they are all heavilybacked off using Chen back-off (the average-countmethod from Chen and Goodman (1996)).
Also,the statistics are lightly pruned to remove those thatare statistically less reliable/useful.
As in (Char-niak and Johnson, 2005) the parser has been mod-ified to produce n-best parses.
However, the n-bestparsing algorithm described in that paper has beenreplaced by the much more efficient algorithm de-scribed in (Jimenez and Marzal, 2000; Huang andChang, 2005).3.2 The MaxEnt RerankerThe second stage of our parser is a Maximum En-tropy reranker, as described in (Charniak and John-son, 2005).
The reranker takes the 50-best parsesfor each sentence produced by the first-stage 50-best parser and selects the best parse from those50 parses.
It does this using the reranking method-ology described in Collins (2000), using a Maxi-mum Entropy model with Gaussian regularizationas described in Johnson et al (1999).
Our rerankerclassifies each parse with respect to 1,333,519 fea-tures (most of which only occur on few parses).The features consist of those described in (Char-niak and Johnson, 2005), together with an additional601,577 features.
These features consist of the parts-of-speech, possibly together with the words, thatsurround (i.e., precede or follow) the left and rightedges of each constituent.
The features actually usedin the parser consist of all singletons and pairs ofsuch features that have different values for at leastone of the best and non-best parses of at least 5 sen-tences in the training data.
There are 147,456 suchfeatures involving only parts-of-speech and 454,101features involving parts-of-speech and words.
Theseadditional features are largely responsible for im-proving the reranker?s performance on section 23to 91.3% f -score (Charniak and Johnson (2005) re-ported an f -score of 91.0% on section 23).3.3 CorporaOur labeled data comes from the Penn Treebank(Marcus et al, 1993) and consists of about 40,000sentences from Wall Street Journal (WSJ) articles153annotated with syntactic information.
We use thestandard divisions: Sections 2 through 21 are usedfor training, section 24 is held-out development, andsection 23 is used for final testing.
Our unlabeleddata is the North American News Text corpus, NANC(Graff, 1995), which is approximately 24 million un-labeled sentences from various news sources.
NANCcontains no syntactic information.
Sentence bound-aries in NANC are induced by a simple discrimina-tive model.
We also perform some basic cleanups onNANC to ease parsing.
NANC contains news articlesfrom various news sources including the Wall StreetJournal, though for this paper, we only use articlesfrom the LA Times.4 Experimental ResultsWe use the reranking parser to produce 50-bestparses of unlabeled news articles from NANC.
Next,we produce two sets of one-best lists from these 50-best lists.
The parser-best and reranker-best listsrepresent the best parse for each sentence accord-ing to the parser and reranker, respectively.
Fi-nally, we mix a portion of parser-best or reranker-best lists with the standard Wall Street Journal train-ing data (sections 2-21) to retrain a new parser (butnot reranker1) model.
The Wall Street Journal train-ing data is combined with the NANC data in thefollowing way: The count of each parsing event isthe (optionally weighted) sum of the counts of thatevent in Wall Street Journal and NANC.
Bacchianiet al (2006) show that count merging is more effec-tive than creating multiple models and calculatingweights for each model (model interpolation).
Intu-itively, this corresponds to concatenating our train-ing sets, possibly with multiple copies of each to ac-count for weighting.Some notes regarding evaluations: All numbersreported are f -scores2.
In some cases, we evaluateonly the parser?s performance to isolate it from thereranker.
In other cases, we evaluate the rerankingparser as a whole.
In these cases, we will use theterm reranking parser.Table 1 shows the difference in parser?s (notreranker?s) performance when trained on parser-best1We attempted to retrain the reranker using the self-trainedsentences, but found no significant improvement.2The harmonic mean of labeled precision (P) and labeledrecall (R), i.e.
f = 2?P?RP+RSentences added Parser-best Reranker-best0 (baseline) 90.350k 90.1 90.7250k 90.1 90.7500k 90.0 90.9750k 89.9 91.01,000k 90.0 90.81,500k 90.0 90.82,000k ?
91.0Table 1: f -scores after adding either parser-best orreranker-best sentences from NANC to WSJ trainingdata.
While the reranker was used to produce thereranker-best sentences, we performed this evalua-tion using only the first-stage parser to parse all sen-tences from section 22.
We did not train a modelwhere we added 2,000k parser-best sentences.output versus reranker-best output.
Adding parser-best sentences recreates previous self-training ex-periments and confirms that it is not beneficial.However, we see a large improvement from addingreranker-best sentences.
One may expect to see amonotonic improvement from this technique, butthis is not quite the case, as seen when we add1,000k sentences.
This may be due to some sec-tions of NANC being less similar to WSJ or contain-ing more noise.
Another possibility is that thesesections contains harder sentences which we can-not parse as accurately and thus are not as usefulfor self-training.
For our remaining experiments, wewill only use reranker-best lists.We also attempt to discover the optimal numberof sentences to add from NANC.
Much of the im-provement comes from the addition of the initial50,000 sentences, showing that even small amountsof new data can have a significant effect.
As we addmore data, it becomes clear that the maximum ben-efit to parsing accuracy by strictly adding reranker-best sentences is about 0.7% and that f -scores willasymptote around 91.0%.
We will return to thiswhen we consider the relative weightings of WSJ andNANC data.One hypothesis we consider is that the rerankedNANC data incorporated some of the features fromthe reranker.
If this were the case, we would not seean improvement when evaluating a reranking parser154Sentences added 1 22 240 (baseline) 91.8 92.1 90.550k 91.8 92.4 90.8250k 91.8 92.3 91.0500k 92.0 92.4 90.9750k 92.0 92.4 91.11,000k 92.1 92.2 91.31,500k 92.1 92.1 91.21,750k 92.1 92.0 91.32,000k 92.2 92.0 91.3Table 2: f -scores from evaluating the rerank-ing parser on three held-out sections after addingreranked sentences from NANC to WSJ training.These evaluations were performed on all sentences.on the same models.
In Table 2, we see that the newNANC data contains some information orthogonal tothe reranker and improves parsing accuracy of thereranking parser.Up to this point, we have only considered givingour true training data a relative weight of one.
In-creasing the weight of the Wall Street Journal datashould improve, or at least not hurt, parsing perfor-mance.
Indeed, this is the case for both the parser(figure not shown) and reranking parser (Figure 1).Adding more weight to the Wall Street Journal dataensures that the counts of our events will be closerto our more accurate data source while still incorpo-rating new data from NANC.
While it appears thatthe performance still levels off after adding aboutone million sentences from NANC, the curves cor-responding to higher WSJ weights achieve a higherasymptote.
Looking at the performance of variousweights across sections 1, 22, and 24, we decidedthat the best combination of training data is to giveWSJ a relative weight of 5 and use the first 1,750kreranker-best sentences from NANC.Finally, we evaluate our new model on the testsection of Wall Street Journal.
In Table 3, we notethat baseline system (i.e.
the parser and rerankertrained purely on Wall Street Journal) has improvedby 0.3% over Charniak and Johnson (2005).
The92.1% f -score is the 1.1% absolute improvementmentioned in the abstract.
The improvement fromself-training is significant in both macro and microtests (p < 10?5).91.791.891.99292.192.292.392.40  5  10  15  20  25  30  35  40f-scoreNANC sentences added (units of 50k sentences)WSJ x1WSJ x3WSJ x5Figure 1: Effect of giving more relative weight toWSJ training data on reranking parser f -score.
Eval-uations were done from all sentences from section1.Model fparser frerankerCharniak and Johnson (2005) ?
91.0Current baseline 89.7 91.3WSJ + NANC 91.0 92.1Table 3: f -scores on WSJ section 23. fparser andfreranker are the evaluation of the parser and rerank-ing parser on all sentences, respectively.
?WSJ +NANC?
represents the system trained on WSJ train-ing (with a relative weight of 5) and 1,750k sen-tences from the reranker-best list of NANC.5 AnalysisWe performed several types of analysis to better un-derstand why the new model performs better.
Wefirst look at global changes, and then at changes atthe sentence level.5.1 Global ChangesIt is important to keep in mind that while thereranker seems to be key to our performance im-provement, the reranker per se never sees the extradata.
It only sees the 50-best lists produced by thefirst-stage parser.
Thus, the nature of the changes tothis output is important.We have already noted that the first-stage parser?sone-best has significantly improved (see Table 1).
InTable 4, we see that the 50-best oracle rate also im-155Model 1-best 10-best 50-bestBaseline 89.0 94.0 95.9WSJ?1 + 250k 89.8 94.6 96.2WSJ?5 + 1,750k 90.4 94.8 96.4Table 4: Oracle f -scores of top n parses producedby baseline, a small self-trained parser, and the?best?
parserproves from 95.5% for the original first-stage parser,to 96.4% for our final model.
We do not show it here,but if we self-train using first-stage one-best, there isno change in oracle rate.The first-stage parser also becomes more ?deci-sive.?
The average (geometric mean) of log2(Pr(1-best) / Pr(50th-best)) (i.e.
the ratios between theprobabilities in log space) increases from 11.959 forthe baseline parser, to 14.104 for the final parser.
Wehave seen earlier that this ?confidence?
is deserved,as the first-stage one-best is so much better.5.2 Sentence-level AnalysisTo this point we have looked at bulk properties of thedata fed to the reranker.
It has higher one best and50-best-oracle rates, and the probabilities are moreskewed (the higher probabilities get higher, the lowsget lower).
We now look at sentence-level proper-ties.
In particular, we analyzed the parsers?
behav-ior on 5,039 sentences in sections 1, 22 and 24 ofthe Penn treebank.
Specifically, we classified eachsentence into one of three classes: those where theself-trained parser?s f -score increased relative to thebaseline parser?s f -score, those where the f -scoreremained the same, and those where the self-trainedparser?s f -score decreased relative to the baselineparser?s f -score.
We analyzed the distribution ofsentences into these classes with respect to four fac-tors: sentence length, the number of unknown words(i.e., words not appearing in sections 2?21 of thePenn treebank) in the sentence, the number of coor-dinating conjunctions (CC) in the sentence, and thenumber of prepositions (IN) in the sentence.
Thedistributions of classes (better, worse, no change)with respect to each of these factors individually aregraphed in Figures 2 to 5.Figure 2 shows how the self-training affects f -score as a function of sentence length.
The top line0 10 20 30 40 50 6020406080100Sentence lengthNumber of sentences(smoothed)BetterNo changeWorseFigure 2: How self-training improves performanceas a function of sentence lengthshows that the f -score of most sentences remain un-changed.
The middle line is the number of sentencesthat improved their f -score, and the bottom are thosewhich got worse.
So, for example, for sentences oflength 30, about 80 were unchanged, 25 improved,and 22 worsened.
It seems clear that there is noimprovement for either very short sentences, or forvery long ones.
(For long ones the graph is hardto read.
We show a regression analysis later in thissection that confirms this statement.)
While we didnot predict this effect, in retrospect it seems reason-able.
The parser was already doing very well onshort sentences.
The very long ones are hopeless,and the middle ones are just right.
We call this theGoldilocks effect.As for the other three of these graphs, their storiesare by no means clear.
Figure 3 seems to indicatethat the number of unknown words in the sentencedoes not predict that the reranker will help.
Figure 4might indicate that the self-training parser improvesprepositional-phrase attachment, but the graph lookssuspiciously like that for sentence length, so the im-provements might just be due to the Goldilocks ef-fect.
Finally, the improvement in Figure 5 is hard tojudge.To get a better handle on these effects we did afactor analysis.
The factors we consider are numberof CCs, INs, and unknowns, plus sentence length.As Figure 2 makes clear, the relative performanceof the self-trained and baseline parsers does not1560 1 2 3 4 50500100015002000Unknown wordsNumber of sentencesBetterNo changeWorseFigure 3: How self-training improves performanceas a function of number of unknown wordsEstimate Pr(> 0)(Intercept) -0.25328 0.3649BinnedLength(10,20] 0.02901 0.9228BinnedLength(20,30] 0.45556 0.1201BinnedLength(30,40] 0.40206 0.1808BinnedLength(40,50] 0.26585 0.4084BinnedLength(50,200] -0.06507 0.8671CCs 0.12333 0.0541Table 5: Factor analysis for the question: does theself-trained parser improve the parse with the high-est probabilityvary linearly with sentence length, so we introducedbinned sentence length (with each bin of length 10)as a factor.Because the self-trained and baseline parsers pro-duced equivalent output on 3,346 (66%) of the sen-tences, we restricted attention to the 1,693 sentenceson which the self-trained and baseline parsers?
f -scores differ.
We asked the program to consider thefollowing factors: binned sentence length, numberof PPs, number of unknown words, and number ofCCs.
The results are shown in Table 5.
The factoranalysis is trying to model the log odds as a sum oflinearly weighted factors.
I.e,log(P (1|x)/(1 ?
P (1|x))) = ?0 +m?j=1?jfj(x)In Table 5 the first column gives the name of the fac-0 2 4 6 8 10200400600Number of INsNumber of sentencesBetterNo changeWorseFigure 4: How self-training improves performanceas a function of number of prepositionstor.
The second the change in the log-odds resultingfrom this factor being present (in the case of CCsand INs, multiplied by the number of them) and thelast column is the probability that this factor is reallynon-zero.Note that there is no row for either PPs or un-known words.
This is because we also asked the pro-gram to do a model search using the Akaike Infor-mation Criterion (AIC) over all single and pairwisefactors.
The model it chooses predicts that the self-trained parser is likely produce a better parse thanthe baseline only for sentences of length 20?40 orsentences containing several CCs.
It did not includethe number of unknown words and the number ofINs as factors because they did not receive a weightsignificantly different from zero, and the AIC modelsearch dropped them as factors from the model.In other words, the self-trained parser is morelikely to be correct for sentences of length 20?40 and as the number of CCs in the sentence in-creases.
The self-trained parser does not improveprepositional-phrase attachment or the handling ofunknown words.This result is mildly perplexing.
It is fair to saythat neither we, nor anyone we talked to, thoughtconjunction handling would be improved.
Conjunc-tions are about the hardest things in parsing, and wehave no grip on exactly what it takes to help parsethem.
Conversely, everyone expected improvementson unknown words, as the self-training should dras-1570 1 2 3 4 50500100015002000Number of CCsNumber of sentencesBetterNo changeWorseFigure 5: How self-training improves performanceas a function of number of conjunctionstically reduce the number of them.
It is also the casethat we thought PP attachment might be improvedbecause of the increased coverage of preposition-noun and preposition-verb combinations that worksuch as (Hindle and Rooth, 1993) show to be so im-portant.Currently, our best conjecture is that unknownsare not improved because the words that are un-known in the WSJ are not significantly representedin the LA Times we used for self-training.
CCsare difficult for parsers because each conjunct hasonly one secure boundary.
This is particularly thecase with longer conjunctions, those of VPs and Ss.One thing we know is that self-training always im-proves performance of the parsing model when usedas a language model.
We think CC improvement isconnected with this fact and our earlier point thatthe probabilities of the 50-best parses are becomingmore skewed.
In essence the model is learning, ingeneral, what VPs and Ss look like so it is becom-ing easier to pull them out of the stew surroundingthe conjunct.
Conversely, language modeling hascomparatively less reason to help PP attachment.
Aslong as the parser is doing it consistently, attachingthe PP either way will work almost as well.6 ConclusionContrary to received wisdom, self-training can im-prove parsing.
In particular we have achieved an ab-solute improvement of 0.8% over the baseline per-formance.
Together with a 0.3% improvement dueto superior reranking features, this is a 1.1% im-provement over the previous best parser results forsection 23 of the Penn Treebank (from 91.0% to92.1%).
This corresponds to a 12% error reduc-tion assuming that a 100% performance is possible,which it is not.
The preponderance of evidence sug-gests that it is somehow the reranking aspect of theparser that makes this possible, but given no idea ofwhy this should be, so we reserve final judgementon this matter.Also contrary to expectations, the error analy-sis suggests that there has been no improvement ineither the handing of unknown words, nor prepo-sitional phrases.
Rather, there is a general im-provement in intermediate-length sentences (20-50words), but no improvement at the extremes: a phe-nomenon we call the Goldilocks effect.
The onlyspecific syntactic phenomenon that seems to be af-fected is conjunctions.
However, this is good newssince conjunctions have long been considered thehardest of parsing problems.There are many ways in which this researchshould be continued.
First, the error analysis needsto be improved.
Our tentative guess for why sen-tences with unknown words failed to improve shouldbe verified or disproven.
Second, there are manyother ways to use self-trained information in pars-ing.
Indeed, the current research was undertakenas the control experiment in a program to try muchmore complicated methods.
We still have themto try: restricting consideration to more accuratelyparsed sentences as training data (sentence selec-tion), trying to learn grammatical generalizations di-rectly rather than simply including the data for train-ing, etc.Next there is the question of practicality.
In termsof speed, once the data is loaded, the new parser ispretty much the same speed as the old ?
just un-der a second a sentence on average for treebank sen-tences.
However, the memory requirements are lar-gish, about half a gigabyte just to store the data.
Weare making our current best self-trained parser avail-able3 as machines with a gigabyte or more of RAMare becoming commonplace.
Nevertheless, it wouldbe interesting to know if the data can be pruned to3ftp://ftp.cs.brown.edu/pub/nlparser158make the entire system less bulky.Finally, there is also the nature of the self-traineddata themselves.
The data we use are from the LATimes.
Those of us in parsing have learned to expectsignificant decreases in parsing accuracy even whenmoving the short distance from LA Times to WallStreet Journal.
This seemingly has not occurred.Does this mean that the reranking parser somehowovercomes at least small genre differences?
On thispoint, we have some pilot experiments that showgreat promise.AcknowledgmentsThis work was supported by NSF grants LIS9720368, andIIS0095940, and DARPA GALE contract HR0011-06-2-0001.We would like to thank Michael Collins, Brian Roark, JamesHenderson, Miles Osborne, and the BLLIP team for their com-ments.ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the 11th Annual Conference on ComputationalLearning Theory (COLT-98).Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 173?180, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of the Fourteenth National Conference on Artifi-cial Intelligence, Menlo Park.
AAAI Press/MIT Press.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In 1st Annual Meeting of the NAACL.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Arivind Joshi and Martha Palmer, editors,Proceedings of the Thirty-Fourth Annual Meeting ofthe Association for Computational Linguistics.Stephen Clark, James Curran, and Miles Osborne.
2003.Bootstrapping POS-taggers using unlabelled data.
InProceedings of CoNLL-2003.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Machine Learning: Pro-ceedings of the 17th International Conference (ICML2000), pages 175?182, Stanford, California.Sanjoy Dasgupta, M.L.
Littman, and D. McAllester.2001.
PAC generalization bounds for co-training.
InAdvances in Neural Information Processing Systems(NIPS), 2001.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Conference on Empirical Methods in Natu-ral Language Processing (EMNLP).David Graff.
1995.
North American News Text Corpus.Linguistic Data Consortium.
LDC95T21.James Henderson.
2004.
Discriminative training of aneural network statistical parser.
In Proc.
42nd Meet-ing of Association for Computational Linguistics (ACL2004), Barcelona, Spain.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational Linguistics,19(1):103?120.Liang Huang and David Chang.
2005.
Better k-best pars-ing.
Technical Report MS-CIS-05-08, Department ofComputer Science, University of Pennsylvania.Victor M. Jimenez and Andres Marzal.
2000.
Computa-tion of the n best parse trees for weighted and stochas-tic context-free grammars.
In Proceedings of the JointIAPR International Workshops on Advances in PatternRecognition.
Springer LNCS 1876.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochas-tic ?unification-based?
grammars.
In The Proceedingsof the 37th Annual Conference of the Association forComputational Linguistics, pages 535?541, San Fran-cisco.
Morgan Kaufmann.Dan Klein and Christopher Manning.
2002.
A genera-tive constituent-context model for improved grammarinduction.
In Proceedings of the 40th Annual Meetingof the ACL.Michell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Anoop Sarkar.
2001.
Applying cotraining methods tostatistical parsing.
In Proceedings of the 2001 NAACLConference.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In Pro-ceedings of EACL 03.159
