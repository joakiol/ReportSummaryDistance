Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 566?571,Dublin, Ireland, August 23-24, 2014.SINAI: Voting System for Aspect Based Sentiment AnalysisSalud Mar?
?a Jim?enez-Zafra, Eugenio Mart??nez-C?amara,M.
Teresa Mart?
?n-Valdivia, L. Alfonso Ure?na-L?opezSINAI Research GroupUniversity of Ja?enE-23071, Ja?en (Spain){sjzafra, emcamara, maite, laurena}@ujaen.esAbstractThis paper describes the participation ofthe SINAI research group in Task 4 of the2014 edition of the International Work-shop SemEval.
This task is concernedwith Aspect Based Sentiment Analysisand its goal is to identify the aspects ofgiven target entities and the sentiment ex-pressed towards each aspect.1 IntroductionThe web has evolved progressively since its be-ginning in 1990.
At first, the user was almost apassive subject who received the information orpublished it, without many possibilities to gener-ate an interaction.
The emergence of the Web 2.0was a social revolution, because it offered usersthe possibility of producing and sharing contents,opinions, experiences, etc.Some years ago it was common to ask familyand friends to know their opinion about a particu-lar topic, but after the emergence of the Web 2.0,the number of Internet users has been greatly in-creased.
The exponential growth of the subjectiveinformation in the last years has created a great in-terest in the treatment of this information.Opinion Mining (OM), also known as Senti-ment Analysis (SA) is the discipline that focuseson the computational treatment of opinion, sen-timent and subjectivity in texts (Pang and Lee,2008).
Currently, OM is a trendy task in the fieldof Natural Language Processing due mainly to thefact of the growing interest in the knowledge ofthe opinion of people from different sectors of thesociety.
However, the study on Opinion Mininggoes back to 2002 when two of the most cited arti-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/cles in this task were published (Pang et al., 2002)(Turney, 2002).OM or SA can be divided into two subtasksthat are known as subjectivity classification andpolarity classification.
Subjectivity classificationis the task concentrated on the identification ofsubjectivity in texts, that is, these systems are bi-nary classifiers that separate the documents in twoclasses, objective and subjective ones.
On theother hand, polarity classification is the task of de-termining the semantic orientation of a subjectivetext.
The ideal OM system has to be composedby a subjectivity classifier and a polarity classifier.However, most of the works in the field of OM arecarried out considering the documents as subjec-tive, so polarity classification systems have beenmore studied than subjectivity classification ones.The reader can find a complete overview about theresearch in OM in (Pang and Lee, 2008) and (Liu,2012).As Liu asserts in (Liu, 2012), the polarity clas-sification systems can be divided into three levels:?
Document level polarity classification:This kind of systems assumes that each doc-ument expresses an opinion on a single entity(Pang et al., 2002) (Turney, 2002).?
Sentence level polarity classification: Inthis case the polarity classification systemsare focused on the identification of the levelof polarity of each sentence of the docu-ment (Wilson et al., 2005) (Yu and Hatzivas-siloglou, 2003).?
Entity and Aspect level polarity classifi-cation: These systems accomplish a finer-grained sentiment classification.
Whereas thedocument-level and sentiment-level only dis-cover the overall sentiment expressed by theauthor, the goal of the entity and aspect po-larity classification is the identification of the566sentiment of the author towards each entity oraspect.An entity usually is composed by several as-pects, for example a telephone is formed by aheadset, which also consists of a speaker and anearphone.
An entity can be regarded as a hierarchyof all the aspects whose head is the entity, so theentity can also be considered as an aspect or gen-eral aspect.
Therefore, the task ?entity and aspectlevel polarity classification?
can be called ?aspectpolarity classification?.The main objective of OM at aspect level is todiscover every quintuple (ei, aij, sijkl, hk, tl) ina given document, where eiis the entity, aijisone of the aspects of the entity or the entity andsijklis the orientation of the opinion expressedby the opinion holder hkin a certain moment tl.To achieve the objective of populate the quintu-ple is needed the splitting of the task into severalsubtasks that correspond with the identification ofthe aspect, the author or the holder of the opinionand the moment when the opinion is expressed orposted.
But in a real scenario, OM at aspect levelis also limited like OM at sentence and documentlevel, and most of the research works are only fo-cused on the identification of the aspect and in thecalculation of the level of intensity of the senti-ment stated about the aspect.
However, there aresome papers that are closely to the goal of findingout each of the components of the quintuple (Kimand Hovy, 2004) (Kim and Hovy, 2006).The task four of the 2014 edition of SemEvalworkshop aims to promote the research polarityclassification systems at aspect level.
The task isdivided into four subtasks, two of them related tothe aspect identification and the other with the po-larity classification.
Due to the fact that OM is adomain-dependent task, the organization proposesthe four subtasks in two different domains, Restau-rants and Laptops.
Task one and three are theones linked to the aspect identification.
Subtaskone is focused on the identification of the aspectsin each review of the two given corpus.
Subtaskthree goes one step further, in which the main ob-jective is for a given predefined set of aspect cate-gories, identify the aspect categories discussed inthe given sentence.
Subtask two proposes the clas-sification of the sentiment expressed by the authorabout each of the aspects extracted, and subtaskfour has as challenge the classification of the po-larity of each of the categories of the aspects.
Awider description of the task and the datasets usedcan be found in the task description paper (Pontikiet al., 2014).The rest of the paper is organized as follows.Section two outlines the two main parts of our pro-posed system, firstly the strategy to solve the sub-task 1 and 2 and then the method used to resolvethe subtask 3 and 4.
To sum up the paper, an anal-ysis of the results and the conclusion of this workare shown in section three and four respectively.2 System descriptionThe guidelines of this task indicate that each teammay submit two runs: constrained (using only theprovided training data and other resources, such aslexicons) and unconstrained (using additional datafor training).
We decided to follow an unsuper-vised approach that we present below.Our system is divided into two subsystems (Fig-ure 1).
The aim of the first subsystem is to extractthe aspect terms related to a given target entity(subtask 1) and calculate the sentiment expressedtowards each aspect in the opinion (subtask 2).The goal of the second is, for a given set of cat-egories, to identify the categories discussed in thereview (subtask 3) and determine its polarity (sub-task 4).2.1 Subsystem 1: Aspects Identification andPolarity ClassificationTo identify the aspects related with the target en-tity (laptops or restaurants) we decided to usea bag of words built from all the aspect termspresent in the training data.
But this methodonly detects previously tagged aspect in the train-ing data, so, we enriched the list of words withdata automatically extracted from the collabora-tive knowledge base Freebase1, in order to im-prove the identification.
For this, we obtainedall categories in restaurants domain and in com-puters domain2(types in a domain) using MQL3(Metaweb Query Language) (Figure 2).Then, for each domain category we extracted allterms (instances of a type) to enrich the bag.
InFigure 3 we can see an example to get all terms of1http://www.freebase.com/2Nowadays, Freebase has more than 70 different domains.But, for this task, we are only interested in these two.3MQL is a language which is used to express Metawebqueries.
This allows you to incorporate knowledge from theFreebase database into your own applications and websites.567Figure 1: Arquitecture of the system.Figure 2: Query for list all categories in food do-main.a category, in particular cheese category of fooddomain.Figure 3: Query for list all term in cheese category.In this way, given a review of the test data, thefirst step is to tokenize it to get a vector of uni-grams with all single words in the text (we do notdivide the reviews into sentences because there isonly one sentence per review).
The second stepis to represent each review as a list of n lists ofunigrams, bigrams, .
.
.
, n-grams where n is thenumber of tokens in the sentence.
This is becausean aspect term can be a nominal phrase, a wordformed from a verb but functioning as a differentpart of speech (e.g.
gerunds and participles) or asimple term.
For example, the review ?The saladwas excellent as was the lamb chettinad?
is repre-sented as shown in Figure 4.After obtaining the possible terms of a review,the next step is to go over the list of lists to ex-tract the aspects.
Each list is traversed backwardsmatching each term with each aspect from the bag.When an aspect is found or the top of the list isreached the search begins in the next list.
In thereview showed in Figure 4, the system will iden-tify two aspects: salad and lamb chettinad.
Thesearch in this example begins in the list 1 with?The salad was excellent as was the lamb chetti-nad?, ends with ?The?
and continues with the nextlist, because the top of the list is reached.
Thesearch in the list 2 begins with ?salad was excel-lent as was the lamb chettinad?, ends with ?salad?because it is an aspect and continues with the list3 and so on.
At last, the search in the list 8 be-gins with the term ?lamb chettinad?, ends with itbecause it is an aspect presents in the bag of wordsand continues with the list 9.Once extracted the aspects related with the tar-get entity, the next step is to determine the wordsthat modify each aspect.
For this, we have usedthe Stanford Dependencies Parser4.
This parser4http://nlp.stanford.edu/software/lex-parser.shtml568Figure 4: Possible terms of the sentence ?The salad was excellent as was the lamb chettinad?.was designed to provide a simple description ofthe grammatical relationships in a sentence thatcan easily be understood and effectively used bypeople without linguistic expertise who want toextract textual relations (De Marneffe and Man-ning, 2008).
It represents all sentence relation-ships uniformly as typed dependency relations.
Inthis work, we have considered the main relation-ships for expressing opinion about an aspect: us-ing a verb (?nsubj?
or ?nsubjpass?
), an adjectivalmodifier (?amod?)
or a dependency relation withanother word (?dep?).
In the review ?The saladwas excellent as was the lamb chettinad?, the sys-tem will identify two modifiers words: the ad-jective excellent that expresses how is the saladthrough the relationship ?nsubj?
and the adjectiveexcellent that also modified the aspect lamb chet-tinad through the relationship ?dep?
Figure 5.To determine the sentiment expressed over anaspect we have calculated the polarity of eachword that modifies it through a voting systembased on three classifiers: Bing Liu Lexicon (Huand Liu, 2004), SentiWordNet (Baccianella et al.,2010) and MPQA (Wilson et al., 2005).
The BingLiu Lexicon is a list of 2006 positive words andanother with 4783 negative ones.
MPQA is alsoa subjectivity lexicon with positive and negativewords and has extra information about each one:the part-of-speech, the strength, etc.
Finally, Sen-tiWordNet is a lexical resource that assigns to eachsynset of WordNet three sentiment scores: positiv-ity, negativity and objectivity.
Therefore, an aspectis positive/negative if there are at least two clas-sifiers that tag it as positive/negative and neutralin another case.
It may happen that a word is af-fected by negation, to treat this problem we haveused a straightforward method, the fixed windowsize method.
We have considered the negative par-ticles: ?not?, ?n?t?, ?no?, ?never?.
So if any of thepreceding or following 3 words to one aspect isone of these negative particles, the aspect polarityis reversed (positive ?> negative, negative ?>positive, neutral ?> neutral).In the example showed in Figure 5, the aspectsalad is modified by the word excellent that alsomodified the aspect lamb chettinad.
This adjectiveis part of the Bing Liu positive list, MPQA classi-fies it as positive and SentiWordNet assigns it thescores: 1 (positivity), 0(objectivity), 0 (objectiv-ity).
Then, the aspects salad and lamb chettinadare classified as positive by the voting system.Figure 5: Dependency analysis of the sentence:?The salad was excellent as was the lamb chetti-nad?.5692.2 Subsystem 2: Categories Identificationand Polarity ClassificationAs we have mentioned above, this subsystem fo-cuses on the treatment of the categories and hasbeen used only with the dataset of restaurants.On the one hand, we have built a bag of wordsfor each of the given categories related to the tar-get entity (restaurants).
We have tagged manu-ally each aspect of the bag of words, built for thefirst subsystem, in one of the categories of thegiven set (food, service, price, ambience, anec-dotes/miscellaneous).
Thus, to determine the cat-egories that are referenced in a review we havesearched each aspect identified with the first sub-system in each bag, if the aspect belongs to anycategory then this category is identified.
If any as-pect belongs to a category, then the category allo-cated is ?anecdotes/miscellaneous?.On the other hand, the sentiment expressedabout each category has been calculated as themost frequent polarity of the aspects that belongsto this category.
In case of a tie between positiveand negative values, the polarity value conflict isassigned to the category.
If any aspect belongs tothe category, then the polarity value of the reviewis assigned to the category.In the above example, the aspects salad andlamb chettinad belong to food?s bag of words, sothat the system will identify that the category foodis discussed in this review and will assign it thepolarity value positive, because the sentiment ex-pressed about the two aspects that belongs to thiscategory is positive.3 Analysis of the resultsThe aim of this section is to provide a meaningfulreport of the results obtained after participation inthe task related to Aspect Based Sentiment Anal-ysis (ABSA).
Table 1 shows the evaluation resultsfor the aspect extraction subtask.
As we can see,the recall overcomes the mean value of results ofparticipants in both domains (laptops and restau-rants), that is, the system identifies quite aspectsof the corpus.
However, the precision is lowerbecause the system identifies aspects that are notconsidered by the organization, due to the fact thatour bag of words contains more aspects than thetagged by the organization.The results reached in the aspect term extractionsubtask are similar (Table 2).
It should be takeninto account that the system is a general-domainLaptops RestaurantsSINAI Average SINAI AveragePrecision 0.3729 0.6890 0.5961 0.7674Recall 0.5765 0.5045 0.72487 0.6726F-score 0.4529 0.5620 0.6542 0.7078Table 1: Aspect Term Extraction results.sentiment classifier, so it does not use specificknowledge for each of the domains.
This fact canbe shown in the results reached in the task of po-larity classification for the two domains, which aresimilar.
Therefore, this subtask could be improvedby taking into account the domain and other rela-tionships for expressing opinion about an aspectapart from that we have treated (?nsubj?, ?nsubj-pass?, ?amod?, ?dep?
).Laptops RestaurantsSINAI Average SINAI AverageAccuracy 0.5872 0.5925 0.5873 0.6910Table 2: Aspect Term Polarity results.On the other hand, the results in the identifica-tion of the categories discussed in a review havebeen high (Table 3) and even overcome the aver-age recall of the participating systems.
At last, Ta-ble 4 shows the result evaluation of the aspect cat-egory polarity subtask that are slightly lower thanthe average.
These tables show that is possible toreach good results using a simple approach as de-scribed in subsection 2.2.RestaurantsSINAI AveragePrecision 0.6659 0.76Recall 0.8244 0.7226F-score 0.7367 0.7379Table 3: Aspect Category Detection results.4 Conclusion and future worksIn SA can be differentiated three levels of study ofa text: document level, sentence level and aspectlevel.
The document level analysis determines theoverall sentiment expressed in a review, while thesentence level analysis specifies for each sentenceof a text, whether express a positive, negative orneutral opinion.
However, these two types of anal-570RestaurantsSINAI AverageAccuracy 0.6030 0.6951Table 4: Aspect Category Polarity results.ysis do not reach the level of detail that an userwants when searches for information about a prod-uct.
The fact that the overall sentiment of a prod-uct is positive does not mean that the author has apositive opinion about all aspects of that product,or the fact that is negative does not involve thateverything about the product is bad.In addition, the large amount of sources andthe high volume of texts with reviews, make dif-ficult for the user to select information of interest.Therefore, it is necessary to develop classificationsystems at aspect level that help users to make de-cisions and, on the other hand, that show compa-nies the opinion that consumers have about theirproducts, in order to help them to decide what tokeep, what to delete and what to improve.In this paper we have presented our first ap-proach for the Aspect Based Sentiment Analysisthat has been developed for the task four of the2014 edition of SemEval workshop.
After analyz-ing the evaluation results we consider that is pos-sible to introduce some improvements we are cur-rently working: domain adaptation in the polaritycalculation, consideration of other relationshipsto determine which words modify an aspect andtreatment of negation (in the system proposed wehave used the fixed window size method).
Also, ina near future we will try to extrapolate it to Span-ish reviews.AcknowledgmentsThis work has been partially supported by a grantfrom the Fondo Europeo de Desarrollo Regional(FEDER), ATTOS project (TIN2012-38536-C03-0) from the Spanish Government, AORESCUproject (P11-TIC-7684 MO) from the regionalgovernment of Junta de Andaluc?
?a and CEATIC-2013-01 project from the University of Ja?en.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In LREC, volume 10, pages 2200?2204.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
Stanford typed dependencies manual.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, New York, NY, USA.
ACM.Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics, COLING ?04, Stroudsburg, PA, USA.Soo-Min Kim and Eduard Hovy.
2006.
Extractingopinions, opinion holders, and topics expressed inonline news media text.
In Proceedings of the Work-shop on Sentiment and Subjectivity in Text, SST ?06,pages 1?8, Stroudsburg, PA, USA.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: Sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 Conference on Empirical Methods in Natu-ral Language Processing - Volume 10, EMNLP ?02,pages 79?86, Stroudsburg, PA, USA.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval).Peter D. Turney.
2002.
Thumbs up or thumbs down?
:Semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, ACL ?02, pages 417?424, Stroudsburg, PA,USA.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354, Stroudsburg, PA, USA.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opin-ion sentences.
In Proceedings of the 2003 Confer-ence on Empirical Methods in Natural LanguageProcessing, EMNLP ?03, pages 129?136, Strouds-burg, PA, USA.571
