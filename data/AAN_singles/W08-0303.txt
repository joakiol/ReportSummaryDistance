Proceedings of the Third Workshop on Statistical Machine Translation, pages 18?25,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsDiscriminative Word Alignment via Alignment Matrix ModelingJan NiehuesInstitut fu?r Theoretische InformatikUniversita?t Karlsruhe (TH)Karlsruhe, Germanyjniehues@ira.uka.deStephan VogelLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAstephan.vogel@cs.cmu.eduAbstractIn this paper a new discriminative word align-ment method is presented.
This approachmodels directly the alignment matrix by a con-ditional random field (CRF) and so no restric-tions to the alignments have to be made.
Fur-thermore, it is easy to add features and so allavailable information can be used.
Since thestructure of the CRFs can get complex, the in-ference can only be done approximately andthe standard algorithms had to be adapted.
Inaddition, different methods to train the modelhave been developed.
Using this approach thealignment quality could be improved by upto 23 percent for 3 different language pairscompared to a combination of both IBM4-alignments.
Furthermore the word alignmentwas used to generate new phrase tables.
Thesecould improve the translation quality signifi-cantly.1 IntroductionIn machine translation parallel corpora are one veryimportant knowledge source.
These corpora are of-ten aligned at the sentence level, but to use themin the systems in most cases a word alignment isneeded.
Therefore, for a given source sentence fJ1and a given target sentence eI1 a set of links (j, i) hasto be found, which describes which source word fjis translated into which target word ei.Most SMT systems use the freely availableGIZA++-Toolkit to generate the word alignment.This toolkit implements the IBM- and HMM-models introduced in (Brown et al, 1993; Vogel etal., 1996).
They have the advantage that they aretrained unsupervised and are well suited for a noisy-channel approach.
But it is difficult to include addi-tional features into these models.In recent years several authors (Moore et al,2006; Lacoste-Julien et al, 2006; Blunsom andCohn, 2006) proposed discriminative word align-ment frameworks and showed that this leads to im-proved alignment quality.
In contrast to generativemodels, these models need a small amount of hand-aligned data.
But it is easy to add features to thesemodels, so all available knowledge sources can beused to find the best alignment.The discriminative model presented in this pa-per uses a conditional random field (CRF) to modelthe alignment matrix.
By modeling the matrix norestrictions to the alignment are required and evenn:m alignments can be generated.
Furthermore, thismakes the model symmetric, so the model will pro-duce the same alignment no matter which languageis selected as source and which as target language.In contrast, in generative models the alignment is afunction where a source word aligns to at most onetarget word.
So the alignment is asymmetric.The training of this discriminative model has to bedone on hand-aligned data.
Different methods weretested.
First, the common maximum-likelihood ap-proach was used.
In addition to this, a method tooptimize the weights directly towards a word align-ment metric was developed.The paper is structured as follows: Section 2 and3 present the model and the training.
In Section 4the model is evaluated in the word alignment task aswell as in the translation task.
The related work andthe conclusion are given in Sections 5 and 6.18Figure 1: Alignment Example2 The ModelIn the approach presented here the word alignmentmatrix is modeled by a conditional random field(CRF).
A CRF is an unidirectional graphical model.It models the conditional distribution over randomvariables.
In most applications like (Tseng et al,2005; Sha and Pereira, 2003), a sequential model isused.
But to model the alignment matrix the graphi-cal structure of the model is more complex.The alignment matrix is described by a randomvariable yji for every source and target word pair(fj , ei).
These variables can have two values, 0and 1, indicating whether these words are transla-tions of each other or not.
An example is shownin Figure 1.
Gray circles represent variables withvalue 1, white circles stand for variables with value0.
Consequently, a word with zero fertility is indi-rectly modeled by setting all associated random vari-ables to a value of 0.The structure of the CRF is described by a fac-tored graph like it was done, for example, in (Lanet al, 2006).
In this bipartite graph there are twodifferent types of nodes.
First, there are hiddennodes, which correspond to the random variables.The second type of nodes are the factored nodes c. These are not drawn in Figure 1 to keep the pic-ture clear, but they are shown in Figure 2.
Theydefine a potential ?c on the random variables Vcthey are connected to.
This potential is used todescribe the probability of an alignment based onthe information encoded in the features.
This po-tential is a log-linear combination of some featuresFc(Vc) = (f1(Vc), .
.
.
, fn(Vc)) and it can be writtenas:?c(Vc) = exp(?
?
Fc(Vc)) = exp(?k?k ?
fk(Vc))(1)with the weights ?.
Then the probability of anassignment of the random variables, which corre-sponds to a word alignment, can be expressed as:p?
(y|e, f) =1Z(e, f)?c?VFN?c(Vc) (2)with VFN the set of all factored nodes in the graph,and the normalization factor Z(e, f) defined as:Z(e, f) =?Y?c?VFN?c(Vc) (3)where Y is the set of all possible alignments.In the presented model there are four differenttypes of factored nodes corresponding to four groupsof features.2.1 FeaturesOne main advantage of the discriminative frame-work is the ability to use all available knowledgesources by introducing additional features.
Differ-ent features have been developed to capture differentaspects of the word-alignment.The first group of features are those that dependonly on the source and target words and may there-fore be called local features.
Consequently, thefactored node corresponding to such a feature isconnected to one random variable only (see Figure2(a)).
The lexical features, which represent the lexi-cal translation probability of the words belong to thisgroup.
In our experiments the IBM4-lexica in bothdirections were used.
Furthermore, there are sourceand target normalized lexical features for every lexi-con.
The source normalized feature, for example, isnormalized in a way, that all translation probabilitiesof one source word to target words in the sentencessum up to one as shown in equation 4.psourceN (fj , ei) =plex(fj , ei)?1?j?J plex(fj , ei)(4)ptargetN (fj , ei) =plex(fj , ei)?1?i?I plex(fj , ei)(5)19Figure 2: Different features(a) Local features (b) Fertility features (c) First order featuresThey compare the possible translations in one sen-tence similar to the rank feature used in the approachpresented by Moore (2006).
In addition, the follow-ing local features are used: The relative distance ofthe sentence positions of both words.
This shouldhelp to aligned words that occur several times in thesentence.
The relative edit distance between sourceand target word was used to improve the align-ment of cognates.
Furthermore a feature indicatingif source and target words are identical was addedto the system.
This helps to align dates, numbersand names, which are quite difficult to align usingonly lexical features since they occur quite rarely.In some of our experiments the links of the IBM4-alignments are used as an additional local feature.In the experiments this leads to 22 features.
Lastly,there are indicator features for every possible com-bination of Parts-of-Speech(POS)-tags and for Nwhigh frequency words.
In the experiments the 50most frequent words were used, which lead to 2500features and around 1440 POS-based features wereused.
The POS-feature can help to align words, forwhich the lexical features are weak.The next group of features are the fertility fea-tures.
They model the probability that a word trans-lates into one, two, three or more words, or does nothave any translation at all.
The corresponding fac-tored node for a source word is connected to all Irandom variables representing the links to the targetwords, and the node for a target word is connectedto all the J nodes for the links to source words (s.Figure 2(b)).
In this group of features there are twodifferent types.
First, there are indicator features forthe different fertilities.
To reduce the complexity ofthe calculation this is only done up to a given max-imal fertility Nf and there is an additional indicatorfeature for all fertilities larger than Nf .
This is anextension of the empty word indicator feature usedin other discriminative word alignment models.
Fur-thermore, there is a real-valued feature, which canuse the GIZA++ probabilities for the different fer-tilities.
This has the advantage compared to the in-dicator feature that the fertility probabilities are notthe same for all words.
But here again, all fertilitieslarger than a givenNf are not considered separately.In the evaluation Nf = 3 was selected.
So 12 fertil-ity features were used in the experiments.The first-order features model the first-order de-pendencies between the different links.
They aregrouped into different directions.
The factored nodefor the direction (s, t) is connected to the variablenodes yji and y(j+s)(i+t).
For example, the mostcommon direction is (1, 1), which describes the sit-uation that if the words at positions j and i arealigned, also the immediate successor words in bothsentences are aligned as shown in Figure 2(c).
Inthe default configuration the directions (1, 1), (2, 1),(1, 2) and (1,?1) are used.
So this feature is able toexplicitly model short jumps in the alignment, likein the directions (2, 1) and (1, 2) as well as crossinglinks like in the directions (1,?1).
Furthermore, itcan be used to improve the fertility modeling.
If aword has got a fertility of two, it is often aligned totwo consecutive words.
Therefore, for example inthe Chinese-English system the directions (1, 0) and(0, 1) were used in addition.
This does not mean,that other directions in the alignment are not possi-ble, but other jumps in the alignment do not improvethe probability of the alignment.
For every direction,an indicator feature that both links are active and anadditional one, which also depends on the POS-pairof the first word pair is used.
For a configurationwith 4 directions this leads to 4 indicator featuresand, for example, 5760 POS-based features.20The last group of features are phrase features,which are introduced to model context dependen-cies.
First a training corpus is aligned.
Then, groupsof source and target words are extracted.
Wordsbuild a group, if all source words in the group arealigned to all target words.
The relative frequencyof this alignment is used as the feature and indicatorfeatures for 1 : 1, 1 : n, n : 1 and n : m alignments.The corresponding factored node is connected to alllinks that are important for this group.2.2 AlignmentThe structure of the described CRF is quite complexand there are many loops in the graphical structure,so the inference cannot be done exactly.
For exam-ple, the random variables y(1,1) and y(1,2) as well asy(2,1) and y(2,2) are connected by the source fertil-ity nodes of the words f1 and f2.
Furthermore thevariables y(1,1) and y(2,1) as well as y(1,2) and y(2,2)are connected by the target fertility nodes.
So thesenodes build a loop as shown in Figure 2(b).
The firstorder feature nodes generate loops as well.
Conse-quently an approximation algorithm has to be used.We use the belief propagation algorithm introducedin (Pearl, 1966).
In this algorithm messages consist-ing of a pair of two values are passed along the edgesbetween the factored and hidden nodes for several it-erations.
In each iterations first messages from thehidden nodes to the connected factored nodes aresent.
These messages describe the belief about thevalue of the hidden node calculated from the incom-ing messages of the other connected factored nodes.Afterwards the messages from the factored nodesto the connected hidden nodes are send.
They arecalculated from the potential and the other incom-ing messages.
This algorithm is not exact in loopygraphs and it is not even possible to prove that it con-verges, but in (Yedidia et al, 2003) it was shown,that this algorithm leads to good results.The algorithm cannot be used directly, since thecalculation of the message sent from a factored nodeto a random variable has an exponential runtimein the number of connected random variables.
Al-though we limit the number of considered fertili-ties, the number of connected random variables canstill be quite large for the fertility features and thephrase features, especially in long sentences.
To re-duce this complexity, we leverage the fact that thepotential can only have a small number of differentvalues.
This will be shown for the fertility featurenode.
For a more detailed description we refer to(Niehues, 2007).
The message sent from a factorednode to a random variable is defined in the algorithmas:mc?
(j,i)(v) =?Vc/v?c(Vc) (6)?(j,i)??N(c)/(j,i)n(j,i)??c(v?
)where Vc is the set of random variables connectedto the factored node and?Vc/v is the sum over allpossible values of Vc where the random variable yjihas the the value v. So the value for the message iscalculated by looking at every possible combinationof the other incoming messages.
Then the belief forthis combination is multiplied with the potential ofthis combination.
This can be rewritten, since thepotential only depends on how many links are active,not on which ones are active.mc?
(j,i)(v) =Nf?n=0?c(n+ v) ?
?
(n) (7)+ ?c(Nf + 1) ?
?
(Nf + 1)with ?
(n) the belief for a fertility of n of the otherconnected nodes and ?
(Nf+1) the belief for a fertil-ity bigger than Nf with ?c(Nf + 1) the correspond-ing potential.
The belief for a configuration of somerandom variables is calculated by the product overall out-going messages.
So ?
(n) is calculated by thesum over all possible configurations that lead to afertility of n over these products.?
(n) =?Vc/v:|Vc|=n?(j,i)??Vc/(j,i)n(j,i)??c(v?)?
(Nf + 1) =?Vc/v:|Vc|>Nf?(j,i)??Vc/(j,i)n(j,i)??c(v?
)The values of the sums can be calculated in lineartime using dynamic programming.3 TrainingThe weights of the CRFs are trained using a gradientdescent for a fixed number of iterations, since thisapproach leads already to quite good results.
In the21experiments 200 iterations turned out to be a goodnumber.The default criteria to train CRFs is to maximizethe log-likelihood of the correct solution, which isgiven by a manually created gold standard align-ment.
Therefore, the feature values of the gold stan-dard alignment and the expectation values have to becalculated for every factored node.
This can be doneusing again the belief propagation algorithm.Often, this hand-aligned data is annotated withsure and possible links and it would be nice, if thetraining method could use this additional informa-tion.
So we developed a method to optimize theCRFs towards the alignment error rate (AER) or theF-score with sure and possible links as introducedin (Fraser and Marcu, 2007).
The advantage of theF-score is, that there is an additional parameter ?,which allows to bias the metric more towards pre-cision or more towards recall.
To be able to usea gradient descent method to optimize the weights,the derivation of the word alignment metric with re-spect to these weights must be computed.
This can-not be done for the mentioned metrics since they arenot smooth functions.
We follow (Gao et al, 2006;Suzuki et al, 2006) and approximate the metrics us-ing the sigmoid function.
The sigmoid function usesthe probabilities for every link calculated by the be-lief propagation algorithm.In our experiments we compared the maximumlikelihood method and the optimization towards theAER.
We also tested combinations of both.
The bestresults were obtained when the weights were firsttrained using the ML method and the resulting fac-tors were used as initial values for the AER opti-mization.
Another problem is that the POS-basedfeatures and high frequency word features have alot more parameters than all other features and withthese two types of features overfitting seems to be abigger problem.
Therefore, these features are onlyused in a third optimization step, in which they areoptimized towards the AER, keeping all other fea-ture weights constant.
Initial results using a Gaus-sian prior showed no improvement.4 EvaluationThe word alignment quality of this approach wastested on three different language pairs.
On theSpanish-English task the hand-aligned data providedby the TALP Research Center (Lambert et al, 2005)was used.
As proposed, 100 sentences were used asdevelopment data and 400 as test data.
The so called?Final Text Edition of the European Parliament Pro-ceedings?
consisting of 1.4 million sentences andthis hand-aligned data was used as training corpus.The POS-tags were generated by the Brill-Tagger(Brill, 1995) and the FreeLing-Tagger (Asterias etal., 2006) for the English and the Spanish text re-spectively.
To limit the number of different tags forSpanish we grouped them according to the first 2characters in the tag names.A second group of experiments was done onan English-French text.
The data from the 2003NAACL shared task (Mihalcea and Pedersen, 2003)was used.
This data consists of 1.1 million sen-tences, a validation set of 37 sentences and a testset of 447 sentences, which have been hand-aligned(Och and Ney, 2003).
For the English POS-tagsagain the Brill Tagger was used.
For the French side,the TreeTagger (Schmid, 1994) was used.Finally, to test our alignment approach with lan-guages that differ more in structure a Chinese-English task was selected.
As hand-aligned data3160 sentences aligned only with sure links wereused (LDC2006E93).
This was split up into 2000sentences of test data and 1160 sentences of devel-opment data.
In some experiments only the first200 sentences of the development data were used tospeed up the training process.
The FBIS-corpus wasused as training corpus and all Chinese sentenceswere word segmented with the Stanford Segmenter(Tseng et al, 2005).
The POS-tags for both sideswere generated with the Stanford Parser (Klein andManning, 2003).4.1 Word alignment qualityThe GIZA++-toolkit was used to train a baselinesystem.
The models and alignment informationwere then used as additional knowledge source forthe discriminative word alignment.
For the first twotasks, all heuristics of the Pharaoh-Toolkit (Koehnet al, 2003) as well as the refined heuristic (Och andNey, 2003) to combine both IBM4-alignments weretested and the best ones are shown in the tables.
Forthe Chinese task only the grow-diag-final heuristicwas used.22Table 1: AER-Results on EN-ES taskName Dev TestIBM4 Source-Target 21.49IBM4 Target-Source 19.23IBM4 grow-diag 16.48DWA IBM1 15.26 20.82+ IBM4 14.23 18.67+ GIZA-fert.
13.28 18.02+ Link feature 12.26 15.97+ POS 9.21 15.36+ Phrase feature 8.84 14.77Table 2: AER-Results on EN-FR taskName Dev TestIBM4 Source-Target 8.6IBM4 Target-Source 9.86IBM4 intersection 5.38DWA IBM1 5.54 6.37+ HFRQ/POS 3.67 5.57+ Link Feature 3.13 4.80+ IBM4 3.60 4.60+ Phrase feature 3.32 4.30The results measured in AER of the discrimina-tive word alignment for the English-Spanish task areshown in Table 1.
In the experiments systems usingdifferent knowledge sources were evaluated.
Thefirst system used only the IBM1-lexica of both di-rections as well as the high frequent word features.Then the IBM4-lexica were used instead and inthe next system the GIZA++-fertilities were added.As next knowledge source the links of both IBM4-alignments were added.
Furthermore, the systemcould be improved by using also the POS-tags.
Forthe last system, the whole EPPS-corpus was alignedwith the previous system and the phrases were ex-tracted.
Using them as additional features, the bestAER of 14.77 could be reached.
This is an improve-ment of 1.71 AER points or 10% relative to the bestbaseline system.Similar experiments have also been done for theEnglish-French task.
The results measured in AERare shown in Table 2.
The IBM4 system usesthe IBM4 lexica and links instead of the IBM1sTable 3: AER-Results on CH-EN taskName TestIBM4 Source-target 44.94IBM4 Target-source 37.43IBM4 Grow-diag-final 35.04DWA IBM4 30.97- similarity 30.24+ Add.
directions 27.96+ Big dev 27.26+ Phrase feature 27.00+ Phrase feature(high P.) 26.90and adds the GIZA++-fertilities.
For the ?phrasefeature?-system the corpus was aligned with the?IBM4?-system and the phrases were extracted.This led to the best result with an AER of 4.30.
Thisis 1.08 points or 20% relative improvement over thebest generative system.
One reason, why less knowl-edge sources are needed to be as good as the base-line system, may be that there are many possiblelinks in the reference alignment and the discrimina-tive framework can better adapt to this style.
So asystem using only features generated by the IBM1-model could already reach an AER of 4.80.In Table 3 results for the Chinese-English align-ment task are shown1.
The first system was onlytrained on the smaller development set and used thesame knowledge source than the ?IBM4?-systemsin the last experiment.
The system could be im-proved a little bit by removing the similarity fea-ture and adding the directions (0, 1) and (1, 0) tothe model.
Then the same system was trained onthe bigger development set.
Again the parallel cor-pus was aligned with the discriminative word align-ment system, once trained towards AER and oncemore towards precision, and phrases were extracted.Overall, an improvement by 8.14 points or 23% overthe baseline system could be achieved.These experiments show, that every knowledgesource that is available should be used.
For all lan-guages pairs additional knowledge sources lead toan improvement in the word alignment quality.
Aproblem of the discriminative framework is, thathand-aligned data is needed for training.
So the1For this task no results on the development task are givensince different development sets were used23Table 4: Translation results for EN-ESName Dev TestBaseline 40.04 47.73DWA 41.62 48.13Table 5: Translation results for CH-ENName Dev TestBaseline 27.13 22.56AER 27.63 23.85?F0.3 26.34 22.35F0.7 26.40 23.52?Phrase feature AER 25.84 23.42?Phrase feature F0.7 26.41 23.92?French-English dev set may be too small, since thebest system on the development set does not cor-respond to the best system on the test set.
And asshown in the Chinese-English task additional datacan improve the alignment quality.4.2 Translation qualitySince the main application of the word alignment isstatistical machine translation, the aim was not onlyto generate better alignments measured in AER, butalso to generate better translations.
Therefore, theword alignment was used to extract phrases and usethem then in the translation system.
In all translationexperiments the beam decoder as described in (Vo-gel, 2003) was used together with a 3-gram languagemodel and the results are reported in the BLUE met-ric.
For test set translations the statistical signifi-cance of the results was tested using the bootstraptechnique as described in (Zhang and Vogel, 2004).The baseline system used the phrases build with thePharaoh-Toolkit.The new word alignment was tested on theEnglish-Spanish translation task using the TC-Star07 development and test data.
The discriminativeword alignment (DWA) used the configuration de-noted by +POS system in Table 1.
With this con-figuration it took around 4 hours to align 100K sen-tences.
But, of course, generating the alignment canbe parallelized to speed up the process.
As shownin Table 4 the new word alignment could generatebetter translations as measured in BLEU scores.For the Chinese-English task some experimentswere made to study the effect of different trainingschemes.
Results are shown in Table 5.
The sys-tems used the MT?03 eval set as development dataand the NIST part of the MT?06 eval set was used astest set.
Scores significantly better than the baselinesystem are mark by a ?.
The first three systems useda discriminative word alignment generated with theconfiguration as the one described as ?+ big dev?-system in Table 3.
The first one was optimized to-wards AER, the other two were trained towards theF-score with an ?-value of 0.3 (recall-biased) and0.7 (precision-biased) respectively.
A higher pre-cision word alignment generates fewer alignmentlinks, but a larger phrase table.
For this task, theprecision seems to be more important.
So the sys-tem trained towards the AER and the F-score withan ?-value of 0.7 performed better than the othersystems.
The phrase features gave improved perfor-mance only when optimized towards the F-score, butnot when optimized towards the AER.5 Comparison to other workSeveral discriminative word alignment approacheshave been presented in recent years.
The one mostsimilar to ours is the one presented by Blunsomand Cohn (2006).
They also used CRFs, but theyused two linear-chain CRFs, one for every direc-tions.
Consequently, they could find the optimal so-lution for each individual CRF, but they still neededthe heuristics to combine both alignments.
Theyreached an AER of 5.29 using the IBM4-alignmenton the English-French task (compared to 4.30 of ourapproach).Lacoste-Julien et al (2006) enriched the bipartitematching problem to model also larger fertilities andfirst-or der dependencies.
They could reach an AERof 3.8 on the same task, but only if they also includedthe posteriors of the model of Liang et al (2006).Using only the IBM4-alignment they generated analignment with an AER of 4.5.
But they did not useany POS-based features in their experiments.Finally, Moore et al (2006) used a log-linearmodel for the features and performed a beam search.They could reach an AER as low as 3.7 with bothtypes of alignment information.
But they presentedno results using only the IBM4-alignment features.246 ConclusionIn this paper a new discriminative word alignmentmodel was presented.
It uses a conditional randomfield to model directly the alignment matrix.
There-fore, the algorithms used in the CRFs had to beadapted to be able to model dependencies betweenmany random variables.
Different methods to trainthe model have been developed.
Optimizing the F-score allows to generate alignments focusing moreon precision or on recall.
For the model a multitudeof features using the different knowledge sourceshave been developed.
The experiments showed thatthe performance could be improved by using theseadditional knowledge sources.
Furthermore, the useof a general machine learning framework like theCRFs enables this alignment approach to benefitfrom future improvements in CRFs in other areas.Experiments on 3 different language pairs haveshown that word alignment quality as well as trans-lation quality could be improved.
In the translationexperiments it was shown that the improvement issignificant at a significance level of 5%.ReferencesAtserias, J., B. Casas, E. Comelles, M. Gonza?lez, L.Padro?
and M. Padro?.
2006.
FreeLing 1.3: Syntacticand semantic services in an open-source NLP library.In LREC?06.
Genoa, Italy.P.
Blunsom and T. Cohn.
2006.
Discriminative wordalignment with conditional random fields.
In ACL?06,pp.
65-72.
Sydney, Australia.E.
Brill.
1995.
Transformation-based error-driven learn-ing and natural language processing: A case study inpart of speech tagging.
Computational Linguistics,21(4):543-565.P.F.
Brown, S. Della Pietra, V. J. Della Pietra, R. L. Mer-cer.
1993.
The Mathematic of Statistical MachineTranslation: Parameter Estimation.
ComputationalLinguistics, 19(2):263-311.A.
Fraser, D. Marcu.
2007.
Measuring Word AlignmentQuality for Statistical Machine Translation Computa-tional Linguistics, 33(3):293-303.S.
Gao, W. Wu, C. Lee, T. Chua.
2006.
A maximalfigure-of-merit (MFoM)-learning approach to robustclassifier design for text categorization.
ACM Trans.Inf.
Syst., 24(2):190-218.D.
Klein and C.D.
Manning.
2003.
Fast Exact Inferencewith a Factored Model for Natural Language Parsing.Advances in Neural Information Processing Systems15 (NIPS 2002), pp.
3-10.P.
Koehn, F. J. Och, D. Marcu.
2003.
Statisticalphrase-based translation.
In HTL-NAACL?03, pp.
48-54.
Morristown, New Jersey, USA.S.
Lacoste-Julien, B. Taskar, D. Klein, M. I. Jordan.2006.
Word alignment via quadratic assignment.
InHTL-NAACL?06.
New York, USA.P.
Lambert, A. de Gispert, R. Banchs and J. b. Marino.2005.
Guidelines for Word Alignment Evaluation andManual Alignment.
Language Resources and Evalua-tion, pp.
267-285, Springer.X.
Lan and S. Roth, D. P. Huttenlocher, M. J. Black.2006.
Efficient Belief Propagation with LearnedHigher-Order Markov Random Fields.
ECCV (2),Lecture Notes in Computer Science, pp.
269-282.P.
Liang, B. Taskar, D. Klein.
2006.
Alignment by agree-ment.
In HTL-NAACL?06, pp.
104-110.
New York,USA.R.
Mihalcea, T. Pedersen.
2003.
An Evaluation Exer-cise for Word Alignment.
In HLT-NAACL 2003 Work-shop, Building and Using Parallel Texts: Data DrivenMachine Translation and Beyond, pp.
1-6.
Edmon-ton,Canada.R.
C. Moore, W. Yih, A.
Bode.
2006.
Improved dis-criminative bilingual word alignment.
In ACL?06, pp.513-520.
Sydney, Australia.J.
Niehues.
2007.
Discriminative Word Alignment Mod-els.
Diplomarbeit at Universita?t Karlsruhe(TH).F.
J. Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguist,29(1):19-51.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.H.
Schmid.
1994.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
In NEMLAP?94.
Manchester,UK.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In HLT-NAACL?03, pp.
134?141.Edmonton, Canada.J.
Suzuki, E. McDermott, H. Isozaki.
2006.
Trainingconditional random fields with multivariate evaluationmeasures In ACL?06, pp 217-224.
Sydney, Australia.H.
Tseng, P. Chang, G. Andrew, D. Jurafsky and C. Man-ning.
2005.
A Conditional Random Field Word Seg-menter.
In SIGHAN-4.
Jeju, Korea.S.
Vogel, H. Ney, C. Tillmann.
1996.
HMM-based wordalignment in statistical translation.
In COLING?96,pp.
836-841.
Copenhagen, Denmark.S.
Vogel.
2003.
SMT Decoder Dissected: Word Reorder-ing.
In NLP-KE?03.
Bejing, China.J.
S. Yedidia, W. T. Freeman, Y. Weiss.
2003.
Un-derstanding belief propagation and its generalizations.Exploring artificial intelligence in the new millennium.Y.
Zhang and S. Vogel.
2004.
Measuring ConfidenceIntervals for MT Evaluation Metrics.
In TMI 2004.Baltimore, MD, USA.25
