Proceedings of the ACL Student Research Workshop, pages 52?58,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAutomated Collocation Suggestion for Japanese Second LanguageLearnersLis W. Kanashiro Pereira Erlyn Manguilimotan Yuji MatsumotoNara Institute of Science and TechnologyDepartment of Information Science{lis-k, erlyn-m, matsu}@is.naist.jpAbstractThis study addresses issues of Japanese lan-guage learning concerning word combinations(collocations).
Japanese learners may be ableto construct grammatically correct sentences,however, these may sound ?unnatural?.
In thiswork, we analyze correct word combinationsusing different collocation measures andword similarity methods.
While other methodsuse well-formed text, our approach makes useof a large Japanese language learner corpus forgenerating collocation candidates, in order tobuild a system that is more sensitive to con-structions that are difficult for learners.
Ourresults show that we get better results com-pared to other methods that use only well-formed text.1 IntroductionAutomated grammatical error correction isemerging as an interesting topic of natural lan-guage processing (NLP).
However, previous re-search in second language learning are focusedon restricted types of learners?
errors, such asarticle and preposition errors (Gamon, 2010;Rozovskaya and Roth, 2010; Tetreault et al2010).
For example, research for Japanese lan-guage mainly focuses on Japanese case particles(Suzuki and Toutanova, 2006; Oyama andMatsumoto, 2010).
It is only recently that NLPresearch has addressed issues of collocation er-rors.Collocations are conventional word combina-tions in a language.
In Japanese, ocha wo ireru??????
?1 [to make tea]?
and yume womiru ?
???
?2 [to have a dream]?
are exam-ples of collocations.
Even though their accurateuse is crucial to make communication preciseand to sound like a native speaker, learning them1 lit.
to put in tea2 lit.
to see a dreamis one of the most difficult tasks for second lan-guage learners.
For instance, the Japanese collo-cation yume wo miru [lit.
to see a dream] is un-predictable, at least, for native speakers of Eng-lish, because its constituents are different fromthose in the Japanese language.
A learner mightcreate the unnatural combination yume wo suru,using the verb suru (a general light verb meaning?do?
in English) instead of miru ?to see?.In this work, we analyze various Japanesecorpora using a number of collocation and wordsimilarity measures to deduce and suggest thebest collocations for Japanese second languagelearners.
In order to build a system that is moresensitive to constructions that are difficult forlearners, we use word similarity measures thatgenerate collocation candidates using a largeJapanese language learner corpus.
By employingthis approach, we could obtain a better resultcompared to other methods that use only well-formed text.The remainder of the paper is organized as fol-lows.
In Section 2, we introduce related work oncollocation error correction.
Section 3 explainsour method, based on word similarity andassociation measures, for suggesting collocations.In Section 4, we describe different wordsimilarity and association measures, as well asthe corpora used in our experiments.
The exper-imental setup and the results are described inSections 5 and 6, respectively.
Section 7 pointsout the future directions for our research.2 Related WorkCollocation correction currently follows a similarapproach used in article and preposition correc-tion.
The general strategy compares the learner'sword choice to a confusion set generated fromwell-formed text during the training phase.
Ifone or more alternatives are more appropriate tothe context, the learner's word is flagged as anerror and the alternatives are suggested as correc-tions.
To constrain the size of the confusion set,52similarity measures are used.
To rank the bestcandidates, the strength of association in thelearner?s construction and in each of thegenerated alternative construction are measured.For example, Futagi et al(2008) generatedsynonyms for each candidate string usingWordNet and Roget?s Thesaurus and used therank ratio measure to score them by theirsemantic similarity.
Liu et al(2009) also usedWordNet to generate synonyms, but usedPointwise Mutual Information as associationmeasure to rank the candidates.
Chang et al(2008) used bilingual dictionaries to derivecollocation candidates and used the log-likelihood measure to rank them.
One drawbackof these approaches is that they rely on resourcesof limited coverage, such as dictionaries, thesau-rus or manually constructed databases to gener-ate the candidates.
Other studies have tried tooffer better coverage by automatically derivingparaphrases from parallel corpora (Dahlmeierand Ng, 2011), but similar to Chang et al(2008),it is essential to identify the learner?s first lan-guage and to have bilingual dictionaries and par-allel corpora for every first language (L1) in or-der to extend the resulting system.
Another prob-lem is that most research does not actually takethe learners' tendency of collocation errors intoaccount; instead, their systems are trained onlyon well-formed text corpora.
Our work followsthe general approach, that is, uses similaritymeasures for generating the confusion set andassociation measures for ranking the best candi-dates.
However, instead of using only well-formed text for generating the confusion set, weuse a large learner corpus created by crawling therevision log of a language learning social net-working service (SNS), Lang-83.
Another workthat also uses data from Lang-8 is Mizumoto etal.
(2011), which uses Lang-8 in creating a large-scale Japanese learner?s corpus.
The biggest ben-efit of using such kind of data is that we can ob-tain in large scale pairs of learners?
sentences andtheir corrections assigned by native speakers.3 Combining Word Similarity and As-sociation Measures to Suggest Collo-cationsIn our work, we focus on suggestions for nounand verb collocation errors in ?noun wo verb(noun-?-verb)?
constructions, where noun is thedirect object of verb.
Our approach consists of3www.lang-8.comthree steps: 1) for each extracted tuple in the se-cond learner?s composition, we created a set ofcandidates by substituting words generated usingword similarity algorithms; 2) then, we measuredthe strength of association in the writer?s phraseand in each generated candidate phrase usingassociation measures to compute collocationscores; 3) the highest ranking alternatives aresuggested as corrections.
In our evaluation, wechecked if the correction given in the learnercorpus matches one of the suggestions given bythe system.
Figure 1 illustrates the method usedin this study.Figure 1 Word Similarity and AssociationMeasures combination method for suggestingcol-locations.We considered only the tuples that containnoun or verb error.
A real application, however,should also deal with error detection.
For eachexample of the construction on the writer?s text,the system should create the confusion set withalternative phrases, measure the strength of asso-ciation in the writer?s phrase and in each gener-ated alternative phrase and flag as error only ifthe association score of the writer?s phrase islower than one or more of the alternatives gener-ated and suggest the higher-ranking alternativesas corrections.4 Approaches to Word Similarity andWord Association Strength4.1 Word SimilaritySimilarity measures are used to generate the col-location candidates that are later ranked usingassociation measures.
In our work, we used thefollowing three measures to analyze word simila-53Confusion SetWord ??
???
???
??
??
??
???
??
?
?Meaning do accept begin make write  say eat do carryWord ??
???
????
??
?
??
??
??
??
?Meaning building beer draft beer money bill amountofmoneyscenery fee buildingTable 1 Confusion Set example for the words suru (??)
and biru (??)??write??read??
?put on??
?diary15 11 8Table 2 Context of a particular noun representedas a co-occurrence vector???rice????
?ramen noodle soup????curry??
?eat164 53 39Table 3 Context of a particular noun representedas a co-occurrence vectorrity: 1) thesaurus-based word similarity, 2) dis-tributional similarity and 3) confusion set derivedfrom learner corpus.
The first two measures gen-erate the collocation candidates by finding wordsthat are analogous to the writer?s choice, a com-mon approach used in the related work oncollocation error correction (Liu et al 2009;?stling and O. Knutsson, 2009; Wu et al 2010)and the third measure generates the candidatesbased on the corrections given by native speakersin the learner corpus.Thesaurus-based word similarity: The intui-tion of this measure is to check if the givenwords have similar glosses (definitions).
Twowords are considered similar if they are neareach other in the thesaurus hierarchy (have a pathwithin a pre-defined threshold length).Distributional Similarity: Thesaurus-basedmethods produce weak recall since many words,phrases and semantic connections are not cov-ered by hand-built thesauri, especially for verbsand adjectives.
As an alternative, distributionalsimilarity models are often used since it giveshigher recall.
On the other hand, distributionalsimilarity models tend to have lower precision(Jurafsky et al 2009), because the candidate setis larger.
The intuition of this measure is that twowords are similar if they have similar word con-texts.
In our task, context will be defined bysome grammatical dependency relation, specifi-cally, ?object-verb?
relation.
Context is repre-sented as co-occurrence vectors that are based onsyntactic dependencies.
We are interested incomputing similarity of nouns and verbs andhence the context of a particular noun is a vectorof verbs that are in an object relation with thatnoun.
The context of a particular verb is a vectorof nouns that are in an object relation with thatverb.
Table 2 and Table 3 show examples of partof co-occurrence vectors for the noun ???
[dia-ry]?
and the verb ????
[eat]?, respectively.The numbers indicate the co-occurrence frequen-cy in the BCCWJ corpus (Maekawa, 2008).
Wecomputed the similarity between co-occurrencevectors using different metrics: Cosine Similarity,Dice coefficient (Curran, 2004), Kullback-Leibler divergence or KL divergence or relativeentropy (Kullback and Leibler, 1951) and theJenson-Shannon divergence (Lee, 1999).Confusion Set derived from learner corpus:In order to build a module that can ?guess?common construction errors, we created a confu-sion set using Lang-8 corpus.
Instead of generat-ing words that have similar meaning to the learn-er?s written construction, we extracted all thepossible noun and verb corrections for each ofthe nouns and verbs found in the data.
Table 1shows some examples extracted.
For instance,the confusion set of the verb suru ???
[to do]?is composed of verbs such as ukeru ????
[toaccept]?, which does not necessarily have similarmeaning with suru.
The confusion set means thatin the corpus, suru was corrected to either one ofthese verbs, i.e., when the learner writes the verbsuru, he/she might actually mean to write one ofthe verbs in the confusion set.
For the nounbiru???
[building]?, the learner may have, forexample, misspelled the word b?ru ????
[beer]?, or may have got confused with the trans-lation of the English words bill (???[money]?,??
[bill]?, ???
[amount of money]?, ???[fee]?)
or view (???
[scenery]?)
to Japanese.544.2 Word Association StrengthAfter generating the collocation candidates usingword similarity, the next step is to identify the?true collocations?
among them.
Here, theassociation strength was measured, in such a waythat word pairs generated by chance from thesampling process can be excluded.
Anassociation measure assigns an association scoreto each word pair.
High scores indicate strongassociation, and can be used to select the ?truecollocations?.
We adopted the Weighted Dicecoefficient (Kitamura and Matsumoto, 1997) asour association measurement.
We also tested us-ing other association measures (results are omit-ted): Pointwise Mutual Information (Church andHanks, 1990), log-likelihood ratio (Dunning,1993) and Dice coefficient (Smadja et al 1996),but Weighted Dice performed best.5 Experiment setupWe divided our experiments into two parts: verbsuggestion and noun suggestion.
For verb sug-gestion, given the learners?
?noun wo verb?
con-struction, our focus is to suggest ?noun wo verb?collocations with alternative verbs other than thelearner?s written verb.
For noun suggestion, giv-en the learners?
?noun wo verb?
construction, ourfocus is to suggest ?noun wo verb?
collocationswith alternative nouns other than the learner?swritten noun.5.1 Data SetFor computing word similarity and associationscores for verb suggestion, the following re-sources were used:1) Bunrui Goi Hyo Thesaurus (The NationalInstitute for Japanese Language, 1964): a Japa-nese thesaurus, which has a vocabulary size ofaround 100,000 words, organized into 32,600unique semantic classes.
This thesaurus was usedto compute word similarity, taking the words thatare in the same subtree as the candidate word.
Bysubtree, we mean the tree with distance 2 fromthe leaf node (learner?s written word) doing thepre-order tree traversal.2) Mainichi Shimbun Corpus (MainichiNewspaper Co., 1991): one of the ma-jor newspapers in Japan that provides raw text ofnewspaper articles used as linguistic resource.One year data (1991) were used to extract the?noun wo verb?
tuples to compute word similari-ty (using cosine similarity metric) and colloca-tion scores.
We extracted 224,185 tuples com-posed of 16,781 unique verbs and 37,300 uniquenouns.3) Balanced Corpus of Contemporary WrittenJapanese, BCCWJ Corpus (Maekawa, 2008):composed of one hundred million words, por-tions of this corpus used in our experiments in-clude magazine, newspaper, textbooks, and blogdata4.
Incorporating a variety of topics and stylesin the training data helps minimize the domaingap problem between the learner?s vocabularyand newspaper vocabulary found in the MainichiShimbun data.
We extracted 194,036 ?noun woverb?
tuples composed of 43,243 unique nounsand 18,212 unique verbs.
These data are neces-sary to compute the word similarity (using cosinesimilarity metric) and collocation scores.4) Lang-8 Corpus: Consisted of two year data(2010 and 2011):A) Year 2010 data, which contain1,288,934 pairs of learner?s sentence and itscorrection, was used to: i) Compute word sim-ilarity (using cosine similarity metric) and col-location scores: We took out the learners?
sen-tences and used only the corrected sentences.We extracted 163,880 ?noun wo verb?
tuplescomposed of 38,999 unique nouns and 16,086unique verbs.
ii) Construct the confusion set(explained in Section 4.1): We constructed theconfusion set for all the 16,086 verbs and38,999 nouns that appeared in the data.B) Year 2011 data were used to con-struct the test set (described in Section 5.2).5.2 Test set selectionWe used Lang-8 (2011 data) for selecting ourtest set.
For the verb suggestion task, we extract-ed all the ?noun wo verb?
tuples with incorrectverbs and their correction.
From the tuples ex-tracted, we selected the ones where the verbswere corrected to the same verb 5 or more timesby the native speakers.
Similarly, for the nounsuggestion task, we extracted all the ?noun woverb?
tuples with incorrect nouns and their cor-rection.
There are cases where the learner?s con-struction sounds more acceptable than its correc-tion, cases where in the corpus, they were cor-rected due to some contextual information.
Forour application, since we are only considering4 Although the language used in blog data is usuallymore informal than the one used in newspaper,maganizes, etc., and might contain errors like spellingand grammar, collocation errors are much less fre-quent compared to spelling and grammar errors, sincecombining words appropriately is one the vital com-petencies of a native speaker of a language.55the noun, particle and verb that the learner wrote,there was a need to filter out such contextuallyinduced corrections.
To solve this problem, weused the Weighted Dice coefficient to computethe association strength between the noun and allthe verbs, filtering out the pairs where the learn-er?s construction has a higher score than the cor-rection.
After applying those conditions, we ob-tained 185 tuples for the verb suggestion test setand 85 tuples for the noun suggestion test set.5.3 Evaluation MetricsWe compared the verbs in the confusion setranked by collocation score suggested by the sys-tem with the human correction verb and noun inthe Lang-8 data.
A match would be counted as atrue positive (tp).
A false negative (fn) occurswhen the system cannot offer any suggestion.The metrics we used for the evaluation are:precision, recall and the mean reciprocal rank(MRR).
We report precision at rank k, k=1, 5,computing the rank of the correction when a truepositive occurs.
The MRR was used to assesswhether the suggestion list contains the correc-tion and how far up it is in the list.
It is calculat-ed as follows:??
?Ni irankNMRR 1 )(11            (1)where N is the size of the test set.
If the systemdid not return the correction for a test instance,we set)(1irankto zero.
Recall rate is calculatedwith the formula below:fntptp?
(2)6 ResultsTable 4 shows the ten models derived from com-bining different word similarity measures and theWeighted Dice measure as association measure,using different corpora.
In this table, for instance,we named M1 the model that uses thesaurus forcomputing word similarity and uses MainichiShimbun corpus when computing collocationscores using the association measure adopted,Weighted Dice.
M2 uses Mainichi Shimbun cor-pus for computing both word similarity and col-location scores.
M10 computes word similarityusing the confusing set from Lang-8 corpus anduses BCCWJ and Lang-8 corpus when compu-ting collocation scores.Considering that the size of the candidate setgenerated by different word similarity measuresvary considerably, we limit the size of the confu-sion set to 270 for verbs and 160 for nouns,which correspond to the maximum values of theconfusion set size for nouns and verbs when us-ing Lang-8 for generating the candidate set.
Set-ting up a threshold was necessary since the sizeof the candidate set generated when using Distri-butional Similarity methods may be quite large,affecting the system performance.
When compu-ting Distributional Similarity, scores are also as-signed to each candidate, thus, when we set up athreshold value n, we consider the list of n can-didates with highest scores.
Table 4 reports theprecision of the k-best suggestions, the recall rateand the MRR for verb and noun suggestion.6.1 Verb SuggestionTable 4 shows that the model using thesaurus(M1) achieved the highest precision rate amongthe other models; however, it had the lowest re-call.
The model could suggest for cases wherethe wrong verb written by the learner and thecorrection suggested in Lang-8 data have similarmeaning, as they are near to each other in thethesaurus hierarchy.
However, for cases wherethe wrong verb written by the learner and thecorrection suggested in Lang-8 data do not havesimilar meaning, M1 could not suggest the cor-rection.In order to improve the recall rate, we generat-ed models M2-M6, which use distributional simi-larity (cosine similarity) and also use corporaother than Mainichi Shimbun corpus to minimizethe domain gap problem between the learner?svocabulary and the newspaper vocabulary foundin the Mainichi Shimbun data.
The recall rateimproved significantly but the precision rate de-creased.
In order to compare it with other distri-butional similarity metrics (Dice, KL-Divergenceand Jenson-Shannon Divergence) and with themethod that uses Lang-8 for generating the con-fusion set, we chose the model with the highestrecall value as baseline, which is the one thatuses BCCWJ and Lang-8 (M6) and generatedother models (M7-M10).
The best MRR valueobtained among all the Distributional Similaritymethods was obtained by Jenson-Shannon diver-gence.
The highest recall and MRR values areachieved when Lang-8 data were used to gener-ate the confusion set (M10).56SimilarityusedforConfusionSetsThesaurusCosine SimilarityDiceCoefficientKLDivergenceJenson-ShannonDivergenceConfusionSetfromLang-8K-BestMainichiShimbunMainichiShimbunBCCWJLang-8MainichiShimbun+BCCWJBCCWJ+Lang-8BCCWJ+Lang-8BCCWJ+Lang-8BCCWJ+Lang-8BCCWJ+Lang-8M1 M2 M3 M4 M5 M6 M7 M8 M9 M10VerbSuggestion1 0.94 0.48 0.42 0.60 0.62 0.56 0.59 0.63 0.60 0.645 1 0.91 0.94 0.90 0.90 0.86 0.86 0.84 0.88 0.95Recall 0.20 0.40 0.30 0.68 0.49 0.71 0.81 0.35 0.74 0.97MRR 0.19 0.26 0.19 0.49 0.36 0.50 0.58 0.26 0.53 0.75NounSuggestion1 0.16 0.20 0.42 0.58 0.50 0.55 0.30 0.63 0.57 0.735 1 0.66 0.94 0.89 1 0.91 0.83 1 0.84 0.98Recall 0.07 0.17 0.22 0.45 0.04 0.42 0.35 0.12 0.38 0.98MRR 0.03 0.06 0.13 0.33 0.02 0.29 0.18 0.10 0.26 0.83Table 4 The precision and recall rate and MRR of the Models of Word Similarity and AssociationStrength method combination.6.2 Noun SuggestionSimilar to the verb suggestion experiments, thebest recall and MRR values are achieved whenLang-8 data were used to generate the confusionset (M10).For noun suggestion, our automatically con-structed test set includes a number of spellingcorrection cases, such as cases for the combina-tion eat ice cream, where the learner wroteaisukurimu wo taberu ????????????
and the correction is aisukur?mu wo taberu ?????????????.
Such phenomenadid not occur with the test set for verb suggestion.For those cases, the fact that only spelling cor-rection is necessary in order to have the rightcollocation may also indicate that the learner ismore confident regarding the choice of the nounthan the verb.
This also justifies the even lowerrecall rate obtained (0.07) when using a thesau-rus for generating the candidates7 Conclusion and Future WorkWe analyzed various Japanese corpora using anumber of collocation and word similaritymeasures to deduce and suggest the best colloca-tions for Japanese second language learners.
Inorder to build a system that is more sensitive toconstructions that are difficult for learners, weuse word similarity measures that generate collo-cation candidates using a large Japanese lan-guage learner corpus, instead of only using well-formed text.
By employing this approach, wecould obtain better recall and MRR values com-pared to thesaurus based method and distribu-tional similarity methods.Although only noun-wo-verb construction isexamined, the model is designed to be applicableto other types of constructions, such as adjective-noun and adverb-noun.
Another straightforwardextension is to pursue constructions with otherparticles, such as ?noun ga verb (subject-verb)?,?noun ni verb (dative-verb)?, etc.
In our experi-ments, only a small context information is con-sidered (only the noun, the particle wo (?)
andthe verb written by the learner).
In order to verifyour approach and to improve our current results,considering a wider context size and other typesof constructions will be the next steps of this re-search.Acknowledgments57Special thanks to Yangyang Xi for maintaining Lang-8.ReferencesY.
C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou.2008.
An automatic collocation writing assistant forTaiwanese EFL learners: A case of corpus-basedNLP technology.
Computer Assisted LanguageLearning,  21(3):283?299.K.
Church, and P. Hanks.
1990.
Word AssociationNorms, Mutual Information and Lexicogra-phy, Computational Linguistics, Vol.
16:1, pp.
22-29.J.
R. Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.D.
Dahlmeier, H. T. Ng.
2011.
Correcting SemanticCollocation Errors with L1-induced Paraphrases.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages107?117, Edinburgh, Scotland, UK, July.
Associa-tion for Computational LinguisticsT.
Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics 19.1 (Mar.
1993), 61-74.Y.
Futagi, P. Deane, M. Chodorow, and J. Tetreault.2008.
A computational approach to detecting collo-cation errors in the writing of non-native speakersof English.
Computer Assisted Language Learning,21, 4 (October 2008), 353-367.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing: A meta-classifier ap-proach.
In Proceedings of Human Language Tech-nologies: The 2010 Annual Conference of theNorth American Chapter of the ACL, pages 163?171, Los Angeles, California, June.
Association forComputational Linguistics.D.
Jurafsky and J. H. Martin.
2009.
Speech and Lan-guage Processing: An Introduction to Natural Lan-guage Processing, Speech Recognition, and Com-putational Linguistics.
2nd edition.
Prentice-Hall.K.
Maekawa.
2008.
Balanced corpus of contemporarywritten japanese.
In Proceedings of the 6thWorkshop on Asian Language Resources (ALR),pages 101?102.M.
Kitamura, Y. Matsumoto.
1997.
Automatic extrac-tion of translation patterns in parallel corpora.
InIPSJ, Vol.
38(4), pp.108-117, April.
In Japanese.S.
Kullback, R.A. Leibler.
1951.
On Information andSufficiency.
Annals of Mathematical Statis-tics 22 (1): 79?86.L.
Lee.
1999.
Measures of Distributional Similarity.In Proc of the 37th annual meeting of the ACL,Stroudsburg, PA, USA, 25.A.
L. Liu, D. Wible, and N. L. Tsao.
2009.
Automatedsuggestions for miscollocations.
In Proceedings ofthe NAACL HLT Workshop on Innovative Use ofNLP for Building Educational Applications, pages47?50, Boulder, Colorado, June.
Association forComputational Linguistics.Mainichi Newspaper Co. 1991.
Mainichi ShimbunCD-ROM 1991.T.
Mizumoto, K. Mamoru, M. Nagata, Y. Matsumoto.2011.
Mining Revision Log of Language LearningSNS for Automated Japanese Error Correction ofSecond Language Learners.
In Proceedings of the5th International Joint Conference on NaturalLanguage Processing, pp.147-155.
Chiang Mai,Thailand, November.
AFNLP.R.
?stling and O. Knutsson.
2009.
A corpus-basedtool for helping writers with Swedish collocations.In Proceedings of the Workshop on Extracting andUsing Constructions in NLP, Nodalida, Odense,Denmark.
70, 77.H.
Oyama and Y. Matsumoto.
2010.
Automatic ErrorDetection Method for Japanese Case Particles inJapanese Language Learners.
In Corpus, ICT, andLanguage Education, pages 235?245.A.
Rozovskaya and D. Roth.
2010.
Generating confu-sion sets for context-sensitive error correction.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages961?970, MIT, Massachusetts, USA, October.
As-sociation for Computational Linguistics.F.
Smadja, K. R. Mckeown, V. Hatzivassiloglou.
1996.Translation collocations for bilingual lexicons:  astatistical approach.
Computational Linguistics,22:1-38.H.
Suzuki and K. Toutanova.
2006.
Learning to Pre-dict Case Markers in Japanese.
In Proceedings ofthe 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theACL, pages 1049?1056 , Sydney, July.
Associationfor Computational Linguistics.J.
Tetreault, J. Foster,and M. Chodorow.
2010.
Using parse features forpreposition selection and error detection.
In Pro-ceedings of ACL 2010 Conference Short Papers,pages 353-358, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.The National Institute for Japanese Language, editor.1964.
Bunrui-Goi-Hyo.
Shuei shuppan.
In Japanese.J.
C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang.2010.
Automatic collocation suggestion in academ-ic writing.
In Proceedings of the ACL 2010 Con-ference Short Papers, pages 115-119, Uppsala,Sweden, July.
Association for Computational Lin-guistics.58
