Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 89?95,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsVocabulary Decomposition for Estonian Open Vocabulary SpeechRecognitionAntti Puurula and Mikko KurimoAdaptive Informatics Research CentreHelsinki University of TechnologyP.O.Box 5400, FIN-02015 HUT, Finland{puurula, mikkok}@cis.hut.fiAbstractSpeech recognition in many morphologi-cally rich languages suffers from a very highout-of-vocabulary (OOV) ratio.
Earlier workhas shown that vocabulary decompositionmethods can practically solve this problemfor a subset of these languages.
This pa-per compares various vocabulary decompo-sition approaches to open vocabulary speechrecognition, using Estonian speech recogni-tion as a benchmark.
Comparisons are per-formed utilizing large models of 60000 lex-ical items and smaller vocabularies of 5000items.
A large vocabulary model based ona manually constructed morphological tag-ger is shown to give the lowest word er-ror rate, while the unsupervised morphol-ogy discovery method Morfessor Baselinegives marginally weaker results.
Only theMorfessor-based approach is shown to ade-quately scale to smaller vocabulary sizes.1 Introduction1.1 OOV problemOpen vocabulary speech recognition refers to au-tomatic speech recognition (ASR) of continuousspeech, or ?speech-to-text?
of spoken language,where the recognizer is expected to recognize anyword spoken in that language.
This capability is a re-cent development in ASR, and is required or benefi-cial in many of the current applications of ASR tech-nology.
Moreover, large vocabulary speech recogni-tion is not possible in most languages of the worldwithout first developing the tools needed for openvocabulary speech recognition.
This is due to a fun-damental obstacle in current ASR called the out-of-vocabulary (OOV) problem.The OOV problem refers to the existence of wordsencountered that a speech recognizer is unable torecognize, as they are not covered in the vocabu-lary.
The OOV problem is caused by three inter-twined issues.
Firstly, the language model trainingdata and the test data always come from differentsamplings of the language, and the mismatch be-tween test and training data introduces some OOVwords, the amount depending on the difference be-tween the data sets.
Secondly, ASR systems alwaysuse finite and preferably small sized vocabularies,since the speed of decoding rapidly slows down asthe vocabulary size is increased.
Vocabulary sizesdepend on the application domain, sizes larger than60000 being very rare.
As some of the words en-countered in the training data are left out of the vo-cabulary, there will be OOV words during recogni-tion.
The third and final issue is the fundamentalone; languages form novel sentences not only bycombining words, but also by combining sub-worditems called morphs to make up the words them-selves.
These morphs in turn correspond to abstractgrammatical items called morphemes, and morphsof the same morpheme are called allomorphs of thatmorpheme.
The study of these facets of languageis aptly called morphology, and has been largely ne-glected in modern ASR technology.
This is due to89ASR having been developed primarily for English,where the OOV problem is not as severe as in otherlanguages of the world.1.2 Relevance of morphology for ASRMorphologies in natural languages are character-ized typologically using two parameters, called in-dexes of synthesis and fusion.
Index of synthesishas been loosely defined as the ratio of morphs perword forms in the language(Comrie, 1989), whileindex of fusion refers to the ratio of morphs per mor-pheme.
High frequency of verb paradigms such as?hear, hear + d, hear + d?
would result in a high syn-thesis, low fusion language, whereas high frequencyof paradigms such as ?sing, sang, sung?
would re-sult in almost the opposite.
Counting distinct itemtypes and not instances of the types, the first ex-ample would have 2 word forms, 2 morphs and 2morphemes, the second 3 word forms, 3 morphs and1 morpheme.
Although in the first example, thereare 3 word instances of the 2 word forms, the lat-ter word form being an ambiguous one referring totwo distinct grammatical constructions.
It shouldalso be noted that the first morph of the first ex-ample has 2 pronunciations.
Pronunciational bound-aries do not always follow morphological ones, anda morph may and will have several pronunciationsthat depend on context, if the language in questionhas significant orthographic irregularity.As can be seen, both types of morphological com-plexity increase the amount of distinct word forms,resulting in an increase in the OOV rate of any fi-nite sized vocabulary for that language.
In prac-tice, the OOV increase caused by synthesis is muchlarger, as languages can have thousands of differ-ent word forms per word that are caused by addi-tion of processes of word formation followed by in-flections.
Thus the OOV problem in ASR has beenmost pronounced in languages with much synthesis,regardless of the amount of fusion.
The morpheme-based modeling approaches evaluated in this workare primarily intended for fixing the problem causedby synthesis, and should work less well or even ad-versely when attempted with low synthesis, high fu-sion languages.
It should be noted that models basedon finite state transducers have been shown to be ad-equate for describing fusion as well(Koskenniemi,1983), and further work should evaluate these typesof models in ASR of languages with higher indexesof fusion.1.3 Approaches for solving the OOV problemThe traditional method for reducing OOV would beto simply increase the vocabulary size so that the rateof OOV words becomes sufficiently low.
Naturallythis method fails when the words are derived, com-pounded or inflected forms of rarer words.
Whilethis approach might still be practical in languageswith a low index of synthesis such as English, itfails with most languages in the world.
For exam-ple, in English with language models (LM) of 60kwords trained from the Gigaword Corpus V.2(Graffet al, 2005), and testing on a very similar Voiceof America -portion of TDT4 speech corpora(Kongand Graff, 2005), this gives a OOV rate of 1.5%.It should be noted that every OOV causes roughlytwo errors in recognition, and vocabulary decompo-sition approaches such as the ones evaluated heregive some benefits to word error rate (WER) evenin recognizing languages such as English(Bisani andNey, 2005).Four different approaches to lexical unit selec-tion are evaluated in this work, all of which havebeen presented previously.
These are hence called?word?, ?hybrid?, ?morph?
and ?grammar?.
Theword approach is the default approach to lexicalitem selection, and is provided here as a baseline forthe alternative approaches.
The alternatives testedhere are all based on decomposing the in-vocabularywords, OOV words, or both, in LM training data intosequences of sub-word fragments.
During recogni-tion the decoder can then construct the OOV wordsencountered as combinations of these fragments.Word boundaries are marked in LMs with tokens sothat the words can be reconstructed from the sub-word fragments after decoding simply by removingspaces between fragments, and changing the wordboundaries tokens to spaces.
As splitting to sub-word items makes the span of LM histories shorter,higher order n-grams must be used to correct this.Varigrams(Siivola and Pellom, 2005) are used inthis work, and to make LMs trained with each ap-proach comparable, the varigrams have been grownto roughly sizes of 5 million counts.
It should benoted that the names for the approaches here aresomewhat arbitrary, as from a theoretical perspec-90tive both morph- and grammar-based approaches tryto model the grammatical morph set of a language,difference being that ?morph?
does this with an un-supervised data-driven machine learning algorithm,whereas ?grammar?
does this using segmentationsfrom a manually constructed rule-based morpholog-ical tagger.2 Modeling approaches2.1 Word approachThe first approach evaluated in this work is the tra-ditional word based LM, where items are simply themost frequent words in the language model trainingdata.
OOV words are simply treated as unknownwords in language model training.
This has beenthe default approach to selection of lexical items inspeech recognition for several decades, and as it hasbeen sufficient in English ASR, there has been lim-ited interest in any alternatives.2.2 Hybrid approachThe second approach is a recent refinement of thetraditional word-based approach.
This is similar towhat was introduced as ?flat hybrid model?
(Bisaniand Ney, 2005), and it tries to model OOV-wordsas sequences of words and fragments.
?Hybrid?refers to the LM histories being composed of hy-brids of words and fragments, while ?flat?
refers tothe model being composed of one n-gram model in-stead of several models for the different item types.The models tested in this work differ in that sinceEstonian has a very regular phonemic orthography,grapheme sequences can be directly used insteadof more complex pronunciation modeling.
Subse-quently the fragments used are just one grapheme inlength.2.3 Morph approachThe morph-based approach has shown superior re-sults to word-based models in languages of highsynthesis and low fusion, including Estonian.
Thisapproach, called ?Morfessor Baseline?
is describedin detail in (Creutz et al, 2007).
An unsupervisedmachine learning algorithm is used to discover themorph set of the language in question, using mini-mum description length (MDL) as an optimizationcriterion.
The algorithm is given a word list of thelanguage, usually pruned to about 100 000 words,that it proceeds to recursively split to smaller items,using gains in MDL to optimize the item set.
Theresulting set of morphs models the morph set well inlanguages of high synthesis, but as it does not takefusion into account any manner, it should not workin languages of high fusion.
It neither preserves in-formation about pronunciations, and as these do notfollow morph boundaries, the approach is unsuitablein its basic form to languages of high orthographicirregularity.2.4 Grammar approachThe final approach applies a manually constructedrule-based morphological tagger(Alum?e, 2006).This approach is expected to give the best results,as the tagger should give the ideal segmentationalong the grammatical morphs that the unsupervisedand language-independent morph approach tries tofind.
To make this approach more comparable tothe morph models, OOV morphs are modeled assequences of graphemes similar to the hybrid ap-proach.
Small changes to the original approachwere also made to make the model comparable tothe other models presented here, such as using thetagger segmentations as such and not using pseudo-morphemes, as well as not tagging the items in anymanner.
This approach suffers from the same handi-caps as the morph approach, as well as from someadditional ones: morphological analyzers are notreadily available for most languages, they must betailored by linguists for new datasets, and it is anopen problem as to how pronunciation dictionariesshould be written for grammatical morphs in lan-guages with significant orthographic irregularity.2.5 Text segmentation and language modelingFor training the LMs, a subset of 43 mil-lion words from the Estonian Segakorpus wasused(Segakorpus, 2005), preprocessed with a mor-phological analyzer(Alum?e, 2006).
After selectingthe item types, segmenting the training corpora andgeneration of a pronunciation dictionary, LMs weretrained for each lexical item type.
Table 1 showsthe text format for LM training data after segmen-tation with each model.
As can be seen, the word-based approach doesn?t use word boundary tokens.To keep the LMs comparable between each model-91model text segmentationword 5k voodis reeglina loemeword 60k voodis reeglina loemehybrid 5k v o o d i s <w> reeglina <w> l o e m ehybrid 60k voodis <w> reeglina <w> loememorph 5k voodi s <w> re e g lina <w> loe memorph 60k voodi s <w> reegli na <w> loe megrammar 5k voodi s <w> reegli na <w> loe megrammar 60k voodi s <w> reegli na <w> loe meTable 1.
Sample segmented texts for each model.ing approach, growing varigram models(Siivola andPellom, 2005) were used with no limits as to the or-der of n-grams, but limiting the number of counts to4.8 and 5 million counts.
In some models this grow-ing method resulted in the inclusion of very frequentlong item sequences to the varigram, up to a 28-gram.
Models of both 5000 and 60000 lexical itemswere trained in order to test if and how the model-ing approaches would scale to smaller and thereforemuch faster vocabularies.
Distribution of counts inn-gram orders can be seen in figure 1.Figure 1.
Number of counts included for each n-gram order in the 60k varigram models.The performance of the statistical language mod-els is often evaluated by perplexity or cross-entropy.However, we decided to only report the real ASRperformance, because perplexity does not suit wellto the comparison of models that use different lex-ica, have different OOV rates and have lexical unitsof different lengths.3 Experimental setup3.1 Evaluation setAcoustic models for Estonian ASR were trained onthe Estonian Speechdat-like corpus(Meister et al,2002).
This consists of spoken newspaper sentencesand shorter utterances, read over a telephone by1332 different speakers.
The data therefore wasquite clearly articulated, but suffered from 8kHzsample rate, different microphones, channel noisesand occasional background noises.
On top of thisthe speakers were selected to give a very broad cov-erage of different dialectal varieties of Estonian andwere of different age groups.
For these reasons, inspite of consisting of relatively common word formsfrom newspaper sentences, the database can be con-sidered challenging for ASR.Held-out sentences were from the same corpusused as development and evaluation set.
8 differentsentences from 50 speakers each were used for eval-uation, while sentences from 15 speakers were usedfor development.
LM scaling factor was optimizedfor each model separately on the development set.On total over 200 hours of data from the databasewas used for acoustic model training, of which lessthan half was speech.3.2 DecodingThe acoustic models were Hidden Markov Models(HMM) with Gaussian Mixture Models (GMM)for state modeling based on 39-dimensionalMFCC+P+D+DD features, with windowed cepstralmean subtraction (CMS) of 1.25 second window.Maximum likelihood linear transformation (MLLT)was used during training.
State-tied cross-wordtriphones and 3 left-to-right states were used, statedurations were modeled using gamma distributions.On total 3103 tied states and 16 Gaussians per statewere used.Decoding was done with the decoder developedat TKK(Pylkk?nen, 2005), which is based on a one-pass Viterbi beam search with token passing on alexical prefix tree.
The lexical prefix tree included across-word network for modeling triphone contexts,and the nodes in the prefix tree were tied at the tri-phone state level.
Bigram look-ahead models were92used in speeding up decoding, in addition to prun-ing with global beam, history, histogram and wordend pruning.
Due to the properties of the decoderand varigram models, very high order n-grams couldbe used without significant degradation in decodingspeed.As the decoder was run with only one pass, adap-tation was not used in this work.
In preliminaryexperiments simple adaptation with just constrainedmaximum likelihood linear regression (CMLLR)was shown to give as much as 20 % relative worderror rate reductions (RWERR) with this dataset.Adaptation was not used, since it interacts with themodel types, as well as with the WER from the firstround of decoding, providing larger RWERR for thebetter models.
With high WER models, adaptationmatrices are less accurate, and it is also probable thatthe decomposition methods yield more accurate ma-trices, as they produce results where fewer HMM-states are misrecognized.
These issues should be in-vestigated in future research.After decoding, the results were post-processedby removing words that seemed to be sequences ofjunk fragments: consonant-only sequences and 1-phoneme words.
This treatment should give verysignificant improvements with noisy data, but in pre-liminary experiments it was noted that the use ofsentence boundaries resulted in almost 10% RW-ERR weaker results for the approaches using frag-ments, as that almost negates the gains achievedfrom this post-processing.
Since sentence bound-ary forcing is done prior to junk removal, it seemsto work erroneously when it is forced to operate onnoisy data.
Sentence boundaries were neverthelessused, as in the same experiments the word-basedmodels gained significantly from their use, mostlikely because they cannot use the fragment itemsfor detection of acoustic junk, as the models withfragments can.4 ResultsResults of the experiments were consistent with ear-lier findings(Hirsim?ki et al, 2006; Kurimo et al,2006).
Traditional word based LMs showed theworst performance, with all of the recently proposedalternatives giving better results.
Hybrid LMs con-sistently outperformed traditional word-based LMsin both large and small vocabulary conditions.
Thetwo morphology-driven approaches gave similar andclearly superior results.
Only the morph approachseems to scale down well to smaller vocabularysizes, as the WER for the grammar approach in-creased rapidly as size of the vocabulary was de-creased.size word hybrid morph grammar60000 53.1 47.1 39.4 38.75000 82.0 63.0 43.5 47.6Table 2.
Word error rates for the models (WER %).Table 2 shows the WER for the large (60000) andsmall (5000) vocabulary sizes and different mod-eling approaches.
Table 3 shows the correspond-ing letter error rates (LER).
LERs are more compa-rable across some languages than WERs, as WERdepends more on factors such as length, morpho-logical complexity, and OOV of the words.
How-ever, for within-language and between-model com-parisons, the RWERR should still be a valid met-ric, and is also usable in languages that do not use aphonemic writing system.
The RWERRs of differ-ent novel methods seems to be comparable betweendifferent languages as well.
Both WER and LER arehigh considering the task.
However, standard meth-ods such as adaptation were not used, as the inten-tion was only to study the RWERR of the differentapproaches.size word hybrid morph grammar60000 17.8 15.8 12.4 12.35000 35.5 20.8 14.4 15.4Table 3.
Letter error rates for the models (LER %).5 DiscussionFour different approaches to lexical item selectionfor large and open vocabulary ASR in Estonianwere evaluated.
It was shown that the three ap-proaches utilizing vocabulary decomposition givesubstantial improvements over the traditional wordbased approach, and make large vocabulary ASRtechnology possible for languages similar to Esto-nian, where the traditional approach fails due to very93high OOV rates.
These include memetic relativesFinnish and Turkish, among other languages thathave morphologies of high fusion, low synthesis andlow orthographic irregularity.5.1 Performance of the approachesThe morpheme-based approaches outperformed theword- and hybrid-based approaches clearly.
The re-sults for ?hybrid?
are in in the range suggested byearlier work(Bisani and Ney, 2005).
One possi-ble explanation for the discrepancy between the hy-brid and morpheme-based approaches would be thatthe morpheme-based approaches capture items thatmake sense in n-gram modeling, as morphs are itemsthat the system of language naturally operates on.These items would then be of more use when try-ing to predict unseen data(Creutz et al, 2007).
Asmodeling pronunciations is much more straightfor-ward in Estonian, the morpheme-based approachesdo not suffer from erroneous pronunciations, result-ing in clearly superior performance.As for the superiority of the ?grammar?
over theunsupervised ?morph?, the difference is marginal interms of RWERR.
The grammatical tagger was tai-lored by hand for that particular language, whereasMorfessor method is meant to be unsupervised andlanguage independent.
There are further argumentsthat would suggest that the unsupervised approachis one that should be followed; only ?morph?
scaledwell to smaller vocabulary sizes, the usual practiceof pruning the word list to produce smaller morphsets gives better results than here and most impor-tantly, it is questionable if ?grammar?
can be takento languages with high indexes of fusion and ortho-graphic irregularity, as the models have to take theseinto account as well.5.2 Comparison to previous resultsThere are few previous results published on Estonianopen vocabulary ASR.
In (Alum?e, 2006) a WER of44.5% was obtained with word-based trigrams anda WER of 37.2% with items similar to ones from?grammar?
using the same speech corpus as in thiswork.
Compared to the present work, the WERfor the morpheme-based models was measured withcompound words split in both hypothesis and ref-erence texts, making the task slightly easier thanhere.
In (Kurimo et al, 2006) a WER of 57.6% wasachieved with word-based varigrams and a WER of49.0% with morphs-based ones.
This used the sameevaluation set as this work, but had slightly differentLMs and different acoustic modelling which is themain reason for the higher WER levels.
In summary,morpheme-based approaches seem to consistentlyoutperform the traditional word based one in Esto-nian ASR, regardless of the specifics of the recogni-tion system, test set and models.In (Hirsim?ki et al, 2006) a corresponding com-parison of unsupervised and grammar-based morphswas presented in Finnish, and the grammar-basedmodel gave a significantly higher WER in one of thetasks.
This result is interesting, and may stem from anumber of factors, among them the different decoderand acoustic models, 4-grams versus varigrams, aswell as differences in post-processing.
Most likelythe difference is due to lack of coverage for domain-specific words in the Finnish tagger, as it has a 4.2%OOV rate on the training data.
On top of this theOOV words are modeled simply as grapheme se-quences, instead of modeling only OOV morphs inthat manner, as is done in this work.5.3 Open problems in vocabularydecompositionAs stated in the introduction, modeling languageswith high indexes of fusion such as Arabic will re-quire more complex vocabulary decomposition ap-proaches.
This is verified by recent empirical re-sults, where gains obtained from simple morpholog-ical decomposition seem to be marginal(Kirchhoffet al, 2006; Creutz et al, 2007).
These languageswould possibly need novel LM inference algorithmsand decoder architectures.
Current research seemsto be heading in this direction, with weighted finitestate transducers becoming standard representationsfor the vocabulary instead of the lexical prefix tree.Another issue in vocabulary decomposition is or-thographic irregularity, as the items resulting fromdecomposition do not necessarily have unambigu-ous pronunciations.
As most modern recognizersuse the Viterbi approximation with vocabularies ofone pronunciation per item, this is problematic.
Onesolution to this is expanding the different items withtags according to pronunciation, shifting the prob-lem to language modeling(Creutz et al, 2007).
Forexample, English plural ?s?
would expand to ?s#1?94with pronunciation ?/s/?, and ?s#2?
with pronunci-ation ?/z/?, and so on.
In this case the vocabularysize increases by the amount of different pronunci-ations added.
The new items will have pronuncia-tions that depend on their language model context,enabling the prediction of pronunciations with lan-guage model probabilities.
The only downside tothis is complicating the search for optimal vocabu-lary decomposition, as the items should make sensein both pronunciational and morphological terms.One can consider the originally presented hybridapproach as an approach to vocabulary decompo-sition that tries to keep the pronunciations of theitems as good as possible, whereas the morph ap-proach tries to find items that make sense in termsof morphology.
This is obviously due to the meth-ods having been developed on very different typesof languages.
The morph approach was developedfor the needs of Finnish speech recognition, whichis a high synthesis, moderate fusion and very low or-thographic irregularity language, whereas the hybridapproach in (Bisani and Ney, 2005) was developedfor English, which has low synthesis, moderate fu-sion, and very high orthographic irregularity.
A uni-versal approach to vocabulary decomposition wouldhave to take all of these factors into account.AcknowledgementsThe authors would like to thank Dr. Tanel Alum?efrom Tallinn University of Technology for help inperforming experiments with Estonian speech andtext databases.
This work was supported by theAcademy of Finland in the project: New adaptiveand learning methods in speech recognition.ReferencesBernard Comrie.
1972.
Language Universals and Lin-guistic Typology, Second Edition.
Athen?um PressLtd, Gateshead, UK.Kimmo Koskenniemi.
1983.
Two-level Morphol-ogy: a General Computational Model for Word-FormRecognition and Production.
University of Helsinki,Helsinki, Finland.Tanel Alum?e.
2006.
Methods for Estonian Large Vo-cabulary Speech Recognition.
PhD Thesis.
TallinnUniversity of Technology.
Tallinn, Estonia.Maximilian Bisani, Hermann Ney.
2005.
Open Vocab-ulary Speech Recognition with Flat Hybrid Models.INTERSPEECH-2005, 725?728.Janne Pylkk?nen.
2005.
An Efficient One-pass Decoderfor Finnish Large Vocabulary Continuous SpeechRecognition.
Proceedings of The 2nd Baltic Con-ference on Human Language Technologies, 167?172.HLT?2005.
Tallinn, Estonia.Vesa Siivola, Bryan L. Pellom.
2005.
Growing an n-Gram Language Model.
INTERSPEECH-2005, 1309?1312.David Graff, Junbo Kong, Ke Chen and KazuakiMaeda.
2005.
LDC Gigaword Corpora: En-glish Gigaword Second Edition.
In LDC link:http://www.ldc.upenn.edu/Catalog/index.jsp.Junbo Kong and David Graff.
2005.
TDT4 Multilin-gual Broadcast News Speech Corpus.
In LDC link:http://www.ldc.upenn.edu/Catalog/index.jsp.Segakorpus.
2005.
Segakorpus - Mixed Corpus of Esto-nian.
Tartu University.
http://test.cl.ut.ee/korpused/.Einar Meister, J?rgen Lasn and Lya Meister 2002.
Esto-nian SpeechDat: a project in progress.
In Proceedingsof the Fonetiikan P?iv?t - Phonetics Symposium 2002in Finland, 21?26.Katrin Kirchhoff, Dimitra Vergyri, Jeff Bilmes, KevinDuh and Andreas Stolcke 2006.
Morphology-based language modeling for conversational Arabicspeech recognition.
Computer Speech & Language20(4):589?608.Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo, AnttiPuurula, Janne Pylkk?nen, Vesa Siivola, Matti Var-jokallio, Ebru Arisoy, Murat Saraclar and AndreasStolcke 2007.
Analysis of Morph-Based SpeechRecognition and the Modeling of Out-of-VocabularyWords Across Languages To appear in Proceedingsof Human Language Technologies: The Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics.
NAACL-HLT2007, Rochester, NY, USAMikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola,Teemu Hirsim?ki, Janne Pylkk?nen, Tanel Alumaeand Murat Saraclar 2006.
Unlimited vocabularyspeech recognition for agglutinative languages.
In Hu-man Language Technology, Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.
HLT-NAACL 2006.
New York,USATeemu Hirsim?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja and Janne Pylkk?nen 2006.Unlimited vocabulary speech recognition with morphlanguage models applied to Finnish.
Computer Speech& Language 20(4):515?541.95
