Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120?128,Sydney, July 2006. c?2006 Association for Computational LinguisticsDomain Adaptation with Structural Correspondence LearningJohn Blitzer Ryan McDonald Fernando Pereira{blitzer|ryantm|pereira}@cis.upenn.eduDepartment of Computer and Information Science, University of Pennsylvania3330 Walnut Street, Philadelphia, PA 19104, USAAbstractDiscriminative learning methods arewidely used in natural language process-ing.
These methods work best when theirtraining and test data are drawn from thesame distribution.
For many NLP tasks,however, we are confronted with newdomains in which labeled data is scarceor non-existent.
In such cases, we seekto adapt existing models from a resource-rich source domain to a resource-poortarget domain.
We introduce structuralcorrespondence learning to automaticallyinduce correspondences among featuresfrom different domains.
We test our tech-nique on part of speech tagging and showperformance gains for varying amountsof source and target training data, as wellas improvements in target domain parsingaccuracy using our improved tagger.1 IntroductionDiscriminative learning methods are ubiquitous innatural language processing.
Discriminative tag-gers and chunkers have been the state-of-the-artfor more than a decade (Ratnaparkhi, 1996; Shaand Pereira, 2003).
Furthermore, end-to-end sys-tems like speech recognizers (Roark et al, 2004)and automatic translators (Och, 2003) use increas-ingly sophisticated discriminative models, whichgeneralize well to new data that is drawn from thesame distribution as the training data.However, in many situations we may have asource domain with plentiful labeled training data,but we need to process material from a target do-main with a different distribution from the sourcedomain and no labeled data.
In such cases, wemust take steps to adapt a model trained on thesource domain for use in the target domain (Roarkand Bacchiani, 2003; Florian et al, 2004; Chelbaand Acero, 2004; Ando, 2004; Lease and Char-niak, 2005; Daume?
III and Marcu, 2006).
Thiswork focuses on using unlabeled data from boththe source and target domains to learn a commonfeature representation that is meaningful acrossboth domains.
We hypothesize that a discrimi-native model trained in the source domain usingthis common feature representation will general-ize better to the target domain.This representation is learned using a methodwe call structural correspondence learning (SCL).The key idea of SCL is to identify correspon-dences among features from different domains bymodeling their correlations with pivot features.Pivot features are features which behave in thesame way for discriminative learning in both do-mains.
Non-pivot features from different domainswhich are correlated with many of the same pivotfeatures are assumed to correspond, and we treatthem similarly in a discriminative learner.Even on the unlabeled data, the co-occurrencestatistics of pivot and non-pivot features are likelyto be sparse, and we must model them in a com-pact way.
There are many choices for modelingco-occurrence data (Brown et al, 1992; Pereiraet al, 1993; Blei et al, 2003).
In this work wechoose to use the technique of structural learn-ing (Ando and Zhang, 2005a; Ando and Zhang,2005b).
Structural learning models the correla-tions which are most useful for semi-supervisedlearning.
We demonstrate how to adapt it for trans-fer learning, and consequently the structural partof structural correspondence learning is borrowedfrom it.1SCL is a general technique, which one can ap-ply to feature based classifiers for any task.
Here,1Structural learning is different from learning with struc-tured outputs, a common paradigm for discriminative nat-ural language processing models.
To avoid terminologi-cal confusion, we refer throughout the paper to a specificstructural learning method, alternating structural optimiza-tion (ASO) (Ando and Zhang, 2005a).120(a) Wall Street JournalDT JJ VBZ DT NN IN DT JJ NNThe clash is a sign of a new toughnessCC NN IN NNP POS JJ JJ NN .and divisiveness in Japan ?s once-cozy financial circles .
(b) MEDLINEDT JJ VBN NNS IN DT NN NNS VBPThe oncogenic mutated forms of the ras proteins areRB JJ CC VBP IN JJ NN NN .constitutively active and interfere with normal signal transduction .Figure 1: Part of speech-tagged sentences from both corporawe investigate its use in part of speech (PoS) tag-ging (Ratnaparkhi, 1996; Toutanova et al, 2003).While PoS tagging has been heavily studied, manydomains lack appropriate training corpora for PoStagging.
Nevertheless, PoS tagging is an impor-tant stage in pipelined language processing sys-tems, from information extractors to speech syn-thesizers.
We show how to use SCL to transfer aPoS tagger from the Wall Street Journal (financialnews) to MEDLINE (biomedical abstracts), whichuse very different vocabularies, and we demon-strate not only improved PoS accuracy but alsoimproved end-to-end parsing accuracy while usingthe improved tagger.An important but rarely-explored setting in do-main adaptation is when we have no labeledtraining data for the target domain.
We firstdemonstrate that in this situation SCL significantlyimproves performance over both supervised andsemi-supervised taggers.
In the case when somein-domain labeled training data is available, weshow how to use SCL together with the classifiercombination techniques of Florian et al (2004) toachieve even greater performance.In the next section, we describe a motivatingexample involving financial news and biomedicaldata.
Section 3 describes the structural correspon-dence learning algorithm.
Sections 6 and 7 reportresults on adapting from the Wall Street Journal toMEDLINE.
We discuss related work on domainadaptation in section 8 and conclude in section 9.2 A Motivating ExampleFigure 1 shows two PoS-tagged sentences, oneeach from the Wall Street Journal (hereafter WSJ)and MEDLINE.
We chose these sentences for tworeasons.
First, we wish to visually emphasize thedifference between the two domains.
The vocab-ularies differ significantly, and PoS taggers suf-fer accordingly.
Second, we want to focus on the(a) An ambiguous instanceJJ vs. NNwith normal signal transduction(b) MEDLINE occurrences ofsignal, together with pivotfeaturesthe signal required tostimulatory signal fromessential signal for(c) Corresponding WSJwords, together with pivotfeaturesof investment requiredof buyouts from buyersto jail for violatingFigure 2: Correcting an incorrect biomedical tag.Corresponding words are in bold, and pivot fea-tures are italicizedphrase ?with normal signal transduction?
from theMEDLINE sentence, depicted in Figure 2(a).
Theword ?signal?
in this sentence is a noun, but a tag-ger trained on the WSJ incorrectly classifies it asan adjective.
We introduce the notion of pivot fea-tures.
Pivot features are features which occur fre-quently in the two domains and behave similarlyin both.
Figure 2(b) shows some pivot featuresthat occur together with the word ?signal?
in ourbiomedical unlabeled data.
In this case our pivotfeatures are all of type <the token on theright>.
Note that ?signal?
is unambiguously anoun in these contexts.
Adjectives rarely precedepast tense verbs such as ?required?
or prepositionssuch as ?from?
and ?for?.We now search for occurrences of the pivot fea-tures in the WSJ.
Figure 2(c) shows some wordsthat occur together with the pivot features in theWSJ unlabeled data.
Note that ?investment?,?buy-outs?, and ?jail?
are all common nouns in thefinancial domain.
Furthermore, since we have la-beled WSJ data, we expect to be able to label atleast some of these nouns correctly.This example captures the intuition behindstructural correspondence learning.
We want touse pivot features from our unlabeled data to putdomain-specific words in correspondence.
That is,121Input: labeled source data {(xt, yt)Tt=1},unlabeled data from both domains {xj}Output: predictor f : X ?
Y1.
Choose m pivot features.
Create m binaryprediction problems, p`(x), ` = 1 .
.
.
m2.
For ` = 1 to mw?` = argminw?Pj L(w ?
xj , p`(xj))+?||w||2?end3.
W = [w?1| .
.
.
|w?m], [U D V T ] = SVD(W ),?
= UT[1:h,:]4.
Return f , a predictor trainedon(?
?xt?xi?, yt?Tt=1)Figure 3: SCL Algorithmwe want the pivot features to model the fact that inthe biomedical domain, the word signal behavessimilarly to the words investments, buyouts andjail in the financial news domain.
In practice, weuse this technique to find correspondences amongall features, not just word features.3 Structural Correspondence LearningStructural correspondence learning involves asource domain and a target domain.
Both domainshave ample unlabeled data, but only the source do-main has labeled training data.
We refer to the taskfor which we have labeled training data as the su-pervised task.
In our experiments, the supervisedtask is part of speech tagging.
We require that theinput x in both domains be a vector of binary fea-tures from a finite feature space.
The first step ofSCL is to define a set of pivot features on the unla-beled data from both domains.
We then use thesepivot features to learn a mapping ?
from the orig-inal feature spaces of both domains to a shared,low-dimensional real-valued feature space.
A highinner product in this new space indicates a high de-gree of correspondence.During supervised task training, we use boththe transformed and original features from thesource domain.
During supervised task testing, weuse the both the transformed and original featuresfrom the target domain.
If we learned a good map-ping ?, then the classifier we learn on the sourcedomain will also be effective on the target domain.The SCL algorithm is given in Figure 3, and theremainder of this section describes it in detail.3.1 Pivot FeaturesPivot features should occur frequently in the un-labeled data of both domains, since we must esti-mate their covariance with non-pivot features ac-curately, but they must also be diverse enoughto adequately characterize the nuances of the su-pervised task.
A good example of this tradeoffare determiners in PoS tagging.
Determiners aregood pivot features, since they occur frequentlyin any domain of written English, but choosingonly determiners will not help us to discriminatebetween nouns and adjectives.
Pivot features cor-respond to the auxiliary problems of Ando andZhang (2005a).In section 2, we showed example pivot fea-tures of type <the token on the right>.We also use pivot features of type <the tokenon the left> and <the token in themiddle>.
In practice there are many thousandsof pivot features, corresponding to instantiationsof these three types for frequent words in both do-mains.
We choose m pivot features, which we in-dex with `.3.2 Pivot PredictorsFrom each pivot feature we create a binary clas-sification problem of the form ?Does pivot fea-ture ` occur in this instance??.
One such ex-ample is ?Is <the token on the right>required??
These binary classification problemscan be trained from the unlabeled data, since theymerely represent properties of the input.
If we rep-resent our features as a binary vector x, we cansolve these problems using m linear predictors.f`(x) = sgn(w?` ?
x), ` = 1 .
.
.
mNote that these predictors operate on the originalfeature space.
This step is shown in line 2 of Fig-ure 3.
Here L(p, y) is a real-valued loss func-tion for binary classification.
We follow Ando andZhang (2005a) and use the modified Huber loss.Since each instance contains features which aretotally predictive of the pivot feature (the featureitself), we never use these features when makingthe binary prediction.
That is, we do not use anyfeature derived from the right word when solvinga right token pivot predictor.The pivot predictors are the key element in SCL.The weight vectors w?` encode the covariance ofthe non-pivot features with the pivot features.
Ifthe weight given to the z?th feature by the `?th122pivot predictor is positive, then feature z is posi-tively correlated with pivot feature `.
Since pivotfeatures occur frequently in both domains, we ex-pect non-pivot features from both domains to becorrelated with them.
If two non-pivot features arecorrelated in the same way with many of the samepivot features, then they have a high degree of cor-respondence.
Finally, observe that w?` is a linearprojection of the original feature space onto R.3.3 Singular Value DecompositionSince each pivot predictor is a projection onto R,we could create m new real-valued features, onefor each pivot.
For both computational and statis-tical reasons, though, we follow Ando and Zhang(2005a) and compute a low-dimensional linear ap-proximation to the pivot predictor space.
Let Wbe the matrix whose columns are the pivot pre-dictor weight vectors.
Now let W = UDV T bethe singular value decomposition of W , so that?
= UT[1:h,:] is the matrix whose rows are the topleft singular vectors of W .The rows of ?
are the principal pivot predictors,which capture the variance of the pivot predictorspace as best as possible in h dimensions.
Further-more, ?
is a projection from the original featurespace onto Rh.
That is, ?x is the desired mappingto the (low dimensional) shared feature represen-tation.
This is step 3 of Figure 3.3.4 Supervised Training and InferenceTo perform inference and learning for the super-vised task, we simply augment the original fea-ture vector with features obtained by applying themapping ?.
We then use a standard discrimina-tive learner on the augmented feature vector.
Fortraining instance t, the augmented feature vectorwill contain all the original features xt plus thenew shared features ?xt.
If we have designed thepivots well, then ?
should encode correspondencesamong features from different domains which areimportant for the supervised task, and the classi-fier we train using these new features on the sourcedomain will perform well on the target domain.4 Model ChoicesStructural correspondence learning uses the tech-niques of alternating structural optimization(ASO) to learn the correlations among pivot andnon-pivot features.
Ando and Zhang (2005a) de-scribe several free paramters and extensions toASO, and we briefly address our choices for thesehere.
We set h, the dimensionality of our low-rankrepresentation to be 25.
As in Ando and Zhang(2005a), we observed that setting h between 20and 100 did not change results significantly, and alower dimensionality translated to faster run-time.We also implemented both of the extensions de-scribed in Ando and Zhang (2005a).
The first isto only use positive entries in the pivot predictorweight vectors to compute the SVD.
This yieldsa sparse representation which saves both time andspace, and it also performs better.
The second is tocompute block SVDs of the matrix W , where oneblock corresponds to one feature type.
We usedthe same 58 feature types as Ratnaparkhi (1996).This gave us a total of 1450 projection features forboth semisupervised ASO and SCL.We found it necessary to make a change to theASO algorithm as described in Ando and Zhang(2005a).
We rescale the projection features to al-low them to receive more weight from a regular-ized discriminative learner.
Without any rescaling,we were not able to reproduce the original ASOresults.
The rescaling parameter is a single num-ber, and we choose it using heldout data from oursource domain.
In all our experiments, we rescaleour projection features to have average L1 norm onthe training set five times that of the binary-valuedfeatures.Finally, we also make one more change to makeoptimization faster.
We select only half of theASO features for use in the final model.
Thisis done by running a few iterations of stochas-tic gradient descent on the PoS tagging problem,then choosing the features with the largest weight-variance across the different labels.
This cut inhalf training time and marginally improved perfor-mance in all our experiments.5 Data Sets and Supervised Tagger5.1 Source Domain: WSJWe used sections 02-21 of the Penn Treebank(Marcus et al, 1993) for training.
This resulted in39,832 training sentences.
For the unlabeled data,we used 100,000 sentences from a 1988 subset ofthe WSJ.5.2 Target Domain: Biomedical TextFor unlabeled data we used 200,000 sentences thatwere chosen by searching MEDLINE for abstractspertaining to cancer, in particular genomic varia-123companytransactioninvestorsofficials yourprettyshort-termpoliticalreceptors mutationassayslesions functionaltransientneuronalmetastaticWSJ OnlyMEDLINE OnlyFigure 4: An example projection of word features onto R. Words on the left (negative valued) behavesimilarly to each other for classification, but differently from words on the right (positive valued).
Theprojection distinguishes nouns from adjectives and determiners in both domains.tions and mutations.
For labeled training and test-ing purposes we use 1061 sentences that have beenannotated by humans as part of the Penn BioIEproject (PennBioIE, 2005).
We use the same 561-sentence test set in all our experiments.
The part-of-speech tag set for this data is a superset ofthe Penn Treebank?s including the two new tagsHYPH (for hyphens) and AFX (for common post-modifiers of biomedical entities such as genes).These tags were introduced due to the importanceof hyphenated entities in biomedical text, and areused for 1.8% of the words in the test set.
Anytagger trained only on WSJ text will automaticallypredict wrong tags for those words.5.3 Supervised TaggerSince SCL is really a method for inducing a setof cross-domain features, we are free to chooseany feature-based classifier to use them.
Forour experiments we use a version of the discrim-inative online large-margin learning algorithmMIRA (Crammer et al, 2006).
MIRA learns andoutputs a linear classification score, s(x,y;w) =w ?
f(x,y), where the feature representation f cancontain arbitrary features of the input, includingthe correspondence features described earlier.
Inparticular, MIRA aims to learn weights so thatthe score of correct output, yt, for input xt isseparated from the highest scoring incorrect out-puts2, with a margin proportional to their Ham-ming losses.
MIRA has been used successfully forboth sequence analysis (McDonald et al, 2005a)and dependency parsing (McDonald et al, 2005b).As with any structured predictor, we need tofactor the output space to make inference tractable.We use a first-order Markov factorization, allow-ing for an efficient Viterbi inference procedure.2We fix the number of high scoring incorrect outputs to 5.6 Visualizing ?In section 2 we claimed that good representationsshould encode correspondences between wordslike ?signal?
from MEDLINE and ?investment?from the WSJ.
Recall that the rows of ?
are pro-jections from the original feature space onto thereal line.
Here we examine word features underthese projections.
Figure 4 shows a row fromthe matrix ?.
Applying this projection to a wordgives a real value on the horizontal dashed lineaxis.
The words below the horizontal axis occuronly in the WSJ.
The words above the axis occuronly in MEDLINE.
The verticle line in the mid-dle represents the value zero.
Ticks to the left orright indicate relative positive or negative valuesfor a word under this projection.
This projectiondiscriminates between nouns (negative) and adjec-tives (positive).
A tagger which gives high pos-itive weight to the features induced by applyingthis projection will be able to discriminate amongthe associated classes of biomedical words, evenwhen it has never observed the words explicitly inthe WSJ source training set.7 Empirical ResultsAll the results we present in this section use theMIRA tagger from Section 5.3.
The ASO andstructural correspondence results also use projec-tion features learned using ASO and SCL.
Sec-tion 7.1 presents results comparing structural cor-respondence learning with the supervised baselineand ASO in the case where we have no labeleddata in the target domain.
Section 7.2 gives resultsfor the case where we have some limited data inthe target domain.
In this case, we use classifiersas features as described in Florian et al (2004).Finally, we show in Section 7.3 that our SCL PoS124(a)100  500  1k 5k 40k75808590Results for 561 MEDLINE Test SentencesNumber of WSJ Training SentencesAccuracysupervisedsemi?ASOSCL(b) Accuracy on 561-sentence test setWordsModel All UnknownRatnaparkhi (1996) 87.2 65.2supervised 87.9 68.4semi-ASO 88.4 70.9SCL 88.9 72.0(c) Statistical Significance (McNemar?s)for all wordsNull Hypothesis p-valuesemi-ASO vs. super 0.0015SCL vs. super 2.1 ?
10?12SCL vs. semi-ASO 0.0003Figure 5: PoS tagging results with no target labeled training data(a)50 100 200 500868890929496Number of MEDLINE Training SentencesAccuracyResults for 561 MEDLINE Test Sentences40k?SCL40k?super1k?SCL1k?supernosource(b) 500 target domain training sentencesModel Testing Accuracynosource 94.51k-super 94.51k-SCL 95.040k-super 95.640k-SCL 96.1(c) McNemar?s Test (500 training sentences)Null Hypothesis p-value1k-super vs. nosource 0.7321k-SCL vs. 1k-super 0.000340k-super vs. nosource 1.9 ?
10?1240k-SCL vs. 40k-super 6.5 ?
10?7Figure 6: PoS tagging results with no target labeled training datatagger improves the performance of a dependencyparser on the target domain.7.1 No Target Labeled Training DataFor the results in this section, we trained astructural correspondence learner with 100,000sentences of unlabeled data from the WSJ and100,000 sentences of unlabeled biomedical data.We use as pivot features words that occur morethan 50 times in both domains.
The supervisedbaseline does not use unlabeled data.
The ASObaseline is an implementation of Ando and Zhang(2005b).
It uses 200,000 sentences of unlabeledMEDLINE data but no unlabeled WSJ data.
ForASO we used as auxiliary problems words that oc-cur more than 500 times in the MEDLINE unla-beled data.Figure 5(a) plots the accuracies of the threemodels with varying amounts of WSJ trainingdata.
With one hundred sentences of trainingdata, structural correspondence learning gives a19.1% relative reduction in error over the super-vised baseline, and it consistently outperformsboth baseline models.
Figure 5(b) gives resultsfor 40,000 sentences, and Figure 5(c) shows cor-responding significance tests, with p < 0.05 be-ing significant.
We use a McNemar paired test forlabeling disagreements (Gillick and Cox, 1989).Even when we use all the WSJ training data avail-able, the SCL model significantly improves accu-racy over both the supervised and ASO baselines.The second column of Figure 5(b) gives un-known word accuracies on the biomedical data.125Of thirteen thousand test instances, approximatelythree thousand were unknown.
For unknownwords, SCL gives a relative reduction in error of19.5% over Ratnaparkhi (1996), even with 40,000sentences of source domain training data.7.2 Some Target Labeled Training DataIn this section we give results for small amounts oftarget domain training data.
In this case, we makeuse of the out-of-domain data by using features ofthe source domain tagger?s predictions in trainingand testing the target domain tagger (Florian et al,2004).
Though other methods for incorporatingsmall amounts of training data in the target domainwere available, such as those proposed by Chelbaand Acero (2004) and by Daume?
III and Marcu(2006), we chose this method for its simplicity andconsistently good performance.
We use as featuresthe current predicted tag and all tag bigrams in a5-token window around the current token.Figure 6(a) plots tagging accuracy for varyingamounts of MEDLINE training data.
The twohorizontal lines are the fixed accuracies of theSCL WSJ-trained taggers using one thousand andforty thousand sentences of training data.
The fivelearning curves are for taggers trained with vary-ing amounts of target domain training data.
Theyuse features on the outputs of taggers from sec-tion 7.1.
The legend indicates the kinds of featuresused in the target domain (in addition to the stan-dard features).
For example, ?40k-SCL?
meansthat the tagger uses features on the outputs of anSCL source tagger trained on forty thousand sen-tences of WSJ data.
?nosource?
indicates a tar-get tagger that did not use any tagger trained onthe source domain.
With 1000 source domain sen-tences and 50 target domain sentences, using SCLtagger features gives a 20.4% relative reductionin error over using supervised tagger features anda 39.9% relative reduction in error over using nosource features.Figure 6(b) is a table of accuracies for 500 tar-get domain training sentences, and Figure 6(c)gives corresponding significance scores.
With1000 source domain sentences and 500 target do-main sentences, using supervised tagger featuresgives no improvement over using no source fea-tures.
Using SCL features still does, however.7.3 Improving Parser PerformanceWe emphasize the importance of PoS tagging in apipelined NLP system by incorporating our SCL100  500  1k 5k 40k58626670747882Dependency Parsing for 561 Test SentencesNumber of WSJ Training SentencesAccuracysupervisedSCLgoldFigure 7: Dependency parsing results using differ-ent part of speech taggerstagger into a WSJ-trained dependency parser andand evaluate it on MEDLINE data.
We use theparser described by McDonald et al (2005b).
Thatparser assumes that a sentence has been PoS-tagged before parsing.
We train the parser and PoStagger on the same size of WSJ data.Figure 7 shows dependency parsing accuracy onour 561-sentence MEDLINE test set.
We parsedthe sentences using the PoS tags output by oursource domain supervised tagger, the SCL taggerfrom subsection 7.1, and the gold PoS tags.
Allof the differences in this figure are significant ac-cording to McNemar?s test.
The SCL tags consis-tently improve parsing performance over the tagsoutput by the supervised tagger.
This is a rather in-direct method of improving parsing performancewith SCL.
In the future, we plan on directly incor-porating SCL features into a discriminative parserto improve its adaptation properties.8 Related WorkDomain adaptation is an important and well-studied area in natural language processing.
Herewe outline a few recent advances.
Roark and Bac-chiani (2003) use a Dirichlet prior on the multi-nomial parameters of a generative parsing modelto combine a large amount of training data from asource corpus (WSJ), and small amount of train-ing data from a target corpus (Brown).
Asidefrom Florian et al (2004), several authors havealso given techniques for adapting classification tonew domains.
Chelba and Acero (2004) first traina classifier on the source data.
Then they use max-imum a posteriori estimation of the weights of a126maximum entropy target domain classifier.
Theprior is Gaussian with mean equal to the weightsof the source domain classifier.
Daume?
III andMarcu (2006) use an empirical Bayes model to es-timate a latent variable model grouping instancesinto domain-specific or common across both do-mains.
They also jointly estimate the parametersof the common classification model and the do-main specific classification models.
Our work fo-cuses on finding a common representation for fea-tures from different domains, not instances.
Webelieve this is an important distinction, since thesame instance can contain some features which arecommon across domains and some which are do-main specific.The key difference between the previous fourpieces of work and our own is the use of unlabeleddata.
We do not require labeled training data inthe new domain to demonstrate an improvementover our baseline models.
We believe this is essen-tial, since many domains of application in naturallanguage processing have no labeled training data.Lease and Charniak (2005) adapt a WSJ parserto biomedical text without any biomedical tree-banked data.
However, they assume other labeledresources in the target domain.
In Section 7.3 wegive similar parsing results, but we adapt a sourcedomain tagger to obtain the PoS resources.To the best of our knowledge, SCL is the firstmethod to use unlabeled data from both domainsfor domain adaptation.
By using just the unlabeleddata from the target domain, however, we can viewdomain adaptation as a standard semisupervisedlearning problem.
There are many possible ap-proaches for semisupservised learning in naturallanguage processing, and it is beyond the scopeof this paper to address them all.
We chose tocompare with ASO because it consistently outper-forms cotraining (Blum and Mitchell, 1998) andclustering methods (Miller et al, 2004).
We didrun experiments with the top-k version of ASO(Ando and Zhang, 2005a), which is inspired bycotraining but consistently outperforms it.
Thisdid not outperform the supervised method for do-main adaptation.
We speculate that this is becausebiomedical and financial data are quite different.In such a situation, bootstrapping techniques arelikely to introduce too much noise from the sourcedomain to be useful.Structural correspondence learning is most sim-ilar to that of Ando (2004), who analyzed asituation with no target domain labeled data.Her model estimated co-occurrence counts fromsource unlabeled data and then used the SVD ofthis matrix to generate features for a named en-tity recognizer.
Our ASO baseline uses unlabeleddata from the target domain.
Since this consis-tently outperforms unlabeled data from only thesource domain, we report only these baseline re-sults.
To the best of our knowledge, this is the firstwork to use unlabeled data from both domains tofind feature correspondences.One important advantage that this work shareswith Ando (2004) is that an SCL model can beeasily combined with all other domain adaptationtechniques (Section 7.2).
We are simply induc-ing a feature representation that generalizes wellacross domains.
This feature representation canthen be used in all the techniques described above.9 ConclusionStructural correspondence learning is a marriageof ideas from single domain semi-supervisedlearning and domain adaptation.
It uses unla-beled data and frequently-occurring pivot featuresfrom both source and target domains to find corre-spondences among features from these domains.Finding correspondences involves estimating thecorrelations between pivot and non-pivot feautres,and we adapt structural learning (ASO) (Ando andZhang, 2005a; Ando and Zhang, 2005b) for thistask.
SCL is a general technique that can be ap-plied to any feature-based discriminative learner.We showed results using SCL to transfer a PoStagger from the Wall Street Journal to a corpusof MEDLINE abstracts.
SCL consistently out-performed both supervised and semi-supervisedlearning with no labeled target domain trainingdata.
We also showed how to combine an SCLtagger with target domain labeled data using theclassifier combination techniques from Florian etal.
(2004).
Finally, we improved parsing perfor-mance in the target domain when using the SCLPoS tagger.One of our next goals is to apply SCL directlyto parsing.
We are also focusing on other po-tential applications, including chunking (Sha andPereira, 2003), named entity recognition (Florianet al, 2004; Ando and Zhang, 2005b; Daume?
IIIand Marcu, 2006), and speaker adaptation (Kuhnet al, 1998).
Finally, we are investigating moredirect ways of applying structural correspondence127learning when we have labeled data from bothsource and target domains.
In particular, the la-beled data of both domains, not just the unlabeleddata, should influence the learned representations.AcknowledgmentsWe thank Rie Kubota Ando and Tong Zhangfor their helpful advice on ASO, Steve Carrolland Pete White of The Children?s Hospital ofPhiladelphia for providing the MEDLINE data,and the PennBioIE annotation team for the anno-tated MEDLINE data used in our test sets.
Thismaterial is based upon work partially supported bythe Defense Advanced Research Projects Agency(DARPA) under Contract No.
NBCHD030010.Any opinions, findings, and conclusions or rec-ommendations expressed in this material are thoseof the author(s) and do not necessarily reflectthe views of the DARPA or the Departmentof Interior-National Business Center (DOI-NBC).Additional support was provided by NSF underITR grant EIA-0205448.ReferencesR.
Ando and T. Zhang.
2005a.
A framework for learn-ing predictive structures from multiple tasks and un-labeled data.
JMLR, 6:1817?1853.R.
Ando and T. Zhang.
2005b.
A high-performancesemi-supervised learning method for text chunking.In ACL.R.
Ando.
2004.
Exploiting unannotated corpora fortagging and chunking.
In ACL.
Short paper.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
JMLR, 3:993?1022.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Workshopon Computational Learning Theory.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, andR.
Mercer.
1992.
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467?479.C.
Chelba and A. Acero.
2004.
Adaptation of maxi-mum entropy capitalizer: Little data can help a lot.In EMNLP.K.
Crammer, Dekel O, J. Keshet, S. Shalev-Shwartz,and Y.
Singer.
2006.
Online passive-aggressive al-gorithms.
JMLR, 7:551?585.H.
Daum e?
III and D. Marcu.
2006.
Domain adaptationfor statistical classifiers.
JAIR.R.
Florian, H. Hassan, A.Ittycheriah, H. Jing,N.
Kambhatla, X. Luo, N. Nicolov, and S. Roukos.2004.
A statistical model for multilingual entity de-tection and tracking.
In of HLT-NAACL.L.
Gillick and S. Cox.
1989.
Some statistical issues inthe comparison of speech recognition algorithms.
InICASSP.R.
Kuhn, P. Nguyen, J.C. Junqua, L. Goldwasser,N.
Niedzielski, S. Fincke, K. Field, and M. Con-tolini.
1998.
Eigenvoices for speaker adaptation.In ICSLP.M.
Lease and E. Charniak.
2005.
Parsing biomedicalliterature.
In IJCNLP.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.R.
McDonald, K. Crammer, and F. Pereira.
2005a.Flexible text segmentation with structured multil-abel classification.
In HLT-EMNLP.R.
McDonald, K. Crammer, and F. Pereira.
2005b.
On-line large-margin training of dependency parsers.
InACL.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Nametagging with word clusters and discriminative train-ing.
In HLT-NAACL.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL.PennBioIE.
2005.
Mining The Bibliome Project.http://bioie.ldc.upenn.edu/.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributionalclustering of english words.
In ACL.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In EMNLP.B.
Roark and M. Bacchiani.
2003.
Supervised andunsupervised PCFG adaptation to novel domains.
InHLT-NAACL.B.
Roark, M. Saraclar, M. Collins, and M. Johnson.2004.
Discriminative language modeling with con-ditional random fields and the perceptron algorithm.In ACL.F.
Sha and F. Pereira.
2003.
Shallow parsing with con-ditional random fields.
In HLT-NAACL.K.
Toutanova, D. Klein, C. D. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In NAACL.128
