Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1048?1053,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Neural Network Architecture for Multilingual Punctuation GenerationMiguel Ballesteros1 Leo Wanner1,21NLP Group, Universitat Pompeu Fabra, Barcelona, Spain2 Catalan Institute for Research and Advanced Studies (ICREA)miguel.ballesteros@upf.edu leo.wanner@upf.eduAbstractEven syntactically correct sentences are per-ceived as awkward if they do not contain cor-rect punctuation.
Still, the problem of au-tomatic generation of punctuation marks hasbeen largely neglected for a long time.
Wepresent a novel model that introduces punc-tuation marks into raw text material withtransition-based algorithm using LSTMs.
Un-like the state-of-the-art approaches, our modelis language-independent and also neutral withrespect to the intended use of the punctuation.Multilingual experiments show that it achieveshigh accuracy on the full range of punctuationmarks across languages.1 IntroductionAlthough omnipresent in (language learner) gram-mar books, punctuation received much less atten-tion in linguistics and natural language processing(Krahn, 2014).
In linguistics, punctuation is gener-ally acknowledged to possess different functions.
Itstraditionally most studied function is that to encodeprosody of oral speech, i.e., the prosodic rhetori-cal function; see, e.g., (Kirchhoff and Primus, 2014)and the references therein.
In particular the commais assumed to possess a strong rhetorical function(Nunberg et al, 2002).
Its other functions are thegrammatical function, which leads it to form a sep-arate (along with semantics, syntax, and phonology)grammatical submodule (Nunberg, 1990), and thesyntactic function (Quirk et al, 1972), which makesit reflect the syntactic structure of a sentence.The different functions of punctuation are also re-flected in different tasks in natural language process-ing (NLP): introduction of punctuation marks into agenerated sentence that is to be read aloud, restora-tion of punctuation in speech transcripts, parsing un-der consideration of punctuation, or generation ofpunctuation in written discourse.
Our work is cen-tered in the last task.
We present a novel punctuationgeneration algorithm that is based on the transition-based algorithm with long short-term memories(LSTMs) by Dyer et al (2015) and character-basedcontinuous-space vector embeddings of words usingbidirectional LSTMs (Ling et al, 2015b; Ballesteroset al, 2015).
The algorithm takes as input raw ma-terial without punctuation and effectively introducesthe full range of punctuation symbols.
Although in-tended, first of all, for use in sentence generation, thealgorithm is function- and language-neutral, whichmakes it different, compared to most of the state-of-the-art approaches, which use function- and/orlanguage-specific features.2 Related WorkThe most prominent punctuation-related NLP taskhas been so far introduction (or restoration) of punc-tuation in speech transcripts.
Most often, classifiermodels are used that are trained on n-gram mod-els (Gravano et al, 2009), on n-gram models en-riched by syntactic and lexical features (Ueffing etal., 2013) and/or by acoustic features (Baron et al,2002; Kola?r?
and Lamel, 2012).
Tilk and Aluma?e(2015) use a lexical and acoustic (pause duration)feature-based LSTM model for the restoration of pe-riods and commas in Estonian speech transcripts.The grammatical and syntactic functions of punctu-ation have been addressed in the context of written1048language.
Some of the proposals focus on the gram-matical function (Doran, 1998; White and Rajku-mar, 2008), while others bring the grammatical andsyntactic functions together and design rule-basedgrammatical resources for parsing (Briscoe, 1994)and surface realization (White, 1995; Guo et al,2010).
Guo et al (2010) is one of the few worksthat is based on a statistical model for the genera-tion of punctuation in the context of Chinese sen-tence generation, trained on a variety of syntacticfeatures from LFG f-structures, preceding punctu-ation bigrams and cue words.Our proposal is most similar to Tilk and Aluma?e(2015), but our task is more complex since we gen-erate the full range of punctuation marks.
Further-more, we do not use any acoustic features.
Com-pared to Guo et al (2010), we do not use any syn-tactic features either since our input is just raw textmaterial.3 ModelOur model is inspired by a number of recent workson neural architectures for structure prediction:Dyer et al (2015)?s transition-based parsing model,Dyer et al (2016)?s generative language model andphrase-structure parser, Ballesteros et al (2015)?scharacter-based word representation for parsing, andLing et al (2015b)?s part-of-speech tagging .3.1 AlgorithmWe define a transition-based algorithm that intro-duces punctuation marks into sentences that do notcontain any punctuation.
In the context of NLG, theinput sentence would be the result of the surface re-alization task (Belz et al, 2011).
As in transition-based parsing (Nivre, 2004), we use two data struc-tures: Nivre?s queue is in our case the input bufferand his stack is in our case the output buffer.
The al-gorithm starts with an input buffer full of words andan empty output buffer.
The two basic actions ofthe algorithm are SHIFT, which moves the first wordfrom the input buffer to the output buffer, and GEN-ERATE, which introduces a punctuation mark afterthe first word in the output buffer.
Figure 1 shows anexample of the application of the two actions.At each stage t of the application of the algorithm,the state, which is defined by the contents of the out-Transition Output Input[ ] [No it was not]SHIFT [No] [it was not]GENERATE(?,?)
[No ,] [it was not]SHIFT [No , it] [was not]SHIFT [No , it was ] [not]SHIFT [No , it was not] [ ]GENERATE(?.?)
[No, it was not .]
[ ]Figure 1: Transition sequence for the input sequence No it wasnot ?
with the output No, it was not.put and input buffers, is encoded in terms of a vectorst; see Section 3.3 for different alternatives of staterepresentation.
As Dyer et al (2015), we use st tocompute the probability of the action at time t as:p(zt | st) =exp(g>ztst + qzt)?z?
?A exp(g>z?st + qz?)
(1)where gz is a vector representing the embeddingof the action z, and qz is a bias term for actionz.
The set A represents the actions (either SHIFTor GENERATE(p)).1 st encodes information aboutprevious actions (since it may include the historywith the actions taken and the generated punctua-tion symbols are introduced in the output buffer, seeSection 3.3), thus the probability of a sequence ofactions z given the input sequence is:p(z | w) =|z|?t=1p(zt | st).
(2)As in (Dyer et al, 2015), the model greedilychooses the best action to take given the state withno backtracking.23.2 Word EmbeddingsFollowing the tagging model of Ling et al (2015b)and the parsing model of Ballesteros et al (2015),we compute character-based continuous-space vec-tor embeddings of words using bidirectional LSTMs(Graves and Schmidhuber, 2005) to learn similarrepresentation for words that are similar from an or-thographic/morphological point of view.1Note that GENERATE(p) includes all possible punctuationsthat the language in question has, and thus the number of classesthe classifier predicts in each time step is #punctuations + 1.2For further optimization, the model could be extended, forinstance, by beam-search.1049The character-based representations may be alsoconcatenated with a fixed vector representation froma neural language model.
The resulting vector ispassed through a component-wise rectifier linearunit (ReLU).
We experiment with and without pre-trained word embeddings.
To pretrain the fixed vec-tor representations, we use the skip n-gram modelintroduced by Ling et al (2015a).3.3 Representing the StateWe work with two possible representations of theinput and output buffers (i.e, the state st): (i) a look-ahead model that takes into account the immediatecontext (two embeddings for the input and two em-beddings for the output), which we use as a base-line, and (ii) the LSTM model, which encodes theentire input sequence and the output sentence withLSTMs.3.3.1 Baseline: Look-ahead ModelThe look-ahead model can be interpreted as a 4-gram model in which two words belong to the inputand two belong to the output.
The representationtakes the average of the two first embeddings of theoutput and the two first embeddings at the front ofthe input.
The word embeddings contain all the rich-ness provided by the character-based LSTMs and thepretrained skip n-gram model embeddings (if used).The resulting vector is passed through a component-wise ReLU and a softmax transformation to obtainthe probability distribution over the possible actionsgiven the state st; see Section 3.1.3.3.2 LSTM ModelThe baseline look-ahead model considers onlythe immediate context for the input and output se-quences.
In the proposed model, we apply recur-rent neural networks (RNNs) that encode the entireinput and output sequences in the form of LSTMs.LSTMs are a variant of RNNs designed to deal withthe vanishing gradient problem inherent in RNNs(Hochreiter and Schmidhuber, 1997; Graves, 2013).RNNs read a vector xt at each time step and com-pute a new (hidden) state ht by applying a linearmap to the concatenation of the previous time step?sstate ht?1 and the input, passing then the outcomethrough a logistic sigmoid non-linearity.We use a simplified version of the stack LSTMmodel of Dyer et al (2015).
The input buffer is en-coded as a stack LSTM, into which we PUSH the en-tire sequence at the beginning and POP words fromit at each time step.
The output buffer is a sequence,encoded by an LSTM, into which we PUSH the fi-nal output sequence.
As in (Dyer et al, 2015), weinclude a third sequence with the history of actionstaken, which is encoded by another LSTM.
As al-ready mentioned above, the three resulting vectorsare passed through a component-wise ReLU and asoftmax transformation to obtain the probability dis-tribution over the possible actions that can be taken(either to shift or to generate a punctuation mark),given the current state st; see Section 3.1.4 ExperimentsTo test our models, we carried experiments onfive languages: Czech, English, French, German,and Spanish.
English, French and Spanish aregenerally assumed to be characterized by prosodicpunctuation, while for German the syntactic punc-tuation is more dominant (Kirchhoff and Primus,2014).
Czech punctuation also leans towards syn-tactic punctuation (Kola?r?
et al, 2004), but due to itsrather free word order we expect it to reflect prosodicpunctuation as well.The punctuation marks that the models attempt topredict (and that also occur in the training sets) foreach language are listed in Table 1.3 Commas rep-resent around 55% and periods around 30% of thetotal number of marks in the datasets.Czech ?.
?, ?,?, ??
?, ?
(?, ?
)?, ?
:?, ?/?, ??
?, ?%?, ?
*?, ?=?, ?|?, ?
?, ?+?,?
;?, ?!
?, ?o?, ??
?, ?&?, ?
[?, ?
]?, ??
?English ??
?, ?
(?, ?
)?, ?,?, ?
?
?, ?.
?, ?.
.
.
?, ?
:?, ?
;?, ??
?, ?
?
?, ?
}?, ?
{?French ?
?
?, ?,?, ??
?, ?
:?, ??
?, ?
(?, ?
)?, ?.
?, ?!
?, ?.
.
.
?German ?
?
?, ?
(?, ?
)?, ?,?, ?.
?, ?/?, ?
:?, ??
?, ?.
.
.
?, ??
?, ?
?
?Spanish ?
?
?, ?
(?, ?
)?, ?,?, ??
?, ?.
?, ?
:?, ??
?, ??
?, ?!
?, ??
?Table 1: Punctuation marks covered in our experiments.4.1 SetupThe stack LSTM model uses two layers, each of di-mension 100 for each input sequence.
For both the3The consideration of some of the symbols listed in Table 1as punctuation marks may be questioned (see, e.g., ?+?
or ???
forCzech).
However, all of them are labeled as punctuation marksin the corresponding tag sets, such that we include them.1050CommasCzech English French German SpanishP R F P R F P R F P R F P R FLookAhead 78.79 43.54 56.09 75.60 38.52 51.04 54.00 22.76 32.02 68.87 32.89 44.52 63.17 19.15 29.39LookAhead + Pre ?
?
?
75.94 40.81 53.09 ?
?
?
71.30 39.62 50.94 58.03 26.67 36.54LSTM 80.79 68.30 74.02 78.88 70.02 74.19 61.73 44.52 51.73 73.78 65.45 69.37 64.01 42.73 51.25LSTM + Pre ?
?
?
80.83 74.81 77.70 ?
?
?
76.56 69.19 72.69 65.65 45.33 53.63PeriodsCzech English French German SpanishP R F P R F P R F P R F P R FLookAhead 82.62 95.64 88.65 88.51 97.76 92.91 71.34 94.61 81.34 77.10 97.76 86.21 73.13 99.13 84.17LookAhead + Pre ?
?
?
87.44 97.71 92.29 ?
?
?
78.26 95.93 86.20 73.16 99.29 84.25LSTM 89.39 93.66 91.48 93.07 98.31 95.62 76.38 95.47 84.86 84.75 98.18 90.97 74.70 98.65 85.02LSTM + Pre ?
?
?
94.44 98.06 96.22 ?
?
?
85.65 98.39 91.58 74.24 98.57 84.69AverageCzech English French German SpanishP R F P R F P R F P R F P R FLookAhead 80.90 58.57 67.95 82.72 52.72 64.40 60.67 32.33 42.18 75.82 52.58 62.10 67.50 33.88 45.12LookAhead + Pre ?
?
?
81.83 53.90 64.99 ?
?
?
75.75 54.57 63.65 64.80 38.58 48.36LSTM 82.42 69.11 75.18 84.89 71.23 77.46 65.34 45.52 53.66 80.03 65.90 72.28 67.78 47.80 56.06LSTM + Pre ?
?
?
83.72 74.56 78.87 ?
?
?
81.60 67.47 73.87 68.09 49.21 57.13Table 2: Results of the LSTM model and the Baseline (Look-ahead model) for precision, recall and F score for commas, periodsand micro average for all punctuation symbols (including commas and periods) listed in Table 1.
+Pre refers to models that includepretrained word embeddings.look-ahead and the stack LSTM models, character-based embeddings, punctuation embeddings andpretrained embeddings (if used) also have 100 di-mensions.
Both models are trained to maximizethe conditional log-likelihood (Eq.
2) of output sen-tences, given the input sequences.For Czech, English, German, and Spanish, we usethe wordforms from the treebanks of the CoNLL2009 Shared Task (Hajic?
et al, 2009); the Frenchdataset is by Candito et al (2010).
Developmentsets are used to optimize the model parameters; theresults are reported for the held-out test sets.4.2 Results and DiscussionTable 2 displays the outcome of the experiments forperiods and commas in all five languages and sum-marizes the overall performance of our algorithmin terms of the micro-average figures.
In order totest whether pretrained word embeddings providefurther improvements, we incorporate them for En-glish, Spanish and German.4The figures show that the LSTMs that encodethe entire context of a punctuation mark are betterthan a strong baseline that takes into account a 4-4Word embeddings for English, Spanish and German aretrained using the AFP portion of the English Gigaword cor-pus (version 5), the German monolingual training data from the2010 Machine Translation Workshop, and the Spanish Giga-word version 3 respectively.gram sliding window of tokens.
They also showthat character-based representations are already use-ful for the punctuation generation task on their own,but when concatenated with pretrained vectors, theyare even more useful.The model is capable of providing good resultsfor all languages, being more consistent for En-glish, Czech and German.
Average sentence lengthmay indicate why the model seems to be worse forSpanish and French, since sentences are longer inthe Spanish (29.8) and French (27.0) datasets, com-pared to German (18.0), Czech (16.8) or English(24.0).
The training set is also smaller in Spanishand French compared to the other languages.
It isworth noting that the results across languages arenot directly comparable since the datasets are differ-ent, and as shown in Table 1, the sets of punctuationmarks that are to be predicted diverge significantly.The figures in Table 2 cannot be directly com-pared with the figures reported by Tilk and Aluma?e(2015) for their LSTM-model on period and commarestoration in speech transcripts: the tasks anddatasets are different.Our results prove that the state representation(through LSTMs, which have already been shown tobe effective for syntax (Dyer et al, 2015; Dyer et al,2016)) and character-based representations (whichallow similar embeddings for words that are mor-1051phologically similar (Ling et al, 2015b; Ballesteroset al, 2015)) are capturing strong linguistic clues topredict punctuation.5 ConclusionsWe presented an LSTM-based architectured that iscapable of adding punctuation marks to sequences oftokens as produced in the context of surface realiza-tion without punctuation with high quality and lin-ear time.5 Compared to other proposals in the field,the architecture has the advantage to operate on se-quences of word forms, without any additional syn-tactic or acoustic features.
This tool could be usedfor ASR (Tilk and Aluma?e, 2015) and grammaticalerror correction (Ng et al, 2014).
In the future, weplan to create cross-lingual models by applying mul-tilingual word embeddings (Ammar et al, 2016).AcknowledgmentsThis work was supported by the European Com-mission under the contract numbers FP7-ICT-610411 (MULTISENSOR) and H2020-RIA-645012(KRISTINA).ReferencesWaleed Ammar, George Mulcaire, Yulia Tsvetkov, Guil-laume Lample, Chris Dyer, and Noah A. Smith.
2016.Massively multilingual word embeddings.
CoRR,abs/1602.01925.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, pages 349?359,Lisbon, Portugal, September.
Association for Compu-tational Linguistics.Don Baron, Elizabeth Shriberg, and Andreas Stolcke.2002.
Automatic punctuation and disfluency detec-tion in multi-party meetings using prosodic and lexi-cal cues.
In Proceedings of the International Confer-ence on Spoken Language Processing, pages 949?952,Denver, CO.Anja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and evalu-ation results.
In Proceedings of the Generation Chal-5The code is available at https://github.com/miguelballesteros/LSTM-punctuationlenges Session at the 13th European Workshop on Nat-ural Language Generation, pages 217?226.Ted Briscoe.
1994.
Parsing (with) punctuation.
Tech-nical report, Rank Xerox Research Centre, Grenoble,France.Marie Candito, Beno?
?t Crabbe?, and Pascal Denis.
2010.Statistical French dependency parsing: treebank con-version and first results.
In Proceedings of the LREC.Christine D. Doran.
1998.
Incorporating Punctuationinto the Sentence Grammar.
Ph.D. thesis, Universityof Pennsylvania.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proceedings of ACL.Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, andNoah A. Smith.
2016.
Recurrent neural networkgrammars.
In Proceedings of NAACL-HLT.Agust?
?n Gravano, Martin Jansche, and Michiel Bacchi-ani.
2009.
Restoring punctuation and capitalizationin transcribed speech.
In Proceedings of the ICASSP2009, pages 4741?4744.Alex Graves and Ju?rgen Schmidhuber.
2005.
Framewisephoneme classification with bidirectional LSTM net-works.
In Proceedings of the International Joint Con-ference on Neural Networks (IJCNN).Alex Graves.
2013.
Generating sequences with recurrentneural networks.
CoRR, abs/1308.0850.Yuqing Guo, Haifeng Wang, and Josef van Genabith.2010.
A linguistically inspired statistical model forchinese punctuation generation.
ACM Transactions onAsian Language Information Processing, 9(2).Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 Shared Task: Syntactic and Semantic Depen-dencies in Multiple Languages.
In Proceedings ofthe Thirteenth Conference on Computational Natu-ral Language Learning (CoNLL 2009): Shared Task,pages 1?18, Boulder, Colorado, June.
Association forComputational Linguistics.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8):1735?1780.Frank Kirchhoff and Beatrice Primus.
2014.
The archi-tecture of punctuation systems.
A historical case studyof the comma in German.
Written Language and Lit-eracy, 17(2):195?224.Ja?chym Kola?r?
and Lori Lamel.
2012.
Development andEvaluation of Automatic Punctuation for French andEnglish Speech-to-Text.
In Proceedings of the 13thInterspeech Conference, Portland, OR.1052Ja?chym Kola?r?, Jan S?vec, and Josef Psutka.
2004.
Au-tomatic Punctuation Annotation in Czech BroadcastNews Speech.
In Proceedings of the 9th ConferenceSpeech and Computer, St. Petersburg, Russia.Albert Edward Krahn.
2014.
A New Paradigm forPunctuation.
Ph.D. thesis, University of Wisconsin-Milwaukee.Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso.2015a.
Two/too simple adaptations of word2vec forsyntax problems.
In Proceedings of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL).Wang Ling, Tiago Lu?
?s, Lu?
?s Marujo, Ramo?n FernandezAstudillo, Silvio Amir, Chris Dyer, Alan W Black, andIsabel Trancoso.
2015b.
Finding function in form:Compositional character models for open vocabularyword representation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP).Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The CoNLL-2014 Shared Taskon Grammatical Error Correction.
In Proceedings ofthe Eighteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?14, Balti-more, Maryland, June.
Association for ComputationalLinguistics.Joakim Nivre.
2004.
Incrementality in deterministic de-pendency parsing.
In Proceedings of the Workshop onIncremental Parsing: Bringing Engineering and Cog-nition Together.Geoffrey Nunberg, Ted Briscoe, and Rodney Huddleston.2002.
Punctuation.
In The Cambridge Grammar ofthe English Language, pages 1723?1764.
CambridgeUniversity Press, Cambridge.Geoffrey Nunberg.
1990.
The Linguistics of Punctua-tion.
CSLI Publications, Stanford, CA.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1972.
A Grammar of ContemporaryEnglish.
Longman, London.Ottokar Tilk and Tanel Aluma?e.
2015.
LSTM for Punc-tuation Restoration in Speech Transcripts.
In Proceed-ings of the 16th Interspeech Conference, Dresden, Ger-many.Nicola Ueffing, Maximilian Bisani, and Paul Vozila.2013.
Improved models for automatic punctuationprediction for spoken and written text.
In Proceedingsof the 14th Interspeech Conference, Lyon, France.Michael White and Rajakrishnan Rajkumar.
2008.
AMore Precise Analysis of Punctuation for Broad-Coverage Surface-Realization with CCG.
In Proceed-ings of the Workshop on Grammar Engineering AcrossFrameworks, pages 17?24, Manchester, UK.Michael White.
1995.
Presenting punctuation.
In Pro-ceedings of the 5th European Workshop on NaturalLanguage Generation, pages 107?125, Lyon, France.1053
