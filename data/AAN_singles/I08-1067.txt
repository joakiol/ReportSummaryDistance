Simple Syntactic and Morphological Processing Can Help English-HindiStatistical Machine TranslationAnanthakrishnan Ramanathan,Pushpak BhattacharyyaDepartment of Computer Scienceand EngineeringIndian Institute of TechnologyPowai, Mumbai-400076India{anand,pb}@cse.iitb.ac.inJayprasad Hegde, Ritesh M. Shah,Sasikumar MCDAC Mumbai (formerly NCST)Gulmohar Cross Road No.
9Juhu, Mumbai-400049India{jjhegde,ritesh,sasi}@cdacmumbai.inAbstractIn this paper, we report our work on incor-porating syntactic and morphological infor-mation for English to Hindi statistical ma-chine translation.
Two simple and compu-tationally inexpensive ideas have proven tobe surprisingly effective: (i) reordering theEnglish source sentence as per Hindi syntax,and (ii) using the suffixes of Hindi words.The former is done by applying simple trans-formation rules on the English parse tree.The latter, by using a simple suffix separa-tion program.
With only a small amount ofbilingual training data and limited tools forHindi, we achieve reasonable performanceand substantial improvements over the base-line phrase-based system.
Our approach es-chews the use of parsing or other sophisti-cated linguistic tools for the target language(Hindi) making it a useful framework forstatistical machine translation from Englishto Indian languages in general, since suchtools are not widely available for Indian lan-guages currently.1 IntroductionTechniques for leveraging syntactic and morpholog-ical information for statistical machine translation(SMT) are receiving a fair amount of attention nowa-days.
For SMT from English to Indian languages,these techniques are especially important for the fol-lowing three reasons: (i) Indian languages differwidely from English in terms of word-order; (ii) In-dian languages are morphologically quite rich; and(iii) large amounts of parallel corpora are not avail-able for these languages, though smaller amounts oftext in specific domains (such as health, tourism, andagriculture) are now becoming accessible.
It mighttherefore be expected that using syntactic and mor-phological information for English to Indian lan-guage SMT will prove highly beneficial in termsof achieving reasonable performance out of limitedparallel corpora.
However, the difficulty in this isthat crucial tools, such as parsers and morphologicalanalyzers, are not widely available for Indian lan-guages yet.In this paper, we present our work on incorporat-ing syntactic and morphological information for En-glish to Hindi SMT.
Our approach, which eschewsthe use of parsing and other tools for Hindi, is two-pronged:1.
Incorporating syntactic information by com-bining phrase-based models with a set of struc-tural preprocessing rules on English2.
Incorporating morphological information byusing a simple suffix separation program forHindi, the likes of which can be created withlimited effort for other Indian languages as wellSignificant improvements over the baselinephrase-based SMT system are obtained using ourapproach.
Table 1 illustrates this with an example 1.Since only limited linguistic effort and tools arerequired for the target language, we believe that theframework we propose is suitable for SMT from En-glish to other Indian languages as well.1This example is discussed further in section 4513input For a celestial trip of the scientific kind, visit the planetarium.reference	 tae	  e , ta ae?vaigyaanika tariike ke eka divya saira ke lie, taaraamandala aaemscientific kind of a celestial trip for, planetarium visit (come)baseline 	 pr	 , 	 ta?ke svargiiya yaatraa ke vaigyaanika prakaara, kaa taaraagruha haiof celestial trip of scientific kind, of planetarium isbaseline+syn	 pr	 e , ta?vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha haiscientific kind of celestial trip for, planetarium isbaseline+syn+morph	 pr	 e , ta?vaigyaanika prakaara ke svargiiya yaatraa ke lie, taaraagruha dekhemscientific kind of celestial trip for, planetarium visit (see)Table 1: Effects of Syntactic and Morphological Processing (reference: human reference translation;baseline: phrase-based system; syn: with syntactic information; morph: with morphological information)The rest of this paper is organized as follows: Sec-tion 2 outlines related work.
Section 3 describes ourapproach ?
first, the phrase-based baseline system issketched briefly, leading up to the techniques usedfor incorporating syntactic and morphological infor-mation within this system.
Experimental results arediscussed in section 4.
Section 5 concludes the pa-per with some directions for future work.2 Related WorkStatistical translation models have evolved from theword-based models originally proposed by Brownet al (1990) to syntax-based and phrase-based tech-niques.The beginnings of phrase-based translation canbe seen in the alignment template model introducedby Och et al (1999).
A joint probability modelfor phrase translation was proposed by Marcu andWong (2002).
Koehn et al (2003) propose certainheuristics to extract phrases that are consistent withbidirectional word-alignments generated by the IBMmodels (Brown et al, 1990).
Phrases extracted us-ing these heuristics are also shown to perform bet-ter than syntactically motivated phrases, the jointmodel, and IBM model 4 (Koehn et al, 2003).Syntax-based models use parse-tree representa-tions of the sentences in the training data to learn,among other things, tree transformation probabili-ties.
These methods require a parser for the targetlanguage and, in some cases, the source languagetoo.
Yamada and Knight (2001) propose a modelthat transforms target language parse trees to sourcelanguage strings by applying reordering, insertion,and translation operations at each node of the tree.Graehl and Knight (2004) and Melamed (2004), pro-pose methods based on tree-to-tree mappings.
Ima-mura et al (2005) present a similar method thatachieves significant improvements over a phrase-based baseline model for Japanese-English transla-tion.Recently, various preprocessing approaches havebeen proposed for handling syntax within SMT.These algorithms attempt to reconcile the word-order differences between the source and target lan-guage sentences by reordering the source languagedata prior to the SMT training and decoding cy-cles.
Nie?en and Ney (2004) propose some restruc-turing steps for German-English SMT.
Popovic andNey (2006) report the use of simple local trans-formation rules for Spanish-English and Serbian-English translation.
Collins et al (2006) proposeGerman clause restructuring to improve German-English SMT.The use of morphological information for SMThas been reported in (Nie?en and Ney, 2004) and(Popovic and Ney, 2006).
The detailed experi-ments by Nie?en and Ney (2004) show that the useof morpho-syntactic information drastically reducesthe need for bilingual training data.Recent work by Koehn and Hoang (2007) pro-514poses factored translation models that combine fea-ture functions to handle syntactic, morphological,and other linguistic information in a log-linearmodel.Our work uses a preprocessing approach for in-corporating syntactic information within a phrase-based SMT system.
For incorporating morphology,we use a simple suffix removal program for Hindiand a morphological analyzer for English.
These as-pects are described in detail in the next section.3 Syntactic & Morphological Informationfor English-Hindi SMT3.1 Phrase-Based SMT: the BaselineGiven a source sentence f , SMT chooses as its trans-lation e?, which is the sentence with the highest prob-ability:e?
= arg maxep(e|f)According to Bayes?
decision rule, this is writtenas:e?
= arg maxep(e)p(f |e)The phrase-based model that we use as our base-line system (defined by Koehn et al (2003)) com-putes the translation model p(f |e) by using a phrasetranslation probability distribution.
The decodingprocess works by segmenting the input sentence finto a sequence of I phrases fI1.
A uniform proba-bility distribution over all possible segmentations isassumed.
Each phrase fiis translated into a targetlanguage phrase eiwith probability ?(fi|ei).
Re-ordering is penalized according to a simple exponen-tial distortion model.The phrase translation table is learnt in the fol-lowing manner: The parallel corpus is word-alignedbidirectionally, and using various heuristics (see(Koehn et al, 2003) for details) phrase correspon-dences are established.
Given the set of collectedphrase pairs, the phrase translation probability is cal-culated by relative frequency:?
(f |e) = count(f, e)?fcount(f, e)Lexical weighting, which measures how wellwords within phrase pairs translate to each other,validates the phrase translation, and addresses theproblem of data sparsity.The language model p(e) used in our baseline sys-tem is a trigram model with modified Kneser-Neysmoothing (Chen and Goodman, 1998).The weights for the various components of themodel (phrase translation model, language model,distortion model etc.)
are set by minimum error ratetraining (Och, 2003).3.2 Syntactic InformationAs mentioned in section 2, phrase-based modelshave emerged as the most successful method forSMT.
These models, however, do not handle syntaxin a natural way.
Reordering of phrases during trans-lation is typically managed by distortion models,which have proved not entirely satisfactory (Collinset al, 2006), especially for language pairs that differa lot in terms of word-order.
We use a preprocess-ing approach to get over this problem, by reorderingthe English sentences in the training and test corporabefore the SMT system kicks in.
This reduces, andoften eliminates, the ?distortion load?
on the phrase-based system.The reordering rules that we use for prepro-cessing can be broadly described by the followingtransformation rule going from English to Hindiword order (Rao et al 2000):SSmV VmOOmCm ?
C?mS?mS?O?mO?V?mV?where,S: SubjectO: ObjectV : VerbCm: Clause modifierX?
: Corresponding constituent in Hindi,where X is S, O, or VXm: modifier of XEssentially, the SVO order of English is changedto SOV order, and post-modifiers are converted topre-modifiers.
Our preprocessing module effectsthis by parsing the input English sentence 2 and ap-2Dan Bikel?s parser was used for parsing(http://www.cis.upenn.edu/d?bikel/license.html).515structural transformationmorph analysis (English) Giza++alignment correctionphrase extractionsuffix separation(Hindi)decoder			suffix separation(Hindi)Figure 1: Syntactic and Morphological Processing: Schematicplying a handful of reordering rules on the parse tree.Table 2 illustrates this with an example.3.3 Morphological InformationIf an SMT system considers different morphologi-cal forms of a word as independent entities, a cru-cial source of information is neglected.
It is con-ceivable that with the use of morphological informa-tion, especially for morphologically rich languages,the requirement for training data might be much re-duced.
This is indicated, for example, in recentwork on German-English statistical MT with limitedbilingual training data (Nie?en and Ney, 2004), andalso in other applications such as statistical part-of-speech tagging of Hindi (Gupta et al, 2006).The separation of morphological suffixes con-flates various forms of a word, which results inhigher counts for both words and suffixes, therebycountering the problem of data sparsity.
As an exam-ple, assume that the following sentence pair is partof the bilingual training corpus:English: Players should just play.Hindi: e?khilaadiyom ko kevala khelanaa caahieHindi (suffix separated):  i  e?khilaada iyom ko kevala khela naa caahieNow, consider the input sentence, ?The men cameacross some players,?
which should be translated as?a!
 ?
(aadmiyom kokucha khilaadii mile).
Without using morphology,the system is constrained to the choice of (khilaadiyom) for the word players (based just on the516EnglishS?
??
?The presidentSm?
??
?of AmericaV?
??
?visitedO?
??
?IndiaVm?
??
?in JuneReordered America of?
??
?Smthe president?
??
?SJune in?
??
?VmIndia?
??
?Ovisited?
??
?VHindia	"#$ta %&  'ta 	 amariikaa ke raashtrapati ne juuna mem bhaarata kii yaatraa kiiTable 2: English and Hindi Word-Ordera ae ata aei aa ata ( e) i * ( aeu i ata a( ae( ai ataa( ae ai ata eaea a+ ata e a)e i+ ata aea*a ai+ ataae iea atae a aaaua ataa a a aieue ae aaaaua aa a aa aTable 3: Hindi Suffix Listevidence from the above sentence pair in the train-ing corpus).
Also, the general relationship betweenthe oblique case (indicated by the suffix i (iyom))and the case marker 	 (ko) is not learnt, but onlythe specific relationship between  (khi-laadiyom) and 	 (ko).
This indicates the necessityof using morphological information for languagessuch as Hindi.To incorporate morphological information, weuse a morphological analyzer (Minnen et al, 2001)for English, and a simple suffix separation programfor Hindi.
The suffix separation program is basedon the Hindi stemmer presented in (Ananthakrish-nan and Rao, 2003), and works by separating fromeach word the longest possible suffix from table 3.
Adetailed analysis of noun, adjective, and verb inflec-tions that were used to create this list can be found in(McGregor, 1977) and (Rao, 1996).
A few examplesof each type are given below:Noun Inflections: Nouns in Hindi are inflectedbased on the case (direct or oblique), the number(singular or plural), and the gender (masculine orfeminine3).
For example, 	 (ladakaa - boy)becomes (ladake) when in oblique case, andthe plural (ladake - boys) becomes 	(ladakom).
The feminine noun	 (ladakii - girl)is inflected as 	+ (ladakiyaam - plural direct)and 	 (ladakiyom - plural oblique), but it re-mains uninflected in the singular direct case.Adjective Inflections: Adjectives which end ina (aa) or a (aam) in their direct singular mascu-line form agree with the noun in gender, number, andcase.
For example, the singular directa,! (accha)is inflected as a,!
(acche) in all other masculineforms, and as a,!
(acchii) in all feminine forms.Other adjectives are not inflected.Verb Inflections: Hindi verbs are inflected basedon gender, number, person, tense, aspect, modality,formality, and voice.
(Rao, 1996) provides a com-plete list of verb inflection rules.The overall process used for incorporating syn-tactic and morphological information, as describedin this section, is shown in figure 1.3Hindi does not possess a neuter gender517TechniqueEvaluation MetricBLEU mWER SSER roughly understandable+ understandable+baseline 12.10 77.49 91.20 10% 0%baseline+syn 16.90 69.18 74.40 42% 12%baseline+syn+morph 15.88 70.69 66.40 46% 28%Table 4: Evaluation Results (baseline: phrase-based system; syn: with syntactic information; morph: withmorphological information)4 Experimental ResultsThe corpus described in the table below was used forthe experiments.#sentences #wordsTraining 5000 120,153Development 483 11,675Test 400 8557Monolingual (Hindi) 49,937 1,123,966The baseline system was implemented by trainingthe phrase-based system described in section 3 onthe 5000 sentence training corpus.For the Hindi language model, we compared var-ious n-gram models, and found trigram models withmodified Kneser-Ney smoothing to be the best per-forming (Chen and Goodman, 1998).
One languagemodel was learnt from the Hindi part of the 5000sentence training corpus.
The larger monolingualHindi corpus was used to learn another languagemodel.
The SRILM toolkit 4 was used for the lan-guage modeling experiments.The development corpus was used to set weightsfor the language models, the distortion model, thephrase translation model etc.
using minimum er-ror rate training.
Decoding was performed usingPharaoh 5.fnTBL (Ngai and Florian, 2001) was used to POStag the English corpus, and Bikel?s parser was usedfor parsing.
The reordering program was written us-ing the perl module Parse::RecDescent.We evaluated the various techniques on the fol-lowing criteria.
For the objective criteria (BLEUand mWER), two reference translations per sentencewere used.?
BLEU (Papineni et al, 2001): This measures4http://www.speech.sri.com/projects/srilm/5http://www.isi.edu/licensed-sw/pharaoh/the precision of n-grams with respect to the ref-erence translations, with a brevity penalty.
Ahigher BLEU score indicates better translation.?
mWER (multi-reference word errorrate) (Nie?en et al, 2000): This measuresthe edit distance with the most similar refer-ence translation.
Thus, a lower mWER score isdesirable.?
SSER (subjective sentence error rate) (Nie?enet al, 2000): This is calculated using humanjudgements.
Each sentence was judged by a hu-man evaluator on the following five-point scale,and the SSER was calculated as described in(Nie?en et al, 2000).0 Nonsense1 Roughly understandable2 Understandable3 Good4 PerfectAgain, the lower the SSER, the better the trans-lation.Table 4 shows the results of the evaluation.
Wefind that using syntactic preprocessing brings sub-stantial improvements over the baseline phrase-based system.
While the impact of morphologicalinformation is not seen in the BLEU and mWERscores, the subjective scores reveal the effectivenessof using morphology.
The last two columns of thetable show the percentage of sentences that werefound by the human judges to be roughly under-standable (or higher) and understandable (or higher)respectively in the evaluation scale.
We find thatincluding syntactic and morphological informationbrings substantial improvements in translation flu-ency.518An Example: Consider, again, the example intable 1.
The word-order in the baseline translationis woeful, while the translations after syntactic pre-processing (baseline+syn and baseline+syn+morph)follow the correct Hindi order (compare with the ref-erence translation).
The effect of suffix separationcan be seen from the verb form ( (dekhem) ?
visitor see) in the last translation (baseline+syn+morph).The reason for this is that the pair ?visit ?
?
isnot available to be learnt from the original and thesyntactically preprocessed corpora, but the follow-ing pairs are: (i) to visit ?
 (ii) worth visit-ing ?
-, and (iii) can visit ?
	ta.
Thus, the baseline and baseline+syn models arenot able to produce the correct verb form for ?visit?.On the other hand, the baseline+syn+morph model,due to the suffix separation process, combines (dekha) and e (em) from different mappings in thealigned corpus, e.g., ?visit +ing ?
?
and ?sing?
 e?, to get the right translation for visit ()in this context.5 ConclusionWe have presented in this paper an effective frame-work for English-Hindi phrase-based SMT.
The re-sults demonstrate that significant improvements arepossible through the use of relatively simple tech-niques for incorporating syntactic and morphologi-cal information.Since all Indian languages follow SOV order,and are relatively rich in terms of morphology, theframework presented should be applicable to En-glish to Indian language SMT in general.
Given thatmorphological and parsing tools are not yet widelyavailable for Indian languages, an approach like ourswhich minimizes use of such tools for the target lan-guage would be quite desirable.In future work, we propose to experiment witha more sophisticated morphological analyzer.
Asmore parallel corpora become available, we also in-tend to measure the effects of using morphology oncorpora requirements.
Finally, a formal evaluationof these techniques for other Indian languages (es-pecially Dravidian languages such as Tamil) wouldbe interesting.AcknowledgementsWe are grateful to Sachin Anklekar and SaurabhKushwaha for their assistance with the tedious taskof collecting and preprocessing the corpora.ReferencesAnanthakrishnan Ramanathan and Durgesh Rao, ALightweight Stemmer for Hindi, Workshopon Computational Linguistics for South-AsianLanguages, EACL, 2003.Peter F. Brown, John Cocke, Stephen Della Pietra,Vincent J. Della Pietra, Frederick Jelinek, JohnD.
Lafferty, Robert L. Mercer, and Paul S.Roossin, A Statistical Approach to MachineTranslation, Computational Linguistics, 16(2),pages 79?85, June 1990.Stanley F. Chen and Joshua T. Goodman, An Empir-ical Study of Smoothing Techniques for Lan-guage Modeling, Technical Report TR-10-98,Computer Science Group, Harvard University,1998.Michael Collins, Philipp Koehn, and IvonaKucerova, Clause Restructuring for StatisticalMachine Translation, Proceedings of ACL,pages 531?540, 2006.Jonathan Graehl and Kevin Knight, Training TreeTransducers, Proceedings of HLT-NAACL,2004.Kuhoo Gupta, Manish Shrivastava, Smriti Singh,and Pushpak Bhattacharyya, MorphologicalRichness Offsets Resource Poverty ?
an Expe-rience in Builing a POS Tagger for Hindi, Pro-ceedings of ACL-COLING, 2006.Kenji Imamura, Hideo Okuma, Eiichiro Sumita,Practical Approach to Syntax-based Statisti-cal Machine Translation, Proceedings of MT-SUMMIT X, 2005.Philipp Koehn and Hieu Hoang, Factored Transla-tion Models, Proceedings of EMNLP, 2007.Philip Koehn, Franz Josef Och, and Daniel Marcu,Statistical Phrase-based Translation, Proceed-ings of HLT-NAACL, 2003.519Daniel Marcu and William Wong, A Phrase-basedJoint Probability Model for Statistical MachineTranslation, Proceedings of EMNLP, 2002.R.
S. McGregor, Outline of Hindi Grammar, OxfordUniversity Press, Delhi, India, 1974.I.
Dan Melamed, Statistical Machine Translation byParsing, Proceedings of ACL, 2004.Guido Minnen, John Carroll, and Darren Pearce,Applied Morphological Processing of English,Natural Language Engineering, 7(3), 207?223,2001.G.
Ngai and R. Florian, Transformation-basedLearning in the Fast Lane, Proceedings ofNAACL, 2001.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney, An Evaluation Tool for Ma-chine Translation: Fast Evaluation for MT Re-search, International Conference on LanguageResources and Evaluation, pages 39?45, 2000.Sonja Nie?en and Hermann Ney, Statistical Ma-chine Translation with Scarce Resources UsingMorpho-syntactic Information, ComputationalLinguistics, 30(2), pages 181?204, 2004.Franz Josef Och, Christoph Tillman, and HermannNey, Improved Alignment Models for Sta-tistical Machine Translation, Proceedings ofEMNLP, pages 20?28, 1999.Franz Josef Och, Minimum Error Rate Training inStatistical Machine Translation, Proceedings ofACL, 2003.Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, BLEU: a Method for AutomaticEvaluation of Machine Translation, IBM Re-search Report, Thomas J. Watson ResearchCenter, 2001.Maja Popovic and Hermann Ney, Statistical Ma-chine Translation with a Small Amount ofBilingual Training Data, 5th LREC SALTMILWorkshop on Minority Languages, pages 25?29, 2006.Durgesh Rao, Natural Language Generation for En-glish to Hindi Human-Aided Machine Transla-tion of News Stories, Master?s Thesis, IndianInstitute of Technology, Bombay, 1996.Durgesh Rao, Kavitha Mohanraj, Jayprasad Hegde,Vivek Mehta, and Parag Mahadane, A Prac-tical Framework for Syntactic Transfer ofCompound-Complex Sentences for English-Hindi Machine Translation, Proceedings ofKBCS, 2000.Kenji Yamada and Kevin Knight A Syntax-basedStatistical Translation Model, Proceedings ofACL, 2001.520
