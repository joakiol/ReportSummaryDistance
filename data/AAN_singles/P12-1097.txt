Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 921?929,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCharacter-Level Machine Translation Evaluationfor Languages with Ambiguous Word BoundariesChang Liu and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417{liuchan1,nght}@comp.nus.edu.sgAbstractIn this work, we introduce the TESLA-CELAB metric (Translation Evaluation ofSentences with Linear-programming-basedAnalysis ?
Character-level Evaluation forLanguages with Ambiguous word Bound-aries) for automatic machine translation eval-uation.
For languages such as Chinese wherewords usually have meaningful internal struc-ture and word boundaries are often fuzzy,TESLA-CELAB acknowledges the advantageof character-level evaluation over word-levelevaluation.
By reformulating the problem inthe linear programming framework, TESLA-CELAB addresses several drawbacks of thecharacter-level metrics, in particular the mod-eling of synonyms spanning multiple char-acters.
We show empirically that TESLA-CELAB significantly outperforms character-level BLEU in the English-Chinese translationevaluation tasks.1 IntroductionSince the introduction of BLEU (Papineni et al,2002), automatic machine translation (MT) eval-uation has received a lot of research interest.The Workshop on Statistical Machine Transla-tion (WMT) hosts regular campaigns comparingdifferent machine translation evaluation metrics(Callison-Burch et al, 2009; Callison-Burch et al,2010; Callison-Burch et al, 2011).
In the WMTshared tasks, many new generation metrics, such asMETEOR (Banerjee and Lavie, 2005), TER (Snoveret al, 2006), and TESLA (Liu et al, 2010) have con-sistently outperformed BLEU as judged by the cor-relations with human judgments.The research on automatic machine translationevaluation is important for a number of reasons.
Au-tomatic translation evaluation gives machine trans-lation researchers a cheap and reproducible way toguide their research and makes it possible to com-pare machine translation methods across differentstudies.
In addition, machine translation systemparameters are tuned by maximizing the automaticscores.
Some recent research (Liu et al, 2011) hasshown evidence that replacing BLEU by a newermetric, TESLA, can improve the human judgedtranslation quality.Despite the importance and the research inter-est on automatic MT evaluation, almost all existingwork has focused on European languages, in partic-ular on English.
Although many methods aim tobe language neutral, languages with very differentcharacteristics such as Chinese do present additionalchallenges.
The most obvious challenge for Chineseis that of word segmentation.Unlike European languages, written Chinese isnot split into words.
Segmenting Chinese sentencesinto words is a natural language processing taskin its own right (Zhao and Liu, 2010; Low et al,2005).
However, many different segmentation stan-dards exist for different purposes, such as MicrosoftResearch Asia (MSRA) for Named Entity Recog-nition (NER), Chinese Treebank (CTB) for parsingand part-of-speech (POS) tagging, and City Univer-sity of Hong Kong (CITYU) and Academia Sinica(AS) for general word segmentation and POS tag-ging.
It is not clear which standard is the best in agiven scenario.The only prior work attempting to address theproblem of word segmentation in automatic MTevaluation for Chinese that we are aware of is Li et921?
?buy umbrella?
?
?buy umbrella?
?
?buy rain umbrellaFigure 1: Three forms of the same expression buy um-brella in Chineseal.
(2011).
The work compared various MT eval-uation metrics (BLEU, NIST, METEOR, GTM, 1?
TER) with different segmentation schemes, andfound that treating every single character as a token(character-level MT evaluation) gives the best corre-lation with human judgments.2 MotivationLi et al (2011) identify two reasons that character-based metrics outperform word-based metrics.
Forillustrative purposes, we use Figure 1 as a runningexample in this paper.
All three expressions are se-mantically identical (buy umbrella).
The first twoforms are identical because ?
?1 and ?
are syn-onyms.
The last form is simply an (arguably wrong)alternative segmented form of the second expres-sion.1.
Word-based metrics do not award partialmatches, e.g., ?_??
and ?_?
would bepenalized for the mismatch between ??
and?.
Character-based metrics award the matchbetween characters?
and?.2.
Character-based metrics do not suffer from er-rors and differences in word segmentation, so?_??
and ?_?_?
would be judged ex-actly equal.Li et al (2011) conduct empirical experiments toshow that character-based metrics consistently out-perform their word-based counterparts.
Despitethat, we observe two important problems for thecharacter-based metrics:1.
Although partial matches are partially awarded,the mechanism breaks down for n-grams where1Literally, rain umbrella.n > 1.
For example, between ?_?_?
and?_?, higher-order n-grams such as?_?
and?_?
still have no match, and will be penal-ized accordingly, even though ?_?_?
and?_?
should match exactly.
N-grams suchas ?_?
which cross natural word boundariesand are meaningless by themselves can be par-ticularly tricky.2.
Character-level metrics can utilize only a smallpart of the Chinese synonym dictionary, such as?
and ?
(you).
The majority of Chinese syn-onyms involve more than one character, suchas ??
and?
(umbrella), and ??
and ??
(child).In this work, we attempt to address both of theseissues by introducing TESLA-CELAB, a character-level metric that also models word-level linguisticphenomenon.
We formulate the n-gram matchingprocess as a real-valued linear programming prob-lem, which can be solved efficiently.
The metricis based on the TESLA automatic MT evaluationframework (Liu et al, 2010; Dahlmeier et al, 2011).3 The Algorithm3.1 Basic MatchingWe illustrate our matching algorithm using the ex-amples in Figure 1.
Let ???
be the reference,and??
be the candidate translation.We use Cilin (?????
)2 as our synonymdictionary.
The basic n-gram matching problem isshown in Figure 2.
Two n-grams are connected ifthey are identical, or if they are identified as syn-onyms by Cilin.
Notice that all n-grams are put inthe same matching problem regardless of n, unlikein translation evaluation metrics designed for Eu-ropean languages.
This enables us to designate n-grams with different values of n as synonyms, suchas??
(n = 2) and?
(n = 1).In this example, we are able to make a total of twosuccessful matches.
The recall is therefore 2/6 andthe precision is 2/3.2http://ir.hit.edu.cn/phpwebsite/index.php?module=pagemaster&PAGE_user_op=view_page&PAGE_id=162922?
?
?
??
??
????
?
?
?Figure 2: The basic n-gram matching problem?
?
?
??
??
????
?
?
?Figure 3: The n-gram matching problem after phrasematching3.2 Phrase MatchingWe note in Figure 2 that the trigram???
and thebigram ??
are still unmatched, even though thematch between??
and?
should imply the matchbetween???
and?
?.We infer the matching of such phrases using adynamic programming algorithm.
Two n-grams areconsidered synonyms if they can be segmented intosynonyms that are aligned.
With this extension,we are able to match ???
and ??
(since ?matches ?
and ??
matches ?).
The matchingproblem is now depicted by Figure 3.The linear programming problem is mathemati-cally described as follows.
The variables w(?, ?)
arethe weights assigned to the edges,w(?,?)
?
[0, 1]w(?,?)
?
[0, 1]w(??,?)
?
[0, 1]w(???,??)
?
[0, 1]We require that for any node N , the sum ofweights assigned to edges linking N must not ex-ceed one.wref(?)
= w(?,?)wref(?)
= w(?,?)wref(??)
= w(??,?)wref(???)
= w(???,??)??
?
?
?Figure 4: A covered n-gram matching problemwcand(?)
= w(?,?)wcand(?)
= w(?,?)
+ w(??,?)wcand(??)
= w(???,??
)wherewref(X) ?
[0, 1] ?Xwcand(X) ?
[0, 1] ?XNow we maximize the total match,w(?,?)+w(?,?)+w(??,?)+w(???,??
)In this example, the best match is 3, resulting in arecall of 3/6 and a precision of 3/3.3.3 Covered MatchingIn Figure 3, n-grams?
and??
in the reference re-main impossible to match, which implies misguidedpenalty for the candidate translation.
We observethat since ???
has been matched, all its sub-n-grams should be considered matched as well, includ-ing ?
and ??.
We call this the covered n-grammatching rule.
This relationship is implicit in thematching problem for English translation evaluationmetrics where words are well delimited.
But withphrase matching in Chinese, it must be modeled ex-plicitly.However, we cannot simply perform covered n-gram matching as a post processing step.
As an ex-ample, suppose we are matching phrases ??
and?, as shown in Figure 4.
The linear programmingsolver may come up with any of the solutions wherew(?,?)
+ w(??,?)
= 1, w(?,?)
?
[0, 1],and w(??,?)
?
[0, 1].To give the maximum coverage for the node ?,only the solution w(?,?)
= 0, w(??,?)
= 1 isaccepted.
This indicates the need to model covered923n-gram matching in the linear programming prob-lem itself.We return to the matching of the reference ???
and the candidate??
in Figure 3.
On top of thew(?)
variables already introduced, we add the vari-ables maximum covering weights c(?).
Each c(X)represents the maximum w(Y ) variable where n-gram Y completely covers n-gram X .cref(?)
?
max(wref(?
), wref(??),wref(???))cref(?)
?
max(wref(?
), wref(??),wref(??
), wref(???))cref(?)
?
max(wref(?
), wref(??),wref(???))cref(??)
?
max(wref(??
), wref(???))cref(??)
?
max(wref(??
), wref(???))cref(???)
?
wref(???)ccand(?)
?
max(wcand(?
), wcand(??))ccand(?)
?
max(wcand(?
), wcand(??))ccand(??)
?
wcand(??
)wherecref(X) ?
[0, 1] ?Xccand(X) ?
[0, 1] ?XHowever, the max(?)
operator is not allowed inthe linear programming formulation.
We get aroundthis by approximating max(?)
with the sum instead.Hence,cref(?)
?
wref(?)
+ wref(??)+wref(???)cref(?)
?
wref(?)
+ wref(??)+wref(??)
+ wref(???).
.
.We justify this approximation by the followingobservation.
Consider the sub-problem consistingof just the w(?, ?
), wref(?
), wcand(?)
variables andtheir associated constraints.
This sub-problem canbe seen as a maximum flow problem where all con-stants are integers, hence there exists an optimal so-lution where each of the w variables is assigned avalue of either 0 or 1.
For such a solution, themax and the sum forms are equivalent, since thecref(?)
and ccand(?)
variables are also constrained tothe range [0, 1].The maximum flow equivalence breaks downwhen the c(?)
variables are introduced, so in the gen-eral case, replacing max with sum is only an approx-imation.Returning to our sample problem, the linear pro-gramming solver simply needs to assign:w(???,??)
= 1wref(???)
= 1wcand(??)
= 1Consequently, due to the maximum coveringweights constraint, we can give the following valueassignment, implying that all n-grams have beenmatched.cref(X) = 1 ?Xccand(X) = 1 ?X3.4 The Objective FunctionWe now define our objective function in terms ofthe c(?)
variables.
The recall is a function of?X cref(X), and the precision is a function of?Y ccand(Y ), where X is the set of all n-grams ofthe reference, and Y is the set of all n-grams of thecandidate translation.Many prior translation evaluation metrics such asMAXSIM (Chan and Ng, 2008) and TESLA (Liuet al, 2010; Dahlmeier et al, 2011) use the F-0.8measure as the final score:F0.8 =Precision?
Recall0.8?
Precision + 0.2?
RecallUnder some simplifying assumptions ?
specifi-cally, that precision = recall ?
basic calculus showsthat F0.8 is four times as sensitive to recall than toprecision.
Following the same reasoning, we wantto place more emphasis on recall than on precision.We are also constrained by the linear programmingframework, hence we set the objective function as1Z(?Xcref(X) + f?Yccand(Y ))0 < f < 1924We set f = 0.25 so that our objective functionis also four times as sensitive to recall than to pre-cision.3 The value of this objective function is ourTESLA-CELAB score.
Similar to the other TESLAmetrics, when there are N multiple references, wematch the candidate translation against each of themand use the average of the N objective function val-ues as the segment level score.
System level score isthe average of all the segment level scores.Z is a normalizing constant to scale the metric tothe range [0, 1], chosen so that when all the c(?)
vari-ables have the value of one, our metric score attainsthe value of one.4 ExperimentsIn this section, we test the effectiveness of TESLA-CELAB on some real-world English-Chinese trans-lation tasks.4.1 IWSLT 2008 English-Chinese CTThe test set of the IWSLT 2008 (Paul, 2008)English-Chinese ASR challenge task (CT) consistsof 300 sentences of spoken language text.
The av-erage English source sentence is 5.8 words long andthe average Chinese reference translation is 9.2 char-acters long.
The domain is travel expressions.The test set was translated by seven MT systems,and each translation has been manually judged foradequacy and fluency.
Adequacy measures whetherthe translation conveys the correct meaning, even ifthe translation is not fully fluent, whereas fluencymeasures whether a translation is fluent, regardlessof whether the meaning is correct.
Due to highevaluation costs, adequacy and fluency assessmentswere limited to the translation outputs of four sys-tems.
In addition, the translation outputs of the MTsystems are also manually ranked according to theirtranslation quality.Inter-judge agreement is measured by the Kappacoefficient, defined as:Kappa =P (A)?
P (E)1?
P (E)where P (A) is the percentage of agreement, andP (E) is the percentage of agreement by pure3Our empirical experiments suggest that the correlations doplateau near this value.
For simplicity, we choose not to tune fon the training data.Judgment Set 2 31 0.4406 0.43552 - 0.4134Table 1: Inter-judge Kappa for the NIST 2008 English-Chinese taskchance.
The inter-judge Kappa is 0.41 for fluency,0.40 for adequacy, and 0.57 for ranking.
Kappa val-ues between 0.4 and 0.6 are considered moderate,and the numbers are in line with other comparableexperiments.4.2 NIST 2008 English-Chinese MT TaskThe NIST 2008 English-Chinese MT task consistsof 127 documents with 1,830 segments, each withfour reference translations and eleven automaticMT system translations.
The data is available asLDC2010T01 from the Linguistic Data Consortiuim(LDC).
The domain is newswire texts.
The averageEnglish source sentence is 21.5 words long and theaverage Chinese reference translation is 43.2 char-acters long.Since no manual evaluation is given for the dataset, we recruited twelve bilingual judges to evalu-ate the first thirty documents for adequacy and flu-ency (355 segments for a total of 355?
11 = 3, 905translated segments).
The final score of a sentenceis the average of its adequacy and fluency scores.Each judge works on one quarter of the sentences sothat each translation is judged by three judges.
Thejudgments are concatenated to form three full sets ofjudgments.We ignore judgments where two sentences areequal in quality, so that there are only two possibleoutcomes (X is better than Y; or Y is better than X),and P (E) = 1/2.
The Kappa values are shown inTable 1.
The values indicate moderate agreement,and are in line with other comparable experiments.4.3 Baseline Metrics4.3.1 BLEUAlthough word-level BLEU has often been foundinferior to the new-generation metrics when thetarget language is English or other European lan-guages, prior research has shown that character-levelBLEU is highly competitive when the target lan-guage is Chinese (Li et al, 2011).
Therefore, we925Segment Pearson Spearman rankMetric Type consistency correlation correlationBLEU character-level 0.7004 0.9130 0.9643TESLA-M word-level 0.6771 0.9167 0.8929TESLA-CELAB?
character-level 0.7018 0.9229 0.9643TESLA-CELAB hybrid 0.7281?
0.9490??
0.9643Table 2: Correlation with human judgment on the IWSLT 2008 English-Chinese challenge task.
* denotes better thanthe BLEU baseline at 5% significance level.
** denotes better than the BLEU baseline at 1% significance level.Segment Pearson Spearman rankMetric Type consistency correlation correlationBLEU character-level 0.7091 0.8429 0.7818TESLA-M word-level 0.6969 0.8301 0.8091TESLA-CELAB?
character-level 0.7158 0.8514 0.8227TESLA-CELAB hybrid 0.7162 0.8923??
0.8909?
?Table 3: Correlation with human judgment on the NIST 2008 English-Chinese MT task.
** denotes better than theBLEU baseline at 1% significance level.use character-level BLEU as our main baseline.The correlations of character-level BLEU and theaverage human judgments are shown in the first rowof Tables 2 and 3 for the IWSLT and the NISTdata set, respectively.
Segment-level consistency isdefined as the number of correctly predicted pair-wise rankings divided by the total number of pair-wise rankings.
Ties are excluded from the calcu-lation.
We also report the Pearson correlation andthe Spearman rank correlation of the system-levelscores.
Note that in the IWSLT data set, the Spear-man rank correlation is highly unstable due to thesmall number of participating systems.4.3.2 TESLA-MIn addition to character-level BLEU, we alsopresent the correlations for the word-level metricTESLA.
Compared to BLEU, TESLA allows moresophisticated weighting of n-grams and measures ofword similarity including synonym relations.
It hasbeen shown to give better correlations than BLEUfor many European languages including English(Callison-Burch et al, 2011).
However, its use ofPOS tags and synonym dictionaries prevents its useat the character-level.
We use TESLA as a represen-tative of a competitive word-level metric.We use the Stanford Chinese word segmenter(Tseng et al, 2005) and POS tagger (Toutanova etal., 2003) for preprocessing and Cilin for synonymdefinition during matching.
TESLA has several vari-ants, and the simplest and often the most robust,TESLA-M, is used in this work.
The various cor-relations are reported in the second row of Tables 2and 3.The scores show that word-level TESLA-M hasno clear advantage over character-level BLEU, de-spite its use of linguistic features.
We consider thisconclusion to be in line with that of Li et al (2011).4.4 TESLA-CELABIn all our experiments here we use TESLA-CELABwith n-grams for n up to four, since the vast majorityof Chinese words, and therefore synonyms, are atmost four characters long.The correlations between the TESLA-CELABscores and human judgments are shown in the lastrow of Tables 2 and 3.
We conducted significancetesting using the resampling method of (Koehn,2004).
Entries that outperform the BLEU base-line at 5% significance level are marked with ?
*?,and those that outperform at the 1% significancelevel are marked with ?**?.
The results indicate thatTESLA-CELAB significantly outperforms BLEU.For comparison, we also run TESLA-CELABwithout the use of the Cilin dictionary, reportedin the third row of Tables 2 and 3 and de-noted as TESLA-CELAB?.
This disables TESLA-926CELAB?s ability to detect word-level synonyms andturns TESLA-CELAB into a linear programmingbased character-level metric.
The performance ofTESLA-CELAB?
is comparable to the character-level BLEU baseline.Note that?
TESLA-M can process word-level synonyms,but does not award character-level matches.?
TESLA-CELAB?
and character-level BLEUaward character-level matches, but do not con-sider word-level synonyms.?
TESLA-CELAB can process word-level syn-onyms and can award character-level matches.Therefore, the difference between TESLA-Mand TESLA-CELAB highlights the contributionof character-level matching, and the differencebetween TESLA-CELAB?
and TESLA-CELABhighlights the contribution of word-level synonyms.4.5 Sample SentencesSome sample sentences taken from the IWSLT testset are shown in Table 4 (some are simplified fromthe original).
The Cilin dictionary correctly identi-fied the following as synonyms:?
= ??
week??
= ??
daughter?
= ?
you??
= ??
workThe dictionary fails to recognize the followingsynonyms:??
= ?
a??
= ??
hereHowever, partial awards are still given for thematching characters?
and?.Based on these synonyms, TESLA-CELAB isable to award less trivial n-gram matches, such as??=??
?, ???=??
?, and ???=??
?,as these pairs can all be segmented into aligned syn-onyms.
The covered n-gram matching rule is thenable to award tricky n-grams such as??,??,?
?, ??
and ?
?, which are covered by ???,???,???,???
and???
respectively.Note also that the word segmentations shown inthese examples are for clarity only.
The TESLA-CELAB algorithm does not need pre-segmentedReference: ?
?
?next week .Candidate: ?
??
?next week .Reference: ?
?
??
??
?I have a daughter .Candidate: ?
?
?
??
?I have a daughter .Reference: ?
?
??
??
?
?you at here work qn ?Candidate: ?
?
??
??
?
?you at here work qn ?Table 4: Sample sentences from the IWSLT 2008 test setSchirm kaufenumbrella buyRegenschirm kaufenumbrella buyRegen schirm kaufenrain umbrella buyFigure 5: Three forms of buy umbrella in Germansentences, and essentially finds multi-character syn-onyms opportunistically.5 Discussion and Future Work5.1 Other Languages with Ambiguous WordBoundariesAlthough our experiments here are limited to Chi-nese, many other languages have similarly ambigu-ous word boundaries.
For example, in German, theexact counterpart to our example exists, as depictedin Figure 5.Regenschirm, literally rain-umbrella, is a syn-onym of Schirm.
The first two forms in Figure 5appear in natural text, and in standard BLEU, theywould be penalized for the non-matching wordsSchirm and Regenschirm.
Since compound nounssuch as Regenschirm are very common in Germanand generate many out-of-vocabulary words, a com-mon preprocessing step in German translation (andtranslation evaluation to a lesser extent) is to splitcompound words, and we end up with the last formRegen schirm kaufen.
This process is analogous to927Chinese word segmentation.We plan to conduct experiments on German andother Asian languages with the same linguistic phe-nomenon in future work.5.2 Fractional Similarity MeasuresIn the current formulation of TESLA-CELAB, twon-grams X and Y are either synonyms which com-pletely match each other, or are completely unre-lated.
In contrast, the linear-programming basedTESLA metric allows fractional similarity measuresbetween 0 (completely unrelated) and 1 (exact syn-onyms).
We can then award partial scores for relatedwords, such as those identified as such by WordNetor those with the same POS tags.Supporting fractional similarity measures is non-trivial in the TESLA-CELAB framework.
We planto address this in future work.5.3 Fractional Weights for N-gramsThe TESLA-M metric allows each n-gram to havea weight, which is primarily used to discount func-tion words.
TESLA-CELAB can support fractionalweights for n-grams as well by the following exten-sion.
We introduce a function m(X) that assigns aweight in [0, 1] for each n-gram X.
Accordingly, ourobjective function is replaced by:1Z(?Xm(X)cref(X) + f?Ym(Y )ccand(Y ))where Z is a normalizing constant so that the metrichas a range of [0, 1].Z =?Xm(X) + f?Ym(Y )However, experiments with different weight func-tions m(?)
on the test data set failed to find a betterweight function than the currently implied m(?)
=1.
This is probably due to the linguistic character-istics of Chinese, where human judges apparentlygive equal importance to function words and con-tent words.
In contrast, TESLA-M found discount-ing function words very effective for English andother European languages such as German.
We planto investigate this in the context of non-Chinese lan-guages.6 ConclusionIn this work, we devise a new MT evaluation met-ric in the family of TESLA (Translation Evaluationof Sentences with Linear-programming-based Anal-ysis), called TESLA-CELAB (Character-level Eval-uation for Languages with Ambiguous word Bound-aries), to address the problem of fuzzy word bound-aries in the Chinese language, although neither thephenomenon nor the method is unique to Chinese.Our metric combines the advantages of character-level and word-level metrics:1.
TESLA-CELAB is able to award scores forpartial word-level matches.2.
TESLA-CELAB does not have a segmentationstep, hence it will not introduce word segmen-tation errors.3.
TESLA-CELAB is able to take full advantageof the synonym dictionary, even when the syn-onyms differ in the number of characters.We show empirically that TESLA-CELABsignificantly outperforms the strong baselineof character-level BLEU in two well knownEnglish-Chinese MT evaluation data sets.
Thesource code of TESLA-CELAB is available fromhttp://nlp.comp.nus.edu.sg/software/.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation.928Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on statisti-cal machine translation and metrics for machine trans-lation.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion.Yee Seng Chan and Hwee Tou Ng.
2008.
MAXSIM:A maximum similarity metric for machine translationevaluation.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies.Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011.TESLA at WMT2011: Translation evaluation and tun-able metric.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing.Maoxi Li, Chengqing Zong, and Hwee Tou Ng.
2011.Automatic evaluation of Chinese translation output:word-level or character-level?
In Proceedings of the49th Annual Meeting of the Association for Computa-tional Linguistics: Short Papers.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
TESLA: Translation evaluation of sentenceswith linear-programming-based analysis.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011.Better evaluation metrics lead to better machine trans-lation.
In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.
Amaximum entropy approach to Chinese word segmen-tation.
In Proceedings of the Fourth SIGHAN Work-shop on Chinese Language Processing.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics.Michael Paul.
2008.
Overview of the iwslt 2008 eval-uation campaign.
In Proceedings of the InternationalWorkshop on Spoken Language Translation.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the Association for Machine Trans-lation in the Americas.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter for SIGHANbakeoff 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing.Hongmei Zhao and Qun Liu.
2010.
The CIPS-SIGHANCLP 2010 Chinese word segmentation bakeoff.
InProceedings of the Joint Conference on Chinese Lan-guage Processing.929
