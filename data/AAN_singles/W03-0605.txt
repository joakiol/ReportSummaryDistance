An Architecture for Word Learning using Bidirectional MultimodalStructural AlignmentKeith Bonawitzbonawitz@mit.eduAnthony Kimtkim42@mit.eduMIT Artificial Intelligence Lab545 Technology SquareCambridge, MA 02139Seth Tardiffstardiff@mit.eduAbstractLearning of new words is assisted by contex-tual information.
This context can come inseveral forms, including observations in non-linguistic semantic domains, as well as the lin-guistic context in which the new word was pre-sented.
We outline a general architecture forword learning, in which structural alignmentcoordinates this contextual information in or-der to restrict the possible interpretations of un-known words.
We identify spatial relations asan applicable semantic domain, and describe asystem-in-progress for implementing the gen-eral architecture using video sequences as ournon-linguistic input.
For example, when thecomplete system is presented with ?The birddove to the rock,?
with a video sequence of abird flying from a tree to a rock, and with themeanings for all the words except the preposi-tion ?to,?
the system will register the unknown?to?
with the corresponding aspect of the bird?strajectory.1 IntroductionMultimodal word learning can be viewed as a problem inwhich inputs are presented concurrently in both the lin-guistic domain and at least one non-linguistic semanticdomain.
It is the responsibility of the word learner to (1)infer the correspondence of each word to some fragmentof the semantic domain, and (2) refine the model of theword based on this correspondence (by generalizing theword semantics, for example).In this paper1, we propose a system aimed at the firsthalf of the problem: inferring a correspondence between1This research is supported, in part, by the National ScienceFoundation, Award Number IIS-0218861.words and non-linguistic semantic domain fragments.
Inparticular, we are interested in using the context of theword?s introduction to limit possible interpretations.
As-suming linguistic inputs are syntactic multiword utter-ances (e.g.
phrases and sentences), this context includesthe semantic and syntactic relationship of the new word toother words in the linguistic input.
In a multimodal learn-ing environment, the context also includes input observedin the non-linguistic domain.
Our system is designed tocoordinate all of these contextual clues in order to restrictthe set of possible interpretations of the new word.
Byleveraging previously learned words to enable the learn-ing of new words, we create a bootstrapping system forword learning.In Section 2, we outline a general architecture basedon symbolic structural alignment (Gentner and Markman,1997) for solving the stated problem.
In this outline,we identify necessary subsystems and requirements thesystem must satisfy.
In Section 3, we identify the vi-sual domain of spatial relations as a potential semanticdomain, and in Sections 4?6 we describe a system-in-progress which instantiates the general architecture forthis semantic domain.2 General ArchitectureWe propose an architecture to answer the following ques-tion: assuming that a new word is embedded in a phrasewith other previously acquired words, how can we exploitthis linguistic context to focus on the fragments of non-linguistic input most likely to correspond with the newword?The semantic principle of compositionality states thatthe meaning of any expression (such as a phrase)is a function of the meaning of its sub-expressions,where the particular function is determined by themethod of composition.
For example, the expression?The quick fox jumped over the log?
can be consid-ered a composition of the sub-expressions ?The quickX/A A/ \ / \2 C B 1/ \ | / \E F D E FFigure 1: Structural alignment between these two struc-tures infers the correspondences C?1 and 2?B(D).Structural alignment between semantic representationswill bring unknown words into correspondence with theirprobable semantics.fox?
and ?over the log?, where syntactic composi-tion with ?jumped?
is the method of composition.
Inother words, the semantics of this sentence can be ex-pressed: jumped(SEMANTICS(?The quick fox?
), SE-MANTICS(?over the log?)).
Recursive application of thisprinciple reveals that the semantic value of an expressionis a structured representation.Intuitively, then, we can approach our bootstrappingproblem by structural alignment (Gentner and Markman,1997).
Structural alignment is a process in which corre-sponding elements in two structured representations areidentified by matching.
Correspondence between non-matching elements is then implied by the structural con-straints of the representations.For example, in Figure 1, structural alignment firstmatches A, E, and F between the two representations.Then, based on structural constraints, 1 is inferred to cor-respond with C, and 2 with B(D).
In our architecture,structural alignment of the semantics of known words(and linguistic constituents formed thereof) with seman-tic structures observed in the non-linguistic domain willcause an alignment of unknown words with probable cor-responding semantic fragments, thereby achieving ourword learning goal of exploiting linguistic context to fo-cus on fragments of the semantic input.The remainder of this section describes the representa-tions and methods required by a system seeking to imple-ment this general architecture.2.1 Semantic RepresentationIn order to perform structural alignment, the representa-tion for the semantic domain must have several key prop-erties:?
The domain must be a structural representation andit must be symbolic, in order to allow alignment ofsymbols.?
For inferences made from structural alignment to bevalid, the representation must obey the principle ofcompositionality.?
The representation should contain orthogonal ele-ments (i.e.
the same piece of semantics is notencoded into multiple symbols) so that there arecanonical ways of expressing particular meanings.?
Finally, the semantic representation must be lexical-ized, implying that the semantics of any linguisticphrase can be cleanly divided amongst the phrase?sconstituent words.
Each word should get a singleconnected semantic structure that does not share se-mantic symbols with any other word.2.2 Semantic ProcessingIt is likely that the actual non-linguistic input modalitywill not be an appropriate structured symbolic represen-tation.
For example, the visual, aural, and kinestheticmodalities are non-symbolic.
In any system dealing withsuch an input modality, it will be necessary to have mod-ules that extract structured symbolic representations fromthe unstructured input.2.3 Linguistic ProcessingOne challenge in performing structural alignment againstlanguage input is that the structured semantic representa-tion of the linguistic input is implicit rather than explicit.Therefore, we need methods for parsing and an appropri-ate grammar.
The grammar and parsing algorithms wechoose must support several non-standard features.First, we expect to encounter word meanings which areunknown, so our selected techniques must support gaps inthe parse.
We also require a reversible grammar, so that,when presented with the meaning of an entire expressionand the meaning of some of its subexpressions, we caninfer the meaning of the remaining subexpressions.2Although it may not be required, parsing techniquesthat use partial structural alignment are preferred.
Wordsand phrases have many possible interpretations and thisproblem is exacerbated by unknown words in the linguis-tic input.
Since targets for the parse are available in thesemantic input domain, use of these targets to guide thesearch through the space of possible linguistic interpreta-tions is advantageous.
Increasing structural alignment be-tween the parsed semantics and the input semantics couldbe such a guiding heuristic.
As a side effect of usingstructural alignment as a parsing heuristic, we should ex-pect the parser to manipulate partial semantic and syntac-tic structures throughout the parsing process, as opposedto generating semantics from a completed syntactic parsetree after parsing is completed.2Mathematically, this is equivalent to saying that there willbe cases where we know a, f , and x in the equality a = f(x, y),and we want to be able to infer the value of y.
In order to doso, we must be able to compute the functional inverse of f withrespect to y.
That is, we want the function f?1ysuch that y =f?1y(x, a).Parsing andStructural Alignment Structures from VisionExtract Semantic?????????????????????????
?|THING Bird????????????????????????????????????????
?|THING PolePLACE AtPATH?ELEMENT From????
?|THING DeborahPLACE AtPATH?ELEMENT ToPATHEVENT Go?4?5 ?6?The bird flew from thepole to Deborah?Figure 2: General system structure of our proposed im-plementation.
Our learning-enabling structure is basedon a semantic representation (Section 4) which is ob-tained by translating video inputs (Section 6).
We thenuse a bidirectional search process to parse the linguisticinput and to structurally align linguistic semantics withthe non-linguistic semantics.
(Section 5).2.4 Structural AlignmentGentner and Markman (1997) describe the requisite com-ponents of a structural alignment system as (1) methodsfor matching structural atoms, (2) methods for identify-ing sets of compatible atom matches (for example, rulingout cases in which two atoms in one structure map to thesame atom in another structure), and (3) methods usingatom matches to guide the matching of large portions ofstructure.3 Implementation with Visual Domain ofSpatial RelationsIn order to validate our general architecture, we outline asystem-in-progress which instantiates the architecture forthe particular semantic domain of spatial relations.
Thedomain of spatial relations captures the relative position-ing, orientation, and movement of objects in space.
Ex-amples of sentences capturing spatial semantics include?The boy threw the ball into the box on the table?, ?Thepath went from the tree to the lakeside?, and ?The signpoints to the door?.The following sections describe the methods and rep-resentations we have chosen to satisfy the requirementsoutlined in Section 2.
Figure 2 shows how the system isdesigned and how the rest of this paper is organized.?????????????????????
?|THING Bird?????????????????????????????????
?|THING PolePLACE AtPATH?ELEMENT From???
?|THING DeborahPLACE AtPATH?ELEMENT ToPATHEVENT GoFigure 3: Lexical-Conceptual Semantics is our semanticdomain representation.
This structure is the LCS modelof ?The bird flew from the pole to Deborah.
?4 Lexical-Conceptual SemanticsWe use Lexical-Conceptual Semantics (LCS) (Jackend-off, 1983) as our semantic representation.
LCS is a cog-nitive representation that focuses on trajectories and spa-tial relations.
Unlike other representations such as Log-ical Form (LF) and Conceptual Dependency (CD), LCSdelineates notions of PATHs and PLACEs.
LCS is moreformally outlined in (Jackendoff, 1983) and is comparedto other semantic representations in (Bender, 2001).The following productions yield a simplified portion ofthe LCS space.
For a complete description, refer to (Jack-endoff, 1983).?
[THING]?
[PLACE] ??
PLACE-FUNC([THING])where PLACE-FUNC ?
{AT, ABOVE, BELOW,ON, IN, etc.}?
[PATH] ??
PATH(PATH-ELEMENT, PATH-ELEMENT, ?
?
?)?
[PATH-ELEMENT] ??
PATH-FUNC([PLACE])where PATH-FUNC ?
{TO, FROM, TOWARD,AWAY-FROM, VIA, etc.}?
[EVENT] ??
GO([THING],[PATH])?
[STATE] ??
BE([THING],[PLACE])?
[CAUSE] ??
CAUSE([THING],[EVENT])?
[CAUSE] ??
CAUSE([EVENT],[EVENT])Using LCS, the trajectory expressed in the sentence?The bird flew from the pole to Deborah,?
is representedas in Figure 3Lexical-Conceptual Semantics focuses on spatial rela-tions in the physical world.
However, it is easily exten-sible to other domains, such as the temporal and posses-sive domains (Jackendoff, 1983; Dorr, 1992).
Researchfocusing on using LCS in the abstract domain of socialpolitics is also ongoing in our lab.
Furthermore, it seemsthat much of language is spatial in nature.
For example,there is significant psychological evidence that humansuse spatial relations to talk about abstract domains suchas time (Boroditsky, 2000).
As a consequence, we believethat techniques for learning Lexical-Conceptual Seman-tics for words, developed here using the concrete spatialrelations domain, will be extendible to many other do-mains.5 Language Parsing and StructuralAlignmentThis section will describe the methods used to simultane-ously parse linguistic input strings and align the resultantsemantic structures with those from vision.
The primaryarchitecture of the system is a constraint propagation net-work using grammatical rules as constraints.
A customconstraint network topology is generated for each linguis-tic input string using a bidirectional search algorithm.5.1 Parsing/Alignment as Constraint PropagationThe parsing and structural alignment system may beviewed as a single large constraint, as in Figure 4.
Thisconstraint has two inputs: on one side, it takes a setof semantic representations originating from the visionprocessor.
On the other side, it takes a linguistic inputstring, together with possible meanings for each wordin the string, as determined by a lexicon (treating un-known words as having any possible meaning).
As out-put, the constraint eliminates, in each input set, all mean-ings which do not lead to a successful structurally alignedparse.In order to achieve such a complicated constraint, itis useful to decompose the constraint into a network ofsimpler constraints, each working over a local domain ofonly a few constituents rather than over the domain of anentire sentence, as in Figure 5.
We can then base thesesubconstraints on grammatical rules over a fixed numberof constituents, and trust the composed network to handlethe complete sentence.5.2 Grammatical Framework for ConstraintsThe grammar framework chosen for our system is Com-binatorial Categorial Grammar (CCG) (Steedman, 2000).CCG has many advantages in a system like ours.
First,there are only a handful of rules for combining con-stituents, and these rules are explicit and well defined.These qualities facilitate the implementation of con-straints.
In addition, CCG is adept at parsing aroundmissing information, because it was designed to handleParsing & Alignment System?The?
{NP/N : ?x.x}?dove???????????
?N : |THING Dove?BirdS\NP : ?x.????????x???
PATHGO Dive????????????flies???????????
?N : |THING Fly?InsectS\NP : ?x.????????x???
PATHGO fly?????????????????????
?S :?????????
?|THING Dove?Bird???
PATHGO??????????
?Figure 4: The parsing and structural alignment systemfunctions as a constraint on linguistic and visual inter-pretations, requiring that expressions follow grammaticalrules and that they align with the semantic domain input.This example shows the system presented with the sen-tence ?The dove flies,?
and with a corresponding concep-tual structure (from vision).
In this situation, all the wordswere known, so the system will simply eliminate the in-terpretations of ?dove?
as a GO and flies as THINGs.
Ifthe word ?dove?
had not been known, the system wouldstill select the verb form of flies (by alignment), whichbrings ?dove?
into alignment with the appropriate frag-ment of semantic structure (the THING frame).linguistic phenomena such as parasitic-gapping3.
Theability to gracefully handle incomplete phrases is crucialin our system, because it enables us to parse around un-known words.In CCG, syntactic categories can either be atomic el-ements, or functors of those elements.
The atomic ele-ments are usually {S, N, NP} corresponding to Sentence,Noun, and Noun Phrase.
Syntactic category functors areexpressed in argument-rightmost curried notation, using/ or \ to indicate whether the argument is expected to theleft or right, respectively.
Thus NP/N indicates a NP re-quiring a N to the right (and is therefore the syntacticcategory of a determiner), while (S\NP)/NP indicates an3An example of a sentence with parasitic gapping is ?Johnhates and Mary loves the movie,?
where both verbs share thesame object.
CCG handles this by treating ?John hates?
and?Mary loves?
as constituents, which can then be conjoined by?and?
into a single ?John hates and Mary loves?
constituent(traditional grammars are unable to recognize ?John hates?
asa constituent.
)Lexical Conceptual Semantics (from Vision)ConstraintGrammar RuleConstraintGrammar RuleSystemAlignmentParsing &?flies??The?
?dove?
?The dove flies?
?The dove?Figure 5: The parsing and alignment constraint in Fig-ure 4 is actually implemented as a network of simplerconstraints, as shown here.
Each constraint implements agrammatical rule, as shown in Figure 6.
The topology ofthe constraint network is dependent on the particular lin-guistic string, and is constructed by bidirectional search,as described in Section 5.3.S requiring one NP to the left and one to the right (thisis the category of a monotransitive verb).
For semantics,the notation is extended to X:f , indicating semantic cat-egory X with lambda calculus semantics f .
These ele-ments combine using a few simple productions, such asthe following functional application rules4:X/Y:f Y:a ?
X:fa (> forward application)Y:a X\Y:f ?
X:fa (< backward application)5.3 Constructing the Constraint NetworkThe constraints we use are parse rules; therefore our con-straint network topology embodies a parse tree for anysentence it can handle.
Since our inputs do not includethe parse tree, we must consider how to generate an ap-propriate constraint network topology.One option is to use the same network topology tohandle all sentences of the same length.
Such a net-work would have to contain every possible parse tree,and thus would essentially result in an exhaustive searchof the parse space.
A better solution would be to avoidthe exhaustive search by constructing a custom constrainttopology for each sentence, using standard heuristic parsetechniques.
The drawback to this approach is that weare not interested in finding just any potential parse ofa phrase/sentence, nor even the most statistically proba-ble parse.
Since our intent is to perform structural align-ment with input from the non-linguistic domain, our goalin parsing is to find the semantic parse structure which4For a full description and analysis of CCG, see (Steedman,2000)CCG ConstraintBackward Application (<)Y:a X\Y:f ?
Y:fa?The boy?NP : |THING Boy?The boy went to the lake?S :????????????????
?|THING Boy????????????????????
?|THING LakePLACE AtPATH?ELEMENT ToPATHEVENT Go?went to the lake?S\NP : ?x.?????????????????x????????????????????
?|THING LakePLACE AtPATH?ELEMENT ToPATHEVENT GoFigure 6: This figure shows one of the CCG Rules, For-ward Functional Application, being treated as a constraintin a constraint propagation network.
Any one of the threeinputs can be left unspecified, and the constraint can com-pletely determine the value based on the other two inputs.aligns best with the semantic structure input from thenon-linguistic domain.
It follows that we should use thenon-linguistic input to guide our search.Our system applies bidirectional search to theparse/alignment problem.
In contrast to traditional searchtechniques, bidirectional search treats both the ?source?and ?goal?
symmetrically; the search-space is traversedboth forward from the source and backward from thegoal.
The search processes operating in each directioninteract with each other whenever their paths reach thesame state in the search-space.
This interaction provideshints for quickly completing the remainder of the search.For example, if the forward and backward paths reachthe same search-state, then the forward searcher quicklyreaches the goal by tracing the backward-path.The specific style of bidirectional search we are inves-tigating is based on Streams and Counterstreams (Ull-man, 1996), in which forward and backward search pathsinteract with each other by means of primed pathways.For each transition, two priming values are maintained:a forward priming and backward priming.
Primings areused when a decision must be made between several pos-sible transitions that could extend a search path; thosetransitions that have a higher priming (using the for-ward priming for forward searches, backward priming forbackward searchers) are preferred for expansion.
Transi-tion primings in a particular direction (either forward orbackward) are increased whenever a search path traversesthe transition in the opposite direction.
The net influenceof the primings is that transitions previously traversed inone direction are more likely to be explored in the op-posite direction, if the opportunity arises.
By extension,primings provide clues for finding a path from any stateto the target state.The Streams and Counterstreams approach to bidirec-tional search facilitates incorporation of other types ofcontext.
For example, some situational context can becaptured by allowing primings from previous parses ofrecent sentences to influence the current parse.
Also, sta-tistical cues such as Lexical Attraction (Yuret, 1999) canbe integrated into the system by using heuristics to biasprimings.5.4 Structural AlignmentThe three components of structural alignment specifiedin Section 2.4 (atomic alignment, identification of com-patible match sets, and structurally implied matches) arewoven into the bidirectional search construction of theconstraint network topology.
When a constraint net-work fragment is constructed which bridges between asmall portion of the linguistic input and the non-linguisticsemantics, this ?atomic alignment?
primes the bidirec-tional search to be more likely to repeat this match whileconstructing larger constraint network fragments; henceatomic alignment leads to larger structural alignment.The constraints in the constraint network ensure that allactive atomic alignments are compatible.
Finally, whenthe constraint network bridges large portions of the lin-guistic and non-linguistic inputs, the non-linguistic se-mantic structure gets partitioned across the words in thelinguistic input by the grammatical constraints.
Thiscompletes the structural alignment by bringing unknownwords into correspondence with their probable seman-tics.5.5 Handling UncertaintyThroughout this discussion, we have considered words asbeing either completely learned or completely unlearned.Clearly, though, there is much middle ground, includingwords whose meanings are still ambiguous among sev-eral options, as well as words for which some meaningshave been well acquired, while other valid meanings haveyet to be learned.
How can our system handle this degree-of-acquisition continuum?Let us consider what we can expect from the meaning-refinement module.
First, it should be able to report a setof witnessed possible meanings for each word, togetherwith a correctness strength for each interpretation.
Thiswould be based on how regularly that interpretation hasbeen witnessed.
Furthermore, the module should be ableto report the likelihood that the word still has unwitnessedinterpretations; for initial occurrences of a word, this like-lihood would be quite high, but with more exposure to theword, this likelihood would fall off.5Returning to our system, we can now treat each wordas having a set of known meanings together with a wild-card unknown meaning.
When using bidirectional searchto construct the constraint network topology, we bias theprimings of transitions which reduce a word?s potentialmeaning set, using the likelihood estimates given by themeaning-refinement module.6 Lexical-Conceptual Structures fromVideoThe proposed system includes a vision component that isresponsible for converting pixel data from a video inputinto the semantic structure described in Section 4.
Thisvision system is an implementation of the ideas presentedby John Bender (2001).
Following Bender?s prescrip-tions, the vision system does not perform object recog-nition.
Instead, the goal of the system is to analyze thedifferent paths and places that are present in a scene and,by relating these paths and places to one another, to con-struct an LCS representation of the actions.6.1 Data FlowThe vision system consists of two parts.
First, videoframes are analyzed in sequence and the objects presentin each scene are tracked using traditional vision algo-rithms and techniques6.
For each object, informationabout the object?s size, shape, and position over the lifeof the scene is stored in a data structure that we call aBlob.
This name was chosen to highlight the fact that thevision system makes no attempt at object recognition orfine-grained analysis and is instead concerned only withpaths along which the objects (blobs) move.Second, the data regarding each object?s progressionthrough the scene is interpreted by an implementation ofBender?s algorithm DESCRIBE v.2 to produce the seman-tic representation that is used by the other components ofthe system.6.2 Pixels to BlobsThe low-level portion of the vision system is fed se-quences of pixel matrices by an external system that cap-tures video data.
In the current implementation, this pixel5These likelihood estimations could be generated, amongother ways, by a meaning-refinement module incorporating aBayesian model.6For details concerning image labeling and object extractionalgorithms see (Horn, 1986)Figure 7: Start and end states of an example scene pro-duced by the simulator.
The left image represents the startstate and the right image represents the final state.data is sent from a simulator in which the actions of sim-ple objects take place.
The pixel matrices include integerdefinitions of each pixel?s value, supplying all color in-formation.When the analysis of a particular scene begins, the vi-sion system captures a snapshot of the background thatit uses as a reference for all subsequent frames related tothe same scene.
As new video frames are input, the storedbackground is subtracted and the new video frames areconverted to binary images.
A noise removal algorithmis applied to the binary images to remove any residualelements of the original background.Once converted to a binary representation, each videoframe is labeled using an object labeling algorithm andeach distinct object is identified.
Each object presentwithin a frame is overlaid with a shape that will be usedin the Blob representation passed along to the next com-ponent of the system.
Each of the overlaid shapes is(possibly) matched to a shape observed in a previousframe.
This matching procedure attempts to identify ob-jects persisting between frames based on proximity insize, shape, color, and position using a 4-dimensionalnearest-neighbors approach.
If a shape matches with apreviously known entry, the Blob structure correspond-ing to that particular object is assigned a new shape forits progression.
If no match is found, a new Blob struc-ture is created for the newly-observed object.Once the analysis of all frames of a scene is complete,the list of Blobs is fed to the next portion of the visionsystem for further interpretation.Figures 7 and 8 show an example of this portion ofthe vision system in use.
Figure 7 shows the raw imagesrepresenting the start and end states of the scene.
Figure 8shows a visualization of the object data created by thelow-level portion of the system.
The trace represents thepath along which the object moved during the scene.Figure 8: Trace of objects during the example scene.The moving object?s position changes are tracked and thetrace of its path is generated.?????????????
?|THING blob1?????????????????
?|THING blob0PLACE AbovePATH?ELEMENT ToPATHEVENT GoFigure 9: LCS frame produced by the vision systembased on the example scene presented in Figure 7.
Notethat no object recognition is in use, so the objects aregiven temporary names (blob0 and blob1).6.3 Blobs to LCSThe generation of semantic structures from vision dataconcludes with an analysis of the Blobs generated by thelow-level vision system.
This analysis is performed byimplementing an algorithm described, but never imple-mented in a system, by Bender (2001).The algorithm first examines the list of objects presentin the scene and computes the simple exists?
and mov-ing?
predicates.
If an object is found and moving, an LCSGO frame is instantiated and the object is compared to allothers present so the appropriate path and place functionscan be calculated.
The calculation of path and place func-tions is based on a set of routines suggested by Bender.These routines compute the direction, intersection, andplace-descriptions (above, on, left-of, etc.)
for each pairof objects.
Finally, the path and place functions describedin Section 4 are found by examining the output of the vi-sual routines and are added to the LCS frame.Figure 9 shows the LCS frame constructed by the sys-tem based on the example shown in Figure 7.
The framecan now be used by the remainder of our system in thestructural alignment phase.7 Related WorkThis work has parallels to MAIMRA, a system forword learning from non-linguistic input (Siskind, 1990).MAIMRA?s semantic structure is also Jackendoff LCS,and its architecture consists of three modules: a parser(which produces syntactic parse trees from linguistic in-put strings), an inference component (which produces se-mantic structures from non-linguistic input), and a linker(which establishes correspondence between the syntacticand semantic structures).
Observing that the parser, infer-ence, and linker components respectively fill the linguis-tic processing, semantic processing, and structural align-ment requirements outlined in Section 2, MAIMRA canbe viewed as an instance of the general architecture wehave described.However, our system is also significantly differ-ent from MAIMRA in two important respects.
First,MAIMRA is designed with the model-refinement aspectof word learning intertwined with the correspondence-inference aspect.
In contrast, our architecture seeks tosystematically isolate these two problems, so that prob-lems of model refinement and correspondence establish-ment may be pursued independently.
Second, MAIMRA?sdesign results in exhaustive searches of many spaces (forexample, the parser must generate all possible parses).Instead, our system seeks to use what we know as soonas possible, for example by using bidirectional searchto guide the parse process.
This implementation detailbecomes important in practical applications because ex-haustive searches of all possible parses severely limits thecomplexity of sentences that can be parsed.The current work is part of larger initiative, the BridgeProject.
Based on the work of the Genesis Group atMIT?s Artificial Intelligence Lab, this project seeks tobuild cognitively complete systems?systems in whichlanguage, vision, motor, and other AI domains work co-operatively to achieve results which would have other-wise been unattainable.8 ContributionsEffectively learning the meanings of words from non-linguistic input requires the development of representa-tions and algorithms to determine correspondences be-tween the linguistic and non-linguistic domains.
Throughthis research, our contributions to this goal include:?
We propose a general architecture, based on struc-tural alignment, for employing linguistic and non-linguistic context in word learning.
The systembootstraps itself by using acquired words to learnnew words.
We define the necessary properties ofsemantic representations used in such a system.
Wealso define the modules this system will require.?
We outline a system which implements this architec-ture for the specific semantic domain of vision.
Weidentify LCS structures as an appropriate semanticrepresentation, and we demonstrate techniques forextracting LCS from video.
We also show a bidirec-tional approach to the parsing and alignment prob-lem.We currently have the components described in our im-plementation functional in isolation.
The true merit ofthe system will be determined as we bring together allthe pieces; thus our final contribution is the actual imple-mentation of the systems described herein.
It is our hopethat our research will act as a springboard for the devel-opment of model refinement algorithms which have theadvantage of support from semantic alignment systemssuch as ours.ReferencesJohn R. Bender.
2001.
Connecting language and visionusing a conceptual semantics.
Master?s thesis, Mas-sachusetts Institute of Technology.Lera Boroditsky.
2000.
Metamorphic structuring: under-standing time through spatial metaphors.
Cognition,75(1):1?28.Bonnie Dorr.
1992.
The use of lexical semantics in in-terlingual machine translation.
Machine Translation,7(3):135?193.Dedre Gentner and Arthur B. Markman.
1997.
Structuremapping in analogy and similarity.
American Psychol-ogist, 52(1):45?56.Berthold Klaus Paul Horn.
1986.
Robot Vision.McGraw-Hill, New York, New York.Ray Jackendoff.
1983.
Semantics and Cognition, vol-ume 8 of Current Studies in Linguistics Series.
MITPress, Cambridge, Massachusetts.Jeffrey Mark Siskind.
1990.
Acquiring core meaningsof words, representated as jackendoff-style concep-tual structures, from correlated streams of linguisticand non-linguistic input.
In Proceedings of the 28thAnnual Meeting of the Association for ComputationalLinguistics (ACL-1990).Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, Massachusetts.Shimon Ullman, 1996.
High Level Vision, chapter 10,pages 317?358.
Sequence Seeking and CounterStreams: A Model for Information Flow in the VisualCortex.
MIT Press, Cambridge, Massachusetts.Deniz Yuret.
1999.
Lexical attraction models of lan-guage.
Submitted to The Sixteenth National Confer-ence on Artificial Intelligence.
