A Syllable Based Word Recognition Modelfor Korean Noun ExtractionDo-Gil Lee and Hae-Chang RimDept.
of Computer Science & EngineeringKorea University1, 5-ka, Anam-dong, Seongbuk-kuSeoul 136-701, Koreadglee, rim@nlp.korea.ac.krHeui-Seok LimDept.
of Information & CommunicationsChonan University115 AnSeo-dongCheonAn 330-704, Korealimhs@infocom.chonan.ac.krAbstractNoun extraction is very important formany NLP applications such as informa-tion retrieval, automatic text classification,and information extraction.
Most of theprevious Korean noun extraction systemsuse a morphological analyzer or a Part-of-Speech (POS) tagger.
Therefore, theyrequire much of the linguistic knowledgesuch as morpheme dictionaries and rules(e.g.
morphosyntactic rules and morpho-logical rules).This paper proposes a new noun extrac-tion method that uses the syllable basedword recognition model.
It finds themost probable syllable-tag sequence ofthe input sentence by using automaticallyacquired statistical information from thePOS tagged corpus and extracts nouns bydetecting word boundaries.
Furthermore,it does not require any labor for construct-ing and maintaining linguistic knowledge.We have performed various experimentswith a wide range of variables influenc-ing the performance.
The experimentalresults show that without morphologicalanalysis or POS tagging, the proposedmethod achieves comparable performancewith the previous methods.1 IntroductionNoun extraction is a process to find every noun ina document (Lee et al, 2001).
In Korean, Nounsare used as the most important terms (features) thatexpress the document in NLP applications such asinformation retrieval, document categorization, textsummarization, information extraction, and etc.Korean is a highly agglutinative language andnouns are included in Eojeols.
An Eojeol is a sur-face level form consisting of more than one com-bined morpheme.
Therefore, morphological anal-ysis or POS tagging is required to extract Koreannouns.The previous Korean noun extraction methods areclassified into two categories: morphological analy-sis based method (Kim and Seo, 1999; Lee et al,1999a; An, 1999) and POS tagging based method(Shim et al, 1999; Kwon et al, 1999).
The mor-phological analysis based method tries to generateall possible interpretations for a given Eojeol byimplementing a morphological analyzer or a sim-pler method using lexical dictionaries.
It may over-generate or extract inaccurate nouns due to lexicalambiguity and shows a low precision rate.
Althoughseveral studies have been proposed to reduce theover-generated results of the morphological analy-sis by using exclusive information (Lim et al, 1995;Lee et al, 2001), they cannot completely resolve theambiguity.The POS tagging based method chooses the mostprobable analysis among the results produced by themorphological analyzer.
Due to the resolution of theambiguities, it can obtain relatively accurate results.But it also suffers from errors not only produced by aPOS tagger but also triggered by the preceding mor-phological analyzer.Furthermore, both methods have serious deficien-???
(Cheol-Su-neun) ????
(sa-lam-deul-eul) ??(bwass-da)??
(Cheol-Su) ?
(neun) ???
(sa-lam-deul) ?
(eul) ??(bwass-da)??
(Cheol-Su) ??
(sa-lam) ?
(deul) ?
(eul) ?
(bo) ?
(ass) ?(da)eojeolwordmorphemepropernoun:personnamepostpositionnoun:personnounsuffix:pluralpostpositionverb:seeprefinalendingending?
(neun)Figure 1: Constitution of the sentence ?(Cheol-Su saw the persons)?cies in that they require considerable manual la-bor to construct and maintain linguistic knowledgeand suffer from the unknown word problem.
Ifa morphological analyzer fails to recognize an un-known noun in an unknown Eojeol, the POS taggerwould never extract the unknown noun.
Althoughthe morphological analyzer properly recognizes theunknown noun, it would not be extracted due to thesparse data problem.This paper proposes a new noun extractionmethod that uses a syllable based word recognitionmodel.
The proposed method does not require laborfor constructing and maintaining linguistic knowl-edge and it can also alleviate the unknown wordproblem or the sparse data problem.
It finds the mostprobable syllable-tag sequence of the input sentenceby using statistical information and extracts nounsby detecting the word boundaries.
The statistical in-formation is automatically acquired from a POS an-notated corpus and the word boundary can be de-tected by using an additional tag to represent theboundary of a word.This paper is organized as follows.
In Section 2,the notion of word is defined.
Section 3 presentsthe syllable based word recognition model.
Section4 describes the method of constructing the trainingdata from existing POS tagged corpora.
Section 5discusses experimental results.
Finally, Section 6concludes the paper.2 A new definition of wordKorean spacing unit is an Eojeol, which is delimitedby whitespace, as with word in English.
In Korean,an Eojeol is made up of one or more words, and aword is made up of one or more morphemes.
Figure1 represents the relationships among morphemes,words, and Eojeols with an example sentence.
Syl-lables are delimited by a hyphen in the figure.All of the previous noun extraction methods re-gard a morpheme as a processing unit.
In order toextract nouns, nouns in a given Eojeol should besegmented.
To do this, the morphological analysishas been used, but it requires complicated processesbecause of the surface forms caused by various mor-phological phenomena such as irregular conjugationof verbs, contraction, and elision.
Most of the mor-phological phenomena occur at the inside of a mor-pheme or the boundaries between morphemes, not aword.
We have also observed that a noun belongs toa morpheme as well as a word.
Thus, we do not haveto do morphological analysis in the noun extractionpoint of view.In Korean linguistics, a word is defined as a mor-pheme or a sequence of morphemes that can be usedindependently.
Even though a postposition is notused independently, it is regarded as a word becauseit is easily segmented from the preceding word.
Thisdefinition is rather vague for computational process-ing.
If we follow the definition of the word in lin-guistics, it would be difficult to analyze a word likethe morphological analysis.
For this reason, we de-fine a different notion of a word.According to our definition of a word, each un-inflected morpheme or a sequence of successiveinflected morphemes is regarded as an individualword.
1 By virtue of the new definition of a word,we need not consider mismatches between the sur-face level form and the lexical level one in recogniz-ing words.The example sentence ?  (Cheol-Su saw the persons)?
represented in Fig-ure 1 includes six words such as ?(Cheol-Su)?,?(neun)?, ?(sa-lam)?, ?(deul)?, ?(eul)?,and ?(bwass-da)?.
Unlike the Korean linguis-tics, a noun suffix such as ?(nim)?, ?(deul)?, or?(jeog)?
is also regarded as a word because it isan uninflected morpheme.3 Syllable based word recognition modelA Korean syllable consists of an obligatory onset(initial-grapheme, consonant), an obligatory peak(nuclear grapheme, vowel), and an optional coda(final-grapheme, consonant).
In theory, the numberof syllables that can be used in Korean is the same asthe number of every combination of the graphemes.2 Fortunately, only a fixed number of syllables isfrequently used in practice.
3 The amount of in-formation that a Korean syllable has is larger thanthat of an alphabet in English.
In addition, there areparticular characteristics in Korean syllables.
Thefact that words do not start with certain syllablesis one of such examples.
Several attempts havebeen made to use characteristics of Korean sylla-bles.
Kang (1995) used syllable information to re-duce the over-generated results in analyzing conju-gated forms of verbs.
Syllable statistics have beenalso used for automatic word spacing (Shim, 1996;Kang and Woo, 2001; Lee et al, 2002).The syllable based word recognition model is rep-resented as a function  like the following equations.It is to find the most probable syllable-tag sequence    , for a given sentence  consist-ing of a sequence of  syllables     .1Korean morphemes can be classified into two types: un-inflected morphemes having fixed word forms (such as noun,unconjugated adjective, postposition, adverb, interjection, etc.
)and inflected morphemes having conjugated word forms (suchas a morpheme with declined or conjugated endings, predicativepostposition, etc.
)2 (   ) of pure Korean syllables are pos-sible3Actually,   of syllables are used in the training data,including Korean characters and non-Korean characters (e.g.
al-phabets, digits, Chinese characters, symbols).   (1)     (2)Two Markov assumptions are applied in Equation2.
One is that the probability of a current syllable tagconditionally depends on only the previous sylla-ble tag.
The other is that the probability of a cur-rent syllable conditionally depends on the currenttag.
In order to reflect word spacing information inEquation 2, which is very useful in Korean POS tag-ging, Equation 2 is changed to Equation 3 which canconsider the word spacing information by calculat-ing the transition probabilities like the equation usedin Kim et al (1998).     	   (3)In the equation, 	 becomes zero if the transition oc-curs in the inside of an Eojeol; otherwise 	 is one.Word boundaries can be detected by an additionaltag.
This method has been used in some tasks suchas text chunking and named entity recognition torepresent a boundary of an element (e.g.
individualphrase or named entity).
There are several possi-ble representation schemes to do this.
The simplestone is the BIO representation scheme (Ramshaw andMarcus, 1995), where a ?B?
denotes the first item ofan element and an ?I?
any non-initial item, and asyllable with tag ?O?
is not a part of any element.Because every syllable corresponds to one syllabletag, ?O?
is not used in our task.
The representationschemes used in this paper are described in detail inSection 4.The probabilities in Equation 3 are estimated bythe maximum likelihood estimator (MLE) using rel-ative frequencies in the training data.
4The most probable sequence of syllable tags in asentence (a sequence of syllables) can be efficientlycomputed by using the Viterbi algorithm.4Since the MLE suffers from zero probability, to avoid zeroprobability, we just assign a very low value such as 		for an unseen event in the training data.Table 1: Examples of syllable tagging by BI, BIS, IE, and IES representation schemessurface level lexical level BI BIS IE IES(syllable) (morpheme/POS tag)(yak)(yak-sok)/nc B-nc B-nc I-nc I-nc(sok) I-nc I-nc E-nc E-nc(jang)(jang-so)/nc B-nc B-nc I-nc I-nc(so) I-nc I-nc E-nc E-nc(in) (i)/co+ (n)/etm B-co etm S-co etm E-co etm S-co etm(Sin)(Sin-la-ho-tel)/ncB-nc B-nc I-nc I-nc(la) I-nc I-nc I-nc I-nc(ho) I-nc I-nc I-nc I-nc(tel) I-nc I-nc E-nc E-nc(keo)(keo-pi-syob)/ncB-nc B-nc I-nc I-nc(pi) I-nc I-nc I-nc I-nc(syob) I-nc I-nc E-nc E-nc(e)(e)/jc B-jc S-jc E-jc S-jc(Jai)(Jai-Ok)/nc B-nc B-nc I-nc I-nc(Ok) I-nc I-nc E-nc E-nc(i) (i)/jc B-jc S-jc E-jc S-jc	(meon)	(meon-jeo)/mag B-mag B-mag I-mag I-mag(jeo) I-mag I-mag E-mag E-mag(wa) (o)/pv+(a)/ec B-pv ec S-pv ec E-pv ec S-pv ec(gi)(gi-da-li)/pv+(go)/ecB-pv ec B-pv ec I-pv ec I-pv ec(da) I-pv ec I-pv ec I-pv ec I-pv ec(li) I-pv ec I-pv ec I-pv ec I-pv ec(go) I-pv ec I-pv ec E-pv ec E-pv ec(iss)(iss)/px+(eoss)/ep+(da)/efB-px ef B-px ef I-px ef I-px ef(eoss) I-px ef I-px ef I-px ef I-px ef(da) I-px ef I-px ef E-px ef E-px ef.
./s B-s S-s E-s S-sGiven a sequence of syllables and syllable tags,it is straightforward to obtain the corresponding se-quence of words and word tags.
Among the wordsrecognized through this process, we can extractnouns by just selecting words tagged as nouns.
54 Constructing training dataOur model is a supervised learning approach, so itrequires a training data.
Because the existing KoreanPOS tagged corpora are annotated by a morphemelevel, we cannot use them as a training data withoutconverting the data suitable for the word recognitionmodel.
The corpus can be modified through the fol-lowing steps:Step 1 For a given Eojeol, segment word bound-aries and assign word tags to each word.Step 2 For each separated word, assign the word tagto each syllable in the word according to one ofthe representations.5For the purpose of noun extraction, we only select com-mon nouns here (tagged as ?nc?
or ?NC?)
among other kinds ofnouns.In step 1, word boundaries are identified by usingthe information of an uninflected morpheme and asequence of successive inflected morphemes.
Anuninflected morpheme becomes one word and itstag is assigned to the morpheme?s tag.
Successiveinflected morphemes form a word and the combinedform of the first and the last morpheme?s tag repre-sents its tag.
For example, the morpheme-unit POStagged form of the Eojeol ?	(gass-eoss-da)?is ?(ga)/pv+(ass)/ep+	(eoss)/ep+(da)/ef?,and all of them are inflected morphemes.
Hence,the Eojeol ?	(gass-eoss-da)?
becomes oneword and its tag is represented as ?pv ef?
by usingthe first morpheme?s tag (?pv?)
and the last one?s(?ef?
).In step 2, a syllable tag is assigned to each of syl-lables forming a word.
The syllable tag should ex-press not only POS tag but also the boundary of theword.
In order to detect the word boundaries, we usethe following four representation schemes:BI representation scheme Assign ?B?
tag to thefirst syllable of a word, and ?I?
tag to the others.BIS representation scheme Assign ?S?
tag to asyllable which forms a word, and other tags(?B?
and ?I?)
are the same as ?BI?
represen-tation scheme.IE representation scheme Assign ?E?
tag to thelast syllable of a word, and ?I?
tag to the others.IES representation scheme Assign ?S?
tag to asyllable which forms a word, and other tags(?I?
and ?E?)
are the same as ?IE?
represen-tation scheme.Table 1 shows an example of assigning word tagby syllable unit to the morpheme unit POS taggedcorpus.Table 2: Description of Tagset 2 and Tagset 3Tag Description Tagset 2 Tagset 3symbol s Sforeign word f Fcommon noun nc NCbound noun nb NBpronoun np NPnumeral nn NNverb pv Vadjective pa Aauxiliary predicate px VXcopula co COgeneral adverb mag MAconjunctive adverb majadnoun mm MMinterjection ii ICprefix xp XPNnoun-derivational suffix xsn XSNverb-derivational suffix xsv XSVadjective-derivational suffix xsmcase particle jcJauxilary particle jxconjunctive particle jjadnominal case particle jmprefinal ending ep EPfinal ending ef EFconjunctive ending ec ECnominalizing ending etn ETNadnominalizing ending etm ETM5 Experiments5.1 Experimental environmentWe used ETRI POS tagged corpus of 288,269Eojoels for testing and the 21st Century SejongProject?s POS tagged corpus (Sejong corpus, forshort) for training.
The Sejong corpus consists ofthree different corpora acquired from 1999 to 2001.The Sejong corpus of 1999 consists of 1.5 millionEojeols and other two corpora have 2 million Eo-jeols respectively.
The evaluation measures for thenoun extraction task are recall, precision, and F-measure.
They measure the performance by docu-ment and are averaged over all the test documents.This is because noun extractors are usually used inthe fields of applications such as information re-trieval (IR) and document categorization.
We alsoconsider the frequency of nouns; that is, if the nounfrequency is not considered, a noun occurring twiceor more in a document is treated as other nouns oc-curring once.
From IR point of view, this takes intoaccount of the fact that even if a noun is extractedjust once as an index term, the document includingthe term can also be retrieved.The performance considerably depends on thefollowing factors: the representation schemes forword boundary detection, the tagset, the amount oftraining data, and the difference between trainingdata and test data.First, we compare four different representationschemes (BI, BIS, IE, IES) in word boundary de-tection as explained in Section 4.
We try to use thefollowing three kinds of tagsets in order to select themost optimal tagset through the experiments:Tagset 1 Simply use two tags (e.g.
noun and non-noun).
This is intended to examine the syllablecharacteristics; that is, which syllables tend tobelong to nouns or not.Tagset 2 Use the tagset used in the training datawithout modification.
ETRI tagset used fortraining is relatively smaller than that of othertagsets.
This tagset is changeable according tothe POS tagged corpus used in training.Tagset 3 Use a simplified tagset for the purpose ofnoun extraction.
This tagset is simplified bycombining postpositions, adverbs, and verbalsuffixes into one tag, respectively.
This tagset isalways fixed even in a different training corpus.Tagset 2 used in Section 5.2 and Tagset 3 are rep-resented in Table 2.5.2 Experimental results with similar dataWe divided the test data into ten parts.
The perfor-mances of the model are measured by averaging overTable 3: Experimental results of the ten-fold cross validationwithout considering frequency with considering frequencyPrecision Recall F-measure Precision Recall F-measureBI-1 72.37 83.61 77.58 74.61 82.47 78.34BI-2 85.99 92.30 89.03 88.96 90.42 89.69BI-3 84.85 91.20 87.90 87.56 89.55 88.54BIS-1 78.50 83.53 80.93 80.36 83.99 82.13BIS-2 88.15 92.34 90.19 90.65 91.58 91.11BIS-3 86.92 91.07 88.94 89.27 90.62 89.94IE-1 73.21 81.38 77.07 75.11 81.04 77.96IE-2 85.12 91.54 88.21 88.37 90.34 89.34IE-3 83.28 89.70 86.37 86.54 88.80 87.65IES-1 78.07 82.69 80.31 79.54 83.08 81.27IES-2 87.30 92.18 89.67 90.05 91.48 90.76IES-3 85.80 90.79 88.22 88.46 90.47 89.4574.0076.0078.0080.0082.0084.0086.0088.0090.0092.00BI BIS IE IESF-measureTagset 1Tagset 2Tagset 3Figure 2: Changes of F-measure according to tagsetsand representation schemes85.0085.5086.0086.5087.0087.5088.0088.5089.0089.5099 99-2000 99-2001training dataF-measureBI-2BIS-2IE-2IES-2Figure 3: Changes of F-measure according to thesize of training datathe ten test sets in the 10-fold cross-validation exper-iment.
Table 3 shows experimental results accordingto each representation scheme and tagset.
In the firstcolumn, each number denotes the tagset used.
Whenit comes to the issue of frequency, the cases of con-sidering frequency are better for precision but worsefor recall, and better for F-measure.
The representa-tion schemes using single syllable information (e.g.
?BIS?, ?IES?)
are better than other representationschemes (e.g.
?BI?, ?IE?).
Contrary to our expec-tation, the results of Tagset 2 consistently outper-form other tagsets.
The results of Tagset 1 are notas good as other tagsets because of the lack of thesyntactic context.
Nevertheless, the results reflectthe usefulness of the syllable based processing.
Thechanges of the F-measure according to the tagsetsand the representation schemes reflecting frequencyare shown in Figure 2.5.3 Experimental results with different dataTo show the influence of the difference between thetraining data and the test data, we have performedthe experiments on the Sejong corpus as a trainingdata and the entire ETRI corpus as a test data.
Table4 shows the experimental results on all of the threetraining data.
Although more training data are usedin this experiment, the results of Table 3 shows bet-ter outcomes.
Like other POS tagging models, thisindicates that our model is dependent on the text do-main.Table 4: Experimental results of Sejong corpus (from 1999 to 2001)without considering frequency with considering frequencyPrecision Recall F-measure Precision Recall F-measureBI-1 71.91 83.92 77.45 73.57 82.95 77.98BI-2 85.38 89.96 87.61 87.19 88.26 87.72BI-3 83.36 89.17 86.17 85.12 87.39 86.24BIS-1 76.77 82.60 79.58 78.40 83.16 80.71BIS-2 87.66 90.41 89.01 88.75 89.75 89.25BIS-3 86.02 88.89 87.43 87.10 88.41 87.75IE-1 70.82 79.97 75.12 72.67 79.64 75.99IE-2 84.18 89.23 86.63 85.99 87.83 86.90IE-3 82.01 87.67 84.74 83.79 86.57 85.16IES-1 76.19 81.84 78.91 77.31 82.32 79.74IES-2 86.41 89.33 87.85 87.66 88.75 88.20IES-3 84.45 88.28 86.33 85.89 87.96 86.91Table 5: Performances of other systemswithout considering frequency with considering frequencyPrecision Recall F-measure Precision Recall F-measureNE2001 84.08 91.34 87.56 87.02 89.86 88.42KOMA 60.10 93.12 73.06 58.07 93.67 71.70HanTag 90.54 88.68 89.60 91.77 88.58 90.15Figure 3 shows the changes of the F-measure ac-cording to the size of the training data.
In this fig-ure, ?99-2000?
means 1999 corpus and 2000 cor-pus are used, and ?99-2001?
means all corpora areused as the training data.
The more training data areused, the better performance we obtained.
However,the improvement is insignificant in considering theamount of increase of the training data.Results reported by Lee et al (2001) are pre-sented in Table 5.
The experiments were performedon the same condition as that of our experiments.NE2001, which is a system designed only to extractnouns, improves efficiency of the general morpho-logical analyzer by using positive and negative in-formation about occurrences of nouns.
KOMA (Leeet al, 1999b) is a general-purpose morphological an-alyzer.
HanTag (Kim et al, 1998) is a POS tagger,which takes the result of KOMA as input.
Accord-ing to Table 5, HanTag, which is a POS tagger, is anoptimal tool in performing noun extraction in termsof the precision and the F-measure.
Although thebest performance of our proposed model (BIS-2) isworse than HanTag, it is better than NE2001 andKOMA.5.4 LimitationAs mentioned earlier, we assume that morphologi-cal variations do not occur at any inflected words.However, some exceptions might occur in a col-loquial text.
For example, the lexical level formsof two Eojeols ?(ddai)+(neun)?
and ?(go-gai)+(leul)?
are changed into the surface levelforms by contractions such as ?(ddain)?
and ?
(go-gail)?, respectively.
Our models alone cannotdeal with these cases.
Such exceptions, however, arevery rare.
6 In these experiments, we do not performany post-processing step to deal with such excep-tions.6 ConclusionWe have presented a word recognition model for ex-tracting nouns.
While the previous noun extraction6Actually, about 0.145% of nouns in the test data belong tothese cases.methods require morphological analysis or POS tag-ging, our noun extraction method only uses the syl-lable information without using any additional mor-phological analyzer.
This means that our methoddoes not require any dictionary or linguistic knowl-edge.
Therefore, without manual labor to constructand maintain those resources, our method can ex-tract nouns by using only the statistics, which can beautomatically extracted from a POS tagged corpus.The previous noun extraction methods take a mor-pheme as a processing unit, but we take a new notionof word as a processing unit by considering the factthat nouns belong to uninflected morphemes in Ko-rean.
By virtue of the new definition of a word, weneed not consider mismatches between the surfacelevel form and the lexical level one in recognizingwords.We have performed various experiments with awide range of variables influencing the performancesuch as the representation schemes for the wordboundary detection, the tag set, the amount of train-ing data, and the difference between the training dataand the test data.
Without morphological analysis orPOS tagging, the proposed method achieves compa-rable performance compared with the previous ones.In the future, we plan to extend the context to im-prove the performance.Although the word recognition model is designedto extract nouns in this paper, the model itself ismeaningful and it can be applied to other fields suchas language modeling and automatic word spacing.Furthermore, our study make some contributions inthe area of POS tagging research.ReferencesD.-U.
An.
1999.
A noun extractor using connectivityinformation.
In Proceedings of the Morphological An-alyzer and Tagger Evaluation Contest (MATEC 99),pages 173?178.S.-S. Kang and C.-W.
Woo.
2001.
Automatic segmenta-tion of words using syllable bigram statistics.
In Pro-ceedings of the 6th Natural Language Processing Pa-cific Rim Symposium, pages 729?732.S.-S. Kang.
1995.
Morphological analysis of Korean ir-regular verbs using syllable characteristics.
Journal ofthe Korea Information Science Society, 22(10):1480?1487.N.-C. Kim and Y.-H. Seo.
1999.
A Korean morpho-logical analyzer CBKMA and a index word extractorCBKMA/IX.
In Proceedings of the MATEC 99, pages50?59.J.-D. Kim, H.-S. Lim, S.-Z.
Lee, and H.-C. Rim.
1998.Twoply hidden Markov model: A Korean pos taggingmodel based on morpheme-unit with word-unit con-text.
Computer Processing of Oriental Languages,11(3):277?290.O.-W. Kwon, M.-Y.
Chung, D.-W. Ryu, M.-K. Lee, andJ.-H. Lee.
1999.
Korean morphological analyzer andpart-of-speech tagger based on CYK algorithm usingsyllable information.
In Proceedings of the MATEC99.J.-Y.
Lee, B.-H. Shin, K.-J.
Lee, J.-E. Kim, and S.-G. Ahn.
1999a.
Noun extractor based on a multi-purpose Korean morphological engine implementedwith COM.
In Proceedings of the MATEC 99, pages167?172.S.-Z.
Lee, B.-R. Park, J.-D. Kim, W.-H. Ryu, D.-G. Lee,and H.-C. Rim.
1999b.
A predictive morphologicalanalyzer, a part-of-speech tagger based on joint inde-pendence model, and a fast noun extractor.
In Pro-ceedings of the MATEC 99, pages 145?150.D.-G. Lee, S.-Z.
Lee, and H.-C. Rim.
2001.
An effi-cient method for Korean noun extraction using nounoccurrence characteristics.
In Proceedings of the 6thNatural Language Processing Pacific Rim Symposium,pages 237?244.D.-G. Lee, S.-Z.
Lee, H.-C. Rim, and H.-S. Lim.
2002.Automatic word spacing using hidden Markov modelfor refining Korean text corpora.
In Proceedings ofthe 3rd Workshop on Asian Language Resources andInternational Standardization, pages 51?57.H.-S. Lim, S.-Z.
Lee, and H.-C. Rim.
1995.
An ef-ficient Korean mophological analysis using exclusiveinformation.
In Proceedings of the 1995 InternationalConference on Computer Processing of Oriental Lan-guages, pages 225?258.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
Textchunking using transformation-based learning.
InPro-ceedings of the Third Workshop on Very Large Cor-pora, pages 82?94.J.-H. Shim, J.-S. Kim, J.-W. Cha, and G.-B.
Lee.
1999.Robust part-of-speech tagger using statistical and rule-based approach.
In Proceedings of the MATEC 99,pages 60?75.K.-S. Shim.
1996.
Automated word-segmentation forKorean using mutual information of syllables.
Journalof the Korea Information Science Society, 23(9):991?1000.
