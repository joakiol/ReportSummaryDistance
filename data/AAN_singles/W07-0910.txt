Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 73?80,Prague, 28 June 2007. c?2007 Association for Computational LinguisticsDeriving a Domain Specific Test Collection from a Query LogAvi Arampatzis1 Jaap Kamps1,2 Marijn Koolen1 Nir Nussbaum21 Archives and Information Science, University of Amsterdam2 ISLA, Informatics Institute, University of AmsterdamAbstractCultural heritage, and other special domains,pose a particular problem for informationretrieval: evaluation requires a dedicatedtest collection that takes the particular doc-uments and information requests into ac-count, but building such a test collection re-quires substantial human effort.
This paperinvestigates methods of generating a docu-ment retrieval test collection from a searchengine?s transaction log, based on submit-ted queries and user-click data.
We test ourmethods on a museum?s search log file, andcompare the quality of the generated testcollections against a collection with manu-ally generated and judged known-item top-ics.
Our main findings are the following.First, the test collection derived from a trans-action log corresponds well to the actualsearch experience of real users.
Second,the ranking of systems based on the derivedjudgments corresponds well to the rankingbased on the manual topics.
Third, derivingpseudo-relevance judgments from a transac-tion log file is an attractive option in do-mains where dedicated test collections arenot readily available.1 IntroductionCultural heritage, and other special domains, posea particular problem for information retrieval.Progress in information retrieval depends heavily onthe availability of suitable test collections consist-ing of a set of documents; a set of search topics;and (human) relevance judgments.
Standard bench-marks, such as those developed at TREC (2007),have been developed using newspaper and newswiredata.
Whilst these test collections are immenselyuseful to evaluate generic properties of retrieval sys-tems, such as fundamental ranking principles, theydo not capture the specific context of particular do-mains (Ingwersen and Ja?rvelin, 2005).
To take cul-tural heritage as an example, the documents arecultural heritage descriptions which are differentin character from newspaper articles, and also thesearch requests and relevance judgments about artare more subjective than factual queries about news(Koolen et al, 2007).
As a result, special domainslike cultural heritage require a dedicated test collec-tion that takes the particular documents and informa-tion requests into account, but building such a testcollection requires substantial human effort.We opt for a different approach.
Search enginescommonly store the actions of users in transactionlogs, which allow an unobtrusive way of studyinguser behaviour.
Logs contain valuable informationsuch as what searchers are looking for, what re-sults they find interesting enough to click on, etc.In this paper, we investigate methods of extractingqueries and user-clicks (on the search result items)from transaction logs in order to create a quality testcollection for Document Retrieval.A quality test collection for Document Retrieval istraditionally considered as a set of queries on a docu-ment collection with complete and reliable relevancejudgements.
Complete in the sense that all docu-ments are judged for relevance against all queries,and reliable in the sense that judgements are sta-73ble across a majority of human assessors.
Never-theless, considering the fact that a test collection isused ?as a mechanism for comparing system per-formance?
(Voorhees, 2002), the requirements forcompleteness and reliability may be relaxed some-what.The Text REtrial Conference (TREC) has tradi-tionally used incomplete judgements for compar-ing system effectiveness via the ?pooling?
method(Jones and van Rijsbergen, 1975), and it is alsowell-known that human assessor agreement is rel-atively low (Voorhees and Harman, 2005).
Conse-quently, test collections which preserve the effec-tiveness ranking of several systems can be consid-ered of equivalent quality in the context of com-paring system effectiveness.
In order to evaluatethe quality of test collections extracted in variousways from a transaction log, it would be sufficientto compare their ability to rank several retrieval sys-tems against a reference system ranking produced byan already known good test collection not producedfrom the log.One can think of several ways of extractingqueries and clicks from a transaction log and turn-ing them into a set of queries with relevance judg-ments.
A simple (and naive) way would be to treatevery query typed by a user as a topic, and every re-sult that the user clicked on as a positive relevancejudgment.
However, such an approach may not leadto a good test set.
Previous research on user clickbehaviour has shown that clicks on search engine re-sults do not directly correspond to explicit, absoluterelevance judgments, but can be considered as rela-tive relevance judgments (Joachims et al, 2005), i.e.,if a user skips result a and clicks on result b, than theuser preference reflects rank(b) > rank(a).
More-over, the occurrence frequencies of queries and thenumbers of retrieved items vary significantly acrossqueries which may lead to wide variation in effec-tiveness.The challenge we take up has several dimensionswhich can be summarized in the following ques-tions:?
How can we derive topics and pseudo-relevance judgments from a transaction log file,and how does this impact the quality of the gen-erated test collection??
How does system effectiveness on the automat-ically generated test collection compare to theeffectiveness on a set of manually constructedknown-item topics?If automatic methods of building test collections areindeed feasible, this opens up a whole new dimen-sion of possibilities for Information Retrieval eval-uation: there is an enormous lengths of transactionlogs generated daily at numerous web-sites and aton-line search engines.The rest of this paper is organized as follows.Next, in Section 2 we discuss transaction logs ingeneral, and the specific transaction log from a mu-seum that we?ll use in the case study of this paper.Section 3 details how we have extracted topics andpseudo-relevance judgments from a museum?s logfile, and their evaluation.
Then, in Section 4, weevaluate the merits of the derived test collection incomparison to human generated and judged topics.We end with Section 5 in which we summarize ourfindings.2 Transaction Logs2.1 Previous WorkThere has been substantial interest in using click-through data from transaction logs as a form ofimplicit feedback (Dumais et al, 2003).
A rangeof implicit feedback techniques have been used forquery expansion and user profiling in informationretrieval tasks (Oard and Kim, 2001; Kelly and Tee-van, 2003).
Joachims et al (2005, p.160) concludethat ?the implicit feedback generated from clicksshows reasonable agreement with the explicit judg-ments of the pages?.Transaction logs have been analysed to studyuser search behaviour in Web search engines (Chauet al, 2005) and digital libraries (Jones et al, 2000),amongst others (Jansen, 2006).
In Chau et al(2005), user behaviour is studied using the transac-tion log of a website?s search engine and is com-pared to that of general purpose search engines.They find that the number of query terms used forwebsite search engines is comparable to queries sub-mitted to general purpose search engines, but thesearch topics and terms are different.In this paper, we go one step further and try to ex-ploit the user behaviour implicit in the data to con-74Figure 1: The search engine of the Gemeentemuseum?s website.struct a test set with real user needs, queries andjudgments.2.2 A Website?s Search EngineThe website of the Haags Gemeentemuseum1 in theHague, the Netherlands, offers a search engine forthree different parts of the Gemeentemuseum, thewebsite content, the on-line shop, and the highlightsof the museum?s object collection (see Figure 2.1).The searchable on-line collection consists of 1,127objects, the highlights of the museum, from a to-tal database of 116,493 museum objects.
The meta-data of these objects are stored in a legacy system,and queries are matched against the title and cre-ator fields (Koolen et al, 2007).
The descriptionscontain many more fields, however.
The objects1http://www.gemeentemuseum.nldatabase treats the query as a Boolean AND query,and returns a warning if there is no object descrip-tion containing all terms in one field.
Although thedatabase allows a drop-back to the individual terms,the website search engine retains a strict BooleanAND query and returns an empty result list.The transaction log contains the transactions fromthe server side.
The website uses a Java script tointeract with the search engine.
The query itself isnot stored in the transaction log.
If a user clicks on aresult that leads to another web page in the domain,or to an item in the shop, this click is registered inthe transaction, but the actual query is not.
If a userclicks on a result from the object collection however,the database query is stored in the transaction log,from which we can extract the actual user query, andthe object that user wants to see.75This has an effect on the queries found in the logfile.
Queries containing both title and creator namesoften lead to an empty result list, as there is no sin-gle field containing both creator and title terms.
Thedatabase looks for all the terms in one field at a time,and will not match with any object.
With an emptyresult list, users cannot click on an object and hence,the query is not logged.
Another effect is that allthe results that users can click on have all the queryterms in either the title or creator field.
Althoughend users sometimes express their information needsin terms different from the terms chosen by indexers,i.e.
the curators in the museum (Markkula and Sor-munen, 2000), this discrepancy cannot be observedin the log-file data.This may lead to the concern that the topics thatcan be extracted from the transaction log are ?easy?topics, since the relevant descriptions necessarilycontain all the query terms.
It is unclear whetherthis affects the extracted topic set significantly, sincewe will look only at the relative ranking of systemsover a set of queries.
We will compare the abil-ity to rank systems of our automatically generatedtopic sets with the system ranking ability of a man-ual topic set.
If the extracted topic sets preserve thesystem ranking of the manual topic set, the bias inthe topic sets towards ?easy?
topics has no negativeinfluence on the quality of the topic sets.3 Experiments and setupWe have obtained the log files covering a period ofone and a half years, between September 14, 2005and February 26, 2007.From the transaction log, we extracted the queriesand the object identifiers from the database query,and turned them into Qrels, i.e., the object is relevantfor the query.We use the following terminology:?
User: the client side of the transaction, identi-fied by ip-address.?
Transaction: any exchange between client(user) and server (system), corresponding to aline in the transaction log.?
Session: A sequence of transactions by thesame user, where the maximum interval be-tween transaction n and n + 1 is 1 hour.Topic # Topics Query length Avg.
#set average median rel.
docsRaw 7,531 1.18 1 2.38Union 1,183 1.38 1 3.86Intersection 974 1.42 1 1.41Manual 150 2.38 2 1.00Table 1: Statistics on the extracted topic sets.More than 1 hour of inactivity signals a sessionboundary.?
Query: the string typed by the user as it ap-pears in the transaction log.?
Result: the identifier of the museum object,used to retrieve the object data from the objectdatabase.3.1 Extraction methodsWe used 3 extraction methods to construct a test set:1.
Raw queries: each query appearing in thelog is used, i.e.
the bag of queries.
Here, atopic consist of a query and the correspondingclicked results from one session.
If the sameuser types the same query in another session,this is treated as a new topic.2.
Unique union: All unique queries are used, i.e.the set of queries.
All the results clicked byall users typing the same query are consideredrelevant documents.3.
Unique intersection: All unique queries areused, i.e.
the set of queries.
The intersectionof the results clicked by all users typing thesame query are considered relevant documents.Thus, a result is relevant only if all users whotyped the query, clicked on that result.Table 1 shows statistics on the resulting topic sets.In calculating these numbers, stop words were re-moved from the queries.
As most queries are inDutch, we used the standard Snowball stopword listfor Dutch (Snowball, 2007).
The queries are veryshort on average.
For the Raw, Union and Intersec-tion topic sets, the queries with 1 term form 84%,70% and 68% of the query sets respectively.
There76are 1,183 unique queries, and on average, 3.86 re-sults are clicked by at least one user.
Understand-ably, the Intersection set has less topics than theUnion set, as there are queries with no single resultclicked on by all users.
Also, the average number ofrelevant documents per topic is lower for the inter-section set.We created 150 Known-Item topics by hand andused this test set, referred to as KI-topics, on thesame collection and include the results as a com-parison with the new test sets.
Table 1 shows thestatistics of these human generated topics in the lastrow.
These search request have more verbose topicstatements with a median length of 2, compared to amedian length of 1 for the query log topics.
Also thenumber of relevant documents differs considerably,with a unique relevant page for the human known-item topics, and several ?clicked?
pages per queryfor the transaction log.3.2 Retrieval systemTo see if our test sets lead to a stable system rank-ing, we need a number of retrieval systems to com-pare their ranking on the different test collections.To get a number of different systems, we simply usea standard retrieval model with different parametersettings to create different runs.We use a standard language model (Hiemstra,2001).
Our system is an extension to Lucene (ILPS,2005) and uses Jelinek-Mercer smoothing, con-trolled by the parameter ?, and a length prior, con-trolled by the parameter ?, i.e., for a collection D,document d and query q:P (d|q) = P (d)?
?t?q((1 ?
?)
?
P (t|D) + ?
?
P (t|d)) ,(1)whereP (t|d) =tft ,d|d|(2)P (t|D) =doc freq(t,D)?t?
?D doc freq(t?, D)(3)P (d) =|d|?d?
?D |d|(4)We assign a prior probability to an document drelative to its length in the following manner:P (d) =|d|?
?d |d|?, (5)System ?
?A 0.10 0B 0.50 0C 0.90 0D 0.10 1E 0.50 1F 0.90 1G 0.10 2H 0.50 2I 0.90 2Table 2: Parameter settings for the different systems.where |d| is the length of a document d. The ?
pa-rameter introduces a length bias which is propor-tional to the document length with ?
= 1 (the de-fault setting).
For more details on language modelsand smoothing, see (Hiemstra, 2001).
For details onthe effect of the length parameter, see (Kamps et al,2004).3.3 Experimental Set-upIn our experiments we will emulate a set of differentretrieval systems by using arbitrary parameter set-tings for smoothing (?)
and length prior (?).
Thiswill result in a range of different rankings of doc-uments, and we can compare their retrieval effec-tiveness on our various topic sets.
In this way, wecan compare the system ranking of the automati-cally generated topic sets with the system rankingof a manually crafted topic set.We made 9 different runs with each topic set, us-ing 3 different values (0.10, 0.50 and 0.90) for thesmoothing parameter ?, corresponding to heavy, av-erage and little smoothing respectively, and 3 differ-ent values (0, 1 and 2) for the length prior ?
corre-sponding to no length normalization and length nor-malization proportional to the document length.To measure the correlation of the system rankingsresulting from the different topic sets, we look atKendall?s tau coefficient.4 ResultsTable 3 shows the detailed results for all runs overall topics sets.
As noted above, we will focus on therelative system rankings over topic sets.
We limitour analysis to the performance in terms of mean-77Topics # Topics MRR Success@10Raw topics ?
= 0, ?
= 0.10 7,527 0.5974 0.8023Raw topics ?
= 0, ?
= 0.50 7,527 0.5970 0.8030Raw topics ?
= 0, ?
= 0.90 7,527 0.5970 0.8031Raw topics ?
= 1, ?
= 0.10 7,527 0.5673 0.7506Raw topics ?
= 1, ?
= 0.50 7,527 0.5765 0.7574Raw topics ?
= 1, ?
= 0.90 7,527 0.5767 0.7574Raw topics ?
= 2, ?
= 0.10 7,527 0.5531 0.7427Raw topics ?
= 2, ?
= 0.50 7,527 0.5618 0.7468Raw topics ?
= 2, ?
= 0.90 7,527 0.5644 0.7474Union ?
= 0, ?
= 0.10 1,183 0.6908 0.8191Union ?
= 0, ?
= 0.50 1,183 0.6925 0.8233Union ?
= 0, ?
= 0.90 1,183 0.6927 0.8233Union ?
= 1, ?
= 0.10 1,183 0.6622 0.7887Union ?
= 1, ?
= 0.50 1,183 0.6772 0.8005Union ?
= 1, ?
= 0.90 1,183 0.6782 0.8005Union ?
= 2, ?
= 0.10 1,183 0.6216 0.7566Union ?
= 2, ?
= 0.50 1,183 0.6477 0.7828Union ?
= 2, ?
= 0.90 1,183 0.6515 0.7870Intersection ?
= 0, ?
= 0.10 974 0.6481 0.8008Intersection ?
= 0, ?
= 0.50 974 0.6505 0.8049Intersection ?
= 0, ?
= 0.90 974 0.6506 0.8049Intersection ?
= 1, ?
= 0.10 974 0.6187 0.7690Intersection ?
= 1, ?
= 0.50 974 0.6329 0.7793Intersection ?
= 1, ?
= 0.90 974 0.6341 0.7793Intersection ?
= 2, ?
= 0.10 974 0.5783 0.7310Intersection ?
= 2, ?
= 0.50 974 0.6053 0.7618Intersection ?
= 2, ?
= 0.90 974 0.6093 0.7659KI-topics ?
= 0.0?
= 0.10 150 0.5446 0.7067KI-topics ?
= 0.0?
= 0.50 150 0.5590 0.7267KI-topics ?
= 0.0?
= 0.90 150 0.5608 0.7200KI-topics ?
= 1.0?
= 0.10 150 0.5253 0.7067KI-topics ?
= 1.0?
= 0.50 150 0.5465 0.7200KI-topics ?
= 1.0?
= 0.90 150 0.5516 0.7200KI-topics ?
= 2.0?
= 0.10 150 0.4602 0.6667KI-topics ?
= 2.0?
= 0.50 150 0.5196 0.7133KI-topics ?
= 2.0?
= 0.90 150 0.5292 0.7133Table 3: Mean Reciprocal Rank and Success@10 for all topic sets on the web site objects.Topic set System rankingRaw A  B  C  F  E  D  I  H  GUnion C  B  A  F  E  D  I  H  GIntersection C  B  A  F  E  D  I  H  GKI-topics C  B  F  E  A  I  D  H  GTable 4: Systems rankings of the 4 topic sets.78KI-topics Raw Union Intersect.KI-topics 1.00Raw 0.67 1.00Union 0.83 0.83 1.00Intersection 0.83 0.83 1.00 1.00Table 5: Rank correlation coefficients between thetopic sets.reciprocal rank (i.e., 1 over the rank at which the firstrelevant document is found).
The rankings over thefour different topic sets are given in Table 4 (basedon the labeling introduced in Table 2).The results show that ranking based on the RawTopic set deviates slightly from ranking based onthe Union and Intersection topic sets.
The Unionand Intersection topic sets result in exactly the sameranking.
There is a clear grouping of systems withthe same length prior.
The systems without a lengthprior (A,B and C) outrank the systems with a lengthprior ?
= 1 (D, E and F), which in turn outrankthe systems with length prior ?
= 2 (systems G, Hand I).
Within these groups, the system ranks corre-spond to the smoothing parameter settings.
A higher?
value corresponds to a higher rank.
The only devi-ation is observed in the ranking based on the RawTopic set.
Here, the lowest value for ?
leads tothe best performance for the systems with no lengthprior.If we compare the three automatically generatedtopic sets to the manual known-item topic set, wesee some more differences.
For the manual topics,systems E and F, which have a unit length prior, out-rank system A, which has no length prior.
A possi-ble explanation for this is that the higher ?
of sys-tems E and F help the longer queries of the manualtopic set.
In the other topic sets, most of the querieshave only one term, so smoothing has very little in-fluence.
This same effect might explain why systemI outranks system D.If we look at the correlation coefficient (Table 5),we see a positive correlation between all topic sets.As the Union and Intersection topic sets lead to thesame system ranking, they have a correlation of 1.The system ranking of the Raw topic set shows thelowest correlation with the other topic sets, but thecorrelation with the manual topic set is still high, in-dicating that all the extraction methods lead to topicsets that have an ability to rank system similar to thatof a manually constructed topic set.
Of course, thenumber of known-item topics is much smaller thanthe other topic sets, but these initial results point outthat the automatic generation of test collections fromtransaction logs makes sense.5 Discussion and ConclusionsCultural heritage, and other special domains, pose aparticular problem for information retrieval: evalu-ation requires a dedicated test collection that takesthe particular documents and information requestsinto account, but building such a test collection re-quires substantial human effort.
We have investi-gated methods of generating a document retrievaltest collection from a search engine?s transactionlog, based on submitted queries and user-click data.We tested our methods on a museum?s search logfile, and compared the quality of the generated testcollections against a collection with manually gen-erated and judged known-item topics.Our main findings are the following.
First, thetest collection derived from a transaction log corre-sponds well to the actual search experience of realusers.
An important criterion of bench-marks is thatthey correspond well to the real-world phenomenonthat they are supposed to measure.
By basing the testcollection directly on a large sample of real end-userinteraction, with real information needs, we can en-sure that the test collection reflects the informationseeking behaviors of users well.
This is of partic-ular importance for domain-specific test collections,where results may be impacted by the particular typeof information available, and the particular sorts ofsearch requests that are likely to be issued.Second, the ranking of systems based on the de-rived judgments corresponds well to the rankingbased on the manual topics.
We extracted threedifferent sets of topics and corresponding pseudo-relevance judgments from the transaction log.
Allthree sets result in very similar system rankings, in-dicating that the results are robust against particularchoices in the extraction phase.
The system rankingsare corresponding well to a ranking based on humangenerated known-item topics.
Given the promisinginitial results, we are currently working on a more79rigorous comparative evaluation, with more humantopics, and more diverse systems to be ranked, aim-ing to understand better the exact conditions underwhich the extracted test collections behave similarto human generated test collections?and when theybehave differently.Third, deriving pseudo-relevance judgments froma transaction log file is an attractive option in do-mains where dedicated test collections are not read-ily available.
The results in the paper should notbe interpreted as a claim to replace human rele-vance judgments with extracted topics and pseudo-relevance judgments.
There are however many do-mains and tasks where no suitable test collection isavailable, and creating a new human test collectionmight be either impractical or even impossible.
Re-call that creating human judged test collections re-quires considerable effort: it is usually a communityeffort where a number of participating teams pro-vide a diverse set of runs needed for pooling, or evenengage in peer-assessments.
Hence, deriving a testcollection from a transaction log?if available?canbe an attractive alternative.AcknowledgmentsThis research is part of the MUSEUM (MUltiple-collection SEarching Using Metadata; http://www.nwo.nl/catch/museum/) project of theCATCH (Continuous Access To Cultural Heritage)research program in the Netherlands.The authors were supported by the NetherlandsOrganization for Scientific Research (NWO, grants# 612.066.513, 639.072.601, and 640.001.501), andby the E.U.
?s 6th FP for RTD (project MultiMatchcontract IST-033104).ReferencesMichael Chau, Xiao Fang, and Olivia R. Liu Sheng.
2005.
Anal-ysis of the query logs of a web site search engine.
J.
Am.
Soc.Inf.
Sci.
Technol., 56(13):1363?1376.Susan Dumais, Thorsten Joachims, Krishna Bharat, and An-dreas Weigend.
2003.
SIGIR 2003 workshop report: implicitmeasures of user interests and preferences.
SIGIR Forum,37:50?54.Djoerd Hiemstra.
2001.
Using Language Models for Informa-tion Retrieval.
Thesis, University of Twente.ILPS.
2005.
The ilps extension of the lucene search engine.http://ilps.science.uva.nl/Resources/.Peter Ingwersen and Kalervo Ja?rvelin.
2005.
The Turn: In-tegration of Information Seeking and Retrieval in Context.The Kluwer International Series on Information Retrieval.Springer Verlag, Heidelberg.Bernard J. Jansen.
2006.
Search log analysis: What is it; what?sbeen done; how to do it.
Library and Information ScienceResearch, 28(3):407?432.Thorsten Joachims, Laura Granka, Bing Pan, Helene Hem-brooke, and Geri Gay.
2005.
Accurately interpreting click-through data as implicit feedback.
In SIGIR ?05: Proceed-ings of the 28th annual international ACM SIGIR conferenceon Research and development in information retrieval, pages154?161.
ACM Press, New York, NY, USA.Karen Sparck Jones and C. van Rijsbergen.
1975.
Report on theneed for and provision of an ?ideal?
information retrieval testcollection.
British Library Research and Development report5266, Computer Laboratory, University of Cambridge.Steve Jones, Sally Jo Cunningham, Rodger J. McNab,and Stefan J. Boddie.
2000.
A transaction log anal-ysis of a digital library.
Int.
j. on Digital Li-braries, 3(2):152?169.
URL citeseer.ist.psu.edu/jones00transaction.html.Jaap Kamps, Maarten de Rijke, and Bo?rkur Sigurbjo?rnsson.2004.
Length normalization in xml retrieval.
In MarkSanderson, Kalervo Ja?rvelin, James Allan, and Peter Bruza,editors, Proceedings of the 27th Annual International ACMSIGIR Conference on Research and Development in Infor-mation Retrieval, pages 80?87.
ACM Press, New York, NY,USA.Diane Kelly and Jaime Teevan.
2003.
Implicit feedback forinferring user preference: a bibliography.
SIGIR Forum,37:18?28.Marijn Koolen, Avi Arampatzis, Jaap Kamps, Nir Nussbaum,and Vincent de Keijzer.
2007.
Unified access to heteroge-neous data in cultural heritage.
To appear.Marjo Markkula and Eero Sormunen.
2000.
End-user searchingchallenges indexing practices in the digital newspaper photoarchive.
Information Retrieval, 1:259?285.Douglas W. Oard and Jinmook Kim.
2001.
Modeling informa-tion content using observable behavior.
In Proceedings ofthe 64th Annual Meeting of the American Society for Infor-mation Science and Technology, pages 38?45.Snowball.
2007.
Stemming algorithms for use in informationretrieval.
http://www.snowball.tartarus.org/.TREC.
2007.
Text REtrieval Conference.
http://trec.nist.gov/.Ellen M. Voorhees.
2002.
The philosophy of information re-trieval evaluation.
In Carol Peters, Martin Braschler, JulioGonzalo, and Michael Kluck, editors, Evaluation of Cross-Language Information Retrieval Systems, CLEF 2001, vol-ume 2406 of Lecture Notes in Computer Science, pages 355?370.
Springer.EllenM.
Voorhees and Donna K. Harman, editors.
2005.
TREC:Experimentation and Evaluation in Information Retrieval.MIT Press.80
