Statistical Approaches toComputer-Assisted TranslationSergio Barrachina?Universitat Jaume IOliver Bender?
?RWTH AachenFrancisco Casacuberta?Universitat Polite`cnica de Vale`nciaJorge Civera?Universitat Polite`cnica de Vale`nciaElsa Cubel?Universitat Polite`cnica de Vale`nciaShahram Khadivi?
?RWTH AachenAntonio Lagarda?Universitat Polite`cnica de Vale`nciaHermann Ney?
?RWTH AachenJesu?s Toma?s?Universitat Polite`cnica de Vale`nciaEnrique Vidal?Universitat Polite`cnica de Vale`nciaJuan-Miguel Vilar?Universitat Jaume ICurrent machine translation (MT) systems are still not perfect.
In practice, the outputfrom these systems needs to be edited to correct errors.
A way of increasing the productivity ofthe whole translation process (MT plus human work) is to incorporate the human correctionactivities within the translation process itself, thereby shifting the MT paradigm to that ofcomputer-assisted translation.
This model entails an iterative process in which the humantranslator activity is included in the loop: In each iteration, a prefix of the translation is validated(accepted or amended) by the human and the system computes its best (or n-best) translationsuffix hypothesis to complete this prefix.
A successful framework for MT is the so-called statis-tical (or pattern recognition) framework.
Interestingly, within this framework, the adaptationof MT systems to the interactive scenario affects mainly the search process, allowing a greatreuse of successful techniques and models.
In this article, alignment templates, phrase-basedmodels, and stochastic finite-state transducers are used to develop computer-assisted translationsystems.
These systems were assessed in a European project (TransType2) in two real tasks: Thetranslation of printer manuals; manuals and the translation of the Bulletin of the EuropeanUnion.
In each task, the following three pairs of languages were involved (in both translationdirections): English?Spanish, English?German, and English?French.?
Departament d?Enginyeria i Cie`ncies dels Computadors, Universitat Jaume I, 12071 Castello?
de la Plana,Spain.??
Lehrstuhl fu?r Informatik VI, RWTH Aachen University of Technology, D-52056 Aachen, Germany.?
Institut Tecnolo`gic d?Informa`tica, Departament de Sistemes Informa`tics i Computacio?, UniversitatPolite`cnica de Vale`ncia, 46071 Vale`ncia, Spain.?
Institut Tecnolo`gic d?Informa`tica, Departament de Comunicacions, Universitat Polite`cnica de Vale`ncia,46071 Vale`ncia, Spain.?
Departament de Llenguatges i Sistemes Informa`tics, Universitat Jaume I, 12071 Castello?
de la Plana,Spain.Submission received: 1 June 2006; revised submission received: 20 September 2007; accepted for publication:19 December 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 11.
Introduction to Computer-Assisted TranslationResearch in the field of machine translation (MT) aims to develop computer systemswhich are able to translate text or speech without human intervention.
However,present translation technology has not been able to deliver fully automated high-qualitytranslations.
Typical solutions to improving the quality of the translations supplied byan MT system require manual post-editing.
This serial process prevents the MT systemfrom taking advantage of the knowledge of the human translator, and the humantranslator cannot take advantage of the adaptive ability of the MT system.An alternative way to take advantage of the existing MT technologies is to usethem in collaboration with human translators within a computer-assisted translation(CAT) or interactive framework (Isabelle and Church 1997).
Historically, CAT and MThave been considered different but close technologies (Kay 1997) and more so for oneof the most popular CAT technologies, namely, translation memories (Bowker 2002;Somers 2003).
Interactivity in CAT has been explored for a long time.
Systems havebeen designed to interact with human translators in order to solve different typesof (lexical, syntactic, or semantic) ambiguities (Slocum 1985; Whitelock et al 1986).Other interaction strategies have been considered for updating user dictionaries or forsearching through dictionaries (Slocum 1985; Whitelock et al 1986).
Specific proposalscan be found in Tomita (1985), Zajac (1988), Yamron et al (1993), and Sen, Zhaoxiong,and Heyan (1997), among others.An important contribution to CAT technology, carried out within the TransTypeproject, is worth mentioning (Foster, Isabelle, and Plamondon 1997; Langlais, Foster,and Lapalme 2000; Foster 2002; Langlais, Lapalme, and Loranger 2002).
It entailed aninteresting focus shift in which interaction is directly aimed at the production of thetarget text, rather than at the disambiguation of the source text, as in earlier interactivesystems.
The idea proposed in that work was to embed data-driven MT techniqueswithin the interactive translation environment.
The hope was to combine the best ofboth paradigms: CAT, in which the human translator ensures high-quality output, andMT, in which the machine ensures a significant gain in productivity.Following these TransType ideas, the innovative embedding proposed here con-sists in using a complete MT system to produce full target sentence hypotheses, orportions thereof, which can be accepted or amended by a human translator.
Each cor-rect text segment is then used by the MT system as additional information to achievefurther, hopefully improved, suggestions.
More specifically, in each iteration, a prefixof the target sentence is somehow fixed by the human translator and, in the next itera-tion, the system predicts a best (or n-best) translation suffix(es)1 to complete this prefix.We will refer to this process as interactive-predictive machine translation (IPMT).This approach introduces two important requirements: First, the models have toprovide adequate completions and, second, this has to happen efficiently.
Taking theserequirements into account, stochastic finite-state transducers (SFSTs), alignment tem-plates (ATs), and phrase-based models (PBMs) are compared in this work.
In previousworks these models have proven adequate for conventional MT (Vidal 1997; Amengualet al 2000; Ney et al 2000; Toma?s and Casacuberta 2001; Och and Ney 2003; Casacubertaand Vidal 2004; Och and Ney 2004; Vidal and Casacuberta 2004).
This article shows that1 The terms prefix and suffix are used here to denote any substring at the beginning and end (respectively)of a string of characters (including spaces and punctuation), with no implication of morphologicalsignificance as is usually implied by these terms in linguistics.4Barrachina et al Statistical Computer-Assisted Translationexisting efficient searching algorithms can be adapted in order to provide completions(rather than full translations) also in a very efficient way.The work presented here has been carried out in the TransType2 (TT2) project(SchlumbergerSema S.A. et al 2001), which is considered as a follow-up to the inter-active MT concepts introduced in the precursory TransType project cited previously.We should emphasize the novel contributions of the present work with respectto TransType.
First, we show how fully fledged statistical MT (SMT) systems can beextended to handle IPMT.
In particular, the TT2 systems always produce completesentence hypotheses on which the human translator can work.
This is an importantdifference to previous work, in which the use of basic MT techniques only allowed theprediction of single tokens (c.f., Section 2.2).
Second, using fully fledged SMT systems,we have performed systematic offline experiments to simulate the specific conditions ofinteractive translation and we report and study the results of these experiments.
Thirdly,the IPMT systems presented in this article were successfully used in several field trialswith professional translators (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).We should finally mention that the work developed in TT2 has gone beyond con-ventional keyboard-and-mouse interaction, leading to the development of advancedmulti-modal interfaces.
Speech is the most natural form of human communication andits use as feedback in the IPMT framework has been explored by Vidal et al (2006).On the other hand, human translators can be faster dictating the translation text ratherthan typing it, thus it has also been investigated how to improve system performanceand usability when the user dictates the translation first and then edits the recognizedtext (Khadivi, Zolnay, and Ney 2005; Khadivi, Zens, and Ney 2006).The rest of the article is structured as follows.
The next section introduces thegeneral setting for SMT and IPMT.
In Section 3, AT, PBM, and SFST are briefly surveyedalong with the corresponding learning procedures.
In Section 4, general search proce-dures for the previous models are outlined and a detailed description of the extensionof these procedures to IPMT scenarios is presented.
Section 5 is devoted to introducingthe tasks used for the assessment of the proposal presented in the previous sections:the pairs of languages, corpora, and assessment procedures.
The results are reported inSection 6.
A discussion of these results and the conclusions which can be drawn fromthis work are presented in the final section.2.
Statistical FrameworkThe statistical or pattern recognition framework constitutes a very successful frame-work for MT.
As we will see here, this framework also proves adequate for IPMT.2.1 Statistical Machine TranslationAssuming that we are given a sentence s in a source language, the text-to-text translationproblem can be stated as finding its translation t in a target language.
Using statisticaldecision theory, the best translation is given by the equation2t?
= argmaxtPr(t|s) (1)2 We follow the common notation of Pr(x) for Pr(X = x) and Pr(x|y) for Pr(X = x|Y = y), for any randomvariables X and Y.
Similarly, Pr() will be used to denote ?true?
probability functions, and p() or q() willdenote model approximations.5Computational Linguistics Volume 35, Number 1Using Bayes?s Theorem, we arrive att?
= argmaxtPr(t) ?
Pr(s|t) (2)This equation is generally interpreted as follows.
The best translation must be a correctsentence in the target language that conveys the meaning of the source sentence.
Theprobability Pr(t) represents the well-formedness of t and it is generally called thelanguage model probability (n-gram models are usually adopted [Jelinek 1998]).
Onthe other hand, Pr(s|t) represents the relationship between the two sentences (the sourceand its translation).
It should be of a high value if the source is a good translation ofthe target and of a low value otherwise.
Note that the translation direction is invertedfrom what would be normally expected; correspondingly the models built around thisequation are often called inverted translation models (Brown et al 1990, 1993).
As wewill see in Section 3, these models are based on the notion of alignment.
It is interesting tonote that if we had perfect models, the use of Equation (1) would suffice.
Given that wehave only approximations, the use of Equation (2) allows the language model to correctdeficiencies in the translation model.In practice all of these models (and possibly others) are often combined into a log-linear model for Pr(t | s) (Och and Ney 2004):t?
= argmaxt{N?i=1?i ?
log fi(t, s)}(3)where fi(t, s) can be a model for Pr(s|t), a model for Pr(t|s), a target language modelfor Pr(t), or any model that represents an important feature for the translation.
N is thenumber of models (or features) and ?i are the weights of the log-linear combination.When using SFSTs, a different transformation can be used.
These transducershave an implicit target language model (which can be obtained from the finite-statetransducer by dropping the source symbols of each transition (Vidal et al 2005)).
There-fore, this separation is no longer needed.
SFSTs model joint probability distributions;therefore, Equation (1) has to be rewritten ast?
= argmaxtPr(s, t) (4)This is the approach followed in GIATI (Casacuberta et al 2004a; Casacuberta and Vidal2004), but other models for the joint probability can be adopted.If the input is a spoken sentence, instead of a written one, the problem becomesmore complex; we will not deal with this here.
The interested reader may consultAmengual et al (2000), Ney et al (2000), or Casacuberta et al (2004a, 2004b), forinstance.2.2 Statistical Interactive-Predictive Machine TranslationUnfortunately, current models and therefore the systems which can be built from themare still far from perfect.
This implies that, in order to achieve good, or even acceptable,translations, manual post-editing is needed.
An alternative to this serial approach (firstMT, then manual correction) is given by the IPMT paradigm.
Under this paradigm,translation is considered as an iterative process where human and computer activity6Barrachina et al Statistical Computer-Assisted TranslationFigure 1Typical example of IPMT with keyboard interaction.
The aim is to translate the English sentenceClick OK to close the print dialog into Spanish.
Each step starts with a previously fixed targetlanguage prefix tp, from which the system suggests a suffix t?s.
Then the user accepts a part of thissuffix (a) and types some keystrokes (k), possibly in order to amend the remaining part of ts.This produces a new prefix, composed by the prefix from the previous iteration and the acceptedand typed text, (a) (k), to be used as tp in the next step.
The process ends when the user entersthe special keystroke ?#?.
System suggestions are printed in italics and user input in boldfacetypewriter font.
In the final translation t, text that has been typed by the user is underlined.are interwoven.
This way, the models take into account both the input sentence and thecorrections of the user.As previously mentioned, this idea was originally proposed in the TransTypeproject (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000;Langlais, Lapalme, and Loranger 2002).
In that project, the parts proposed by the sys-tems were produced using a linear combination of a target language model (trigrams)and a lexicon model (so-called IBM-1 or -2) (Langlais, Lapalme, and Loranger 2002).
Asa result, TransType allowed only single-token completions, where a token could be eithera word or a short sequence of words from a predefined set of sequences.
This proposalwas extended to complete full target sentences in the TT2 project, as discussed hereafter.The approach taken in TT2 is exemplified in Figure 1.
Initially, the system providesa possible translation.
From this translation, the user marks a prefix as correct andprovides, as a hint, the beginning of the rest of the translation.
Depending on the systemor the user preferences, the hint can be the next word or some letters from it (in thefigure, hints are assumed to be words and are referred to as k).
Let us use tp for the prefixvalidated by the user together with the hint.
The system now has to produce (predict)a suffix ts to complete the translation.
The cycle continues with a new validation andhint from the user until the translation is completed.
This justifies our choice of the term?interactive-predictive machine translation?
for this approach.The crucial step of the process is the production of the suffix.
Again, decision theorytells us to maximize the probability of the suffix given the available information.
Thatis, the best suffix will bet?s = argmaxtsPr(ts|s, tp) (5)which can be straightforwardly rewritten ast?s = argmaxtsPr(tp, ts|s) (6)7Computational Linguistics Volume 35, Number 1Note that, because tpts = t, this equation is very similar to Equation (1).
The maindifference is that the argmax search now is performed over the set of suffixes ts thatcomplete tp instead of complete sentences (t in Equation (1)).
This implies that we canuse the same models if the search procedures are adequately modified (Och, Zens, andNey 2003).The situation with respect to finite-state models is similar.
Now, Equation (5) isrewritten ast?s = argmaxtsPr(tp, ts, s) (7)which allows the use of the same models as in Equation (4) as long as the searchprocedure is changed appropriately (Cubel et al 2003, 2004; Civera et al 2004a,2004b).3.
Statistical and Finite-State ModelsThe models used are presented in the following subsections: Section 3.1 for the condi-tional distribution Pr(s|t) in Equation (2) and Section 3.2 for the joint distribution Pr(s, t)in Equation (4).3.1 Statistical Alignment ModelsThe translation models which Brown et al (1993) introduced to deal with Pr(s|t) inEquation (2) are based on the concept of alignment between the components of a pair(s, t) (thus they are called statistical alignment models).
Formally, if the number ofthe source words in s is J and the number of target words in t is I, an alignment is afunction a : {1, ..., J} ?
{0, ..., I}.
The image of j by a will be denoted as aj, in which theparticular case aj = 0 means that the position j in s is not aligned with any position of t.By introducing the alignment as a hidden variable in Pr(s|t),Pr(s|t) =?aPr(s, a|t) (8)The alignment that maximizes Pr(s, a|t) is shown to be very useful in practice fortraining and for searching.Different approaches have been proposed for modeling Pr(s, a|t) in Equation (8):Zero-order models such as model 1, model 2, and model 3 (Brown et al 1993) and the first-order models such as model 4, model 5 (Brown et al 1993), hidden Markov model (Neyet al 2000), and model 6 (Och and Ney 2003).In all these models, single words are taken into account.
Moreover, in practice thesummation operator is replaced with the maximization operator, which in turn reducesthe contribution of each individual source word in generating a target word.
On theother hand, modeling word sequences rather than single words in both the alignmentand lexicon models cause significant improvement in translation quality (Och and Ney8Barrachina et al Statistical Computer-Assisted Translation2004).
In this work, we use two closely related models: ATs (Och and Ney 2004) andPBMs (Toma?s and Casacuberta 2001; Koehn, Och, and Marcu 2003; Zens and Ney 2004).Both models are based on bilingual phrases3 (pairs of segments or word sequences)in which all words within the source-language phrase are aligned only to words ofthe target-language phrase and vice versa.
Note that at least one word in the source-language phrase must be aligned to one word of the target-language phrase, that is,there are no empty phrases similar to the empty word of the word-based models.
Inaddition, no gaps and no overlaps between phrases are allowed.We introduce some notation to deal with phrases.
As before, s denotes a source-language sentence; ?s denotes a generic phrase in s, and ?sk the kth phrase in s. sj denotesthe jth source word in s; sj?j denotes the contiguous sequence of words in s beginningat position j and ending at position j?
(inclusive); obviously, if s has J words, sJ1 denotesthe whole sentence s. An analogous notation is used for target words, phrases, andsequences in target sentence t.3.1.1 Alignment Templates.
The ATs are based on the bilingual phrases but they aregeneralized by replacing words with word classes and by storing the alignment in-formation for each phrase pair.
Formally, an AT Z is a triple (S,T,?a), where S andT are a source class sequence and a target class sequence, respectively, and ?a is analignment from the set of positions in S to the set of positions in T.4 Mapping of sourceand target words to bilingual word classes is automatically trained using the methoddescribed by Och (1999).
The method is actually an unsupervised clustering methodwhich partitions the source and target vocabularies, so that assigning words to classesis a deterministic operation.
It is also possible to employ parts-of-speech or semanticcategories instead of the unsupervised clustering method used here.
More details canbe found in Och (1999) and Och and Ney (2004).
However, it should be mentionedthat the whole AT approach (and similar PBM approaches as they are now called) isindependent of the word clustering concept.
In particular, for large training corpora,omitting the word clustering in the AT system does not much affect the translationaccuracy.To arrive at our translation model, we first perform a segmentation of the sourceand target sentences into K ?blocks?
dk ?
(ik; bk, jk) (ik ?
{1, .
.
.
, I} and jk, bk ?
{1, .
.
.
, J}for 1 ?
k ?
K).
For a given sentence pair (sJ1, tI1), the kth bilingual segment (?sk,?tk)is (sjkbk?1+1, tikik?1+1) (Och and Ney 2003).
The AT Zk = (Sk,Tk,?ak) associated with the kthbilingual segment is: Sk the sequence of word classes in ?sk; Tk the sequence of wordclasses in ?tk, and ?ak the alignment between positions in a source class sequence S andpositions in a target class sequence T.For translating a given source sentence s we use the following decision rule as anapproximation to Equation (1):(I?, t?I?1) = argmaxI,tI1{maxK,dK1 ,?aK1log PAT(sJ1, tI1; dK1 ,?aK1 )}(9)3 Although the term ?phrase?
has a more restricted meaning, in this article it refers to a word sequence.4 Note that the phrases in an AT are sequences of word classes rather than words, which motivates the useof a different notation.9Computational Linguistics Volume 35, Number 1We use a log-linear model combination:log PAT(sJ1, tI1; dK1 ,?aK1 ) =I?i=1[?1 + ?2 ?
log p(ti|ti?1i?2)+ ?3 ?
log p(Ti|Ti?1i?4 )]+K?k=1[ ?4 + ?5 ?
log q(bk|jk?1)+ ?6 ?
log p(Tk,?ak|Sk)+ik?i=ik?1+1?7 ?
log p(ti|?sk,?ak) ] (10)with weights ?i, i = 1, ?
?
?
, 7.
The weights ?1 and ?4 play a special role and are usedto control the number I of words and number K of segments for the target sentenceto be generated, respectively.
The log-linear combination uses the following set ofmodels: p(ti|ti?1i?2): Word-based trigram language model p(Ti|Ti?1i?4 ): Class-based five-gram language model p(Tk,?ak|Sk): AT at class level, model parameters are estimated directlyfrom frequency counts in a training corpus p(ti|?sk,?ak): Single word model based on a statistical dictionary and ?ak.
Asin the preceding model, the model parameters are estimated by usingfrequency counts q(bk|jk?1) = e|bk?jk?1+1|: Re-ordering model using absolute j distance ofthe phrases.As can be observed, all models are implemented as feature functions which depend onthe source and the target language sentences, as well as on the two hidden variables(?aK1 , bK1 ).
Other feature functions can be added to this sort of model as needed.
For amore detailed description the reader is referred to Och and Ney (2004).Learning alignment templates.
To learn the probability of applying an AT, p(Z =(S,T,?a)|?s ), all bilingual phrases that are consistent with the segmentation are extractedfrom the training corpus together with the alignment within these phrases.
Thus, weobtain a count N(Z) of how often an AT occurred in the aligned training corpus.
Usingthe relative frequencyp(Z) = (S,T,?a)|?s) =N(Z) ?
?
(S,C(?s))N(C(?s))(11)we estimate the probability of applying an AT Z to translate the source language phrase?s, in which ?
is Kronecker?s delta function.
The class function C maps words onto their10Barrachina et al Statistical Computer-Assisted Translationclasses.
To reduce the memory requirements, only probabilities for phrases up to amaximal length are estimated, and phrases with a probability estimate below a certainthreshold are discarded.The weights ?i in Equation (10) are usually estimated using held-out data withrespect to the automatic evaluation metric employed using the downhill simplex al-gorithm from Press et al (2002).3.1.2 Phrase-Based Models.
A simple alternative to AT has been introduced in recentworks: The PBM approach (Toma?s and Casacuberta 2001; Marcu and Wong 2002; Zens,Och, and Ney 2002; Toma?s and Casacuberta 2003; Zens and Ney 2004).
These methodslearn the probability that a sequence of contiguous words?the source phrase?
(as awhole unit) in a source sentence is a translation of another sequence of contiguouswords?the target phrase?
(as a whole unit) in the target sentence.
In this case, thestatistical dictionaries of single word pairs are substituted by statistical dictionaries ofbilingual phrases or bilingual segments.
These models are simpler than ATs, because noalignments are assumed between word positions inside a bilingual segment and wordclasses are not used in the definition of a bilingual phrase.The simplest formulation is for monotone PBMs (Toma?s and Casacuberta 2007),assuming a uniform distribution of the possible segmentations of the source and of thetarget sentences.
In this case, the approximation to Equation (1) is:(I?, t?I?1) = argmaxI,tI1{maxK,dK1log PPBM(sJ1, tI1; dK1 )}(12)In our implementation of this approach, we have also adopted a log-linear modellog PPBM(sJ1, tI1; dK1 ) =I?i=1[?1 + ?2 ?
log p(ti|ti?1i?2)+ ?3 ?
log p(Ti|Ti?1i?4 )]+K?k=1[?4 + ?5 ?
log p(?tk|?sk)](13)with weights ?i, i = 1, ?
?
?
, 5.
The weights ?1 and ?4 play a special role and are usedto control the number I of words and number K of segments for the target sentenceto be generated, respectively.
The log-linear combination uses the following set ofmodels: p(ti|ti?1i?2): Word-based trigram language model p(Ti|Ti?1i?4 ): Class-based five-gram language model p(?tk|?sk): Statistical dictionary of bilingual phrases.11Computational Linguistics Volume 35, Number 1If segment re-ordering is desired (non-monotone models), the probability of phrase-alignment q can be introduced (a first-order distortion model is assumed):log PPBM(sJ1, tI1; dK1 ) =I?i=1[?1 + ?2 ?
log p(ti|ti?1i?2)+ ?3 ?
log p(Ti|Ti?1i?4 )]+K?k=1[?4 + ?5 ?
log p(?tk|?sk)+ ?6 ?
log q(bk|jk?1)](14)with the additional model q, similar to the one used for AT.Learning phrase-based alignment models.
The parameters of each model and the weights?i in Equations (13) and (14) have to be estimated.
There are different approaches toestimating the parameters of each model (Toma?s and Casacuberta 2007).
Some of thesetechniques correspond to a direct learning of the parameters from a sentence-alignedcorpus using a maximum likelihood approach (Toma?s and Casacuberta 2001; Marcuand Wong 2002).
Other techniques are heuristics based on the previous computationof word alignments in the training corpus (Zens, Och, and Ney 2002; Koehn, Och, andMarcu 2003).
On the other hand, as for AT, the weights ?i in Equation (13) are usuallyoptimized using held-out data.3.2 Stochastic Finite-State TransducersSFSTs constitute an important framework in syntactic pattern recognition and nat-ural language processing.
The simplicity of finite-state models has given rise to someconcerns about their applicability to real tasks.
Specifically in the field of languagetranslation, it is often argued that natural languages are so complex that these simplemodels are never able to cope with the required source-target mappings.
However, oneshould take into account that the complexity of the mapping between the source andtarget domains of a transducer is not always directly related to the complexity of thedomains themselves.
Instead, a key factor is the degree of monotonicity or sequentialitybetween source and target subsequences of these domains (Casacuberta, Vidal, andPico?
2005).
Finite-state transducers have been shown to be adequate to handle complexmappings efficiently (Berstel 1979) and SFSTs are closely related to monotone PBMs.In Equation (4), Pr(s, t) can be modeled by an SFST T, which is defined as a tuple?
?,?,Q, q0, p, f ?, where ?
is a finite set of source symbols,?
is a finite set of target symbols(?
??
= ?
), Q is a finite set of states, q0 is the initial state, p and f are two functionsp : Q ?
??
? ?
Q ?
[0, 1] (for the probabilities of transitions) and f : Q ?
[0, 1] (for theprobabilities of final states) that satisfy ?q ?
Q:f (q) +?(s,?t,q?
)???
??Qp(q, s,?t, q?)
= 1 (15)Given T, a path with J transitions associated with the translation pair (s, t) ???
???
is a sequence of transitions ?
= (q0, s1 , t?1, q1) (q1, s2 , t?2, q2) (q2, s3 , t?3, q3) .
.
.
(qJ?1, sJ , t?J, qJ ), such that s1 s2 .
.
.
sJ = s and t?1 t?2 .
.
.
t?J = t. The probability of a path is12Barrachina et al Statistical Computer-Assisted Translationthe product of its transition probabilities, times the final-state probability of the laststate in the path:PT(?)
=J?j=1p(qj?1, sj , t?j, qj) ?
f (qJ ) (16)The probability of a translation pair (s, t) according to T is then defined as the sum ofthe probabilities of all the paths associated with (s, t):PT(s, t) =??PT(?)
(17)Learning finite-state transducers.
There are different families of techniques to train anSFST from a parallel corpus of source?target sentences (Casacuberta and Vidal 2007).One of the techniques that has been adopted in this work is the grammatical inferenceand alignments for transducer inference (GIATI) technique.
This technique is in thecategory of hybrid methods which use statistical techniques to guide the SFST structurelearning and simultaneously train the associated probabilities.Given a finite sample of string pairs, the inference of SFSTs using the GIATI tech-nique is performed as follows (Casacuberta and Vidal 2004; Casacuberta, Vidal, andPico?
2005): i) Building training strings: Each training pair is transformed into a singlestring from an extended alphabet to obtain a new sample of strings.
ii) Inferring a(stochastic) regular grammar.
Typically, a smoothed n-gram is inferred from the sampleof strings obtained in the previous step.
iii) Transforming the inferred regular grammarinto a transducer: The symbols associated with the grammar rules are converted backinto input/output symbols, thereby transforming the grammar inferred in the previousstep into a transducer.
The transformation of a parallel corpus into a string corpusis performed using statistical alignments.
These alignments are obtained using theGIZA++ software (Och and Ney 2003).4.
SearchingSearching is an important computational problem in SMT.
Algorithmic solutions de-veloped for SMT can be adapted to the IPMT framework.
The main general searchprocedures for each model in Section 3 are presented in the following subsections,each followed by a detailed description of the necessary adaptations to the interactiveframework.4.1 Searching with Alignment TemplatesIn offline MT, the generation of the best translation for a given source sentence s iscarried out by producing the target sentence in left-to-right order using the model ofEquation (10).
At each step of the generation algorithm we maintain a set of activehypotheses and choose one of them for extension.
A word of the target language isthen added to the chosen hypothesis and its costs get updated.
This kind of generationfits nicely into a dynamic programming (DP) framework, as hypotheses which areindistinguishable by both language and translation models (and that have coveredthe same source positions) can be recombined.
Because the DP search space grows13Computational Linguistics Volume 35, Number 1Figure 2Example of a word graph for the source German sentence was hast du gesagt?
(English referencetranslation: ?what did you say??
).exponentially with the size of the input, standard DP search is prohibitive, and we resortto a beam-search heuristic.4.1.1 Adaptation to the Interactive-Predictive Scenario.
The most important modificationis to rely on a word graph that represents possible translations of the given sourcesentence.
This word graph is generated once for each source sentence.
During theprocess of human?machine interaction the system makes use of this word graph inorder to complete the prefixes accepted by the human translator.
In other words, afterthe human translator has accepted a prefix string, the system finds the best path in theword graph associated with this prefix string so that it is able to complete the targetsentence.
Using the word graph in such a way, the system is able to interact with thehuman translator in a time efficient way.
In Och, Zens, and Ney (2003), an efficientalgorithm for interactive generation using word graphs was presented.
A word graphis a weighted directed acyclic graph, in which each node represents a partial translationhypothesis and each edge is labeled with a word of the target sentence and is weightedaccording to the language and translation model scores.
In Ueffing, Och, and Ney (2002),the authors give a more detailed description of word graphs and show how they can beeasily produced as a by-product of the search process.
An example of a word graph isshown in Figure 2.The computational cost of this approach is much lower, as the whole search for thetranslation must be carried out only once, and the generated word graph can be reusedfor further completion requests.For a fixed source sentence, if no pruning is applied in the production of the wordgraph, it represents all possible sequences of target words for which the posteriorprobability is greater than zero, according to the models used.
However, because ofthe pruning generally needed to render the problem computationally feasible, theresulting word graph only represents a subset of the possible translations.
Therefore,it may happen that the user sets prefixes which cannot be found in the word graph.
Tocircumvent this problem some heuristics need to be implemented.First, we look for the node with minimum edit distance to the prefix except forits last (partial) word.5 Then we select the completion path which starts with the last5 The edit distance concept for finding the prefix string in a word graph could be refined by casting the editdistance operations into a suitable probabilistic model.14Barrachina et al Statistical Computer-Assisted Translation(partial) word of the prefix and has the best backward score?this is the score associatedwith a path going from the node to the final node.
Now, because the original word graphmay not be compatible with the new information provided by the prefix, it might beimpossible to find a completion in this word graph due to incompatibility with thelast (partial) word in the prefix.
This problem can be solved to a certain degree bysearching for a completion of the last word with the highest probability using only thelanguage model.
This supplementary heuristic to the usual search increases the perfor-mance of the system, because some of the rejected words in the pruning process canbe recovered.A desirable feature of an IPMT system is the possibility of producing a list ofalternative target suffixes, instead of only one.
This feature can be easily added bycomputing the n-best hypotheses.
Of course, these n-best hypotheses do not refer tothe whole target sentence, but only to the suffixes.
However, the problem is that inmany cases the sentence hypotheses in the n-best list differ in only one or two words.Therefore, we introduce the additional requirement that the first four words of the n-best hypotheses must be different.4.2 Searching with Phrase-Based ModelsThe generation of the best translation with PBMs is similar to the one described in theprevious section.
Each hypothesis is composed of a prefix of the target sentence, a subsetof source positions that are aligned with the positions of the prefix of the target sentence,and a score.
In this case, we adopted an extension of the best-first strategy where thehypotheses are stored in several sorted lists, depending on which words in the sourcesentence have been translated.
This strategy is related to the well-known multi-stack-decoding algorithm (Berger et al 1996; Toma?s and Casacuberta 2004).
In each iteration,the algorithm extends the best hypothesis from each available list.While target words are always generated from left to right, there are two alter-natives in the source word extraction: Monotone search, which takes the source wordsfrom left to right, and non-monotone search, which can take source words in anyorder.4.2.1 Adaptation to the Interactive-Predictive Scenario.
Only a simple modification of thissearch algorithm is necessary: If the new extended hypothesis is not compatible withthe fixed target prefix, tp, then this hypothesis is not considered.
This compatibility isverified at the character level; therefore the user does not need to type the whole targetword at the end of the target prefix.In the interactive scenario, speed is a critical aspect.
In the PBM approach, monotonesearch is much faster than non-monotone search in the tasks which are considered in thiswork (Toma?s and Casacuberta 2006).
However, monotone search presents a problem forinteractive operation: If a user introduces a prefix that cannot be obtained in a monotoneway from the source, the search algorithm is not able to complete this prefix.
In orderto solve this problem without losing computational efficiency, we use the following ap-proach: Non-monotone search is used for target prefixes, whereas completions (suffixes)are generated using monotone search.As for AT models, a list of target suffixes can also be produced.
This list can beobtained easily by keeping the n-best hypotheses in each sorted list.
To avoid generatingvery similar hypotheses in the n-best list, we apply the following procedure: Startingfrom the n-best list resulting from the normal search, we first add hypotheses obtained15Computational Linguistics Volume 35, Number 1by translating a single untranslated word from the source, along with hypothesesconsisting of a single high-probability word according to the target language model; wethen re-order the hypotheses, maximizing the diversity at the beginning of the suffixes,and keep only the n first hypotheses in the re-ordered list.4.3 Searching with Stochastic Finite-State TransducersAs discussed by Pico?
and Casacuberta (2001), the computation of Equation (4) for SFSTsunder a maximum approximation (i.e., using maximization in Equation (17) insteadof the sum) amounts to a conventional Viterbi search.
The algorithm finds the mostprobable path among those paths in the SFST which are compatible with the sourcesentence s. The corresponding translation, t?, is simply obtained by concatenating thetarget strings of the edges of this path.4.3.1 Adaptation to the Interactive-Predictive Scenario.
Here, Equation (7) is used whereinthe optimization is performed over the set of target suffixes (completions) rather thanthe set of complete target sentences.
To solve this maximization problem, an approachsimilar to that proposed for AT in Section 4.1 has been adopted.First, given the source sentence, a word graph is extracted from the SFST.
In thiscase, the word graph is just (a pruned version of) the Viterbi search trellis obtained whentranslating the whole source sentence.
The main difference between the word graphsgenerated with ATs and SFSTs is how the nodes and edges are defined in each case.
Onthe one hand, the nodes are defined as partial hypotheses of the search procedure inthe AT approach, whereas the nodes in the case of SFSTs can be directly mapped intostates in the SFST representing a joint (source word/target string) language model.
Onthe other hand, the scores associated with the edges in the AT approach are computedfrom a combination of the language and translation models, whereas in the case ofSFSTs these scores simply come from the joint language model estimated by the GIATItechnique.Once the word graph has been generated, the search for the most probable com-pletion as stated in Equation (6) is carried out in two steps, in a similar way to thatexplained for the AT approach.
In this case, the computation entailed by both the edit-distance (prefix error-correcting) and the remaining search is significantly acceleratedby visiting the nodes in topological order and by the incorporation of the beam-searchtechnique (Amengual and Vidal 1998).
Moreover, the error-correcting algorithm takesadvantage of the incremental way in which the user prefix is generated, parsing onlythe new suffix appended by the user in the last interaction.It may be the case that a user prefix ends in an incomplete word during the inter-active translation process.
Therefore, it is necessary to start the translation completionwith a word whose prefix matches this unfinished word.
The proposed algorithm thussearches for such a word.
First, it considers the target words of the edges leavingthe nodes returned by the error-correcting algorithm.
If this initial search fails, thena matching word is looked up in the word-graph vocabulary.
Finally, as a last resort,the whole transducer vocabulary is taken into consideration to find a matching word;otherwise this incomplete word is treated as an entire word.This error-correcting algorithm returns a set of nodes from which the best comple-tion would be selected according to the best backward score.
Moreover, n-best com-pletions can also be produced.
Among many weighted-graph n-best path algorithmswhich are available, the recursive enumeration algorithm presented in Jime?nez and16Barrachina et al Statistical Computer-Assisted TranslationMarzal (1999) was adopted for its simplicity in calculating best paths on demand and itssmooth integration with the error-correcting algorithm.5.
Experimental FrameworkThe models and search procedures introduced in the previous sections were assessedthrough a series of IPMT experiments with different corpora.
These corpora, along withthe corresponding pre- and post-processing and assessment procedures, are presentedin this section.5.1 Pre- and Post-ProcessingUsually, MT models are trained on a pre-processed version of an original corpus.
Pre-processing provides a simpler representation of the training corpus which makes tokenor word forms more homogeneous.
In this way automatic training of the MT models isboosted, and the amount of computation decreases.The pre-processing steps are: tokenization, removing unnecessary case information,and tagging some special tokens like numerical sequences, e-mail addresses, and URLs(?categorization?).
In translation from a source language to a target language, there aresome words which are translated identically (because they have the same spelling inboth languages).
Therefore, we identify them in the corpus and replace them with somegeneric tags to help the translation system.Post-processing takes place after the translation in order to hide the internal repre-sentation of the text from the user.
Thus, the user will only work with an output whichis very similar to human-generated texts.
In detail, the post-processing steps are: de-tokenization, true-casing, and replacing the tags with their corresponding words.In an IPMT scenario, the pre-/post-processing must run in real-time and should bereversible as much as possible.
In each human?machine interaction, the current prefixhas to be pre-processed for the interactive-predictive engine and then the generatedcompletion has to be post-processed for the user.
It is crucial that the pre-processing ofprefixes is fully compatible with the training corpus.5.2 Xerox and EU CorporaSix bilingual corpora were used for two different tasks and three different languagepairs in the framework of the TT2 project (SchlumbergerSema S.A. et al 2001).The language pairs involved were English?Spanish, English?French, and English?German (Khadivi and Goutte 2003), and the tasks were Xerox (Xerox printer manuals)and EU (Bulletin of the European Union).The three Xerox corpora were obtained from different user manuals for Xerox print-ers (SchlumbergerSema S.A. et al 2001).
The main features of these corpora are shownin Table 1.
Dividing the corpora into training and test sets was performed by randomlyselecting (without replacement) a specified amount of test sentences and leaving theremaining ones for training.
It is worth noting that the manuals were not the same ineach pair of languages.
Even though all training and test sets have similar size, thisprobably explains why the perplexity varies considerably over the different languagepairs.
The vocabulary size was computed using the tokenized and true-case corpus.The three bilingual EU corpora were extracted from the Bulletin of the EuropeanUnion, which exists in all official languages of the European Union (Khadivi and Goutte17Computational Linguistics Volume 35, Number 1Table 1The Xerox corpora.
For all the languages, the training/test full-sentence overlap and the rate ofout-of-vocabulary test-set words were less than 10% and 1%, respectively.
Trigram models wereused to compute the test word perplexity.
(K and M denote thousands and millions,respectively.
)English/Spanish English/German English/FrenchTrain Sent.
pairs (K) 56 49 53Running words (M) 0.7/0.7 0.6/0.5 0.6/0.7Vocabulary (K) 15/17 14/25 14/16TestSentences (K) 1.1 1.0 1.0Running words (K) 8/10 12/12 11/12Running chars.
(K) 46/59 63/73 56/65Perplexity 99/58 57/93 109/702003) and is publicly available on the Internet.
The corpora used in the experimentswhich are described subsequently were again acquired and processed in the frameworkof the TT2 project.
The main features of these corpora are shown in Table 2.
Thevocabulary size and the training and test set partitions were obtained in a similar wayas with the Xerox corpora.5.3 AssessmentIn all the experiments reported in this article, system performance is assessed bycomparing test sentence translations produced by the translation systems with thecorresponding target language references of the test set.
Some of the computed assess-ment figures measure the quality of the translation engines without any system?userinteractivity: Word error rate (WER): The minimum number of substitution, insertion,and deletion operations needed to convert the word strings produced bythe translation system into the corresponding single-reference wordstrings.
WER is normalized by the overall number of words in thereference sentences (Och and Ney 2003).Table 2The EU corpora.
For all the languages, the training/test full-sentence overlap and the rate ofout-of-vocabulary test-set words were less than 3% and 0.2%, respectively.
Trigram models wereused to compute the test word perplexity.
(K and M denote thousands and millions,respectively.
)English/Spanish English/German English/FrenchTrain Sent.
pairs (K) 214 223 215Running words (M) 5.2/5.9 5.7/5.4 5.3/6.0Vocabulary (K) 84/97 86/153 84/91TestSentences (K) 0.8 0.8 0.8Running words (K) 20/23 20/19 20/23Running chars.
(K) 119/135 120/134 119/134Perplexity 58/46 57/87 58/4518Barrachina et al Statistical Computer-Assisted Translation Bilingual evaluation understudy (BLEU): This is based on the coverage ofn-grams of the hypothesized translation which occur in the referencetranslations (Papineni et al 2001).Other assessment figures are aimed at estimating the effort needed by a humantranslator to produce correct translations using the interactive system.
To this end, thetarget translations which a real user would have in mind are simulated by the givenreferences.
The first translation hypothesis for each given source sentence is comparedwith a single reference translation and the longest common character prefix (LCP) isobtained.
The first non-matching character is replaced by the corresponding referencecharacter and then a new system hypothesis is produced.
This process is iterated untila full match with the reference is obtained.Each computation of the LCP would correspond to the user looking for the nexterror and moving the pointer to the corresponding position of the translation hypothesis.Each character replacement, on the other hand, would correspond to a keystroke ofthe user.
If the first non-matching character is the first character of the new systemhypothesis in a given iteration, no LCP computation is needed; that is, no pointermovement would be made by the user.
Bearing this in mind, we define the followinginteractive-predictive performance measures: Keystroke ratio (KSR): Number of keystrokes divided by the total numberof reference characters. Mouse-action ratio (MAR): Number of pointer movements plus one morecount per sentence (aimed at simulating the user actionneeded to accept the final translation), divided by the total number ofreference characters. Keystroke and mouse-action ratio (KSMR): KSR plus MAR.Note that KSR estimates only the user?s actions on the keyboard whereas MARestimates actions for which the user would typically use the mouse.
From a userpoint of view the two types of actions are different and require different types ofeffort (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
In any case, as anapproximation, KSMR accounts for both KSR and MAR, assuming that both actionsrequire a similar effort.In the case of SMT systems, it is well known that an automatically computedquality measure like BLEU correlates quite well with human judgment (Callison-Burch,Osborne, and Koehn 2006).
In the case of IPMT, we should keep in mind that themain goal of (automatic) assessment is to estimate the effort of the human translator.Moreover, translation quality is not an issue here, because the (simulated) humanintervention ensures ?perfect?
translation results.
The important question is whetherthe (estimated) productivity of the human translator can really be increased or not bythe IPMT approach.
In order to answer this question, the KSR and KSMR measures willbe used in the IPMT experiments to be reported in the next section.In order to show the statistical significance of the results, all the assessment figuresreported in the next section are accompanied by the corresponding 95% confidenceintervals.
These intervals have been computed using bootstrap sampling techniques, asproposed by Bisani and Ney (2004), Koehn (2004), and Zhang and Vogel (2004).19Computational Linguistics Volume 35, Number 16.
ResultsTwo types of results are reported for each corpus and for each translation approach.The first are conventional MT results, obtained as a reference to give an idea of the?classical?
MT difficulty of the selected tasks.
The second aim is to assess the interactiveMT (IPMT) approach proposed in this article.The results are presented in different subsections.
The first two subsections presentthe MT and IPMT results for the 1-best translation obtained by the different techniquesin the Xerox and EU tasks, respectively.
The third subsection presents further IPMTresults for the 5-best translations on a single pair of languages.Some of these results may differ from results presented in previous works (Cubelet al 2003; Och, Zens, and Ney 2003; Civera et al 2004a; Cubel et al 2004; Benderet al 2005).
The differences are due to variations in the pre-/post-processing proceduresand/or recent improvements of the search techniques used by the different systems.6.1 Experiments with the Xerox CorporaIn this section, the translation results obtained using ATs, PBMs, and SFSTs for all sixlanguage pairs of the Xerox corpus are reported.
Word-based trigram and class-basedfive-gram target-language models were used for the AT models (the parameters of thelog-linear model are tuned so as to minimize WER on a development corpus); word-based trigram target-language models were used for PBMs and trigrams were used toinfer GIATI SFSTs.Off-line MT Results.
MT results with ATs, PBMs, and SFSTs are presented in Figure 3.Results obtained using the PBMs are slightly but consistently better that those achievedusing the other models.
In general, the different techniques perform similarly for thevarious translation directions.
However, the English?Spanish language pair is the onefor which the best translations can be produced.IPMT Results.
Performance has been measured in terms of KSRs and MARs (KSR andMAR are represented as the lower and upper portions of each bar, respectively, andKSMR is the whole bar length).
The results are shown in Figure 4.Figure 3Off-line MT results (BLEU and WER) for the Xerox corpus.
Segments above the bars show the95% confidence intervals.
En = English; Sp = Spanish; Fr = French; Ge = German.20Barrachina et al Statistical Computer-Assisted TranslationFigure 4IPMT results for the Xerox corpus.
In each bar, KSR is represented by the lower portion, MAR bythe upper portion, and KSMR is the whole bar.
Segments above the bars show the 95%confidence intervals.
En = English; Sp = Spanish; Fr = French; Ge = German.According to these results, a human translator assisted by an AT-based or a SFST-based interactive system would only need an effort equivalent to typing about 20% ofthe characters in order to produce the correct translations for the Spanish to Englishtask; or even less than 20% if a PBM-based system is used.For the Xerox task, off-line MT performance and IPMT results show similar tenden-cies.
The PBMs show better performance for both the off-line MT and for the IPMTassessment figures.
The AT and SFST models perform more or less equivalently.
Inboth scenarios, the best results were achieved for the Spanish?English language pairfollowed by French?English and German?English.The computing times needed by all the systems involved in these experiments werewell within the range of the on-line operational requirements.
The average initial timefor each source test sentence was very low (less than 50 msec) for PBMs and SFSTsand adequate for ATs (772 msec).
In the case of ATs and SFSTs, this included the timerequired for the generation of the initial word-graph of each sentence.
Moreover, themost critical times incurred in the successive IPMT iterations were very low in allthe cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs.
Note, however,that these average times are not exactly comparable because of the differences in thecomputer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 GhzPentium for ATs, PBMs, and SFSTs, respectively).6.2 Experiments with the EU CorporaThe translation results using the AT, PBM, and SFST approaches for all six languagepairs of the EU corpus are reported in this section.
As for the Xerox corpora, in the ATexperiments, word-based trigram and class-based five-gram target-language modelswere used; in the PBM experiments, word-based trigram and class-based five-gramtarget-language models were also used and five-grams were used to infer GIATI SFSTs.Off-line MT Results.
Figure 5 presents the results obtained using ATs, PBMs, and SFSTs.Generally speaking, the results are comparable to those obtained on the Xerox corpuswith the exception of the English?Spanish language pair, which were better.
With thesecorpora, the best results were obtained with the ATs and PBMs for all the pairs and thebest translation direction was French-to-English with all the models used.21Computational Linguistics Volume 35, Number 1Figure 5Off-line MT results (BLEU and WER) for the EU corpus.
Segments above the bars show the 95%confidence intervals.
En = English; Sp = Spanish; Fr = French; Ge = German.IPMT Results.
Figure 6 shows the performance of the AT, PBM, and SFST systems interms of KSRs and MARs in a similar way as for the Xerox corpora.As in the MT experiments, the results are comparable to those obtained on the Xeroxcorpus, with the exception of the English?Spanish pair.
Similarly, as in MT, the bestresults were obtained for the French-to-English translation direction.Although EU is a more open-domain task, the results demonstrate again the poten-tial benefit of computer-assisted translation systems.
Using PBMs, a human translatorwould only need an effort equivalent to typing about 20% of the characters in orderto produce the correct translations for French-to-English translation direction, whereasfor ATs and SFSTs the effort would be about 30%.
For the other language pairs, theefforts would be about 20?30% and 35% of the characters for PBMs and ATs/SFSTs,respectively.The systemwise correlation between MT and IPMT results on this corpus is notas clear as in the Xerox case.
One possible cause is the much larger size of the EUcorpus compared to the Xerox corpus.
In order to run the EU experiments within rea-sonable time limits, all the systems have required the use of beam search and/or otherFigure 6IPMT results for the EU corpus.
In each bar, KSR is represented by the lower portion, MAR bythe upper portion and KSMR is the whole bar.
Segments above the bars show the 95%confidence intervals.
En = English; Sp = Spanish; Fr = French; Ge = German.22Barrachina et al Statistical Computer-Assisted TranslationTable 3IPMT results (%) for the Xerox corpus (English?Spanish) using ATs, PBMs, and SFSTs for the1-best hypothesis and 5-best hypotheses.
95% confidence intervals are shown.1-best 5-bestTechnique KSR KSMR KSR KSMRAT 12.9?0.9 23.2?1.3 11.1?0.8 20.3?1.2PBM 8.9?0.8 16.7?1.2 7.3?0.6 15.4?1.1SFST 13.0?1.0 21.8?1.4 11.2?1.0 19.2?1.3suboptimal pruning techniques, although this was largely unnecessary for the Xeroxcorpus.
Clearly, the pruning effects are different in the off-line (MT) and the on-line(IPMT) search processes and the differences may lead to wide performance variationsfor the AT, PBM, and SFST approaches.Nevertheless, as can be seen in Bender et al (2005), the degradation in systemperformance due to pruning is generally not too substantial and sufficiently accuratereal-time interactive operation could also be achieved in the EU task with the threesystems tested.6.3 Results with n-Best HypothesesFurther experiments were carried out to study the usefulness of n-best hypotheses inthe interactive framework.
In this scenario, the user can choose one out of n proposedtranslation suffixes and then proceed as in the usual IPMT paradigm.
As with theprevious experiments, the automated evaluation is based on a selected target sentencethat best matches a prefix of the reference translation in each IPMT iteration (thereforeKSR is minimized).Here, only IPMT results for the English-to-Spanish translation direction are re-ported for both Xerox and EU tasks, using a list of the five best translations.
These resultsare shown in Tables 3 and 4.In all the cases there is a clear and significant accuracy improvement when movingfrom single-best to 5-best translations.
This gain in translation quality diminishes in alog-wise fashion as we increase the number of best translations.
From a practical pointof view, the improvements provided by using n-best completions would come at thecost of the user having to ponder which of these completions is more suitable.
In areal operational environment, this additional user effort may or may not outweigh theTable 4IPMT results (%) for the EU corpus (English?Spanish) using ATs, PBMs, and SFSTs for the 1-besthypothesis and 5-best hypotheses.
95% confidence intervals are shown.1-best 5-bestTechnique KSR KSMR KSR KSMRAT 20.2?0.9 32.6?1.3 18.5?0.8 29.9?1.2PBM 16.3?0.7 27.8?1.1 13.2?0.6 25.0?1.1SFST 21.3?0.9 33.0?1.3 19.3?0.9 29.9?1.323Computational Linguistics Volume 35, Number 1benefits of the n-best increased accuracy.
Consequently, this feature should be offered tothe users as an option.7.
Practical IssuesIPMT results reported in the previous section provide reasonable estimations of potentialsavings of human translator effort, assuming that the goal is to obtain high qualitytranslations.
In real work, however, several practical issues not discussed in this articlemay significantly affect the actual system usability and overall user productivity.One of the most obvious issues is that a carefully designed graphical user interface(GUI) is needed to let the users actually be in command of the translation process, sothat they really feel the system is assisting them rather than the other way around.
Inaddition, an adequate GUI has to provide adequate means for the users to easily andintuitively change at will IPMT engine parameters that may have an impact on theirway of working with the system.
To name just a few: The maximum length of systemhypotheses, the value of n for n-best suggestions, or the ?interaction step granularity?
;that is, whether the system should react at each user keystroke, or at the end of eachcomplete typed word, or after a sufficiently long typing pause, and so on.Clearly, all these important issues are beyond the scope of the present article.
Butwe can comment that, in the TT2 project, complete prototypes of some of the systemspresented in this article, including the necessary GUI, were actually implemented andthoroughly evaluated by professional human translators in their working environ-ment (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).The results of these field tests showed that the actual productivity depended notonly on the individual translators, but also on the given test texts.
In cases where thesetexts were quite unrelated to the training data, the system did not significantly helpthe human translators to increase their productivity.
However, when the test texts werereasonably well related to the training data, high productivity gains were registered?close to what could be expected according to the KSR/MAR empirical results.8.
Concluding RemarksThe IPMT paradigm proposed in this article allows for a close collaboration between ahuman translator and a machine translation system.
This paradigm entails an iterativeprocess where, in each iteration, a data-driven machine translation engine suggests acompletion for the current prefix of a target sentence which a human translator canaccept, modify, or ignore.This idea was originally proposed in the TransType project (Langlais, Foster, andLapalme 2000), where a simple engine was used which only supported single-tokensuggestions.
Furthering these ideas, in the TransType2 project (SchlumbergerSema S.A.et al 2001), state-of-the-art statistical machine translation systems have been developedand integrated in the IPMT framework.In a laboratory environment, results on two different tasks suggest that the pro-posed techniques can reduce the typing effort needed to produce a high-quality transla-tion of a given source text by as much as 80% with respect to the effort needed to simplytype the whole translation.
In real conditions, a high productivity gain was achieved inmany cases.We have studied here IPMT from the point of view of a standalone CAT tool.Nevertheless, IPMT can of course be easily and conveniently combined with otherpopular translator workbench tools.
More specifically, IPMT lends itself particularly24Barrachina et al Statistical Computer-Assisted Translationwell to addressing the typical lack of generalization capabilities of translation memories.When used as a CAT tool, translation memories allow the human translator to keepproducing increasingly long segments of correct target text.
Clearly, these segments canbe used by an IPMT engine to suggest to the translator possible translations for sourcetext segments that are not found in the translation memories as exact matches.AcknowledgmentsThis work has been partially supported bythe ST Programme of European Union undergrant IST-2001-32091, by the Spanish projectTIC?2003-08681-C02-02, and the Spanishresearch programme ConsoliderIngenio-2010 CSD2007-00018.
The authorswish to thank the anonymous reviewers fortheir criticisms and suggestions.ReferencesAmengual, J. C., J. M.
Bened?
?, A. Castan?o,A.
Castellanos, V. M. Jime?nez, D. Llorens,A.
Marzal, M. Pastor, F. Prat, E. Vidal, andJ.
M. Vilar.
2000.
The EuTrans-I speechtranslation system.
Machine Translation,15:75?103.Amengual, J. C. and E. Vidal.
1998.
Efficienterror-correcting Viterbi parsing.
IEEETransactions on Pattern Analysis and MachineIntelligence, 20(10):1109?1116.Bender, O., S. Hasan, D. Vilar, R. Zens, andH.
Ney.
2005.
Comparison of generationstrategies for interactive machinetranslation.
In Proceedings of the 10thAnnual Conference of the EuropeanAssociation for Machine Translation (EAMT05), pages 33?40, Budapest.Berger, A. L., P. F. Brown, S. A. Della Pietra,V.
J. Della Pietra, J. R. Gillett, A. S. Kehler,and R. L. Mercer.
1996.
Languagetranslation apparatus and method of usingcontext-based translation models.
UnitedStates Patent No.
5510981, April.Berstel, J.
1979.
Transductions and Context-FreeLanguages.
B. G. Teubner, Stuttgart.Bisani, M. and H. Ney.
2004.
Bootstrapestimates for confidence intervals in ASRperformance evaluation.
In Proceedings ofthe International Conference on Acoustic,Speech and Signal Processing (ICASSP 04),volume 1, pages 409?412, Montreal.Bowker, L. 2002.
Computer-Aided TranslationTechnology: A Practical Introduction,chapter 5: Translation-memory systems.Didactics of Translation.
University ofOttawa Press, pages 92?127.Brown, P. F., J. Cocke, S. A. Della Pietra,V.
J. Della Pietra, F. Jelinek, J. D. Lafferty,R.
L. Mercer, and P. S. Roosin.
1990.A statistical approach to machinetranslation.
Computational Linguistics,16(2):79?85.Brown, P. F., S. A. Della Pietra, V. J.Della Pietra, and R. L. Mercer.
1993.
Themathematics of statistical machinetranslation: Parameter estimation.Computational Linguistics, 19(2):263?310.Callison-Burch, C., M. Osborne, andP.
Koehn.
2006.
Re-evaluating the role ofBLEU in machine translation research.
InProceedings of the 10th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL 06),pages 249?256, Trento.Casacuberta, F., H. Ney, F. J. Och, E. Vidal,J.
M. Vilar, S. Barrachina, I.
Garc??a-Varea,D.
Llorens, C.
Mart?
?nez, S. Molau,F.
Nevado, M. Pastor, D.
Pico?, A. Sanchis,and C. Tillmann.
2004a.
Some approachesto statistical and finite-statespeech-to-speech translation.
ComputerSpeech and Language, 18:25?47.Casacuberta, F. and E. Vidal.
2004.
Machinetranslation with inferred stochasticfinite-state transducers.
ComputationalLinguistics, 30(2):205?225.Casacuberta, F. and E. Vidal.
2007.
Learningfinite-state models for machine translation.Machine Learning, 66(1):69?91.Casacuberta, F., E. Vidal, and D. Pico?.
2005.Inference of finite-state transducers fromregular languages.
Pattern Recognition,38:1431?1443.Casacuberta, F., E. Vidal, A. Sanchis, andJ.
M. Vilar.
2004b.
Pattern recognitionapproaches for speech-to-speechtranslation.
Cybernetic and Systems: anInternational Journal, 35(1):3?17.Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,S.
Barrachina, E. Vidal, F. Casacuberta,D.
Pico?, and J. Gonza?lez.
2004a.
Frommachine translation to computer assistedtranslation using finite-state models.
InProceedings of the Conference on EmpiricalMethods for Natural Language Processing(EMNLP 04), pages 349?356, Barcelona.Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,S.
Barrachina, F. Casacuberta, E. Vidal,D.
Pico?, and J. Gonza?lez.
2004b.
A syntacticpattern recognition approach to computerassisted translation.
In Advances in25Computational Linguistics Volume 35, Number 1Statistical, Structural and Syntactical PatternRecognition, Proceedings of the Joint IAPRInternational Workshops on Syntactical andStructural Pattern Recognition (SSPR 04)and Statistical Pattern Recognition(SPR 04)), Lisbon, Portugal, August 18?20,volume 3138 of Lecture Notes in ComputerScience.
Springer-Verlag, Heidelberg,pages 207?215.Cubel, E., J. Civera, J. M. Vilar, A. L. Lagarda,S.
Barrachina, E. Vidal, F. Casacuberta,D.
Pico?, J. Gonza?lez, and L.
Rodr??guez.2004.
Finite-state models for computerassisted translation.
In Proceedings of the16th European Conference on ArtificialIntelligence (ECAI 04), pages 586?590,Valencia.Cubel, E., J. Gonza?lez, A. Lagarda,F.
Casacuberta, A. Juan, and E. Vidal.
2003.Adapting finite-state translation to theTransType2 project.
In Proceedings of theJoint Conference Combining the 8thInternational Workshop of the EuropeanAssociation for Machine Translation and the4th Controlled Language Applications Workshop(EAMT-CLAW 03), pages 54?60, Dublin.Foster, G. 2002.
Text Prediction for Translators.Ph.D.
thesis, Universite?
de Montre?al,Canada.Foster, G., P. Isabelle, and P. Plamondon.1997.
Target-text mediated interactivemachine translation.
Machine Translation,12(1?2):175?194.Isabelle, P. and K. Church.
1997.
Special issueon new tools for human translators.Machine Translation, 12(1?2).Jelinek, F. 1998.
Statistical Methods for SpeechRecognition.
The MIT Press, Cambridge,MA.Jime?nez, V. M. and A. Marzal.
1999.Computing the k shortest paths: a newalgorithm and an experimentalcomparison.
In Algorithm Engineering:Proceedings of the 3rd InternationalWorkshop (WAE 99), London, UK, July 19?21,volume 1668 of Lecture Notes in ComputerScience.
Springer-Verlag, Heidelberg,pages 15?29.Kay, M. 1997.
The proper place of men andmachines in language translation.
MachineTranslation, 12:3?23.
[This article firstappeared as a Xerox PARC Working Paperin 1980].Khadivi, S. and C. Goutte.
2003.
Tools forcorpus alignment and evaluation of thealignments (deliverable d4.9).
Technicalreport, TransType2 (IST-2001-32091).Khadivi, S., R. Zens, and H. Ney.
2006.Integration of speech to computer-assistedtranslation using finite-state automata.In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguisticsand 21th International Conference onComputational Linguistics (COLING/ACL06), pages 467?474, Sydney.Khadivi, S., A. Zolnay, and H. Ney.
2005.Automatic text dictation incomputer-assisted translation.
InProceedings of the European Conference onSpeech Communication and Technology,(INTERSPEECH 05-EUROSPEECH),pages 2265?2268, Lisbon.Koehn, P. 2004.
Statistical significancetests for machine translation evaluation.In Proceedings of the Conference onEmpirical Methods for Natural LanguageProcessing (EMNLP 04), pages 388?395,Barcelona.Koehn, P., F. J. Och, and D. Marcu.
2003.Statistical phrase-based translation.
InProceedings of the 2003 Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL 03),pages 127?133, Edmonton.Langlais, P., G. Foster, and G. Lapalme.
2000.TransType: a computer-aided translationtyping system.
In Proceedings of theNAACL/ANLP Workshop on EmbeddedMachine Translation Systems, pages 46?52,Seattle, WA.Langlais, P., G. Lapalme, and M. Loranger.2002.
Transtype: Development-evaluationcycles to boost translator?s productivity.Machine Translation, 15(4):77?98.Macklovitch, E. 2006.
TransType2: The lastword.
In Proceedings of the 5th InternationalConference on Languages Resources andEvaluation (LREC 06), pages 167?172,Genoa.Macklovitch, E., N. T. Nguyen, and R. Silva.2005.
User evaluation report.
Technicalreport, TransType2 (IST-2001-32091).Marcu, D. and W. Wong.
2002.
Aphrase-based, joint probability modelfor statistical machine translation.In Proceedings of the Conference onEmpirical Methods for Natural LanguageProcessing (EMNLP 02), pages 133?139,Philadelphia, PA.Ney, H., S. Nie?en, F. Och, H. Sawaf,C.
Tillmann, and S. Vogel.
2000.Algorithms for statistical translation ofspoken language.
IEEE Transactions onSpeech and Audio Processing, 8(1):24?36.Och, F. J.
1999.
An efficient method fordetermining bilingual word classes.
InProceedings of the 9th Conference of theEuropean Chapter of the Association for26Barrachina et al Statistical Computer-Assisted TranslationComputational Linguistics (EACL 99),pages 71?76, Bergen.Och, F. J. and H. Ney.
2003.
A systematiccomparison of various statisticalalignment models.
ComputationalLinguistics, 29(1):19?51.Och, F. J. and H. Ney.
2004.
The alignmenttemplate approach to statistical machinetranslation.
Computational Linguistics,30(4):417?450.Och, F. J., R. Zens, and H. Ney.
2003.Efficient search for interactive statisticalmachine translation.
In Proceedings ofthe 10th Conference of the European Chapterof the Association for ComputationalLinguistics (EACL 03), pages 387?393,Budapest.Papineni, K., S. Roukos, T. Ward, andW.
Zhu.
2001.
BLEU: a method forautomatic evaluation of machinetranslation.
Technical Report RC22176,Thomas J. Watson Research Center.Pico?, D. and F. Casacuberta.
2001.
Somestatistical-estimation methods forstochastic finite-state transducers.
MachineLearning, 44:121?142.Press, W. H., S. A. Teukolsky, W. T.Vetterling, and B. P. Flannery.
2002.Numerical Recipes in C++: The Art ofScientific Computing.
Cambridge UniversityPress, Cambridge, UK.SchlumbergerSema S.A., Intituto Tecnolo?gicode Informa?tica, Rheinisch Westfa?lischeTechnische Hochschule Aachen Lehrstulfu?r Informatik VI, Recherche Applique?een Linguistique Informatique LaboratoryUniversity of Montreal, Celer Soluciones,Socie?te?
Gamma, and XeroxResearch Centre Europe.
2001.
TT2.TransType2?computer-assistedtranslation.
Project technical annex.Information Society Technologies (IST)Programme, IST-2001-32091.Sen, Z., Ch.
Zhaoxiong, and H. Heyan.
1997.Interactive approach in machine translationsystems.
In Proceedings of IEEE InternationalConference on Intelligent Processing Systems(ICIPS 97), pages 1814?1819, Beijing.Slocum, J.
1985.
A survey of machinetranslation: Its history, current status andfuture prospects.
Computational Linguistics,11(1):1?17.Somers, H., 2003.
Computers and Translation: aTranslator?s Guide, chapter 3: Translationmemory systems.
John Benjamins,Amsterdam, pages 31?48.Toma?s, J. and F. Casacuberta.
2001.Monotone statistical translation usingword groups.
In Proceedings of the MachineTranslation Summit VIII (MT SUMMITVIII), pages 357?361, Santiago deCompostela.Toma?s, J. and F. Casacuberta.
2003.Combining phrase-based andtemplate-based alignment models instatistical translation.
In Pattern Recognitionand Image Analysis, Proceedings of the FirstIberian Conference (IbPRIA 03), Puertode Andratx, Mallorca, Spain, June 4-6,volume 2652 of Lecture Notes in ComputerScience.
Springer-Verlag, Heidelberg,pages 1020?1031.Toma?s, J. and F. Casacuberta.
2004.
Statisticalmachine translation decoding usingtarget word reordering.
In Advances inStatistical, Structural and Syntactical PatternRecognition, Proceedings of the Joint IAPRInternational Workshops on Syntacticaland Structural Pattern Recognition(SSPR 04) and Statistical Pattern Recognition(SPR 04), Lisbon, Portugal, August 18?20,volume 3138 of Lecture Notes in ComputerScience.
Springer-Verlag, Heidelberg,pages 734?743.Toma?s, J. and F. Casacuberta.
2006.
Statisticalphrase-based models for interactivecomputer-assisted translation.
InProceedings of the 44th Annual Meetingof the Association for ComputationalLinguistics and 21th InternationalConference on Computational Linguistics(COLING/ACL 06), pages 835?841, Sydney.Toma?s, J. and F. Casacuberta.
2007.
Apattern recognition approach tomachine translation: Monotone andnon-monotone phrase-based statisticalmodels.
Technical Report DSIC-II/18/07,Departamento de Sistemas Informa?ticos yComputacio?n, Universidad Polite?cnicade Valencia.Tomita, M. 1985.
Feasibility study ofpersonal/interactive machine translationsystems.
In Proceedings of the FirstInternational Conference on Theoreticaland Methodological Issues in MachineTranslation (TMI 85), pages 289?297,New York, NY.Ueffing, N., F. J. Och, and H. Ney.
2002.Generation of word graphs in statisticalmachine translation.
In Proceedings ofthe Conference on Empirical Methods forNatural Language Processing (EMNLP 02),pages 156?163, Philadelphia, PA.Vidal, E. 1997.
Finite-state speech-to-speechtranslation.
In Proceedings of theInternational Conference on Acoustic,Speech and Signal Processing (ICASSP 97),volume 1, pages 111?114, Munich.27Computational Linguistics Volume 35, Number 1Vidal, E. and F. Casacuberta.
2004.
Learningfinite-state models for machine translation.In Grammatical Inference: Algorithms andApplications, Proceedings of the 7thInternational Coloquium on GrammaticalInference (ICGI 04), Athens, Greece,October 11?13, volume 3264 of LectureNotes in Artificial Intelligence.
Springer,Heidelberg, pages 16?27.Vidal, E., F. Casacuberta, L.
Rodr??guez,J.
Civera, and C.
Mart??nez.
2006.Computer-assisted translation usingspeech recognition.
IEEE Transactionson Speech and Audio Processing,14(3):941?951.Vidal, E., F. Thollard, F. CasacubertaC.
de la Higuera, and R. Carrasco.
2005.Probabilistic finite-state machines?part II.
IEEE Transactions on PatternAnalysis and Machine Intelligence,27(7):1025?1039.Whitelock, P. J., M. McGee Wood, B. J.Chandler, N. Holden, and H. J. Horsfall.1986.
Strategies for interactive machinetranslation: The experience andimplications of the UMIST Japaneseproject.
In Proceedings of the 11thInternational Conference on ComputationalLinguistics (COLING 86), pages 329?334,Bonn.Yamron, J., J. Baker, P. Bamberg,H.
Chevalier, T. Dietzel, J. Elder,F.
Kampmann, M. Mandel, L. Manganaro,T.
Margolis, and E. Steele.
1993.LINGSTAT: an interactive, machine-aidedtranslation system.
In Proceedings of theWorkshop on Human Language Technology,pages 191?195, Princeton, NJ.Zajac, R. 1988.
Interactive translation: A newapproach.
In Proceedings of the 12thInternational Conference on ComputationalLinguistics (COLING 88), pages 785?790,Budapest.Zens, R. and H. Ney.
2004.
Improvementsin phrase-based statistical machinetranslation.
In Proceedings of the HumanLanguage Technology Conference / NorthAmerican Chapter of the Association forComputational Linguistics Annual Meeting(HLT-NAACL 04), pages 257?264,Boston, MA.Zens, R., F. J. Och, and H. Ney.
2002.Phrase-based statistical machinetranslation.
In Advances in ArtificialIntelligence.
25th Annual German Conferenceon Artificial Intelligence (KI 02), Aachen,Germany, September 16?22, Proceedings,volume 2479 of Lecture Notes on ArtificialIntelligence.
Springer Verlag, Heidelberg,pages 18?32.Zhang, Y. and S. Vogel.
2004.
Measuringconfidence intervals for the machinetranslation evaluation metrics.
InProceedings of the Tenth InternationalConference on Theoretical andMethodological Issues in MachineTranslation (TMI 04), pages 294?301,Baltimore, MD.28
