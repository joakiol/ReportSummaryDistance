Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1087?1096,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEncoding World Knowledge in the Evaluation of Local CoherenceMuyu Zhang1?, Vanessa Wei Feng2, Bing Qin1, Graeme Hirst2, Ting Liu1and Jingwen Huang11Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, Harbin, China2Department of Computer Science, University of Toronto, Toronto, ON, Canada{myzhang,qinb,tliu,jwhuang}@ir.hit.edu.cn{weifeng,gh}@cs.toronto.eduAbstractPrevious work on text coherence was primar-ily based on matching multiple mentions ofthe same entity in different parts of the text;therefore, it misses the contribution from se-mantically related but not necessarily coref-erential entities (e.g., Gates and Microsoft).In this paper, we capture such semantic relat-edness by leveraging world knowledge (e.g.,Gates is the person who created Microsoft),and use two existing evaluation frameworks.First, in the unsupervised framework, we in-troduce semantic relatedness as an enrichmentto the original graph-based model of Guin-audeau and Strube (2013).
In addition, weincorporate semantic relatedness as additionalfeatures into the popular entity-based modelof Barzilay and Lapata (2008).
Across bothframeworks, our enriched model with seman-tic relatedness outperforms the original meth-ods, especially on short documents.1 IntroductionIn a well-written document, sentences are organizedand presented in a logical and coherent form, whichmakes the text fluent and easily understood.
There-fore, coherence is a fundamental aspect of high textquality, and the evaluation of coherence is a crucialcomponent of many NLP applications, such as essayscoring (Miltsakaki and Kukich, 2004), story gener-ation (McIntyre and Lapata, 2010), and documentsummarization (Barzilay et al, 2002).
?This work was partly done while the first author was vis-iting University of Toronto.A particularly popular model for evaluating textcoherence is the entity-based local coherence modelof Barzilay and Lapata (2008) (B&L), which ex-tracts mentions of entities in adjacent sentences, andcaptures local coherence in terms of the transitionsin the grammatical role of each mention.
Follow-ing this direction, a number of extensions have beenproposed (Elsner and Charniak, 2008; Elsner andCharniak, 2011; Lin et al, 2011; Feng et al, 2014),the majority of which focus on enriching the origi-nal entity features.
An exception is the unsupervisedmodel of Guinaudeau and Strube (2013) (G&S),which converts the document into a graph of sen-tences, and evaluates the text coherence by comput-ing the average out-degree over the entire graph.However, despite the apparent success of thesemethods, they rely merely on matching mentions ofthe same entity, but neglect the contribution fromsemantically related but not necessarily coreferen-tial entities.
For example, the text in Figure 1a1hasno common entity in s2and s3.
However, the tran-sition between them is perfectly coherent, becausethere exists close semantic relatedness between twodistinct entities, Gates in s2and Microsoft in s3,which can be captured by the world knowledge thatGates is the person who created Microsoft (repre-sented by Gates-create-Microsoft).
In fact, the is-sue of absence of common entities between adjacentsentences is quite prevalent.
Analyzing the CoNLL2012 dataset (Pradhan et al, 2012), we found that42.34% of the time, adjacent sentences do not sharecommon entities.
As a result, methods which relyon strict entity matching would fail on these cases.1Based on a news item: http://www.cnbc.com/id/1015769261087s1: In 1980, [ Gates ]S licensed [ 86-DOS ]O from [ Tim Paterson ]X for$50,000, which marketed it as [ PC-DOS ]X.s2: [ Gates' smartest move ]S was retaining [ ownership of the sourcecode ]O of what he and [ Allen ]X would develop as [ MS-DOS ]X.s3: [ Microsoft ]S got a [ licensing fee ]O every time [ IBM ]S sold a [ PC ]O.s1: In 1980, [ Gates ]S licensed [ 86-DOS ]O from [ Tim Paterson ]X for$50,000, which marketed it as [ PC-DOS ]X.s2: [ Gates' smartest move ]S was retaining [ ownership of the sourcecode ]O of what he and [ Allen ]X would develop as [ MS-DOS ]X.s3: [ Microsoft ]S got a [ licensing fee ]O every time [ IBM ]S sold a [ PC ]O.
(a) A fragment of news texts1S     -     -     -    S    O   O   O    O    -     -     -     -     - s2s3MoveOwnershipSourceCodeMS-DOSFeeTimeIBMPC-     -     -     -     -     -    -     -     -     S    O    X    S    OMicrosoftGates86-DOSPatersonPC-DOSS    O    X    X    -     -    -     -      -    -     -     -     -     - s1S     -     -     -    S    O   O   O    O    -     -     -     -     - s2s3MoveOwnershipSourceCodeMS-DOSFeeTimeIBMPC-     -     -     -     -     -    -     -     -     S    O    X    S    OMicrosoftGates86-DOSPatersonPC-DOSS    O    X    X    -     -    -     -      -    -     -     -     -     -(b) The corresponding entity gridFigure 1: A news text fragment with its corresponding entity grid constructed following B&L (2008).
Al-though s2and s3share no entity, their transition is still coherent, because Gates and Microsoft are semanti-cally related by the knowledge Gates-create-Microsoft.We wish to incorporate semantic relatedness be-tween different entities into existing models to tacklethe problem described above.
In particular, we pro-pose to capture such semantic relatedness betweendifferent entities with world knowledge representedas triples, e.g., Gates-create-Microsoft.
Given atext to be evaluated, we first retrieve relevant worldknowledge from multiple sources.
For the unsuper-vised framework of G&S, we integrate knowledgeinto the original graph-based document representa-tion, in which sentences are the nodes and edges areformed by shared entities and our world knowledge.Then, we adopt a dynamic programming algorithmto produce a coherence score for the text.
For thesupervised framework of B&L, we incorporate theworld knowledge as a novel set of features into theoriginal entity-based model, and train a model to dis-criminate different degrees of text coherence.To evaluate the impact of incorporating seman-tic relatedness, we conduct experiments on twodatasets, each of which resembles a real sub-taskin the text coherence modeling: sentence orderingand summary coherence rating.
On both tasks,across two frameworks, supervised and unsuper-vised, we perform a direct comparison between ourenhanced model and the original one.
On both tasks,our models are shown to be more powerful than themodels relying on entity matching only.
Moreover,for sentence ordering, world knowledge is shown tobe especially useful on short documents.2 Background2.1 Entity-based local coherence modelingThe initial entity-based model was developed byB&L.
It is based on the intuition that there existsa canonical order of how entities occur in the text.Therefore, we can model text coherence by measur-ing how mentions of various entities are distributedwithin the text.
Specifically, for a given document d,an entity grid is constructed in which the rows rep-resent the sentences and the columns represent enti-ties.
Each grid cell ri jcorresponds to the syntacticrole of entity ejin sentence si: subject (S ), object(O), other (X), or nothing (?).
For example, Figure1b shows the entity grid of the text shown in Figure1a.
If an entity serves multiple syntactic roles in asentence, its grammatical role is resolved accordingto the priority order: S  O  X  ?.Based on the entity grid representation, a lo-cal coherence transition is defined as a sequence{S ,O, X,?
}n, representing the grammatical roles orabsence of a particular entity across n adjacent sen-tences.
Then, the document is encoded as a featurevector ?
(d) = (p1(d), p2(d), .
.
.
, pm(d)), where pt(d)is the normalized frequency of the transition t in theentity grid, and m is the number of predefined tran-sitions.
pt(d) is computed as the number of occur-rences of transition t among all entities in the entitygrid, divided by the total number of transitions of thesame length.
Using this feature encoding, the modelis then trained as a preference ranking problem be-tween documents of different degrees of coherence.2.2 Graph-based local coherence modelingAs mentioned previously, most extensions to theentity-based local coherence model focus on enrich-ing the feature set (Filippova and Strube, 2007; El-sner and Charniak, 2011; Lin et al, 2011; Feng etal., 2014), all of which follow a supervised learningframework.
To the best of our knowledge, the onlyexception is the unsupervised method proposed by1088G&S, which transforms the entity grid into a sen-tence graph and measures text coherence by comput-ing the average out-degree of the graph.
For a docu-ment d, its entity grid is constructed first, followingthe method described in Section 2.1.
Then, a bipar-tite graph G = (Vs,Ve, L,W) is constructed, whereVsis the set of nodes representing sentences in thetext; Veis the set of nodes representing entities; Lis the set of edges associated with a weight w ?
W.An edge exists between a sentence sxand an entitye, if and only if e occurs in sx.
Each edge is furtherassociated with a weight w(e, sx), determined by thegrammatical role of the entity e in sentence sx: 3 forsubject (S ), 2 for object (O), 1 for other (X), and 0for nothing (?).
Note that graph G consists of bothsentence nodes and entity nodes.Then, G is converted to another graph P, whichconsists of sentence nodes only, where an edge con-nects two sentence nodes if and only if at least oneentity is shared between these two sentences.
In P,the weight of each edge is computed by aggregatingthe edge weights in the original bipartite graph G:w(P)(sx, sy) =?e?Exyw(G)(e, sx) ?
w(G)(e, sy),(1)where Exyis the set of entities shared by two sen-tences sxand sy, and w(G)(e, sx) is the weight of edgebetween entity e and sentence sxas illustrated be-fore.
The coherence of the document is thus mea-sured by the average out-degree of graph P.Although this method is purely unsupervised, itachieves a performance comparable with its super-vised counterparts, e.g., B&L.
However, since thismethod still relies on matching multiple mentions ofthe same entity, it misses the important contributionfrom those semantically related yet distinct entities,e.g.
Gates and Microsoft in Figure 1a.3 Finding relevant world knowledgeTo supplement existing models with information de-rived from semantic relatedness, given a documentd to be evaluated, we first retrieve all world knowl-edge related to d. There are two major issues for thisprocess: (1) knowledge sources: where can we ob-tain this knowledge?, and (2) knowledge selection:how do we pinpoint the most relevant ones?Knowledge sources There are two main kinds ofknowledge sources: (1) manually edited knowledgebases, such as YAGO (Hoffart et al, 2013), whichconsists of about 4 million human-edited instancesfrom on-line encyclopedias such as WikiPedia (De-noyer and Gallinari, 2007) and FreeBase (Bol-lacker et al, 2008), and (2) automatically con-structed knowledge bases, such as Reverb (Faderet al, 2011), which covers about 20 million in-stances extracted from raw texts.
Generally speak-ing, manually edited knowledge bases have betteraccuracy but lower coverage, while automaticallyextracted knowledge bases are the opposite.
Toseek a good balance, we use both YAGO and Re-verb as our knowledge sources.
In addition, theautomatically constructed knowledge bases can beextracted from raw texts of any domain, whichmakes our method adaptable.
Both sources are pre-sented in triples, argument1-predicate-argument2,(e.g., Gates-create-Microsoft), where the two argu-ments are usually entities and the predicate is therelation between them (Zhang et al, 2014).Knowledge selection For each document d, wethen select the subset of relevant knowledge in-stances, in the sense that they represent relations be-tween the entities in d. In particular, we extract allentities in d, and query the knowledge bases to ob-tain all the knowledge instances in which both of thetwo arguments, argument1and argument2, matchsome of the entities in d.One issue in knowledge selection is whether toretrieve knowledge instances using exact or partialmatching.
For a given pair of entities in the text, thechance is rather low to find instances in the knowl-edge bases where the two arguments perfectly matchthe pair of entities, because entities in the sourcedocument might appear in aliases or abbreviations.In contrast, partial matching between arguments andentities usually increases coverage but at risk of in-troducing more noise.
In our work, to balance ac-curacy and coverage, when retrieving world knowl-edge, we use partial matching and form queries onlyfor those entities realized as noun phrases in the text.4 World knowledge encoding4.1 Unsupervised graph-based frameworkAs described in Section 2.2, G&S represents the textas a graph and measures the coherence by the aver-age out-degree of the graph.
In this part, we describe1089S1 S2 S3e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4Gates MS-DOS Microsoft PC ProductS1 S2 S2S1 S2 S2S1 S2 S2(a) Entity graph with background knowledgeb-1.
Without BKb-2.
With BK between adjacent sentence(b) Sentence graphb-3.
With BK between non-adjacent sentenceS1S2S3Source targete1 e1 e3 e4 e5e2e1 e1 e3 e4 e5e2(a)-1(a)-2S1 S2 S3S1 S2 S3S1 S2 S3e1 e1 e3 e4 e5e2(a)-3S1 S2 S3S1 S2 S3S1 S2 S3(b)-1(b)-2(b)-3(a) Entity graph (b) Sentence graphS1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2S1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3S1S2S3S1S2S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3(a) Traditional entity graph relying on entity matchingS1 S2 S3e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4Gates MS-DOS Microsoft PC ProductS1 S2 S2S1 S2 S2S1 S2 S2(a) Entity graph with background knowledgeb-1.
Without BKb-2.
With BK between adjacent sentence(b) Sentence graph-3.
With BK between non-adjacent sentenceS1S2S3Source targete1 e1 e3 e4 e5e2e1 e1 e3 e4 e5e2(a)-1(a)-2S1 S2 S3S1 S2 S3S1 S2 S3e1 e1 e3 e4 e5e2(a)-3S1 S2 S3S1 S2 S3S1 S2 S3(b)-1(b)-2(b)-3(a) Entity graph (b) Sentence graphS1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2S1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3S1S2S3S1S2S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3S1 S2 S3S1 S2 S3(b) Traditional sentence graph converted from (a)S1 S2 S3e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4Gates MS-DOS Microsoft PC ProductS1 S2 S2S1 S2 S2S1 S2 S2(a) Entity graph with background knowledgeb-1.
Without BKb-2.
With BK between adjacent sentence(b) Sentence graphb-3.
With BK between non-adjacent sentenceS1S2S3Source targete1 e1 e3 e4 e5e2e1 e1 e3 e4 e5e2(a)-1(a)-2S1 S2 S3S1 S2 S3S1 S2 S3e1 e1 e3 e4 e5e2(a)-3S1 S2 S3S1 S2 S3S1 S2 S3(b)-1(b)-2(b)-3(a) Entity graph (b) Sentence graphS1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2S1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3S1S2S3S1S2S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3(c) Our entity graph encoding relatedness between entitiesS1 S2 S3e1 e2 e3 e4 e5 e6 e1 e2 e3 e4 e5 e1 e2 e3 e4Gates MS-DOS Microsoft PC ProductS1 S2 S2S1 S2 S2S1 S2 S2(a) Entity graph with background knowledgeb-1.
Without BKb-2.
With BK between adjacent sentence(b) Sentence graphb-3.
With BK between non-adjacent sentenceS1S2S3Source targete1 e1 e3 e4 e5e2e1 e1 e3 e4 e5e2(a)-1(a)-2S1 S2 S3S1 S2 S3S1 S2 S3e1 e1 e3 e4 e5e2(a)-3S1 S2 S3S1 S2 S3S1 S2 S3(b)-1(b)-2(b)-3(a) Entity graph (b) Sentence graphS123Source targete1 e e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2S1S2S3Source targete1 e1 e3 e4 e5e2(a)-1S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2(a)-2S1 S2 S3 S1 S2 S3(b)-1(b)-2e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3S1S2S3S1S2S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3 S1 S2 S3e1 e1 e3 e4 e5e2S1 S2 S3S1 S2 S3S1 S2 S3(d) Our sentence graph converted from (c)Figure 2: (a) and (b) show the traditional entity and sentence graph based on matching multiple mentionsof the same entity; while (c) and (d) represent our entity and sentence graph encoding semantic relatednessbetween those semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft) byadding world-knowledge edges (dashed lines) accroding to world knowledge (e.g., Gates-create-Microsoft).how we capture semantic relatedness by encodingworld knowledge to the graph-based model.
Theoutline of our method is as follows.
Given a docu-ment d, we first retrieve relevant world knowledgefrom multiple sources (see Section 3).
Then, weconstruct an entity graph with world knowledge tocapture both the distribution information and seman-tic relatedness between entities (see Section 4.1.1).After that, we convert the entity graph into a sen-tence graph (see Section 4.1.2), in which two sen-tences are connected not only through common en-tities but also through world knowledge.
Finally, weapply our novel reachability score computation overthe sentence graph to produce a coherence scorefor the text to be evaluated (see Section 4.1.3).4.1.1 Entity graphAs shown in Figure 2c, there are two kinds ofedges in our entity graph: (1) common-entity edges(solid lines), which connect different mentions of thesame entity, such as Gates in s1and Gates in s2; (2)world-knowledge edges (dashed lines), which con-nect different entities through certain world knowl-edge, such as Gates in s2and Microsoft in s3re-lated by Gates-create-Microsoft.
This representa-tion captures not only the distribution informationof individual entities but also the semantic related-ness between different entities.
In contrast, the orig-inal graph-based model by G&S (Figures 2a and2b) includes common-entity edges only and missesthe semantic relatedness information.
Formally, fora document d, we define its entity graph as G =(V, Lm, Lk,Wm,Wk), where V denotes the nodes ofentities; Lmdenotes the set of common-entity edgesand Lkdenotes the set of world-knowledge edges;and Wmand Wkare the two sets of weights associ-ated with Lmand Lkrespectively.Following G&S, for each common-entity edgelm?
Lm, which connects two mentions of the sameentity e appearing in different sentences sxand sy,we compute its weight as w(e, sx) ?
w(e, sy), wherethe value of w(e, sx) is based on the grammaticalrole of the entity e in the sentence sxas follows:3 for subject (S ), 2 for object (O), 1 for other (X),and 0 for nothing(?).
When multiple mentions ofthe same entity appear with different grammaticalroles in the same sentence, the role with the high-est weight is chosen to represent the entity.
For eachworld-knowledge edge lk?
Lk, which connects twodifferent entities eixand ejyappearing in sentence sxand syrespectively, we consider three factors whenassigning the weight to the edge: (1) semantic relat-edness between eixand ejy: higher relatedness leadsto a higher weight; (2) the grammatical roles of eixand ejyin sxand sy: different roles correspond to dif-ferent weights; and (3) textual distance between eixand ejy: longer distance results in a lower weight.Therefore we compute the weight of lkbetween eixand ejyas below:wk(eix, ejy) =r(ei, ej) ?
w(eix, sx) ?
w(ejy, sy)d(eix, ejy) ?
2,(2)where w(eix, sx) is associated to the grammatical roleof eixin sxas illustrated before; d(eix, ejy) is the dis-tance between eixand ejy; and r(ei, ej) is the seman-tic relatedness between eixand ejyas shown in For-1090mula 3 below.
Note that the value of r(ei, ej) is in-dependent of the sentence in which eiand ejappear,so we denote it as r(ei, ej) instead of r(eix, ejy).r(ei, ej) =????????????
?log n(ei, ej)maxlmn?Lklog n(em, en)if n(ei, ej) > 2,0 otherwise.
(3)where n(ei, ej) corresponds to the number of worldknowledge instances relating eiand ej.
For instance,Figure 1a contains an edge between Gates (e1ins1) and Microsoft (e2in s3), in which w(e11, s1)and w(e23, s3) are 3, and d(e11, e23) is 2.
Note thatwe consider the grammatical roles in both com-mon edges and background knowledge edges be-cause we treat them independently from each other.The grammatical information is important to both ofthese two kinds of edges.4.1.2 Sentence graphFigure 2d shows the sentence graph convertedfrom the entity graph in Figure 2c.
In ourwork, in order to incorporate world knowledge, weadopt an enriched representation of sentence graph,G?= (V?, L?m, L?k,W?m,W?k), where V?is the sentencenodes; L?mdenotes the set of common-entity edges(solid lines); L?kdenotes the set of world-knowledgeedges (dashed lines), and W?mand W?kcorrespond tothe weights associated with the edges in L?mand L?k.Intuitively, the semantic relatedness between twosentences can be measured as the total relatednessof each entity pair in the two sentences.
There-fore, in our enhanced sentence graph representa-tion, for a pair of sentences sxand sy, the weightof their common-entity edge, w?m(sx, sy), is com-puted as w?m(sx, sy) =?ei?Vxywm(eix, eiy), where Vxyis the set of entities shared by two sentences sxand sy, and wm(eix, eiy) is the weight of the corre-sponding common-entity edge in the entity graph(see Section 4.1.1).
Similarly, the weight of theirworld-knowledge edge, w?k(sx, sy), is computed asw?k(sx, sy) =?ei?Vx,ej?Vywk(eix, ejy), where VxandVydenote the set of entities in sxand syrespectively,and wk(eix, ejy) is the weight of the correspondingworld-knowledge edge in the entity graph.Algorithm 1: Reachability score computation.Input: G?= (V?, L?m, L?k,W?m,W?k).Output: The final reachability score S .1 n?
|V?|2 for j = 1?
n do3 score(v?j)?
04 for j = 1?
n do5 for i = 0?
j ?
1 do6 if l?m(i, j) ?
L?mthen7 score(v?j)?
score(v?i) + w?m(i, j)8 if l?k(i, j) ?
L?kthen9 score(v?j)?
score(v?i) + w?k(i, j)10 return S =?v?j?V?
?out(v?j)=0score(v?j)|{v?j: v?j?
V??
out(v?j) = 0}|4.1.3 Reachability score computationBased on our sentence graph representation, wecompute a reachability score for each sentence node.To produce a final coherence score for the text,we compute the average reachability score amongthose nodes whose out-degree is equal to 0 in thegraph, rather than among all nodes, because of theintuition that, if a sentence node has no subsequentnodes, their reachability score therefore reflects thetightness between this sentence and the precedingpart of the text.
For a certain sentence node v?j,its reachability score is defined as the sum of edgeweights on all paths from the starting node (i.e., thefirst sentence) to v?j, and the contribution of each pathto the final reachability score depends on the totalweight of that path as shown in Equation 4.score(v?j) =?v?i?V?,v?i,v?j(score(v?i) + w?m(i, j) + w?k(i, j))(4)where score(v?i) denotes the reachability score ofv?i, and w?m(i, j) and w?k(i, j) are the weights of thecommon-entity edge and the world-knowledge edgebetween v?iand v?j; if there is no such edge in thegraph, the corresponding weight is set to 0.Algorithm 1 summarizes our reachability scorecomputation, in which the reachability score of eachnode is initially 0, and iteratively updated.1091S1 S2 S2( a)  E ntity graph ( b )  Sentence graphS1 S2 S2S1 S2 S2S1 S2 S2S1 S2 S2S1 S2 S2S1 S2 S2S1 S2 S2S1 S2 S2 S1 S2 S2S1 S2 S2 S1 S2 S2S1 S2 S2 S1 S2 S2S1 S2 S2 S1 S2 S2Figure 3: Eight patterns of how world knowledge isdistributed among three adjacent sentences.4.2 Supervised entity-based frameworkAs mentioned previously, numerous extensions havebeen proposed to the original entity-based model ofB&L.
However, those extensions mostly rely on en-tity matching and thus fail to incorporate the infor-mation from semantically related yet distinct enti-ties.
We propose a novel extension by introducingworld knowledge to capture entity-wise relatedness.Inspired by the original entity-based model, inwhich local coherence is reflected by the patterns ofhow entities act grammatically from one sentence tothe next, we believe that local coherence can also becharacterized by the patterns of how world knowl-edge relates a pair of sentences.
Specifically, givena set of sentences, there are different patterns of howknowledge instances are distributed among them.We consider modeling those patterns within a win-dow of 3 sentences, in which there are 23= 8 differ-ent distribution patterns, as shown in Figure 3.
Wethen use the frequencies of these distribution pat-terns over the entire document as additional featuresinto the entity-based model.
In particular, for eachparticular distribution pattern bk, its correspondingfrequency p(bk) =|bk||V?| ?
2, where |bk| is the numberof occurrences of bkin the sentence graph.In addition to p(bk), we also compute another fea-ture, p(E), which is the frequency that two nodesare connected by certain world knowledge over thesentence graph, reflecting the overall semantic re-latedness within the graph.
p(E) is computed asp(E) =|L?k||V?|.
With these world knowledge featuresincorporated into the original entity-based model,we obtain an enhanced model with an emphasis onsemantic relatedness between different entities.5 ExperimentsTo evaluate the impact of incorporating semantic re-latedness, we conduct experiments on two datasets,each of which resembles a real sub-task in modelingtext coherence: sentence ordering and summarycoherence rating.
Since text coherence is a rela-tive concept rather than a binary distinction, in bothtasks, we formulate the problem as pairwise prefer-ence ranking.
Specifically, given a set of texts withdifferent degrees of coherence, we train a ranker toprefer the mor oherent text over the less coherentone.
Performance is therefore measured as the frac-tion of correct pairwise rankings as recognized bythe ranker.
We use SVMlight(Joachims, 2002) withthe ranking configuration to train and evaluate ourmodels, with all parameters set to default values.On both tasks, across two frameworks, supervisedand unsupervised, we directly compare our modifiedmodel against the original one, i.e., B&L in the su-pervised framework and G&S in the unsupervisedframework.
In our experiments, we use the Stan-ford parser (Marneffe et al, 2006) to automaticallyextract the grammatical role for each entity mention.5.1 Sentence orderingThe task of sentence ordering attempts to simu-late the situation where, given a predefined set ofinformation-bearing items, we need to determine thebest order to arrange those items.In this paper, we follow G&S and introduceCoNLL 20122(Pradhan et al, 2012) as our dataset,which is composed of documents from multiplenews sources.
For each text, we randomly shuffle itssentences to generate 20 permutations with incorrectsentence order.
For a fair comparison, we also evalu-ate our model on a filtered subset of documents withan average length of 31.8 sentences.
Therefore, ourdataset contains 72 documents and 72 ?
20 = 1440permutations, among which the shortest one con-tains 25 sentences.
For our enhanced graph-basedmodel (introduced in Section 4.1), which is purelyunsupervised, we evaluate our model over the entiredataset.
For our enhanced entity-based model (in-troduced in Section 4.2), which is purely supervised,we use half of the complete CoNLL dataset for train-ing (237 documents plus permutations) and use half2http://conll.cemantix.org/2012/data.html1092of the filtered subset (36 documents plus permuta-tions).
The training and test sets do not overlap.In this task, each training and test instance is com-posed of a pair of a source document and one ofits permutations, and the source document is alwaysconsidered more coherent than its permutation.5.2 Summary coherence ratingThe second task is summary coherence rating, inwhich, given a pair of summaries about the sameset of source documents, we determine the rank-ing of these two summaries based on their de-grees of coherence.
The performance of the modelis assessed by comparing model-induced rankingsagainst the rankings given by human judges.
We usethe same dataset (DUC 2003) as B&L and G&S did,which consists of summaries generated either byhuman writers or by automatic summarization sys-tems.
Each summary was given a coherence scoreby averaging among seven judges.
Often, machine-generated summaries receive low coherence scoresbecause they contain sentences taken out of contextand thus display problems with respect to coherence.This dataset consists of 16 input document clus-ters, each of which is associated with five machine-generated summaries along with a human-writtensummary.
In total, we have 96 summaries (for moredetails, see B&L).
We form pairwise rankings bytaking any two summaries originating from the samedocument cluster, given that the two summaries re-ceive different coherence scores: 144 of the resultingrankings are used for training and 80 are for testing.5.3 Experiment resultsIn this section, we demonstrate the performance ofour models with world knowledge encoded in one ofthe two ways: paths in a sentence graph or featuresin an entity grid.
We compare our models againstthe original graph-based model (G&S) and entity-based model (B&L).
The evaluation is conducted onthe two tasks, sentence ordering and summary co-herence rating, and the accuracy is the fraction ofcorrect pairwise rankings.Table 1 shows the performance of various modelson both tasks.
The first section shows the results ofG&S?s graph-based local coherence model, includ-ing the performance reported in their original paperand that achieved by our re-implementation, repre-Model SO SCRGraph model (G&S) 88.9 80.0Graph model (Implemented) 89.6 48.8Graph model + K 91.3** 50.0Graph model + K + Avg R 93.4** 55.0*Entity model (B&L) 88.9 83.8Entity model (Implemented) 93.7 90.0Entity model + K 95.1** 91.3Table 1: Accuracies (%) of various models onthe two tasks, sentence ordering (SO) and sum-mary coherence rating (SCR).
Models that per-form significantly better than their corresponding re-implemented basic models are denoted by ** (p <.01) or * (p < .05), verified using paired t-test.senting the effect with no world knowledge encoded.The second section shows the performance of ourtwo graph-based models with world knowledge en-coded.
Graph model + K is the basic model withworld knowledge encoded, but coherence is simplymeasured as the average out-degree as in G&S?s ap-proach.
Graph model + K + Avg R replaces theout-degree measurement by our average reachabil-ity score (described in Section 4.1.3), which mea-sures coherence in a more sophisticated way.
Thethird section shows the results of B&L?s entity-basedlocal coherence model, including the originally re-ported performance and that obtained by our re-implementation, in which no world knowledge fea-tures are included.
The last section, Entity model +K, shows the result of entity-based model with ourworld knowledge features encoded.
Note that therandom baseline of both tasks is 50%.Firstly, for graph-based models, our Graph model+ K outperforms the original models, suggestingthat world knowledge is truly helpful for capturingmore coherence information3.
Moreover, by intro-3The large discrepancy between the performance reportedby G&S and that of our re-implementation in Task 2 is due tothe fact that G&S experimented with a set of specially formedsummary pairs (see their paper for detail), which we have noaccess to.
They also did not give sufficient details about howthey constructed those summary pairs, which has a great im-pact on the final result.
This made it difficult for us to fully re-implement their experiment.
So we use B&L?s set of summarypairs, which are generated randomly and are more difficult todistinguish, which explains our differing results from theirs.1093Figure 4: Graph-based models, with and withoutworld knowledge (labeled as With K and Without K),tested on sets with different numbers of sentences.ducing the scoring scheme of average reachabilityscore, our Graph model + K + Avg R achieves thebest performance among all graph-based models.Secondly, for entity-based models, our en-hanced model with knowledge features encodedalso achieves superior performance than our re-implemented model, again confirming the useful-ness of world knowledge.
Interestingly, we observethat our re-implementation obtains higher accuracycompared to the performance reported by B&L.
Thisis partly due to the fact that the documents in ourdataset have an average length of 31.5 sentences,which are longer than those used in B&L?s experi-ments.
We will further discuss this problem in Sec-tion 5.4 and show that document length is an impor-tant factor to the overall performance.However, on the task of summary coherence rat-ing, the difference between our extended models andthe original ones is generally not significant, primar-ily due to the fact that the sample size for this task istoo small, i.e., 80 pairwise rankings.5.4 Effect of document length5.4.1 Effect of document length on the overallperformanceWe further analyze the impact of document lengthon the task of sentence ordering.
We partition ouroriginal dataset, which consists of 214 documentsand their permutations, into 8 non-overlapping sub-sets, according to the length of documents: 1-5, 6-10,. .
.
, and 36-40 sentences.
To illustrate the cor-relation between the performance and the lengthModel Accuracy (%)Graph Model 27.4Graph Model + K 44.2Entity Model 65.8Entity Model + K 71.1Table 2: Performance of various models with andwithout world knowledge in the sentence orderingtask, tested on short documents with 1?5 sentences.of document, we test our models with and with-out world knowledge encoded on each subset sepa-rately.
Since the size of the available training data ineach subset is relatively small, the supervised entity-based model suffers from sparsity.
Therefore, we fo-cus on the unsupervised graph-based models only.Figure 4 shows the performance on different sub-sets.
We can see that the performance of bothmodels generally improves as the number of sen-tences increases.
This observation is quite intuitive,because the longer a document is, the higher thechance is that, after being shuffled, adjacent sen-tences in the resulting permutation would be com-pletely irrelevant to each other.
Therefore, for longerdocuments, it is much easier for the model to distin-guish a permutation from its source document.5.4.2 Effect of document length on the modelwith world knowledgeMoreover, we also observe that the documentlength has a non-universal effect, in terms of howthe model could benefit from incorporating worldknowledge.
Specifically, we find that world knowl-edge has a greater effect on short documents, asdemonstrated in Table 2.
Evaluated on a set of doc-uments composed of 30 extremely short documentsonly (1?5 sentences), we see that our enhancedgraph-based model is able to improve the perfor-mance by 16.8% over the basic model, and our en-hanced entity-based model achieves 5.3% improve-ment (both differences are significant at p < .01).We postulate that it is primarily because a documentwith fewer sentences tends to shift to another sub-topic immediately without elaborating on the previ-ous one, and strict entity matching would find it dif-ficult to establish coherent transitions between them.Therefore, the contribution from semantic related-ness tends to dominate the overall performance.10946 Conclusions and future workIn this paper, for the evaluation of text coherence, wego beyond strict entity matching and model the se-mantic relatedness between distinct entities throughthe use of world knowledge.
Specifically, we in-corporate world knowledge into two existing frame-works: (1) the unsupervised graph-based model(G&S), and (2) the supervised entity-grid model(B&L).
Across the two frameworks, on both of ourevaluation tasks, sentence ordering and summarycoherence rating, our enhanced models with worldknowledge encoded are shown to be stronger thanthe corresponding basic models, confirming that se-mantic relatedness is truly important for coherencemodeling and such relatedness can be effectivelycaptured by world knowledge.
Moreover, we ob-serve that world knowledge is particularly useful forshort documents in sentence ordering, as it providesadditional clues to relate sub-topics in the text.In our future work, we wish to explore the ef-fect of our world knowledge in conjunction with dis-course relations.
Specifically, we plan to incorporateworld knowledge into the framework of discourserole matrix (Lin et al, 2011; Feng et al, 2014).
Inaddition, we also plan to develop a more sophis-ticated feature encoding by distinguishing differenttypes of predicates in world knowledge triples.AcknowledgmentsWe would like to thank Mao Zheng and YanyanZhao for their great help.
This work was partlysupported by National Natural Science Founda-tion of China via grant 61133012, the National863 Leading Technology Research Project via grant2012AA011102 and the National Natural ScienceFoundation of China Surface Project via grant61273321.ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Regina Barzilay, Noemie Elhadad, and Kathleen R.McKeown.
2002.
Inferring strategies for sentence or-dering in multidocument news summarization.
Jour-nal of Artificial Intelligence Research, 17(1):35?55.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Management ofdata, pages 1247?1250.
ACM.Ludovic Denoyer and Patrick Gallinari.
2007.
TheWikipedia XML corpus.
In Comparative Evaluationof XML Information Retrieval Systems, pages 12?19.Springer.Micha Elsner and Eugene Charniak.
2008.
Coreference-inspired coherence modeling.
In Proceedings of the46th Annual Meeting of the Association for Computa-tional Linguistics on Human Language Technologies:Short Papers, pages 41?44.
Association for Computa-tional Linguistics.Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies: short papers-Volume 2, pages 125?129.Association for Computational Linguistics.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1535?1545.
Association for Computational Linguis-tics.Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.
2014.The impact of deep hierarchical discourse structuresin the evaluation of text coherence.
In Proceedings ofthe 25th International Conference on ComputationalLinguistics, pages 940?949.Katja Filippova and Michael Strube.
2007.
Extend-ing the entity-grid coherence model to semanticallyrelated entities.
In Proceedings of the Eleventh Eu-ropean Workshop on Natural Language Generation,pages 139?142.
Association for Computational Lin-guistics.Camille Guinaudeau and Michael Strube.
2013.
Graph-based local coherence modeling.
In Proceedings of the51st Annual Meeting of the Association for Computa-tional Linguistics, pages 93?103.Johannes Hoffart, Fabian M Suchanek, Klaus Berberich,and Gerhard Weikum.
2013.
Yago2: a spatially andtemporally enhanced knowledge base from Wikipedia.Artificial Intelligence, 194:28?61.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 133?142.ACM.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Au-tomatically evaluating text coherence using discourse1095relations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies-Volume 1, pages 997?1006.
Association for Computational Linguistics.M.
Marneffe, B. Maccartney, and C. Manning.
2006.Generating typed dependency parses from phrasestructure parses.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation, volume 6, pages 449?454.
European LanguageResources Association (ELRA).Neil McIntyre and Mirella Lapata.
2010.
Plot inductionand evolutionary search for story generation.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 1562?1572.Association for Computational Linguistics.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring systems.Natural Language Engineering, 10(1):25?55.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 shared task: Modeling multilingual unrestrictedcoreference in ontonotes.
In Joint Conference onEMNLP and CoNLL-Shared Task, pages 1?40.
Asso-ciation for Computational Linguistics.Muyu Zhang, Bing Qin, Ting Liu, and Mao Zheng.
2014.Triple based background knowledge ranking for doc-ument enrichment.
In Proceedings of the 25th In-ternational Conference on Computational Linguistics,pages 917?927.1096
