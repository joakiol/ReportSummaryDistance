Proceedings of NAACL HLT 2007, pages 113?120,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Systematic Exploration of the Feature Space for Relation ExtractionJing Jiang and ChengXiang ZhaiDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{jiang4,czhai}@cs.uiuc.eduAbstractRelation extraction is the task of find-ing semantic relations between entitiesfrom text.
The state-of-the-art methodsfor relation extraction are mostly basedon statistical learning, and thus all haveto deal with feature selection, which cansignificantly affect the classification per-formance.
In this paper, we systemat-ically explore a large space of featuresfor relation extraction and evaluate the ef-fectiveness of different feature subspaces.We present a general definition of fea-ture spaces based on a graphic represen-tation of relation instances, and explorethree different representations of relationinstances and features of different com-plexities within this framework.
Our ex-periments show that using only basic unitfeatures is generally sufficient to achievestate-of-the-art performance, while over-inclusion of complex features may hurtthe performance.
A combination of fea-tures of different levels of complexity andfrom different sentence representations,coupled with task-oriented feature prun-ing, gives the best performance.1 IntroductionAn important information extraction task is relationextraction, whose goal is to detect and characterizesemantic relations between entities in text.
For ex-ample, the text fragment ?hundreds of Palestiniansconverged on the square?
contains the located rela-tion between the Person entity ?hundreds of Pales-tinians?
and the Bounded-Area entity ?the square?.Relation extraction has applications in many do-mains, including finding affiliation relations fromweb pages and finding protein-protein interactionsfrom biomedical literature.Recent studies on relation extraction have shownthe advantages of discriminative model-based sta-tistical machine learning approach to this problem.There are generally two lines of work following thisapproach.
The first utilizes a set of carefully se-lected features obtained from different levels of textanalysis, from part-of-speech (POS) tagging to fullparsing and dependency parsing (Kambhatla, 2004;Zhao and Grishman, 2005; Zhou et al, 2005)1.
Thesecond line of work designs kernel functions onsome structured representation (sequences or trees)of the relation instances to capture the similarity be-tween two relation instances (Zelenko et al, 2003;Culotta and Sorensen, 2004; Bunescu and Mooney,2005a; Bunescu and Mooney, 2005b; Zhang et al,2006a; Zhang et al, 2006b).
Of particular interestamong the various kernels proposed are the convolu-tion kernels (Bunescu and Mooney, 2005b; Zhang etal., 2006a), because they can efficiently compute thesimilarity between two instances in a huge featurespace due to their recursive nature.
Apart from theircomputational efficiency, convolution kernels alsoimplicitly correspond to some feature space.
There-fore, both lines of work rely on an appropriately de-1Although Zhao and Grishman (2005) defined a number ofkernels for relation extraction, the method is essentially similarto feature-based methods.113fined set of features.
As in any learning problem, thechoice of features can affect the performance signif-icantly.Despite the importance of feature selection, therehas not been any systematic exploration of the fea-ture space for relation extraction, and the choicesof features in existing work are somewhat arbitrary.In this paper, we conduct a systematic study of thefeature space for relation extraction, and evaluatethe effectiveness of different feature subspaces.
Ourmotivations are twofold.
First, based on previousstudies, we want to identify and characterize thetypes of features that are potentially useful for rela-tion extraction, and define a relatively complete andstructured feature space that can be systematicallyexplored.
Second, we want to compare the effective-ness of different features.
Such a study can guide usto choose the most effective feature set for relationextraction, or to design convolution kernels in themost effective way.We propose and define a unified graphic repre-sentation of the feature space, and experiment withthree feature subspaces, corresponding to sequences,syntactic parse trees and dependency parse trees.Experiment results show that each subspace is ef-fective by itself, with the syntactic parse tree sub-space being the most effective.
Combining the threesubspaces does not generate much improvement.Within each feature subspace, using only the basicunit features can already give reasonably good per-formance.
Adding more complex features may notimprove the performance much or may even hurtthe performance.
Task-oriented heuristics can beused to prune the feature space, and when appropri-ately done, can improve the performance.
A com-bination of features of different levels of complex-ity and from different sentence representations, cou-pled with task-oriented feature pruning, gives thebest performance.2 Related WorkZhao and Grishman (2005) and Zhou et al (2005)explored a large set of features that are potentiallyuseful for relation extraction.
However, the featurespace was defined and explored in a somewhat adhoc manner.
We study a broader scope of featuresand perform a more systematic study of differentfeature subspaces.
Zelenko et al (2003) and Culottaand Sorensen (2004) used tree kernels for relationextraction.
These kernels can achieve high precisionbut low recall because of the relatively strict match-ing criteria.
Bunescu and Mooney (2005a) proposeda dependency path kernel for relation extraction.This kernel also suffers from low recall for the samereason.
Bunescu and Mooney (2005b) and Zhanget.
al.
(2006a; 2006b) applied convolution string ker-nels and tree kernels, respectively, to relation extrac-tion.
The convolution tree kernels achieved state-of-the-art performance.
Since convolution kernelscorrespond to some explicit large feature spaces, thefeature selection problem still remains.General structural representations of natural lan-guage data have been studied in (Suzuki et al,2003; Cumby and Roth, 2003), but these modelswere not designed specifically for relation extrac-tion.
Our feature definition is similar to these mod-els, but more specifically designed for relation ex-traction and systematic exploration of the featurespace.
Compared with (Cumby and Roth, 2003), ourfeature space is more compact and provides moreguidance on selecting meaningful subspaces.3 Task DefinitionGiven a small piece of text that contains two entitymentions, the task of relation extraction is to decidewhether the text states some semantic relation be-tween the two entities, and if so, classify the rela-tion into one of a set of predefined semantic rela-tion types.
Formally, let r = (s, arg1, arg2) de-note a relation instance, where s is a sentence, arg1and arg2 are two entity mentions contained in s, andarg1 precedes arg2 in the text.
Given a set of rela-tion instances {ri}, each labeled with a type ti ?
T ,where T is the set of predefined relation types plusthe type nil, our goal is to learn a function that mapsa relation instance r to a type t ?
T .
Note that wedo not specify the representation of s here.
Indeed, scan contain more structured information in additionto merely the sequence of tokens in the sentence.4 Feature Space for Relation ExtractionIdeally, we would like to define a feature space withat least two properties: (1) It should be complete inthe sense that all features potentially useful for the114classification problem are included.
(2) It shouldhave a good structure so that a systematic search inthe space is possible.
Below we show how a unifiedgraph-based feature space can be defined to satisfythese two properties.4.1 A Unified View of Features for RelationExtractionBefore we introduce our definition of the featurespace, let us first look at some typical features usedfor relation extraction.
Consider the relation in-stance ?hundreds of Palestinians converged on thesquare?
with arg1 = ?hundreds of Palestinians?
andarg2 = ?the square?.
Various types of informationcan be useful for classifying this relation instance.For example, knowing that arg1 is an entity of typePerson can be useful.
This feature involves the sin-gle token ?Palestinians?.
Another feature, ?the headword of arg1 (Palestinians) is followed by a verb(converged)?, can also be useful.
This feature in-volves two tokens, ?Palestinians?
and ?converged?,with a sequence relation.
It also involves the knowl-edge that ?Palestinians?
is part of arg1 and ?con-verged?
is a verb.
If we have the syntactic parse treeof the sentence, we can obtain even more complexand discriminative features.
For example, the syn-tactic parse tree of the same relation instance con-tains the following subtree: [VP ?
VBD [PP ?
[IN?
on] NP] ].
If we know that arg2 is contained in theNP in this subtree, then this subtree states that arg2is in a PP that is attached to a VP, and the propositionis ?on?.
This subtree therefore may also a usefulfeature.
Similarly, if we have the dependency parsetree of the relation instance, then the dependencylink ?square ?
on?
states that the token ?square?is dependent on the token ?on?, which may also bea useful feature.Given that useful features are of various forms, inorder to systematically search the feature space, weneed to first have a unified view of features.
Thisproblem is not trivial because it is not immediatelyclear how different types of features can be unified.We observe, however, that in general features fallinto two categories: (1) properties of a single token,and (2) relations between tokens.
Features that in-volve attributes of a single token, such as bag-of-word features and entity attribute features, belongto the first category, while features that involve se-quence, syntactic or dependency relations betweentokens belong to the second category.
Motivated bythis observation, we can represent relation instancesas graphs, with nodes denoting single tokens or syn-tactic categories such as NPs and VPs, and edges de-noting various types of relations between the nodes.4.2 Relation Instance GraphsWe represent a relation instance as a labeled, di-rected graph G = (V,E,A,B), where V is the setof nodes in the graph, E is the set of directed edgesin the graph, and A, B are functions that assign la-bels to the nodes.First, for each node v ?
V , A(v) ={a1, a2, .
.
.
, a|A(v)|} is a set of attributes associatedwith node v, where ai ?
?, and ?
is an alphabet thatcontains all possible attribute values.
The attributesare introduced to help generalize the node.
For ex-ample, if node v represents a token, then A(v) caninclude the token itself, its morphological base form,its POS, its semantic class (e.g.
WordNet synset),etc.
If v also happens to be the head word of arg1 orarg2, then A(v) can also include the entity type andother entity attributes.
If node v represents a syntac-tic category such as an NP or VP, A(v) can simplycontain only the syntactic tag.Next, function B : V ?
{0, 1, 2, 3} is introducedto distinguish argument nodes from non-argumentnodes.
For each node v ?
V , B(v) indicates hownode v is related to arg1 and arg2.
0 indicates thatv does not cover any argument, 1 or 2 indicates thatv covers arg1 or arg2, respectively, and 3 indicatesthat v covers both arguments.
We will see shortlythat only nodes that represent syntactic categories ina syntactic parse tree can possibly be assigned 3.
Werefer to B(v) as the argument tag of v.We now consider three special instantiations ofthis general definition of relation instance graphs.See Figures 1, 2 and 3 for examples of each of thethree representations.Sequence: Without introducing any additionalstructured information, a sequence representationpreserves the order of the tokens as they occur in theoriginal sentence.
Each node in this graph is a tokenaugmented with its relevant attributes.
For example,head words of arg1 and arg2 are augmented with thecorresponding entity types.
A token is assigned theargument tag 1 or 2 if it is the head word of arg1 or115NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area00 1 0 0 0 2Person VBD1 0 Bounded-Area2Figure 1: An example sequence representation.
Thesubgraph on the left represents a bigram feature.
Thesubgraph on the right represents a unigram featurethat states the entity type of arg2.NNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area00 1 0 0 0 2NPB NPBPPNP1101SVPPPNPB3222on DT Bounded-Area0 0 2PPNPB2 2Figure 2: An example syntactic parse tree represen-tation.
The subgraph represents a subtree feature(grammar production feature).arg2.
Otherwise, it is assigned the argument tag 0.There is a directed edge from u to v if and only ifthe token represented by v immediately follows thatrepresented by u in the sentence.Syntactic Parse Tree: The syntactic parse treeof the relation instance sentence can be augmentedto represent the relation instance.
First, we modifythe tree slightly by conflating each leaf node in theoriginal parse tree with its parent, which is a preter-minal node labeled with a POS tag.
Then, each nodeis augmented with relevant attributes if necessary.Argument tags are assigned to the leaf nodes in thesame way as in the sequence representation.
For aninternal node v, argument tag 1 or 2 is assigned ifeither arg1 or arg2 is inside the subtree rooted at v,and 3 is assigned if both arguments are inside thesubtree.
Otherwise, 0 is assigned to v.Dependency Parse Tree: Similarly, the depen-dency parse tree can also be modified to representthe relation instance.
Assignment of attributes andargument tags is the same as for the sequence repre-sentation.
To simplify the representation, we ignoreNNShundreds INof NNPPalestiniansPerson VBDconverged INon DTthe NNsquareBounded-Area00 1 0 0 0 2of Palestinians10Figure 3: An example dependency parse tree rep-resentation.
The subgraph represents a dependencyrelation feature between arg1 ?Palestinians?
and?of?.the dependency relati types.4.3 FeaturesGiven the above definition of relation instancegraphs, we are now ready to define features.
Intu-itively, a feature of a relation instance captures partof the attributive and/or structural properties of therelation instance graph.
Therefore, it is natural to de-fine a feature as a subgraph of the relation instancegraph.
Formally, given a graph G = (V,E,A,B),which represents a single relation instance, a fea-ture that exists in this relation instance is a sub-graph G?
= (V ?, E?, A?, B?)
that satisfies the fol-lowing conditions: V ?
?
V , E?
?
E, and ?v ?V ?, A?
(v) ?
A(v), B?
(v) = B(v).We now show that many features that have beenexplored in previous work on relation extraction canbe transformed into this graphic representation.
SeeFigures 1, 2 and 3 for some examples.Entity Attributes: Previous studies have shownthat entity types and entity mention types of arg1and arg2 are very useful (Zhao and Grishman, 2005;Zhou et al, 2005; Zhang et al, 2006b).
To representa single entity attribute, we can take a subgraph thatcontains only the node representing the head word ofthe argument, labeled with the entity type or entitymention type.
A particularly useful type of featuresare conjunctive entity features, which are conjunc-tions of two entity attributes, one for each argument.To represent a conjunctive feature such as ?arg1 isa Person entity and arg2 is a Bounded-Area entity?,we can take a subgraph that contains two nodes, onefor each argument, and each labeled with an en-tity attribute.
Note that in this case, the subgraphcontains two disconnected components, which is al-lowed by our definition.Bag-of-Words: These features have also been116explore by Zhao and Grishman (2005) and Zhouet.
al.
(2005).
To represent a bag-of-word feature,we can simply take a subgraph that contains a singlenode labeled with the token.
Because the node alsohas an argument tag, we can distinguish between ar-gument word and non-argument word.Bigrams: A bigram feature (Zhao and Grishman,2005) can be represented by a subgraph consistingof two connected nodes from the sequence represen-tation, where each node is labeled with the token.Grammar Productions: The features in convo-lution tree kernels for relation extraction (Zhang etal., 2006a; Zhang et al, 2006b) are sequences ofgrammar productions, that is, complete subtrees ofthe syntactic parse tree.
Therefore, these featurescan naturally be represented by subgraphs of the re-lation instance graphs.Dependency Relations and Dependency Paths:These features have been explored by Bunescu andMooney (2005a), Zhao and Grishman (2005), andZhou et.
al.
(2005).
A dependency relation can berepresented as an edge connecting two nodes fromthe dependency tree.
The dependency path betweenthe two arguments can also be easily represented asa path in the dependency tree connecting the twonodes that represent the two arguments.There are some features that are not covered byour current definition, but can be included if wemodify our relation instance graphs.
For example,gapped subsequence features in subsequence ker-nels (Bunescu and Mooney, 2005b) can be repre-sented as subgraphs of the sequence representationif we add more edges to connect any pair of nodes uand v provided that the token represented by u oc-curs somewhere before that represented by v in thesentence.
Since our feature definition is very gen-eral, our feature space also includes many featuresthat have not been explored before.4.4 Searching the Feature SpaceAlthough the feature space we have defined is rel-atively complete and has a clear structure, it is stilltoo expensive to exhaustively search the space be-cause the number of features is exponential in termsof the size of the relation instance graph.
We thuspropose to search the feature space in the follow-ing bottom-up manner: We start with the conjunc-tive entity features (defined in Section 4.3), whichhave been found effective in previous studies andare intuitively necessary for relation extraction.
Wethen systematically add unit features with differentgranularities.
We first consider the minimum (i.e.most basic) unit features.
We then gradually includemore complex features.
The motivations for thisstrategy are the following: (1) Using the smallestfeatures to represent a relation instance graph pre-sumably covers all unit characteristics of the graph.
(2) Using small subgraphs allows fuzzy matching,which is good for our task because relation instancesof the same type may vary in their relation instancegraphs, especially with the noise introduced by ad-jectives, adverbs, or irrelevant propositional phrases.
(3) The number of features of a fixed small size ispolynomial in terms of the size of the relation in-stance graph.
It is therefore feasible to generate allthe small unit features and use any classifier such asa maximum entropy classifier or an SVM.In our experiments, we consider three levels ofsmall unit features in increasing order of their com-plexity.
First, we consider unigram features Guni =({u}, ?, Auni , B), where Auni(u) = {ai} ?
A(u).In another word, unigram features consist of a sin-gle node labeled with a single attribute.
Examplesof unigram features include bag-of-word featuresand non-conjunctive entity attribute features.
At thesecond level, we consider bigram features Gbi =({u, v}, {(u, v)}, Auni , B).
Bigram features aretherefore single edges connecting two nodes, whereeach node is labeled with a single attribute.
Thethird level of attributes we consider are trigram fea-tures Gtri = ({u, v, w}, {(u, v), (u,w)}, Auni , B)or Gtri = ({u, v, w}, {(u, v), (v, w)}, Auni , B).Thus trigram features consist of two connectededges and three nodes, where each node is also la-beled with a single attribute.We treat the three relation instance graphs (se-quences, syntactic parse trees, and dependency parsetrees) as three feature subspaces, and search in eachsubspace.
For each feature subspace, we incremen-tally add the unigram, bigram and trigram featuresto the working feature set.
For the syntactic parsetree representation, we also consider a fourth level ofsmall unit features, which are single grammar pro-ductions such as [VP ?
VBD PP], because theseare the smallest features in convolution tree kernels.After we explore each feature subspace, we try to117combine the features from the three subspaces to seewhether the performance can be improved, that is,we test whether the sequence, syntactic and depen-dency relations can complement each other.5 Experiments5.1 Data Set and Experiment SetupWe used the data set from ACE (Automatic Con-tent Extraction) 2004 evaluation to conduct our ex-periments.
This corpus defines 7 types of relations:Physical, Personal / Social, Empolyment / Memeber-ship / Subsidiary, Agent-Artifact, PER / ORG Affili-ation, GPE Affiliation and Discourse.We used Collins parser to parse the sentences inthe corpus because Collins parser gives us the headof each syntactic category, which allows us to trans-form the syntactic parse trees into dependency trees.We discarded sentences that could not be parsedby Collins parser.
The candidate relation instanceswere generated by considering all pairs of entitiesthat occur in the same sentence.
We obtained 48625candidate relation instances in total, among which4296 instances were positive.As in most existing work, instead of using the en-tire sentence, we used only the sequence of tokensthat are inside the minimum complete subtree cov-ering the two arguments.
Presumably, tokens out-side of this subtree are not so relevant to the task.
Inour graphic representation of relation instances, theattribute set for a token node includes the token it-self, its POS tag, and entity type, entity subtype andentity mention type when applicable.
The attributeset for a syntactic category node includes only thesyntactic tag.
We used both maximum entropy clas-sifier and SVM for all experiments.
We adopted onevs.
others strategy for the multi-class classificationproblem.
In all experiments, the performance shownwas based on 5-fold cross validation.5.2 General Search in the Feature SubspacesFollowing the general search strategy, we conductedthe following experiments.
For each feature sub-space, we started with the conjunctive entity featuresplus the unigram features.
We then incrementallyadded bigram and trigram features.
For the syntac-tic parse tree feature space, we conducted an addi-tional experiment: We added basic grammar produc-tion features on top of the unigram, bigram and tri-gram features.
Adding production features allows usto study the effect of adding more complex and pre-sumably more specific and discriminative features.Table 1 shows the precision (P), recall (R) and F1measure (F) from the experiments with the maxi-mum entropy classifier (ME) and the SVM classi-fier (SVM).
We can compare the results in two di-mensions.
First, within each feature subspace, whilebigram features improved the performance signifi-cantly over unigrams, trigrams did not improve theperformance very much.
This trend is observed forboth classifiers.
In the case of the syntactic parse treesubspace, adding production features even hurt theperformance.
This suggests that inclusion of com-plex features is not guaranteed to improve the per-formance.Second, if we compare the best performanceachieved in each feature subspace, we can see thatfor both classifiers, syntactic parse tree is the mosteffective feature space, while sequence and depen-dency tree are similar.
However, the difference inperformance between the syntactic parse tree sub-space and the other two subspaces is not very large.This suggests that each feature subspace alone al-ready captures most of the useful structural informa-tion between tokens for relation extraction.
The rea-son why the sequence feature subspace gave goodperformance although it contained the least struc-tural information is probably that many relations de-fined in the ACE corpus are short-range relations,some within single noun phrases.
For such kind ofrelations, sequence information may be even morereliable than syntactic or dependency information,which may not be accurate due to parsing errors.Next, we conducted experiments to combine thefeatures from the three subspaces to see whetherthis could further improve the performance.
For se-quence subspace and dependency tree subspace, weused up to bigram features, and for syntactic parsetree subspace, we used up to trigram features.
In Ta-ble 2, we show the experiment results.
We can seethat for both classifiers, adding features from the se-quence subspace or from the dependency tree sub-space to the syntactic parse tree subspace can im-prove the performance slightly.
But combining se-quence subspace and dependency tree subspace doesnot generate any performance improvement.
Again,118Uni +Bi +Tri +ProdP 0.647 0.662 0.717Seq R 0.614 0.701 0.653 N/AF 0.630 0.681 0.683P 0.651 0.695 0.726 0.702ME Syn R 0.645 0.698 0.688 0.691F 0.648 0.697 0.707 0.696P 0.647 0.673 0.718Dep R 0.614 0.676 0.652 N/AF 0.630 0.674 0.683P 0.583 0.666 0.684Seq R 0.586 0.650 0.648 N/AF 0.585 0.658 0.665P 0.598 0.645 0.679 0.674SVM Syn R 0.611 0.663 0.681 0.672F 0.604 0.654 0.680 0.673P 0.583 0.644 0.682Dep R 0.586 0.638 0.645 N/AF 0.585 0.641 0.663Table 1: Comparison among the three feature sub-spaces and the effect of including larger features.Seq+Syn Seq+Dep Syn+Dep AllP 0.737 0.687 0.695 0.724ME R 0.694 0.682 0.731 0.702F 0.715 0.684 0.712 0.713P 0.689 0.669 0.687 0.691SVM R 0.686 0.653 0.682 0.686F 0.688 0.661 0.684 0.688Table 2: The effect of combining the three featuresubspaces.this suggests that since many of the ACE relationsare local, there is likely much overlap between se-quence information and dependency information.We also tried the convolution tree kernelmethod (Zhang et al, 2006a), using an SVM treekernel package2.
The performance we obtained wasP = 0.705, R = 0.685, and F = 0.6953.
This F mea-sure is higher than the best SVM performance in Ta-ble 1.
The convolution tree kernel uses large subtreefeatures, but such features are deemphasized withan exponentially decaying weight.
We found thatthe performance was sensitive to this decaying fac-tor, suggesting that complex features can be usefulif they are weighted appropriately, and further studyof how to optimize the weights of such complex fea-tures is needed.2http://ai-nlp.info.uniroma2.it/moschitti/Tree-Kernel.htm3The performance we achieved is lower than that reportedin (Zhang et al, 2006b), due to different data preprocessing,data partition, and parameter setting.5.3 Task-Oriented Feature PruningApart from the general bottom-up search strategy wehave proposed, we can also introduce some task-oriented heuristics based on intuition or domainknowledge to prune the feature space.
In our ex-periments, we tried the following heuristics.H1: Zhang et al (2006a) found that using path-enclosed tree performed better than using minimumcomplete tree, when convolution tree kernels wereapplied.
In path-enclosed trees, tokens before arg1and after arg2 as well as their links with other nodesin the tree are removed.
Based on this previousfinding, our first heuristic was to change the syntac-tic parse tree representation of the relation instancesinto path-enclosed trees.H2: We hypothesize that words such as articles,adjectives and adverbs are not very useful for rela-tion extraction.
We thus removed sequence unigramfeatures and bigram features that contain an article,adjective or adverb.H3: Similar to H2, we can remove bigrams in thesyntactic parse tree subspace if the bigram containsan article, adjective or adverb.H4: Similar to H1, we can also remove the to-kens before arg1 and after arg2 from the sequencerepresentation of a relation instance.In Table 3, we show the performance after apply-ing these heuristics.
We started with the best con-figuration from our previous experiments, that is,combing up to bigram features in the sequence sub-space and up to trigram features in the syntactic treesubspace.
We then applied heuristics H1 to H4 in-crementally unless we saw that a heuristic was noteffective.
We found that H1, H2 and H4 slightlyimproved the performance, but H3 hurt the perfor-mance.
On the one hand, the improvement suggeststhat our original feature configuration included someirrelevant features, and in turn confirmed that over-inclusion of features could hurt the performance.
Onthe other hand, since the improvement brought byH1, H2 and H4 was rather small, and H3 even hurtthe performance, we could see that it is in generalvery hard to find good feature pruning heuristics.6 Conclusions and Future WorkIn this paper, we conducted a systematic study ofthe feature space for relation extraction.
We pro-119ME SVMP R F P R FBest 0.737 0.694 0.715 0.689 0.686 0.688+H1 0.714 0.729 0.721 0.698 0.699 0.699+H2 0.730 0.723 0.726 0.704 0.704 0.704+H3 0.739 0.704 0.721 0.701 0.696 0.698-H3+H4 0.746 0.713 0.729 0.702 0.701 0.702Table 3: The effect of various heuristic feature prun-ing methods.posed and defined a unified graphic representationof features for relation extraction, which serves as ageneral framework for systematically exploring fea-tures defined on natural language sentences.
Withthis framework, we explored three different repre-sentations of sentences?sequences, syntactic parsetrees, and dependency trees?which lead to threefeature subspaces.
In each subspace, starting withthe basic unit features, we systematically exploredfeatures of different levels of complexity.
The stud-ied feature space includes not only most of the ef-fective features explored in previous work, but alsosome features that have not been considered before.Our experiment results showed that using a set ofbasic unit features from each feature subspace, wecan achieve reasonably good performance.
Whenthe three subspaces are combined, the performancecan improve only slightly, which suggests that thesequence, syntactic and dependency relations havemuch overlap for the task of relation extraction.
Wealso found that adding more complex features maynot improve the performance much, and may evenhurt the performance.
A combination of featuresof different levels of complexity and from differentsentence representations, coupled with task-orientedfeature pruning, gives the best performance.In our future work, we will study how to auto-matically conduct task-oriented feature search, fea-ture pruning and feature weighting using statisticalmethods instead of heuristics.
In this study, we onlyconsidered features from the local context, i.e.
thesentence that contains the two arguments.
Some ex-isting studies use corpus-based statistics for relationextraction (Hasegawa et al, 2004).
In the future, wewill study the effectiveness of these global features.AcknowledgmentsThis work was in part supported by the National Sci-ence Foundation under award numbers 0425852 and0428472.
We thank Alessandro Moschitti for pro-viding the SVM tree kernel package.
We also thankMin Zhang for providing the implementation detailsof the convolution tree kernel for relation extraction.ReferencesRazvan C. Bunescu and Raymond J. Mooney.
2005a.A shortest path dependency kenrel for relation extrac-tion.
In Proceedings of HLT/EMNLP.Razvan C. Bunescu and Raymond J. Mooney.
2005b.Subsequence kernels for relation extraction.
In Pro-ceedings of NIPS.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofACL.Chad Cumby and Dan Roth.
2003.
On kernel methodsfor relational learning.
In Proceedings of ICML.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings ACL.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy modelsfor extracting relations.
In Proceedings of ACL.Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and EisakuMaeda.
2003.
Hierarchical directed acyclic graph ker-nel: Methods for structured natural language data.
InProceedings of ACL.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
Journal of Machine Learning Research,3:1083?1106.Min Zhang, Jie Zhang, and Jian Su.
2006a.
Exploringsyntactic features for relation extraction using a con-volution tree kernel.
In Proceedings of HLT/NAACL.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006b.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of ACL.Shubin Zhao and Ralph Grishman.
2005.
Extracting re-lations with integrated information using kernel meth-ods.
In Proceedings of ACL.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation extrac-tion.
In Proceedings of ACL.120
