Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 455?460,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTwo Easy Improvements to Lexical WeightingDavid Chiang and Steve DeNeefe and Michael PustUSC Information Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292{chiang,sdeneefe,pust}@isi.eduAbstractWe introduce two simple improvements to thelexical weighting features of Koehn, Och, andMarcu  (2003)  for  machine  translation: onewhich  smooths  the  probability  of  translatingword f to word e by simplifying English mor-phology, and one which conditions it  on thekind of training data that f and e co-occurredin.
These new variations lead to improvementsof up to +0.8 BLEU, with an average improve-ment of +0.6 BLEU across two language pairs,two genres, and two translation systems.1 IntroductionLexical weighting features (Koehn et al, 2003) es-timate the probability of a phrase pair or translationrule word-by-word.
In this paper, we introduce twosimple improvements to these features: one whichsmooths  the  probability  of  translating  word f toword e using English morphology, and one whichconditions it on the kind of training data that f ande co-occurred in.
These new variations lead to im-provements of up to+0.8BLEU, with an average im-provement of +0.6BLEU across two language pairs,two genres, and two translation systems.2 BackgroundSince there  are  slight  variations  in  how the  lexi-cal weighting features are computed, we begin bydefining the baseline lexical weighting features.
Iff = f1 ?
?
?
fn and e = e1 ?
?
?
em are a training sentencepair, let ai (1 ?
i ?
n) be the (possibly empty) set ofpositions in f that ei is aligned to.First, compute a word translation table from theword-aligned parallel text: for each sentence pair andeach i, letc( f j, ei)?
c( f j, ei) +1|ai|for j ?
ai (1)c(NULL, ei)?
c(NULL, ei) + 1 if |ai| = 0 (2)Thent(e | f ) = c( f , e)?e c( f , e)(3)where f can be NULL.Second, during phrase-pair extraction, store witheach phrase pair the alignments between the wordsin the phrase pair.
If it is observed with more thanone word alignment pattern, store the most frequentpattern.Third, for each phrase pair ( f?
, e?, a), computet(e?
| f? )
=|e?|?i=1??????????
?1|ai|?j?ait(e?i | f?
j) if |ai| > 0t(e?i | NULL) otherwise(4)This generalizes to synchronous CFG rules in the ob-vious way.Similarly, compute the reverse probability t( f?
| e?
).Then add two new model features?
log t(e?
| f? )
and ?
log t( f?
| e?
)455translationfeature (7) (8)small LM 26.7 24.3large LM 31.4 28.2?
log t(e?
| f? )
9.3 9.9?
log t( f?
| e?)
5.8 6.3Table 1: Although the language models prefer translation(8), which translates ??
and ??
as singular nouns, thelexical weighting features prefer translation (7), which in-correctly generates plural nouns.
All features are negativelog-probabilities, so lower numbers indicate preference.3 Morphological smoothingConsider the following example Chinese sentence:(5) ??
?W?n Ji?b?oWen Jiabao??bi?osh?said,,,???
?K?t?d?w?C?te d?Ivoire?sh?is??Zh?nggu?China?z?iin??F?izh?uAfrica?de?s?h?ogood??p?ngy?ufriend,,,?h?ogood??hu?b?npartner...
(6) Human: Wen Jiabao said that C?te d?Ivoire isa good friend and a good partner of China?s inAfrica.
(7) MT (baseline): Wen  Jiabao  said  that  Coted?Ivoire  is  China?s  good friends, and  goodpartners in Africa.
(8) MT (better):Wen Jiabao said that Cote d?Ivoireis  China?s  good friend and  good partner inAfrica.The baseline machine translation (7) incorrectly gen-erates plural nouns.
Even though the language mod-els (LMs) prefer singular nouns, the lexical weight-ing features prefer plural nouns (Table 1).1The reason for this is that the Chinese words do nothave any marking for number.
Therefore the infor-mation needed to mark friend and partner for num-ber must come from the context.
The LMs are ableto capture this context: the 5-gram is China?s good1The presence of an extra comma in translation (7) affectsthe LM scores only slightly; removing the comma would makethem 26.4 and 32.0.f e t(e | f ) t( f | e) tm(e | f ) tm( f | e)??
friends 0.44 0.44 0.47 0.48??
friend 0.21 0.58 0.19 0.48??
partners 0.44 0.60 0.40 0.53??
partner 0.13 0.40 0.17 0.53Table 2: The morphologically-smoothed lexical weight-ing features weaken the preference for singular or pluraltranslations, with the exception of t(friends | ??
).friend is observed in our large LM, and the 4-gramChina?s good friend in our small LM, but China?sgood friends is not observed in either LM.
Likewise,the 5-grams good friend and good partner and goodfriends and good partners are both observed in ourLMs, but neither good friend and good partners norgood friends and good partner is.By contrast, the lexical weighting tables (Table 2,columns 3?4), which ignore context, have a strongpreference for plural translations, except in the caseof t(??
| friend).
Therefore  we hypothesize  that,for Chinese-English translation, we should weakenthe lexical weighting features?
morphological pref-erences so that more contextual features can do theirwork.Running a morphological stemmer (Porter, 1980)on  the  English  side  of  the  parallel  data  gives  athree-way parallel text: for each sentence, we haveFrench f, English e, and stemmed English e?.
We canthen build two word translation tables, t(e?
| f ) andt(e | e?
), and form their producttm(e | f ) =?e?t(e?
| f )t(e | e?)
(9)Similarly, we can compute tm( f | e) in the oppositedirection.2(See Table 2, columns 5?6.)
These tablescan then be extended to phrase pairs or synchronousCFG rules as before and added as two new featuresof the model:?
log tm(e?
| f? )
and ?
log tm( f?
| e?
)The feature tm(e?
| f? )
does still prefer certain word-forms, as can be seen in Table 2.
But because e isgenerated from e?
and not from f , we are protectedfrom the situation where a rare f leads to poor esti-mates for the e.2Since the Porter stemmer is deterministic, we always havet(e?
| e) = 1.0, so that tm( f | e) = t( f | e?
), as seen in the lastcolumn of Table 2.456When  we  applied  an  analogous  approach  toArabic-English translation, stemming  both  Arabicand English, we generated very large lexicon tables,but saw no statistically significant change in BLEU.Perhaps this is  not surprising, because  in  Arabic-English translation (unlike Chinese-English transla-tion), the source language is morphologically richerthan the target language.
So we may benefit from fea-tures that preserve this information, while smoothingover morphological differences blurs important dis-tinctions.4 Conditioning on provenanceTypical machine translation systems are trained ona fixed set of training data ranging over a variety ofgenres, and if the genre of an input sentence is knownin advance, it is usually advantageous to use modelparameters tuned for that genre.Consider the following Arabic sentence, from aweblog (words written left-to-right):(10) ????wlElperhaps???h*Athis???AHdone???Ahmmain??????Alfrwqdifferences???bynbetween???Swrimages?????AnZmpsystems?????AlHkmruling????????AlmqtrHpproposed...
(11) Human: Perhaps this is one of the most impor-tant differences between the images of the pro-posed ruling systems.
(12) MT (baseline): This may be one of the mostimportant differences between pictures of theproposed ruling regimes.
(13) MT (better): Perhaps this is one of the most im-portant differences between the images of theproposed regimes.The Arabic word ????
can be translated asmay or per-haps (among others), with the latter more commonaccording to t(e | f ), as shown in Table 3.
But somegenres favor perhaps more or less strongly.
Thus,both translations (12) and (13) are good, but the lat-ter uses a slightly more informal register appropriateto the genre.Following Matsoukas et al (2009), we assign eachtraining sentence pair a set of binary features whichwe call s-features:t(e | f ) ts(e | f )f e ?
nw web bn un????
may 0.13 0.12 0.16 0.09 0.13????
perhaps 0.20 0.23 0.32 0.42 0.19Table 3: Different genres have different preferences forword translations.
Key: nw = newswire, web = Web, bn =broadcast news, un = United Nations proceedings.?
Whether the sentence pair came from a particu-lar genre, for example, newswire or web?
Whether the sentence pair came from a particu-lar collection, for example, FBIS or UNMatsoukas et  al.
(2009)  use these s-features  tocompute  weights  for  each  training  sentence  pair,which are in turn used for computing various modelfeatures.
They found that the sentence-level weightswere most helpful for computing the lexical weight-ing  features  (p.c.).
The  mapping  from  s-featuresto  sentence  weights  was  chosen  to  optimize  ex-pected TER on held-out data.
A drawback of thismethod is that we must now learn the mapping froms-features to sentence-weights and then the modelfeature weights.
Therefore, we tried an alternativethat incorporates s-features into the model itself.For each s-feature s, we compute new word trans-lation tables ts(e | f ) and ts( f | e) estimated fromonly those sentence pairsf on which s fires, and ex-tend them to phrases/rules as before.
The idea is touse these probabilities as new features in the model.However, two  challenges  arise: first, many  wordpairs are unseen for a given s, resulting in zero orundefined probabilities; second, this adds many newfeatures for each rule, which requires a lot of space.To address the problem of unseen word pairs, weuse Witten-Bell smoothing (Witten and Bell, 1991):t?s(e | f ) = ?
f sts(e | f ) + (1 ?
?
f s)t(e | f ) (14)?
f s =c( f , s)c( f , s) + d( f , s)(15)where c( f , s) is the number of times f has been ob-served in sentences with s-feature s, and d( f , s) is thenumber of e types observed aligned to f in sentenceswith s-feature s.For each s-feature s, we add two model features?
log t?s(e?
| f?
)t(e?
| f?
)and ?
log t?s( f?
| e?
)t( f?
| e?
)457Arabic-English Chinese-Englishnewswire web newswire websystem features Dev Test Dev Test Dev Test Dev Teststring-to-string baseline 47.1 43.8 37.1 38.4 28.7 26.0 23.2 25.9full247.7 44.2?37.4 39.0 29.5 26.8 23.8 26.3string-to-tree baseline 47.3 43.6 37.7 39.6 29.2 26.4 23.0 26.0full 47.7 44.3 38.3 40.2 29.8 27.1 23.4 26.6Table 4: Our variations on lexical weighting improve translation quality significantly across 16 different test conditions.All improvements are significant at the p < 0.01 level, except where marked with an asterisk (?
), indicating p < 0.05.In order to address the space problem, we use thefollowing heuristic: for any given rule, if the absolutevalue of one of these features is less than log 2, wediscard it for that rule.5 ExperimentsSetup We  tested  these  features  on  two  ma-chine  translation  systems: a  hierarchical  phrase-based (string-to-string) system (Chiang, 2005) anda syntax-based (string-to-tree) system (Galley et al,2004; Galley et al, 2006).
For Arabic-English trans-lation, both systems were trained on 190+220 mil-lion words of parallel data; for Chinese-English, thestring-to-string system was trained on 240+260 mil-lion words of parallel data, and the string-to-tree sys-tem, 58+65 million words.
Both used two languagemodels, one trained on the combined English sidesof the Arabic-English and Chinese-English data, andone trained on 4 billion words of English data.The baseline string-to-string system already incor-porates some simple provenance features: for eachs-feature s, there is a feature P(s | rule).
Both base-line also include a variety of other features (Chianget al, 2008; Chiang et al, 2009; Chiang, 2010).Both systems were trained using MIRA (Cram-mer et al, 2006; Watanabe et al, 2007; Chiang et al,2008) on a held-out set, then tested on two more sets(Dev and Test) disjoint from the data used for ruleextraction and for  MIRA training.
These datasetshave roughly 1000?3000 sentences (30,000?70,000words) and are drawn from test sets from the NISTMT evaluation and development sets from the GALEprogram.Individual  tests We  first  tested  morphologicalsmoothing  using  the  string-to-string  system  onChinese-English  translation.
The  morphologicallysmoothed system generated the improved translation(8) above, and generally gave a small improvement:task features DevChi-Eng nw baseline 28.7morph 29.1We then tested the provenance-conditioned fea-tures on both Arabic-English and Chinese-English,again using the string-to-string system:task features DevAra-Eng nw baseline 47.1(Matsoukas et al, 2009) 47.3provenance247.7Chi-Eng nw baseline 28.7provenance229.4The  translations  (12)  and  (13)  come  from  theArabic-English baseline and provenance systems.For Arabic-English, we also compared against lex-ical  weighting  features  that  use  sentence  weightskindly provided to us by Matsoukas et al Our fea-tures performed better, although it should be notedthat those sentence weights had been optimized fora different translation model.Combined  tests Finally, we  tested  the  featuresacross a wider range of tasks.
For Chinese-Englishtranslation, we  combined  the  morphologically-smoothed  and  provenance-conditioned  lexicalweighting  features; for  Arabic-English, we  con-tinued  to  use  only  the  provenance-conditionedfeatures.
We  tested  using  both  systems, and  onboth  newswire  and  web  genres.
The  results  areshown in Table 4.
The features produce statisticallysignificant improvements across all 16 conditions.2In these systems, an error crippled the t( f | e), tm( f | e), andts( f | e) features.
Time did not permit rerunning all of these sys-tems with the error fixed, but partial results suggest that it didnot have a significant impact.458-0.4-0.3-0.2-0.1 00.1 0.2 0.3 0.4 0.5-0.8 -0.6 -0.4 -0.2  0  0.2  0.4  0.6  0.8WebNewswirebc bn LDC2005T06 NameEntityLDC2006E24LDC2006E92LDC2006G05LDC2007E08LDC2007E101LDC2007E103 LDC2008G05lexiconng nwNewsExplorer UNwebwlFigure 1: Feature  weights  for  provenance-conditioned  features: string-to-string, Chinese-English, web  versusnewswire.
A higher weight indicates a more useful source of information, while a negative weight indicates a lessuseful or possibly problematic source.
For clarity, only selected points are labeled.
The diagonal line indicates wherethe two weights would be equal relative to the original t(e | f ) feature weight.Figure 1 shows the feature weights obtained forthe provenance-conditioned features ts( f | e) in thestring-to-string Chinese-English system, trained onnewswire and web data.
On the diagonal are cor-pora that were equally useful in either genre.
Surpris-ingly, the UN data received strong positive weights,indicating usefulness in both genres.
Two lists  ofnamed entities received large weights: the LDC list(LDC2005T34)  in  the  positive  direction  and  theNewsExplorer  list  in  the  negative  direction, sug-gesting  that  there  are  noisy  entries  in  the  latter.The corpus LDC2007E08, which contains paralleldata mined from comparable corpora (Munteanu andMarcu, 2005), received strong negative weights.Off the diagonal are corpora favored in only onegenre or the other: above, we see that the wl (we-blog)  and ng (newsgroup)  genres  are  more help-ful for web translation, as expected (although weboddly seems less helpful), as well as LDC2006G05(LDC/FBIS/NVTC Parallel Text V2.0).
Below arecorpora  more  helpful  for  newswire  translation,like LDC2005T06 (Chinese News Translation TextPart 1).6 ConclusionMany  different  approaches  to  morphology  andprovenance in machine translation are possible.
Wehave chosen to implement our approach as exten-sions  to  lexical  weighting  (Koehn  et  al., 2003),which is nearly ubiquitous, because it is defined atthe level of word alignments.
For this reason, thefeatures we have introduced should be easily ap-plicable to a wide range of phrase-based, hierarchi-cal phrase-based, and syntax-based systems.
Whilethe improvements obtained using them are not enor-mous, we have demonstrated that they help signif-icantly across many different conditions, and oververy strong baselines.
We therefore fully expect thatthese  new  features  would  yield  similar  improve-ments in other systems as well.AcknowledgementsWe would like to thank Spyros Matsoukas and col-leagues at BBN for providing their sentence-levelweights  and  important  insights  into  their  corpus-weighting work.
This work was supported in part byDARPA contract HR0011-06-C-0022 under subcon-tract to BBN Technologies.459ReferencesDavid Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of  syntactic  and struc-tural translation features.
In Proc.
EMNLP 2008, pages224?233.David  Chiang, Kevin  Knight, and  Wei  Wang.
2009.11,001 new features for statistical machine translation.In Proc.
NAACL HLT, pages 218?226.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ACL 2005,pages 263?270.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proc.
ACL, pages 1443?1452.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.HLT-NAACL 2004, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe,Wei Wang, and Ignacio Thayer.2006.
Scalable inference and training of context-richsyntactic translation models.
In Proc.
COLING-ACL2006, pages 961?968.Philipp  Koehn, Franz Josef  Och, and  Daniel  Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003, pages 127?133.Spyros  Matsoukas, Antti-Veikko I.  Rosti, and  BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proc.
EMNLP 2009,pages 708?717.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31:477?504.M.
F. Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proc.
EMNLP-CoNLL2007, pages 764?773.Ian H.  Witten  and  Timothy C.  Bell.
1991.
Thezero-frequency problem: Estimating the probabilitiesof novel events in adaptive text compression.
IEEETrans.
Information Theory, 37(4):1085?1094.460
