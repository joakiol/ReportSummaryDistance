Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 430?438,Beijing, August 2010A Structured Vector Space Model for Hidden Attribute Meaningin Adjective-Noun PhrasesMatthias Hartung and Anette FrankComputational Linguistics DepartmentHeidelberg University{hartung, frank}@cl.uni-heidelberg.deAbstractWe present an approach to model hid-den attributes in the compositional se-mantics of adjective-noun phrases in adistributional model.
For the represen-tation of adjective meanings, we refor-mulate the pattern-based approach for at-tribute learning of Almuhareb (2006) ina structured vector space model (VSM).This model is complemented by a struc-tured vector space representing attributedimensions of noun meanings.
The com-bination of these representations along thelines of compositional semantic principlesexposes the underlying semantic relationsin adjective-noun phrases.
We show thatour compositional VSM outperforms sim-ple pattern-based approaches by circum-venting their inherent sparsity problems.1 IntroductionIn formal semantic theory, the compositional se-mantics of adjective-noun phrases can be modeledin terms of selective binding (Pustejovsky, 1995),i.e.
the adjective selects one of possibly severalroles or attributes1 from the semantics of the noun.
(1) a. a blue carb.
COLOR(car)=blueIn this paper, we define a distributional frame-work that models the compositional process un-derlying the modification of nouns by adjectives.1In the original statement of the theory, adjectives se-lect qualia roles that can be considered as collections of at-tributes.We focus on property-denoting adjectives as theyare valuable for acquiring concept representationsfor, e.g., ontology learning.
An approach for au-tomatic subclassification of property-denoting ad-jectives is presented in Hartung and Frank (2010).Our goal is to expose, for adjective-noun phrasesas in (1a), the attribute in the semantics of thenoun that is selected by the adjective, while notbeing overtly realized on the syntactic level.
Thesemantic information we intend to capture for (1a)is formalized in (1b).Ideally, this kind of knowledge could be ex-tracted from corpora by searching for patterns thatparaphrase (1a), e.g.
the color of the car is blue.However, linguistic patterns that explicitly relatenouns, adjectives and attributes are very rare.We avoid these sparsity issues by reducingthe triple r=?noun, attribute, adjective?
thatencodes the relation illustrated in (1b) to tu-ples r?=?noun, attribute?
and r?
?=?attribute,adjective?, as suggested by Turney and Pantel(2010) for similar tasks.
Both r?
and r??
can beobserved much more frequently in text corporathan r. Moreover, this enables us to model adjec-tive and noun meanings as distinct semantic vec-tors that are built over attributes as dimensions.Based on these semantic representations, we makeuse of vector composition operations in order toreconstruct r from r?
and r??.
This, in turn, al-lows us to infer complete noun-attribute-adjectivetriples from individually acquired noun-attributeand adjective-attribute representations.The contributions of our work are as follows:(i) We propose a framework for attribute selectionbased on structured vector space models (VSM),using as meaning dimensions attributes elicited430by adjectives; (ii) we complement this novel rep-resentation of adjective meaning with structuredvectors for noun meanings similarly built on at-tributes as meaning dimensions; (iii) we propose acomposition of these representations that mirrorsprinciples of compositional semantics in mappingadjective-noun phrases to their corresponding on-tological representation; (iv) we propose and eval-uate several metrics for the selection of meaning-ful components from vector representations.2 Related WorkAdjective-noun meaning composition has notbeen addressed in a distributional framework be-fore (cf.
Mitchell and Lapata (2008)).
Our ap-proach leans on related work on attribute learningfor ontology induction and recent work in distri-butional semantics.Attribute learning.
Early approaches to at-tribute learning include Hatzivassiloglou andMcKeown (1993), who cluster adjectives that de-note values of the same attribute.
A weaknessof their work is that the type of the attributecannot be made explicit.
More recent attemptsto attribute learning from adjectives are Cimiano(2006) and Almuhareb (2006).
Cimiano uses at-tributes as features to arrange sets of concepts in alattice.
His approach to attribute acquisition har-nesses adjectives that occur frequently as conceptmodifiers in corpora.
The association of adjec-tives with their potential attributes is performed bydictionary look-up in WordNet (Fellbaum, 1998).Similarly, Almuhareb (2006) uses adjectives andattributes as (independent) features for the pur-pose of concept learning.
He acquires adjective-attribute pairs using a pattern-based approach.As a major limitation, these approaches areconfined to adjective-attribute pairs.
The poly-semy of adjectives that can only be resolved in thecontext of the modified noun is entirely neglected.From a methodological point of view, our workis similar to Almuhareb?s, as we will also buildon lexico-syntactic patterns for attribute selection.However, we extend the task to involve nouns andrephrase his approach in a distributional frame-work based on the composition of structured vec-tor representations.Distributional semantics.
We observe two re-cent trends in distributional semantics research:(i) The use of VSM tends to shift from mea-suring unfocused semantic similarity to captur-ing increasingly fine-grained semantic informa-tion by incorporating more linguistic structure.Following Baroni and Lenci (to appear), we re-fer to such models as structured vector spaces.
(ii) Distributional methods are no longer confinedto word meaning, but are noticeably extended tocapture meaning on the phrase level.
Prominentexamples for (i) are Pado?
and Lapata (2007) andRothenha?usler and Schu?tze (2009) who use syn-tactic dependencies rather than single word co-occurrences as dimensions of semantic spaces.Erk and Pado?
(2008) extend this idea to the ar-gument structure of verbs, while also accountingfor compositional meaning aspects by modellingpredication over arguments.
Hence, their work isalso representative for (ii).Baroni et al (2010) use lexico-syntactic pat-terns to represent concepts in a structured VSMwhose dimensions are interpretable as empiricalmanifestations of properties.
We rely on similartechniques for the acquisition of structured vec-tors, whereas our work focusses on exposing thehidden meaning dimensions involved in composi-tional processes underlying concept modification.The commonly adopted method for modellingcompositionality in VSM is vector composition(Mitchell and Lapata, 2008; Widdows, 2008).Showing the benefits of vector composition forlanguage modelling, Mitchell and Lapata (2009)emphasize its potential to become a standardmethod in NLP.The approach pursued in this paper builds onboth lines of research sketched in (i) and (ii) inthat we model a specific meaning layer in the se-mantics of adjectives and nouns in a structuredVSM.
Vector composition is used to expose theirhidden meaning dimensions on the phrase level.3 Structured Vector Representations forAdjective-Noun Meaning3.1 MotivationContrary to prior work, we model attribute selec-tion as involving triples of nouns, attributes and431COLORDIRECTIONDURATIONSHAPESIZESMELLSPEEDTASTETEMPERATUREWEIGHTve 1 1 0 1 45 0 4 0 0 21vb 14 38 2 20 26 0 45 0 0 20ve ?
vb 14 38 0 20 1170 0 180 0 0 420ve + vb 15 39 2 21 71 0 49 0 0 41Figure 1: Vectors for enormous (ve) and ball (vb)adjectives, as in (2).
The triple r can be bro-ken down into tuples r?
= ?noun, attribute?
andr??
= ?attribute, adjective?.
Previous learningapproaches focussed on r?
(Cimiano, 2006) or r??
(Almuhareb, 2006) only.
(2) a. a bluevalue carconceptb.
ATTR(concept) = valueIn semantic composition of adjective-nouncompounds, the adjective (e.g.
blue) contributes avalue for an attribute (here: COLOR) that charac-terizes the concept evoked by the noun (e.g.
car).Thus, the attribute in (2) constitutes a ?hiddenvariable?
that is not overtly expressed in (2a), butconstitutes the central axis that relates r?
and r?
?.Structured vectors built on extraction patterns.We model the semantics of adjectives and nounsin a structured VSM that conveys the hidden re-lationship in (2).
The dimensions of the modelare defined by attributes, such as COLOR, SIZEor SPEED, while the vector components are deter-mined on the basis of carefully selected acquisi-tion patterns that are tailored to capturing the par-ticular semantic information of interest for r?
andr??.
In this respect, lexico-syntactic patterns servea similar purpose as dependency relations in Pado?and Lapata (2007) or Rothenha?usler and Schu?tze(2009).
The upper part of Fig.
1 displays exam-ples of vectors we build for adjectives and nouns.Composing vectors along hidden dimensions.The fine granularity of lexico-syntactic patternsthat capture the triple r comes at the cost of theirsparsity when applied to corpus data.
Therefore,we construct separate vector representations forr?
and r??.
Eventually, these representations arejoined by vector composition to reconstruct thetriple r. Apart from avoiding sparsity issues,this compositional approach has several prospectsfrom a linguistic perspective as well.Ambiguity and disambiguation.
Building vec-tors with attributes as meaning dimensions en-ables us to model (i) ambiguity of adjectives withregard to the attributes they select, and (ii) the dis-ambiguation capacity of adjective and noun vec-tors when considered jointly.
Consider, for exam-ple, the phrase enormous ball that is ambiguousfor two reasons: enormous may select a set of pos-sible attributes (SIZE or WEIGHT, among others),while ball elicits several attributes in accordancewith its different word senses2.
As seen in Fig.
1,these ambiguities are nicely captured by the sep-arate vector representations for the adjective andthe noun (upper part); by composing these repre-sentations, the ambiguity is resolved (lower part).3.2 Building a VSM for Adjective-NounMeaningIn this section, we introduce the methods we ap-ply in order to (i) acquire vector representationsfor adjectives and nouns, (ii) select appropriate at-tributes from them, and (iii) compose them.3.2.1 Attribute Acquisition PatternsWe use the following patterns3 for the ac-quisition of vectors capturing the tuple r??
=?attribute, adjective?.
Even though some ofthese patterns (A1 and A4) match triples of nouns,attributes and adjectives, we only use them for theextraction of binary tuples (underlined), thus ab-stracting from the modified noun.
(A1) ATTR of DT?
NN is|was JJ(A2) DT?
RB?
JJ ATTR(A3) DT?
JJ or JJ ATTR(A4) DT?
NN?s ATTR is|was JJ(A5) is|was|are|were JJ in|of ATTRTo acquire noun vectors capturing the tupler?
= ?noun, attribute?, we rely on the follow-ing patterns.
Again, we only extract pairs, as indi-cated by the underlined elements.
(N1) NN with|without DT?
RB?
JJ?
ATTR(N2) DT ATTR of DT?
RB?
JJ?
NN(N3) DT NN?s RB?
JJ?
ATTR(N4) NN has|had a|an RB?
JJ?
ATTR2WordNet senses for the noun ball include, among others:1. round object [...] in games; 2. solid projectile, 3. objectwith a spherical shape, 4. people [at a] dance.3Some of these patterns are taken from Almuhareb (2006)and Sowa (2000).
The descriptions rely on the Penn Tagset(Marcus et al, 1999).
?
marks optional elements.4323.2.2 Target FilteringSome of the adjectives extracted by A1-A5 arenot property-denoting and thus represent noise.This affects in particular pattern A2, which ex-tracts adjectives like former or more, or relationalones such as economic or geographic.This problem may be addressed in differentways: By target filtering, extractions can bechecked against a predicative pattern P1 that issupposed to apply to property-denoting adjectivesonly.
Vectors that fail this test are suppressed.
(P1) DT NN is|was JJAlternatively, extractions obtained from low-confidence patterns can be awarded reducedweights by means of a pattern value function (de-fined in 3.3; cf.
Pantel and Pennacchiotti (2006)).3.2.3 Attribute SelectionWe intend to use the acquired vectors in orderto detect attributes that are implicit in adjective-noun meaning.
Therefore, we need a methodthat selects appropriate attributes from each vec-tor.
While, in general, this task consists in dis-tinguishing semantically meaningful dimensionsfrom noise, the requirements are different depend-ing on whether attributes are to be selected fromadjective or noun vectors.
This is illustrated inFig.
1, a typical configuration, with one vectorrepresenting a typical property-denoting adjectivethat exhibits relatively strong peaks on one ormore dimensions, whereas noun vectors show atendency for broad and flat distributions over theirdimensions.
This suggests using a strict selectionfunction (choosing few very prominent dimen-sions) for adjectives and a less restrictive one (li-censing the inclusion of more dimensions of lowerrelative prominence) for nouns.
Moreover, we areinterested in finding a selection function that re-lies on as few free parameters as possible in orderto avoid frequency or dimensionality effects.MPC Selection (MPC).
An obvious methodfor attribute selection is to choose the most promi-nent component from any vector (i.e., the highestabsolute value).
If a vector exhibits several peaks,all other components are rejected, their relativeimportance notwithstanding.
MPC obviously failsto capture polysemy of targets, which affects ad-jectives such as hot, in particular.Threshold Selection (TSel).
TSel recasts theapproach of Almuhareb (2006), in selecting all di-mensions as attributes whose components exceeda frequency threshold.
This avoids the drawbackof MPC, but introduces a parameter that needs tobe optimized.
Also, it is difficult to apply absolutethresholds to composed vectors, as the range oftheir components is subject to great variation, andit is unclear whether the method will scale withincreased dimensionality.Entropy Selection (ESel).
In information the-ory, entropy measures the average uncertainty ina probability distribution (Manning and Schu?tze,1999).
We define the entropy H(v) of avector v=?v1, .
.
.
, vn?
over its components asH(v) = ?
?ni=1 P (vi) log P (vi), where P (vi) =vi/?ni=1 vi.We use H(v) to assess the impact of singularvector components on the overall entropy of thevector: We expect entropy to detect componentsthat contribute noise, as opposed to those that con-tribute important information.We define an algorithm for entropy-based at-tribute selection that returns a list of informa-tive dimensions.
The algorithm successively sup-presses (combinations of) vector components oneby one.
Given that a gain of entropy is equiva-lent to a loss of information and vice versa, we as-sume that every combination of components thatleads to an increase in entropy when being sup-pressed is actually responsible for a substantialamount of information.
The algorithm includes aback-off to MPC for the special case that a vectorcontains a single peak (i.e., H(v) = 0), so that,in principle, it should be applicable to vectors ofany kind.
Vectors with very broad distributionsover their dimensions, however, pose a problemto this method.
For ball in Fig.
1, for instance, themethod does not select any dimension.Median Selection (MSel).
As a further methodwe rely on the median m that can be informallydefined as the value that separates the upper fromthe lower half of a distribution (Krengel, 2003).It is less restrictive than MPC and TSel and over-comes the particular drawback of ESel.
Using thismeasure, we choose all dimensions whose compo-nents exceed m. Thus, for the vector representing433Pattern Label # Hits (Web) # Hits (ukWaC)A1 2249 815A2 36282 72737A3 3370 1436A4 ?
7672A5 ?
3768N1 ?
682N2 ?
5073N3 ?
953N4 ?
56Table 1: Number of pattern hits on the Web (Al-muhareb, 2006) and on ukWaCball, WEIGHT, DIRECTION, SHAPE, SPEED andSIZE are selected.3.2.4 Vector CompositionWe use vector composition as a hinge to com-bine adjective and noun vectors in order to recon-struct the triple r=?noun, attribute, adjective?.Mitchell and Lapata (2008) distinguish two majorclasses of vector composition operations, namelymultiplicative and additive operations, that can beextended in various ways.
We use their standarddefinitions (denoted ?
and +, henceforth).
Forour task, we expect ?
to perform best as it comesclosest to the linguistic function of intersective ad-jectives, i.e.
to select dimensions that are promi-nent both for the adjective and the noun, whereas+ basically blurs the vector components, as canbe seen in the lower part of Fig.
1.3.3 Model ParametersWe follow Pado?
and Lapata (2007) in defining asemantic space as a matrix M = B?T relating aset of target elements T to a set of basis elementsB.
Further parameters and their instantiations weuse in our model are described below.
We use p todenote an individual lexico-syntactic pattern.The basis elements of our VSM are nouns de-noting attributes.
For comparison, we use the at-tributes selected by Almuhareb (2006): COLOR,DIRECTION, DURATION, SHAPE, SIZE, SMELL,SPEED, TASTE, TEMPERATURE, WEIGHT.The context selection function cont(t) deter-mines the set of patterns that contribute to the rep-resentation of each target word t ?
T .
These arethe patterns A1-A5 and N1-N4 (cf.
Section 3.2.1).The target elements represented in the vectorspace comprise all adjectives TA that match thepatterns A1 to A5 in the corpus, provided they ex-ceed a frequency threshold n. During develop-ment, n was set to 5 in order to filter noise.As for the target nouns TN , we rely on a repre-sentative dataset compiled by Almuhareb (2006).It contains 402 nouns that are balanced with re-gard to semantic class (according to the WordNetsupersenses), ambiguity and frequency.As association measure that captures thestrength of the association between the elementsof B and T , we use raw frequency counts4 as ob-tained from the PoS-tagged and lemmatized ver-sion of the ukWaC corpus (Baroni et al, 2009).Table 1 gives an overview of the number of hitsreturned by these patterns.The basis mapping function ?
creates the di-mensions of the semantic space by mapping eachextraction of a pattern p to the attribute it contains.The pattern value function enables us to sub-divide dimensions along particular patterns.
Weexperimented with two instantiations: pvconstconsiders, for each dimension, all patterns, whileweighting them equally.
pvf (p) awards the ex-tractions of pattern p with weight 1, while settingthe weights for all patterns different from p to 0.4 ExperimentsWe evaluate the performance of the structuredVSM on the task of inferring attributes fromadjective-noun phrases in three experiments: InExp1 and Exp2, we evaluate vector representa-tions capturing r?
and r??
independently of one an-other.
Exp3 investigates the selection of hiddenattributes from vector representations constructedby composition of adjective and noun vectors.We compare all results against different goldstandards.
In Exp1, we follow Almuhareb (2006),evaluating against WordNet 3.0.
For Exp2 andExp3, we establish gold standards manually: ForExp2, we construct a test set of nouns annotatedwith their corresponding attributes.
For Exp3, wemanually annotate adjective-noun phrases withthe attributes appropriate for the whole phrase.
Allexperiments are evaluated in terms of precision,recall and F1 score.4We experimented with the conditional probability ratioproposed by Mitchell and Lapata (2009).
As it performedworse on our data, we did not consider it any further.4344.1 Exp1: Attribute Selection for AdjectivesThe first experiment evaluates the performance ofstructured vector representations on attribute se-lection for adjectives.
We compare this modelagainst a re-implementation of Almuhareb (2006).Experimental settings and gold standard.
Toreconstruct Almuhareb?s approach, we ran his pat-terns A1-A3 on the ukWaC corpus.
Table 1 showsthe number of hits when applied to the Web (Al-muhareb, 2006) vs. ukWaC.
A1 and A3 yield lessextractions on ukWaC as compared to the Web.5We introduced two additional patterns, A4 andA5, that contribute about 10,000 additional hits.We adopted Almuhareb?s manually chosen thresh-olds for attribute selection for A1-A3; for A4, A5and a combination of all patterns, we manually se-lected optimal thresholds.We experiment with pvconst and all variants ofpvf (p) for pattern weighting (see sect.
3.3).
Forattribute selection, we compare TSel (as used byAlmuhareb), ESel and MSel.The gold standard consists of all adjectives thatare linked to at least one of the ten attributeswe consider by WordNet?s attribute relation(1063 adjectives in total).Evaluation results.
Results for Exp1 are dis-played in Table 2.
The settings of pv are given inthe rows, the attribute selection methods (in com-bination with target filtering6) in the columns.The results for our re-implementation of Al-muhareb?s individual patterns are comparable tohis original figures7, except for A3 that seems tosuffer from quantitative differences of the under-lying data.
Combining all patterns leads to animprovement in precision over (our reconstruc-tion of) Almuhareb?s best individual pattern whenTSel and target filtering are used in combina-tion.
MPC and MSel perform worse (not reportedhere).
As for target filtering, A1 and A3work best.Both TSel and ESel benefit from the combina-tion with the target filter, where the largest im-provement (and the best overall result) is observ-5The difference for A2 is an artifact of Almuhareb?s ex-traction methodology.6Regarding target filtering, we only report the best filterpattern for each configuration.7P(A1)=0.176, P(A2)=0.218, P(A3)=0.504MPC ESel MSelP R F P R F P R Fpvf (N1) 0.22 0.06 0.10 0.29 0.04 0.07 0.22 0.09 0.13pvf (N2) 0.29 0.18 0.23 0.20 0.06 0.09 0.28 0.39 0.33pvf (N3) 0.34 0.05 0.09 0.20 0.02 0.04 0.25 0.08 0.12pvf (N4) 0.25 0.02 0.04 0.29 0.02 0.03 0.26 0.02 0.05pvconst 0.29 0.18 0.22 0.20 0.06 0.09 0.28 0.43 0.34Table 3: Evaluation results for Experiment 2able for ESel on pattern A1 only.
This is thepattern that performs worst in Almuhareb?s orig-inal setting.
From this, we conclude that bothESel and target filtering are valuable extensionsto pattern-based structured vector spaces if preci-sion is in focus.
This also underlines a findingof Rothenha?usler and Schu?tze (2009) that VSMsintended to convey specific semantic informationrather than mere similarity benefit primarily froma linguistically adequate choice of contexts.Similar to Almuhareb, recall is problematic.Even though ESel leads to slight improvements,the scores are far from satisfying.
With Al-muhareb, we note that this is mainly due to ahigh number of extremely fine-grained adjectivesin WordNet that are rare in corpora.84.2 Exp2: Attribute Selection for NounsExp2 evaluates the performance of attribute selec-tion from noun vectors tailored to the tuple r?
?.Construction of the gold standard.
For eval-uation, we created a gold standard by manuallyannotating a set of nouns with attributes.
Thisgold standard builds on a random sample ex-tracted from TN (cf.
section 3.3).
Running N1-N4 on ukWaC returned semantic vectors for 216concepts.
From these, we randomly sampled 100concepts that were manually annotated by threehuman annotators.The annotators were provided a matrix consist-ing of the nouns and the set of ten attributes foreach noun.
Their task was to remove all inappro-priate attributes.
They were free to decide howmany attributes to accept for each noun.
In orderto deal with word sense ambiguity, the annotatorswere instructed to consider all senses of a nounand to retain every attribute that was acceptablefor at least one sense.Inter-annotator agreement amounts to ?= 0.69(Fleiss, 1971).
Cases of disagreement were ad-judicated by majority-voting.
The gold standard435Almuhareb (reconstr.)
VSM (TSel + Target Filter) VSM (ESel) VSM (ESel + Target Filter)P R F Thr P R F Patt Thr P R F P R F Pattpvf (A1) = 1 0.183 0.005 0.009 5 0.300 0.004 0.007 A3 5 0.231 0.045 0.076 0.519 0.035 0.065 A3pvf (A2) = 1 0.207 0.039 0.067 50 0.300 0.033 0.059 A1 50 0.084 0.136 0.104 0.240 0.049 0.081 A3pvf (A3) = 1 0.382 0.020 0.039 5 0.403 0.014 0.028 A1 5 0.192 0.059 0.090 0.375 0.027 0.050 A1pvf (A4) = 1 0.301 0.020 0.036 A3 10 0.135 0.055 0.078 0.272 0.020 0.038 A1pvf (A5) = 1 0.295 0.008 0.016 A3 24 0.105 0.056 0.073 0.315 0.024 0.045 A3pvconst 0.420 0.024 0.046 A1 183 0.076 0.152 0.102 0.225 0.054 0.087 A3Table 2: Evaluation results for Experiment 1contains 424 attributes for 100 nouns.Evaluation results.
Results for Exp2 are givenin Table 3.
Performance is lower in comparison toExp1.
We hypothesize that the tuple r??
might notbe fully captured by overt linguistic patterns.
Thisneeds further investigation in future research.Against this background, MPC is relatively pre-cise, but poor in terms of recall.
ESel, beingdesigned to select more than one prominent di-mension, counterintuitively fails to increase re-call, suffering from the fact that many noun vec-tors show a rather flat distribution without anystrong peak.
MSel turns out to be most suitablefor this task: Its precision is comparable to MPC(with N3 as an outlier), while recall is consider-ably higher.
Overall, these results indicate that at-tribute selection for adjectives and nouns, thoughsimilar, should be viewed as distinct tasks that re-quire different attribute selection methods.4.3 Exp3: Attribute Selection forAdjective-Noun PhrasesIn this experiment, we compose noun and adjec-tive vectors in order to yield a new combined rep-resentation.
We investigate whether the seman-tic information encoded by the components of thisnew vector is sufficiently precise to disambiguatethe attribute dimensions of the original represen-tations (see section 3.1) and, thus, to infer hiddenattributes from adjective-noun phrases (see (2)) asadvocated by Pustejovsky (1995).Construction of the gold standard.
For evalu-ation, we created a manually annotated test set ofadjective-noun phrases.
We selected a subset ofproperty-denoting adjectives that are appropriatemodifiers for the nouns from TN using the pred-icative pattern P1 (see sect.
3) on ukWaC.
This8For instance: bluish-lilac, chartreuse or pink-lavenderas values of the attribute COLOR.yielded 2085 adjective types that were further re-duced to 386 by frequency filtering (n = 5).
Wesampled our test set from all pairs in the carte-sian product of the 386 adjectives and 216 nouns(cf.
Exp2) that occurred at least 5 times in a sub-section of ukWaC.
To ensure a sufficient numberof ambiguous adjectives in the test set, samplingproceeded in two steps: First, we sampled fournouns each for a manual selection of 15 adjectivesof all ambiguity levels in WordNet.
This leads to60 adjective-noun pairs.
Second, another 40 pairswere sampled fully automatically.The test set was manually annotated by thesame annotators as in Exp2.
They were asked toremove all attributes that were not appropriate fora given adjective-noun pair, either because it is notappropriate for the noun or because it is not se-lected by the adjective.
Further instructions wereas in Exp2, in particular regarding ambiguity.The overall agreement is ?=0.67.
After adjudi-cation by majority voting, the resulting gold stan-dard contains 86 attributes for 76 pairs.
24 pairscould not be assigned any attribute, either becausethe adjective did not denote a property, as in pri-vate investment, or the most appropriate attributewas not offered, as in blue day or new house.We evaluate the vector composition methodsdiscussed in section 3.2.4.
Individual vectors forthe adjectives and nouns from the test pairs wereconstructed using all patterns A1-A5 and N1-N4.For attribute selection, we tested MPC, ESel andMSel.
The results are compared against threebaselines: BL-P implements a purely pattern-based method, i.e.
running the patterns that ex-tract the triple r (A1, A4, N1, N3 and N4, withJJ and NN instantiated accordingly) on the pairsfrom the test set.
BL-N and BL-Adj are back-offsfor vector composition, taking the respective nounor adjective vector, as investigated in Exp1 andExp2, as surrogates for a composed vector.436MPC ESel MSelP R F P R F P R F?
0.60 0.58 0.59 0.63 0.46 0.54 0.27 0.72 0.39+ 0.43 0.55 0.48 0.42 0.51 0.46 0.18 0.91 0.30BL-Adj 0.44 0.60 0.50 0.51 0.63 0.57 0.23 0.83 0.36BL-N 0.27 0.35 0.31 0.37 0.29 0.32 0.17 0.73 0.27BL-P 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Table 4: Evaluation results for Experiment 3Evaluation results.
Results are given in Table4.
Attribute selection based on the composition ofadjective and noun vectors yields a considerableimprovement of both precision and recall as com-pared to the individual results obtained in Exp1and Exp2.
Comparing the results of Exp3 againstthe baselines reveals two important aspects of ourwork.
First, the complete failure of BL-P9 un-derlines the attractiveness of our method to buildstructured vector representations from patterns ofreduced complexity.
Second, vector compositionis suitable for selecting hidden attributes fromadjective-noun phrases that are jointly encodedby adjective and noun vectors: Both compositionmethods we tested outperform BL-N.However, the choice of the composition methodmatters: ?
performs best with a maximum pre-cision of 0.63.
This confirms our expectationthat vector multiplication is a good approxima-tion for attribute selection in adjective-noun se-mantics.
Being outperformed by BL-Adj in mostcategories, + is less suited for this task.All selection methods outperform BL-Adj inprecision.
Comparing MPC and ESel, ESelachieves better precision when combined with the?-operator, while doing worse for recall.
Therobust performance of MPC is not surprising asthe test set contains only ten adjective-noun pairsthat are still ambiguous with regard to the at-tributes they elicit.
The stronger performance ofthe entropy-based method with the ?-operator ismainly due to its accuracy on detecting false posi-tives, in that it is able to return ?empty?
selections.In terms of precision, MSel did worse in general,while recall is decent.
This underlines that vectorcomposition generally promotes meaningful com-ponents, but MSel is too inaccurate to select them.Given the performance of the baselines andthe noun vectors in Exp2, we consider this avery promising result for our approach to attribute9The patterns used yield no hits for the test pairs at all.selection from structured vector representations.The results also corroborate the insufficiency ofprevious approaches to attribute learning from ad-jectives alone.5 Conclusions and OutlookWe proposed a structured VSM as a frameworkfor inferring hidden attributes from the composi-tional semantics of adjective-noun phrases.By reconstructing Almuhareb (2006), weshowed that structured vector representations ofadjective meaning consistently outperform sim-ple pattern-based learning, up to 13 pp.
in preci-sion.
A combination of target filtering and pat-tern weighting turned out to be effective here, byselecting particulary meaningful lexico-syntacticcontexts and filtering adjectives that are notproperty-denoting.
Further studies need to inves-tigate this phenomenon and its most appropriateformulation in a vector space framework.Moreover, the VSM offers a natural represen-tation for sense ambiguity of adjectives.
Compar-ing attribute selection methods on adjective andnoun vectors shows that they are sensitive to thedistributional structure of the vectors, and need tobe chosen with care.
Future work will investigatethese selection methods in high-dimensional vec-tors spaces, by using larger sets of attributes.Exp3 shows that the composition of pattern-based adjective and noun vectors robustly reflectsaspects of meaning composition in adjective-nounphrases, with attributes as a hidden dimension.It also suggests that composition is effective indisambiguation of adjective and noun meanings.This hypothesis needs to be substantiated in fur-ther experiments.Finally, we showed that composition of vectorsrepresenting complementary meaning aspects canbe beneficial to overcome sparsity effects.
How-ever, our compositional approach meets its lim-its if the patterns capturing adjective and nounmeaning in isolation are too sparse to acquire suf-ficiently populated vector components from cor-pora.
For future work, we envisage using vectorsimilarity to acquire structured vectors for infre-quent targets from semantic spaces that conveyless linguistic structure to address these remain-ing sparsity issues.437ReferencesAlmuhareb, Abdulrahman.
2006.
Attributes in Lexi-cal Acquisition.
Ph.D. Dissertation, Department ofComputer Science, University of Essex.Baroni, Marco and Alessandro Lenci.
to appear.Distributional Memory.
A General Framework forCorpus-based Semantics.
Computational Linguis-tics.Baroni, Marco, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The wacky wide web:A collection of very large linguistically processedweb-crawled corpora.
Journal of Language Re-sources and Evaluation, 43(3):209?226.Baroni, Marco, Brian Murphy, Eduard Barbu, andMassimo Poesio.
2010.
Strudel.
A Corpus-basedSemantic Model of Based on Properties and Types.Cognitive Science, 34:222?254.Cimiano, Philipp.
2006.
Ontology Learning and Pop-ulation from Text.
Algorithms, Evaluation and Ap-plications.
Springer.Erk, Katrin and Sebastian Pado?.
2008.
A StructuredVector Space Model for Word Meaning in Context.In Proceedings of EMNLP, Honolulu, HI.Fellbaum, Christiane, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press, Cam-bridge, Mass.Fleiss, Joseph L. 1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.Hartung, Matthias and Anette Frank.
2010.
ASemi-supervised Type-based Classification of Ad-jectives.
Distinguishing Properties and Relations.In Proceedings of the 7th International Conferenceon Language Resources and Evaluation, Valletta,Malta, May.Hatzivassiloglou, Vasileios and Kathleen McKeown.1993.
Towards the Automatic Identification of Ad-jectival Scales.
Clustering Adjectives According toMeaning.
In Proceedings of the 31st Annual Meet-ing of the Association of Computational Linguistics,pages 172?182.Krengel, Ulrich.
2003.
Wahrscheinlichkeitstheorieund Statistik.
Vieweg, Wiesbaden.Manning, Christopher D. and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, Cambridge, Mas-sachusetts.Marcus, Mitchell P., Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3,ldc99t42.
CD-ROM.
Philadelphia, Penn.
: Linguis-tic Data Consortium.Mitchell, Jeff and Mirella Lapata.
2008.
Vector-basedModels of Semantic Composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.Mitchell, Jeff and Mirella Lapata.
2009.
Lan-guage Models Based on Semantic Composition.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Singa-pore, August 2009, pages 430?439, Singapore, Au-gust.Pado?, Sebastian and Mirella Lapata.
2007.Dependency-based Construction of Semantic SpaceModels.
Computational Linguistics, 33:161?199.Pantel, Patrick and Marco Pennacchiotti.
2006.Espresso: Leveraging generic patterns for automati-cally harvesting semantic relations.
In Proceedingsof the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, Sydney,Australia, 17?21 July 2006, pages 113?120.Pustejovsky, James.
1995.
The Generative Lexicon.MIT Press, Cambridge, Mass.Rothenha?usler, Klaus and Hinrich Schu?tze.
2009.
Un-supervised Classification with Dependency BasedWord Spaces.
In Proceedings of the EACL Work-shop on Geometrical Models of Natural LanguageSemantics (GEMS), pages 17?24, Athens, Greece,March.Sowa, John F. 2000.
Knowledge Representation.Logical, Philosophical, and Computational Foun-dations.
Brooks Cole.Turney, Peter D. and Patrick Pantel.
2010.
From Fre-quency to Meaning.
Vector Space Models of Se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Widdows, Dominic.
2008.
Semantic Vector Products.Some Initial Investigations.
In Proceedings of the2nd Conference on Quantum Interaction, Oxford,UK, March.438
