The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1?11,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsQuestion Ranking and Selection in Tutorial DialoguesLee Beckera and Martha Palmer 2a and Sarel van Vuuren 3a and Wayne Ward 4a,baThe Center for Computational Language and Education Research (CLEAR)University of Colorado BoulderbBoulder Language Technologies{lee.becker,martha.palmer,sarel.vanvuuren}@colorado.eduwward@bltek.comAbstractA key challenge for dialogue-based intelligenttutoring systems lies in selecting follow-upquestions that are not only context relevantbut also encourage self-expression and stimu-late learning.
This paper presents an approachto ranking candidate questions for a given di-alogue context and introduces an evaluationframework for this task.
We learn to rank us-ing judgments collected from expert humantutors, and we show that adding features de-rived from a rich, multi-layer dialogue actrepresentation improves system performanceover baseline lexical and syntactic features toa level in agreement with the judges.
The ex-perimental results highlight the important fac-tors in modeling the questioning process.
Thiswork provides a framework for future workin automatic question generation and it rep-resents a step toward the larger goal of di-rectly learning tutorial dialogue policies di-rectly from human examples.1 IntroductionSocratic tutoring styles place an emphasis on elicit-ing information from the learner to help them buildtheir own connections to the material.
The role of atutor in a Socratic dialogue is to scaffold the materialand present questions that ultimately lead the studentto an ?A-ha!?
moment.
Numerous studies have il-lustrated the effectiveness of Socratic-style tutoring(VanLehn et al, 2007; Rose et al, 2001; Collins andStevens, 1982); consequently recreating the behav-ior on a computer has long been a goal of researchin Intelligent Tutoring Systems (ITS).
Recent suc-cesses have shown the efficacy of conversational ITS(Graesser et al, 2005; Litman and Silliman, 2004;Ward et al, 2011b), however these systems are stillnot as effective as human tutors, and much improve-ment is needed before they can truly claim to be So-cratic.
Furthermore, development and tuning of tu-torial dialogue behavior requires significant humaneffort.While our overarching goal is to improve ITSby automatically learning tutorial dialogue strategiesdirectly from expert tutor behavior, we focus on thecrucial subtask of selecting follow-up questions.
Al-though asking questions is only a subset of the over-all tutoring process, it is still a complex process thatrequires understanding of the dialogue state, the stu-dent?s ability, and the learning goals.This work frames question selection as a task ofscoring and ranking candidate questions for a spe-cific point in the tutorial dialogue.
Since dialogueis a dynamic process with multiple correct possibil-ities, we do not restrict ourselves only to the movesand questions found in a corpus of transcripts.
In-stead we posit ?What if we had a fully automaticquestion generation system??
and subsequently usecandidate questions hand-authored for each dialoguecontext.
To explore the mechanisms involved inranking follow-up questions against one other, wepair these questions with judgments of quality fromexpert human tutors and extract surface form anddialogue-based features to train machine learningclassification models to rank the appropriateness ofquestions for specific points in a dialogue.Our results show promise with our best question1ranking models exhibiting performance on par withexpert human tutors.
Furthermore these experimentsdemonstrate the utility and importance of rich dia-logue move annotation for modeling decision mak-ing in conversation and tutoring.2 Background and Related WorksLearning tutorial dialogue policies from corpora isa growing area of research in natural language pro-cessing and intelligent tutoring systems.
Past studieshave made use of hidden Markov models (Boyer etal., 2009a) and reinforcement learning (Chi et al,2010; Chi et al, 2009; Chi et al, 2008) to discovertutoring strategies.
However, these approaches aretypically optimized to maximize learning gains, andare not necessarily focused on replicating human tu-tor behavior.
Other work has explored specific fac-tors in questioning such as when to ask ?why?
ques-tions (Rose et al, 2003), provide hints (Tsovaltziand Matheson, 2001), or insert discourse markers(Kim et al, 2000).There is also an expanding body of work that ap-plies ranking algorithms toward the task of ques-tion generation (QG) using approaches such as over-generation-and-ranking (Heilman and Smith, 2010),language model ranking (Yao, 2010), and heuristics-based ranking (Agarwal and Mannem, 2011).
Whilethe focus of these efforts centers on issues of gram-maticality, fluency, and content selection for auto-matic creation of standalone questions, we move tothe higher level task of choosing context appropri-ate questions.
Our work merges aspects of theseQG approaches with the sentence planning tradi-tion from natural language generation (Walker et al,2001; Rambow et al, 2001).
In sentence planningthe goal is to select lexico-structural resources thatencode communicative action.
Rather than select-ing representations, we use them directly as part ofthe feature space for learning functions to rank thequestions?
actual surface form realization.
To ourknowledge there has been no research in ranking thequality and suitability of questions within a tutorialdialogue context.Because questioning tactics depend heavily on thecurriculum and choice of pedagogy, we ground ourinvestigations within the context of the My ScienceTutor (MyST) intelligent tutoring system (Ward etal., 2011b), a conversational virtual tutor designedto improve science learning and understanding forstudents in grades 3-5 (ages 8-11).
Students usingMyST investigate and discuss science through nat-ural spoken dialogues and multimedia interactionswith a virtual tutor named Marni.
The MyST dia-logue design and tutoring style is based on a ped-agogy called Questioning the Author (QtA) (Becket al, 1996) which emphasizes open-ended ques-tions and keying in on student language to promoteself-explanation of concepts, and its curriculum isbased on the Full Option Science System (FOSS) 1a proven system for inquiry based learning.3 Data Collection3.1 MyST Logfiles and TranscriptsFor these experiments, we use MyST transcripts col-lected in a Wizard-of-Oz (WoZ) condition with a hu-man tutor inserted into the interaction loop.
Projecttutors trained in both QtA and in the tutorial sub-ject matter served as the wizards.
During a ses-sion tutors were responsible for accepting, overrid-ing, and/or authoring system actions.
Tutor wizardswere also responsible for setting the current dialogueframe to indicate which of the learning goals wascurrently in focus.
Students talked to MyST via mi-crophone while MyST communicates using Text-to-Speech (TTS) in the WoZ setting.
A typical MySTsession revolves around a single FOSS lesson andlasts approximately 15 minutes.
To obtain a dia-logue transcript, tutor moves are taken directly fromthe system logfile, while student speech is manu-ally transcribed from audio.
In addition to the di-alogue text, MyST records additional informationsuch as timestamps and the current dialogue frame(i.e.
learning goal).
In total we make use of tran-scripts from 122 WoZ dialogues covering 10 unitson magnetism and electricity and 2 in measurementand standards.3.2 Dialogue AnnotationLesson-independent analysis of dialogue requiresa level of abstraction that reduces a dialogue toits underlying actions and intentions.
To addressthis need we use the Dialogue Schema UnifyingSpeech and Semantics (DISCUSS) (Becker et al,1http://www.fossweb.com22011), a multidimensional dialogue move taxon-omy that captures both the pragmatic and seman-tic interpretation of an utterance.
Instead of us-ing one label, a DISCUSS move is a tuple com-posed of three dimensions: Dialogue Act, Rhetor-ical Form, Predicate Type.
Together these labelsaccount for the action, function, and content of anutterance.
This scheme draws from past work intask-oriented dialogue acts (Bunt, 2009; Core andAllen, 1997), tutorial act taxonomies (Pilkington,1999; Tsovaltzi and Karagjosova, 2004; Buckleyand Wolska, 2008; Boyer et al, 2009b) discourserelations (Mann and Thompson, 1986) and questiontaxonomies (Graesser and Person, 1994; Nielsen etal., 2008).Dialogue Act (22 tags): The dialogue act dimen-sion is the top-level dimension in DISCUSS, and itsvalues govern the possible values for the other di-mensions.
Though the DISCUSS dialogue act layerseeks to replicate the learnings from other well-established taxonomies like DIT++ (Bunt, 2009) orDAMSL (Core and Allen, 1997) wherever possible,the QtA style of pedagogy driving our tutoring ses-sions dictated the addition of two tutorial specificacts: marking and revoicing.
A mark act highlightskey words from the student?s speech to draw atten-tion to a particular term or concept.
Like with mark-ing, revoicing keys in on student language, but in-stead of highlighting specific words, a revoice actwill summarize or refine the student?s language tobring clarity to a concept.Rhetorical Form (22 tags): Although the dia-logue act is useful for identifying the speaker?s in-tent, it gives no indication of how the speaker is ad-vancing the conversation.
The rhetorical form re-fines the dialogue act by providing a link to its sur-face form realization.
Consider the questions ?Whatis the battery doing??
and ?Which one is the bat-tery??.
They would both be labeled with Ask dia-logue acts, but they elicit two very different kindsof responses.
The former, which elicits some formof description, would be labeled with a Describerhetorical form, while the latter is seeking to Iden-tify an object.
Similarly an Assert act from a tutorcould be coupled with a Describe rhetorical form tointroduce new information or with a Recap to recon-vey a major point.Predicate Type (19 tags): Beyond knowing theReliability Metric DA RF PTCohen?s Kappa 0.75 0.72 0.63Exact Agreement 0.80 0.66 0.56Partial Agreement 0.89 0.77 0.68Table 1: Inter-annotator agreement for DISCUSS types(DA=Dialogue Act, RF=Rhetorical Form, PT=PredicateType)propositional content of an utterance, it is useful toknow how the entities and predicates in a responserelate to one another.
A student may mention severalkeywords that are semantically similar to the learn-ing goals, but it is important for a tutor to recognizewhether the student?s language provides a deeper de-scription of some phenomena or if it is simply a su-perficial observation.
The Predicate Type aims tocategorize the semantic relationships a student maytalk about; whether it is a Procedure, a Function, aCausal Relation, or some other predicate type.3.2.1 AnnotationAll transcripts used in this experiment have beenannotated with DISCUSS labels at the turn level.
Areliability study using 15% of the transcripts wasconducted to assess inter-rater agreement of DIS-CUSS tagging.
This consisted of 18 doubly anno-tated transcripts comprised of 828 dialogue utter-ances.To assess inter-rater reliability we use Cohen?sKappa (?)
(Carletta, 1996).
Because DISCUSS per-mits multiple labels per instance, we compute a ?value for each label and provide a mean for eachDISCUSS dimension.
To get an additional sense ofagreement, we use two other metrics: exact agree-ment and partial agreement.
For each of these met-rics, we treat each annotators?
annotations as a perclass bag-of-labels.
For exact agreement, each an-notators?
set of labels must match exactly to receivecredit.
Partial agreement is defined as the numberof intersecting labels divided by the total numberof unique labels.
Together these statistics help tobound the reliability of the DISCUSS annotation.Table 1 lists all three metrics broken down by DIS-CUSS dimension.
The ?
values show fair agreementfor the dialogue act and rhetorical form dimensions,whereas the predicate type shows more moderateagreement.
This difference reflects the relative diffi-3culty in labeling each dimension, and the agreementas a whole illustrates the open-endedness of the task.3.3 Question AuthoringWhile the long-term plan for this work is to inte-grate fully automatic question generation into a tu-toring system, for this study we opted to use manu-ally authored questions.
This allows us to remainfocused on learning to identify context appropri-ate questions rather than confounding our experi-ments with issues of question grammaticality andwell-formedness.
Even though using multiple au-thors would provide greater diversity of questions,to avoid repeated effort and to maintain consistencyin authoring we trained a single question authorin both the FOSS material and MyST QtA tech-niques.
Although he was free to author any ques-tion he found appropriate, our guidelines primar-ily emphasized authoring by making permutationsaligned with DISCUSS dimensions while also per-mitting the author to incorporate changes in word-ing, learning-goal content, and tutoring tactics.
Forexample, we taught him to consider how QtA movessuch as Revoicing, Marking, or Recapping could al-ter otherwise similar questions.
To minimize the riskof rater bias, we explicitly told our author to avoidusing positive feedback expressions such as ?Goodjob!?
or ?Great!?.
Table 2 illustrates how the com-binations of DISCUSS labels, QtA tactics, and dia-logue context drives the question generation process.To simulate the conditions available to both thehuman WoZ and computer MyST tutors, the authorwas presented with the entire dialogue history pre-ceding the decision point, the current dialogue frame(learning goal), and any visuals that may be on-screen.
Question authoring contexts were manuallyselected to capture points where students providedresponses to tutor questions.
This eliminated theneed to account for other dialogue behavior such asgreetings, closings, or meta-behavior, and allowedus to focus on follow-up style questions.
Becausethese question authoring contexts came from actualtutorial dialogues, we also extracted the original turnprovided by the tutor, and we filtered out turns thatdid not contain questions related to the lesson con-tent.
Our corpus has 205 question authoring contextscomprised of 1025 manually authored questions and131 questions extracted from the original transcriptyielding 1156 questions in total.3.4 Ratings CollectionTo rate questions, we enlisted the help of four tu-tors who had previously served as project tutors andwizards.
The raters were presented with much ofthe same information used during question author-ing.
The interface included the entire dialogue his-tory preceding the question decision point and a listof up to 6 candidate questions (5 manually authored,1 taken from the original transcript if applicable).
Togive a more complete tutoring context, raters alsohad access to the lessons?
learning goals and the in-teractive visuals used by MyST.Previous studies in rating questions (Becker et al,2009) have found poor inter-rater agreement whenrating questions in isolation.
To decrease the task?sdifficulty we instead ask raters to simultaneouslyscore all candidate questions.
Because we did notwant to bias raters, we did not specify specific cri-teria for question quality.
Instead we instructed theraters to consider the question?s role in assisting stu-dent understanding of the learning goals and to thinkabout factors such as tutorial pacing, context appro-priateness, and content.
Scores were collected us-ing an ordinal 10-point scale ranging from 1 (low-est/worst) to 10 (highest/best).Each set of questions was rated by at least threetutors, and rater assignments were selected to ensureraters never score questions from sessions they tu-tored themselves.
In total we collected ratings for1156 question representing a total of 205 questioncontexts distributed across 30 transcripts.3.4.1 Rater AgreementBecause these judgments are subjective, a keychallenge in this work centers on understanding towhat degree the tutors agree with one another.
Sinceour goal is to rank questions and not to score ques-tions, we convert each tutors scores for a given con-text into a rank-ordered list.
To compute inter-rater agreement in ranking, we use Kendall?s-Tau(? )
rank correlation coefficient.
This measure is anon-parametric statistic that quantifies the similarityin orderings of data, and it is closely tied to AUC,the area under the receiver operating characteristics(ROC) curve.
Though Kendall?s-?
can vary from -1to 1, its value is highly task dependent, and it is typ-4.
.
.T: Tell me more about what is happening with the electricity in a complete circuit.S: Well the battery sends all the electricity in a circuit to the motor so the motor starts to go.Candidate Question Frame Element DISCUSSQ1 Roll over the switch and then in your ownwords, tell me again what a complete orclosed circuit is all about.Same Same Direct/Task/VisualAsk/Describe/ConfigurationQ2 How is this circuit setup?
Is it open or closed?
Same Same Ask/Select/ConfigurationQ3 To summarize, a closed circuit allows theelectricity to flow and the motor to spin.
Nowin this circuit, we have a new component.
Theswitch.
What is the switch all about?Diff Diff Assert/Recap/PropositionDirect/Task/VisualAsk/Describe/FunctionQ4 You said something about the motor spinningin a complete circuit.
Tell me more about that.Same Same Revoice/None/NoneAsk/Elaborate/CausalRelationTable 2: Example dialogue context snippet and a collection of candidate questions.
The frame, element, and DISCUSScolumns show how the questions vary from one another.ically lower when the range of possible choices isnarrow as it is in this task.
To get a single score weaverage ?
values across all sets of questions (con-texts) and all pairs of raters.
The mean value for allpairs of raters and contexts is ?
= 0.1478.
The inter-rater statistics are shown in table 3.
While inter-rateragreement is fairly modest, we do see lots of vari-ation between different pairs of tutors.
Addition-ally, we found that a pair of raters agreed on the toprated question 33% of the time.
This suggests thatdespite their common training and experience, theraters may be using different criteria in rating.To assess the tutors?
internal consistency, we hadeach tutor re-rate 60 sets of questions approximatelytwo months after their first trial, and we computedself-agreement Kendall?s-?
values using the methodabove.
These statistics are listed in the bottom rowof table 3.
In contrast with the inter-rater agreement,self-agreement is much more consistent giving fur-ther evidence for a difference in criteria.
Togetherself and inter-rater agreement help bound expectedsystem performance in ranking.4 Automatic RankingBecause we are more interested in learning to pre-dict which questions are more suitable for a giventutoring scenario than we are in assigning specificscores to questions, we approach the task of ques-tion selection as a ranking task.
To create a gold-rater A rater B rater C rater Drater A X 0.2590 0.1418 0.0075rater B 0.2590 X 0.1217 0.2370rater C 0.1418 0.1217 X 0.0540rater D 0.0075 0.2370 0.0540 Xmean 0.1361 0.2059 0.1058 0.0995self 0.4802 0.4022 0.2327 0.3531Table 3: Inter-rater rank agreement (Kendall?s-?
).
Thebottom row is the self-agreement for contexts they ratedin two separate trials.standard for training and evaluation we first need toconvert the collective ratings for a set of questionsinto a rank-ordered list.
While the most straight-forward way to make this conversion is to averagethe ratings for each item, this approach assumes allraters operate on the same scale.
Furthermore, a sin-gle score does not account for how a question re-lates to other candidate questions.
Instead we createa single rank-order by tabulating pairwise wins forall pairs of questions qi, qj , (i 6= j) within a givendialogue context C. If rating(qi) > rating(qj),questions qi receives a win.
This is summed acrossall raters for the context.
The question(s) with themost wins has rank 1.
Questions with an equal num-ber of wins are considered tied and are given the av-erage ranking of their ordinal positions.
For exam-ple if two questions are tied for second place, they5are each assigned a ranking of 2.5.Using this rank-ordering we then train a pairwiseclassifier to learn a preferences function (Cohen etal., 1998) that determines if one question has a bet-ter rank than another.
For each question qi within acontextC, we construct a vector of features ?i.
For apair of questions qi and qj , we then create a new vec-tor using the difference of features: ?
(qi, qj , C) =?i ?
?j .
For training, if rank(qi) < rank(qj), theclassification is positive otherwise it is negative.
Toaccount for the possibility of ties, and to make thedifference measure appear symmetric, we train bothcombinations (qi, qj) and (qj , qi).
During decoding,we run the trained classifier on all pairs and tabulatewins using the approach described above.For our experiments we train pairwise classi-fiers using Mallet?s Maximum Entropy (McCallum,2002) and SVMLight?s Support Vector Machinesmodels (Joachims, 1999).
We also use SVMRank(Joachims, 1999), which performs the same max-imum margin separation as SVMLight, but usesKendall?s-?
as a loss function to optimize for rankordering.
We run SVMRank with a linear kerneland model parameters of c = 2.0 and  = 0.0156.For MaxEnt, we use Mallet?s default model param-eters.
Training and evaluation are carried out us-ing 10-fold cross validation (3 transcripts per fold,approximately 7 dialogue contexts per transcript).Folds are partitioned by FOSS unit, to ensure train-ing and evaluation are on different lessons.
To ex-plore the impact of DISCUSS representations on thisquestion ranking task, we train and evaluate modelsby incrementally adding additional information ex-tracted from the DISCUSS annotation.4.1 FeaturesWhen designing features for this task, we wanted tocapture the factors that may play a role in the tutor?sdecision making process during question selection.When rating, scorers may consider factors such asthe question?s surface form, lesson relevance, con-textual relevance.
The subsections below detail themotivations and intuitions behind these factors.4.1.1 Surface Form FeaturesWhen presented with a list of questions, a raterlikely bases the decision on his or her initial reactionto the questions?
wording.
In some cases, wordingmay supercede any other decisions regarding edu-cational value or dialogue cohesiveness.
Questionverbosity is captured by the number of words in thequestion feature.
Analysis of rater comments alsosuggested that preferences are often tied to the ques-tion?s form and structure.
A rough measure of formcomes from the Wh-word features to mark the pres-ence of the following question words: who, what,why, where, when, which, and how.
Additionally weuse the bag-of-part-of-speech-tags (POS) features toprovide another aspect of the question?s structure.4.1.2 Lexical Similarity FeaturesPast work (Ward et al, 2011a) has shown that en-trainment, the process of automatic alignment be-tween dialogue partners, is a useful predictor oflearning and is a key factor in facilitating a success-ful conversation.
For question selection, we hypoth-esize that successful tutors ask questions that dis-play some degree of semantic entrainment with stu-dent utterances.
In MyST-based tutoring, dialogueactions are driven by the goal of eliciting student re-sponses that address the learning goals for the les-son.
Consequently, choosing an appropriate ques-tion may depend on how closely student responsesalign with the learning goals.
To model both en-trainment and lexical similarity we extract featuresfor unigram and bigram overlap of words, word-lemmas, and part-of-speech tags between the pairsbelow.?
The candidate question and the student?s lastutterance?
The candidate question and the last tutor?s ut-terance?
The candidate question and the text of the cur-rent learning goal?
The candidate question and the text of the otherlearning goalsExample learning goals for a lesson on circuits areprovided in table 4.
The current learning goal is sim-ply the learning goal in focus at the point of questionasking according to the MyST logfile.
Other learn-ing goals are all other goals for the lesson.
Usingthe example from the table, if goal 2 is the currentlearning goal, then goals 1 and 3 are the other goals.6Goal 1: Wires carry electricity and can connectcomponentsGoal 2: Bulb receives electricity and transformselectricity into heatGoal 3: A circuit provides a pathway for energyto flowTable 4: Example learning goals4.1.3 DISCUSS FeaturesThe lexical and surface form features providesome cues about the content of the question, butthey do not account for the action or intent in tutor-ing.
The DISCUSS annotation allows us to bridgebetween the question?s semantics and pragmaticallyand focus on what differentiates one question fromanother.
Basic DISCUSS features include bags ofDialogue Acts (DA), Rhetorical Forms (RF), andPredicate types (PT) found in the question?s DIS-CUSS annotation.
We capture the question?s dia-logue cohesiveness with binary features indicatingwhether or not the question?s RF and PT match thosefound in the previous student and tutor turns.4.1.4 Contextualized DISCUSS FeaturesIn tutoring, follow-up questions are licensed bythe questions that precede them.
For example a tutormay be less likely to ask how an object functions un-til after the object has first been identified by the stu-dent.
Along a different dimension, a tutor?s line ofquestioning may change to match a student?s under-standing of the material.
Struggling students may re-quire additional opportunities to explain themselves,while advanced students may benefit more from amore rapid pace of instruction.We model the conditional relevance of movesby computing dialogue act transition probabilitiesfrom our corpus of DISCUSS annotated tutorial di-alogues.
Although DISCUSS allows multiple tagsper dialogue turn, we simplify probability calcula-tions by treating each DISCUSS tuple as a separateevent, and tallying all pairs of turn-turn labels.
ADISCUSS tuple consists of a Dialogue Act (DA),Rhetorical Form (RF), and Predicate Type (PT),and we use different subsets of the tuple to com-pute the transition probabilities listed in equations 1-3.
All probabilities are computed using Laplace-smoothing.
When extracting features, we sum thelog of the probabilities for each DISCUSS labelpresent in the question.MyST models dialogue as a sequence of seman-tic frames which correspond to specific learninggoals.
For natural language understanding, MySTuses Phoenix semantic grammars (Ward, 1994) toidentify which elements within these frames havebeen filled.
To account for student progress in ques-tion asking, we compute the conditional probabil-ity of a DISCUSS label given the percentage of el-ements filled in the current dialogue frame (equa-tion 4).
This progress percentage is discretized intobins of 0-25%, 25-50%, 50-75%, and 75-100%.p(DA,RF, PTquestion|DA,RF, PTstud.
turn) (1)p(DA,RFquestion|DA,RFstudent turn) (2)p(PTquestion|PTstudent turn) (3)p(DA,RF, PTques.|% elements filled) (4)4.2 EvaluationTo evaluate our systems?
performance in ranking,we use two measures commonly used in informationretrieval: the Mean Kendall?s-?
measure describedin section 3.4.1 and Mean Reciprocal Rank (MRR).MRR is the average of the multiplicative inverse ofthe rank of the highest ranking question across allcontexts.
To account for ties we use the Tau-b vari-ant of Kendall?s-?
, and for MRR we compute re-ciprocal rank by averaging the system rankings forall of the questions tied for first.
To obtain a gold-standard ranking for comparison, we combine indi-vidual raters?
ratings using the approached describedin section 4.5 Results and DiscussionWe trained several models to investigate how differ-ent feature classes influence overall performance inranking.
The results for these experiments are listedin Table 5.
Because we found comparable perfor-mance between MaxEnt and SVMLight, we onlyreport results for MaxEnt and SVMRank models.In addition to MRR and Kendall?s-?
, we list thenumber of concordances and discordances in pair-wise classification to give the reader another senseof the accuracy associated with rank agreement.Random Baseline: On average, assigning ran-dom ranks will yield mean ?=0 and MRR=0.408.7Model Features Mean Num.
Num.
Pairwise MRRKendall?s-?
Concord.
Discord.
AccuracyMaxEnt CONTEXT+DA+PT+MATCH+POS- 0.211 1560 974 0.616 0.516SVMRank CONTEXT+DA+PT+MATCH+POS- 0.190 1725 1154 0.599 0.555MaxEnt CONTEXT+DA+RF+PT+MATCH+POS- 0.185 1529 1014 0.601 0.512MaxEnt DA+RF+PT+MATCH+POS- 0.179 1510 1009 0.599 0.503MaxEnt DA+RF+PT+MATCH+ 0.163 1506 1044 0.591 0.485MaxEnt DA+RF+PT+ 0.147 1500 1075 0.583 0.480MaxEnt DA+RF+ 0.130 1458 1082 0.574 0.476MaxEnt DA+ 0.120 1417 1076 0.568 0.458SVMRank Baseline 0.108 1601 1278 0.556 0.473MaxEnt Baseline 0.105 1410 1115 0.558 0.448Table 5: System scores by feature set and and machine learning model.
Presence or absence of specific features isdenoted with a ?+?
or ?-?
otherwise the label refers to a set of features.
The Baseline features consist of the Surface Formand Lexical Similarity features described in sections 4.1.1 and 4.1.2.
POS are the bag-of-POS surface form features.DA, RF, and PT refer to the DISCUSS presence features for the Dialogue Act, Rhetorical Form, and Predicate Typedimensions described in section 4.1.3.
MATCH refers specifically to the RF and PT match features.
CONTEXTrefers to the Contextualized DISCUSS features described in section 4.1.4.
The best scores for each column appear inboldface.-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.001020304050Frequency?mean=0.211-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.0Kendall's Tau(?)
Range01020304050Frequency?mean=0.105Figure 1: Distribution of per-context Kendall?s-?
valuesfor the top-scoring system (top), and the baseline system(bottom).Baseline System: Our baseline system used allof the surface form and lexical similarity featuresdescribed above.
This set of features achieves thehighest rank agreement (?
= 0.105) using max-imum entropy and the highest MRR (0.473) withSVMRank .
This improvement over the randombaseline suggests there is a correlation between aquestion?s ranking and its surface form.DISCUSS System: Table 5 shows system per-formance steadily improves as additional DISCUSSfeatures are included in the model.
When us-1 2 3 4 5 6 70.000.050.100.150.200.250.300.350.40p(Rank)1/MRR=1.801 2 3 4 5 6 7Mean System Rank0.000.050.100.150.200.250.300.350.40p(Rank)1/MRR=2.11Figure 2: Distribution of per-context system ranks for thehighest rated question for the top-scoring system (top),and the baseline system (bottom).
These ranks are theinverse of the reciprocal rank used to calculate MRR.ing DISCUSS features, removing the part-of-speechfeatures gives an additional bump in performancesuggesting that there is an overlap in informationbetween DISCUSS representations and POS tags.Finally, adding contextualized DISCUSS featurespushes our ranking models to their highest levelof agreement with ?
= 0.211 using MaxEnt andMRR=0.555 using SVMRank .
Inspection of theMRR values shows that without taking into accountthe possibility of ties the baseline system selects8the top-ranked question in 44/205 (21.4%) contexts.While the system with the best MRR score, correctlychooses the top-ranked question in 71/205 (34.6%)contexts ?
a rate comparable to how often a pair ofraters agreed on the number-one item (33.4%).Application of the Wilcoxon signed-rank testshows the DISCUSS system exhibits statisticallysignificant improvement over the baseline system inits distribution of Kendall?s-?
values (n = 205, z =7350, p < 0.001) and distribution of reciprocalranks (n = 205, z = 3739, p < 0.001).
Figures 1and 2 give visual confirmation of this improvement,and highlight the overall reduction in negative ?
val-ues as well as the greater-than-50% increase in like-lihood of selecting the best question first.To get another perspective on system perfor-mance, we evaluated our human raters on the gold-standard rankings from the subset of questions usedfor assessing internal agreement.
This yielded amean ?
between 0.2589 and 0.3619.
If we removeratings so that the gold-standard does not include therater under evaluation, tutor performance drops toa range of 0.1523 to 0.2432, which is roughly cen-tered around the agreement exhibited by our best-performing system.Looking at the impact of learning algorithmswe see that SVMRank tends to perform better onMRR while the pairwise maximum entropy mod-els yield higher ?
?s.
One possible explanation forthis discrepancy may stem from the ranking algo-rithms?
different treatment of ties.
The pairwisemodel permits ties, whereas the scores produced bySVMRank produce a strict order.
Without ties, it isdifficult to exactly match the raters?
orderings whichhad numerous ties, which can in turn produce anoverall higher number of concordances and discor-dances than the pairwise classification model.6 Conclusions and Future WorkWe have introduced a framework for learning andevaluating models for ranking and selecting ques-tions for a given point in a tutorial dialogue.
Fur-thermore these experiments show that it is feasibleto learn this behavior by coupling predefined ques-tions with ratings from trained tutors.
Supplement-ing our baseline surface form and lexical similarityfeatures with additional features extracted from thedialogue context and DISCUSS dialogue act anno-tation improves system performance in ranking to alevel on par with expert human tutors.
These resultsillustrate how question asking depends not only onthe form of the question but also on the underlyingdialogue action, function and content.In the near future we plan to train models on indi-vidual tutors to investigate which factors drive in-dividual preferences in question asking.
We alsoplan to characterize system performance using auto-matically labeled DISCUSS annotation.
Lastly, wefeel these results provide a natural starting point toexplore automatic generation of questions from theDISCUSS dialogue move representation.AcknowledgmentsThis work was supported by grants from theNSF (DRL-0733322, DRL-0733323), the IES(R3053070434) and the DARPA GALE program(Contract No.
HR0011-06-C-0022, a supplementfor VerbNet attached to the subcontract from theBBN-AGILE Team).
Any findings, recommenda-tions, or conclusions are those of the author and donot necessarily represent the views of NSF, IES, orDARPA.ReferencesManish Agarwal and Prashanth Mannem.
2011.
Auto-matic gap-fill question generation from text books au-tomatic gap-fill question generation from text booksautomatic gap-fill questions from text books.
In Pro-ceedings of the Sixth Workshop on Innovative Use ofNLP for Building Educational Applications.I.
L. Beck, M. G. McKeown, J.
Worthy, C. A. San-dora, and L. Kucan.
1996.
Questioning the au-thor: A year-long classroom implementation to engagestudents with text.
The Elementary School Journal,96(4):387?416.L.
Becker, R. D. Nielsen, and W. Ward.
2009.
What apilot study says about running a question generationchallenge.
In Proceedings of the Second Workshop onQuestion Generation, Brighton, England, July.L.
Becker, W. Ward, S. van Vuuren, and M. Palmer.
2011.Discuss: A dialogue move taxonomy layered over se-mantic representations.
In In Proceedings of the In-ternational Conference on Computational Semantics(IWCS) 2011, Oxford, England, January 12-14.K.E.
Boyer, E.Y.
Ha, M. Wallis, R. Phillips, M.A.
Vouk,and J.C. Lester.
2009a.
Discovering tutorial dialogue9strategies with hidden markov models.
In Proceed-ings of the 14th International Conference on ArtificialIntelligence in Education (AIED ?09), pages 141?148,Brighton, U.K.K.E.
Boyer, W.J.
Lahti, R. Phillips, M. D. Wallis, M. A.Vouk, and J. C. Lester.
2009b.
An empirically derivedquestion taxonomy for task-oriented tutorial dialogue.In Proceedings of the Second Workshop on QuestionGeneration, pages 9?16, Brighton, U.K.M.
Buckley and M. Wolska.
2008.
A classification ofdialogue actions in tutorial dialogue.
In Proceedingsof COLING 2008, pages 73?80.
ACL.H.
C. Bunt.
2009.
The DIT++ taxonomy for functionaldialogue markup.
In Proc.
EDAML 2009.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):pp.
249?254.M.
Chi, P. Jordan, K. VanLehn, and M. Hall.
2008.
Re-inforcement learning-based feature selection for devel-oping pedagogically effective tutorial dialogue tactics.In Ryan S. Baker, Tiffany Barnes, and Joseph Becker,editors, Proceedings of the 1st International Confer-ence on Educational Data Mining, pages pp258?265.M.
Chi, P. W. Jordan, K. VanLehn, and D. J. Litman.2009.
To elicit or to tell: Does it matter?
In Artifi-cial Intelligence in Education, pages 197?204.M.
Chi, K. VanLehn, and D. Litman.
2010.
Do micro-level tutorial decisions matter: Applying reinforce-ment learning to induce do micro-level tutorial deci-sions matter.
In Vincent Aleven, Judy Kay, and JackMostow, editors, Preceedings of the 10th InternationConfernce on Intelligent Tutoring Systems (ITS 2010).William W. Cohen, Robert E. Schapire, and YoramSinger.
1998.
Learning to order things.
In Advancesin Neural Information Processing Systems 10 (NIPS1998).A.
Collins and A. Stevens.
1982.
Goals and methods forinquiry teachers.
Advances in Instructional Psychol-ogy, 2.M.
G. Core and J.F.
Allen.
1997.
Coding dialogs with theDAMSL annotation scheme.
In AAAI Fall Symposium,pages 28?35.A.C.
Graesser and N.K.
Person.
1994.
Question ask-ing during tutoring.
American Educational ResearchJournal, 31:104?137.A.C.
Graesser, P. Chipman, B.C Haynes, and A. Olney.2005.
Autotutor: An intelligent tutoring system withmixed-initiative dialogue.
IEEE Transactions in Edu-cation, 48:612?618.M.
Heilman and N. A. Smith.
2010.
Good question!
sta-tistical ranking for question generation.
In Proceed-ings of NAACL/HLT 2010.T.
Joachims.
1999.
Making large-scale svm learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
MIT-Press.J.H.
Kim, M. Glass, R. Freedman, and M.W.
Evens.2000.
Learning the use of discourse markers in tuto-rial dialogue learning the use of discourse markers intutorial dialogue.
In Proceedings of the 22nd AnnualConference of the Cognitive Science Society.D.
Litman and S. Silliman.
2004.
Itspoke: An intel-ligent tutoring spoken dialogue system.
In Compan-ion Proceedings of the Human Language TechnologyConference: 4th Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT/NAACL).W.C.
Mann and S.A Thompson.
1986.
Rhetorical struc-ture theory: Description and construction of text struc-tures.
In In Proceedings of the Third InternationalWorkshop on Text Generation, August.A.
K. McCallum, 2002.
MALLET: A Machine Learningfor Language Toolkit.
http://mallet.cs.umass.edu.R.
D. Nielsen, J. Buckingham, G. Knoll, B. Marsh, andL.
Palen.
2008.
A taxonomy of questions for ques-tion generation.
In Proceedings of the Workshop onthe Question Generation Shared Task and EvaluationChallenge, September.R.M.
Pilkington.
1999.
Analysing educational dis-course: The discount scheme.
Technical Report 99/2,Computer Based Learning Unit, University of Leeds.Owen Rambow, Monica Rogati, and Marilyn A. Walker.2001.
Evaluating a trainable sentence planner for aspoken dialogue system evaluating a trainable sen-tence planner for a spoken dialogue system.
In Pro-ceedings of the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL 2001).C.P.
Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-Lehn, and A. Weinstein.
2001.
A comparative evalu-ation of socratic versus didactic tutoring.
In Proceed-ings of Cognitive Sciences Society.C.P.
Rose, D. Bhembe, S. Siler, R. Srivastava, andK.
VanLehn.
2003.
The role of why questions in ef-fective human tutoring.
In Proceedings of ArtificialIntelligence in Education (AIED 2003).D.
Tsovaltzi and E. Karagjosova.
2004.
A view on dia-logue move taxonomies for tutorial dialogues.
In Pro-ceedings of SIGDIAL 2004, pages 35?38.
ACL.D.
Tsovaltzi and C. Matheson.
2001.
Formalising hint-ing in tutorial dialogues.
In In EDILOG: 6th workshopon the semantics and pragmatics of dialogue, pages185?192.K.
VanLehn, A.C. Graesser, G.T.
Jackson, P. Jordan,A.
Olney, and C.P.
Rose.
2007.
When are tutorialdialogues more effective than reading?
Cognitive Sci-ence, 31(1):3?62.10Marilyn A. Walker, Owen Rambow, and Monica Rogati.2001.
SPOT: A trainable sentence planner.
In Pro-ceedings of the North American Meeting of the Asso-ciation for Computational Linguistics (NAACL).A.
Ward, D. Litman, and M. Eskenazi.
2011a.
Predict-ing change in student motivation by measuring cohe-sion between predicting change in student motivationby measuring cohesion between tutor and student.
InProceedings of the Sixth Workshop on Innovative Useof NLP for Building Educational Applications, pages136?141.W.
Ward, R. Cole, D. Bolan?os, C. Buchenroth-Martin,E.
Svirsky, S. van Vuuren, T. Weston, J. Zheng, andL.
Becker.
2011b.
My science tutor: A conversa-tional multi-media virtual tutor for elementary schoolscience.
ACM Transactions on Speech and LanguageProcessing (TSLP), 7(4), August.W.
Ward.
1994.
Extracting information from sponta-neous speech.
In Proceedings of the InternationalConference on Speech and Language Processing (IC-SLP).Xuchen Yao.
2010.
Question generation with minimalrecursion semantics.
Master?s thesis, Saarland Uni-versity.11
