Restoring an Elided Entry Word in a Sentencefor Encyclopedia QA SystemSoojong LimSpeech/LanguageInformation ResearchDepartmentETRI, Koreaisj@etri.re.krChangki LeeSpeech/Language Informa-tion Research DepartmentETRI, Korealeeck@etri.re.krMyoung-Gil JangSpeech/Language Informa-tion Research DepartmentETRI, Koreamgjang@etri.re.krAbstractThis paper presents a hybrid model forrestoring an elided entry word for en-cyclopedia QA system.
In Korean en-cyclopedia, an entry word is frequentlyomitted in a sentence.
If the QA systemuses a sentence without an entry word,it cannot provide a right answer.
Forresolving this problem, we combine arule-based approach with MaximumEntropy model to use the merit of eachapproach.
A rule-based approach usescaseframes and sense classes.
The re-sult shows that combined approachgives a 20% increase over our baseline.1 IntroductionEllipsis is a linguistic phenomenon that peo-ple omit a word or phrase not to repeat a sameword or phrase in a sentence or a document.Usually, ellipsis involves the use of clauses thatare not syntactically complete sentences (Allen,1995) but the fact does not apply to all cases.
Anellipsis occurring in encyclopedia documents inKorean is an example.
(Entry word: Kim Daejung)Korean: ??[gongro]?
[ro] 2000?[nyeon]?????
[nobel pyeonghwasang]?[eul]???
[batatda].English: won the Nobel prize for peace in2000 by meritorious deed.In QA system(Kim et al 2004), it answers aquestion using the predicate-argument relationas in the following example.Korean: 2000?[nyeon]?
[e] ?????
[no-belpyeonghwasang ] ?
[eul]  ??
[bateun ]??[saram]?
[eun]?English: Who?s the winner of the Nobel prize forpeace on 2000???(subj:?
?, obj:????
?, adv:2000?
)( batda(subj:saram, obj: nobelpyeonghwasang,adv:ichunnyeon)win(subj:who, obj:the Nobel prize for peace,adv:2000)Entry word: ???
(Entry word: Kim Daejun)??(subj:NULL(???
), obj:????
?,adv:2000?, ??
)(batda(subj:NULL(kimdaejung), obj, nobelpyeongh-wasang, adv: ichunnyeon, gongro)win(subj:NULL(Kim Daejung), obj:the Nobel prizefor peace, adv:2000, deed)If an entry word of Korean encyclopedia per-forms a function of a subject or an objects, it isfrequently omitted in the sentences of the Ko-rean encyclopedia.
If the QA system uses theresult in the above example, it cannot find whowon the Nobel prize for peace in the year of2000.
We need to restore an entry word as asubject or an object to answer a right question.In this paper, to overcome this problem, wefirst try to classify entry words in encyclopediainto sense classes and determine which senseclasses are restored to the subjects or the objects.Then we use caseframes for determining sense215classes which are not restored using senseclasses.
If there is no caseframes, we use a sta-tistical method, ME model, for determiningwhether the entry word is restored or not.
Be-cause each approach has both strength andweakness, we combine three approaches toachieve a better performance.2 Related WorkEllipsis is a pervasive phenomenon in naturallanguages.
While previous work provides im-portant insight into the abstract syntactic andsemantic representations that underlie ellipsisphenomena, there has been little empiricallyoriented work on ellipsis.There are only two similar empirical experi-ments done for this task.
First is Hardt?s algo-rithm(Hardt, 1997) for detecting VPE in thePenn Treebank.
It achieves precision levels of44% and recall of 53%, giving an F-Measure of48% using a simple search technique, whichrelies on the annotation having identified emptyexpressions correctly.
Second is Nielsen?s ma-chine learning techniques(Nielsen, 2003).
Theyonly try to detect of elliptical verbs using fourdifferent machine learning techniques,  Trans-formation-based learning, Maximum entropymodeling, Decision Tree Learning, MemoryBased Learning.
It achieves precision levels of85.14% and recall of 69.63%, giving an F-Measure of 76.61%.
There are 4 steps: detection,identification of antecedents, difficult antece-dents, resolving antecedents.
Because this studyonly concentrates on the detection, a comparisonwith our study is inadequate.We combine rule-based techniques with ma-chine learning technique for using the merit ofeach technique.3 Restoring an Elided Entry WordWe use three kinds of algorithms: A caseframealgorithm, an acceptable sense class algorithm,and Maximum Entropy (ME) algorithm.
Forknowing a strength and weakness points of eachalgorithm, we do experiments on each algorithm.Then we combine algorithms for higher per-formance.Our system answers in three ways: restoringan  entry word as a subject, restoring an entryword as an object, and does not restore an entryword.
We evaluate an algorithm in two ways.First, we evaluate all answers with precision.Second,  we  evaluate just two answers, restor-ing an entry word as a subject and object, withF-measure.recallprecisionrecallprecisionmeasureFfoundwordsentryelidedallfoundwordsentryelidedcorrectprecisionsettestinwordsentryelidedallfoundwordsentryelidedcorrectrecall+?
?=?==23.1 Using CaseframesWe use modified caseframes constructed forKorean-Chinese machine translation.
The formatof Korean-Chinese machine translation caseframe is as the following:A=Sense_code!case_particle verb > Chinese >Korean SentenceA=??(saram)!?
(ga) B=??(jangso)!?(ro)?(ga)!?
(da) > A 0x53bb:v B [?(geu)[A]?(ga)??(bada)[B]?
(ro) ??
(gada)]A=Person!subj B=Location!adv go.In the caseframe, we only use Sense Class,case particle marker, and the verb.
The case-frame used in this research consists of 30,000verbs and 153,000 caseframes.The sense class used in this research is se-lected from the nodes of the ETRI Lexical Con-cept Network for Korean Nouns which consistsof about 60,000 nodes.
(If we include propernouns, the total entry of ETRI Lexical ConceptNetwork for Korean Nouns is about 300,000nodes).First, we analyze a sentence using depend-ency parser (LIM, 2004), and then we convert aresult of a parser into the caseframe format.
Wedetermine to restore an entry word if there is anexactly matched caseframe of a target except asense class of an entry word.Table 1 shows an example.First, we analyze a sentence using depend-ency parser (LIM, 2004), and then we convert aresult of a parser into the caseframe format.
Wedetermine to restore an entry word if there is anexactly matched caseframe of a target except asense class of an entry word.216Table 1.
An Example of Caserframe AlgorithmInput Entry word: Along BaySense: LocationSentence: Located in East of HaiphongParsing Locate(subj:NULL, obj:NULL, adv: eastof Haiphong)Caseframe of sentencedirection!e locateMatching 24265-2 A=Location!ga B=Location!eseoC=direction!e24265-4 A=Location!ga B=direction!e24265-8 A=weather!ga B=direction!e24265-12 A=direction!e24265-17 A=body!ga B=direction!edecision Restoring an entry word as a subjectThe result of caseframe algorithm is in table2.
The result of caseframe algorithm shows thatit has a high precision but a relatively low recallbecause it is impossible to construct caseframesfor all sentences.Table 2.
Result of Caseframe AlgorithmSubject Object SumPrecision 88.16 6.38 56.91Recall 59.29 27.28 56.45F-measure 70.90 10.34 56.683.2 Acceptable Sense ClassAll entry words in the encyclopedia belong to atleast one sense class.
We verify all 444 senseclasses to see whether they could be restored ina sentence.
We set a precision threshold 50%and we fix 36 sense classes to ?acceptable senseclass?.
An acceptable sense class is a sense classthat if an entry word is included in an acceptablesense class, we unconditionally restore an entryword in a sentence.
Our verification tells thatthere is only acceptable sense classes for sub-jects.
Table 3 shows acceptable sense classes.Table 3.
Acceptable Sense ClassesPERSON, ORGANIZATION, STUDY, WORK,LOCATION, ANIMAL, PLANT, ART,BUILDING, BUSINESS MATTERS, POSITION,SPORTS, CLOTHES, ESTABLISHMENT,PUBLICATION, MEANS of TRANSPORTATION,EQUIPMENT, SITUATION, HARDWARE,BROADCASTING, HUMAN RACE, EXISTENCE,BRANCH, MATERIAL OBJECT, WEAPON,EXPLOSIVE, LANGUAGE, FACILITIES,ACTION, SYMBOL, TOPOGRAPHY, ROAD,ECONOMY, ADVERTISEMENT, EVENT, TOMBThe result of acceptable sense class algo-rithm is presented in table 4.
Because we cannotget acceptable sense classes for objects, F-measure of object is 0.Table 4.
Result of ASC AlgorithmSubject Object SumPrecision 58.14 0.0 58.14Recall 66.37 0.0 60.48F-measure 61.98 0.0 59.293.3 Maximum Entropy ModelingMaximum entropy modeling uses features,which can be complex, to provide a statisticalmodel of the observed data which has the high-est possible entropy, such that no assumptionsabout the data are made.
)(maxarg* pHp =*p( pHCp?where is the most uniform distribution, C is a setof probability distributions under the constraints andis entropy of ) p .Ratnaparkhi(Ratnaparkhi 98) makes a strongargument for the use of maximum entropymodes, and demonstrates their use in a variety ofNLP tasks.The Maximum Entropy Toolkit was used forthe experiments.1Because maximum entropy allows for a widerange of features, we can use various features,such as lexical feature, POS feature, sense fea-ture, and syntactic feature.
Each feature consistsof subfeatures:Lexical feature;Verb_lex : lexeme of a target verbVerb_e_lex : lexeme of a suffix attatchedto a target verbPOS feature;Verb_pos : pos of a target verbVerb_e_pos : pos of a suffix attatch to atarget verbSense feature;1 Downloadable fromhttp://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html217Ti_res_code: where sense of an entryword is included in acceptable sense classVerb_cf_subj, obj: whether a sense of en-try word is included in caseframe of atarge verbTi_sense : sense class of entry wordSyntactic feature;Tree_posi: position of parse treeRel_type: relation type between verbs ina sentenceSen_subj, sen_obj : existence of subjector objectHybrid feature;Pair =(sense class of entry word, verb)Table 5 shows an example of features thatwe use for finding an elided entry word.Previous work using ME model adopted dis-tance-based context for training.
Because we usesyntactic features, we can use not only distance-based context but also predicate-argument basedcontext.
The training data for ME algorithmconsist of verbs in the encyclopedia documentand their syntactic arguments.
Each verb-arguments set is augmented with the informationthat signifies whether a subject, an object or nei-ther of them should be restored.
For training, weuse a dependency parser[Lim, 2004].
A preci-sion of this parser is about 75%.
The results ofME model algorithm is shown in table 6.
Theresults of ME model shows that its score is thelowest of all.
We guess the reason is that there isnot enough training data for covering all senseclasses.Table 5.
An Example of FeaturesEntry word,Sentence!TI Cirsotrema perplexam!SENSE Animal!VERB live!SENT lives in a tidal zoneLexicalfeatureverb_lex=??
(salda) verb_e_lex=?
(myeo)POS feature verb_pos=4 verb_e_pos=24Sense fea-tureti_res_code=1 verb_cf_subj=1verb_cf_obj=0 ti_sense=AnimalSyntacticfeaturetree_posi=high rel_type=-1 sen_subj=0 sen_obj=0Hybrid fea-turepair=(Animal, live)Table 6.
Result of ME ModelSubject Object SumPrecision 62.50 40.0 60.87Recall 35.40 18.18 33.87F-measure 45.20 25.00 43.523.4 Combining AlgorithmsDifferent algorithms have different characteris-tics.
For example, the acceptable sense classalgorithm has relatively high recall but low pre-cision, while the opposite holds true for thecaseframe algorithm,  we need to combine algo-rithms for maximizing advantages of each algo-rithm.First, we combine the acceptable sense classalgorithm with the ME model.
We process theproblem using the sense class algorithm.
Insteadof applying the algorithm exactly, we use theME model for helping the acceptable sense classalgorithm.
If the acceptable sense class algo-rithm determines a restoration, we adopt thecase to ME model.
Then if the score of MEmodel is over the negative threshold, we deter-mine not to restore an entry word.Second, we combine the caseframe algorithmwith the ME model.
We process the cases notresolved in the first processing time using thecaseframe algorithm.
We try to match case-frames exactly to sentence with an entry wordsense code.
If we cannot find the exactly match-ing caseframe, we try matching caseframes par-tially.
In this case, a precision is maybe lowerthan an exact match, we also use the ME modelfor reliability.
If the score of ME model is overthe positive threshold, we determine to restorean entry word.4 Result and ConclusionFor ME model, we made a training setmanually.
The training set consists of 2895 sen-tences: 916 sentences for restoring an entryword as a subject, 232 sentences for restoring anentry word as an object, 1756 sentences for notrestoring any.
For a test, we randomly selected277 sentences.We did 6 kinds of experiments.
Using Case-frame algorithm(CF), Acceptable sense classalgorithm(ASC), ME model(ME) and combineASC with CF(ASC_CF), ASC with ME218(ASC_ME), and ASC with CF andME(ASC_CF_ME).Table 7.
Result of Combined AlgorithmRecall Precision F-measurebaseline 100.00 31.64 48.07ASC_CF_ME 78.23 60.25 68.07ASC_CF 68.55 50.00 57.82ASC_ ME 79.03 59.39 67.82The performance of the methods is calculatedusing recall, precision and F-measure.Table 7 and Figure 1 show the performanceof each experiment.Our proposed approach (ASC_CF_ME)gives the best results among all experiments,with an F-measure of 68.1%, followed closelyby ASC_ME.
This gives a 20% increase overour baseline.
For testing a portability of our ap-proach, we experiment the noun phrase ellipsis(NPE) detection.
The performance of NPE isalike an elided entry word.
Recall is 69.31, Pre-cision is 65.05, and F-measure is 67.12.
So weexpect the performance of our approach not todrop when applied to NPE or other ellipsis prob-lem.
The results so far are encouraging, andshow that the approach taken is capable of pro-ducing a robust and accurate system.In this paper, we suggested the approach thatrestores an elided entry word for EncyclopediaQA systems combining an acceptable senseclass algorithm, a caseframe algorithm, and MEmodel.For future work, we plan to pursue the fol-lowing research.
First, we will use various ma-chine learning methods and compare them withthe ME model.
Second, because we plan to ap-ply this approach in the encyclopedia document,we need to design the more general approach touse other ellipsis phenomenon.
Third, we try tofind a method for enhancing performance ofrestoring elided entry words as the object.ReferencesJames Allen.
1995.
Natural Language Under-standing, Benjamin/Cummings PublishingCompany, 449~455Leif Arda Nielsen.
2003.
Using Machine Learn-ing Techniques for VPE detection, RANLP 03,Bulgaria.Daniel Hardt.
1997.
An empirical approach tovp ellipsis, Computational Linguistics, 23(4).0102030405060CF ASC MEASC_CFASC_MEASC_CF_ME708090PrecisionRecallF-MeasureFigure 1.
Comparison of All ResultsAdwait Ratnaparkhi.
1998.
Maximum EntropyModels for Natural LANGUAGE AmbiguityResolution, Unpublished PhDthesis, Universityof Pennsylvania.Lim soojong.
2004.
Dependency RelationAnalysis Using Caseframe for EncyclopediaQuestion-Answering Systems, IECON, Korea.H.
J. Kim, H. J. Oh, C. H.
Lee., et al 2004.
The3-step Answer Processing Method for Encyclo-pedia Question-Answering System: AnyQues-tion 1.0.
The Proceedings of Asia InformationRetrieval Symposium (AIRS) 309-312219
