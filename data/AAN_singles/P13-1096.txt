Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 973?982,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUtterance-Level Multimodal Sentiment AnalysisVero?nica Pe?rez-Rosas and Rada MihalceaComputer Science and EngineeringUniversity of North Texasveronicaperezrosas@my.unt.edu, rada@cs.unt.eduLouis-Philippe MorencyInstitute for Creative TechnologiesUniversity of Southern Californiamorency@ict.usc.eduAbstractDuring real-life interactions, people arenaturally gesturing and modulating theirvoice to emphasize specific points or toexpress their emotions.
With the recentgrowth of social websites such as YouTube,Facebook, and Amazon, video reviews areemerging as a new source of multimodaland natural opinions that has been left al-most untapped by automatic opinion anal-ysis techniques.
This paper presents amethod for multimodal sentiment classi-fication, which can identify the sentimentexpressed in utterance-level visual datas-treams.
Using a new multimodal datasetconsisting of sentiment annotated utter-ances extracted from video reviews, weshow that multimodal sentiment analysiscan be effectively performed, and that thejoint use of visual, acoustic, and linguisticmodalities can lead to error rate reductionsof up to 10.5% as compared to the bestperforming individual modality.1 IntroductionVideo reviews represent a growing source of con-sumer information that gained increasing interestfrom companies, researchers, and consumers.
Pop-ular web platforms such as YouTube, Amazon,Facebook, and ExpoTV have reported a signifi-cant increase in the number of consumer reviewsin video format over the past five years.
Comparedto traditional text reviews, video reviews provide amore natural experience as they allow the viewer tobetter sense the reviewer?s emotions, beliefs, andintentions through richer channels such as intona-tions, facial expressions, and body language.Much of the work to date on opinion analysis hasfocused on textual data, and a number of resourceshave been created including lexicons (Wiebe andRiloff, 2005; Esuli and Sebastiani, 2006) or largeannotated datasets (Maas et al, 2011).
Given theaccelerated growth of other media on the Web andelsewhere, which includes massive collections ofvideos (e.g., YouTube, Vimeo, VideoLectures), im-ages (e.g., Flickr, Picasa), audio clips (e.g., pod-casts), the ability to address the identification ofopinions in the presence of diverse modalities is be-coming increasingly important.
This has motivatedresearchers to start exploring multimodal clues forthe detection of sentiment and emotions in videocontent (Morency et al, 2011; Wagner et al, 2011).In this paper, we explore the addition of speechand visual modalities to text analysis in order toidentify the sentiment expressed in video reviews.Given the non homogeneous nature of full-videoreviews, which typically include a mixture of posi-tive, negative, and neutral statements, we decidedto perform our experiments and analyses at the ut-terance level.
This is in line with earlier work ontext-based sentiment analysis, where it has beenobserved that full-document reviews often containboth positive and negative comments, which led toa number of methods addressing opinion analysisat sentence level.
Our results show that relyingon the joint use of linguistic, acoustic, and visualmodalities allows us to better sense the sentimentbeing expressed as compared to the use of only onemodality at a time.Another important aspect of this paper is the in-troduction of a new multimodal opinion databaseannotated at the utterance level which is, to ourknowledge, the first of its kind.
In our work, thisdataset enabled a wide range of multimodal senti-ment analysis experiments, addressing the relativeimportance of modalities and individual features.The following section presents related workin text-based sentiment analysis and audio-visualemotion recognition.
Section 3 describes our newmultimodal datasets with utterance-level sentimentannotations.
Section 4 presents our multimodal sen-973timent analysis approach, including details aboutour linguistic, acoustic, and visual features.
Ourexperiments and results on multimodal sentimentclassification are presented in Section 5, with adetailed discussion and analysis in Section 6.2 Related WorkIn this section we provide a brief overview of re-lated work in text-based sentiment analysis, as wellas audio-visual emotion analysis.2.1 Text-based Subjectivity and SentimentAnalysisThe techniques developed so far for subjectivityand sentiment analysis have focused primarily onthe processing of text, and consist of either rule-based classifiers that make use of opinion lexicons,or data-driven methods that assume the availabilityof a large dataset annotated for polarity.
These toolsand resources have been already used in a largenumber of applications, including expressive text-to-speech synthesis (Alm et al, 2005), trackingsentiment timelines in on-line forums and news(Balog et al, 2006), analysis of political debates(Carvalho et al, 2011), question answering (Oh etal., 2012), conversation summarization (Carenini etal., 2008), and citation sentiment detection (Atharand Teufel, 2012).One of the first lexicons used in sentiment anal-ysis is the General Inquirer (Stone, 1968).
Sincethen, many methods have been developed to auto-matically identify opinion words and their polarity(Hatzivassiloglou and McKeown, 1997; Turney,2002; Hu and Liu, 2004; Taboada et al, 2011), aswell as n-gram and more linguistically complexphrases (Yang and Cardie, 2012).For data-driven methods, one of the most widelyused datasets is the MPQA corpus (Wiebe et al,2005), which is a collection of news articles manu-ally annotated for opinions.
Other datasets are alsoavailable, including two polarity datasets consist-ing of movie reviews (Pang and Lee, 2004; Maas etal., 2011), and a collection of newspaper headlinesannotated for polarity (Strapparava and Mihalcea,2007).While difficult problems such as cross-domain(Blitzer et al, 2007; Li et al, 2012) or cross-language (Mihalcea et al, 2007; Wan, 2009; Menget al, 2012) portability have been addressed, notmuch has been done in terms of extending the ap-plicability of sentiment analysis to other modalities,such as speech or facial expressions.The only exceptions that we are aware of are thefindings reported in (Somasundaran et al, 2006;Raaijmakers et al, 2008; Mairesse et al, 2012;Metze et al, 2009), where speech and text havebeen analyzed jointly for the purpose of subjectiv-ity or sentiment identification, without, however,addressing other modalities such as visual cues;and the work reported in (Morency et al, 2011;Perez-Rosas et al, 2013), where multimodal cueshave been used for the analysis of sentiment inproduct reviews, but where the analysis was doneat the much coarser level of full videos rather thanindividual utterances as we do in our work.2.2 Audio-Visual Emotion Analysis.Also related to our work is the research done onemotion analysis.
Emotion analysis of speech sig-nals aims to identify the emotional or physicalstates of a person by analyzing his or her voice(Ververidis and Kotropoulos, 2006).
Proposedmethods for emotion recognition from speech fo-cus both on what is being said and how is be-ing said, and rely mainly on the analysis of thespeech signal by sampling the content at utteranceor frame level (Bitouk et al, 2010).
Several re-searchers used prosody (e.g., pitch, speaking rate,Mel frequency coefficients) for speech-based emo-tion recognition (Polzin and Waibel, 1996; Tato etal., 2002; Ayadi et al, 2011).There are also studies that analyzed the visualcues, such as facial expressions and body move-ments (Calder et al, 2001; Rosenblum et al, 1996;Essa and Pentland, 1997).
Facial expressions areamong the most powerful and natural means forhuman beings to communicate their emotions andintentions (Tian et al, 2001).
Emotions can bealso expressed unconsciously, through subtle move-ments of facial muscles such as smiling or eyebrowraising, often measured and described using theFacial Action Coding System (FACS) (Ekman etal., 2002).De Silva et.
al.
(De Silva et al, 1997) and Chenet.
al.
(Chen et al, 1998) presented one of theearly works that integrate both acoustic and visualinformation for emotion recognition.
In addition towork that considered individual modalities, thereis also a growing body of work concerned withmultimodal emotion analysis (Silva et al, 1997;Sebe et al, 2006; Zhihong et al, 2009; Wollmer etal., 2010).974Utterance transcription LabelEn este color, creo que era el color frambuesa.
neuIn this color, I think it was raspberryPinta hermosisimo.
posIt looks beautiful.Sinceramente, con respecto a lo que pinta y a que son hidratante, si son muy hidratantes.
posHonestly, talking about how they looks and hydrates, yes they are very hydrant.Pero el problema de estos labiales es que cuando uno se los aplica, te dejan un gusto asqueroso en la boca.
negBut the problem with those lipsticks is that when you apply them, they leave a very nasty tasteSinceramente, es no es que sea el olor sino que es mas bien el gusto.
negHonestly, is not the smell, it is the taste.Table 1: Sample utterance-level annotations.
The labels used are: pos(itive), neg(ative), neu(tral).More recently, two challenges have been or-ganized focusing on the recognition of emotionsusing audio and visual cues (Schuller et al,2011a; Schuller et al, 2011b), which included sub-challenges on audio-only, video-only, and audio-video, and drew the participation of many teamsfrom around the world.
Note however that most ofthe previous work on audio-visual emotion analy-sis has focused exclusively on the audio and videomodalities, and did not consider textual features, aswe do in our work.3 MOUD: Multimodal OpinionUtterances DatasetFor our experiments, we created a dataset of ut-terances (named MOUD) containing product opin-ions expressed in Spanish.1 We chose to work withSpanish because it is a widely used language, andit is the native language of the main author of thispaper.We started by collecting a set of videos fromthe social media web site YouTube, using severalkeywords likely to lead to a product review or rec-ommendation.
Starting with the YouTube searchpage, videos were found using the following key-words: mis products favoritos (my favorite prod-ucts), products que no recomiendo (non recom-mended products), mis perfumes favoritos (my fa-vorite perfumes), peliculas recomendadas (recom-mended movies), peliculas que no recomiendo (nonrecommended movies) and libros recomendados(recommended books), libros que no recomiendo(non recommended books).
Notice that the key-words are not targeted at a specific product type;rather, we used a variety of product names, so thatthe dataset has some degree of generality withinthe broad domain of product reviews.1Publicly available from the authors webpage.Among all the videos returned by the YouTubesearch, we selected only videos that respected thefollowing guidelines: the speaker should be in frontof the camera; her face should be clearly visible,with a minimum amount of face occlusion duringthe recording; there should not be any backgroundmusic or animation.
The final video set includes 80videos randomly selected from the videos retrievedfrom YouTube that also met the guidelines above.The dataset includes 15 male and 65 female speak-ers, with their age approximately ranging from 20to 60 years.All the videos were first pre-processed to elimi-nate introductory titles and advertisements.
Sincethe reviewers often switched topics when express-ing their opinions, we manually selected a 30 sec-onds opinion segment from each video to avoidhaving multiple topics in a single review.3.1 Segmentation and TranscriptionAll the video clips were manually processed totranscribe the verbal statements and also to extractthe start and end time of each utterance.
Since thereviewers utter expressive sentences that are nat-urally segmented by speech pauses, we decidedto use these pauses (>0.5seconds) to identify thebeginning and the end of each utterance.
The tran-scription and segmentation were performed usingthe Transcriber software.Each video was segmented into an average ofsix utterances, resulting in a final dataset of 498utterances.
Each utterance is linked to the corre-sponding audio and video stream, as well as itsmanual transcription.
The utterances have an aver-age duration of 5 seconds, with a standard deviationof 1.2 seconds.975Figure 1: Multimodal feature extraction3.2 Sentiment AnnotationTo enable the use of this dataset for sentiment de-tection, we performed sentiment annotations at ut-terance level.
Annotations were done using Elan,2which is a widely used tool for the annotation ofvideo and audio resources.
Two annotators indepen-dently labeled each utterance as positive, negative,or neutral.
The annotation was done after seeingthe video corresponding to an utterance (along withthe corresponding audio source).
The transcriptionof the utterance was also made available.
Thus, theannotation process included all three modalities: vi-sual, acoustic, and linguistic.
The annotators wereallowed to watch the video segment and their cor-responding transcription as many times as needed.The inter-annotator agreement was measured at88%, with a Kappa of 0.81, which represents goodagreement.
All the disagreements were reconciledthrough discussions.Table 1 shows the five utterances obtained from avideo in our dataset, along with their corresponding2http://tla.mpi.nl/tools/tla-tools/elan/sentiment annotations.
As this example illustrates,a video can contain a mix of positive, negative, andneutral utterances.
Note also that sentiment is notalways explicit in the text: for example, the lastutterance ?Honestly, it is not the smell, it is thetaste?
has an implicit reference to the ?nasty taste?expressed in the previous utterance, and thus it wasalso labeled as negative by both annotators.4 Multimodal Sentiment AnalysisThe main advantage that comes with the analysis ofvideo opinions, as compared to their textual coun-terparts, is the availability of visual and speech cues.In textual opinions, the only source of informationconsists of words and their dependencies, whichmay sometime prove insufficient to convey the ex-act sentiment of the user.
Instead, video opinionsnaturally contain multiple modalities, consisting ofvisual, acoustic, and linguistic datastreams.
We hy-pothesize that the simultaneous use of these threemodalities will help create a better opinion analysismodel.9764.1 Feature ExtractionThis section describes the process of automaticallyextracting linguistic, acoustic and visual featuresfrom the video reviews.
First, we obtain the streamcorresponding to each modality, followed by theextraction of a representative set of features foreach modality, as described in the following sub-sections.
These features are then used as cues tobuild a classifier of positive or negative sentiment.Figure 1 illustrates this process.4.1.1 Linguistic FeaturesWe use a bag-of-words representation of the videotranscriptions of each utterance to derive unigramcounts, which are then used as linguistic features.First, we build a vocabulary consisting of all thewords, including stopwords, occurring in the tran-scriptions of the training set.
We then removethose words that have a frequency below 10 (valuedetermined empirically on a small developmentset).
The remaining words represent the unigramfeatures, which are then associated with a valuecorresponding to the frequency of the unigram in-side each utterance transcription.
These simpleweighted unigram features have been successfullyused in the past to build sentiment classifiers ontext, and in conjunction with Support Vector Ma-chines (SVM) have been shown to lead to state-of-the-art performance (Maas et al, 2011).4.1.2 Acoustic FeaturesAcoustic features are automatically extracted fromthe speech signal of each utterance.
We used theopen source software OpenEAR (Schuller, 2009)to automatically compute a set of acoustic features.We include prosody, energy, voicing probabilities,spectrum, and cepstral features.?
Prosody features.
These include intensity,loudness, and pitch that describe the speechsignal in terms of amplitude and frequency.?
Energy features.
These features describe thehuman loudness perception.?
Voice probabilities.
These are probabilitiesthat represent an estimate of the percentage ofvoiced and unvoiced energy in the speech.?
Spectral features.
The spectral features arebased on the characteristics of the human ear,which uses a nonlinear frequency unit to simu-late the human auditory system.
These fea-tures describe the speech formants, whichmodel spoken content and represent speakercharacteristics.?
Cepstral features.
These features emphasizechanges or periodicity in the spectrum fea-tures measured by frequencies; we modelthem using 12 Mel-frequency cepstral coeffi-cients that are calculated based on the Fouriertransform of a speech frame.Overall, we have a set of 28 acoustic features.During the feature extraction, we use a frame sam-pling of 25ms.
Speaker normalization is performedusing z-standardization.
The voice intensity isthresholded to identify samples with and withoutspeech, with the same threshold being used for allthe experiments and all the speakers.
The featuresare averaged over all the frames in an utterance, toobtain one feature vector for each utterance.4.1.3 Facial FeaturesFacial expressions can provide important clues foraffect recognition, which we use to complementthe linguistic and acoustic features extracted fromthe speech stream.The most widely used system for measuring anddescribing facial behaviors is the Facial ActionCoding System (FACS), which allows for the de-scription of face muscle activities through the useof a set of Action Units (AUs).
According with(Ekman, 1993), there are 64 AUs that involve theupper and lower face, including several face posi-tions and movements.3 AUs can occur either bythemselves or in combination, and can be used toidentify a variety of emotions.
While AUs are fre-quently annotated by certified human annotators,automatic tools are also available.
In our work, weuse the Computer Expression Recognition Toolbox(CERT) (Littlewort et al, 2011), which allows us toautomatically extract the following visual features:?
Smile and head pose estimates.
The smilefeature is an estimate for smiles.
Head posedetection consists of three-dimensional esti-mates of the head orientation, i.e., yaw, pitch,and roll.
These features provide informationabout changes in smiles and face positionswhile uttering positive and negative opinions.?
Facial AUs.
These features are the raw es-timates for 30 facial AUs related to musclemovements for the eyes, eyebrows, nose, lips,3http://www.cs.cmu.edu/afs/cs/project/face/www/facs.htm977and chin.
They provide detailed informationabout facial behaviors from which we expectto find differences between positive and nega-tive states.?
Eight basic emotions.
These are estimatesfor the following emotions: anger, contempt,disgust, fear, joy, sad, surprise, and neutral.These features describe the presence of two ormore AUs that define a specific emotion.
Forexample, the unit A12 describes the pullingof lip corners movement, which usually sug-gests a smile but when associated with acheck raiser movement (unit A6), representsa marker for the emotion of happiness.We extract a total of 40 visual features, eachof them obtained at frame level.
Since only oneperson is present in each video clip, most of thetime facing the camera, the facial tracking wassuccessfully applied for most of our data.
For theanalysis, we use a sampling rate of 30 frames persecond.
The features extracted for each utteranceare averaged over all the valid frames, which areautomatically identified using the output of CERT.4Segments with more than 60% of invalid framesare simply discarded.5 Experiments and ResultsWe run our sentiment classification experimentson the MOUD dataset introduced earlier.
Fromthe dataset, we remove utterances labeled as neu-tral, thus keeping only the positive and negativeutterances with valid visual features.
The removalof neutral utterances is done for two main reasons.First, the number of neutral utterances in the datasetis rather small.
Second, previous work in subjec-tivity and sentiment analysis has demonstrated thata layered approach (where neutral statements arefirst separated from opinion statements followedby a separation between positive and negative state-ments) works better than a single three-way classifi-cation.
After this process, we are left with an exper-imental dataset of 412 utterances, 182 of which arelabeled as positive, and 231 are labeled as negative.From each utterance, we extract the linguis-tic, acoustic, and visual features described above,which are then combined using the early fusion(or feature-level fusion) approach (Hall and Llinas,4There is a small number of frames that CERT could notprocess, mostly due to the brief occlusions that occur whenthe speaker is showing the product she is reviewing.Modality AccuracyBaseline 55.93%One modality at a timeLinguistic 70.94%Acoustic 64.85%Visual 67.31%Two modalities at a timeLinguistic + Acoustic 72.88%Linguistic + Visual 72.39%Acoustic + Visual 68.86%Three modalities at a timeLinguistic+Acoustic+Visual 74.09%Table 2: Utterance-level sentiment classificationwith linguistic, acoustic, and visual features.1997; Atrey et al, 2010).
In this approach, the fea-tures collected from all the multimodal streams arecombined into a single feature vector, thus result-ing in one vector for each utterance in the datasetwhich is used to make a decision about the senti-ment orientation of the utterance.We run several comparative experiments, usingone, two, and three modalities at a time.
We usethe entire set of 412 utterances and run ten foldcross validations using an SVM classifier, as imple-mented in the Weka toolkit.5 In line with previouswork on emotion recognition in speech (Haq andJackson, 2009; Anagnostopoulos and Vovoli, 2010)where utterances are selected in a speaker depen-dent manner (i.e., utterances from the same speakerare included in both training and test), as well aswork on sentence-level opinion classification wheredocument boundaries are not considered in the splitperformed between the training and test sets (Wil-son et al, 2004; Wiegand and Klakow, 2009), thetraining/test split for each fold is performed at ut-terance level regardless of the video they belongto.Table 2 shows the results of the utterance-levelsentiment classification experiments.
The baselineis obtained using the ZeroR classifier, which as-signs the most frequent label by default, averagedover the ten folds.6 DiscussionThe experimental results show that sentiment clas-sification can be effectively performed on multi-modal datastreams.
Moreover, the integration of5http://www.cs.waikato.ac.nz/ml/weka/978Figure 2: Visual and acoustic feature weights.
Thisgraph shows the relative importance of the infor-mation gain weights associated with the top mostinformative acoustic-visual features.visual, acoustic, and linguistic features can improvesignificantly over the use of one modality at a time,with incremental improvements observed for eachadded modality.Among the individual classifiers, the linguisticclassifier appears to be the most accurate, followedby the classifier that relies on visual clues, and bythe audio classifier.
Compared to the best indi-vidual classifier, the relative error rate reductionobtained with the tri-modal classifier is 10.5%.The results obtained with this multimodal utter-ance classifier are found to be significantly betterthan the best individual results (obtained with thetext modality), with significance being tested witha t-test (p=0.05).Feature analysis.To determine the role played by each of the vi-sual and acoustic features, we compare the fea-ture weights assigned by the learning algorithm,as shown in Figure 2.
Interestingly, a distressedbrow is the strongest indicator of sentiment, fol-lowed, this time not surprisingly, by the smile fea-ture.
Other informative features for sentiment clas-sification are the voice probability, representing theenergy in speech, the combined visual features thatrepresent an angry face, and two of the cepstralcoefficients.To reach a better understanding of the relationbetween features, we also calculate the Pearsoncorrelation between the visual and acoustic fea-tures.
Table 3 shows a subset of these correlationfigures.
As we expected, correlations between fea-tures of the same type are higher.
For example,the correlation between features AU6 and AU12or the correlation between intensity and loudnessis higher than the correlation between AU6 and in-tensity.
Nonetheless, we still find some significantcorrelations between features of different types, forinstance AU12 and AU45 which are both signifi-cantly correlated with the intensity and loudnessfeatures.
This give us confidence about using themfor further analysis.Video-level sentiment analysis.To understand the role played by the size of thevideo-segments considered in the sentiment classi-fication experiments, as well as the potential effectof a speaker-independence assumption, we also runa set of experiments where we use full videos forthe classification.In these experiments, once again the sentimentannotation is done by two independent annotators,using the same protocol as in the utterance-basedannotations.
Videos that were ambivalent aboutthe general sentiment were either labeled as neu-tral (and thus removed from the experiments), orlabeled with the dominant sentiment.
The inter-annotator agreement for this annotation was mea-sured at 96.1%.
As before, the linguistic, acoustic,and visual features are averaged over the entirevideo, and we use an SVM classifier in ten-foldcross validation experiments.Table 4 shows the results obtained in thesevideo-level experiments.
While the combination ofmodalities still helps, the improvement is smallerthan the one obtained during the utterance-levelclassification.
Specifically, the combined effect ofacoustic and visual features improves significantlyover the individual modalities.
However, the com-bination of linguistic features with other modalitiesdoes not lead to clear improvements.
This may bedue to the smaller number of feature vectors usedin the experiments (only 80, as compared to the412 used in the previous setup).
Another possi-ble reason is the fact that the acoustic and visualmodalities are significantly weaker than the lin-guistic modality, most likely due to the fact thatthe feature vectors are now speaker-independent,which makes it harder to improve over the linguis-tic modality alone.7 ConclusionsIn this paper, we presented a multimodal approachfor utterance-level sentiment classification.
Weintroduced a new multimodal dataset consisting979AU6 AU12 AU45 AUs 1,1+4 Pitch Voice probability Intensity LoudnessAU6 1.00 0.46* -0.03 -0.05 0.06 -0.14* -0.04 -0.02AU12 1.00 -0.23* -0.33* 0.04 0.05 0.15* 0.16*AU45 1.00 0.05 -0.05 -0.11* -.163* 0.16*AUs 1,1+4 1.00 -0.11* -0.16* 0.06 0.07Pitch 1.00 -0.04 -0.01 -0.08Voice probability 1.00 0.19* 0.38*Intensity 1.00 0.85*Loudness 1.00Table 3: Correlations between several visual and acoustic features.
Visual features: AU6 Cheek raise,AU12 Lip corner pull, AU45 Blink eye and closure, AU1,1+4 Distress brow.
Acoustic features: Pitch,Voice probability, Intensity, Energy.
*Correlation is significant at the 0.05 level (1-tailed).Modality AccuracyBaseline 55.93%One modality at a timeLinguistic 73.33%Acoustic 53.33%Visual 50.66%Two modalities at a timeLinguistic + Acoustic 72.00%Linguistic + Visual 74.66%Acoustic + Visual 61.33%Three modalities at a timeLinguistic+Acoustic+Visual 74.66%Table 4: Video-level sentiment classification withlinguistic, acoustic, and visual features.of sentiment annotated utterances extracted fromvideo reviews, where each utterance is associatedwith a video, acoustic, and linguistic datastream.Our experiments show that sentiment annotationof utterance-level visual datastreams can be ef-fectively performed, and that the use of multiplemodalities can lead to error rate reductions of up to10.5% as compared to the use of one modality at atime.
In future work, we plan to explore alternativemultimodal fusion methods, such as decision-leveland meta-level fusion, to improve the integrationof the visual, acoustic, and linguistic modalities.AcknowledgmentsWe would like to thank Alberto Castro for his helpwith the sentiment annotations.
This material isbased in part upon work supported by National Sci-ence Foundation awards #0917170 and #1118018,by DARPA-BAA-12-47 DEFT grant #12475008,and by a grant from U.S. RDECOM.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authorsand do not necessarily reflect the views of the Na-tional Science Foundation, the Defense AdvancedResearch Projects Agency, or the U.S. Army Re-search, Development, and Engineering Command.ReferencesC.
Alm, D. Roth, and R. Sproat.
2005.
Emotionsfrom text: Machine learning for text-based emotionprediction.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 347?354, Vancouver, Canada.C.
Anagnostopoulos and E. Vovoli.
2010.
Sound pro-cessing features for speaker-dependent and phrase-independent emotion recognition in berlin database.In Information Systems Development, pages 413?421.
Springer.A.
Athar and S. Teufel.
2012.
Context-enhanced cita-tion sentiment detection.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, Montre?al, Canada, June.P.
K. Atrey, M. A. Hossain, A. El Saddik, andM.
Kankanhalli.
2010.
Multimodal fusion for mul-timedia analysis: a survey.
Multimedia Systems, 16.M.
El Ayadi, M. Kamel, and F. Karray.
2011.
Surveyon speech emotion recognition: Features, classifica-tion schemes, and databases.
Pattern Recognition,44(3):572 ?
587.K.
Balog, G. Mishne, and M. de Rijke.
2006.
Why arethey excited?
identifying and explaining spikes inblog mood levels.
In Proceedings of the 11th Meet-ing of the European Chapter of the As sociation forComputational Linguistics (EACL-2006).Dmitri Bitouk, Ragini Verma, and Ani Nenkova.
2010.Class-level spectral features for emotion recognition.Speech Commun., 52(7-8):613?625, July.980J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biogra-phies, bollywood, boom-boxes and blenders: Do-main adaptation for sentiment classification.
In As-sociation for Computational Linguistics.A.
J. Calder, A. M. Burton, P. Miller, A. W. Young, andS.
Akamatsu.
2001.
A principal component analysisof facial expressions.
Vision research, 41(9):1179?1208, April.G.
Carenini, R. Ng, and X. Zhou.
2008.
Summarizingemails with conversational cohesion and subjectivity.In Proceedings of the Association for ComputationalLinguistics: Human Language Technologies (ACL-HLT 2008), Columbus, Ohio.P.
Carvalho, L. Sarmento, J. Teixeira, and M. Silva.2011.
Liars and saviors in a sentiment annotatedcorpus of comments to political debates.
In Proceed-ings of the Association for Computational Linguis-tics (ACL 2011), Portland, OR.L.
S. Chen, T. S. Huang, T. Miyasato, and R. Nakatsu.1998.
Multimodal human emotion/expression recog-nition.
In Proceedings of the 3rd.
International Con-ference on Face & Gesture Recognition, pages 366?,Washington, DC, USA.
IEEE Computer Society.L C De Silva, T Miyasato, and R Nakatsu, 1997.
Facialemotion recognition using multi-modal information,volume 1, page 397401.
IEEE Signal Processing So-ciety.P.
Ekman, W. Friesen, and J. Hager.
2002.
Facial ac-tion coding system.P.
Ekman.
1993.
Facial expression of emotion.
Ameri-can Psychologist, 48:384?392.I.A.
Essa and A.P.
Pentland.
1997.
Coding, analy-sis, interpretation, and recognition of facial expres-sions.
Pattern Analysis and Machine Intelligence,IEEE Transactions on, 19(7):757 ?763, jul.A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion min-ing.
In Proceedings of the 5th Conference on Lan-guage Resources and Evaluation (LREC 2006), Gen-ova, IT.D.L.
Hall and J. Llinas.
1997.
An introduction to mul-tisensor fusion.
IEEE Special Issue on Data Fusion,85(1).S.
Haq and P. Jackson.
2009.
Speaker-dependentaudio-visual emotion recognition.
In InternationalConference on Audio-Visual Speech Processing.V.
Hatzivassiloglou and K. McKeown.
1997.
Predict-ing the semantic orientation of adjectives.
In Pro-ceedings of the Conference of the European Chap-ter of the Association for Computational Linguistics,pages 174?181.M.
Hu and B. Liu.
2004.
Mining and summariz-ing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, Seattle, Washing-ton.F.
Li, S. J. Pan, O. Jin, Q. Yang, and X. Zhu.
2012.Cross-domain co-extraction of sentiment and topiclexicons.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics,Jeju Island, Korea.G.
Littlewort, J. Whitehill, Tingfan Wu, I. Fasel,M.
Frank, J. Movellan, and M. Bartlett.
2011.
Thecomputer expression recognition toolbox (cert).
InAutomatic Face Gesture Recognition and Workshops(FG 2011), 2011 IEEE International Conference on,pages 298 ?305, march.A.
Maas, R. Daly, P. Pham, D. Huang, A. Ng, andC.
Potts.
2011.
Learning word vectors for sentimentanalysis.
In Proceedings of the Association for Com-putational Linguistics (ACL 2011), Portland, OR.F.
Mairesse, J. Polifroni, and G. Di Fabbrizio.
2012.Can prosody inform sentiment analysis?
experi-ments on short spoken reviews.
In Acoustics, Speechand Signal Processing (ICASSP), 2012 IEEE Inter-national Conference on, pages 5093 ?5096, march.X.
Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and H. Wang.2012.
Cross-lingual mixture model for sentimentclassification.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics, Jeju Island, Korea.F.
Metze, T. Polzehl, and M. Wagner.
2009.
Fusionof acoustic and linguistic features for emotion detec-tion.
In Semantic Computing, 2009.
ICSC ?09.
IEEEInternational Conference on, pages 153 ?160, sept.R.
Mihalcea, C. Banea, and J. Wiebe.
2007.
Learningmultilingual subjective language via cross-lingualprojections.
In Proceedings of the Association forComputational Linguistics, Prague, Czech Republic.L.P.
Morency, R. Mihalcea, and P. Doshi.
2011.
To-wards multimodal sentiment analysis: Harvestingopinions from the web.
In Proceedings of the In-ternational Conference on Multimodal Computing,Alicante, Spain.J.
Oh, K. Torisawa, C. Hashimoto, T. Kawada,S.
De Saeger, J. Kazama, and Y. Wang.
2012.Why question answering using sentiment analysisand word classes.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, Jeju Island, Korea.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndMeeting of the Association for Computational Lin-guistics, Barcelona, Spain, July.981V.
Perez-Rosas, R. Mihalcea, and L.-P. Morency.
2013.Multimodal sentiment analysis of spanish onlinevideos.
IEEE Intelligent Systems.T.
Polzin and A. Waibel.
1996.
Recognizing emotionsin speech.
In In ICSLP.S.
Raaijmakers, K. Truong, and T. Wilson.
2008.
Mul-timodal subjectivity analysis of multiparty conversa-tion.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages466?474, Honolulu, Hawaii.M.
Rosenblum, Y. Yacoob, and L.S.
Davis.
1996.
Hu-man expression recognition from motion using a ra-dial basis function network architecture.
Neural Net-works, IEEE Transactions on, 7(5):1121 ?1138, sep.B.
Schuller, M. Valstar, R. Cowie, and M. Pantic, edi-tors.
2011a.
Audio/Visual Emotion Challenge andWorkshop (AVEC 2011).B.
Schuller, M. Valstar, F. Eyben, R. Cowie, andM.
Pantic, editors.
2011b.
Audio/Visual EmotionChallenge and Workshop (AVEC 2011).F.
Eyben M. Wollmer B. Schuller.
2009.
Openear in-troducing the munich open-source emotion and af-fect recognition toolkit.
In ACII.N.
Sebe, I. Cohen, T. Gevers, and T.S.
Huang.
2006.Emotion recognition based on joint visual and audiocues.
In ICPR.D.
Silva, T. Miyasato, and R. Nakatsu.
1997.
Facialemotion recognition using multi-modal information.In Proceedings of the International Conference onInformation and Communications Security.S.
Somasundaran, J. Wiebe, P. Hoffmann, and D. Lit-man.
2006.
Manual annotation of opinion cate-gories in meetings.
In Proceedings of the Work-shop on Frontiers in Linguistically Annotated Cor-pora 2006.P.
Stone.
1968.
General Inquirer: Computer Approachto Content Analysis.
MIT Press.C.
Strapparava and R. Mihalcea.
2007.
Semeval-2007task 14: Affective text.
In Proceedings of the 4th In-ternational Workshop on the Semantic Evaluations(SemEval 2007), Prague, Czech Republic.M.
Taboada, J. Brooke, M. Tofiloski, K. Voli, andM.
Stede.
2011.
Lexicon-based methods for sen-timent analysis.
Computational Linguistics, 37(3).R.
Tato, R. Santos, R. Kompe, and J. M. Pardo.
2002.Emotional space improves emotion recognition.
InIn Proc.
ICSLP 2002, pages 2029?2032.Y.-I.
Tian, T. Kanade, and J.F.
Cohn.
2001.
Recogniz-ing action units for facial expression analysis.
Pat-tern Analysis and Machine Intelligence, IEEE Trans-actions on, 23(2):97 ?115, feb.P.
Turney.
2002.
Thumbs up or thumbs down?
seman-tic orientation applied to unsupervised classificationof reviews.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL 2002), pages 417?424, Philadelphia.D.
Ververidis and C. Kotropoulos.
2006.
Emotionalspeech recognition: Resources, features, and meth-ods.
Speech Communication, 48(9):1162?1181,September.J.
Wagner, E. Andre, F. Lingenfelser, and JonghwaKim.
2011.
Exploring fusion methods for multi-modal emotion recognition with missing data.
Af-fective Computing, IEEE Transactions on, 2(4):206?218, oct.-dec.X.
Wan.
2009.
Co-training for cross-lingual sentimentclassification.
In Proceedings of the Joint Confer-ence of the Association of Computational Linguisticsand the International Joint Conference on NaturalLanguage Processing, Singapore, August.J.
Wiebe and E. Riloff.
2005.
Creating subjective andobjective sentence classifiers from unannotated texts.In Proceedings of the 6th International Conferenceon Intelligent Text Processing and ComputationalLinguistics (CICLing-2005) (invited paper), MexicoCity, Mexico.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotatingexpressions of opinions and emotions in language.Language Resources and Evaluation, 39(2-3):165?210.M.
Wiegand and D. Klakow.
2009.
The role ofknowledge-based features in polarity classificationat sentence level.
In Proceedings of the Interna-tional Conference of the Florida Artificial Intelli-gence Research Society.T.
Wilson, J. Wiebe, and R. Hwa.
2004.
Just how madare you?
finding strong and weak opinion clauses.In Proceedings of the American Association for Arti-ficial Intelligence.M.
Wollmer, B. Schuller, F. Eyben, and G. Rigoll.2010.
Combining long short-term memory and dy-namic bayesian networks for incremental emotion-sensitive artificial listening.
IEEE Journal of Se-lected Topics in Signal Processing, 4(5), October.B.
Yang and C. Cardie.
2012.
Extracting opinionexpressions with semi-markov conditional randomfields.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,Jeju Island, Korea.Z.
Zhihong, M. Pantic G.I.
Roisman, and T.S.
Huang.2009.
A survey of affect recognition methods: Au-dio, visual, and spontaneous expressions.
PAMI,31(1).982
