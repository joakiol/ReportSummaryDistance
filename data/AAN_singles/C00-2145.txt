A Model of Competence for Corpus-Based Machine TranslationMichael CarlInstitut fiir Angewandte Informationsforschung,Martin-Luther-Strafle 14,66111 Saarbrficken, Germany,carl@iai.uni-sb.deAbst rac tIn this paper I claborate a model of colnpetenccfor corpus-based machine translation (CBMT) alongthe lines of the representations u ed in the transla-tion system.
Representations in CBMT-systems canbe rich or austere, molecular or holistic and they canbe fine-grained or coarse-grained.
The paper showsthat different CBMT architectures are required de-pendent on whether a better translation quality ora broader coverage is preferred according to Boitct(1999)'s formula: "Coverage * Quality = K".1 In t roduct ionIn the machine translation (MT) literature, it hasoften been argued that translations of natural lan-guage texts are valid if and only if the source lan-guage text and the target language text have thesame meaning cf.
e.g.
(Nagao, 1989).
If we assumethat MT systems produce meaningflfl translationsto a certain extent, wc must assmne that such sys-tems have a notion of the source text meaning to asimilar extent.
Hence, the translation algorithm to-gether with the data it uses encode a formal model ofmeaning.
Despite 50 years of intense research, thereis no existing system that could map arbitrary inputtexts onto meaning-equivalent output texts.
How isthat possible?According to (Dummett, 1975) a theory of mean-lug is a theory of understanding: having a theoryof meaning means that one has a theory of under-standing.
In linguistic research, texts are describedon a number of levels and dimensions each contribut-ing to its understanding and hence to its memfing.l~raditionally, the main focus has been on semanticaspects.
In this research it is assumed that know-ing the propositional structure of a text means tounderstand it.
Under the same premise, research inM.q?
has focused on semantic aspects assmning thattexts have the same meaning if they are semanticallyequivalent.Recent research in corpus-based MT has differ-ent premisses.
Corpus-Based Machine Translation(CBMT) systems make use of a set of referencetranslations on which the translation of a new textis based.
In CBMT-systems, it is assumed thatthe reference translations given to the system in atraining phase have equivalence meanings.
Accord-ing to their intelligence, these systems try to fig-urc out of what the meaning invariance consists inthe reference text and learn an appropriate sourcelanguage/target language mapping mechanism.
Atranslation can only be generated if an appropriateexample translation is available in the reference text.An interesting question in CBMT systems is thus:what theory of meaning should the learning pro-cess implement in order to generate an appropriateunderstanding of the source text such that it canbe mapped iuto a meaning equivalent arget text?Dulmnett (Dummett, 1975) suggests a distinctionof theories of meaning along the following lines:* In a rich theory of meaning, the knowledge ofthe concepts is achieved by knowing the featuresof these concepts.
An ausle'ce theory merely re-lies upon simple recognition of the shape of theconcepts.
A rich theory can justify the use of aconcept by means of the characteristic featuresof that concept, whereas an austere theory canjustify the use of a concept merely by enmner-ating all occurrences of the use of that concept.. A moh'.euIar theory of meaning derives theunderstanding of an expression from a finitenumber of axioms.
A holistic theory, in con-trast, derives the understanding of an expres-sion through its distinction from all other ex-pressions in that language.
A molecular theory,therefore, provides criteria to associate a cer-tain meaning to a sentence and can explain theconcepts used in the language.
In a holistic the-ory nothing is specified about the knowledge ofthe language other than in global constraintsrelated to the language as a whole.In addition, the granularity of concepts eems cru-cial for CBMT implementations.
* A fine-grained theory of meaning derives con-cepts from single morphemes or separable wordsof the language, whereas in a coar~'e-qrained997theory of meaning, concepts are obtained frommorpheme clusters.
In a fine-grained theory ofmeaning, complex concepts can be created byhierarchical composition of their components,whereas in a coarse-grained theory of meaning,complex meanings can only be achieved througha concatenation of concept sequences.The next three sections discuss the dichotomies oftheories of nleaning, rich 'vs.
auz~ere, molecular vs.holis*ic and coarse-grained vs. fine-grained where afew CBMT systems are classified according to theterminology introduced.
This leads to a model ofcompetence for CBMT.
It appears that translationsystems can either be designed to have a broad cov-erage or a high quali@.2 R ich  vs .
Austere  CBMTA common characteristic of all CBMT systems isthat the understanding of the translation task is de-rived fronl the understanding of the reference trans-lations.
The inferred translation knowledge is usedin the translation phase to generate new transla-tions.Collins (1998) distinguishes between Memory-Based MT, i.e.
menlory heavy, linguistic light andExample-Based MT i.e.
memory light and linguisticheavy.
While the former systems implement an aus-tere theory of meaning, the latter make use of richrepresentations.The most superficial theory of understandingis implenlented in purely menlory-based MT ap-proaches where learning takes place only by extend-ing the reference text.
No abstraction or generaliza-tion of the reference xamples takes place.Translation Memories (TMs) are such purelymemory based MT-systems.
A TM e.g.
TRADOS'sTranslator's Workbench (Heyn, 1996), and STAR'sTRANSIT calculates the graphenfic similarity of theinput text and the source side of the reference trans-lations and return the target string of the nlost sim-ilar translation examples as output.
TMs make useof a set of reference translation examples and a (k-nn) retrieval algorithm.
They iulplement an austeretheory of nleaning because they cannot justify theuse of a word other than by looking up all contextsin which the word occurs.
They can, however, enu-merate all occurrences of a word in the referencetext.The TM distributed by ZERES (Zer, 1997) followsa richer approach.
The reference translations andthe input sentence to be translated are lemmatizedand part-of-speech tagged.
The source language sen-tence is nlapped against the reference translationson a surface string level, on a lemma level and ona part-of-speech level.
Those example translationswhich show greatest similarity to the input sentencewith respect to the three levels of description arereturned as the best available translation.Example Based Machine Translation (EBMT)systems (Sato and Nagao, 1990; Collins, 1998;Gilvenir and Cicekli, 1998; Carl, 1999; Brown, 1997)are richer systems.
Translation examples are storedas feature and tree structures.
Translation tenlplatesare generated which contain - SOuletinles weighted- connections in those positions where the sourcelanguage and the target language equivalences arestrong.
In the translation phase, a multi-layeredmapping from the source language into the targetlanguage takes place on the level of templates andon the level of fillers.The ReVerb EBMT system (Collins, 1998) per-forms sub-sentential chunking and seeks to link con-stituents with the same function in the source andthe target language.
A source language subject istranslated as a target language subject and a sourcelanguage object as a target language object.
In casethere is no appropriate translation template avail-able, single words can be replaced as well, at theexpense of translation quality.The EBMT approach described in (Giivenir andCicekli, 1998) makes use of morphological knowl-edge and relies on word stems as a basis for trans-lation.
Translation templates are generalized fronlaligned sentences by substituting differences in sen-tence pairs with variables aud leaving the identicalsubstrings unsubstituted.
An iterative applicationof this nlethod generates translation examples andtranslation templates which serve as the basis foran example based MT system.
An understandingconsists of extraction of compositionally translatablesubstriugs and the generation of translation tem-plates.A similar approach is followed in EDGAR (Carl,1999).
Sentences are morphologically analyzed andtranslation templates are decorated with features.Fillers in translation template slots are constrainedto unify with these features.
In addition to this,a shallow linguistic formalism is used to percolatefeatures in derivation trees.Sato and Nagao (1990) proposed still richer repre-sentations where syntactically analyzed phrases andsentences are stored in a database.
In the translationphase, most similar derivation trees are retrievedfrom the database and a target language deriva-tion tree is conlposed fronl the translated parts.
Bymeans of a thesaurus emantically similar lexicalitems may be exchanged in the derivation trees.Statistics based MT (SBMT) approaches imple-ment austere theories of lncaning.
For instance, inBrown et al (1990) a couple of models are pre-sented starting with simple stochastic translationmodels getting incrementally more complex and richby introducing more random variables.
No linguistic998Richness of Representation Richness of Representation Granularity of Representationseresyrtmotgra?
1 8e?T~o2, 3 ?
G syn$4 TY~Or"5 *r "S,9 gramolecular nfixed holisticAtomicity of RepresentationOlO2,3 060408 o7 09,5word phrase sentenceGranularity of Representationsent.phras,wordO4,5 $9o6,7qb 1 o2,3 e8molecular mixed holisticAtonricity of Representationel: Sato and Nagao (1990)o4: ZERES Zer (1997)or: Brown (1997)?
2: EDGAR Carl (1999)?
5: TRADOS Heyn (1996)?
s: Brown et al (1990)?
3: GEBMT Gfivenir and Cicekli (1998)?
6: ReVerb Collins (1998)?
9: McLean (1992)\]figure 1: Atomicity, Granularity and Richness of CBMTanalyses are taken into account in these approaches.However, in further research the authors plan tointegrate linguistic knowledge such as inflectionalanalysis of verbs, nouns and adjectives.McLean (McLean, 1992) has proposed an austereapproach where lie uses neural networks (NN) totranslate surface strings from English to French.
Hisapproach functions similar to TM where the NN isu,;ed to classify the sequences of surface word formsaccording to the examples given in the referencetranslations.
On a small set of examples hc showsthat NN can successfully be applied for MT.3 Molecu lar  vs.  Ho l i s t i c  CBMTAs discussed in the previous section, all CBMT sys-tems make use of sonle text dimensions in order tomap a source language text into the target language.TMs, for instance, rely on the set of graphenficalsymbols i.e.
the ASCII set.
Richer systems use lcx-ical, morphological, syntactic and/or semantic de-scriptions.
The degree to which the set of descrip-tions is independent from the reference translationsdetermines the molecularity of the theory.
The morethe descriptions are learned from and thus dependon the reference translations the more the systembecomes holistic.
Learning descriptions from refer-cnce translations makes the system more robust andeasy to adjust to a new text domain.SBMT approaches e.g.
(Brown et al, 1990) havea purely holistic view on languages.
Every sentenceof one language is considered to bca  possible trans-lation of any sentence in the other language.
Noaccount is given for the equivalence of the sourcelanguage meaning and the target language meaningother than by means of global considerations con-cenfing frequencies of occurrence in the referencetext.
In order to compute the most probable trails-lations, each pair of items of the source language andthe target language is associated with a certain prob-ability.
This prior probability is derived from thereference text.
In the translation phase, several tar-get language sequences are considered and the onewith the highest posterior probability is then takento be the translation of the source language string.Similarly, neural network based CBMT systems(McLean, 1992) are holistic approaches.
The train-ing of the weights and the nfinimization of the classi-fication error relies oil the reference text as a whole.Temptations to extract rules from the trained neu-ral networks eek to isolate and make explicit aspectson how the net successfully classifies new sequences.The training process, however, remains holistic.TMs implement the molecular CBMT approach asthey rely on a static distance metric which is inde-pendent from the size and content of the case base.TMs are molecular because they rely on a fixed andlimited set of graphic symbols.
Adding further ex-ample translations to the data base does not increasethe set of the graphic symbols nor does it modify thedistance metric.
Learning capacities in TMs are triv-ial as their only way to learn is through extension ofthe example base.The translation templates generated by Giivenirand Cicekli (1998), for instance, differ according tothe similarities and dissinfilarities found in the ref-erence text.
Translation templates in this systemthus reflect holistic aspects of the example transla-tions.
The way in which morphological analyses isprocessed is, however, independent front the transla-tion examples and is thus a molecular aspect in thesystem.Similarly, the ReVerb EBMT system (Collins,1998) makes use of holistic components.
The ref-erence text is part-of-speech tagged.
The length oftranslation segments as well as their most likely lift-tim and final words arc calculated based on proba-999Expected Coverage of the Systemhighlow%low highExpected 2?anslation Quality?
1: Sato and Nagao (1990)?
2: Carl (1999)?
3: Giivenir and Cicekli (1998)e4: Zer (1997)e~:  Heyn (1996)?
6: Collins (1998)?
?
: Brown (1997)?
s: Brown et al (1990)?
9: McLean (1992)Figure 2: A Model of Competence for CBMTbilities found in the reference text.4 Coarse vs. F ine Graining CBMTOne task that all MT systems perform is to segmentthe text to be translated into translation units which- -  to a certain extent - -  can be translated indepen-dently.
The ways in which segmentation takes placeand how the translated segments are joined togetherin the target language are different in each MT sys-tem.In (Collins, 1998) segmentation takes place on aphrasal level.
Due to the lack of a rich morphologicalrepresentation, agreement cannot always be grantedin the target language when translating single wordsfrom English to German.
Reliable translation can-not be guaranteed when phrases in the target lan-guage - or parts of it - are moved from one position(e.g.
the object position) into another one (e.g.
asubject position).In (Giivenir and Cicekli, 1998), this situation iseven more problematic because there are no restric-tions on possible fillers of translation template slots.Thus, a slot which has originally been filled with anobject can, in the translation process, even accom-modate an adverb or the subject.SBMT approaches perform fine-grained segmen-tation.
Brown et al (1990) segment he inputsentences into words where for each source-targetlanguage word pair translation probabilities, fertil-ity probabilities, alignment probabilities etc.
arecomputed.
Coarse-grained segmentation are unre-alistic because sequences of 3 or more words (so-called n-grams) occur very rarely for n > 3 even illhuge learning corpora 1.
Statistical (and probabilis-tic) systems rely on word frequencies found in textsand usually cannot extrapolate from a very smallnumber of word occurrences.
A statistical language1 Brown et al (1990) uses the Hansard French-English textcontaining several million words.model assigns to each n-gram a probability whichenables the system to generate the most likely tar-get language strings.5 A Competence  Mode l  fo r  CBMTA competence model is presented as two indepen-dent parameters, i.e.
Coverage and Quality (see Fig-ure 2).?
Coverage  of the system refers to the extent towhich a variety of source language texts can betranslated.
A system has a high coverage if agreat variety of texts can be translated.
A low-coverage system can translate only restrictedtexts of a certain domain with limited ternfi-nology and linguistic structures.?
Qua l i ty  refers to the degree to which an MTsystem produces uccessful translations.
A sys-tem has a low quality if the produced transla-tions are not even informative in the sense thata user cannot understand what the source textis about.
A high quality MT-system producesuser-oriented and correct translations with re-spect to text type, terminological preferences,personal style, etc.An MT systenr with low coverage and low qualityis completely uninteresting.
Such a system comesclose to a randonr number generator as it translatesfew texts in an unpredictable way.An MT system with high coverage and "not-too-bad" quality can be useful in a Web-applicationwhere a great variety of texts are to be translatedfor occasional users which want to grasp the basicideas of a foreign text.
On the other hand a systemwith high quality and restricted coverage might beuseful for in-house MT-applications or a controlledlanguage.An MT sys tem with high coverage and high qual-ity would translate any type of text to everyone's1000satisfaction, lIowever, as one can expect, such asystem seems to bc not feasible.Boitct (1999) proposes "the (tentative) formula:Coverage ?
Quality -= K "where K depends on theMT technology and the amount of work encoded inthe system.
The question, then, is when is the max~imum K possible and how nluch work do we wantto invest for what purpose.
Moreover a given K canmean high coverage and low quality, or it can meanthe reverse.The expected quality of a CBMT system increaseswhen segmenting more coarsely the input text.
Con-sequently, a low coverage must bc expected ue tothe combinatorial explosion of the number of longer(:hunks.
in order for a fine-grailfing system to gencr-z~te at least informative translations, further knowl-edge resources need be considered.
These knowledgeresources may be either pre-defined and molecular orthey can be derived fronl reference translations andholistic.TMs focus on the quality of translations.
Onlylarge clusters of nlcaning entities are translated intothe target language in the hope that such clus-ters will not interfere with the context from whichthey are taken.
Broader coverage can be achievedthrough finer grained segmentation f the input intophrases or single terms.
Systems which finely seg-ment texts use rich representation languages in or-der to adapt the translation units to the target lan-guage context or, as in the case of SBMT systems,use holistic derived constraints.What can bc learned and what should be learnedfrom the reference text, how to represent he in-ferred knowledge, how to combine it with pre-defincdknowledge and the impact of difl'erent settings on theconstant K in the formula of Boitet (1999) are allstill open question for CBMT-design.6 ConclusionMachine Tr'anslation (MT) is a lneaning preservingraapping from a source language text into a targetlanguage text.
In order to enable a computer systemto perform such a mapping, it is provided with aformalized theory of meaning.Theories of meaning are characterized by three di-chotomies: they call be holistic or molecular, aus-tere or rich and they can be fine-grained or coarsc-Muiucd.A number of CBMT systems - translation mem-ories, example-based and statistical-based nlachinetranslation systenls - arc examined with respect tothese dichotomies.
Ill a system that uses a rich the-ory of meaning, complex representations arc com-puted including morphological, syntactical and se-rnantical representations, while with an austere the-ory the system relics on the mere graphcnfic surfaceform of the text.
In a holistie implementation mean-ing descriptions are derived from reference transla-tions while in a molecular approach the meaning dc-scriptions are obtained from a finite set of prede-fined features.
In a fine-grained theory, the minimallength of a translation unit is equivalent o a mor-1)heme while in a coarse-grained theory this amountsto a morphenle cluster, a phrase or a sentence.According to the implemented theory of meaning,one can expect o obtain high quality translations ora good covera.qc of the CBMT system.The more the system makes use of coarse-grainedtranslation units, the higher is tlle expected trans-lation quality.
The more the theory uses rich repre-sentations thc more the system may achieve broadcoverage.
CBMT systems can be tuned to achievecither of the two goals.ReferencesChristian Boitet.
1999.
A research perspectiveon how to democratize machine translation andtranslation aides aiming at high quality final out-put.
In MT-Summit  '99.Peter F. Brown, 3.
Cockc, Stephen A. Della Pietra,Vincent 3.
Della Pietra, F. Jelinek, Mercer RobertL., and Roossiu P.S.
1990.
A statistical approachto machine translation.
Computational Linguis-tics, 16:79-85.Ralf D. Brown.
1997.
Automated Dictionary Ex-traction for "Knowledge-Free" Example-BasedTranslation.
In TMI-97, pages 111-118.Michael Carl.
1999.
Inducing Translation Templatesfor Example-Based Machine Translation.
hi MT-Summit VII:Br6na Collins.
1998.
Example-Based MachineTra'nMation: An Adaptation-Guided Retrieval Ap-pTvach.
Ph.D. thesis, Trinity College, Dublin.Michael Dummett.
1975.
What is a Theory ofMeaning?
In Mind and Language.
Oxford Uni-versity Press, Oxford.Halil Altay Giivenir and Ilyas Cicekli.
1998.
Learn-ing Translation Templates from Examples.
Infor-mation Systems, 23(6):353-363.Matthias Hcyn.
1996.
Integrating machine trans-lation into translation memory systems.
In Eu-ropean Asscociation for Machine Translation -Workshop Proceedings, pages 111-123, ISSCO~Geneva.Ian J. McLean.
1992.
Example-Based MachineTranslation using Connectionist Matching.
InTMI-92.Makoto Nagao.
1989.
Machine Translation ftbw FarCan It Go.
Oxford University Press, Oxford.S.
Sato and M. Nagao.
1990.
Towards memory-based translation.
In COLING-90.Zeres GmbH, Bochunl, Germany, 1997.
ZERE-STRANS Bcnutzcrhandbuch.1001
