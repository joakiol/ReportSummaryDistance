A Hybr id  Japanese  Parser wi th  Hand-craf ted Grammar  andStat ist icsHiroshi Kanayama 1, Kentaro Torisawa 1:*,Yutaka Mitsuishi* a,nd Jun' ichi  Tsujiit*\[ Tokyo Research Lal)oratory, IBM Jal)an, Ltd.1623-1 d Shimo-tsuruma, Yamato-shi, Kanagawa 242-8502, Jal)an:!
: Del)artment of Inlbrnmtion Science, Graduate School of Science, University of 2bkyo7-a-1 Hongo, Bunkyo-ku, Tokyo 113-0033, .\]al)an?
hfformation and Human Behavior, PII.I~3S'\].
'O, Jal)an Scie, tce and Technology CorporationKawaguehi l ion-the 4-1-8, Kawaguchi-shi, Saitmna a32-0012, Japan, CCL, UMIST, U.K.{kanayama, torisawa, mitsuisi, tsujii}@is, s.u-tokyo, ac.
jpAbstractThis l)al)er (leseril)es a hybrid t)arsing method forJal)anese which uses both a hand-crafted gram-mar and a statistical 1;e(:hniqlte.
The key featm'eof our  syst (nn is that  in o rder  to es t imate  likeli-hood  for a parse tree, the sys te ln  Ilses informa-tion taken from Mternative 1)artial parse trees gen-erated by the grammar.
This utilization of alter-native trees enables us to construct a new statis-tical model called 'l}'it)let/Quadrul)let Model.
Weshow that this model can capture a certain ten-dency in .\]apalmse yntactic structures and this pointCOlltril)lltes to i lnl)rovetl lel l t  of l)al'sill~ acct l racy oila shallow level.
We rel)ort that, with an under-Sl)ecified HPSG-1)ased grammar and ~t maximum en-tropy estilnation, our parser achieved high a(:curacy:88.6% accuracy in del)endency analysis of the EDRannotated eorlms, and that it, outI)erformed oth(u'lmrely statistical l)arsing methods on the same cor-pus.
This result suggests that 1)rol)er t reatlnent ofhand-crafted gra, mnml'S ca,n contribute to l )arsi ltg ac-curacy  on a shallow level.1 Introduct ionThere have been many attempts to combine hand-crafted high-level gramnmrs, such as FB-UfAG,HPSG and LFG, and statistical disambiguationtechniques to ol)tain precise linguistic struc, tures(Schabes, 1992; Almey, 1996; Carroll el, al., 1998).One evident advantage of this apl)roaeh over lmrelystatistical parsing techniques is that grammars canprovide precise smnantie representations.
However,considering that remarkable parsing accuracy in ashallow level has been achieved by purely statisti-cal techniques (e.g.
Ratnal)arkhi (1997)), it may bethought more reasonable to use high-level gramnmrsjust tin' 1)osti)rocessing which nmps results of shallowsyntactical analyses onto dee 1) analyses.'l.
'his work was conducted while the first ~mthor was agraduate student at Univ.
of Tokyo.Figure 1: A tree M with a non-head aughter NH anda }mad aughter H.In this work we prol)ose that hand-crafl, ed high-level grammars (:all be useful in shallow-level analy-ses and statistical models.
In our fl'amework, gram-mars are used to obtain precise features for probabil-ity estimation, which are difficult to obtain without agrannnar, and we show that such features contributeto high parsing accuracy on a shallow level.In this lmper, the most preferable parse trees arechosen with a statistical model.
In our method, thelikelihood value L(M) of a (partial) tree M in Fig-ure 1 is detined as in (1):L(M)  dor L(NH) x L(H)  x P('n ~ h) (1)where NH is M's non-head daughter (whose lexicalhead is n), H is the head-daughter (whose lexicalhead is h), and /)(n -~ h) is the probability of nt)eing related to h. For a. single lexical iteni W, L(W)is defined as 1.0.In most models already proposed, the probabilityP (n  ~ h) is calculated with the conditional proba-bility (2):- ,  h) d?d P(T I ?
',,, %, (2)where T indicates that the dependency is true; (1)~and q~h are attributes of 'n and h, respectively.
AndAn,h, the distance between the two words, is widelyused, because this attribute is believed to stronglyaffect whether those two words are going to be re-late&In contrast, in the statistical model proposed inthis paper, P(n  -~ It) depends not only on the at-trilmtes of the tree M, but also on alternative trees411Input smltence : < .
.
.
n .
.
.
h l  .
.
.
h i  .
.
.
h t  .
.
.
>Mi MzM1n h ,1  " " " ?z  h i  ~,  h , lFigure 2: Pro'tiM trees whose non-head aughter's lexi-cal head is n.Ml rFigure 3: ~h'anstbrmation fl'om a tree to a dependency.l' and r' denote the bunsets,~s l and r belong to, respec-tively.in the parse forest generated by the grmmnar.
Moreprecisely, when P(n  --+ h) is calculated, we considerpartial trees whose non-head aughter's lexical headis n, as displayed in Figure 2.
Here alternative pos-sible hk (k = 1, - .
.
,  l) are taken into consideration,and ordered according to their distance to n. Wecall such set of hk modification candidates, and allmodification candidates are placed together in theconditional part of the probability as in (3).
Nowassume h = hi.P(i I %,, %2, ,  %, , ,  %,)(3)where "i" indicates the ith candidate mnong themodification candidates.
Equation (3) shows twoimportant properties of our model.
One point lies inthe new distance metric.
(3) is the probability that nchooses the ith candidate as the modifiee among themodification candidates which are ordered accordingto their distance to n. Thus, we no longer requirethe distance metric A~,h, instead we use the relativeposition among the modification candidates, whichworks as an attribute of the modification.
The otherpoint is the use of the attributes of the alternativeparse trees, that is, attributes of the modifier and allits modification candidates are considered simulta-neously.
We show that these techniques ophisticateour model, by providing linguistic examples in Sec-tion 3.2.In practice, however, treating all candidates i notfeasible because of data-sparseness.
We thereforeapply a strategy of restricting the modification can-didates to at most three.
The strategy and its justi-fication are discussed in Section 3.1.Applying the strategy to the equation (3), we ob-tain equations (4) and (5):P(It hi) de=f P(i I (i = 1, 2) (4)hi) der P(i I %, ,  %=, %,) (i = *, 2, t)(5)When there are only two candidates, equation (4)is used; otherwise, equation (5) is used.
Our statis-tical model is called the ~Hplet/Quadrut)let Modal,which was named after the nmnbcr of constituentsin the conditional parts of the equations.We report that our parsing framework achievedhigh accuracy (88.6%) in dependency analysis ofJapanese with a combination of an underspecifiedHPSG-based Japanese grammar, SLUNG (Mitsu-ishi et al, 1998) and the maximum entropy method(Berger et al, 1996).
Moreover, the resulting parsetrees generated by our hybrid parser are legitimatetrees in terms of given hand-crafted grammars, andwe are expecting that we can enjoy advantages pro-vided by high-level gramnmr formalisms, such asconstruction of semantic structures.In the above explanation, we used the notion oflexical heads for the estimation of probabilities oftrees for the sake of simplicity.
But, in the presentimplementation, we use bunscts,Ls instead of lexicalheads, and a relation on a tree is converted to abunsetsu-dependency as shown in Figure 3.
A bun-sctsu is a basic syntactic unit in Japanese.
It consistsof a content word and some flmctional morphemessuch as a particle.In Section 2, we describe some existing statisti-cal parsers, and the Japanese grannnar which weadopted.
Section 3 describes our statistical methodand its adwmtages in detail.
We report ext)erimentalresults in Section 4.2 BackgroundIn this section, we describe several models forJapanese dependency analysis and works on statisti-cal approaches with gramlnars.
Next, we introduceSLUNG, the HPSG-based Japanese grammar whichis used in our hybrid parser.2.1 P rev ious  Dependency Analysis Mode lsof JapaneseSeveral statistical models for Japanese dependencyanalysis which do not utilize a lland-crafted granl-mar have been proposed.
We evaluate the accuracyof bunsetsu-dependencies as they do, thus here weintroduce thenl for comparison.
All models intro-duced below are based on the likelihood value of thedependency between two bunsetsus.
But they differfrom each other in the attributes or outputs whichare considered when a likelihood value is calculated.There are some models which calculate the likeli-hood values of a dependency between bunsetsu i andj as in (6), such as a decision tree model (Haruno etal., 1998), a maximum entropy model (Uchimoto etal., 1999), a model based on distance and lexical in-formation (Pujio and Matsumoto, 1998).
Attributes(I)i and ~I,j consist of a part-of-speech (POS), a lexi-cal item, presence of a comma, and so on.
And Ai,j412is the number of intervening bnnscts'us between i andj.p(i -~ j) d,j ~'Crl ,I,i, %, a~,j) ((0However, these lnodels Nil to reftect contextualinformation because attributes of the surroundingbunsets,tts are not considered.Uchimoto et al (2000) proposed a model us-ing posterior context;.
The model utilizes not onlyattributes about bunscts~s i, j but also attributesabout all bunsets~> (including j) wlfich tbllow bun-setsu i.
That is, instead of learning two output val-ues "T(true)" or ':F(false)" for the del)endency be-tween two bunsets~zs, three output values are used*br leanfing: the b~m.setsu i is "bynd (dependent ona bunsctsu beyond j)" ,  "dpnd (del)endent on theb~tsets~t 3)" or "btwn (dependent on a b'unscts~t be-tween i and j)".
The 1)robability is calculated bymultiplying probabilities for all bunscts,~ls which tbl-low b~trtsctsu i as in (7).
'l'hey report that this kindof contextual information improves accuracy.
How-ever, the model has to assume, the independency ofall the random variables, which may cause some er-ro rs .P(i --, j) "?Z H ~'(by.d I ?
'~, %, &,k)i<k<jxP(dpnd I (1)i, il)5, Aid) x Hl'(btw,, \[ (I,i, q?k, A<k)(7)k>jThe difference between our model and these pre-vious models are discussed in Section 3.2.2 Stat is t ica l  Approaches  w i th  a grmnnmrThere have been nlally l)rOl)osals tbr statisticalt'rameworks particularly designed tbr 1)arsers withhand-crafted grmnmars (Schal)es, 1992; Briscoe andCarroll, 1993; Abney, 1996; Inui et al, 1!)97).
Themain issue in tiffs type of research is how to assignlikelihoods to a single linguistic structure generatedby a gramlnar.
Some of tlmm (Briscoe and Carroll,1!
)93; hmi et al, 1997) treat information on contexts,but the contextual intbrmation is de.rived only fl'oma structure to wlfich the parser is trying to assigna likelihood value.
Then, tim major difference be.-tween their method and ours is that we consider theattributes of alternative linguistic structures gener-ated by the grammar in order to deternfine the like-lihood for linguistic structures.2.3 SLUNG : J apanese  GrammarThe Japanese grammar which we adopted, SLUNG(Mitsuishi et al, 1998), is an HPSG-based under-specified grammar.
It consists of 8 rule schemata,48 lexical templates for POSs and 105 lexical entriesfor functional words.
As can be seen fl'om these fig-ures, the granmmr does not contain detailed lexk:alinformation that needs intensive labor for develop-ment.
However, it is precise in the sense that itaclfieves 83.7% dependency accuracy with a silnpleheuristics 2 for the El)I{ almotated corl)us , and itcan produce at least one parse tree for 98.4% sen-tences in the EDR annotated corpus.
We use thegrammar for generating parse tree forests, and our'l~'iplet/Quadruplet Model is used tbr picking Ul) asingle tree fl'om a forest.3 The  Hybr id  Pars ing  MethodThis section describes tim procedure of parsing withthe ~l"riplet/Quadrul)let Model.
Our hybrid 1)arsingmethod proceeds as tbllows:?
At; the beginning, dependency structures areobtained from trees generated by SLUNG.
Foreach bunsctsu, modification candidates are enu-merated, and if there are four or more candi-dates, tlmy are restricted to three.
The lmuristicused in this process is described in Section 3.1.?
Then, with the ~'il)let/Quadruplef; Mode.l andmaxinnnn entropy estimation, prol)abilities ofthe del)endencies are calculated.
Secti(m 3.2discusses the characteristics and advantages ofthe model.?
Finally, the most preferable trees for the wholesentence are selected.3.1 Rest r i c t ion  o f  Modi f i ca t ion  Cand idatesKanayama et al (1999) report that when mod-ification candidates are emnnerated according toSLUNG, 98.6% of the correct modifie.es are in one ofthe following three 1)ositions among the candidates:1;11(; nearest one from the modifier, the second nearestone, and the.
farthest one.As a consequence, we can siml)lil\[y I;11(; problemby considering only these three candidates and dis-carding tim other candidates, with only 1.4% poten-tial errors.
We therefore assume that the.
number ofmodification candidates ix always three or less.This idea is sinfilar to that of Sekine (2000)'sstudy, which restricts the candidates to five, i)ut inhis case, without a granmmr.3.2 The  Tr ip le t /Quadrup le t  Mode lThe 'Diplel,/Quadruplet Model calculates the like-lihood of the dependency between bunsetsu i andbunsctsu cn; P( i  --, cn) with the formulas (8) and(9), where c,~ denotes the nth candidate among b,m-sctsu i's candidates; (I,i denotes some attributes ofi; and ~I~?,~ denotes attributes of c,~ (including at-tributes between i and cn).P(i -~ c,d dJ P(n I ?,~, %.,, %~) (.n = 1,2)(8)P(~ -~ c,~) ,,~r p(,~ I ?
'i, %,, ,I,~, ,I%) (,n = 1, 2 , / ) (9 )2This heuristics is a Japanese version of a left-associationrule: see (Mitsuishi et M., 1998) for detail.413As (8) and (9) suggest, the model considers at-tributes of the modifier bunsetsu and attributes of allmodification candidates imultaneously in the condi-tional parts of the probabilities.
Moreover, what iscalculated is not tile probability of "whether the de-pendency is correct (T, see Formula(6))", but theprobability of "which of tile given candidates i cho-sen as tile nlodifiee (n =1, 2, or 1)".
These charac-teristics imply the fbllowing two advantages.Advantage  1 A new distance metric.
The correctmodifiee can be chosen by considering relativeposition among grannnatically licensed candi-dates, instead of the absolute distance betweenbunsets~as.Advantage  2 2)'eating alternative trees.
The can-didates are taken into consideration simultane-ously.
But because the nlodifica?ion candidatesare restricted to at most three, we considerablyavoid data-sparseness 1)rot)lems.Below we discuss these advantages in order.
Theseadvantages clarify the differences fl'om previousmodels described in Section 2.1, and are empMcallyconfirmed through the experiments in Section 4.3.2 .1  Advantage  1 : A new dis tance  metr i cAs discussed in Section 2.1, the distance metric Ai,jused in previous statistical methods was obtainedsimply by counting intervening words or b'unscts,t~l)etween i and j.
On the other hand, we use the rel-ative position among the modification candidates asthe distance metric.
Tile following examples illus-trate a difference between those two types of melric.The correct modifiee of kare-ga is hashir'u-no-wo inboth (10a) ~u~d (lOb).(10)a.
kare-ga hashiru-no-wo mira kotohe-SUBJ mm see fact(the fact that I saw him run)b. kare-ga yukkuri hashiru-no-wo mira kotohe-SUB.}
slowly run see fact(the fact that I saw him run slowly)In previous models, (10a) and (10b) would yield,P.
( kare-o,~--* t~ashir'u-no-wo)=P(T I kar,~-ga, h,~shiru-no-~vo,A1)\])b(kare-ga--+ hashi,.tt-?zo-wo)=\])(~l'll~a~ve-ga, hashi,'u-,zo-wo,A2)respectively, where A1 = 1 and A 2 = 2.
Then, thetwo probabilities above do not have the same valuein general.Our grammar does not allow the dependency"kare-.qa --~yukkurY tbr (10b).
The modificationcandidates of karc-ga are hashiru-no-wo and mita,hence (8) gives the probabilities between kare-ga andhashiru-no-wo as follows, in both examples.\]~ (kare-ga -~ hashiru- no- wo )= Pb(karc-,qa --~hashiru-no-wo)= P(llkare-ga, hashiru-no-wo, mita)Thus ,  P(kare-ga --+hashiru-no-wo) has the  same va luefor both examples.
Our interl)retation of this difl'er-enee is sumnlarized as follows.
The word yukk'uri san adverb modifying the verb h, ash&'u.
Our linguis-tic intuition tells us that the presence of such adverbshould not affect the strength tbr the dependencybetween kare-ga and hashiru-no-wo.
According tothis intuition, the existence of the adverb should beconsidered as a noise.
Our model allows us to ignoresuch a noise in learning from annotated corpus, whileprevious nlodels are atfected by such noisy elements.3.2 .2  Advantage  2 : Treat ing  a l te rnat ivet rees  or contextua l  in fo rmat ionConsider the following examples.
(11) a. Ta~v-no kawaii musumeNP Adj NPTaro-POgS 1)retty daughter(~\[h.ro's pretty daughter)b. Taro-no yuujin-no musumcNP NP NPTaro-POSS friend-POSS daughter(Taro's fl'iend's daughter)Contrary to tim previous examl)les, TaTv-no ill(11) ntodifies different nlodification candidates.
Inexample (11a), "~hr'o-no --+musume" is the correctdependency while "Taro-no -~musume" is not cor-rect in (11l)).
This difference is caused t)y the b'u'a-setsu between Taro-no and musume, kawaii (Adj)in ( l la)  and y,u~lfin-no (NP) in ( l lb) .
Actually, thegrannnar allows Taro-no to depend on either of thesetypes of words.
Thus, in our model,/',('late-no --, musume)l't, ( ~1aro-7~o --.
m~*sume)= P(21Then, P(varo-no-+musume) has different valuesfor the two examples, hi the annotated corpus,l'(21~laro-no, kawaii, musume) tends to have a highvalue since kawaii is an adjective.
However, sinceyuujin-no is an NP, P(2\[Taro-no, yuujin-no, musume)tends to have a low value.Now consider previous models.Pb(Taro-,~o--+ m**s~mz~) = P(TI Tin'o-no, musume, 2)Then, contrary to our model, P(Taro-no --~musumc)lms exactly the same wdue for both examples.
Theoutconle is determined by= P(T I  Taro-no, kawaii, 1)In text corpora, P(TITaro-no , yu~,jin-no, 1) tendsto be high, and consequently, P(T ITaro-no, musume,2) is very small.
These values will make the correctprediction for (111)) as yuujin-no will be favored overmusume.
However, for (11a), these models are likelyto incorrectly favor kawaii over musume.
This is414because 1'('.171 Tin'o-no, mus'ume, 2), being very small, islikely to be snlaller than P(T\] :late-no, t~,,waii, 1).4 Exper iments  and  D iscuss ion\].
'his section reports a series of parsing experimentswith our mode, l and gives some discussion.4.1 Env i ron lnentsWe used the EDR ,lal)anese Corl)us (El)R, 1996)for training and evaluation of 1)arsing accuracy.
TheEI)R Corpus ix a ,Japanese treebank which consistsof 208,1.57 sentences from newspapers and maga-zines.
We.
used 192,778 sentences for training, (1,744for pro-analysis (as reported in Section 3.1), and3,372 tbr testing 3.With tril)lets constitute(\] of a modifice and twomodi f i ca t ion  eandida.te.s ext ractc ( l  t i 'onl the learn-ing corl)uS l;hc Triplet Model is ('.onstructed.
\Viththe quadruplets constituted of a moditiee and threecandidates, the Quadruplet Model is constructed.
'?hese~ inodels arc estimated by the ChoiccMakerMaxinmm Entropy Estimator (Borthwick, 19!
)9).The features fin' the estimation are listed in Ta-/)le 1.
The values partially fo lk)w other researchese.g.
Uchimoto el; al.
(\]999), and JUMAN's outputsare used for POS classification.
Mainly the head ofthe b'unsc.tsu (the rightmost morl)helnc in a b'unscts'u,except for whose major POS is "peculiar", "auxiliaryverb", "particle", "suffix" or "copula") and type  ofthe b'ltnscts'u (the rightmost morphenm in a b'wnsel.s'ltexcept br whose major P()S is "l)eculiar") are usedas thc at.tributes.
\~;e show the meaning of somef( 'atures below.POS JUMAN's  minor  \]?
()S (for both  "head" and':type").part ic le,  adverb  Frequent words: 26 lmrticles and69 adverbs.head lex 2.
(14 lexical forms regardless of their POS.type  lex 70 suffixes or auxiliary verbs.inf lection 6 types of inttcction : "normal", "a(lver-l)ial", "adnominal", "tc-fornf', "ta-tbrm", and"others".The cohmm %aria(ion" in Tal)h; 1 denotes themnnbcr of possible values tbr the feature.
"Validfeatures" indicates the nmnber of features which al)-peared three times or more in the training corlms.4.2 Resu l tsWil;h our model and the features described above,the accuracy shown in Tal)lc 2 is achieved.
We oval-uate the following two tyl)eS of accuracy:35,263 SOld,enccs were rOllloved 1)eCmlSe the order of thewords in the annotal;ion ditl'ered front that in the originalSOl l tO l lCeS .\[ I i \,,re,it, ........ <,~I l l  Pea l ,  u re  type  Va l ' i a t ion  ' l ' r ip .
\] Q ,md,I I I oad  I 'OS  ?~f nmdi f ie r  24  ,12 6,t2 Type  P ( )S  o f  i l l od i l ' i e r  34  0(l !
)D3 l ' a r t i ch ,  o f  l / l ed( t ie r  27  , t7  7:{4 Adverb  ,fl' i n (~d i f io r  70  131 1035 Ty lm l(}X ( i f  i i l t ld i f io l"  71 110  225d In f lec t i (n l  ()|" i .
lod i f i (n"  {; 12 187 \Vh( !
th l !
r  lx l ( id i t i ( !r  has  a COlll l l l lt *2 4 ~-8 l Iead  I 'OS  , f f  l tH~dil ' ioo 24  7(I 1589 Type  I 'OS  o f  lnqMi | ' i ee  34  96  23110 lh ,ad  \]?~x ~,1 in i )d i f ieo  2D5 l lG , I  259711 lh t r t ; i c lo  \[d" Inod i l ie (~ 27  92  20.112 ' l ' ypo  Iox  o |  i l l od i | i /~o  71 210 45413 \]nlh~l:t .
i tHI  o f  In ( .d i f le ( ,  6 2,1 5:11.l ~ .Vhothor  Inodi l ' ioc~ has  it GI)Ill l IIII 2 8 1815 \Vhethor  n lod i f ioe  has  %rod' 2 8 18IG ~Vho l ,h~r  inod l f ioe  l i l ts " /o"  2 6 1717 :\]/ ( if  (~()1\[111~ii~ b(!tlg(~(!ll l;l';?l IHtlt.utlt.uos ,1 16 3618 # i) f  "i l ia" l)tl~,~v(!
(*lI t~V() bltYl~t:~gllH 3 12 2719 2 X 8 816 1187 272720  2 x 7 x 14 13G 38(} 871121 3 X 10 7905 6,t(15 13,10:{22  2 x 9 1156 1213 311)823  3 X 11 729 618 16372.1 2 X 11 i118 1025 2 .194'25 2 X 12 241, t  1483 351 '126  2 x 3 x 7 x 8 132192 1331 3O5827  1 X 2 X 6 X 8 X 13 7( )5024 ( i( i05 1,t7{10I t 'r''':''~ I - I  '224:':~ I """~" JTable 1: Used f'eaturcs : l,k,,atures from 8 to 27 are re-lated to the nm(litiee, thus they are considered for eachcandidate, li'eatures from 19 to 27 are combination fea-\ [ ; I l I 'CS .I I I -COVCI 'a~OS(~.l l tO l  ICCS\]htnset.su accuracy 88.ooX,(23078/2(i062)Sentence accu,'acy /16.
{)0% (1560/3326)All lhm.set.su accuracy 88.33% (2335()/26436)sentences ,qentcncc~ accuracy 46.35% (1563/3372)Tal)h; 2: l{csulls of parsing with the Tril)let/Quadrul)letModcl.Bunsetsu  accuracy  The percentage of bu.n,~cts'uswhose rood(rice is correctly identified.
The dc-nonfinator includes all b'unsets'us except for thelast bun,~cts'u of a sentence.Sentence accuracy  The percentage of sentenceswhose detmndencies art'.
perfectly correct.
"h>coverage sentences" is the accuracy for thesentences flw which SLUNG could generate parsetrees.
We give the accuracy for "All sentences" too,by 1)art(ally 1)arsing sentences which SLUNG fail toparse.
The coverage of SLUNG is al)out 99%, thushigh accuracy is achieved even for "All sentences".Moreover, we conducted a series of experimentsin order to evaluate the COld;ribution of each charac-teristic in our parsing model.
The parsing schemesused are the four in Figure 3.
Major differencesamong them are (I) whether a gralnlnar is used,(II) whether modification candidates are restrictedto three, and (III) whether a previous pair modelwith Formula (6) or the 'lS'iplet/Quadrulflet Modelwith Formula (8),(9) was used.W/O Grammar  Mode l  This model does not usea grammar.
Likelihood values for dcpenden-4I 5W/O GrammarW/O RestrictionPair~IMplet/QuadrupletI G R PP\]~Un8gts~L accuracy86.70%(22594/26062)+ - P 87.37%(22770/26062)+ + P 87.67%(22849/26062)+ + T 88.55%(23078/26062)Table 3: Bunsetsu accuracies for four models.
Cohmm"G" indicates whether the grmmnar is used, "R" indi-cates whether the modification candidates are restrictedto three, and "F" denotes the formula; "P" is the pairtbrmula (6), and "T" is the %'iplet/Quadruplet formula(s), (9).cies are calculated for all bunsctsiLs that followa modifier bunsctsu.
Formula (6) is used, andas a distance metric Ai,j, the mnnl)er of bun-scts~ls between the modifier and tile modifiee 4are combined with all features.
In general lines,this model corresponds to models such as (Fu-lie and Matsumoto, 1998; Haruno et al, 1998;Uchimoto et al, 1999).W/O Rest r i c t ion  Mode l  Modification candi-dates are restricted by SLUNG.
Tim remainingis the same as the W/O Grannnar Model.Pa i r  Mode l  Modification candidates are restrictedto three, in the way described in Section 3.1.The remaining is the same as W/O GrannnarModel.T r ip le t /Quadrup le t  Mode l  Tiffs is the modelproposed in the paper.
Modification candidatesare restricted to tln'ee, and Fornmla (8) or (9)are used.From the result shown in Table 3, we can sayour method contributes to the improvement of ourparser, because of the following reasons:?
The %'iplet/Quadruplet Model outperforms thePair Model by 0.9%.
Both of them restrictsmodification candidates to three, l)nt tim accu-racy got higher when all candidates are consid-ered simultaneously.
It is because of the twoadwmtages described in Section a.2.?
TILe Pair Model outperforms the W/O Restric-tion Model by 0.3%.
Thus the restriction ofmodification candidates does slot reduce tile ac-curacy.?
TILe W/O Restriction Model outperforms tileW/O Grammar Model by 0.7%.
This meansthat the use of a grammar as a preprocessorworks well to pick up possible modifice.We found that many structures imilar to theones described iLL Section 3.2 appeared in the EDR4Three vahms: "1", "from 2 to 5", "6 or more" are distin-guished.In-coverage \]3unsct.su accuracy 87.08% (8299/9530)sentences Sentence accuracy 44.70% (493/\]103)Table 4: Accuracy tbr Kyoto University Corpuscorpus.
Our Tl'iplet/Quadruplet model could treatthese structures precisely as we intended.
Tlfis is themain factor that contributed to the improvement ofthe overall parsing accuracy.Based on tim above experiments, we can say thatour approach to use the grammar as a preprocessorbefore the calculating of the probability is appropri-ate for the improvement of parsing accuracy.4.3 Compar i son  to  o ther  mode ls4.3.1 Mode ls  us ing the  EDR corpusThere are several works which use the EDR corpusfor evaluation.
The decision tree model (Haruno etal., 1998) achieves around 85%, the integrated modelof lexical/syntactic information (Slfirai et al, 1998)achieves around 86%, and the lexicalized statisticalmodel (Ft0io and Matsumoto, 1999) achieves 86.8%in bunsets'u accuracy.
Our model outperforms all ofthem by 2 or 3%.4.3.2 Mode ls  us ing the  Kyoto  corpusSlfirai et al (1998) used the Kyoto University textcorpus (Kurohashi and Nagao, 1997) for evaluationand achieved around 86%.
Uclfimoto et al (2000)also used the Kyoto corlms , and their accuracy was87.9%.
For comparison, we applied our method tothe same 1,246 sentences that Uclfimoto et al (2000)used.
The result is shown in Table 4.Our result is worse than theirs.
The reason isthought o l)e as follows:?
g~re use tim EDR corpus for training.
Althoughwe used around 24 times the amount of train-ing data that Uchimoto et al used, our trainingdata lead to ca'ors in tile analysis of the KyotoCorpus, because of differences in tile mmotationschenms adopted.?
Uchimoto et al used the correct morphologicalanalyses, but we used JUMAN.
Solnetimes thismay cause errors.?
The grammar SLUNG was designed for tileEDR corpus, and some types of structures inthe Kyoto Corpus are not allowed.Clearly, our parser should be improved to overcomethese problems and compared with other works di-rectly.4.4 D iscuss ion  and I~lture WorkTILe following are some observations about the speedof our parser.
Existing statistical parsers are quiteetficient compared to grammar-based systems.
Par-ticularly, our system used an HPSG-1)ased grmmnar,416whose speed is said to be slow.
However, recent ad-vances in HPSG 1)arsing (~Ibrisawa et al, 2000) en-abled us to obtain a unique parse tree with our sys-gem in 0.5 sec.
in average tbr sentences in the EDRcorpus.Future work shall extend SLUNG so that senmntierepresentatkms are produced.
Carroll el; al.
(1.998)discussed i;he 1)recisiol~ of argument si;ruetures.
V~Te1)elieve that the focus of ore' study will shift; from ashallow level to such a deeper level for ()Ill' tinal aim,realization of intelligent natural anguage processingsystems.5 Conclusion\?e 1)resenl;ed a hyl)rid 1)arsing scheme l;hat uses ahand-crafted grammar and a statist.teal technique.As other hybrid pa.rsing ntethods, l;he st.al;isi;icaltechnique is used for 1licking u 1) the most l)re, ferable,lmrs(; ire(; fl'om l;he parse fol"(;sI; gent'.rai;e,d I)y t;h(~grammar.
The difference fl'om other works is thatthe precise contexi;ual information needed to esti-mate |;he likelihood of a parse, 1;ree is obtained fl'ontadternative 1)arse trees generated 1)5' the grammar,and that such contextual information from alterna-tive I;rees enables Its to eonsl;ruel; our new statisti-cal model called the ' l?iplet/Quadruplet model.
Wehave shown that these poinl;s contributed to sul)sl;an-tia l illlprovenlenl; of parsing acellra(:y ill ,lal)ane~se dc-1)en(lency analysis, through a serie, s of ext)(~rimentsusing an I iPSG-based .lalmnese grammar SLUNC,and the, maxinmm entropy method.ReferencesSt;even Abney.
1996.
Sl;ochasti(: aH;ribut(',-vahmgrannnars.
The Computation and \]\]anguage E-Print Archive, October.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent.
J. Della Pietra.
\]996.
A itiaxilnuln entropyapproach to natural anguag('~ processing.
Compu-tatio'n, al Li'n.gui.stics, 22(1.
)::/9 71.Andrew Borthwiek.
199.().
Choieemaker maximmnentropy estimator.
ChoieeMaker 'lbch., Inc. Emailborthwic~cs .nyu.
edu for information.~\[l'~d Briseoe and John Carroll.
1993.
Generalized1)robabilistic LR parsing of natural anguage (col1)Ol'~t) with unifieation-I)ased gramnmrs.
Compu-tational Linguistics, 19(1):25-50.Jolm Carroll, Guido Minnen, and ~lL'd Briscoe.
1998,Can subeategorisation probabilities help a statis-tical parser?
In Proc.
of th, e 5th, ACL/SIGDATWorkshop on Very Lawe Corpora, pages 118 126.EDR.
1996.
EDR (Japan Electronic Dictionary Re-search Institute, Ltd.) dictionary version 1.5 tech-nical guide,.
Second edition is awdlable viah t tp  ://www.
i i j ne t ,  or.
j p/edr/E_TG .html.Masakazu Fujio and Yuji Matsumot;o.
1998.,htl)anes<', <lel)endeney structure analysis 1)ased onlexicalized statistics.
In PTvc.
of the 3rd Cm@r-ence on Empirical Methods in Natural LanguageProcc.ssin 9, pages 88 96.Masakazu FI0io and Ymtji Ma?sumoto.
1999.
Sta-tistieal syntactic analysis based on co-occurrenceprobability of words.
In P'roc.
of 5th workshopof Nat'u~nl Language Processing, pages 71 78.
(inJal)altese ) .Masahiko Haruno, Satoshi Shirai, and goshiflmfiOoyama.
1998.
Using decision trees to constructa.
practical parser.
In Prec.
COLING ACL '98,pages 505 -511.Kentaro hmi, Virach Sornlertbunwmich, HozumiTanaka, and Takenobu 9bkmmga.
1997.
A newl>robal)ilistic LR language lnodel t'(31' statisticalparsing.
'l'echnical I{eport TR974)005, Dept.
ofComl)uter Science, Tokyo Institute of 'lbehnology.l liroshi Kanayanm, Kentaro 'l.brisawa, Yutaka Mit-suishi, and Jun'i(:hi Tsujii.
1999.
Statistical de-1)e, ndency analysis with an HPSG-1)ased Jal)anesegrannnar.
In P'roc.
5th NLPRb', pages 138-143.Sadao I(urohashi att(l Makoto Nagao.
1.997.
KyotoUniversity text corpus in'ojecK In Prec.
of 3rdAnn'ual M(~cti~ N of Nat,u, raI Language i)roccssi'ng,l)ages 115 118.
(in Japanese).Yutaka Mii;suishi, Kentaro Torisawa, and Jun'ichiTsujii.
1.(/98.
HPSG-stsde undersl)e(:ified .Japanesegrammar with wide coverage.
In P'mc.
COLING-ACL '98, 1)ages 876 880, Augusl;.Adwait llatnalmrkhi.
1997.
A linear obse, rved tinl(;statistical lm.rser based Oll maximum entropymodels.
In P'mc.
th.c Empirical Mt~thods in Nat'u-'ral \])a,'n,guag(: \])'roce, ssi'n,9 Co~@rence.Yves Sehabes.
\]992.
Stoclmsti(: lexicalize, d tree-adjohfing granmwms.
In P'mc.
1dth COLINO,pages d26 432.S~toshi Sekim,.
2000.
Japanese dependency analysisusing a deternfinistic, tinite state, transdue(;r. InPrec.
COLING 2000.
(this proceedings).Kiyoaki Shirai, Kentaro huff, Takenolm Tokunaga,and Hozumi Tanaka.
1998.
A framework of inl;e-grating ,syntactic and lexical statistics in si;atisti-cal 1)arsing..lo,urnal ofNat'ural Langua9c l)~vccss -int.\], 5(3).
(in Japanese).Kentaro 'Ibrisawa, Kenji Nishida, Yusuke Miyao,and Jun'ichi Tsujii.
2000.
An HPSG parser withCFG filtering.
Jounal of Nat'mal Language E'n, gi-nccrin.q.
(to al)pear ).Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi 1sa-hara.
19!19.
Japanese dependency structure anal-ysis based on maximum entropy models.
In P'roc.13th EACL, pages 196 -203.Kiyotaka Uchimoto, Masaki Mural;a, Satoshi Sekine,and Ititoshi isahara.
2000.
\])el)endeney model us-ing posterior context.
In Prec.
of Sixth, intcrna-lionel Workshop on Parsing 7'cch, nolo9ics.41 7
