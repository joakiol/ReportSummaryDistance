ANNOTATION OF ATIS DATAKate Hunicke-Smith, Project LeaderJared Bernstein, Principal InvestigatorSRI InternationalMenlo Park, California 94025PROJECT GOALSThe performance ofspoken language systems on utterancesfrom the ATIS domain is evaluated by comparing system-produced responses with hand-crafted (and -verified) stan-dard responses to the same utterances.
The objective ofSRI's annotation project is to provide SLS system develop-ers with the correct responses to human utterances pro-duced during experimental sessions with ATIS domaininteractive systems.
These correct responses are then usedin system training and evaluation.RECENT RESULTSIn a previous project, SRI devised aset of procedures fortranscribing and annotating utterances collected uringinteractions between human subjects and a simulated voice-input computer system that provided information on airtravel, i.e.
utterances in the ATIS domain.
During spring of1991, it was decided to expand the collection of thesehuman-machine i teractions so that most DARPA speechand natural language sites would be collecting this type ofdata.
However, SRI was to remain the only site providingthe 'standard' answers.At the start of the project, abasic set of principles for inter-preting the meaning of ATIS utterances was agreed upon bythe DARPA community and documented in a network-accessible file known as the Principles of Interpretation.Initial annotation procedures used at that time at SRI weredocumented in a net note dated July 12, 1991.During the earlier project, SPRI had installed software thatproduced answer files in the format required by NIST.
Theessential component of the software used by the annotatorswas and is NLParse, a menu-driven program developed byTexas Instruments hat converts English-like sentences intodatabase queries expressed inSQL.As standard responses were generated for use in systemtraining, some aspects of the Principles oflnterpretationwere changed.
This process has continued throughout theproject.
In July, SRI worked with NIST to establish acom-mittee of representatives from each data collection site tomodify the Principles of Interpretation document asneeded.
The SRI annotators have worked closely with thiscommittee, contributing knowledge of the data corpusgained in the annotation process.Software used in the production of the standard responsefiles was modified and expanded upon.
SRI modifiedNLParse itself to accommodate changes in the softwareenvironment and new testing roles which limited the size oflegal answers.
Also, a few high-level programs were writ-ten to drive and monitor the results of the various low-levelroutines that had been used in SRI's previous project.
Thisconsolidation eliminated the need for annotators tomonitorthe process at each stage, thus eliminating opportunities forhuman error.A dry-run system evaluation was held in October, 1991, forwhich SRI produced the standard responses.
The dry runoffered an opportunity o measure the real accuracy of asample of 'standard' responses in the annotated data.
In thedry run test, about 6% of the annotations were incompleteor inappropriate in some way; some due to human error andsome to software rror.
In an effort to improve data quality,SRI revised its human checking procedures and added newchecking programs to the software involved in the produc-tion of the answer files.
It had originally been hoped thathuman double checking could be decreased after an imtialperiod of annotation, but based on the adjudication of thedry run data, 100% double checking has continued.Since June, 1991, SRI has produced classification andresponse files for 8000 utterances of training data, andnearly 1000 utterances of test data.
Currently, we annotateabout 190 utterances per annotator-week.For the first official system evaluations in February, 1992,SRI again worked with NIST to produce the standardresponse files.PLANS FOR THE COMING YEARIn the next year, SRI will work with NIST and the DARPAcommunity odevelop and implement more efficient evalu-ation procedures for SLS systems.484
