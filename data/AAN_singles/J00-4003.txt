An Empirically Based System forProcessing Definite DescriptionsRenata Vieira*Universidade do Vale do Rio dos SinosMass imo Poesio tUniversity of EdinburghWe present an implemented system for processing definite descriptions in arbitrary domains.The design of the system is based on the results of a corpus analysis previously reported, whichhighlighted the prevalence ofdiscourse-new descriptions in newspaper corpora.
The annotatedcorpus was used to extensively evaluate the proposed techniques for matching definite descriptionswith their antecedents, discourse segmentation, recognizing discourse-new descriptions, andsuggesting anchors for bridging descriptions.1.
IntroductionMost models of definite description processing proposed in the literature tend toemphasise the anaphoric role of these elements.
1 (Heim \[1982\] is perhaps the best for-malization of this type of theory).
This approach is challenged by the results of exper-iments we reported previously (Poesio and Vieira 1998), in which subjects were askedto classify the uses of definite descriptions in Wall Street Journal articles accordingto schemes derived from proposals by Hawkins (1978) and Prince (1981).
The resultsof these experiments indicated that definite descriptions are not primarily anaphoric;about half of the time they are used to introduce a new entity in the discourse.
Inthis paper, we present an implemented system for processing definite descriptionsbased on the results of that earlier study.
In our system, techniques for recognizingdiscourse-new descriptions play a role as important as techniques for identifying theantecedent of anaphoric ones.A central characteristic of the work described here is that we intended from thestart to develop a system whose performance could be evaluated using the texts an-notated in the experiments mentioned above.
Assessing the performance of an NLPsystem on a large number of examples is increasingly seen as a much more thoroughevaluation of its performance than trying to come up with counterexamples; it is con-sidered essential for language ngineering applications.
These advantages are thoughtby many to offset some of the obvious disadvantages of this way of developing NLPtheories-- in particular, the fact that, given the current state of language processingtechnology, many hypotheses of interest cannot be tested yet (see below).
As a result,quantitative valuation is now commonplace in areas of language engineering suchas parsing, and quantitative valuation techniques are being proposed for semantic* Universidade do Vale do Rio dos Sinos - UNISINOS, Av.
Unisinos 950 - Cx.
Postal 275, 93022-000 SaoLeopoldo RS Brazil.
E-mail: renata@exatas.unisinos.brt University of Edinburgh, ICCS and Informatics, 2 Buccleuch Place, EH8 9LW Edinburgh UK.
E-maihMassimo.Poesio@ed.ac.uk1 We use the term definite description (Russell 1905) to indicate definite noun phrases with the definitearticle the, such as the car.
We are not concerned with other types of definite noun phrases uch aspronouns, demonstratives, or possessive descriptions.
Anaphoric expressions are those linguisticexpressions u ed to signal, evoke, or refer to previously mentioned entities.
(~) 2001 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 4interpretation as well, for example, at the Sixth and Seventh Message Understand-ing Conferences (MUC-6 and MUC-7) (Sundheim 1995; Chinchor 1997), which alsoincluded evaluations of systems on the so-called coreference task, a subtask of whichis the resolution of definite descriptions.
The system we present was developed to beevaluated in a quantitative fashion, as well, but because of the problems concerningagreement between annotators observed in our previous tudy, we evaluated the sys-tem both by measuring precision/recall against a "gold standard," as done in MUC,and by measuring agreement between the annotations produced by the system andthose proposed by the annotators.The decision to develop a system that could be quantitatively evaluated on a largenumber of examples resulted in an important constraint: we could not make use ofinference mechanisms such as those assumed by traditional computational theoriesof definite description resolution (e.g., Sidner 1979; Carter 1987; Alshawi 1990; Poesio1993).
Too many facts and axioms would have to be encoded by hand for theoriesof this type to be tested even on a medium-sized corpus.
Our system, therefore, isbased on a shallow-processing approach more radical even than that attempted bythe first advocate of this approach, Carter (1987), or by the systems that participatedin the MUC evaluations (Appelt et al 1995; Gaizaukas et al 1995; Humphreys et al1988), since we made no attempt o fine-tune the system to maximize performanceon a particular domain.
The system relies only on structural information, on the in-formation provided by preexisting lexical sources uch as WordNet (Fellbaum 1998),on minimal amounts of general hand-coded information, or on information that couldbe acquired automatically from a corpus.
As a result, the system does not really havethe resources to correctly resolve those definite descriptions whose interpretation doesrequire complex reasoning (we grouped these in what we call the "bridging" class).We nevertheless developed heuristic techniques for processing these types of definitesas well, the idea being that these heuristics may provide a baseline against which thegains in performance due to the use of commonsense knowledge can be assessed moreclearly.
2The paper is organized as follows: We first summarize the results of our previouscorpus study (Poesio and Vieira 1998) (Section 2) and then discuss the model of deft-nite description processing that we adopted as a result of that work and the generalarchitecture of the system (Section 3).
In Section 4 we discuss the heuristics that wedeveloped for resolving anaphoric definite descriptions, recognizing discourse-newdescriptions, and processing bridging descriptions, and, in Section 5, how the per-formance of these heuristics was evaluated using the annotated corpus.
Finally, wepresent he final configuration of the two versions of the system that we developed(Section 6), review other systems that perform similar tasks (Section 7), and presentour conclusions and indicate future work (Section 8).2.
Preliminary Empirical WorkAs mentioned above, the architecture of our system is motivated by the results con-cerning definite description use in our corpus, discussed in Poesio and Vieira (1998).In this section we briefly review the results presented in that paper.2 In fact, it is precisely because we are interested in identifying the types of commonsense r asoningactually used in language processing that we focused on definite descriptions rather than on othertypes of anaphoric expressions ( uch as pronouns and ellipsis) that can be processed much moreeffectively on the basis of syntactic information alone (Lappin and Leass 1994; Hardt 1997).540Vieira and Poesio Processing Definite Descriptions2.1 The CorpusWe used a subset of the Penn Treebank I corpus (Marcus, Santorini, and Marcinkiewicz1993) from the ACL/DCI  CD-ROM, containing newspaper articles from the Wall StreetJournal.
We divided the corpus into two parts: one, containing about 1,000 definitedescriptions, was used as a source during the development of the system; we will referto these texts as Corpus 1.
3 The other part, containing about 400 definite descriptions,was kept aside during development and used for testing; we will refer to this subsetas Corpus 2.
42.2 Classifications of Anaphoric ExpressionsThe best-known studies of definite description use (Hawkins 1978; Prince 1992; Frau-rud 1990; L6bner 1987; Clark 1977; Sidner 1979; Strand 1996) classify definite descrip-tions on the basis of their relation with their antecedent.
A fundamental distinctionmade in these studies is between descriptions that denote the same discourse ntityas their antecedent (which we will call anaphoric or, following Fraurud, subsequentmention), descriptions that denote an object hat is in some way "associated" with theantecedent-- for example, it is part of it, as in a car.
.
,  the wheel (these definite expres-sions are called "associative descriptions" by Hawkins and "inferrables" by Prince),and descriptions that introduce a new entity into the discourse.In the case of semantic identity between definite description and antecedent, afurther distinction can be made depending on the semantic relation between the pred-icate used in the description and that used for the antecedent.
The predicate used inan anaphoric definite description may be a synonym of the predicate used for the an-tecedent (a house ... the home), a general izat ion/hypernym (an oak.., the tree), and even,sometimes, a special izat ion/hyponym (a tree.., the oak).
In fact, the NP introducing theantecedent may not have a head noun at all, e.g., when a proper name is used, as inBill Clinton... the president.
We will use the term direct anaphora when both descriptionand antecedent have the same head noun, as in a house.., the house.
Direct anaphors arethe easiest definite descriptions for a shallow system to resolve; in all other cases, aswell as when the antecedent and the definite description are related in a more indirectway, lexical knowledge, or more generally encyclopedic knowledge, is needed.All of the classifications mentioned above also acknowledge the fact that not alldefinite descriptions depend on the previous discourse for their interpretation.
Somerefer to an entity in the physical environment, others to objects which are assumed tobe known on the basis of common knowledge (Prince's "discourse-new/hearer-old"expressions, uch as the pope), and still others are licensed by virtue of the semanticsof their head noun and complement (as in the fact that Milan won the Italian footballchampionship).2.3 A Study of Definite Description UseIn the experiments discussed in Poesio and Vieira (1998) we asked our subjects toclassify all definite description uses in our two corpora.
These experiments had thedual objective of verifying how easy it was for human subjects to agree on the distinc-tions between definite descriptions just discussed, and producing data that we coulduse to evaluate the performance of a system, The classification schemes we used weresimpler than those proposed in the literature just mentioned and were motivated, on3 The texts in question are w0203, w0207, w0209, w0301, w0305, w0725, w0760, w0761, w0765, w0766,w0767, w0800, w0803, w0804, w0808, w0820, w1108, w1122, w1124, and w1137.4 The articles in this second subset are w0766, wsj_0003, wsj_0013, wsj_0015, wsj_0018, wsj_0020, wsj_0021,wsj_0022, wsj_0024, wsj_0026, wsj_0029, wsj_0034, wsj_0037, and wsj_0039.541Computational Linguistics Volume 26, Number 4the one hand, by the desire to make the annotation uncomplicated for the subjectsemployed in the empirical analysis and, on the other hand, by our intention to use theannotation to get an estimate of how well a system using only limited lexical and en-cyclopedic knowledge could do.
s We ran two experiments, using two slightly differentclassification schemes.
In the first experiment we used the following three classes: 6?
direct anaphora: subsequent-mention definite descriptions that refer toan antecedent with the same head noun as the description;?
br idging descriptions: definite descriptions that either (i) have anantecedent denoting the same discourse ntity, but using a different headnoun (as in house  .
.
.
bu i ld ing) ,  or (ii) are related by a relation other thanidentity to an entity already introduced in the discourse; 7?
discourse-new: first-mention definite descriptions that denote objects notrelated by shared associative knowledge to entities already introduced inthe discourse.In the second experiment we treated all anaphoric definite descriptions as part ofone class (direct anaphora + bridging (i)), and all inferrables as part of a different class(bridging (ii)), without significant changes in the agreement results.Agreement among annotators was measured using the K statistic (Siegel andCastellan 1988; Carletta 1996).
K measures agreement among k annotators over andabove chance agreement (Siegel and Castellan 1988).
The K coefficient of agreement isdefined as:K - -  P (a )  - P (E )1 -where P(A) is the proportion of times the annotators agree, and P(E) the proport ion oftimes that we would expect hem to agree by chance.
The interpretation of K figures isan open question, but in the field of content analysis, where reliability has long beenan issue (Krippendorff 1980), K > 0.8 is generally taken to indicate good reliability,whereas 0.68 < K < 0.8 allows tentative conclusions to be drawn.
Carletta et al (1997)observe, however, that in other areas, such as medical research, much lower levels ofK are considered acceptable (Landis and Koch 1977).An interesting overall result of our study was that the most reliable distinction thatour annotators could make was that between first-mention and subsequent-mention(K = 0.76); the measure of agreement for the three-way distinction just discussed wasK = 0.73.
The second interesting result concerned the distribution of definite descrip-tions in the three classes above: we found that about half of the definite descriptionswere discourse-new.
The distribution of the definite descriptions in classes in our firstexperiment according to annotators A and B are shown in Tables 1 and 2, respec-tively.
(Class IV includes cases of idiomatic expressions or doubts expressed by theannotators).The third main result was that we found very little agreement between our sub-jects on identifying briding descriptions: in our second experiment, he agreement on5 Previous attempts o annotate anaphoric relations had resulted in very low agreement levels; forexample, in the coreference annotation experiments for MUC-6 (Sundheim 1995), relations other thanidentity were dropped ue to difficulties in annotating them.6 In this experiment, our subjects could also classify adefinite description as "idiomatic" or"doubt'--see tables below.7 In Poesio and Vieira (1998), Hawkins's term "associative" was used for this class; but in fact, thedefinition we used for the class is closest to the sense of "bridging" used by Clark (1977).542Computational Linguistics Volume 26, Number 4ined, Fraurud (1990, 421) claims that:a model where the processing of first-mention definites always in-volves a failing search for an already established iscourse referent asa first step seems less attractive.
A reverse ordering of the proceduresis, quite obviously, no solution to this problem, but a simultaneousprocessing as proposed by Bosch and Geurts (1989) might be.Fraurud proposes, contra Heim (1982), that processing a definite NP may in-volve establishing a new discourse entity.
8 This new discourse entity may then belinked to one or more anchors in the text or to a background referent.
9 Fraurud dis-cusses the example of the description the king, interpreted relationally, encountered ina text in which no king has been previously mentioned.
Lexicoencyclopedic knowl-edge would provide the information that a king is related to a period and a country;these would constitute the anchors.
The selection of the anchors would identify thepertinent period and country, and this would make possible the identification of areferent: say, for the anchors 1989 and Sweden, the referent identified would be CarlGustav XVI.
1?The most interesting aspect of Fraurud's proposal is the hypothesis that first-mention definites are not necessarily recognized simply because no suitable antecedenthas been found; independent strategies for recognizing them may be involved.
Thishypothesis is consistent with L6bner's proposal (L6bner 1987) that the fundamentalproperty of a definite description is that it denotes a function (in a logical sense); thisfunction can be part of the meaning assigned to the definite description by the gram-mar (as in the beginning of X), or can be specified by context (as in the case of anaphoricdefinites).
Fraurud's and L6bner's ideas can be translated into a requirement that asystem have separate methods or rules for recognizing discourse-new descriptions(and in particular, L6bner's "semantically functional" definites) in addition to rulesfor resolving anaphoric definite descriptions; these rules may run in parallel with therules for resolving anaphoric definites, rather than after them.Rather than deciding a priori on the question of whether the heuristic rules (inour case) for identifying discourse-new descriptions hould be run in parallel withresolution or after it, we treated this as an empirical question.
We made the archi-tecture of the system fairly modular, so that we could both try different heuristicsand try applying them in a different order, using the corpus for evaluation.
We dis-cuss all the heuristics that we tried in Section 4, and our evaluation of them in Sec-tion 5.3.2 Architecture of Our SystemThe overall architecture of our system is shown in Figure 1.
The system attempts toclassify each definite description as either direct anaphora, discourse-new, or bridgingdescription.
In addition to this classification, the system tries to identify the antecedentsof anaphoric descriptions and the anchors (Fraurud 1990) of bridging descriptions.
The8 Discourse entities are representations in the discourse model of entities explicitly mentioned (Webber1979; Heim 1982).9 Background referents are entities that have not been mentioned in the discourse--those entities thatGrosz (1977) would call "elements ofthe implicit focus.
"10 Fraurud oes not explain what it is that justifies the use of definite descriptions, if not familiarity.
InPoesio and Vieira (1998) we suggest that L/3bner's proposal (LSbner 1987) seems to account for themost data.544Vieira and Poesio Processing Definite DescriptionsTreebank 1~xt ract ion~I\[NP, a, house\]\[NP, the, house\]\[S,...\[...\]\]-1 List of NPs and Sentencespotential_antecedent( l,np(...),...).potential_antecedent(2,np(...),...).definite_description(3,np(...),...).definite_description(5,np(...),...)./Text p rocess ing~\L i~ngu ist ic ~CZ?Discourse r presentationdd_class(3,anaphora).dd_class(5,bridging).dd_class(6,discourse_new).corer(3,1 ).bridging(5,2).coref_chain(\[ 13\]).total(anaphora,22).total(bridging,9).total(discourse_new,28).System's resultsFigure 1System architecture.system processes parsed newswire texts from the Penn Treebank I, constructing a fairlysimple discourse model that consists of a list of discourse ntities that may serve aspotential antecedents (which we call simply potential antecedents), according to thechosen segmentation algorithm (see below).
The system uses the discourse model,syntactic information, and a small amount of lexical information to classify definitedescriptions as discourse-new or to link them to anchors in the text; WordNet is alsoconsulted by the version of the system that attempts to resolve bridging descriptions.The system is implemented in Sicstus Prolog.545Computational Linguistics Volume 26, Number 4Input.
The texts in the Penn Treebank corpus consist of parsed sentences representedas Lisp lists.
During a preprocessing phase, a representation i Prolog list formatis produced for each sentence, and the noun phrases it contains are extracted.
Theoutput of this preprocessing phase is passed to the system proper.
For example, thesentence in (1) is represented in the Treebank as (2) and the input to the system after thepreprocessing phase is (3).
11 Note that all nested NPs are extracted, and that embeddedNPs such as the Organization of Petroleum Exporting Countries are processed before theNPs that embed them (in this case, the squabbling within the Organization of PetroleumExporting Countries).
(1)(2)(3)Mideast politics have calmed down and the squabbling within theOrganization of Petroleum Exporting Countries eems under control for now.
( (s (s(NP Mideast politics)have(VP calmeddown))and(S (NP the squabbling(PP within(NP the Organization(PP of(NP Petroleum Exporting Countries)))))(VP seems(PP under(NP control)))(PP for(NP now)))).)\[NP,Mideast,politics\].\[NP,Petroleum,Exporting,Countries\].\[NP,the,Organization,\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\].\[NP,the,squabbling,\[PP,within,\[NP,the,0rganization,\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]\]\].\[NP,control\].\[\[S,\[S,\[NP,Mideast,politics\],have,\[VP,calmed,\[PP,down\]\]\],and,\[S,\[NP,the,squabbling,\[PP,within,\[NP,the,Drganization,\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]\]\],\[VP,seems,\[PP,under,\[NP,control\]\],\[PP,for,now\]\]\]\],.\].Output.
The system outputs the classification it has assigned to each definite descrip-tion in the text, together with the coreferential nd bridging links it has identified.11 Prolog variables will be indicated in the rest of the paper by the use of .... at the beginning and end ofthe variables; e.g., _X_ for variable X.546Computational Linguistics Volume 26, Number 4is coordination: for example, our algorithm does not recognize that a noun such asreporters below is a head noun:\[NP, reporters,and, editors,\[PP, of \[NP, The,WSJ\]\]\].4.1.2 Potential Antecedents.
The second problem is to determine which NPs shouldbe used to resolve definite descriptions, among all those in the text.
The system keepstrack of NP index, NP structure, head noun, and NP type (definite, indefinite, bareplural, possessive n) of each potential antecedent, as illustrated by (5) below.
(5) potential_antecedent(l,np(NP),head(H),type(T)).Examples of potential antecedents extracted from (6) are shown in (7):(6)(7)In an interview with reporters of the Wall Street Journal, the candidateappears quite confident of victory and of his ability to handle themayoralty.a.potential_antecedent(l,np(_NPstructure),head(reporters),type(indef)).potential antecedent(2,np( NPstructure ),head(interview),type(indef)).b?potential_antecedent(3,np(_NPstructure_),head(3ournal),type(def)).potential antecedent(4,np( NPstructure ),head(candidate),type(def)).potential antecedent(5,np(NPstructure_),head(mayoralty),type(def)).C.potential_antecedent(6,np(_NPstructure_),head(ability),type(possessive)).d.potential_antecedent(7,np(_NPstructure_),head(victory),type(other)).We found that different recall/precision trade-offs can be achieved epending on thechoice of potential antecedents--i.e., depending on whether all NPs are consideredas possible antecedents, or only indefinite NPs, or various other subsets--so we ranexperiments to identify the best group of potential antecedents.
Four different NP12 Other NPs not included in any of these categories are identified as type(other).548 ,Vieira and Poesio Processing Definite Descriptionssubsets were considered:....indefinite NPs, defined as NPs containing the indefinite articles a, an,some and bare/cardinal plurals, as in (7a); 13indefinite NPs and definite descriptions (NPs beginning with the definitearticle) ((7a) and (7b));indefinite NPs, definite descriptions, and possessive NPs (NPs with apossessive pronoun or possessive mark) ((7a), (7b) and (7c));all NPs ((7a), (7b), (7c)) and (7d)).The results obtained by considering each subset of the total number of NPs as potentialantecedents are discussed in Section 5.2.4.1.3 Segmentation.
The set of potential antecedents of anaphoric expressions i alsorestricted by the fact that antecedents end to have a limited "life span"--i.e., theyonly serve as antecedents for anaphoric expressions within pragmatically determinedsegments of the whole text (see, for example, Reichman \[1985\], Grosz and Sidner\[1986\] and Fox \[1987\]).
In our corpus we found that about 10% of direct anaphoricdefinite descriptions have more than one possible antecedent if segmentation is nottaken into account (Vieira and Poesio 1996).
In (8), for example, the antecedent ofthe housej mentioned in sentence 50 is not the house mentioned earlier in sentences 2and 19, but another (nonmobile) house implicitly introduced in sentence 49 by thereference to the yard.
(8) 2.
A deep trench now runs along its north wall, exposed when the houseilurched two feet off its foundation during last week's earthquake??
?
?19.
Others grab books, records, photo albums, sofas and chairs, workingfrantically in the fear that an aftershock will jolt the housei again.20.
The owners, William and Margie Hammack, are luckier than manyothers?49.
When Aetna adjuster Bill Schaeffer visited a retired couple inOakland last Thursday, he found them living in a mobile homek parked infront of their yard.50.
The housej itself, located about 50 yards from the collapsed section ofdouble-decker highway Interstate 880, was pushed about four feet off itsfoundation and then collapsed into its basement.?
.
?65.
As Ms. Johnson stands outside the Hammack housei after winding upher chores there, the house/begins to creak and sway.13 Only plural nouns ending in s are handled by the system.549Computational Linguistics Volume 26, Number 4In general, it is not sufficient o look at the most recent antecedents only: thisis because segments are organized hierarchically, and the antecedents introduced ina segment at a lower level are typically not accessible from a segment at a higherlevel (Fox 1987; Grosz 1977; Grosz and Sidner 1986; Reichman 1985), whereas theantecedents introduced in a prior segment at the same level may be.
Later in (8),for example, the housej in sentence 50 becomes inaccessible again, and in sentence 65,the text starts referring again to the house introduced in sentence 2.
Automaticallyrecognizing the hierarchical structure of texts is an unresolved problem, as it involvesreasoning about intentions; 14 better esults have been achieved on the simpler task of"chunking" the text into sequences of segments, generally by means of lexical densitymeasures (Hearst 1997; Richmond, Smith, and Amitay 1997).The methods for limiting the life span of discourse ntities that we consideredfor our system are even simpler.
One type of heuristic we looked at are window-based techniques, i.e., considering only the antecedents within fixed-size windows ofprevious entences, although we allow some discourse ntities to have a longer lifespan: we call this method loose segmentation.
More specifically, a discourse ntityis considered a potential antecedent for a definite description when the antecedent'shead is identical to the description's head, and?
the potential antecedent is within the established window, or else?
the potential antecedent is itself a subsequent mention, or else?
the definite description and the antecedent are identical NPs (includingthe article).We also considered an even simpler ecency heuristic: this involves keeping atableindexed by the heads of potential antecedents, such that the entry for noun N containsthe index of the last occurrence of an antecedent with head N. Finally, we consideredcombinations of segmentation a d recency.
(The results of these two heuristics arecompared in Section 5.2.4.1.4 Noun Modifiers.
Once the head nouns of the antecedent and of the descriptionhave been identified, the system attempts to match them.
This head-matching strategyworks correctly in simple cases like (9):(9) Grace Energy hauled a rig here ...
The rig was built around 1980.In general, however, when matching a definite description with a potential an-tecedent the information provided by the prenominal nd the postnominal part of thenoun phrases also has to be taken into account.
For example, a blue car cannot serveas the antecedent for the red car, or the house on the left for the house on the right.
In ourcorpus, cases of antecedents that would incorrectly match by simply matching headswithout regarding premodification include:(10) a. the business community .. .
the younger, more activist black politicalcommunity;b. the population.., the voting population.14 See, however, Marcu (1999).550Vieira and Poesio Processing Definite DescriptionsAgain, taking proper account of the semantic ontribution of these premodif iers would,in general, require commonsense reasoning.
For the moment,  we only developedheuristic solutions to the problem, including:?
al lowing an antecedent to match with a definite description if thepremodif iers of the description are a subset of the premodif iers of theantecedent.
This heuristic deals with definites that contain lessinformation than the antecedent, such as an old Victorian house .. .
thehouse, and prevents matches uch as the business community .. .
theyounger, more activist black political community.?
al lowing a nonpremodif ied antecedent to match with any same headdefinite.
This second heuristic deals with definites that contain additionalinformation, such as a check.. ,  the lost check.The information that two discourse entities are disjoint may come from postmod-ification, as well, although same head antecedents with different postmodification arenot as common as those with differences in premodification.
An example from ourcorpus is shown in (11).
(11) a chance to accomplish several objectives ... the chanceto demonstrate an entrepreneurlike himself could run Pinkerton's better than an unfocused conglomerate orinvestment banker.The heuristic method we developed to deal with postmodification is to compare thedescription and antecedent, preventing resolution in those cases where both are post-modif ied and the modifications are not the same.
(These results are also discussed inSection 5.2.
)4.2 Discourse-New DescriptionsAs mentioned above, a fundamental  characteristic of our system is that it also includesheuristics for recognizing discourse-new descriptions (i.e., definite descriptions thatintroduce new discourse entities) on the basis of syntactic and lexical features of thenoun phrase.
Our heuristics are based on the discussion in Hawkins (1978), whoidentified a number  of correlations between certain types of syntactic structure anddiscourse-new descriptions, particularly those he called "unfamil iar" definites (i.e.,those whose existence cannot be expected to be known on the basis of generallyshared knowledge), including: isthe presence of "special predicates":the occurrence of premodif iers uch as first or best whenaccompanied with full relatives, e.g., the first person to sail toAmerica (Hawkins calls these "unexplanatory modifiers"; LObner15 Hawkins himself proposes a transformation-based account of unfamiliar definites, but the correlationshe identified proved to be a useful source of heuristics for identifying these uses of definitedescriptions even though the existence of counterexamples to these heuristics uggests that asyntactic-based account cannot be entirely correct.
Most of these examples can be accounted for interms of L6bner's theory of definiteness.551Computational Linguistics Volume 26, Number 4\[1987\] showed how these predicates may license the use ofdefinite descriptions in an account of definite descriptions basedon functionality);a head noun taking a complement such as the fact that there is lifeon Earth (Hawkins calls this subclass "NP complements");the presence of restrictive modification, as in the inequities of the currentland-ownership system.Our system attempts to recognize these syntactic patterns.
We also added heuristicsclassifying as unfamiliar some definites occurring in?
appositive constructions (e.g., Glenn Cox, the president of PhillipsPetroleum Co.);?
copular constructions (e.g., the man most likely to gain custody of all this is acareer politician named David Dinkins).
(The reason definite descriptions in appositive and copular constructions tend to bediscourse-new, in fact unfamiliar, is that the information eeded for the identificationis given by the NP to which the apposition is attached and the predicative part of thecopular construction, respectively.
16)Finally, we found that three classes of what Hawkins called "larger situation"definites (those whose existence can be assumed to be known on the basis of encyclo-pedic knowledge, such as the pope) can also be recognized on the basis of heuristicsexploiting syntactic and lexical features:?
definites that behave like proper nouns, like the United States;?
definites that have proper nouns in their premodification, such as theIran-Iraq war;?
definites referring to time, such as the time or the morning.In our corpus study we found that our subjects did much better at identifyingdiscourse-new descriptions all together (K = 0.68) than they did at distinguishingunfamiliar from larger situation cases (K = 0.63).
This finding was confirmed by ourimplementation: although each of the heuristics is designed, in principle, to identifyonly one of the uses (larger situation or unfamiliar), they work better at identifyingtogether the whole class of discourse-new descriptions.4.2.1 Special Predicates.
Some cases of discourse-new definite descriptions can beidentified by comparing the head noun or modifiers of the definite NP with a list ofpredicates that are either functional or likely to take a complement (L6bner 1987).
Ourlist of predicates that, when taking NP complements, are generally used to introducediscourse-new entities, was compiled by hand and currently includes the nouns fact,result, conclusion, idea, belief, saying, and remark.
In these cases, what licenses the use ofa definite is not anaphoricity, but the fact that the head noun can be interpreted as16 In the systems participating inMUC, definite descriptions occurring in appositions are treated asanaphoric on the preceding NP; our system considers the NP and the apposition as a unit thatintroduces a new referent to the discourse.552Vieira and Poesio Processing Definite Descriptionssemantically functional; the noun complement specifies the argument of the function.Functionality is enough to license the use of the definite description (LObner 1987).
Anexample of definite description classified as discourse-new on these grounds is givenin (12).
(12) Mr. Dinkins also has failed to allay Jewish voters' fears about hisassociation with the Rev.
Jesse Jackson, despite the fact that few localnon-Jewish politicians have been as vocal for Jewish causes in the past 20 years asMr.
Dinkins has.When encountering a definite whose head noun occurs in this list, the systemchecks if a complement is present or if the definite appears in a copular construction(e.g., the fact is that...).A second list of special predicates consulted by the system includes what Hawkinscalled unexplanatory modifiers: these include adjectives uch as -first, last, best, most,maximum, minimum, and only and superlatives in general.
17.
All of these adjectives arepredicate modifiers that turn a head noun into a function, therefore again--accordingto LObner--licensing the use of a definite even when no antecedent is present (seeexamples below).
When applying this heuristic, the system verifies the presence of acomplement for some of the modifiers (first, last), but not for superlatives.
(13) a. Mr. Ramirez just got the first raise he can remember in eight years, to $8.50an hour from $8.b.
She jumps at the slightest noise.Finally, our system uses a list of special predicates that we found to correlate wellwith larger situation uses (i.e., definite descriptions referring to objects whose existenceis generally known).
This list consists mainly of terms indicating time reference, andincludes the nouns hour, time, morning, afternoon, night, day, week, month, period, quarter,year, and their respective plurals.
An example from the corpus is:(14) Only 14,505 wells were drilled for oil and natural gas in the U.S. in thefirst nine months of the year.Other definites typically used with a larger situation interpretation are the moon,the sky, the pope, and the weather.It should be noted that although these constructions may indicate a discourse-new interpretation, these expressions may also be used anaphorically; this is one ofthe cases in which a decision has to be made concerning the relative priority of differ-ent heuristics.
We discuss this issue further in connection with the evaluation of thesystem's performance in Section 5.184.2.2 Restrictive and Nonrestrictive Modification.
A second set of heuristics for iden-tifying discourse-new descriptions that we derived from Hawkins's suggestions and17 The list should be made more comprehensive; so far it includes the cases observed in the corpusanalysis and a few other similar modifiers.18 More recently, Bean and Riloff (1999) have proposed methods for automatically extracting from acorpus heads that correlate well with discourse novelty.553Computational Linguistics Volume 26, Number 4Table 3Distribution of prepositional phrases and relative clauses.Restrictive Postmodification # %Prepositional phrases 152 77%Relative clauses 45 23%Total 197 100 %from our corpus analysis look for restrictive modification.
19We developed patternsto recognize restrictive postmodification and nonrestrictive postmodification; we alsotested the correlation between discourse novelty and premodification.
We discuss eachof these heuristics in turn.Restrictive Postmodification.
Hawkins (1978) pointed out that unfamiliar definites ofteninclude referent-establishing relative clauses and associative clauses, while warningthat not all relative clauses are referent-establishing.
Some statistics about this corre-lation were reported by Fraurud (1990): she found that in her corpus 75% of complexdefinite NPs (i.e., modified by genitives, postposed PPs, restrictive adjectival modifiers)were first-mention.
A great number of definite descriptions with restrictive postmod-ifiers are unfamiliar in our corpus as well (Poesio and Vieira 1998); in fact, restrictivepostmodification was found to be the single most frequent feature of first-mention de-scriptions.
Constructions of this type are good indicators of discourse novelty becausea restrictive postmodifier may license the use of a definite description either by pro-viding a link to the rest of the discourse (as in Prince's "containing inferrables') or bymaking the description into a functional concept.
Looking for restrictive postmodifiersmight therefore be a good way of identifying discourse-new descriptions.The distribution of restrictive postmodifiers in our corpus is shown in Table 3;examples of each type of postmodifier are given below.
(15)(16)Relative clauses: these are finite clauses sometimes (but not always)introduced by relative pronouns uch as who, whom, which, where, when,why, and that:a.
The place where he lives .. .b.
The guy we met .. .Nonfinite postmodifiers: these include ing, ed (participle), and infinitivalclauses.a.
The man writing the letter is my friend.b.
The man to consult is Wilson.Prepositional phrases and of-clauses: Quirk et al (1985) found thatprepositional phrases are the most common type of postmodification iEnglish---three or four times more frequent han either finite or nonfiniteclausal postmodification.
This was confirmed by our corpus study (see19 The term restrictive modification is used when the modifier provides information that is essential toidentify the discourse ntity referred to by the NP (Quirk et al 1985).
The modification is nonrestrictivewhen the head provides ufficient information to identify the discourse ntity, so that the informationprovided by the modification is not essential for identification.554Vieira and Poesio Processing Definite DescriptionsTable 4Distribution of prepositions (1).Prepositional Phrases # %Of-phrases 120 79%Other prepositions 32 21%Total 152 100%Table 3).
The types of prepositions observed for 188 postmodifieddescriptions are shown in Table 4; of-clauses are the most common.Our program uses the following patterns to identify restrictive postmodifiers: 2?
(17) a.\[HP,the,_Premodifiers_,_Head_, \[SBAKQI_\] i_\] ;b.\[NP, the, _Premodif iers_, _Head_, \[SBAR i _\] \[ _\] ;C.\[HP ,the, _Premodifiers_, _Head_, \[S i_\] I_\] ;d.\[NP, the, _Premodif iers_, _Head_, \[VP J _\] l _\] ;e.\[NP, the, _Premodifiers_, _Head_, \[PP,_ I_\] i_\] ;f.\[NP, the, _Premodifiers_, _Head_, \[WHPP, _ I _\] I _\] ?In the Treebank, sometimes the modified NP is embedded in another NP, so structureslike (18) are also considered (again for all types of clauses just shown above):(18) \[NP,\[NP,the,_Premodifiers_,_Head_\],\[Clause\]\].Nonrestrictive postmodiJi'cation.
We found it important to distinguish restrictive fromnonrestrictive postmodification, since in our corpus, definite descriptions with nonre-strictive postmodifiers were generally not discourse-new.
Our system recognizes non-restrictive postmodifiers by the simple yet effective heuristic of looking for commas.This heuristic orrectly recognizes nonrestrictive postmodification in cases like:(19) The substance, discovered almost by accident, is very important.which are annotated in the Penn Treebank I as follows:(20) \[NP,the,proposal,',',\[SBAR,\[WHHP,which\],also,\[S,\[HP,T\],would,\[VP,create,\[NP,a,new,type,\[PP,of,\[HP,individual,retirement,account\]Ill\]I, ',' \]...20 Note that an NP may have zero, one, or more premodifiers.555Computational Linguistics Volume 26, Number 4Restrictive Premodification.
Restrictive modification is not as common in prenominalposition as in posthead position, but it is often used, and was also found to correlatewell with larger situation and unfamiliar uses of definite descriptions (Poesio andVieira 1998).
A restrictive premodifier may be a noun (as in (21)), a proper noun, oran adjective, m Sometimes numerical figures (usually referring to dates) are used asrestrictive premodifiers, as in (22)).
(21)(22)A native of the area, he is back now after riding the oil-field boom to thetop, then surviving the bust running an Oklahoma City conveniencestore.the 1987 stock market crash;The heuristic we tested was to classify definite descriptions premodified by a propernoun as larger situation.4.2.3 Appositions.
During our corpus analysis we found additional syntactic patternsthat appeared to correlate well with discourse novelty yet had not been discussedby Hawkins, such as definite descriptions occurring in appositive constructions: theyusually refer to the NP modified by the apposition, therefore there is no need for thesystem to look for an antecedent.
Appositive constructions are treated in the Treebankas NP modifiers; therefore the system recognizes an apposition by checking whetherthe definite occurs in a complex noun phrase with a structure consisting of a sequenceof noun phrases (which might be separated by commas, or not) one of which is aname or is premodified by a name, as in the examples in (23).
(23) a. Glenn Cox, the president of Phillips Petroleumb.
\[NP, \[NP,Glenn,Cox\] , ', ', \[NP,the,president,\[PP, of, \[NP, Phil l ips, Petroleum\] \] \] \] ;c. the oboist, Heinz Holligerd.
\[NP, \[NP, the, oboist\] , \[NP, Heinz, Holl iger\] \] .In fact a definite description may itself be modified by an apposition, e.g., an indefiniteNP, as shown by (24).
Such cases of appositive constructions are also recognized bythe system.
(24) the Sandhills Luncheon Care, a tin building in midtown.Other examples of apposition recognized by the system are:(25) a. the very countercultural chamber group Tashi;b. the new chancellor, John Major;c. the Sharpshooter, a freshly drilled oil well two miles deep;21 Our system cannot distinguish adjectives orverbs from nouns in premodification because it worksdirectly off the parsed version of the Treebank, without looking at part-of-speech tags.556Vieira and Poesio Processing Definite Descriptions4.2.4 Copular Phrases.
Copular phrases such as the Prime Minister is Tony Blair alsooften involve discourse-new descriptions.
We developed the following heuristic forhandling copula constructions.
If a description occurs in subject position, the systemlooks at the VP.
If the head of the VP is the verb to be, to seem, or to become, and thecomplement of the verb is not an adjectival phrase, the system classifies the descriptionas discourse-new.
Two examples correctly handled by this heuristic are shown in (26);the syntactic representation f these cases in the Penn Treebank I is shown in (27).
(26) a.
The bottom line is that he is a very genuine and decent guy.b.
When the dust and dirt settle in an extra-nasty mayoral race, the manmost likely to gain custody of all this is a career politician named DavidDinkins.
(27) \[S,\[NP,The,bottom,line\],\[VP,is,\[NP,\[SBAR,that... l \ ] l \ ] .If the complement of the verb is an adjective, the subject is typically interpretedreferentially and should not be considered iscourse-new on the basis of its comple-ment (e.g., The president of the US is tall).
Adjectival complements are represented asfollows in the Treebank:(28) \[S,\[NP,The,missing,watch\],\[VP,is,\[AD3P,emblematic...\]\]\].Definite descriptions in object position of the verb to be, such as the one shown in(29), are also considered iscourse-new.
(29) What the investors object o most is the effect hey say the proposal wouldhave on their ability to spot telltale "clusters" of trading activity.4.2.5 Proper Names.
Proper names preceded by the definite article, such as (30), arecommon in the genre we are dealing with, newspaper articles.
(30) the Securities and Exchange Commission.The first appearance of these definite descriptions in the text is usually a discourse-new description; subsequent mentions of proper names are regarded as cases ofanaphora.
To recognize proper names, the system simply checks whether the headis capitalized.
If the test succeeds, the definite is classified as a larger situation use .
224.3 Bridging DescriptionsBridging descriptions are the definite descriptions that a shallow processing systemis least equipped to handle.
Linguistic and computational theories of bridging de-scriptions identify two main subtasks involved in their resolution: finding the elementin the text to which the bridging description is related (anchor) and identifying therelation (link) holding between the bridging description and its anchor (Clark 1977;Sidner 1979; Heim 1982; Carter 1987; Fraurud 1990; Strand 1996).
The speaker is h-censed to use a bridging description when he or she can assume that the commonsense22 Note that this test is performed just after trying to find an antecedent, so that the second instance ofthe same proper (head) noun will be classified as an anaphoric use.557Computational Linguistics Volume 26, Number 4knowledge required to identify the relation is shared by the listener (Hawkins 1978;Clark and Marshall 1981; Prince 1981).
This dependence on commonsense knowledgemeans that, in general, a system can only resolve bridging descriptions when suppliedwith an adequate knowledge base; for this reason, the typical way of implementinga system for resolving bridging descriptions has been to restrict he domain and feedthe system with hand-coded world knowledge (see, for example, Sidner \[1979\] andespecially Carter \[1987\]).
A broader view of bridging phenomena (not only bridgingdescriptions) is presented in Hahn, Strube, and Markert (1996).
They make use of aknowledge base from which they extract conceptual links to feed an adaptation of thecentering model (Grosz, Joshi, and Weinstein 1995).The relation between bridging descriptions and their anchors may be arbitrarilycomplex (Clark 1977; Sidner 1979; Prince 1981; Strand 1996), and the same descriptionmay relate to different anchors in a text: this makes it difficult to decide what theintended anchor and the intended link are (Poesio and Vieira 1998).
For all thesereasons, this class has been the most challenging problem we have dealt with in thedevelopment of our system, and the results we have obtained so far can only beconsidered very preliminary.
Nevertheless, we feel that trying to process these definitedescriptions i the only way to discover which types of commonsense knowledge areactually needed.4.4 Types of Bridging DescriptionsOur work on bridging descriptions began with the development of a classification ofbridging descriptions (Vieira and Teufel 1997) according to the kind of informationneeded to resolve them, rather than on the basis of the possible relations betweendescriptions and their anchors as is typical in the literature.
This allowed us to getan estimate of what types of bridging descriptions we might expect our system toresolve.
The classification is as follows:?
cases based on well-defined lexical relations, such as synonymy,hypernymy, and meronymy, that can be found in a lexical database suchas WordNet (Fellbaum 1998), as in theyqat .
.
.
the living room;?
bridging descriptions in which the antecedent is a proper name and thedescription a common oun, whose resolution requires ome way ofrecognizing the type of object denoted by the proper name, as in Bach .
.
.the composer;?
cases in which the anchor is not the head noun but a noun modifying anantecedent, asin the company has been selling discount packages .
.
.
thediscounts?
cases in which the antecedent (anchor) is not introduced by an NP butby a VP, as in Kadane oil is currently drilling two oil wells.
The activity .
.
.?
descriptions whose antecedent is not explicitly mentioned in the text, butis implicitly available because it is a discourse topic, e.g., the industry in atext referring to oil companies;?
cases in which the relation with the anchor is based on more generalcommonsense knowledge, e.g., about cause-consequence relations.In the rest of this section, we describe the heuristics we developed for handlingthe first three of these classes: lexical bridges, bridges based on names, and bridges558Vieira and Poesio Processing Definite Descriptionsto entities introduced by nonhead nouns in a compound nominal (Poesio, Vieira, andTeufel 1997).4.4.1 Bridging Descriptions and WordNet.
In order to get a system that could be eval-uated on a corpus containing texts in different domains, we used WordNet (Fellbaum1998) as an approximation of a lexical knowledge source.
We developed a WordNetinterface (Vieira and Teufel 1997) that reports a possible semantic link between twonouns when one of the following is true:?
the nouns are in the same synset (i.e., they are synonyms of each other),as in suit~lawsuit;?
the nouns are in a hyponymy/hypernymy relation with each other, as indollar~currency;?
there is a direct or indirect meronymy/holonymy (part of/has parts)relation between them, as in door~house;?
the nouns are coordinate sisters, i.e.
hyponyms of the same hypernym,such as home~house, which are hyponyms of housing, lodging.Sometimes, finding a relation between two predicates involves complex searchesthrough WordNet's hierarchy.
For example, there may be no relation between twohead nouns, but there is a relation between compound nouns in which these nounsappear: thus, there is no semantic relation between record~album, but only a synonymyrelation between record_album~album.
We found that extended searches of this type, orsearches for indirect meronymy relations, yielded extremely ow recall and precision ata very high computational cost; both types of search were dropped at the beginning ofthe tests we ran to process the corpus consulting WordNet (Poesio, Vieira, and Teufel1997).
The results of our tests with WordNet are presented in Section 5.4.4.4.2 Bridging Descriptions and Named Entity Recognition.
Definite descriptionsthat refer back to entities introduced by proper names (such as Pinkerton Inc ... thecompany) are very common in newspaper articles.
Processing such descriptions requiresdetermining an entity type for each name in the text, that is, if we recognize PinkertonInc.
as an entity of type company, we can then resolve the subsequent descriptionthe company, or even a description such as the firm by finding a synonymy relationbetween company and firm using WordNet.This so-called named entity recognition task has received considerable attentionrecently (Mani and MacMillan 1996; McDonald 1996; Paik et al 1996; Bikel et al1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover1999) and was one of the tasks evaluated in the Sixth and Seventh Message Under-standing Conferences.
In MUC-6, 15 different systems participated in the competition(Sundheim 1995).
For the version of the system discussed and evaluated here, we im-plemented a preliminary algorithm for named entity recognition that we developedourselves; a more recent version of the system (Ishikawa 1998) uses the named en-tity recognition software developed by HCRC for the MUC-7 competition (Mikheev,Moens, and Grover 1999).WordNet contains the types of a few names--typically, of famous people, coun-tries, states, cities, and languages.
Other entity types can be identified using appositiveconstructions and abbreviations ( uch as Mr., Co., and Inc.) as cues.
Our algorithm forassigning a type to proper names is based on a mixture of the heuristics just described.559Computational Linguistics Volume 26, Number 4The system first looks for the above-mentioned cues to try to identify the name type.
Ifno cue is found, pairs consisting of the proper name and each of the elements from thelist country, city, state, continent, language, person are consulted in our WordNet interfaceto verify the existence of a semantic relation.The recall of this algorithm was increased by including a backtracking mechanismthat reprocesses a text, filling in the discourse representation with missing name types.With this mechanism we can identify later the type for the name Morishita in a textualsequence in which the first occurrence of the name does not provide surface indicationof the entity type: e.g., Morishita .
.
.
Mr. Morishita.
The second mention includes sucha clue (Mr.); by processing the text twice, we recover such missing types.After finding the types for names, the system uses the techniques previously de-scribed for same-head matching or WordNet lookup to match the descriptions withthe types found for previous named entities.4.4.3 Compound Nouns.
Sometimes, the anchor for a bridging description is a non-head noun in a compound noun:(31) stock market crash..,  the markets;One way to process these definite descriptions would be to update the discoursemodel with discourse referents not only for the NP as a whole, but also for the em-bedded nouns.
For example, after processing stock market crash, we could introduce adiscourse referent for stock market, and another discourse referent for stock market crash.
23The description the markets would be coreferring with the first of these referents (withan identical head noun), and then we could simply use our anaphora resolution algo-rithms.
This solution, however, makes available discourse referents that are generallyinaccessible for anaphora (Postal 1969).
For example, it is generally accepted that in(32), a deer is not accessible for anaphoric reference.
24(32) I saw la deeri hunter\]j. ItT was dead.Therefore, we followed a different route.
Our algorithm for identifying anchors at-tempts to match not only heads with heads, but also:.(33)2.(34)3.
(35)The head of a description with the premodifiers of a previous NP:the stock market crash..,  the markets;The premodifiers of a description with the premodifiers of itsantecedents:his art business .. .
the art gallery.And finally, the premodifiers of the description with the head of aprevious NP:a 15-acre plot and main home .. .
the home site.23 Note that the collection of potential antecedents containing all NPs will just have the NP head crash forstock market crash.
The system considers the whole NP structure as only one discourse referent,according to the structure of the Penn Treebank: \[NP, the,1987,stock,market,crash\].24 These proposed constraints have been challenged by Ward, Sproat, and McKoon (1991).560Vieira and Poesio Processing Definite Descriptions5.
Evaluation of the HeuristicsIn this section we discuss the tests we ran to arrive at a final configuration of thesystem.
The performance of the heuristics discussed in Section 4 was evaluated bycomparing the results of the system with the human annotation of the corpus pro-duced during the experiments discussed in Poesio and Vieira (1998).
Several variantsof our heuristics were tried using Corpus 1 as training data; after deciding upon anoptimal version, our algorithms were evaluated using Corpus 2 as test data.
Becauseour proposals concerning bridging descriptions are much less developed than thoseconcerning anaphoric descriptions and discourse-new descriptions, we ran separateevaluations of two versions of the system: Version 1, which does not attempt o re-solve bridging descriptions, and Version 2, which does; we will point out below whichversion of the system is considered in each evaluation.5.1 Evaluation MethodsThe fact that the annotators working on our corpus did not always agree either onthe classification of a definite description or on its anchor aises the question of howto evaluate the performance of our system.
We tried two different approaches: eval-uating the performance of the system by measuring its precision and recall against astandardized annotation based on majority voting (as done in MUC), and measuringthe extent of the system's agreement with the rest of the annotators by means of thesame metric used to measure agreement among the annotators themselves (the kappastatistic).
We used the first form of evaluation to measure both the performance ofthe single heuristics and the performance of the system as a whole; the agreementmeasure was only used to measure the overall performance of the system.
We discusseach of these in turn.
as5.1.1 Precision and Recall.
Recall and precision are measures commonly used in In-formation Retrieval to evaluate a system's performance.
Recall is the percentage ofcorrect answers reported by the system in relation to the number of cases indicatedby the annotated corpus:R = number of correct responsesnumber of caseswhereas precision is the percentage of correctly reported results in relation to the totalreported:p = number of correct responsesnumber of responsesThese two measures may be combined to form one measure of performance, the Fmeasure, which is computed as follows:F - (W + 1)RP(WR) + PW represents he relative weight of recall to precision and typically has the value 1.A single measure gives us a balance between the two results; 100% of recall may bedue to a precision of 0% and vice versa.
The F measure penalizes both very low recalland very low precision.25 For a rather thorough discussion ofthe problem of evaluating anaphora esolution algorithms, seeMitkov (2000).561Computational Linguistics Volume 26, Number 45.1.2 Semiautomatic Evaluation against a Standardized Annotation.
The precisionand recall figures for the different variants of the system were obtained by com-paring the classification produced by each version with a standardized annotation,extracted from the annotations produced by our human annotators by majority judge-ment: whenever at least two of the three coders agreed on a class, that class was chosen.Details of how the standard annotation was obtained are given in Vieira (1998).
26The system's performance as a classifier was automatically evaluated against hestandard annotation of the corpus as follows.
Each NP in a text is given an index:(36) A house1?6...
The house135...When a text is annotated or processed, the coder or system associates each index of adefinite description with a type of use; both the standard annotation and the system'soutput are represented as Prolog assertions.
(37) a .system:b.coder:dd_class(135,anaphoric).dd_class(135,anaphoric).To assess the system's performance on the identification of a coreferential an-tecedent, it is necessary to compare the links that indicate the antecedent of each de-scription classified as anaphora.
These links are also represented as Prolog assertions,as follows:(38) a .coder : corer (135,106).b.system: corer (135,106).The system uses these assertions to build an equivalence class of discourse ntities,called a coreference chain.
When comparing an antecedent indicated by the systemfor a given definite description with that in the annotated corpus, the correspondingcoreference chain is checked--that is, the system's indexes and the annotated indexesdo not need to be exactly the same as long as they belong to the same coreferencechain.
In this way, both (40a) and (40b) would be evaluated as correct answers if thecorpus is annotated with the links shown in (39).
(39) A house1?6...
The house135...
The house154...coder: corer(135,106).coder: corer(154,135).
(40) a .system: corer (154,135).b.system: corer(154,106).26 An alternative method is to give fractional values to a classification depending on the number ofagreements (Hatzivassiloglou and McKeown (1993).562Vieira and Poesio Processing Definite DescriptionsIn the end, we still need to check the results manually, because our annotated coref-erence chains are not complete: our annotators did not annotate all types of anaphoricexpressions, o it may happen that the system indicates as antecedent an element out-side an annotated coreference chain, such as a bare noun or possessive.
In (41), forexample, suppose that all references to the house are coreferential:(41) A house1?6...
The house135... His house14?...
The house154...corer ( 154,140).If NP 135 is indicated as the antecedent for NP 154 in the corpus annotation (so that140 is not part of the annotated coreference hain), and the system indicates 140 as theantecedent for 154, an error is reported by the automatic evaluation, even though allof these NPs refer to the same entity.
A second consequence of the fact that the coref-erence chains in our standard annotation are not complete is that in the evaluation ofdirect anaphora resolution, we only verify if the antecedents indicated are correct; wedo not evaluate how complete the coreferential chains produced by the system are.
Bycontrast, in the evaluation of the MUC coreference task, where all types of referringexpressions are considered, the resulting co-reference hains are evaluated, rather thanjust the indicated antecedent (Vilain et al 1995).
Even our limited notion of corefer-ence chain was, nevertheless, very helpful in the automatic evaluation, considerablyreducing the number of cases to be checked manually.5.1.3 Measuring the Agreement of the System with the Annotators.
Because theagreement between our annotators in Poesio and Vieira (1998) was often only partial,in addition to precision and recall measures, we evaluated the system's performanceby measuring its agreement with the annotators using the K statistic we used in Poesioand Vieira (1998) to measure agreement among annotators.
Because the proper inter-pretation of K figures is still open to debate, we interpret the K figures resulting fromour tests comparatively, rather than absolutely, (by comparing better and worse levelsof agreement).5.2 Anaphora ResolutionWe now come to the results of the evaluation of alternative versions of the heuristicsdealing with the resolution of direct anaphora (segmentation, selection of potentialantecedents, and premodification) discussed in Section 4.1.
The optimal version of oursystem is based on the best results we could get for resolving direct anaphora, becausewe wanted to establish the coreferential relations among discourse NPs as preciselyas possible.5.2.1 Life Span of Discourse Entities.
In Section 4.1 we discussed two heuristics forlimiting the life span of discourse ntities.
The first segmentation heuristic discussedthere, loose segmentation, is window based, but the restriction on sentence distanceis relaxed (i.e., the resolver will consider an antecedent outside the window) wheneither:?
the antecedent is itself a subsequent-mention; or?
the antecedent is identical to the definite description being resolved(including the article).With loose segmentation, it is possible for the system to identify more than onecoreference link for a definite description: all antecedents satisfying the requirements563Computational Linguistics Volume 26, Number 4Table 5Evaluation of loose segmentation a d recency heuristics.Heuristics R P FSegmentation: 1-sentence window 71.79% 86.48% 78.45%Segmentation: 4-sentence window 76.92% 82.75% 79.73%Segmentation: 8-sentence window 78.20% 80.26% 79.22%Recency: all sentences 80.76% 78.50% 79.62%Table 6Evaluation of the strict segmentation heuristic.Strict Segmentation R P F1-sentence window 29.48% 89.32% 44.33%4-sentence window 57.69% 88.23% 69.76%8-sentence window 67.94% 84.46% 75.31%within the current w indow will be indicated as a possible antecedent.
Therefore, whenevaluating the system's results, we may find that all antecedents indicated for theresolution of a description were right, or some were right and some wrong, or that allwere wrong.
The recall and precision figures reported here relate to those cases wereall resolutions indicated were right according to the annotated corpus.In Section 4.1 we also discussed a second segmentation heuristic, which we calledrecency: the system does not collect all candidate NPs as potential antecedents, butonly keeps the last occurrence of an NP from all those having the same head noun,and there are no restrictions regarding the antecedent's distance.The results of these two methods for different w indow sizes are shown in Ta-ble 5.
The results in this table were obtained by considering as potential antecedentsindefinites (i.e., NPs with determiners a,an, and some; bare NPs; and cardinal plurals),possessives, and definite descriptions, as in Vieira and Poesio (1996); we also used thepremodif ication heuristics proposed there.
Alternatives to these heuristics were alsoevaluated; the results are discussed later in this section.The resulting F measures were almost the same for all heuristics, but there wasclearly an increase in recall with a loss of precision when enlarging the window size} 7The recency heuristic had the best recall, but the lowest precision, although not muchlower than the others.
The best precision was achieved with a one-sentence window,and recall was not dramatical ly affected, but this only happened because the windowsize constraint was relaxed.To show what happens when a strict version of the window-based segmentationapproach is used, consider Table 6.
(Strict segmentation means that the system onlyconsiders those antecedents that are inside the sentence window for resolving a de-scription, with no exceptions.)
As the table shows, this form of segmentation resultsin higher precision, but has a strong negative ffect on recall.
The overall F values areall worse than for the heuristics in Table 5.Finally, we tried a combination of the recency and segmentation heuristics: just onepotential antecedent for each different head noun is available for resolution, the last27 In our experiments small differences in recall, precision, and F measures are frequent.
We generallyassume in this paper that such differences are not significant, but a more formal significance t st alongthe lines of that in Chinchor (1995) will eventually be necessary to verify this.564Vieira and Poesio Processing Definite DescriptionsTable 7Combining loose segmentation a d recency heuristics.Combined Heuristics R P F4 sentences + recency 75.96% 87.77% 81.44%8 sentences + recency 77.88% 84.96% 81.27%Table 8Evaluation of the heuristics for choosing potential antecedents.Antecedents Selection R P FIndefinites, definite descriptions, and possessives 75.96% 87.77% 81.44%All NPS 77.88% 86.17% 81.81%Indefinites and definite descriptions 73.39% 88.41% 80.21%Indefinites only 12.17% 77.55% 21.05%occurrence of that head noun.
The resolution still respects the segmentation heuristic(loose version).
The results are presented inTable 7.
This table shows that by combiningthe recency and loose segmentation approaches to segmentation we obtain a bettertrade-off between recall and precision than using each heuristic separately.
The versionwith higher F value in Table 7 (four-sentence window plus recency) was chosen asstandard and used in the tests discussed in the rest of this section.5.2.2 Potent ia l  Antecedents .
Next, we evaluated the various ways of restricting the setof potential antecedents discussed in Section 4.1, using four-sentence-window l osesegmentation with recency.
In an earlier version of the system (Vieira and Poesio 1996),only those definite descriptions that were not resolved with a same-head antecedentwere considered as potential antecedents; resolved definite descriptions would belinked to previous NPs, but would not be made available for subsequent resolution.
(The idea was that the same antecedent used in one resolution could be used to resolveall subsequent mentions cospecifying with that definite description.)
An important dif-ference between that implementation a d the current one is that in the new version,the definltes resolved by the system are also made available as potential antecedents ofsubsequent definites.
This is because in our previous prototype, errors in identifyingan indefinite antecedent were sometimes propagated through a coreference chain, sothat the right antecedent would be missed.
The results are shown in Table 8.If we only consider indefinites as potential antecedents, recall is extremely low(12%); we also get the worst precision.
In other words, considering only indefinitesfor the resolution of definite descriptions i too restrictive; this is because our corpuscontains alarge number of first-mention definite descriptions that serve as antecedentsfor subsequent references ( imilar results were also reported in Fraurud \[1990\]).
Theversion with the highest precision (88%) is the one that only considers indefinites anddefinite descriptions as antecedents, but recall is lower compared to the version thatconsidered other NPs.
We chose, as the basis for further testing, aversion that combinesnear-optimal values for F and precision, i.e., the version that takes indefinites, definitedescriptions, and possessives (first row in Table 8).5.2.3 Premodifiers.
Finally, we tested our heuristics for dealing with premodifiers.
Wetested the matching algorithm from Vieira and Poesio (1996) in the present versionof the system; the results are presented in Table 9.
In that table, we also show the565Computational Linguistics Volume 26, Number 4Table 9Evaluation of the heuristics for premodification (Version 1).Antecedents Selection R P F1.
Ant-set/Desc-subset 69.87% 91.21% 79.12%2.
Ant-empty 55.12% 88.20% 67.85%3.
Ant-subset/Desc-set 64.74% 88.59% 74.81%1 and 2 (basic v.) 75.96% 87.77% 81.44%1 and 3 75.96% 87.13% 81.16%None 78.52% 81.93% 80.19%results obtained with a modified matching algorithm including a third rule, whichallows a premodified antecedent to match with a definite whose set of premodifiersis a superset of the set of modifiers of the antecedent (an elaboration of rule 2).
Wetested each of these three heuristics alone and in combination.
(The fourth line simplyrepeats the results hown in Table 7.
)The main result of this evaluation is that using a modified segmentation heuristic(including recency) reduces the overall impact of the heuristics for premodificationthe performance of the algorithm in comparison with the system discussed in Vieiraand Poesio (1996)?
The best precision is still achieved by the matching algorithm thatdoes not allow for new information in the anaphoric expression, but the best resultsoverall are again obtained by combining rule I and rule 2, although either 2 or 3 worksequally well when combined with 1.
(Note that the combination of heuristics 2 and 3is equivalent to heuristic 3alone, since rule 3 subsumes rule 2.)
Heuristic 2 and 3 aloneare counterintuitive and indeed give the poorest results; however, the impact is greateron recall than precision, which suggests that the introduction of new information innoun modification is not very frequent?One of the problems with our premodifier heuristics i that although a difference inpremodification usually indicates noncoreference, as for the company's abrasive segmentand the engineering materials egment, there are a few cases in our corpus in whichcoreferent descriptions have totally different premodification from their antecedents,as in:(42) the pixie-like clarinetist ... the soft-spoken clarinetist?These cases would be hard even for a system using real commonsense reasoning, sinceoften the information i  the premodifier isnew; we consider these examples one of thebest arguments for including in the system a focus-tracking mechanism along the linesof Sidner (1979).
Our heuristic matching algorithm also suggests wrong antecedentsin cases like the rules in (43), when the last mention refers to a modified concept (thenew rules are different from the previous ones).
(43) Currently, the rules force executives ...The rule changes would ...The rules will eliminate ...Finally, the matching algorithm gets the wrong result in cases uch as the population?
.
.
the voting population where the new information i dicates a subset, superset, or partof a previously mentioned referent.566Vieira and Poesio Processing Definite DescriptionsTable 10Evaluation of the heuristics for direct anaphora (Version 1).Anaphora Classification # + - R P FTraining data 312 243 27 78% 90% 83%Test data 154 103 12 67% 90% 77%Anaphora Resolution # + - R P FTraining data 312 237 33 76% 88% 81%Test data 154 96 19 62% 83% 71%5.2.4 Overa l l  Resu l ts  for Anaphor ic  Def in i te  Descr ipt ions .
To summarize, on thebasis of the tests just discussed, the heuristics that achieve the best results for anaphoricdefinite descriptions are:.2.3..combined loose segmentation a d recency,four-sentence window,considering indefinites, definites, and possessives as potentialantecedents,the premodification f the description must be contained in thepremodification f the antecedent or when the antecedent has nopremodifiers.In Table 10 we present the overall results on anaphora classification and anaphoraresolution for the version of the system that does not attempt o resolve bridgingdescriptions, for both training data and test data.
The reason there are different figuresfor anaphora resolution and classification is that the system may correctly classify adescription as anaphoric, but then find the wrong antecedent.
We used this set ofheuristics when evaluating the heuristics for discourse-new and bridging descriptionsin the rest of the paper.The column headed # represents he number of cases of descriptions classified asanaphora in the standard annotation; +indicates the total number of anaphora (clas-sification and resolution) correctly identified; - indicates the total number of errors.5.2.5 Errors in Anaphora  Reso lu t ion .
Before discussing the results of the other heuris-tics used by the system, we will discuss in more detail some of the errors in theresolution of anaphoric descriptions made by using the heuristics just discussed.Some errors are simply caused by misspellings in the Treebank, as in the examplebelow, where the antecedent is misspelled as spokewoman.
(44) A Lorillard spokewoman ...
The Lorillard spokeswomanThe most common problems are due to the heuristics limiting the search for an-tecedents.
In (45), both sentence 7 and sentence 30 are outside the window consideredby the system when trying to resolve the adjusters in 53.
(45) 7.
She has been on the move almost incessantly since last Thursday,when an army of adjusters, employed by major insurers, invaded the SanFrancisco area.567Computational Linguistics Volume 26, Number 4?
?
?30.
Aetna, which has nearly 3,000 adjusters, had deployed about 750 ofthem53.
Many of the adjusters employed by Aetna and other insurersLimiting the type of potential antecedents o indefinites, definite descriptions,and possessives, while improving precision, also leads to problems, because the an-tecedents introduced by other NPs, such as proper names, are missed--e.g., Toni John-son in (46).
The following definite description is then classified by the system as largersituation/unfamiliar.
Some of these problems are corrected in Version 2 of the system,which also attempts to handle bridging descriptions and therefore uses algorithms forassigning a type to such entities?
(46) Toni Johnson pulls a tape measure across the front of what was once astately Victorian home.The petite, 29-year-old Ms. Johnson ...The premodification heuristics prevent the system from finding the right an-tecedent in the (rare) cases of coreferent descriptions with different premodifiers, asin (47).
(47) The Victorian house that Ms. Johnson is inspecting has been deemedunsafe by town officials?Once inside, she spends nearly four hours measuring and diagrammingeach room in the 80-year-old house?In the following example, it is the lack of a proper treatment of postmodificationthat causes the problem.
The system classifies the description the earthquake-relatedclaims as anaphoric to claims from that storm, but it is discourse-new according to thestandardized annotation.
(48) Most companies till are trying to sort through the wreckage caused byHurricane Hugo in the Carolinas last month?Aetna, which has nearly 3,000 adjusters, had deployed about 750 of themin Charlotte, Columbia, and Charleston?Adjusters who had been working on the East Coast say the insurer willstill be processing claims from that storm through December.It could take six to nine months to handle the earthquake-related claims?In (49), the system correctly classifies the definite description the law as anaphoric,but suggests as antecedent an income tax law, whereas a majority of our annotators568Vieira and Poesio Processing Definite DescriptionsTable 11Evaluation of the heuristics for identifying discourse-new descriptions.Discourse -new # + - R P FTraining data 492 368 60 75% 86% 80%Test data 218 151 58 69% 72% 70%indicated a money lending law as the antecedent.
28(49) Nearly 20 years ago, Mr. Morishita, founder and chairman of AichiCorp., a finance company, received a 10-month suspended sentence froma Tokyo court for violating a money-lending law and an income tax law.He was convicted of charging interest rates much higher than what thelaw permitted, and attempting to evade income taxes by using a doubleaccounting system.Finally, the system is incapable of resolving plural references to collections of ob-jects introduced by singular NPs, even when these collections were introduced bycoordinated noun phrases.
Although it would be relatively easy to add rules for han-dling the simplest cases (possibly at the expense of a decrease in precision), many ofthese references can only be resolved by means of nontrivial operations.
(50) The owners, William and Margie Hammack, are luckier than many others.The Hammacks ...5.3 Ident i f i ca t ion  o f  D iscourse -New Descr ip t ionsThe overall recall and precision results for the heuristics for identifying discourse-new descriptions presented in Section 4.2 are shown in Table 11.
In this table we donot distinguish between the two types of discourse-new descriptions, unfamiliar andlarger-situation (Hawkins 1978).
As already mentioned in Section 4.2, distinguishingbetween the two types of discourse-new descriptions identified by Hawkins, Prince,and others isn't easy even for humans (Fraurud 1990; Poesio and Vieira 1998); andindeed, our heuristics for recognizing discourse-new descriptions work better whenevaluated together.
The column headed # represents the number of cases of descrip-tions classified as discourse-new in the standard annotation; +indicates the total num-ber of discourse-new descriptions correctly identified; - the number of errors.
Theseresults are for the version of the system that uses the best version of the heuristicsfor dealing with anaphoric descriptions discussed above, and that doesn't attempt oresolve bridging descriptions (Version 1).The performance of the specific heuristics discussed in Section 4.2 is shown inTables 12 to 15.
Table 12 shows the results of the heuristics for larger situation useson the training data, whereas Table 13 reports the performance on the same data of28 The law could also be interpreted as referring to "the law system in general," in which case none of theantecedents would be correct (or either could be taken as anchor for a bridging interpretation f thedefinite).569Computational Linguistics Volume 26, Number 4Table 12Evaluation of heuristics for larger situation uses (training data).Larger Situation Total Found Errors PrecisionNames 73 10 86%Time references 50 7 86%Premodification 41 19 54%Total 164 36 78%Table 13Evaluation of heuristics for unfamiliar uses (training data).Unfamiliar Total Found Errors PrecisionNP compl/Unexp mod 32 2 93%Apposition 27 2 92%Copula 8 2 75%Postmodification 197 18 91%Total 264 24 91%Table 14Evaluation of heuristics for larger situation uses (test data).Larger Situation Total Found Errors PrecisionNames 44 14 68%Time references 21 5 64%Premodification 17 9 47%Total 82 28 66%the heuristics for unfamiliar uses.
We report only precision figures because our stan-dard annotation only gives us information about the classification of these discoursedescriptions as discourse-new, not about the reason they were classified in a certainway (larger situation or unfamiliar).
The most common feature of discourse-new de-scriptions is postmodification; the least satisfactory results are those for proper namesin premodification.
As expected, the heuristics for recognizing unfamiliar uses (manyof which are licensed by linguistic knowledge) achieve better precision than those forlarger situation uses, which depend more on commonsense knowledge.Tables 14 and 15 summarize the results of the heuristics for discourse-new de-scriptions on the test data (Corpus 2).
Again, the best results were obtained by theheuristics for recognizing unfamiliar uses.
The biggest difference in performance wasshown by the heuristic hecking the presence of the definite in a copula construction,which performed very well on the training data, but poorly on the test data.
The actualperformance of that heuristic is difficult to evaluate, however, as a very low recall wasreported for both training and test data.In the following sections, we analyze some of the problems encountered by theversion of the system using these heuristics.Apposition.
Coordinated NPs with more than two conjuncts are a problem for thisheuristic, since in the Penn Treebank I, coordinated NPs have a structure that matchesthe pattern used by the system for recognizing appositions.
For example, the coordi-nated NP in the sentence G-7 consists of the U.S., Japan, Britain, West Germany, Canada,570Vieira and Poesio Processing Definite DescriptionsTable 15Evaluation of heuristics for unfamiliar uses (test data).Unfamiliar Total Found Errors PrecisionNP compl/Unexp mod 16 2 87%Apposition 10 2 80%Copula 6 4 33%Postmodification 95 22 77%Total 127 30 76%France and Italy has the structure in (51).
(51) \[NP, \[NP,the,U.S.\] ,,, \[NP, Japan\] ,,, \[NP,Britain\] ,,, \[NP,West,Germany\] ,,, \[NP, Canada\],,,  \[NP, France\], and, \[NP, Italy\] \]Copula.
This heuristic was difficult to evaluate because there few examples, and theprecision in the two data sets is very different (see Tables 13 and 15 above).
Oneproblem is that the descriptions in copula constructions might also be interpretedas bridging descriptions.
For instance, the description the result in (52a) below is theresult of something mentioned previously, while the copula construction specifies itsreferent.
Other ambiguous examples are (52b) and (52c):(52) a.
The result is that those rich enough to own any real estate at all haveboosted their holdings ubstantially.b.
The chief culprits, he says, are big companies and business groups thatbuy huge amounts of land not for their corporate use, but for resale athuge profit.c.
The key man seems to be the campaign manager, Mr. Lynch.Restrictive premod~cation.
One problem with this heuristic is that although propernouns in premodifier positions are often used with discourse-new definites (e.g.,the Iran-Iraq war), they may also be used as additional information in associative oranaphoric uses:(53) Others grab books, records, photo albums, sofas and chairs, workingfrantically in the fear that an aftershock will jolt the house again.As Ms. Johnson stands outside the Hammack house after winding up herchores there, the house begins to creak and sway.Restrictive postmodification.
If the system fails to find an antecedent or anchor and thedescription is postmodified, it may wrongly be classified as discourse-new.
In (54) thefiling on the details of the spinoff was classified as bridging on documents filed ... by thecoders, but the system classified it as discourse-new.
(54) Documents filed with the Securities and Exchange Commission on the pendingspinoff disclosed that Cray Research Inc. will withdraw the almost $100million in financing it is providing the new firm if Mr. Cray leaves or ifthe product-design project he heads is scrapped.571Computational Linguistics Volume 26, Number 4The filing on the details of the spinoff caused Cray Research stock to jump$2.875 yesterday to close at $38 in New York Stock Exchange compositetrading.Proper nouns.
As we have already seen--(46), repeated below as (55)--a definite de-scription that looks like a proper noun (the petite, 29-year-old Ms. Johnson) may in factbe anaphoric.
This is not always a problem, as the system does attempt to find an-tecedents for these definites, as well, but if the antecedent is not found (as in theexample below) the description is incorrectly classified as discourse-new.
(55) Toni Johnson pulls a tape measure across the front of what was once astately Victorian home.The petite, 29-year-old Ms. Johnson ...Special predicates?
In this example the system classified as discourse-new a time refer-ence (the same time), which is classified as bridging in the standard annotation.
(56) Newsweek's circulation for the first six months of 1989 was 3,288,453, flatfrom the same period last year.U.S.
News' circulation in the same time was 2,303,328, down 2.6%.5.4 Bridging DescriptionsAs mentioned in Section 2, our corpus annotation experiments showed bridging de-scriptions to be the most difficult class for humans to agree on.
Even when our anno-tators agreed that a particular expression was a bridging description, different anchorswould be available in the text for the interpretation f that bridging description?
Thismakes the results of the system for this class very difficult to evaluate; furthermore,the results must be evaluated by hand?We first tested the heuristics individually on the training data (the same dataused in a previous analysis of the performance ofour system on bridging descriptions\[Vieira nd Teufel 1997\]) by adding them to Version I of the system one at a time.
Theseseparate tests were manually evaluated?
We then integrated all of these heuristics intoa version of the system called Version 2, using both automatic and manual evaluation.In this section we discuss only the results of the individual heuristics; the overallresults of Version 2 are discussed in Section 6.Bridging descriptions are much more sensitive than other types of definite de-scriptions to the local focus (Sidner 1979); for this reason, Version 2 uses a differentsearch strategy for bridging descriptions than for other definite descriptions.
Ratherthan considering all definite descriptions in the current window simultaneously, it goesback one sentence at a time and stops as soon as a relation with a potential anchor isfound.5.4.1 Using WordNet to Identify Anchors.
Our system consults WordNet o determineif a definite description may be semantically related to one of the NPs in the previous572Vieira and Poesio Processing Definite DescriptionsTable 16Evaluation of the search for anchors using WordNet.Bridging Class Relations Found Right Anchors % RightSynonymy 11 4 36%Hyponymy 59 18 30%Meronymy 6 2 33%Sister 30 6 20%Total 106 30 28%five sentences.
29 The results of this search over our training corpus, in which 204descriptions were classified as bridging, are shown in Table 16.
It is interesting tonote that the semantic relations found in this automatic search were not always thoseobserved in our manual analysis.The main reason the figures are so low is that the existence of a semantic relationin WordNet is not a sufficient condition (nor a strong indication) to establish a linkbetween an antecedent and a bridging description.
In only about a third of the caseswas a potential antecedent for which we could find a semantic relation in WordNet anappropriate anchor.
An example is (57): although there is a semantic relation betweenargument and information in WordNet, the description the argument is related to theVP contend rather than to the NP information.
Some form of focusing seems to play acrucial role in restricting the range of antecedents ( ee also the discussion in Hitzemanand Poesio \[1998\]).
(57) A SEC proposal to ease reporting requirements for some companyexecutives would undermine the usefulness of information on insidertrades as a stock-picking tool, individual investors and professionalmoney managers contend.They make the argument in letters to the agency about rule changesproposed this past summer that, among other things, would exemptmany middle-management executives from reporting trades in their owncompanies' shares.Sense ambiguity is responsible for some of the false positives.
For instance, thenoun company has at least two distinct senses: "visitor" (as in Ihave company) and "busi-ness."
A relation of hypernymy was found between company and human (its "visitor"sense), whereas in the text the noun company was used in the "business" sense.
Amore important problem, however, is the incompleteness of the information encodedin WordNet.
To have an idea of how complete the information in WordNet is con-cerning the relations that are encoded, we selected from our two corpora 70 bridgingdescriptions that we had manually identified as being linked to their anchors by one ofthe semantic relations encoded in WordNet--synonymy, hypernymy (hyponymy), andmeronymy (holonymy).
In Table 17 we show the percentages of such relations actuallyencoded in WordNet.
(The fourth column in the table indicates the cases in which theexpected relation is not encoded, but the two nouns are sisters in the hierarchy.
)As we can see from the table, the recall figure was quite disappointing, especiallyfor synonymy relations.
In some cases, the problem was simply that some of the29 We found that for bridging descriptions, a five-sentence window worked better than a four-sentenceone.573Computational Linguistics Volume 26, Number 4Table 17Evaluation of the encoding of semantic relations in WordNet.Bridging Class Anchor/DD Pairs Found in WN Found Sister %Syn 20 5 2 35%Hyp 32 17 1 56%Mer 18 5 2 38%Total 70 27 5 46%artifactI is_astructurehousinglodgingi s -~"x .~.
-ahouse homebuildingedificepart_ofroompart_~"x...part_ofwall floorFigure 2An example of problematic organization i WordNet.words we looked for were not in WordNet: examples include newsweekly (news-weekly),crocidolite, countersuit (counter-suit).
Other times, the word we looked for was containedin WordNet, but not in the same typographic format as it was presented in the text;for example we had spinoff in a text, whereas WordNet had only an entry for spin-off.
A second source of problems was the use in the WSJ articles of domain-specificterminology with context-dependent senses, such as slump, crash, and bust, which inarticles about the economy are all synonyms.
Finally, in other cases the relations weremissing due to the structure of WordNet: for instance, in WordNet the nouns room,wall, and floor are encoded as part of building but not of house (see Figure 2).In summary, our tests have shown that the knowledge encoded in WordNet isnot sufficient o interpret al semantic relations between a bridging description andits antecedent found in the kind of text we are dealing with: only 46% of the rela-tions observed were encoded in WordNet.
The possibility of using domain-specific,automatically acquired lexical information for this purpose is being explored: see, forexample, Poesio, Schulte im Walde, and Brew (1998).
In addition, we found that justlooking for the closest semantic relative is not enough to find anchors for bridgingdescriptions; this search has to be constrained by some type of focusing mechanism.5.4.2 Evaluating the Results for Bridging Descriptions Based on Proper Names.Identifying named entity types is a prerequisite for resolving descriptions based onnames.
The simple heuristics discussed in Section 5.4 identified entity types for 66%574Vieira and Poesio Processing Definite Descriptions(535/814) of all names in the corpus (organizations, persons, and locations); precisionwas 95%.
30 The errors we found were sometimes due to name or sense ambiguity.
Inthe same text a name may refer both to a person and a company, as in Cray Comput-ers Corp. and Seymour Cray.
When looking in WordNet for a type for the name SteveReich we found for the name Reich the type country.
These problems have also beennoted by the authors of systems participating in MUC-6 (Appelt 1995).
We also foundundesirable relations uch as hypernymy for person and company.5.4.3 Evaluating the Results for Bridging Descriptions Based on Compound Nouns.We had 25 definite descriptions manual ly  identified as based on compound nouns.For these 25 cases, our implemented heuristics achieved a recall of 36% (9/25) but,in some cases, found valid relations other than the ones we identified.
The low recallwas sometimes due to segmentation.
Sometimes the spelling of the premodif icationwas slightly different from the one of the description, as in a 15-acre plot.. ,  the 15 acres.6.
Overall Evaluation of the SystemAs mentioned above, we implemented two versions of the system.
Version 1 onlyresolves direct anaphora nd identifies discourse-new descriptions; Version 2 also dealswith bridging descriptions.
Both versions of the system have at their core a decisiontree in which the heuristics discussed in the previous sections are tried in a fixed orderto classify a certain definite description and find its anchor.
Determining the optimalorder of application of the heuristics in the decision tree is crucial to the performanceof the system.
In both versions of the system we used a decision tree developed byhand on the basis of extensive valuation; we also attempted to determine the orderof application automatically, by means of decision tree learning algorithms (Quinlan1993).In this section we first present he hand-crafted ecision tree and the results ob-tained using this decision tree for Version 1 and Version 2; we then present he resultsconcerning the agreement between system and annotators, and we briefly discuss theresults obtained using the decision tree acquired automatically.6.1 Integration of the HeuristicsThe hand-crafted order of the heuristics in both versions is the following.
For each NPof the input,.2.The system assigns an index to it.The NPs that may serve as potential antecedents are made available fordescription resolution by means of the optimal selection criteriondiscussed in Section 4.1.30 By comparison, the systems participating in MUC-6 had a recall for the named entity task rangingfrom 82% to 96%, and precision from 89% to 97%, but used comprehensive lists of cue words orconsulted ictionaries of names.
The system from Sheffield (Gaizauskas et al 1995), for instance, used alist of 2,600 names of organizations, 94company designators (Co., Ltd, PLC, etc.
), 160 titles (Dr., Mr.,etc.
), about 500 human ames from the Oxford Advanced Learner's Dictionary, 2,268 place names(country, province, and city names), and other trigger words for locations, government institutions andorganizations (Golf, Mountain, Agency, Ministry, Airline, etc.).
In MUC-7, the best combined precisionscore, 93.39%, was achieved by the system from LTG in Edinburgh (Mikheev, Moens, and Grover 1999),which doesn't use such knowledge sources.
We used this system in a version of our prototype thatonly attempts to resolve bridging descriptions (Ishikawa 1998).575Vieira and Poesio Processing Definite DescriptionsSpec-PredY/1NY/3NY/3NY/3Spec-Pred = special predicateAppos = appositionDir-Ana = same head antecedentPropN = proper nounRPostm = restrictive postmodificationRPrem = restrictive premodificationCopC = copular constructionNY/3N1 Direct anaphora2 Bridging3 Discourse new~C\NN2 FailFigure 3Hand-designed decision tree for Version 1 and Version 2.?
only then try to interpret he definite description as a bridge (last test).The heuristics for recognizing bridging descriptions are only applied when theother heuristics fail.
This is because the performance of these heuristics is very poorand also because some of the heuristics that deal with bridging descriptions are com-putationally expensive; the idea was to eliminate those cases less likely to be bridg-ing before applying these heuristics.
The system does not classify all occurrences ofdefinite descriptions: when none of the tests succeeds, the definite description is notclassified.
We observed in our first tests that definite descriptions not resolved as directanaphora nd not identified as discourse-new by our heuristics were mostly classifiedin the standardized annotation as bridging descriptions or discourse-new.
Examplesof discourse-new descriptions not identified by our heuristics are larger situation usessuch as the world, the nation, the government, the economy, the marketplace, the spring, the577Vieira and Poesio Processing Definite DescriptionsTOTAL TYPES IDENTIF IED BY THE SYSTEManaphoric:  270larger s it .
/unfam: 428total: 698TOTAL NON CLASSIF IEDanaphoric:  41larger s it .
/unfam: 113associat ive:  162idiom: 20doubt: 6total: 342TOTAL TYPES CLASSIF IED BY HANDanaphoric:  312larger sit .
/unfam: 492associat ive:  204idiom: 22doubt: i0total: 1040Figure 5Summary of the results of Version 1 on training data.Table 18Global results of Version 1 on training data.System's tasks R P FAnaphora classification 78% 90% 83%Anaphora resolution 76% 88% 81%Discourse-new 75% 86% 80%Overall 59% 88% 70%Table 19Evaluation of Version l on the test data.System's tasks R P FAnaphora classification 67% 90% 77%Anaphora resolution 62% 83% 71%Discourse-new 69% 72% 70%Overall 53% 76% 63%The recall and precision figures for the system's performance over the test data arepresented in Table 19.
This corpus consisted of 14 texts, containing 2,990 NPs.
Again,almost half of the NPs were considered as potential antecedents.
The system processed464 defir~te descriptions; of these, the system could classify 324:115 as direct anaphora,209 as discourse-new.
Of the antecedents, 88 were definites themselves.
The systemincorrectly resolved 77 definite descriptions: 19 anaphoric definites and 58 discourse-new.
As before, there were just a few more errors in anaphora resolution than inanaphora classification.
The overall recall for the test data was 53% (247/464); precisionwas 76% (247/324).One difference between the results on the two data sets is the distribution intoclasses of those descriptions that the system fails to classify.
In the first corpus, thelargest number of cases not classified are bridging descriptions.
By contrast, the largestnumber of cases not classified by the system in Corpus 2 are discourse-new.579Computational Linguistics Volume 26, Number 4NR.
OF TEXTS:  14 NR.
OF NOUN PHRASES:  2990NR.
OF ANTECEDENTS CONSIDERED:  1226Indefinites: 657Possessives: 144Def in i tes :  425NR.
OF DEF IN ITE  DESCRIPT IONS:  464D IRECT ANAPHORA:  115 ANTECEDENTS FOUND: Indef in i tes :  21Possessives: 6Def in i tes :  88D ISCOURSE NEW DESCRIPT IONS:  209LARGER S ITUAT ION USES:  82 UNFAMIL IAR USES : 127NAMES : 44 NP  COMP.
/UN.MOD.
:  16T IME REFERENCES : 21 APPOSIT IONS : i0REST.PREMOD.
: 17 REST.
POSTMOD.
: 95COPULA : 6NON- IDENTIF IED:  140TOTAL  EST IMATED ERRORS (for anaphora  c lass i f i ca t ion)  : 12TOTAL  EST IMATED ERRORS (for anaphora  reso lu t ion)  : 19TOTAL  EST IMATED ERRORS (for la rger  s i tuat ion /unfami l ia r ) :  58Figure 6Global results of Version 1 on test data,TOTAL TYPES IDENTIF IED BY  THE SYSTEManaphoric: 115la rger  s i t .
/un fam:  209total :  324TOTAL  NON CLASSIF IEDanaphoric: 29la rger  s i t .
/un fam:  61assoc ia t ive :  46doubt :  4total :  140TOTAL  TYPES CLASS IF IED BY  HANDanaphoric: 154la rger  s i t .
/un fam:  218assoc ia t ive :  81doubt :  I itotal :  464Figure 7Summary of the results for test data.6.3 Results for Bridging DescriptionsAs discussed in Section 5.4, the results of the heuristics for bridging descriptions pre-sented in Section 4.3 were not very good.
We nevertheless included these heuristics inVersion 2 of the system, which, as discussed above, applied them to those descriptionsthat failed to be recognized as direct anaphora or discourse-new.
The heuristics wereapplied in the following order:1. proper names,580Vieira and Poesio Processing Definite DescriptionsTable 20Evaluation of the bridging heuristics all together.Bridging Found by System FalseClass PositiveNames 12 14Common ouns 15 10WordNet 34 76Total 61 100Table 21Comparative evaluation of the two versions (test data).System's versions R P FV.1 Overall 53% 76% 62%V.2 Overall 57% 70% 62%2.
compound nouns,3.
WordNet,Training Data.
The manual evaluation of the results of Version 2 on the training datais presented in Table 20.
The table lists the number of acceptable anchors and thenumber of false positives found by each heuristic.
Note that the system sometimesfinds anchors that are not those identified manually, but are nevertheless acceptable.We found fewer bridging relations than the number we observed in the corpusanalysis (204); furthermore, the number of false positives produced by such heuristicsis almost wice the number of right answers.Test Data.
Version 2 was tested over the test data using automatic evaluation--i.e., thesystem was only evaluated as a classifier, and the anchors found were not analyzed.A total of 57 bridging relations were found, but only 19 of the definite descriptionsclassified as bridges by the system had been classified as bridging descriptions in thestandard annotation.
Compared to Version 1 of the system, which does not resolvebridging descriptions, Version 2 has higher recall but lower precision, as shown inTable 21.6.4 Agreement  among System and Annotators for Version 1 and Version 2As a second form of evaluation of the performance of the system, we measured itsagreement with the annotators on the test data using the K statistic.Version 1 of the system finds a classification for 318 out of 464 definite descrip-tions in Corpus 2 (the test data).
If all the definite descriptions that the system cannotclassify are treated as discourse-new, the agreement between the system and the threesubjects that annotated this corpus on the two classes first-mention (= discourse-old)and subsequent-mention (= discourse-new or bridges) is K = 0.7; this should be com-pared with an agreement of K = 0.77 between the three annotators themselves.
If,instead of counting these definite descriptions as discourse-new, e simply do notinclude them in our measure of agreement, then the agreement between the systemand the annotators i  K = 0.78, as opposed to K = 0.81 between the annotators.
(Noticethat the fact that the agreement between annotators goes up, as well, indicates thatthe definite descriptions that the system can't handle are "harder" than the rest.
)581Computational Linguistics Volume 26, Number 4Version 2 finds a classification for 355 out of 464 definite descriptions; however,its agreement figures are worse.
If we count the cases that the system can't classifyas discourse-new, the agreement between the system and the three annotators for thethree classes is K = 0.57; if we count hem as bridges, K = 0.63; if we just discard thosecases, K = 0.63 again.
(By comparison, the agreement among annotators on the threeclasses was K -~ 0.68 overall and K = 0.70 on just the cases that the system was ableto classify.)
As mentioned above, the cases that the system can't handle are mainlydiscourse-new descriptions ( ee Figure 7).6.5 Deriving the Order of Application of the Heuristics Automatically6.5.1 Inducing a Decision Tree.
The decision tree discussed in Section 6.1 was derivedmanually, by trial and error.
We also tried to derive the order of application of theheuristics automatically.
To do this, we used a modified version of the system toassign Boolean feature values to each definite description in the training corpus (i.e.,the system checked if the features applied to a definite description instance or not).The following features were used:...4.5.Special predicates (Spec-Pred): this feature has the value yes if a specialpredicate occurs in the definite description (as specified in Section 4.2),and if a complement is there when needed.Direct anaphora (Dir-Ana): this feature has the value yes if the systemcan find an antecedent with a same-head noun for that description(respecting the constraints discussed in Section 4.1).Apposition (Appos): yes when the description is in appositiveconstruction.Proper noun (PropN): yes when the description has a capitalized initial.Restrictive postmodification (RPostm): yes if the definite description ismodified by relative or associative clauses.This list of features, together with the classification assigned to each description ithe standard annotation (DDUse), was used to train an implementation f Quinlan'slearning algorithm ID3 (Quinlan 1993).
We excluded the verification of restrictive pre-modification and copula constructions, since these parameters had given the poorestresults before (see Section 6.2).
An example of the samples used to train ID3 is shownin (58).
(58) Spec-Pred Dir-Ana Appos PropN RPostm DDUseno no  no  yes  no  3no no  no  no  yes  3no no no  no  no  2no no  no  no  no  2no no no  no  no 1no yes  no  no  no  1The algorithm generates a decision tree on the basis of the samples given.
The resultingdecision tree is presented in Figure 8.The main difference between this algorithm and the algorithm we arrived at byhand is that the first feature checked by the decision tree generated by ID3 is thepresence of an antecedent with a same-head noun.
The presence of special predicates,which we adopted as the first test in our decision tree, is only the fourth test in thetree in Figure 8.582Vieira and Poesio Processing Definite DescriptionsDir-Ana Dir-Ana = same head antecedent/////NNNNN RPostm = restrictive postmodification/ /~stm Appos = appositionSpec-Pred =special predicate1 /~// ~ &  N PropN = proper noun3 ~ec-Pred2 Bridging3 Discourse new 3 2Figure 8Generated ecision tree.6.5.2 Evaluation of the Automatically Learned Decision Tree.
The performance ofthe learned decision tree was compared with that of the algorithm we arrived at bytrial and error as follows: The first 14 texts of Corpus 1 (845 descriptions) were usedas training data to generate the decision tree.
We then tested the learned algorithmover the other 6 texts of that corpus (195 instances of definite descriptions).Two different ests were undertaken:first, we gave as input to the learning algorithm all cases classified asdirect anaphora, discourse-new, or bridging, 818 in total (this testproduces the decision tree presented in the previous section);in a second test, the algorithm was trained only with direct anaphoraand discourse-new descriptions (639 descriptions); all cases classified asbridging, idiom, or doubt in the standard annotation were not given asinput in the learning process.
This algorithm was then only able toclassify descriptions as one of those two classes.
The resulting decisiontree classifies descriptions with a same-head antecedent as anaphoric; allthe rest as discourse-new.Here we present he results evaluated all together, considering the system as aclassifier only, i.e., without considering the tasks of anaphora resolution and of identi-fication of discourse-new descriptions separately.
The output produced by the learnedalgorithm is compared to the standard annotation.
Since the learned algorithm classi-fies all cases, the number of responses i equal to the number of cases, as a consequence,recall is the same as precision, and so is the F measure.The tests over 6 texts with 195 definite descriptions gave the following results:?
R = P = F = 69% when the algorithm was trained with three classes;?
R = P = F = 75%, when training with two classes only.583Computational Linguistics Volume 26, Number 4The best results were achieved by the algorithm trained for two classes only.This is not surprising, especially considering how difficult it was for our subjects todistinguish between discourse-new and bridging descriptions.The hand-crafted decision tree (Version 2) achieved 62% recall and 85% precision(F = 71.70%) on those same texts: i.e., a higher precision, but a lower F measure, dueto a lower recall, since---unlike the learned algorithm--it does not classify all instancesof definite descriptions.
If, however, we take the class discourse-new as a default forall cases of definite descriptions not resolved by the system, recall, precision, and Fvalue go to 77%, slightly higher than the rates achieved by the decision tree producedby ID3.As the learned decision tree has the search for a same-head antecedent as the firsttest, we modified our algorithm to work in the same way, and tested it again with thetwo corpora.
The results with this configuration were:?
R = 0.75, P = 0.87, F = 0.80, for the training data (compared withR = 0.76, P = 0.88, F = 0.81) ;?
R = 0.59, P = 0.83, F = 0.69, for the test data (compared with R = 0.62,P = 0.83, F = 0.71).In other words, the results were about the same, although a slightly better performancewas obtained when the tests to identify discourse-new descriptions were tried first.7.
Other Computational Models of Definite Description ProcessingA major difference between our proposal and almost all others (theoretical and im-plemented) is that we concentrate on definite descriptions; most of the systems wediscuss below attempt o resolve all types of anaphoric expressions, often concentrat-ing on pronouns.
Focusing on definite descriptions allowed us to investigate whattypes of lexical knowledge and commonsense inference are actually used in naturallanguage comprehension.From an architectural standpoint, he main difference between our work and otherproposals in the literature is that we paid considerably more attention to the problemof identifying discourse-new definite descriptions.
32Previous work on computational methods for definite description resolution canbe divided in two camps: proposals that rely on commonsense r asoning (and aretherefore ither mainly theoretical or domain dependent), and systems that can bequantitatively evaluated, such as those competing on the coreference task in the Sixthand Seventh Message Understanding Conference (Sundheim 1995).
We discuss thesetwo types of work in turn.7.1 Models Based On Commonsense ReasoningThe crucial characteristic of these proposals is that they exploit hand-coded common-sense knowledge, and cannot therefore be tested on just any arbitrary text.
Some ofthem are simply tested on texts that were especially built for the purpose of testingthe system (Carter 1987; Carbonell and Brown 1988); systems like the Core LanguageEngine are more robust, but they have to be applied to a domain restricted enoughthat all relevant knowledge can be encoded by hand.32 This problem is also a central concern in the work by Bean and Riloff (1999).584Vieira and Poesio Processing Definite DescriptionsSidner's Theory of Definite Anaphora Comprehension.
I  her dissertation, Sidner (1979)proposed a complete theory of definite NP resolution, including detailed algorithmsfor resolving pronouns, anaphoric definite descriptions, and bridging descriptions.
Shealso proposed methods for resolving larger situation uses; the one class her methodsdo not handle are those definite descriptions that, following Hawkins, we have calledunfamiliar uses.The main contribution of Sidner's dissertation is her theory of focus and its rolein resolving definite NPs; to this day, her focus-tracking algorithms are arguably themost detailed account of the phenomenon.
The main problem with Sidner's work fromour perspective is that her algorithms rely heavily on the availability of a semanticnetwork and causal reasoner; furthermore, some of the inference mechanisms are leftrelatively underspecified (this latter problem was in part corrected in subsequent workby Carter--see below).
Lexical and con~nonsense knowledge play three importantroles in Sidner's system: they are used to track focus, to resolve bridging descriptionsand larger situation uses, and to evaluate interpretive hypotheses, discarding thosethat seem implausible.
Only recently have robust knowledge-based methods for someof these tasks begun to appear, and their performance is still not very good, as seenabove in our discussion of using WordNet as a semantic network; 33 as for checkingthe plausibility of a hypothesis on the basis of causal knowledge about the world, wenow have a much better theoretical grasp of how such inferences could be made (see,for example, Hobbs et al \[1993\] and Lascarides and Asher \[1993\]), but we are stillquite a long way from a general inference ngine.We also found that some of Sidner's resolution rules are too restrictive.
For ex-ample, her Cospecification rule 1 prescribes that definite description and focus musthave the same head, and no new information can be introduced by the definite; butthis rule is violated fairly frequently in our corpus.
This criticism is not new: In 1983,it was already recognized that an anaphoric full noun phrase may include some newand unshared information about a previously mentioned entity (Grosz, Joshi, and We-instein 1983), and Carter (1987) weakened some of the restrictions proposed by Sidnerin his system.Carter's Shallow Processing Anaphor Resolver.
Carter (1987) implemented a modified ver-sion of Sidner's algorithm and integrated it with an implemented version of Wilks'theory of commonsense r asoning.
This work is interesting for two reasons: first of all,because Carter, unlike Sidner, attempted to evaluate the performance ofhis system; andbecause, in doing so, he addressed the commonsense r asoning problem in some detail.Carter's ystem, SPAR, is based on the Shallow Processing Hypothesis: that in re-solving anaphors, reasoning should be avoided as much as possible.
This is, of course,the same approach taken in our own work, which could be seen as pushing Carter's ap-proach to the extreme.
The difference is that when it becomes necessary, SPAR does usetwo commonsense knowledge sources: a semantic network based on Alshawi's theoryof memory for text interpretation (Alshawi 1987) and a causal reasoner based on Wilks'work (Wilks 1975).
In both cases, the necessary information was encoded by hand.Carter's system was tested over short stories specifically designed for the testingof the system: about 40 written by Carter himself, and 23 written by others.
Theselatter contain about 80 definite descriptions.
SPAR correctly resolved all anaphors inthe stories written by Carter, and 66 out of 80 of the descriptions in the 23 other stories.33 An implementation f a (simplified) version of Sidner's focus-tracking algorithms capable of beingused by a system like ours was presented in Azzam, Humphreys, and Gaizauskas (1998).585Computational Linguistics Volume 26, Number 4(Carter himself points out that these results are "of limited significance because of thesimplicity of the texts processed compared to 'real' texts" \[p.
238\].
)The Core Language Engine.
The Core Language Engine (CLE) (Alshawi 1992) is a domain-independent system developed at SRI Cambridge, which translates English sentencesinto formal representations.
The system was used by SRI for a variety of applications,including spoken language translation and airline reservations.
The CLE makes use ofa core lexicon (to which new entries can be added) and uses an abductive common-sense reasoner to produce an interpretation a d to verify the plausibility of choice ofreferents from an ordered list; the required world knowledge has to be added by handfor each domain, together with whatever lexical knowledge is needed.The construction of the formal representation goes through an intermediate stagecalled quasi-logical form (QLF).
The QLF may contain unresolved terms correspond-ing to anaphoric NPs including, among others, definite descriptions.
The resolutionprocess that transforms QLFs into resolved logical form representations of entences idescribed in Alshawi (1990).
Definite descriptions are represented asquantified terms.The referential readings of definite descriptions are handled by proposing referentsfrom the external application context (larger situation uses) as well as the CLE contextmodel (anaphoric uses).
Attributive readings may also be proposed uring QLF reso-lution; some of these seem to correspond to our unfamiliar uses.
Thus, the CLE seemsto account for discourse-new descriptions, although they are not explicitly mentioned,and the methods used for choosing a referential or an attributive interpretation arenot discussed.
To our knowledge, no analysis of the performance of the system hasbeen published.7.2 The Systems Involved in the MUC-6 Coreference TaskThe seven systems that participated in the MUC-6 competition can all be quantitativelyevaluated; they achieved recall scores ranging from 35.69% to 62.78% and precisionscores ranging from 44.23% to 71.88% on nominal coreference.It is important to note that the evaluation in MUC-6 differed from ours in threeimportant aspects.
First of all, these systems have to parse the texts, which often in-troduces errors; furthermore, these systems often cannot get complete parses for thesentences they are processing.
Secondly, the evaluation i  MUC-6 considers the coref-erential chain as a whole, and not only one correct antecedent.
The third difference isthat these systems process a wider range of referring expressions, including pronounsand bare nouns, while our system only processes definite NPs.
On the other hand, notall definite descriptions are marked in the MUC-6 coreference task: these systems areonly required to identify identity relations, and only if the antecedent was introducedby a noun phrase (not if it was a clause or a conjoined NP).
This leaves out discourse-new descriptions and, especially, bridging descriptions, which, as we have seen, areby far the most difficult cases.Kameyama (1997) analyzes in detail the coreference module of the SRI systemthat participated in MUC-6 (Appelt et al 1995).
This system achieved one of the topscores for the coreference task: a recall of 59% and a precision of 72%.
The SRI systemuses a sort hierarchy claimed to be sparse and incomplete.
For definite descriptions,Kameyama reports the results of a test on five articles, containing 61 definite descrip-tions in total; recall was 46% (28/61), and for proper names, 69% (22/32).
The precisionfigures for these two subclasses are not reported.
Some of the errors in definite de-scriptions are said to be due to nonidentity referential relations; however, there is nomention of differences between discourse-new and bridging descriptions.
Other errorswere said to be related to failure in recognizing synonyms.586Vieira and Poesio Processing Definite Descriptions7.3 Probabilistic Methods in Anaphora ResolutionAone and Bennet (1995) propose an automatically trainable anaphora resolution sys-tem.
They train a decision tree using the C 4.5 algorithm by feeding feature vectorsfor pairs of anaphor and antecedent.
They use 66 features, including lexical, syntac-tic, semantic, and positional features.
Their overall recall and precision figures are66.56% and 72.18%.
Considering only definite NPs whose referent is an organization(that is the only distinction available in their report), recall is 35.19% and precision50% (measured on 54 instances).
Their training and test texts were newspaper articlesabout joint ventures, and they claim that because ach article always talked aboutmore than one organization, finding the antecedents of organizational naphora wasnot straightforward.In Burger and Connolly (1992) a Bayesian network is used to resolve anaphoraby probabilistically combining linguistic evidence.
Their sources of evidence are c-command (syntactic onstraints), semantic agreement (gender, person, and numberplus a term subsumption hierarchy), discourse focus, discourse structure, recency, andcentering.
Their methods are described and exemplified but not evaluated.
A Bayesianframework is also proposed by Cho and Maida (1992) for the identification of definitedescriptions' referents.8.
Conclusions and Future Work8.1 ContributionsWe have presented a domain-independent system for definite description interpreta-tion whose development was based on an empirical study of definite description usethat included multiannotator experiments.
Our system not only attempts to find anantecedent for a definite description, it also uses methods for recognizing discourse-new descriptions, which our previous studies revealed to be the largest class of def-inite descriptions in our corpus.
Our algorithms for segmentation, matching, andidentification of discourse-new descriptions only rely on syntax-based heuristics andon on-line lexical sources such as WordNet; the final configuration of these heuris-tics, as well as their order of application, was arrived at on the basis of extensiveexperiments using our training corpus.
Because our system only relies on "shal-low" information, it encounters problems when commonsense r asoning is actuallyneeded; on the other hand, it can be tested on any domain without extensive hand-coding.As far as direct anaphora is concerned, we evaluated heuristic algorithms forsegmentation and matching.
Our system achieved 62% recall and 83% precision fordirect anaphora resolution on our test data.
For identifying discourse-new descriptions,we exploited the correlation between certain types of syntactic onstructions and typeof use noted by Hawkins (1978) and semantically explained by L6bner (1987).
Oursystem achieved 69% recall and 72% precision for this class on the test data.
Overall,the version of the system that only attempts to recognize first-mention and subsequent-mention definite descriptions achieved a recall of 53% and a precision of 76% on thetest corpus if we count the definite descriptions the system can't handle as errors; ifwe count them as discourse-new, both recall and precision are 66%.The class of bridging descriptions i  the most difficult to process: this is in partbecause humans themselves do not agree much on which definites count as bridgesand what their anchors are, in part because lexical knowledge and commonsensereasoning are necessary to solve them.
Our results for this class are, therefore, stillvery tentative; this did not much affect the performance of the system, however, sincein the texts we tried, bridging descriptions are a relatively small class.
Noncoreferent587Computational Linguistics Volume 26, Number 4bridging descriptions were around 8% of the definite descriptions in the corpus, andthe class of bridging descriptions including those with a coreferent antecedent witha different head noun were about 15% of the total.
We tried techniques that do notinvolve heavy axiomatization f commonsense knowledge, and only used an existinglexical source, WordNet.In other text genres the distribution of definite descriptions into classes mightchange; spoken dialogue, for example, tends to have a higher number of deictic def-inite descriptions.
However, other researchers (Fraurud 1990) found a similar distri-bution of first-mention and subsequent-mention definites in text corpora; we believetherefore that the heuristics we propose here, and their ordering, will still be ade-quate.
Direct anaphora nd discourse-new descriptions can be processed with muchsimpler methods and it seems that the distinguishing features do not usually over-lap.8.2 What's Needed Next?We would like to emphasize again that we are not trying to suggest hat shallowmethods will be sufficient for processing definite descriptions in the long run.
Whatwe do believe is that hypotheses about processing should be evaluated; unfortunately,only fairly simple techniques can be tested in this way at the moment, but this workcan serve to motivate more clearly the use of more complex methods.We highlighted throughout the paper, and particularly in Section 5, some of thepoints where shallow methods break down, and better lexical sources or commonsenseknowledge are needed.
By far the worse results are obtained for bridging descriptions;in this area, the most urgent needs are better sources of lexical knowledge, 34 and somerobust focusing mechanism.
Finding better ways of segmenting the text is perhaps thearea in which the most progress has been made since we started this project; robustmethods for text segmentation are now available (Hearst 1997; Richmond, Smith, andAmitay 1997).
A proper treatment of modification seems harder; as discussed in Sec-tion 4.1, it seems necessary to rely heavily on reasoning in some cases.
In order toimprove our treatment of discourse-new descriptions it will be necessary, on the onehand, to find ways of automatically acquiring lexical information about the function-ality of nouns and adjectives, and on the other hand, to have sources of encyclopedicknowledge available.8.3 Future Work8.3.1 Simple Extensions.
In this project we were more interested in clearly identify-ing the subtasks of the definite description process that in achieving optimum per-formance; as a consequence, there are a number of fairly simple ways in which thefinal version of the system could be improved.
The next step in making our systemtruly testable on any type of text would be to make it work off the output of a robustparser: we are currently testing Abney's CASS parser (Abney 1991) for this purpose.See Ishikawa (1998), for some initial results.
We are also experimenting with existingsoftware that performs in a more sophisticated way some of the tasks that our systemcurrently implements in a fairly crude fashion, including lemmatization, proper namerecognition, and named entity typing.Another aspect of the system that deserves further examination is the constructionof coreference chains and cases of multiple resolutions.
We did not get a clear picture34 As mentioned above, we have done some preliminary work on acquiring this informationautomatically (Poesio, Schulte im Walde, and Brew 1998; Ishikawa 1998).588Vieira and Poesio Processing Definite Descriptionsof how complete or incomplete, or how broken, the coreferential chains resulting fromthe processing of one text are, nor did we relate them to the chains of the annotatedtexts; to do so, the system and the annotation would have to be extended to cover allcases of anaphoric expressions.8.3.2 The Role of Focus in Definite Descriptions Processing.
Our tests with bridgingdescriptions resulted in a great number of false positives.
Our analysis of these data,as well as of other corpora (Hitzeman and Poesio 1998), suggests that a local focusingmechanism as proposed in Grosz (1977), Sidner (1979), Grosz, Joshi, and Weinstein(1983, 1995), and Grosz and Sidner (1986) would improve the results obtained by oursystem.There are several reasons why our system does not yet include such a mechanism.One problem already mentioned is that Sidner's algorithms as stated, and even asimplemented by Carter, are difficult to implement, since considerably more lexicalinformation is needed than we have available (e.g., about the thematic roles of verbs),a rich knowledge base is needed both to resolve bridging descriptions and largersituation uses, and commonsense inference is needed to evaluate the plausibility ofhypotheses.
A second problem with Sidner's theory of local focus, as well as otherssuch as Centering Theory (Grosz, Joshi, and Weinstein 1995), is the lack of a precisecharacterization f how to deal with complex sentences.
Revisions and extensions ofSidner's proposal related to these problems have been proposed in Suri and McCoy(1994), and include algorithms for updating focus in complex sentences containingadjunct clauses uch as before- and after-clauses.We plan to incorporate simpler focus-tracking mechanisms in future versions ofthe system, possibly along the lines of Azzam, Humphreys, and Gaizauskas (1998) orTetreault (1999).8.3.3 Theoretical Developments.
We defended the importance of developing methodsfor identifying discourse-new descriptions, and we believe that there is still need forresearch into the semantics of this class; that is, what, exactly, licenses the use of adefinite description to refer to a discourse-new entity?
The role of premodificationand postmodification should also be further examined.
Postmodification is one ofthe most frequent features of discourse-new descriptions; additional empirical studiesconsidering a detailed subclassification f discourse-new descriptions would give usa better understanding of the problem.
The postmodification of a description oftenacts as an explicit anchor (what LObner \[1987\] calls "disambiguating arguments andattributes"); understanding how the head noun of a postmodified escription relates"semantically" with its complement is a problem similar to that of identifying thesemantic relation between a bridging description and its anaphoric anchor, but to datethere hasn't been much research on this topic (while there has been a lot of work onidentifying the relations that hold between the premodifiers, especially in noun-nouncompounds).
An NP's head noun may also corefer with its complement, as seen inthe examples in (59):(59) a. the dream of home ownershipb.
the issue of student grantsWe also observed that definite descriptions with premodification were responsible forconsiderable disagreement among the annotators, the reasons for which are still to beexplained.589Computational Linguistics Volume 26, Number 4We wish to thank Ellen Bard, RafaelBordini, Jean Carletta, Miriam Eckert, KariFraurud, Rob Gaizauskas, Janet Hitzeman,Chris Mellish, and our anonymousreviewers for comments, help, andsuggestions.
Renata Vieira was supported inpart by a fellowship from CNPq, Brazil;Massimo Poesio is supported by an EPSRCAdvanced Research Fellowship.ReferencesAbney, Steve.
1991.
Parsing by chunks.
InR.
Berwick, S. Abney, and C. Tenny,editors, Principle-based Parsing.
Kluwer,Dordrecht, pages 257-278.Alshawi, Hiyan.
1987.
Memory and Contextfor Language Interpretation.
CambridgeUniversity Press, Cambridge.Alshawi, Hiyan.
1990.
Resolvingquasiqogical forms.
ComputationalLinguistics, 16(3):133-144.Alshawi, Hiyan, editor.
1992.
The CoreLanguage Engine.
MIT Press, Cambridge,MA.Aone, Chinatsu and Scott W. Bennett.
1995.Automated acquisition of anaphoraresolution strategies.
In Proceedings oftheAAAI Spring Symposium on EmpiricalMethods in Discourse Interpretation andGeneration, pages 1-7, Stanford.Appelt, Douglas, Jerry R. Hobbs, John Bear,David Israel, Megumi Kameyama, AndyKehler, David Martin, Karen Myers, andMabry Tyson.
1995.
SRI InternationalFASTUS system MUC-6 test results andanalysis.
In Proc.
of the Sixth MessageUnderstanding Conference, pages 237-248,Columbia, MD, November.Azzam, Saliha, Kevin Humphreys, andRobert Gaizauskas.
1998.
Evaluating afocus-based approach to anaphoraresolution.
In COLING-ACL "98: 36thAnnual Meeting of the Association forComputational Linguistics and the 17thInternational Conference on ComputationalLinguistics, pages 74-78, Montreal,Quebec, Canada.Bean, David L. and Ellen Riloff.
1999.Corpus-based i entification ofnon-anaphoric noun phrases.
InProceedings ofthe 37th Annual Meeting,pages 373-380, University of Maryland.Association for ComputationalLinguistics.Bikel, Daniel, Scott Miller,Richard Schwartz, and Ralph Weischedel.1997.
Nymble: A high-performancelearning name finder.
In Proceedings ofthe5th Conference on Applied Natural LanguageProcessing, pages 194-201, Washington,DC Association for ComputationalLinguistics.Bosch, Peter and Bart Geurts.
1989.Processing definite NPs.
IWBS Report 78,IBM Germany, July.Burger, John D. and Dennis Connolly.
1992.Probabilistic resolution of anaphoricreference.
In Proceedings ofthe AAAI FallSymposium on Probabilistic Approaches toNatural Language, pages 17-24,Cambridge, MA.Carbonell, Jamie and Ralf D. Brown.
1988.Anaphora resolution: A multi-strategyapproach.
In Proceedings ofthe 12thInternational Conference on ComputationalLinguistics (COLING-88), pages 96-101,Budapest, Hungary.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):249-254.Carletta, Jean, Amy Isard, Stephen Isard,Jacqueline C. Kowtko, GwynethDoherty-Sneddon, and Anne H.Anderson.
1997.
The reliability of adialogue structure coding scheme.Computational Linguistics, 23(1):13-32.Carter, David M. 1987.
Interpreting Anaphorsin Natural Language Texts.
Ellis Horwood,Chichester, UK.Chinchor, Nancy A.
1995.
Statisticalsignificance of MUC-6 results.
InProceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),pages 39-44, Columbia, MD,November 6-8.Chinchor, Nancy A.
1997.
Overview ofMUC-7/MET-2.
In Proceedings oftheSeventh Message Understanding Conference(MUC-7).
Available at http://www.muc.saic.com/proceedings/muc_7_proceedings/overview, html.Cho, Sehyeong and Anthony S. Maida.
1992.Using a Bayesian framework to identifythe referents of definite descriptions.
InProceedings ofthe AAAI Fall Symposium onProbabilistic Approaches to Natural Language,pages 39-46, Cambridge, MA.Clark, Herbert H. 1977.
Inferences incomprehension.
I  D. Laberge and S. J.Samuels, editors, Basic Process in Reading:Perception and Comprehension.
LawrenceErlbaum, pages 243-263.Clark, Herbert H. and Catherine R.Marshall.
1981.
Definite reference andmutual knowledge.
In A. Joshi,B.
Webber, and I.
Sag, editors, Elements ofDiscourse Understanding.
CambridgeUniversity Press, New York.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.590Vieira and Poesio Processing Definite DescriptionsFox, Barbara A.
1987.
Discourse Structure andAnaphora.
Cambridge University Press,Cambridge, UK.Fraurud, Keri.
1990.
Definiteness and theprocessing of NPs in natural discourse.Journal o/Semantics, 7:395-433.Gaizauskas, Robert, Takahiro Wakao,Kevin Humphreys, Hamish Cunningham,and Yorick Wilks.
1995.
University ofSheffield: Description of the LaSIE Systemas used for MUC-6.
In Proceedings o/theSixth Message Understanding Conference(MUC-6), pages 207-220.
MorganKaufmann.Grosz, Barbara J.
1977.
The Representation a dUse of Focus in Dialogue Understanding.Ph.D.
thesis, Stanford University.Grosz, Barbara J., Aravind K. Joshi, andScott Weinstein.
1983.
Providing a unifiedaccount of definite noun phrases indiscourse.
In Proceedings o/the 21st AnnualMeeting, pages 44-50.
Association forComputational Linguistics.Grosz, Barbara.
J , Aravind.
K. Joshi, andScott Weinstein.
1995.
Centering: Aframework for modeling the localcoherence of discourse.
ComputationalLinguistics, 21(2):202-225.
(The paperoriginally appeared as an unpublishedmanuscript in 1986.
).Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intention, and thestructure of discourse.
ComputationalLinguistics, 12(3):175-204.Hahn, Udo; Michael Strube, and KatjaMarkert.
1996.
Bridging textual ellipsis.
InCOLING '96: Proceedings ofthe 16thInternational Conference on ComputationalLinguistics, pages 496-501, Kopenhagen,Aug 5-9 1996.Hardt, Daniel.
1997.
An empirical approachto VP ellipsis.
Computational Linguistics,23(4):525-541.Hatzivassiloglou, Vasileios andKathleen McKeown.
1993.
Towards theautomatic identification of adjectivalscales: clustering adjectives according tomeaning.
In Proceedings o/the 31st AnnualMeeting, pages 172-182, Ohio StateUniversity.
Association for ComputationalLinguistics.Hawkins, John A.
1978.
DeJiniteness andIndefiniteness.
Croom Helm, London.Hearst, Marti A.
1997.
TextTiling:Segmenting text into multi-paragraphsubtopic passages.
ComputationalLinguistics, 23(1):33-64.Helm, Irene.
1982.
The Semantics o/Definiteand Indefinite Noun Phrases.
Ph.D. thesis,University of Massachusetts atAmherst.Hitzeman, Janet and Massimo Poesio.
1998.Long-distance pronominalisation a dglobal focus.
In COLING/ACL "98: 36thAnnual Meeting of the Association/orComputational Linguistics and 17thInternational Conference on ComputationalLinguistics.
Volume 1, pages 550-556,Montreal, Quebec, Canada.Hobbs, Jerry R., Mark E. Stickel, Douglas A.Appelt, and Paul Martin.
1993.Interpretation asabduction.
Arti~cialIntelligence Journal, 63:69-142.Humphreys, Kevin, Robert Gaizauskas,Saliha Azzam, Chris Huyck, B. Mitchell,and Hamish Cunningham.
1998.University of Sheffield: Description of theLaSIE-II System as used for MUC-7.
InProceedings ofthe Seventh MessageUnderstanding Conference (MUC-7).Available on the Web atwww.muc.saic.com.Ishikawa, Tomonori.
1998.
Acquisition ofassociative information and resolution ofbridging descriptions.
Master's thesis,University of Edinburgh, Department ofLinguistics, Edinburgh, Scotland.Kameyama, Megumi.
1997.
Recognizingreferential links: An informationextraction perspective.
In Proceedings off theACL Workshop on Operational Factors inPractical, Robust Anaphora Resolution/orUnrestricted Texts, pages 46-53, Madrid,Spain, July.
Association forComputational Linguistics.Krippendorff, Klaus.
1980.
Content Analysis:An Introduction to its Methodology.
SagePublications, Beverly Hills, London.Landis, J. R., and G. G. Koch.
1977.
Themeasurement of observer agreement forcategorial data.
Biometrics, 36:159-174.Lappin, Shalom and H. J. Leass.
1994.
Analgorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535-562.Lascarides, Alex and Nicholas Ashen 1993.Temporal interpretation, discourserelations and commonsense entailment.Linguistics and Philosophy, 16(5):437-493.LObner, Sebastian.
1987.
Natural anguageand generalised quantifier theory.
InP.
G/irdenfors, editor, GeneralizedQuantifiers.
D. Reidel, Dordrecht, TheNetherlands, pages 93-108.Mani, Inderjeet and T. Richard MacMillan.1996.
Identifying unknown proper namesin newswire text.
In Bran Boguraev andJames Pustejovsky, editors, CorpusProcessing/or Lexical Acquisition.
MITPress, Cambridge, MA, pages 41-59.Marcu, Daniel.
1999.
A decision-basedapproach to rhetorical parsing.
InProceedings off the 37th Annual Meeting,591Computational Linguistics Volume 26, Number 4pages 365--372, University of Maryland,June.
Association for ComputationalLinguistics.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.McDonald, David.
1996.
Internal andexternal evidence in the identification andsemantic ategorization f proper names.In Bran Boguraev and James Pustejovsky,editors, Corpus Processing for LexicalAcquisition.
MIT Press, Cambridge, MA,pages 21-39.Mikheev, Andrei, Marc Moens, andClaire Grover.
1999.
Named entityrecognition without gazetteers.
InProceedings ofEACL, pages 1-8, Bergen,Norway.
EACL.Mitkov, Ruslan.
2000.
Towards morecomprehensive evaluation in anaphoraresolution.
In Proceedings ofthe 2ndInternational Conference on LanguageResources and Evaluation,pages 1,309-1,314, Athens.Paik, Woojin, Elizabeth D. Liddy,Edmund Yu, and Mary McKenna.
1996.Categorizing and standardizing propernouns for efficient information retrieval.In Bran Boguraev and James Pustejovsky,editors, Corpus Processing for LexicalAcquisition.
MIT Press, Cambridge, MA,pages 61-73.Palmer, David D. and David S. Day.
1997.
Astatistical profile of the named entity task.In Proceedings ofthe 5th Conference onApplied Natural Language Processing,pages 190-193, Washington, DC, March.Association for ComputationalLinguistics.Poesio, Massimo.
1993.
A situation-theoreticformalization of definite descriptioninterpretation i  plan elaborationdialogues.
In Peter Aczel, David Israel,Yasuhiro Katagiri, and Stanley Peters,editors, Situation Theory and itsApplications, Volume 3.
CSLI, Stanford,chapter 12, pages 339-374.Poesio, Massimo, Sabine Schulte im Walde,and Chris Brew.
1998.
Lexical clusteringand definite description interpretation.
InProceedings ofthe AAAI Spring Symposiumon Learning for Discourse, pages 82-89,Stanford, CA, March.
AAAI.Poesio, Massimo and Renata Vieira.
1998.
Acorpus-based investigation of definitedescription use.
Computational Linguistics,24(2):183-216.Poesio, Massimo, Renata Vieira, andSimone Teufel.
1997.
Resolving bridgingreferences in unrestricted text.
InR.
Mitkov, editor, Proceedings ofthe ACLWorkshop on Operational Factors in RobustAnaphora Resolution, pages 1-6, Madrid.Also available as HCRC Research PaperHCRC/RP-87, University of Edinburgh.Postal, Paul M. 1969.
Anaphoric islands.
InR.
I. Binnick et al, editor, Papers~om theFifth Regional Meeting of the ChicagoLinguistic Society, pages 205-235.University of Chicago.Prince, Ellen F. 1981.
Toward a taxonomy ofgiven-new information.
In Peter Cole,editor, Radical Pragmatics.
Academic Press,New York, pages 223-256.Prince, Ellen F. 1992.
The ZPG letter:Subjects, definiteness, and informationstatus.
In S. Thompson and W. Mann,editors, Discourse Description: DiverseAnalyses of a Fund-Raising Text.
JohnBenjamins, pages 295-325.Quinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann, SanMateo, CA.Quirk, Randolph, Sydney Greenbaum,Geoffrey Leech, and Jan Svartvik.
1985.
AComprehensive Grammar of the EnglishLanguage.
Longman, London.Reichman, Rachel.
1985.
Getting Computers toTalk Like You and Me.
MIT Press,Cambridge, MA.Richmond, Kevin, Andrew Smith, andEinat Amitay.
1997.
Detecting subjectboundaries within text: Alanguage-independent sta isticalapproach.
In Proceedings ofThe SecondConference on Empirical Methods in NaturalLanguage Processing (EMNLP-2),pages 47-54, Brown University.Russell, Bertrand.
1905.
On denoting.
Mind,14:479-493.
Reprinted in Logic andKnowledge, R. C. Marsh, editor, GeorgeAllen and Unwin, London.Sidner, Candace L. 1979.
Towards acomputational theory of definite anaphoracomprehension in English discourse.
Ph.D.thesis, MIT.Siegel, Sydney and N. John Castellan.
1988.Nonparametric statistics for the BehavioralSciences.
2nd edition.
McGraw-Hill.Strand, Kjetil.
1996.
A taxonomy of linkingrelations.
Manuscript.
A preliminaryversion presented at the Workshop onIndirect Anaphora, Lancaster University,1996.Sundheim, Beth M. 1995.
Overview of theresults of the MUC-6 evaluation.
InProceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),pages 13-31, Columbia, MD,November 6-8.592Vieira and Poesio Processing Definite DescriptionsSuri, Linda Z. and Kathleen F. McCoy.
1994.RAFT/RAPR and centering: Acomparison and discussion of problemsrelated to processing complex sentences.Computational Linguistics, 20(2):301-317.Tetreault, Joel R. 1999.
Analysis ofsyntax-based pronoun resolutionmethods.
In Proceedings ofthe 37th AnnualMeeting, pages 602-605, University ofMaryland, June.
Association forComputational Linguistics.Vieira, Renata.
1998.
Definite DescriptionResolution i  Unrestricted Texts.
Ph.D.thesis, University of Edinburgh, Centrefor Cognitive Science, February.Vieira, Renata and Massimo Poesio.
1996.Processing definite descriptions incorpora.
Presented at the DiscourseAnaphora nd Resolution Colloquium(DAARC), Lancaster University,Lancaster, UK.
Also available as ResearchPaper HCRC/RP-86, University ofEdinburgh, Human CommunicationResearch Centre.Vieira, Renata and Simone Teufel.
1997.Towards resolution of bridgingdescriptions.
In Proceedings ofthe 35th JointMeeting of the Association for ComputationalLinguistics, pages 522-524, Madrid.Vilain, Marc, John Burger, John Aberdeen,Dennis Connolly, and Lynette Hirschman.1995.
A model-theoretic coreferencescoring scheme.
In Proc.
of the SixthMessage Understanding Conference,pages 45-52.Wacholder, Nina and Yael.
Ravin.
1997.Disambiguation ofproper names in text.In Proceedings ofthe 5th Conference onApplied Natural Language Processing,pages 202-208, Washington, DC, March.Association for ComputationalLinguistics.Ward, Gregory, Richard Sproat, andGail McKoon.
1991.
A pragmatic analysisof so-called anaphoric islands.
Language,67:439-474.Webber, Bonnie L. 1979.
A Formal Approach toDiscourse Anaphora.
Garland, New York.Wilks, Yorick A.
1975.
A preferentialpattern-matching semantics for naturallanguage.
Artificial Intelligence, 6:53-74.593
