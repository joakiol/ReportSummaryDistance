Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 313?324,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Graph-based Approach for Contextual Text NormalizationC?a?g?l S?onmez and Arzucan?Ozg?urDepartment of Computer EngineeringBogazici UniversityBebek, 34342 Istanbul, Turkey{cagil.ulusahin,arzucan.ozgur}@boun.edu.trAbstractThe informal nature of social media textrenders it very difficult to be automati-cally processed by natural language pro-cessing tools.
Text normalization, whichcorresponds to restoring the non-standardwords to their canonical forms, provides asolution to this challenge.
We introduce anunsupervised text normalization approachthat utilizes not only lexical, but also con-textual and grammatical features of socialtext.
The contextual and grammatical fea-tures are extracted from a word associationgraph built by using a large unlabeled so-cial media text corpus.
The graph encodesthe relative positions of the words with re-spect to each other, as well as their part-of-speech tags.
The lexical features are ob-tained by using the longest common sub-sequence ratio and edit distance measuresto encode the surface similarity amongwords, and the double metaphone algo-rithm to represent the phonetic similarity.Unlike most of the recent approaches thatare based on generating normalization dic-tionaries, the proposed approach performsnormalization by considering the contextof the non-standard words in the input text.Our results show that it achieves state-of-the-art F-score performance on standarddatasets.
In addition, the system can betuned to achieve very high precision with-out sacrificing much from recall.1 IntroductionSocial text, which has been growing and evolvingsteadily, has its own lexical and grammatical fea-tures (Choudhury et al., 2007; Eisenstein, 2013).lol meaning laughing out loud, xoxo meaning kiss-ing, 4u meaning for you are among the most com-monly used examples of this jargon.
In addition,these informal expressions in social text usuallytake many different lexical forms when generatedby different individuals (Eisenstein, 2013).
Thelimited accuracies of the Speech-to-Text (STT)tools in mobile devices, which are increasingly be-ing used to post messages on social media plat-forms, along with the scarcity of attention ofthe users result in additional divergence of so-cial text from more standard text such as fromthe newswire domain.
Tools such as spellcheckerand slang dictionaries have been shown to be in-sufficient to cope with this challenge long timeago (Sproat et al., 2001).
In addition, most Nat-ural Language Processing (NLP) tools includingnamed entity recognizers and dependency parsersgenerally perform poorly on social text (Ritter etal., 2010).Text normalization is a preprocessing step torestore non-standard words in text to their origi-nal (canonical) forms to make use in NLP applica-tions or more broadly to understand the digitizedtext better (Han and Baldwin, 2011).
For exam-ple, talk 2 u later can be normalized as talk to youlater or similarly enormoooos, enrmss and enour-mos can be normalized as enormous.
Other exam-ples of text messages from Twitter and their corre-sponding normalized forms are shown in Table 1.The non-standard words in text are referred toas Out of Vocabulary (OOV) words.
The nor-malization task restores the OOV words to theirIn Vocabulary (IV) forms.
Social text is contin-uously evolving with new words and named en-tities that are not in the vocabularies of the sys-tems (Hassan and Menezes, 2013).
Therefore, notevery OOV word (e.g.
iPhone, WikiLeaks or tok-313Hav guts to say wat u desire.. Dnt beat behind da bush!
!And 1 mre thng no mre say y r people?s man!
!Have guts to say what you desire.. Don?t beat behind the bush!
!And one more thing no more say you are people?s man!
!There r sm songs u don?t want 2 listen 2 yl walking coswhen u start dancing ppl won?t knw y.There are some songs you don?t want to listen to while walkingbecause when you start dancing people won?t know why.Table 1: Sample tweets and their normalized forms.enizing) should be considered for normalization.The OOV tokens that should be considered fornormalization are referred to as ill-formed words.Ill-formed words can be normalized to differentcanonical words depending on the context of thetext.
For example, let?s consider the two examplesin Table 1.
?y?
is normalized as ?you?
in the firstone and as ?why?
in the second one.In this paper, we propose a graph-based textnormalization method that utilizes both contex-tual and grammatical features of social text.
Thecontextual information of words is modeled bya word association graph that is created from alarge social media text corpus.
The graph repre-sents the relative positions of the words in the so-cial media text messages and their Part-of-Speech(POS) tags.
The lexical similarity features amongthe words are modeled using the longest commonsubsequence ratio and edit distance that encodethe surface similarity and the double metaphonealgorithm that encodes the phonetic similarity.The proposed approach is unsupervised, which isan important advantage over supervised systems,given the continuously evolving language in thesocial media domain.
The same OOV word mayhave different appropriate normalizations depend-ing on the context of the input text message.
Re-cently proposed dictionary-based text normaliza-tion systems perform dictionary look-up and al-ways normalize the same OOV word to the sameIV word regardless of the context of the input text(Han et al., 2012; Hassan and Menezes, 2013).
Onthe other hand, the proposed approach does notonly make use of the general context informationin a large corpus of social media text, but it alsomakes use of the context of the OOV word in theinput text message.
Thus, an OOV word can benormalized to different IV words depending on thecontext of the input text.2 Related WorkEarly work on text normalization mostly madeuse of the noisy channel model.
The first workthat had a significant performance improvementover the previous research was by Brill and Moore(2000).
They proposed a novel noisy channelmodel for spell checking based on string to stringedits.
Their model depended on probabilistic mod-eling of sub-string transformations.Toutanova and Moore (2002) improved this ap-proach by extending the error model with phoneticsimilarities over words.
Their approach is basedon learning rules to predict the pronunciation of asingle letter in the word depending on the neigh-bouring letters in the word.Choudhury et al.
(2007) developed a super-vised Hidden Markov Model based approach fornormalizing Short Message Service (SMS) texts.They proposed a word for word decoding ap-proach and used a dictionary based method tonormalize commonly used abbreviations and non-standard usage (e.g.
?howz?
to ?how are?
or?aint?
to ?are not?).
Cook and Stevenson (2009)extended this model by introducing an unsuper-vised noisy channel model.
Rather than usingone generic model for all word formations asin (Choudhury et al., 2007), they used a mix-ture model in which each different word formationtype is modeled explicitly.The limitations of these methods were that theydid not consider contextual features and assumedthat tokens have unique normalizations.
In the textnormalization task several OOV tokens are am-biguous and without contextual information it isnot possible to build models that can disambiguatetransformations correctly.Aw et al.
(2006) proposed a phrase-based statis-tical machine translation (MT) model for the textnormalization task.
They defined the problem astranslating the SMS language to the English lan-guage and based their model on two submodels:a word based language model and a phrase basedlexical mapping model (channel model).
Theirsystem also benefits from the input context andthey argue that the strength of their model is inits ability to disambiguate mapping as in ?2?
??two?
or ?to?, and ?w??
?with?
or ?who?.
Mak-ing use of the whole conversation, this is the clos-est approach to ours in the sense of utilizing con-textual sensitivity and coverage.314Pennell and Liu (2011) on the other hand, pro-posed a character level MT system, that is robustto new abbreviations.
In their two phased system,a character level trained MT model is used to pro-duce word hypotheses and a trigram LM is used tochoose a hypothesis that fits into the input context.The MT based models are supervised models,a drawback of which is that they require anno-tated data.
Annotated training data is not readilyavailable and is difficult to create especially forthe rapidly evolving social media text (Yang andEisenstein, 2013).More recent approaches handled the text nor-malization task by building normalization lexi-cons.
Han and Baldwin (2011) developed a twophased model, where they only consider the ill-formed OOV words for normalization.
First, aconfusion set is generated using the lexical andphonetic distance features.
Later, the candidatesin the confusion set are ranked using a mixtureof dictionary look up, word similarity based onlexical edit distance, phonemic edit distance, pre-fix sub-string, suffix sub-string and longest com-mon subsequence (LCS), as well as context sup-port metrics.
Chrupala (2014) on the other handachieved lower word error rates without using anylexical resources.Gouws et al.
(2011) investigated the distinctcontributions of features that are highly dependedon user-centric information such as the geologi-cal location of the users and the twitter client thatthe tweet is received from.
Using such user-basedcontextual metrics they modelled the transforma-tion distributions across populations.Liu et al.
(2012) proposed a broad coverage nor-malization system, which integrates an extendednoisy channel model, that is based on enhancedletter transformations, visual priming, string andphonetic similarity.
They try to improve the per-formance of the top n normalization candidates byintegrating human perspective modeling.Yang and Eisenstein (2013) introduced an unsu-pervised log linear model for text normalization.Their joint statistical approach uses local contextbased on language modeling and surface similar-ity.
Along with dictionary based models, Yang andEisenstein?s model have obtained a significant im-provement on the performance of text normaliza-tion systems.Another relevant study is conducted by Hassanand Menezes (2013), who generated a normaliza-tion lexicon using Markov random walks on a con-textual similarity lattice that they created using 5-gram sequences of words.
The best normaliza-tion candidates are chosen using the average hit-ting time and lexical similarity features.
Contextof a word in the center of a 5-gram sequence is de-fined by the other words in the 5-gram.
Even if oneword is not the same, the context is considered tobe different.
This is a relatively conservative wayfor modeling the prior contexts of words.
In ourmodel, we filtered candidate words based on theirgrammatical properties and let each neighbouringtoken to contribute to the prior context of a word,which leads to both a higher recall and a higherprecision.3 MethodologyIn this paper, we propose a graph-based approachthat models both contextual and lexical similar-ity features among an ill-formed OOV word andcandidate IV words.
An input text is first prepro-cessed by tokenizing and Part-Of-Speech (POS)tagging.
If the text contains an OOV word, thenormalization candidates are chosen by makinguse of the contextual features, which are extractedfrom a pre-generated directed word associationgraph, as well as lexical similarity features.
Lexi-cal similarity features are based on edit distance,longest common subsequence ratio, and doublemetaphone distance.
In addition, a slang dictio-nary1is used as an external resource to enrichthe normalization candidate set.
The details ofthe approach are explained in the following sub-sections.3.1 PreprocessingAfter tokenization, the next step in the pipelineis POS tagging each token using a POS taggerspecifically designed for social media text.
Unlikethe regular POS taggers designed for well-writtennewswire-like text, social media POS taggers pro-vide a broader set of tags specific to the peculiari-ties of social text (Owoputi et al., 2013; Gimpel etal., 2011).
Using this extended set of tags we canidentify tokens such as discourse markers (e.g.
rtfor retweets, cont.
for a tweet whose content fol-lows up in the coming tweet) or URLs.
This en-ables us to model better the context of the words insocial media text.
A sample preprocessed sentenceis shown in Table 3.1http://www.noslang.com315As shown in Table 2, after preprocessing, eachtoken is assigned a POS tag with a confidencescore between 0 and 12.
Later, we use these confi-dence scores in calculating the edge weights in ourcontext graph.
Note that even though the words wand beatiful are misspelled, they are tagged cor-rectly by the tagger, with lower confidence scoresthough.Token POS tag Tag confidencewith Preposition 0.9963a Determiner 0.9980beautiful Adjective 0.9971smile Noun 0.9712w Preposition 0.7486a Determiner 0.9920beatiful Adjective 0.9733smile Noun 0.9806Table 2: Sample POS tagger output3.2 Graph constructionContextual information of words is modeledthrough a word association graph created by us-ing a large corpus of social media text.
The graphencodes the relative positions of the POS taggedwords in the text with respect to each other.
Af-ter preprocessing, each text message in the corpusis traversed in order to extract the nodes and theedges of the graph.
A node is defined with fourproperties: id, oov, freq and tag.
The token itself isthe id field.
The freq property indicates the node?sfrequency count in the dataset.
The oov field is setto True if the token is an OOV word.
Following theprior work by Han and Baldwin, (2011) we usedthe GNU Aspell dictionary (v0.60.6) to determinewhether a word is OOV or not.
We also edited theoutput of Aspell dictionary to accept letters otherthan ?a?
and ?i?
as OOV words.
A portion of thegraph that covers parts of the sample sentence inTable 3 is shown in Figure 1.In the created word association graph, eachnode is a unique set of a token and its POS tag.This helps us to identify the candidate IV wordsfor a given OOV word by considering not onlylexical and contextual similarity, but also gram-matical similarity in terms of POS tags.
For ex-ample, if the token smile has been frequently seenas a Noun or a Verb, and not in other forms in thedataset (e.g.
Table 4), this provides evidence that itis not a good normalization candidate for an OOVtoken that has been tagged as a Pronoun.
On the2CMU Ark Tagger (v0.3.2)Figure 1: Portion of the word association graphfor part of the sample sentence in Table 3.
(d: dis-tance, w: edge weight).other hand, smile can be a good candidate for aNoun or a Verb OOV token, if it is lexically andcontextually similar to it.node id freq oov tagsmile 3 False Asmile 3403 False Nsmile 2796 False VTable 4: The different nodes in the word associ-ation graph representing the token smile taggedwith different POS tags.An edge is created between two nodes in thegraph, if the corresponding word pair (i.e.
to-ken/POS pair) are contextually associated.
Twowords are considered to be contextually associatedif they satisfy the following criteria:?
The two words co-occur within a maximumword distance of tdistancein a text messagein the corpus.?
Each word has a minimum frequency oftfrequencyin the corpus.The directionality of the edges is based on thesequence of words in the text messages in the cor-pus.
In other words, an edge between two nodesis directed from the earlier seen token towardsthe later seen token in a message.
For example,Figure 2 shows the edges that would be derived316Let?sLstartVthisDmorningNwPaDbeatifulAsmileN.CTable 3: Sample tokenized, POS tagged sentence (L: nominal+verbal, V: verb, D: determiner, N: noun,P: Preposition, A: adjective, C: punctuation).from a text including the phrase ?with a beautifulsmile?.
The direction (from,to) and the distancetogether represent a unique triplet.
For each pairof nodes with a specific distance there is an edgewith a positive weight, if the two nodes are con-textually associated.
Each co-occurrence of twocontextually associated nodes increases the weightof the edge between them with an average of thenodes?
POS tag confidence scores in the text mes-sage considered.
If we are to expand the graphwith the example phrase ?with a beautiful smile?,the weight of the edge with distance 2 from thenode with|P to the node smile|N would increase by(0.9963 + 0.9712)/2, since the confidence scoreof the POS tag for the token with is 0.9963 and theconfidence score of the POS tag of the token smileis 0.9712 as shown in Table 2.20with!Pa!D smile!Ndistance: 0 weight: 25011 beautiful!Adistance: 0 weight: 2918 distance:0 weight: 305distance:1 weight: 322distance:1 weight: 198distance:2 weight: 89Figure 2: Sample nodes and edges from the wordassociation graph.3.3 Graph-based Contextual SimilarityOur graph-based contextual similarity method isbased on the assumption that an IV word that isthe canonical form of an OOV word appears in thesame context with the corresponding OOV word.In other words, the two nodes in the graph shareseveral neighbors that co-occur within the samedistances to the corresponding two words in socialmedia text.
We also assume that an OOV word andits canonical form should have the same POS tag.Given an input text for normalization, the nextstep after preprocessing is finding the normaliza-tion candidates for each OOV token in the inputtext.
For each ill-formed OOV token oiin the in-put text, first the list of tokens that co-occur withoiin the input text and their positional distances tooiare extracted.
This list is called the neighbor listof token oi, i.e., NL(oi).For each neighbor node njin NL(oi), the wordassociation graph is traversed, and the edges fromor to the node njare extracted.
The resulting edgelist EL(oi) has edges in the form of (nj, ck) or (ck,nj), where ckis a candidate canonical form of theOOV word oi.
Here the neighbor node njcan bean OOV node, but the candidate node ckis chosenamong the IV nodes.
The edges in EL(oi) are fil-tered by the relative distance of njto oias given inthe NL(oi).
Any edge between njand ck, whosedistance is not the same as the distance betweennjand oiis removed.In addition to distance based filtering, POS tagbased filtering is also performed on the edges inEL(oi).
Each candidate node should have thesame POS tag with the corresponding OOV token.For the OOV token oithat has the POS tag Ti, allthe edges that include candidates with a tag otherthan Tiare removed from the edge list EL(oi).Figure 3 represents a portion from the graphwhere the neighbors and candidates of the OOVnode ?beatiful?
are shown.
In the sample sentencein Table 3 there are two OOV tokens to be normal-ized, o1= w and o2= beatiful.
The neighborlist of o2, NL(o2) includes n1= w, n2= a andn3= smile.
For each neighbor in NL(o2), the can-didate nodes (c1= broken, c2= nice, c3= new,c4= beautiful, c5= big, c6= best, c7= great)are extracted.
As shown in Figure 3, there are 11lines representing the edges between the neighborsof the OOV token and the candidate nodes.
Theseare representative edges in EL(o2).
Each memberof the edge list has the same tag (A for Adjective)as the OOV node ?beatiful?
and the same distanceto the corresponding neighbor node of the OOVnode.Each edge in EL(oi) consists of a neighbornode nj, a candidate node ckand an edge weightedgeWeight(nj, ck).
The edge weight representsthe likelihood or the strength of association be-tween the neighbor node njand the candidatenode ck.
As described in the previous section theedge weights are computed based on the frequency317w!P a!D smile!NDistance: 0broken Abeautiful Anice A new A big Abest Agreat ADistance: 1 Distance: 0c1c2c3 c5c6c7n1 n2 n3c435 26224388 2918 75020305 12553beatiful!Ao2Figure 3: A portion of the graph that includesthe OOV token ?beatiful?, its neighbors and thecandidate nodes that each neighbor is connectedto.
Thick lines show the edge list with relativeweights.of co-occurrence of two tokens, as well as the con-fidence scores of their POS tags.The edge weights of the edges in EL(o2) areshown in Figure 3.
The edges that are connected tothe OOV neighbor ?w?
have smaller edge weightssuch as 3, 5, and 26.
On the other hand, the edgesthat are connected to common words have higherweights.
For example, the weight of the edge be-tween the nodes ?a?
and ?new?
is 24388.
Thisindicates that they are more common words, andfrequently co-occur in the same form (?a new?
).Although this edge weight metric is reasonablefor identifying the most likely canonical form forthe OOV word oi, it has the drawback of favoringwords with high frequencies like common wordsor stop words.
Therefore, to avoid overrated wordsand get contextually related candidates, we nor-malize the edge weight edgeWeight(nj, ck) withthe frequency of the candidate node ckas shownin Equation 1.Equation 1 provides a metric that captures con-textual similarity based on binary associations.
Inorder to achieve a more comprehensive contex-tual coverage, a contextual similarity feature isbuilt based on the sum of the binary associationscores of several neighbors.
As shown in Equa-tion 2, for a candidate node ckthe total edgeweight score is the sum of the normalized edgeweight scores EWNorm(nj, ck), which are theedge weights coming from the different neighborsof the OOV token oi.
We expect this contextualsimilarity feature to favor and identify the candi-dates which are (i) related to many neighbors, and(ii) have a high association score with each neigh-bor.EWNorm(nj, ck) = edgeWeight(nj, ck)/freq(ck)(1)EW Score(oi, ck) =?EL(oi)EWNorm(nj, ck)(2)Our word association graph includes both OOVand IV tokens, and our OOV detection dependson the spellchecker which fails to identify someOOV tokens that have the same spelling with an IVword.
In order to propose better canonical forms,the frequencies of the normalization candidates inthe social media corpus have also been incorpo-rated to the contextual similarity feature.
Nodeswith higher frequencies lead to tokens that are intheir most likely grammatical forms.The final contextual similarity of the token oiand the candidate ckis the weighted sum of thetotal edge weight score and the frequency scoreof the candidate (see Equation 3).
The frequencyscore of the candidate is a real number between 0and 1.
It is proportional to the frequency of thecandidate with respect to the frequencies of theother candidates in the corpus.
Since the total edgeweight score is our primary contextual resource,we may want to favor edge weight scores.
We givethe frequency score a weight 0 ?
?
?
1 to be ableto limit its effect on the total contextual similarityscore.contSimScore(oi, ck) = EW Score(oi, ck)+ ?
?
freqScore(ck)(3)Hereby, we have the candidate list CL(oi) for theOOV token oithat includes all the unique can-didates in EL(oi) and their contextual similarityscores calculated.3.4 Lexical SimilarityFollowing the prior work in (Han and Baldwin,2011; Hassan and Menezes, 2013), our lexicalsimilarity features are based on edit distance (Lev-enshtein, 1966), double metaphone (phonetic editdistance) (Philips, 2000), and a similarity function318(simCost) (Contractor et al., 2010) which is de-fined as the ratio of the Longest Common Sub-sequence Ratio (LCSR) (Melamed, 1999) of twowords and the Edit Distance (ED) between theirskeletons (Equations 4 and 5), where the skeletonof a word is obtained by removing its vowels.LCSR(oj, ck) = LCS(oj, ck)/maxLength(oj, ck) (4)simCost(oj, ck) = LCSR(oj, ck)/ED(oj, ck) (5)Following the tradition that is inspiredfrom (Kaufmann and Kalita, 2010), beforelexical similarity calculations, any repetitions ofcharacters three or more times in OOV tokens arereduced to two (e.g.
goooood is reduced to good).Then, the edit distance, phonetic edit distance, andsimCost between each candidate in CL(oi) andthe OOV token oiare calculated.
Edit distanceand phonetic edit distance are used to filter thecandidates.
Any candidate in CL(oi) with anedit distance greater than teditand phonetic editdistance greater than tphoneticto oiis removedfrom the candidate list CL(oi).lexSimScore(oi, ck) = simCost(oi, ck)+ ?
?
editScore(oi, ck)(6)For the remaining candidates, the total lexicalsimilarity score (Equation 6) is calculated usingsimCost and edit distance score3.
Similar to con-textual similarity score, here we have one mainlexical similarity feature and one minor lexicalsimilarity feature.
The major lexical similarityfeature is simCost, whereas the edit distance scoreis the minor feature.
We assigned a weight 0 ??
?
1 to the edit distance score to be able to lowerits contribution while calculating the total lexicalsimilarity score.3.5 External ScoreSince some social media text messages are ex-tremely short and contain several OOV words,they do not provide sufficient context, i.e., IVneighbors, to enable the extraction of good candi-dates from the word association graph.
Therefore,we extended the candidate list obtained throughcontextual similarity as described in the previoussection, by including all the tokens in the word as-sociation graph that satisfy the edit distance and3an approximate string comparison measure(between 0.0 and 1.0) using the edit distancehttps://sourceforge.net/projects/febrl/phonetic edit distance criteria.
We also incorpo-rated candidates from external resources, in otherwords from a slang dictionary and a transliterationtable of numbers and pronouns.
If a candidate oc-curs in the slang dictionary or in the transliterationtable as a correspondence to its OOV word, it isassigned an external score of 1, otherwise it is as-signed an external score of 0.The transliterations were first used by (Gouwset al., 2011).
Besides the token and its transliter-ation we also use its POS tag information, whichwas not available in their system.The external score favors the well known inter-pretations of common OOV words.
However, un-like the dictionary based methodologies, our sys-tem does not return the corresponding unabbrevi-ated word in the slang dictionary or in the translit-eration table directly.
Only an external score getsassigned and the candidate still needs to com-pete with other candidates which may have highercontextual similarities and one of those contextu-ally more similar candidates may be returned asthe correct normalization instead of the candidatefound equivalent to the OOV word in the slang dic-tionary (or in the transliteration table).3.6 Overall ScoringAs shown in Equation 7, the final score of a can-didate IV token ckfor an OOV token oiis the sumof its lexical similarity score, contextual similarityscore and external score with respect to oi.candScore(oi, ck) = lexSimScore(oi, ck)+ contSimScore(oi, ck)+ externalScore(oi, ck)(7)4 Experiments4.1 DatasetsWe used the LexNorm1.1 (LN) dataset (Han andBaldwin, 2011) and Pennell and Liu (2014)?s tri-gram dataset to evaluate our proposed approach.LexNorm1.1 contains 549 tweets with 1184 manu-ally annotated ill-formed OOV tokens.
It has beenused by recent text normalization studies for eval-uation, which enables us to directly compare ourperformance results with results obtained by therecent previous work (Han and Baldwin, 2011;Pennell and Liu, 2011; Han et al., 2012; Liu etal., 2012; Hassan and Menezes, 2013; Yang andEisenstein, 2013; Chrupala, 2014).
The trigram319dataset is an SMS-like corpus collected from twit-ter status updates sent via SMS.
The dataset doesnot include the complete tweet text but trigramsfrom tweets and one OOV word in each trigramis annotated.
In total 4661 twitter status messagesand 7769 tokens are annotated.4.2 Graph GenerationWe used a large corpus of social media text to con-struct our word association graph.
We extracted1.5 GB of English tweets from Stanford?s 476 mil-lion Twitter Dataset (Yang and Leskovec, 2011).The language identification of tweets was per-formed by using the langid.py Python library (Luiand Baldwin, 2012; Baldwin and Lui, 2010).CMU Ark Tagger (v0.3.2), which is a social me-dia specific POS tagger achieving an accuracy of95% over social media text (Owoputi et al., 2013;Gimpel et al., 2011), is used for tokenizing andPOS tagging the tweets.
We used the twitter tagsetwhich includes some extra POS tags specific to so-cial media including URLs and emoticons, Twit-ter hashtags (#), and twitter at-mentions (@).
Wemade use of these social media specific tags to dis-ambiguate some OOV tokens.After tokenization, we removed the tokens thatwere POS tagged as mention (e.g.
@brendon),discourse marker (e.g.
RT), URL, email address,emoticon, numeral, and punctuation.
The remain-ing tokens are used to build the word associationgraph.
After constructing the graph we only keptthe nodes with a frequency greater than 8.
Forthe performance related reasons, the relatednessthresholds tdistanceand tfrequencywere chosen as3 and 8, respectively.
The resulting graph contains105428 nodes and 46609603 edges.4.3 Candidate Set GenerationWhile extending the candidate set with lexical fea-tures we use tedit?
2 ?
tphonetic?
1 to keepup with the settings in (Han and Baldwin, 2011).In other words, IV words that are within 2 char-acter edit distance or 1 character edit distance ofa given OOV word under phonemic transcriptionwere chosen as lexical similarity candidates.
Thevalues for the ?
and ?
parameters in Equations 3and 6 are set to 0.5.
We did not tune these pa-rameters for optimized performance.
We selectedthe value of 0.5 in order to give less weight (halfweight) to our minor contextual and lexical simi-larity features compared to the major ones.4.4 Normalization CandidatesMost of the prior work assume perfect detectionof ill-formed words during test set decoding (Liuet al., 2012; Han and Baldwin, 2011; Pennell andLiu, 2011; Yang and Eisenstein, 2013).
To beable to compare our results with studies that donot assume that ill-formed words have been pre-identified (Chrupala, 2014; Hassan and Menezes,2013; Han et al., 2012) we used our graph andbuilt a dictionary to identify the ill-formed words.Following Han and Baldwin (2011) and Yangand Eisenstein (2013), we created a dictionary bychoosing the nodes in our graph that have a fre-quency property higher than 20.
Filtering this dic-tionary of 49657 words using GNU Aspell dictio-nary (v0.60.6) we produced a set of 26773 ?in-vocabulary?
(IV) words.
In our second setup oursystem does not attemp to normalize the words inthis set.4.5 Results and AnalysisIn this paper we introduced a new contextual ap-proach for text normalization.
The lexical similar-ity score described in Section 3.4 and the externalscore described in Section 3.5 depend on the workof Han and Baldwin (2011).
With small changesmade to the previously proposed method we tookit as a baseline in our study.As contextual layer we proposed two metricsextracted from the word association graph.
Thefirst one depends on the total edge weights be-tween candidates and OOV neighbours, the sec-ond one is based on the frequencies of the candi-dates in the corpus.As the evaluation metrics we used precision,recall, and F-Measure.
Precision calculates theproportion of correctly normalized words amongthe words for which we produced a normaliza-tion.
Recall shows the amount of correct nor-malizations over the words that require normal-ization (ill-formed OOV words).
The main metricthat we consider while evaluating the performanceof our system is F-Measure which is the harmonicmean of precision and recall.We investigated the impact of lexSimScore andexternalScore seperately on both datasets (Ta-ble 5).
Using only lexSimScore the sys-tem achieved an F-measure of 28.24% on theLexNorm1.1 dataset and 38.70% on the Trigramdataset, which shows that lexical similarity aloneis not enough for a good normalization system.320However, the externalScore which is the layer thatis more aware of the Internet jargon, along withsome social text specific rule based transliterationsperforms better than expected on both datasets.Mixing these two layers we reach our baseline thatis adopted from (Han and Baldwin, 2011).
Thisbaseline setup obtained an F-measure of 77.12%on LexNorm1.1, which is slightly better than theresult (75.30%) reported by the original systemof Han and Baldwin (2011).The results obtained by our proposed Contex-tual Word Association Graph (CWA-Graph) sys-tem on the LexNorm1.1 and trigram datasets, aswell as the results of recent studies that used thesame datasets for evaluation are presented in Ta-ble 5.
The ill-formed words are assumed to havebeen pre-identified in advance.Method Dataset Precision Recall F-measurelexSimScore LN 28.28 28.20 28.24externalScore LN 64.69 64.52 64.60lexSimScore+externalScore LN 77.22 77.02 77.12Han and Baldwin (2011) LN 75.30 75.30 75.30Liu et al.
(2012) LN 84.13 78.38 81.15Yang and Eisenstein (2013) LN 82.09 82.09 82.09CWA-Graph LN 85.50 79.22 82.24lexSimScore Trigram 39.10 38.40 38.70externalScore Trigram 44.20 43.30 43.80lexSimScore+externalScore Trigram 65.50 64.20 64.80Pennell and Liu (2011) Trigram 69.7 69.7 69.7CWA-Graph Trigram 77.2 68.8 72.8Table 5: Results obtained when ill-formed wordsare assumed to have been pre-identified in ad-vance.Our CWA-Graph approach achieves the best F-measure (82.24%) and precision (85.50%) amongthe recent previous studies.
The high precisionvalue is obtained without compromising muchfrom recall (79.22%).
Our recall is the second bestamong others.
The F-score (82.09%) obtainedby Yang and Eisenstein (2013)?s system is closeto ours and the second best F-score, which on theother hand, has a lower precision.Without any modification to our system or tothe parameters, we were able to improve the re-sults obtained by Pennell and Liu (2011) on thetrigram SMS-like dataset.
The trigram nature ofthe dataset resulted in input texts which are (shortthus) very limited with regard to contextual infor-mation.
Nevertheless, our system achieved 72.8%F-Measure using this contextual information eventhough it is limited.Along the systems (presented in Table 5) thatassume ill-formed tokens have been pre-identifiedperfectly by an oracle, there are also systems thatare not based on this assumption but contain ill-formed word identification components (Han etal., 2012; Hassan and Menezes, 2013; Chrupala,2014).
We used the method described in Section4.4 to identify the candidate tokens for normaliza-tion.
Table 6 shows our results compared with theresults of other systems that perform ill-formedword detection prior to normalization.
We couldlabel 1141 tokens correctly as ill-formed among1184 ill-formed tokens.
We achieved a word errorrate (WER) of 2.6%, where Chrupala (2014) re-ported 4.8% and Han et al.
(2012) reported 6.6%WER on the Lexnorm1.1 dataset.Method Dataset Precision Recall F-measureHan et al.
(2012) LN 70.00 17.90 28.50Hassan and Menezes (2013) LN 85.37 56.40 69.93CWA-Graph LN 85.87 76.52 80.92Table 6: Results obtained without assuming thatill-formed words have been pre-identified.As shown in Table 5 some systems have equalprecision and recall values (Yang and Eisenstein,2013; Han and Baldwin, 2011; Pennell and Liu,2011).
Those systems normalize all ill-formedwords.
On the other hand, our system does notreturn a normalization, if there are no candidatesthat are lexically similar, grammatically correct,and contextually close enough.
For this reason,we managed to achieve a higher precision com-pared to the other systems.
Our system returns anormalization candidate for an OOV word only ifit achieves a similarity score (contextual, lexical,external, or some degree of each feature) above athreshold value.
The default threshold used in thesystem is set equal to the maximum score that canbe obtained by lexical features.
Thus, we only re-trieve candidates that obtain a non-zero contextualsimilarity score (conSimScore).
The results shownat Table 7 and Table 8 demonstrate that CWA-Graph can obtain even higher values of precisionby increasing the percentage of contextual contextof candidates.
It achieved 94.1% precision on theLexNorm1.1 dataset, where the highest precisionreported at the same recall level is 85.37% (Hassanand Menezes, 2013).
The precision of the normal-ization system can be set (e.g.
as high, medium,low) depending on the application where it will beused.Our motivation behind introducing the ?
and?
parameters was to investigate the importance321conSimScore > Precision Recall F-measure0 85.5 79.2 82.20.1 88.8 75.1 81.40.2 91.1 72.8 80.90.3 92.3 67.6 78.00.5 94.1 56.4 70.5Table 7: Comparison of results for differentthreshold values on LexNorm1.1, the setup wehave used for our other experiments is shown inbold.conSimScore > Precision Recall F-measure0 77.2 68.8 72.80.1 80.9 65.8 72.60.2 84.2 60.8 70.60.3 87.6 54.6 67.30.4 89.5 47.1 61.70.5 90.8 42.1 57.6Table 8: Comparison of results for differentthreshold values on trigram dataset, the setup wehave used for our other experiments is shown inbold.of the minor features compared to our major fea-tures (described in Sections 3.3 and 3.4).
For theexperiments reported in Tables 5, 6, 7 and 8 we setthe ?
and ?
values to 0.5.
We did not tune these pa-rameters for optimized performance.
Rather, ouraim was to give less weight (half weight) to theminor features compared to the major ones.
Toanalyze the effects of the lambda and beta param-eters, we plotted the performance of the system onthe LexNorm1.1 data set by varying their values(see Figure 4).
It is shown that for ?
and ?
valuesgreater than 0.3 the performance of the system isquite robust.
The F-score varies between 80.4%and 82.9%.Figure 4: The effect of ?
and ?
on the system per-formance.5 ConclusionIn this paper, we present an unsupervised graph-based approach for contextual text normalization.The task of normalization is highly dependent onunderstanding and capturing the dynamics of theinformal nature of social text.
Our word associ-ation graph is built using a large unlabeled socialmedia corpus.
It helps to derive contextual analy-sis on both clean and noisy data.It is important to emphasize the difference be-tween corpus based contextual information andcontextual information of the input text (input con-text).
Most recent unsupervised systems for textnormalization only make use of corpus based con-text information.
However, this approach is ledby statistical information.
In other words, it findswhich IV word the OOV word is commonly nor-malized to, regardless of the context of the OOVword in the input text message.
A major strengthof our approach is that it utilizes both corpus basedcontextual information and input based contextualinformation.
We use corpus based statistical infor-mation to connect/associate the words in the con-textual word association graph.
On the other hand,the neighbors of an OOV word in the input textprovide us input based context information.
Usinginput context to find normalizations helps us iden-tify the correct normalization, even if it is not thestatistically dominant one.We compared our approach with the recentsocial media text normalization systems andachieved state-of-the-art precision and F-measurescores.
We reported our results on two datasets.The first one is the standard text normalizationdataset (Lexnorm1.1) derived from Twitter.
Ourresults on this dataset showed that our system canserve as a high precision text normalization sys-tem which is highly preferable as an NLP pre-processing step.
The second dataset we testedour approach is a SMS-like trigram dataset.
Thetests showed that the proposed system can performgood on SMS data as well.The system does not require a clean corpus oran annotated corpus.
The contextual word asso-ciation graph can be built by using the publiclyavailable social media text.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
APhrase-based Statistical Model for SMS Text Nor-322malization.
Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for Computa-tional Linguistics, pages 33?40.Timothy Baldwin and Marco Lui.
2010.
LanguageIdentification: The Long and the Short of the Matter.Human Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages229?237.Eric Brill and Robert C. Moore.
2000.
An ImprovedError Model for Noisy Channel Spelling Correction.Proceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, pages 286?293.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and Modeling of the Structureof Texting Language.
International Journal on Doc-ument Analysis and Recognition, 10(3):157?174.Grzegorz Chrupala.
2014.
Normalizing tweets withedit scripts and recurrent neural embeddings.
Pro-ceedings of the 52st Annual Meeting of the Associa-tion for Computational Linguistics, pages 680?686.Danish Contractor, Tanveer A. Faruquie, andL.
Venkata Subramaniam.
2010.
Unsuper-vised Cleansing of Noisy Text.
Proceedings of the23rd International Conference on ComputationalLinguistics: Posters, pages 189?196.Paul Cook and Suzanne Stevenson.
2009.
An Un-supervised Model for Text Message Normalization.Proceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity, pages 71?78.Jacob Eisenstein.
2013.
What to Do About Bad Lan-guage on the Internet.
Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics : Human Language Technologies,pages 359?369.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech Taggingfor Twitter: Annotation, Features, and Experiments.Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: Short Papers - Volume 2, pages42?47.Stephan Gouws, Donald Metzler, Congxing Cai, andEduard Hovy.
2011.
Contextual Bearing on Lin-guistic Variation in Social Media.
Proceedings ofthe Workshop on Languages in Social Media, pages20?29.Bo Han and Timothy Baldwin.
2011.
Lexical Normal-isation of Short Text Messages: Makn Sens a #Twit-ter.
Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, pages 368?378.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Au-tomatically constructing a normalisation dictionaryfor microblogs.
Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 421?432.Hany Hassan and Arul Menezes.
2013.
SocialText Normalization Using Contextual Graph Ran-dom Walks.
Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 1577?1586.Max Kaufmann and Jugal Kalita.
2010.
Syntactic Nor-malization of Twitter Messages.
Proceedings of the8th International Conference on Natural LanguageProcessing, pages 149?158.Vladimir Iosifovich Levenshtein.
1966.
Binary CodesCapable of Correcting Deletions, Insertions and Re-versals.
Soviet Physics Doklady, 10:707.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
ABroad-Coverage Normalization System for SocialMedia Language.
Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Long Papers-Volume 1, pages 1035?1044.Marco Lui and Timothy Baldwin.
2012.
Langid.Py:An Off-the-shelf Language Identification Tool.
Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations, pages 25?30.I.
Dan Melamed.
1999.
Bitext Maps and Alignmentvia Pattern Recognition.
Computational Linguistics,25(1):107?130.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved Part-of-Speech Taggingfor Online Conversational Text with Word Clusters.Proceedings of the North American Chapter of theAssociation for Computational Linguistics : HumanLanguage Technologies, pages 380?390.Deana Pennell and Yang Liu.
2011.
A Character-Level Machine Translation Approach for Normal-ization of SMS Abbreviations.
Fifth InternationalJoint Conference on Natural Language Processing,pages 974?982.Deana Pennell and Yang Liu.
2014.
Normalizationof informal text.
Computer Speech & Language,28(1):256 ?
277.Lawrence Philips.
2000.
The Double Meta-phone Search Algorithm.
C/C++ Users Journal,18(6):38?43, June.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 172?180.323Richard Sproat, Alan W. Black, Stanley Chen, ShankarKumar, Mari Ostendorf, and Christopher Richards.2001.
Normalization of Non-Standard Words.Computer Speech & Language, 15(3):287?333.Kristina Toutanova and Robert C. Moore.
2002.
Pro-nunciation Modeling for Improved Spelling Correc-tion.
Proceedings of the 40th Annual Meeting on As-sociation for Computational Linguistics, pages 144?151.Yi Yang and Jacob Eisenstein.
2013.
A Log-LinearModel for Unsupervised Text Normalization.
Pro-ceedings of the Empirical Methods on Natural Lan-guage Processing, pages 61?72.Jaewon Yang and Jure Leskovec.
2011.
Patterns ofTemporal Variation in Online Media.
Proceedingsof the Forth International Conference onWeb Searchand Web Data Mining, pages 177?186.324
