Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 39?48,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLearning word-level dialectal variation as phonological replacement rulesusing a limited parallel corpusMans HuldenUniversity of HelsinkiLanguage Technologymans.hulden@helsinki.fiIn?aki AlegriaIXA taldeaUPV-EHUi.alegria@ehu.esIzaskun EtxeberriaIXA taldeaUPV-EHUizaskun.etxeberria@ehu.esMontse MaritxalarIXA taldeaUPV-EHUmontse.maritxalar@ehu.esAbstractThis paper explores two different methods oflearning dialectal morphology from a smallparallel corpus of standard and dialect-formtext, given that a computational descriptionof the standard morphology is available.
Thegoal is to produce a model that translates in-dividual lexical dialectal items to their stan-dard dialect counterparts in order to facili-tate dialectal use of available NLP tools thatonly assume standard-form input.
The resultsshow that a learning method based on induc-tive logic programming quickly converges tothe correct model with respect to many phono-logical and morphological differences that areregular in nature.1 IntroductionIn our work with the Basque language, a morpho-logical description and analyzer is available for thestandard language, along with other tools for pro-cessing the language (Alegria et al, 2002).
How-ever, it would be convenient to be able to analyzevariants and dialectal forms as well.
As the dialectaldifferences within the Basque language are largelylexical and morphophonological, analyzing the di-alectal forms would in effect require a separate mor-phological analyzer that is able to handle the uniquelexical items in the dialect together with the differ-ing affixes and phonological changes.Morphological analyzers are traditionally hand-written by linguists, most commonly using somevariant of the popular finite-state morphology ap-proach (Beesley and Karttunen, 2002).
This entailshaving an expert model a lexicon, inflectional andderivational paradigms as well as phonological al-ternations, and then producing a morphological an-alyzer/generator in the form of a finite-state trans-ducer.As the development of such wide-coverage mor-phological analyzers is labor-intesive, the hope isthat an analyzer for a variant could be automaticallylearned from a limited parallel standard/dialect cor-pus, given that an analyzer already exists for thestandard language.
This is an interesting problembecause a good solution to it could be applied tomany other tasks as well: to enhancing access todigital libraries (containing diachronic and dialectalvariants), for example, or to improving treatment ofinformal registers such as SMS messages and blogs,etc.In this paper we evaluate two methods of learninga model from a standard/variant parallel corpus thattranslates a given word of the dialect to its standard-form equivalent.
Both methods are based on finite-state phonology.
The variant we use for experimentsis Lapurdian,1 a dialect of Basque spoken in the La-purdi (fr.
Labourd) region in the Basque Country.Because Basque is an agglutinative, highly in-flected language, we believe some of the results canbe extrapolated to many other languages facing sim-ilar challenges.One of the motivations for the current work isthat there are a large number of NLP tools avail-able and in development for standard Basque (alsocalled Batua): a morphological analyzer, a POS tag-ger, a dependency analyzer, an MT engine, among1Sometimes also called Navarro-Labourdin or Labourdin.39others (Alegria et al, 2011).
However, these toolsdo not work well in processing the different dialectsof Basque where lexical items have a different ortho-graphic representation owing to slight differences inphonology and morphology.Here is a brief contrastive example of the kindsof differences found in the (a) Lapurdian dialect andstandard Basque (b) parallel corpus:2(a) Ez gero uste izan nexkatxa guziek tu egiten dautatela(b) Ez gero uste izan neskatxa guztiek tu egiten didatelaAs the example illustrates, the differences are mi-nor overall?the word order and syntax are unaf-fected, and only a few lexical items differ.
This re-flects the makeup of our parallel corpus quite well?in it, slightly less than 20% of the word tokensare distinct.
However, even such relatively smalldiscrepancies cause great problems in the poten-tial reuse of current tools designed for the standardforms only.We have experimented with two approaches thatattempt to improve on a simple baseline of mem-orizing word-pairs in the dialect and the standard.The first approach is based on work by Almeidaet al (2010) on contrasting orthography in Brazil-ian Portuguese and European Portuguese.
In thisapproach differences between substrings in distinctword-pairs are memorized and these transformationpatterns are then applied whenever novel words areencountered in the evaluation.
To prevent over-generation, the output of this learning process islater subject to a morphological filter where only ac-tual standard-form outputs are retained.
The secondapproach is an Inductive Logic Programming-style(ILP) (Muggleton and De Raedt, 1994) learningalgorithm where phonological transformation rulesare learned from word-pairs.
The goal is to find aminimal set of transformation rules that is both nec-essary and sufficient to be compatible with the learn-ing data, i.e.
the word pairs seen in the training data.The remainder of the paper is organized as fol-lows.
The characteristics of the corpus available tous are described in section 2.
In sections 3, 4, and 5,we describe the steps and variations of the methodswe have applied and how they are evaluated.
Sec-tion 6 presents the experimental results, and finally,2English translation of the example: Don?t think all girls spiton mewe discuss the results and present possibilities forpotential future work in section 7.1.1 Related workThe general problem of supervised learning of di-alectal variants or morphological paradigms hasbeen discussed in the literature with various connec-tion to computational phonology, morphology, ma-chine learning, and corpus-based work.
For exam-ple, Kestemont et al (2010) presents a language-independent system that can ?learn?
intra-lemmaspelling variation.
The system is used to producea consistent lemmatization of texts in Middle Dutchliterature in a medieval corpus, Corpus-Gysseling,which contains manuscripts dated before 1300 AD.These texts have enormous spelling variation whichmakes a computational analysis difficult.Koskenniemi (1991) provides a sketch of a dis-covery procedure for phonological two-level rules.The idea is to start from a limited number ofparadigms (essentially pairs of input-output formswhere the input is the surface form of a word and theoutput a lemmatization plus analysis).
The problemof finding phonological rules to model morpholog-ical paradigms is essentially similar to the problempresented in this paper.
An earlier paper, Johnson(1984), presents a ?discovery procedure?
for learningphonological rules from data, something that can beseen as a precursor to the problem dealt with by ourILP algorithm.Mann and Yarowsky (2001) present a methodfor inducing translation lexicons based on transduc-tion models of cognate pairs via bridge languages.Bilingual lexicons within languages families are in-duced using probabilistic string edit distance mod-els.
Inspired by that paper, Scherrer (2007) usesa generate-and-filter approach quite similar to ourfirst method.
He compares different measures ofgraphemic similarity applied to the task of bilin-gual lexicon induction between Swiss German andStandard German.
Stochastic transducers are trainedwith the EM algorithm and using handmade trans-duction rules.
An improvement of 11% in F-score isreported over a baseline method using LevenshteinDistance.40Full corpus 80% part.
20% part.Sentences 2,117 1,694 423Words 12,150 9,734 2,417Unique wordsStandard Basque 3,553 3,080 1,192Lapurdian 3,830 3,292 1,239Filtered pairs 3,610 3,108 1,172Identical pairs 2,532 2,200 871Distinct pairs 1,078 908 301Table 1: Characteristics of the parallel corpus used forexperiments.2 The corpusThe parallel corpus used in this research is part of?TSABL?
project developed by the IKER group inBaiona (fr.
Bayonne).3 The researchers of the IKERproject have provided us with examples of the La-purdian dialect and their corresponding forms instandard Basque.
Our parallel corpus then containsrunning text in two variants: complete sentences ofthe Lapurdian dialect and equivalent sentences instandard Basque.The details of the corpus are presented in table 1.The corpus consists of 2,117 parallel sentences, to-taling 12,150 words (roughly 3,600 types).
In orderto provide data for our learning algorithms and alsoto test their performance, we have divided the cor-pus into two parts: 80% of the corpus is used for thelearning task (1,694 sentences) and the remaining20% (423 sentences) for evaluation of the learningprocess.
As is seen, roughly 23% of the word-pairsare distinct.
Another measure of the average devi-ation between the word pairs in the corpus is givenby aligning all word-pairs by minimum edit distance(MED): aligning the 3,108 word-pairs in the learn-ing corpus can be done at a total MED cost of 1,571.That is, roughly every 14th character in the dialectdata is different from the standard form.3 The baselineThe baseline of our experiments is a simple method,based on a dictionary of equivalent words with thelist of correspondences between words extracted3Towards a Syntactic Atlas of the Basque Language, website: http://www.iker.cnrs.fr/-tsabl-towards-a-syntactic-atlas-of-.htmlfrom the learning portion (80%) of the corpus.
Thislist of correspondences contains all different wordpairs in the variant vs. standard corpus.
The baselineapproach consists simply of memorizing all the dis-tinct word pairs seen between the dialectal and stan-dard forms, and subsequently applying this knowl-edge during the evaluation task.
That is, if an in-put word during the evaluation has been seen in thetraining data, we provide the corresponding previ-ously known output word as the answer.
Otherwise,we assume that the output word is identical to theinput word.4 Overview of methodsWe have employed two different methods to producean application that attempts to extract generaliza-tions from the training corpus to ultimately be ableto produce the equivalent standard word correspond-ing to a given dialectal input word.
The first methodis based on already existing work by Almeida et al(2010) that extracts all substrings from lexical pairsthat are different.
From this knowledge we then pro-duce a number of phonological replacement rulesthat model the differences between the input andoutput words.
In the second method, we likewiseproduce a set of phonological replacement rules, us-ing an ILP approach that directly induces the rulesfrom the pairs of words in the training corpus.The core difference between the two methods isthat while both extract replacement patterns fromthe word-pairs, the first method does not considernegative evidence in formulating the replacementrules.
Instead, the existing morphological analyzeris used as a filter after applying the rules to unknowntext.
The second method, however, uses negativeevidence from the word-pairs in delineating the re-placement rules as is standard in ILP-approaches,and the subsequent morphological filter for the out-put plays much less of a role.
Evaluating and com-paring both approaches is motivated because the firstmethod may produce much higher recall by virtueof generating a large number of input-output candi-dates during application, and the question is whetherthe corresponding loss in precision may be mitigatedby judicious application of post-processing filters.414.1 Format of rulesBoth of the methods we have evaluated involvelearning a set of string-transformation rules toconvert words, morphemes, or individual letters(graphemes) in the dialectal forms to the stan-dard variant.
The rules that are learned are inthe format of so-called phonological replacementrules (Beesley and Karttunen, 2002) which we havelater converted into equivalent finite-state transduc-ers using the freely available foma toolkit (Hulden,2009a).
The reason for the ultimate conversion ofthe rule set to finite-state transducers is twofold:first, the transducers are easy to apply rapidly toinput data using available tools, and secondly, thetransducers can further be modified and combinedwith the standard morphology already available tous as a finite transducer.In its simplest form, a replacement rule is of theformatA?
B || C D (1)where the arguments A,B,C,D are all single sym-bols or strings.
Such a rule dictates the transfor-mation of a string A to B, whenever the A occursbetween the strings C and D. Both C and D areoptional arguments in such a rule, and there maybe multiple conditioning environments for the samerule.For example, the rule:h -> 0 || p , t , l , a s o(2)would dictate a deletion of h in a number of con-texts; when the h is preceded by a p, t, or l, or suc-ceeded by the sequence aso, for instance transform-ing ongiethorri (Lapurdian) to ongietorri (Batua).As we will be learning several rules that each tar-get different input strings, we have a choice as to themode of application of the rules in the evaluationphase.
The learned rules could either be applied insome specific order (sequentially), or applied simul-taneously without regard to order (in parallel).For example, the rules:u -> i || z a (3)k -> g || z a u (4)would together (in parallel) change zaukun into zai-gun.
Note that if we imposed some sort of orderingon the rules and the u ?
i rule in the set wouldapply first, for example, the conditioning environ-ment for the second rule would no longer be metafter transforming the word into zaikun.
We haveexperimented with sequential as well as parallel pro-cessing, and the results are discussed below.4.2 Method 1 (lexdiff) detailsThe first method is based on the idea of identi-fying sequences inside word pairs where the out-put differs from the input.
This was done throughthe already available tool lexdiff which has beenused in automatic migration of texts between differ-ent Portuguese orthographies (Almeida et al, 2010).The lexdiff program tries to identify sequences ofchanges from seen word pairs and outputs string cor-respondences such as, for example: 76 ait ->at ; 39 dautz -> diz (stemming from pairssuch as (joaiten/joaten and dautzut/dizut), indicatingthat ait has changed into at 76 times in the cor-pus, etc., thus directly providing suggestions as tophonologically regular changes between two texts,with frequency information included.With such information about word pairs we gen-erate a variety of replacement rules which are thencompiled into finite transducers with the foma ap-plication.
Even though the lexdiff program providesa direct string-to-string change in a format that isdirectly compilable into a phonological rule trans-ducer, we have experimented with some possiblevariations of the specific type of phonological rulewe want to output:?
We can restrict the rules by frequency and re-quire that a certain type of change be seen atleast n times in order to apply that rule.
Forexample, if we set this threshold to 3, we willonly apply a string-to-string changing rule thathas been seen three or more times.?
We limit the number of rules that can beapplied to the same word.
Sometimes thelexdiff application divides the change be-tween a pair of words into two separate rules.For example the word-word correspondenceagerkuntza/agerpena is expressed by two rules:rkun -> rpen and ntza -> na.
Now,given these two rules, we have to be able toapply both to produce the correct total change42Figure 1: The role of the standard Basque (Batua) ana-lyzer in filtering out unwanted output candidates createdby the induced rule set produced by method 1.agerkuntza/agerpena.
By limiting the numberof rules that can apply to a single input word wecan avoid creating many spurious outputs, butalso at the same time we may sacrifice someability to produce the desired output forms.?
We can also control the application mode of therules: sequential or parallel.
If the previoustwo rules are applied in parallel, the form ob-tained from agerkuntza will not be correctsince the n overlaps with the two rules.
Thatis, when applying rules simultaneously in par-allel, the input characters for two rules may notoverlap.
However, if these two rules appliedin sequence (the order in this example is irrel-evant), the output will be the correct: we firstchange rkun -> rpen and later ntza ->na.
We have not a priori chosen to use parallelor sequential rules and have decided to evaluateboth approaches.?
We can also compact the rules output by lex-diff by eliminating redundancies and construct-ing context-sensitive rules.
For example: givena rule such as rkun -> rpen, we can con-vert this into a context-sensitive rule that onlychanges ku into pe when flanked by r and nto the left and right, respectively, i.e.
producinga rule:k u -> p e || r n (5)This has a bearing on the previous point andwill allow more rewritings within a single wordin parallel replacement mode since there arefewer characters overlapping.Once a set of rules is compiled with some instanti-ation of the various parameters discussed above andconverted to a transducer, we modify the transducerin various ways to improve on the output.First, since we already have access to a large-scalemorphological transducer that models the standardBasque (Batua), we restrict the output from the con-version transducer to only allow those words as out-put that are legitimate words in standard Basque.Figure 1 illustrates this idea.
In that figure, we see aninput word in the dialect (emaiten) produce a num-ber of candidates using the rules induced.
However,after adding a morphological filter that models theBatua, we retain only one output.Secondly, in the case that even after applyingthe Batua filter we retain multiple outputs, we sim-ply choose the most frequent word (these unigramcounts are gathered from a separate newspaper cor-pus of standard Basque).4.3 Method 2 (ILP) detailsThe second method we have employed worksdirectly from a collection of word-pairs (di-alect/standard in this case).
We have developed analgorithm that from a collection of such pairs seeksa minimal hypothesis in the form of a set of replace-ment rules that is consistent with all the changesfound in the training data.
This approach is gener-ally in line with ILP-based machine learning meth-ods (Muggleton and De Raedt, 1994).
However, incontrast to the standard ILP, we do not learn state-ments of first-order logic that fit a collection of data,but rather, string-to-string replacement rules.44Phonological string-to-string replacement rules can be de-fined as collections of statements in first-order logic and com-piled into transducers through such logical statements as well;43The two parameters to be induced are (1) the col-lection of string replacements X ?
Y needed tocharacterize the training data, and (2) the minimalconditioning environments for each rule, such thatthe collection of rules model the string transforma-tions found in the training data.The procedure employed for the learning task isas follows:(1) Align all word pairs (using minimum edit dis-tance by default).
(2) Extract a collection of phonological rewriterules.
(3) For each rule, find counterexamples.
(4) For each rule, find the shortest conditioning en-vironment such that the rule applies to all pos-itive examples, and none of the negative exam-ples.
Restrict rule to be triggered only in thisenvironment.The following simple example should illustratethe method.
Assuming we have a corpus of onlytwo word pairs:emaiten ematenigorri igorriin step (1) we would perform the alignment and pro-duce the outpute m a i t e n i g o r r ie m a ?
t e n i g o r r iFrom this data we would in step (2) gather thatthe only active phonological rule is i ?
?, sinceall other symbols are unchanged in the data.
How-ever, we find two counterexamples to this rule (step3), namely two i-symbols in igorri which do not al-ternate with ?.
The shortest conditioning environ-ment that accurately models the data and producesno overgeneration (does not apply to any of the is inigorri) is therefore:i -> ?
|| a (6)see e.g.
Hulden (2009b) for details.
In other words, in thiswork, we skip the intermediate step of defining our observa-tions as logical statements and directly convert our observationsinto phonological replacement rules.the length of the conditioning environment being 1(1 symbol needs to be seen to the left plus zero sym-bols to the right).
Naturally, in this example we havetwo competing alternatives to the shortest general-ization: we could also have chosen to condition thei-deletion rule by the t that follows the i.
Both con-ditioning environments are exactly one symbol long.To resolve such cases, we a priori choose to favorconditioning environments that extend farther to theleft.
This is an arbitrary decision?albeit one thatdoes have some support from phonology as mostphonological assimilation rules are conditioned bypreviously heard segments?and very similar resultsare obtained regardless of left/right bias in the learn-ing.
Also, all the rules learned with this method areapplied simultaneously (in parallel) in the evaluationphase.4.3.1 String-to-string vs. single-symbol rulesIn some cases several consecutive input symbolsfail to correspond to the output in the learning data,as in for example the pairingd a u td i ?
tcorresponding to the dialect-standard pair daut/dit.Since there is no requirement in our formalism ofrewrite rules that they be restricted to single-symbolrewrites only, there are two ways to handle this: ei-ther one can create a string-to-string rewriting rule:au?
i / CONTEXTor create two separate rulesa?
i / CONTEXT , u?
?
/ CONTEXTwhere CONTEXT refers to the minimal condition-ing environment determined by the rest of the data.We have evaluated both choices, and there is no no-table difference between them in the final results.5 EvaluationWe have measured the quality of different ap-proaches by the usual parameters of precision, re-call and the harmonic combination of them, the F1-score, and analyzed how the different options in thetwo approaches affect the results of these three pa-rameters.
Given that we, especially in method 1,extract quite a large number of rules and that each44input word generates a very large number of candi-dates if we use all the rules extracted, it is possible toproduce a high recall on the conversion of unknowndialect words to the standard form.
However, thedownside is that this naturally leads to low precisionas well, which we try to control by introducing anumber of filters to remove some of the candidatesoutput by the rules.
As mentioned above, we usetwo filters: (1) an obligatory filter which removesall candidate words that are not found in the stan-dard Basque (by using an existing standard Basquemorphological analyzer), and (2) using an optionalfilter which, given several candidates in the standardBasque, picks the most frequently occurring one bya unigram count from the separate newspaper cor-pus.
This latter filter turns out to serve a much moreprominent role in improving the results of method 1,while it is almost completely negligible for method2.6 ResultsAs mentioned above, the learning process has madeuse of 80% of the corpus, leaving 20% of the corpusfor evaluation of the above-mentioned approaches.In the evaluation, we have only tested those wordsin the dialect that differ from words in the standard(which are in the minority).
In total, in the evalu-ation part, we have tested the 301 words that differbetween the dialect and the standard in the evalua-tion part of the corpus.The results for the baseline?i.e.
simple memo-rization of word-word correspondences?are (in %):P = 95.62, R = 43.52 and F1 = 59.82.
As ex-pected, the precision of the baseline is high: whenthe method gives an answer it is usually the correctone.
But the recall of the baseline is low, as is ex-pected: slightly less than half the words in the eval-uation corpus have been encountered before.56.1 Results with the lexdiff methodTable 2 shows the initial experiment of method1 with different variations on the frequency5The reason the baseline does not show 100% precision isthat the corpus contains minor inconsistencies or accepted al-ternative spellings, and our method of measuring the precisionsuffers from such examples by providing both learned alterna-tives to a dialectal word, while only one is counted as beingcorrect.P R F1f ?
1 38.95 66.78 49.20f ?
2 46.99 57.14 51.57f ?
3 49.39 53.82 51.51Table 2: Values obtained for Precision, Recall and F-scores with method 1 by changing the minimum fre-quency of the correspondences to construct rules forfoma.
The rest of the options are the same in all threeexperiments: only one rule is applied within a word.P R F1f ?
1 70.28 58.13 63.64f ?
2 70.18 53.16 60.49f ?
3 71.76 51.50 59.96Table 3: Values obtained for Precision, Recall and F-score with method 1 by changing the threshold frequencyof the correspondences and applying a post-filter.threshold?this is the limit on the number of timeswe must see a string-change to learn it.
The re-sults clearly show that the more examples we extract(frequency 1), the better results we obtain for recallwhile at the same time the precision suffers sincemany spurious outputs are given?even many differ-ent ones that each legitimately correspond to a wordin the standard dialect.
The F1-score doesn?t varyvery much and it maintains similar values through-out.
The problem with this approach is one whichwe have noted before: the rules produce a largenumber of outputs for any given input word andthe consequence is that the precision suffers, eventhough only those output words are retained that cor-respond to actual standard Basque.With the additional unigram filter in place, theresults improve markedly.
The unigram-filtered re-sults are given in table 3.We have also varied the maximum number ofpossible rule applications within a single word aswell as applying the rules in parallel or sequentially,and compacting the rules to provide more context-sensitivity.
We shall here limit ourselves to present-ing the best results of all these options in terms ofthe F1-score in table 4.In general, we may note that applying more than45P R F1Exp1 72.20 57.81 64.21Exp2 72.13 58.47 64.59Exp3 75.10 60.13 66.79Table 4: Method 1.
Exp1: frequency 2; 2 rules applied;in parallel; without contextual conditioning.
Exp2: fre-quency 1; 1 rule applied; with contextual conditioning.Exp3: frequency 2; 2 rules applied; in parallel; with con-textual conditioning.one rule within a word has a negative effect onthe precision while not substantially improving therecall.
Applying the unigram filter?choosing themost frequent candidate?yields a significant im-provement: much better precision but also slightlyworse recall.
Choosing either parallel or sequentialapplication of rules (when more than one rule is ap-plied to a word) does not change the results signifi-cantly.
Finally, compacting the rules and producingcontext-sensitive ones is clearly the best option.In all cases the F1-score improves if the unigramfilter is applied; sometimes significantly and some-times only slightly.
All the results of the table 4which lists the best performing ones come from ex-periments where the unigram filter was applied.Figure 2 shows how precision and recall val-ues change in some of the experiments done withmethod 1.
There are two different groups of pointsdepending on if the unigram filter is applied, illus-trating the tradeoff in precision and recall.6.2 Results with the ILP methodThe ILP-based results are clearly better overall, andit appears that the gain in recall by using method1 does not produce F1-scores above those producedwith the ILP-method, irrespective of the frequencyfilters applied.
Crucially, the negative evidenceand subsequent narrowness of the replacement ruleslearned with the ILP method is responsible for thehigher accuracy.
Also, the results from the ILP-based method rely very little on the post-processingfilters, as will be seen.The only variable parameter with the ILP methodconcerns how many times a word-pair must be seento be used as learning evidence for creating a re-placement rule.
As expected, the strongest result0.40.50.60.70.80.910  0.2  0.4  0.6  0.8  1RecallPrecisionP vs Rwithout filterwith filterFigure 2: Tradeoffs of precision and recall values in theexperiments with method 1 using various different pa-rameters.
When the unigram filter is applied the precisionis much better, but the recall drops.P R F1n = 1 85.02 (86.13) 58.47 (57.80) 69.29 (69.18)n = 2 82.33 (83.42) 54.15 (53.49) 65.33 (65.18)n = 3 80.53 (82.07) 50.83 (50.17) 62.32 (62.26)n = 4 81.19 (82.32) 50.17 (49.50) 62.01 (61.83)Table 5: Experiments with the ILP method using a thresh-old of 1?4 (times a word-pair is seen) to trigger rule learn-ing.
The figures in parentheses are the same results withthe added postprocessing unigram filter that, given sev-eral output candidates of the standard dialect, chooses themost frequent one.is obtained by using all word-pairs, i.e.
setting thethreshold to 1.
Table 5 shows the degradation of per-formance resulting from using higher thresholds.Interestingly, adding the unigram filter that im-proved results markedly in method 1 to the output ofthe ILP method slightly worsens the results in mostcases, and gives no discernible advantage in others.In other words, in those cases where the method pro-vides multiple outputs, choosing the most frequentone on a unigram frequency basis gives no improve-ment over not doing so.Additionally, there is comparatively little advan-tage with this method in adding the morphologicalfilter to the output of the words in method 2 (thisis the filter that rules out non-standard words).
Theresults in table 5 include the morphological filter,but omitting it altogether brings down the best F146P R F1Baseline 95.62 43.52 59.82Method 1 (lexdiff) 75.10 60.13 66.79Method 2 (ILP) 85.02 58.47 69.29Table 6: The best results (per F1-score of the two meth-ods).
The parameters of method 1 included using onlythose string transformations that occur at least 2 times inthe training data, and limiting rule application to a maxi-mum of 2 times within a word, and including a unigrampost-filter.
Rules were contextually conditioned.
Formethod 2, all the examples (threshold 1) in the trainingdata were used as positive and negative evidence, with-out a unigram filter.to 56.14 from 69.29.
By contrast, method 1 de-pends heavily on it and omitting the filter bringsdown the F1-score from 66.79 to 11.53 with theotherwise strongest result of method 1 seen in ta-ble 6.
The most prominent difference between thetwo approaches is that while method 1 can be fine-tuned using frequency information and various fil-ters to yield results close to method 2, the ILP ap-proach provides equally robust results without anyadditional information?in particular, frequency in-formation of the target language.
We also find amuch lower rate of errors of commission with theILP method; this is somewhat obvious as it takes ad-vantage of negative evidence directly while the firstmethod only does so indirectly through filters addedlater.7 Conclusions and future workWe have presented a number of experiments to solvea very concrete task: given a word in the Lapurdiandialect of Basque, produce the equivalent standardBasque word.
As background knowledge, we havea complete standard Basque morphological analyzerand a small parallel corpus of dialect and standardtext.
The approach has been based on the idea ofextracting string-to-string transformation rules fromthe parallel corpus, and applying these rules to un-seen words.
We have been able to improve on theresults of a naive baseline using two methods to in-fer phonological rules of the information extractedfrom the corpus and applying them with finite statetransducers.
In particular, the second method, in-ferring minimal phonological rewrite rules usingan Inductive Logic Programming-style approach,seems promising as regards inferring phonologicaland morphological differences that are quite regu-lar in nature between the two language variants.
Weexpect that a larger parallel corpus in conjunctionwith this method could potentially improve the re-sults substantially?with a larger set of data, thresh-olds could be set so that morphophonological gener-alizations are triggered only after a sufficient num-ber of training examples (avoiding overgeneration),and, naturally, many more unique, non-regular, lexi-cal correspondences could be learned.During the current work, we have also accumu-lated a small but valuable training and test corpuswhich may serve as a future resource for evaluationof phonological and morphological rule inductionalgorithms.In order to improve the results, we plan to re-search the combination of the previous methods withother ones which infer dialectal paradigms and rela-tions between lemmas and morphemes for the di-alect and the standard.
These inferred relationscould be contrasted with the information of a largercorpus of the dialect without using an additional par-allel corpus.AcknowledgmentsWe are grateful for the insightful commentsprovided by the anonymous reviewers.
This re-search has been partially funded by the SpanishScience and Innovation Ministry via the OpenMT2project (TIN2009-14675-C03-01) and the EuropeanCommission?s 7th Framework Program under grantagreement no.
238405 (CLARA).ReferencesAlegria, I., Aranzabe, M., Arregi, X., Artola, X.,D?
?az de Ilarraza, A., Mayor, A., and Sarasola, K.(2011).
Valuable language resources and applica-tions supporting the use of Basque.
In Vetulani,Z., editor, Lecture Notes in Artifitial Intelligence,volume 6562, pages 327?338.
Springer.Alegria, I., Aranzabe, M., Ezeiza, N., Ezeiza, A.,and Urizar, R. (2002).
Using finite state tech-nology in natural language processing of basque.47In LNCS: Implementation and Application of Au-tomata, volume 2494, pages 1?12.
Springer.Almeida, J. J., Santos, A., and Simoes, A.(2010).
Bigorna?a toolkit for orthography migra-tion challenges.
In Seventh International Con-ference on Language Resources and Evaluation(LREC2010), Valletta, Malta.Beesley, K. R. and Karttunen, L. (2002).
Finite-statemorphology: Xerox tools and techniques.
Stud-ies in Natural Language Processing.
CambridgeUniversity Press.Hulden, M. (2009a).
Foma: a finite-state compilerand library.
In Proceedings of the 12th Confer-ence of the European Chapter of the Associationfor Computational Linguistics: DemonstrationsSession, pages 29?32, Athens, Greece.
Associa-tion for Computational Linguistics.Hulden, M. (2009b).
Regular expressions and pred-icate logic in finite-state language processing.
InPiskorski, J., Watson, B., and Yli-Jyra?, A., edi-tors, Finite-State Methods and Natural LanguageProcessing?Post-proceedings of the 7th Interna-tional Workshop FSMNLP 2008, volume 191 ofFrontiers in Artificial Intelligence and Applica-tions, pages 82?97.
IOS Press.Johnson, M. (1984).
A discovery procedure for cer-tain phonological rules.
In Proceedings of the10th international conference on Computationallinguistics, COLING ?84, pages 344?347.
Asso-ciation for Computational Linguistics.Kestemont, M., Daelemans, W., and Pauw, G. D.(2010).
Weigh your words?memory-basedlemmatization for Middle Dutch.
Literary andLinguistic Computing, 25(3):287?301.Koskenniemi, K. (1991).
A discovery procedure fortwo-level phonology.
Computational Lexicologyand Lexicography: A Special Issue Dedicated toBernard Quemada, pages 451?446.Mann, G. S. and Yarowsky, D. (2001).
Multi-path translation lexicon induction via bridge lan-guages.
In Proceedings of the second meeting ofthe North American Chapter of the Associationfor Computational Linguistics on Language tech-nologies, NAACL ?01, pages 1?8.Muggleton, S. and De Raedt, L. (1994).
InductiveLogic Programming: theory and methods.
TheJournal of Logic Programming, 19:629?679.Scherrer, Y.
(2007).
Adaptive string distance mea-sures for bilingual dialect lexicon induction.
InProceedings of the 45th Annual Meeting of theACL: Student Research Workshop, ACL ?07,pages 55?60.
Association for Computational Lin-guistics.48
