Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1266?1277, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsEntity based Q&A retrievalAmit SinghIBM ResearchBangalore, Indiaamising3@in.ibm.comAbstractBridging the lexical gap between the user?squestion and the question-answer pairs in theQ&A archives has been a major challenge forQ&A retrieval.
State-of-the-art approachesaddress this issue by implicitly expanding thequeries with additional words using statisticaltranslation models.
While useful, the effec-tiveness of these models is highly dependanton the availability of quality corpus in the ab-sence of which they are troubled by noise is-sues.
Moreover these models perform wordbased expansion in a context agnostic mannerresulting in translation that might be mixedand fairly general.
This results in degradedretrieval performance.
In this work we ad-dress the above issues by extending the lex-ical word based translation model to incor-porate semantic concepts (entities).
We ex-plore strategies to learn the translation proba-bilities between words and the concepts usingthe Q&A archives and a popular entity cata-log.
Experiments conducted on a large scalereal data show that the proposed techniquesare promising.1 IntroductionOver the past few years community-based ques-tion answering (CQA) portals like Naver, Ya-hoo!
Answers, Baidu Zhidao and WikiAnswershave attracted great attention from both academiaand industry (Adamic et al 2008; Singh andVisweswariah, 2011).
These portals foster collab-orative creation of content by allowing the users toboth submit questions to be answered and answerquestions asked by other users.
These portals aimto provide highly focused access to this informationby directly returning pertinent question and answer(Q&A) pairs to the users questions, instead of a longlist of ranked URLs.
This is in noted contrast to theusual search paradigm, where the question is used tosearch the database of potential answers, in this casethe question is used to search the database of pre-vious questions, which in turn are associated withanswers.
This involves addressing the word mis-match problem between the users question and thequestion-answer pairs in the archive.
This is the ma-jor challenge for Q&A retrieval.Researchers have proposed the use of translationmodels (Berger and Lafferty, 1999; Jeon et al 2005;Xue et al 2008) to solve this problem.
As a princi-pled approach to capturing semantic word relations,statistical translation language models are built byusing the IBM model 1 (Brown et al 1993) andhave been shown to outperform traditional docu-ment language models on Q&A retrieval task.
Thebasic idea is to estimate the likelihood of translat-ing a document1 to a query by exploiting the depen-dencies that exists between query words and doc-ument words.
For example the document contain-ing the word Wheezing may well answer the ques-tion containing the term Asthma.
They learn thethese dependencies (encoded as translation proba-bilities) between words using parallel mono-lingualcorpora created from the Q&A pairs.
While useful,the effectiveness of these models is highly depen-dant on the availability of quality corpus (Lee et al1we will use (Q&A, document), (word, term) and (userquery, question) interchangeably1266Figure 1: Need for entity based expansions2008).
Also these models only capture shallow se-mantics between words via the co-occurrence statis-tics, while some of the more explicit relationshipsbetween words and entities is freely available exter-nally.
Being context agnostic (Zhou et al 2007) isanother very common criticism hailed on translationmodels as it results in noisy and generic translations.Example shown in Figure 1 captures these prob-lems.
Specifically, the word Blizzard canrefer to an American game development com-pany that develops World of Warcraft game orit could refer to a severe snowstorm.
Expand-ing query without taking the gaming context es-tablished by the word WOW (acronym for Worldof Warcraft) into account would lead to topicdrift.
Also it would be difficult to learn relation-ships between World of Warcraft BurningCrusade and Blizzard from the Q&A corpusalone due to the sparsity of co-occurance counts asthese can be expressed in several lexical forms, someof which are multi word phrases.In this paper we argue that solution to all theabove problems lies in a unified model in which en-tities are a primary citizen.
The guiding hypothesisbeing, an entity based representation provides a lessambiguous representation of the users question andprovides for a more semantically accurate expansionif the relationship between entities and words can beestimated more reliably.
Our main contributions are1.
We propose Entity based Translation LanguageModel (ETLM) for Q&A retrieval that accom-modates semantic information associated be-tween entities and words.
Being closely re-lated to the general source-channel framework(Berger and Lafferty, 1999), the model enjoysits benefits, while mitigating some of its short-comings.
Specifically it provides for contextaware expansions of the query by exploitingentity annotations on both, the document andthe query side.
Entity annotations also providea means to handle the ?many-to-one?
(Moore,2004) translation limitation in the IBM model,due to which each word in the target documentcan be generated by at most one word in thequestion2.
For the same reasons, it also al-leviates another related limitation by enablingtranslation between contiguous words acrossthe query and documents (Moore, 2004).2.
We learn relationships between entities andterms by proposing new ways of organiz-ing monolingual parallel corpus and simul-taneously leveraging external resources likeWikipedia from which one can derive these re-lationships reliably.
This helps alleviate thenoise problem associated with learning transla-tion models on Q&A archive described above.An important point to note is that, our tech-nique has merits independent to the choiceof the entity catalog.
In this work we use2entity mentions can be of more than unit word length1267D original Q&A collectionE set of all entities in catalogd(e) description of entity eC D annotated with e ?
Equser users questionq quser annotated with e ?
Et token spantq, td token span in q and dV word vocabularyTable 1: Notation.Wikipedia, as it is a popular choice due to itslarge and ever expanding coverage and its abil-ity to keep up with world events on a timelybasis.3.
We provide detailed evaluation of impact ofmodelling assumptions and model componentson retrieval performance on a large scale realdata from Yahoo Answers comprising ?5 mil-lion Q&A pairs.Rest of the paper is organized as follows: In thenext section, we define ETLM and outline its de-tails.
This is followed by Section 3 which givesthe details of entity annotators and its performance.Section 4 describes our experiments on the retrievalmethod used Q&A retrieval.
In Section 5 we com-pare and contrast related literature.
Finally, we con-clude in Section 6.2 Our ApproachProblem Definition: Let D = d1, d2, ..., dn denotethe Q&A collection.
Here di refers to the i-th Q&Adata consisting of a question qi and its answer ai.Given the user question quser, the task of Q&Aretrieval is to rank di according to score(quser, di).Figure 2 outlines the approach to compute this scorein the ETLM framework.Offline processing: Using the entity catalog E, welearn the entity annotation models EAoffline andEAonline for annotation of entities in the Q&A cor-pus and the query respectively.
Refer Section 3 fordetails.
For each di ?
D, we then annotate refer-ences to entities in Wikipedia using EAoffline re-sulting in annotated Q&A corpus C. We then com-pute relationships between entities and words usingC and E. These relationships are used to learn ourETLM model.Online processing: At runtime, annotate the userquery quser with entities using EAonlineto create anenriched question q.
Issue this query over the an-notated corpus C and rank the candidates as per theETLM model described below.2.1 ETLM ModelLet the annotated query q (and similarly annotatedQ&A pair d) be composed of sequence of tokenspans Tq (and Td).
Each token span tq (similarlytd) corresponds to sequence of contiguous words oc-curring in the running text.
These tq?s can corre-spond to entity mentions, phrases or words.
Let eqdenote the tokens spans that are annotated and neqthat are not (Tq = eq ?
neq).
For example, in thequery , What?
??
?neqis????neqa???
?neqQuadratic Formula?
??
?eq?,token span Quadratic Formula is linked to anentity corresponding to Quadratic Equation3, whileall other token spans are marked as neq .For the sake of simplicity, in this work we donot identify phrases i.e.
neq is always of unit wordlength4.
In the ETLM framework, the similarity be-tween a query q and a document dwithin a collectionC is given by the probabilityscore(q, d) ?
P (q|d) =?tq?qtq=eq?neqP (tq|d)P (tq|d) = (1?
?
)Pml(tq|d) + ?Pml(tq|C)Pml(tq|d) =?td?dT (tq|td)Pml(td|d) (1)Intuitively this indicates a generative process for cre-ating q from d. Ideally both q and d are ?only?
com-posed of e i.e.
?tq ?
q; tq ?
EU , where EU is theuniversal set of entities 5 (similarly for all td).
Thisis because when the document was created, each andevery td ?
d had a sense attached to it.
however inreality, for various reasons, set of target entities areclearly a subset of EU (for e.g.
E: set of all entities3http://en.wikipedia.org/wiki/Quadratic equation4its not a restriction as the model is valid for neq consistingof more than one word.5language for creating q and d1268Figure 2: ETLM Architecture (gray and brown arrows indicate offline and online processes respectivelyin the catalog) also not all of them may be recog-nized by the annotation system.T (tq|td) in Equation 1 denotes the probabilitythat a token span tq is the translation of token spantd.
This induces the desired query expansion effect.The key task is to estimate Pml(tq|C), T (tq|td) andPml(td|d); tq ?
eq ?
neq and td ?
ed ?
ned2.2 Estimating Model ParametersWe adopt 2 different approach for estimatingT (tq|td), leading to 2 different configurations ofETLM system .
As the name suggests, ETLMqa isestimated from Q&A data (C andD) while we lever-age the entity catalog (in our case it is Wikipedia) forETLMwiki.2.3 ETLMqa: Estimate from parallel corpusFollowing (Xue et al 2008) we pool the questionand answers from D to create a master parallel cor-pus P = (q1, a1), .., (qn, an) ?
(a1, q1), ., (an, qn).This is used for learning T (ne|ne?)6.
Similarly wecreate P ?
from C. We then derive 2 different paral-lel corpora from P and P ?
as followsPentity We remove all non linked tokens ne fromP ?
thereby reducing it to parallel corpus over e.This is used for learning T (e|e?)
i.e.
translationprobabilities between two entities e and e?
inE.Phybrid This is hybrid of Pentity and P where inone part of Q&A pair consists on only newhileother consists of only e. This is used for learn-ing T (ne|e) and T (e|ne).To handle entities e, we introduce special id?s in thene space.
Thus our universal token span set is given6subscript of q and d has been dropped as translation proba-bility learnt agnostic to it, due to pooling.by V ?
E. This is done so that T (tq|td) is learntfrom P , Pentity and Phybrid, w/o any modificationto the corresponding translation algorithm (Brownet al 1993).
Lets call this approach ETLMqa?
.We explored another intuitive approach ETLMqa,to learn T (e|e?
), T (e|ne), T (ne|e) and T (ne|ne?
)directly by using only P ?
as our parallel corpus.We do so by redistributing the probability massi.e.
when calculating T (e|e?
), we redistribute prob-ability mass spread over all the ne to e given byEquation 2 and 3.
Similar process is followed forT (e|ne), T (ne|e) and T (ne|ne?).S(e|e?)
=T (e|e?
)?t?V T (t|e?
)(2)T (e|e?)
=S(e|e?
)?t?E T (t|e?
)(3)Remaining model components are calculated usingEquation 4 and 5.
Here d refers to question part ofthe Q&A pair.Pml(tq|C) =tftq ,C + 1?t?
?C tft?,C + |C|(4)Pml(tq|d) =tftq ,d?t?
?d tft?,d(5)2.4 ETLMwiki: Estimating from WikipediaNumber of symmetric measures have been pro-posed (Medelyan et al 2009) to measure seman-tic relationships between entities and words usingWikipedia.
For our problem we need an asym-metric measure.
We use co-citation informationin Wikipedia to detect relatedness between enti-ties (T (e|e?))
and co-occurrence counts to estimate1269T (ne|ne?)
as follows: .T (e|e?)
=co(e, e?)?e??
co(e?
?, e?
)(6)T (ne|ne?)
=cf(ne, ne?)?ne??
cf(ne?
?, ne?
)(7)T (ne|e) =tfne,D(e) + 1|D(e)|+ |V |(8)T (e|ne) =tfne,D(e) + 1?e?
?E tfne,D(e?)
+ |E|(9)Here d(e) represents the page corresponding toentity e. D(e) represents concatenation of d(e)and all context of size 5 surrounding anchor text inWikipedia that link to e. cf(ne, ne?)
is the numbercontext windows of fixed size containing bothne and ne?
in Wikipedia.
In our case, we set thewindow size at 10 (because this size turned out to bereasonable in our pilot experiments).
tft,d(e) is thefrequency of t in d(e); co(e, e?)
indicates numberof entities in Wikipedia that have a hyperlink toboth e and e?.
As links from pages with a smallnumber of outgoing links are generally consideredto be more valuable than links from pages with ahigh outgoing link degree, we tried with weightedversion of 6 where the co-citations are weightedby the outdegree of Wikipedia page correspondingto entity s that link to e and e?.
Lets denote theweighted version by ETLMwiki and unweightedversion by ETLMwiki?
.
Pml(tq|C) and Pml(tq|d)are estimated as per Equation 4 and 5 respectively.2.5 Self translation probabilityTo make sure self translation probability is not un-derestimated i.e.
T (t|t) ?
T (t?|t) always holdstrue, we introduce new parameter ?
as T (t|t?)
=?
+ (1?
?
)T (t|t?
); ?
= 0 when t 6= t?
and ?
> 0.5otherwise.2.6 ETLMcombo: Combining ETLMqa andETLMwikiOften, combining language models yields better re-sults than any of the individual language modelsthemselves.
Linear interpolation is often the tech-nique of choice in language modelling for combin-ing models to exploit complementary features of thecomponent models.
It involves taking a weightedsum of the probabilities given by the component lan-guage models.
An advantage of the linear interpola-tion is that it is simple and fast to calculate.
If theinputs are probability estimates, also the output is aprobability estimate.
The mixture translation modelTcombo(e|e?)
over M component models is given byEquation 10.Tcombo(t|t?)
=M?j=1?jTj(t|t?
)t ?
E ?
V ;M?j=1?j = 1; ?j ?
0(10)One can immediately notice that Tcombo(t|t?)
hasone global weight for each of the M componentmodels which might not be ideal.
With access tolarge training data one could employ more power-ful context dependent interpolation techniques (Liuet al 2008).
In our case we have 2 componentsTqa and Twiki and four classes for each ; ?(e,e?)wiki7,?
(e,ne)wiki , ?(ne,ne?
)wiki and ?
(ne,e)wiki ), one corresponding toeach class of T (t|t?).
respectively.3 Entity AnnotationIn this section we describe our entity annotationsystem.
Recently there has been lot of work ad-dressing the problem of annotating text with linksto Wikipedia entities (Mihalcea and Csomai, 2007;Bunescu and Pasca, 2006; Milne and Witten, 2008;Kulkarni et al 2009; Ratinov et al 2011; Ferrag-ina and Scaiella, 2010).
We adopt a similar ap-proach, wherein we first find the best disambigua-tion (BESTDISAMBIGUATION) for a given mentionand then decide to prune it (PRUNE), via the dummymapping NA (similar to ?no assignment?
(Kulkarniet al 2009)).3.1 BESTDISAMBIGUATIONAs defined earlier, e ?
E represent an entity cor-responding to URN of a Wikipedia article.
LetEm = {em,1, em,2, ?
?
?
, em,|Em|} em,i ?
E repre-sent the set of possible disambiguations for a men-tion m (m is an index over all mentions in the cor-pus).
Given a mention m, task is to find best disam-biguation e from Wikipedia.
Without loss of gener-7?(e,e?
)qa = 1?
?(e,e?
)wiki1270ality, we consider em,?
?
Em as the correct answer.Let ?
(m, em,j) represent the mapping onto featuresbetween an entity mention m and the Wikipedia en-tity em,j and???
be the corresponding weight vectorand D(em,j) =???
?
(m, em,j) represent the disam-biguation score.
The task is to learn ???
such thatargmaxem,jD(em,j) gives the best disambiguation forthe mention m.We pose this as a ranking problem and solveit using max-margin technique (Joachims, 2002;Joachims, 2006) as followsminimize???
,???12???
?
???
+ C?
?i,jsubject to?m,?em,j ?
Em ?
em,?
:???
?
(m, em,?)
>???
?
(m, em,j) + ?i,j?i,?j : ?i,j ?
0(11)where?
?i,j is the total training error that upperbounds the number of pair preferences violations.This is controlled by adjusting the parameter C. Notethat Equation 11 means pairwise comparison be-tween the correct disambiguation em,?
and other dis-ambiguation candidates em,j such that j 6= indexcorresponding to *.3.2 PRUNEThe disambiguation phase produces one candidatedisambiguation per mention.
To discard any un-meaningful annotations a simple strategy similar toLOCAL (Kulkarni et al 2009) is followed where theD(em,?)
is compared against a predefined threshold?na, so that if D(em,?)
< ?na then that annotationfor menton m is discarded by linking m to NA.
Theparameter ?na allows the algorithm to back-off whenshort of evidence.3.3 FEATUREMAP ?
(m, em,j)Sense probability prior (SP): It represents the priorprobability that a mention name s points to a specificentity in Wikipedia.
For example, without any otherinformation, mention name ?tree?
will more likelyrefer to the entity woody plant8, rather than the less8en.wikipedia.org/wiki/Treepopular notion related to graphs 9.Entity Probability prior (EP): It captures the pop-ularity knowledge as a distribution of entities, i.e.,the EP (ei) should be larger than EP (ej) if ei ismore popular than ej .
This score is independent ofthe mention name.Context specific features: It captures the tex-tual similarity between weighted word vectors cor-responding to the context of the mention (windowaround the mention) and textual description associ-ated with the entity (Wikipedia page).Let EAonline and EAoffline represent config-urations for annotating user question and corpusrespectively.
For EAonline, user question repre-sents the document from which context specificfeatures are computed.
For EAoffline, questionand the answer(best) is concatenated to representsthe document.
Based on the ?one sense per dis-course?
assumption, one additional heuristic is usedin EAoffline where, for the same Q&A pair, if samename mention is repeated multiple times across thequestion and the answer then one with the maximumD(em,?)
> ?na is annotated for all instances.3.4 Annotation ExperimentsWe used 2010 version of Wikipedia as our knowl-edge base.
It contains more than 2.5 million en-tities.
Annotations were done by volunteers fluentin english.
Volunteers were told to be as exhaus-tive as possible and tag all possible name mentions,even if to mark them as ?NA?.
Inter-annotator agree-ment=92.1%; Kappa coefficient = 0.72.
As our cor-pus, we collected 8.3K manual annotations spanning1315 Q&A pairs.
2.8K of the annotations were as-signed to NA.
2.1K annotations (out of 8.3K) weremade in the question of which 551 were assigned toNA.
We use Precision, Recall and F1 score micro-averaged across documents as the evaluation mea-sures.
We do a linear scan of data to identify en-tity mentions by first tokenizing and then identify-ing token sequences that maximally match an en-tity ID in the entity name dictionary (constructedusing Wikipedia anchor text, redirect pages).
Fig-ure 3 outlines the performance of EAoffline andEAonline.
We measured EAoffline in 3 test dataconfigurations; (1) EAoffline: measured over entire9en.wikipedia.org/wiki/Tree (data structure)1271Figure 3: Precision v/s Recallannotation set; 2) EAoffline?
is measured only onannotations made in question.
this is done to com-pare it with EAonline; 3) EAoffline?
is similar to(2), only difference is that for (2) entire Q&A pairis the context, while here only question part is thecontext.
This is done to check if separate annotatorsare required for online and online phase.
As seenin Figure 3, this indeed is necessary as EAoffline?performs worse than EAonline.
Closer look at thefeature weights revealed that in EAoffline contextspecific features have much more weightage whencompared to its weight in EAonline, on the contraryEAonline weighs SP significantly higher.4 EvaluationWe now describe the empirical evaluation wherewe compare our techniques against the baselinetechniques.
We use several standard measures (R-Precision, MAP, MRR, Precision@k) in evaluation.We first describe the dataset used followed by de-scribing an exhaustive set of results across tech-niques and performance measures.4.1 DatasetWe crawled a dataset of ?5 million questions andanswers from Yahoo!
Answers spanning all the leaflevel categories.
Tokenization and stop word re-moval were the only preprocessing steps performed.We have used a stoplist10 having a vocabulary of 429common words to remove the stopwords.In our retrieval experiments we used 339 queries(average length 5.6 words).
We employed pool-ing technique used in the TREC conference series.10http://truereader.com/manuals/onix/stopwords1.htmlWe pooled the top 25 Q&A pairs from retrieval re-sults generated by varying the retrieval algorithmsand the search field.
Relevance judgments weremarked by human annotators without disclosing theidentity of method used for retrieval.
The annota-tors were asked to label candidate as ?relevant?
or?irrelevant?
based on semantic similarity with thequery.
Answer quality/correctness was not a crite-ria.
In case of disagreement between two volunteers,authors made the final judgment.
Inter-annotatoragreement was 87.9% and Kappa coefficient = 0.68.Over all we had collected more than 12K relevancejudgements corresponding to these queries, of which>2.3K were marked as relevant.4.2 BaselinesTo evaluate the effectiveness of our models wecompared them against the following baselinesTraditional models: VSM (Zobel and Moffat,2006) and OKAPI BM25 (Robertson et al 1996)(k1, b, and k3 are parameters that are set to 1.2, 0.75and?
respectively).Translation based language models: TLM (Jeonet al 2005), TransLM (using answers) (Xue et al2008) and CTM (Lee et al 2008).For our experiments we used a set of 50 queries toselect the model parameters.
Translation based lan-guage model use 2 parameters; smoothing parameter?
in the Language Model and ?
to control the self-translation impact in the TransLM.
Final values ofparameters used in our experiments were ?
= 0.2(Zhai and Lafferty, 2004) and ?
= 0.75 (Xue etal., 2008).
For CTM, we used tf-idf based weigh-ing scheme (Lee et al 2008) to remove words fromthe (Q?A) corpus P .
Word elimination threshold of20% was selected based on the above 50 queries.
Fi-nal values of ETLM parameters used in our experi-ments were ?
= 0.18 and ?
= 0.65.4.3 Result AnalysisTable 2 presents the performance of the various tech-niques.
Under each measure, we highlight the bestperforming technique.
Performance of all the trans-lation based models is better than VSM and OKAPIthereby confirming the importance of addressing thelexical gap.
Using high confidence annotations for1272MAP %chg MRR %chg R-Prec %chg Prec@5 %chg Prec@10 %chgVSM 0.221 0.421 0.21 0.202 0.15OKAPI 0.298 0.532 0.271 0.264 0.214TLM 0.337 0.583 0.318 0.297 0.239TransLM 0.352 0.612 0.347 0.332 0.261CTM 0.361 0.641 0.351 0.341 0.279ETLMqa 0.390?
8.03 0.699?
9.05 0.379?
7.98 0.367?
7.62 0.302?
8.24ETLMwiki 0.413?
14.40 0.719?
12.17 0.399?
13.68 0.391?
14.66 0.323?
15.77ETLMcombo 0.427?
18.28 0.726?
13.26 0.413?
17.66 0.407?
19.35 0.331?
18.64Table 2: Comparisons of retrieval models.
?
indicate a statistically significant improvement over the CTM using pairedt-test with p-value < 0.05.
%chg indicates change over CTM as it is the most competitive baselinequery expansion in ETLM, leads to an improvedperformance as compared to the all the baselinemethods that do not consider this signal.
This isvalidated by the fact that ETLMqa and ETLMwikican achieve statistically significant improvements interms of all the measures.
The reason for this im-provement is the context sensitive computation ofT (t|t?)
leading to reduced spurious expansions andimproved top expansions, this is made possible be-cause of entity disambiguation.
This computation inbaselines happens on word by word basis withoutexploiting contextual information.
ETLMqa per-forms worse than ETLMwiki.
On close inspec-tion of failure cases and translation probability ta-bles we found that T (e|e?)
for ETLMqa was muchworse than ETLMwiki.
This is because for gettinggood estimates of T (e|e?
), we need enough instanceswhere both e and e?
need to be correctly annotatedin the same Q&A pair.
Failure in this leads to sparsecounts thereby reducing the gap in T (e|e?)
scoresfor related and unrelated e. Figure 4 shows theimpact of choices made for learning the translationprobabilities T (t|t?).
We found that ETLMwiki per-forms slightly better than ETLMwiki?
, indicating theutility of weighted co-citation measure for comput-ing T (e|e?).
We believe that embedding other mea-sures that are better in capturing relationships fromWikipedia, should improve the performance.
Simi-larly ETLMqa also performs better than ETLMqa?
.This is because for creating Pentity all ne are re-moved.
This leads to count sparsity problem dis-Figure 4: Performance of ETLM configurationscussed above, but slightly worse.
Due to absenceof ne, in ETLMqa?
e?
in d are thought be beinggenerated ?only?
from e in q.
On the contrary inETLMqa, e?
had an option of mapping to ne in q.An interesting observation is that while the perfor-mances of different configurations vary, all of themperform better than CTM which is the best baseline.4.4 Impact of Annotations on retrievalSince entities are central in our model, impact ofentity annotation on quser is one of the most im-portant aspect to be studied.
Figure 5 shows theplot of retrieval measures obtained by varying ?nain EAonline.
CTM is shown by horizontal lines.
Asexplained in Section 3, value of ?na is inversely pro-portional to aggressiveness of annotation.
Whichimplies for high values, EAonline will annotate onlythose mentions in query that its highly confidentabout.
Beyond a value no annotations are made.1273Figure 5: Impact of query annotation on retrieval.
x-axisrepresents value of ?na used to control annotationThis is represented on the extreme right in Figure 5.One important observation is that, even with no an-notations made in query, performance of ETLMqaand ETLMwiki is competitive to CTM.
This is ev-idence for addressing the noise related issue forwhich CTM is designed.
For large range of values,all ETLM configurations are above CTM.
As we de-crease ?na performance increases smoothly and thenafter a certain point (?na = 5) is starts decreasing.4.5 ETLMcomboWe believe that T (t|t?)
learnt from one source wouldencode word association characteristics which mightnot be exactly the same across sources.ETLMcombotries to address this by combining the two models.Values for mixing parameters are : ?(e,e?
)wiki = 0.9511,?
(e,ne)wiki = 0.75, ?(ne,ne?
)wiki = 0.7 and ?
(ne,e)wiki = 0.75).The interpolation weights were obtained by optimiz-ing the retrieval performance by doing a using gridsearch over the parameter space.
Same 50 querieswere used for tuning.
As seen entity relationshipsobtained from Wikipedia are far superior to onefrom Q&A corpus.
As seen in Table 2 combiningthe two models improves the performance.5 Related WorkRecently Q&A retrieval has been garnering lot of at-tention.
Translation model (TLM) (Jeon et al 2005)has been extensively employed in question searchand has been shown to outperform the traditional IRmethods significantly (VSM, BM25, LM).
Existing11?(e,e?
)qa = (1?
0.95)work can be broadly grouped under the followingtopics:(a) Improved training of translation models by ex-ploiting answer content/inter-word co-occurrencerelations and restriction to reliable parallel cor-pora: Translation-based language model (TRLM)(Xue et al 2008) improved stability of TLM byproviding better probability estimates and also ex-ploited answers for question retrieval.
It further im-proved the retrieval results and obtained the state-of-the-art performance.
Another line of work on trans-lation models focused on providing suitable paralleldata to learn the translation probabilities.
Compacttranslation models (CTM) (Lee et al 2008) tried tofurther improve the translation probabilities basedon question-answer pairs by selecting the most im-portant terms to build compact translation models.We show that such special-purpose models to con-trol noisy translations may not be necessary becausemodels learnt using entity annotations are robust tonoise in Q&A data.Instead of using noisy Q&A data, new approach(Bernhard and Gurevych, 2009) to build parallel cor-pus from reliable sources has showed improvements.They proposed to use as a parallel training data com-prising of set the definitions and glosses providedfor the same term by different lexical semantic re-sources.
We move beyond terms and capture rela-tionships between entities and terms using the pagecontents and link structure in Wikipedia.Apart from translation models there are otherapproaches (Gao et al 2004) that try to extend theexisting language models for adhoc retrieval byincorporating term relationships or dependencies.Some expand queries using word relationshipsderived from co-occurrence thesaurus (Bai et al2005; Qiu and Frei, 1993), hand-crafted thesaurus(Liu et al 2004; Voorhees, 1994) and combinationof both (Cao et al 2005).
(b) Incorporation of query context information intranslation models using latent factor modelingand smoothing approaches: All these existingapproaches mentioned above are considered to becontext independent, in that they do not take intoaccount any contextual information in modelingword word relationships.
Topic signature model(Zhou et al 2007) exploited contextual information1274by decomposing a document into a set of weightedtopic signatures and use it for model smoothing.This model turns out to be inefficient when con-fronted with ambiguous words and phrases becauseit is unable to disambiguate the sense of topicsignatures.
Others (Liu and Croft, 2004) performsemantic smoothing by means of clustering.
Re-cently (Tu et al 2010; Cai et al 2011; Zhouet al 2011) showed improved performance byperforming sense based smoothing for documentretrieval, latent topic mining and phase basedretrieval respectively.
Contrary to these approacheswe used entity disambiguation to capture contextualinformation for improving Q&A retrieval.
(c) Complementary ideas for improving retrievalperformance that can be used alongside translationmodels: Other work on question retrieval includethe use of category information available (Cao etal., 2010), learning-to-rank techniques (Bian et al2008; Surdeanu et al 2008; Bunescu and Huang,2010), proposed a syntactic tree matching ((Wang etal., 2009) or question structure for important phrasematching (Duan et al 2008)).
These methods seemorthogonal to ours, in some cases complementaryand can be leveraged to get an even better perfor-manceThere also exists work where exploiting entitybased representation has been found helpful in in-formation retrieval (Singh et al 2009; Egozi et al2011; Meij et al 2008; Grootjen and van der Weide,2006).
In our work we use entity annotations inQ&A retrieval context.
There is also some work onusing Wikipedia in general web search (Xu et al2009).6 ConclusionIn this work we extend word based model to in-corporate semantic concepts for addressing the lex-ical gap issue in retrieval models for large onlineQ&A collections.
Compared to the existing trans-lation based model, our model is more robust andeffective in that it can perform context aware expan-sions.
We proposed ways to embed rich informationfreely available in Wikipedia into our models andcombine it one learnt from Q&A corpus.
Experi-ments performed on a large real Q&A data demon-strate that all configurations of ETLM significantlyoutperforms existing models for Q&A retrieval.AcknowledgmentsThanks to Srujana Merugu for helpful discussions.Thanks to the anonymous reviewers for helping usimprove the presentation.ReferencesLada A. Adamic, Jun Zhang, Eytan Bakshy, and Mark S.Ackerman.
2008.
Knowledge sharing and yahoo an-swers: everyone knows something.
In Proceedings ofthe 17th international conference on World Wide Web,WWW ?08, pages 665?674, New York, NY, USA.ACM.Jing Bai, Dawei Song, Peter Bruza, Jian-Yun Nie, andGuihong Cao.
2005.
Query expansion using termrelationships in language models for information re-trieval.
In Proceedings of the 14th ACM interna-tional conference on Information and knowledge man-agement, CIKM ?05, pages 688?695, New York, NY,USA.
ACM.Adam Berger and John Lafferty.
1999.
Informationretrieval as statistical translation.
In Proceedings ofthe 22nd annual international ACM SIGIR conferenceon Research and development in information retrieval,SIGIR ?99, pages 222?229, New York, NY, USA.ACM.Delphine Bernhard and Iryna Gurevych.
2009.
Combin-ing lexical semantic resources with question & answerarchives for translation-based answer finding.
In ACL-IJCNLP ?09: Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 2, pages 728?736, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Jiang Bian, Yandong Liu, Eugene Agichtein, andHongyuan Zha.
2008.
Finding the right facts in thecrowd: factoid question answering over social media.In WWW ?08: Proceeding of the 17th internationalconference on World Wide Web, pages 467?476, NewYork, NY, USA.
ACM.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19:263?311, June.Razvan Bunescu and Yunfeng Huang.
2010.
Learningthe relative usefulness of questions in community qa.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP1275?10, pages 97?107, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of EACL, volume 6, pages 9?16.Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.
2011.Learning the latent topics for question retrieval in com-munity qa.
In Proceedings of 5th International JointConference on Natural Language Processing, pages273?281, Chiang Mai, Thailand, November.
AsianFederation of Natural Language Processing.Guihong Cao, Jian-Yun Nie, and Jing Bai.
2005.
Inte-grating word relationships into language models.
InProceedings of the 28th annual international ACM SI-GIR conference on Research and development in in-formation retrieval, SIGIR ?05, pages 298?305, NewYork, NY, USA.
ACM.Xin Cao, Gao Cong, Bin Cui, and Christian S. Jensen.2010.
A generalized framework of exploring categoryinformation for question retrieval in community ques-tion answer archives.
In WWW ?10: Proceedings ofthe 19th international conference on World wide web,pages 201?210, New York, NY, USA.
ACM.Huizhong Duan, Yunbo Cao, Chin yew Lin, and YongYu.
2008.
Searching questions by identifying ques-tion topic and question focus.
In In Proceedings of46th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Tchnologies(ACL:HLT).Ofer Egozi, Shaul Markovitch, and Evgeniy Gabrilovich.2011.
Concept-based information retrieval using ex-plicit semantic analysis.
ACM Trans.
Inf.
Syst.,29(2):8:1?8:34, April.Paolo Ferragina and Ugo Scaiella.
2010.
Tagme: on-the-fly annotation of short text fragments (by wikipediaentities).
In Proceedings of the 19th ACM interna-tional conference on Information and knowledge man-agement, CIKM ?10, pages 1625?1628, New York,NY, USA.
ACM.Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-hong Cao.
2004.
Dependence language model forinformation retrieval.
In Proceedings of the 27thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?04, pages 170?177, New York, NY, USA.
ACM.F.
A. Grootjen and Th.
P. van der Weide.
2006.
Concep-tual query expansion.
Data Knowl.
Eng., 56(2):174?193, February.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowledgemanagement, CIKM ?05, pages 84?90, New York, NY,USA.
ACM.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, KDD ?02, pages 133?142, New York, NY, USA.
ACM.Thorsten Joachims.
2006.
Training linear svms in lin-ear time.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discovery anddata mining, KDD ?06, pages 217?226, New York,NY, USA.
ACM.S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proceedings ofthe 15th ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages457?466.
ACM.Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-Chang Rim.
2008.
Bridging lexical gaps betweenqueries and questions on large online q&#38;a collec-tions with compact translation models.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?08, pages 410?418,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Xiaoyong Liu and W. Bruce Croft.
2004.
Cluster-basedretrieval using language models.
In Proceedings ofthe 27th annual international ACM SIGIR conferenceon Research and development in information retrieval,SIGIR ?04, pages 186?193, New York, NY, USA.ACM.Shuang Liu, Fang Liu, Clement Yu, and Weiyi Meng.2004.
An effective approach to document retrievalvia utilizing wordnet and recognizing phrases.
In Pro-ceedings of the 27th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, SIGIR ?04, pages 266?272, New York,NY, USA.
ACM.Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.2008.
Context dependent language model adaptation.In INTERSPEECH 2008, 9th Annual Conference ofthe International Speech Communication Association,Brisbane, Australia, September 22-26, 2008, pages837?840.
ISCA.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning from wikipedia.Int.
J. Hum.-Comput.
Stud., 67:716?754, September.Edgar Meij, Dolf Trieschnigg, Maarten de Rijke, andWessel Kraaij.
2008.
Parsimonious concept model-ing.
In Proceedings of the 31st annual internationalACM SIGIR conference on Research and developmentin information retrieval, SIGIR ?08, pages 815?816,New York, NY, USA.
ACM.R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking1276documents to encyclopedic knowledge.
In CIKM, vol-ume 7, pages 233?242.D.
Milne and I.H.
Witten.
2008.
Learning to link withwikipedia.
In Proceeding of the 17th ACM conferenceon Information and knowledge management, pages509?518.
ACM.Robert C. Moore.
2004.
Improving ibm word-alignmentmodel 1.
In Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics, ACL?04, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yonggang Qiu and Hans-Peter Frei.
1993.
Conceptbased query expansion.
In Proceedings of the 16thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?93, pages 160?169, New York, NY, USA.
ACM.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Volume1, HLT ?11, pages 1375?1384, Stroudsburg, PA, USA.Association for Computational Linguistics.S.E.
Robertson, S. Walker, S. Jones, M.M.
Hancock-Beaulieu, and M. Gatford.
1996.
Okapi at trec-3.pages 109?126.Amit Singh and Karthik Visweswariah.
2011.
CQC:classifying questions in cqa websites.
In Proceedingsof the 20th ACM international conference on Informa-tion and knowledge management, CIKM ?11, pages2033?2036, New York, NY, USA.
ACM.Amit Singh, Sayali Kulkarni, Somnath Banerjee, GaneshRamakrishnan, and Soumen Chakrabarti.
2009.
Cu-rating and searching the annotated web.
In In SIGKDDConference, 2009.
ACM.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2008.
Learning to rank answers on largeonline qa collections.
In In Proceedings of the 46thAnnual Meeting for the Association for ComputationalLinguistics: Human Language Technologies (ACL-08:HLT, pages 719?727.Xinhui Tu, Tingting He, Long Chen, Jing Luo, andMaoyuan Zhang.
2010.
Wikipedia-based semanticsmoothing for the language modeling approach to in-formation retrieval.
In Proceedings of the 32nd Eu-ropean conference on Advances in Information Re-trieval, ECIR?2010, pages 370?381, Berlin, Heidel-berg.
Springer-Verlag.Ellen M. Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings of the 17thannual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?94, pages 61?69, New York, NY, USA.
Springer-Verlag New York, Inc.Kai Wang, Zhaoyan Ming, and Tat-Seng Chua.
2009.A syntactic tree matching approach to finding similarquestions in community-based qa services.
In Pro-ceedings of the 32nd international ACM SIGIR con-ference on Research and development in informationretrieval, SIGIR ?09, pages 187?194, New York, NY,USA.
ACM.Yang Xu, Gareth J.F.
Jones, and Bin Wang.
2009.Query dependent pseudo-relevance feedback based onwikipedia.
In Proceedings of the 32nd internationalACM SIGIR conference on Research and developmentin information retrieval, SIGIR ?09, pages 59?66, NewYork, NY, USA.
ACM.Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008.Retrieval models for question and answer archives.
InProceedings of the 31st annual international ACM SI-GIR conference on Research and development in in-formation retrieval, SIGIR ?08, pages 475?482, NewYork, NY, USA.
ACM.Chengxiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied to in-formation retrieval.
ACM Trans.
Inf.
Syst., 22(2):179?214, April.Xiaohua Zhou, Xiaohua Hu, and Xiaodan Zhang.
2007.Topic signature language models for ad hoc retrieval.IEEE Trans.
on Knowl.
and Data Eng., 19(9):1276?1287, September.Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.
2011.Phrase-based translation model for question retrievalin community question answer archives.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 653?662,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Justin Zobel and Alistair Moffat.
2006.
Inverted files fortext search engines.
ACM Comput.
Surv., 38, July.1277
