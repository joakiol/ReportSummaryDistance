Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 972?983, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA Systematic Comparison of Phrase Table Pruning TechniquesRichard Zens and Daisy Stanton and Peng XuGoogle Inc.{zens,daisy,xp}@google.comAbstractWhen trained on very large parallel corpora,the phrase table component of a machinetranslation system grows to consume vastcomputational resources.
In this paper, we in-troduce a novel pruning criterion that placesphrase table pruning on a sound theoreticalfoundation.
Systematic experiments on fourlanguage pairs under various data conditionsshow that our principled approach is superiorto existing ad hoc pruning methods.1 IntroductionOver the last years, statistical machine translationhas become the dominant approach to machinetranslation.
This is not only due to improved mod-eling, but also due to a significant increase in theavailability of monolingual and bilingual data.
Hereare just two examples of very large data resourcesthat are publicly available:?
The Google Web 1T 5-gram corpus availablefrom the Linguistic Data Consortium consist-ing of the 5-gram counts of about one trillionwords of web data.1?
The 109-French-English bilingual corpus withabout one billion tokens from the Workshop onStatistical Machine Translation (WMT).2These enormous data sets yield translation modelsthat are expensive to store and process.
Even with1LDC catalog No.
LDC2006T132http://www.statmt.org/wmt11/translation-task.htmlmodern computers, these large models lead to a longexperiment cycle that hinders progress.
The situa-tion is even more severe if computational resourcesare limited, for instance when translating on hand-held devices.
Then, reducing the model size is ofthe utmost importance.The most resource-intensive components of a sta-tistical machine translation system are the languagemodel and the phrase table.
Recently, compact rep-resentations of the language model have attractedthe attention of the research community, for instancein Talbot and Osborne (2007), Brants et al(2007),Pauls and Klein (2011) or Heafield (2011), to namea few.
In this paper, we address the other problemof any statistical machine translation system: largephrase tables.Johnson et al(2007) has shown that large por-tions of the phrase table can be removed without lossin translation quality.
This motivated us to performa systematic comparison of different pruning meth-ods.
However, we found that many existing methodsemploy ad-hoc heuristics without theoretical foun-dation.The pruning criterion introduced in this work isinspired by the very successful and still state-of-the-art language model pruning criterion based on en-tropy measures (Stolcke, 1998).
We motivate itsderivation by stating the desiderata for a good phrasetable pruning criterion:?
Soundness: The criterion should optimizesome well-understood information-theoreticmeasure of translation model quality.972?
Efficiency: Pruning should be fast, i. e., run lin-early in the size of the phrase table.?
Self-containedness: As a practical considera-tion, we want to prune phrases from an existingphrase table.
This means pruning should useonly information contained in the model itself.?
Good empirical behavior: We would like tobe able to prune large parts of the phrase tablewithout significant loss in translation quality.Analyzing existing pruning techniques based onthese objectives, we found that they are commonlydeficient in at least one of them.
We thus designeda novel pruning criterion that not only meets theseobjectives, it also performs very well in empiricalevaluations.The novel contributions of this paper are:1. a systematic description of existing phrase tablepruning methods.2.
a new, theoretically sound phrase table pruningcriterion.3.
an experimental comparison of several pruningmethods for several language pairs.2 Related WorkThe most basic pruning methods rely on probabil-ity and count cutoffs.
We will cover the techniquesthat are implemented in the Moses toolkit (Koehn etal., 2007) and the Pharaoh decoder (Koehn, 2004) inSection 3.
We are not aware of any work that ana-lyzes their efficacy in a systematic way.
It is thus notsurprising that some of them perform poorly, as ourexperimental results will show.The work of Johnson et al(2007) is promis-ing as it shows that large parts of the phrase ta-ble can be removed without affecting translationquality.
Their pruning criterion relies on statisti-cal significance tests.
However, it is unclear howthis significance-based pruning criterion is related totranslation model quality.
Furthermore, a compari-son to other methods is missing.
Here we close thisgap and perform a systematic comparison.
The sameidea of significance-based pruning was exploited in(Yang and Zheng, 2009; Tomeh et al 2009) for hi-erarchical statistical machine translation.A different approach to phrase table pruning wasundertaken by Eck et al(2007a; 2007b).
They relyon usage statistics from translating sample data, so itis not self-contained.
However, it could be combinedwith the methods proposed here.Another approach to phrase table pruning is trian-gulation (Chen et al 2008; Chen et al 2009).
Thisrequires additional bilingual corpora, namely fromthe source language as well as from the target lan-guage to a third bridge language.
In many situationsthis does not exist or would be costly to generate.Duan et al(2011), Sanchis-Trilles et al(2011)and Tomeh et al(2011) modify the phrase extrac-tion methods in order to reduce the phrase table size.The work in this paper is independent of the way thephrase extraction is done, so those approaches arecomplementary to our work.3 Pruning Using Simple StatisticsIn this section, we will review existing pruningmethods based on simple phrase table statistics.There are two common classes of these methods: ab-solute phrase table pruning and relative phrase tablepruning.3.1 Absolute pruningAbsolute pruning methods rely only on the statisticsof a single phrase pair (f?
, e?).
Hence, they are in-dependent of other phrases in the phrase table.
Asopposed to relative pruning methods (Section 3.2),they may prune all translations of a source phrase.Their application is easy and efficient.?
Count-based pruning.
This method prunesa phrase pair (f?
, e?)
if its observation countN(f?
, e?)
is below a threshold ?c:N(f?
, e?)
< ?c (1)?
Probability-based pruning.
This methodprunes a phrase pair (f?
, e?)
if its probability isbelow a threshold ?p:p(e?|f?)
< ?p (2)Here the probability p(e?|f?)
is estimated via rel-ative frequencies.9733.2 Relative pruningA potential problem with the absolute pruning meth-ods is that it can prune all occurrences of a sourcephrase f?
.3 Relative pruning methods avoid this byconsidering the full set of target phrases for a spe-cific source phrase f?
.?
Threshold pruning.
This method discardsthose phrases that are far worse than the besttarget phrase for a given source phrase f?
.
Givena pruning threshold ?t, a phrase pair (f?
, e?)
isdiscarded if:p(e?|f?)
< ?t ?maxe?{p(e?|f?)}(3)?
Histogram pruning.
An alternative to thresh-old pruning is histogram pruning.
For eachsource phrase f?
, this method preserves the Ktarget phrases with highest probability p(e?|f?
)or, equivalently, their count N(f?
, e?
).Note that, except for count-based pruning, none ofthe methods take the frequency of the source phraseinto account.
As we will confirm in the empiricalevaluation, this will likely cause drops in translationquality, since frequent source phrases are more use-ful than the infrequent ones.4 Significance PruningIn this section, we briefly review significance prun-ing following Johnson et al(2007).
The idea of sig-nificance pruning is to test whether a source phrasef?
and a target phrase e?
co-occur more frequently ina bilingual corpus than they should just by chance.Using some simple statistics derived from the bilin-gual corpus, namely?
N(f?)
the count of the source phrase f??
N(e?)
the count of the target phrase e??
N(f?
, e?)
the co-occurence count of the sourcephrase f?
and the target phrase e??
N the number of sentences in the bilingual cor-pus3Note that it has never been systematically investigatedwhether this is a real problem or just speculation.we can compute the two-by-two contingency tablein Table 1.Following Fisher?s exact test, we can calculate theprobability of the contingency table via the hyperge-ometric distribution:ph(N(f?
, e?))
=(N(f?)N(f?
,e?))?(N?N(f?)N(e?)?N(f?
,e?))(NN(e?))
(4)The p-value is then calculated as the sum of allprobabilities that are at least as extreme.
The lowerthe p-value, the less likely this phrase pair occurredwith the observed frequency by chance; we thusprune a phrase pair (f?
, e?)
if:????k=N(f?
,e?)ph(k)??
> ?F (5)for some pruning threshold ?F .
More details of thisapproach can be found in Johnson et al(2007).
Theidea of using Fisher?s exact test was first explored byMoore (2004) in the context of word alignment.5 Entropy-based PruningIn this section, we will derive a novel entropy-basedpruning criterion.5.1 Motivational ExampleIn general, pruning the phrase table can be consid-ered as selecting a subset of the original phrase table.When doing so, we would like to alter the originaltranslation model distribution as little as possible.This is a key difference to previous approaches: Ourgoal is to remove redundant phrases, whereas previ-ous approaches usually try to remove low-quality orunreliable phrases.
We believe this to be an advan-tage of our method as it is certainly easier to measurethe redundancy of phrases than it is to estimate theirquality.In Table 2, we show some example phrasesfrom the learned French-English WMT phrase table,along with their counts and probabilities.
For theFrench phrase le gouvernement franc?ais, we have,among others, two translations: the French govern-ment and the government of France.
If we haveto prune one of those translations, we can ask our-selves: how would the translation cost change if the974N(f?
, e?)
N(f?)?N(f?
, e?)
N(f?)N(e?)?N(f?
, e?)
N ?N(f?)?N(e?)
+N(f?
, e?)
N ?N(f?)N(e?)
N ?N(e?)
NTable 1: Two-by-two contingency table for a phrase pair (f?
, e?
).Source Phrase f?
Target Phrase e?
N(f?
, e?)
p(e?|f?
)le the 7.6 M 0.7189gouvernement government 245 K 0.4106franc?ais French 51 K 0.6440of France 695 0.0046le gouvernement franc?ais the French government 148 0.1686the government of France 11 0.0128Table 2: Example phrases from the French-English phrase table (K=thousands, M=millions).same translation were generated from the remain-ing, shorter, phrases?
Removing the phrase the gov-ernment of France would increase this cost dramat-ically.
Given the shorter phrases from the table, theprobability would be 0.7189 ?
0.4106 ?
0.0046 =0.0014?, which is about an order of a magnitudesmaller than the original probability of 0.0128.On the other hand, composing the phrase theFrench government out of shorter phrases has prob-ability 0.7189 ?
0.4106 ?
0.6440 = 0.1901, which isvery close to the original probability of 0.1686.
Thismeans it is safe to discard the phrase the French gov-ernment, since the translation cost remains essen-tially unchanged.
By contrast, discarding the phrasethe government of France does not have this effect:it leads to a large change in translation cost.Note that here the pruning criterion only considersredundancy of the phrases, not the quality.
Thus, weare not saying that the government of France is abetter translation than the French government, onlythat it is less redundant.
?We use the assumption that we can simply multiply theprobabilities of the shorter phrases.5.2 Entropy CriterionNow, we are going to formalize the notion of re-dundancy.
We would like the pruned model p?(e?|f?
)to be as similar as possible to the original modelp(e?|f?).
We use conditional Kullback-Leibler di-vergence, also called conditional relative entropy(Cover and Thomas, 2006), to measure the modelsimilarity:D(p(e?|f?)||p?(e?|f?))=?f?p(f?)?e?p(e?|f?)
log[p(e?|f?)p?(e?|f?)](6)=?f?
,e?p(e?, f?
)[log p(e?|f?)?
log p?(e?|f?
)](7)Computing the best pruned model of a given sizewould require optimizing over all subsets with thatsize.
Since that is computationally infeasible, we in-stead apply the equivalent approximation that Stol-cke (1998) uses for language modeling.
This as-sumes that phrase pairs affect the relative entropyroughly independently.We can then choose a pruning threshold ?E andprune those phrase pairs with a contribution to therelative entropy below that threshold.
Thus, we975prune a phrase pair (f?
, e?
), ifp(e?, f?
)[log p(e?|f?)?
log p?(e?|f?
)]< ?E (8)We now address how to assign the probabilityp?(e?|f?)
under the pruned model.
A phrase-basedsystem selects among different segmentations of thesource language sentence into phrases.
If a segmen-tation into longer phrases does not exist, the systemhas to compose a translation out of shorter phrases.Thus, if a phrase pair (f?
, e?)
is no longer available,the decoder has to use shorter phrases to producethe same translation.
We can therefore decomposethe pruned model score p?(e?|f?)
by summing over allsegmentations sK1 and all reorderings piK1 :p?(e?|f?)
=?sK1 ,piK1p(sK1 , piK1 |f?)
?
p(e?|sK1 , piK1 , f?)
(9)Here the segmentation sK1 divides both the sourceand target phrases into K sub-phrases:f?
= f?pi1 ...f?piK and e?
= e?1...e?K (10)The permutation piK1 describes the alignment ofthose sub-phrases, such that the sub-phrase e?k isaligned to f?pik .
Using the normal phrase translationmodel, we obtain:p?(e?|f?)
=?sK1 ,piK1p(sK1 , piK1 |f?
)K?k=1p(e?k|f?pik) (11)Virtually all phrase-based decoders use the so-called maximum-approximation, i. e. the sum is re-placed with the maximum.
As we would like thepruning criterion to be similar to the search criterionused during decoding, we do the same and obtain:p?(e?|f?)
?
maxsK1 ,piK1K?k=1p(e?k|f?pik) (12)Note that we also drop the segmentation probabil-ity, as this is not used at decoding time.
This leavesthe pruning criterion a function only of the modelp(e?|f?)
as stored in the phrase table.
There is no needfor a special development or adaptation set.
We candetermine the best segmentation using dynamic pro-gramming, similar to decoding with a phrase-basedmodel.
However, here the target side is constrainedto the given phrase e?.It can happen that a phrase is not compositional,i.
e., we cannot find a segmentation into shorterphrases.
In these cases, we assign a small, constantprobability:p?(e?|f?)
= pc (13)We found that the value pc = e?10 works well formany language pairs.5.3 ComputationIn our experiments, it was more efficient to vary thepruning threshold ?E without having to re-computethe entire phrase table.
Therefore, we computed theentropy criterion in Equation (8) once for the wholephrase table.
This introduces an approximation forthe pruned model score p?(e?|f?).
It might happenthat we prune short phrases that were used as partof the best segmentation of longer phrases.
As theseshorter phrases should not be available, the prunedmodel score might be inaccurate.
Although we be-lieve this effect is minor, we leave a detailed experi-mental analysis for future work.One way to avoid this approximation would beto perform entropy pruning with increasing phraselength.
Starting with one-word phrases, which aretrivially non-compositional, the entropy criterionwould be straightforward to compute.
Proceed-ing to two-word phrases, one would decompose thephrases into sub-phrases by looking up the proba-bilities of some of the unpruned one-word phrases.Once the set of unpruned two-word phrases was ob-tained, one would continue with three-word phrases,etc.6 Experimental Evaluation6.1 Data SetsIn this section, we describe the data sets used for theexperiments.
We perform experiments on the pub-licly available WMT shared translation task for thefollowing four language pairs:?
German-English?
Czech-English?
Spanish-English976Number of WordsLanguage Pair Foreign EnglishGerman - English 42 M 45 MCzech - English 56 M 65 MSpanish - English 232 M 210 MFrench - English 962 M 827 MTable 3: Training data statistics.
Number of words in thetraining data (M=millions).?
French-EnglishFor each pair, we train two separate system, one foreach direction.
Thus it can happen that a phrase ispruned for X-to-Y, but not for Y-to-X.These four language pairs represent a nice rangeof training corpora sizes, as shown in Table 3.6.2 Baseline SystemPruning experiments were performed on top of thefollowing baseline system.
We used a phrase-based statistical machine translation system similarto (Zens et al 2002; Koehn et al 2003; Och andNey, 2004; Zens and Ney, 2008).
We trained a 4-gram language model on the target side of the bilin-gual corpora and a second 4-gram language modelon the provided monolingual news data.
All lan-guage models used Kneser-Ney smoothing.The baseline system uses the common phrasetranslation models, such as p(e?|f?)
and p(f?
|e?
), lex-ical models, word and phrase penalty, distortionpenalty as well as a lexicalized reordering model(Zens and Ney, 2006).The word alignment was trained with six itera-tions of IBM model 1 (Brown et al 1993) and 6 it-erations of the HMM alignment model (Vogel et al1996) using a symmetric lexicon (Zens et al 2004).The feature weights were tuned on a developmentset by applying minimum error rate training (MERT)under the Bleu criterion (Och, 2003; Macherey et al2008).
We ran MERT once with the full phrase tableand then kept the feature weights fixed, i. e., we didnot rerun MERT after pruning to avoid adding un-necessary noise.
We extract phrases up to a lengthof six words.
The baseline system already includesphrase table pruning by removing singletons andkeeping up to 30 target language phrases per sourcephrase.
We found that this does not affect transla-8101214161820222426281  2  4  8BLEU[%]Number of Phrases [millions]ProbThresHistFigure 1: Comparison of probability-based pruningmethods for German-English.tion quality significantly4.
All pruning experimentsare done on top of this.6.3 ResultsIn this section, we present the experimental results.Translation results are reported on the WMT?07news commentary blind set.We will show translation quality measured withthe Bleu score (Papineni et al 2002) as a functionof the phrase table size (number of phrases).
Beingin the upper left corner of these figures is desirable.First, we show a comparison of severalprobability-based pruning methods in Figure 1.We compare?
Prob.
Absolute pruning based on Eq.
(2).?
Thres.
Threshold pruning based on Eq.
(3).?
Hist.
Histogram pruning as described in Sec-tion 3.2.5We observe that these three methods performequally well.
There is no difference between abso-lute and relative pruning methods, except that thetwo relative methods (Thres and Hist) are limited by4The Bleu score drops are as follows: English-French 0.3%,French-English 0.4%, Czech-English 0.3%, all other are lessthan 0.1%.5Instead of using p(e?|f?)
one could use the weighted modelscore including p(f?
|e?
), lexical weightings etc.
; however, wefound that this does not give significantly different results; butit does introduce a undesirable dependance between featureweights and phrase table pruning.977the number of source phrases.
Thus, they reach apoint where they cannot prune the phrase table anyfurther.
The results shown are for German-English;the results for the other languages are very similar.The results that follow use only the absolute prun-ing method as a representative for probability-basedpruning.In Figures 2 through 5, we show the transla-tion quality as a function of the phrase table size.We vary the pruning thresholds to obtain differentphrase table sizes.
We compare four pruning meth-ods:?
Count.
Pruning based on the frequency of aphrase pair, c.f.
Equation (1).?
Prob.
Pruning based on the absolute probabil-ity of a phrase pair, c.f.
Equation (2).?
Fisher.
Pruning using significance tests, c.f.Equation (5).?
Entropy.
Pruning using the novel entropy cri-terion, c.f.
Equation (8).Note that the x-axis of these figures is on a logarith-mic scale, so the differences between the methodscan be quite dramatic.
For instance, entropy pruningrequires less than a quarter of the number of phrasesneeded by count- or significance-based pruning toachieve a Spanish-English Bleu score of 34 (0.4 mil-lion phrases compared to 1.7 million phrases).These results clearly show how the pruning meth-ods compare:1.
Probability-based pruning performs poorly.
Itshould be used only to prune small fractions ofthe phrase table.2.
Count-based pruning and significance-basedpruning perform equally well.
They are muchbetter than probability-based pruning.3.
Entropy pruning consistently outperforms theother methods across translation directions andlanguage pairs.Figures 6 and 7 show compositionality statisticsfor the pruned Spanish-English phrase table (we ob-served similar results for the other language pairs).Total number of phrases 4 137 MCompositional 3 970 MNon-compositional 167 Mof those: one-word phrases 85 Mno segmentation 82 MTable 4: Statistics of phrase compositionality(M=millions).Each figure shows the composition of the phrase ta-ble for a type of pruning for different phrase tablessizes.
Along the x-axis, we plotted the phrase ta-ble size.
These are the same phrase tables used toobtain the Bleu scores in Figure 2 (left).
The dif-ferent shades of grey correspond to different phraselengths.
For instance, in case of the smallest phrasetable for count-based pruning, the 1-word phrasesaccount for about 30% of all phrases, the 2-wordphrases account for about 35% of all phrases, etc.With the exception of the probability-based prun-ing, the plots look comparable.
The more aggres-sive the pruning, the larger the percentage of shortphrases.
We observe that entropy-based pruning re-moves many more long phrases than any of the othermethods.
The plot for probability-based pruning isdifferent in that the percentage of long phrases ac-tually increases with more aggressive pruning (i. e.smaller phrase tables).
A possible explanation isthat probability-based pruning does not take the fre-quency of the source phrase into account.
Thisdifference might explain the poor performance ofprobability-based pruning.To analyze how many phrases are compositional,we collect statistics during the computation of theentropy criterion.
These are shown in Table 4, ac-cumulated across all language pairs and all phrases,i.
e., including singleton phrases.
We see that 96%of all phrases are compositional (3 970 million outof 4 137 million phrases).
Furthermore, out ofthe 167 million non-compositional phrases, morethan half (85 million phrases), are trivially non-compositional: they consist only of a single sourceor target language word.
The number of non-trivialnon-compositional phrases is, with 82 million or 2%of the total number of phrases, very small.In Figure 8, we show the effect of the constant97818202224262830323436380.01  0.1  1  10  100BLEU[%]Number of Phrases [M]ProbCountFisherEntropy101520253035400.01  0.1  1  10  100BLEU[%]Number of Phrases [M]ProbCountFisherEntropyFigure 2: Translation quality as a function of the phrase table size for Spanish-English (left) and English-Spanish(right).101520253035400.1  1  10  100  1000BLEU[%]Number of Phrases [M]ProbCountFisherEntropy5101520253035400.1  1  10  100  1000BLEU[%]Number of Phrases [M]ProbCountFisherEntropyFigure 3: Translation quality as a function of the phrase table size for French-English (left) and English-French (right).681012141618202224260.001  0.01  0.1  1  10  100BLEU[%]Number of Phrases [M]ProbCountFisherEntropy468101214160.001  0.01  0.1  1  10  100BLEU[%]Number of Phrases [M]ProbCountFisherEntropyFigure 4: Translation quality as a function of the phrase table size for Czech-English (left) and English-Czech (right).8101214161820222426280.001  0.01  0.1  1  10BLEU[%]Number of Phrases [M]ProbCountFisherEntropy24681012141618200.001  0.01  0.1  1  10BLEU[%]Number of Phrases [M]ProbCountFisherEntropyFigure 5: Translation quality as a function of the phrase table size for German-English (left) and English-German(right).97902040608010010  100Percentage of Phrases[%]Number of Phrases [millions]Prob6-word5-word4-word3-word2-word1-word0204060801000.01  0.1  1  10  100Percentage of Phrases[%]Number of Phrases [millions]Count6-word5-word4-word3-word2-word1-wordFigure 6: Phrase length statistics for Spanish-English for probability-based (left) and count-based pruning (right).0204060801000.01  0.1  1  10  100Percentage of Phrases[%]Number of Phrases [millions]Fisher6-word5-word4-word3-word2-word1-word0204060801000.01  0.1  1  10  100Percentage of Phrases[%]Number of Phrases [millions]Entropy6-word5-word4-word3-word2-word1-wordFigure 7: Phrase length statistics for Spanish-English for significance-based (left) and entropy-based pruning (right).pc for non-compositional phrases.6 The resultsshown are for Spanish-English; additional experi-ments for the other languages and translation direc-tions showed very similar results.
Overall, there isno big difference between the values.
Hence, wechose a value of 10 for all experiments.The results in Figure 2 to Figure 5 show thatentropy-based pruning clearly outperforms the al-ternative pruning methods.
However, it is a bithard to see from the graphs exactly how much ad-ditional savings it offers over other methods.
In Ta-ble 5, we show how much of the phrase table wehave to retain under various pruning criteria with-out losing more than one Bleu point in translationquality.
We see that probability-based pruning al-lows only for marginal savings.
Count-based andsignificance-based pruning results in larger savingsbetween 70% and 90%, albeit with fairly high vari-6The values are in neg-log-space, i. e., a value of 10 corre-sponds to pc = e?10.ability.
Entropy-based pruning achieves consistentlyhigh savings between 85% and 95% of the phrase ta-ble.
It always outperforms the other pruning meth-ods and yields significant savings on top of count-based or significance-based pruning methods.
Of-ten, we can cut the required phrase table size in halfcompared to count or significance based pruning.As a last experiment, we want to confirm thatphrase-table pruning methods are actually betterthan simply reducing the maximum phrase length.In Figure 9, we show a comparison of differentpruning methods and a length-based approach forSpanish-English.
For the ?Length?
curve, we firstdrop all 6-word phrases, then all 5-word phrases, etc.until we are left with only single-word phrases; thephrase length is measured as the number of sourcelanguage words.
We observe that entropy-based,count-based and significance-based pruning indeedoutperform the length-based approach.
We obtainedsimilar results for the other languages.980Method ES-EN EN-ES DE-EN EN-DE FR-EN EN-FR CS-EN EN-CSProb 77.3 % 82.7 % 61.2 % 67.3 % 84.8 % 94.1 % 85.6 % 86.3 %Count 24.9 % 11.9 % 19.9 % 14.3 % 11.4 % 9.0 % 20.2 % 10.4 %Fisher 23.5 % 12.6 % 21.7 % 14.0 % 14.5 % 13.6 % 31.9 % 9.9 %Entropy 7.2 % 6.0 % 10.2 % 11.1 % 7.1 % 8.1 % 14.8 % 6.4 %Table 5: To what degree can we prune the phrase table without losing more than 1 Bleu point?
The table showspercentage of phrases that we have to retain.
ES=Spanish, EN=English, FR=French, CS=Czech, DE=German.24262830323436380.01  0.1  1  10  100BLEU[%]Number of Phrases [M]5101520253050Figure 8: Translation quality (Bleu) as a function of thephrase table size for Spanish-English for entropy pruningwith different constants pc.7 ConclusionsPhrase table pruning is often addressed in an ad-hocway using the heuristics described in Section 3.
Wehave shown that some of those do not work well.Choosing the wrong technique can result in sig-nificant drops in translation quality without savingmuch in terms of phrase table size.
We introduceda novel entropy-based criterion and put phrase ta-ble pruning on a sound theoretical foundation.
Fur-thermore, we performed a systematic experimentalcomparison of existing methods and the new entropycriterion.
The experiments were carried out for fourlanguage pairs under small, medium and large dataconditions.
We can summarize our conclusions asfollows:?
Probability-based pruning performs poorlywhen pruning large parts of the phrase table.This might be because it does not take the fre-quency of the source phrase into account.?
Count-based pruning performs as well as18202224262830323436380.01  0.1  1  10  100BLEU[%]Number of Phrases [M]LengthProbCountFisherEntropyFigure 9: Translation quality (Bleu) as a function of thephrase table size for Spanish-English.significance-based pruning.?
Entropy-based pruning gives significantlylarger savings in phrase table size than anyother pruning method.?
Compared to previous work, the novel entropy-based pruning often achieves the same Bleuscore with only half the number of phrases.8 Future WorkCurrently, we take only the model p(e?|f?)
into ac-count when looking for the best segmentation.
Wemight obtain a better estimate by also consider-ing the distortion costs, which penalize reordering.We could also include other phrase models such asp(f?
|e?)
and the language model.The entropy pruning criterion could be appliedto hierarchical machine translation systems (Chiang,2007).
Here, we might observe even larger reduc-tions in phrase table size as there are many more en-tries.981ReferencesThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 858?867, Prague, Czech Republic, June.
Association forComputational Linguistics.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Yu Chen, Andreas Eisele, and Martin Kay.
2008.Improving statistical machine translation efficiencyby triangulation.
In Proceedings of the Sixth In-ternational Conference on Language Resources andEvaluation (LREC?08), Marrakech, Morocco, May.European Language Resources Association (ELRA).http://www.lrec-conf.org/proceedings/lrec2008/.Yu Chen, Martin Kay, and Andreas Eisele.
2009.
In-tersecting multilingual data for faster and better sta-tistical translations.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 128?136, Boulder,Colorado, June.
Association for Computational Lin-guistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228,June.Thomas M. Cover and Joy A. Thomas.
2006.
Elementsof information theory.
Wiley-Interscience, New York,NY, USA.Nan Duan, Mu Li, and Ming Zhou.
2011.
Improvingphrase extraction via MBR phrase scoring and prun-ing.
In Proceedings of MT Summit XIII, pages 189?197, Xiamen, China, September.Matthias Eck, Stephan Vogel, and Alex Waibel.
2007a.Estimating phrase pair relevance for machine transla-tion pruning.
In Proceedings of MT Summit XI, pages159?165, Copenhagen, Denmark, September.Matthias Eck, Stephan Vogel, and Alex Waibel.
2007b.Translation model pruning via usage statistics for sta-tistical machine translation.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Companion Volume, Short Papers,pages 21?24, Rochester, New York, April.
Associationfor Computational Linguistics.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197, Edinburgh, Scotland, July.
Association forComputational Linguistics.Howard Johnson, Joel Martin, George Foster, and RolandKuhn.
2007.
Improving translation quality by dis-carding most of the phrasetable.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics An-nual Meeting (HLT-NAACL), pages 127?133, Edmon-ton, Canada, May/June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Constan-tine, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In 45th An-nual Meeting of the Assoc.
for Computational Linguis-tics (ACL): Poster Session, pages 177?180, Prague,Czech Republic, June.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In 6th Conf.
of the Assoc.
for Machine Translationin the Americas (AMTA), pages 115?124, WashingtonDC, September/October.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum errorrate training for statistical machine translation.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 725?734,Honolulu, HI, October.
Association for ComputationalLinguistics.Robert C. Moore.
2004.
On log-likelihood-ratios andthe significance of rare events.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 333?340.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449, De-cember.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In 41st Annual Meet-ing of the Assoc.
for Computational Linguistics (ACL),pages 160?167, Sapporo, Japan, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalua-tion of machine translation.
In 40th Annual Meeting of982the Assoc.
for Computational Linguistics (ACL), pages311?318, Philadelphia, PA, July.Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages258?267, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.German Sanchis-Trilles, Daniel Ortiz-Martinez, Je-sus Gonzalez-Rubio, Jorge Gonzalez, and FranciscoCasacuberta.
2011.
Bilingual segmentation forphrasetable pruning in statistical machine translation.In Proceedings of the 15th Conference of the EuropeanAssociation for Machine Translation, pages 257?264,Leuven, Belgium, May.Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In Proc.
DARPA Broadcast NewsTranscription and Understanding Workshop, pages270?274.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale LMs on thecheap.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 468?476, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.2009.
Complexity-based phrase-table filtering for sta-tistical machine translation.
In Proceedings of MTSummit XII, Ottawa, Ontario, Canada, August.Nadi Tomeh, Marco Turchi, Guillaume Wisniewski,Alexandre Allauzen, and Franc?ois Yvon.
2011.
Howgood are your phrases?
Assessing phrase quality withsingle class classification.
In Proceedings of the Inter-national Workshop on Spoken Language Translation,pages 261?268, San Francisco, California, December.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In 16th Int.
Conf.
on Computational Linguistics(COLING), pages 836?841, Copenhagen, Denmark,August.Mei Yang and Jing Zheng.
2009.
Toward smaller, faster,and better hierarchical phrase-based SMT.
In Pro-ceedings of the ACL-IJCNLP 2009 Conference ShortPapers, pages 237?240, Suntec, Singapore, August.Association for Computational Linguistics.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Human Language Technology Conf.
/ North Ameri-can Chapter of the Assoc.
for Computational Linguis-tics Annual Meeting (HLT-NAACL): Workshop on Sta-tistical Machine Translation, pages 55?63, New YorkCity, NY, June.Richard Zens and Hermann Ney.
2008.
Improvements indynamic programming beam search for phrase-basedstatistical machine translation.
In Proceedings of theInternational Workshop on Spoken Language Transla-tion, pages 195?205, Honolulu, Hawaii, October.Richard Zens, Franz Josef Och, and Hermann Ney.2002.
Phrase-based statistical machine translation.
InM.
Jarke, J. Koehler, and G. Lakemeyer, editors, 25thGerman Conf.
on Artificial Intelligence (KI2002), vol-ume 2479 of Lecture Notes in Artificial Intelligence(LNAI), pages 18?32, Aachen, Germany, September.Springer Verlag.Richard Zens, Evgeny Matusov, and Hermann Ney.2004.
Improved word alignment using a symmetriclexicon model.
In 20th Int.
Conf.
on ComputationalLinguistics (COLING), pages 36?42, Geneva, Switzer-land, August.983
