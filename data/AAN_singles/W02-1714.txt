XiSTS ?
XML in Speech Technology SystemsMichael Walsh Stephen Wilson Julie Carson-BerndsenDepartment of Computer ScienceUniversity College DublinIreland{michael.j.walsh, stephen.m.wilson, julie.berndsen}@ucd.ieAbstract: This paper describes the use of XML in three generic interacting speech technologysystems.
The first, a phonological syllable recognition system, generates feature-based finite-stateautomaton representations of phonotactic constraints in XML.
It employs axioms of event logic tointerpret multilinear representations of speech utterances and outputs candidate syllables to the secondsystem, an XML syllable lexicon.
This system enables users to generate their own lexicons and its defaultlexicon is used to accept or reject the candidate syllables output by the speech recognition system.Furthermore its XML representation facilitates its use by the third system which generates additionallexicons, based on different feature sets, by means of a transduction process.
The applicability of thesealternative feature sets in the generation of synthetic speech can then be tested using these new lexicons.1.
IntroductionThe flexibility and portability provided byXML, and its related technologies, result inthem being well suited to the developmentof robust, generic, Natural LanguageProcessing applications.
In this paper wedescribe the use of XML within the contextof speech technology software, with aparticular focus on speech recognition.
Wepresent a framework, based on the model ofTime Map Phonology (Carson-Berndsen,1998), for the development  and testing ofphonological well- formedness constraintsfor generic speech technology applications.Furthermore, we illustrate how the use of asyllable lexicon, specified in terms ofphonological features, and marked-up inXML, contributes to both speech recognitionand synthesis.
In the following sectionsthree inter-connected systems are discussed.The first, the Language IndependentPhonotactic System, LIPS, a syllablerecognition application based on Time MapPhonology and a significant departure fromcurrent ASR technology, is described.
Thesecond system, Realising Enforced Feature-based Lexical Entries in XML, REFLEX, isoutlined and finally, the third system,Transducing Recognised Entities via XML,T-REX, is discussed.
All three systems buildon earlier work on generic speech tools(Carson-Berndsen, 1999; Carson-Berndsen& Walsh, 2000a).2.
The Time Map ModelThis paper focuses on representing speechutterances in terms of non-segmentalphonology, such as autosegmentalphonology (Goldsmith, 1990), whereutterances are represented in terms of tiersof autonomous features (autosegments)which can spread across a number ofsounds.
The advantage of this approach isthat coarticulation can be modelled byallowing features to overlap.
The Time Mapmodel (Carson-Berndsen, 1998, 2000)builds on this autosegmental approach byallowing multilinear representations ofautonomous features to be interpreted by anevent-based computational linguistic modelof phonology.
The Time Map modelemploys a phonotactic automaton (finite-state representation of the permissiblecombinations of sounds in a language), andaxioms of event logic, to interpretmultilinear feature representations.
Indeed,much recent research (e.g.
Ali et al, 1999;Chang, Greenberg & Wester, 2001) hasfocused on extracting  similar features tothose used in our model.
Figure 1 below,illustrates a mulitlinear feature-basedrepresentation of the syllable [So:n] 1.Figure 1.
Multilinear representation of [So:n]Two temporal domains are distinguished bythe Time Map model.
The first, absolute(signal) time, considers features as eventswith temporal endpoints.
The second,relative time, considers only the temporalrelations of overlap and precedence assalient.
Input to the model is in absolutetime.
Parsing, however, is performed in therelative time domain using only the overlapand precedence relations, and is guided bythe phonotactic automaton which imposestop-down constraints on the relations thatcan occur in a particular language.
Theconstruction of the phonotactic automatonand the actual parsing process is carried outby LIPS.3.
LIPSLIPS is the generic framework for the TimeMap model.
It incorporates finite-state1All phonemes are specified in  SAMPA notation.methodology which enables users toconstruct their own phonotactic automata forany language by means of a graphical userinterface.
Furthermore, LIPS employs anevent logic, enabling it to map from absolutetime to relative time, and in a novelapproach to ASR, carry out parsing on thephonological feature level.
The system iscomprised of two principal components, thenetwork generator and the parser, outlined inthe following subsections.3.1.
The Network GeneratorThe network generator interface allowsusers to build their own phonotacticautomata.
Users input node values and selectfrom a list of feature overlap relations thosethat a given arc is to represent.
Theserelations can be selected from a default listof IPA-like features or the user can specifytheir own set.
In this way LIPS is feature-setindependent.
The network generatorconstructs feature-based networks andparsing takes place at the feature level.
Oncethe user has completed the networkspecification, the system generates an XMLrepresentation of the phonotactic automaton.An automaton representing a smallsubsection of the phonotactics of English isillustrated in Figure 2.
It is clear from thisautomaton that English permits an [S]followed by a [r] in syllable-initial position,but not the other way around.Figure 2.
Phonotactic automatonFigure 3.
XML representation of subsection of phonotactic automaton for English.Figure 3 illustrates a subsection of the XMLrepresentation of the English phonotacticsoutput by the network generator.
A singlearc with a single phoneme, [S], and itsoverlap constraints, is shown.The motivation for generating an XMLrepresentation for our phonotactic  automatais that XML enables us to specify a well-defined, easy to interpret, portable template,without compromising the generic nature ofthe network generator.
That is to say theuser can still specify a phonotacticautomaton independent of any language orfeature-set.
The generated phonotacticautomaton is then used to guide the secondprincipal component of the system, theparser.3.2 The ParserLIPS employs a top-down and breadth-firstparsing strategy and is best explainedthrough exemplification.Purely for the purposes of describing howthe parsing procedure takes place, we returnto the phonotactic automaton of Figure 2,which of course represents only a very smallsubsection of English.
This automaton willrecognise such syllables as shum, shim,shem, shown, shrun, shran etc., some beingactual lexicalised syllables of English andothers being phonotactically well- formed,potential, syllables of English.
For ourexample we take the multilinearrepresentation of the utterance [So:n] asdepicted in Figure 4 as our input to theparser.Figure 4.
Interaction between the input and theautomaton.At the beginning of the parsing process thephonotactic automaton is anticipating a [S]sound, that is it requires three temporaloverlap constraints to be satisfied, thefeature voiceless must overlap the featurefricative,  the feature palato  must overlapthe feature voiceless, and the featurefricative must overlap the feature palato.
Avariable window is applied over the inpututterance and the features within the windoware examined to see if they satisfy theoverlap constraints.
As can be seen fromFigure 4 the three features are indeedpresent and all overlap in time.
Thus the [S]is recognised and the two arcs bearing the[S] symbol are traversed and the windowmoves on.
At this point then the automatonis anticipating either an [r] or a vowel sound.In a similar fashion the contents of the newwindow are examined and in the case of ourexample the vowel [o:] is recognised (the [r]is rejected).
The vowel transition istraversed, the window moves on, and theautomaton is expecting an [n] or an [m].
Forfull details of the parsing process seeCarson-Berndsen & Walsh (2000b).
Outputfrom LIPS is then fed through the REFLEXsystem to determine if actual or potentialsyllables have been found.4.
REFLEXREFLEX is a generic, language independentapplication, which allows for the rapiddesign and construction of syllable lexicons,for any language.
One of the main focusesof other research working on broadening thescope of the lexicon across languages, hasbeen in the development of multilinguallexicons.
One such project, PolyLex (Cahill& Gazdar, 1999), captures commonalitiesacross related languages using hierarchicalinheritance mechanisms.
One of the mainconcerns of the work presented herehowever, is to provide generic, reusable,tools which facilitate the development andtesting of phonological systems, rather thanthe creation of such multilingual lexicons.Work on phonological features and lexicaldescription has either been within thismultilingual context (Tiberius & Evans,2000) or has concentrated on using afeature-based lexicon for comparison withfeatures extracted from a sound signal(Reetz, 2000).
By removing reference tospecific languages and concentrating onproviding mechanisms for lexicalgeneration, REFLEX can generate a syllablelexicon for any language that can beadequately represented in a phoneticnotation.Furthermore, the decision to use XML torepresent the output data means that it isreadily available for use and manipulationby other outside systems with minimaleffort.
All background processing iscompletely hidden; one deals only with themarked-up output, from which idiosyncraticuser-required structures can be rapidlygenerated.The REFLEX system outputs a feature-based syllable lexicon.
This lexicon is avalid XML document, meaning that itconforms to the given REFLEX DocumentType Definition (DTD).
The DTD stipulatesthe structure, order and number of XMLelement tags and attributes, modelling allpotential syllable structures (e.g.
V, CV,CVC etc).An example of a typical lexical entry, in thiscase corresponding to the multilinearrepresentation specified in Figure 5, [So:n]is given below.Figure 5.
Typical lexical entry in XMLThe syllable element shown has fourchildren, described as follows:1) A text child, in this case So:n, theSAMPA representation of the entiresyllable.
2) An <onset> element whoseattribute list denotes its position within thesyllable, i.e.<onset type=?first?>, <onsettype=?second?> etc.
3) Nucleus and 4)coda elements are similarly defined.Each of the syllable?s elements, <onset>,<nucleus> and <coda>, may have only onechild element, <segment>, which tags thegiven phoneme.
Its attribute list describesthe phonemes specification in terms ofphonological features.
It also has a durationattribute, which is derived from corpusanalysis.<segment phonation=?voiced?manner=?nasal?
place=?apical?duration=?null?>n</segment>REFLEX provides two methods by whichsyllables can be added to the lexicon.
Thefirst, requires users to specify an input file ofmonosyllables represented in a phoneticnotation, in this case SAMPA.
The second,enables the user to specify syllables, interms of phonemes, position, and if desired,a typical duration, by means of a GUIillustrated below in Figure 6.Figure 6.
REFLEX lexicographer interfaceRegardless of the input option chosen, newentries are added to the lexicon via abackground process.
REFLEX makes use ofDATR, a non-monotonic inheritance basedlexical representation language (Evans &Gazdar, 1996) to carry out this process.DATR is used to quickly andcomprehensively define the phonologicalfeature descriptions for a given language.For a greater understanding of how this canbe achieved see Cahill, Carson-Berndsen &Gazdar (2000).
Using DATR?s inferencemechanisms, REFLEX manipulates theoutput into a valid XML document, creatinga sophisticated phonological feature-basedlexicon, shown in Figure 5.All syllable elements are enclosed within theroot <lexicon> tag, whose sole attributespecifies the lexicon?s language.<lexicon language=?English?><syllable>?</syllable>:<syllable>?</syllable></lexicon>The REFLEX lexicon is a versatile tool thathas a number of potential applicationswithin the domain of speech technologysystems.
The following sub-sectionsillustrate how this syllable lexicon, by virtueof its being marked up in XML, cancontribute to both speech recognition andsynthesis.4.1 LIPS and REFLEXBy allowing feature overlap constraints to berelaxed in the case of underspecified input,LIPS can produce a number of candidatesyllables.
In Figure 4 above, at the finaltransition, the automaton is expecting eitheran [m] or an [n].
The input, however, isunderspecified, no feature distinguishingbetween [m] or [n], or indeed any voicednasal, is present.
By allowing the overlapconstraints for the [m]  and the [n] to berelaxed, LIPS can consider both [So:n] and[So:m] to be candidate syllables for theutterance.
Both candidate syllables are well-formed, adhering to the phonotactics ofEnglish, however only one, [So:n], is anactual syllable of English.
Thus at this pointa lexicon providing good coverage of thelanguage should reject [So:m] and accept[So:n].
In order to achieve this, REFLEXmakes use of the XPath specification (ameans for locating nodes in an XMLdocument) and formulates a query beforeapplying it to the syllable lexicon.
2 In the2 The full W3C XPath specification can be found athttp://www.w3c.org/TR/xpathexample given, REFLEX searches thedocument, checking the value of the textchild of each syllable element, against eachcandidate syllable output by LIPS.
Anysuccessful matches returned are thereforenot only well- formed, but are deemed to beactual syllables.
Thus at this point, thelexicon is searched and the syllable [So:n] isrecognised.
The granularity of the REFLEXsearch capability is such, that it can beextended to the feature level.
Users cansearch the lexicon for syllables that containa number of specific features in certainpositions, e.g.
search for syllables thatcontain a voiced, labial, plosive in the firstonset.
Again, REFLEX forms an XPathexpression and queries the lexicon, returningall matches.
REFLEX also functions as aknowledge source for the T-REX system.This system is responsible for mappingoutput from the lexicon into syllablerepresentations using different feature sets,e.g.
features from other phonologies, and isdiscussed below in the context of speechsynthesis.5.
T-REXThe role of this module is to enablelexicographers and speech scientists etc.
togenerate, via a transduction process,syllable lexicons based on differentphonological feature sets.
The defaultfeature set employed by REFLEX is basedon IPA-like features.
However, T-REXprovides a GUI that permits lexicographersto define phoneme to feature attributemappings.
Given this functionality T-REXoperates as a testbed for investigating themerits of different feature sets in the contextof speech synthesis.
Different lexicons aregenerated by associating new feature setswith the same phonetic alphabet (SAMPA)via a GUI.
The new lexicon is thentransduced by T-REX which maps allsyllable entries from the default lexicon(with IPA-like features) to the new lexicon,applying the features input by the user, totheir associated phonemes.
In order toexemplify this we return to our samplesyllable, [So:n].
Figure 2 above shows thelexical representation, using IPA-likefeatures, for [So:n].
Figure 7 below showsnew features being associated with thephoneme [S].Figure 7.
GUI for T-REXSimilarly, new features are associated withthe remaining phonemes, [o:]  and [n], andindeed the rest of the SAMPA alphabet.
Oncompletion the user initiates the transductionprocess and a new lexicon is produced.
TheXML representation of the phoneme [S], inthe new lexicon, is depicted in Figure 8.Note how the feature attributes differ fromthose in the default lexicon.Figure 8.
Phoneme with transduced featuresThe advantages of this transductioncapability are that numerous lexicons can berapidly developed and used to investigatethe appropriateness of specific formalmodels of phonological representation forthe purposes of speech synthesis.Furthermore, the same computationalphonological model, i.e.
the Time Mapmodel, can be employed.
Bohan et al(2001)describe how the phonotactic automaton isused to generate a multilinear eventrepresentation of overlap and precedenceconstraints for an utterance, which is thenmapped to control parameters of the HLsyn(Sensimetrics Corporation) synthesis engine.Different feature sets can be evaluated byassessing how they influence the variouscontrol parameters of the HLsyn engine andthe quality of the synthesised speech.6.
ConclusionThis paper has described how the use ofXML together with a computationalphonological model can contributesignificantly to the tasks of speechrecognition, speech synthesis and lexicondevelopment.
Phonotactic automata andmultilinear representations were introducedand the interpretation of theserepresentations was discussed.
Three robust,well-defined systems, LIPS, REFLEX, andT-REX, were outlined.
These systems offergeneric structures coupled with theportability of XML.
In doing so, they enableusers to recognise speech, synthesise speech,and develop lexicons for different languagesusing different feature sets whilemaintaining a common interface.
Thegeneric and portable nature of these systemsmeans that languages with significantlydifferent phonologies are supported.
Inaddition, languages which, to date, havereceived little attention with respect tospeech technology are equally provided for.Ongoing projects include work on Irish,which has a notably different phonologyfrom English and on developing phonotacticautomata and phonological lexicons forother languages.
Furthermore, the modelsare being extended to include phoneme-grapheme mappings based on the contextsdefined by the phonotactic automata.7.
BibliographyAli, A.M..A.; J.
Van der Spiegel; P. Mueller; G.Haentjaens & J. Berman (1999): AnAcoustic-Phonetic Feature-Based System forAutomatic Phoneme Recognition inContinuous Speech.
In: IEEE InternationalSymposium on Circuits and Systems (ISCAS-99), III-118-III-121, 1999.Bohan, A.; E. Creedon, , J. Carson-Berndsen &F. Cummins (2001): Application of aComputational Model of Phonology toSpeech Synthesis.
In: Proceedings ofAICS2001, Maynooth, September 2001.Cahill, L. & G. Gazdar (1999).
The PolyLexarchitecture: multilingual lexicons forrelated languages.
Traitement Automatiquedes Langues, 40(2), 5-23.Cahill, L.; J. Carson-Berndsen & G. Gazdar(2000), Phonology-based LexicalKnowledge Representation.
In: F. van Eynde& D. Gibbon (eds.)
Lexicon Developmentfor Speech and Language Processing,Kluwer Academic Publishers, Dordrecht.Carson-Berndsen, J.
(1998): Time MapPhonology: Finite State Models and EventLogics in Speech Recognition.
KluwerAcademic Publishers, Dordrecht.Carson-Berndsen, J.
(1999): A Generic LexiconTool for Word Model Definition inMultimodal Applications.
Proceedings ofEUROSPEECH 99, 6th EuropeanConference on Speech Communication andTechnology, Budapest, September 1999.Carson-Berndsen, J.
(2000): Finite State Models,Event Logics and Statistics in SpeechRecognition, In: Gazdar, G.; K. SparckJones & R. Needham (eds.
): Computers,Language and Speech: Integrating formaltheories and statistical data.
PhilosophicalTransactions of the Royal Society, Series A,358(1770), 1255-1266.Carson-Berndsen, J.
& M. Walsh (2000a):Generic techniques for multilingual speechtechnology applications, Proceedings of the7th Conference on Automatic NaturalLanguage Processing, Lausanne,Switzerland, 61-70.Carson-Berndsen, J.
& M. Walsh (2000b):Interpreting Multilinear Representations inSpeech.
In: Proceedings of the EightInternational Conference on Speech Scienceand Technology, Canberra, December 2000.Chang, S.; S. Greenberg & M. Wester (2001):An Elitist Approach to Articulatory-Acoustic Feature Classification.
In:Proceedings of Eurospeech 2001, Aalborg.Evans, R & G. Gazdar (1996), DATR: Alanguage for lexical knowledgerepresentation.
In: ComputationalLinguistics 22, 2, pp.
167-216.Goldsmith, J.
(1990): Autosegmental andMetrical Phonology.
Basil Blackwell,Cambridge, MA.Reetz, H. (2000) UnderspecifiedPhonological Features for LexicalAccess.
In: PHONUS 5, pp.
161-173.Saarbr?cken: Institute of Phonetics,University of the Saarland.Tiberius, C. & R. Evans, 2000"Phonological feature based MultilingualLexical Description," Proceedings ofTALN 2000, Geneva, Switzerland.
