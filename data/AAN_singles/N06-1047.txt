Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 367?374,New York, June 2006. c?2006 Association for Computational LinguisticsIncorporating Speaker and Discourse Features into SpeechSummarizationGabriel Murray, Steve Renals,Jean Carletta, Johanna MooreUniversity of Edinburgh, School of InformaticsEdinburgh EH8 9LW, Scotlandgabriel.murray@ed.ac.uk, s.renals@ed.ac.uk,jeanc@inf.ed.ac.uk, j.moore@ed.ac.ukAbstractWe have explored the usefulness of incorporat-ing speech and discourse features in an automaticspeech summarization system applied to meetingrecordings from the ICSI Meetings corpus.
By an-alyzing speaker activity, turn-taking and discoursecues, we hypothesize that such a system can out-perform solely text-based methods inherited fromthe field of text summarization.
The summariza-tion methods are described, two evaluation meth-ods are applied and compared, and the resultsclearly show that utilizing such features is advanta-geous and efficient.
Even simple methods relyingon discourse cues and speaker activity can outper-form text summarization approaches.1.
IntroductionThe task of summarizing spontaneous spoken di-alogue from meetings presents many challenges:information is sparse; speech is disfluent and frag-mented; automatic speech recognition is imper-fect.
However, there are numerous speech-specificcharacteristics to be explored and taken advantageof.
Previous research on summarizing speech hasconcentrated on utilizing prosodic features [1, 2].We have examined the usefulness of additionalspeech-specific characteristics such as discoursecues, speaker activity, and listener feedback.
Thisspeech features approach is contrasted with a sec-ond summarization approach using only textualfeatures?a centroid method [3] using a latent se-mantic representation of utterances.
These indi-vidual approaches are compared to a combined ap-proach as well as random baseline summaries.This paper also introduces a new evalua-tion scheme for automatic summaries of meetingrecordings, using a weighted precision score basedon multiple human annotations of each meetingtranscript.
This evaluation scheme is describedin detail below and is motivated by previous find-ings [4] suggesting that n-gram based metrics likeROUGE [5] do not correlate well in this domain.2.
Previous WorkIn the field of speech summarization in general, re-search investigating speech-specific characteristicshas focused largely on prosodic features such as F0mean and standard deviation, pause information,syllable duration and energy.
Koumpis and Re-nals [1] investigated prosodic features for summa-rizing voicemail messages in order to send voice-mail summaries to mobile devices.
Hori et al [6]have developed an integrated speech summariza-tion approach, based on finite state transducers, inwhich the recognition and summarization compo-nents are composed into a single finite state trans-ducer, reporting results on a lecture summariza-tion task.
In the Broadcast News domain, Maskeyand Hirschberg [7] found that the best summariza-tion results utilized prosodic, lexical, and structuralfeatures, while Ohtake et al [8] explored usingonly prosodic features for summarization.
Maskeyand Hirschberg similarly found that prosodic fea-tures alone resulted in good quality summaries of367Broadcast News.In the meetings domain (using the ICSI cor-pus), Murray et al [2] compared text summariza-tion approaches with feature-based approaches us-ing prosodic features, with human judges favoringthe feature-based approaches.
Zechner [9] inves-tigated summarizing several genres of speech, in-cluding spontaneous meeting speech.
Though rel-evance detection in his work relied largely on tf.idfscores, Zechner also explored cross-speaker infor-mation linking and question/answer detection, sothat utterances could be extracted not only accord-ing to high tf.idf scores, but also if they were linkedto other informative utterances.Similarly, this work aims to detect importantutterances that may not be detectable accordingto lexical features or prosodic prominence, butare nonetheless linked to high speaker activity,decision-making, or meeting structure.3.
Summarization ApproachesThe following subsections give detailed descrip-tions of our two summarization systems, one ofwhich focuses on speech and discourse featureswhile the other utilizes text summarization tech-niques and latent semantic analysis.3.1.
Speech and Discourse FeaturesIn previous summarization work on the ICSI cor-pus [2, 4], Murray et al explored multiple waysof applying latent semantic analysis (LSA) to aterm/document matrix of weighted term frequen-cies from a given meeting, a development of themethod in [10].
A central insight to the presentwork is that additional features beyond simple termfrequencies can be included in the matrix beforesingular value decomposition (SVD) is carried out.We can use SVD to project this matrix of featuresto a lower dimensionality space, subsequently ap-plying the same methods as used in [2] for extract-ing sentences.The features used in these experiments in-cluded features of speaker activity, discourse cues,listener feedback, simple keyword spotting, meet-ing location and dialogue act length (in words).For each dialogue act, there are features indi-cating which speaker spoke the dialogue act andwhether the same speaker spoke the preceding andsucceeding dialogue acts.
Another set of featuresindicates how many speakers are active on eitherside of a given dialogue act: specifically, howmany speakers were active in the preceding andsucceeding five dialogue acts.
To further gaugespeaker activity, we located areas of high speakerinteraction and indicated whether or not a givendialogue act immediately preceded this region ofactivity, with the motivation being that informa-tive utterances are often provocative in eliciting re-sponses and interaction.
Additionally, we includeda feature indicating which speakers most often ut-tered dialogue acts that preceded high levels ofspeaker interaction, as one way of gauging speakerstatus in the meeting.
Another feature relating tospeaker activity gives each dialogue act a score ac-cording to how active the speaker is in the meetingas a whole, based on the intuition that the most ac-tive speakers will tend to utter the most importantdialogue acts.The features for discourse cues, listener feed-back, and keyword spotting were deliberately su-perficial, all based simply on detecting informativewords.
The feature for discourse cues indicates thepresence or absence of words such as decide, dis-cuss, conclude, agree, and fragments such as weshould indicating a planned course of action.
Lis-tener feedback was based on the presence or ab-sence of positive feedback cues following a givendialogue act; these include responses such as right,exactly and yeah.
Keyword spotting was basedon frequent words minus stopwords, indicating thepresence or absence of any of the top twenty non-stopword frequent words.
The discourse cues ofinterest were derived from a manual corpus analy-sis rather than being automatically detected.A structural feature scored dialogue acts ac-cording to their position in the meeting, with di-alogue acts from the middle to later portion of themeeting scoring higher and dialogue acts at the be-ginning and very end scoring lower.
This is a fea-ture that is well-matched to the relatively unstruc-tured ICSI meetings, as many meetings would beexpected to have informative proposals and agen-das at the beginning and perhaps summary state-ments and conclusions at the end.Finally, we include a dialogue act length fea-ture motivated by the fact that informative utter-ances will tend to be longer than others.The extraction method follows [11] by rank-ing sentences using an LSA sentence score.
The368matrix of features is decomposed as follows:A = USV Twhere U is an m?n matrix of left-singular vectors,S is an n ?
n diagonal matrix of singular values,and V is the n?n matrix of right-singular vectors.Using sub-matrices S and V T , the LSA sentencescores are obtained using:ScLSAi =???
?n?k=1v(i, k)2 ?
?
(k)2 ,where v(i, k) is the kth element of the ith sen-tence vector and ?
(k) is the corresponding singularvalue.Experiments on a development set of 55 ICSImeetings showed that reduction to between 5?15dimension was optimal.
These development ex-periments also showed that weighting some fea-tures slightly higher than others resulted in muchimproved results; specifically, the discourse cuesand listener feedback cues were weighted slightlyhigher.3.2.
LSA CentroidThe second summarization method is a textual ap-proach incorporating LSA into a centroid-basedsystem [3].
The centroid is a pseudo-documentrepresenting the important aspects of the docu-ment as a whole; in the work of [3], this pseudo-document consists of keywords and their modi-fied tf.idf scores.
In the present research, we takea different approach to constructing the centroidand to representing sentences in the document.First, tf.idf scores are calculated for all words inthe meeting.
Using these scores, we find the toptwenty keywords and choose these as the basis forour centroid.
We then perform LSA on a very largecorpus of Broadcast News and ICSI data, using theInfomap tool1.
Infomap provides a query languagewith which we can retrieve word vectors for ourtwenty keywords, and the centroid is thus repre-sented as the average of its constituent keywordvectors [12] [13].Dialogue acts from the meetings are repre-sented in much the same fashion.
For each dia-logue act, the vectors of its constituent words are1http://infomap.stanford.eduretrieved, and the dialogue act as a whole is the av-erage of its word vectors.
Extraction then proceedsby finding the dialogue act with the highest cosinesimilarity with the centroid, adding this to the sum-mary, then continuing until the desired summarylength is reached.3.3.
CombinedThe third summarization method is simply a com-bination of the first two.
Each system produces aranking and a master ranking is derived from thesetwo rankings.
The hypothesis is that the strengthof one system will differ from the other and thatthe two will complement each other and producea good overall ranking.
The first system would beexpected to locate areas of high activity, decision-making, and planning, while the second would lo-cate information-rich utterances.
This exempli-fies one of the challenges of summarizing meetingrecordings: namely, that utterances can be impor-tant in much different ways.
A comprehensive sys-tem that relies on more than one idea of importanceis ideal.4.
Experimental SetupAll summaries were 350 words in length, muchshorter than the compression rate used in [2] (10%of dialogue acts).
The ICSI meetings themselvesaverage around 10,000 words in length.
The rea-sons for choosing a shorter length for summariesare that shorter summaries are more likely to beuseful to a user wanting to quickly overview andbrowse a meeting, they present a greater summa-rization challenge in that the summarizer must bemore exact in pinpointing the important aspects ofthe meeting, and shorter summaries make it morefeasible to enlist human evaluators to judge the nu-merous summaries on various criteria in the future.Summaries were created on both manual tran-scripts and speech recognizer output.
The unit ofextraction for these summaries was the dialogueact, and these experiments used human segmentedand labeled dialogue acts rather than try to detectthem automatically.
In future work, we intend toincorporate dialogue act detection and labeling aspart of one complete automatic summarization sys-tem.3694.1.
Corpus DescriptionThe ICSI Meetings corpus consists of 75 meetings,lasting approximately one hour each.
Our test setconsists of six meetings, each with multiple hu-man annotations.
Annotators were given accessto a graphical user interface (GUI) for browsingan individual meeting that included earlier humanannotations: an orthographic transcription time-synchronized with the audio, and a topic segmen-tation based on a shallow hierarchical decompo-sition with keyword-based text labels describingeach topic segment.
The annotators were told toconstruct a textual summary of the meeting aimedat someone who is interested in the research beingcarried out, such as a researcher who does similarwork elsewhere, using four headings:?
general abstract: ?why are they meeting andwhat do they talk about??;?
decisions made by the group;?
progress and achievements;?
problems describedThe annotators were given a 200 word limit foreach heading, and told that there must be text forthe general abstract, but that the other headingsmay have null annotations for some meetings.
An-notators who were new to the data were encour-aged to listen to a meeting straight through beforebeginning to author the summary.Immediately after authoring a textual sum-mary, annotators were asked to create an extractivesummary, using a different GUI.
This GUI showedboth their textual summary and the orthographictranscription, without topic segmentation but withone line per dialogue act based on the pre-existingMRDA coding [14].
Annotators were told to ex-tract dialogue acts that together would convey theinformation in the textual summary, and could beused to support the correctness of that summary.They were given no specific instructions about thenumber or percentage of acts to extract or aboutredundant dialogue acts.
For each dialogue act ex-tracted, they were then required in a second passto choose the sentences from the textual summarysupported by the dialogue act, creating a many-to-many mapping between the recording and thetextual summary.
Although the expectation wasthat each extracted dialogue act and each summarysentence would be linked to something in the op-posing resource, we told the annotators that undersome circumstances dialogue acts and summarysentences could stand alone.We created summaries using both manual tran-scripts as well as automatic speech recognition(ASR) output.
The AMI-ASR system [15] is de-scribed in more detail in [4] and the average worderror rate (WER) for the corpus is 29.5%.4.2.
Evaluation FrameworksThe many-to-many mapping of dialogue acts tosummary sentences described in the previous sec-tion allows us to evaluate our extractive summariesaccording to how often each annotator linked agiven extracted dialogue act to a summary sen-tence.
This is somewhat analogous to Pyramidweighting [16], but with dialogue acts as the SCUs.In fact, we can calculate weighted precision, recalland f-score using these annotations, but becausethe summaries created are so short, we focus onweighted precision as our central metric.
For eachdialogue act that the summarizer extracts, we countthe number of times that each annotator links thatdialogue act to a summary sentence.
For a givendialogue act, it may be that one annotator links it0 times, one annotator links it 1 time, and the thirdannotator links it two times, resulting in an aver-age score of 1 for that dialogue act.
The scores forall of the summary dialogue acts can be calculatedand averaged to create an overall summary score.ROUGE scores, based on n-gram overlap be-tween human abstracts and automatic extracts,were also calculated for comparison [5].
ROUGE-2, based on bigram overlap, is considered the moststable as far as correlating with human judgments,and this was therefore our ROUGE metric of inter-est.
ROUGE-SU4, which evaluates bigrams withintervening material between the two elements ofthe bigram, has recently been shown in the con-text of the Document Understanding Conference(DUC)2 to bring no significant additional informa-tion as compared with ROUGE-2.
Results from[4] and from DUC 2005 also show that ROUGEdoes not always correlate well with human judg-ments.
It is therefore included in this research inthe hope of further determining how reliable the2http://duc.nist.gov370ROUGE metric is for our domain of meeting sum-marization.5.
ResultsThe experimental results are shown in figure 1(weighted precision) and figure 2 (ROUGE-2) andare discussed below.5.1.
Weighted Precision ResultsFor weighted precision, the speech features ap-proach was easily the best and scored significantlybetter than the centroid and random approaches(ANOVA,p<0.05), attaining an averaged weightedprecision of 0.52.
The combined approach didnot improve upon the speech features approachbut was not significantly worse either.
The ran-domly created summaries scored much lower thanall three systems.The superior performance of the speech fea-tures approach compared to the LSA centroidmethod closely mirrors results on the ICSI devel-opment set, where the centroid method scored 0.23and the speech features approach scored 0.42.
Forthe speech features approach on the test set, thebest feature by far was dialogue act length.
Re-moving this feature resulted in the precision scorebeing nearly halved.
This mirrors results fromMaskey and Hirschberg [7], who found that thelength of a sentence in seconds and its length inwords were the two best features for predictingsummary sentences.
Both the simple keywordspotting and the discourse cue detection featurescaused a lesser decline in precision when removed,while other features of speaker activity had a neg-ligible impact on the test results.Interestingly, the weighted precision scores onASR were not significantly worse for any of thesummarization approaches.
In fact, the centroidapproach scored very slightly higher on ASR out-put than on manual transcripts.
In [17] and [2] itwas similarly found that summarizing with ASRoutput did not cause great deterioration in the qual-ity of the summaries.
It is not especially surpris-ing that the speech features approach performedsimilarly on both manual and ASR transcripts, asmany of its features based on speaker exchangesand speaker activity would be unaffected by ASRerrors.
The speech features approach is still signif-icantly better than the random and centroid sum-00.10.20.30.40.50.6CombinedSpeechFeatsCentroidRandomSummarization ApproachesPRECISION-MANPRECISION-ASRFigure 1: Weighted Precision Results on Test Setmaries, and is not significantly better than the com-bined approach on ASR.5.2.
ROUGE ResultsThe ROUGE results greatly differed from theweighted precision results in several ways.
First,the centroid method was considered to be the best,with a ROUGE-2 score of 0.047 compared with0.041 for the speech features approach.
Second,there were not as great of differences between thefour systems according to ROUGE as there wereaccording to weighted precision.
In fact, the ran-dom summaries of manual transcripts are not sig-nificantly worse than the other approaches, accord-ing to ROUGE-2.
Neither the combined approachnor the speech features approach is significantlyworse than the centroid system, with the combinedapproach generally scoring on par with the cen-troid scores.The third difference relates to summarizationon ASR output.
ROUGE-2 has the random systemand the combined system showing sharp declineswhen applied to ASR transcripts.
The speech fea-tures and centroid approaches do not show de-clines.
Random summaries are significantly worsethan both the centroid summaries (p<0.1) andspeech features summaries (p<0.05).
Though thecombined approach declines on ASR output, it isnot significantly worse than the other systems.To get an idea of a ROUGE-2 upper bound, foreach meeting in the test set we left one human ab-stract out and compared it with the remaining ab-stracts.
The result was an average ROUGE-2 scoreof .086.3710.020.040.060.080.1CombinedSpeechFeatsCentroidRandomSummarization ApproachesROUGE2-MANROUGE2-ASRUPPER BOUNDFigure 2: ROUGE-2 Results on Test SetROUGE-1 and ROUGE-SU4 show no signif-icant differences between the centroid and speechfeatures approaches.5.3.
CorrelationsThere is no significant correlation betweenmacroaveraged ROUGE and weighted precisionscores across the meeting set, on both ASR andmanual transcripts.
The Pearson correlation is0.562 with a significance of p < 0.147.
The Spear-man correlation is 0.282 with a significance of p <0.498.
The correlation of scores across each testmeeting is worse yet, with a Pearson correlationof 0.185 (p<0.208) and a Spearman correlation of0.181 (p<0.271).5.4.
Sample SummaryThe following is the text of a summary of meetingBed004 using the speech features approach:-so its possible that we could do something like a summarynode of some sort that-and then the question would be if if those are the things that youcare about uh can you make a relatively compact way of getting fromthe various inputs to the things you care about-this is sort of th the second version and i i i look at this maybe justas a you know a a whatever uml diagram or you know as just a uhscreen shot not really as a bayes net as john johno said-and um this is about as much as we can do if we dont w if we wantto avoid uh uh a huge combinatorial explosion where we specify ok ifits this and this but that is not the case and so forth it just gets reallyreally messy-also it strikes me that we we m may want to approach the pointwhere we can sort of try to find a uh a specification for some interfacehere that um takes the normal m three l looks at it-so what youre trying to get out of this deep co cognitive linguistics isthe fact that w if you know about source source paths and goals andnnn all this sort of stuff that a lot of this is the same for different tasks-what youd really like of course is the same thing youd always likewhich is that you have um a kind of intermediate representationwhich looks the same o over a bunch of inputs and a bunch of outputs-and pushing it one step further when you get to constructiongrammar and stuff what youd like to be able to do is say you havethis parser which is much fancier than the parser that comes with uhsmartkom-in independent of whether it about what is this or where is it orsomething that you could tell from the construction you could pullout deep semantic information which youre gonna use in a generalway6.
DiscussionThough the speech features approach was consid-ered the best system, it is unclear why the com-bined approach did not yield improvement.
Onepossibility relates to the extreme brevity of thesummaries: because the summaries are only 350words in length, it is possible to have two sum-maries of the same meeting which are equallygood but completely non-overlapping in content.In other words, they both extract informative dia-logue acts, but not the same ones.
Combining therankings of two such systems might create a thirdsystem which is comparable but not any better thaneither of the first two systems alone.
However, itis still possible that the combined system will bebetter in terms of balancing the two types of im-portance discussed above: utterances that contain alot of informative content and keywords and utter-ances that relate to decision-making and meetingstructure.ROUGE did not correlate well with theweighted precision scores, a result that adds to theprevious evidence that this metric may not be reli-able in the domain of meeting summarization.It is very encouraging that the summarizationapproaches in general seem immune to the WERof the ASR output.
This confirms previous find-ings such as [17] and [2], and the speech andstructural features used herein are particularly un-affected by a moderately high WER.
The reasonfor the random summarizaton system not suffering372a sharp decline when applied to ASR may be dueto the fact that its scores were already so low thatit couldn?t deteriorate any further.7.
Future WorkThe above results show that even a relatively smallset of speech, discourse, and structural features canoutperform a text summarization approach on thisdata, and there are many additional features to beexplored.
Of particular interest to us are featuresrelating to speaker status, i.e.
features that help usdetermine who is leading the meeting and who it isthat others are deferring to.
We would also like tomore closely investigate the relationship betweenareas of high speaker activity and informative ut-terances.In the immediate future, we will incorporatethese features into a machine-learning framework,building support vector models trained on the ex-tracted and non-extracted classes of the trainingset.Finally, we will apply these methods to theAMI corpus [18] and create summaries of compa-rable length for that meeting set.
There are likelyto be differences regarding usefulness of certainfeatures due to the ICSI meetings being relativelyunstructured and informal and the AMI hub meet-ings being more structured with a higher informa-tion density.8.
ConclusionThe results presented above show that using fea-tures related to speaker activity, listener feedback,discourse cues and dialogue act length can outper-form the lexical methods of text summarization ap-proaches.
More specifically, the fact that there aremultiple types of important utterances requires thatwe use multiple methods of detecting importance.Lexical methods and prosodic features are not nec-essarily going to detect utterances that are relevantto agreement, decision-making or speaker activity.This research also provides further evidence thatROUGE does not correlate well with human judg-ments in this domain.
Finally, it has been demon-strated that high WER for ASR output does notsignificantly decrease summarization quality.9.
AcknowledgementsThanks to Thomas Hain and the AMI-ASR groupfor speech recognition output.
This work waspartly supported by the European Union 6th FWPIST Integrated Project AMI (Augmented Multi-party Interaction, FP6-506811, publication AMI-150).10.
References[1] K. Koumpis and S. Renals, ?Automatic sum-marization of voicemail messages using lex-ical and prosodic features,?
ACM Transac-tions on Speech and Language Processing,vol.
2, pp.
1?24, 2005.
[2] G. Murray, S. Renals, and J. Carletta, ?Ex-tractive summarization of meeting record-ings,?
in Proceedings of the 9th EuropeanConference on Speech Communication andTechnology, Lisbon, Portugal, September2005.
[3] D. Radev, S. Blair-Goldensohn, andZ.
Zhang, ?Experiments in single and multi-document summarization using mead,?
inThe Proceedings of the First DocumentUnderstanding Conference, New Orleans,LA, September 2001.
[4] G. Murray, S. Renals, J. Carletta, andJ.
Moore, ?Evaluating automatic summariesof meeting recordings,?
in Proceedings ofthe 43rd Annual Meeting of the Associa-tion for Computational Linguistics, Work-shop on Machine Translation and Summa-rization Evaluation (MTSE), Ann Arbor, MI,USA, June 2005.
[5] C.-Y.
Lin and E. H. Hovy, ?Automaticevaluation of summaries using n-gram co-occurrence statistics,?
in Proceedings ofHLT-NAACL 2003, Edmonton, Calgary,Canada, May 2003.
[6] T. Hori, C. Hori, and Y. Minami, ?Speechsummarization using weighted finite-statetransducers,?
in Proceedings of the 8th Eu-ropean Conference on Speech Communica-tion and Technology, Geneva, Switzerland,September 2003.373[7] S. Maskey and J. Hirschberg, ?Compar-ing lexial, acoustic/prosodic, discourse andstructural features for speech summariza-tion,?
in Proceedings of the 9th EuropeanConference on Speech Communication andTechnology, Lisbon, Portugal, September2005.
[8] K. Ohtake, K. Yamamoto, Y. Toma, S. Sado,S.
Masuyama, and S. Nakagawa, ?Newscastspeech summarization via sentence shorten-ing based on prosodic features,?
in Proceed-ings of the ISCA and IEEE Workshop onSpontaneous Speech Processing and Recog-nition, Tokyo, Japan, April 2003,.
[9] K. Zechner, ?Automatic summarization ofopen-domain multiparty dialogues in diversegenres,?
Computational Linguistics, vol.
28,no.
4, pp.
447?485, 2002.
[10] Y. Gong and X. Liu, ?Generic text sum-marization using relevance measure and la-tent semantic analysis,?
in Proceedings ofthe 24th Annual International ACM SI-GIR Conference on Research and Develop-ment in Information Retrieval, New Orleans,Louisiana, USA, September 2001, pp.
19?25.
[11] J. Steinberger and K. Jez?ek, ?Using latentsemantic analysis in text summarization andsummary evaluation,?
in Proceedings of ISIM2004, Roznov pod Radhostem, Czech Repub-lic, April 2004, pp.
93?100.
[12] P. Foltz, W. Kintsch, and T. Landauer, ?Themeasurement of textual coherence with la-tent semantic analysis,?
Discourse Processes,vol.
25, 1998.
[13] B. Hachey, G. Murray, and D. Reitter, ?Theembra system at duc 2005: Query-orientedmulti-document summarization with a verylarge latent semantic space,?
in Proceedingsof the Document Understanding Conference(DUC) 2005, Vancouver, BC, Canada, Octo-ber 2005.
[14] E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, ,and H. Carvey, ?The ICSI meeting recorderdialog act (MRDA) corpus,?
in Proceedingsof the 5th SIGdial Workshop on Discourseand Dialogue, Cambridge, MA, USA, April-May 2004, pp.
97?100.
[15] T. Hain, J. Dines, G. Garau, M. Karafiat,D.
Moore, V. Wan, R. Ordelman,I.Mc.Cowan, J.Vepa, and S.Renals, ?Aninvestigation into transcription of conferenceroom meetings,?
Proceedings of the 9thEuropean Conference on Speech Commu-nication and Technology, Lisbon, Portugal,September 2005.
[16] A. Nenkova and B. Passonneau, ?Evaluat-ing content selection in summarization: Thepyramid method,?
in Proceedings of HLT-NAACL 2004, Boston, MA, USA, May 2004.
[17] R. Valenza, T. Robinson, M. Hickey, andR.
Tucker, ?Summarization of spoken audiothrough information extraction,?
in Proceed-ings of the ESCA Workshop on Accessing In-formation in Spoken Audio, Cambridge UK,April 1999, pp.
111?116.
[18] J. Carletta, S. Ashby, S. Bourban, M. Flynn,M.
Guillemot, T. Hain, J. Kadlec,V.
Karaiskos, W. Kraaij, M. Kronen-thal, G. Lathoud, M. Lincoln, A. Lisowska,I.
McCowan, W. Post, D. Reidsma, andP.
Wellner, ?The AMI meeting corpus:A pre-announcement,?
in Proceedings ofMLMI 2005, Edinburgh, UK, June 2005.374
