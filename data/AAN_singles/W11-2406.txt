Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 40?49,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsRepresenting and resolving ambiguitiesin ontology-based question answeringChristina UngerCognitive Interaction Technology ?
Center of Excellence (CITEC),Universit?t Bielefeld, Germany{cunger|cimiano}@cit-ec.uni-bielefeld.dePhilipp CimianoAbstractAmbiguities are ubiquitous in natural lan-guage and pose a major challenge for the au-tomatic interpretation of natural language ex-pressions.
In this paper we focus on differ-ent types of lexical ambiguities that play a rolein the context of ontology-based question an-swering, and explore strategies for capturingand resolving them.
We show that by employ-ing underspecification techniques and by us-ing ontological reasoning in order to filter outinconsistent interpretations as early as possi-ble, the overall number of interpretations canbe effectively reduced by 44 %.1 IntroductionAmbiguities are ubiquitous in natural language.They pose a key challenge for the automatic inter-pretation of natural language expressions and havebeen recognized as a central issue in question an-swering (e.g.
in (Burger et al, 2001)).
In gen-eral, ambiguities comprise all cases in which nat-ural language expressions (simple or complex) canhave more than one meaning.
These cases roughlyfall into two classes: They either concern structuralproperties of an expression, e.g.
different parses dueto alternative preposition or modifier attachmentsand different quantifier scopings, or they concern al-ternative meanings of lexical items.
It is these lat-ter ambiguities, ambiguities with respect to lexicalmeaning, that we are interested in.
More specifi-cally, we will look at ambiguities in the context ofontology-based interpretation of natural language.The meaning of a natural language expression inthe context of ontology-based interpretation is theontology concept that this expression verbalizes.
Forexample, the expression city can refer to a classgeo:city (where geo is the namespace of the corre-sponding ontology), and the expression inhabitantscan refer to a property geo:population.
The cor-respondence between natural language expressionsand ontology concepts need not be one-to-one.
Onthe one hand side, different natural language expres-sions can refer to a single ontology concept, e.g.flows through, crosses through and traverses couldbe three ways of expressing an ontological propertygeo:flowsThrough.
On the other hand, one natu-ral language expression can refer to different ontol-ogy concepts.
For example, the verb has is vaguewith respect to the relation it expresses ?
it couldmap to geo:flowsThrough (in the case of rivers)as well as geo:inState (in the case of cities).
Suchmismatches between the linguistic meaning of anexpression, i.e.
the user?s conceptual model, and theconceptual model in the ontology give rise to a num-ber of ambiguities.
We will give a detailed overviewof those ambiguities in Section 3, after introducingpreliminaries in Section 2.For a question answering system, there are mainlytwo ways to resolve ambiguities: by interactive clar-ification and by means of background knowledgeand the context with respect to which a question isasked and answered.
The former is, for example,pursued by the question answering system FREyA(Damljanovic et al, 2010).
The latter is incorporatedin some recent work in machine learning.
For exam-ple, (Kate & Mooney, 2007) investigate the task of40learning a semantic parser from a corpus whith sen-tences annotated with multiple, alternative interpre-tations, and (Zettlemoyer & Collins, 2009) explorean unsupervised algorithm for learning mappingsfrom natural language sentences to logical forms,with context accounted for by hidden variables in aperceptron.In ontology-based question answering, context aswell as domain knowledge is provided by the ontol-ogy.
In this paper we explore how a given ontologycan be exploited for ambiguity resolution.
We willconsider two strategies in Section 4.
The first oneconsists in simply enumerating all possible interpre-tations.
Since this is not efficient (and maybe noteven feasible), we will use underspecification tech-niques for representing ambiguities in a much morecompact way and then present a strategy for resolv-ing ambiguities by means of ontological reasoning,so that the number of interpretations that have to beconsidered in the end is relatively small and does notcomprise inconsistent and therefore undesired inter-pretations.
We will summarize with quantitative re-sults in Section 5.2 PreliminariesAll examples throughout the paper will be based onRaymond Mooney?s GeoBase1 dataset and the DB-pedia question set published in the context of the1st Workshop on Question Answering Over LinkedData (QALD-1)2.
The former is a relatively smalland well-organized domain, while the latter is con-siderably larger and much more heterogenous.
It isinteresting to note that ontological ambiguituies turnout to be very wide-spread even in a small and ho-mogenuous domain like GeoBase (see Section 3 forspecific results).For specifying entries of a grammar that a ques-tion answering system might work with, we will usethe general and principled linguistic representationsthat our question answering system Pythia3 (Ungeret al, 2010) relies on, as they are suitable for dealingwith a wide range of natural language phenomena.Syntactic representations will be trees from Lexi-calized Tree Adjoining Grammar (LTAG (Schabes,1cs.utexas.edu/users/ml/nldata/geoquery.html2http://www.sc.cit-ec.uni-bielefeld.de/qald-13http://www.sc.cit-ec.uni-bielefeld.de/pythia1990)).
The syntactic representation of a lexicalitem is a tree constituting an extended projection ofthat item, spanning all of its syntactic and semanticarguments.
Argument slots are nodes marked with adown arrow (?
), for which trees with the same rootcategory can be substituted.
For example, the treefor a transitive verb like borders looks as follows:1.
SDP1 ?
VPVbordersDP2 ?The domain of the verb thus spans a whole sentence,containing its two nominal arguments ?
one in sub-ject position and one in object position.
The corre-sponding nodes, DP1 and DP2, are slots for whichany DP-tree can be substituted.
For example, substi-tuting the two trees in 2 for subject and object DP,respectively, yields the tree in 3.2.
(a) DPDETnoNPstate(b) DPHawaii3.
SDPDETnoNPstateVPVbordersDPHawaiiAs semantic representations we take DUDEs(Cimiano, 2009), representations similar to struc-tures from Underspecified Discourse RepresentationTheory (UDRT (Reyle, 1993)), extended with someadditional information that allows for flexible mean-ing composition in parallel to the construction ofLTAG trees.
The DUDE for the verb to border, forexample, would be the following (in a slightly sim-plified version):geo:borders (x, y)(DP1, x), (DP2, y)41It provides the predicate geo:borders correspond-ing to the intended concept in the ontology.
Thiscorrespondence is ensured by using the vocabularyof the ontology, i.e.
by using the URI4 of the con-cept instead of a more generic predicate.
The pre-fix geo specifies the namespace, in this case the oneof the GeoBase ontology.
Furthermore, the seman-tic representation contains information about whichsubstitution nodes in the syntactic structure providethe semantic arguments x and y.
That is, the seman-tic referent provided by the meaning of the tree sub-stituted for DP1 corresponds to the first argument xof the semantic predicate, while the semantic refer-ent provided by the meaning of the tree substitutedfor DP2 corresponds to the second argument y. Theuppermost row of the box contains the referent thatis introduced by the expression.
For example, theDUDE for Hawaii (paired with the tree in 2b) wouldbe the following:hgeo:name (h,?hawaii?
)It introduces a referent h which is related to the lit-eral ?hawaii?
by means of the relation geo:name.As it does not have any arguments, the third rowis empty.
The bottom-most row, empty in bothDUDEs, is for selectional restrictions of predicates;we will see those in Section 4.Parallel to substituting the DP-tree in 2b for theDP1-slot in 1, the DUDE for Hawaii is combinedwith the DUDE for borders, amounting to the satu-ration of the argument (DP2, y) by unifying the vari-ables h and y, yielding the following DUDE:hgeo:borders (x, h)geo:name (h,?hawaii?
)(DP1, x)Substituting the subject argument no state involvesquantifier representations which we will gloss overas they do not play a role in this paper.
At this point4URI stands for Uniform Resource Identifier.
URIs uniquelyidentify resources on the Web.
For an overview, see, e.g.,http://www.w3.org/Addressing/.it suffices to say that we implement the treatment ofquantifier scope in UDRT without modifications.Once a meaning representation for a question isbuilt, it is translated into a SPARQL query, whichcan then be evaluated with respect to a given dataset.Not a lot hinges on the exact choice of the for-malisms; we could as well have chosen any othersyntactic and semantic formalism that allows the in-corporation of underspecification mechanisms.
Thesame holds for the use of SPARQL as formal querylanguage.
The reason for choosing SPARQL is thatit is the standard query language for the Seman-tic Web5; we therefore feel safe in relying on thereader?s familiarity with SPARQL and use SPARQLqueries without further explanation.3 Types of ambiguitiesAs described in the introduction above, a central taskin ontology-based interpretation is the mapping ofa natural language expression to an ontology con-cept.
And this mapping gives rise to several differentcases of ambiguities.First, ambiguities can arise due to homonymy of anatural language expression, i.e.
an expression thathas several lexical meanings, where each of thesemeanings can be mapped to one ontology conceptunambiguously.
The ambiguity is inherent to the ex-pression and is independent of any domain or ontol-ogy.
This is what in linguistic contexts is called alexical ambiguity.
A classical example is the nounbank, which can mean a financial institution, a kindof seating, the edge of a river, and a range of otherdisjoint, non-overlapping alternatives.
An examplein the geographical domain is New York.
It can meaneither New York city, in this case it would be mappedto the ontological entity geo:new york city, orNew York state, in this case it would be mapped tothe entity geo:new york.
Ambiguous names are ac-tually the only case of such ambiguities that occur inthe GeoBase dataset.Another kind of ambiguities is due to mismatchesbetween a user?s concept of the meaning of anexpression and the modelling of this meaningin the ontology.
For example, if the ontologymodelling is more fine-grained than the meaning5For the W3C reference, seehttp://www.w3.org/TR/rdf-sparql-query/.42of a natural language expression, then an expres-sion with one meaning can be mapped to severalontology concepts.
These concepts could differextensionally as well as intensionally.
An exampleis the above mentioned expression starring, thatan ontology engineer could want to comprise onlyleading roles or also include supporting roles.
Ifhe decides to model this distinction and introducestwo properties, then the ontological model ismore fine-grained than the meaning of the naturallanguage expression, which could be seen as corre-sponding to the union of both ontology properties.Another example is the expression inhabitantsin question 4, which can be mapped either to<http://dbpedia.org/property/population>or to <http://dbpedia.org/ontology/popula-tionUrban>.
For most cities, both alternatives givea result, but they differ slightly, as one captures onlythe core urban area while the other also includesthe outskirts.
For some city, even only one of themmight be specified in the dataset.4.
Which cities have more than two million inhab-itants?Such ambiguities occur in larger datasets like DB-pedia with a wide range of common nouns and tran-sitive verbs.
In the QALD-1 training questions forDBpedia, for example, at least 16 % of the questionscontain expressions that do not have a unique onto-logical correspondent.Another source for ambiguities is the large num-ber of vague and context-dependent expressions innatural language.
While it is not possible to pin-point such expressions to a fully specified lexicalmeaning, a question answering system needs to mapthem to one (or more) specific concept(s) in the on-tology.
Often there are several mapping possibili-ties, sometimes depending on the linguistic contextof the expression.An example for context-dependent expressionsin the geographical domain is the adjective big: itrefers to size (of a city or a state) either with respectto population or with respect to area.
For the ques-tion 5a, for example, two queries could be intended?
one refering to population and one refering to area.They are given in 5b and 5c.5.
(a) What is the biggest city?
(b) SELECT ?s WHERE {?s a geo:city .
?s geo:population ?p .
}ORDER BY DESC ?p LIMIT 1(c) SELECT ?s WHERE {?s a geo:city .
?s geo:area ?a .
}ORDER BY DESC ?a LIMIT 1Without further clarification ?
either by means ofa clarification dialog with the user (e.g.
employedby FREyA (Damljanovic et al, 2010)) or an ex-plicit disambiguation as in What is the biggest cityby area?
?
both interpretations are possible and ade-quate.
That is, the adjective big introduces two map-ping alternatives that both lead to a consistent inter-pretation.A slightly different example are vague expres-sions.
Consider the questions 6a and 7a.
Theverb has refers either to the object propertyflowsThrough, when relating states and rivers, orto the object property inState, when relating statesand cities.
The corresponding queries are given in6b and 7b.6.
(a) Which state has the most rivers?
(b) SELECT COUNT(?s) AS ?n WHERE {?s a geo:state .
?r a geo:river .
?r geo:flowsThrough ?s.
}ORDER BY DESC ?n LIMIT 17.
(a) Which state has the most cities?
(b) SELECT COUNT(?s) AS ?n WHERE {?s a geo:state .
?c a geo:city .
?c geo:inState ?s.
}ORDER BY DESC ?n LIMIT 1In contrast to the example of big above, these twointerpretations, flowsThrough and inState, areexclusive alternatives: only one of them is admis-sible, depending on the linguistic context.
Thisis due to the sortal restrictions of those proper-ties: flowsThrough only allows rivers as domain,whereas inState only allows cities as domain.This kind of ambiguities are very frequent, as alot of user questions contain semantically light ex-pressions, e.g.
the copula verb be, the verb have,43and prepositions like of, in and with (cf.
(Cimiano& Minock, 2009)) ?
expressions which are vagueand do not specify the exact relation they are de-noting.
In the 880 user questions that Mooney pro-vides, there are 1278 occurences of the light expres-sions is/are, has/have, with, in, and of, in additionto 151 ocurrences of the context-dependent expres-sions big, small, and major.4 Capturing and resolving ambiguitiesWhen constructing a semantic representation anda formal query, all possible alternative meaningshave to be considered.
We will look at two strate-gies to do so: simply enumerating all interpretations(constructing a different semantic representation andquery for every possible interpretation), and under-specification (constructing only one underspecifiedrepresentation that subsumes all different interpreta-tions).4.1 EnumerationConsider the example of a lexically ambiguousquestion in 8a.
It contains two ambiguous expres-sions: New York can refer either to the city or thestate, and big can refer to size either with respectto area or with respect to population.
This leads tofour possible interpretations of the questions, givenin 8b?8e.8.
(a) How big is New York?
(b) SELECT ?a WHERE {geo:new york city geo:area ?a .
}(c) SELECT ?p WHERE {geo:new york city geo:population ?p.
}(d) SELECT ?a WHERE {geo:new york geo:area ?a .
}(e) SELECT ?p WHERE {geo:new york geo:population ?p .
}Since the question in 8a can indeed have all four in-terpretations, all of them should be captured.
Theenumeration strategy amounts to constructing allfour queries.
In order to do so, we specify twolexical entries for New York and two lexical en-tries for the adjective big ?
one for each reading.For big, these two entries are given in 9 and 10.The syntactic tree is the same for both, while thesemantic representations differ: one refers to theproperty geo:area and one refers to the propertygeo:population.9.
NADJbigN?ageo:area (x, a)(N, x)10.
NADJbigN?pgeo:population (x, p)(N, x)When parsing the question How big is New York,both entries for big are found during lexical lookup,and analogously two entries for New York are found.The interpretation process will use all of them andtherefore construct four queries, 8b?8e.Vague and context-dependent expressions can betreated similarly.
The verb to have, for example,can map either to the property flowsThrough, inthe case of rivers, or to the property inState, inthe case of cities.
Now we could simply spec-ify two lexical entries to have ?
one using themeaning flowsThrough and one using the mean-ing inState.
However, contrary to lexical ambigu-ities, these are not real alternatives in the sense thatboth lead to consistent readings.
The former is onlypossible if the relevant argument is a river, the lat-ter is only relevant if the relevant argument is a city.So in order not to derive inconsistent interpretations,we need to capture the sortal restrictions attached tosuch exclusive alternatives.
This will be discussedin the next section.4.2 Adding sortal restrictionsA straightforward way to capture ambiguities con-sists in enumerating all possible interpretationsand thus in constructing all corresponding formalqueries.
We did this by specifying a separate lex-ical entry for every interpretation.
The only diffi-culty that arises is that we have to capture the sor-tal restrictions that come with some natural languageexpressions.
In order to do so, we add sortal restric-tions to our semantic representation format.Sortal restrictions will be of the general formvariable?class.
For example, the sortal restrictionthat instances of the variable x must belong to theclass river in our domain would be represented asx?geo:river.
Such sortal restrictions are added as44a list to our DUDEs.
For example, for the verb haswe specify two lexical entries.
One maps has tothe property flowThrough, specifying the sortal re-striction that the first argument of this property mustbelong to the class river.
This entry looks as fol-lows:SDP1 ?
VPVhasDP2 ?geo:flowsThrough (y, x)(DP1, x), (DP2, y)x?geo:riverThe other lexical entry for has consists of thesame syntactic tree and a semantic representationthat maps has to the property inState and containsthe restriction that the first argument of this propertymust belong to the class city.
It looks as follows:SDP1 ?
VPVhasDP2 ?geo:inState (y, x)(DP1, x), (DP2, y)x?geo:cityWhen a question containg the verb has, like 11a,is parsed, both interpretations for has are found dur-ing lexical lookup and two semantic representationsare constructed, both containing a sortal restriction.When translating the semantic representations into aformal query, the sortal restriction is simply added asa condition.
For 11a, the two corresponding queriesare given in 11b (mapping has to flowsThrough)and 11c (mapping has inState).
The contributionof the sortal restriction is boxed.11.
(a) Which state has the most rivers?
(b) SELECT COUNT(?r) as ?c WHERE {?s a geo:state .
?r a geo:river .
?r geo:flowsThrough ?s .
?r a geo:river .
}ORDER BY ?c DESC LIMIT 1(c) SELECT COUNT(?r) as ?c WHERE {?s a geo:state .
?r a geo:river .
?r geo:inState ?s .
?r a geo:city .
}ORDER BY ?c DESC LIMIT 1In the first case, 11b, the sortal restriction adds aredundant condition and will have no effect.
We cansay that the sortal restriction is satisfied.
In the sec-ond case, in 11c, however, the sortal restriction addsa condition that is inconsistent with the other condi-tions, assuming that the classes river and city areproperly specified as disjoint.
The query will there-fore not yield any results, as no instantiiation of rcan be found that belongs to both classes.
That is,in the context of rivers only the interpretation usingflowsThrough leads to results.Actually, the sortal restriction in 11c is al-ready implicitly specified in the ontological relationinState: there is no river that is related to a statewith this property.
However, this is not necessar-ily the case and there are indeed queries where thesortal restriction has to be included explicitly.
Oneexample is the interpretation of the adjective majorin noun phrases like major city and major state.
Al-though with respect to the geographical domain ma-jor always expresses the property of having a pop-ulation greater than a certain threshold, this thresh-old differs for cities and states: major with respectto cities is interpreted as having a population greaterthan, say, 150 000, while major with respect to statesis interpreted as having a population greater than,say, 10 000 000.
Treating major as ambiguous be-tween those two readings without specifying a sortalrestriction would lead to two readings for the nounphrase major city, sketched in 12.
Both would yieldnon-empty results and there is no way to tell whichone is the correct one.12.
(a) SELECT ?c WHERE {?c a geo:city .
?c geo:population ?p .FILTER ( ?p > 150000 ) }(b) SELECT ?c WHERE {?c a geo:city .
?c geo:population ?p .FILTER ( ?p > 10000000 ) }Specifying sortal restrictions, on the other hand,would add the boxed material in 13, thereby caus-ing the wrong reading in 13b to return no results.13.
(a) SELECT ?c WHERE {?c a geo:city .
?c geo:population ?p .45FILTER ( ?p > 150000 ) .
?c a geo:city .
}(b) SELECT ?c WHERE {?c a geo:city .
?c geo:population ?p .FILTER ( ?p > 10000000 ) .
?c a geo:state .
}The enumeration strategy thus relies on a conflictthat results in queries which return no result.
Un-wanted interpretations are thereby filtered out auto-matically.
But two problems arise here.
The firstone is that we have no way to distinguish betweenqueries that return no result due to an inconsistencyintroduced by a sortal restriction, and queries thatreturn no result, because there is none, as in the caseof Which states border Hawaii?.
The second prob-lem concerns the number of readings that are con-structed.
In view of the large number of ambiguities,even in the restricted geographical domain we used,user questions easily lead to 20 or 30 different pos-sible interpretations.
In cases in which several natu-ral language terms can be mapped to many differentontological concepts, this number rises.
Enumerat-ing all alternative interpretations is therefore not ef-ficient.
A more practical alternative is to constructone underspecified representation instead and theninfer a specific interpretation in a given context.
Wewill explore this strategy in the next section.4.3 UnderspecificationIn the following, we will explore a strategy for rep-resenting and resolving ambiguities that uses under-specification and ontological reasoning in order tokeep the number of constructed interpretations to aminimum.
For a general overview of underspecifica-tion formalisms and their applicability to linguisticphenomena see (Bunt, 2007).In order not to construct a different query forevery interpretation, we do not any longer specifyseparate lexical entries for each mapping but rathercombine them by using an underspecified semanticrepresentation.
In the case of has, for example, wedo not specify two lexical entries ?
one with a se-mantic representation using flowsThrough and oneentry with a representation using inState ?
but in-stead specify only one lexical entry with a represen-tation using a metavariable, and additionally specifywhich properties this metavariable stands for underwhich conditions.So first we extend DUDEs such that they now cancontain metavariables, and instead of a list of sor-tal restrictions contain a list of metavariable speci-fications, i.e.
possible instantiations of a metavari-able given that certain sortal restrictions are satis-fied, where sortal restrictions can concern any of theproperty?s arguments.
Metavariable specificationstake the following general form:P ?
p1 (x = class1, .
.
.
, y = class2)| p2 (x = class3, .
.
.
, y = class4)| .
.
.| pn (x = classi, .
.
.
, y = classj)This expresses that some metavariable P stands fora property p1 if the types of the arguments x, .
.
.
, yare equal to or a subset of class1,.
.
.
,class2, andstands for some other property if the types of thearguments correspond to some other classes.
Forexample, as interpretation of has, we would chosea metavariable P with a specification stating that Pstands for the property flowsThrough if the first ar-gument belongs to class river, and stands for theproperty inState if the first argument belongs tothe class city.
Thus, the lexical entry for has wouldcontain the following underspecified semantic repre-sentation.14.
Lexical meaning of ?has?
:P (y, x)(DP1, x), (DP2, y)P ?
geo:flowsThrough (y = geo:river)| geo:inState (y = geo:city)Now this underspecified semantic representation hasto be specified in order to lead to a SPARQL querythat can be evaluated w.r.t.
the knowledge base.That means, in the course of interpretation we needto determine which class an instantiation of y be-longs to and accordingly substitute P by the prop-erty flowsThrough or inState.
In the followingsection, we sketch a way of exploiting the ontologyto this end.464.4 Reducing alternatives with ontologicalreasoningIn order to filter out interpretations that are inconsis-tent as early as possible and thereby reduce the num-ber of interpretations during the course of a deriva-tion, we check whether the type information of avariable that is unified is consistent with the sor-tal restrictions connected to the metavariables.
Thischeck is performed at every relevant step in a deriva-tion, so that inconsistent readings are not allowed topercolate and multiply.
Let us demonstrate this strat-egy by means of the example Which state has thebiggest city?.In order to build the noun phrase the biggestcity, the meaning representation of the superlativebiggest, given in 15, is combined with that of thenoun city, which simply contributes the predicationgeo:city (y), by means of unification.15.zQ (y, z)(N, y)Q ?
geo:area(y = geo:city unionsq geo:state)| geo:population(y = geo:city unionsq geo:state)The exact details of combining meaning represen-tations do not matter here.
What we want to fo-cus on is the metavariable Q that biggest introduces.When combining 15 with the meaning of city, wecan check whether the type information connectedto the unified referent y is compatible with the do-main restrictions of Q?s interpretations.
One wayto do this is by integrating an OWL reasoner andchecking the satisfiability ofgeo:city u (geo:city unionsq geo:state)(for both interpretations of Q, as the restrictions ony are the same).
Since this is indeed satisfiable,both interpretations are possible, thus cannot be dis-carded, and the resulting meaning representation ofthe biggest city is the following:y zgeo:city(y)Q (y, z)max(z)Q ?
geo:area(y = geo:city unionsq geo:state)| geo:population(y = geo:city unionsq geo:state)This is desired, as the ambiguity of biggest is a lexi-cal ambiguity that could only be resolved by the userspecifying which reading s/he intended.In a next step, the above representation is com-bined with the semantic representation of the verbhas, given in 14.
Now the type information of theunified variable y has to be checked for compati-bility with instantiations of an additional metavari-able, P .
The OWL reasoner would therefore have tocheck the satisfiability of the following two expres-sions:16.
(a) geo:city u geo:river(b) geo:city u geo:cityWhile 16b succeeds trivially, 16a fails, assumingthat the two classes geo:river and geo:city arespecified as disjoint in the ontology.
Thereforethe instantiation of P as geo:flowsThrough isnot consistent and can be discarded, leading to thefollowing combined meaning representation, whereP is replaced by its only remaining instantiationgeo:inState:y zgeo:city(y)geo:inState (y, x)Q (y, z)(DP1, x)Q ?
geo:area(y = geo:city unionsq geo:state)| geo:population(y = geo:city unionsq geo:state)Finally, this meaning representation is com-bined with the meaning representation of whichstate, which simply contributes the predicationgeo:state (x).
As the unified variable x does notoccur in any metavariable specification, nothing fur-ther needs to be checked.
The final meaning repre-sentation thus leaves one metavariable with two pos-sible instantiations and will lead to the following twocorresponding SPARQL queries:4717.
(a) SELECT ?x WHERE {?x a geo:city .
?y a geo:state.
?x geo:population ?z .
?x geo:inState ?y .
}ORDER BY DESC(?z) LIMIT 1(b) SELECT ?x WHERE {?x a geo:city .
?y a geo:state.
?x geo:area ?z .
?x geo:inState ?y .
}ORDER BY DESC(?z) LIMIT 1Note that if the ambiguity of the metavariableP were not resolved, we would have ended upwith four SPARQL queries, where two of them usethe relation geo:flowsThrough and therefore yieldempty results.
So in this case, we reduced the num-ber of constructed queries by half by discarding in-consistent readings.
We therefore solved the prob-lems mentioned at the end of 4.2: The number ofconstructed queries is reduced, and since we discardinconsistent readings, null answers can only be dueto the lack of data in the knowledge base but not can-not anymore be due to inconsistencies in the gener-ated queries.5 Implementation and resultsIn order to see that the possibility of reducing thenumber of interpretations during a derivation doesnot only exist in a small number of cases, we ap-plied Pythia to Mooney?s 880 user questions, imple-menting the underspecification strategy in 4.3 andthe reduction strategy in 4.4.
Since Pythia does notyet integrate a reasoner, it approximates satisfiabil-ity checks by means of SPARQL queries.
When-ever meaning representations are combined, it ag-gregates type information for the unified variable,together with selectional information connected tothe occuring metavariables, and uses both to con-struct a SPARQL query.
This query is then evalu-ated against the underlying knowledge base.
If thequery returns results, the interpetations are taken tobe compatible, if it does not return results, the in-terpretations are taken to be incompatible and theaccording instantiation possibility of the metavari-able is discarded.
Note that those SPARQL queriesare only an approximation for the OWL expressionsused in 4.4.
Furthermore, the results they return areonly an approximation of satisfiability, as the reasonfor not returning results does not necessarily need tobe unsatisfiability of the construction but could alsobe due the absence of data in the knowledge base.In order to overcome these shortcomings, we plan tointegrate a full-fledged OWL reasoner in the future.Out of the 880 user questions, 624 can be parsedby Pythia (for an evaluation on this dataset and rea-sons for failing with the remaining 256 questions,see (Unger & Cimiano, 2011)).
Implementing theenumeration strategy, i.e.
not using disambiguationmechanisms, there was a total of 3180 constructedqueries.
With a mechanism for removing scope am-biguities by means of simulating a linear scope pref-erence, a total of 2936 queries was built.
Addi-tionally using the underspecification and resolutionstrategies described in the previous section, by ex-ploiting the ontology with respect to which naturallanguage expressions are interpreted in order to dis-card inconsistent interpretations as early as possiblein the course of a derivation, the number of totalqueries was further reduced to 2100.
This amountsto a reduction of the overall number of queries by44 %.
The average and maximum number of queriesper question are summarized in the following table.Avg.
# queries Max.
# queriesEnumeration 5.1 96Linear scope 4.7 (-8%) 46 (-52%)Reasoning 3.4 (-44%) 24 (-75%)6 ConclusionWe investigated ambiguities arising from mis-matches between a natural language expressions?lexical meaning and its conceptual modelling in anontology.
Employing ontological reasoning for dis-ambiguation allowed us to significantly reduce thenumber of constructed interpretations: the averagenumber of constructed queries per question can bereduced by 44 %, the maximum number of queriesper question can be reduced even by 75 %.48ReferencesBunt, H.: Semantic Underspecification: Which Tech-nique For What Purpose?
In: Computing Meaning,vol.
83, pp.
55?85.
Springer Netherlands (2007)Cimiano, P.: Flexible semantic composition withDUDES.
In: Proceedings of the 8th International Con-ference on Computational Semantics (IWCS).
Tilburg(2009)Unger, C., Hieber, F., Cimiano, P.: Generating LTAGgrammars from a lexicon-ontology interface.
In: S.Bangalore, R. Frank, and M. Romero (eds.
): 10th In-ternational Workshop on Tree Adjoining Grammarsand Related Formalisms (TAG+10), Yale University(2010)Unger, C., Cimiano, P.: Pythia: Compositional mean-ing construction for ontology-based question answer-ing on the Semantic Web.
In: Proceedings of the 16thInternational Conference on Applications of NaturalLanguage to Information Systems (NLDB) (2011)Schabes, Y.: Mathematical and Computational Aspectsof Lexicalized Grammars.
Ph.
D. thesis, University ofPennsylvania (1990)Reyle, U.: Dealing with ambiguities by underspecifica-tion: Construction, representation and deduction.
Jour-nal of Semantics 10, 123?179 (1993)Kamp, H., Reyle, U.: From Discourse to Logic.
Kluwer,Dordrecht (1993)Cimiano, P., Minock, M.: Natural Language Interfaces:What?s the Problem?
?
A Data-driven QuantitativeAnalysis.
In: Proceedings of the International Confer-ence on Applications of Natural Language to Informa-tion Systems (NLDB), pp.
192?206 (2009)Damljanovic, D., Agatonovic, M., Cunningham, H.:Natural Language Interfaces to Ontologies: Combin-ing Syntactic Analysis and Ontology-based Lookupthrough the User Interaction.
In: Proceedings of the7th Extended Semantic Web Conference, SpringerVerlag (2010)Zettlemoyer, L., Collins, M.: Learning Context-dependent Mappings from Sentences to Logical Form.In: Proceedings of the Joint Conference of the As-sociation for Computational Linguistics and Interna-tional Joint Conference on Natural Language Process-ing (ACL-IJCNLP), pp.
976?984 (2009)Burger, J., Cardie, C., Chaudhri, V., Gaizauskas,R., Israel, D., Jacquemin, C., Lin, C.-Y., Maio-rano, S., Miller, G., Moldovan, D., Ogden,B., Prager, J., Riloff, E., Singhal, A., Shrihari,R., Strzalkowski, T., Voorhees, E., Weischedel,R.
: Issues, tasks, and program structures toroadmap research in question & answering (Q & A).http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper v2.doc (2001)Kate, R., Mooney, R.: Learning Language Semanticsfrom Ambiguous Supervision.
In: Proceedings of the22nd Conference on Artificial Intelligence (AAAI-07),pp.
895?900 (2007)49
