The Grammar Matrix: An Open-Source Starter-Kit for the RapidDevelopment of Cross-Linguistically Consistent Broad-Coverage PrecisionGrammarsEmily M. Bender and Dan Flickinger and Stephan OepenCenter for the Study of Language and InformationStanford Universityfbender jdan joeg@csli.stanford.eduAbstractThe grammar matrix is an open-sourcestarter-kit for the development of broad-coverage HPSGs.
By using a type hierar-chy to represent cross-linguistic generaliza-tions and providing compatibility with otheropen-source tools for grammar engineering,evaluation, parsing and generation, it facil-itates not only quick start-up but also rapidgrowth towards the wide coverage necessaryfor robust natural language processing andthe precision parses and semantic represen-tations necessary for natural language under-standing.1 IntroductionThe past decade has seen the development ofwide-coverage implemented grammars represent-ing deep linguistic analysis of several languagesin several frameworks, including Head-DrivenPhrase Structure Grammar (HPSG), Lexical-Functional Grammar (LFG), and Lexicalized TreeAdjoining Grammar (LTAG).
In HPSG, the most ex-tensive grammars are those of English (Flickinger,2000), German (Mu?ller & Kasper, 2000), andJapanese (Siegel, 2000; Siegel & Bender, 2002).Despite being couched in the same general frame-work and in some cases being written in thesame formalism and consequently being compati-ble with the same parsing and generation software,these grammars were developed more or less inde-pendently of each other.
They each represent be-tween 5 and 15 person years of research efforts,and comprise 35?70,000 lines of code.
Unfor-tunately, most of that research is undocumentedand the accumulated analyses, best practices forgrammar engineering, and tricks of the trade areonly available through painstaking inspection ofthe grammars and/or consultation with their au-thors.
This lack of documentation holds acrossframeworks, with certain notable exceptions, in-cluding Alshawi (1992), Mu?ller (1999), and Butt,King, Nin?o, & Segond (1999).Grammars which have been under developmentfor many years tend to be very difficult to mine forinformation, as they contain layers upon layers ofinteracting analyses and decisions made in light ofvarious intermediate stages of the grammar.
As aresult, when embarking on the creation of a newgrammar for another language, it seems almosteasier to start from scratch than to try to model it onan existing grammar.
This is unfortunate?beingable to leverage the knowledge and infrastructureembedded in existing grammars would greatly ac-celerate the process of developing new ones.
At thesame time, these grammars represent an untappedresource for the bottom-up exploration of languageuniversals.As part of the LinGO consortium?s multi-lingualgrammar engineering effort, we are developing a?grammar matrix?
or starter-kit, distilling the wis-dom of existing grammars and codifying and doc-umenting it in a form that can be used as the basisfor new grammars.In the following sections, we outline the inven-tory of a first, preliminary version of the grammarmatrix, discuss the interaction of basic construc-tion types and semantic composition in unificationgrammars by means of a detailed example, andconsider extensions to the core inventory that weforesee and an evaluation methodology for the ma-trix proper.2 Preliminary Development of MatrixWe have produced a preliminary version of thegrammar matrix relying heavily on the LinGOproject?s English Resource Grammar, and to alesser extent on the Japanese grammar developedjointly between DFKI Saarbru?cken (Germany) andYY Technologies (Mountain View, CA).
This earlyversion of the matrix comprises the following com-ponents: Types defining the basic feature geometry andtechnical devices (e.g., for list manipulation). Types associated with Minimal Recursion Se-mantics (see, e.g., Copestake, Lascarides, &Flickinger, 2001), a meaning representationlanguage which has been shown to be well-suited for semantic composition in typed fea-ture structure grammars.
This portion of thegrammar matrix includes a hierarchy of rela-tion types, types and constraints for the prop-agation of semantic information through thephrase structure tree, a representation of illo-cutionary force, and provisions for grammarrules which make semantic contributions. General classes of rules, including deriva-tional and inflectional (lexical) rules, unaryand binary phrase structure rules, headed andnon-headed rules, and head-initial and head-final rules.
These rule classes include im-plementations of general principles of HPSG,like, for example, the Head Feature and Non-Local Feature Principles. Types for basic constructions such as head-complement, head-specifier, head-subject,head-filler, and head-modifier rules, coordi-nation, as well as more specialized classesof constructions, such as relative clauses andnoun-noun compounding.
Unlike in specificgrammars, these types do not impose any or-dering on their daughters in the grammar ma-trix.Included with the matrix are configuration andparameter files for the LKB grammar engineeringenvironment (Copestake, 2002).Although small, this preliminary version ofthe matrix already reflects the main goals ofthe project: (i) Consistent with other work inHPSG, semantic representations and in particularthe syntax-semantics interface are developed in de-tail; (ii) the types of the matrix are each represen-tations of generalizations across linguistic objectsand across languages; and (iii) the richness of thematrix and the incorporation of files which connectit with the LKB allow for extremely quick start-upas the matrix is applied to new languages.Since February 2002, this preliminary version ofthe matrix has been in use at two Norwegian uni-versities, one working towards a broad-coveragereference implementation of Norwegian (NTNU),the other?for the time being?focused on specificaspects of clause structure and lexical description(Oslo University).
In the first experiment withthe matrix, at NTNU, basic Norwegian sentenceswere parsing and producing reasonable semanticswithin two hours of downloading the matrix files.Linguistic coverage should scale up quickly, sincethe foundation supplied by the matrix is designednot only to provide a quick start, but also to supportlong-term development of broad-coverage gram-mars.
Both initiatives have confirmed the utility ofthe matrix starter kit and already have contributedto a series of discussions on cross-lingual HPSGdesign aspects, specifically in the areas of argu-ment structure representations in the lexicon andbasic assumptions about constituent structure (inone view, Norwegian exhibits a VSO topology inthe main clause).
The user groups have suggestedrefinements and extensions of the basic inventory,and it is expected that general solutions, as they areidentified jointly, will propagate into the existinggrammars too.3 A Detailed ExampleAs an example of the level of detail involved inthe grammar matrix, in this section we considerthe analysis of intersective and scopal modifica-tion.
The matrix is built to give Minimal RecursionSemantics (MRS; Copestake et al, 2001; Copes-take, Flickinger, Sag, & Pollard, 1999; Copestake,Flickinger, Malouf, Riehemann, & Sag, 1995) rep-resentations.
The two English examples in (1)exemplify the difference between intersective andscopal modification:1(1) a. Keanu studied Kung Fu on a spaceship.b.
Keanu probably studied Kung Fu.The MRSs for (1a-b) (abstracting away fromagreement information) are given in (2) and (3).The MRSs are ordered tuples consisting of a tophandle (h1 in both cases), an instance or event vari-able (e in both cases), a bag of elementary predica-tions (eps), and a bag of scope constraints (in thesecases, QEQ constraints or ?equal modulo quanti-fiers?).
In a well-formed MRS, the handles can be1These examples also differ in that probably is a pre-head modifier while on a spaceship is a post-head modifier.This word-order distinction cross-cuts the semantic distinc-tion, and our focus is on the latter, so we won?t consider theword-order aspects of these examples here.identified in one or more ways respecting the scopeconstraints such that the dependencies between theeps form a tree.
For a detailed description of MRS,see the works cited above.
Here, we will focus onthe difference between the intersective modifier on(a spaceship) and the scopal modifier probably.In (2), the ep contributed by on (?on-rel?)
sharesits handle (h7) with the ep contributed by the verbit is modifying (?study-rel?).
As such, the two willalways have the same scope; no quantifier can in-tervene.
Further, the second argument of the on-rel(e) is the event variable of the study-rel.
The firstargument, e0, is the event variable of the on-rel andthe third argument, z, is the instance variable of thespaceship-rel.
(2) h h1, e,f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),h6:named-rel(x, ?Keanu?
), h7:study-rel(e, x, y),h8:def-np-rel(y, h9, h10),h11:named-rel(y, ?Kung Fu?
), h7:on-rel(e0, e, z),h12:a-quant-rel(z, h13, h14),h15:spaceship-rel(z) g,f h2 QEQ h7, h4 QEQ h6, h19 QEQ h11,h13 QEQ h15 g iIn (3), the ep contributed by the scopal modifierprobably (?probably-rel?)
has its own handle (h7)which is not shared by anything.
Furthermore, ittakes a handle (h8) rather than the event variableof the study-rel as its argument.
h8 is equal mod-ulo quantifiers (QEQ) to the handle of the study-rel(h9), and h7 is equal modulo quantifiers to the ar-gument of the prpstn-rel (h2).
The prpstn-rel is theep representing the illocutionary force of the wholeexpression.
This means that quantifiers associatedwith the NPs Keanu and Kung Fu can scope insideor outside probably.
(3) h h1, e,f h1:prpstn-rel(h2), h3:def-np-rel(x, h4, h5),h6:named-rel(x, ?Keanu?
),h7:probably-rel(h8), h9:study-rel(e, x, y),h10:def-np-rel(y, h11, h12),h13:named-rel(y, ?Kung Fu?)
g,f h2 QEQ h7, h4 QEQ h6, h8 QEQ h9,h11 QEQ h13 g iWhile the details of modifier placement, whichparts of speech can modify which kinds of phrases,etc., differ across languages, we believe that alllanguages display a distinction between scopal andintersective modification.
Accordingly, the typesisect-mod-phrase := head-mod-phr-simple &[ HEAD-DTR.SYNSEM.LOCAL[ CONT [ TOP #hand,INDEX #index ],KEYS.MESSAGE 0-dlist ],NON-HEAD-DTR.SYNSEM.LOCAL[ CAT.HEAD.MOD <[ LOCAL isect-mod ]>,CONT.TOP #hand ],C-CONT.INDEX #index ].Figure 1: TDL description of isect-mod-phrasescopal-mod-phrase := head-mod-phr-simple &[ NON-HEAD-DTR.SYNSEM.LOCAL[ CAT.HEAD.MOD <[ LOCAL scopal-mod ]>,CONT.INDEX #index ],C-CONT.INDEX #index ].Figure 2: TDL description of scopal-mod-phrasenecessary for describing these two kinds of modi-fication are included in the matrix.The types isect-mod-phrase and scopal-mod-phrase (shown in Figures 1 and 2) encode the in-formation necessary to build up in a compositionalmanner the modifier portions of the MRSs in (2)and (3).These types are embedded in the type hierar-chy of the matrix.
Through their supertype head-mod-phr-simple they inherit information commonto many types of phrases, including the basic fea-ture geometry, head feature and non-local featurepassing, and semantic compositionality.
Thesetypes also have subtypes in the matrix specifyingthe two word-order possibilities (pre- or post-headmodifiers), giving a total of four subtypes.2The most important difference between thesetypes is in the treatment of the handle of the headdaughter?s semantics, to distinguish intersectiveand scopal modification.
In isect-mod-phrase, thetop handles (TOP) of the head and non-head (i.e.,modifier) daughters are identified (#hand).
Thisallows for MRSs like (2) where the eps contributedby the head (?study-rel?)
and the modifier (?on-rel?
)take the same scope.
The type scopal-mod-phrasebears no such constraint.
This allows for MRSslike (3) where the modifier?s semantic contribution(?probably-rel?)
takes the handle of the head?s se-mantics (?study-rel?)
as its argument, so that themodifier outscopes the head.
In both types of mod-2All four subtypes are provided on the theory that mostlanguages will make use of all or most of them.ifier phrase, a constraint inherited from the super-type ensures that the handle of the modifier is alsothe handle of the whole phrase.The constraints on the LOCAL value insidethe modifier?s MOD value regulate which lexi-cal items can appear in which kind of phrase.Intersective modifiers specify lexically that theyare [ MOD h [ LOCAL isect-mod ] i] and sco-pal modifiers specify lexically that they are[ MOD h [ LOCAL scopal-mod ] i].3 These con-straints exemplify the kind of information that willbe developed in the lexical hierarchy of the matrix.It is characteristic of broad-coverage grammarsthat every particular analysis interacts with manyother analyses.
Modularization is an on-going con-cern, both for maintainability of individual gram-mars, and for providing the right level of abstrac-tion in the matrix.
For the same reasons, we haveonly been able to touch on the highlights of the se-mantic analysis of modification here, but hope thatthis quick tour will suffice to illustrate the extentof the jump-start the matrix can give in the devel-opment of new grammars.4 Future ExtensionsThe initial version of the matrix, while sufficient tosupport some useful grammar work, will requiresubstantial further development on several fronts,including lexical representation, syntactic gener-alization, sociolinguistic variation, processing is-sues, and evaluation.
This first version drew mostheavily from the implementation of the Englishgrammar, with some further insights drawn fromthe grammar of Japanese.
Extensions to the ma-trix will be based on careful study of existing im-plemented grammars for other languages, notablyGerman, Spanish and Japanese, as well as feed-back from those using the first version of the ma-trix.For lexical representation, one of the most ur-gent needs is to provide a language-independenttype hierarchy for the lexicon, at least for majorparts of speech, establishing the mechanisms usedfor linking syntactic subcategorization to seman-tic predicate-argument structure.
Lexical rules pro-vide a second mechanism for expressing general-3Note that there are no further subtypes of LOCAL valuesbeyond isect-mod and scopal-mod.
Since these grammars donot make extensive use of subtypes of LOCAL values, theywere available for encoding this distinction.
Alternative solu-tions include positing a new feature.izations within the lexicon, and offer ready oppor-tunities for cross-linguistic abstractions for bothinflectional and derivational regularities.
Work isalso progressing on establishing a standard rela-tional database (using PostgreSQL) for storing in-formation for the lexical entries themselves, im-proving both scalability and clarity compared tothe current simple text file representation.
Form-based tools will be provided both for constructinglexical entries and for viewing the contents of thelexicon.The primary focus of work on syntactic general-ization in the matrix is to support more freedomin word order, for both complements and modi-fiers.
The first step will be a relatively conserva-tive extension along the lines of Netter (1996), al-lowing the grammar writer more control over howa head combines with complements of differenttypes, and their interleaving with modifier phrases.Other areas of immediate cross-linguistic interestinclude the hierarchy of head types, control phe-nomena, clitics, auxiliary verbs, noun-noun com-pounds, and more generally, phenomena that in-volve the word/phrase distinction, such as noun in-corporation.
A study of the existing grammars forEnglish, German, Japanese, and Spanish revealsa high degree of language-specificity for severalof these phenomena, but also suggests promise ofreusable abstractions.Several kinds of sociolinguistic variation requireextensions to the matrix, including grammaticizedaspects of pragmatics such as politeness and em-pathy, as well as dialect and register alternations.The grammar of Japanese provides a starting pointfor representations of both empathy and politeness.Implementations of familiar vs. formal verb formsin German and Spanish provide further instancesof politeness to help build the cross-linguistic ab-stractions.
Extensions for dialect variation willbuild on some exploratory work in adapting theEnglish grammar to support American, British,and Australian regionalisms, both lexical and syn-tactic, while restricting dialect mixture in genera-tion and associated spurious ambiguity in parsing.While the development of the matrix will bebuilt largely on the LKB platform, support will alsobe needed for using the emerging grammars onother processing platforms, and for linking to otherpackages for pre-processing the linguistic input.Several other platforms exist which can efficientlyparse text using the existing grammars, includ-ing the PET system developed in C++ at SaarlandUniversity (Germany) and the DFKI (Callmeier,2000); the PAGE system developed in Lisp at theDFKI (Uszkoreit et al, 1994); the LiLFeS systemdeveloped at Tokyo University (Makino, Yoshida,Torisawa, & Tsujii, 1998), and a parallel process-ing system developed in Objective C at Delft Uni-versity (The Netherlands; van Lohuizen, 2002).As part of the matrix package, sample configura-tion files and documentation will be provided forat least some of these additional platforms.Existing pre-processing packages can also sig-nificantly reduce the effort required to developa new grammar, particularly for coping with themorphology/syntax interface.
For example, theChaSen package for segmenting Japanese inputinto words and morphemes (Asahara & Mat-sumoto, 2000) has been linked to at least the LKBand PET systems.
Support for connecting im-plementations of language-specific pre-processingpackages of this kind will be preserved and ex-tended as the matrix develops.
Likewise, config-uration files are included to support generation, atleast within the LKB, provided that the grammarconforms to certain assumptions about semanticrepresentation using the Minimal Recursion Se-mantics framework.Finally, a methodology is under development forconstructing and using test suites organized arounda typology of linguistic phenomena, using the im-plementation platform of the [incr tsdb()] profil-ing package (Oepen & Flickinger, 1998; Oepen& Callmeier, 2000).
These test suites will enablebetter communication about current coverage of agiven grammar built using the matrix, and serve asthe basis for identifying additional phenomena thatneed to be addressed cross-linguistically within thematrix.
Of course, the development of the typol-ogy of phenomena is itself a major undertakingfor which a systematic cross-linguistic approachwill be needed, a discussion of which is outsidethe scope of this report.
But the intent is to seedthis classification scheme with a set of relativelycoarse-grained phenomenon classes drawn fromthe existing grammars, then refine the typology asit is applied to these and new grammars built usingthe matrix.5 Case StudiesOne important part of the matrix package will be alibrary of phenomenon-based analyses drawn fromthe existing grammars and over time from users ofthe matrix, to provide working examples of howthe matrix can be applied and extended.
Each casestudy will be a set of grammar files, simplified forrelevance, along with documentation of the anal-ysis, and a test suite of sample sentences whichdefine the range of data covered by the analysis.This library, too, will be organized around the ty-pology of phenomena introduced above, but willalso make explicit reference to language families,since both similarities and differences among re-lated languages will be of interest in these casestudies.
Examples to be included in the first re-lease of this library include numeral classifiers inJapanese, subject pro drop in Spanish, partial-VPfronting in German, and verb diathesis in Norwe-gian.6 Evaluation and EvolutionThe matrix itself is not a grammar but a collec-tion of generalizations across grammars.
As such,it cannot be tested directly on corpora from partic-ular languages, and we must find other means ofevaluation.
We envision overall evaluation of thematrix based on case studies of its performancein helping grammar engineers quickly start newgrammars and in helping them scale those gram-mars up.
Evaluation in detail will based on au-tomatable deletion/substitution metrics, i.e., toolsthat determine which types from the matrix getused as is, which get used with modifications, andwhich get ignored in various matrix-derived gram-mars.
Furthermore, if the matrix evolves to includedefeasible constraints, these tools will check whichconstraints get overridden and whether the valuechosen is indeed common enough to be motivatedas a default value.
This evaluation in detail shouldbe paired with feedback from the grammar engi-neers to determine why changes were made.The main goal of evaluation is, of course, to im-prove the matrix over time.
This raises the ques-tion of how to propagate changes in the matrix togrammars based on earlier versions.
The followingthree strategies (meant to be used in combination)seem promising: (i) segregate changes that are im-portant to sync to (e.g., changes that affect MRSoutputs, fundamental changes to important anal-yses), (ii) develop a methodology for communi-cating changes in the matrix, their motivation andtheir implementation to the user community, and(iii) develop tools for semi-automating resynchingof existing grammars to upgrades of the matrix.These tools could use the type hierarchy to predictwhere conflicts are likely to arise and bring theseto the engineer?s attention, possibly inspired by theapproach under development at CSLI for the dy-namic maintenance of the LinGO Redwoods tree-bank (Oepen et al, 2002).Finally, while initial development of the ma-trix has been and will continue to be highly cen-tralized, we hope to provide support for proposedmatrix improvements from the user community.User feedback will already come in the form ofcase studies for the library as discussed in Sec-tion 5 above, but also potentially in proposals formodification of the matrix drawing on experiencesin grammar development.
In order to provideusers with some cross-linguistic context in whichto develop and evaluate such proposals themselves,we intend to provide some sample matrix-derivedgrammars and corresponding testsuites with thematrix.
A user could thus make a proposed changeto the matrix, run the testsuites for several lan-guages using the supplied grammars which drawfrom that changed matrix, and use [incr tsdb()]to determine which phenomena have been affectedby the change.
It is clear that full automation ofthis evaluation process will be difficult, but at leastsome classes of changes to the matrix will per-mit this kind of quick cross-linguistic feedback tousers with only a modest amount of additional in-frastructure.7 ConclusionThis project carries linguistic, computational, andpractical interest.
The linguistic interest lies in theHPSG community?s general bottom-up approachto language universals, which involves aiming forgood coverage of a variety of languages first, andleaving the task of what they have in common forlater.
(Of course, theory building is never purelydata-driven, and there are substantive hypotheseswithin HPSG about language universals.)
Nowthat we have implementations with fairly extensivecoverage for a somewhat typologically diverse setof languages, it is a good time to take the next stepin this program, working to extract and generalizewhat is similar across these existing wide-coveragegrammars.
Moreover, the central role of types inthe representation of linguistic generalizations en-ables the kind of underspecification which is usefulfor expressing what is common among related lan-guages while allowing for the further specializa-tion which necessarily distinguishes one languagefrom another.The computational interest is threefold.
Firstthere is the question of what formal devices thegrammar matrix will require.
Should it includedefaults?
What about domain union (linearizationtheory)?
The selection and deployment of formaldevices should be informed by on-going researchon processing schemes, and here the crosslinguis-tic perspective can be particularly helpful.
Wherethere are several equivalent analyses of the samelinguistic phenomena (e.g., morphosyntactic am-biguity or optionality), the choice of analysis canhave processing implications that aren?t necessar-ily apparent in a single grammar.
Second, havinga set of wide-coverage HPSGs with fairly standard-ized fundamentals could prove interesting for re-search on stochastic processing and disambigua-tion, especially if the languages differ in gross ty-pological features such as word order.
Finally,there are also computational issues involved inhow the grammar matrix would evolve over timeas it is used in new grammars.
The matrix en-ables the developer of a grammar for a new lan-guage to get a quick start on producing a systemthat parses and generates with non-trivial seman-tics, while also building the foundation for a wide-coverage grammar of the language.
But the matrixitself may well change in parallel with the devel-opment of the grammar for a particular language,so appropriate mechanisms must be developed tosupport the merging of enhancements to both.There is also practical industrial benefit to thisproject.
Companies that are consumers of thesegrammars benefit when grammars of multiple lan-guages work with the same parsing and generationalgorithms and produce standardized semantic rep-resentations derived from a rich, linguistically mo-tivated syntax-semantics interface.
More impor-tantly, the grammar matrix will help to remove oneof the primary remaining obstacles to commercialdeployment of grammars of this type and indeed ofthe commercial use of deep linguistic analysis: theimmense cost of developing the resource.AcknowledgementsSince the grammar matrix draws on prior re-search and existing grammars, it necessarily re-flects contributions from many people.
RobMalouf, Jeff Smith, John Beavers, and KathrynCampbell-Kibler have contributed to the LinGOERG; Melanie Siegel is the original developer forthe Japanese grammar.
Tim Baldwin, Ann Copes-take, Ivan Sag, Tom Wasow, and other membersof the LinGO Laboratory at CSLI have had a greatdeal of influence on the design of the grammaticalanalyses and corresponding MRS representations.Warmest thanks to Lars Hellan and his colleaguesat NTNU and Jan Tore L?nning and his studentsat Oslo University for their cooperation, patience,and tolerance.ReferencesAlshawi, H.
(Ed.).
(1992).
The Core Language Engine.Cambridge, MA: MIT Press.Asahara, M., & Matsumoto, Y.
(2000).
Extended mod-els and tools for high-performance part-of-speechtagger.
In Proceedings of the 18th InternationalConference on Computational Linguistics (pp.
21 ?27).
Saarbru?cken, Germany.Butt, M., King, T. H., Nin?o, M.-E., & Segond, F.(1999).
A grammar writer?s cookbook.
Stanford,CA: CSLI Publications.Callmeier, U.
(2000).
PET ?
A platform for ex-perimentation with efficient HPSG processing tech-niques.
Natural Language Engineering, 6 (1) (Spe-cial Issue on Efficient Processing with HPSG), 99 ?108.Copestake, A.
(2002).
Implementing typed featurestructure grammars.
Stanford, CA: CSLI Publica-tions.Copestake, A., Flickinger, D., Malouf, R., Riehemann,S., & Sag, I.
(1995).
Translation using minimal re-cursion semantics.
In Proceedings of the Sixth In-ternational Conference on Theoretical and Method-ological Issues in Machine Translation.
Leuven,Belgium.Copestake, A., Flickinger, D. P., Sag, I.
A., & Pol-lard, C. (1999).
Minimal Recursion Semantics.
Anintroduction.
in preparation, CSLI Stanford, Stan-ford, CA.Copestake, A., Lascarides, A., & Flickinger, D. (2001).An algebra for semantic construction in constraint-based grammars.
In Proceedings of the 39th Meet-ing of the Association for Computational Linguistics.Toulouse, France.Flickinger, D. (2000).
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6 (1) (Special Issue on Efficient Process-ing with HPSG), 15 ?
28.van Lohuizen, M. (2002).
Efficient and thread-safeunification with LinGO.
In S. Oepen, D. Flickinger,J.
Tsujii, & H. Uszkoreit (Eds.
), Collaborativelanguage engineering.
A case study in efficientgrammar-based processing.
Stanford, CA: CSLIPublications.
(forthcoming)Makino, T., Yoshida, M., Torisawa, K., & Tsujii, J.(1998).
LiLFeS ?
towards a practical HPSG parser.In Proceedings of the 17th International Conferenceon Computational Linguistics and the 36th AnnualMeeting of the Association for Computational Lin-guistics (pp.
807 ?
11).
Montreal, Canada.Mu?ller, S. (1999).
Deutsche syntax deklarativ.
Head-Driven Phrase Structure Grammar fu?r das Deutsche.Tu?bingen, Germany: Max Niemeyer Verlag.Mu?ller, S., & Kasper, W. (2000).
HPSG analysis ofGerman.
In W. Wahlster (Ed.
), Verbmobil.
Foun-dations of speech-to-speech translation (ArtificialIntelligence ed., pp.
238 ?
253).
Berlin, Germany:Springer.Netter, K. (1996).
Functional categories in an HPSGfor German.
Unpublished doctoral dissertation,Saarland University, Saarbru?cken, Germany.Oepen, S., & Callmeier, U.
(2000).
Measure for mea-sure: Parser cross-fertilization.
Towards increasedcomponent comparability and exchange.
In Pro-ceedings of the 6th International Workshop on Pars-ing Technologies (pp.
183 ?
194).
Trento, Italy.Oepen, S., & Flickinger, D. P. (1998).
Towards sys-tematic grammar profiling.
Test suite technology tenyears after.
Journal of Computer Speech and Lan-guage, 12 (4) (Special Issue on Evaluation), 411 ?436.Oepen, S., Toutanova, K., Shieber, S., Manning, C.,Flickinger, D., & Brants, T. (2002).
The LinGORedwoods treebank.
Motivation and preliminary ap-plications.
In Proceedings of the 19th InternationalConference on Computational Linguistics.
Taipei,Taiwan.Siegel, M. (2000).
HPSG analysis of Japanese.In W. Wahlster (Ed.
), Verbmobil.
Foundations ofspeech-to-speech translation (Artificial Intelligenceed., pp.
265 ?
280).
Berlin, Germany: Springer.Siegel, M., & Bender, E. M. (2002).
Efficient deepprocessing of japanese.
In Proceedings of the 19thInternational Conference on Computational Linguis-tics.
Taipei, Taiwan.Uszkoreit, H., Backofen, R., Busemann, S., Diagne,A.
K., Hinkelman, E. A., Kasper, W., Kiefer, B.,Krieger, H.-U., Netter, K., Neumann, G., Oepen, S.,& Spackman, S. P. (1994).
DISCO ?
an HPSG-based NLP system and its application for appoint-ment scheduling.
In Proceedings of the 15th Inter-national Conference on Computational Linguistics.Kyoto, Japan.
