Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?34,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsClustered Word Classes for Preordering in Statistical MachineTranslationSara StymneLinko?ping University, Swedensara.stymne@liu.seAbstractClustered word classes have been used inconnection with statistical machine transla-tion, for instance for improving word align-ments.
In this work we investigate if clus-tered word classes can be used in a pre-ordering strategy, where the source lan-guage is reordered prior to training andtranslation.
Part-of-speech tagging has pre-viously been successfully used for learn-ing reordering rules that can be appliedbefore training and translation.
We showthat we can use word clusters for learn-ing rules, and significantly improve on abaseline with only slightly worse perfor-mance than for standard POS-tags on anEnglish?German translation task.
We alsoshow the usefulness of the approach forthe less-resourced language Haitian Creole,for translation into English, where the sug-gested approach is significantly better thanthe baseline.1 IntroductionWord order differences between languages areproblematic for statistical machine translation(SMT).
If the word orders of two languages havelarge differences, the standard methods do nottend to work well, with difficulties in many stepssuch as word alignment and modelling of reorder-ing in the decoder.
This can be addressed by ap-plying a preordering method, that is, to reorder thesource side of the corpus to become similar to thetarget side, prior to training and translation.
Therules used for reordering are generally based onsome kind of linguistic annotation, such as part-of-speech tags (POS-tags).For many languages in the world, so called less-resourced languages, however, part-of-speechtaggers, or part-of-speech tagged corpora that canbe used for training a tagger, are not available.
Inthis study we investigate if it is possible to useunsupervised POS-tags, in the form of clusteredword classes, as a basis for learning reorderingrules for SMT.
Unsupervised tagging methods canbe used for any language where a corpus is avail-able.
This means that we can potentially benefitfrom preordering even for languages where tag-gers are available.We present experiments on two data sets.
Firstan English?German test set, where we can com-pare the results of clustered word classes withstandard tags.
We show that both types of tagsbeat a baseline without preordering, and that clus-tered tags perform nearly as well as standard tags.English and German is an interesting case for re-ordering experiments, since there are both longdistance movement of verbs and local word or-der differences, for instance due to differences inadverb placements.
We also apply the methodto translation from the less-resourced languageHaitian Creole into English, and show that it leadsto an improvement over a baseline.
The differ-ences in word order between these two languagesare smaller than for English?German.Besides potentially improving SMT for less-resourced languages, the presented approach canalso be used as an extrinsic evaluation method forunsupervised POS-tagging methods.
This is espe-cially useful for the task of word class clusteringwhich is hard to evaluate.2 Unsupervised POS-taggingThere have been several suggestions of clusteringmethods for obtaining word classes that are com-pletely unsupervised, and induce classes from raw28text.
Brown et al (1992) described a hierarchicalword clustering method which maximizes the mu-tual information of bigrams.
Schu?tze (1995) de-scribed a distributional clustering algorithm thatuses global context vectors as a basis for clus-tering.
Biemann (2006) described a graph-basedclustering methods for word classes.
Goldwa-ter and Griffiths (2007) used Bayesian reasoningfor word class induction.
Och (1999) describeda method for determining bilingual word classes,used to improve the extraction of alignment tem-plates through alignments between classes, notonly between words.
He also described a mono-lingual word clustering method, which is basedon a maximum likelihood approach, using the fre-quencies of unigrams and bigrams in the trainingcorpus.The above methods are fully unsupervised, andproduce unlabelled classes.
There has also beenwork on what Goldwater and Griffiths (2007)call POS disambiguation, where the learning ofclasses is constrained by a dictionary of the al-lowable tags for each word.
Such work has forinstance been based on hidden Markov models(Merialdo, 1994), log-linear models (Smith andEisner, 2005), and Bayesian reasoning (Goldwa-ter and Griffiths, 2007).Word clusters have previously been used forSMT for improving word alignment (Och, 1999),in a class-based language model (Costa-jussa` etal., 2007) or for extracting gappy patterns (Gim-pel and Smith, 2011).
To the best of our knowl-edge this is the first study of applying clusteredword classes for creating pre-translation reorder-ing rules.
The most similar work we are awareof is Costa-jussa` and Fonollosa (2006) who usedclustered word classes in a strategy they call sta-tistical machine reordering, where the corpus istranslated into a reordered language using stan-dard SMT techniques in a pre-processing step.The addition of word classes led to improvementsover just using surface form, but no comparisonto using POS-tags were shown.
Clustered wordclasses have also been used in a discriminate re-ordering model (Zens and Ney, 2006), and wereshown to reduce the classification error rate.Word clusters have also been used for unsu-pervised and semi-supervised parsing.
Klein andManning (2004) used POS-tags as the basis of afully unsupervised parsing method, both for de-pendency and constituency parsing.
They showedthat clustered word classes can be used instead ofconventional POS-tags, with some result degra-dation, but that it is better than several baselinesystems.
Koo et al (2008) used features based onclustered word classes for semi-supervised depen-dency parsing and showed that using word classfeatures together with POS-based features led toimprovements, but using word class features in-stead of POS-based features only degraded resultssomewhat.3 Reordering for SMTThere is a large amount of work on reorderingfor statistical machine translation.
One way toapproach reordering is by extending the transla-tion model, either by adding extra models, suchas lexicalized (Koehn et al, 2005) or discrimina-tive (Zens and Ney, 2006) reordering models orby directly modelling reordering in hierarchical(Chiang, 2007) or syntactical translation models(Yamada and Knight, 2002).Preordering is another common strategy forhandling reordering.
Here the source side of thecorpus is transformed in a preprocessing step tobecome more similar to the target side.
Therehave been many suggestions of preordering strate-gies.
Transformation rules can be handwrit-ten rules targeting known syntactic differences(Collins et al, 2005; Popovic?
and Ney, 2006),or they can be learnt automatically (Xia and Mc-Cord, 2004; Habash, 2007).
In these studies thereordering decision was taken deterministicallyon the source side.
This decision can be delayedto decoding time by presenting several reorderingoptions to the decoder as a lattice (Zhang et al,2007; Niehues and Kolss, 2009) or as an n-bestlist (Li et al, 2007).Generally reordering rules are applied to thesource language, but there have been attempts attarget side reordering as well (Na et al, 2009).Reordering rules can be based on different lev-els of linguistic annotation, such as POS-tags(Niehues and Kolss, 2009), chunks (Zhang et al,2007) or parse trees (Xia and McCord, 2004).Common for all these levels is that a tool like atagger or parser is needed for them to work.In all the above studies, the reordering rules areapplied to the translation input, but they are onlyapplied to the training data in a few cases, for in-stance in Popovic?
and Ney (2006).
Rottmann andVogel (2007) compared two strategies for reorder-29ing the training corpus, by using alignments, andby applying the reordering rules to create a lat-tice from which they extracted the 1-best reorder-ing.
They found that it was better to use the latteroption, to reorder the training data based on therules, than to use the original order in the train-ing data.
Using alignment-based reordering wasnot successful, however.
Another option for us-ing reorderings in the training data was presentedby Niehues et al (2009), who directly extractedphrase pairs from reordering lattices, and showeda small gain over non-reordered training data.3.1 POS-based PreorderingOur work is based on the POS-based reorder-ing model described by Niehues and Kolss(2009), in which POS-based rules are extractedfrom a word aligned corpus, where the sourceside is part-of-speech tagged.
There are twotypes of rules.
Short-range rules (Rottmannand Vogel, 2007) contain a pattern of POS-tags,and a possible reordering to resemble the tar-get language, such as VVIMP VMFIN PPER ?PPER VMFIN VVIMP, which moves a personalpronoun to a position in front of a verb group.Long-range rules were designed to cover move-ments over large spans, and also contain gapsthat can match one or several words, such asVAFIN * VVPP ?
VAFIN VVPP *, whichmoves the two parts of a German verbs togetherpast an object of any size, so as to resemble En-glish.Short-range rules are extracted by identifyingPOS-sequences in the training corpus where thereare crossing alignments.
The rules are stored asthe part-of-speech pattern of the source on the lefthand side of the rule, and the pattern correspond-ing to the target side word order on the right handside.Long-range rules are extracted in a similar way,by identifying two neighboring POS-sequenceson the source side that have crossed alignments.Gaps are introduced into the rules by replacingeither the right hand side or the left hand sideby a wild card.
In order to constrain the appli-cation of these rules, the POS-tag to the left of therule is included in the rule.
Depending on the lan-guage pair it might be advantageous to use rulesthat have wildcards either on the left or right handside.
For German-to-English translation, the mainlong distance movement is that verbs move to theleft, and, as shown by Niehues and Kolss (2009),it is advantageous to use only long-range ruleswith left-wildcards, as in the example rule above.For the other translation direction, it is importantto move verbs to the right, and thus right-wildcardrules were better.The probability of both short and long rangerules is calculated by relative frequencies as thenumber of times a rule occurs divided by the num-ber of times the source side occurs in the trainingdata.In a preprocessing step to decoding, all rulesare applied to each input sentence, and when arule applies, the alternative word order is addedto a word lattice.
To keep lattices of a reason-able size, Niehues and Kolss (2009) suggested us-ing a threshold of 0.2 for the probability of short-range rules, of 0.05 for the probability of longrange rules, and blocked rules that could be ap-plied more than 5 times to the same sentence.
Weadopt these threshold values.In this work we use the short-range reorder-ing rules of Rottmann and Vogel (2007) and thelong-range rules of Niehues and Kolss (2009).
Assuggested we use only right-wildcard rules forEnglish?German translation.
For Haitian Creole,we have no prior knowledge of the reordering di-rection, and thus choose to use both left and rightlong-range rules.
In previous work only one stan-dard POS-tagset was explored.
In this work we in-vestigate the effect of different type of annotationschemes, besides only POS-tags.
We use severaltypes of tags from a parser, and compare them tousing unsupervised tags in the form of clusteredword classes.
We also apply the reordering tech-niques to translation from Haitian Creole, a less-resourced language for which no POS-tagger isavailable.4 Experimental SetupWe conducted experiments for two languagepairs, English?German and Haitian Creole?English.
We always applied the reordering rulesto the translation input, creating a lattice of pos-sible reorderings as input to the decoder.
For thetraining data we applied two strategies.
As thefirst option we used training data from the base-line system with original word order.
As the sec-ond option we reordered the training data as well,using the learnt reordering rules to create reorder-ing lattices for the training data, from which we30ID Form Lemma Dependency Functional tag Syntax POS Morphology1 Resumption resumption main:>0 @NH %NH N NOM SG2 of of mod:>1 @<NOM-OF %N< PREP3 the the attr:>4 @A> %>N DET4 session session pcomp:>2 @<P %NH N NOM SGTable 1: Parser outputextracted the 1-best reordering, as suggested byRottmann and Vogel (2007).For the supervised tagging of the Englishsource side we use a commercial functional de-pendency parser.1 The main reason for using aparser instead of a tagger was that we wanted toexplore the effect of different tagging schemes,which was available from this parser.
An exampleof a tagged English text can be seen in Table 1.In this work we used four types of tags extractedfrom the parser output, part-of-speech tags (pos),dependency tags (dep), functional tags (func) andshallow syntax tags (syntax).
The dependencytags consist of the dependency label of the wordand the POS-tag of its dependent.
For the exam-ple in Table 1, the sequence of dependency tagsis: main TOP mod N attr N pcomp PREP.The other tag types are directly exemplified in Ta-ble 1.
The tagsets have different sizes, as shownin Table 2.For the unsupervised tags, we used clusteredword classes obtained using the mkcls software,2which implements the approach of Och (1999).We explored three different numbers of clusters,50, 125, and 625.
The clustering was performedon the same corpus as the SMT training.The translation system used is a standardphrase-based SMT system.
The translation modelwas trained by first creating unidirectional wordalignments in both directions using GIZA++ (Ochand Ney, 2003), which are then symmetrizedby the grow-diag-final-and method (Koehn et al,2005).
From this many-to-many alignment, con-sistent phrases of up to length 7 were extracted.A 5-gram language model was used, producedby SRILM (Stolcke, 2002).
For training and de-coding we used the Moses toolkit (Koehn et al,2007) and the feature weights were optimizedusing minimum error rate training (Och, 2003).1http://www.connexor.eu/technology/machinese/machinesesyntax/2http://www-i6.informatik.rwth-aachen.de/web/Software/mkcls.htmlTagset Classes Rules Pathspos 23 319147 2.1e09dep 523 328415 2.8e09func 49 325091 1.5e10syntax 20 315407 4.5e11class50 50 303292 6.2e09class125 125 271348 1.3e07class625 625 211606 31654Table 2: Number of tags for each tagset in the Englishtraining corpus, number of rules extracted for eachtagset, and average numbers of paths per sentence inthe testset lattice using each tagset to create rulesThe baseline systems were trained using no ad-ditional preordering, only a distance-based re-ordering penalty for modelling reordering.
Forthe Haitian Creole?English experiments we alsoadded a lexicalized reordering model (Koehn etal., 2005), both to the baseline and to the re-ordered systems.For the English?German experiments, thetranslation system was trained and tested using apart of the Europarl corpus (Koehn, 2005).
Thetraining part contained 439513 sentences and 9.4million words.
Sentences longer than 40 wordswere filtered out.
The test set has 2000 sentencesand the development set has 500 sentences.For the Haitian Creole?English experimentswe used the SMS corpus released for WMT11(Callison-Burch et al, 2011).
The corpus con-tains 17192 sentences and 352326 words.
Thetest and development data both contain 900 sen-tences each.
Since we know of no POS-tagger forHaitian Creole, we only compare the clustered re-sult to a baseline system.Reordering rules were extracted from the samecorpora that were used for training the SMT sys-tem.
The word alignments needed for reorderingwere created using GIZA++ (Och and Ney, 2003),an implementation of the IBM models (Brown etal., 1993) of alignment, which is trained in a fullyunsupervised manner based on the EM algorithm(Dempster et al, 1977).315 ResultsTable 2 shows the number of rules, and the av-erage number of paths for each sentence in thetest data lattice, using each tagset.
For the stan-dard tagsets the number of rules is relatively con-stant, despite the fact that the number of tags inthe tagsets are quite different.
For the clusteredword classes, there are slightly fewer rules with50 classes than for the standard tags, and the num-ber of rules decreases with a higher number ofclasses.
For the average number of lattice pathsper sentence, there are some differences for thestandard tags, but it is not related to tagset size.Again, the clustering with 50 classes has a simi-lar number as the standard classes, but here thereis a sharp decrease of lattice paths with a highernumber of classes.The translation results for the English?Germanexperiments are shown in Table 3.
We reporttranslation results for two metrics, Bleu (Papineniet al, 2002) and NIST (Doddington, 2002), andsignificance testing is performed using approxi-mate randomization (Riezler andMaxwell, 2005),with 10,000 iterations.
All the systems with re-ordering have higher scores than the baseline onboth metrics.
This difference is always significantfor NIST, and significant for Bleu in all cases ex-cept for two systems, one with standard tags andone with clustered tags.
Between most of the sys-tems with reordering the differences are small andmost of them are not significant.
Overall the sys-tems with standard word classes perform slightlybetter than the clustered systems, especially thefunc tagset gives consistently high results, and issignificantly better than four of the clustered sys-tems on Bleu, and than one system on NIST.
Thefact that the number of paths were much smallerfor a high number of clustered classes than for theother tagsets does not seem to have influenced thetranslation results.Clustering of word classes is nondeterministic,and several runs of the cluster methods give dif-ferent results, which could influence the transla-tion results as well.
To investigate this, we reranthe experiment with 50 classes and baseline train-ing data three times.
The differences of the re-sults between these runs were small, Bleu variedbetween 20.08?20.19 and NIST varied between5.99?6.01.
This variation is smaller than the dif-ference between the baseline and the reorderingBaseline training Reordered trainingTagset Bleu NIST Bleu NISTBaseline 19.84 5.92 ?
?pos 20.34** 6.05** 20.26** 5.98*dep 20.11 6.03** 20.25** 6.06**func 20.40** 6.05** 20.40** 6.06**syntax 20.29** 6.07** 20.32** 6.06**class50 20.15* 6.05** 20.15* 5.99**class125 20.15* 6.03** 20.17* 6.02**class625 20.19** 6.05** 20.07 6.05**Table 3: Translation results for English?German.
Sta-tistically significant differences from baseline scoresare marked * (p < 0.05), ** (p < 0.01).Tagset Classes Rules Pathsclass50 50 4588 3.70class125 125 3554 1.46class625 625 2388 1.42Table 4: Number of classes for Haitian Creole, numberof rules extracted for each tagset, and average numbersof paths per sentence in the testset lattice using eachtagset to create rulessystems, and should not influence the overall con-clusions.For the Haitian Creole testset both the averagenumber of reorderings per sentence, and the num-ber of rules, are substantially lower than for theEnglish testset.
As shown in Table 4, the trendsare the same, however.
With a higher number ofclasses there are both fewer rules and fewer ruleapplications.
That there are few rules and pathscan both depend on the fact that there are fewerword order differences between these languages,that the corpus is smaller, and that the sentencelength is shorter.Even though the number of reorderings is rel-atively small, there are consistent significant im-provements for all reordered options on both Bleuand NIST compared to the baseline, as shown inTable 5.
Between the clustered systems the dif-ferences are relatively small, and the only sig-nificant differences are that the system with 50classes and reordered training data is worse onBleu than 50 classes with baseline reordering and125 classes with reordered training data, at the0.05-level.
The trend for the systems with 125 and625 classes is in the other direction with slightlyhigher results with reordered data.
There is hardlyany difference between these two systems, whichis not surprising, seeing that the number of ap-32Baseline training Reordered trainingTagset Bleu NIST Bleu NISTBaseline 29.04 5.58 ?
?class50 29.59** 5.73** 29.60** 5.69**class125 29.52** 5.70** 29.78** 5.73**class625 29.55** 5.70** 29.75** 5.74**Table 5: Translation results for Haitian Creole?English.
Statistically significant differences frombaseline BLEU score are marked ** (p < 0.01).plied rules is very similar.6 Conclusion and Future WorkWe have presented experiments of using clusteredword classes as input to a preordering method forSMT.
We showed that the proposed method per-form better than a baseline and nearly on par withusing standard tags for an English?German trans-lation task.
We also showed that it can improveresults over a baseline when translating from theless-resourced language Haitian Creole into En-glish, even though the word order differences be-tween these languages are relatively small.The suggested preordering algorithm withword classes is fully unsupervised, since unsuper-vised methods are used both for word classes andword alignments that are the basis of the preorder-ing algorithm.
This means that the method canbe applied to less-resourced languages where notaggers or parsers are available, which is not thecase for the many preordering methods which arebased on POS-tags or parse trees.This initial study is quite small, and in the fu-ture we plan to extend it to larger corpora andother language pairs.
We would also like to com-pare the performance of different unsupervisedword clustering and POS-tagging methods on thistask.AcknowledgmentsI would like to thank Jan Niehues for sharing hiscode, and for his help on the POS-based reorder-ing, and Joakim Nivre and the anonymous review-ers for their insightful comments.ReferencesChris Biemann.
2006.
Unsupervised part-of-speechtagging employing efficient graph clustering.
InProceedings of the COLING/ACL 2006 Student Re-search Workshop, pages 7?12, Sydney, Australia.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational linguistics, 18(4):467?479.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011workshop on statistical machine translation.
In Pro-ceedings of WMT, pages 22?64, Edinburgh, Scot-land.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):202?228.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Ann Arbor, Michigan, USA.Marta R. Costa-jussa` and Jose?
A. R. Fonollosa.
2006.Statistical machine reordering.
In Proceedings ofEMNLP, pages 70?76, Sydney, Australia.Marta R. Costa-jussa`, Josep M. Crego, Patrik Lam-bert, Maxim Khalilov, Jose?
A. R. Fonollosa, Jose?
B.Marin?o, and Rafael E. Banchs.
2007.
Ngram-basedstatistical machine translation enhanced with mul-tiple weighted reordering hypotheses.
In Proceed-ings of WMT, pages 167?170, Prague, Czech Re-public.Arthur E. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of the RoyalStatistical Society, 39(1):1?38.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology, pages 228?231, San Diego, California,USA.Kevin Gimpel and Noah A. Smith.
2011.
Genera-tive models of monolingual and bilingual gappy pat-terns.
In Proceedings of WMT, pages 512?522, Ed-inburgh, Scotland.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proceedings of ACL, pages 744?751,Prague, Czech Republic.Nizar Habash.
2007.
Syntactic preprocessing for sta-tistical machine translation.
In Proceedings of MTSummit XI, pages 215?222, Copenhagen, Denmark.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models ofdependency and constituency.
In Proceedings ofACL, pages 478?485, Barcelona, Spain.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, and33David Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.In Proceedings of the International Workshop onSpoken Language Translation, Pittsburgh, Pennsyl-vania, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of ACL, demonstrationsession, pages 177?180, Prague, Czech Republic.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofMT Summit X, pages 79?86, Phuket, Thailand.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL, pages 595?603, Columbus,Ohio.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A probabilisticapproach to syntax-based reordering for statisticalmachine translation.
In Proceedings of the 45th An-nual Meeting of the ACL, pages 720?727, Prague,Czech Republic.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?172.Hwidong Na, Jin-Ji Li, Jungi Kim, and Jong-HyeokLee.
2009.
Improving fluency by reordering tar-get constituents using MST parser in English-to-Japanese phrase-based SMT.
In Proceedings ofMT Summit XII, pages 276?283, Ottawa, Ontario,Canada.Jan Niehues and Muntsin Kolss.
2009.
A POS-basedmodel for long-range reorderings in SMT.
In Pro-ceedings of WMT, pages 206?214, Athens, Greece.Jan Niehues, Teresa Herrmann, Muntsin Kolss, andAlex Waibel.
2009.
The Universita?t Karlsruhetranslation system for the EACL-WMT 2009.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 80?84, Athens, Greece.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proceedings ofEACL, pages 71?76, Bergen, Norway.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: A method for auto-matic evaluation of machine translation.
In Pro-ceedings of ACL, pages 311?318, Philadelphia,Pennsylvania, USA.Maja Popovic?
and Hermann Ney.
2006.
POS-basedreorderings for statistical machine translation.
InProceedings of LREC, pages 1278?1283, Genoa,Italy.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significancetesting for MT.
In Proceedings of the Workshopon Intrinsic and Extrinsic Evaluation Measures forMT and/or Summarization at ACL?05, pages 57?64,Ann Arbor, Michigan, USA.Kay Rottmann and Stephan Vogel.
2007.
Word re-ordering in statistical machine translation with aPOS-based distortion model.
In Proceedings ofthe 11th International Conference on Theoreticaland Methodological Issues in Machine Translation,pages 171?180, Sko?vde, Sweden.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of EACL, pages 141?148,Dublin, Ireland.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of ACL, pages 354?362, AnnArbor, Michigan, USA.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,pages 901?904, Denver, Colorado, USA.Fei Xia and Michael McCord.
2004.
Improving astatistical MT system with automatically learnedrewrite patterns.
In Proceedings of CoLing, pages508?514, Geneva, Switzerland.Kenji Yamada and Kevin Knight.
2002.
A decoderfor syntax-based statistical MT.
In Proceedings ofACL, pages 303?310, Philadelphia, Pennsylvania,USA.Richard Zens and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machine trans-lation.
In Proceedings of WMT, pages 55?63, NewYork City, USA.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Improved chunk-level reordering for statistical ma-chine translation.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation,pages 21?28, Trento, Italy.34
