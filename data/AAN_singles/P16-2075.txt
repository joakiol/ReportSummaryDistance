Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 460?466,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMachine Translation Evaluation Meets Community Question AnsweringFrancisco Guzm?an, Llu?
?s M`arquez and Preslav NakovArabic Language Technologies Research GroupQatar Computing Research Institute, HBKU{fguzman,lmarquez,pnakov}@qf.org.qaAbstractWe explore the applicability of machinetranslation evaluation (MTE) methods to avery different problem: answer ranking incommunity Question Answering.
In par-ticular, we adopt a pairwise neural net-work (NN) architecture, which incorpo-rates MTE features, as well as rich syntac-tic and semantic embeddings, and whichefficiently models complex non-linear in-teractions.
The evaluation results showstate-of-the-art performance, with sizeablecontribution from both the MTE featuresand from the pairwise NN architecture.1 Introduction and MotivationIn a community Question Answering (cQA) task,we are given a question from a community forumand a thread of associated text comments intendedto answer the given question; and the goal is torank the comments according to their appropriate-ness to the question.
Since cQA forum threads arenoisy (e.g., because over time people tend to en-gage in discussion and to deviate from the originalquestion), as many comments are not answers tothe question, the challenge lies in learning to rankall good comments above all bad ones.Here, we adopt the definition and the datasetsfrom SemEval?2016 Task 3 (Nakov et al, 2016)on ?Community Question Answering?, focus-ing on subtask A (Question-Comment Similarity)only.1See the task description paper and the taskwebsite2for more detail.
An annotated example isshown in Figure 1.1SemEval-2016 Task 3 had two more subtasks: subtask Bon Question-Question Similarity, and subtask C on Question-External Comment Similarity, which are out of our scope.However, they could be potentially addressed within our gen-eral MTE-NN framework, with minor variations.2http://alt.qcri.org/semeval2016/task3/In this paper, we tackle the task from a novelperspective: by using ideas from machine trans-lation evaluation (MTE) to decide on the qual-ity of a comment.
In particular, we extend ourMTE neural network framework from (Guzm?anet al, 2015), showing that it is applicable to thecQA task as well.
We believe that this neural net-work is interesting for the cQA problem because:(i) it works in a pairwise fashion, i.e., given twotranslation hypotheses and a reference translationto compare to, the network decides which transla-tion hypothesis is better, which is appropriate fora ranking problem; (ii) it allows for an easy incor-poration of rich syntactic and semantic embeddedrepresentations of the input texts, and it efficientlymodels complex non-linear relationships betweenthem; (iii) it uses a number of machine translationevaluation measures that have not been exploredfor the cQA task before, e.g., TER (Snover et al,2006), METEOR (Lavie and Denkowski, 2009),and BLEU (Papineni et al, 2002).The analogy we apply to adapt the neural MTEarchitecture to the cQA problem is the following:given two comments c1and c2from the ques-tion thread?which play the role of the two com-peting translation hypotheses?we have to decidewhether c1is a better answer than c2to questionq?which plays the role of the translation refer-ence.
If we have a function f(q, c1, c2) to makethis decision, then we can rank the finite list ofcomments in the thread by comparing all possiblepairs and by accumulating for each comment thescores for it given by f .From a general point of view, MTE and the cQAtask addressed in this paper seem similar: bothreason about the similarity of two competing textsagainst a reference text in order to decide whichone is better.
However, there are some profounddifferences, which have implications on how eachtask is solved.460Figure 1: Annotated English question from the CQA-QL corpus.
Shown are the first two comments only.In MTE, the goal is to decide whether a hypoth-esis translation conveys the same meaning as thereference translation.
In cQA, it is to determinewhether the comment is an appropriate answer tothe question.
Furthermore, in MTE we can ex-pect shorter texts, which are typically much moresimilar.
In contrast, in cQA, the question and theintended answers might differ significantly both interms of length and in lexical content.
Thus, it isnot clear a priori whether the MTE network canwork well to address the cQA problem.
Here, weshow that the analogy is not only convenient, butalso that using it can yield state-of-the-art resultsfor the cQA task.To validate our intuition, we present series ofexperiments using the publicly available SemEval-2016 Task 3 datasets, with focus on subtask A. Weshow that a na?
?ve application of the MTE architec-ture and features on the cQA task already yieldsresults that are largely above the task baselines.Furthermore, by adapting the models with in-domain data, and adding lightweight task-specificfeatures, we are able to boost our system to reachstate-of-the-art performance.More interestingly, we analyze the contributionof several features and parts of the NN architectureby performing an ablation study.
We observe thatevery single piece contributes important informa-tion to achieve the final performance.
While task-specific features are crucial, other aspects of theframework are relevant as well: syntactic embed-dings, machine translation evaluation measures,and pairwise training of the network.The rest of the paper is organized as follows:Section 2 introduces some related work.
Section 3presents the overall architecture of our MTE-inspired NN framework for cQA.
Section 4 sum-marizes the features we use in our experiments.Section 5 describes the experimenal settings andpresents the results.
Finally, Section 6 offers fur-ther discussion and presents the main conclusions.2 Related WorkRecently, many neural network (NN) models havebeen applied to cQA tasks: e.g., question-questionsimilarity (Zhou et al, 2015; dos Santos et al,2015; Lei et al, 2016) and answer selection (Sev-eryn and Moschitti, 2015; Wang and Nyberg,2015; Shen et al, 2015; Feng et al, 2015; Tanet al, 2015).
Most of these papers concentrate onproviding advanced neural architectures in orderto better model the problem at hand.
However, ourgoal here is different: we extend and reuse an ex-isting pairwise NN framework from a different butrelated problem.There is also work that uses machine translationmodels as a features for cQA (Berger et al, 2000;Echihabi and Marcu, 2003; Jeon et al, 2005; Sori-cut and Brill, 2006; Riezler et al, 2007; Li andManandhar, 2011; Surdeanu et al, 2011; Tran etal., 2015) e.g., a variation of IBM model 1, to com-pute the probability that the question is a possible?translation?
of the candidate answer.
Unlike thatwork, here we port an entire MTE framework tothe cQA problem.
A preliminary version of thiswork was presented in (Guzm?an et al, 2016).461f(q,c1,c2)?(q,c1)?
(q,c2)hq1hq2h12vxc1xc2xqqc1c2sentences  embeddings pairwise nodes pairwise featuresoutput layerFigure 2: Overall architecture of the NN.3 Neural Model for Answer RankingThe NN model we use for answer ranking is de-picted in Figure 2.
It is a direct adaptation of ourfeed-forward NN for MTE (Guzm?an et al, 2015).Technically, we have a binary classification taskwith input (q, c1, c2), which should output 1 ifc1is a better answer to q than c2, and 0 other-wise.
The network computes a sigmoid functionf(q, c1, c2) = sig(wTv?
(q, c1, c2) + bv), where?
(x) transforms the input x through the hiddenlayer, wvare the weights from the hidden layerto the output layer, and bvis a bias term.We first map the question and the comments toa fixed-length vector [xq,xc1,xc2] using syntacticand semantic embeddings.
Then, we feed this vec-tor as input to the neural network, which modelsthree types of interactions, using different groupsof nodes in the hidden layer.
There are two eval-uation groups hq1and hq2that model how goodeach comment ciis to the question q.
The input tothese groups are the concatenations [xq,xc1] and[xq,xc2], respectively.
The third group of hiddennodes h12, which we call similarity group, modelshow close c1and c2are.
Its input is [xc1,xc2].
Thismight be useful as highly similar comments arelikely to be comparable in appropriateness, irre-spective of whether they are good or bad answersin absolute terms.In summary, the transformation ?
(q, c1, c2) =[hq1,hq2,h12] can be written ashqi= g(Wqi[xq,xci] + bqi), i = 1, 2h12= g(W12[xc1,xc2] + b12),where g(.)
is a non-linear activation function (ap-plied component-wise), W ?
RH?Nare the asso-ciated weights between the input layer and the hid-den layer, and b are the corresponding bias terms.We use tanh as an activation function, ratherthan sig, to be consistent with how the word em-bedding vectors we use were generated.The model further allows to incorporate exter-nal sources of information in the form of skip arcsthat go directly from the input to the output, skip-ping the hidden layer.
These arcs represent pair-wise similarity feature vectors between q and ei-ther c1or c2.
In these feature vectors, we en-code MT evaluation measures (e.g., TER, ME-TEOR, and BLEU), cQA task-specific features, etc.See Section 4 for detail about the features im-plemented as skip arcs.
In the figure, we indi-cate these pairwise external feature sets as ?
(q, c1)and ?
(q, c2).
When including the external fea-tures, the activation at the output is f(q, c1, c2) =sig(wTv[?
(q, c1, c2), ?
(q, c1), ?
(q, c2)] + bv).4 FeaturesWe experiment with three kinds of features: (i) in-put embeddings, (ii) features from MTE (Guzm?anet al, 2015) and (iii) task-specific features fromSemEval-2015 Task 3 (Nicosia et al, 2015).A.
Embedding Features We used two types ofvector-based embeddings to encode the input textsq, c1and c2: (1) GOOGLE VECTORS: 300-dimensional embedding vectors, trained on 100billion words from Google News (Mikolov et al,2013).
The encoding of the full text is just theaverage of the word embeddings.
(2) SYNTAX:We parse the entire question/comment using theStanford neural parser (Socher et al, 2013), andwe use the final 25-dimensional vector that is pro-duced internally as a by-product of parsing.Also, we compute cosine similarity featureswith the above vectors: cos(q, c1) and cos(q, c2).B.
MTE features We use the following MTEmetrics (MTFEATS), which compare the similar-ity between the question and a candidate answer:(1) BLEU (Papineni et al, 2002); (2) NIST (Dod-dington, 2002); (3) TER v0.7.25 (Snover et al,2006).
(4) METEOR v1.4 (Lavie and Denkowski,2009) with paraphrases; (5) Unigram PRECISION;(6) Unigram RECALL.BLEUCOMP.
We further use as features var-ious components involved in the computation ofBLEU: n-gram precisions, n-gram matches, totalnumber of n-grams (n=1,2,3,4), lengths of the hy-potheses and of the reference, length ratio betweenthem, and BLEU?s brevity penalty.462C.
Task-specific features First, we traindomain-specific vectors using WORD2VEC on allavailable QatarLiving data, both annotated andraw (QL VECTORS).Second, we compute various easy task-specific features (TASK FEATURES), mostof them proposed for the 2015 edition of thetask (Nicosia et al, 2015).
This includessome comment-specific features: (1) num-ber of URLs/images/emails/phone numbers;(2) number of occurrences of the string ?thank?
;3(3) number of tokens/sentences; (4) aver-age number of tokens; (5) type/token ratio;(6) number of nouns/verbs/adjectives/adverbs/pronouns; (7) number of positive/negativesmileys; (8) number of single/double/tripleexclamation/interrogation symbols; (9) numberof interrogative sentences (based on pars-ing); (10) number of words that are not inWORD2VEC?s Google News vocabulary.4Alsosome question-comment pair features: (1) ques-tion to comment count ratio in terms of sen-tences/tokens/nouns/verbs/adjectives/adverbs/pro-nouns; (2) question to comment count ratio ofwords that are not in WORD2VEC?s Google Newsvocabulary.
Finally, we also have two metafeatures: (1) is the person answering the questionthe one who asked it; (2) reciprocal rank of thecomment in the thread.5 Experiments and ResultsWe experiment with the data from SemEval-2016Task 3.
The task offers a higher quality train-ing dataset TRAIN-PART1, which includes 1,412questions and 14,110 answers, and a lower-qualityTRAIN-PART2 with 382 questions and 3,790 an-swers.
We train our model on TRAIN-PART1 withhidden layers of size 3 for 100 epochs with mini-batches of size 30, regularization of 0.005, and adecay of 0.0001, using stochastic gradient descentwith adagrad (Duchi et al, 2011); we use Theano(Bergstra et al, 2010) for learning.
We normal-ize the input feature values to the [?1; 1] inter-val using minmax, and we initialize the networkweights by sampling from a uniform distributionas in (Bengio and Glorot, 2010).
We train themodel using all pairs of good vs. bad comments,in both orders, ignoring ties.3When an author thanks somebody, this post is typicallya bad answer to the original question.4Can detect slang, foreign language, etc., which wouldindicate a bad answer.System MAP AvgRec MRRMTE-CQApairwise78.20 88.01 86.93MTE-CQAclassification77.62 87.85 85.79MTEvanilla70.17 81.84 78.60Baselinetime59.53 72.60 67.83Baselinerand52.80 66.52 58.71Table 1: Main results on the ranking task.At test time, we get the full ranking by scoringall possible pairs, and we accumulate the scores atthe comment level.We evaluate the model on TRAIN-PART2 aftereach epoch, and ultimately we keep the model thatachieves the highest accuracy;5in case of a tie, weprefer the parameters from an earlier epoch.
Weselected the above parameter values on the DEVdataset (244 questions and 2,440 answers) usingthe full model, and we used them for all exper-iments below, where we evaluate on the officialTEST dataset (329 questions and 3,270 answers).We report mean average precision (MAP), whichis the official evaluation measure, and also averagerecall (AvgRec) and mean reciprocal rank (MRR).5.1 ResultsTable 1 shows the evaluation results for three con-figurations of our MTE-based cQA system.
Wecan see that the vanilla MTE system (MTEvanilla),which only uses features from our original MTEmodel, i.e., it does not have any task-specific fea-tures (TASK FEATURES and QL VECTORS), per-forms surprisingly well despite the differences inthe MTE and cQA tasks.
It outperforms a ran-dom baseline (Baselinerand) and a chronologicalbaseline that assumes that early comments are bet-ter than later ones (Baselinetime) by large margins:by about 11 and 17 MAP points absolute, respec-tively.
For the other two measures the results aresimilar.We can further see that adding the task-specificfeatures in MTE-CQApairwiseimproves the re-sults by another 8 MAP points absolute.
Finally,the second line shows that adapting the networkto do classification (MTE-CQAclassification), giv-ing it a question and a single comment as input,yields a performance drop of 0.6 MAP points ab-solute compared to the proposed pairwise learningmodel.
Thus, the pairwise training strategy is con-firmed to be better for the ranking task, althoughnot by a large margin.5We also tried Kendall?s Tau (?
), but it performed worse.463System MAP AvgRec MRR ?MAPMTE-CQA 78.20 88.01 86.93?BLEUCOMP 77.83 87.85 86.32 -0.37?MTFEATS 77.75 87.76 86.01 -0.45?SYNTAX 77.65 87.65 85.85 -0.55?GOOGLE VECT.
76.96 87.66 84.72 -1.24?QL VECTORS 75.83 86.57 83.90 -2.37?TASK FEATS.
72.91 84.06 78.73 -5.29Table 2: Results of the ablation study.Table 2 presents the results of an ablation study,where we analyze the contribution of various fea-tures and feature groups to the performance of theoverall system.
For the purpose, we study ?MAP,i.e., the absolute drop in MAP when the featuregroup is excluded from the full system.Not surprisingly, the most important turn outto be the TASK FEATURES (contributing over fiveMAP points) as they handle important informa-tion sources that are not available to the systemfrom other feature groups, e.g., the reciprocal rankalone contributes about two points.Next in terms of importance come word embed-dings, QL VECTORS (contributing over 2 MAPpoints), trained on text from the target forum,QatarLiving.
Then come the GOOGLE VECTORS(contributing over one MAP point), which aretrained on 100 billion words, and thus are stilluseful even in the presence of the domain-specificQL VECTORS, which are in turn trained on fourorders of magnitude less data.Interestingly, the MTE-motivated SYNTAX vec-tors contribute half a MAP point, which shows theimportance of modeling syntax for this task.
Theother two MTE features, MTFEATS and BLEU-COMP, together contribute 0.8 MAP points.
It isinteresting that the BLEU components manage tocontribute on top of the MTFEATS, which alreadycontain several state-of-the-art MTE measures, in-cluding BLEU itself.
This is probably becausethe other features we have do not model n-grammatches directly.Finally, Table 3 puts the results in perspective.We can see that our system MTE-CQA wouldrank first on MRR, second on MAP, and fourthon AvgRec in SemEval-2016 Task 3 competition.6These results are also 5 and 16 points above the av-erage and the worst systems, respectively.6The full results can be found on the task website:http://alt.qcri.org/semeval2016/task3/index.php?id=resultsSystem MAP AvgRec MRR1st (Filice et al, 2016) 79.19 88.82 86.42MTE-CQA 78.20 88.01 86.932nd (Barr?on-Cede?no et al, 2016) 77.66 88.05 84.933rd (Mihaylov and Nakov, 2016) 77.58 88.14 85.21. .
.
.
.
.
.
.
.
.
.
.Average 73.54 84.61 81.54. .
.
.
.
.
.
.
.
.
.
.12th (Worst) 62.24 75.41 70.58Table 3: Comparative results with the bestSemEval-2016 Task 3, subtask A systems.This is remarkable given the lightweight task-specific features we use, and confirms the validityof the proposed neural approach to produce state-of-the-art systems for this particular cQA task.6 ConclusionWe have explored the applicability of machinetranslation evaluation methods to answer rankingin community Question Answering, a seeminglyvery different task, where the goal is to rank thecomments in a question-answer thread accordingto their appropriateness to the question, placing allgood comments above all bad ones.In particular, we have adopted a pairwise neu-ral network architecture, which incorporates MTEfeatures, as well as rich syntactic and semantic em-beddings of the input texts that are non-linearlycombined in the hidden layer.
The evaluationresults on benchmark datasets have shown state-of-the-art performance, with sizeable contributionfrom both the MTE features and from the networkarchitecture.
This is an interesting and encourag-ing result, as given the difference in the tasks, itwas not a-priori clear that an MTE approach wouldwork well for cQA.In future work, we plan to incorporate othersimilarity measures and better task-specific fea-tures into the model.
We further want to explorethe application of this architecture to other seman-tic similarity problems such as question-questionsimilarity, and textual entailment.AcknowledgmentsThis research was performed by the Arabic Lan-guage Technologies (ALT) group at the QatarComputing Research Institute (QCRI), Hamad binKhalifa University, part of Qatar Foundation.
It ispart of the Interactive sYstems for Answer Search(Iyas) project, which is developed in collaborationwith MIT-CSAIL.464ReferencesAlberto Barr?on-Cede?no, Giovanni Da San Martino,Shafiq Joty, Alessandro Moschitti, Fahad A. AlObaidli, Salvatore Romeo, Kateryna Tymoshenko,and Antonio Uva.
2016.
ConvKN at SemEval-2016Task 3: Answer and question selection for questionanswering on Arabic and English fora.
In Proceed-ings of the 10th International Workshop on SemanticEvaluation, SemEval ?16, San Diego, CA.Yoshua Bengio and Xavier Glorot.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In Proceedings of the 13th InternationalConference on Artificial Intelligence and Statistics,AISTATS ?10, pages 249?256, Sardinia, Italy.Adam Berger, Rich Caruana, David Cohn, Dayne Fre-itag, and Vibhu Mittal.
2000.
Bridging the lexi-cal chasm: Statistical approaches to answer-finding.In Proceedings of the 23rd Annual InternationalACM Conference on Research and Development inInformation Retrieval, SIGIR ?00, pages 192?199,Athens, Greece.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU andGPU math expression compiler.
In Proceedingsof the Python for Scientific Computing Conference,SciPy ?10, Austin, Texas, USA.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the Sec-ond International Conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanDiego, California, USA.Cicero dos Santos, Luciano Barbosa, Dasha Bog-danova, and Bianca Zadrozny.
2015.
Learn-ing hybrid representations to retrieve semanticallyequivalent questions.
In Proceedings of the 53rdAnnual Meeting of the Association for Computa-tional Linguistics and the 7th International JointConference on Natural Language Processing, ACL-IJCNLP ?15, pages 694?699, Beijing, China.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics, ACL ?03,pages 16?23, Sapporo, Japan.Minwei Feng, Bing Xiang, Michael R Glass, LidanWang, and Bowen Zhou.
2015.
Applying deeplearning to answer selection: A study and an opentask.
In Proceedings of the 2015 IEEE AutomaticSpeech Recognition and Understanding Workshop,ASRU ?15, Scottsdale, Arizona, USA.Simone Filice, Danilo Croce, Alessandro Moschitti,and Roberto Basili.
2016.
KeLP at SemEval-2016Task 3: Learning semantic relations between ques-tions and answers.
In Proceedings of the 10th In-ternational Workshop on Semantic Evaluation, Se-mEval ?16, San Diego, California, USA.Francisco Guzm?an, Shafiq Joty, Llu?
?s M`arquez, andPreslav Nakov.
2015.
Pairwise neural machinetranslation evaluation.
In Proceedings of the 53rdAnnual Meeting of the Association for Computa-tional Linguistics and the 7th International JointConference on Natural Language Processing, ACL-IJCNLP ?15, pages 805?814, Beijing, China.Francisco Guzm?an, Llu?
?s M`arquez, and Preslav Nakov.2016.
MTE-NN at SemEval-2016 Task 3: Can ma-chine translation evaluation help community ques-tion answering?
In Proceedings of the 10th In-ternational Workshop on Semantic Evaluation, Se-mEval ?16, San Diego, California, USA.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM In-ternational Conference on Information and Knowl-edge Management, CIKM ?05, pages 84?90, Bre-men, Germany.Alon Lavie and Michael Denkowski.
2009.
The ME-TEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115.Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi S.Jaakkola, Kateryna Tymoshenko, Alessandro Mos-chitti, and Llu?
?s M`arquez.
2016.
Semi-supervisedquestion retrieval with gated convolutions.
In Pro-ceedings of the 15th Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL-HLT ?16, San Diego, California, USA.Shuguang Li and Suresh Manandhar.
2011.
Improvingquestion recommendation by exploiting informationneed.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, HLT ?11, pages 1425?1434, Portland, Oregon, USA.Todor Mihaylov and Preslav Nakov.
2016.
SemanticZat SemEval-2016 Task 3: Ranking relevant answersin community question answering using semanticsimilarity based on fine-tuned word embeddings.
InProceedings of the 10th International Workshop onSemantic Evaluation, SemEval ?16, San Diego, Cal-ifornia, USA.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, NAACL-HLT ?13, pages746?751, Atlanta, Georgia, USA.465Preslav Nakov, Llu?
?s M`arquez, Alessandro Moschitti,Walid Magdy, Hamdy Mubarak, Abed AlhakimFreihat, Jim Glass, and Bilal Randeree.
2016.SemEval-2016 task 3: Community question answer-ing.
In Proceedings of the 10th International Work-shop on Semantic Evaluation, SemEval ?16, SanDiego, California, USA.Massimo Nicosia, Simone Filice, Alberto Barr?on-Cede?no, Iman Saleh, Hamdy Mubarak, Wei Gao,Preslav Nakov, Giovanni Da San Martino, Alessan-dro Moschitti, Kareem Darwish, Llu?
?s M`arquez,Shafiq Joty, and Walid Magdy.
2015.
QCRI: An-swer selection for community question answering -experiments for Arabic and English.
In Proceedingsof the 9th International Workshop on Semantic Eval-uation, SemEval ?15, pages 203?209, Denver, Col-orado, USA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meting of the Association for Com-putational Linguistics, ACL ?02, pages 311?318,Philadelphia, Pennsylvania, USA.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical machine translation for query expansionin answer retrieval.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, ACL ?07, pages 464?471, Prague,Czech Republic.Aliaksei Severyn and Alessandro Moschitti.
2015.Learning to rank short text pairs with convolutionaldeep neural networks.
In Proceedings of the 38thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR?15, pages 373?382, Santiago, Chile.Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng,Jie Tang, and Zhang Xiong.
2015.
Word embeddingbased correlation model for question/answer match-ing.
arXiv preprint arXiv:1511.04646.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Ameri-cas, AMTA ?06, pages 223?231, Cambridge, Mas-sachusetts, USA.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics, ACL ?13, pages 455?465, Sofia,Bulgaria.Radu Soricut and Eric Brill.
2006.
Automatic questionanswering using the web: Beyond the factoid.
Inf.Retr., 9(2):191?206, March.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Comput.Linguist., 37(2):351?383, June.Ming Tan, Bing Xiang, and Bowen Zhou.
2015.LSTM-based deep learning models for non-factoidanswer selection.
arXiv preprint arXiv:1511.04108.Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, andSon Bao Pham.
2015.
JAIST: Combining multiplefeatures for answer selection in community questionanswering.
In Proceedings of the 9th InternationalWorkshop on Semantic Evaluation, SemEval ?15,pages 215?219, Denver, Colorado, USA.Di Wang and Eric Nyberg.
2015.
A long short-term memory model for answer sentence selectionin question answering.
In Proceedings of the 53rdAnnual Meeting of the Association for Computa-tional Linguistics and the 7th International JointConference on Natural Language Processing, ACL-IJCNLP ?15, pages 707?712, Beijing, China.Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu.2015.
Learning continuous word embedding withmetadata for question retrieval in community ques-tion answering.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Con-ference on Natural Language Processing, ACL-IJCNLP ?15, pages 250?259, Beijing, China.466
