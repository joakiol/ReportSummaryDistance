Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93?103,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGraph-based Local Coherence ModelingCamille Guinaudeau and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germany(camille.guinaudeau|michael.strube)@h-its.orgAbstractWe propose a computationally efficientgraph-based approach for local coherencemodeling.
We evaluate our system onthree tasks: sentence ordering, summarycoherence rating and readability assess-ment.
The performance is comparable toentity grid based approaches though theserely on a computationally expensive train-ing phase and face data sparsity problems.1 IntroductionMany NLP applications which process or gener-ate texts rely on information about local coher-ence, i.e.
information about which entities occurin which sentence and how the entities are dis-tributed in the text.
This led to the developmentof many theories and models accounting for lo-cal coherence.
One popular model, the center-ing model (Grosz et al, 1995), uses a ranking ofdiscourse entities realized in particular sentencesand computes transitions between adjacent sen-tences to provide insight in the felicity of texts.Centering models local coherence rather generallyand has been applied to the generation of refer-ring expressions (Kibble and Power, 2004), to re-solve pronouns (Brennan et al, 1987, inter alia),to score essays (Miltsakaki and Kukich, 2004), toarrange sentences in the correct order (Karamaniset al, 2009), and to many other tasks.
Poesio etal.
(2004) observe that it is not clear how to setparameters in the centering model so that optimalperformance in different tasks and languages canbe achieved.
Barzilay and Lapata (2008) criticizeresearch on centering to be too dependent on man-ually annotated input.
This led them to propose alocal coherence model relying on a more parsimo-nious representation, the entity grid model.The entity grid is a two dimensional array wherethe rows represent sentences and the columns dis-course entities.
From this grid Barzilay and La-pata (2008) derive probabilities of transitions be-tween adjacent sentences which are used as fea-tures for machine learning algorithms.
They eval-uate this approach successfully on sentence order-ing, summary coherence rating, and readability as-sessment.
However, their approach has some dis-advantages which they point out themselves: datasparsity, domain dependence and computationalcomplexity, especially in terms of feature space is-sues while building their model (Barzilay and La-pata (2008, p.8, p.10, p.30), Elsner and Charniak(2011, p.126, p.127)).In order to overcome these problems we pro-pose to represent entities in a graph and thenmodel local coherence by applying centrality mea-sures to the nodes in the graph (Section 3).
Weclaim that a graph is a more powerful representa-tion for local coherence than the entity grid (Barzi-lay and Lapata, 2008) which is restricted to transi-tions between adjacent sentences.
The graph caneasily span the entire text without leading to com-putational complexity and data sparsity problems.Similar to the application of graph-based methodsin other areas of NLP (e.g.
work on word sensedisambiguation by Navigli and Lapata (2010); foran overview over graph-based methods in NLPsee Mihalcea and Radev (2011)) we model localcoherence by relying only on centrality measuresapplied to the nodes in the graph.
We apply ourgraph-based model to the three tasks handled byBarzilay and Lapata (2008) to show that it pro-vides the same flexibility over disparate tasks asthe entity grid model: sentence ordering (Section4.1), summary coherence ranking (Section 4.2),and readability assessment (Section 4.3).
In the93The Turkish government fell after mob-tie allegations.Turkey?s constitution mandates a secular republic despite itsMuslim majority.Military and secular leaders pressured President Demirel tokeep the Islamic-oriented Virtue Party on the fringe.Business leaders feared Virtue would alienate the EU.Table 1: Excerpt of a manual summary M fromDUC2003experiments sections, we discuss the impact ofgenre and stylistic properties of documents on thelocal coherence computation.
We also show that,though we do not need a computationally expen-sive learning phase, our model achieves state-of-the-art performance.
From this we conclude that agraph is an alternative to the entity grid model: it iscomputationally more tractable for modeling localcoherence and does not suffer from data sparsityproblems (Section 5).2 The Entity Grid ModelBarzilay and Lapata (2005; 2008) introduced theentity grid, a method for local coherence modelingthat captures the distribution of discourse entitiesacross sentences in a text.An entity grid is a two dimensional array, whererows correspond to sentences and columns to dis-course entities.
For each discourse entity ej andeach sentence si in the text, the corresponding gridcell cij contains information about the presence orabsence of the entity in the sentence.
If the entitydoes not appear in the sentence, the correspond-ing grid cell contains an absence marker ???.
Ifthe entity is present in the sentence, the cell con-tains a representation of the entity?s syntactic role:?S?
if the entity is a subject, ?O?
if it is an objectand ?X?
for all other syntactic roles (cf.
Table 2).When a noun is attested more than once with adifferent grammatical role in the same sentence,the role with the highest grammatical ranking ischosen to represent the entity (a subject is rankedhigher than an object, which is ranked higher thanother syntactic roles).Barzilay and Lapata (2008) capture local coher-ence by means of local entity transitions, i.e.
se-quences of grid cells (c1j .
.
.
cij .
.
.
cnj) represent-ing the syntactic function or absence of an entity inadjacent sentences1.
The coherence of a sentencein relation to its local context is determined by the1For complexity reasons, Barzilay and Lapata consideronly transitions between at most three sentences.GOVERNMENTALLEGATIONTURKEYCONSTITUTIONSECULARREPUBLICMAJORITYMILITARYLEADERPRESIDENTDEMIRELVIRTUEPARTYFRINGEBUSINESSEUs1 S X ?
?
?
?
?
?
?
?
?
?
?
?
?
?s2 ?
?
X S X O X ?
?
?
?
?
?
?
?
?s3 ?
?
?
?
X ?
?
X S X S X O X ?
?s4 ?
?
?
?
?
?
?
?
S ?
?
S ?
?
X OTable 2: Entity Grid representation of summary Mlocal entity transitions of the entities present or ab-sent in the sentence.
To make this representationaccessible to machine learning algorithms, Barzi-lay and Lapata (2008) compute for each documentthe probability of each transition and generate fea-ture vectors representing the sentences.
Coherenceassessment is then formulated as a ranking learn-ing problem where the ranking function is learnedwith SVMlight (Joachims, 2002).The entity grid approach has already been ap-plied to many applications relying on local co-herence estimation: summary rating (Barzilayand Lapata, 2005), essay scoring (Burstein et al,2010) or story generation (McIntyre and Lapata,2010).
It was also used successfully in com-bination with other systems or features.
Sori-cut and Marcu (2006) show that the entity gridmodel is a critical component in their sentence or-dering model for discourse generation.
Barzilayand Lapata (2008) combine the entity grid withreadability-related features to discriminate docu-ments between easy- and difficult-to-read cate-gories.
Lin et al (2011) use discourse relations totransform the entity grid representation into a dis-course role matrix that is used to generate featurevectors for machine learning algorithms similarlyto Barzilay and Lapata (2008).Several studies propose to extend the entity gridmodel using different strategies for entity selec-tion.
Filippova and Strube (2007) aim to improvethe entity grid model performance by grouping en-tities by means of semantic relatedness.
In theirstudies, Elsner and Charniak extend the numberand type of entities selected and consider that eachentity has to be dealt with accordingly with its in-formation status (Elsner et al, 2007) or its named-entity category (Elsner and Charniak, 2011).
Fi-nally, they include a heuristic coreference resolu-tion component by linking mentions which share a94s1 s2 s3 s4e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 e163 1 1 3 1 2 1 1 1 3 1 3 1 2133 1 2 s1 s2s3 s411s1 s2s3 s412(a) Bipartite Graph (b) Unweighted One-mode (c) Weighted One-modeProjection Projectione1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11 e12 e13 e14 e15 e16s1 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0s2 0 0 1 3 1 2 1 0 0 0 0 0 0 0 0 0s3 0 0 0 0 1 0 0 1 3 1 3 1 2 1 0 0s4 0 0 0 0 0 0 0 0 3 0 0 3 0 0 1 2s1 s2 s3 s4s1 0 0 0 0s2 0 0 1 0s3 0 0 0 1s4 0 0 0 0s1 s2 s3 s4s1 0 0 0 0s2 0 0 1 0s3 0 0 0 2s4 0 0 0 0(d) Incidence Matrix (e) Unweighted Adjacency (f) Weighted AdjacencyMatrix MatrixFigure 1: Bipartite graph for summary M from Table 1, one-mode projections and associated incidenceand adjacency matrices.
Weights in Figure 1(a) are assigned as follows: ?S?
= 3, ?O?
= 2, ?X?
= 1,???
= 0 (no edge).head noun.
These extensions led to the best resultsreported so far for the sentence ordering task.3 MethodOur model is based on the insight that the en-tity grid (Barzilay and Lapata, 2008) correspondsto the incidence matrix of a bipartite graph rep-resenting the text (see Newman (2010) for moredetails on graph representation).
A fundamentalassumption underlying our model is that this bi-partite graph contains the entity transition infor-mation needed for local coherence computation,rendering feature vectors and learning phase un-necessary.
The bipartite graph G = (Vs, Ve, L, w)is defined by two independent sets of nodes ?
thatcorrespond to the set of sentences Vs and the set ofentities Ve of the text ?
and a set of edges L associ-ated with weights w. An edge between a sentencenode si and an entity node ej is created in the bi-partite graph if the corresponding cell cij in theentity grid is not equal to ???.
Each edge is asso-ciated with a weight w(ej , si) that depends on thegrammatical role of the entity ej in the sentencesi2.
In contrast to Barzilay and Lapata?s entitygrid that contains information about absent enti-ties, our graph-based representation only contains?positive?
information.
Figure 1(a) shows an ex-ample of the bipartite graph that corresponds to thegrid in Table 2.
The incidence matrix of this graph(Figure 1(d)) is very similar to the entity grid.2The assignment of weights is described in Section 4.By modeling entity transitions, Barzilay andLapata rely on links that exist between sentencesto model local coherence.
In the same spirit, weapply different kinds of one-mode projections tothe sentence node set Vs of the bipartite graph torepresent the connections that exist between ?
po-tentially non adjacent ?
sentences in the graph.These projections result in graphs where nodescorrespond to sentences.
An edge is created be-tween two nodes if the corresponding sentenceshave a least one entity in common.
Contrary to thebipartite graph, one-mode projections are directedas they follow the text order.
Therefore, in projec-tion graphs an edge can exist between the first andthe second sentence while the inverse is not pos-sible.
In our model, we define three kinds of pro-jection graphs, PU , PW and PAcc, depending onthe weighting scheme associated with their edges.In PU , weights are binary and equal 1 when twosentences have a least one entity in common (Fig-ure 1(b)).
In PW , edges are weighted according tothe number of entities ?shared?
by two sentences(Figure 1(c)).
In PAcc syntactic information is ac-counted for by integrating the edge weights in thebipartite graph.
In this case, weights are equal toWik =?e?Eikw(e, si) ?
w(e, sk) ,where Eik is the set of entities shared by si andsk.
Distance between sentences si and sk can alsobe integrated in the weight of one-mode projec-tions to decrease the importance of links that ex-95ists between non adjacent sentences.
In this case,the weights of the projection graphs are divided byk ?
i.From this graph-based representation, the localcoherence of a text T can be measured by comput-ing the average outdegree of a projection graph P .This centrality measure was chosen for two mainreasons.
First, it allows us to evaluate to which ex-tent a sentence is connected, in terms of discourseentities, with the other sentences of the text.
Sec-ond, compared to other centrality measures, thecomputational complexity of the average outde-gree is low (O(N?
(N?1)2 ) for a document com-posed by N sentences), keeping the local coher-ence estimation feasible on large documents andon large corpora.
Formally, the local coherence ofa text T is equal toLocalCoherence(T ) = AvgOutDegree(P )= 1N?i=1..NOutDegree(si) ,where OutDegree(si) is the sum of the weights as-sociated to edges that leave si and N is the num-ber of sentences in the text.
This value can also beseen as the sum of the values of the adjacency ma-trix of the projection graph (Figures 1(e) and 1(f))divided by the number of sentences.4 ExperimentsWe compare our model with the entity grid ap-proach and evaluate the influence of the differentweighting schemes used in the projection graphs,either PW or PAcc, where weights are potentiallydecreased by distance information Dist.
Ourbaseline corresponds to local coherence computa-tion based on the unweighted projection graph PU .For graph construction, all nouns in a documentare considered as discourse entities, even thosewhich do not head NPs as this is beneficial forthe entity grid model as described in Elsner andCharniak (2011).
We also propose to use a coref-erence resolution system and consider coreferententities to be the same discourse entity.
To do so,we use one of the top performing systems from theCoNLL 2012 shared task (Martschat et al, 2012).As the coreference resolution system is trained onwell-formed textual documents and expects a cor-rect sentence ordering, we use in all our experi-ments only features that do not rely on sentenceorder (e.g.
alias relations, string matching, etc.
).Grammatical information associated with eachentity is extracted automatically thanks to theStanford parser using dependency conversion (deMarneffe et al, 2006).
Syntactic weights in thebipartite graph are defined following the linguisticintuition that subjects are more important than ob-jects, which are themselves more important thanother syntactic roles.
Preliminary experimentsshow that as long as weight assignment followsthe scheme S > O > X, then more coherent docu-ments are associated with a higher local coherencevalue than less coherent document in 90% of cases(while this value equals 49% when no restric-tion is given on syntactic weights order).
More-over, as the local coherence computation is a lin-ear combination of the syntactic weights, the func-tion is smooth and no large variations of the localcoherence values are observed for small changesof weights?
values.
For these reasons, weightsw(e, si) are set as follows: 3 if e is subject in si, 2if e is an object and 1 otherwise.We evaluate the ability of our graph-basedmodel to estimate the local coherence of a tex-tual document with three different experiments.First, we perfom a sentence ordering task (Sec-tion 4.1) as proposed in Barzilay and Lapata(2008).
Then, as the first task uses ?artificial?
doc-uments, we also work on two other tasks that in-volve ?real?
documents: summary coherence rat-ing (Section 4.2), and readability assessment (Sec-tion 4.3).
In these experiments, distance compu-tation and syntactic weights are the same for alltasks and all corpora.
However, the model is alsoflexible and can be adaptated to the different tasksby optimizing the parameters on a developmentdata set, which may give better results.4.1 Sentence OrderingThe first experiment consists in ranking alternativesentence orderings of a document, as proposed byBarzilay and Lapata (2008) and Elsner and Char-niak (2011).4.1.1 Experimental SettingsThe sentence ordering task can be performed intwo ways: discrimination and insertion.
Discrimi-nation consists in comparing a document to a ran-dom permutation of its sentences.
For this, oursystem associates local coherence values with theoriginal document and its permutation, the outputof our system being considered as correct if thescore for the original document is higher than the96score of its permutation.
In the insertion task, pro-posed by Elsner and Charniak (2011), we evaluatethe ability of our system to retrieve the originalposition of a sentence previously removed from adocument.
For this, each sentence is removed inturn and a local coherence score is computed forevery possible reinsertion position.
The systemoutput is considered as correct if the document as-sociated with the highest local coherence score isthe one in which the sentence is reinserted in thecorrect position.These two tasks were performed on docu-ments extracted from the English test part of theCoNLL 2012 shared task (Pradhan et al, 2012).This corpus, composed by documents of multiplenews sources ?
spoken or written ?
was preferredto the ACCIDENTS and EARTHQUAKES corporaused by Barzilay and Lapata (2008) for two rea-sons.
First, as mentioned by Elsner and Charniak(2008), these corpora use a very constrained styleand are not typical of normal informative docu-ments3.
Second, we want to evaluate the influenceof automatically performed coreference resolutionin a controlled fashion.
The coreference resolutionsystem used performs well on the CoNLL 2012data.
In this dataset, documents composed by theconcatenation of differents news articles or tooshort to have at least 20 permutations were dis-carded from the corpus.
This filtering results in 61documents composed of 36.1 sentences or 2064word tokens on average.
In both discriminationand insertion, we compare our system against arandom baseline where random values are associ-ated with the different orderings.4.1.2 DiscriminationAccuracy is used to evaluate the ability of our sys-tem to discriminate a document from 20 differ-ent permutations.
It equals the number of timesour system gives the highest score to the originaldocument, divided by the number of comparisons.Since the model can give the same score for a per-mutation and the original document, we also com-pute F-measure where recall is correct/total andprecision equals correct/decisions.
We test sig-nificance using the Student?s t-test that can detectsignificant differences between paired samples.Moreover, as increasing the number of hypotheses3Our graph-based model obtains for the discriminationtask an accuracy of 0.846 and 0.635 on the ACCIDENTS andEARTHQUAKES datasets, respectively, compared to 0.904 and0.872 as reported by Barzilay and Lapata (2008).Acc F Acc FRandom 0.496 0.496B&L 0.877 0.877E&C 0.915 0.915wo coref w corefPU , Dist 0.830 0.830 0.833 0.833PW , Dist 0.871 0.871 0.849 0.849PAcc, Dist 0.889 0.889 0.852 0.852Table 3: Discrimination, reproduced baselines(B&L: Barzilay and Lapata (2008); E&C Elsnerand Charniak (2011)) vs. graph-basedin a test can also increase the likelihood of wit-nessing a rare event, and therefore, the chance toreject the null hypothesis when it is true, we usethe Bonferroni correction to adjust the increasedrandom likelihood of apparent significance.Table 3 presents the values obtained by threebaseline systems when applied to our corpus.
Re-sults for the entity grid models described by Barzi-lay and Lapata (2008) and Elsner and Charniak(2011) are obtained by using Micha Elsner?s reim-plementation in the Brown Coherence Toolkit4.The system was trained on the English trainingpart of the CoNLL 2012 shared task filtered in thesame way as the test part.Table 3 also displays the results for our model.These values show that our system performs com-parable to the state-of-the-art.
Indeed, the differ-ence between our best results and those of Elsnerand Charniak are not statistically significant.In this experiment, distance information is criti-cal.
Without it, it is not possible to distinguish be-tween an original document and one of its permu-tation as both contain the same number and kindof entities.
Distance however can detect changesin the distribution of entities within the documentas space between entities is significantly modi-fied when sentence order is permuted.
When thenumber of entities ?shared?
by two sentences istaken into account (PW ), the accuracy of our sys-tem grows (from 0.830 to 0.871).
Table 3 finallyshows that syntactic information improves the per-formance of our system (yet not significantly) andgives the best results (PAcc).We also evaluated the influence of coreferenceresolution on the performance of our system.
Us-4https://bitbucket.org/melsner/browncoherence; B&L is Elsner?s ?baseline entitygrid?
(command line option ?-n?
), E&C is Elsner?s ?extendedentity grid?
(?-f?)97Acc.
Ins.
Acc.
Ins.Random 0.028 0.071E&C 0.068 0.167wo coref w corefPU , Dist 0.062 0.101 0.068 0.120PW , Dist 0.075 0.114 0.070 0.138PAcc, Dist 0.071 0.102 0.067 0.097Table 4: Insertion, reproduced baselines vs. graph-baseding coreference resolution improves the perfor-mance of the system when distance information isused alone in the system (Table 3).
However, thisimprovement is not statistically significant.4.1.3 InsertionSentence insertion is much more difficult than dis-crimination for two reasons.
First, in insertion,permutations only differ by one sentence.
Second,a document is compared to many more permuta-tions in insertion task than in discrimination.In complement to accuracy, we use the insertionscore introduced by Elsner and Charniak (2011)for evaluation.
This score ?
the higher, the better?
computes the proximity between the initial andthe proposed position of a sentence, averaged bythe number of sentences.Table 4 shows that, as expected, results for thistask are much lower than those obtained for dis-crimination.
However they are still comparablewith the results of Elsner and Charniak (2011)5.As previously and for the same reasons, dis-tance information is critical for this task.
The bestresults, that present a statistically significant im-provement when compared to the random base-line, are obtained when distance information andthe number of entities ?shared?
by two sentencesare taken into account (PW ).
We can see that theaccuracy value obtained with our system is higherthan the one provided with the entity grid model.However, the entity grid model reaches a signifi-cantly higher insertion score.
This means that, if itmakes more mistakes than our system, the positionchosen by the entity grid model is usually closerto the correct position.
Finally, contrary to thediscrimination task, syntactic information (PAcc)does not improve the performance of our system.5Their results are slightly lower than those presented intheir paper, probably because our corpus is composed by doc-uments that can be longer than the ones used in their experi-ments (Wall Street Journal articles).When the coreference resolution system is used,the best accuracy value decreases while the inser-tion score increases from 0.114 to 0.138 (Table 4).Therefore, coreference resolution tends to asso-ciate positions that are closer to the original ones.4.2 Summary Coherence RatingTo reconfirm the hypothesis that our model can es-timate the local coherence of a textual document,we perform a second experiment, summary co-herence rating.
To this end, we apply our modelon the corpus used and proposed by Barzilay andLapata (2008).
As the objective of our model isto estimate the coherence of a summary, we pre-fer this dataset to other summarization evaluationtask corpora, as these account for other dimen-sions of the summaries: content selection, fluency,etc.
Starting with a pair of summaries, one slightlymore coherent than the other, the objective of thetask is to order the two summaries according tolocal coherence.4.2.1 Experimental SettingsFor the summary coherence rating experiment,pairs to be ordered are composed of summariesextracted from the Document Understanding Con-ference (DUC 2003).
Summaries, provided eitherby humans or by automatic systems, were judgedby seven humans annotators and associated witha coherence score (for more details on this scoresee Barzilay and Lapata (2008)).
80 pairs werethen created, each of these being composed by twosummaries of a same document where the scoreof one of the summaries is significantly higherthan the score of the second one.
Even though allsummaries are of approximately the same length(114.2 words on average), their sentence lengthcan vary considerably.
Indeed, more coherentsummaries tend to have more sentences and con-tain less entities.For evaluation purposes, the accuracy still cor-responds to the number of correct ratings di-vided by the number of comparisons, while the F-measure combines recall and precision measures.As before, significance is tested with the Student?st-test accounting for the Bonferroni correction.4.2.2 ResultsTable 5 compares the results reported by Barzilayand Lapata (2008) on the exact same corpus withthe results obtained with our system.
It shows that98Acc.
F Acc.
FB&L 0.833wo coref w corefPU 0.800 0.815 0.700 0.718PW 0.613 0.613 0.538 0.548PAcc 0.700 0.704 0.638 0.638PU , Dist 0.650 0.658 0.550 0.557PW , Dist 0.525 0.525 0.513 0.513PAcc, Dist 0.700 0.700 0.588 0.588Table 5: Summary Coherence Rating, reported re-sults from Barzilay and Lapata (2008) vs. graph-basedour system gives results comparable to those ob-tained by Barzilay and Lapata (2008).This table also shows that, contrary to sentenceordering task, accounting for the distance betweentwo sentences (Dist) tends to decrease the results.This difference is explained by the fact that a man-ual summary, usually considered as more coher-ent by humans annotators, tends to contain more(and shorter) sentences than an automatic one.
Asadding distance information decreases the value ofour local coherence score, our graph-based modelgives better results without it.Moreover, in contrast to the first experiment,when accounting for the number of entities?shared?
by two sentences (PW ), values of accu-racy and F-measure are lower.
We explain thisbehaviour by the number of sentences containedin the less coherent documents.
Indeed, they arecomposed by a smaller number of sentences butcontain more entities on average.
This means that,in these documents, two sentences tend to sharea larger number of entities and therefore have ahigher local coherence score when the PW projec-tion graph is used.When combined with distance information,syntactic information still improves the results(PAcc), though not significantly, but does not leadto the best results for this task.Finally, Table 5 also shows that using a coref-erence resolution system for document represen-tation does not improve the performance of oursystem.
We believe that, as mentioned by Barzi-lay and Lapata (2008), this degradation is relatedto the fact that automatic summarization systemsdo not use anaphoric expressions which makes thecoreference resolution system useless in this case.With our graph-based model, the best results areobtained by the baseline (PU ), and experimentsshow that adding information about distance orsyntax does not help in this context.
It seemstherefore necessary to integrate information that ismore appropriate to summaries.
Although makingthe model more appropriate for a specific task isout of the scope of this paper, our model is flex-ible and accounting for information about genredifferences or sentence length, by adding weightsin the graph-based representation of the document,is feasible without any modification of the model.4.3 Readability AssessmentBarzilay and Lapata (2008) argue that grid modelsare domain and style dependent.
Therefore theyproposed a readability assessment task to test if theentity grid model can be used for style classifica-tion.
They combined their model with Schwarmand Ostendorf?s (2005) readability features anduse Support Vector Machines to classify docu-ments in two categories.
With the same intention,we evaluate the ability of our model to differenti-ate ?easy to read?
documents from difficult ones.4.3.1 Experimental SettingsThe objective of the readability assessment taskis to evaluate how difficult to read a document is.We perform this task on the data used by Barzilayand Lapata (2008), a corpus collected originallyby Barzilay and Elhadad (2003) from the Ency-clopedia Britannica and its version for children,the Britannica Elementary.
Both versions contain107 articles.
In Encyclopedia Britannica, docu-ments are composed by an average of 83.1 sen-tences while they contain 36.6 sentences in Bri-tannica Elementary.
Although these texts are notexplicitly annotated with grade levels, they repre-sent two broad readability categories.In order to estimate the complexity of a doc-ument, our model computes the local coherencescore for each article in the two categories.
Thearticle associated with the higher score is consid-ered to be the more readable as it is more coherent,needing less interpretation from the reader than adocument associated with a lower local coherencescore.
Values presented in the following sectioncorrespond to accuracy, where the system is cor-rect if it assigns the higher local coherence score tothe most ?easy to read?
document, and F-measure.99Acc.
F Acc.
FS&O 0.786B&L 0.509B&L + S&O 0.888wo coref w corefPU 0.589 0.589 0.374 0.374PW 0.579 0.579 0.383 0.383PAcc 0.645 0.645 0.421 0.421PU , Dist 0.589 0.589 0.280 0.280PW , Dist 0.570 0.570 0.290 0.290PAcc, Dist 0.766 0.766 0.308 0.308Table 6: Readability, reported results from Barzi-lay and Lapata (2008) vs. graph-based (S&O:Schwarm and Ostendorf (2005))4.3.2 ResultsIn order to compare our results with those reportedby Barzilay and Lapata (2008), entities used forthe graph-based representation are discourse enti-ties that head NPs.Table 6 shows that, for this task, syntactic in-formation plays a dominant role (PAcc).
A sta-tistically significant improvement is provided byincluding syntactic information.
It gives moreweight to subject entities that are more numerousin the Britannica Elementary documents whichare composed by simpler and shorter sentences.Finally, when distance is accounted for togetherwith syntactic information, the accuracy is signif-icantly improved (p < 0.01) with regard to the re-sults obtained with syntactic information only.Table 6 also shows that when the number of en-tities ?shared?
by two sentences is accounted for(PW ), the results are lower.
Indeed, Encyclope-dia Britannica documents are composed by longersentences, that contain a higher number of enti-ties.
This increases the local coherence value ofdifficult documents more than the value of ?easyto read?
documents, that contain less entities.When our graph-based representation used thecoreference resolution system, unlike the observa-tion of Barzilay and Lapata (2008), the results ofour model decrease significantly.
The poor perfor-mance of our system in this case can be explainedby the fact that the coreference resolution systemregroups more entities in Encyclopedia Britannicadocuments than in Britannica Elementary ones.Therefore, the number of entities that are ?shared?by two sentences increases more importantly inthe Encyclopedia Britannica corpus, while the dis-tance between two occurrences of one entity de-creases in a more significant manner.
For thesereasons, the coherence scores associated with ?dif-ficult to read?
documents tend to be higher whencoreference resolution is performed on our data,which reduces the performance of our system.
Asbefore, syntactic information leads to the best re-sults, but does not allow the accuracy to be higherthan random anymore.Compared to the results provided by Barzi-lay and Lapata (2008) with the entity grid modelalone, our representation outperforms their modelsignificantly.
We believe that this difference iscaused by how syntactic information is introducedin the document representation and by the factthat our system can deal with entities that appearthroughout the whole document while the entitygrid model only looks at entities within a threesentences windows.
Our model which capturesexclusively local coherence is almost on par withthe results reported for Schwarm & Ostendorf?s(2005) system which relies on a wide range of lex-ical, syntactic and semantic features.
Only whenBarzilay and Lapata (2008) combine the entitygrid with Schwarm & Ostendorf?s features theyreach performance considerably better than ours.In addition to the experiments proposed byBarzilay and Lapata (2008), we used a third read-ability category, the Britannica Student, that con-tains articles targeted for youths (from 11 to 14years old).
These documents, which are quite sim-ilar to the Encyclopedia Britannica ones, are com-posed by an average of 44.1 sentences.
As wewere only able to find 99 articles out of the 107original ones in this category, sub corpora of thethree categories were used for the comparison withthe Britannica Student articles.Table 7 shows the results obtained for the com-parisons between the two first categories and theBritannica Student articles.
As previously, coref-erence resolution tends to lower the results, there-fore only values obtained without coreference res-olution are reported in the table.When articles from Britannica Student are com-pared to articles extracted from Encyclopedia Bri-tannica, Table 7 shows that the different param-eters have the same influence as for comparingbetween Encyclopedia Britannica and BritannicaElementary: statistically significant improvementwith syntactic information, higher values whendistance is taken into account, etc.
However, it100Brit.
vs. Stud.
Stud.
vs. Elem.Acc.
F Acc.
FPU 0.444 0.444 0.667 0.667PW 0.434 0.434 0.636 0.636PAcc 0.465 0.465 0.707 0.707PU , Dist 0.475 0.475 0.646 0.646PW , Dist 0.485 0.485 0.616 0.616PAcc, Dist 0.556 0.556 0.657 0.657Table 7: Readability, comparison between Ency-clopedia Britannica, Britannica Elementary andBritannica Studentcan also be seen that accuracy and F-measure arelower for comparing these two corpora.
This isprobably due to the stylistic difference betweenthese two kinds of articles, which is less signifi-cant than the difference between articles from En-cyclopedia Britannica and Britannica Elementary.Concerning the comparison between BritannicaStudent and Britannica Elementary articles, Ta-ble 7 shows that integrating distance informationgives slightly different results and tends to de-crease the values of accuracy and F-measure.
Thisis explained by the fact that Britannica Elementarydocuments contain fewer entities than BritannicaStudent articles.
As the length of the two kinds ofarticles is similar, distance between entities in Bri-tannica Elementary documents is more important.As a result, accounting for distance informationlowers the local coherence values for the more co-herent document, which reduces the performanceof our model.
As previously, syntactic informationimproves the results and, for this comparison, thebest result is obtained when syntactic informationalone is accounted for.
This leads to an accuracywhich is almost equal to the one when comparingEncyclopedia Britannica and Britannica Elemen-tary (0.707 against 0.766).These two additional experiments show that ourmodel is style dependent.
It obtains better resultswhen it has to distinguish between EncyclopediaBritannica and Britannica Elementary or Britan-nica Student and Britannica Elementary articleswhich present a more important difference froma stylictic point of view than articles from Ency-clopedia Britannica and Britannica Elementary.5 ConclusionsIn this paper, we proposed an unsupervised andcomputationally efficient graph-based local coher-ence model.
Experiments show that our model isrobust among tasks and domains, and reaches rea-sonable results for three tasks with the same pa-rameter values and settings (i.e.
accuracy valuesof 0.889, 0.70 and 0.766 for sentence ordering,summary coherence rating and readability assess-ment tasks respectively (PAcc, Dist)).
Moreover,our model can be optimized and obtains resultscomparable with entity grid based methods whenproper settings are used for each task.Our model has two main advantages over theentity grid model.
First, as the graph used for doc-ument representation contains information aboutentity transitions, our model does not need a learn-ing phase.
Second, as it relies only on graph cen-trality, our model does not suffer from the com-putational complexity and data sparsity problemsmentioned by Barzilay and Lapata (2008).Our current model leaves space for improve-ment.
Future work should first investigate the inte-gration of information about entities.
Indeed, ourmodel only uses entities as indications of sentenceconnection although it has been shown that distin-guishing important from unimportant entities, ac-cording to their named-entity category, has a pos-itive impact on local coherence computation (El-sner and Charniak, 2011).
Moreover, future workshould also examine the use of discourse relationinformation, as proposed in (Lin et al, 2011).
Thiscan be easily done by adding edges in the projec-tion graphs when sentences contain entities relatedfrom a discourse point of view while Lin et al?sapproach suffers from complexity and data spar-sity problems similar to the entity grid model.Finally, these promising results on local coher-ence modeling make us believe that our graph-based representation can be used without muchmodification for other tasks, e.g.
extractive sum-marization or topic segmentation.
This could beachieved with link analysis algorithms such asPageRank, that decide on the importance of a (sen-tence) node within a graph based on global infor-mation recursively drawn from the entire graph.Acknowledgments.
This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author has been supported by aHITS postdoctoral scholarship.
We would liketo thank Mirella Lapata and Regina Barzilay formaking their data available and Micha Elsner forproviding his toolkit.101ReferencesRegina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, Sapporo,Japan, 11?12 July 2003, pages 25?32.Regina Barzilay and Mirella Lapata.
2005.
Model-ing local coherence: An entity-based approach.
InProceedings of the 43rd Annual Meeting of the As-sociation for Computational Linguistics, Ann Arbor,Mich., 25?30 June 2005, pages 141?148.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Susan E. Brennan, Marilyn W. Friedman, and Carl J.Pollard.
1987.
A centering approach to pronouns.In Proceedings of the 25th Annual Meeting of the As-sociation for Computational Linguistics, Stanford,Cal., 6?9 July 1987, pages 155?162.Jill Burstein, Joel Tetreault, and Slava Andreyev.
2010.Using entity-based features to model coherence instudent essays.
In Proceedings of Human LanguageTechnologies 2010: The Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics, Los Angeles, Cal., 2?4 June2010, pages 681?684.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, Genoa, Italy,22?28 May 2006, pages 449?454.Micha Elsner and Eugene Charniak.
2008.Coreference-inspired coherence modeling.
InProceedings ACL-HLT 2008 Conference ShortPapers, Columbus, Ohio, 15?20 June 2008, pages41?44.Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the ACL 2011 Conference Short Papers,Portland, Oreg., 19?24 June 2011, pages 125?129.Micha Elsner, Joseph Austerweil, and Eugene Char-niak.
2007.
A unified local and global modelfor discourse coherence.
In Proceedings of Hu-man Language Technologies 2007: The Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Rochester, N.Y., 22?27 April 2007, pages 436?443.
Read this version:http://www.cs.brown.edu/ melsner/order.pdf.Katja Filippova and Michael Strube.
2007.
Ex-tending the entity-grid coherence model to seman-tically related entities.
In Proceedings of the 11thEuropean Workshop on Natural Language Genera-tion, Schloss Dagstuhl, Germany, 17?20 June 2007,pages 139?142.Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-stein.
1995.
Centering: A framework for model-ing the local coherence of discourse.
ComputationalLinguistics, 21(2):203?225.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the 8thInternational Conference on Knowledge Discoveryand Data Mining, Edmonton, Alberta, Canada, 23?26 July 2002, pages 133?142.Nikiforos Karamanis, Chris Mellish, Massimo Poesio,and Jon Oberlander.
2009.
Evaluating centering forinformation ordering using corpora.
ComputationalLinguistics, 35(1):29?46.Rodger Kibble and Richard Power.
2004.
Optimizingreferential coherence in text generation.
Computa-tional Linguistics, 30(4):401?416.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.Automatically evaluating text coherence using dis-course relations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics, Portland, Oreg., 19?24 June 2011, pages997?1006.Sebastian Martschat, Jie Cai, Samuel Broscheit, ?EvaMu?jdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012, pages 100?106.Neil McIntyre and Mirella Lapata.
2010.
Plot induc-tion and evolutionary search for story generation.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, Uppsala,Sweden, 11?16 July 2010, pages 1562?1572.Rada Mihalcea and Dragomir Radev.
2011.
Graph-based Natural Language Processing and Informa-tion Retrieval.
Cambridge Univ.
Press, Cambridge,U.K.Eleni Miltsakaki and Karen Kukich.
2004.
Evaluationof text coherence for electronic essay scoring sys-tems.
Natural Language Engineering, 10(1):25?55.Roberto Navigli and Mirella Lapata.
2010.
An ex-perimental study of graph connectivity for unsuper-vised word sense disambiguation.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(4):678?692.Mark E.J.
Newman.
2010.
Networks: An Introduction.Oxford University Press, New York, N.Y.Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-genio, and Janet Hitzeman.
2004.
Centering: Aparametric theory and its instantiations.
Computa-tional Linguistics, 30(3).
309-363.Sameer Pradhan, Alessandro Moschitti, and NianwenXue.
2012.
CoNLL-2012 Shared Task: Modelingmultilingual unrestricted coreference in OntoNotes.102In Proceedings of the Shared Task of the 16th Con-ference on Computational Natural Language Learn-ing, Jeju Island, Korea, 12?14 July 2012, pages 1?40.Sarah E. Schwarm and Mari Ostendorf.
2005.
Readinglevel assessment using support vector machines andstatistical language models.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, Ann Arbor, Mich., 25?30 June2005, pages 523?530.Radu Soricut and Daniel Marcu.
2006.
Discourse gen-eration using utility-trained coherence models.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the Association for Computational Linguistics,Sydney, Australia, 17?21 July 2006, pages 1105?1112.103
