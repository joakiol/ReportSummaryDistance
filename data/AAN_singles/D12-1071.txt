Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 777?789, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsResolving Complex Cases of Definite Pronouns:The Winograd Schema ChallengeAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractWe examine the task of resolving complexcases of definite pronouns, specifically thosefor which traditional linguistic constraintson coreference (e.g., Binding Constraints,gender and number agreement) as well ascommonly-used resolution heuristics (e.g.,string-matching facilities, syntactic salience)are not useful.
Being able to solve this task hasbroader implications in artificial intelligence:a restricted version of it, sometimes referredto as the Winograd Schema Challenge, hasbeen suggested as a conceptually and practi-cally appealing alternative to the Turing Test.We employ a knowledge-rich approach to thistask, which yields a pronoun resolver that out-performs state-of-the-art resolvers by nearly18 points in accuracy on our dataset.1 IntroductionDespite the significant amount of work on pronounresolution in the natural language processing com-munity in the past forty years, the problem is stillfar from being solved.
Its difficulty stems in partfrom its reliance on sophisticated knowledge sourcesand inference mechanisms.
The sentence pair below,which we will subsequently refer to as the shout ex-ample, illustrates how difficult the problem can be:(1a) Ed shouted at Tim because he crashed the car.
(1b) Ed shouted at Tim because he was angry.The pronoun he refers to Tim in 1a and Ed in 1b.Humans can resolve the pronoun easily, but state-of-the-art coreference resolvers cannot.
The reasonis that humans have the kind of world knowledgeneeded to resolve the pronouns that machines do not.Our world knowledge tells us that if someone is an-gry, he may shout at other people.
Since Ed shouted,he should be the one who was angry.
Our worldknowledge also tells us that we may shout at some-one who made a mistake and that crashing a car isa mistake.
Combining these two pieces of evidence,we can easily infer that Tim crashed the car.Our goal in this paper is to examine the resolu-tion of complex cases of definite pronouns that ap-pear in sentences exemplified by the shout example.Specifically, each sentence (1) has two clauses sepa-rated by a discourse connective (i.e., the connectiveappears between the two clauses, just like becausein the shout example), where the first clause con-tains two or more candidate antecedents (e.g., Edand Tim), and the second clause contains the tar-get pronoun (e.g., he); and (2) the target pronounagrees in gender, number, and semantic class witheach candidate antecedent, but does not have anyoverlap in content words with any of them.
For con-venience, we will refer to the target pronoun that ap-pears in this kind of sentences as a difficult pronoun.Note that many traditional linguistic constraintson coreference are no longer useful for resolving dif-ficult pronouns.
For instance, syntactic constraintssuch as the Binding Constraints will not be useful,since the pronoun and the candidate antecedents ap-pear in different clauses separated by a discourseconnective; and constraints concerning agreement ingender, number, and semantic class will not be use-ful, since the pronoun and the candidate antecedentsare compatible with respect to all these attributes.Traditionally important clues provided by various777I(a) The city councilmen refused the demonstrators a permit because they feared violence.I(b) The city councilmen refused the demonstrators a permit because they advocated violence.II(a) James asked Robert for a favor, but he refused.II(b) James asked Robert for a favor, but he was refused.III(a) Keith fired Blaine but he did not regret.III(b) Keith fired Blaine although he is diligent.IV(a) Emma did not pass the ball to Janie, although she was open.IV(b) Emma did not pass the ball to Janie, although she should have.V(a) Medvedev will cede the presidency to Putin because he is more popular.V(b) Medvedev will cede the presidency to Putin because he is less popular.Table 1: Sample twin sentences.
The target pronoun in each sentence is italicized, and its antecedent is boldfaced.string-matching facilities will not be useful either,since the pronoun and its candidate antecedents donot have any words in common.As in the shout example, we ensure that each sen-tence has a twin.
Twin sentences were used ex-tensively by researchers in the 1970s to illustratethe difficulty of pronoun resolution (Hirst, 1981).We consider two sentences as twins if (1) theyare identical up to and possibly including the dis-course connective; and (2) the difficult pronouns inthem are lexically identical but have different an-tecedents.
The presence of twins implies that syn-tactic salience, a commonly-used heuristic in pro-noun resolution that prefers the selection of syntac-tically salient candidate antecedents, may no longerbe useful, since the candidate in the subject positionis not more likely to be the correct antecedent thanthe other candidates.To enable the reader to get a sense of how hard it isto resolve difficult pronouns, Table 1 shows sampletwin sentences from our dataset.
Note that state-of-the-art pronoun resolvers (e.g., JavaRAP (Qiu et al2004), GuiTaR (Poesio and Kabadjov, 2004), as wellas those designed by Mitkov (2002) and Charniakand Elsner (2009)) and coreference resolvers (e.g.,BART (Versley et al2008), CherryPicker (Rahmanand Ng, 2009), Reconcile (Stoyanov et al2010),the Stanford resolver (Raghunathan et al2010; Leeet al2011)) cannot accurately resolve the difficultpronouns in these structurally simple sentences, asthey do not have the mechanism to capture the finedistinctions between twin sentences.
In other words,when given these sentences, the best that the existingresolvers can do to resolve the pronouns is guess-ing.
This could be surprising to a non-coreferenceresearcher, but it is indeed the state of the art.A natural question is: why do existing resolversnot attempt to handle difficult pronouns?
One rea-son could be that these difficult pronouns do notappear frequently in standard evaluation corporasuch as MUC, ACE, and OntoNotes (Bagga, 1998;Haghighi and Klein, 2009).
In fact, the Stanfordcoreference resolver (Lee et al2011), which wonthe CoNLL-2011 shared task on coreference resolu-tion, adopts the once-popular rule-based approach,resolving pronouns simply with rules that encodethe aforementioned traditional linguistic constraintson coreference, such as the Binding constraints andgender and number agreement.The infrequency of occurrences of difficult pro-nouns in these standard evaluation corpora by nomeans undermines their significance, however.
Infact, being able to automatically resolve difficultpronouns has broader implications in artificial intel-ligence.
Recently, Levesque (2011) has argued thatthe problem of resolving the difficult pronouns ina carefully chosen set of twin sentences, which herefers to as the Winograd Schema Challenge1, couldserve as a conceptually and practically appealingalternative to the well-known Turing Test (Turing,1Levesque (2011) defines a Winograd Schema as a smallreading comprehension test involving the question of which ofthe two candidate antecedents for the definite pronoun in a givensentence is its correct antecedent.
Levesque names this chal-lenge after Winograd because of his pioneering attempt to use awell-known pair of twin sentences ?
specifically the first pairin Table 1 ?
to illustrate the difficulty of natural language un-derstanding (Winograd, 1972).
Strictly speaking, we are ad-dressing a relaxed version of the Challenge: while Levesquefocuses solely on definite pronouns whose resolution requiresbackground knowledge not expressed in the words of a sen-tence, we do not impose such a condition on a sentence.7781950).
The reason should perhaps be clear given theabove discussion: this is an easy task for a subjectwho can ?understand?
natural language but a chal-lenging task for one who can only make intelligentguesses.
Levesque believes that ?with a very highprobability?, anything that can resolve correctly aseries of difficult pronouns ?is thinking in the full-bodied sense we usually reserve for people?.
Hence,being able to make progress on this task enables usto move one step closer to building an intelligent ma-chine that can truly understand natural language.To sum up, an important contribution of our workis that it opens up a new line of research involvinga problem whose solution requires a deeper under-standing of a text.
With recent advances in knowl-edge extraction from text, we believe that time is ripeto tackle this problem.
It is worth noting that someresearchers have focused on other kinds of anaphorsthat are hard to resolve, including bridging anaphors(e.g., Poesio et al2004)) and anaphors referringto abstract entities, such as those realized by verbphrases in dialogs (e.g., Byron (2002), Strube andMu?ller (2003), Mu?ller (2007)).
Nevertheless, to ourknowledge, there has been little work that specifi-cally targets difficult pronouns.Given the complexity of our task, we investigatea variety of sophisticated knowledge sources for re-solving difficult pronouns, and combine them via amachine learning approach.
Note that there has beena recent surge of interest in extracting world knowl-edge from online encyclopedias such as Wikipedia(e.g., Ponzetto and Strube (2006, 2007), Poesio etal.
(2007)), YAGO (e.g., Bryl et al2010), Rahmanand Ng (2011), Uryupina et al2011)), and Free-base (e.g., Lee et al2011)).
However, the resultingextractions are primarily IS-A relations (e.g., BarackObama IS-A U. S. president), which would not beuseful for resolving definite pronouns.2 Dataset CreationWe asked 30 undergraduate students who are not af-filiated with this research to compose sentence pairs(i.e., twin sentences) that conform to the constraintsspecified in the introduction.
Each student was alsoasked to annotate the candidate antecedents, the tar-get pronoun, and the correct antecedent for eachsentence she composed.
Note that a sentence maycontain multiple pronouns, but exactly one of them?
the one explicitly annotated by its author ?
isthe target pronoun.
Each sentence pair was cross-checked by one other student to ensure that it (1)conforms to the desired constraints and (2) does notcontain pronouns with ambiguous antecedents (inother words, a human should not be confused asto which candidate antecedent is the correct one).At the end of the process, 941 sentence pairs wereconsidered acceptable, and they formed our dataset.These sentences cover a variety of topics, rangingfrom real events (e.g., Iran?s plan to attack the Saudiambassador to the U.S.), to events and characters inmovies (e.g., Batman and Robin), to purely imagi-nary situations (e.g., the shout example).
We parti-tion these sentence pairs into a training set and a testset following a 70/30 ratio.While not requested by us, the students annotatedexactly two candidate antecedents for each sentence.For ease of exposition, we will assume below thatthere are two candidate antecedents per sentence.3 Machine Learning FrameworkSince our goal is to determine which of the two can-didate antecedents is the correct antecedent for thetarget pronoun in each sentence, our system assumesas input the sentence, the target pronoun, and the twocandidate antecedents.We employ machine learning to combine thefeatures derived from different knowledge sources.Specifically, we employ a ranking-based approach.Ranking-based approaches have been shown to out-perform their classification-based counterparts (De-nis and Baldridge, 2007, 2008; Iida et al2003;Yang et al2003).
Given a pronoun and two can-didate antecedents, we aim to train a ranking modelthat ranks the two candidates such that the correctantecedent is assigned a higher rank.More formally, given training sentence Sk con-taining target pronoun Ak, correct antecedent Ckand incorrect antecedent Ik, we create two featurevectors, xCAk and xIAk , where xCAk is generatedfrom Ak and Ck, and xIAk is generated from Akand Ik.
The training set consists of ordered pairsof feature vectors (xCAk , xIAk ), and the goal of thetraining procedure is to acquire a ranker that mini-mizes the number of violations of pairwise rankings779provided in the training set.
We train this ranker us-ing Joachims?
(2002) SVMlight package.
It is worthnoting that we do not exploit the fact that each sen-tence has a twin in training or testing.After training, the ranker can be applied to the testinstances, which are created in the same way as thetraining instances.
For each test instance, the targetpronoun is resolved to the higher-ranked candidateantecedent.4 Linguistic FeaturesWe derive linguistic features for resolving difficultpronouns from eight components, as described be-low.
To enable the reader to keep track of these fea-tures more easily, we summarize them in Table 2.4.1 Narrative ChainsConsider the following sentence:(2) Ed punished Tim because he tried to escape.Humans resolve he to Tim by exploiting the worldknowledge that someone who tried to escape is badand therefore should be punished.
Such kind ofknowledge can be extracted from narrative chains.Narrative chains are partially ordered sets ofevents centered around a common protagonist, aim-ing to encode the kind of knowledge provided byscripts (Schank and Abelson, 1977).
While scriptsare hand-written, narrative chains can be learnedfrom unannotated text.
Below is a chain learned byChambers and Jurafsky (2008):borrow-s invest-s spend-s pay-s raise-s lend-sAs we can see, a narrative chain is composed of asequence of events (verbs) together with the roles ofthe protagonist.
Here, ?s?
denotes the subject role,even though a chain can contain a mix of ?s?
and ?o?
(the object role).
From this chain, we know that theperson who borrows something (probably money)may invest, spend, pay, or lend it.We employ narrative chains to heuristically pre-dict the antecedent for the target pronoun, and en-code the prediction as a feature.
The heuristic de-cision procedure operates as follows.
Given a sen-tence, we first determine the event the target pro-noun participates in and its role in the event.
Asan example, we determine that in sentence (2) heparticipates in the try event and the escape eventComponent # Features FeaturesNarrative Chains 1 NCGoogle 4 G1, G2, G3, G4FrameNet 4 FN1, FN2, FN3, FN4Heuristic Polarity 3 HPOL1, HPOL2, HPOL3Learned Polarity 3 LPOL1, LPOL2, LPOL3Connective-Based 1 CBRRelationSemantic Compat.
3 SC1, SC2, SC3Lexical Features 68,331 antecedent- independentand dependent featuresTable 2: Summary of the features described in Section 4.as a subject.2 Second, we determine the event(s)that the candidate antecedents participate in.
In (2),both candidate antecedents participate in the pun-ish event.
Third, we pair each event participatedby each candidate antecedent with each event par-ticipated by the pronoun.
In our example, we wouldcreate two pairs, (punish, try-s) and (punish, escape-s).
Note that try and escape are associated with therole of the pronoun that we extracted in the first step.Fourth, for each such pair, we extract all the narra-tive chains containing both elements in the pair fromChambers and Jurafsky?s output.3 This step resultsin one chain being extracted, which contains punish-o and escape-s.
In other words, the protagonist inthis chain is the subject of an escape event and theobject of a punish event.
Fifth, from the extractedchain, we obtain the role played by the pronoun (i.e.,the protagonist) in the event in which the candidateantecedents participate.
In our example, the pronounplays an object role in the punish event.
Finally, weextract the candidate antecedent that plays the ex-tracted role, which in our example is the second an-tecedent, Tim.4We create a binary feature, NC, which encodesthis heuristic decision, and compute its value as fol-lows.
Assume in the rest of the paper that i1 andi2 are the feature vectors corresponding to the firstcandidate antecedent and the second candidate an-2Throughout the paper, the subject/object of an event refersto its deep rather than surface subject/object.
We determinethe grammatical role of an NP using the Stanford dependencyparser (de Marneffe et al2006) and a set of simple heuristics.3We employ narrative chains of length 12, which areavailable from http://cs.stanford.edu/people/nc/schemas/schemas-size12.4For an alternative way of using narrative chains for coref-erence resolution, see Irwin et al2011).780tecedent, respectively.5 For our running example,since Tim is predicted to be the antecedent of he,the value of NC in i2 is 1, and its value in i1 is 0.For notational convenience, we write NC(i1)=0 andNC(i2)=1, and will follow this convention when de-scribing the features in the rest of the paper.Finally, we note that NC(i1) and NC(i2) willboth be set to zero if (1) the pronoun and the an-tecedents do not participate in events, or (2) no nar-rative chains can be extracted in step 4 above, or (3)step 4 enables us to extract more than one chain andthese chains indicate that the candidate antecedentcan have both a subject role and an object role.4.2 GoogleConsider the following sentences:(3a) Lions eat zebras because they are predators.
(3b) The knife sliced through the flesh becauseit was sharp.Humans resolve they to Lions in (3a) by exploitingthe world knowledge that predators attack and eatother animals.
Similarly, humans resolve it to theknife in (3b) by exploiting the world knowledge thatthe word sharp can be used to describe a knife butnot flesh.
To acquire this kind of world knowledge,we learn patterns of word usage from the Web byissuing search queries.
To facilitate our discussion,let us first introduce some notation.
Let a sentenceS be denoted by a triple (Z1, Conn, Z2), where Z1and Z2 are the clauses preceding and following thediscourse connective Conn, respectively; A ?
Z2be the pronoun governed by the verb V ; W be thesequence of words following V in S; and C1, C2 ?Z1 be the candidate antecedents.Given a sentence, we generate four queries: (Q1)C1V ; (Q2) C2V ; (Q3) C1V W ; and (Q4) C2V W .
Ifv is a verb-to-be followed by an adjective J , we gen-erate two more queries: (Q5) JC1 and (Q6) JC2.To exemplify, six queries are generated for (3b):(Q1) ?knife was?
; (Q2) ?flesh was?
; (Q3) ?knife wassharp?
; (Q4) ?flesh was sharp?
; (Q5) ?sharp knife?
;and (Q6) ?sharp flesh?.
On the other hand, only fourqueries are generated for (3a): (Q1) ?lions are?
; (Q2)5The nth candidate antecedent in a sentence is the nth an-notated NP encountered when processing the sentence in a left-to-right manner.
In sentence (2), Ed is the first candidate an-tecedent and Tim is the second.
?zebras are?
; (Q3) ?lions are predators?
; and (Q4)?zebras are predators?.Using the counts returned by Google for thesequeries, we create four features, G1, G2, G3, andG4, whose values are determined by Rules 1, 2, 3,and 4, respectively, as described below.Rule 1: if count(Q1) > count(Q2) by atleast x% then G1(i1)=1 and G1(i2)=0;else if count(Q2) > count(Q1) by at leastx% then G1(i2)=1 and G1(i1)=0; elseG1(i1)=G1(i2)=0.Rule 2: if count(Q3) > count(Q4) by atleast x% then G2(i1)=1 and G2(i2)=0;else if count(Q4) > count(Q3) by at leastx% then G2(i2)=1 and G2(i1)=0; elseG2(i1)=G2(i2)=0.Rule 3: if count(Q5) > count(Q6) by atleast x% then G3(i1)=1 and G3(i2)=0;else if count(Q6) > count(Q5) by at leastx% then G3(i2)=1 and G3(i1)=0; elseG3(i1)=G3(i2)=0.Rule 4: if one of G1(i1) and G1(i2) is 1,then G4(i1)=G1(i1) and G4(i2)=G1(i2);else if one of G2(i1) and G2(i2) is 1,then G4(i1)=G2(i1) and G4(i2)=G2(i2);else if one of G3(i1) and G3(i2) is 1,then G4(i1)=G3(i1) and G4(i2)=G3(i2);else G4(i1)=G4(i2)=0.The role of the threshold x should be obvious: itensures that a heuristic decision is made only if thedifference between the counts for the two queries aresufficiently large, because otherwise there is no rea-son for us to prefer one candidate antecedent to theother.
In all of our experiments, we set x to 20.Note that other researchers have also used lexico-syntactic patterns to generate search queries forbridging anaphora resolution (e.g., Poesio et al(2004)), other-anaphora resolution (e.g., Modjeskaet al2003)), and learning selectional preferencesfor pronoun resolution (e.g., Yang et al2005)).However, in each of these three cases, the target re-lations (e.g., the part-whole relation in the case ofbridging anaphora resolution, and the subject-verband verb-object relations in the case of selectionalpreferences) are specific enough that they can be ef-fectively captured by specific patterns.
For example,781to determine whether the wheel is part of the car inbridging anaphora resolution, Poesio et almployqueries of the form ?X of Y?, where X and Y wouldbe replaced with the wheel and the car, respectively.On the other hand, we are not targeting a particulartype of relation.
Rather, we intend to capture worldknowledge like lions rather than zebras are preda-tors.
Such knowledge may not be expressed as arelation and hence may not be easily captured usingspecific patterns.
For this reason, we need to employpatterns as general as those such as Q3 and Q4.4.3 FrameNetIf we generate search queries as described in the pre-vious subsection for the shout example, it is unlikelythat Google will return meaningful counts to us.
Thereason is that both candidate antecedents in the sen-tence are proper names belonging to the same type(which in this case is PERSON).However, in some cases, we may be able to gener-ate more meaningful queries from such kind of sen-tences.
Consider the following sentence:(4) John killed Jim, so he was arrested.To generate meaningful queries, we make one ob-servation: John and Jim played different roles in akill event.
Hence, we can replace these proper nameswith their roles.
We propose to obtain these rolesfrom FrameNet (Baker et al1998).
More gener-ally, for each proper name e in a given sentence, we(1) determine the event in which e is involved (usingthe Stanford dependency parser); (2) search for theFrameNet frame corresponding to the event as wellas e?s role in the event; and (3) replace the namewith its FrameNet role.
In our example, since bothnames are involved in the kill event, we retrieve theFrameNet frame for kill.
Given that John and Jim arethe subject and object of kill, we can extract their se-mantic roles directly from the frame, which are killerand victim, respectively.6 Consequently, we replacethe two names with their extracted semantic roles,and generate the search queries from the resultingsentence in the same way as before.Note that if no frames can be found for the verb inthe first clause, no search queries will be generated.After obtaining the query counts, we generate fourbinary features, FN1, FN2, FN3, FN4, whose values6We heuristically map grammatical roles to semantic roles.are computed based on the same four heuristic rulesthat were discussed in the previous subsection.4.4 Heuristic PolaritySome sentences involve comparing the two candi-date antecedents.
Consider the following sentences:(5a) John was defeated by Jim in the electioneven though he is more popular.
(5b) John was defeated by Jim in the electionbecause he is more popular.The pronoun he refers to John in (5a) and Jim in(5b).
To see how we can design an algorithm for re-solving these pronouns, it would be useful to under-stand how humans resolve them.
The phrase morepopular has a positive sentiment.
In (5a), the useof even though yields a clause of concession, whichflips the polarity of more popular (from positive tonegative), whereas in (5b), the use of because yieldsa clause of cause, which does not change the po-larity of more popular (i.e., more popular remainspositive).
Since more popular is used to describe he,he is ?better?
in (5b) but ?worse?
in (5a).
Now, theword defeat has a positive sentiment, and since Jimis the deep subject of defeat, Jim is ?better?
and Johnis ?worse?.
Finally, in (5b), he and Jim are ?better?,so he is resolved to Jim; on the other hand, in (5a),he and John are ?worse?, so he is resolved to John.We automate this (human) method for resolv-ing pronouns as follows.
We begin by determin-ing whether we can assign a rank value (i.e., ?bet-ter?
or ?worse?)
to the pronoun and the two can-didate antecedents.
For instance, to determine therank value of the pronoun A, we first determine thepolarity value pA of its anchor word wA, which iseither the verb v for which A serves as the deep sub-ject, or the adjective modifying A if v does not ex-ist,7 using Wilson et al (2005b) subjectivity lex-icon.8 If pA is not NEUTRAL, we check whetherit can be flipped by the context of wA.
We con-sider three kinds of polarity-reversing context: nega-tion, comparative adverb, and discourse connective.Specifically, we determine whether wA is negatedusing the Stanford dependency parser, which explic-7In the sentiment analysis and opinion mining literature,(wA, pA) is known as an opinion-target pair.8The lexicon contains 8221 words, each of which is handlabeled with a polarity of POSITIVE, NEGATIVE, or NEUTRAL.782itly annotates instances of negation; we determinethe existence of a comparative adverb (e.g., ?more?,?less?)
using the POS tag ?RBR?
; and we determinewhether A exists in a clause headed by a polarity-reversing connective, such as although.
After flip-ping pA by context, we can infer A?s rank value fromit.
Specifically, A?s rank value is ?better?
if pA ispositive; ?worse?
if pA is negative; and ?cannot bedetermined?
if pA is neutral.
The polarity values ofthe two candidate antecedents can be determined ina similar fashion.
Note that sometimes we may needto infer rank values.
For example, given the sentence?Jane is prettier than Jill?, prettier has a positive po-larity, so its modifying NP, Jane, has a ?better?
rank,and we can infer that Jill?s rank is ?worse?.We create three features, HPOL1, HPOL2, andHPOL3, based on our heuristic polarity determina-tion component.
Specifically, if the rank value ofthe pronoun or the rank value of one or both of thecandidate antecedents cannot be determined, the val-ues of all three binary features will be set to zerofor both i1 and i2.
Otherwise, we compute the val-ues of the three features as follows.
To computeHPOL1, which is a binary feature, we (1) employa heuristic resolution procedure, which resolves thepronoun to the candidate antecedent with the samerank value, and then (2) encode the outcome of thisheuristic procedure as the value of HPOL1.
For ex-ample, since the first candidate antecedent, John, ispredicted to be the antecedent in (5a), HPOL1(i1)=1and HPOL1(i2)=0.
The value of HPOL2 is theconcatenation of the polarity values determinedfor the pronoun and the candidate antecedent.Referring again to (5a), HPOL2(i1)=positive-positive and HPOL2(i2)=positive-negative.
Tocompute HPOL3 for a given instance, we sim-ply take its HPOL2 value and append theconnective to it.
Using (5a) as an exam-ple, HPOL3(i1)=positive-positive-even-though andHPOL3(i1)=positive-negative-even-though.4.5 Machine-Learned PolarityIn the previous subsection, we compute the polarityof a word by updating its prior polarity heuristicallywith contextual information.
We hypothesized thatpolarity could be computed more accurately by em-ploying a sentiment analyzer that can capture richercontextual information.
For this reason, we employOpinionFinder (Wilson et al2005a), which has apre-trained classifier for annotating the phrases in asentence with their contextual polarity values.Given a sentence and the polarity values of thephrases annotated by OpinionFinder, we determinethe rank values of the pronoun and the two candi-date antecedents by mapping them to the polarizedphrases using the dependency relations provided bythe Stanford dependency parser.
We create three bi-nary features, LPOL1, LPOL2, and LPOL3, whosevalues are computed in the same way as HPOL1,HPOL2, and HPOL3, respectively, except that thecomputation here is based on the machine-learnedpolarity values rather than the heuristically deter-mined polarity values.4.6 Connective-Based RelationsConsider the following sentences:(6a) Google bought Motorola because theywant its customer base.
(6b) Google bought Motorola because theyare rich.Humans resolve they to Google in (6a) by exploit-ing the world knowledge that there is a causal rela-tion (signaled by the discourse connective because)between the want event and the buy event.
A simi-lar mechanism is used to resolve they to Google in(6b): from world knowledge we know that there is acausal relation between rich and buy.We automate this (human) method for resolvingpronouns as follows.
First, we gather connective-based relations of this kind from a large, unanno-tated corpus.
In our experiments, we use as ourunannotated corpus the documents in three text cor-pora (namely, BLLIP, Reuters, and English Giga-word), but retain only those sentences that con-tain a single discourse connective and do not be-gin with the connective.
From these sentences,we collect triples and their frequencies of occur-rences in the corpus.
Each triple is of the form(V ,Conn,X), where Conn is a discourse connec-tive, V is a stemmed verb in the clause precedingConn, and X is a stemmed verb or an adjective inthe clause following Conn. Each triple essentiallydenotes a relation between V and X expressed byConn.
Conceivably, the strength of the relation in atriple increases with its frequency count.783We use the frequency counts of these triples toheuristically predict the correct antecedent for a tar-get pronoun.
Given a sentence where Conn is thediscourse connective, X is the stemmed verb gov-erning the target pronoun A or the adjective modify-ing A (if X is a to be verb), and V is the stemmedverb governing the candidate antecedents, we re-trieve the frequency count of the triple (V ,Conn,X).If the count is at least 100, we employ a procedurefor heuristically selecting the antecedent for the tar-get anaphor.
Specifically, if X is a verb, then it re-solves the target pronoun to the candidate antecedentthat has the same grammatical role as the pronoun.However, if X is an adjective and the sentence doesnot involve comparison, then it resolves the targetpronoun to the candidate antecedent serving as thesubject of V .We create a binary feature, CBR, that encodesthis heuristic decision.
In our running example, thetriple (buy, because, want) occurs 860 times in ourcorpus, so the pronoun they is resolved to the can-didate antecedent that occurs as the subject of buy.Hence, CBR(i1)=1 and CBR(i2)=0.
However, hadthe triple occurred less than 100 times, both of thesefeatures would have been set to zero.4.7 Semantic CompatibilitySome of the queries generated by the Google com-ponent, such as Q1 and Q2, aim to capture thesemantic compatibility between a candidate an-tecedent, C , and the verb governing the target pro-noun, V .
However, using web search queries to esti-mate semantic compatibility has potential problems,including (1) a precision problem: the fact that Cand V appear next to each other in a query doesnot necessarily imply that a subject-verb relation ex-ists between them; and (2) a recall problem: thesequeries fail to capture subject-verb relations whereC and V are not immediately adjacent to each other.To address these potential problems, we com-pute knowledge of selectional preferences from alarge, unannotated corpus.
As before, we cre-ate our unannotated corpus using the documents inBLLIP, Reuters, and English Gigaword.
Specifi-cally, we first parse each sentence in the corpus us-ing the Stanford dependency parser.
Then, for eachstemmed verb v and each stemmed noun n in thecorpus, we collect the following statistics: (1) thenumber of times n is the subject of v; (2) the num-ber of times n is the direct object of v; (3) the mutualinformation (MI) of v and n (with n as the subjectof v); and (4) the MI of v and n (with n as the directobject of v).9To understand how we use these statistics to gen-erate features for resolving pronouns, consider thefollowing sentence:(7) The man stole the neighbor?s bike becausehe needed one.Assuming that the target pronoun and its govern-ing verb V has grammatical relation GR, we createthree features, SC1, SC2, and SC3, based on our se-mantic compatibility component.
SC1 encodes theMI value of the head noun of a candidate antecedentand V (and GR).
SC2 is a binary feature whosevalue indicates which of the candidate antecedentshas a larger MI value with V (and GR).
SC3 is thesame as SC2, except that MI is replaced with corpusfrequency.
In other words, SC2 and SC3 employdifferent measures to heuristically predict the cor-rect antecedent for the target pronoun.
If the targetpronoun is governed by a to be verb, the values ofthese three features will all be set to zero.Given our running example, we first retrievethe following corpus-based statistics: MI(need:subj,man)=0.6322; MI(need:subj, neighbor)=0.3975;count(need:subj, man)=474; and count(need:subj,neighbor)=68.
Using these statistics, we can thencompute the aforementioned features for our exam-ple.
Specifically, SC1(i1)=0.6322, SC1(i2)=0.3975,SC2(i1)=1, SC2(i2)=0, SC3(i1)=1, and SC3(i2)=0.4.8 Lexical FeaturesWe exploit the coreference-annotated training docu-ments by creating lexical features from them.
Theselexical features can be divided into two categories,depending on whether they are computed based onthe candidate antecedents.Let us begin with the antecedent-independent fea-tures.
Assuming that W is an arbitrary word in asentence S that is not part of a candidate antecedentand Conn is the connective in S, we create threetypes of binary-valued antecedent-independent fea-tures, namely (1) unigrams, where we create one9We use the same formula as described in Section 4.2 ofBergsma and Lin (2006) to compute MI values.784feature for each W ; (2) word pairs, where we cre-ate features by pairing each W appearing beforeConn with each W appearing after Conn, exclud-ing adjective-noun and noun-adjective pairs10; and(3) word triples, where we augment each word pairin (2) with Conn.
The value of each feature f indi-cates the presence or absence of f in S.Next, we compute the antecedent-dependent fea-tures.
Let (1) HC1 and HC2 be the head words ofcandidate antecedents C1 and C2, respectively; (2)VC1 , VC2 , and VA be the verbs governing C1, C2,and the target pronoun A, respectively; and (3) JC1 ,JC2 , and JA be the adjectives modifying C1, C2, andA, respectively.11 We create from each candidate an-tecedent four features, each of which is a word pair.From C1, we create (HC1 , VC1), (HC1 , JC1 ), (HC1 ,VA), and (HC1 , JA), all of which will appear in thefeature vector corresponding to C1.
A similar set offour features are created from C2.
These antecedent-dependent features are all binary-valued.It is worth mentioning that while we also consid-ered word triples in the connective-based relationscomponent and word pairs in the semantic compat-ibility component, in those components we deter-mine their usefulness in an unsupervised manner,whereas by employing them as lexical features wedetermine their usefulness in a supervised manner.5 Evaluation5.1 Experimental SetupDataset.
We report results on the test set, whichcomprises 30% of our hand-annotated sentence pairs(see Section 2 for details).Evaluation metrics.
Results are expressed interms of accuracy, which is the percentage of cor-rectly resolved target pronouns.
We also report thepercentages of these pronouns that are (1) not re-solved and (2) incorrectly resolved.5.2 Results and DiscussionThe Random baseline.
Our first baseline is a re-solver that randomly guesses the antecedent for the10Pairing an adjective A in one clause with a noun N in an-other clause may mislead the learner into thinking that N ismodified by A, and hence we do not create such pairs.11If C1, C2, and A are not modified by adjectives, noadjective-based features will be created.target pronoun in each sentence.
Since there aretwo candidate antecedents per sentence, the Randombaseline should achieve an accuracy of 50%.The Stanford resolver.
Our second baseline is theStanford resolver (Lee et al2011), which achievesthe best performance in the CoNLL 2011 shared task(Pradhan et al2011).
As a rule-based resolver, itdoes not exploit any coreference-annotated data.Recall from Section 3 that our system assumes asinput not only a sentence containing a target pronounbut also the two candidate antecedents.
To ensure afair comparison, the same input is provided to thisand other baselines.
Hence, if the Stanford resolverdecides to resolve the target pronoun, it will resolveit to one of the two candidate antecedents.
However,if it does not have enough confidence about resolv-ing it, it will leave it unresolved.
Its performance onthe test set is shown in the ?Unadjusted Scores?
col-umn in row 1 of Table 3.
As we can see, it correctlyresolves 40.1% of the pronouns, incorrectly resolves29.8% of them, and does not make any decision onthe remaining 30.1%.Given that the Random baseline correctly resolves50% of pronouns and the Stanford resolver correctlyresolves only 40.1% of the pronouns, it is temptingto conclude that Stanford does not perform as wellas Random.
However, recall that Stanford leaves30.1% of the pronouns unresolved.
Hence, to ensurea fairer comparison, we produce ?adjusted?
scoresfor the Stanford resolver, where we ?force?
it to re-solve all of the unresolved target pronouns by as-suming that probabilistically half of them will be re-solved correctly.
This adjusted score is shown in the?Adjusted Scores?
column in row 1 of Table 3.
Aswe can see, Stanford achieves an accuracy of 55.1%,which is 5.1 points higher than that of Random.The Baseline Ranker.
To understand whether thesomewhat unsatisfactory Stanford results can be at-tributed to its inability to exploit the training data,we employ as our third baseline a mention rankerthat is trained in the same way as our system (seeSection 3), except that it employs 39 commonly-used linguistic features for learning-based corefer-ence resolution (see Table 1 of Rahman and Ng(2009) for a description of these features).
Hence,the performance difference between this BaselineRanker and our system can be attributed entirely785Unadjusted Scores Adjusted ScoresCoreference System Correct Wrong No Decision Correct Wrong No Decision1 Stanford 40.07% 29.79% 30.14% 55.14% 44.86% 0.00%2 Baseline Ranker 47.70% 47.16% 5.14% 50.27% 49.73% 0.00%3 Stanford+Baseline Ranker 53.49% 43.12% 3.39% 55.19% 44.77% 0.00%4 Our system 73.05% 26.95% 0.00% 73.05% 26.95% 0.00%Table 3: Results of the Stanford resolver, the Baseline Ranker, the Combined resolver, and our system.to the difference between the two linguistic featuresets.
Results of the Baseline Ranker are shown inrow 2 of Table 3.
Before score adjustment, it cor-rectly resolves 47.7% of the target pronouns, incor-rectly resolves 47.2% of them, and leaves the re-maining 5.1% unresolved.
(Note that we output ?nodecision?
if the ranker assigns the same rank valueto both candidate antecedents.)
After score adjust-ment, its accuracy is 50.3%, which is 0.3 pointshigher than that of Random but statistically indis-tinguishable from it.12 On the other hand, its accu-racy is 4.9 points lower than that of Stanford, andthe difference between their performance is signifi-cant.
While it seems somewhat surprising that a su-pervised resolver does not perform as well as a rule-based resolver, neither of them employs knowledgesources that are particularly useful for our dataset.
Inother words, despite given access to annotated data,the Baseline Ranker may not be able to make effec-tive use of it due to the lack of useful features.The Combined resolver.
We create a fourth base-line by combining the Stanford resolver and theBaseline Ranker.
The motivation is that the formercan provide better precision and the latter can pro-vide better recall by handling ?no decision?
casesnot covered by the former.
Note that the BaselineRanker will be applied to resolve only those pro-nouns that are left unresolved by Stanford.
Resultsin row 3 of Table 3 show that the adjusted accuracyof this Combined resolver is 55.2%, which is sta-tistically indistinguishable from Stanford?s adjustedaccuracy.
Hence, these results show that the addi-tion of the Baseline Ranker does not help improveStanford?s resolution accuracy.Our system.
Results of our system, which istrained using the features described in Section 4 incombination with a ranking model, are shown inrow 4 of Table 3.
As we can see, our system achieves12All statistical significance test results in this paper are ob-tained using the paired t-test, with p < 0.05.Feature Type Correct Wrong No DecisionAll features 73.05% 26.95% 0.00%?Narrative Chains 68.97% 31.03% 0.00%?Google 65.96% 34.04% 0.00%?FrameNet 72.16% 27.84% 0.00%?Heuristic Polarity 71.45% 28.55% 0.00%?Learned Polarity 72.70% 27.30% 0.00%?Connective-Based Rel.
71.28% 28.72% 0.00%?Semantic Compat.
71.81% 28.19% 0.00%?Lexical Features 60.11% 25.35% 14.54%Table 4: Results of feature ablation experiments.an accuracy of 73.1%, significantly outperformingthe Combined resolver by 17.9 points in accuracy.These results suggest that our features are more use-ful for resolving difficult pronouns than those com-monly used for coreference resolution.5.3 Feature AnalysisIn an attempt to gain additional insight into the per-formance contribution of each of the eight types offeatures used in our system, we conduct feature ab-lation experiments.
The unadjusted scores of theseexperiments are shown in Table 4, where each rowshows the performance of the model trained on alltypes of features except for the one shown in thatrow.
For easy reference, the performance of themodel trained on all types of features is shown inrow 1 of the table.A few points deserve mention.
First, perfor-mance drops significantly whichever feature type isremoved.
This suggests that all eight feature typesare contributing positively to overall accuracy.
Sec-ond, the Google-based features and the Lexical Fea-tures are the most useful, and those generated viaFrameNet and Learned Polarity are the least use-ful in the presence of other feature types.
While itis somewhat surprising that Learned Polarity is notmore useful than Heuristic Polarity, we speculatethe reason can be attributed to the fact that the cor-pus on which OpinionFinder was trained was quitedifferent from ours.
Finally, even without using the786Feature Type Correct Wrong No DecisionNarrative Chains 30.67% 24.47% 44.86%Google 33.16% 7.09% 59.75%FrameNet 7.27% 4.08% 88.65%Learned Polarity 4.79% 2.66% 92.55%Heuristic Polarity 7.27% 1.77% 90.96%Connective-Based Rel.
14.01% 8.69% 77.30%Semantic Compat.
23.58% 13.12% 63.30%Lexical Features 56.91% 43.09% 0.00%Table 5: Results of single-feature coreference models.Lexical Features, our system still outperforms all thebaseline resolvers: as can been implied from the lastrow of Table 4, in the absence of the Lexical Fea-tures, our resolver achieves an adjusted accuracy of67.4%, which is only 5.7 points less than that ob-tained when the full feature set is employed.
Hence,while the Lexical Features are useful, their impor-tance should not be over-emphasized.To get a better idea of the utility of each featuretype, we conduct another experiment in which wetrain eight models, each of which employs exactlyone type of features.
Their unadjusted scores areshown in Table 5.
As we can see, Learned Polarityhas the smallest contribution, whereas the LexicalFeatures have the largest contribution.5.4 Error AnalysisWhile our resolver significantly outperforms state-of-the-art resolvers, there is a lot of room for im-provement.
To help direct future research on the res-olution of difficult pronouns, we analyze the majorsources of errors made by our resolver.Our analysis reveals that many of the errors cor-respond to cases that cannot be handled by any ofthe eight components of our resolver.
To understandthese cases, consider first the strengths and weak-nesses of Narrative Chains and Google, the twocomponents that contribute the most to overall per-formance after Lexical Features.Google is especially good at capturing facts, suchas lions are predators and zebras are not predators,helping us correctly resolve sentences such as (5a)and (5b), as well as those in sentence pair (I) in Ta-ble 1.
However, it may not be good at handling pro-nouns whose resolution requires an understanding ofthe connection between the facts or events describedin the two clauses of a sentence.
The reason is thatestablishing such a connection requires that we con-struct a search query composed of information ex-tracted from both clauses, and the resulting, possi-bly long, query is likely to receive no hit count dueto data sparseness.
Investigating how to constructsuch queries while avoiding data sparseness wouldbe an interesting line of future work.Narrative chains, on the other hand, are usefulfor capturing the relationship between the events de-scribed in the two clauses.
However, they are com-puted over verbs, and therefore cannot capture sucha relationship when one or both of the events in-volved are not described by verbs.
For example,narrative chains fail to capture the causal relationbetween the event expressed by angry and shout insentence (1b).
It is also worth mentioning that somepronouns that could have been resolved using nar-rative chains are not owing to the coverage and ac-curacy of Chambers and Jurafsky?s (2008) chains,but we believe that these recall and precision prob-lems could be addressed by (1) inducing chains froma larger corpus and (2) using semantic roles ratherthan grammatical roles in the induction process.Some resolution errors arise from errors in polar-ity analysis.
This can be attributed to the simplicityof our Heuristic Polarity component: determiningthe polarity of a word based on its prior polarity istoo na??ve.
Fine-grained polarity analysis would bea promising solution to this problem (see Pang andLee (2008) and Liu (2012) for related work).6 ConclusionsWe investigated the resolution of complex cases ofdefinite pronouns, a problem that was under exten-sive discussion by coreference researchers in the1970s but has received revived interest owing in partto its relevance to the Turing Test.
Our experimentalresults indicate that it is a challenge for state-of-the-art resolvers, and while we proposed new knowledgesources for addressing this challenge, our resolverstill has a lot of room for improvement.
In partic-ular, our error analysis indicates that further gainscould be achieved via more accurate sentiment anal-ysis and induction of world knowledge from corporaor the Web.
In addition, we plan to integrate ourresolver into a general-purpose coreference systemand evaluate the resulting resolver on standard eval-uation corpora such as MUC, ACE, and OntoNotes.787AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlier draftof the paper.
This work was supported in part byNSF Grants IIS-0812261 and IIS-1147644.ReferencesAmit Bagga.
1998.
Coreference, Cross-DocumentCoreference, and Information Extraction Methodolo-gies.
Ph.D. thesis, Duke University.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 36th Annual Meeting of the Association forComputational Linguistics and the 17th InternationalConference on Computational Linguistics, pages 86?90.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 33?40.Volha Bryl, Claudio Guiliano, Luciano Serafini, andKateryna Tymoshenko.
2010.
Using backgroundknowledge to support coreference resolution.
In Pro-ceedings of the 19th European Conference on ArtificialIntelligence, pages 759?764.Donna K. Byron.
2002.
Resolving pronominal referenceto abstract entities.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics, pages 80?87.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of the 46th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 787?797.Eugene Charniak and Micha Elsner.
2009.
EM works forpronoun anaphora resolution.
In Proceedings of the12th Conference of the European Chapter of the Asso-ciation for Computational Linguistics, pages 148?156.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of the 5th International Conference on Lan-guage Resources and Evaluation, pages 449?454.Pascal Denis and Jason Baldridge.
2007.
A ranking ap-proach to pronoun resolution.
In Proceedings of theTwentieth International Conference on Artificial Intel-ligence, pages 1588?1593.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1152?1161.Graeme Hirst.
1981.
Anaphora in Natural LanguageUnderstanding.
Springer Verlag.Ryu Iida, Kentaro Inui, Hiroya Takamura, and Yuji Mat-sumoto.
2003.
Incorporating contextual cues in train-able models for coreference resolution.
In Proceed-ings of the EACL Workshop on The ComputationalTreatment of Anaphora.Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.2011.
Narrative schema as world knowledge forcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 86?92.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 133?142.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28?34.Hector J. Levesque.
2011.
The Winograd Schema Chal-lenge.
In AAAI Spring Symposium: Logical Formal-izations of Commonsense Reasoning.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool Publishers.Ruslan Mitkov, Richard Evans, and Constantin Orasan.2002.
A new, fully automatic version of Mitkov?sknowledge-poor pronoun resolution method.
In Al.Gelbukh, editor, Computational Linguistics and Intel-ligent Text Processing, pages 169?187.
Springer.Natalia N. Modjeska, Katja Markert, and Malvina Nis-sim.
2003.
Using the web in machine learning forother-anaphora resolution.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, pages 176?183.Christoph Mu?ller.
2007.
Resolving it, this, and that inunrestricted multi-party dialog.
In Proceedings of the45th Annual Meeting of the Association of Computa-tional Linguistics, pages 816?823.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval 2(1?2):1?135.Massimo Poesio and Mijail A. Kabadjov.
2004.A general-purpose, off-the-shelf anaphora resolutionmodule: Implementation and preliminary evaluation.In Proceedings of the 4th International Conference onLanguage Resources and Evaluation, pages 663?666.788Massimo Poesio, Rahul Mehta, Axel Maroudas, andJanet Hitzeman.
2004.
Learning to resolve bridgingreferences.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,pages 143?150.Massimo Poesio, David Day, Ron Artstein, Jason Dun-can, Vladimir Eidelman, Claudio Giuliano, Rob Hall,Janet Hitzeman, Alan Jern, Mijail Kabadjov, StanleyYong Wai Keong, Gideon Mann, Alessandro Mos-chitti, Simone Ponzetto, Jason Smith, Josef Stein-berger, Michael Strube, Jian Su, Yannick Versley,Xiaofeng Yang, and Michael Wick.
2007.
EL-ERFED: Final report of the research group on Exploit-ing Lexical and Encyclopedic Resources For EntityDisambiguation.
Technical report, Summer Workshopon Language Engineering, Center for Language andSpeech Processing, Johns Hopkins University, Balti-more, MD.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedingsof the Human Language Technology Conference andConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 192?199.Simone Paolo Ponzetto and Michael Strube.
2007.Knowledge derived from Wikipedia for computing se-mantic relatedness.
Journal of Artificial IntelligenceResearch, 30:181?212.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1?27.Long Qiu, Min-Yen Kan, and Tat-Seng Chua.
2004.
Apublic reference implementation of the RAP anaphoraresolution algorithm.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation, pages 291?294.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-sky, and Christopher Manning.
2010.
A multi-passsieve for coreference resolution.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 492?501.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977.Altaf Rahman and Vincent Ng.
2011.
Coreference res-olution with world knowledge.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 814?824.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals, and Understanding.
Lawrence Erl-baum.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
REC-ONCILE: A coreference resolution research platform.In Proceedings of the ACL 2010 Conference Short Pa-pers, pages 156?161.Michael Strube and Christoph Mu?ller.
2003.
A machinelearning approach to pronoun resolution in spoken di-alogue.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages168?175.Alan M. Turing.
1950.
Computing machinery and intel-ligence.
Mind, 59:433?460.Olga Uryupina, Massimo Poesio, Claudio Giuliano, andKateryna Tymoshenko.
2011.
Disambiguation andfiltering methods in using Web knowledge for coref-erence resolution.
In Proceedings of the 24th Interna-tional Florida Artificial Intelligence Research SocietyConference, pages 317?322.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Proceedings of the ACL-08: HLT Demo Session,pages 9?12.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi,Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.2005a.
Opinionfinder: A system for subjectivity anal-ysis.
In Proceedings of HLT/EMNLP 2005 InteractiveDemonstrations, pages 34?35.Theresa Wilson, Janyce M. Wiebe, and Paul Hoffmann.2005b.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the JointHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 347?354.Terry Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press, Inc., New York.Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitivelearning approach.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 176?183.Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2005.
Im-proving pronoun resolution using statistics-based se-mantic compatibility information.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 165?172.789
