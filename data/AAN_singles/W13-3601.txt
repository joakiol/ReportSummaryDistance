Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1?12,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsThe CoNLL-2013 Shared Task on Grammatical Error CorrectionHwee Tou NgDepartment of Computer ScienceNational University of Singaporenght@comp.nus.edu.sgSiew Mei WuCentre for English Language CommunicationNational University of Singaporeelcwusm@nus.edu.sgYuanbin Wu and Christian HadiwinotoDepartment of Computer ScienceNational University of Singapore{wuyb,chrhad}@comp.nus.edu.sgJoel TetreaultNuance Communications, Inc.Joel.Tetreault@nuance.comAbstractThe CoNLL-2013 shared task was devotedto grammatical error correction.
In thispaper, we give the task definition, presentthe data sets, and describe the evaluationmetric and scorer used in the shared task.We also give an overview of the variousapproaches adopted by the participatingteams, and present the evaluation results.1 IntroductionGrammatical error correction is the shared taskof the Seventeenth Conference on ComputationalNatural Language Learning in 2013 (CoNLL-2013).
In this task, given an English essay writtenby a learner of English as a second language, thegoal is to detect and correct the grammatical errorspresent in the essay, and return the corrected essay.This task has attracted much recent research in-terest, with two shared tasks Helping Our Own(HOO) 2011 and 2012 organized in the past twoyears (Dale and Kilgarriff, 2011; Dale et al2012).
In contrast to previous CoNLL shared taskswhich focused on particular subtasks of naturallanguage processing, such as named entity recog-nition, semantic role labeling, dependency pars-ing, or coreference resolution, grammatical errorcorrection aims at building a complete end-to-endapplication.
This task is challenging since formany error types, current grammatical error cor-rection systems do not achieve high performanceand much research is still needed.
Also, tacklingthis task has far-reaching impact, since it is esti-mated that hundreds of millions of people world-wide are learning English and they benefit directlyfrom an automated grammar checker.The CoNLL-2013 shared task provides a forumfor participating teams to work on the same gram-matical error correction task, with evaluation onthe same blind test set using the same evaluationmetric and scorer.
This overview paper contains adetailed description of the shared task, and is orga-nized as follows.
Section 2 provides the task def-inition.
Section 3 describes the annotated trainingdata provided and the blind test data.
Section 4 de-scribes the evaluation metric and the scorer.
Sec-tion 5 lists the participating teams and outlines theapproaches to grammatical error correction usedby the teams.
Section 6 presents the results of theshared task.
Section 7 concludes the paper.2 Task DefinitionThe goal of the CoNLL-2013 shared task is toevaluate algorithms and systems for automati-cally detecting and correcting grammatical errorspresent in English essays written by second lan-guage learners of English.
Each participatingteam is given training data manually annotatedwith corrections of grammatical errors.
The testdata consists of new, blind test essays.
Prepro-cessed test essays, which have been sentence-segmented and tokenized, are also made availableto the participating teams.
Each team is to submitits system output consisting of the automaticallycorrected essays, in sentence-segmented and tok-enized form.Grammatical errors consist of many differenttypes, including articles or determiners, preposi-tions, noun form, verb form, subject-verb agree-ment, pronouns, word choice, sentence structure,punctuation, capitalization, etc.
Of all the er-ror types, determiners and prepositions are among1the most frequent errors made by learners of En-glish.
Not surprisingly, much published researchon grammatical error correction focuses on arti-cle and preposition errors (Han et al 2006; Ga-mon, 2010; Rozovskaya and Roth, 2010; Tetreaultet al 2010; Dahlmeier and Ng, 2011b), with rel-atively less work on correcting word choice errors(Dahlmeier and Ng, 2011a).
Article and preposi-tion errors were also the only error types featuredin the HOO 2012 shared task.
Likewise, althoughall error types were included in the HOO 2011shared task, almost all participating teams dealtwith article and preposition errors only (besidesspelling and punctuation errors).In the CoNLL-2013 shared task, it was feltthat the community is now ready to deal withmore error types, including noun number, verbform, and subject-verb agreement, besides arti-cles/determiners and prepositions.
Table 1 showsexamples of the five error types in our shared task.Since there are five error types in our shared taskcompared to two in HOO 2012, there is a greaterchance of encountering multiple, interacting errorsin a sentence in our shared task.
This increases thecomplexity of our shared task relative to that ofHOO 2012.
To illustrate, consider the followingsentence:Although we have to admit some badeffect which is brought by the newtechnology, still the advantages of thenew technologies cannot be simply dis-carded.The noun number error effect needs to be corrected(effect?
effects).
This necessitates the correctionof a subject-verb agreement error (is ?
are).
Apipeline system in which corrections for subject-verb agreement errors occur strictly before correc-tions for noun number errors would not be ableto arrive at a fully corrected sentence for this ex-ample.
The ability to correct multiple, interactingerrors is thus necessary in our shared task.
The re-cent work of (Dahlmeier and Ng, 2012a), for ex-ample, is designed to deal with multiple, interact-ing errors.Note that the essays in the training data and thetest essays naturally contain grammatical errors ofall types, beyond the five error types focused in ourshared task.
In the automatically corrected essaysreturned by a participating system, only correc-tions necessary to correct errors of the five typesare made.
The other errors are to be left uncor-rected.3 DataThis section describes the training and test datareleased to each participating team in our sharedtask.3.1 Training DataThe training data provided in our shared task isthe NUCLE corpus, the NUS Corpus of LearnerEnglish (Dahlmeier et al 2013).
As noted by(Leacock et al 2010), the lack of a manually an-notated and corrected corpus of English learnertexts has been an impediment to progress in gram-matical error correction, since it prevents com-parative evaluations on a common benchmark testdata set.
NUCLE was created precisely to fill thisvoid.
It is a collection of 1,414 essays writtenby students at the National University of Singa-pore (NUS) who are non-native speakers of En-glish.
The essays were written in response to someprompts, and they cover a wide range of topics,such as environmental pollution, health care, etc.The grammatical errors in these essays have beenhand-corrected by professional English instructorsat NUS.
For each grammatical error instance, thestart and end character offsets of the erroneous textspan are marked, and the error type and the cor-rection string are provided.
Manual annotation iscarried out using a graphical user interface specif-ically built for this purpose.
The error annotationsare saved as stand-off annotations, in SGML for-mat.To illustrate, consider the following sentence atthe start of the first paragraph of an essay:From past to the present, many impor-tant innovations have surfaced.There is an article/determiner error (past ?
thepast) in this sentence.
The error annotation, alsocalled correction or edit, in SGML format isshown in Figure 1. start par (end par) de-notes the paragraph ID of the start (end) of the er-roneous text span (paragraph ID starts from 0 byconvention).
start off (end off) denotes thecharacter offset of the start (end) of the erroneoustext span (again, character offset starts from 0 byconvention).
The error tag is ArtOrDet, and thecorrection string is the past.2Error tag Error type Example sentence Correction (edit)ArtOrDet Article or determiner In late nineteenth century, therewas a severe air crash happeningat Miami international airport.late ?
the latePrep Preposition Also tracking people is verydangerous if it has been con-trolled by bad men in a not goodpurpose.in ?
forNn Noun number I think such powerful deviceshall not be made easily avail-able.device ?
devicesVform Verb form However, it is an achievement asit is an indication that our soci-ety is progressed well and peo-ple are living in better condi-tions.progressed ?
progressingSVA Subject-verb agreement People still prefers to bear therisk and allow their pets to havemaximum freedom.prefers ?
preferTable 1: The five error types in our shared task.<MISTAKE start par="0" start off="5" end par="0" end off="9"><TYPE>ArtOrDet</TYPE><CORRECTION>the past</CORRECTION></MISTAKE>Figure 1: An example error annotation.The NUCLE corpus was first used in(Dahlmeier and Ng, 2011b), and has beenpublicly available for research purposes sinceJune 20111.
All instances of grammatical errorsare annotated in NUCLE, and the errors areclassified into 27 error types (Dahlmeier et al2013).To help participating teams in their prepara-tion for the shared task, we also performed au-tomatic preprocessing of the NUCLE corpus andreleased the preprocessed form of NUCLE.
Thepreprocessing operations performed on the NU-CLE essays include sentence segmentation andword tokenization using the NLTK toolkit (Birdet al 2009), and part-of-speech (POS) tagging,constituency and dependency tree parsing usingthe Stanford parser (Klein and Manning, 2003;de Marneffe et al 2006).
The error annotations,which are originally at the character level, arethen mapped to error annotations at the word to-ken level.
Error annotations at the word token1http://www.comp.nus.edu.sg/?nlp/corpora.htmllevel also facilitate scoring, as we will see in Sec-tion 4, since our scorer operates by matching to-kens.
Note that although we released our ownpreprocessed version of NUCLE, the participatingteams were however free to perform their own pre-processing if they so preferred.3.1.1 Revised version of NUCLENUCLE release version 2.3 was used in theCoNLL-2013 shared task.
In this version, 17 es-says were removed from the first release of NU-CLE since these essays were duplicates with mul-tiple annotations.In the original NUCLE corpus, there is not anexplicit preposition error type.
Instead, prepo-sition errors are part of the Wcip (wrong collo-cation/idiom/preposition) and Rloc (local redun-dancy) error types.
The Wcip error type combineserrors concerning collocations, idioms, and prepo-sitions together into one error type.
The Rloc er-ror type annotates extraneous words which are re-dundant and should be removed, and they includeredundant articles, determiners, and prepositions.3Training data Test data(NUCLE)# essays 1,397 50# sentences 57,151 1,381# word tokens 1,161,567 29,207Table 2: Statistics of training and test data.In our shared task, in order to facilitate the detec-tion and correction of article/determiner errors andpreposition errors, we performed automatic map-ping of error types in the original NUCLE cor-pus.
The mapping relies on POS tags, constituentparse trees, and error annotations at the word tokenlevel.
Specifically, we map the error types Wcipand Rloc to Prep, Wci, ArtOrDet, and Rloc?.Prepositions in the error type Wcip or Rloc aremapped to a new error type Prep, and redundantarticles or determiners in the error type Rloc aremapped to ArtOrDet.
The remaining unaffectedWcip errors are assigned the new error type Wciand the remaining unaffected Rloc errors are as-signed the new error type Rloc?.
The code thatperforms automatic error type mapping was alsoprovided to the participating teams.The statistics of the NUCLE corpus (release 2.3version) are shown in Table 2.
The distributionof errors among the five error types is shown inTable 3.
The newly added noun number error typein our shared task accounts for the second highestnumber of errors among the five error types.
Thefive error types in our shared task constitute 35%of all grammatical errors in the training data, and47% of all errors in the test data.
These figuressupport our choice of these five error types to bethe focus of our shared task, since they accountfor a large percentage of all grammatical errors inEnglish learner essays.While the NUCLE corpus is provided in ourshared task, participating teams are free to not useNUCLE, or to use additional resources and toolsin building their grammatical error correction sys-tems, as long as these resources and tools are pub-licly available and not proprietary.
For example,participating teams are free to use the CambridgeFCE corpus (Yannakoudakis et al 2011; Nicholls,2003) (the training data provided in HOO 2012(Dale et al 2012)) as additional training data.Error tag Training % Test %data data(NUCLE)ArtOrDet 6,658 14.8 690 19.9Prep 2,404 5.3 312 9.0Nn 3,779 8.4 396 11.4Vform 1,453 3.2 122 3.5SVA 1,527 3.4 124 3.65 types 15,821 35.1 1,644 47.4all types 45,106 100.0 3,470 100.0Table 3: Error type distribution of the training andtest data.3.2 Test Data25 NUS students, who are non-native speakers ofEnglish, were recruited to write new essays to beused as blind test data in the shared task.
Eachstudent wrote two essays in response to the twoprompts shown in Table 4, one essay per prompt.Essays written using the first prompt are presentin the NUCLE training data, while the secondprompt is a new prompt not used previously.
Asa result, 50 test essays were collected.
The statis-tics of the test essays are shown in Table 2.Error annotation on the test essays was carriedout by a native speaker of English who is a lecturerat the NUS Centre for English Language Commu-nication.
The distribution of errors in the test es-says among the five error types is shown in Ta-ble 3.
The test essays were then preprocessed inthe same manner as the NUCLE corpus.
The pre-processed test essays were released to the partici-pating teams.Unlike the test data used in HOO 2012 whichwas proprietary and not available after the sharedtask, the test essays and their error annotations inthe CoNLL-2013 shared task are freely availableafter the shared task.4 Evaluation Metric and ScorerA grammatical error correction system is evalu-ated by how well its proposed corrections or editsmatch the gold-standard edits.
An essay is firstsentence-segmented and tokenized before evalua-tion is carried out on the essay.
To illustrate, con-sider the following tokenized sentence S writtenby an English learner:There is no a doubt, tracking system4ID Prompt1 Surveillance technology such as RFID (radio-frequency identification) should not be used totrack people (e.g., human implants and RFID tags on people or products).
Do you agree?
Sup-port your argument with concrete examples.2 Population aging is a global phenomenon.
Studies have shown that the current average life spanis over 65.
Projections of the United Nations indicate that the population aged 60 or over indeveloped and developing countries is increasing at 2% to 3% annually.
Explain why rising lifeexpectancies can be considered both a challenge and an achievement.Table 4: The two prompts used for the test essays.has brought many benefits in this infor-mation age .The set of gold-standard edits of a human annota-tor is g = {a doubt ?
doubt, system ?
systems,has ?
have}.
Suppose the tokenized output sen-tence H of a grammatical error correction systemgiven the above sentence is:There is no doubt, tracking system hasbrought many benefits in this informa-tion age .That is, the set of system edits is e = {a doubt?
doubt}.
The performance of the grammaticalerror correction system is measured by how wellthe two sets g and e match, in the form of recallR, precision P , and F1 measure: R = 1/3, P =1/1, F1 = 2RP/(R + P ) = 1/2.More generally, given a set of n sentences,where gi is the set of gold-standard edits for sen-tence i, and ei is the set of system edits for sen-tence i, recall, precision, and F1 are defined asfollows:R =?ni=1 |gi ?
ei|?ni=1 |gi|(1)P =?ni=1 |gi ?
ei|?ni=1 |ei|(2)F1 =2?R?
PR + P(3)where the intersection between gi and ei for sen-tence i is defined asgi ?
ei = {e ?
ei|?g ?
gi,match(g, e)} (4)Evaluation by the HOO scorer (Dale and Kilgar-riff, 2011) is based on computing recall, precision,and F1 measure as defined above.Note that there are multiple ways to specify aset of gold-standard edits that denote the same cor-rections.
For example, in the above learner-writtensentence S, alternative but equivalent sets of gold-standard edits are {a ?
, system ?
systems, has?
have}, {a ?
, system has ?
systems have},etc.
Given the same learner-written sentence Sand the same system output sentence H shownabove, one would expect a scorer to give the sameR,P, F1 scores regardless of which of the equiv-alent sets of gold-standard edits is specified by anannotator.However, this is not the case with the HOOscorer.
This is because the HOO scorer usesGNU wdiff2 to extract the differences betweenthe learner-written sentence S and the system out-put sentence H to form a set of system edits.Since in general there are multiple ways to spec-ify a set of gold-standard edits that denote thesame corrections, the set of system edits com-puted by the HOO scorer may not match the set ofgold-standard edits specified, leading to erroneousscores.
In the above example, the set of systemedits computed by the HOO scorer for S and H is{a ?
}.
Given that the set of gold-standard editsg is {a doubt ?
doubt, system ?
systems, has ?have}, the scores computed by the HOO scorer areR = P = F1 = 0, which are erroneous.The MaxMatch (M2) scorer3 (Dahlmeier andNg, 2012b) was designed to overcome this limita-tion of the HOO scorer.
The key idea is that theset of system edits automatically computed andused in scoring should be the set that maximallymatches the set of gold-standard edits specified bythe annotator.
The M2 scorer uses an efficient al-gorithm to search for such a set of system editsusing an edit lattice.
In the above example, givenS, H , and g, the M2 scorer is able to come upwith the best matching set of system edits e = {adoubt ?
doubt}, thus giving the correct scoresR = 1/3, P = 1/1, F1 = 1/2.
We use the M22http://www.gnu.org/s/wdiff/3http://www.comp.nus.edu.sg/?nlp/software.html5scorer in the CoNLL-2013 shared task.The original M2 scorer implemented in(Dahlmeier and Ng, 2012b) assumes that thereis one set of gold-standard edits gi for eachsentence i.
However, it is often the case thatmultiple alternative corrections are acceptable fora sentence.
As we allow participating teams tosubmit alternative sets of gold-standard edits fora sentence, we also extend the M2 scorer to dealwith multiple alternative sets of gold-standardedits.Based on Equations 1 and 2, Equation 3 can bere-expressed as:F1 =2?
?ni=1 |gi ?
ei|?ni=1 (|gi|+ |ei|)(5)To deal with multiple alternative sets of gold-standard edits gi for a sentence i, the extendedM2 scorer chooses the gi that maximizes the cu-mulative F1 score for sentences 1, .
.
.
, i. Tiesare broken based on the following criteria: firstchoose the gi that maximizes the numerator?ni=1 |gi ?
ei|, then choose the gi that minimizesthe denominator?ni=1 (|gi|+ |ei|), finally choosethe gi that appears first in the list of alternatives.5 Approaches54 teams registered to participate in the sharedtask, out of which 17 teams submitted the outputof their grammatical error correction systems bythe deadline.
These teams are listed in Table 5.Each team is assigned a 3 to 4-letter team ID.
Inthe remainder of this paper, we will use the as-signed team ID to refer to a participating team.Every team submitted a system description paper(the only exception is the SJT2 team).Many different approaches are adopted by par-ticipating teams in the CoNLL-2013 shared task,and Table 6 summarizes these approaches.
A com-monly used approach in the shared task and ingrammatical error correction research in generalis to build a classifier for each error type.
For ex-ample, the classifier for noun number returns theclasses {singular, plural}, the classifier for articlereturns the classes {a/an, the, }, etc.
The classi-fier for an error type may be learned from train-ing examples encoding the surrounding context ofan error occurrence, or may be specified by deter-ministic hand-crafted rules, or may be built usinga hybrid approach combining both machine learn-ing and hand-crafted rules.
These approaches aredenoted by M, R, and H respectively in Table 6.The machine translation approach (denoted byT in Table 6) to grammatical error correctiontreats the task as ?translation?
from bad Englishto good English.
Both phrase-based translationand syntax-based translation approaches are usedby teams in the CoNLL-2013 shared task.
An-other related approach is the language modelingapproach (denoted by L in Table 6), in whichthe probability of a learner sentence is comparedwith the probability of a candidate corrected sen-tence, based on a language model built from abackground corpus.
The candidate correction ischosen if it results in a corrected sentence with ahigher probability.
In general, these approachesare not mutually exclusive.
For example, thework of (Dahlmeier and Ng, 2012a; Yoshimoto etal., 2013) includes elements of machine learning-based classification, machine translation, and lan-guage modeling approaches.When different approaches are used to tackledifferent error types by a system, we break downthe error types into different rows in Table 6, andspecify the approach used for each group of errortypes.
For instance, the HIT team uses a machinelearning approach to deal with article/determiner,noun number, and preposition errors, and a rule-based approach to deal with subject-verb agree-ment and verb form errors.
As such, the entry forHIT is sub-divided into two rows, to make it clearwhich particular error type is handled by whichapproach.Table 6 also shows the linguistic features usedby the participating teams, which include lexicalfeatures (i.e., words, collocations, n-grams), parts-of-speech (POS), constituency parses, dependencyparses, and semantic features (including semanticrole labels).While all teams in the shared task use the NU-CLE corpus, they are also allowed to use addi-tional external resources (both corpora and tools)so long as they are publicly available and not pro-prietary.
The external resources used by the teamsare also listed in Table 6.6 ResultsAll submitted system output was evaluated usingthe M2 scorer, based on the error annotations pro-vided by our annotator.
The recall (R), precision(P ), and F1 measure of all teams are shown in Ta-ble 7.
The performance of the teams varies greatly,6Team ID AffiliationCAMB University of CambridgeHIT Harbin Institute of TechnologyIITB Indian Institute of Technology, BombayKOR Korea UniversityNARA Nara Institute of Science and TechnologyNTHU National Tsing Hua UniversitySAAR Saarland UniversitySJT1 Shanghai Jiao Tong University (Team #1)SJT2 Shanghai Jiao Tong University (Team #2)STAN Stanford UniversitySTEL Stellenbosch UniversitySZEG University of SzegedTILB Tilburg UniversityTOR University of TorontoUAB Universitat Auto`noma de BarcelonaUIUC University of Illinois at Urbana-ChampaignUMC University of MacauTable 5: The list of 17 participating teams.Rank Team R P F11 UIUC 23.49 46.45 31.202 NTHU 26.35 23.80 25.013 HIT 16.56 35.65 22.614 NARA 18.62 27.39 22.175 UMC 17.53 28.49 21.706 STEL 13.33 27.00 17.857 SJT1 10.96 40.18 17.228 CAMB 10.10 39.15 16.069 IITB 4.99 28.18 8.4810 STAN 4.69 25.50 7.9211 TOR 4.81 17.67 7.5612 KOR 3.71 43.88 6.8513 TILB 7.24 6.25 6.7114 SZEG 3.16 5.52 4.0215 UAB 1.22 12.42 2.2216 SAAR 1.10 27.69 2.1117 SJT2 0.24 13.33 0.48Table 7: Scores (in %) without alternative an-swers.from barely half a per cent to 31.20% for the topteam.The nature of grammatical error correction issuch that multiple, different corrections are of-ten acceptable.
In order to allow the participatingteams to raise their disagreement with the originalgold-standard annotations provided by the anno-tator, and not understate the performance of theteams, we allow the teams to submit their pro-posed alternative answers.
This was also the prac-tice adopted in HOO 2011 and HOO 2012.
Specif-ically, after the teams submitted their system out-put and the error annotations on the test essayswere released, we allowed the teams to propose al-ternative answers (gold-standard edits), to be sub-mitted within four days after the initial error an-notations were released.
The same annotator whoprovided the error annotations on the test essaysalso judged the alternative answers proposed bythe teams, to ensure consistency.
In all, five teams(NTHU, STEL, TOR, UIUC, UMC) submitted al-ternative answers.The same submitted system output was thenevaluated using the extended M2 scorer, with theoriginal annotations augmented with the alterna-tive answers.
Table 8 shows the recall (R), preci-sion (P ), and F1 measure of all teams under thisnew evaluation setting.The F1 measure of every team improves when7TeamErrorApproachDescriptionofApproachLinguisticFeaturesExternalResourcesCAMBANPSVTfactoredphrase-basedtranslationmodelwithIRSTlanguagemodellexical,POSCambridgeLearnerCorpusHITANPMmaximumentropywithconfidencetuning,andgeneticalgo-rithmforfeatureselectionlexical,POS,constituencyparse,dependencyparse,semanticWordNet,LongmandictionarySVRrule-basedPOS,dependencyparse,semanticIITBANMmaximumentropylexical,POS,nounpropertiesWiktionarySRrule-basedPOS,constituencyparse,dependencyparseKORANPMmaximumentropylexical,POS,head-modifier,dependencyparse(none)NARAAPTphrase-basedstatisticalmachinetranslationlexicalLang-8NMadaptiveregularizationofweightvectorslexical,lemma,constituencyparseGigawordSVLtreelet(tree-based)languagemodellexical,POS,constituencyparsePennTreebank,GigawordNTHUANPVLn-gram-basedanddependency-basedlanguagemodellexical,POS,constituencyparse,dependencyparseGoogleWeb-1TSAARAMmulti-classSVMandnaiveBayeslexical,POS,constituencyparseCMUPronouncingDictionarySRrule-basedPOS,dependencyparseSJT1ANPSVMmaximumentropy(withLMpost-filtering)lexical,lemma,POS,constituencyparse,dependencyparseEuroparlSTANANPSVHEnglishResourceGrammar(ERG),head-drivenphrasestruc-ture,extendedwithhand-codedmal-ruleslexical,POS,constituencyparse,semanticEnglishResourceGrammarSTELANPSVTtree-to-stringwithGHKMtransducerconstituencyparseWikipedia,WordNetSZEGANMmaximumentropyLFG,lexical,constituencyparse,dependencyparse(none)TILBANPSVMbinaryandmulti-classIGTreelexical,lemma,POSGoogleWeb-1T,GigawordTORANPSVTnoisychannelmodelinvolvingtransformationofsinglewordslexical,POSWikipediaUABANPSVRrule-basedlexical,dependencyparseTop250uncountablenouns,FreeLingmorphologicaldictionaryUIUCANPSVMA:multi-classaveragedperceptron;others:naiveBayeslexical,POS,shallowparseGoogleWeb-1T,GigawordUMCANPSVHpipeline:rule-basedfilter?semi-supervisedmulti-classmaximumentropyclassifier?LMconfidencescorerlexical,POS,dependencyparseNewscorpus,JMySpelldictionary,GoogleWeb-1T,PennTreebankTable6:Profileoftheparticipatingteams.TheErrorcolumnshowstheerrortype,whereeachletterdenotestheerrortypebeginningwiththatinitialletter.TheApproachcolumnshowstheapproachadoptedbyeachteam,sometimesbrokendownaccordingtotheerrortype:Hdenotesahybridclassifierapproach,Ldenotesalanguagemodeling-basedapproach,Mdenotesamachinelearning-basedclassifierapproach,Rdenotesarule-basedclassifier(non-machinelearning)approach,andTdenotesamachinetranslationapproach8evaluated with alternative answers.
Not surpris-ingly, the teams which submitted alternative an-swers tend to show the greatest improvements intheir F1 measure.
Overall, the UIUC team (Ro-zovskaya et al 2013) achieves the best F1 mea-sure, with a clear lead over the other teams in theshared task, under both evaluation settings (with-out and with alternative answers).For future research which uses the test data ofthe CoNLL-2013 shared task, we recommend thatevaluation be carried out in the setting that doesnot use alternative answers, to ensure a fairer eval-uation.
This is because the scores of the teamswhich submitted alternative answers tend to behigher in a biased way when evaluated with alter-native answers.Rank Team R P F11 UIUC 31.87 62.19 42.142 NTHU 34.62 30.57 32.463 UMC 23.66 37.12 28.904 NARA 24.05 33.92 28.145 HIT 20.29 41.75 27.316 STEL 18.91 37.12 25.057 CAMB 14.19 52.11 22.308 SJT1 13.67 47.77 21.259 TOR 8.77 30.67 13.6410 IITB 6.55 34.93 11.0311 STAN 5.86 29.93 9.8112 KOR 4.78 53.24 8.7713 TILB 9.29 7.60 8.3614 SZEG 4.07 6.67 5.0615 UAB 1.81 17.39 3.2816 SAAR 1.68 40.00 3.2317 SJT2 0.33 16.67 0.64Table 8: Scores (in %) with alternative answers.We are also interested in the analysis of scoresof each of the five error types.
To compute therecall of an error type, we need to know the er-ror type of each gold-standard edit, which is pro-vided by the annotator.
To compute the precisionof each error type, we need to know the error typeof each system edit, which however is not avail-able since the submitted system output only con-tains the corrected sentences with no indication ofthe error type of the system edits.In order to determine the error type of systemedits, we first perform POS tagging on the submit-ted system output using the Stanford parser (KleinandManning, 2003).
We also make use of the POStags assigned in the preprocessed form of the testessays.
We then assign an error type to a systemedit based on the automatically determined POStags, as follows:?
ArtOrDet: The system edit involves a change(insertion, deletion, or substitution) of wordstagged as article/determiner, i.e., DT or PDT.?
Prep: The system edit involves a change ofwords tagged as preposition, i.e., IN or TO.?
Nn: The system edit involves a change ofwords such that a word in the source stringis a singular noun (tagged as NN or NNP)and a word in the replacement string is a plu-ral noun (tagged as NNS or NNPS), or viceversa.
Since a word tagged as JJ (adjective)can serve as a noun, a system edit that in-volves a change of POS tags from JJ to one of{NN, NNP, NNS, NNPS} or vice versa alsoqualifies.?
Vform/SVA: The system edit involves achange of words tagged as one of the verbPOS tags, i.e., VB, VBD, VBG, VBN, VBP,and VBZ.The verb form and subject-verb agreement errortypes are grouped together into one category, sinceit is difficult to automatically distinguish the two ina reliable way.The scores when distinguished by error type areshown in Tables 9 and 10.
Based on the F1 mea-sure of each error type, the noun number error typegives the highest scores, and preposition errors re-main the most challenging error type to correct.7 ConclusionsThe CoNLL-2013 shared task saw the participa-tion of 17 teams worldwide to evaluate their gram-matical error correction systems on a common testset, using a common evaluation metric and scorer.The five error types included in the shared taskaccount for at least one-third to close to one-halfof all errors in English learners?
essays.
The bestsystem in the shared task achieves an F1 score of42%, when it is scored with multiple acceptableanswers.
There is still much room for improve-ment, both in the accuracy of grammatical errorcorrection systems, and in the coverage of systemsto deal with a more comprehensive set of error9TeamArtOrDetPrepNnVform/SVARPF1RPF1RPF1RPF1CAMB15.0738.6621.693.5440.746.517.5855.5613.338.5431.8213.46HIT24.2042.8230.932.8928.125.2517.1729.6921.7611.3826.4215.91IITB1.3021.432.46(notdone)9.8528.6814.6613.8230.0918.94KOR4.7853.238.780.324.760.606.8249.0911.97(notdone)NARA20.4334.0625.5412.5429.1017.5316.4148.8724.5724.8014.8118.54NTHU21.0135.8026.4812.8612.0112.4245.9640.9043.2826.8312.2216.79SAAR0.7262.501.43(notdone)(notdone)5.2823.218.61SJT116.8147.1524.791.2912.502.3313.6442.1920.612.4414.634.18SJT20.000.000.000.000.000.001.0113.331.880.000.000.00STAN3.9120.456.570.3220.000.636.0629.6310.0610.1632.0515.43STEL12.6127.7117.339.3225.6613.6818.1846.7526.1812.6017.6114.69SZEG1.161.701.38(notdone)11.1113.6212.24(notdone)TILB4.494.494.4910.615.076.867.0721.2110.6110.989.5710.23TOR8.5525.5412.812.255.383.171.7731.823.352.4412.244.07UAB0.000.000.000.000.000.000.000.000.008.1312.429.83UIUC25.6547.8433.404.1826.537.2238.3852.2344.2517.8938.9424.51UMC21.0130.2724.811.9335.293.6623.2327.9625.3818.2928.8522.39Table9:Scores(in%)withoutalternativeanswers,distinguishedbyerrortype.Ifateamindicatesthatitssystemdoesnothandleaparticularerrortype,itsentryforthaterrortypeismarkedas?(notdone)?.10TeamArtOrDetPrepNnVform/SVARPF1RPF1RPF1RPF1CAMB19.6249.8128.155.0450.009.159.5069.0916.7016.5254.4125.34HIT27.4147.4434.744.5837.508.1619.9535.3725.5117.9038.3224.40IITB1.7928.573.37(notdone)11.9135.2917.8118.6736.8424.78KOR5.9564.5210.901.5319.052.837.5254.5513.22(notdone)NARA25.7943.3432.3417.6034.5623.3319.1557.0428.6834.6219.1024.62NTHU25.3042.5431.7320.4516.1718.0652.3550.8751.6043.0319.2226.57SAAR1.0487.502.06(notdone)(notdone)8.6033.9313.72SJT119.6554.0728.821.9215.623.4216.4652.3425.054.0521.956.84SJT20.000.000.000.000.000.001.2616.672.350.000.000.00STAN4.9124.818.200.3820.000.756.7833.3311.2713.5137.9719.93STEL16.2335.6922.3112.8329.8217.9426.0570.0037.9720.5226.5523.15SZEG1.502.131.76(notdone)12.8715.9514.25(notdone)TILB5.785.645.7113.915.688.078.2524.2612.3116.3612.7714.34TOR13.1039.1319.635.9712.318.043.5263.646.678.1435.2913.24UAB0.000.000.000.000.000.000.000.000.0012.6117.3914.62UIUC31.9959.8441.698.8146.9414.8446.8870.0056.1528.5760.7138.86UMC25.8836.7430.373.4756.256.5530.6138.6434.1626.8140.1332.14Table10:Scores(in%)withalternativeanswers,distinguishedbyerrortype.Ifateamindicatesthatitssystemdoesnothandleaparticularerrortype,itsentryforthaterrortypeismarkedas?(notdone)?.11types.
The evaluation data sets and scorer usedin our shared task serve as a benchmark for futureresearch on grammatical error correction4.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesSteven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media.Daniel Dahlmeier and Hwee Tou Ng.
2011a.
Cor-recting semantic collocation errors with L1-inducedparaphrases.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 107?117.Daniel Dahlmeier and Hwee Tou Ng.
2011b.
Gram-matical error correction with alternating structureoptimization.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics, pages 915?923.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A beam-search decoder for grammatical error correction.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages568?578.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 pilot shared task.
In Proceed-ings of the 13th EuropeanWorkshop on Natural Lan-guage Generation, pages 242?249.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the 7th Workshop on the Innovative Use ofNLP for Building Educational Applications, pages54?62.4http://www.comp.nus.edu.sg/?nlp/conll13st.htmlMarie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth Conference on LanguageResources and Evaluation, pages 449?454.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: A meta-classifierapproach.
In Proceedings of the Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics, pages 163?171.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(2):115?129.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan &Claypool Publishers.Diane Nicholls.
2003.
The Cambridge Learner Cor-pus: Error coding and analysis for lexicography andELT.
In Proceedings of the Corpus Linguistics 2003Conference, pages 572?581.Alla Rozovskaya and Dan Roth.
2010.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages961?970.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,and Dan Roth.
2013.
The University of Illinoissystem in the CoNLL-2013 shared task.
In Pro-ceedings of the Seventeenth Conference on Compu-tational Natural Language Learning: Shared Task.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selectionand error detection.
In Proceedings of the ACL 2010Conference Short Papers, pages 353?358.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 180?189.Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,Keisuke Sakaguchi, Tomoya Mizumoto, YutaHayashibe, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at 2013 CoNLL grammatical errorcorrection shared task.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning: Shared Task.12
