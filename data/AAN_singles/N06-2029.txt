Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 113?116,New York, June 2006. c?2006 Association for Computational LinguisticsExploiting Variant Corpora for Machine TranslationMichael Paul??
and Eiichiro Sumita???
National Institute of Information and Communications Technology?
ATR Spoken Language Communication Research LabsHikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto{Michael.Paul,Eiichiro.Sumita}@{nict.go.jp,atr.jp}AbstractThis paper proposes the usage of variantcorpora, i.e., parallel text corpora that areequal in meaning but use different waysto express content, in order to improvecorpus-based machine translation.
The us-age of multiple training corpora of thesame content with different sources resultsin variant models that focus on specificlinguistic phenomena covered by the re-spective corpus.
The proposed methodapplies each variant model separately re-sulting in multiple translation hypotheseswhich are selectively combined accord-ing to statistical models.
The proposedmethod outperforms the conventional ap-proach of merging all variants by reducingtranslation ambiguities and exploiting thestrengths of each variant model.1 IntroductionCorpus-based approaches to machine translation(MT) have achieved much progress over the lastdecades.
Despite a high performance on average,these approaches can often produce translations withsevere errors.
Input sentences featuring linguisticphenomena that are not sufficiently covered by theutilized models cannot be translated accurately.This paper proposes to use multiple variant cor-pora, i.e., parallel text corpora that are equal inmeaning, but use different vocabulary and grammat-ical constructions in order to express the same con-tent.
Using training corpora of the same content withdifferent sources result in translation models that fo-cus on specific linguistic phenomena, thus reducingtranslation ambiguities compared to models trainedon a larger corpus obtained by merging all variantcorpora.
The proposed method applies each variantmodel separately to an input sentence resulting inmultiple translation hypotheses.
The best translationis selected according to statistical models.
We showthat the combination of variant translation modelsis effective and outperforms not only all single vari-ant models, but also is superior to translation modelstrained on the union of all variant corpora.In addition, we extend the proposed method tomulti-engine MT.
Combining multiple MT enginescan boost the system performance further by exploit-ing the strengths of each MT engine.
For each vari-ant, all MT engines are trained on the same corpusand used in parallel to translate the input.
We firstselect the best translation hypotheses created by allMT engines trained on the same variant and thenverify the translation quality of the translation hy-potheses selected for each variant.VariantCorporaMTEnginesHypothesisSelectionVariantSelection(cf.
Section 2) (cf.
Section 3.1) (cf.
Section 3.2)BTEC VBTEC OECVCOtranslationknowledgeacuisitionhyp 6Vhyp 1Vhyp 6Ohyp 1OSELOSELVteststatisticalsignificantdifferenceinstatisticalscoresMT7VMT1VMT7OMT1OFigure 1: System outlineThe outline of the proposed system is given inFigure 1.
For the experiments described in this pa-per we are using two variants of a parallel text cor-pus for Chinese (C) and English (E) from the traveldomain (cf.
Section 2).
These variant corpora areused to acquire the translation knowledge for sevencorpus-based MT engines.
The method to select thebest translation hypotheses of MT engines trainedon the same variant is described in Section 3.1.
Fi-nally, the selected translations of different variantsare combined according to a statistical significancetest as described in Section 3.2.
The effectivenessof the proposed method is verified in Section 4 for113the Chinese-English translation task of last year?sIWSLT1 evaluation campaign.2 Variant CorporaThe Basic Travel Expressions Corpus (BTEC) is acollection of sentences that bilingual travel expertsconsider useful for people going to or coming fromanother country and cover utterances in travel situ-ations (Kikui et al, 2003).
The original Japanese-English corpus consists of 500K of aligned sen-tence pairs whereby the Japanese sentences werealso translated into Chinese.In addition, parts of the original English corpuswere translated separately into Chinese resulting in avariant corpus comprising 162K CE sentence pairs.Details of both, the original (BTECO) and the variant(BTECV ) corpus, are given in Table 1, where wordtoken refers to the number of words in the corpusand word type refers to the vocabulary size.Table 1: Statistics of variant corporacorpus lang sentence count avg word wordtotal unique len tokens typesBTECO C 501,809 299,347 6.8 3,436,750 40,645E 501,809 344,134 8.3 4,524,703 21,832BTECV C 162,320 97,512 7.1 1,302,761 14,222E 162,320 96,988 7.5 1,367,981 9,795Only 4.8% of the sentences occured in both cor-pora and only 68.1% of the BTECV vocabulary wascovered in the BTECO corpus.The comparison of both corpora revealed fur-ther that each variant closely reflects the linguisticstructure of the source language which was used toproduce the Chinese translations of the respectivedata sets.
The differences between the BTECO andBTECV variants can be categorized into:(1) literalness: BTECO sentences are translated onthe basis of their meaning and context resulting infreer translations compared to the BTECV sentenceswhich are translated more literally;(2) syntax: The degree of literalness also has an im-pact on the syntactic structure like word order vari-ations (CV sentences reflect closely the word orderof the corresponding English sentences) or the sen-tence type (question vs. imperative);(3) lexical choice: Alternations in lexical choice1http://penance.is.cs.cmu.edu/iwslt2005also contribute largely to variations between the cor-pora.
Moreover, most of the pronouns found inthe English sentences are translated explicitly in theCV sentences, but are omitted in CO;(4) orthography: Orthographic differences espe-cially for proper nouns (Kanji vs. transliteration)and numbers (numerals vs. spelling-out).3 Corpus-based Machine TranslationThe differences in variant corpora directly effect thetranslation quality of corpus-based MT approaches.Simply merging variant corpora for training in-creases the coverage of linguistic phenomena by theobtained translation model.
However, due to an in-crease in translation ambiguities, more erroneoustranslations might be generated.In contrast, the proposed method trains separatelyMT engines on each variant focusing on linguisticphenomena covered in the respective corpus.
If spe-cific linguistic phenomena are not covered by a vari-ant corpus, the translation quality of the respectiveoutput is expected to be significantly lower.Therefore, we first judge the translation qualityof all translation hypotheses created by MT enginestrained on the same variant corpus by testing statis-tical significant differences in the statistical scores(cf.
Section 3.1).
Next, we compare the outcomesof the statistical significance test between the trans-lation hypotheses selected for each variant in orderto identify the variant that fits best the given inputsentence (cf.
Section 3.2).3.1 Hypothesis SelectionIn order to select the best translation among outputsgenerated by multiple MT systems, we employ anSMT-based method that scores MT outputs by usingmultiple language (LM) and translation model (TM)pairs trained on different subsets of the training data.It uses a statistical test to check whether the obtainedTM?LM scores of one MT output are significantlyhigher than those of another MT output (Akiba et al,2002).
Given an input sentence, m translation hy-potheses are produced by the element MT engines,whereby n different TM?LM scores are assigned toeach hypothesis.
In order to check whether the high-est scored hypothesis is significantly better then theother MT outputs, a multiple comparison test basedon the Kruskal-Wallis test is used.
If one of the MToutputs is significantly better, this output is selected.114Otherwise, the output of the MT engine that per-forms best on a develop set is selected.3.2 Variant SelectionIn order to judge which variant should be selectedfor the translation of a given input sentence, the out-comes of the statistical significance test carried outduring the hypothesis selection are employed.The hypothesis selection method is applied foreach variant separately, i.e., the BTECO corpus isused to train multiple statistical model pairs (SELO)and the best translation (MTOSEL) of the set of trans-lation hypotheses created by the MT engines trainedon the BTECO corpus is selected.
Accordingly, theSELV models are trained on the BTECV corpus andapplied to select the best translation (MTVSEL) of theMT outputs trained on the BTECV corpus.
In addi-tion, the SELO models were used in order to verifywhether a significant difference can be found for thetranslation hypothesis MTVSEL, and, vice versa, theSELV models were applied to MTOSEL.The outcomes of the statistical significance testsare then compared.
If a significant difference be-tween the statistical scores based on one variant, butnot for the other variant is obtained, the significantlybetter hypothesis is selected as the output.
However,if a significant difference could be found for both ornone of the variants, the translation hypothesis pro-duced by the MT engine that performs best on a de-velop set is selected.4 ExperimentsThe effectiveness of the proposed method is veri-fied for the CE translation task (500 sentences) oflast year?s IWSLT evaluation campaign.
For the ex-periments, we used the four statistical (SMT) andthree example-based (EBMT) MT engines describedin detail in (Paul et al, 2005).For evaluation, we used the BLEU metrics, whichcalculates the geometric mean of n-gram precisionfor the MT outputs found in reference translations(Papineni et al, 2002).
Higher BLEU scores indi-cate better translations.4.1 Performance of Element MT EnginesTable 2 summarizes the results of all element MTengines trained on the BTECO and BTECV corpora.The result show that the SMT engines outperformTable 2: BLEU evaluation of element MT enginesSMT BTECO BTECV EBMT BTECO BTECVMT1 0.4020 0.4633 MT5 0.2908 0.3445MT2 0.4474 0.4595 MT6 0.2988 0.4100MT3 0.5342 0.5110 MT7 0.0002 0.0074MT4 0.3575 0.4460the EBMT engines whereby the best performing sys-tem is marked with bold-face.However, depending on the variant corpus usedto train the MT engines, quite different system per-formances are achieved.
Most of the element MTengines perform better when trained on the smallerBTECV corpus indicating that the given test set isnot covered well by the BTECO corpus.4.2 Effects of Hypothesis SelectionThe performance of the hypothesis selection method(SEL) is summarized in Table 3 whereby the ob-tained gain relative to the best element MT engineis given in parentheses.
In addition, we performedan ?oracle?
translation experiment in order to inves-tigate in an upper boundary for the method.
Eachinput sentence was translated by all element MT en-gines and the translation hypothesis with the lowestword error rate2 relative to the reference translationswas output as the translation, i.e., the ORACLE sys-tem simulates an optimal selection method accord-ing to an objective evaluation criterion.Table 3: BLEU evaluation of hypothesis selectionMT engine BTECO BTECVSEL 0.5409 (+ 0.7%) 0.5470 (+ 3.6%)ORACLE 0.6385 (+10.4%) 0.6502 (+13.9%)MT engine BTECO?VSEL 0.4648 (?7.0%)ORACLE 0.6969 (+16.3%)The results show that the selection method is ef-fective for both variant corpora whereby a largergain is achieved for BTECV .
However, the ORA-CLE results indicate that the method fails to tap thefull potential of the element MT engines.In addition, we trained the statistical models of thehypothesis selection method on the corpus obtained2The word error rate (WER) is an objective evaluation mea-sures that, in contrast to BLEU, can be applied on sentence-level.
It penalizes edit operations for the translation outputagainst reference translations.115by merging all variant corpora (BTECO?V ).
Despitethe larger amount of training data, the BLEU scoredecreases drastically which shows that an increasein training data not necessarily leads to improvedtranslation quality.
Moreover, the ORACLE selec-tion applied to all translation hypotheses based onthe BTECO as well as the BTECV corpus indicatesthat both variants can contribute significantly in or-der to improve the overall system performance.4.3 Effects of Variant SelectionThe effects of combining selected variant hypothe-ses by testing whether significant differences in sta-tistical scores were obtained are summarized in Ta-ble 4.
The variant selection method is applied tothe translation outputs of each element MT engine(MTOj ?
MTVj ) as well as the selected translation hy-potheses (MTOSEL ?
MTVSEL).
The gain of the pro-posed variant selection method relative the best ele-ment MT output based on a single variant corpus isgiven in parentheses.Table 4: BLEU evaluation of variant selectionMT engine BTECO ?
BTECVSMT MTO1 ?
MTV1 0.5010 (+ 3.8%)MTO2 ?
MTV2 0.4847 (+ 2.5%)MTO3 ?
MTV3 0.5594 (+ 2.5%)MTO4 ?
MTV4 0.4733 (+ 2.7%)EBMT MTO5 ?
MTV5 0.3863 (+ 4.2%)MTO6 ?
MTV6 0.4338 (+ 2.4%)MTO7 ?
MTV7 0.0181 (+10.7%)MTOSEL ?
MTVSEL 0.5765 (+ 4.2%)The results show that the variant selection methodis effective for all element MT engines.
The high-est BLEU score is achieved for MTOSEL ?
MTVSELgaining 4.2% in BLEU score.
Moreover, the pro-posed method outperforms the hypothesis selectionmethod based on the merged corpus BTECO?V by11.2% in BLEU score.A comparison of the proposed method withthe best performing system (C-STAR data track,BLEU=0.5279) of the IWSLT 2005 workshopshowed that our system outperforms the top-rankedsystem gaining 4.8% in BLEU score.5 ConclusionThis paper proposed the usage of variant corpora toimprove the translation quality of a multi-engine-based approach to machine translation.
The ele-ment MT engines were used to translate the sameinput whereby the best translation was selected ac-cording to statistical models.
A test on the signifi-cance of differences between statistical scores judg-ing the translation quality of a given hypothesis wasexploited to identify the model that fits the input sen-tence best and the respective translation hypothesiswas selected as the translation output.The proposed method was evaluated on the CEtranslation task of the IWSLT 2005 workshop.
Theresults showed that the proposed method achieving aBLEU score of 0.5765 outperformed not only all el-ement MT engines (gaining 3.6% in BLEU score),but also a selection method using a larger corpusobtained from merging all variant corpora (gaining11.2% in BLEU score) due to less ambiguity in theutilized models.
In addition, the proposed methodalso outperformed the best MT system (C-STARdata track) of the IWSLT 2005 workshop gaining4.8% in BLEU score.Further investigations should analyze the charac-teristics of the variant corpora in more detail and fo-cus on the automatic identification of specific lin-guistic phenomena that could be helpful to measurehow good an input sentence is covered by a spe-cific model.
This would allow us to select the mostadequate variant beforehand, thus reducing com-putational costs and improving the system perfor-mance.
This would also enable us to cluster verylarge corpora according to specific linguistic phe-nomena, thus breaking down the full training corpusto consistent subsets that are easier to manage andthat could produce better results.ReferencesK.
Papineni et al 2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of the 40thACL, pages 311?318.Y.
Akiba et al 2002.
Using language and translationmodels to select the best among outputs from multipleMT systems.
In Proc.
of COLING, pages 8?14.G.
Kikui et al 2003.
Creating corpora for speech-to-speech translation.
In Proc.
of EUROSPEECH03,pages 381?384.M.
Paul et al 2005.
Nobody is Perfect: ATR?s HybridApproach to Spoken Language Translation.
In Proc.of the IWSLT, pages 55?62.116
