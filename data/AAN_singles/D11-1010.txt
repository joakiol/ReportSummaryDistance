Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107?117,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsCorrecting Semantic Collocation Errors with L1-induced ParaphrasesDaniel Dahlmeier1 and Hwee Tou Ng1,21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore{danielhe,nght}@comp.nus.edu.sgAbstractWe present a novel approach for automaticcollocation error correction in learner Englishwhich is based on paraphrases extracted fromparallel corpora.
Our key assumption is thatcollocation errors are often caused by se-mantic similarity in the first language (L1-language) of the writer.
An analysis of alarge corpus of annotated learner English con-firms this assumption.
We evaluate our ap-proach on real-world learner data and showthat L1-induced paraphrases outperform tradi-tional approaches based on edit distance, ho-mophones, and WordNet synonyms.1 IntroductionGrammatical error correction (GEC) is emerging asa commercially attractive application of natural lan-guage processing (NLP) for the booming market ofEnglish as foreign or second language (EFL/ESL1).The de facto standard approach to GEC is to builda statistical model that can choose the most likelycorrection from a confusion set of possible correc-tion choices.
The way the confusion set is defineddepends on the type of error.
Work in context-sensitive spelling error correction (Golding andRoth, 1999) has traditionally focused on confusionsets with similar spelling (e.g., {dessert, desert}) orsimilar pronunciation (e.g., {there, their}).
In otherwords, the words in a confusion set are deemed con-fusable because of orthographic or phonetic simi-larity.
Other work in GEC has defined the confu-1For simplicity, we will collectively refer to both terms asEnglish as a foreign language (EFL)sion sets based on syntactic similarity, for exam-ple all English articles or the most frequent Englishprepositions form a confusion set (see for example(Tetreault et al, 2010; Rozovskaya and Roth, 2010;Gamon, 2010; Dahlmeier and Ng, 2011) among oth-ers).In contrast, we investigate in this paper a class ofgrammatical errors where the source of confusion isthe similar semantics of the words, rather than or-thography, phonetics, or syntax.
In particular, wefocus on collocation errors in EFL writing.
Theterm collocation (Firth, 1957) describes a sequenceof words that is conventionally used together in aparticular way by native speakers and appears moreoften together than one would expect by chance.
Thecorrect use of collocations is a major difficulty forEFL students (Farghal and Obiedat, 1995).In this work, we present a novel approach for au-tomatic correction of collocation errors in EFL writ-ing.
Our key observation is that words are poten-tially confusable for an EFL student if they havesimilar translations in the writer?s first language (L1-language), or in other words if they have the samesemantics in the L1-language of the writer.
TheChinese word ?
(k?n), for example, has over adozen translations in English, including the wordssee, look, read, and watch.
A Chinese speaker whostill ?thinks?
in Chinese has to choose from all thesepossible translations when he wants to express a sen-tence like I like to watch movies and might insteadproduce a sentence like *I like to look movies.
Al-though the meanings of watch and look are simi-lar, the former is clearly the more fluent choice inthis context.
While these types of L1-transfer er-107rors have been known in the EFL teaching litera-ture (Swan and Smith, 2001; Meng, 2008), researchin GEC has mostly ignored this fact.We first analyze collocation errors in the NUSCorpus of Learner English (NUCLE), a fully an-notated one-million-word corpus of learner Englishwhich we will make available to the community forresearch purposes (see Section 3 for details aboutthe corpus).
Our analysis confirms that many col-location errors can be traced to similar translationsin the writer?s L1-language.
Based on this result,we propose a novel approach for automatic collo-cation error correction.
The key component in ourapproach generates L1-induced paraphrases whichwe automatically extract from an L1-English par-allel corpus.
Our proposed approach outperformstraditional approaches based on edit distance, ho-mophones, and WordNet synonyms on a test set ofreal-world learner data in an automatic and a humanevaluation.
Finally, we present a detailed analysis ofunsolved instances in our data set to highlight direc-tions for future work.Our work adds to a growing body of research thatleverages parallel corpora for semantic NLP tasks,for example in word sense disambiguation (Ng etal., 2003; Chan and Ng, 2005; Ng and Chan, 2007;Zhong and Ng, 2009), paraphrasing (Bannard andCallison-Burch, 2005; Liu et al, 2010a), and ma-chine translation evaluation (Snover et al, 2009; Liuet al, 2010b).The remainder of this paper is organized as fol-lows.
The next section reviews related work.
Sec-tion 3 presents our analysis of collocation errors.Section 4 describes our approach for automatic col-location error correction.
The experimental setupand the results are described in Sections 5 and 6, re-spectively.
Section 7 provides further analysis.
Sec-tion 8 concludes the paper.2 Related WorkIn this section, we give an overview of related workon collocation error correction.
We also highlightdifferences between collocation error correction andrelated NLP tasks like context-sensitive spelling er-ror correction, synonym extraction, lexical substitu-tion, and paraphrasing.Most work in collocation error correction has re-lied on dictionaries or manually created databasesto generate collocation candidates (Shei and Pain,2000; Wible et al, 2003; Futagi et al, 2008).
Otherwork has focused on finding candidates that collo-cate with similar words, e.g., verbs that appear withthe same noun objects form a confusion set (Liu etal., 2009; Wu et al, 2010).
The work most similarto ours is probably the one presented by Chang etal.
(2008), as they also use translation information togenerate collocation candidates.
However, they donot use automatically derived paraphrases from par-allel corpora but bilingual dictionaries.
Dictionariesusually have lower coverage, do not contain longerphrases or inflected forms, and do not provide anytranslation probability estimates.
Also, their workfocuses solely on verb-noun collocations, while wetarget collocations of arbitrary syntactic type.Context-sensitive spelling error correction is thetask of correcting spelling mistakes that result inanother valid word, see for example (Golding andRoth, 1999).
It has traditionally focused on a smallnumber of pre-defined confusion sets, like homo-phones or frequent spelling errors.
Even when theconfusion sets are formed automatically, the simi-larity of words in a confusion set has been basedon edit distance or phonetic similarity (Carlson etal., 2001).
In contrast, we focus on words that areconfusable due to their similar semantics instead ofsimilar spelling or pronunciation.
Also, we do notassume that the set of confusion sets is already givento us.
Instead, we automatically extract confusablecandidates from a parallel corpus.Synonym extraction (Wu and Zhou, 2003), lexi-cal substitution (McCarthy and Navigli, 2007) andparaphrasing (Madnani and Dorr, 2010) are relatedto collocation correction in the sense that they try tofind semantically equivalent words or phrases.
How-ever, there is a subtle but important difference be-tween these tasks and collocation correction.
In theformer, the main criterion is whether the originalphrase and the synonym/paraphrase candidate aresubstitutable, i.e., both form a grammatical sentencewhen substituted for each other in a particular con-text.
In contrast, in collocation correction, we areprimarily interested in finding candidates which arenot substitutable in their English context but appearto be substitutable in the L1-language of the writer,i.e., one forms a grammatical English sentence butthe other does not.108Sentences 52,149Words 1,149,100Distinct words 27,593Avg.
sentence length (words) 22.04Collocation errors 2,747Avg.
collocation error length (words) 1.17Avg.
correction length (words) 1.13Table 1: Statistics of the NUS Corpus of Learner En-glish (NUCLE)3 Analysis of EFL collocation errorsWhile the fact that collocation errors can be causedby L1-transfer has been ascertained by EFL re-searchers (Meng, 2008), we need to quantify howfrequent collocation errors can be traced to thesetypes of transfer errors in order to estimate howmany errors in EFL writing we can potentially hopeto correct with information about the writer?s L1-language.We base our analysis on the NUS Corpus ofLearner English (NUCLE).
The corpus consists ofabout 1,400 essays written by EFL university stu-dents on a wide range of topics, like environmen-tal pollution or healthcare.
Most of the students arenative Chinese speakers.
The corpus contains overone million words which are completely annotatedwith error tags and corrections.
All annotations havebeen performed by professional English instructors.The statistics of the corpus are summarized in Ta-ble 1.
The annotation is stored in a stand-off fashion.Each error tag consists of the start and end offset ofthe annotation, the type of the error, and the appro-priate gold correction as deemed by the annotator.The annotators were asked to provide a correctionthat would result in a grammatical sentence if theselected word or phrase would be replaced by thecorrection.In this work, we focus on errors which havebeen marked with the error tag wrong colloca-tion/idiom/preposition.
As preposition errors are notthe focus of this work, we automatically filter outall instances which represent simple substitutions ofprepositions, using a fixed list of frequent Englishprepositions.
In a similar way, we filter out a smallnumber of article errors which were marked as collo-cation errors.
Finally, we filter out instances wherethe annotated phrase or the suggested correction islonger than 3 words, as we observe that they containhighly context-specific corrections and are unlikelyto generalize well (e.g., ?for the simple reasons thatthese can help them??
?simply to?
).After filtering, we end up with 2,747 collocationerrors and their respective corrections, which ac-count for about 6% of all errors in NUCLE.
Thismakes collocation errors the 7th largest class of er-rors in the corpus after article errors, redundancies,prepositions, noun number, verb tense, and mechan-ics.
Not counting duplicates, there are 2,412 distinctcollocation errors and corrections.
Although thereare other error types which are more frequent, collo-cation errors represent a particular challenge as thepossible corrections are not restricted to a closed setof choices and they are directly related to seman-tics rather than syntax.
We analyzed the collocationerrors and found that they can be attributed to thefollowing sources of confusion:Spelling: We suspect that an error is caused by simi-lar orthography if the edit distance between the erro-neous phrase and its correction is less than a certainthreshold.Homophones: We suspect that an error is caused bysimilar pronunciation if the erroneous word and itscorrection have the same pronunciation.
We use theCuVPlus English dictionary (Mitton, 1992) to mapwords to their phonetic representations.Synonyms: We suspect that an error is caused bysynonymy if the erroneous word and its correctionare synonyms in WordNet (Fellbaum, 1998).
We useWordNet 3.0.L1-transfer: We suspect that an error is caused byL1-transfer if the erroneous phrase and its correctionshare a common translation in a Chinese-Englishphrase table.
The details of the phrase table con-struction are described in Section 4.
We note thatalthough we focus on Chinese-English translation,our method is applicable to any language pair whereparallel corpora are available.As CuVPlus and WordNet are defined for indi-vidual words, we extend the matching process tophrases in the following way: two phrases A and Bare deemed homophones/synonyms if they have thesame length and the i-th word in phrase A is a ho-mophone/synonym of the corresponding i-th wordin phrase B.109Spelling .
.
.
it received critics (criticism) as much as complaints .
.
.. .
.
budget for the aged to improvise (improve) other areas.Homophones .
.
.
diverse spending can aide (aid) our country.. .
.
insure (ensure) the safety of civilians .
.
.Synonyms .
.
.
rapid increment (increase) of the seniors .
.
.. .
.
energy that we can apply (use) in the future .
.
.L1-transfer .
.
.
and give (provide, ?? )
reasonable fares to the public .
.
.. .
.
and concerns (attention, ?? )
that the nation put on technology and engineering .
.
.Table 3: Examples of collocation errors with different sources of confusion.
The correction is shown in parenthesis.For L1-transfer, we also show the shared Chinese translation.
The L1-transfer examples shown here do not belong toany of the other categories.Suspected Error Source Tokens TypesSpelling 154 131Homophones 2 2Synonyms 74 60L1-transfer 1016 782L1-transfer w/o spelling 954 727L1-transfer w/o homophones 1015 781L1-transfer w/o synonyms 958 737L1-transfer w/o spelling,homophones, 906 692synonymsTable 2: Analysis of collocation errors.
The threshold forspelling errors is one for phrases of up to six charactersand two for the remaining phrases.The results of the analysis are shown in Table 2.Tokens refer to running erroneous phrase-correctionpairs including duplicates, and types refer to distincterroneous phrase-correction pairs.
As a collocationerror can be part of more than one category, the rowsin the table do not sum up to the total number oferrors.
The number of errors that can be traced toL1-transfer greatly outnumbers all other categories.The table also shows the number of collocation er-rors that can be traced to L1-transfer but not theother sources.
906 collocation errors with 692 dis-tinct collocation error types can be attributed only toL1-transfer but not to spelling, homophones, or syn-onyms.
Table 3 shows some examples of collocationerrors for each category from our corpus.
We notethat there are also collocation error types that cannotbe traced to any of the above sources.
We will returnto these errors in Section 7.4 Correcting Collocation ErrorsIn this section, we propose a novel approach for cor-recting collocation errors in EFL writing.4.1 L1-induced ParaphrasesWe use the popular technique of paraphrasingwith parallel corpora (Bannard and Callison-Burch,2005) to automatically find collocation candidatesfrom a sentence-aligned L1-English parallel corpus.As most of the essays in our corpus are written bynative Chinese speakers, we use the FBIS Chinese-English corpus, which consists of about 230,000Chinese sentences (8.5 million words) from newsarticles, each with a single English translation.
Wetokenize and lowercase the English half of the cor-pus in the standard way.
We segment the Chinesehalf of the corpus using the maximum entropy seg-menter from (Ng and Low, 2004; Low et al, 2005).Subsequently, we automatically align the texts at theword level using the Berkeley aligner (Liang et al,2006; Haghighi et al, 2009).
We extract English-L1and L1-English phrases of up to three words fromthe aligned texts using the widely used phrase ex-traction heuristic in (Koehn et al, 2003).
The para-phrase probability of an English phrase e1 given anEnglish phrase e2 is defined asp(e1|e2) =?fp(e1|f)p(f |e2) (1)where f denotes a foreign phrase in the L1 language.The phrase translation probabilities p(e1|f) andp(f |e2) are estimated by maximum likelihood es-timation and smoothed using Good-Turing smooth-ing (Foster et al, 2006).
Finally, we only keep para-110phrases with a probability above a certain threshold(set to 0.001 in our work).4.2 Collocation Correction with Phrase-basedSMTWe implement our approach in the frameworkof phrase-based statistical machine transla-tion (SMT) (Koehn et al, 2003).
Phrase-basedSMT tries to find the highest scoring translation egiven an input sentence f .
The decoding process offinding the highest scoring translation is guided by alog-linear model which scores translation candidatesusing a set of feature functions hi, i = 1, .
.
.
, nscore(e|f) = exp( n?i=1?ihi(e, f)).
(2)Typical features include a phrase translation proba-bility p(e|f), an inverse phrase translation probabil-ity p(f |e), a language model score p(e), and a con-stant phrase penalty.
The optimization of the featureweights ?i, i = 1, .
.
.
, n can be done using mini-mum error rate training (MERT) (Och, 2003) on adevelopment set of input sentences and their refer-ence translations.Because of the great flexibility of the log-linearmodel, researchers have used the framework forother tasks outside SMT, including grammatical er-ror correction (Brockett et al, 2006).
We adopt asimilar approach in this work.
We modify the phrasetable of the popular phrase-based SMT decoderMOSES (Koehn et al, 2007) to include collocationcorrections with features derived from spelling, ho-mophones, synonyms, and L1-induced paraphrases.?
Spelling: For each English word, the phrase ta-ble contains entries consisting of the word itselfand each word that is within a certain edit dis-tance from the original word.
Each entry has aconstant feature of 1.0.?
Homophones: For each English word, thephrase table contains entries consisting of theword itself and each of the word?s homophones.We determine homophones using the CuVPlusdictionary.
Each entry has a constant feature of1.0.?
Synonyms: For each English word, the phrasetable contains entries consisting of the word it-self and each of its synonyms in WordNet.
If aword has more than one sense, we consider allits senses.
Each entry has a constant feature of1.0.?
L1-paraphrases: For each English phrase, thephrase table contains entries consisting of thephrase and each of its L1-derived paraphrasesas described in Section 4.1.
Each entry has tworeal-valued features: a paraphrase probabilityaccording to Equation 1 and an inverse para-phrase probability.?
Baseline We combine the phrase tables builtfor spelling, homophones, and synonyms.
Thecombined phrase table contains three binaryfeatures for spelling, homophones, and syn-onyms, respectively.?
All We combine the phrase tables fromspelling, homophones, synonyms, and L1-paraphrases.
The combined phrase table con-tains five features: three binary features forspelling, homophones, and synonyms, andtwo real-valued features for the L1-paraphraseprobability and inverse L1-paraphrase proba-bility.Additionally, each phrase table contains the standardconstant phrase penalty feature.
The first four ta-bles only contain collocation candidates for individ-ual words.
We leave it to the decoder to constructcorrections for longer phrases during the decodingprocess if necessary.5 ExperimentsIn this section, we empirically evaluate our approachon real collocation errors in learner English.5.1 Data SetWe randomly sample a development set of 770 sen-tences and a test set of 856 sentences from our cor-pus.
Each sentence contains exactly one collocationerror.
The sampling is performed in a way that sen-tences from the same document cannot end up inboth the development and the test set.
In order to111keep conditions as realistic as possible, we make noattempt to filter the test set in any way.We build phrase tables as described in Section 4.2.For the purpose of the experiments reported in thispaper, we only need to generate phrase table entriesfor words and phrases which actually appear in thedevelopment or test set.5.2 Evaluation MetricsWe conduct an automatic and a human evalua-tion.
Our main evaluation metric is mean recipro-cal rank (MRR) which is the arithmetic mean of theinverse ranks of the first correct answer returned bythe systemMRR = 1NN?i=11rank(i) (3)where N is the size of the test set.
If the system didnot return a correct answer for a test instance, we set1rank(i) to zero.In the human evaluation, we additionally reportprecision at rank k, k = 1, 2, 3, which we calculateas follows:P@k =?a?A score(a)|A| (4)where A is the set of returned answers of rank k orless and score(?)
is a real-valued scoring functionbetween zero and one.5.3 Collocation Error ExperimentsAutomatic correction of collocation errors can con-ceptually be divided into two steps: i) identificationof wrong collocations in the input, and ii) correc-tion of the identified collocations.
In this work, wefocus on the second step and assume that the erro-neous collocation has already been identified.
Whilethis might seem like a simplification, it has been thecommon evaluation setup in collocation error cor-rection (see for example (Wu et al, 2010)).
It alsohas a practical application where the user first selectsa word or phrase and the system displays possiblecorrections.In our experiments, we use the start and end offsetof the collocation error provided by the human anno-tator to identify the location of the collocation error.We fix the translation of the rest of the sentence toits identity.
We remove phrase table entries wherethe phrase and the candidate correction are identi-cal, thus practically forcing the system to changethe identified phrase.
We set the distortion limit ofthe decoder to zero to achieve monotone decoding.We previously observed that word order errors arevirtually absent in our collocation errors.
For thelanguage model, we use a 5-gram language modeltrained on the English Gigaword corpus with modi-fied Kneser-Ney smoothing.
All experiments use thesame language model to allow a fair comparison.We perform MERT training with the popularBLEU metric (Papineni et al, 2002) on the devel-opment set of erroneous sentences and their correc-tions.
As the search space is restricted to changinga single phrase per sentence, training converges rel-atively quickly after two or three iterations.
Afterconvergence, the model can be used to automaticallycorrect new collocation errors.6 ResultsWe evaluate the performance of the proposedmethod on our test set of 856 sentences, each withone collocation error.
We conduct both an automaticand a human evaluation.
In the automatic evalua-tion, the system?s performance is measured by com-puting the rank of the gold answer provided by thehuman annotator in the n-best list of the system.
Welimit the size of the n-best list to the top 100 out-puts.
If the gold answer is not found in the top 100outputs, the rank is considered to be infinity, or inother words, the inverse of the rank is zero.
We alsoreport the number of test instances for which thegold answer was ranked among the top k answers,k = 1, 2, 3, 10, 100.
The results of the automaticevaluation are shown in Table 4For collocation errors, there are usually more thanone possible correct answer.
Therefore, automaticevaluation underestimates the actual performance ofthe system by only considering the single gold an-swer as correct and all other answers as wrong.
Assuch, we carried out a human evaluation for the sys-tems BASELINE and ALL.
We recruited two Englishspeakers to judge a subset of 500 test sentences.
Foreach sentence, a judge was shown the original sen-tence and the 3-best candidates of each of the twosystems.
We restricted human evaluation to the 3-best candidates, as we believe that answers at a rank112Model Rank = 1 Rank ?
2 Rank ?
3 Rank ?
10 Rank ?
100 MRRSpelling 35 41 42 44 44 4.51Homophones 1 1 1 1 1 0.11Synonyms 32 47 52 60 61 4.98Baseline 49 68 80 93 96 7.61L1-paraphrases 93 133 154 216 243 15.43All 112 150 166 216 241 17.21Table 4: Results of automatic evaluation.
Columns two to six show the number of gold answers that are ranked withinthe top k answers.
The last column shows the mean reciprocal rank in percentage.
Bigger values are better.P(A) 0.8076Kappa 0.6152Table 5: Inter-annotator agreement.
P (E) = 0.5.larger than three will not be very useful in a prac-tical application.
The candidates are displayed to-gether in alphabetical order without any informationabout their rank or which system produced them orthe gold answer by the annotator.
The differencebetween the candidates and the original sentence ishighlighted.
The judges were asked to make a bi-nary judgment for each of the candidates on whetherthe proposed candidate is a valid correction of theoriginal or not.
We represent valid corrections witha score of 1.0 and invalid corrections with a scoreof 0.0.
Inter-annotator agreement is reported in Ta-ble 5.
The chance of agreement P (A) is the percent-age of times that the annotators agree, and P (E) isthe expected agreement by chance, which is 0.5 inour case.
The Kappa coefficient is defined asKappa = P(A)?
P(E)1?
P(E)We obtain a Kappa coefficient of 0.6152.
A Kappacoefficient between 0.6 and 0.8 is considered asshowing substantial agreement according to Landisand Koch (1977).
To compute precision at rank k,we average the judgments.
Thus, a system can re-ceive a score of 0.0 (both judgments negative), 0.5(judges disagree), or 1.0 (both judgments positive)for each returned answer.
To compute MRR, wecannot simply average the judgments as MRR re-quires binary judgments on whether an item is cor-rect or not.
Instead, we report MRR on the union andthe intersection of the judgments.
In the first case,the rank of the first correct item is the minimumrank of any item judged correct by either judge.
Inthe second case, the rank of the first correct itemis the minimum rank of any item judged correct byboth judges.
The results for the human evaluationare shown in Table 6.
Our best system ALL outper-forms the BASELINE approach on all measures.
Itreceives a precision at rank 1 of 38.20% and a MRRof 33.16% (intersection) and 57.26% (union).
Ta-ble 7 shows some examples from our test set.Unfortunately, comparison of our results with pre-vious work is complicated by the fact that there cur-rently exists no standard data set for collocation er-ror correction.
We will make our corpus availablefor research purposes in the hope that it will allowresearchers to more directly compare their results infuture.7 AnalysisIn this section, we analyze and categorize those testinstances for which the ALL system could not pro-duce an acceptable correction in the top 3 candi-dates.
We manually analyze 100 test sentences forwhich neither judge had deemed any candidate an-swer to be a valid correction.
Based on our findings,we categorize the 100 sentences into eight categorieswhich are shown below.
Table 8 shows examplesfrom each category.Out-of-vocabulary (21/100) The most frequent rea-son why the system does not produce a good correc-tion is that the erroneous collocation is out of vocab-ulary.
These collocations often involve compoundwords, like man-hours or carefully-nurturing, or in-frequent expressions, like copy phenomena, whichdo not appear in the FBIS parallel corpus.
We ex-pect that this problem can be reduced by using largerparallel corpora for paraphrase extraction.Near miss (18/100) The second largest category113Model Rank = 1 Rank ?
2 Rank ?
3 P@1 P@2 P@3 MRRBaseline 43 | 141 69 | 201 83 | 237 18.40 16.68 15.36 12.13 | 36.60All 137 | 245 176 | 303 204 | 340 38.20 32.87 29.30 33.16 | 57.26Table 6: Results of human evaluation.
Rank and MRR results are shown for the intersection (first value) and union(second value) of human judgments.Original it must be clear, concise and unambiguous to prevent any off-trackGold it must be clear, concise and unambiguous to avoid any off-trackAll it must be clear, concise and unambiguous to avoid any off-trackit must be clear, concise and unambiguous to stop any off-trackit must be clear, concise and unambiguous to block any off-trackBaseline *it must be clear, concise and unambiguous to present any off-trackit must be clear, concise and unambiguous to forestall any off-track*it must be clear, concise and unambiguous to lock any off-trackOriginal although many may agree that public spending on the elderly should be limited .
.
.Gold although many may argue that public spending on the elderly should be limited .
.
.All although many may believe that public spending on the elderly should be limited .
.
.although many may think that public spending on the elderly should be limited .
.
.although many may accept that public spending on the elderly should be limited .
.
.Baseline *although many may agreed that public spending on the elderly should be limited .
.
.
*although many may hold that public spending on the elderly should be limited .
.
.
*although many may agrees that public spending on the elderly should be limited .
.
.Table 7: Examples of test sentences with the top 3 answers of the ALL and BASELINE system.
An answer judgedincorrect by at least one judge is marked with an asterisk (*).Out of vocabulary .
.
.
many illegal copy phenomena (copy phenomena, copies) in china.. .
.
lead to reduced man-hours (man-hours, productivity) as people fall sick .
.
.Near miss .
.
.
smaller groups of people, sometimes even (more, only) individual .. .
.
take pre-emptive actions (activities, measures) .
.
.Function/auxiliary words .
.
.
entertainment an elderly person can have (be, enjoy) .. .
.
and the security issue is solved also (and, too)Discourse specific .
.
.
make other countries respect and fear you (<question mark>, a country).
.
.
will contribute nothing to the accident (explosion, problem) .Spelling errors this incidence (rate, incident) had also resulted in 4 fatalities .
.
.refrigerator did not compromise (yield, comprise) of any moving parts .
.
.Word sense .
.
.
refers to the desire or shortage of a good (better, commodity) and .
.
.. .
.
members are always from different majors (major league, specialties)Preposition constructions .
.
.
can be an area worth investing (investing, investing in).
.
.
in spending their resources (resources, resources on)Others this might redirect (make sound, reduce) foreign investments .
.
.. .
.
a trading hub since british ?s (british ?s, british) rule.Table 8: Examples of sentences without valid corrections by the ALL model.
The top-1 suggestion of the system andthe gold answer (in bold) are shown in parenthesis.114consists of instances where the system barely missesthe gold standard answer.
This includes cases wherethe extracted L1-paraphrases do not contain the ex-act phrase required, e.g., the paraphrase table con-tains even|||only get when the gold correction waseven ?
only, or the phrase table actually containsthe gold answer but fails to rank it among the top 3answers.
The first problem could be addressed bymodifying the phrase extraction heuristic to producemore fine-grained phrase pairs.
The second prob-lem requires a better language model.
Although ourlanguage model is trained on the large English Giga-word corpus, it is not always successful in promot-ing the correct candidate to the top.
The domain mis-match between the newswire domain of Gigawordand student essays could be one reason for this.Function/auxiliary words (14/100) We observethat collocation errors that involve function wordsor auxiliary words are not handled very well by ourmodel.
Function words and auxiliary words in En-glish lack direct counterparts in Chinese, which iswhy the word alignments and therefore the extractedphrases for these words contain a high amount ofnoise.
As function words and auxiliaries are essen-tially a closed set, it might be more promising tobuild separate models with fixed confusion sets forthem.Discourse specific (14/100) Some of the gold an-swers are highly specific to the particular discoursethat they appear in.
As our model corrects colloca-tion errors at the sentence level, such gold answerswill be very difficult or impossible to determine cor-rectly.
Including more context beyond the sentencelevel might help to overcome this problem, althoughit is not easy to integrate this larger context informa-tion.Spelling errors (9/100) Some of the collocation er-rors are caused by spelling mistakes, e.g., incidenceinstead of incident.
Although the ALL model in-cludes candidates which are created through edit dis-tance, paraphrase candidates created from the mis-spelled word can dominate the top 3 ranks, e.g., rateand frequently are paraphrases of incidence.
A pos-sible solution would be to perform spell-checking asa separate pre-processing step prior to collocationcorrection.Word sense (7/100) Some of the failures of themodel can be attributed to ambiguous senses of thecollocation phrase.
As we do not perform wordsense disambiguation in our current work, candi-dates from other word senses can end up as the topcandidates.
Including word sense disambiguationinto the model might help, although accurate wordsense disambiguation on noisy learner text may notbe easy.Preposition constructions (6/100) Some of the col-location errors involve preposition constructions,e.g., the student wrote attend instead of attendto.
Because prepositions do not have a directcounterpart in Chinese, the L1-paraphrases do notmodel their semantics very well.
This category isclosely related to the function/auxiliary word cate-gory.
Again, since prepositions are a closed set, itmight be more promising to build a separate modelfor them.Others (11/100) Other mistakes include collocationerrors where the gold answer slightly changed thesemantics of the target word, e.g., redirect potentialforeign investments ?
reduce potential foreign in-vestments, active-passive alternation (enhanced eco-nomics?
was economical), and noun possessive er-rors (british ?s rule?
british rule).8 Conclusion and Future WorkWe have presented a novel approach for correctingcollocation errors in written learner text.
Our ap-proach exploits the semantic similarity of words inthe writer?s L1-language based on paraphrases ex-tracted from an L1-English parallel corpus.
Our ex-periments on real-world learner data show that ourapproach outperforms traditional approaches basedon edit distance, homophones, and synonyms by alarge margin.In future work, we plan to extend our system tofully automatic collocation correction that involvesboth identification and correction of collocation er-rors.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.115ReferencesC.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceedings ofACL.C.
Brockett, W. B. Dolan, and M. Gamon.
2006.
Cor-recting ESL errors using phrasal SMT techniques.
InProceedings of ACL.A.
J. Carlson, J. Rosen, and D. Roth.
2001.
Scalingup context-sensitive text correction.
In Proceedings ofIAAI.Y.
S. Chan and H. T. Ng.
2005.
Scaling up word sensedisambiguation via parallel texts.
In Proceedings ofAAAI.Y.
C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou.2008.
An automatic collocation writing assistant forTaiwanese EFL learners: A case of corpus-based NLPtechnology.
Computer Assisted Language Learning,21(3):283?299.D.
Dahlmeier and H. T. Ng.
2011.
Grammatical errorcorrection with alternating structure optimization.
InProceedings of ACL.M.
Farghal and H. Obiedat.
1995.
Collocations: A ne-glected variable in EFL.
International Review of App-plied Linguistics, 33.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press, Cambridge,MA.J.
R. Firth.
1957.
Papers in Linguistics 1934-1951.
Ox-ford University Press, London.G.
Foster, R. Kuhn, and H. Johnson.
2006.
Phrasetablesmoothing for statistical machine translation.
In Pro-ceedings of EMNLP.Y.
Futagi, P. Deane, M. Chodorow, and J. Tetreault.
2008.A computational approach to detecting collocation er-rors in the writing of non-native speakers of English.Journal of Computer-Assisted Learning, 21.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing: A meta-classifier approach.In Proceedings of HLT-NAACL.A.
R. Golding and D. Roth.
1999.
A winnow-based ap-proach to context-sensitive spelling correction.
Ma-chine Learning, 34:107?130.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised ITG models.In Proceedings of ACL-IJCNLP.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisti-cal phrase-based translation.
In Proceedings of HLT-NAACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACLdemonstration session.J.
R. Landis and G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1).P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proceedings of HLT-NAACL.A.
L. Liu, D. Wible, and N. L. Tsao.
2009.
Automatedsuggestions for miscollocations.
In Proceedings of theNAACL HLT Workshop on Innovative Use of NLP forBuilding Educational Applications.C.
Liu, D. Dahlmeier, and H. T. Ng.
2010a.
PEM: aparaphrase evaluation metric exploiting parallel texts.In Proceedings of EMNLP.C.
Liu, D. Dahlmeier, and H. T. Ng.
2010b.TESLA: Translation evaluation of sentences withlinear-programming-based analysis.
In Proceedings ofWMT and MetricsMATR.J.
K. Low, H. T. Ng, and W. Guo.
2005.
A maximumentropy approach to Chinese word segmentation.
InProceedings of the 4th SIGHAN Workshop on ChineseLanguage Processing.N.
Madnani and B. J. Dorr.
2010.
Generating phrasal andsentential paraphrases: A survey of data-driven meth-ods.
Computational Linguistics, 36(3):341?387.D.
McCarthy and R. Navigli.
2007.
Semeval-2007 task10: English lexical substitution task.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations (SemEval-2007).J.
Meng.
2008.
Erroneous collocations caused by lan-guage transfer in Chinese EFL writing.
US-China For-eign Language, 6:57?61.R.
Mitton.
1992.
A description of a computer-usable dic-tionary file based on the Oxford Advanced Learner?sDictionary of Current English.H.
T. Ng and Y. S. Chan.
2007.
Semeval-2007 task 11:English lexical sample task via english-chinese paral-lel text.
In Proceedings of the 4th International Work-shop on Semantic Evaluations (SemEval 2007).H.
T. Ng and J. K. Low.
2004.
Chinese part-of-speechtagging: One-at-a-time or all-at-once?
word-based orcharacter-based?
In Proceedings of EMNLP.H.
T. Ng, B. Wang, and Y. S. Chan.
2003.
Exploitingparallel texts for word sense disambiguation: An em-pirical study.
In Proceedings of ACL.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A method for automatic evaluation of machinetranslation.
In Proceedings of ACL.A.
Rozovskaya and D. Roth.
2010.
Generating confu-sion sets for context-sensitive error correction.
In Pro-ceedings of EMNLP.C.
C. Shei and H. Pain.
2000.
An ESL writer?s collo-cational aid.
Computer Assisted Language Learning,13.116M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009.Fluency, adequacy, or HTER?
Exploring different hu-man judgments with a tunable MT metric.
In Proceed-ings of WMT.M.
Swan and B. Smith.
2001.
Learner English: ATeacher?s Guide to Interference and Other Problems.Cambridge University Press, Cambridge, UK.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Usingparse features for preposition selection and error de-tection.
In Proceedings of ACL.D.
Wible, C. H. Kuo, N. L. Tsao, A. Liu, and H. L. Lin.2003.
Bootstrapping in a language learning environ-ment.
Journal of Computer-Assisted Learning, 19.H.
Wu and M. Zhou.
2003.
Synonymous collocation ex-traction using translation information.
In Proceedingsof ACL.J.
C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang.2010.
Automatic collocation suggestion in academicwriting.
In Proceedings of the ACL 2010 ConferenceShort Papers.Z.
Zhong and H. T. Ng.
2009.
Word sense disambigua-tion for all words without hard labor.
In Proceedingsof IJCAI.117
