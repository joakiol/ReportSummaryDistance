Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 57?65,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsParaphrase assessment in structured vector space:Exploring parameters and datasetsKatrin ErkDepartment of LinguisticsUniversity of Texas at Austinkatrin.erk@mail.utexas.eduSebastian Pad?Department of LinguisticsStanford Universitypado@stanford.eduAbstractThe appropriateness of paraphrases for words de-pends often on context: ?grab?
can replace ?catch?in ?catch a ball?, but not in ?catch a cold?.
Struc-tured Vector Space (SVS) (Erk and Pad?, 2008) isa model that computes word meaning in contextin order to assess the appropriateness of such para-phrases.
This paper investigates ?best-practice?
pa-rameter settings for SVS, and it presents a method toobtain large datasets for paraphrase assessment fromcorpora with WSD annotation.1 IntroductionThe meaning of individual occurrences or tokens ofa word can change vastly according to its context.
Acentral challenge for computational lexical semanticsis describe these token meanings and how they can becomputed for new occurrences.One prominent approach to this question is thedictionary-based model of token meaning: The differ-ent meanings of a word are a set of distinct, disjointsenses enumerated in a lexicon or ontology, such asWordNet.
For each new occurrence, determining tokenmeaning means choosing one of the senses, a classifica-tion task known as Word Sense Disambiguation (WSD).Unfortunately, this task has turned out to be very hardboth for human annotators and for machines (Kilgarriffand Rosenzweig, 2000), not at least due to granularityproblems with available resources (Palmer et al, 2007;McCarthy, 2006).
Some researchers have gone so faras to suggest fundamental problems with the concept ofcategorical word senses (Kilgarriff, 1997; Hanks, 2000).An interesting alternative is offered by vector spacemodels of word meaning (Lund and Burgess, 1996; Mc-Donald and Brew, 2004) which characterize the mean-ing of a word entirely without reference to word senses.Word meaning is described in terms of a vector in a high-dimensional vector space that is constructed with dis-tributional methods.
Semantic similarity is then simplydistance to vectors of other words.
Vector space modelshave been most successful in modeling the meaning ofword types (i.e.
in constructing type vectors).
The char-acterization of token meaning by corresponding tokenvectors would represent a very interesting alternative todictionary-based methods by providing a direct, graded,unsupervised measure of (dis-)similarity between wordsin context that completely avoids reference to dictionarysenses.
However, there are still considerable theoreticaland practical problems, even though there is a substan-tial body of work (Landauer and Dumais, 1997; Sch?tze,1998; Kintsch, 2001; Mitchell and Lapata, 2008).In a recent paper (Erk and Pad?, 2008), we have intro-duced the structured vector space ( SVS) model whichaddresses this challenge.
It yields one token vector perinput word.
Token vectors are not computed by com-bining the lexical meaning of the surrounding words ?which risks resulting in a ?topicality?
vector ?
but bymodifying the type meaning of a word with the semanticexpectations of syntactically related words, which canbe thought of as selectional preferences.
For example,in catch a ball, the token vector for ball is computed bycombining the type vector of ball with a vector for theselectional preferences of catch for its object.
The to-ken vector for catch, conversely, is constructed from thetype vector of catch and the inverse object preferencevector of ball.
The resulting token vectors describe themeaning of a word in a particular sentence not through asense label, but through the distance of the token vectorto other vectors.A natural question that arises is how vector-basedmodels of token meaning can be evaluated.
It is ofcourse possible to apply them to a traditional WSDtask.
However, this strategy remains vulnerable to allcriticism concerning the annotation of categorical wordsenses, and also does not take advantage of the vec-tor models?
central asset, namely gradedness.
Thus,paraphrase-based assessment for models of token mean-ing was proposed as a representation-neutral disam-biguation task that can replace WSD (McCarthy andNavigli, 2007; Mitchell and Lapata, 2008).
Given aword token in context and a set of potential paraphrases,the task consists of identifying the subset of valid para-phrases.
For example, in the following example, thefirst paraphrase is appropriate, but the second is not:(1) Google acquired YouTube ?Google bought YouTube(2) How children acquire skills 6?How children buy skillsThis task is graded in the sense that there is no dis-joint set of labels from which exactly one is picked foreach token; rather, the paraphrases form a set of labelsof which a subset is appropriate for each word token,57and the appropriate sets for two tokens may overlap tovarying degrees.
In an ideal vector-based model, validparaphrases such as (1) should possess similar vectors,and invalid ones such as (2) dissimilar ones.In Erk and Pad?
(2008), we evaluated SVS on twovariants of the paraphrase assessment test: first, the pre-diction of human judgments on a seven-point scale forparaphrases for verb-subject pairs (Mitchell and Lap-ata, 2008); and second, the original Lexical Substitutiontask by McCarthy and Navigli (2007).
To avoid overfit-ting, we optimized our parameters on the first datasetand evaluated only the best model on the second dataset.However, given evidence for substantial inter-task differ-ences, it is unclear to what extent these parameters areoptimal beyond the Mitchell and Lapata dataset.
Thispaper addresses this question with two experiments:Impact of parameters.
We re-examine three centralparameters of SVS.
The first one is the choice of vectorcombination function.
Following Mitchell and Lap-ata (2008), we previously used componentwise multi-plication, whose interpretation in vector space is notstraightforward.
The second one is reweighting.
Weobtained the best performance when the context expec-tations were reweighted by taking each component toa (high) n-th power, which is counterintuitive.
Finally,we found subjects to be more informative in judgingthe appropriateness of paraphrases than objects.
Thisappears to contradict work in theoretical syntax (Levinand Rappaport Hovav, 2005).To reassess the role of these parameters, we constructa controlled dataset of transitive instances from the Lex-ical Substitution corpus to reexamine and investigatethese issues, with the aim of providing ?best practice?settings for SVS.
This turns out to be more difficult thanexpected, leading us to suspect that a globally optimalparameter setting across tasks may simply not exist.
Wealso test a simple extension of SVS that uses a richercontext (both subject and object) to construct the tokenvector, with first positive results.Dataset creation.
The Lexical Substitution datasetused in Erk and Pad?
(2008) was very small, which lim-its the conclusions that can be drawn from it.
This pointstowards a more general problem of paraphrase-basedassessment for models of token meaning: Until now, alldatasets for this task were specifically created by hand.It would provide a strong boost for paraphrase assess-ment if the large annotated corpora that are available forWSD could be reused.We present an experiment on converting the WordNet-annotated SemCor corpus into a set of ?pseudo-paraphrases?
for paraphrase-based assessment.
We usethe synonyms and direct hypernyms of an annotatedsynset as these ?pseudo-paraphrases?.
While the syn-onyms and hypernyms are not guaranteed to work asdirect replacements of the target word in the given con-text, they are semantically similar to the target word.The result is a dataset ten times larger than the Lex-Sub dataset.
As we describe in this paper, we find thatthis method is nevertheless problematic: The resultingdataset is considerably more difficult to model than theexisting hand-built paraphrase corpora, and its proper-ties differ considerably from the manually constructedLexical Substitution dataset.2 The structured vector space modelThe main intuition behind the SVS model is to treat theinterpretation of a word in context as guided by expecta-tions about typical events.
This move to include typicalarguments and predicates into a model of word meaningis motivated both on cognitive and linguistic grounds.In cognitive science, the central role of expectationsabout typical events on almost all aspects of humanlanguage processing is well-established (McRae et al,1998; Narayanan and Jurafsky, 2002).
In linguistics, ex-pectations have long been used in semantic theories inthe form of selectional restrictions and selectional pref-erences (Wilks, 1975), and more recently induced fromcorpora (Resnik, 1996).
Attention has mostly been lim-ited to selectional preferences of verbs, which have beenused for for a variety of tasks (Hindle and Rooth, 1993;Gildea and Jurafsky, 2002).
A recent result that the SVSmodel builds on is that selectional preferences can berepresented as prototype vectors constructed from seenarguments (Erk, 2007; Pad?
et al, 2007).Representing lemma meaning.
To accommodate in-formation about semantic expectations, the SVS modelextends the traditional representation of word meaningas a single vector by a set of vectors, each of whichrepresents the word?s selectional preferences for eachrelation that the word can assume in its linguistic con-text.
While we ultimately think of these relations as?properly semantic?
in the sense of semantic roles, theinstantiation of SVS we consider in this paper makesuse of dependency relations as a level of representationthat generalizes over a substantial amount of surfacevariation but that can be obtained automatically withhigh accuracy using current NLP tools.The idea is illustrated in Figure 1.
In the representa-tion of the verb catch, the central square stands for thelexical vector of catch itself.
The three arrows link it tocatch ?s preferences for dependency relations it can par-ticipate in, such as for its subjects, its objects, and forverbs for which it appears as a complement (comp?1).The figure shows the head words that enter into the com-putation of the selectional preference vector.
Likewise,ball is represented by one vector for ball itself, one forball ?s preferences for its modifiers (mod), and two forthe verbs of which it can occur as a subject (subj?1)and an object (obj?1), respectively.This representation includes selectional preferences(like subj, obj, mod) exactly parallel to inverse selec-tional preferences (subj?1, obj?1, comp?1).
The SVSmodel is then formalized as follows.
Let D be a vectorspace, and let R be some set of relation labels.
We then58catchhefielderdogcoldbaseballdriftobjsubjaccusesayclaimcomp-1ballwhirlflyprovidethrowcatchorganiseobj-1subj-1modredgolfelegantFigure 1: Structured Vector Space representations fornoun ball and verb catch : Each box represents onevector (lexical information or expectations)represent the meaning of a lemma w as a triplem(w) = (vw, R,R?1)where vw ?
D is the type vector of the word w itself,R : R ?
D maps each relation label onto a vectorthat describes w?s selectional preferences, and R?1 :R ?
D maps from role labels to vectors describinginverse selectional preferences of w. Both R and R?1are partial functions.
For example, the direct objectpreference is undefined for intransitive verbs.1Computing meaning in context.
SVS computes themeaning of a word a in the context of another wordb via their selectional preferences as follows: Letm(a) = (va, Ra, R?1a ) and m(b) = (vb, Rb, R?1b ) bethe representations of the two words, and let r ?
Rbe the relation linking a to b.
Then, the meaning of aand b in this context is defined as a pair of structuredvector triples: m(ar?
b) is the meaning of a with b asits r-argument, and m(br?1?
a) the meaning of b as ther-argument of a:m(ar?
b) =(va R?1b (r), Ra ?
{r}, R?1a)m(br?1?
a) =(vb Ra(r), Rb, R?1b ?
{r})(3)where v1  v2 is a direct vector combination functionas in traditional models, e.g.
addition or component-wise multiplication.
If either Ra(r) or R?1b (r) are notdefined, the combination fails.
Afterward, the filledargument position r is deleted from Ra and R?1b .Figure 2 illustrates the procedure on the representa-tions from Figure 1.
The dotted lines indicate that thelexical vector for catch is combined with the inverseobject preference of ball.
Likewise, the lexical vectorfor ball combines with the object preference vector ofcatch.Recursive application.
In Erk and Pad?
(2008), weconsidered only one combination step; however, the1We use separate functions R, R?1 rather than a jointsyntactic context preference function because (a) this sepa-ration models the conceptual difference between predicatesand arguments, and (b) it allows for a simpler, more elegantformulation of the computation of meaning in context in Eq.
3.catch...coldbaseballdriftobjsubj...comp-1ball...throwcatchorganiseobj-1subj-1mod...!
!Figure 2: Combining predicate and argument viarelation-specific semantic expectationssyntactic context of a word in a dependency tree oftenconsists of more than one word.
It seems intuitivelyplausible that disambiguation should profit from morecontext information.
Thus, we extend SVS with recur-sive application.
Let a stand in relation r to b. Asdefined above, the result of combining m(a) and m(b)by relation r are two structured vector triples m(ar?
b)and m(br?1?
a).
If a also stands in relation s 6= r toa word c with m(c) = (va, Ra, R?1a ), we define themeaning of a in the context of b and c canonically asm(m(ar?
b)s?
c) =((va R?1b (r)) R?1c (s),Ra ?
{r, s}, R?1a)(4)If  is associative and commutative, then m(m(ar?b)s?
c) = m(m(as?
c)r?
b).
This will be the casefor all the combination functions we use in this paper.Note that this is a simplistic model of the influenceof multiple context words: it computes only lexicalmeaning recursively, but does not model the influenceof context on the selectional preferences.
For example,the subject selectional preferences of catch are identicalto those of catch the ball, even though one would ex-pect that the outfielder corresponds much better to theexpectations of catch the ball than of just catch.3 Experimental SetupThe task that we are considering is paraphrase assess-ment in context.
Given a predicate-argument pair anda paraphrase candidate, the models have to decide howappropriate the paraphrase is for the predicate-argumentcombination.
This is the main task against which tokenvector models have been evaluated in the past (Mitchelland Lapata, 2008; Erk and Pad?, 2008).
In Experi-ment 1, we use manually created paraphrases.
In Exper-iment 2, we replaces human-generated paraphrases with?pseudo-paraphrases?, contextually similar words thatmay not be completely appropriate as paraphrases in thegiven context, but can be collected automatically.
Ourparameter choices for SVS are as similar as possible tothe second experiment of our earlier paper.Vector space.
We use a dependency-based vectorspace that counts a target word and a context word59as co-occurring in a sentence if they are connected byan ?informative?
path in the dependency graph for thesentence.2 We build the space from a Minipar-parsedversion of the British National Corpus with dependencyparses obtained from Minipar (Lin, 1993).
It uses rawco-occurrence counts and 2000 dimensions.Selectional preferences and reweighting.
We usea prototype-based selectional preference model (Erk,2007).
It models the selectional preferences of a predi-cate for an argument position as the weighted centroidof the vectors for all head words seen for this positionin a large corpus.
Let f(a, r, b) denote the frequency ofa occurring in relation r to b in the parsed BNC.
Then,we compute the selectional preferences as:R?b(r) =1N?a:f(a,r,b)>0f(a, r, b) ?
~va (5)where N is the number of fillers a with f(a, r, b) > 0.In Erk and Pad?
(2008), we found that applying areweighting step to the selectional preference vector bytaking each component of the centroid vector R?b(r) tothe n-th power lead to substantial improvements.
Themotivation for this technique is to alleviate noise aris-ing from the use of unfiltered head words for the con-struction.
The reweighted selectional preference vectorRb(r) is defined as:Rb(r) = ?vn1 , .
.
.
, vnm?
for R?b(r) = ?v1, .
.
.
, vm?
(6)where we write ?v1, .
.
.
, vm?
for the sequence of valuesthat make up a vector R?b(r).
Inverse selectional pref-erences R?1b (r) of nouns are defined analogously, bycomputing the centroid of the verbs seen as governorsof the noun in relation r.In this paper, we test reweighting parameters of n be-tween 0.5 and 30.
Generally, small ns will decrease theinfluence of the selectional preference vector.
The resultcan be thought of as a ?word type vector modified bycontext expectations?, while large ns increase the roleof context, until we arrive at a ?contextual expectationvector modified by the word type vector?.
3Vector combination.
We test three vector combina-tion functions , which have different interpretationsin vector space.
The simplest one is componentwiseaddition, abbreviated as add, i.e., simple vector addi-tion.4 With addition, context dimensions receive a highcount whenever either of the two vectors has a highco-occurrence count for the context.2We used the minimal context specification and plainweight of the DependencyVectors software package.3For the component-wise minimum combination (see be-low), where we normalize the vectors before the combination,the reweighting has a different effect.
It shifts most of the massonto the largest-value dimensions and sets smaller dimensionsto values close to zero.4Since we subsequently focus on cosine similarity, whichis length-invariant, vector addition can also be interpreted ascentroid computation.Next, we test component-wise multiplication (mult).This operation is more difficult to interpret in terms ofvector space, since it does not correspond to the standardinner or outer vector products.
The most straightforwardinterpretation is to reinterpret the second vector as a di-agonal matrix, i.e., as a linear transformation of the firstvector.
Large entries in the second vector increase theweight of the corresponding contexts; small entries de-crease it.
Mitchell and Lapata (2008) found this methodto yield the best results.The third vector combination function we consideris component-wise minimum (min).
This combinationfunction results in a vector with high counts only forcontexts which co-occur frequently with both input vec-tors and can thus be understood as an intersection be-tween the two context sets.
Since the entries of twovectors need to be on the same order to magnitude forthis method to yield meaningful results, we normalizevectors before the combination for min.Assessing models of token meaning.
Given a transi-tive verb v with subject a and direct object b, we testthree variants of computing a token vector for v. Thefirst two involve only one combination step.
In the subjcondition, v?s type vector is combined with the inversesubject preference vector of a.
In the obj condition, v?stype vector is combined with the inverse object pref-erence vector of b.
The third variant is the recursiveapplication of the SVS combination procedure describedin Section 2 (condition both).
Specifically, we combinev?s type vector with both a?s inverse subject preferenceand with b?s inverse object preference to obtain a ?richer?token vector.In all three cases, the resulting token vector is com-pared to the type vector of the paraphrase (in Experi-ment 1) or the semantically related word (in Experiment2).
We use Cosine Similarity, a standard choice as vectorspace similarity measure.4 Experiments4.1 Experiment 1: The impact of parametersIn our 2008 paper, we tested the LexSub data only withthe parameters that showed best results on the Mitchelland Lapata data: vector combination using component-wise multiplication (mult), and the computation of (in-verse) selectional preference vectors with high powersof n = 20 or n = 30.
However, there were indicationsthat the two datasets showed fundamental differences.In particular, the Mitchell and Lapata data could only bemodeled using a PMI-transformed vector space, whilethe LexSub data could only be modeled using raw co-occurrence count vectors.Another one of our findings that warrants further in-quiry stems from our comparison of different contextchoices (verb plus subject, verb plus object, noun plusembedding verb).
We found that subjects are better dis-ambiguators than objects.
This seems counterintuitiveboth on theoretical and empirical grounds.
Theoretically,60Sentence SubstitutesBy asking people who workthere, I have since determinedthat he didn?t.
(# 2002)be employed 4;labour 1Remember how hard your ances-tors worked.
(# 2005)toil 4; labour 3;task 1Figure 3: Lexical substitution example items for ?work?the notion of verb phrase has been motivated, amongother things, with the claim that direct objects contributemore to a verb?s disambiguation than subjects (Levinand Rappaport Hovav, 2005).
Empirically, subjectsare known to be realized more often as pronouns thanobjects, which makes their vector representations lesssemantically specific.
However, we used two differentdatasets ?
the subject results on a set of intransitiveverbs, and the object results on a set of transitive verbs,so the results are not comparable.In this experiment, we construct a new, more con-trolled dataset from the Lexical Substitution corpus tosystematically assess the importance of the three mainparameters: the relation used for disambiguation, thecombination function, and the reweighting parameter.Construction of the LEXSUB-PARA dataset.
Theoriginal Lexical Substitution corpus, constructed for theSemEval-1 lexical substitution task (McCarthy and Nav-igli, 2007), consists of 10 instances each of 200 targetwords in sentential contexts, drawn from a large inter-net corpus (Sharoff, 2006).
Contextually appropriateparaphrases for each instance of each target word wereelicited from up to 6 participants.
Figure 3 shows two in-stances for the verb to work.
The frequency distributionover paraphrases can be understood as a characterizationof the target word?s meaning in each context.For the current paper, we constructed a new subset ofLexSub we call LEXSUB-PARA by parsing LexSub withMinipar (Lin, 1993) and extracting all 177 sentenceswith transitive verbs that had overtly realized subjectsand objects, regardless of voice.
We did not manuallyverify the correctness of the parses, but discarded 17sentences where we were not able to compute inverseselectional preferences for the subject or object headword (these were mostly rare proper names).
This left160 transitive instances of 42 verbs.Evaluation For evaluation, we use a variant of the Se-mEval ?out of ten?
(OOT) evaluation metrics defined byMcCarthy and Navigli (2007).
They developed two met-rics, OOT Precision and Recall, which compare where apredicted set of appropriate paraphrases must be evalu-ated against a gold standard set.
Their metrics are called?out of ten?
because they are measure the accuracy of thefirst ten paraphrases predicted by the system.
Since theyallow systems to abstain from predictions for any num-ber of tokens, their two variants average this accuracy(a), over the tokens with a prediction (OOT Precision),and (b), over all tokens (OOT Recall).
Since our system0.5 1 2 5 10 20add obj 61.5 59.7 58.9 56.1 56.0 55.7add subj 61.7 61.7 59.5 58.4 57.3 57.0add both 61.3 60.0 60.2 57.7 57.1 56.7mult obj 59.8 59.7 57.8 55.7 55.7 55.4mult subj 60.3 59.7 59.3 57.3 57.7 56.7mult both 59.9 58.8 57.1 55.8 55.3 <1Prmin obj 60.2 60.0 59.5 57.3 55.7 55.8min subj 62.2 60.5 59.1 58.5 57.8 57.0min both 62.3 60.2 59.8 57.3 55.8 55.1Table 1: OOT accuracy on the LEXSUB-PARA datasetacross models and reweighting values (best results foreach model boldfaced).
Random baseline: 53.7.
Targettype vector baseline: 57.1.
Pr: Numerical problem.produces predictions for all tokens, OOT Precision andRecall become identical.Formally, let Gi be the gold paraphrases for occur-rence i, and let f(s, i) be the frequency with which shas been named as paraphrase for i.
Let Mi be the tenparaphrase candidates top-ranked by the SVS model fori.
We write out-of-ten accuracy (OOT) as:OOT = 1/|I|?i?s?Mi?Gif(s, i)?s?Gif(s, i)(7)We compute two baselines.
The first one is randombaseline that guesses whether paraphrases are appropri-ate.
The second baseline uses the original type vectorof the target verb without any combination, i.e., its ?outof context meaning?, as representation for the token.Results.
Table 1 shows the results on the LEXSUB-PARA dataset.
Recall that the task is to decide the ap-propriateness of paraphrases for verb instances, disam-biguated by the inverse selectional preferences of theirsubjects (subj), their objects (obj), and both.
The ran-dom baseline attains an OOT accuracy of 53.7, and thetype vector of the target vector performs at 57.1.SVS is able to outperform both baselines for all val-ues of the reweighting parameter n <2, and we find thebest results for the lowest value, n = 0.5.
As for theinfluence of the vector combination function, the bestresult is yielded by min (OOT=62.3), followed by add(OOT=61.7), while mult shows generally worse results(OOT=60.3).
For both add andmult, using only the sub-ject as context only is optimal.
The overall best result,using min, is seen for both; however, the improvementover subj is very small.In the model mult-both-20, where target vectors weremultiplied with two very large expectation vectors, al-most all instances failed due to overflow errors.Discussion.
Our results indicate that our parameteroptimization strategy in Erk and Pad?
(2008) was in factflawed.
The parameters that were best for the Mitchelland Lapata (2008) data (mult, n = 20) are suboptimalfor LEXSUB-PARA data.5 The good results for low val-5We assume that our results hold for the Pad?
& Erk (2008)lexical substitution dataset as well, due to its similar nature.61ues of n indicate that good discrimination between validand invalid paraphrases can be obtained by relativelysmall modifications of the target vector in the directionindicated by the context.
Surprisingly, we still find thatthe results in the subj condition are almost always betterthan those in the obj condition, even though the datasetconsists only of transitive verbs, where we would haveexpected the inverse result.
We have two partial ex-planations.
First, we find that pronouns, which occurfrequently in subject position (I, he), are still informa-tive enough to distinguish ?animate?
from ?inanimate?paraphrases of verbs such as touch.
Second, we seea higher number of Minipar errors in for object posi-tions than for subject positions, and consequently moredata both for object fillers and for object selectionalpreferences.The overall best result was yielded by a condition thatused both (subject plus object) for disambiguation, usingthe recursive modification from Eq.
(4).
While we seethis as a promising result, the difference to the second-best result is very small, in almost all other conditionsthe performance of both is close to the average of objand subj and thus a suboptimal choice.4.2 Experiment 2: Creating larger datasets withpseudo-paraphrasesWith a size of 2,000 sentences, even the completeLexSub dataset is tiny in comparison to many otherresources in NLP.
Limiting attention to successfullyparsed transitive instances results in an even smallerdataset on which it is difficult to distinguish noise fromgenuine differences between models.
This is a largeproblem for the use of paraphrase appropriateness asevaluation task for models of word meaning in context.In consequence, the automatic creation of largerdatasets is an important task.
While unsupervised meth-ods for paraphrase induction are becoming available(e.g., Callison-Burch (2008)), they are still so noisythat the created datasets cannot serve as gold standards.However, there is an alternative strategy: there is aconsiderable amount of data in different languages an-notated with categorical word sense, created (e.g.)
forWord Sense Disambiguation exercises such as Senseval.We suggest to convert these data for use in a task similarto paraphrase assessment, interpreting available infor-mation about the word sense as pseudo-paraphrases.Of course, the caveat is that these pseudo-paraphrasesmay behave differently than genuine paraphrases.
Toinvestigate this issue, we repeat Experiment 1 on thisdataset.Construction of the SEMCOR-PARA dataset TheSemCor corpus is a subset of the Brown corpus thatcontains 23,346 lemmas annotated with senses accord-ing to WordNet 1.6.
Fortunately, WordNet provides arich characterization of word senses.
This allows usto use the WordNet synonyms of a given word senseas pseudo-paraphrases.
Since it can be the case thatthe target word is the only word in a synset, we also0.5 1 2 5 10 20add obj 21.7 20.7 23.2 24.3 24.2 21.8add subj 20.6 20.1 22.9 24.4 23.3 19.7add both 21.1 20.3 23.2 24.4 23.3 18.9mult obj 22.6 24.8 25.0 24.4 24.2 21.4mult subj 21.1 23.9 24.4 24.4 23.5 19.8mult both 24.5 24.5 25.6 24.3 20.0 17.4min obj 20.9 19.5 23.6 24.4 24.3 21.9min subj 20.1 19.6 22.5 24.2 23.9 19.6min both 20.1 19.8 25.2 24.5 24.3 19.0Table 2: OOT accuracy on the SEMCOR-PARA datasetacross models and reweighting values (best results foreach line boldfaced).
Random baseline: 19.6.
Targettype vector baseline: 20.8need to add direct hypernyms.
Direct hypernyms havebeen used in annotation tasks to characterize WordNetsenses (Mihalcea and Chklovski, 2003), an indicatorthat they are usually close enough in meaning to func-tion as pseudo-paraphrases.Again, we parsed the corpus with Minipar and iden-tified all sense-tagged instances of the verbs fromLEXSUB-PARA, to keep the two corpora as compa-rable as possible.
For each instance wi of word w, wecollected all synonyms and direct hypernyms of thesynset as the set of appropriate paraphrases.
The listof synonyms and direct hypernyms of all other sensesof w, whether they occur in SemCor or not, were con-sidered inappropriate paraphrases for the instance wi.This method does not provide us with frequencies forthe pseudo-paraphrases; we thus assumed a uniform fre-quency of 1.
This does not do away with the gradednessof the meaning representation, though, since each tokenis still associated with a set of appropriate paraphrases.Out of 2242 transitive verb instances, we further re-moved 153 since we could not compute selectional pref-erences for at least one of the fillers.
484 instances wereremoved because WordNet did not list any verbal para-phrases for the annotated synset or its direct hypernym.This resulted in 1605 instances for 40 verbs, a datasetan order of magnitude larger than LEXSUB-PARA.
(SeeSection 4.3 for an example verb with paraphrases.
)Results and Discussion.
We again use the OOT ac-curacy measure.
The results for paraphrase assessmenton SEMCOR-PARA are shown in Table 2.
The numbersare substantially lower than for LEXSUB-PARA.
Thisis first and foremost a consequence of the higher ?poly-semy?
of the pseudo-paraphrases.
In LEXSUB-PARA,the average numbers of possible paraphrases per tar-get word is 20; in SEMCOR-PARA, 54.
This is to beexpected and also reflected in the much lower randombaseline (19.6% OOT).
However, we also observe thatthe reduction in error rate over the baseline is consider-ably lower for SEMCOR-PARA than for LEXSUB-PARA(10% vs. 20% reduction).Among the parameters of the model, we find thelargest impact for the reweighting parameter.
The bestresults occur in the middle range(n = 2 and n = 5),62with both lower and higher weights yielding consid-erably lower scores.
Apparently, it is more difficultto strike the right balance between the target and theexpectations on this dataset.
This is also mirrored inthe smaller improvement of the target type vector base-line over the random baseline.
As for vector combi-nation functions, we find the best results for the more?intersection?-like mult and min combinations, withsomewhat lower results for add; however, the differ-ences are rather small.
Finally, combination with objworks better than combination with subj.
At least amongthe best results, both is able to improve over the use of ei-ther individual relation.
The best result uses mult-both,with an OOT accuracy of 25.6.4.3 Further analysisIn our two experiments, we have found systematic rela-tionships between the SVS model parameters and theirperformance within the LEXSUB-PARA and SEMCOR-PARA datasets.
Unfortunately, few of the parameter set-tings we found to work well appear to generalize acrossthe two datasets; neither do they correspond to the op-timal parameter values we established for the Mitchelland Lapata dataset in our 2008 paper.
Variables thatvary particularly strikingly are the reweighting parame-ter and the performance of different relations.
To betterunderstand these differences, we perform a further vali-dation analysis that attempts to link model performanceto a variable that (a) behaves consistently across the twodatasets used in this paper and (b) sheds light onto thepatterns we have observed for the parameters.The quantity we will use for this purpose is the aver-age discriminativity of the model.
We define discrimina-tivity as the degree to which the token vector computedby the model is on average more similar to the valid thanto the invalid paraphrases.
For a paraphrase orderingtask such as the one we are considering, we want thisquantity to be as large as possible; very small quantitiesindicate that the model is basically ?guessing?
an order.Figure 4 plots disciminativity against model perfor-mance.
As can be expected, it is indeed a very strongcorrelation between discriminativity and OOT accu-racy across all models.
A Pearson?s correlation testconfirms that the correlation is highly significant forboth datasets (LEXSUB-PARA: r=0.65, p < 0.0001;SEMCOR-PARA: r=0.76, p < 0.0001).Next, we considered the relationship between themean discriminativity for different combinations andreweighting values n. Figure 5 shows the resulting plots,which reveal two main differences between the datasets.The first one is the influence of the reweighting parame-ter.
For LEXSUB-PARA, the highest discriminativity isfound for small values of n, with decreasing values forhigher parameter values.
In contrast, SEMCOR-PARAshows the highest discriminativity for middle values ofn (on the order of 5?10), with lowest values on eitherside.
The second difference is the relative discrimina-tivity of obj and subj.
On LEXSUB-PARA, the subjpredictions are more discriminative than obj predictionsfor all values of n. On SEMCOR-PARA, this picture isreversed, with more discriminative obj predictions forthe best (and thus relevant) values of n.We interpret these patterns, which fit the observedOOT accuracy numbers well, as additional evidence thatthe variations we see between the datasets are not noiseor artifacts of the setup, but arise due to the differentmakeup of the two datasets.
This ties in with our intu-itions about the differences between human-generatedparaphrases and WordNet ?pseudo-paraphrases?.
Com-pare the following paraphrase lists:dismiss (LexSub): banish, deride, discard, discharge, dis-patch, excuse, fire, ignore, reject, release, remove, sackdismiss (SemCor/WordNet): alter, axe, brush, can, change,discount, displace, disregard, dissolve, drop, farewell,fire, force, ignore, modify, notice, packing, push, reject,remove, sack, send, terminate, throw, usherThe SEMCOR-PARA list contains a larger number ofunspecific pseudo-paraphrases such as change, push,send, which stem from direct WordNet hypernyms ofthe more specific dismiss senses.
Presumably, theseterms are assigned rather general vectors which the SVSfinds difficult to rule out as paraphrases.
This lowersthe discriminativity of the models, in particular for subj,and results in the smaller relative improvement overthe baseline we observe for SEMCOR-PARA.
This sug-gests that the usability of word sense-derived datasetsin evaluations could be improved by taking depth in theWordNet hierarchy into account when including directhypernyms among the pseudo-paraphrases.5 ConclusionsIn this paper, we have explored the parameter spacefor the computation of vector-based representations oftoken meaning with the SVS model.Our evaluation scenario was paraphrase assessment.To systematically assess the impact of parameter choice,we created two new controlled datasets.
The first one,the LEXSUB-PARA dataset, is a small subset of the Lex-ical Substitution corpus (McCarthy and Navigli, 2007)that was specifically created for this task.
The seconddataset, SEMCOR-PARA, which is considerably larger,consists in instances from the SemCor corpus whoseWordNet annotation was automatically converted into?pseudo-paraphrase?
annotation.6We found a small number of regularities that hold forboth datasets: namely, that the reweighting parameteris the most important choice for a SVS model, followedby the relation used as context, while the influence ofthe vector combination function is comparatively small.Unfortunately, the actual settings of these parametersappeared not to generalize well from one dataset tothe other.
We have collected evidence that these diver-gences are not due to noise, but to genuine differences6Both datasets can be obtained from the authors.63ll ll llllllllllll l lllllllllllll l lllllllllll0.018 0.020 0.022 0.024 0.026 0.028 0.0300.560.570.580.590.600.610.62mean sim(val)?sim(inval)outof ten precisionlllllll llllll llllllll llllll llllllllll llll0.005 0.010 0.015 0.020 0.025 0.0300.180.200.220.24mean sim(val)?sim(inval)out of tenprecisionFigure 4: Scatterplot of "out of ten" accuracy against model discriminativity between valid and invalid paraphrases.Left: LEXSUB-PARA, right: SEMCOR-PARA.0 5 10 15 20 25 300.0200.0220.0240.0260.028exponentmeansim(val)?sim(inval)target + object selpreftarget + subject selpref0 5 10 15 20 25 300.0050.0100.0150.0200.025exponentmeansim(val)?sim(inval)target + object selpreftarget + subject selprefFigure 5: Average amount to which predictions are more similar to valid than to invalid paraphrases, for differentreweighting values.
Left: LEXSUB-PARA, right: SEMCOR-PARA.in the datasets.
We describe an auxiliary quantity, dis-criminativity, that measures the ability of the model?spredictions to distinguish between valid and invalid para-phrases.The consequence we draw from this study is that itis surprisingly difficult to establish generalizable ?bestpractice?
parameter setting for SVS.
Good parametervalues appear to be sensitive to the properties of datasets.For example, we have attributed the observation thatsubjects are more informative on LEXSUB-PARA, whileobjects work better on SEMCOR-PARA, to differencesin the set of paraphrase competitors.
In this regard,the conversion of the WSD corpus can be considered apartial success.
We have constructed the largest existingparaphrase assessment corpus.
However, the use ofWordNet information to create paraphrases results in avery difficult corpus.
We will investigate methods thatexclude overly general hypernyms of the target words asparaphrases to alleviate the problems we see currently.Discriminativity further suggests that paraphrase as-sessment can be improved by selectional preferencerepresentations that are trained to maximize the dis-tance between valid and invalid paraphrases.
Such arepresentation could be provided by discriminative for-mulations (Bergsma et al, 2008), or by exemplar-basedmodels that are able to deal better with the ambiguitypresent in the preferences of very general words.Another important topic for further research is thecomputation of token vectors that incorporate more thanone context word.
The current results we obtain for?both?
are promising but limited; it appears that the suc-cessful integration of multiple context words requiresstrategies that go beyond simplistic addition or intersec-tion of observed contexts.ReferencesS.
Bergsma, D. Lin, and R. Goebel.
2008.
Discrimina-tive learning of selectional preference from unlabeledtext.
In Proceedings of EMNLP, pages 59?68.C.
Callison-Burch.
2008.
Syntactic constraints on para-phrases extracted from parallel corpora.
In Proceed-ings of EMNLP, pages 196?205.K.
Erk and S. Pad?.
2008.
A structured vector spacemodel for word meaning in context.
In Proceedingsof EMNLP.64K.
Erk.
2007.
A simple, similarity-based model for se-lectional preferences.
In Proceedings of ACL, pages216?223.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.P.
Hanks.
2000.
Do word meanings exist?
Computersand the Humanities, 34(1-2):205?215.D.
Hindle and M. Rooth.
1993.
Structural ambigu-ity and lexical relations.
Computational Linguistics,19(1):103?120.A.
Kilgarriff and J. Rosenzweig.
2000.
Frameworkand results for English Senseval.
Computers and theHumanities, 34(1-2).A.
Kilgarriff.
1997.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113.W.
Kintsch.
2001.
Predication.
Cognitive Science,25:173?202.T.
Landauer and S. Dumais.
1997.
A solution to Platosproblem: the latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.Psychological Review, 104(2):211?240.B.
Levin and M. Rappaport Hovav.
2005.
ArgumentRealization.
Research Surveys in Linguistics Series.CUP.D.
Lin.
1993.
Principle-based parsing without overgen-eration.
In Proceedings of ACL, pages 112?120.K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-ments, and Computers, 28.D.
McCarthy and R. Navigli.
2007.
SemEval-2007Task 10: English Lexical Substitution Task.
In Pro-ceedings of SemEval, pages 48?53.D.
McCarthy.
2006.
Relating WordNet senses for wordsense disambiguation.
In Proceedings of the ACLWorkshop on Making Sense of Sense, pages 17?24.S.
McDonald and C. Brew.
2004.
A distributionalmodel of semantic context effects in lexical process-ing.
In Proceedings of ACL, pages 17?24.K.
McRae, M. Spivey-Knowlton, and M. Tanenhaus.1998.
Modeling the influence of thematic fit (andother constraints) in on-line sentence comprehension.Journal of Memory and Language, 38:283?312.R.
Mihalcea and T. Chklovski.
2003.
Open MindWord Expert: Creating large annotated data collec-tions with web users?
help.
In Proceedings of theEACL 2003 Workshop on Linguistically AnnotatedCorpora (LINC 2003), Budapest, Hungary.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL,pages 236?244.S.
Narayanan and D. Jurafsky.
2002.
A Bayesian modelpredicts human parse preference and reading time insentence processing.
In Proceedings of NIPS, pages59?65.S.
Pad?, U.
Pad?, and K. Erk.
2007.
Flexible, corpus-based modelling of human plausibility judgements.In Proceedings of EMNLP/CoNLL, pages 400?409.M.
Palmer, H. Dang, and C. Fellbaum.
2007.
Makingfine-grained and coarse-grained sense distinctions,both manually and automatically.
Journal of NaturalLanguage Engineering.
To appear.P.
Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computationalrealization.
Cognition, 61:127?159.H.
Sch?tze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1):97?124.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
International Journalof Corpus Linguistics, 11(4):435?462.Y.
Wilks.
1975.
Preference semantics.
In FormalSemantics of Natural Language.
CUP.65
