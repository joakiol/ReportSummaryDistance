Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 126?134,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsGeneralizing a Strongly Lexicalized Parser using Unlabeled DataTejaswini Deoskar1, Christos Christodoulopoulos2, Alexandra Birch1, Mark Steedman11School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB2University of Illinois, Urbana-Champaign, Urbana, IL 61801{tdeoskar,abmayne,steedman}@inf.ed.ac.uk, christod@illinois.eduAbstractStatistical parsers trained on labeled datasuffer from sparsity, both grammatical andlexical.
For parsers based on stronglylexicalized grammar formalisms (such asCCG, which has complex lexical cate-gories but simple combinatory rules), theproblem of sparsity can be isolated tothe lexicon.
In this paper, we show thatsemi-supervised Viterbi-EM can be usedto extend the lexicon of a generative CCGparser.
By learning complex lexical entriesfor low-frequency and unseen words fromunlabeled data, we obtain improvementsover our supervised model for both in-domain (WSJ) and out-of-domain (ques-tions and Wikipedia) data.
Our learntlexicons when used with a discriminativeparser such as C&C also significantly im-prove its performance on unseen words.1 IntroductionAn important open problem in natural languageparsing is to generalize supervised parsers, whichare trained on hand-labeled data, using unlabeleddata.
The problem arises because further hand-labeled data in the amounts necessary to signif-icantly improve supervised parsers are very un-likely to be made available.
Generalization is alsonecessary in order to achieve good performance onparsing in textual domains other than the domainof the available labeled data.
For example, parserstrained on Wall Street Journal (WSJ) data suffer afall in accuracy on other domains (Gildea, 2001).In this paper, we use self-training to generalizethe lexicon of a Combinatory Categorial Gram-mar (CCG) (Steedman, 2000) parser.
CCG is astrongly lexicalized formalism, in which everyword is associated with a syntactic category (sim-ilar to an elementary syntactic structure) indicat-ing its subcategorization potential.
Lexical en-tries are fine-grained and expressive, and containa large amount of language-specific grammaticalinformation.
For parsers based on strongly lexical-ized formalisms, the problem of grammar general-ization can be cast largely as a problem of lexicalextension.The present paper focuses on learning lexi-cal categories for words that are unseen or low-frequency in labeled data, from unlabeled data.Since lexical categories in a strongly lexicalizedformalism are complex, fine-grained (and far morenumerous than simple part-of-speech tags), theyare relatively sparse in labeled data.
Despite per-forming at state-of-the-art levels, a major sourceof error made by CCG parsers is related to unseenand low-frequency words (Hockenmaier, 2003;Clark and Curran, 2007; Thomforde and Steed-man, 2011).
The unseen words for which we learncategories are surprisingly commonplace words ofEnglish; examples are conquered, apprehended,subdivided, scoring, denotes, hunted, obsessed,residing, migrated (Wikipedia).
Correctly learn-ing to parse the predicate-argument structures as-sociated with such words (expressed as lexical cat-egories in the case of CCG), is important for open-domain parsing, not only for CCG but indeed forany parser.We show that a simple self-training method,Viterbi-EM (Neal and Hinton, 1998) when usedto enhance the lexicon of a strongly-lexicalizedparser can be an effective strategy for self-trainingand domain-adaptation.
Our learnt lexicons im-prove on the lexical category accuracy of two su-pervised CCG parsers (Hockenmaier (2003) andthe Clark and Curran (2007) parser, C&C) onwithin-domain (WSJ) and out-of-domain test sets(a question corpus and a Wikipedia corpus).In most prior work, when EM was initializedbased on labeled data, its performance did not im-prove over the supervised model (Merialdo, 1994;126Charniak, 1993).
We found that in order for per-formance to improve, unlabeled data should beused only for parameters which are not well cov-ered by the labeled data, while those that are wellcovered should remain fixed.In an additional contribution, we compare twostrategies for treating unseen words (a smoothing-based, and a part-of-speech back-off method) andfind that a smoothing-based strategy for treat-ing unseen words is more effective for semi-supervised learning than part-of-speech back-off.2 Combinatory Categorial GrammarCombinatory Categorial Grammar (CCG) (Steed-man, 2000) is a strongly lexicalized grammarformalism, in which the lexicon contains alllanguage-specific grammatical information.
Thelexical entry of a word consists of a syntactic cat-egory which expresses the subcategorization po-tential of the word, and a semantic interpretationwhich defines the compositional semantics (Lewisand Steedman, 2013).
A small number of combi-natory rules are used to combine constituents, andit is straightforward to map syntactic categories toa logical form for semantic interpretation.For statistical CCG parsers, the lexicon is learntfrom labeled data, and is subject to sparsity dueto the fine-grained nature of the categories.
Fig-ure 1 illustrates this with a simple CCG deriva-tion.
In this sentence, bake is used as a ditransi-tive verb and is assigned the ditransitive categoryS\NP/NP/NP .
This category defines the verb syn-tactically as mapping three NP arguments to a sen-tence S , and semantically as a ternary relation be-tween its three arguments, thus providing a com-plete analysis of the sentence.
[NNPJohn ] [VBDbaked ] [NNPMary] [DTa ] [NNcake]NP S\NP/NP/NP NP NP/N N> >S\NP/NP NP>S\NP<S?John baked Mary a cake?Figure 1: Example CCG derivationFor a CCG parser to obtain the correct deriva-tion above, its lexicon must include the ditransitivecategory S\NP/NP/NP for the verb bake.
It is notsufficient to have simply seen the verb in anothercontext (say a transitive context like ?John baked acake?, which is a more common context).
This isin contrast to standard treebank parsers where theverbal category is simply VBD (past tense verb)and a ditransitive analysis of the sentence is notruled out as a result of the lexical category.In addition to sparsity related to open-classwords like verbs as in the above example, there arealso missing categories in labeled data for closed-class words like question words, due to the smallnumber of questions in the Penn Treebank.
In gen-eral, lexical sparsity for a statistical CCG parsercan be broken down into three types: (i) where aword is unseen in training data but is present intest data, (ii) where a word is seen in the train-ing data but not with the category type requiredin the test data (but the category type is seen withother words) and (iii) where a word bears a cate-gory type required in the test data but the categorytype is completely unseen in the training data.In this paper, we deal with the first two kinds.The third kind is more prevalent when the sizeof labeled data is comparatively small (although,even in the case of the English WSJ CCG tree-bank, there are several attested category types thatare entirely missing from the lexicon, Clark et al.,2004).
We make the assumption here that all cat-egory types in the language have been seen in thelabeled data.
In principle new category types maybe introduced independently without affecting oursemi-supervised process (for instance, manually,or via a method that predicts new category typesfrom those seen in labeled data).3 Related WorkPrevious attempts at harnessing unlabeled data toimprove supervised CCG models using methodslike self-training or co-training have been unsat-isfactory (Steedman et al., 2003, 43-44).
Steed-man et al.
(2003) experimented with self-traininga generative CCG parser, and co-training a genera-tive parser with an HMM-based supertagger.
Co-training (but not self-training) improved the resultsof the parser when the seed labeled data was small.When the seed data was large (the full treebank),i.e., the supervised baseline was high, co-trainingand self-training both failed to improve the parser.More recently, Honnibal et al.
(2009) improvedthe performance of the C&C parser on a domain-adaptation task (adaptation to Wikipedia text) us-ing self-training.
Instead of self-training the pars-ing model, they re-train the supertagging model,which in turn affects parsing accuracy.
Theyobtained an improvement of 1.09% (dependency127score) on supertagger accuracy on Wikipedia (al-though performance on WSJ text dropped) but didnot attempt to re-train the parsing model.An orthogonal approach for extending a CCGlexicon using unlabeled data is that of Thomfordeand Steedman (2011), in which a CCG category foran unknown word is derived from partial parsesof sentences with just that one word unknown.The method is capable of inducing unseen cate-gories types (the third kind of sparsity mentionedin ?2.1), but due to algorithmic and efficiency is-sues, it did not achieve the broad-coverage neededfor grammar generalisation of a high-end parser.
Itis more relevant for low-resource languages whichdo not have substantial labeled data and categorytype discovery is important.Some notable positive results for non-CCGparsers are McClosky et al.
(2006) who use aparser-reranker combination.
Koo et al.
(2008)and Suzuki et al.
(2009) use unsupervised word-clusters as features in a dependency parser to getlexical dependencies.
This has some notional sim-ilarity to categories, since, like categories, clus-ters are less fine-grained than words but more fine-grained than POS-tags.4 Supervised ParserThe CCG parser used in this paper is a re-implementation of the generative parser of Hock-enmaier and Steedman (2002) and Hockenmaier(2003)1, except for the treatment of unseen andlow-frequency words.We use a model (the LexCat model in Hock-enmaier (2003)) that conditions the generation ofconstituents in the parse tree on the lexical cate-gory of the head word of the constituent, but not onthe head word itself.
While fully-lexicalized mod-els that condition on words (and thus model word-to-word dependencies) are more accurate than un-lexicalized ones like the LexCat model, we usean unlexicalized model2for two reasons: first,1These generative models are similar to the Collins?
head-based models (Collins, 1997), where for every node, a head isgenerated first, and then a sister conditioned on the head.
De-tails of the models are in Hockenmaier and Steedman (2002)and Hockenmaier 2003:pg 166.2A terminological clarification: unlexicalized here refersto the model, in the sense that head-word information isnot used for rule-expansion.
The formalism itself (CCG)is referred to as strongly-lexicalized, as used in the title ofthe paper.
Formalisms like CCG and LTAG are consid-ered strongly-lexicalized since linguistic knowledge (func-tions mapping words to syntactic structures/semantic inter-pretations) is included in the lexicon.our lexicon smoothing procedure (described in thenext section) introduces new words and new cat-egories for words into the lexicon.
Lexical cate-gories are added to the lexicon for seen and un-seen words, but no new category types are intro-duced.
Since the LexCat model conditions rule ex-pansions on lexical categories, but not on words, itis still able to produce parses for sentences withnew words.
In contrast, a fully lexicalized modelwould need all components of the grammar to besmoothed, a task that is far from trivial due to theresulting explosion in grammar size (and one thatwe leave for future work).Second, although lexicalized models performbetter on in-domain WSJ data (the LexCat modelhas an accuracy of 87.9% on Section 23, as op-posed to 91.03% for the head-lexicalized modelin Hockenmaier (2003) and 91.9% for the C&Cparser), our parser is more accurate on a questioncorpus, with a lexical category accuracy of 82.3%,as opposed to 71.6% and 78.6% for the C&C andHockenmaier (2003) respectively.4.1 Handling rare and unseen wordsExisting CCG parsers (Hockenmaier (2003) andClark and Curran (2007)) back-off rare and unseenwords to their POS tag.
The POS-backoff strategyis essentially a pipeline approach, where wordsare first tagged with coarse tags (POS tags) andfiner tags (CCG categories) are later assigned, bythe parser (Hockenmaier, 2003) or the supertag-ger (Clark and Curran, 2007).
As POS-taggersare much more accurate than parsers, this strat-egy has given good performance in general forCCG parsers, but it has the disadvantage that POS-tagging errors are propagated.
The parser cannever recover from a tagging error, a problem thatis serious for words in the Zipfian tail, where thesewords might also be unseen for the POS taggerand hence more likely to be tagged incorrectly.This issue is in fact more generally relevant thanfor CCG parsers alone?the dependence of parserson POS-taggers was cited as one of the problemsin domain-adaptation of parsers in the NAACL-2012 shared task on parsing the web (Petrov andMcDonald, 2012).
Lease and Charniak (2005)obtained an improvement in the accuracy of theCharniak (2000) parser on a biomedical domainsimply by training a new POS tagger model.In the following section, we describe an alter-native smoothing-based approach to handling un-128seen and rare words.
This method is less sen-sitive to POS tagging errors, as described below.In this approach, in a pre-processing step priorto parsing, categories are introduced into the lex-icon for unseen and rare words from the data tobe parsed.
Some probability mass is taken fromseen words/categories and given to unseen wordand category pairs.
Thus, at parse time, no word isunseen for the parser.4.1.1 SmoothingIn our approach, we introduce lexical entries forwords from the unlabeled corpus that are unseenin the labeled data, and also add categories to ex-isting entries for rarely seen words.
The most gen-eral case of this would be to assign all known cat-egories to a word.
However, doing this reducesthe lexical category accuracy.3A second option,chosen here, is to limit the number of categoriesassigned to the word by using some informationabout the word (for instance, its part-of-speech).Based on the part-of-speech of an unseen word inthe unlabeled or test corpus, we add an entry to thelexicon of the word with the top n categories thathave been seen with that part-of-speech in the la-beled data.
Each new entry of (w, cat), where wis a word and cat is a CCG category, is associatedwith a count c(w, cat), obtained as described be-low.
Once all (w, cat) entries are added to the lex-icon along with their counts, a probability modelP (w|cat) is calculated over the entire lexicon.Our smoothing method is based on a methodused in Deoskar (2008) for smoothing a PCFGlexicon.
Eq.
1 and 2 apply it to CCG entries forunseen and rare words.
In the first step, an out-of-the-box POS tagger is used to tag the unlabeledor test corpus (we use the C&C tagger).
Countsof words and POS-tags ccorpus(w, T ) are obtainedfrom the tagged corpus.
For the CCG lexicon, weultimately need a count for a word w and a CCGcategory cat.
To get this count, we split the countof a word and POS-tag amongst all categories seenwith that tag in the supervised data in the sameratio as the ratio of the categories in the super-vised data.
In Eq.
1, this ratio is ctb(catT)/ctb(T )where ctb(catT) is the treebank count of a cate-gory catTseen with a POS-tag T , and ctb(T ) is themarginal count of the tag T in the treebank.
This3For instance, we find that assigning all categories to un-seen verbs gives a lexical category accuracy of 52.25 %, asopposed to an accuracy of 65.4% by using top 15 categories,which gave us the best results, as reported later in Table 3.ratio makes a more frequent category type morelikely than a rarer one for an unseen word.
For ex-ample, for unseen verbs, it would make the transi-tive category more likely than a ditransitive one(since transitives are more frequent than ditran-sitives).
There is an underlying assumption herethat relative frequencies of categories and POS-tags in the labeled data are maintained in the un-labeled data, which in fact can be thought of asa prior while estimating from unlabeled data (De-oskar et al., 2012).ccorpus(w, cat) =ctb(catT)ctb(T )?
ccorpus(w, T ) (1)Additionally, for seen but low-frequency words,we make use of the existing entry in the lexicon.Thus in a second step, we interpolate the countccorpus(w, cat) of a word and category with thesupervised count of the same ctb(w, cat) (if it ex-ists) to give the final smoothed count of a word andcategory csmooth(w, cat) (Eq.
2).csmooth(w, cat) = ?
?
ctb(w, cat) +(1?
?)
?
ccorpus(w, cat)(2)When this smoothed lexicon is used with aparser, POS-backoff is not necessary since allneeded words are now in the lexicon.
Lexical en-tries for words in the parse are determined not bythe POS-tag from a tagger, but directly by the pars-ing model, thus making the parse less susceptibleto tagging errors.5 Semi-supervised LearningWe use Viterbi-EM (Neal and Hinton, 1998) asthe self-training method.
Viterbi-EM is an alter-native to EM where instead of using the modelparameters to find a true posterior from unlabeleddata, a posterior based on the single maximum-probability (Viterbi) parse is used.
Viterbi-EMhas been used in various NLP tasks before andoften performs better than classic EM (Cohenand Smith, 2010; Goldwater and Johnson, 2005;Spitkovsky et al., 2010).
In practice, a given pars-ing model is used to obtain Viterbi parses of un-labeled sentences.
The Viterbi parses are thentreated as training data for a new model.
This pro-cess is iterated until convergence.Since we are interested in learning the lexi-con, we only consider lexical counts from Viterbiparses of the unlabeled sentences.
Other parame-ters of the model are held at their supervised val-ues.
We conducted some experiments where we129self-trained all components of the parsing model,which is the usual case of self-training.
We ob-tained negative results similar to Steedman et al.
(2003), where self-training reduced the perfor-mance of the parsing model.
We do not reportthem here.
Thus, using unlabeled data only to es-timate parameters that are badly estimated fromlabeled data (lexical entries in CCG, due to lexi-cal sparsity) results in improvements, in contrastto prior work with semi-supervised EM.As is common in semi-supervised settings, wetreated the count of each lexical event as theweighted count of that event in the labeled data(treebank)4and the count from the Viterbi-parsesof unlabeled data.
Here we follow Bacchiani et al.
(2006) and McClosky et al.
(2006) who show thatcount merging is more effective than model inter-polation.We placed an additional constraint on the con-tribution that the unlabeled data makes to the semi-supervised model?we only use counts (from un-labeled data) of lexical events that are rarelyseen/unseen in the labeled data.
Our reasoningwas that many lexical entries are estimated accu-rately from the treebank (for example, those re-lated to function words and other high-frequencywords) and estimation from unlabeled data mighthurt them.
We thus had a cut-off frequency (ofwords in labeled data) above which we did notallow the unlabeled counts to affect the semi-supervised model.
In practise, our experimentsturned out to be fairly insensitive to the value ofthis parameter, on evaluations over rare or un-seen verbs.
However, overall accuracy would dropslightly if this cut-off was increased.
We experi-mented with cut-offs of 5, 10 and 15, and foundthat the most conservative value (of 5) gave thebest results on in-domain WSJ experiments, and ahigher value of 10 gave the best results for out-of-domain experiments.We also conducted some limited experimentswith classical semi-supervised EM, with similarsettings of weighting labeled counts, and using un-labeled counts only for rare/unseen events.
Sinceit is a much more computationally expensive pro-cedure, and most of the results did not come closeto the results of Viterbi-EM, we did not pursue it.4The labeled count is weighted in order to scale up the la-beled data which is usually smaller in size than the unlabeleddata, to avoid swamping the labeled counts with much largerunlabeled counts.5.1 DataLabeled: Sec.
02-21 of CCGbank (Hockenmaierand Steedman, 2007).
In one experiment, we usedSec.
02-21 minus 1575 sentences that were heldout to simulate test data containing unseen verbs?see ?6.2 for details.Unlabeled: For in-domain experiments, we usedsentences from the unlabeled WSJ portion of theACL/DCI corpus (LDC93T1, 1993), and the WSJportion of the ANC corpus (Reppen et al., 2005),limited to sentences containing 20 words or less,creating datasets of approximately 10, 20 and 40million words each.
Additionally, we have adataset of 140 million words ?
40M WSJ wordsplus an additional 100M from the New YorkTimes.For domain-adaptation experiments, we usetwo different datasets.
The first one consistsof question-sentences ?
1328 unlabeled ques-tions, obtained by removing the manual annota-tion of the question corpus from Rimell and Clark(2008).
The second out-of-domain dataset con-sists of Wikipedia data, approximately 40 millionwords in size, with sentence length < 20 words.5.2 Experimental setupWe ran our semi-supervised method using ourparser with a smoothed lexicon (from ?4.1.1) asthe initial model, on unlabeled data of differentsizes/domains.
For comparison, we also ran ex-periments using a POS-backed off parser (the orig-inal Hockenmaier and Steedman (2002) LexCatmodel) as the initial model.
Viterbi-EM convergedat 4-5 iterations.
We then parsed various test setsusing the semi-supervised lexicons thus obtained.In all experiments, the labeled data was scaled tomatch the size of the unlabeled data.
Thus, thescaling factor of labeled data was 10 for unlabeleddata of 10M words, 20 for 20M words, etc.5.3 EvaluationWe focused our evaluations on unseen and low-frequency verbs, since verbs are the most impor-tant open-class lexical entries and the most am-biguous to learn from unlabeled data (approx.
600categories, versus 150 for nouns).
We report lexi-cal category accuracy in parses produced using oursemi-supervised lexicon, since it is a direct mea-sure of the effect of the lexicon.5We discuss four5Dependency recovery accuracy is also used to evaluateperformance of CCG parsers and is correlated with lexical130All words All Verbs UnseenVerbsSUP 87.76 78.10 52.54SEMISUP 88.14 78.46 **57.28SUPbkoff87.91 76.08 54.14SEMISUPbkoff87.79 75.68 54.60Table 1: Lexical category accuracy on TEST-4SEC**: p < 0.004, McNemar testexperiments below.
The first two are on in-domain(WSJ) data.
The last two are on out-of-domaindata ?
a question corpus and a Wikipedia corpus.6 Results6.1 In-domain: WSJ unseen verbsOur first testset consists of a concatenation of 4sections of CCGbank (01, 22, 24, 23), a total of7417 sentences, to form a testset called TEST-4SEC.
We use all these sections in order to geta reasonable token count of unseen verbs, whichwas not possible with Sec.
23 alone.Table 1 shows the performance of the smoothedsupervised model (SUP) and the semi-supervisedmodel (SEMISUP) on this testset.
There is a sig-nificant improvement in performance on unseenverbs, showing that the semi-supervised modellearns good entries for unseen verbs over andabove the smoothed entry in the supervised lexi-con.
This results in an improvement in the over-all lexical category accuracy of the parser on allwords, and all verbs.We also performed semi-supervised training us-ing a supervised model that treated unseen wordswith a POS-backoff strategy SUPbkoff.
We usedthe same settings of cut-off and the same scal-ing of labeled counts as before.
The supervisedbacked-off model performs somewhat better thanthe supervised smoothed model.
However, it didnot improve as much as the smoothed one fromunlabeled data.
Additionally, the overall accuracyof SEMISUPbkofffell below the supervised level,in contrast to the smoothed model, where overallnumbers improved.
This could indicate that theaccuracy of a POS tagger on unseen words, es-pecially verbs, may be an important bottleneck insemi-supervised learning.Low-frequency verbs We also obtain improve-ments on verbs that are seen but with a low fre-quency in the labeled data (Table 2).
We dividedcategory accuracy, but a dependency evaluation is more rele-vant when comparing performance with parsers in other for-malisms and does not have much utility here.Freq.
Bin 1-5 6-10 11-20SUP 64.13 75.19 77.6SEMISUP 66.72 76.21 79.8Table 2: Seen but rare verbs, TEST-4SECverbs occurring in TEST-4SEC into different binsaccording to their occurrence frequency in the la-beled data (bins of frequency 1-5, 6-10 and 11-20).Semi-supervised training improves over the super-vised baseline for all bins of low-frequency verbs.Note that our cut-off frequency for using unlabeleddata is 5, but there are improvements in the 6-10and 11-20 bins as well, suggesting that learningbetter categories for rare words (below the cut-off)impacts the accuracy of words above the cut-off aswell, by affecting the rest of the parse positively.6.2 In-domain : heldout unseen verbsThe previous section showed significant improve-ment in learning categories for verbs that are un-seen in the training sections of CCGbank.
How-ever, these verbs are in the Zipfian tail, and for thisreason have fairly low occurrence frequencies inthe unlabeled corpus.
In order to estimate whetherour method will give further improvements in thelexical categories for these verbs, we would needunlabeled data of a much larger size.
We there-fore designed an experimental scenario in whichwe would be able to get high counts of unseenverbs from a similar size of unlabeled data.
Wefirst made a list of N verbs from the treebank andthen extracted all sentences containing them (ei-ther as verbs or otherwise) from CCGbank trainingsections.
These sentences form a testset of 1575sentences, called TEST-HOV (for held out verbs).The verbs in the list were chosen based on occur-rence frequency f in the treebank, choosing allverbs that occurred with a frequency of f = 11.This number gave us a large enough set and agood type/token ratio to reliably evaluate and ana-lyze our semi-supervised models?112 verb types,with 1115 token occurrences6.
Since these verbsare actually mid-frequency verbs in the superviseddata, they have a correspondingly large occurrencefrequency in the unlabeled data, occurring muchmore often than true unseen verbs.
Thus, the un-labeled data size is effectively magnified?as faras these verbs are concerned, the unlabeled data isapproximately 11 times larger than it actually is.Table 3 shows lexical category accuracy on6Selecting a different but close value of f such as f = 10or f = 12 would have also served this purpose.131All Words All Verbs UnseenVerbsSUP 87.26 74.55 65.49SEMISUP 87.78 75.30 *** 70.43SUPbkoff87.58 73.06 67.25SEMISUPbkoff87.52 72.89 68.05Table 3: Lexical category accuracy in TEST-HOV.
***p<0.0001, McNemar test556065700 10 20 40 140Size of Unlabelled Data (in millions of words)Lexical Category Accuracyfor Unseen VerbsTest:HOVTest:4SecFigure 2: Increasing accuracy on unseen verbswith increasing amounts of unlabeled data.this testset.
The baseline accuracy of the parseron these verbs is much higher than that on thetruly unseen verbs.7The semi-supervised model(SEMISUP) improves over the supervised modelSUP very significantly on these unseen verbs.
Wealso see an overall improvement on all verbs (seenand unseen) in the test data, and in the over-all lexical category accuracy as well.
Again, thebacked-off model does not improve as much asthe smoothed model, and moreover, overall per-formance falls below the supervised level.Figure 2 shows the effect of different sizes ofunlabeled data on accuracy of unseen verbs forthe two testsets TEST-HOV and TEST-4SEC .
Im-provements are monotonic with increasing unla-beled data sizes, up to 40M words.
The additional100M words of NYT also improve the models butto a lesser degree, possibly due to the difference indomain.
The graphs indicate that the method willlead to more improvements as more unlabeled data(especially WSJ data) is added.7This could be because verbs in the Zipfian tail have moreidiosyncratic subcategorization patterns than mid-frequencyverbs, and thus are harder for a parser.
Another reason is thatthey may have been seen as nouns or other parts of speech,leading to greater ambiguity in their case.QUESTIONS WIKIPEDIAAll wh All Unseenwords words words wordsSUP 82.36 61.77 84.31 79.5SEMISUP *83.21 63.22 *85.6 80.25Table 4: Out-of-domain: Questions andWikipedia, *p<0.05, McNemar test6.2.1 Out-of-DomainQuestions The question corpus is not strictly adifferent domain (since questions form a differ-ent kind of construction rather than a different do-main), but it is an interesting case of adaptationfor several reasons: WSJ parsers perform poorlyon questions due to the small number of questionsin the Penn Treebank/CCGbank.
Secondly, unsu-pervised adaptation to questions has not been at-tempted before for CCG (Rimell and Clark (2008)did supervised adaptation of their supertagger).The supervised model SUP already performsat state-of-the-art on this corpus, on both overallscores and on wh(question)-words alone.
C&Cand Hockenmaier (2003) get 71.6 and 78.6% over-all accuracies respectively, and only 33.6 and 50.7on wh-words alone.
To our original unlabeledWSJ data (40M words), we add 1328 unlabeledquestion-sentences from Rimell and Clark, 2008,scaled by ten, so that each is counted ten times.
Wethen evaluated on a testset containing questions(500 question sentences, from Rimell and Clark(2008)).
The overall lexical category accuracy onthis testset improves significantly as a result of thesemi-supervised learning (Table 4).
The accuracyon the question words alone (who, what, where,when, which, how, whose, whom) also improvesnumerically, but by a small amount (the numberof tokens that improve are only 7).
This could bean effect of the small size of the testset (500 sen-tences, i.e.
500 wh-words).Wikipedia We obtain statistically significant im-provements in overall scores over a testset consist-ing of Wikipedia sentences hand-annotated withCCG categories (from Honnibal et al.
(2009)) (Ta-ble 4).
We also obtained improvements in lexicalcategory accuracy on unseen words, and on un-seen verbs alone (not shown), but could not provesignificance.
This testset contains only 200 sen-tences, and counts for unseen words are too smallfor significance tests, although there are numericimprovements.
However, the overall improvementis statistically significantly, showing that adaptingthe lexicon alone is effective for a new domain.1326.3 Using semi-supervised lexicons with theC&C parserTo show that the learnt lexical entries may be use-ful to parsers other than our own, we incorpo-rate our semi-supervised lexical entries into theC&C parser to see if it benefits performance.
Wedo this in a naive manner, as a proof of concept,making no attempt to optimize the performanceof the C&C parser (since we do not have accessto its internal workings).
We take all entries ofunseen words from our best semi-supervised lex-icon (word, category and count) and add them tothe dictionary of the C&C supertagger (tagdict).The C&C is a discriminative, lexicalized modelthat is more accurate than an unlexicalized model.Even so, the lexical entries that we learn improvethe C&C parsers performance over and above itsback-off strategy for unseen words.
Table 5 showsthe results on WSJ data TEST-4SEC and TEST-HOV.
There were numeric improvements on theTEST-4SEC test set as shown in Table 58.
We ob-tain significance on the TEST-HOV testset whichhas a larger number of tokens of unseen verbs andentries that were learnt from effectively larger un-labeled data.
We tested two cases: when theseverbs were seen for the POS tagger used to tagthe test data, and when they were unseen for thePOS tagger, and found statistically significant im-provement for the case when the verbs were un-seen for the POS tagger9, indicating sensitivity toPOS-tagger errors.6.4 Entropy and KL-divergenceWe also evaluated the quality of the semi-supervised lexical entries by measuring the over-all entropy and the average Kullback-Leibler (KL)divergence of the learnt entries of unseen verbsfrom entries in the gold testset.
The gold entryfor each verb from the TEST-HOV testset was ob-tained from the heldout gold treebank trees.
Su-pervised (smoothed) and semi-supervised entrieswere obtained from the respective lexicons.
Thesemetrics use the conditional probability of a cate-gory given a word, which is not a factor in thegenerative model (which considers probabilities of8There were also improvements on the question andWikipedia testsets (not shown) (8 and 6 tokens each) but thesize of these testsets is too small for significance.9Note that for this testset TEST-HOV, the numbers are thesupertagger?s accuracy, and not the parser?s.
We were onlyable to retrain the supertagger on training data with TEST-HOV sentences heldout, but could not retrain the parser, de-spite consultation with the authors.TEST-4SEC TEST-HOVPOS-seen POS-unseen(590) (1134) (1134)C&C 62.03 (366) 76.71 (870) 72.39 (821)C&C(enhanced) 63.89 (377) 77.34 (877) *73.98 (839)Table 5: TEST-4SEC: Lexical category accuracy ofC&C parser on unseen verbs.
Numbers in bracketsare the number of tokens.
*p<0.05, McNemar testwords given categories), but provide a good mea-sure of how close the learnt lexicons are to the goldlexicon.
We find that the average KL divergencereduces from 2.17 for the baseline supervised en-tries to 1.40 for the semi-supervised entries.
Theoverall entropy for unseen verb distributions alsogoes down from 2.23 (supervised) to 1.37 (semi-supervised), showing that semi-supervised distri-butions are more peaked, and bringing them closerto the true entropy of the gold distribution (0.93).7 ConclusionsWe have shown that it is possible to learn CCG lex-ical entries for unseen and low-frequency wordsfrom unlabeled data.
When restricted to learningonly lexical entries, Viterbi-EM improved the per-formance of the supervised parser (both in-domainand out-of-domain).
Updating all parameters ofthe parsing model resulted in a decrease in the ac-curacy of the parser.
We showed that the entrieswe learnt with an unlexicalized model were accu-rate enough to also be useful to a highly-accuratelexicalized parser.
It is likely that a lexicalizedparser will provide even better lexical entries.
Thelexical entries continued to improve with increas-ing size of unlabeled data.
For the out-of-domaintestsets, we obtained statistically significant over-all improvements, but we were hampered by thesmall sizes of the testsets in evaluating unseen/whwords.In future work, we would like to add unseen butpredicted category types to the initial lexicon usingan independent method, and then apply the samesemi-supervised learning to words of these types.AcknowledgementsWe thank Mike Lewis, Shay Cohen and the threeanonymous EACL reviewers for helpful com-ments.
This work was supported by the ERC Ad-vanced Fellowship 249520 GRAMPLUS.133ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, and RichardSproat.
2006.
MAP adaptation of stochastic grammars.Computer Speech and Language, 20(1):41?68.Eugene Charniak.
1993.
Statistical Language Learning.
MITPress.Stephen Clark and James R. Curran.
2007.
Wide-CoverageEfficient Statistical Parsing with CCG and Log-LinearModels.
Computational Linguistics, 33(4):493?552.Stephen Clark, Mark Steedman, and James Curran.
2004.Object-extraction and question-parsing using CCG.
InProceedings of EMNLP 2004.Shay Cohen and Noah Smith.
2010.
Viterbi Training forPCFGs: Hardness Results and Competitiveness of Uni-form Initialization.
In Proceedings of ACL 2010.Michael Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of the 35th ACL.Tejaswini Deoskar.
2008.
Re-estimation of Lexical Param-eters for Treebank PCFGs.
In Proceedings of COLING2008.Tejaswini Deoskar, Markos Mylonakis, and Khalil Sima?an.2012.
Learning Structural Dependencies of Words in theZipfian Tail.
Journal of Logic and Computation.Daniel Gildea.
2001.
Corpus Variation and Parser Perfor-mance.
In Proceedings of EMNLP 2001.Sharon Goldwater and Mark Johnson.
2005.
Bias in learningsyllable structure.
In Proceedings of CoNLL05.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D.thesis, School of Informatics, University of Edinburgh.Julia Hockenmaier and Mark Steedman.
2002.
GenerativeModels for Statistical Parsing with Combinatory Catego-rial Grammar.
In ACL40.Julia Hockenmaier and Mark Steedman.
2007.
CCGbank: ACorpus of CCG Derivations and Dependency StructuresExtracted from the Penn Treebank.
Computational Lin-guistics, 33:355?396.Matthew Honnibal, Joel Nothman, and James R. Curran.2009.
Evaluating a Statistial CCG Parser on Wikipedia.In Proceedings of the 2009 Workshop on the People?s WebMeets NLP, ACL-IJCNLP.Terry Koo, Xavier Carreras, and Michael Collins.
2008.
Sim-ple Semi-supervised Dependency Parsing.
In Proceedingsof ACL-08: HLT , pages 595?603.
Association for Com-putational Linguistics, Columbus, Ohio.LDC93T1.
1993.
LDC93T1.
Linguistic Data Consortium,Philadelphia.Matthew Lease and Eugene Charniak.
2005.
Parsing Biomed-ical Literature.
In R. Dale, K.-F. Wong, J. Su, andO.
Kwong, eds., Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing (IJC-NLP?05), vol.
3651 of Lecture Notes in Computer Science,pages 58 ?
69.
Springer-Verlag, Jeju Island, Korea.Mike Lewis and Mark Steedman.
2013.
Combined Distribu-tional and Logical Semantics.
Transactions of the Associ-ation for Computational Linguistics.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective Self-Training for Parsing.
In Proceedingsof HLT-NAACL 2006.Bernard Merialdo.
1994.
Tagging English Text with a Prob-abilistic Model.
Computational Linguistics, 20(2):155?171.Radford M. Neal and Geoffrey E. Hinton.
1998.
A view ofthe EM algorithm that justifies incremental, sparse, andother variants.
In Learning and Graphical Models, pages355 ?
368.
Kluwer Academic Publishers.Slav Petrov and Ryan McDonald.
2012.
Overview of the2012 Shared Task on Parsing the Web.
In First Work-shop on Syntactic Analysis of Non-Canonical Language(SANCL) Workshop at NAACL 2012.Randi Reppen, Nancy Ide, and Keith Suderman.
2005.LDC2005T35, American National Corpus (ANC) SecondRelease.
Linguistic Data Consortium, Philadelphia.Laura Rimell and Stephen Clark.
2008.
Adapting aLexicalized-Grammar Parser to Contrasting Domains.
InProceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP-08).Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, andChristopher D. Manning.
2010.
Viterbi Training ImprovesUnsupervised Dependency Parsing.
In Proceedings ofCoNLL-2010.Mark Steedman.
2000.
The Syntactic Process.
MITPress/Bradford Books.Mark Steedman, Steven Baker, Jeremiah Crim, StephenClark, Julia Hockenmaier, Rebecca Hwa, Miles Osbornn,Paul Ruhlen, and Anoop Sarkar.
2003.
Semi-SupervisedTraining for Statistical Parsing.
Tech.
rep., CLSP WS-02.Jun Suzuki, Hideki Isozaki, Xavier Carreras, and MichaelCollins.
2009.
An Empirical Study of Semi-supervisedStructured Conditional Models for Dependency Parsing.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 551?560.
Association for Computational Linguistics, Singa-pore.Emily Thomforde and Mark Steedman.
2011.
Semi-supervised CCG Lexicon Extension.
In Proceedings of theConference on Empirical Methods in Natural LanguageProcessing, Edinburgh UK.134
