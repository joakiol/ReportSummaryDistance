Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 88?93,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 7: Argument Selection and CoercionJames PustejovskyComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAjamesp@cs.brandeis.eduAnna RumshiskyComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAarum@cs.brandeis.eduAbstractIn this paper, we describe the Argument Se-lection and Coercion task, currently in devel-opment for the SemEval-2 evaluation exercisescheduled for 2010.
This task involves char-acterizing the type of compositional operationthat exists between a predicate and the argu-ments it selects.
Specifically, the goal is toidentify whether the type that a verb selects issatisfied directly by the argument, or whetherthe argument must change type to satisfy theverb typing.
We discuss the problem in detailand describe the data preparation for the task.1 IntroductionIn recent years, a number of annotation schemes thatencode semantic information have been developedand used to produce data sets for training machinelearning algorithms.
Semantic markup schemes thathave focused on annotating entity types and, moregenerally, word senses, have been extended to in-clude semantic relationships between sentence ele-ments, such as the semantic role (or label) assignedto the argument by the predicate (Palmer et al, 2005;Ruppenhofer et al, 2006; Kipper, 2005; Burchardtet al, 2006; Ohara, 2008; Subirats, 2004).In this task, we take this one step further, in thatthis task attempts to capture the ?compositional his-tory?
of the argument selection relative to the pred-icate.
In particular, this task attempts to identify theoperations of type adjustment induced by a predicateover its arguments when they do not match its selec-tional properties.
The task is defined as follows: foreach argument of a predicate, identify whether theentity in that argument position satisfies the type ex-pected by the predicate.
If not, then one needs toidentify how the entity in that position satisfies thetyping expected by the predicate; that is, to identifythe source and target types in a type-shifting (or co-ercion) operation.Consider the example below, where the verb re-port normally selects for a human in subject po-sition as in (1).
Notice, however, that through ametonymic interpretation, this constraint can be vi-olated as demonstrated in (1).
(1) a. John reported in late from Washington.b.
Washington reported in late.Neither the surface annotation of entity extents andtypes, nor assigning semantic roles associated withthe predicate would reflect in this case a crucialpoint: namely, that in order for the typing require-ments of the predicate to be satisfied, what has beenreferred to a type coercion or a metonymy (Hobbs etal., 1993; Pustejovsky, 1991; Nunberg, 1979; Egg,2005) has taken place.The SemEval Metonymy task (Markert and Nis-sim, 2007) was a good attempt to annotate suchmetonymic relations over a larger data set.
This taskinvolved two types with their metonymic variants:(2) i.
Categories for Locations: literal, place-for-people,place-for-event, place-for-product;ii.
Categories for Organizations: literal, organization-for-members, organization-for-event, organization-for-product, organization-for-facility.One of the limitations of this approach, how-ever, is that, while appropriate for these special-ized metonymy relations, the annotation specifica-tion and resulting corpus are not an informative88guide for extending the annotation of argument se-lection more broadly.In fact, the metonymy example in (1) is an in-stance of a much more pervasive phenomenon oftype shifting and coercion in argument selection.For example, in (3) below, the sense annotation forthe verb enjoy should arguably assign similar valuesto both (3a) and (3b).
(3) a. Mary enjoyed drinking her beer .b.
Mary enjoyed her beer.The consequence of this, however, is that, under cur-rent sense and role annotation strategies, the map-ping to a syntactic realization for a given sense ismade more complex, and is in fact, perplexing for aclustering or learning algorithm operating over sub-categorization types for the verb.2 Methodology of AnnotationBefore introducing the specifics of the argument se-lection and coercion task, let us review briefly ourassumptions regarding the role of annotation withinthe development and deployment of computationallinguistic systems.We assume that the features we use for encodinga specific linguistic phenomenon are rich enough tocapture the desired behavior.
These linguistic de-scriptions are typically distilled from extensive the-oretical modeling of the phenomenon.
The descrip-tions in turn form the basis for the annotation valuesof the specification language, which are themselvesthe features used in a development cycle for trainingand testing an identification or labeling algorithmover text.
Finally, based on an analysis and evalu-ation of the performance of a system, the model ofthe phenomenon may be revised, for retraining andtesting.We call this particular cycle of development theMATTER methodology:(4) a.
Model: Structural descriptions providetheoretically-informed attributes derived fromempirical observations over the data;b. Annotate: Annotation scheme assumes a featureset that encodes specific structural descriptions andproperties of the input data;c. Train: Algorithm is trained over a corpus annotatedwith the target feature set;Figure 1: The MATTER Methodologyd.
Test: Algorithm is tested against held-out data;e. Evaluate: Standardized evaluation of results;f. Revise: Revisit the model, annotation specification,or algorithm, in order to make the annotation morerobust and reliable.Some of the current and completed annotation ef-forts that have undergone such a development cycleinclude:?
PropBank (Palmer et al, 2005)?
NomBank (Meyers et al, 2004)?
TimeBank (Pustejovsky et al, 2005)?
Opinion Corpus (Wiebe et al, 2005)?
Penn Discourse TreeBank (Miltsakaki et al, 2004)3 Task DescriptionThis task involves identifying the selectional mech-anism used by the predicate over a particular argu-ment.1 For the purposes of this task, the possible re-lations between the predicate and a given argumentare restricted to selection and coercion.
In selection,the argument NP satisfies the typing requirements ofthe predicate, as in (5).
(5) a.
The spokesman denied the statement (PROPOSITION).b.
The child threw the stone (PHYSICAL OBJECT).c.
The audience didn?t believe the rumor (PROPOSI-TION).Coercion encompasses all cases when a type-shifting operation must be performed on the com-plement NP in order to satisfy selectional require-ments of the predicate, as in (6).
Note that coercionoperations may apply to any argument position in asentence, including the subject, as seen in (6b).
Co-ercion can also be seen as an object of a propositionas in (6c).
(6) a.
The president denied the attack (EVENT ?
PROPOSI-TION).b.
The White House (LOCATION ?
HUMAN) denied thisstatement.c.
The Boston office called with an update (EVENT ?INFO).1This task is part of a larger effort to annotate text with com-positional operations (Pustejovsky et al, 2009).89The definition of coercion will be extended to in-clude instances of type-shifting due to what we termthe qua-relation.
(7) a.
You can crush the pill (PHYSICAL OBJECT) betweentwo spoons.
(Selection)b.
It is always possible to crush imagination (ABSTRACTENTITY qua PHYSICAL OBJECT) under the weight ofnumbers.
(Coercion/qua-relation)In order to determine whether type-shifting hastaken place, the classification task must then in-volve the following (1) identifying the verb senseand the associated syntactic frame, (2) identifyingselectional requirements imposed by that verb senseon the target argument, and (3) identifying semantictype of the target argument.
Sense inventories forthe verbs and the type templates associated with dif-ferent syntactic frames will be provided to the par-ticipants.3.1 Semantic TypesIn the present task, we use a subset of semantic typesfrom the Brandeis Shallow Ontology (BSO), whichis a shallow hierarchy of types developed as a partof the CPA effort (Hanks, 2009; Pustejovsky et al,2004; Rumshisky et al, 2006).
The BSO types wereselected for their prevalence in manually identifiedselection context patterns developed for several hun-dreds English verbs.
That is, they capture commonsemantic distinctions associated with the selectionalproperties of many verbs.The following list of types is currently being usedfor annotation:(8) HUMAN, ANIMATE, PHYSICAL OBJECT, ARTIFACT,ORGANIZATION, EVENT, PROPOSITION, INFORMA-TION, SENSATION, LOCATION, TIME PERIOD, AB-STRACT ENTITY, ATTITUDE, EMOTION, PROPERTY,PRIVILEGE, OBLIGATION, RULEThe subset of types chosen for annotation is pur-posefully shallow, and is not structured in a hierar-chy.
For example, we include both HUMAN and AN-IMATE in the type system along with PHYSICAL OB-JECT.
While HUMAN is a subtype of both ANIMATEand PHYSICAL OBJECT, the system should simplychoose the most relevant type (i.e.
HUMAN) and notbe concerned with type inheritance.
The present setof types may be revised if necessary as the annota-tion proceeds.Figure 2: Corpus Development Architecture4 Resources and Corpus DevelopmentPreparing the data for this task will be done in twophases: the data set construction phase and the an-notation phase.
The first phase consists of (1) select-ing the target verbs to be annotated and compiling asense inventory for each target, and (2) data extrac-tion and preprocessing.
The prepared data is thenloaded into the annotation interface.
During the an-notation phase, the annotation judgments are enteredinto the database, and the adjudicator resolves dis-agreements.
The resulting database representation isused by the exporting module to generate the corre-sponding XML markup or stand-off annotation.
Thecorpus development architecture is shown in Fig.
2.4.1 Data Set Construction PhaseIn the set of target verbs selected for the task, pref-erence will be given to the verbs that are stronglycoercive in at least one of their senses, i.e.
tend toimpose semantic typing on one of their arguments.The verbs will be selected by examining the datafrom several sources, using the Sketch Engine (Kil-garriff et al, 2004) as described in (Rumshisky andBatiukova, 2008).An inventory of senses will be compiled for eachverb.
Whenever possible, the senses will be mappedto OntoNotes (Pradhan et al, 2007) and to the CPApatterns (Hanks, 2009).
For each sense, a set of type90templates will be compiled, associating each sensewith one or more syntactic patterns which will in-clude type specification for all arguments.
For ex-ample, one of the senses of the verb deny is refuseto grant.
This sense is associated with the followingtype templates:(9) HUMAN deny ENTITY to HUMANHUMAN deny HUMAN ENTITYThe set of type templates for each verb will be builtusing a modification of the CPA technique (Hanksand Pustejovsky, 2005; Pustejovsky et al, 2004)).A set of sentences will be randomly extracted foreach target verb from the BNC (BNC, 2000) andthe American National Corpus (Ide and Suderman,2004).
This choice of corpora should ensure a morebalanced representation of language than is availablein commonly annotated WSJ and other newswiretext.
Each extracted sentence will be automaticallyparsed, and the sentences organized according to thegrammatical relation involving the target verb.
Sen-tences will be excluded from the set if the target ar-gument is expressed as anaphor, or is not present inthe sentence.
Semantic head for the target grammat-ical relation will be identified in each case.4.2 Annotation PhaseWord sense disambiguation will need to be per-formed as a preliminary stage for the annotation ofcompositional operations.
The annotation task isthus divided into two subtasks, presented succes-sively to the annotator:(1) Word sense disambiguation of the target predi-cate(2) Identification of the compositional relationshipbetween target predicate and its argumentsIn the first subtask, the annotator is presented witha set of sentences containing the target verb and thechosen grammatical relation.
The annotator is askedto select the most fitting sense of the target verb, orto throw out the example (pick the ?N/A?
option) ifno sense can be chosen either due to insufficient con-text, because the appropriate sense does not appearin the inventory, or simply no disambiguation can bemade in good faith.
The interface is shown in Fig.3.
After this step is complete, the appropriate senseis saved into the database, along with the associatedtype template.In the second subtask, the annotator is presentedwith a list of sentences in which the target verbis used in the same sense.
The data is annotatedone grammatical relation at a time.
The annotatoris asked to determine whether the argument in thespecified grammatical relation to the target belongsto the type associated with that sense in the corre-sponding template.
The illustration of this can beseen in Fig.
4.
We will perform double annotationand subsequent adjudication at each of the above an-notation stages.5 Data FormatThe test and training data will be provided in XMLformat.
The relation between the predicate (viewedas function) and its argument will be represented bya composition link (CompLink) as shown below.In case of coercion, there is a mismatch between thesource and the target types, and both types need tobe identified:The State Department repeatedly denied the attack.The State Department repeatedly<SELECTOR sid="s1">denied</SELECTOR>the<NOUN nid="n1">attack</NOUN> .<CompLink cid="cid1" sID="s1"relatedToNoun="n1" gramRel="dobj"compType="COERCION"sourceType="EVENT"targetType="PROPOSITION"/>When the compositional operation is selection, thesource and the target types must match:The State Department repeatedly denied this statement.The State Department repeatedly<SELECTOR sid="s1">denied</SELECTOR>this<NOUN nid="n1">statement</NOUN> .<CompLink cid="cid1" sID="s1"relatedToNoun="n1" gramRel="dobj"compType="selection"sourceType="PROPOSITION"targetType="PROPOSITION"/>6 Evaluation MethodologyPrecision and recall will be used as evaluation met-rics.
A scoring program will be supplied for partic-ipants.
Two subtasks will be evaluated separately:91Figure 3: Predicate Sense Disambiguation for deny.
(1) identifying the compositional operation (i.e.
se-lection vs. coercion) and (2) identifying the sourceand target argument type, for each relevant argu-ment.
Both subtasks require sense disambiguationwhich will not be evaluated separately.Since type-shifting is by its nature a relativelyrare event, the distribution between different typesof compositional operations in the data set will benecessarily skewed.
One of the standard samplingmethods for handling class imbalance is downsiz-ing (Japkowicz, 2000; Monard and Batista, 2002),where the number of instances of the major class inthe training set is artificially reduced.
Another possi-ble alternative is to assign higher error costs to mis-classification of minor class instances (Chawla et al,2004; Domingos, 1999).7 ConclusionIn this paper, we have described the Argument Se-lection and Coercion task for SemEval-2, to be heldin 2010.
This task involves the identifying the rela-tion between a predicate and its argument as one thatencodes the compositional history of the selectionprocess.
This allows us to distinguish surface formsthat directly satisfy the selectional (type) require-ments of a predicate from those that are coerced incontext.
We described some details of a specifica-tion language for selection and the annotation taskusing this specification to identify argument selec-tion behavior.
Finally, we discussed data preparationfor the task and evaluation techniques for analyzingthe results.ReferencesBNC.
2000.
The British National Corpus.The BNC Consortium, University of Oxford,http://www.natcorp.ox.ac.uk/.Aljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado, and Manfred Pinkal.
2006.The salsa corpus: a german corpus resource for lexicalsemantics.
In Proceedings of LREC, Genoa, Italy.N.
Chawla, N. Japkowicz, and A. Kotcz.
2004.
Editorial:special issue on learning from imbalanced data sets.ACM SIGKDD Explorations Newsletter, 6(1):1?6.P.
Domingos.
1999.
Metacost: A general method formaking classifiers cost-sensitive.
In Proceedings ofthe fifth ACM SIGKDD international conference onKnowledge discovery and data mining, pages 155?164.
ACM New York, NY, USA.Marcus Egg.
2005.
Flexible semantics for reinterpreta-tion phenomena.
CSLI, Stanford.P.
Hanks and J. Pustejovsky.
2005.
A pattern dictionaryfor natural language processing.
Revue Franc?aise deLinguistique Applique?e.P.
Hanks.
2009.
Corpus pattern analysis.
CPAProject Page.
Retrieved April 11, 2009, fromhttp://nlp.fi.muni.cz/projekty/cpa/.J.
R. Hobbs, M. Stickel, and P. Martin.
1993.
Interpreta-tion as abduction.
Artificial Intelligence, 63:69?142.N.
Ide and K. Suderman.
2004.
The American NationalCorpus first release.
In Proceedings of LREC 2004,pages 1681?1684.92Figure 4: Identifying Compositional Relationship for deny.N.
Japkowicz.
2000.
Learning from imbalanced datasets: a comparison of various strategies.
In AAAIworkshop on learning from imbalanced data sets,pages 00?05.A.
Kilgarriff, P. Rychly, P. Smrz, and D. Tugwell.
2004.The Sketch Engine.
Proceedings of Euralex, Lorient,France, pages 105?116.Karin Kipper.
2005.
VerbNet: A broad-coverage, com-prehensive verb lexicon.
Phd dissertation, Universityof Pennsylvania, PA.K.
Markert and M. Nissim.
2007.
Metonymy resolutionat SemEval I: Guidelines for participants.
In Proceed-ings of the ACL 2007 Conference.A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
The Nom-Bank project: An interim report.
In HLT-NAACL 2004Workshop: Frontiers in Corpus Annotation, pages 24?31.E.
Miltsakaki, R. Prasad, A. Joshi, and B. Webber.
2004.The Penn Discourse Treebank.
In Proceedings of the4th International Conference on Language Resourcesand Evaluation.M.C.
Monard and G.E.
Batista.
2002.
Learning withskewed class distributions.
Advances in logic, artifi-cial intelligence and robotics (LAPTEC?02).Geoffrey Nunberg.
1979.
The non-uniqueness of seman-tic solutions: Polysemy.
Linguistics and Philosophy,3:143?184.Kyoko Hirose Ohara.
2008.
Lexicon, grammar, and mul-tilinguality in the japanese framenet.
In Proceedingsof LREC, Marrakech, Marocco.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007.Semeval-2007 task-17: English lexical sample, srl andall words.
In Proceedings of the Fourth InternationalWorkshop on Semantic Evaluations (SemEval-2007),pages 87?92, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.J.
Pustejovsky, P. Hanks, and A. Rumshisky.
2004.
Au-tomated Induction of Sense in Context.
In COLING2004, Geneva, Switzerland, pages 924?931.J.
Pustejovsky, R. Knippen, J. Littman, and R. Sauri.2005.
Temporal and event information in naturallanguage text.
Language Resources and Evaluation,39(2):123?164.J.
Pustejovsky, A. Rumshisky, J. Moszkowicz, andO.
Batiukova.
2009.
GLML: Annotating argumentselection and coercion.
IWCS-8: Eighth InternationalConference on Computational Semantics.J.
Pustejovsky.
1991.
The generative lexicon.
Computa-tional Linguistics, 17(4).A.
Rumshisky and O. Batiukova.
2008.
Polysemy inverbs: systematic relations between senses and theireffect on annotation.
In COLING Workshop on Hu-man Judgement in Computational Linguistics (HJCL-2008), Manchester, England.A.
Rumshisky, P. Hanks, C. Havasi, and J. Pustejovsky.2006.
Constructing a corpus-based ontology usingmodel bias.
In The 19th International FLAIRS Confer-ence, FLAIRS 2006, Melbourne Beach, Florida, USA.J.
Ruppenhofer, M. Ellsworth, M. Petruck, C. Johnson,and J. Scheffczyk.
2006.
FrameNet II: Extended The-ory and Practice.Carlos Subirats.
2004.
FrameNet Espan?ol.
Una redsema?ntica de marcos conceptuales.
In VI InternationalCongress of Hispanic Linguistics, Leipzig.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39(2):165?210.93
