Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671?681,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiscovering User Interactions in Ideological DiscussionsArjun Mukherjee     Bing LiuDepartment of Computer ScienceUniversity of Illinois at Chicagoarjun4787@gmail.com  liub@cs.uic.eduAbstractOnline discussion forums are a popularplatform for people to voice their opinions onany subject matter and to discuss or debateany issue of interest.
In forums where usersdiscuss social, political, or religious issues,there are often heated debates among users orparticipants.
Existing research has studiedmining of user stances or camps on certainissues, opposing perspectives, and contentionpoints.
In this paper, we focus on identifyingthe nature of interactions among user pairs.The central questions are: How does eachpair of users interact with each other?
Doesthe pair of users mostly agree or disagree?What is the lexicon that people often use toexpress agreement and disagreement?
Wepresent a topic model based approach toanswer these questions.
Since agreement anddisagreement expressions are usually multi-word phrases, we propose to employ aranking method to identify highly relevantphrases prior to topic modeling.
Aftermodeling, we use the modeling results toclassify the nature of interaction of each userpair.
Our evaluation results using real-lifediscussion/debate posts demonstrate theeffectiveness of the proposed techniques.1 IntroductionOnline discussion/debate forums allow peoplewith common interests to freely ask and answerquestions, to express their views and opinions onany subject matter, and to discuss issues ofcommon interest.
A large part of suchdiscussions is about social, political, andreligious issues.
On such issues, there are oftenheated discussions/debates, i.e., people agree ordisagree and argue with one another.
Suchideological discussions on a myriad of social andpolitical issues have practical implications in thefields of communication and political science asthey give social scientists an opportunity to studyreal-life discussions/debates of almost any issueand analyze participant behaviors in a large scale.In this paper, we present such an application,which aims to perform fine-grained analysis ofuser-interactions in online discussions.There have been some related works that focuson discovering the general topics and ideologicalperspectives in online discussions (Ahmed andXing, 2010), placing users in support/opposecamps (Agarwal et al, 2003), and classifyinguser stances (Somasundaran and Wiebe, 2009).However, these works are at a rather coarserlevel and have not considered more fine-grainedcharacteristics of debates/discussions where usersinteract with each other by quoting/replying eachother to express agreement or disagreement andargue with one another.
In this work, we want tomine the following information:1.
The nature of interaction of each pair of usersor participants who have engaged in thediscussion of certain issues, i.e., whether thetwo persons mostly agree or disagree witheach other in their interactions.2.
What language expressions are often used toexpress agreement (e.g., ?I agree?
and ?you?reright?)
and disagreement (e.g., ?I disagree?and ?you speak nonsense?
).We note that although agreement anddisagreement expressions are distinct fromtraditional sentiment expressions (words andphrases) such as good, excellent, bad, andhorrible, agreement and disagreement clearlyexpress a kind of sentiment as well.
They areusually emitted during interactive exchanges ofarguments in ideological discussions.
This ideaprompted us to introduce the concept of AD-sentiment.
We define the polarity of agreementexpressions as positive and the polarity ofdisagreement expressions as negative.
We referagreement and disagreement expressions as AD-sentiment expressions, or AD-expressions forshort.
AD-expressions are crucial for the analysisof interactive discussions and debates just assentiment expressions are instrumental insentiment analysis (Liu, 2012).
We thus regardthis work as an extension to traditional sentiment671analysis (Pang and Lee, 2008; Liu, 2012).In our earlier work (Mukherjee and Liu,2012a), we proposed three topic models to minecontention points, which also extract AD-expressions.
In this paper, we further improve thework by coupling an information retrievalmethod to rank good candidate phrases with topicmodeling in order to discover more accurate AD-expressions.
Furthermore, we apply the resultingAD-expressions to the new task of classifying thearguing or interaction nature of each pair ofusers.
Using discovered AD-expressions forclassification has an important advantage overtraditional classification because they are domainindependent.
We employ a semi-supervisedgenerative model called JTE-P to jointly modelAD-expressions, pair interactions, and discussiontopics simultaneously in a single framework.With such complex interactions mined, we canproduce many useful summaries of discussions.For example, we can discover the mostcontentious pairs for each topic and ideologicalcamps of participants, i.e., people who oftenagree with each other are likely to belong to thesame camp.
The proposed framework alsofacilitates tracking users?
ideology shifts and theresulting arguing nature.The proposed methods have been evaluatedboth qualitatively and quantitatively using a largenumber of real-life discussion/debate posts fromfour domains.
Experimental results show that theproposed model is highly effective in performingits tasks and outperforms several baselines.2 Related WorkThere are several research areas that are relatedto our work.
We compare with them below.Sentiment analysis: Sentiment analysisdetermines positive and negative opinionsexpressed on entities and aspects (Hu and Liu,2004).
Main tasks include aspect extraction (Huand Liu, 2004; Popescu and Etzioni, 2005),polarity identification (Hassan and Radev, 2010;Choi and Cardie, 2010) and subjectivity analysis(Wiebe, 2000).
As discussed earlier, agreementand disagreement are a special form ofsentiments and are different from the sentimentstudied in the mainstream research.
Traditionalsentiment is mainly expressed with sentimentterms (e.g., great and bad), while agreement anddisagreement are inferred by AD-expressions(e.g., I agree and I disagree), which we also callAD-sentiment expressions.
Thus, this workexpands the sentiment analysis research.Topic models: Our work is also related to topicmodeling and joint modeling of topics and otherinformation as we jointly model several aspectsof discussions/debates.Topic models like pLSA (Hofmann, 1999) andLDA (Blei et al, 2003) have proved to be verysuccessful in mining topics from large textcollections.
There have been various extensionsto multi-grain (Titov and McDonald, 2008),labeled (Ramage et al, 2009), and sequential (Duet al, 2010) topic models.
Yet other approachesextend topic models to produce author specifictopics (Rosen-Zvi et al, 2004), author persona(Mimno and McCallum, 2007), social roles(McCallum et al, 2007), etc.
However, thesemodels do not model debates and hence areunable to discover AD-expressions andinteraction natures of author pairs.Also related are topic models in sentimentanalysis which are often referred to as Aspectand Sentiment models (ASMs).
ASMs come intwo main flavors: Type-1 ASMs discover aspect(or topic) words sentiment-wise (i.e., discoveringpositive and negative topic words and sentimentsfor each topic without separating topic andsentiment terms) (e.g., Lin and He, 2009; Brodyand Elhadad, 2010, Jo and Oh, 2011).
Type-2ASMs separately discover both aspects andsentiments (e.g., Mei et al, 2007; Zhao et al,2010).
Recently, domain knowledge inducedASMs have also been proposed (Mukherjee andLiu, 2012b; Chen et al, 2013).
The generativeprocess of ASMs is, however, different from ourmodel.
Specifically, Type-1 ASMs useasymmetric hyper-parameters for aspects whileType-2 assumes that sentiments and aspects areemitted in the same sentence.
However, AD-expressions are emitted differently.
They aremostly interleaved with users?
topical viewpointsand span different sentences.
Further, we capturethe key characteristic of discussions by encodingpair-wise user interactions.
Existing models donot model pair interactions.In terms of discussions and comments, Yanoet al, (2009) proposed the CommentLDA modelwhich builds on the work of LinkLDA (Eroshevaet al, 2004).
Mukherjee and Liu (2012d) minedcomment expressions.
These works, however,don?t model pair interactions in debates.Support/oppose camp classification: Severalworks have attempted to put debate authors intosupport/oppose camps.
Agrawal et al (2003)used a graph based method.
Murakami andRaymond (2010) used a rule-based method.
In(Galley et al, 2004; Hillard et al, 2003), speaker672utterances were classified into agreement,disagreement and backchannel classes.Stances in online debates: Somasundaran andWiebe (2009), Thomas et al (2006), Bansal et al(2008), Burfoot et al (2011), and Anand et al(2011) proposed methods to recognize stances inonline debates.
Some other research directionsinclude subgroup detection (Abu-Jbara et al,2012), tolerance analysis (Mukherjee et al,2013), mining opposing perspectives (Lin andHauptmann, 2006), linguistic accommodation(Mukherjee and Liu, 2012c), and contentionpoint mining (Mukherjee and Liu, 2012a).
Forthis work, we adopt the JTE-P model in(Mukherjee and Liu, 2012a), and make twomajor advances.
We propose a new method toimprove the AD-expression mining  and a newtask of classifying pair interaction nature todetermine whether each pair of users who haveinteracted based on replying relations mostlyagree or disagree with each other.3 ModelWe now introduce the JTE-P model withadditional details.
JTE-P is a semi-supervisedgenerative model motivated by the jointoccurrence of expression types (agreement anddisagreement), topics in discussion posts, anduser pairwise interactions.
Before proceeding, wemake the following observation about onlinediscussions.In a typical debate/discussion post, the user(author) mentions a few topics (usingsemantically related topical terms) and expressessome viewpoints with one or more AD-expression types (using agreement anddisagreement expressions).
AD-expressions aredirected towards other user(s), which we calltarget(s).
In this work, we focus on explicitmentions (i.e., using @name or quoting otherauthors?
posts).
In our crawled dataset, 77% ofall posts exhibit explicit quoting/reply-torelations excluding the first posts of threadswhich start the discussions and usually havenobody to quote/reply-to.
Such author-targetexchanges usually go back and forth betweenpairs of users populating a thread of discussion.The discussion topics and AD-expressionsemitted are thus caused by the author-pairs?topical interests and their nature of interaction(agreeing vs. disagreeing).In our discussion data obtained fromVolconvo.com, we found that a pair of userstypically exhibited a dominant arguing nature(agreeing vs. disagreeing) towards each otheracross various topics or threads.
We believe thisis because our data consists of topics likeelections, theism, terrorism, vegetarianism, etc.which are often heated and attract people withpre-determined, strong, and polarized stances1.This observation motivates the generativeprocess of our model.
Referring to the notationsin Table 1, we explain the generative process ofJTE-P.
Given a document (post) ?, its author, ?
?,and the list of targets to whom ??
replies/quotes1 These hardened perspectives are supported by theoreticalstudies in communications like the polarization effect(Sunstein, 2002), and the hostile media effect, a scenariowhere partisans rigidly hold on to their stances (Hansen andHyunjung, 2011).Figure 1: JTE-P Model in plate notation.Variable/Function Description?
; ?
?A document (post) ?
; author ?
ofdocument, ???
= [?1?
??
]List of targets to whom ?
?replies/quotes in d.?
= (?, ??
)Pair of two authors interacting byreply/quote.???;???(??,???
,??,??????
)Pair ?
?s distribution over topics ;expression types (Agreement: ??,???
,Disagreement: ??,??????
)???
;  ???{??,?????}?
Topic ?
?s ; Expression type ?
?sdistribution over vocabulary terms?;?
Total number  of topics; expression types?;?
Total number of vocabulary terms; pairs??,?
; ??
???
term in ?
;  Total # of terms in ??
?,?
Distribution over topics and AD-expressions?
?,?Associated feature context of theobserved term ??,??
Learned Max-Ent parameters??,?
?
{??
?, ???
}Binary indicator/switch variable ( topic(???)
or AD-expression (???)
) for ??,???,?
Topic/Expression type of ??,???
; ??
; ??
; ??
Dirichlet priors of ???
;  ???
;???
;  ?????,???
; ??,??
?# of times topic ?
; expression type ?assigned to ???,???
; ??,??
?# of times term ?
appears in topic ?
;expression type ?Table 1: List of Notations?TT?EE?E?T?E?E?TP?TzrwcpDNd?x?wadbd673in ?
, ??
= [?1?
??]
, the document ?
exhibitsshared topics and arguing nature of various pairs,?
= (??
, ?)
, where ?
?
??
.
More precisely, thepair specific topic and AD-expressiondistributions (???
; ??? )
?shape?
the topics andAD-expressions emitted in ?
as agreement anddisagreement on topical viewpoints are directedtowards certain target authors.
Each topic (???
)and AD-expression type (???)
is characterized bya multinomial distribution over terms(words/phrases).
Assume we have ?
= 1 ?
?topics and ?
= 1 ??
expression types in ourcorpus.
Note that in our case of discussion/debateforums, we hypothesize ?
= 2 as in debates, wemostly find two expression types: agreement anddisagreement (more details in ?6.1).
Like mostgenerative models for text, a post (document) isviewed as a bag of n-grams and each n-gram(word/phrase) takes one value from a predefinedvocabulary.
In this work, we use up to 4-grams,i.e., n = 1, 2, 3, 4.
Instead of using all n-grams, arelevance based ranking method is proposed toselect a subset of highly relevant n-grams formodel building (details in ?4).
For notationalconvenience, we use terms to denote both words(unigrams) and phrases (n-grams).JTE-P is a switching graphical model (Ahmedand Xing, 2010; Zhao et al, 2010) performing aswitch between AD-expressions and topics.
?
?,?denotes the distribution over topics and AD-expressions with ??,?
?
{??
?, ???}
denoting the binaryindicator/switch variable (topic or AD-expression) for the ?
th term of ?
, ??,?
.
Toperform the switch we use a maximum entropy(Max-Ent) model.
The idea is motivated by theobservation that topical and AD-expression termsusually play different roles in a sentence.
Topicalterms (e.g., ?elections?
and ?income tax?)
tend tobe noun and noun phrases while AD-expressionterms (?I refute?, ?how can you say?, and?probably agree?)
usually contain pronouns,verbs, wh-determiners, and modals.
In order toutilize the part-of-speech (POS) tag information,we place the topic/AD-expression distribution??,?
(the prior over the indicator variable ??,?)
inthe term plate (see Figure 1) and set it from aMax-Ent model conditioned on the observedfeature context ??,?
associated with ??,?
and thelearned Max-Ent parameters, ?
(details in ?6.1).In this work, we use both lexical and POSfeatures of the previous, current, and next POStags/lexemes of the term ??,?
as the contextualinformation, i.e., ??,?
= [?????,?
?1 , ?????,?
,????
?,?+1 , ??,??1,??,?
, ?
?,?+1], which is used toproduce the feature functions for Max-Ent.
Forphrasal terms (n-grams), all POS tags andlexemes of ??,?
are considered as contextualinformation for computing feature functions inMax-Ent.
We now detail the generative processof JTE-P (plate notation in Figure 1) as follows:1.
For each AD-expression type ?, draw ???~???(??)2.
For each topic ?, draw ???~???(??)3.
For each pair ?, draw ???~???(??
); ???~???(??)4.
For each forum discussion post ?
?
{1 ??}:i.
Given the author ??
and the list of targets ??
, foreach term ?
?,?, ?
?
{1 ???}:a.
Draw a target ?~???(??)b.
Form pair ?
= (??
, ?
), ?
?
??c.
Set ??,?
?
??????(??,?
; ?)d.
Draw ??,?~????(??,?)e.
if (??,?
= ???)
// ??,?
is an AD-expression termDraw ??,?~????(???
)else // ??,?
= ??
?, ??,?
is a topical termDraw ??,?~????(???)f.
Emit ??,?~????(???,???,?)???
, ????
, ????
, and ???
correspond to theDirichlet, Multinomial, Bernoulli, and Uniformdistributions respectively.
To learn JTE-P, weemploy approximate posterior inference usingMonte Carlo Gibbs sampling.
Denoting therandom variables {?, ?,?, ?}
associated with eachterm by singular subscripts {?
?, ??,?
?, ??
}, ?1??
,?
= ?
???
, a single Gibbs sweep consists ofperforming the following sampling.?(??
= ?,??
= ?, ??
= ??
?| ? )
?1|??|?????
???????,?,?????
?=1 ??
?????
???????,?,???
?=1 ???{??,??}???,?????+????,(?)????+?????,?????+????,(?)????+???(1)?(??
= ?,??
= ?, ??
= ??
?| ? )
?1|??|?????
???????,?,?????
?=1 ??
?????
???????,?,???
?=1 ???{??,??}???,?????+????,(?)????+?????,?????+????,(?)????+???
(2)Count variables ??,???
, ??,???
, ??,???
, and ??,???
aredetailed in Table 1.
Omission of a latter indexdenoted by (?)
represents the marginalized sumover the latter index.
?
= (?, ?)
denotes the ?
thterm of document ?
and the subscript ??
denotesthe counts excluding the term at (?, ?).
?1??
arethe parameters of the learned Max-Ent modelcorresponding to the ?
binary feature functions?1??
for Max-Ent.
These learned Max-Ent ?parameters in conjunction with the observedfeature context, ??,?
feed the supervision signalfor topic/expression switch parameter, r which isupdated during inference in equations (1) and (2).6744 Phrase Ranking based on RelevanceWe now detail our method of pre-processing n-grams (phrases) based on relevance to select asubset of highly relevant n-grams for modelbuilding.
This has two advantages: (i).
A largenumber of irrelevant n-grams slow inference.
(ii).Filtering irrelevant terms in the vocabularyimproves the quality of AD-expressions.
Beforeproceeding, we review some existing approaches.Topics in most topic models like LDA areusually unigram distributions.
This offers a greatcomputational advantage compared to morecomplex models which consider word ordering(Wallach, 2006; Wang et al, 2007).
This threadof research models bigrams by encoding theminto the generative process.
For each word, atopic is sampled first, then its status as a unigramor bigram is sampled, and finally the word issampled from a topic-specific unigram or bigramdistribution.
This method, however, is expensivecomputationally and has a limitation for arbitrarylength n-grams.
In (Tomokiyo and Hurst, 2003),a language model approach is used for bigramphrase extraction.Yet another thread of research post-processesthe discovered topical unigrams to form multi-word phrases using likelihood scores (Blei andLafferty, 2009).
This approach considers adjacentword pairs and identifies n-grams which occurmuch more often than one would expect bychance alone by computing likelihood ratios.While this is reasonable, a significant n-gramwith high likelihood score may not necessarily berelevant to the problem domain.
For instance, inour case of discovering AD-expressions, thelikelihood score 2  of ?1  = ?the government of?happens to be more than ?2  = ?I completelydisagree?.
Clearly, the former is irrelevant for thetask of discovering AD-expressions.
The reasonfor this is that likelihood scores or otherstatistical test scores rely on the relative counts inthe multi-way contingency table to computesignificance.
Since the relative counts of differentfragments of the irrelevant phrase ?1 , e.g.
?thegovernment?, and ?government of?, happen toappear more than the corresponding counts in thecontingency table of ?2, the tests assign a higherscore.
This is nothing wrong per se because thestatistical tests only judge significance of an n-gram, but a significant n-gram may notnecessarily be relevant in a given problemdomain.2 Computed using N-gram statistics package, NSP; http://n-gram.sourceforge.netThus, the existing approaches have somemajor shortcomings for our task.
As our goal isto enhance the expressiveness of our models byconsidering relevant n-grams preserving theadvantages of exchangeable modeling, weemploy a pre-processing technique to rank n-grams based on relevance and consider certainnumber of top ranked n-grams based on coverage(details follow) in our vocabulary.
The ideaworks as follows.We first induce a unigram JTE-P whereby wecluster the relevant AD-expression unigrams in????
and ???????
.
Our notion of relevance of AD-expressions is already encoded into the modelusing priors set from Max-Ent.
Next, we rank thecandidate phrases (n-grams) using ourprobabilistic ranking function.
The rankingfunction is grounded on the followinghypothesis: a relevant phrase is one whoseunigrams are closely related to (or appear withhigh probabilities in) the given AD-expressiontype, ?
: Agreement ( ?? )
or disagreement(?????).
Continuing from the previous example,given the expression type ??=??????
, ?2 is relevantwhile ?1 is not as ?government?
and ?disagree?are highly unlikely and likely respectively to beclustered in ??=??????
.
Thus, we want to rankphrases based on ?(???
= 1|?,?)
where ?
denotesthe expression type (Agreement/Disagreement),?
denotes a candidate phrase.
Following theprobabilistic relevance model in (Lafferty andZhai, 2003), we use a similar technique to that in(Zhao et al, 2011) for deriving our relevanceranking function as follows:?(???
= 1|?,?)
=?(???=1|?,?)?(???=0|?,?)+?(???=1|?,?)=11+?(???=0|?,?)?(???=1|?,?)=11+?(??
?=0,?| ?)?(???=1,?|?)=11+[?(?|???=0,?)??(???=0|?)][?(?|???=1,?)??(???=1|?
)](3)We further define ?
= ?(???=0|?)?(???=1|?).
Without loss ofgenerality, one can say that ?(???
= 0|?)
??(???
= 1|?)
, because there are many moreirrelevant phrases than relevant ones, i.e., ?
?
1.Thus, taking log, from equation (3), we get,log?(???
= 1|?,?)
= log?11+???(?|???=0,?)?(?|???=1,?)?
?log ??(?|???=1,?)?(?|???=0,?)?1??
= log ??(?|???=1,?)?(?|???=0,?)?
?
log ?
(4)Thus, our ranking function actually computes therelevance score log ??(?|???=1,?)?(?|???=0,?)?
.
The last term,log ?
being a constant is ignored because itcancels out while comparing candidate n-grams.675We now estimate the relevance score of a phrase?
= (?1,?2, ?
,??).
Using the conditionalindependence assumption of words given theindicator variable ???
and expression type ?, wehave:log ??(?|???=1,?)?(?|???=0,?)?
= ?
log?(??|???=1,?)?(??|???=0,?)?
?=1              (5)Given the expression model ???
previouslylearned by inducing the unigram JTE-P, it isintuitive to set ?(??|???
= 1, ?)
to the pointestimate of the posterior on ??,???
=??,????
+????,(?)??
+??
?,where ??,????
is the number of times ??
wasassigned to AD-expression type ?
and ??,(?)?
?denotes the marginalized sum over the latterindex.
On the other hand, ?(??|???
= 0, ?)
can beestimated using a Laplace smoothed ( ?
= 1)background model, i.e., (??|???
= 0, ?)
=???+???+?
?,where ???
denotes the number of times ?
?appears in the whole corpus and ??
denotes thenumber of terms in the entire corpus.Next, we throw light on the issue of choosingthe number of top k phrases from the rankedcandidate n-grams.
Precisely, we want to analyzethe coverage of our proposed ranking based onrelevance models.
By coverage, we mean thathaving selected top k candidate n-grams based onthe proposed relevance ranking, we want to getan estimate of how many relevant terms from asample of the collection were covered.
Tocompute coverage, we randomly sampled 500documents from the corpus and listed thecandidate n-grams3 in the collection of sampled500 documents.
For this and subsequent humanjudgment tasks, we use two judges (graduatestudents well versed in English).
We asked ourjudges to mark all relevant AD-expressions.Agreement study yielded ?Cohen = 0.77 showingsubstantial agreement according to scale 4provided in (Landis and Koch, 1977).
This isunderstandable as identifying AD-expressions isa relatively easy task.
Finally, a term wasconsidered to be relevant if both judges marked itso.
We then computed the coverage to see howmany of the relevant terms in the random samplewere also present in top k phrases from theranked candidate n-grams.
We summarize the3 These are terms appearing at least 20 times in the entirecollection.
We do this for computational reasons as therecan be many n-grams and n-grams with very low frequencyare less likely to be relevant.4 No agreement (?
< 0), slight agreement (0 < ?
?
0.2), fairagreement (0.2 < ?
?
0.4), moderate agreement (0.4 < ?
?0.6), substantial agreement (0.6 < ?
?
0.8), and almostperfect agreement 0.8 < ?
?
1.0.coverage results below in Table 2.k 3000 4000 5000JTE-PAgreement 81.34 84.24 87.01Disagreement 84.96 87.86 89.64Table 2: Coverage (in %) of AD-expressions.We find that choosing top k = 5000 candidate n-grams based on our proposed ranking, we obtaina coverage of 87% for agreement and 89.64 fordisagreement expression types which arereasonably good.
Thus, we choose top 5000candidate n-grams for each expression type andadd them to the vocabulary beyond all unigrams.Like expression types ?1??
, we also rankedcandidate phrases for topics ?1??
using?(???
= 1|?,?).
However, for topics, selecting kbased on coverage of each topic is more difficultbecause we induce 50 topics and it is also muchmore difficult to manually find relevant topicalphrases in the sampled data as a topical phrasemay belong to more than one topic.
We selectedtop 2000 ranked candidate phrases for each topicusing ?(???
= 1|?,?)
as we feel that is sufficientfor a topic.
Note that phrases for topics are not ascrucial as for AD-expressions because topics canmore or less be defined by unigrams.5 Classifying Pair Interaction NatureWe now determine whether two users (alsocalled a user pair) mostly agree or disagree witheach other in their exchanges, i.e., their pairinteraction or arguing nature.
This is a relativelynew task.
We first summarize the closest relatedworks.
In (Galley et al, 2004; Hillard et al,2003; Thomas et al, 2006, Bansal et al, 2008),conversational speeches (i.e., U.S. Congressmeeting transcripts) are classified into for oragainst an issue using various types of features:durational (e.g., time taken by a speaker; speechrate, etc.
), structural (e.g., no.
of speakers perside, no.
of votes cast by a speaker on a bill, etc.
),and lexical (e.g., first word, last word, n-grams,etc.).
Burfoot et al, (2011) builds on the work of(Thomas et al, 2006) and proposes collectiveclassification using speaker contextual features(e.g., speaker intentions based on vote labels).However, above works do not discover pairinteractions (arguing nature) in debate authors.Online discussion forums are textual rather thanconversational (e.g., U.S. Congress meetingtranscripts).
Thus, the durational, structural, andcontextual features used in prior works are notdirectly applicable.Instead, the model posterior on ???
for JTE-P676can actually give an estimate of the overallinteraction nature of a pair, i.e., the probabilitymasses assigned to expression types, ?
=??
(Agreement) and ?
= ?????
(Disagreement).As ???~???(??
), we have ??,?=???
+ ??,?=??????
= 1.Hence, if the probability mass assigned to anyone of the expression types (agreement,disagreement) > 0.5 then according to the modelposterior, that expression type is dominant, i.e., if??,???
> 0.5, the pair is agreeing else disagreeing.However, this approach is not the best.
As wewill see in the experiment section, supervisedclassification using labeled training data withdiscovered AD-expressions as features performsbetter.6 Empirical EvaluationWe now evaluate the proposed techniques in thecontext of the JTE-P model.
We first evaluate thediscovered AD-expressions by comparing resultswith and without using the phrase rankingmethod in Section 4, and then evaluate theclassification of interaction nature of pairs.6.1 Dataset and Experiment SettingsWe crawled debate/discussion forum posts fromVolconvo.com.
The forum is divided into variousdomains.
Each domain consists of multiplethreads of discussions.
For each post, weextracted the post id, author, domain, ids of allposts to which it replies/quotes, and the postcontent.
In all, we extracted 26137, 34986,22354, and 16525 posts from Politics, Religion,Society and Science domains respectively.Experiment Data: As it is not interesting tostudy pairs who only exchanged a few posts, werestrict to pairs with at least 20 post exchanges.This resulted in 1241 authors and 1461 pairs.
Thereduced dataset consists of 1095586 tokens (aftern-gram preprocessing in ?4), 40102 posts with anaverage of 27 posts or interactions per pair.
Datafrom all 4 domains are combined for modeling.Parameter Settings: For all our experiments, weset the hyper-parameters to the heuristic values??
= 50/?, ??
= 50/?, ??
= ??
= 0.1 suggestedin (Griffiths and Steyvers, 2004).
We set thenumber of topics, ?
= 50 and the number of AD-expression types, ?
= 2 (agreement anddisagreement) as in discussion/debate forums,there are usually two expression types5.
To learn5 Values for ?
> 2 were also tried.
However, they did notproduce any new dominant expression type.
There was alsoa slight increase in the model perplexity showing that valuesof ?
> 2 do not fit the debate forum data well.the Max-Ent parameters ?, we randomly sampled500 terms from the held-out data (10 threads inour corpus which were excluded from theevaluation of tasks in ?6.2, ?6.3) appearing atleast 10 times and labeled them as topical (361)or AD-expressions (139) and used thecorresponding features of each term (in thecontext of posts where it occurs, ?3) to train theMax-Ent model.6.2 AD-Expression EvaluationWe first list some discovered top AD-expressionsin Table 3 for qualitative inspection.
From Table3, we can see that JTE-P can cluster many correctAD-expressions, e.g., ?I accept?, ?I agree?,?you?re correct?, etc.
in agreement and ?Idisagree?, ?don?t accept?, ?I refute?, etc.
indisagreement.
In addition, it also discovers andclusters highly specific and more ?distinctive?expressions beyond those used in Max-Enttraining, e.g., ?valid point?, ?I do support?, and?rightly said?
in agreement; and phrases like ?canyou prove?, ?I don?t buy your?, and ?you fail to?in disagreement.
Note that terms in black inTable 3 were used in Max-Ent training.
Thenewly discovered terms are marked blue initalics.
Clustering errors are in red (bold).For quantitative evaluation, topic models areoften compared using perplexity.
However,perplexity does not reflect our purpose since weare not trying to evaluate how well the AD-expressions in an unseen discussion data fit ourlearned models.
Instead our focus is to evaluatehow well our learned AD-expression typesperform in clustering semantic phrases ofagreement/disagreement.
Since AD-expressions(according to top terms in ??)
produced by JTE-P are rankings, we choose precision @ n (p@n)as our metric.
p@n is commonly used to evaluatea ranking when the total number of correct itemsis unknown (e.g., Web search results, aspectterms in topic models for sentiment analysis(Zhao et al, 2010), etc.).
This situation is similarto our AD-expression rankings, ??
.
Further, as??~??
?, the Dirichlet smoothing effect ensuresthat every term in the vocabulary has some non-zero mass to agreement or disagreementexpression type.
Thus, it is the ranking of termsin each AD-expression type that matters (i.e.,whether the model is able to rank highly relevantterms at the top).The above method evaluates the originalranking.
Another way of evaluating the AD-expression rankings is to evaluate only thosenewly discovered terms, i.e., beyond those677labeled terms used in Max-Ent training.
For thisevaluation, we remove those terms that havebeen used in Max-Ent (ME) training.
We reportboth results in Table 4.
We also studied inter-rater agreement using two judges whoindependently labeled the top n terms as corrector incorrect.
A term was marked correct if bothjudges deemed it so which was then used tocompute p@n. Agreement using ??????
wasgreater than 0.78 for all p@n computationsimplying substantial and good agreements asidentifying whether a phrase implies agreementor disagreement or none is an easy task.
P@nexcluding ME labeled terms (Table 4, secondcolumn) are slightly lower than those using allterms but are still decent.
This is because p@nexcluding ME labeled terms removes manycorrect AD-expressions used in training.Further to evaluate the sensitivity ofperformance on the amount of labeled terms forMax-Ent, we computed p@n across differentsizes of labeled terms.
Table 4 shows p@n foragreement and disagreement expressions acrossdifferent sizes of labeled terms (L).
We find thatmore labeled terms improves p@n which isintuitive.
We used 500 labeled terms in all oursubsequent experiments.
The result in Table 4uses relevance ranking (?4).Disagreement expressions (??=?????????????
)I, disagree, I don?t, I disagree, argument, reject, claim, I reject, I refute, and, your, I refuse, won?t, the claim,nonsense, I contest, dispute, I think, completely disagree, don?t accept, don?t agree, incorrect, doesn?t, hogwash, Idon?t buy your, I really doubt, your nonsense, true, can you prove, argument fails, you fail to, your assertions,bullshit, sheer nonsense, doesn?t make sense, you have no clue, how can you say, do you even, contradict yourself, ?Agreement expressions (??=??????????
)agree, I, correct, yes, true, accept, I agree, don?t, indeed correct, your, I accept, point, that, I concede, is valid, yourclaim, not really, would agree, might, agree completely, yes indeed, absolutely, you?re correct, valid point,argument, the argument, proves, do accept, support, agree with you, rightly said, personally, well put, I dosupport, personally agree, doesn?t necessarily, exactly, very well put, kudos, point taken, ...Table 3: Top terms (comma delimited) of two expression types.
Red (bold) terms denote possible errors.Blue (italics) terms are newly discovered; rest (black) terms have been used in Max-Ent training.P@nLJTE-P (all terms) JTE-P (excluding labeled ME terms)Agreement Disagreement Agreement Disagreement50 100 150 50 100 150 50 100 150 50 100 150100 0.62 0.63 0.61 0.64 0.62 0.63 0.58 0.56 0.57 0.60 0.59 0.58200 0.66 0.67 0.65 0.68 0.66 0.67 0.62 0.59 0.60 0.64 0.63 0.62300 0.70 0.70 0.71 0.70 0.68 0.67 0.66 0.66 0.65 0.66 0.66 0.65400 0.72 0.72 0.73 0.74 0.71 0.70 0.68 0.67 0.69 0.70 0.68 0.69500 0.76 0.77 0.75 0.76 0.73 0.74 0.70 0.71 0.70 0.72 0.71 0.70Table 4: Results using terms based on phrase relevance ranking for P @ n= 50, 100, 150 across 100, 200,?, 500 labeled examples (L) used for Max-Ent (ME) training.P@nLJTE-P (all terms) JTE-P (excluding ME terms)Agreement Disagreement Agreement Disagreement50 100 150 50 100 150 50 100 150 50 100 150500 0.66 0.69 0.69 0.72 0.70 0.70 0.66 0.65 0.64 0.68 0.66 0.65Table 5: Results using all tokens (without applying phrase relevance ranking) for P@50, 100, 150 and 500labeled examples were used for Max-Ent (ME) training).Feature SettingAgreeing DisagreeingP R F1 P R F1JTE-P-posterior 0.59 0.61 0.60 0.81 0.70 0.75W+POS 1-4 grams 0.63 0.66 0.64 0.83 0.82 0.82W+POS 1-4grams + IG (top 1%) 0.64 0.67 0.65 0.84 0.82 0.83W+POS 1-4 grams + IG (top 2%) 0.65 0.67 0.66 0.84 0.82 0.83W+POS 1-4 grams + ?2 (top 1%) 0.65 0.68 0.66 0.84 0.83 0.83W+POS 1-4 grams + ?2(top 2%) 0.64 0.68 0.69 0.84 0.82 0.83AD-Expressions, ??
(top 1000) 0.73 0.74 0.73 0.87 0.87 0.87AD-Expressions, ??
(top 2000) 0.77 0.81 0.78 0.90 0.88 0.89Table 6: Precision (P), recall (R), and F1 scores of pair interaction evaluation.
Improvements in F1 usingAD-expression features (??)
are statistically significant (p<0.01) using paired t-test across 5-fold CV.678We now compare with the performance of themodel without using phrase relevance ranking.P@n results using all tokens (4356787) areshown in Table 5 (with 500 labeled terms forMax-Ent training).
Clearly, P@n is lower than inTable 4 (last row; with phrase relevance ranking)because without phrase relevance ranking (Table5) many irrelevant terms can rank high due to co-occurrences which may not be semanticallyrelated.
This shows that relevance ranking ofphrases is beneficial.6.3 Pair Interaction NatureWe now evaluate the overall interaction nature ofeach pair of users.
The evaluation of this taskrequires human judges to read all the posts wherethe two users forming the pair have interacted.Thus, it is hard to evaluate all 1461 pairs in ourdataset.
Instead, we randomly sampled 500 pairs(?
34% of the population) for evaluation.
Twohuman judges were asked to independently readall the post interactions of 500 pairs and labeleach pair as overall ?disagreeing?
or overall?agreeing?
or ?none?.
The ??????
for this taskwas 0.81.
Pairs were finally labeled as agreeingor disagreeing if both judges deemed them so.This resulted in 320 disagreeing and 152agreeing pairs.
Out of the rest 28 pairs, 10 weremarked ?none?
by both judges while 18 pairs haddisagreement in labels.
We only focus on the 472agreeing and disagreeing pairs.As we have labeled data for 472 pairs, we cantreat identifying pair arguing nature as a textclassification problem where all interactionsbetween a pair are merged in one documentrepresenting the pair along with the label givenby judges: agreeing or disagreeing.
To compareclassification performance, we use two featuresets: (i) standard word + POS 1-4 grams and (ii)AD-expressions from ??.
We use TF-IDF as ourfeature value assignment scheme.
We also trytwo well-known feature selection schemes Chi-Squared Test (?2) and Information Gain (IG).
Weuse the linear kernel6 SVM (SVMlight system in(Joachims, 1999)) as our text classifier.
Forfeature selection using ?2 and IG, we use twosettings: top 1% and 2% of all features rankedaccording to the selection metric.
Also, forestimated AD-expressions (according toprobabilities in ??
), we experiment with top1000 and 2000 AD-expressions terms for bothagreement and disagreement.
We summarize6  Other kernels polynomial, RBF, and sigmoid did notperform as well.comparison results using 5-fold Cross Validation(CV) with two classes: agreeing and disagreeingin Table 6.
JTE-P-posterior represents themethod using simply the model posterior on ??
?to make the decision (see ?5).
From Table 6, wecan make the following observations.Predicting agreeing arguing nature is harderthan that of disagreeing across all featuresettings.
Feature selection improves performance.
?2 and IG perform similarly.
AD-expressions,?
?yields the best performance showing that thediscovered AD-expressions are of high qualityand reflect the user pair arguing nature well.Selecting certain top terms in ??
can also beviewed as a form of feature selection.
Althoughprediction performance using model posterior(JTE-P-posterior) is slightly lower thansupervised SVM (Table 6, second row), the F1scores are decent.
Using the discovered AD-expressions (Table 6, last low) as featuresrenders a statistically significant (see Table 6caption) improvement over other baseline featuresettings.
This shows that discovered AD-expressions are useful for downstreamapplications, e.g., the task of identifying pairinteractions.7 ConclusionThis paper studied the problem of modeling userpair interactions in online discussions with thepurpose of discovering the interaction or arguingnature of each author pair and various AD-expressions emitted in debates.
A noveltechnique was also proposed to rank n-gramphrases where relevance based ranking was usedin conjunction with a semi-supervised generativemodel.
This method enables us to find better AD-expressions.
Experiments using real-life onlinedebate data showed the effectiveness of themodel.
In our future work, we intend to extendthe model to account for stances, and issuespecific interactions which would pave the wayfor user profiling and behavioral modeling.AcknowledgmentsWe would like thank Sharon Meraz (Departmentof communication, University of Illinois atChicago) and Dennis Chong (Department ofPolitical Science, Northwestern University) forseveral valuable discussions.
This work wassupported in part by a grant from the NationalScience Foundation (NSF) under grant no.
IIS-1111092.679ReferencesAbu-Jbara, A., Dasigi, P., Diab, M. and DragomirRadev.
2012.
Subgroup detection in ideologicaldiscussions.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL-2012).Agrawal, R., Rajagopalan, S., Srikant, R., and Xu.
Y.2003.
Mining newsgroups using networks arisingfrom social behavior.
In Proceedings of theInternational Conference on World Wide Web(WWW-2003).Ahmed, A and Xing, E. 2010.
Staying informed:supervised and semi-supervised multi-view topicalanalysis of ideological perspective.
In Proceedingsof the Empirical Methods in Natural LanguageProcessing (EMNLP-2010).Anand, P., Walker, M., Abbott, R., Tree, J., Bowmani,R., and Minor, M. 2011.
Cats rule and dogs drool!
:Classifying stance in online debate.
In Proceedingsof the 2nd Workshop on Computational Approachesto Subjectivity and Sentiment Analysis.Bansal, M., Cardie, C., and Lee, L. 2008.
The powerof negative thinking: Exploiting label disagreementin the min-cut classification framework.
InProceedings of the International Conference onComputational Linguistics (Short Paper).Blei, D., Ng, A., and Jordan, M. 2003.
LatentDirichlet Allocation.
Journal of Machine LearningResearch.Blei, D. and Lafferty J.
2009.
Visualizing topics withmulti-word expressions.
Tech.
Report.arXiv:0907.1013v1.Brody, S. and Elhadad, S. 2010.
An UnsupervisedAspect-Sentiment Model for Online Reviews.
InProceedings of the Annual Conference of the NorthAmerican Chapter of the ACL (NAACL-2010).Burfoot, C., Bird, S., and Baldwin, T. 2011.
CollectiveClassification of Congressional Floor-DebateTranscripts.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL-2001).Chang, J., Boyd-Graber, J., Wang, C.  Gerrish, S.Blei, D. 2009.
Reading tea leaves: How humansinterpret topic models.
In Proceedings of the NeuralInformation Processing Systems (NIPS-2009).Chen, Z., Mukherjee, A., Liu, B., Hsu, M.,Castellanos, M., Ghosh, R. 2013.
Leveraging Multi-Domain Prior Knowledge in Topic Models.
InProceedings of the International Joint Conference inArtificial Intelligence (IJCAI-2013).Choi, Y. and Cardie, C. 2010.
Hierarchical sequentiallearning for extracting opinions and their attributes(Short Paper).
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL-2010).Du, L., Buntine, W. L., and Jin, H. 2010.
SequentialLatent Dirichlet Allocation: Discover UnderlyingTopic Structures within a Document.
InProceedings of the IEEE International Conferenceon Data Mining (ICDM-2010).Erosheva, E., Fienberg, S. and Lafferty, J.
2004.Mixed membership models of scientificpublications.
In Proceedings of the NationalAcademy of Sciences (PNAS-2004).Galley, M., McKeown, K., Hirschberg, J., andShriberg, E. 2004.
Identifying agreement anddisagreement in conversational speech: Use ofBayesian networks to model pragmaticdependencies.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL-2004).Griffiths, T. and Steyvers, M. 2004.
Finding scientifictopics.
In Proceedings of the National Academy ofSciences (PNAS-2004).Hansen, G. J., and Hyunjung, K. 2011.
Is the mediabiased against me?
A meta-analysis of the hostilemedia effect research.
Communication ResearchReports, 28, 169-179.Hillard, D., Ostendorf, M., and Shriberg, E. 2003.Detection of agreement vs. disagreement inmeetings: Training with unlabeled data.
InProceedings of the Conference of the NorthAmerican Chapter of the Association forComputational Linguistics: Human LanguageTechnologies (NAACL-HLT-2003).Hassan, A. and Radev, D. 2010.
Identifying textpolarity using random walks.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL-2010).Hofmann, T. 1999.
Probabilistic latent semanticanalysis.
In Proceedings of the Conference onUncertainty in Artificial Intelligence (UAI-1999).Hu, M. and Liu, B.
2004.
Mining and summarizingcustomer reviews.
In Proceedings of the SIGKDDInternational Conference on Knowledge Discoveryand Data Mining (KDD-2004).Jo, Y. and Oh, A.
2011.
Aspect and sentimentunification model for online review analysis.
InProceedings of the International Conference onWeb Search and Data Mining (WSDM-2011).Joachims, T. Making large-Scale SVM LearningPractical.
1999.
Advances in Kernel Methods -Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed.
), MIT-Press, 1999.Lafferty, J. and Zhai, C. 2003.
Probabilistic relevancemodels based on document and query generation.Language Modeling and Information Retrieval.Landis, J. R. and Koch, G. G. 1977.
The measurementof observer agreement for categorical data.Biometrics.Lin, C. and He, Y.
2009.
Joint sentiment/topic modelfor sentiment analysis.
In Proceedings of the680International Conference on KnowledgeManagement (CIKM-2009).Lin, W. H., and Hauptmann, A.
2006.
Are thesedocuments written from different perspectives?
: atest of different perspectives based on statisticaldistribution divergence.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL-2006).Liu, B.
2012.
Sentiment Analysis and Opinion Mining.Morgan & Claypool Publisher, USA.McCallum, A., Wang, X., and Corrada-Emmanuel, A.2007.
Topic and Role Discovery in Social Networkswith Experiments on Enron and Academic Email.Journal of Artificial Intelligence Research.Mei, Q., Ling, X., Wondra, M., Su, H., and Zhai, C.2007.
Topic sentiment mixture: modeling facets andopinions in weblogs.
In Proceedings of theInternational Conference on World Wide Web(WWW-2007).Mimno, D. and McCallum, A.
2007.
Expertisemodeling for matching papers with reviewers.
InProceedings of the SIGKDD InternationalConference on Knowledge Discovery and DataMining (KDD-2007).Mukherjee, A., Venkataraman, V., Liu, B., Meraz, S.2013.
Public Dialogue: Analysis of Tolerance inOnline Discussions.
In Proceedings of the AnnualMeeting of the Association for ComputationalLinguistics (ACL-2013).Mukherjee, A. and Liu, B.
2012a.
Mining Contentionsfrom Discussions and Debates.
Proceedings ofSIGKDD Conference on Knowledge Discovery andData Mining (KDD-2012).Mukherjee, A. and Liu, B.
2012b.
Aspect Extractionthrough Semi-Supervised Modeling.
In Proceedingsof the Annual Meeting of the Association forComputational Linguistics (ACL-2012).Mukherjee, A. and Liu, B.
2012c.
Analysis ofLinguistic Style Accommodation in Online Debates.In Proceedings of the International Conference onComputational Linguistics (COLING-2012).Mukherjee, A. and Liu, B.
2012d.
Modeling ReviewComments.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics(ACL-2012).Murakami A. and Raymond, R. 2010.
Support orOppose?
Classifying Positions in Online Debatesfrom Reply Activities and Opinion Expressions.
InProceedings of the International Conference onComputational Linguistics (Coling-2010).Pang, B. and Lee, L. 2008.
Opinion mining andsentiment analysis.
Foundations and Trends inInformation Retrieval.Popescu, A. and Etzioni, O.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2005).Ramage, D., Hall, D., Nallapati, R, Manning, C. 2009.Labeled LDA: A supervised topic model for creditattribution in multi-labeled corpora.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2009).Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smith,P.
2004.
The author-topic model for authors anddocuments.
In Proceedings of the Conference onUncertainty in Artificial Intelligence (UAI-2004).Sunstein, C. R. 2002.
The law of group polarization.Journal of political philosophy.Somasundaran, S. and Wiebe, J.
2009.
Recognizingstances in online debates.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing (ACL-IJCNLP-2009).Titov, I. and R. McDonald.
2008.
Modeling onlinereviews with multi-grain topic models.
InProceedings of the International Conference onWorld Wide Web (WWW-2008).Thomas, M., Pang, B., and Lee, L. 2006.
Get out thevote: Determining support or opposition fromcongressional floor-debate transcripts.
In Proc.
ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2006).Tomokiyo, T., and Hurst, M. 2003.
A language modelapproach to keyphrase extraction.
In Proceedings ofthe ACL 2003 workshop on Multiword expressions:analysis, acquisition and treatment-Volume 18.Wallach, H. 2006.
Topic modeling: Beyond bag ofwords.
In Proceedings of the InternationalConference on Machine Learning (ICML-2006).Wang, X., McCallum, A., Wei, X.
2007.
Topical N-grams: Phrase and topic discovery, with anapplication to information retrieval.
In Proceedingsof the IEEE International Conference on DataMining (ICDM-2007).Wiebe, J.
2000.
Learning subjective adjectives fromcorpora.
In Proc.
of National Conference on AI(AAAI-2000).Yano, T., Cohen, W. and Smith, N. 2009.
Predictingresponse to political blog posts with topic models.In Proceedings of the N. American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies (NAACL-HLT-2009).Zhao, X., J. Jiang, J.
He, Y.
Song, P. Achananuparp,E.P.
LiM, and X. Li.
2011.
Topical keyphraseextraction from twitter.
In Proceedings of theAnnual Meeting of the Association forComputational Linguistics (ACL-2011).Zhao, X., Jiang, J., Yan, H., and Li, X.
2010.
Jointlymodeling aspects and opinions with a MaxEnt-LDA hybrid.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2010).681
