Proceedings of the ACL 2010 Conference Short Papers, pages 1?5,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsParaphrase Lattice for Statistical Machine TranslationTakashi Onishi and Masao Utiyama and Eiichiro SumitaLanguage Translation Group, MASTAR ProjectNational Institute of Information and Communications Technology3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, JAPAN{takashi.onishi,mutiyama,eiichiro.sumita}@nict.go.jpAbstractLattice decoding in statistical machinetranslation (SMT) is useful in speechtranslation and in the translation of Ger-man because it can handle input ambigu-ities such as speech recognition ambigui-ties and German word segmentation ambi-guities.
We show that lattice decoding isalso useful for handling input variations.Given an input sentence, we build a latticewhich represents paraphrases of the inputsentence.
We call this a paraphrase lattice.Then, we give the paraphrase lattice as aninput to the lattice decoder.
The decoderselects the best path for decoding.
Us-ing these paraphrase lattices as inputs, weobtained significant gains in BLEU scoresfor IWSLT and Europarl datasets.1 IntroductionLattice decoding in SMT is useful in speech trans-lation and in the translation of German (Bertoldiet al, 2007; Dyer, 2009).
In speech translation,by using lattices that represent not only 1-best re-sult but also other possibilities of speech recogni-tion, we can take into account the ambiguities ofspeech recognition.
Thus, the translation qualityfor lattice inputs is better than the quality for 1-best inputs.In this paper, we show that lattice decoding isalso useful for handling input variations.
?Inputvariations?
refers to the differences of input textswith the same meaning.
For example, ?Is therea beauty salon??
and ?Is there a beauty par-lor??
have the same meaning with variations in?beauty salon?
and ?beauty parlor?.
Since thesevariations are frequently found in natural languagetexts, a mismatch of the expressions in source sen-tences and the expressions in training corpus leadsto a decrease in translation quality.
Therefore,we propose a novel method that can handle in-put variations using paraphrases and lattice decod-ing.
In the proposed method, we regard a givensource sentence as one of many variations (1-best).Given an input sentence, we build a paraphrase lat-tice which represents paraphrases of the input sen-tence.
Then, we give the paraphrase lattice as aninput to the Moses decoder (Koehn et al, 2007).Moses selects the best path for decoding.
By usingparaphrases of source sentences, we can translateexpressions which are not found in a training cor-pus on the condition that paraphrases of them arefound in the training corpus.
Moreover, by usinglattice decoding, we can employ the source-sidelanguage model as a decoding feature.
Since thisfeature is affected by the source-side context, thedecoder can choose a proper paraphrase and trans-late correctly.This paper is organized as follows: Relatedworks on lattice decoding and paraphrasing arepresented in Section 2.
The proposed method isdescribed in Section 3.
Experimental results forIWSLT and Europarl dataset are presented in Sec-tion 4.
Finally, the paper is concluded with a sum-mary and a few directions for future work in Sec-tion 5.2 Related WorkLattice decoding has been used to handle ambigu-ities of preprocessing.
Bertoldi et al (2007) em-ployed a confusion network, which is a kind of lat-tice and represents speech recognition hypothesesin speech translation.
Dyer (2009) also employeda segmentation lattice, which represents ambigui-ties of compound word segmentation in German,Hungarian and Turkish translation.
However, tothe best of our knowledge, there is no work whichemployed a lattice representing paraphrases of aninput sentence.On the other hand, paraphrasing has been usedto enrich the SMT model.
Callison-Burch et1Input sentenceParaphrase LatticeOutput sentenceParaphraseListSMT modelParallel Corpus(for paraphrase)Parallel Corpus(for training)ParaphrasingLattice DecodingFigure 1: Overview of the proposed method.al.
(2006) and Marton et al (2009) augmentedthe translation phrase table with paraphrases totranslate unknown phrases.
Bond et al (2008)and Nakov (2008) augmented the training data byparaphrasing.
However, there is no work whichaugments input sentences by paraphrasing andrepresents them in lattices.3 Paraphrase Lattice for SMTOverview of the proposed method is shown in Fig-ure 1.
In advance, we automatically acquire aparaphrase list from a parallel corpus.
In order toacquire paraphrases of unknown phrases, this par-allel corpus is different from the parallel corpusfor training.Given an input sentence, we build a latticewhich represents paraphrases of the input sentenceusing the paraphrase list.
We call this lattice aparaphrase lattice.
Then, we give the paraphraselattice to the lattice decoder.3.1 Acquiring the paraphrase listWe acquire a paraphrase list using Bannard andCallison-Burch (2005)?s method.
Their idea is, iftwo different phrases e1, e2 in one language arealigned to the same phrase c in another language,they are hypothesized to be paraphrases of eachother.
Our paraphrase list is acquired in the sameway.The procedure is as follows:1.
Build a phrase table.Build a phrase table from parallel corpus us-ing standard SMT techniques.2.
Filter the phrase table by the sigtest-filter.The phrase table built in 1 has many inappro-priate phrase pairs.
Therefore, we filter thephrase table and keep only appropriate phrasepairs using the sigtest-filter (Johnson et al,2007).3.
Calculate the paraphrase probability.Calculate the paraphrase probability p(e2|e1)if e2 is hypothesized to be a paraphrase of e1.p(e2|e1) =?cP (c|e1)P (e2|c)where P (?|?)
is phrase translation probability.4.
Acquire a paraphrase pair.Acquire (e1, e2) as a paraphrase pair ifp(e2|e1) > p(e1|e1).
The purpose of thisthreshold is to keep highly-accurate para-phrase pairs.
In experiments, more than 80%of paraphrase pairs were eliminated by thisthreshold.3.2 Building paraphrase latticeAn input sentence is paraphrased using the para-phrase list and transformed into a paraphrase lat-tice.
The paraphrase lattice is a lattice which rep-resents paraphrases of the input sentence.
An ex-ample of a paraphrase lattice is shown in Figure 2.In this example, an input sentence is ?is there abeauty salon ??.
This paraphrase lattice containstwo paraphrase pairs ?beauty salon?
= ?beautyparlor?
and ?beauty salon?
= ?salon?, and rep-resents following three sentences.?
is there a beauty salon ??
is there a beauty parlor ??
is there a salon ?In the paraphrase lattice, each node consists ofa token, the distance to the next node and featuresfor lattice decoding.
We use following four fea-tures for lattice decoding.?
Paraphrase probability (p)A paraphrase probability p(e2|e1) calculatedwhen acquiring the paraphrase.hp = p(e2|e1)?
Language model score (l)A ratio between the language model proba-bility of the paraphrased sentence (para) andthat of the original sentence (orig).hl = lm(para)lm(orig)20 -- ("is"     , 1, 1, 1, 1)1 -- ("there"  , 1, 1, 1, 1)2 -- ("a"      , 1, 1, 1, 1)3 -- ("beauty" , 1, 1, 1, 2) ("beauty" , 0.250, 1.172, 1, 1) ("salon" , 0.133, 0.537, 0.367, 3)4 -- ("parlor" , 1, 1, 1, 2)5 -- ("salon"  , 1, 1, 1, 1)6 -- ("?"
, 1, 1, 1, 1)Paraphrase probability (p)Language model score (l)Paraphrase length (d)Distance to the next node Features for lattice decodingTokenFigure 2: An example of a paraphrase lattice, which contains three features of (p, l, d).?
Normalized language model score (L)A language model score where the languagemodel probability is normalized by the sen-tence length.
The sentence length is calcu-lated as the number of tokens.hL = LM(para)LM(orig) ,where LM(sent) = lm(sent)1length(sent)?
Paraphrase length (d)The difference between the original sentencelength and the paraphrased sentence length.hd = exp(length(para)?
length(orig))The values of these features are calculated onlyif the node is the first node of the paraphrase, forexample the second ?beauty?
and ?salon?
in line3 of Figure 2.
In other nodes, for example ?par-lor?
in line 4 and original nodes, we use 1 as thevalues of features.The features related to the language model, suchas (l) and (L), are affected by the context of sourcesentences even if the same paraphrase pair is ap-plied.
As these features can penalize paraphraseswhich are not appropriate to the context, appropri-ate paraphrases are chosen and appropriate trans-lations are output in lattice decoding.
The featuresrelated to the sentence length, such as (L) and (d),are added to penalize the language model scorein case the paraphrased sentence length is shorterthan the original sentence length and the languagemodel score is unreasonably low.In experiments, we use four combinations ofthese features, (p), (p, l), (p, L) and (p, l, d).3.3 Lattice decodingWe use Moses (Koehn et al, 2007) as a decoderfor lattice decoding.
Moses is an open sourceSMT system which allows lattice decoding.
Inlattice decoding, Moses selects the best path andthe best translation according to features added ineach node and other SMT features.
These weightsare optimized using Minimum Error Rate Training(MERT) (Och, 2003).4 ExperimentsIn order to evaluate the proposed method, weconducted English-to-Japanese and English-to-Chinese translation experiments using IWSLT2007 (Fordyce, 2007) dataset.
This dataset con-tains EJ and EC parallel corpus for the traveldomain and consists of 40k sentences for train-ing and about 500 sentences sets (dev1, dev2and dev3) for development and testing.
We usedthe dev1 set for parameter tuning, the dev2 setfor choosing the setting of the proposed method,which is described below, and the dev3 set for test-ing.The English-English paraphrase list was ac-quired from the EC corpus for EJ translation and53K pairs were acquired.
Similarly, 47K pairswere acquired from the EJ corpus for EC trans-lation.4.1 BaselineAs baselines, we used Moses and Callison-Burchet al (2006)?s method (hereafter CCB).
In Moses,we used default settings without paraphrases.
InCCB, we paraphrased the phrase table using theautomatically acquired paraphrase list.
Then,we augmented the phrase table with paraphrasedphrases which were not found in the originalphrase table.
Moreover, we used an additional fea-ture whose value was the paraphrase probability(p) if the entry was generated by paraphrasing and3Moses (w/o Paraphrases) CCB Proposed MethodEJ 38.98 39.24 (+0.26) 40.34 (+1.36)EC 25.11 26.14 (+1.03) 27.06 (+1.95)Table 1: Experimental results for IWSLT (%BLEU).1 if otherwise.
Weights of the feature and otherfeatures in SMT were optimized using MERT.4.2 Proposed methodIn the proposed method, we conducted experi-ments with various settings for paraphrasing andlattice decoding.
Then, we chose the best settingaccording to the result of the dev2 set.4.2.1 Limitation of paraphrasingAs the paraphrase list was automatically ac-quired, there were many erroneous paraphrasepairs.
Building paraphrase lattices with all erro-neous paraphrase pairs and decoding these para-phrase lattices caused high computational com-plexity.
Therefore, we limited the number of para-phrasing per phrase and per sentence.
The numberof paraphrasing per phrase was limited to three andthe number of paraphrasing per sentence was lim-ited to twice the size of the sentence length.As a criterion for limiting the number of para-phrasing, we use three features (p), (l) and (L),which are same as the features described in Sub-section 3.2.
When building paraphrase lattices, weapply paraphrases in descending order of the valueof the criterion.4.2.2 Finding optimal settingsAs previously mentioned, we have three choicesfor the criterion for building paraphrase latticesand four combinations of features for lattice de-coding.
Thus, there are 3 ?
4 = 12 combinationsof these settings.
We conducted parameter tuningwith the dev1 set for each setting and used as bestthe setting which got the highest BLEU score forthe dev2 set.4.3 ResultsThe experimental results are shown in Table 1.
Weused the case-insensitive BLEU metric for eval-uation.
In EJ translation, the proposed methodobtained the highest score of 40.34%, whichachieved an absolute improvement of 1.36 BLEUpoints over Moses and 1.10 BLEU points overCCB.
In EC translation, the proposed method alsoobtained the highest score of 27.06% and achievedan absolute improvement of 1.95 BLEU pointsover Moses and 0.92 BLEU points over CCB.
Asthe relation of three systems is Moses < CCB <Proposed Method, paraphrasing is useful for SMTand using paraphrase lattices and lattice decod-ing is especially more useful than augmenting thephrase table.
In ProposedMethod, the criterion forbuilding paraphrase lattices and the combinationof features for lattice decoding were (p) and (p, L)in EJ translation and (L) and (p, l) in EC transla-tion.
Since features related to the source-side lan-guage model were chosen in each direction, usingthe source-side language model is useful for de-coding paraphrase lattices.We also tried a combination of ProposedMethod and CCB, which is a method of decodingparaphrase lattices with an augmented phrase ta-ble.
However, the result showed no significant im-provements.
This is because the proposed methodincludes the effect of augmenting the phrase table.Moreover, we conducted German-Englishtranslation using the Europarl corpus (Koehn,2005).
We used the WMT08 dataset1, whichconsists of 1M sentences for training and 2K sen-tences for development and testing.
We acquired5.3M pairs of German-German paraphrases froma 1M German-Spanish parallel corpus.
We con-ducted experiments with various sizes of trainingcorpus, using 10K, 20K, 40K, 80K, 160K and 1M.Figure 3 shows the proposed method consistentlyget higher score than Moses and CCB.5 ConclusionThis paper has proposed a novel method for trans-forming a source sentence into a paraphrase latticeand applying lattice decoding.
Since our methodcan employ source-side language models as a de-coding feature, the decoder can choose properparaphrases and translate properly.
The exper-imental results showed significant gains for theIWSLT and Europarl dataset.
In IWSLT dataset,we obtained 1.36 BLEU points over Moses in EJtranslation and 1.95 BLEU points over Moses in1http://www.statmt.org/wmt08/42021222324252627282910 100 1000Corpus size (K)BLEU score (%)MosesCCBProposedFigure 3: Effect of training corpus size.EC translation.
In Europarl dataset, the proposedmethod consistently get higher score than base-lines.In future work, we plan to apply this methodwith paraphrases derived from a massive corpussuch as the Web corpus and apply this method to ahierarchical phrase based SMT.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages597?604.Nicola Bertoldi, Richard Zens, and Marcello Federico.2007.
Speech translation by confusion network de-coding.
In Proceedings of the International Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP), pages 1297?1300.Francis Bond, Eric Nichols, Darren Scott Appling, andMichael Paul.
2008.
Improving Statistical MachineTranslation by Paraphrasing the Training Data.
InProceedings of the International Workshop on Spo-ken Language Translation (IWSLT), pages 150?157.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved Statistical Machine Trans-lation Using Paraphrases.
In Proceedings of theHuman Language Technology conference - NorthAmerican chapter of the Association for Computa-tional Linguistics (HLT-NAACL), pages 17?24.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for MT.
In Proceed-ings of the Human Language Technology confer-ence - North American chapter of the Associationfor Computational Linguistics (HLT-NAACL), pages406?414.Cameron S. Fordyce.
2007.
Overview of the IWSLT2007 Evaluation Campaign.
In Proceedings of theInternational Workshop on Spoken Language Trans-lation (IWSLT), pages 1?12.J Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving Translation Qual-ity by Discarding Most of the Phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 967?975.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 177?180.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe 10th Machine Translation Summit (MT Summit),pages 79?86.Yuval Marton, Chris Callison-Burch, and PhilipResnik.
2009.
Improved Statistical MachineTranslation Using Monolingually-Derived Para-phrases.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 381?390.Preslav Nakov.
2008.
Improved Statistical MachineTranslation Using Monolingual Paraphrases.
InProceedings of the European Conference on Artifi-cial Intelligence (ECAI), pages 338?342.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 160?167.5
