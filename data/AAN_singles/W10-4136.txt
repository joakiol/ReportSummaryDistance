Chinese word segmentation model using bootstrappingBaobao Chang and Mairgup MansurInstitute of Computational Linguistics, Peking UniversityKey Laboratory of Computational Linguistics(Peking University),Ministry Education, Chinachbb@pku.edu.cn, mairgup@yahoo.com.cnAbstractWe participate in the CIPS-SIGHAN-2010 bake-off task of Chinese wordsegmentation.
Unlike the previousbakeoff series, the purpose of thebakeoff 2010 is to test the cross-domain performance of Chinese seg-mentation model.
This paper summa-rizes our approach and our bakeoff re-sults.
We mainly propose to use ?2 sta-tistics to increase the OOV recall anduse bootstrapping strategy to increasethe overall F score.
As the resultsshows, the approach proposed in thepaper does help, both of the OOV re-call and the overall F score are im-proved.1 IntroductionAfter more than twenty years of intensive re-searches, considerable progress has been madein improving the performance of Chinese wordsegmentation.
The bakeoff series hosted by theACL SIGHAN shows that high F scores can beachieved in the closed test tracks, in whichonly specified training materials can be used inlearning segmentation models.Instead of using lexicon-driven approaches,state-of-art Chinese word segmenter now usecharacter tagging model as Xue(2003) firstlyproposed.
In character tagging model, no pre-defined Chinese lexicons are required; a tag-ging model is learned using manually seg-mented training texts.
The model is then usedto assign each character a tag indicating theposition of this character within word.
Xue?sapproach has been become the most popularapproach to Chinese word segmentation for itshigh performance and unified way to deal withOOV issues.
Most of the segmentation workssince then follow this approach.
Major im-provements in this line of research including: 1)More sophisticated learning models were in-troduced instead of the maximum entropymodel that Xue used, like conditional randomfields (CRFs) model which fit the sequencetagging tasks much better than maximum en-tropy model (Tseng et al,2005).
2) More tagswere introduced, as Zhao et al (2006) shows 6tags are superior to 4 tags in achieving highperformance.
3) New feature templates wereadded, such as templates used in representingnumbers, dates, letters etc.
(Low et al, 2005)Usually, the performance of segmentationmodel is evaluated on a test set from the samedomain as the training set.
Such evaluationdoes not reveal its ability to deal with domainvariation.
It is believed that, when test set isfrom other domains than the domain wheretraining set is from, the learned model nor-mally underperforms substantially.The CIPS-SIGHAN-2010 bake-off task ofChinese word segmentation is set to focus onthe cross-domain performance of Chineseword segmentation model.We participate in the closed test track forsimplified Chinese.
Different with the previousbakeoffs, CIPS-SIGHAN-2010 bake-off pro-vides both label corpus and unlabeled corpora.The labeled corpus is composed of texts fromnewspaper and has about 1.1 million words intotal.
The two unlabeled corpora cover twodomains: literature and computer science, andeach domain have about 100K characters insize.
The test corpora cover four domains, twoof which are literature and computer science,and the other two domains are unknown beforereleasing.We build the Chinese word segmenter fol-lowing the character tagging model.
Instead ofusing CRF model, we use the hidden Markovsupport vector machines (Altun et al, 2003),which is also a sequence labeling model likeCRF.
We just show it can also be used tomodel Chinese segmentation tasks as an alter-native other than CRF.
To increase the abilityof the model to recall OOV words, we proposeto use ?2 statistics and bootstrapping strategyto the overall performance of the model to out-of-domain texts.2 The hidden Markov support vectormachinesThe hidden Markov support vector machine(SVM-HMM) is actually a special case of thestructural support vector machines proposed byTsochantaridis et al(2005) which is a powerfulmodel to structure predication problem.
It dif-fers from support vector machine in its abilityto model complex structured problems andshares the max-margin training principles withsupport vector machines.
The hidden Markovsupport vector machine model is inspired bythe hidden Markov model and is an instance ofstructural support vector machine dedicated tosolve sequence labeling learning, a problemthat CRF model is assumed to solve.
In theSVM-HMM model, the sequence labelingproblems is modeled by learning a discrimi-nant function F: X?Y?R over the input se-quence and the label sequence pairs, thus pre-diction of label sequence can be derived bymaximizing F over all possible label sequencesfor a specific given input sequence x.
);,(maxarg);( wyxwxyFfY?=In the structural SVMs, F is assumed to be lin-ear in some combined feature representation ofthe input sequence and the label sequence?
(x,y), i.e.
),(,);,( yx?wwyx =Fwhere w denotes a parameter vector.
For theSVM-HMMs, the discriminant function is de-fined as follows.?
?????==??
??+?
?+=1..1..1'1', )',(),(?
),()(,);,(TtTty yttyyyttyyyyyyyyxF???
?wx?wwHere )?,( www = , ?
(xt) is the vector of fea-tures of the input sequence.Like SVMs, parameter vector w is learnedwith maximum margin principle using trainingdata.
To control the complexity of the trainingproblem, cutting plane method is proposed tosolve the resulted constrained optimizationproblem.
Thus only small subset of constraintsfrom the full-sized optimization is checked toensure a sufficiently accurate solution.Roughly speaking, SVM-HMM differs withCRF in its principle of training, both of themcould be used to deal with sequence labelingproblem like Chinese word segmentation.3 The tag set and the basic featuretemplatesAs most of other works on segmentation, weuse a 4-tag tagset, that is S for character beinga single-character-word by itself, B for charac-ter beginning a multi-character-word, E forcharacter ending a multi-character-word and Mfor a character occurring in the middle of amulti-character-word.We use the following feature template, likemost of segmentation works widely used:(a) Cn (n = -2, -1, 0, 1, 2)(b) CnCn+1 (n = -2, -1, 0, 1)(c) C-1C+1Here C refers to character; n refers to the posi-tion index relative to the current character.
Bysetting the above feature templates, we actuallyset a 5-character window to extract features,the current character, 2 characters to its leftand 2 characters to its right.In addition, we also use the following fea-ture templates to extract features representingcharacter type.
The closed test track of CIPS-SIGHAN-2010 bake-off allows participants touse four character types, which are ChineseCharacter, English Letter, digits and punctua-tions:(d) Tn (n = -2, -1, 0, 1, 2)(e) TnTn+1 (n = -2, -1, 0, 1)(f) T-1T+1Here T refers to character type, its value can bedigit, letter, punctuation or Chinese character.4 The ?2 statistic featuresOne of reasons of the performance degrada-tion lies in the model?s ability to cope withOOV words while working with the out-of-domain texts.
Aiming at preventing the OOVrecall from dropping sharply, we propose touse ?2 statistics as features to the segmentationmodel.
?2 test is one of hypothesis test methods,which can be used to test if two events co-occur just by chance or not.
A lower ?2 scorenormally means the two co-occurred events areindependent; otherwise they are dependent oneach other.
Hence, ?2 statistics could also beused to deal with the OOV issue in segmenta-tion models.
The idea is very straightforward.If two adjacent characters in the test set have ahigher ?2 score, it is highly likely they form aword or are part of a word even they are notseen in the training set.We only compute ?2 score for character bi-grams in the training texts and test texts.
The?2 score of a bigram  C1C2 can be computed bythe following way.
)()()()()(),(2212dcdbcabacbdanCC +?+?+?+???
?=?Here,a refers to all counts of bigram C1C2 in thetext;b refers to all counts of bigrams that C1 oc-curs but C2 does not;c refers to all counts of bigrams that C1 doesnot occur but C2 occurs;d refers to all counts of bigrams that both C1and C2 do not occur.n refers to total counts of all bigrams in thetext, apparently, n=a+b+c+d.We do the ?2 statistics computation to thetraining texts and the test texts respectively.
Tomake the ?2 statistics from the training textsand test texts comparable, we normalize the ?2score by the following formula.??????
??
?= 10),(),( 2min2max2min212212?????
CCCCnormThen we incorporate the normalized ?2 statis-tics into the SVM-HMM model by adding twomore feature templates as follows:(g) XnXn+1 (n = -2, -1, 0, 1)(h) X-1X+1The value of the feature XnXn+1 is the normal-ized ?2 score of the bigram CnCn+1.
Note wealso compute the normalized ?2 score to bi-gram C-1C+1.Because the normalized ?2 score is one of11 possible values 0, 1, 2, ?, 10,  templates(g)-(h) generate 55 features in total.All features generated from the templates(a)-(f) together with the 55 ?2 features form thewhole feature set.
The training texts and testtexts are then converted into their feature rep-resentations.
The feature representation of thetraining texts is then used to learn the modeland the feature representation of the test textsis used for segmentation.
By this way, we ex-pect that an OOV word in the test texts mightbe found by the segmentation model if the bi-grams extracted from this word take higher ?2scores.5 the bootstrapping strategyThe addition of the ?2 features can be alsoharmful.
Even though it could increase theOOV recall, it also leads to drops in IV recallas we found.
To keep the IV recall from fallingdown, we propose to use bootstrapping strat-egy.
Specifically, we choose to use both mod-els with ?2 features and without ?2 features.We train two models firstly, one is ?2-basedand another not.
Then we do the segmentationto the test text with the two models simultane-ously.
Two segmentation results can be ob-tained.
One result is produced by the ?2-basedmodel and has a high OOV recall.
The otherresult is produced by the non-?2-based modeland has higher IV recall.
Then we do intersec-tion operation to the two results.
It is not diffi-cult to understand that the intersection of thetwo results has both high OOV recall and highIV recall.
We then put the intersection resultsinto the training texts to form a new trainingset.
By this new training set, we train again toget two new models, one ?2-based and anothernot.
Then the two new models are used tosegment the test texts.
Then we do again inter-section to the two results and the commonparts are again put into the training texts.
Werepeat this process until a plausible result isobtained.The whole process can be informally de-scribed as the following algorithm:1. let training set T to be the originaltraining set;2. for I = 0 to K1) train a ?2-based model and a non-?2-base model separately usingtraining set T;2) use both models to segment testtexts;3) do intersection to the two segmen-tation results4) put the intersection results into thetraining set and get the enlargedtraining set T3.
train the non-?2-based model usingtraining set T, and take the output ofthis model as the final output;4. end.6 The evaluation resultsThe labeled training texts released by thebakeoff are mainly composed of texts fromnewspaper.
A peculiarity of the training data isthat all Arabic numbers, Latin letters and punc-tuations in the data are double-byte codes.
Asin Chinese texts, there are actually two ver-sions of codes for Arabic numbers, Latin let-ters and punctuations: one is single-byte codesdefined by the western character encodingstandard; another is double-byte codes definedby the Chinese character encoding standards.Chinese normally use both versions withoutdistinguishing them strictly.The four final test sets released by the bake-off cover four domains, the statistics of the testsets are shown in table-1.
(the size is measuredin characters)Table-1.
Test sets statisticstest set domain  size  OOV rateA Literature 51K 0.069B Computer 64K 0.152C Medicines 52K 0.110D Finance 56K 0.087We train all models using SVM-HMMs1, weset ?
to 0.25.
This is a parameter to control theaccuracy of the solution of the optimizationproblem.
We set C to half of the number of thesentences in the training data.
The C parameteris set to trade off the margin size and trainingerror.
We also set a cutoff frequency to featureextraction.
Only features are seen more thanthree times in training data are actually used inthe models.
We set K = 3 and run the algo-rithm shown in section 5.
This gives our finalbakeoff results shown in Table-2.To illustrate whether the ?2 statistics andbootstrapping strategy help or not, we alsoshow two intermediate results using the onlinescoring system provided by the bakeoff2.Table-3 shows the results of the initial non-?2-basedmodel using feature template (a)-(f), table-4shows results of the initial ?2-based model us-ing feature template (a)-(h).As we see from the table-1, table-3 and ta-ble-4, the approach present in this paper doesimprove both the overall performance and theOOV recalls in all four domains.Table-3 Results of initial non-?2-based modeltest set R P F RoovA 0.921 0.924 0.923 0.632B 0.930 0.904 0.917 0.758C 0.919 0.906 0.913 0.687D 0.946 0.924 0.935 0.7501http://www.cs.cornell.edu/People/tj/svm_light/svm_hmm.html2 http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/#Table-2.
The bakeoff resultstest set R P F Riv RoovA 0.925 0.931 0.928 0.944 0.667B 0.941 0.916 0.928 0.967 0.796C 0.928 0.918 0.923 0.953 0.730D 0.948 0.928 0.937 0.965 0.761Table-4 Results of initial ?2-based modeltest set R P F RoovA 0.898 0.921 0.910 0.673B 0.925 0.914 0.920 0.801C 0.916 0.922 0.919 0.764D 0.931 0.937 0.934 0.821We also do a rapid manual check to the finalresults; one of the main sources of errors lies inthe approach failing to recall numbers encodedby one-byte codes digits.
For the labeled train-ing corpus provided by the bakeoff almost donot use one-byte codes for digits, and the typefeature seems do not help too much.
Actually,such numbers can be recalled by simple heuris-tics using regular expressions.
We do a simplenumber recognition to the test set of domain D.this will increase the F score from 0.937 to0.957.7 ConclusionsThis paper introduces the approach we usedin the CIPS-SIGHAN-2010 bake-off task ofChinese word segmentation.
We propose touse ?2 statistics to increase OOV recall and usebootstrapping strategy to increase the overallperformance.
As our final results shows, theapproach works in increasing both of the OOVrecall and overall F-score.We also show in this paper that hiddenMarkov support vector machine can be used tomodel the Chinese word segmentation problem,by which high f-score results can be obtainedlike CRF model.AcknowledgementsThis work was supported by National Natu-ral Science Foundation of China under GrantNo.
60975054 and National Social ScienceFoundation of China under Grant No.06BYY048.We want to thank Professor Duan Huimingand Mr. Han Dongxu for their generous help atthe data preprocessing works.ReferencesLiang, Nanyuan, 1987.
?
?written Cinese text seg-mentation system--cdws?.
Journal of Chinese In-formation Processing, Vol.2, NO.2,pp44?52.
(inChinese)Gao, Jianfeng et al, 2005, Chinese Word Segmen-tation and Named Entity Recognition: A Prag-matic Approach, Computational Linguis-tics,Vol.31, No.4, pp531-574.Huang, Changning et al 2007, Chinese word seg-mentation: a decade review.
Journal of ChineseInformation Processing, Vol.21, NO.3,pp8?19.
(in Chinese)Tseng, Huihsin et al, 2005, A conditional randomfield word segmenter for SIGHAN 2005, Pro-ceedings of the fourth SIGHAN workshop onChinese language processing.
Jeju Island, Korea.pp168-171Xue, Nianwen, 2003, Chinese Word Segmentationas Character Tagging, Computational Linguisticsand Chinese Language Processing.
Vol.8, No.1,pp29-48.Zhao, Hai et al, 2006, Effective tag set selection inChinese word segmentation via conditional ran-dom field modeling, Proceedings of the 20th Pa-cific Asia Conference on language, Informationand Computation (PACLIC-20), Wuhan, China,pp87-94Tsochantaridis,Ioannis et al, 2005, Large MarginMethods for Structured and Interdependent Out-put Variables, Journal of Machine Learning Re-search (JMLR), No.6, pp1453-1484.Altun,Yasemin et al,2003, Hidden Markov SupportVector Machines.
Proceedings of the TwentiethInternational Conference on Machine Learning(ICML-2003), Washington DC, 2003.Low, Jin Kiat et al,2005, A Maximum EntropyApproach to Chinese Word Segmentation.
Pro-ceedings of the Fourth SIGHAN Workshop onChinese Language Processing, Jeju Island, Ko-rea,.
pp161-164
