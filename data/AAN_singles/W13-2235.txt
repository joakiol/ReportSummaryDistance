Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 281?291,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsDramatically Reducing Training Data Size Through VocabularySaturationWilliam D. LewisMicrosoft ResearchOne Microsoft WayRedmond, WA 98052wilewis@microsoft.comSauleh EetemadiMicrosoft ResearchOne Microsoft Way, Redmond, WA 98052Michigan State University, East Lansing, MI 48824saulehe@microsoft.comAbstractOur field has seen significant improve-ments in the quality of machine translationsystems over the past several years.
Thesingle biggest factor in this improvementhas been the accumulation of ever largerstores of data.
However, we now find our-selves the victims of our own success, inthat it has become increasingly difficult totrain on such large sets of data, due tolimitations in memory, processing power,and ultimately, speed (i.e., data to mod-els takes an inordinate amount of time).Some teams have dealt with this by focus-ing on data cleaning to arrive at smallerdata sets (Denkowski et al 2012a; Rarricket al 2011), ?domain adaptation?
to ar-rive at data more suited to the task at hand(Moore and Lewis, 2010; Axelrod et al2011), or by specifically focusing on datareduction by keeping only as much data asis needed for building models e.g., (Ecket al 2005).
This paper focuses on tech-niques related to the latter efforts.
We havedeveloped a very simple n-gram countingmethod that reduces the size of data setsdramatically, as much as 90%, and is ap-plicable independent of specific dev andtest data.
At the same time it reducesmodel sizes, improves training times, and,because it attempts to preserve contexts forall n-grams in a corpus, the cost in qualityis minimal (as measured by BLEU ).
Fur-ther, unlike other methods created specif-ically for data reduction that have similareffects on the data, our method scales tovery large data, up to tens to hundreds ofmillions of parallel sentences.1 IntroductionThe push to build higher and higher quality Sta-tistical Machine Translation systems has led theefforts to collect more and more data.
TheEnglish-French (nearly) Gigaword Parallel Corpus(Callison-Burch et al 2009), which we will referto henceforth as EnFrGW, is the result of one sucheffort.
The EnFrGW is a publicly available cor-pus scraped from Canadian, European and inter-national Web sites, consisting of over 22.5M par-allel English-French sentences.
This corpus hasbeen used regularly in the WMT competition since2009.As the size of data increases, BLEU scores in-crease, but the increase in BLEU is not linear in re-lation to data size.
The relationship between datasize and BLEU flattens fairly quickly, as demon-strated in Figure 1.
Here we see that BLEU scoresincrease rapidly with small amounts of data, butthey taper off and flatten at much larger amounts.Clearly, as we add more data, the value of the newdata diminishes with each increase, until very littlevalue is achieved through the addition of each newsentence.
However, given that this figure repre-sents samples from EnFrGW, can we be more effi-cient in the samples we take?
Can we achieve nearequivalent BLEU scores on much smaller amountsof data drawn from the same source, most espe-cially better than what we can achieve through ran-dom sampling?The focus of this work is three-fold.
First, weseek to devise a method to reduce the size of train-ing data, which can be run independently of par-ticular dev and test data, so as to maintain the in-dependence of the data, since we are not interestedhere in domain adaptation or selective tuning.
Sec-ond, we desire an algorithm that is (mostly) qual-ity preserving, as measured by BLEU, OOV rates,and human eval, ultimately resulting in decreasedtraining times and reduced model sizes.
Reduced281Figure 1: BLEU score increase as more data isadded (in millions of words), random samplesfrom EnFrGWtraining times provide for greater iterative capac-ity, since we can make more rapid algorithmicimprovements and do more experimentation onsmaller data than we can on much larger data.Since we operate in a production environment, de-ploying smaller models is also desirable.
Third,we require a method that scales to very large data.We show in the sections below the application ofan algorithm at various settings to the 22.5M sen-tence EnFrGW corpus.
Although large, 22.5Msentences does not represent the full total of theEnglish-French data on the Web.
We require analgorithm that can apply to even larger samples ofdata, on the order of tens to hundreds of millionsof sentences.2 Related WorkIn statistical machine translation, selection, prepa-ration and processing of parallel training data isoften done to serve one of the following scenarios:?
Low Resource Languages: In languages withlow parallel data availability, a subset of amonolingual corpus is selected for humantranslation ((Ananthakrishnan et al 2010),(Eck et al 2005) and (Haffari et al 2009)).?
Mobile device deployment: For many lan-guages, translation model sizes built on allavailable parallel data are too large to behosted on mobile devices.
In addition totranslation model pruning, a common solu-tion is selecting a subset of the data to betrained on ((Ananthakrishnan et al 2010)and (Yasuda et al 2008)).?
Quick turn-around time during development:A common motivation for training on a sub-set of a parallel corpus is to reduce trainingtime during the development cycle of a sta-tistical machine translation system ((Lin andBilmes, 2011) and (Chao and Li, 2011a)).?
Noise reduction: Simple noise reductiontechniques like sentence length and alpha nu-meric ratio are often used in data preparation.However, more sophisticated techniques havebeen developed to filter out noise from par-allel data ((Denkowski et al 2012a) and(Taghipour et al 2010)).?
Domain Adaptation: Recently there has beensignificant interest in domain adaptation forstatistical machine translation.
One of the ap-proaches to domain adaptation is selecting asubset of a data that is closer to the target do-main ((Moore and Lewis, 2010), (Axelrod etal., 2011)).?
Improve translation quality: An interestingarea of research is selecting a subset of thetraining data that is more suitable for sta-tistical machine translation learning ((Okita,2009)).In comparison, the goal of this work is to effi-ciently reduce very large parallel data sets (in ex-cess of tens of billions of tokens) to a desired sizein a reasonable amount of time.
In the related workreferenced above two primary methods have beenused.1.
Maximizing n-gram coverage with minimaldata.2.
Filtering out noisy data based on sentence-pair based features.One of the earliest and most cited works usingthe first method is (Eck et al 2005).
In this work,a greedy algorithm is developed to select a subsetof the entire corpus that covers most n-grams withminimum number of words.
In a later work by thesame author, the algorithm was modified to givehigher weight to more frequent words.
Althoughthis is a greedy algorithm and does not provide theoptimum solution, its complexity is quadratic inthe number of sentences.
Hence it is not practicalto run this algorithm over very large data sets.Recently (Ananthakrishnan et al 2010) intro-duced a new algorithm that is an improvement282over (Eck et al 2005).
In this work discriminativetraining is used to train a maximum entropy pair-wise comparator with n-gram based features.
Thepair-wise comparator is used to select the highestscoring sentence followed by discounting featuresused for the sentence, which are drawn from theglobal pool of features.
The complexity of this al-gorithm after training the pairwise comparator isO (N ?K ?
log(F )) where N is the number ofsentences in the entire corpus, K is the number ofsentences to be selected and F is the size of the fea-ture space.
Although this method works well fora constant K, its complexity is quadratic when Kis a fraction of N .
This method is reported to im-prove the BLEU score close to 1% over the workdone by (Eck et al 2005).
(Denkowski et al 2012a) have developed rela-tively scalable algorithms that fit in the second cat-egory above.
This algorithm automatically filtersout noisy data primarily based on the followingfeature functions: normalized source and targetlanguage model scores, word alignment scores andfraction of aligned words.
Sentences that don?tscore above a certain threshold (mean minus oneor two standard deviations) for all their featuresare filtered out.
In a similar work, (Taghipouret al 2010) use an approach where they incor-porate similar features based on translation tableentries, word alignment models, source and targetlanguage models and length to build a binary clas-sifier that filters out noisy data.Our work incorporates both methods listedabove in a scalable fashion where it selects a sub-set of the data that is less noisy with a reasonablen-gram representation of the superset parallel cor-pus.
To put the scalability of our work in perspec-tive we compiled Table 1, which shows the max-imum size of the data sets reported in each of therelevant papers on the topic.
Despite the publicavailability of parallel corpora in excess of tensof millions of sentence pairs, none of the relatedworks, using the first method above, exceed cou-ple of millions of sentences pairs.
This demon-strates the importance of developing a scalable al-gorithm when addressing the data selection prob-lem.The careful reader may observe that an alter-nate strategy for reducing model sizes (e.g., use-ful for the Mobile scenario noted above, but alsoin any scenario where space concerns are an is-sue), would be to reduce phrase table size ratherReference Total Sentences(Ananthakrishnan et al 2010) 253K(Eck et al 2005) 123K(Haffari et al 2009) 1.8M1(Lin and Bilmes, 2011) 1.2M2(Chao and Li, 2011b) 2.3MTable 1: Data Sizes for Related Systemsthan reduce training data size.
A good exampleof work in this space is shown in (Johnson et al2007), who describe a method for phrase table re-duction, sometimes substantial (>90%), with noimpact on the resulting BLEU scores.
The prin-cipal of our work versus theirs is where the datareductions occur: before or after training.
The pri-mary benefit of manipulating the training data di-rectly is the impact on training performance.
Fur-ther, given the increasing sizes of training data,it has become more difficult and more time con-suming to train on large data, and in the case ofvery large data (say tens to hundreds of millionsof sentence pairs), it may not even be possible totrain models at all.
Reduced training data sizes in-creases iterative capacity, and is possible in caseswhere phrase table reduction may not be (i.e., withvery big data).3 Vocabulary Saturation Filter (VSF)The effects of more data on improving BLEUscores is clearly discernible from Figure 1: asmore data is added, BLEU scores increase.
How-ever, the relationship between quantity of data andBLEU is not linear, such that the effects of moredata diminishes with each increase in data size, ef-fectively approaching some asymptote.
One mightsay that the vocabulary of the phrase mappings de-rived from model training ?saturate?
as data sizeincreases, since less and less novel informationcan be derived from each succeeding sentence ofdata added to training.
It is this observation thatled us to develop the Vocabulary Saturation Filter(VSF).VSF makes the following very simple assump-tion: for any given vocabulary item v there is somepoint where the contexts for v?that is, the n-gram1Sentence count was not reported.
We estimated it basedon 18M tokens.2This is a very interesting work, but is only done for se-lecting speech data.
The total number of sentences is not re-ported.
We given a high-end estimate based on 128K selectedtokens.283sequences that contain v?approach some levelof saturation, such that each succeeding sentencecontaining v contributes few or no additional con-texts, and thus has little impact on the frequencydistributions over v. In other words, at a pointwhere the diversity of contexts for v approach amaximum, there is little value in adding additionalcontexts containing v, e.g., to translation models.The optimal algorithm would then, for each v?
V, identify the number of unique contexts thatcontain v up to some threshold and discard all oth-ers.
An exhaustive algorithm which sets thresh-olds for all n-gram contexts containing v, however,would take a large amount of time to run (mini-mally quadratic), and may also overrun memorylimitations on large data sets.For VSF, we made the following simplifying as-sumption: we set an arbitrary count threshold t forall vocabulary items.
For any given v, when wereach t, we no longer need to keep additional sen-tences containing v. However, since each instanceof v does not exist in isolation, but is rather con-tained within sentences that also contain other vo-cabulary items v, which, in turn, also need to becounted and thresholded, we simplified VSF evenfurther with the following heuristic: for any givensentence s, if all v ?
V within s have not reachedt, then the sentence is kept.
This has the directconsequence that many vocabulary items will havefrequencies above t in the output corpus.The implementation of VSF is described in Al-gorithm 1 below.VSF clearly makes a number of simplifying as-sumptions, many of which one might argue wouldreduce the value of the resulting data.
Althougheasy to implement, it may not achieve the mostoptimal results.
Assuming that VSF might be de-fective, we then looked into other algorithms at-tempting to achieve the same or similar results,such as those described in Section 2, and exploredin-depth the algorithms described in (Eck et al2005).4 An Alternative: (Eck et al 2005)In our pursuit of better and generic data reductionalgorithms, we did a number of experiments usingthe algorithms described in (Eck et al 2005).
Inthe n-gram based method proposed by this workthe weight of each function is calculated usingEquation 1, where j is the n-gram length.
Ineach iteration of the algorithm, the weight of eachInput: ParallelCorpus, N, LOutput: SelectedCorpusforeach sp ?
ParallelCorpus doS ?
EnumNgrams(sp.src, L);T ?
EnumNgrams(sp.tgt, L);selected?
false;foreach (s, t) ?
(S, T ) doif SrcCnt [s]<N ?
TgtCnt [t]<Nthenselected?
true;endendif selected thenSelectedCorpus.Add(sp);foreach (s, t) ?
(S, T ) doSrcCnt [s]++;TgtCnt [t]++;endendendAlgorithm 1: Pseudocode for implementingVSF.
L: n-gram length, N: n-gram threshold.sentence is calculated and the sentence with thehighest weight is selected.
Once a sentence is se-lected, the n-grams in the sentence are marked asseen and have a zero weight when they appear insubsequent sentences.
Therefore, the weights ofall remaining sentences have to be recalculated be-fore the next sentence can be selected.
We refer tothis algorithm henceforth as the Eck algorithm.Wj (sentence) =j?i=1??
?unseenngramsFreq(ngram)?
?|sentence| (1)To compare VSF against the Eck algorithmwe selected the English-Lithuanian parallel corpusfrom JRC-ACQUIS (Steinberger et al 2006).
Weselected the corpus for the following reasons:?
VSF performance on this particular data setwas at its lowest compared to a number ofother data sets, so there was room for im-provement by a potentially better algorithm.?
With almost 50 million tokens combined (En-glish and Lithuanian) we were able to opti-mize the Eck algorithm and run it on this dataset in a reasonable amount of time.
The ex-periments run by the original paper in 2005were run on only 800,000 tokens.284Using the Eck algorithm with n-gram length setto one (j ?
1 in Equation 1) only 10% (5,020,194tokens total) of the data is sorted, since all n-gramsof size one have been observed by that point andthe weight function for the remaining sentencesreturns zero.
In other words, since there are nounseen unigrams after 10% of the data has beensorted, in Equation 1, the numerator becomes zerothere after and therefore the remaining 90% ofsentence pairs are not sorted.
This must be takeninto consideration when examining the compari-son between unigram VSF and the Eck algorithmwith n-gram length set to one in Figure 2.
VSFwith its lowest setting, that is threshold t=1, se-lects 20% of the data, so this chart may not be afair comparison between the two algorithms.Figure 2: Unigram Eck vs. Unigram VSFIn an attempt to do a fairer comparison, we alsotried n-grams of length two in the Eck algorithm,where 50% of the data can be sorted (since all uni-grams and bigrams are observed by that point).
Asseen in Figure 3, the BLEU scores for the Eck andVSF systems built on the similar sized data scorevery closely on the WMT 2009 test set.3Further exploring options using Eck, we devel-oped the following two extensions to the Eck algo-rithm, none of which resulted in a significant gainin BLEU score over VSF with n-gram lengths setup to three.?
Incorporating target sentence n-grams in ad-dition to source side sentence n-grams.?
Dividing the weight of an n-gram (its fre-3The careful reader may note that there is no officialWMT09 test set for Lithuanian, since Lithuanian is not (yet)a language used in the WMT competition.
The test set men-tioned here was created from a 1,000 sentence sample fromthe English-side of the WMT09 test sets, which we then man-ually translated into Lithuanian.quency) by a constant number each time asentence that contains the n-gram is selected,as opposed to setting the weight of an n-gramto zero after it has been seen for the firsttime.4In relatively small data sets there is not a signif-icant difference between the two algorithms.
TheEck algorithm does not scale to larger data setsand higher n-grams.
Since a principal focus of ourwork is on scaling to very large data sets, and sinceEck could not scale to even moderately sized datasets, we decided to continue our focus on VSF andimprovements to that algorithm.Figure 3: Bigram Eck vs. Unigram VSF5 Data OrderUnlike the Eck algorithm, VSF is sensitive to theorder of the input data due to the nature of the al-gorithm.
Depending on the order of sentences inthe input parallel corpus, VSF could select differ-ent subsets of the parallel corpus that would even-tually (after training and test) result in differentBLEU scores.
To address this concern we usea feature function inspired by (Denkowski et al2012a) which is a normalized combined alignmentscore.
This feature score is obtained by geomet-ric averaging of the normalized forward and back-ward alignment scores which in turn are calculatedusing the process described in (Denkowski et al2012a).
To keep the algorithm as scalable as pos-sible we use radix sort.
This ordering of the dataensures sentences with high normalized alignmentscores appear first and sentences with low normal-ized alignment appear last.
As a result, for eachn-gram, VSF will choose the top-N highest scor-ing sentence pairs that contain that n-gram.4Further details of the modifications to the Eck algorithmare not discussed here as they did not yield improvementsover the baseline algorithm and the focus of our work pre-sented here was shifted to improvements over VSF.2855.1 Data Ordering ComplexityOrdering the data based on normalized combinedalignment score requires two steps.
First, thenormalized combined alignment score is com-puted for each sentence pair using an exist-ing HMM alignment model.
Next, sentencepairs are sorted based on the calculated score.The computational complexity of aligning a sin-gle sentence pair is O (J + I2) where J is thenumber of words in the source sentence and Iis the number of words in the target sentence(Gao and Vogel, 2008).
Therefore the com-plexity of calculating the combined alignmentscore would be O (N ?
(J2 + I + I2 + J)) orO(N ?max(I, J)2) after simplification.
Sinceradix sort is used for sorting the data, the data canbe sorted in O(d ?
N) where d is the number ofsignificant digits used for sorting.
Since d is keptconstant5, the overall computational complexityfor data ordering is O (N +N ?max(I, J)2).6 Experiments6.1 The Machine Translation and TrainingInfrastructureWe used a custom-built tree-to-string (T2S) sys-tem for training the models for all experiments.The T2S system that we developed uses tech-nology described in (Quirk et al 2005), and re-quires a source-side dependency parser, which wehave developed for English.6 We trained a 5-gram French LM over the entire EnFrGW, whichwe used in all systems.
We used Minimum ErrorRate Training (MERT) (Och, 2003) for tuning thelambda values for all systems, tuned using the of-ficial WMT2010 dev data.6.2 Test and Training DataIn all experiments, we used the EnFrGW cor-pus, or subsets thereof.
7 We used three test sets5In experiments described in Section 6 five significantdigits were used for radix sort.6Further details about the decoders is beyond the scope ofthis paper.
The reader is encouraged to refer to the sourcesprovided for additional information.7Because of some data cleaning filters we applied to thedata, the actual full sized corpus we used consisted of slightlyless data than that used in the WMT competitions.
Everyteam has its own set of favorite data cleaning heuristics, andours is no different.
The filters applied to this data are focusedmostly on noise reduction, and consist of a set of filters re-lated to eliminating content that contains badly encoded char-acters, removing content that is too long (since there is littlevalue in training on very long sentences), removing contentwhere the ratio between numeric versus alphabetic characterst = Random VSF Ordered VSF1 1.83 M 1.83 M 1.68 M2 2.53 M 2.53 M 2.34 M5 3.62 M 3.62 M 3.35 M10 4.62 M 4.62 M 4.29 M20 5.83 M 5.83 M 5.44 M40 7.26 M 7.26 M 6.83 M60 8.21 M 8.21 M 7.78 M100 9.53 M 9.53 M 9.13 M150 10.67 M 10.67 M 10.33 M200 11.53 M 11.53 M 11.23 M250 12.22 M 12.22 M 11.97 MAll 22.5 MTable 2: English-side Sentence Counts (in mil-lions) for different thresholds for VSF, VSF afterordering the data based on normalized combinedalignment score and random baselines.in all experiments, as well.
Two consisted ofthe WMT 2009 and 2010 test sets, used in theWMT competitions in the respective years.
Thethird consisted of 5,000 parallel English/Frenchsentences sampled from logs of actual traffic re-ceived by our production service, Bing Transla-tor (http://bing.com/translator), which were thenmanually translated.
The first two test sets arepublicly available, but are somewhat news fo-cused.
The third, which we will call ReqLog, con-sists of a mix of content and sources, so can beconsidered a truly ?general?
test set.To discern the effects of VSF at different de-grees of ?saturation?, we tried VSF with differentthreshold values t, ranging from 1 to 250.
For eacht value we actually ran VSF twice.
In the first case,we did no explicit sorting of the data.
In the sec-ond case, we ranked the data using the method de-scribed in Section 5.Finally, we created random baselines for eacht, where each random baseline is paired with therelevant VSF run, controlled for the number ofsentences (since t has no relevance for randomsamples).
The different t values and the resultingtraining data sizes (sentence and word counts) areshown in Tables 2 and 3.Since our interest in this study is scaling paral-lel data, for all trainings we used the same LM,which was built over all training data (the Frenchside of the full EnFrGW).
Because monolingualtraining scales much more readily than parallel,is excessively large, deleting content where the script of thecontent is mostly not in latin1 (relevant for French), and someadditional filters described in (Denkowski et al 2012b).
Ifthe reader wishes additional material on data filtration, pleasesee (Denkowski et al 2012b) and (Lewis and Quirk, 2013).286t = Random VSF Ordered VSF1 46.1 M 64.52 M 65.74 M2 63.99 M 87.41 M 88.12 M5 91.55 M 121.3 M 120.86 M10 116.83 M 151.53 M 149.95 M20 147.31 M 186.99 M 184.14 M40 183.46 M 228.14 M 224.29 M60 207.42 M 254.89 M 250.68 M100 240.88 M 291.45 M 287.02 M150 269.77 M 322.5 M 318.33 M200 291.4 M 345.37 M 341.69 M250 308.83 M 363.44 M 360.32 MAll 583.97 MTable 3: English-side Word Counts for differentthresholds for VSF, VSF after ordering the databased on normalized combined alignment scoreand random baselines.this seemed reasonable.
Further, using one LMcontrols one parameter that would otherwise fluc-tuate across trainings.
The result is a much morefocused view on parallel training diffs.6.3 ResultsWe trained models over each set of data.
In ad-dition to calculating BLEU scores for each result-ing set of models in (Table 5), we also comparedOOV rates (Table 6) and performance differences(Table 4).
The former is another window into the?quality?
of the resulting models, in that it de-scribes vocabulary coverage (in other words, howmuch vocabulary is recovered from the full data).The latter gives some indication regarding the timesavings after running VSF at different thresholds.On the WMT09 data set, both sets of VSFmodels outperformed the relevant random base-lines.
On the WMT10 and ReqLog test sets, thepre-sorted VSF outperformed all random base-lines, with the unsorted VSF outperforming mostrandom baselines, except at t=60 and t=200 forWMT10.
For the ReqLog, unsorted VSF drops be-low random starting at t=200.
Clearly, the t=200results show that there is less value in VSF as weapproach the total data size.The most instructive baseline, however, is theone built over all training data.
It is quite obvi-ous that at low threshold values, the sampled datais not a close approximation of the full data: notenough vocabulary and contextual information ispreserved for the data to be taken as a proxy forthe full data.
However, with t values around 20-60 we recover enough BLEU and OOVs to makethe datasets reasonable proxies.
Further, becauset = Random VSF Ordered VSF1 1:07 2:17 1:562 1:33 2:55 2:395 2:15 4:05 3:4710 2:43 4:49 4:5020 3:23 5:25 5:1440 4:12 6:16 5:5660 4:45 6:41 7:15100 5:31 7:32 7:55150 6:07 8:20 8:18200 6:36 8:31 8:52250 7:30 9:19 9:11All 13:12Table 4: Word alignment times (hh:mm) for dif-ferent thresholds for VSF, VSF after model scoreordering, and a random baselinewe see a relative reduction in data sizes of 32-44%, model size reductions of 27-39%, and per-formance improvements of 41-50% at these t val-ues further argues for the value of VSF at these set-tings.
Even at t=250, we have training data that is54% of the full data size, yet fully recovers BLEU.7 DiscussionVSF is a simple but effective algorithm for reduc-ing the size of parallel training data, and does soindependently of particular dev or test data.
It per-forms as well as related algorithms, notably (Ecket al 2005), but more importantly, it is able toscale to much larger data sets than other algo-rithms.
In this paper, we showed VSF appliedto the EnFrGW corpus.
It should be noted, how-ever, that we have also been testing VSF on muchlarger sets of English-French data.
Two notabletests are one applied to 65.2M English-French sen-tence pairs and another applied to one consistingof 162M.
In the former case, we were able to re-duce the corpus size from 65.2M sentences/1.28Bwords8 to 26.2M sentences/568M words.
TheBLEU score on this test was stable on the threetest sets, as shown in Table 7.
When applied to the162M sentence/2.1B word data set, we were ableto reduce the data size to 40.5M sentences/674Mwords.
In this case, sorting the data using modelscores produced the most desirable results, actu-ally increasing BLEU by 0.90 on WMT09, but,unfortunately, showing a 0.40 drop on WMT10.The fact that VSF runs in one pass is both anasset and a liability.
It is an asset since the algo-rithm is able to operate linearly with respect to thesize the data.
It is a liability since the algorithm is8Word counts based on the English-side, unwordbroken.287WMT09 WMT10 ReqLogt = Random VSF S+VSF Random VSF S+VSF Random VSF S+VSF1 23.76 23.83 23.84 25.69 25.78 25.68 26.34 26.63 26.672 23.91 24.04 24.07 25.76 26.21 26.14 26.54 26.99 26.945 24.05 24.29 24.40 26.10 26.40 26.32 26.79 27.22 27.1210 24.15 24.37 24.45 26.21 26.63 26.32 26.98 27.37 27.6220 24.20 24.40 24.55 26.30 26.46 26.56 27.22 27.38 27.4440 24.37 24.43 24.65 26.40 26.55 26.53 27.30 27.38 27.6260 24.32 24.43 24.64 26.56 26.56 26.61 27.38 27.50 27.64100 24.37 24.49 24.71 26.46 26.75 26.70 27.37 27.52 27.75150 24.37 24.61 24.71 26.67 26.67 26.70 27.48 27.62 27.75200 24.48 24.63 24.69 26.56 26.65 26.78 27.57 27.47 27.72250 24.41 24.57 24.85 26.62 26.74 26.68 27.63 27.45 27.76All 24.37 26.54 27.63Table 5: BLEU Score results for VSF, S+VSF (Sorted VSF), and Random Baseline at different thresholdst.WMT09 WMT10 ReqLogt = Random VSF S+VSF Random VSF S+VSF Random VSF S+VSF1 630 424 450 609 420 445 1299 973 10002 588 374 395 559 385 393 1183 906 9195 520 343 347 492 350 356 1111 856 85310 494 336 335 458 344 344 1092 837 84820 453 335 335 432 339 341 1016 831 83440 423 330 331 403 336 337 986 828 83360 419 329 330 407 333 336 964 831 832100 389 330 329 391 333 335 950 830 830150 397 330 330 384 332 332 930 828 828200 381 328 330 371 331 332 912 827 826250 356 329 328 370 333 331 884 823 823All 325 331 822Table 6: OOV rates for VSF, S+VSF (Sorted VSF), and Random Baseline at different thresholds t.Figure 4: Comparative BLEU scores for two VSF implementations, against a randomly sampled baseline.Figure 5: Comparative OOV rates for two VSF implementations, against a randomly sampled baseline.288ReqLog WMT09 WMT1065.2 snts 32.90 26.77 29.05VSF 26.2M snts 33.34 26.75 29.07Table 7: VSF applied to a 65.2M sentence baselinesystem.sensitive to the order of the data.
The latter leadsto issues of reproducibility: with poorly ordereddata, one could easily arrive at a much less thanoptimal set of data.
However, by adding an addi-tional pass to build model scores, and then rankingthe data by these scores, we address the serious is-sue of reproducibility.
Further, the ranking tendsto arrive at a better selection of data.In an attempt to better understand the behaviorof VSF and how VSF changes the n-gram distribu-tions of vocabulary items in a sample as comparedto the full corpus, we created log2-scale scatterplots, as seen in Figure 6.
In these plots, uni-gram frequencies of unfiltered data (i.e., the fullcorpus, EnFrGW) are on the vertical axis, and un-igram frequencies of the VSF filtered data are onthe horizontal axis.
The three plots show three dif-ferent settings for t. There following observationscan be made about these plots:1.
On the horizontal axis before we reachlog2(t), all data points fall on the x = y line.2.
As the threshold increases the scatter plotgets closer to the x = y line.3.
VSF has the highest impact on the ?medium?frequency unigrams, that is, those with a fre-quency higher than the threshold.The third point speaks the most to the ef-fects that VSF has on data: Very low frequencyitems, specifically those with frequencies belowthe threshold t, are unaffected by the algorithm,since we guarantee including all contexts in whichthey occur.
Low frequency items are at the lowerleft of the plots, and their frequencies follow thex = y line (point 1 above).
Medium frequencyitems, however, specifically those with frequen-cies immediately above t, are the most affectedby the algorithm.
The ?bulge?
in the plots showswhere these medium frequency items begin, andone can see plainly that their distributions are per-turbed.
The ?bulge?
dissipates as frequencies in-crease, until the effects diminish as we approachmuch higher frequencies.
The latter is a conse-quence of a simplifying heuristic applied in VSF(as described in Section 3): t is not a hard ceil-ing, but rather a soft one.
Vocabulary items thatoccur very frequently in a corpus will be countedmany more times than t; for very high frequencyitems, their sampled distributions may approachthose observed in the full corpus, and converge onthe x = y line.
The authors suspect that the BLEUloss that results from the application of VSF is theresult of the perturbed distributions for mediumfrequency items.
Adjusting to higher t values de-creases the degree of the perturbation, as noted inthe second point, which likewise recovers some ofthe BLEU loss observed in lower settings.8 Future WorkThere are several future directions we see withwork on VSF.
Because one threshold t for all vo-cabulary items may be too coarse a setting, we firstplan to explore setting t based on frequency, es-pecially for vocabulary in the most affected mid-range (at and above t).
If we set t based on uni-grams falling into frequency buckets, rather thanone setting for all unigrams, we may arrive ear-lier at a more distributionally balanced corpus, onethat may better match the full corpus.
That said,additional passes over the data come at additionalcost.Second, we plan to explore applying the VSF al-gorithm to higher order n-grams (all experimentsthus far have been on unigrams).
Preliminaryexperiments on bigram VSF, however, show thatwith even the lowest setting (t=1), we already pre-serve well over 50% of the data.In this work we only experimented with sortingthe data based on the normalized combined align-ment score inspired by (Eck et al 2005).
A thirddirection for future work would be to consider or-dering the data based on other feature functionspresented in Eck, e.g., source and target languagemodel, alignment ratio, as well as and featurefunctions introduced in (Taghipour et al 2010),or a combination of all of these feature functions.In the fourth case, we plan to do more sophis-ticated statistical analysis of the effects of VSFon n-gram distributions and phrase-table entropy.We also plan to explore the interactions betweenVSF and data ?diversity?.
For instance, VSF mayhave a greater positive impact on more narrowlyfocused domains than on those that are more gen-erally focused.289(a) VSF t = 10 5 10 15 20 25 30051015202530Unfiltered Token Log FrequencyFiltered Token Log Frequency(b) VSF t = 400 5 10 15 20 25 30051015202530Unfiltered Token Log FrequencyFiltered Token Log Frequency(c) VSF t = 200Figure 6: log2-scale Unigram Frequency scatter plot before VSF versus after VSFReferencesS.
Ananthakrishnan, R. Prasad, D. Stallard, andP.
Natarajan.
2010.
Discriminative sample selectionfor statistical machine translation.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, page 626635.A.
Axelrod, X.
He, and J. Gao.
2011.
Domain adap-tation via pseudo in-domain data selection.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, page 355362.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.W.
Chao and Z. Li.
2011a.
A graph-based bilingualcorpus selection approach for SMT.
In Proceedingsof the 25th Pacific Asia Conference on Language,Information and Computation.WenHan Chao and ZhouJun Li.
2011b.
Improvedgraph-based bilingual corpus selection with sen-tence pair ranking for statistical machine transla-tion.
In 2011 23rd IEEE International Conferenceon Tools with Artificial Intelligence (ICTAI), pages446 ?451, November.Michael Denkowski, Greg Hanneman, and Alon Lavie.2012a.
The CMU-Avenue French-English transla-tion system.
In Proceedings of the NAACL 2012Workshop on Statistical Machine Translation.Michael Denkowski, Greg Hanneman, and Alon Lavie.2012b.
The CMU-Avenue French-English Transla-tion System.
In Proceedings of the NAACL 2012Workshop on Statistical Machine Translation.M.
Eck, S. Vogel, and A. Waibel.
2005.
Lowcost portability for statistical machine translationbased in n-gram frequency and TF-IDF.
In Inter-national Workshop on Spoken Language Translation(IWSLT).Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, page 4957.G.
Haffari, M. Roy, and A. Sarkar.
2009.
Active learn-ing for statistical phrase-based machine translation.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, page 415423.Howard Johnson, Joel D. Martin, George F. Foster, andRoland Kuhn.
2007.
Improving translation qualityby discarding most of the phrasetable.
In Proceed-ings of EMNLP, pages 967?975.William D. Lewis and Chris Quirk.
2013.
Con-trolled Ascent: Imbuing Statistical MT with Lin-guistic Knowledge.
In Proceedings of the SecondHytra (Hybrid Approaches to Translation) Work-shop, Sofia, Bulgaria, August.H.
Lin and J. Bilmes.
2011.
Optimal selection oflimited vocabulary speech corpora.
In Proc.
Inter-speech.Robert C. Moore and William D. Lewis.
2010.
Intel-ligent Selection of Language Model Training Data.In Proceedings of the ACL 2010 Conference ShortPapers, Uppsala, Sweden, July.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st ACL, Sapporo, Japan.T.
Okita.
2009.
Data cleaning for word alignment.
InProceedings of the ACL-IJCNLP 2009 Student Re-search Workshop, page 7280.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency tree translation: Syntactically informedphrasal smt.
In Proceedings of ACL 2005.Spencer Rarrick, Chris Quirk, and William D. Lewis.2011.
MT Detection in Web-Scraped Parallel Cor-pora.
In Proceedings of MT Summit XIII, Xiamen,China, September.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma Erjavec, and Dan Tufi.
2006.290The JRC-Acquis: a multilingual aligned paral-lel corpus with 20+ languages.
In In Proceed-ings of the 5th International Conference on Lan-guage Resources and Evaluation (LREC?2006, page21422147.K.
Taghipour, N. Afhami, S. Khadivi, and S. Shiry.2010.
A discriminative approach to filter out noisysentence pairs from bilingual corpora.
In 20105th International Symposium on Telecommunica-tions (IST), pages 537 ?541, December.K.
Yasuda, R. Zhang, H. Yamamoto, and E. Sumita.2008.
Method of selecting training data to builda compact and efficient translation model.
In Pro-ceedings of the Third International Joint Conferenceon Natural Language Processing, volume 2, page655660.291
