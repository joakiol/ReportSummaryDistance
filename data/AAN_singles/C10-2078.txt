Coling 2010: Poster Volume, pages 683?691,Beijing, August 2010Linguistic Cues for Distinguishing Literal and Non-Literal UsagesLinlin Li and Caroline SporlederDepartment of Computational LinguisticsSaarland University{linlin, csporled}@coli.uni-saarland.deAbstractWe investigate the effectiveness of differ-ent linguistic cues for distinguishing lit-eral and non-literal usages of potentiallyidiomatic expressions.
We focus specif-ically on features that generalize acrossdifferent target expressions.
While id-ioms on the whole are frequent, instancesof each particular expression can be rela-tively infrequent and it will often not befeasible to extract and annotate a suffi-cient number of examples for each expres-sion one might want to disambiguate.
Weexperimented with a number of differentfeatures and found that features encodinglexical cohesion as well as some syntac-tic features can generalize well across id-ioms.1 IntroductionNonliteral expressions are a major challenge inNLP because they are (i) fairly frequent and (ii)often behave idiosyncratically.
Apart from typi-cally being semantically more or less opaque, theycan also disobey grammatical constraints (e.g., byand large, lie in wait).
Hence, idiomatic expres-sions are not only a problem for semantic anal-ysis but can also have a negative effect on otherNLP applications (Sag et al, 2001), such as pars-ing (Baldwin et al, 2004).To process non-literal language correctly, NLPsystems need to recognise such expressions au-tomatically.
While there has been a significantbody of work on idiom (and more generally multi-word expression) detection (see Section 2), un-til recently most approaches have focused ona type-based classification, dividing expressionsinto ?idiomatic?
or ?not idiomatic?
irrespective oftheir actual use in a discourse context.
However,while some expressions, such as by and large, al-ways have a non-compositional, idiomatic mean-ing, many other expressions, such as break the iceor spill the beans, can be used literally as well asidiomatically and for some expressions, such asdrop the ball, the literal usage can even dominatein some domains.
Consequently, those expres-sions have to be disambiguated in context (token-based classification).We investigate how well models for distin-guishing literal and non-literal use can be learnedfrom annotated examples.
We explore differenttypes of features, such as the local and global con-text, syntactic properties of the local context, theform of the expression itself and properties re-lating to the cohesive structure of the discourse.We show that several feature types work well forthis task.
However, some features can generalizeacross specific idioms, for instance features whichcompute how well an idiom ?fits?
its surroundingcontext under a literal or non-literal interpretation.This property is an advantage because such fea-tures are not restricted to training data for a spe-cific target expression but can also benefit fromdata for other idioms.
This is important because,while idioms as a general linguistic class are rela-tively frequent, instances of each particular idiomare much more difficult to find in sufficient num-bers.
The situation is exacerbated by the fact thedistributions of literal vs. non-literal usage tendto be highly skewed, with one usage (often thenon-literal one) being much more frequent thanthe other.
Finding sufficient examples of the mi-nority class can then be difficult, even if instancesare extracted from large corpora.
Furthermore, forhighly skewed distributions, many more majorityclass examples have to be annotated to obtain anacceptable number of minority class instances.We show that it is possible to circumvent thisproblem by employing a generic feature space that683looks at the cohesive ties between the potential id-iom and its surrounding discourse.
Such featuresgeneralize well across different expressions andlead to acceptable performance even on expres-sions unseen in the training set.2 Related WorkUntil recently, most studies on idiom classifi-cation focus on type-based classification; sofarthere are only comparably few studies on token-based classification.
Among the earliest studieson token-based classification were the ones byHashimoto et al (2006) on Japanese and Katzand Giesbrecht (2006) on German.
Hashimoto etal.
(2006) present a rule-based system in whichlexico-syntactic features of different idioms arehard-coded in a lexicon and then used to distin-guish literal and non-literal usages.
The featuresencode information about the passivisation, argu-ment movement, and the ability of the target ex-pression to be negated or modified.
Katz andGiesbrecht (2006) compute meaning vectors forliteral and non-literal examples in the training setand then classify test instances based on the close-ness of their meaning vectors to those of the train-ing examples.
This approach was later extendedby Diab and Krishna (2009), who take a largercontext into account when computing the featurevectors (e.g., the whole paragraph) and who alsoinclude prepositions and determiners in additionto content words.Cook et al (2007) and Fazly et al (2009) take adifferent approach, which crucially relies on theconcept of canonical form (CForm).
It is as-sumed that for each idiom there is a fixed form(or a small set of those) corresponding to the syn-tactic pattern(s) in which the idiom normally oc-curs (Riehemann, 2001).The canonical form al-lows for inflectional variation of the head verb butnot for other variations (such as nominal inflec-tion, choice of determiner etc.).
It has been ob-served that if an expression is used idiomatically,it typically occurs in its canonical form (Riehe-mann, 2001).
Cook et al exploit this behaviourand propose an unsupervised method in which anexpression is classified as idiomatic if it occurs incanonical form and literal otherwise.
Canonicalforms are determined automatically using a statis-tical, frequency-based measure.Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambigua-tion task and use a clustering algorithm whichcompares test instances to two seed sets (one withliteral and one with non-literal expressions), as-signing the label of the closest set.Sporleder and Li (2009) propose another un-supervised method which detects the presence orabsence of cohesive links between the componentwords of the idiom and the surrounding discourse.If such links can be found the expression is clas-sified as literal otherwise as non-literal.
Li andSporleder (2009) later extended this work by com-bining the unsupervised classifier with a second-stage supervised classifier.Hashimoto and Kawahara (2008) present a su-pervised approach to token-based idiom distinc-tion for Japanese, in which they implement severalfeatures, such as features known from other wordsense disambiguation tasks (e.g., collocations)and idiom-specific features taken from Hashimotoet al (2006).
Finally, Boukobza and Rappoport(2009) also experimented with a supervised clas-sifier, which takes into account various surfacefeatures.In the present work, we also investigate super-vised models for token-based idiom detection.
Weare specifically interested in which types of fea-tures (e.g., local context, global context, syntac-tic properties) perform best on this task and morespecifically which features generalize across id-ioms.3 DataWe used the data set created by Sporleder and Li(2009), which consists of 13 English expressions(mainly V+PP or V+NP) that can be used bothliterally and idiomatically, such as break the iceor play with fire.1 To create the data set al in-stances of the target expressions were extractedfrom the Gigaword corpus together with five para-graphs of context and then labelled manually as?literal?
or ?non-literal?.
Overall the data set con-sists of just under 4,000 instances.
For most ex-1We excluded four expressions from the original data setbecause their number of literal examples was very small (<2).684pressions the distribution is heavily skewed to-wards the idiomatic interpretation, however forsome, like drop the ball, the literal reading is morefrequent.
The number of instances varies, rang-ing from 15 for pull the trigger to 903 for dropthe ball.
While the instances were extracted froma news corpus, none of them are domain-specificand all expressions also occur in the BNC, whichis a balanced, multi-domain corpus.To compute the features which we extract inthe next section, all instances in our data setswere part-of-speech tagged by the MXPOST tag-ger (Ratnaparkhi, 1996), parsed with the Malt-Parser2, and named entity tagged with the Stan-ford NE tagger (Finkel et al, 2005).
The lemma-tization was done by RASP (Briscoe and Carroll,2006).4 Indicators of Idiomatic and LiteralUsageIn this study we are particularly interested inwhich linguistic indicators work well for the taskof distinguishing literal and idiomatic languageuse.
The few previous studies have mainly lookedat the lexical context in which and expressionoccurs (Katz and Giesbrecht, 2006; Birke andSarkar, 2006).
However, other properties of thelinguistic context might also be useful.
We dis-tinguish these features into different groups anddiscuss them in the following sections.4.1 Global Lexical Context (glc)That the lexical context might be a good indica-tor for the usage of an expression is obvious whenone looks at examples as in (1) and (2), which sug-gest that literal and non-literal usages of a specificidiom co-occur with different sets of words.
Non-literal uses of break the ice (1), for instance, tendto occur with words like discuss, bilateral or re-lations, while literal usages (2) predictably occurwith, among others, frozen, cold or water.
Whatwe are looking at here is the global lexical contextof an expression, i.e., taking into account previ-ous and following sentences.
We are specificallylooking for words which are either semanticallyrelated (in a wide sense) to the literal or the non-2http://maltparser.org/index.htmlliteral sense of the target expression.
The presenceor absence of such words can be a good indicatorof how the expression is used in a context.
(1) ?Gujral will meet Sharif on Monday and dis-cuss bilateral relations,?
the Press Trust of Indiaadded.
The minister said Sharif and Gujral wouldbe able to ?break the ice?
over Kashmir.
(2) Meanwhile in Germany, the cold penetratedCologne cathedral, where worshippers had tobreak the ice on the frozen holy water in the font.We implemented two sets of features which en-code the global lexical context: salient words andrelated words as described in Li and Sporleder(2009).
The former feature uses a variant oftf.idf to identify words that are particulary salientfor different usages.
The latter feature identifieswords which are most strongly related to the com-ponent words of the idiom.We notice that sometimes several idioms co-occur within the same instance.
This is to say thatnonliteral usages may be indicators of each othersince authors may put them in a same context toconvey a specific opinion (e.g., irony).
Due to this,global lexical context features may also generalizeacross idioms to some extend.4.2 Local Lexical Context (locCont)In addition to the global context, the local lex-ical context, i.e., the words preceding and fol-lowing the target expression, might also provideimportant information.
One obvious local clueare words like literally or metaphorically speak-ing, which when preceding or following an ex-pression might indicate its usage.
Unfortunately,such clues are not only very rare (we only founda handful in nearly 4,000 annotated examples) butalso not always reliable.
For instance, it is notdifficult to find examples like (3) and (4) wherethe word literally is used even though the idiomclearly has a non-literal meaning.
(3) In the documentary the producer literallyspills the beans on the real deal behind the movieproduction.
(4) The new philosophy is blatantly permissive and lit-erally passes the buck to the House?s other com-mittees.685However, there are other local cues.
For exam-ple, we found that the word just before get onesfeet wet tends to indicate non-literal usage as in(5).
Non-literal usage can also be indicated by theoccurrence of the prepositions over or between af-ter break the ice as in (1) and (6).
While suchcues are not perfect they often make one usagemore likely than the other.
Unlike the semanti-cally based global cues, many local clues are morerooted in syntax, i.e., local cues work because spe-cific constructions tend to be more frequent forone or the other usage.
(5) The wiki includes a page of tasks suitable for thosejust getting their feet wet.
(6) Would the visit of the minister help break the icebetween India and Pakistan?Another type of local cues involves selectionalpreferences.
For example, idiomatic usage isprobable if the subject of play with fire is a coun-try as in (7) or if break the ice is followed by awith-PP whose NP refers to a person (8).
(7) Dudayev repeated his frequent warnings that Rus-sia was playing with fire.
(8) Edwards usually manages to break the ice with thetaciturn monarch.Based on those observations, we encode whichwords occur in a ten word window around the tar-get expression, five pre-target words and five post-target words, as the locCont features.4.3 Discourse Cohesion (dc)We implemented two features, related score anddiscourse connectivity, which take into accountthe cohesive structure of an expression in its con-text as described by Li and Sporleder (2009).In addition, we also included the prediction ofthe cohesion graph proposed by Sporleder and Li(2009) as an additional feature.
These featureslook at the lexical cohesion between an expressionand the surrounding discourse, so they are morelikely to generalize across different idioms.4.4 Syntactic Structure (allSyn)To capture syntactic effects, we encoded infor-mation of the head node (heaSyn) of the tar-get expression in the dependency tree (e.g., breakmay:ROOTvisit:SUBthe:NMOD of:NMODminister:PMODthe:NMODbreak:VMODice:OBJthe:NMOD between...Figure 1: Dependency tree for a nonliteral exam-ple of break the ice (The visit of the minister maybreak the ice between India and Pakistan.
)in the dependency tree in Figure 1).
The syn-tactic features we encoded are the parent node(parSyn), sibling nodes (sibSyn) and childrennodes (chiSyn) of the head node.
These nodes in-clude the following type of syntactic information:Dependency Relation of the Verb Phrase Thewhole idiomatic expression used as an object ofa preposition can be an indicative factor of id-iomatic usage (see Example 9).
This property iscaptured by the heaSyn feature.
(9) Ross headed back last week to Washington to briefpresident Bill Clinton on the Hebron talks afterachieving a breakthrough in breaking the ice in theHebron talks by arranging an Arafat-Netanyahusummit .Modal Verbs usually appear in the parent posi-tion of the head verb (parSyn).
Modals can be anindicator of idiomatic usage such as may in Figure1.
In contrast, the modal had to is indicative thatthe same phrase is used literally (Example 10).
(10) Dad had to break the ice on the chicken troughs.Subjects can also provide clues about the usageof an expression, e.g., if selectional preferencesare disobeyed.
For instance, visit as a subject ofthe verb phrase break the ice is an indicator of id-iomatic usage (see Figure 1).
Subjects typicallyappear in the children position of the head verb(chiSyn), but sometimes may appear in the siblingposition (sibSyn) as in Figure 1 .Verb Subcat We also encode the arguments ofthe head verb of the target expression.
These ar-guments can be, for example, additional PPs.
Thisfeature encodes syntactic constraints and attempts686to model selectional restrictions.
The likelihoodof subcategorisation frames may differ for the twousages of an expression, e.g., non-literal expres-sions often tend to have a shorter argument list.For instance, the subcat frame <PP-on, PP-for>intuitively seems more likely for literal usages ofthe expression drop the ball (see Example 11)than for non-literal ones, for which <PP-on> ismore likely (12).
To capture subcategorisation be-haviour, we encode the children nodes of the headnode (chiSyn).
(11) US defender Alexi Lalas twice went close to forc-ing an equaliser , first with a glancing equaliserfrom a Paul Caligiuri free kick and then from aWynalda corner when Prunea dropped the ball [onthe ground] only [for Tibor Selyme to kick fran-tically clear] .
(12) ?Clinton dropped the ball [on this],?
said JohnParachini.Modifiers of the verb can also be indicative ofthe usage of the target expression.
For example,in 13, the fact that the phrase get one?s feet wet ismodified by the adverb just suggest that it is usedidiomatically.
Similar to verb subcat, modifiersare often appear in the children position (chiSyn).
(13) The wiki includes a page of tasks suitable for thosejust getting their feet wet.Coordinated Verb Which verbs are coordi-nated with the target expression, if any, can alsoprovide cues for the intended interpretation.
Forexample, in (14), the fact that break the ice is co-ordinated with fall suggest that it is used literally.The coordinated verb can appear at the sibling po-sition, children position, or some other position ofthe head verb depending on the parser.
The Malt-parser tends to put the coordinated verbs in thechildren position (chiSyn).
(14) They may break the ice and fall through.4.5 Other FeaturesNamed Entities (ne) can also indicate the us-age of an expression.
For instance, a countryname in the subject position of the target expres-sion break the ice is a strong indicator of thisphrase being used idiomatically (see Example 7).Diab and Bhutada (2009) find that NE-featuresperform best.
They used a commercial NE-taggerwith 19 classes.
We used the Stanford NE tag-ger (Finkel et al, 2005), and encoded three namedentity classes (?person?, ?location?, ?organisza-tion?)
in the feature vector.Indicative Terms (iTerm) Some words such asliterally, proverbially are also indicative of literalor idiomatic usages.
We encoded the frequenciesof those indicative terms as features.Scare Quotes (quote) This feature encodeswhether the idiom is marked off by scare quotes,which often indicates non-literal usage (15).
(15) Do consider ?getting your feet wet?
online, usingsome of the technology that is now available to us.5 ExperimentsIn the previous section we discussed different lin-guistic cues for idiom usage.
To determine whichof these cues work best for the task and whichones generalize across different idioms, we car-ried out three experiments.
In the first one (Sec-tion 5.1) we trained one model for each idiom (seeSection 3) and tested the predictiveness of eachfeature type individually as well as all features to-gether.
In the second experiment (Section 5.2), wetrained one generic model for all idioms and deter-mined how the performance of this model differsfrom the idiom-specific models.
Specifically wewanted to know whether the model would bene-fit from the additional training data available bycombining information from several idioms.
Fi-nally (Section 5.3), we tested the generic model onunseen idioms to determine whether these couldbe classified based on generic properties even iftraining data for the target expressions had notbeen seen.5.1 Idiom Specific ModelsThe first question we wanted to answer was howdifficult token-based idiom classification is andwhich of the features we defined in the previoussection work well for this task.
We implementeda specific classifier for each of the idioms in thedata set.
We trained one model for all featuresin combination and one for each individual fea-ture.
Because the data set is not very big we de-cided to run these experiments in 10-fold stratified687cross-validation mode.
We used the SVM classi-fier (SMO) from Weka.3Table 1 shows the results.
We report the pre-cision (Prec.
), recall (Rec.)
and F-Score for theliteral class, as well as the accuracy.
Note that dueto the imbalance in the data set, accuracy is not avery informative measure here; a classifier alwayspredicting the majority class would already obtaina relatively high accuracy.
The literal F-Score ob-tained for individual idioms varies from 38.10%for bite one?s tongue to 96.10% for bounce of thewall.
However, the data sets for the different id-ioms are relatively small and it is impossible tosay whether performance differences on individ-ual idioms are accidental, or due to differencesin training set size or due to some inherent dif-ficulty of the individual idiom.
Thus we chose notto report the performance of our models on indi-vidual idioms but on the whole data set for whichthe numbers are much more reliable.
The finalperformance confusion matrix is the sum over allindividual idiom confusion matrices.Avg.
literal Avg.feature Prec.
Rec.
F-Score Acc.all 89.84 77.06 82.96 93.36glc+dc 90.42 76.44 82.85 93.36allSyn 76.30 86.13 80.92 91.48heaSyn 76.64 85.77 80.95 91.53parSyn 76.43 88.34 81.96 91.84chiSyn 76.49 88.22 81.94 91.84sibSyn 76.27 88.34 81.86 91.78locCont 76.51 88.34 82.00 91.86ne 76.49 88.22 81.94 91.84iTerm 76.51 88.34 82.00 91.86quote 76.51 88.34 82.00 91.86Basemaj 76.71 88.34 82.00 91.86Table 1: Performance of idiom-specific models(averaged over different idioms), 10-fold stratifiedcross-validation.The Baseline (Base) is built based on predict-ing the majority class for each expression.
Thismeans predicting literal for the expressions whichconsist of more literal examples and nonliteral forthe expressions consisting of more nonliteral ex-3http://www.cs.waikato.ac.nz/ml/weka/amples.
We notice the baseline gets a fairly highperformance (Acc.=91.86%).The results show that the expressions can beclassified relatively reliably by the proposed fea-tures.
The performance beats the majority base-line statistically significantly (p = 0.01, ?2 test).We noticed that parSyn, chiSyn, locCont, iTermand quote features are too sparse.
These indi-vidual features cannot guide the classifier.
Asa result, the classifier only predicts the majorityclass which results in a performance similar tothe baseline.
Some of the syntactic features areless sparse and they get different results from thebaseline classifier, however, the performances ofthese features are actually worse than the baseline.This may be due to the relatively small trainingsize in each idiom specific model.
When addingthose features together with statistical-based fea-tures (glc+dc), the performance of the literal classcan be improved slightly.
However, we did not ob-serve any performance increase on the accuracy.5.2 Generic ModelsHaving verified that literal and idiomatic usagescan be distinguished with some success by train-ing expression-specific models, we carried outa second experiment in which we merged thedata sets for different expressions and trained onegeneric model.
We wanted to see whether ageneric model, which has access to more trainingdata, performs better and whether some features,e.g., the cohesion features profit more from this.The experiment was again run in 10-fold stratifiedcross-validation mode (using 10% from each id-iom in the test set in each fold).Table 2 shows the results.
The baseline classi-fier always predict the majority class ?nonliteral?.Note that the result of this baseline is differentfrom the majority baseline in the idiom specificmodel.
In the idiom specific model, there are threeexpressions 4 for which the majority class is ?lit-eral?.Unsurprisingly, the F-Score and accuracy of thecombined feature set drops a bit.
However, theperformance still statistically significantly beatsthe majority baseline classifier (p << 0.01,?2 test).
Similar to previous observation, the4I.e., bounce off the wall, drop the ball, pull the trigger688Avg.
literal Avg.feature Prec.
Rec.
F-Score Acc.all 89.59 65.77 73.22 89.90glc+dc 82.53 60.86 70.06 89.08allSyn 50.83 59.88 54.99 79.42heaSyn 50.57 59.88 54.83 79.29sibSyn 33.33 0.86 1.67 78.83ne 62.45 20.00 30.30 80.69iTerm 40.00 0.25 0.49 78.99Basemaj ?
?
?
79.01Table 2: Performance of the generic model (av-eraged over different idioms), 10-fold stratifiedcross-validation.statistical-based features (glc+dc) work the best,while the syntactic features are also helpful.
How-ever, the local context, iTerm, quote features arevery sparse and, as in the idiom-specific experi-ments, the performances of these features are sim-ilar to the majority baseline classifier.
We ex-cluded them from the Table 2.The numbers show that the syntactic featureshelp more in this model compared with the idiom-specific model.
When including these features, lit-eral F-Score increases by 3.16% while accuracyincreases by 0.9%.
It seems that the syntacticfeatures benefit from the increased training set.This is evidence that these features can generalizeacross idioms.
For instance, the phrase ?The US?on the subject position may be not only indicativeof the idiomatic usage of break the ice, but also ofidiomatic usage of drop the ball.We found that the indicative terms are rare inour corpus.
This is the reason why the recall rateof the indicative terms is very low (0.25%).
Theindicative terms are not very predictive of literal ornon-literal usage, since the precision rate is alsorelatively low (40%), which means those wordscan be used in both literal and nonliteral cases.5.3 Unseen IdiomsIn our final experiment, we tested whether ageneric model can also be applied to completelynew expressions, i.e., expressions for which noinstances have been seen in the data set.
Such abehaviour would be desireable for practical pur-poses as it is unrealistic to label training data foreach idiom the model might possibly encounter ina text.
To test whether the generic model does in-deed generalize to unseen expressions, we test iton all instances of a given expression while train-ing on the rest of the expressions in the dataset.That is, we used a modified cross-validation set-ting, in which each fold contains instances fromone expression in the test set.
Since our datasetcontains 13 expressions, we run a 13-fold crossvalidation.
The final confusion matrix is the sumover each confusion matrix in each round.Avg.
literal Avg.feature Prec.
Rec.
F-Score Acc.all 96.70 81.65 88.54 95.41glc+dc 96.93 77.00 85.83 94.48allSyn 52.54 58.77 55.48 79.52heaSyn 51.35 59.47 55.11 78.96sibSyn 55.56 2.32 4.46 78.38ne 61.89 19.05 29.13 79.87iTerm 66.67 0.7 1.38 78.36Basemaj ?
?
?
79.01Table 3: Performance of the generic model on un-seen idioms (cross validation, instances from eachidiom are chosen as test set for each fold)The results are shown in Table 3.
Similar to thegeneric model, we found that the cohesion fea-tures and syntactic features do generalize acrossexpressions.
Statistical features (glc+dc) performwell in this experiment.
When including morelinguistically orientated features, the performancecan be further increased by almost 1%.
In linewith former observations, the sparse features men-tioned in the former two experiments also do notwork for this experiments.
We also excluded themfrom the table.One interesting finding about this experiment ofthis model is that the F-Score is higher than for the?generic model?.
This is counter-intuitive, sincein the generic model, each idiom in the testing sethas examples in the training set, thus, we mightexpect the performance to be better due to the factthat instances from the same expression appear-ing in the training set are more informative com-pared with instances from different idioms.
Fur-ther analysis revealed that there are some expres-sions for which it may actually be beneficial to689train on other expressions, as the evidence of somefeatures may be misleading.literal F-S. Acc.feature Spe.
Gen. Spe.
Gen.all 86.85 91.79 80.67 88.37glc+dc 86.75 88.84 80.67 84.61allSyn 85.71 71.94 75.28 61.13heaSyn 85.79 71.94 75.39 61.13Table 4: Comparing the performance of the idiomdrop the ball on the idiom specific model (Spe.
)and generic model (Gen.)Table 4 shows the comparison of the perfor-mance of drop the ball on the idiom specificmodel and the generic model on unseen idioms.It can be seen that the statistical features (glc+dc)work better for the model that is trained on the in-stances from other idioms than the model whichis trained on the instances of the target expressionitself.
We found this is due to the fact that drop theball is especially difficult to classify with the dis-course cohesion features (dc).
The literal cases areoften found in a context containing words, suchas fault, mistake, fail, and miss, which are usedto describe a scenario in a baseball game,5 while,on the other hand, those context words are alsoclosely semantically related to the idiomatic read-ing of drop the ball.
This means the classifier canbe mislead by the cohesion features of the literalinstances of this idiom in the training set, sincethey exhibit strong idiomatic cohesive links withthe target expression.
When excluding drop theball from the training set, the cohesive links inthe training data are less noisy.
Thus, the perfor-mance increases.
Unsurprisingly, the performanceof syntactic features works better for the idiomspecific model compared with the unseen idiommodel.6 ConclusionIdioms on the whole are frequent but instances ofeach particular idiom can be relatively infrequent(even for common idioms like ?spill the beans?
).The classes can also be fairly imbalanced, withone class (typically the nonliteral interpretation)5The corpus contains many sports news textbeing much more frequent than the other.
Thiscauses problems for training data generation.
Foridiom specific classifiers, it is difficult to obtainlarge data sets even when extracting from largecorpora and it is even more difficult to find suf-ficient examples of the minority class.
In orderto address this problem, we looked for featureswhich can generalize across idioms.We found that statistical features (glc+dc) workbest for distinguishing literal and nonliteral read-ings.
Certain linguistically motivated features canfurther boost the performance.
However, thoselinguistic features are more likely to suffer fromdata sparseness, as a result, they often only predictthe majority class if used on their own.
We alsofound that some of the features that we designedgeneralize well across idioms.
The cohesion fea-tures have the best generalization ability, whilesyntactic features can also generalize to some ex-tent.AcknowledgmentsThis work was funded by the DFG within theCluster of Excellence MMCI.ReferencesBaldwin, Timothy, Emily M. Bender, Dan Flickinger,Ara Kim, and Stephen Oepen.
2004.
Road-testingthe English resource grammar over the British Na-tional Corpus.
In Proc.
LREC-04, pages 2047?2050.Birke, Julia and Anoop Sarkar.
2006.
A clusteringapproach for the nearly unsupervised recognition ofnonliteral language.
In Proceedings of EACL-06.Boukobza, Ram and Ari Rappoport.
2009.
Multi-word expression identification using sentence sur-face features.
In Proceedings of EMNLP-09.Briscoe, Ted and John Carroll.
2006.
Evaluatingthe accuracy of an unlexicalized statistical parseron the PARC DepBank.
In Proceedings of theCOLING/ACL on Main conference poster sessions,pages 41?48.Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multi-word Expressions.690Diab, Mona and Pravin Bhutada.
2009.
Verb nounconstruction mwe token classification.
In Proceed-ings of the Workshop on Multiword Expressions:Identification, Interpretation, Disambiguation andApplications, pages 17?22.Diab, Mona T. and Madhav Krishna.
2009.
Unsuper-vised classification of verb noun multi-word expres-sion tokens.
In CICLing 2009, pages 98?110.Fazly, Afsaneh, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
Computational Linguistics,35(1):61?103.Finkel, Jenny Rose, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of ACL-05, pages 363?370.Hashimoto, Chikara and Daisuke Kawahara.
2008.Construction of an idiom corpus and its applicationto idiom identification based on WSD incorporatingidiom-specific features.
In Proceedings of EMNLP-08, pages 992?1001.Hashimoto, Chikara, Satoshi Sato, and Takehito Ut-suro.
2006.
Japanese idiom recognition: Drawinga line between literal and idiomatic meanings.
InProceedings of COLING/ACL-06, pages 353?360.Katz, Graham and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the ACL/COLING-06 Workshop onMultiword Expressions: Identifying and ExploitingUnderlying Properties.Li, Linlin and Caroline Sporleder.
2009.
Contextualidiom detection without labelled data.
In Proceed-ings of EMNLP-09.Ratnaparkhi, Adwait.
1996.
A maximum entropypart-of-speech tagger.
In Proceedings of EMNLP-96.Riehemann, Susanne.
2001.
A Constructional Ap-proach to Idioms and Word Formation.
Ph.D. thesis,Stanford University.Sag, Ivan A., Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2001.
Multiwordexpressions: a pain in the neck for NLP.
In LectureNotes in Computer Science.Sporleder, Caroline and Linlin Li.
2009.
Unsuper-vised recognition of literal and non-literal use of id-iomatic expressions.
In Proceedings of EACL-09.691
