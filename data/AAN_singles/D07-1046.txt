Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
439?447, Prague, June 2007. c?2007 Association for Computational LinguisticsMorphological Disambiguation of Hebrew:A Case Study in Classifier CombinationDanny ShachamDepartment of Computer ScienceUniversity of HaifaHaifa, Israeldannysh@gmail.comShuly WintnerDepartment of Computer ScienceUniversity of HaifaHaifa, Israelshuly@cs.haifa.ac.ilAbstractMorphological analysis and disambiguationare crucial stages in a variety of naturallanguage processing applications, especiallywhen languages with complex morphologyare concerned.
We present a system whichdisambiguates the output of a morphologi-cal analyzer for Hebrew.
It consists of sev-eral simple classifiers and a module whichcombines them under linguistically moti-vated constraints.
We investigate a numberof techniques for combining the predictionsof the classifiers.
Our best result, 91.44% ac-curacy, reflects a 25% reduction in error ratecompared with the previous state of the art.1 IntroductionMorphological analysis and disambiguation are cru-cial pre-processing steps for a variety of natural lan-guage processing applications, from search and in-formation extraction to machine translation.
Forlanguages with complex morphology these are non-trivial processes.
This paper presents a morphologi-cal disambiguation module for Hebrew which usesa sophisticated combination of classifiers to rankthe analyses produced by a morphological analyzer.This work has a twofold contribution: first, our sys-tem achieves over 91% accuracy on the full disam-biguation task, reducing the error rate of the pre-vious state of the art by 25%.
More generally, weexplore several ways for combining the predictionsof simple classifiers under constraints; the insightgained from these experiments will be useful forother applications of machine learning to complex(morphological and other) problems.In the remainder of this section we discuss thecomplexity of Hebrew morphology, the challengeof morphological disambiguation and related work.We describe our methodology in Section 2: we usebasic, na?
?ve classifiers (Section 3) to predict somecomponents of the analysis, and then combine themin several ways (Section 4) to predict a consistent re-sult.
We analyze the errors of the system in Section 5and conclude with suggestions for future work.1.1 Linguistic backgroundHebrew morphology is rich and complex.1 The ma-jor word formation machinery is root-and-pattern,and inflectional morphology is highly productiveand consists of prefixes, suffixes and circumfixes.Nouns, adjectives and numerals inflect for number(singular, plural and, in rare cases, also dual) andgender (masculine or feminine).
In addition, allthese three types of nominals have two phonologi-cally and morphologically distinct forms, known asthe absolute and construct states.
In the standardorthography approximately half of the nominals ap-pear to have identical forms in both states, a factwhich substantially increases the ambiguity.
In ad-dition, nominals take possessive pronominal suffixeswhich inflect for number, gender and person.Verbs inflect for number, gender and person (first,second and third) and also for a combination of tenseand aspect/mood, referred to simply as ?tense?
be-low.
Verbs can also take pronominal suffixes, whichare interpreted as direct objects, and in some casescan also take nominative pronominal suffixes.
A pe-culiarity of Hebrew verbs is that the participle form1To facilitate readability we use a straight-forward translit-eration of Hebrew using ASCII characters, where the characters(in Hebrew alphabetic order) are: abgdhwzxviklmnsypcqr$t.439can be used as present tense, but also as a noun or anadjective.These matters are complicated further due to twosources: first, the standard Hebrew orthographyleaves most of the vowels unspecified.
On top ofthat, the script dictates that many particles, includ-ing four of the most frequent prepositions, the def-inite article, the coordinating conjunction and somesubordinating conjunctions, all attach to the wordswhich immediately follow them.
When the definitearticle h is prefixed by one of the prepositions b, kor l, it is assimilated with the preposition and theresulting form becomes ambiguous as to whetheror not it is definite.
For example, bth can be readeither as b+th ?in tea?
or as b+h+th ?in the tea?.Thus, the form $bth can be read as an inflected stem(the verb ?capture?, third person singular femininepast), as $+bth ?that+field?, $+b+th ?that+in+tea?,$+b+h+th ?that in the tea?, $bt+h ?her sitting?
oreven as $+bt+h ?that her daughter?.An added complexity stems from the fact thatthere are two main standards for the Hebrew script:one in which vocalization diacritics, known asniqqud ?dots?, decorate the words, and another inwhich the dots are missing, and other characters rep-resent some, but not all of the vowels.
Most of thetexts in Hebrew are of the latter kind; unfortunately,different authors use different conventions for theundotted script.
Thus, the same word can be writ-ten in more than one way, sometimes even withinthe same document.
This fact adds significantly tothe degree of ambiguity.Our departure point in this work is HAMSAH(Yona and Wintner, 2007), a wide coverage, lin-guistically motivated morphological analyzer of He-brew, which was recently re-implemented in Javaand made available from the Knowledge Cen-ter for Processing Hebrew (http://mila.cs.technion.ac.il/).
The output that HAMSAHproduces for the form $bth is illustrated in Table 1.In general, it includes the part of speech (POS)as well as sub-category, where applicable, alongwith several POS-dependent features such as num-ber, gender, tense, nominal state, definitness, etc.1.2 The challenge of disambiguationIdentifying the correct morphological analysis of agiven word in a given context is an important andnon-trivial task.
Unlike POS tagging, the task doesnot involve assigning an analysis to words which theanalyzer does not recognize.
However, selecting ananalysis immediately induces a POS tagging for thetarget word (by projecting the analysis on the POScoordinate).
Our main contribution in this work is asystem that solves this problem with high accuracy.Compared with POS tagging of English, morpho-logical disambiguation of Hebrew is a much morecomplex endeavor due to the following factors:Segmentation A single token in Hebrew can ac-tually be a sequence of more than one lexi-cal item.
For example, analysis 4 of Table 1($+b+h+th ?that+in+the+tea?)
corresponds tothe tag sequence IN+IN+DT+NN.Large tagset The number of different tags in a lan-guage such as Hebrew (where the POS, mor-phological features and prefix and suffix parti-cles are considered) is huge.
HAMSAH pro-duces 22 different parts of speech, some withsubcategories; 6 values for the number feature(including disjunctions of values), 4 for gender,5 for person, 7 for tense and 3 for nominal state.Possessive pronominal suffixes can have 15 dif-ferent values, and prefix particle sequences cantheoretically have hundreds of different forms.While not all the combinations of these valuesare possible, we estimate the number of possi-ble analyses to be in the thousands.Ambiguity Hebrew is highly ambiguous: HAM-SAH outputs on average approximately 2.64analyses per word token.
Oftentimes two ormore alternative analyses share the same partof speech, and in some cases two or more anal-yses are completely identical, except for theirlexeme (see analyses 7 and 8 in Table 1).
Mor-phological disambiguation of Hebrew is hencecloser to the problem of word sense disam-biguation than to standard POS tagging.Anchors, which are often function words, are al-most always morphologically ambiguous inHebrew.
These include most of the high-frequency forms.
Many of the function wordswhich help boost the performance of EnglishPOS tagging are actually prefix particles whichadd to the ambiguity in Hebrew.440# Lexical ID lexeme POS Num Gen Per Ten Stat Def Pref Suf1 17280 $bt noun sing fem N/A N/A abs no h2 1379 bt noun sing fem N/A N/A abs no $ h3 19130 bth noun sing fem N/A N/A abs no $4 19804 th noun sing masc N/A N/A abs yes $+b+h5 19804 th noun sing masc N/A N/A abs no $+b6 19804 th noun sing masc N/A N/A cons no $+b7 1541 $bh verb sing fem 3 past N/A N/A8 9430 $bt verb sing fem 3 past N/A N/ATable 1: The analyses of the form $bthWord order in Hebrew is freer than in English.1.3 Related workThe idea of using short context for morphologicaldisambiguation dates back to Choueka and Lusig-nan (1985).
Levinger et al (1995) were the firstto apply it to Hebrew, but their work was ham-pered by the lack of annotated corpora for trainingand evaluation.
The first work which uses stochas-tic contextual information for morphological disam-biguation in Hebrew is Segal (1999): texts are an-alyzed using the morphological analyzer of Segal(1997); then, each word in a text is assigned itsmost likely analysis, defined by probabilities com-puted from a small tagged corpus.
In the next phasethe system corrects its own decisions by using shortcontext (one word to the left and one to the rightof the target word).
The corrections are also au-tomatically learned from the tagged corpus (usingtransformation-based learning).
In the last phase,the analysis is corrected by the results of a syntac-tic analysis of the sentence.
The reported resultsare excellent: 96.2% accuracy.
More reliable tests,however, reveal accuracy of 85.5% only (Lember-ski, 2003, page 85).
Furthermore, the performanceof the program is unacceptable (the reported runningtime on ?two papers?
is thirty minutes).Bar-Haim et al (2005) use Hidden Markov Mod-els (HMMs) to implement a segmenter and a tag-ger for Hebrew.
The main innovation of this work isthat it models word-segments (morphemes: prefixes,stem and suffixes), rather than full words.
The accu-racy of this system is 90.51% for POS tagging (atagset of 21 POS tags is used) and 96.74% for seg-mentation (which is defined as identifying all pre-fixes, including a possibly assimilated definite arti-cle).
As noted above, POS tagging does not amountto full morphological disambiguation.Recently, Adler and Elhadad (2006) presented anunsupervised, HMM-based model for Hebrew mor-phological disambiguation, using a morphologicalanalyzer as the only resource.
A morpheme-basedmodel learns both segmentation and tagging in par-allel from a large (6M words) un-annotated corpus.Reported results are 92.32% for POS tagging and88.5% for full morphological disambiguation.
Werefer to this result as the state of the art and use thesame data for evaluation.A supervised approach to morphological disam-biguation of Arabic is given by Habash and Rambow(2005), who use two corpora of 120K words eachto train several classifiers.
Each morphological fea-ture is predicted separately and then combined into afull disambiguation result.
The accuracy of the dis-ambiguator is 94.8%-96.2% (depending on the testcorpus).
Note, however, the high baseline of eachclassifier (96.6%-99.9%, depending on the classi-fier) and the full disambiguation task (87.3%-92.1%,depending on the corpus).
We use a very similar ap-proach below, but we experiment with more sophis-ticated methods for combining simple classifiers toinduce a coherent prediction.2 MethodologyFor training and evaluation, we use a corpus ofapproximately 90,000 word tokens, consisting ofnewspaper texts, which was automatically analyzedusing HAMSAH and then manually annotated (El-hadad et al, 2005).
Annotation consists simply ofselecting the correct analysis produced by the an-alyzer, or an indication that no such analysis ex-441ists.
When the analyzer does not produce the cor-rect analysis, it is added manually.
This is the exactsetup of the experiments reported by Adler and El-hadad (2006).Table 2 lists some statistics of the corpus, and ahistogram of analyses is given in Table 3.
Table 4lists the distribution of POS in the corpus.Tokens 89347Types 23947Tokens with no correct analysis 8218Tokens with no analysis 130Degree of ambiguity 2.64Table 2: Statistics of training corpus# analyses # tokens # analyses # tokens1 38468 7 19772 15480 8 13093 11194 9 7854 9934 10 6225 5341 11 2386 3472 >12 397Table 3: Histogram of analysesIn all the experiments described in this paper weuse SNoW (Roth, 1998) as the learning environ-ment, with winnow as the update rule (using per-ceptron yielded very similar results).
SNoW is amulti-class classifier that is specifically tailored forlearning in domains in which the potential numberof information sources (features) taking part in de-cisions is very large, of which NLP is a principalexample.
It works by learning a sparse network oflinear functions over the feature space.
SNoW hasalready been used successfully as the learning vehi-cle in a large collection of natural language relatedtasks and compared favorably with other classifiers(Punyakanok and Roth, 2001; Florian, 2002).
Typi-cally, SNoW is used as a classifier, and predicts us-ing a winner-take-all mechanism over the activationvalues of the target classes.
However, in addition tothe prediction, it provides a reliable confidence levelin the prediction, which enables its use in an infer-ence algorithm that combines predictors to producea coherent inference.Following Daya et al (2004) and Habash andPOS # tokens % tokensNoun 25836 28.92Punctuation 13793 15.44Proper Noun 7238 8.10Verb 7192 8.05Preposition 7164 8.02Adjective 5855 6.55Participle 3213 3.60Pronoun 2688 3.01Adverb 2226 2.49Conjunction 2021 2.26Numeral 1972 2.21Quantifier 951 1.06Negation 848 0.95Interrogative 80 0.09Prefix 29 0.03Interjection 12 0.01Foreign 6 0.01Modal 5 0.01Table 4: POS frequenciesRambow (2005), we approach the problem of mor-phological disambiguation as a complex classifica-tion task.
We train a classifier for each of the at-tributes that can contribute to the disambiguationof the analyses produced by HAMSAH (e.g., POS,tense, state).
Each classifier predicts a small set ofpossible values and hence can be highly accurate.In particular, the basic classifiers do not suffer fromproblems of data sparseness.
Of course, each sim-ple classifier cannot fully disambiguate the outputof HAMSAH, but it does induce a ranking on theanalyses (see Table 6 below for the level of ambigu-ity which remains after each simple classifier is ap-plied).
Then, we combine the outcomes of the sim-ple classifiers to produce a consistent ranking whichinduces a linear order on the analyses.For evaluation we consider only the words thathave at least one correct analysis in the annotatedcorpus.
Accuracy is defined as the ratio between thenumber of words classified correctly and the totalnumber of words in the test corpus that have a cor-rect analysis.
The remaining level of ambiguity isdefined as the average number of analyses per wordwhose score is equal to the score of the top rankedanalysis.
This is greater than 1 only for the simple442classifiers, where more than one analysis can havethe same tag.
In all the experiments we perform 10-fold cross-validation runs and report the average ofthe 10 runs, both on the entire corpus and on a subsetof the corpus in which we only test on words whichdo not occur in the training corpus.The baseline tag of the token wi is the mostprominent tag of all the occurrences of wi in thecorpus.
The baseline for the combination is the mostprominent analysis of all the occurrences ofwi in thecorpus.
If wi does not occur in the corpus, we backoff and select the most prominent tag in the corpusindependently of the word wi.
For the combinationbaseline, we select the analysis of the most promi-nent lexical ID, chosen from the list of all possiblelexical IDs of wi.
If there is more than one possiblevalue, one top-ranking value is chosen at random.3 Basic ClassifiersThe simple classifiers are all built in the same way.They are trained on feature vectors that are gener-ated from the output of the morphological analyzer,and tested on a clean output of the same analyzer.We defined several classifiers for the attributes ofthe morphological analyses.
Since some attributesdo not apply to all the analyses, we add a value of?N/A?
for the inapplicable attributes.
An annotatedcorpus was needed in all those classifiers for train-ing.
We list the basic classifiers below.POS 22 values (only 18 in our corpus), see Table 4.Gender ?Masculine?, ?Feminine?, ?Masculine andfeminine?, ?N/A?.Number ?Singular?, ?Plural?, ?Dual?, ?N/A?.Person ?First?, ?Second?, ?Third?, ?N/A?.Tense ?Past?, ?Present?, ?Participle?, ?Future?, ?Im-perative?, ?Infinitive?, ?Bare Infinitive?, ?N/A?.Definite Article ?Def?, ?indef?, ?N/A?.
Identifiesalso implicit (assimilated) definiteness.Status ?Absolute?, ?Construct?
and ?N/A?.Segmentation Predicts the number of letters whichare prefix particles.
Possible values are [0-6], 6being the length of longest possible prefix se-quence.
Does not identify implicit definiteness.Has properties A binary classifier which distin-guishes between atomic POS categories (e.g.,conjunction or negation) and categories whosewords have attributes (such as nouns or verbs).Each word in the training corpus induces featuresthat are generated for itself and its immediate neigh-bors, using the output of the morphological ana-lyzer.
For each word in the window, we generatethe following features: POS, number, gender, per-son, tense, state, definiteness, prefixes (where eachpossible prefix is a binary feature), suffix (binary: isthere word suffixed?
), number/gender/person of suf-fix, surface form, lemma, conjunction of the surfaceform and the POS, conjunction of the POS and thePOS of prefixes and suffixes, and some disjunctionsof POS.
The total number of features for each exam-ple is huge (millions), but feature vectors are verysparse.The simple classifiers can be configured in severalways.
First, the size of the window around the targetword had to be determined, and we experimentedwith several sizes, up to ?3 words.
Another issueis feature generation.
It is straight-forward duringtraining, but during evaluation and testing the fea-ture extractor is presented only with the set of anal-yses produced by HAMSAH for each word, and hasno access to the correct analysis.
We experimentedwith two methods for tackling this problem: producethe union of all possible values for each feature; orselect a single analysis, the baseline one, for eachword, and generate only the features induced by thisanalysis.
While this problem is manifested only dur-ing testing, it impacts also the training procedure,and so we experimented with feature generation attraining using the correct analysis, the union of theanalyses or the baseline analysis.
The results of theexperiments for the POS classifier are shown in Ta-ble 5.
The best configuration uses a window of twowords before and one word after the target word.
Forboth testing and training we generate features usingthe baseline analysis.With this setup, the accuracy of all the classifiersis shown in Table 6.
We report results on two tasks:the entire test corpus; and words in the test corpuswhich do not occur in the training corpus, a muchharder task.
We list the accuracy, remaining levelof ambiguity and reduction in error rate ERR, com-443Training Testing 1 - 2 2 - 1 2 - 2 1 - 3 3 - 1 2 - 3 3 - 2 3 - 3correct baseline 91.37 91.53 91.69 91.55 91.69 91.83 91.75 92.01correct all 79.15 79.55 80.53 80.07 80.13 80.75 81.00 82.07all baseline 93.41 93.38 93.22 93.42 93.53 93.59 93.51 93.61all all 93.37 93.42 93.28 93.2 93.61 93.05 93.48 93.15baseline baseline 94.93 94.97 94.8 94.86 94.84 94.72 94.67 94.61baseline all 84.48 84.78 84.82 85.65 84.97 85.13 85.03 85.45Table 5: Architectural configurations of the POS classifier: columns reflect the window size, rows refer totraining and testing feature generationAll words Unseen wordsbaseline classifier baseline classifieraccuracy accuracy ambiguity ERR accuracy accuracy ERRPOS 93.01 94.97 1.46 28.04 84.67 88.65 25.96Gender 96.34 96.74 1.86 10.93 92.15 94.38 28.41Number 96.79 97.92 1.91 35.20 92.35 95.91 46.54Person 98.14 98.62 2.25 25.81 94.04 96.50 41.28Tense 98.40 98.69 2.21 18.12 94.80 96.37 30.19Definite Article 93.90 95.76 1.83 30.49 85.38 91.77 43.71Status 92.73 95.06 1.57 32.05 84.46 89.85 34.68Segmentation 99.12 97.80 2.25 ?
97.67 97.66 ?Has properties 97.63 98.11 2.26 20.25 95.91 95.97 1.47Table 6: Accuracy of the simple classifiers: ERR is reduction in error rate, compared with the baselinepared with the baseline.4 Combination of ClassifiersGiven a set of simple classifiers, we now investi-gate various ways for combining their predictions.These predictions may be contradicting (for exam-ple, the POS classifier can predict ?noun?
while thetense classifier predicts ?past?
), and we use the con-straints imposed by the morphological analyzer toenforce a consistent analysis.First, we define a na?
?ve combination along thelines of Habash and Rambow (2005).
The scoresassigned by the simple classifiers (except segmenta-tion, for which we use the baseline) to each analysisare accumulated, and the score of the complete anal-ysis is their sum (experiments with different weightsto the various classifiers proved futile).
Even afterthe combination, the remaining level of ambiguityis 1.05; in ambiguous cases back off to the baselineanalysis, and then choose at random one of the top-ranking analyses.
The result of the combination isshown in Table 7.baseline classifier ERRAll words 86.11 90.26 29.88Unseen words 67.53 78.52 33.85Table 7: Results of the na?
?ve combinationNext, we define a hierarchical combination inwhich we try to incorporate more linguistic knowl-edge pertaining to the dependencies between theclassifiers.
As a pre-processing step we classify thetarget word to one of two groups, using the has prop-erties classifier.
Then, we predict the main POS ofthe target word, and take this prediction to be true;we then apply only the subset of the other classifiersthat are relevant to the main POS.The results of the hierarchical combination areshown in Table 8.
As can be seen, the hierarchicalcombination performs worse than the na?
?ve one.
Weconjecture that this is because the hierarchical com-bination does not fully disambiguate, and a randomtop-ranking analysis is chosen more often than in thecase of the na?
?ve combination.444na?
?ve hierarchical ERRAll words 90.26 89.61 ?Unseen words 78.52 78.08 ?Table 8: Results of the hierarchial combinationThe combination of independent classifiers un-der the constraints imposed by the possible mor-phological analyses is intended to capture context-dependent constraints on possible sequences of anal-yses.
Such constraints are stochastic in nature, butlinguistic theory tells us that several hard (determin-istic) constraints also exist which rule out certain se-quences of otherwise possible analyses.
We now ex-plore the utility of implementing such constraints tofilter out linguistically impossible sequences.Using several linguistic sources, we defined a setof constraints, each of which is a linguistically im-possible sequence of analyses (all sequences are oflength 2, although in principle longer ones couldhave been defined).
We then checked the annotatedcorpus for violations of these constraints; we usedthe corpus to either verify the correctness of a con-straint or further refine it (or abandon it altogether,in some cases).
We then re-iterated the process withthe new set of constraints.The result was a small set of six constraints whichare not violated in our annotated corpus.
We usedthe constraints to rule out some of the paths de-fined by the possible outcomes of the morphologi-cal analyzer on a sequence of words.
Each of theconstraints below contributes a non-zero reductionin the error rate of the disambiguation module.The(slightly simplified) constraints are:1.
A verb in any tense but present cannot be fol-lowed by the genitive preposition ?$l?
(of).2.
A preposition with no attached pronomial suf-fix must be followed by a nominal phrase.
Thisrule is relaxed for some prepositions which canbe followed by the prefix ?$?.3.
The preposition ?at?
must be followed by a def-inite nominal phrase.4.
Construct-state words must be followed by anominal phrase.5.
A sequence of two verbs is only allowed if: oneof them is the verb ?hih?
(be); one of them hasa prefix; the second is infinitival; or the first isimperative and the second is in future tense.6.
A non-numeral quantifier must be followed byeither a nominal phrase or a punctuation.Imposing the linguistically motivated constraintson the classifier combination improved the results tosome extent, as depicted in Table 9.
The best resultsare obtained when the constraints are applied to thehierarchical combination.5 Error analysisWe conducted extensive error analysis of both thesimple classifiers and the combination module.
Theanalysis was performed over one fold of the anno-tated corpus (8933 tokens).
Table 10 depicts, forsome classifiers, a subset of the confusion matrix:it lists the correct tag, the chosen, or predicted, tag,the number of occurrences of the specific error andthe total number of errors made by the classifier.classifier correct chosen # totalhas props yes no 110 167no yes 57segmentation 1 0 160 176state const abs 154 412definiteness def indef 98 300Table 10: Simple classifiers, confusion matrixSeveral patterns can be observed in Table 10.
The?has properties?
classifier is biased towards predict-ing ?yes?
instead of ?no?.
The ?segmentation?
clas-sifier, which predicts the length of the prefix, alsodisplays a clear bias.
In almost 90% of its errors itpredicts no prefix instead of a prefix of length one.?Status?
and ?definiteness?
are among the weakestclassifiers, biased towards the default.Other classifiers make more sporadic types of er-rors.
Of particular interest is the POS classifier.Here, when adjectives are mis-predicted, they arepredicted as nouns.
This can be explained by themorphological similarity of the two categories, andin particular by the similar syntactic contexts inwhich they occur.
Similarly, almost 90% of mis-predicted verbs are predicted to be either nouns445na?
?ve na?
?ve + consts ERR hier.
hier.
+ cons ERRAll words 90.26 90.90 6.57 89.61 91.44 17.61Unseen words 78.52 79.56 4.84 78.08 81.74 16.70Table 9: Accuracy results of various combination architectures.
ERR is reduction in error rate due to thehard constraints.
The best results are obtained using the hierarchical combination with hard constraints.or adjectives, probably resulting from present-tenseverbs in the training corpus which, in Hebrew, havesimilar distribution to nouns and adjectives.The analysis of errors in the combination is moreinteresting.
On the entire corpus, the disambigua-tor makes 7927 errors.
Of those, 1476 (19%) areerrors in which the correct analysis differs from thechosen one only in the value of the ?state?
feature.Furthermore, in 1341 of the errors (17%) the systempicks the correct analysis up to the value of ?definite-ness?
; of those, 1275 (16% of the errors) are wordsin which the definite article is assimilated in a prepo-sition.
In sum, many of the errors seem to be in thereal tough cases.6 ConclusionsMorphological disambiguation of Hebrew is a dif-ficult task which involves, in theory, thousands ofpossible tags.
We reconfirm the results of Dayaet al (2004) and Habash and Rambow (2005),which show that decoupling complex morphologi-cal tasks into several simple tasks improves the ac-curacy of classification.
Our best result, 91.44%accuracy, reflects a reduction of 25% in error ratecompared to the previous state of the art (Adlerand Elhadad, 2006), and almost 40% comparedto the baseline.
We also show that imposingfew context-dependent constraints on possible se-quences of analyses improves the accuracy of thedisambiguation.
The disambiguation module willbe made available through the Knowledge Cen-ter for Processing Hebrew (http://mila.cs.technion.ac.il/).We believe that these results can be further im-proved in various ways.
The basic classifiers canbenefit from more detailed feature engineering andcareful tuning of the parameters of the learning en-vironment.
There are various ways in which inter-related classifiers can be combined; we only ex-plored three here.
Using other techniques, such asinference-based training, in which the feature gen-eration for training is done step by step, using infor-mation inferred in the previous step, is likely to yieldbetter accuracy.
We also believe that further linguis-tic exploration, based on deeper error analysis, willresult in more hard constraints which can reduce theerror rate of the combination module.
Finally, weare puzzled by the differences between Hebrew andArabic (for which the baseline and the current stateof the art are significantly higher) on this task.
Weintend to investigate the linguistic sources for thispuzzle in the future.AcknowledgementsWe are extremely grateful to Dan Roth for his con-tinuing support and advise; to Meni Adler for pro-viding the annotated corpus; to Dalia Bojan andAlon Itai for the implementation of the morpholog-ical analyzer; to Yariv Louck for the implementa-tion of the deterministic constraints; to Nurit Melnikfor help with error analysis; and to Yuval Nardi hishelp with statistical analysis.
Thanks are due to IdoDagan, Alon Lavie and Michael Elhadad for usefulcomments and advise.
This research was supportedby THE ISRAEL SCIENCE FOUNDATION (grantNo.
137/06); by the Israel Internet Association; bythe Knowledge Center for Processing Hebrew; andby the Caesarea Rothschild Institute for Interdisci-plinary Application of Computer Science at the Uni-versity of Haifa.ReferencesMeni Adler and Michael Elhadad.
2006.
An unsuper-vised morpheme-based hmm for hebrew morpholog-ical disambiguation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 665?672, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2005.Choosing an optimal architecture for segmentation and446POS-tagging of Modern Hebrew.
In Proceedings ofthe ACL Workshop on Computational Approaches toSemitic Languages, pages 39?46, Ann Arbor, Michi-gan, June.
Association for Computational Linguistics.Yaacov Choueka and Serge Lusignan.
1985.
Disam-biguation by short context.
Computers and the Hu-manities, 19:147?157.Ezra Daya, Dan Roth, and Shuly Wintner.
2004.
Learn-ing Hebrew roots: Machine learning with linguisticconstraints.
In Proceedings of EMNLP?04, pages 357?364, Barcelona, Spain, July.Michael Elhadad, Yael Netzer, David Gabay, and MeniAdler.
2005.
Hebrew morphological tagging guide-lines.
Technical report, Department of Computer Sci-ence, Ben Gurion University.Radu Florian.
2002.
Named entity recognition as ahouse of cards: Classifier stacking.
In Proceedingsof CoNLL-2002, pages 175?178.
Taiwan.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Gennadiy Lemberski.
2003.
Named entity recognitionin Hebrew.
Master?s thesis, Department of ComputerScience, Ben Gurion University, Beer Sheva, Israel,March.
In Hebrew.Moshe Levinger, Uzzi Ornan, and Alon Itai.
1995.Learning morpho-lexical probabilities from an un-tagged corpus with an application to Hebrew.
Com-putational Linguistics, 21(3):383?404, September.Vasin Punyakanok and Dan Roth.
2001.
The use of clas-sifiers in sequential inference.
In NIPS-13; The 2000Conference on Advances in Neural Information Pro-cessing Systems 13, pages 995?1001.
MIT Press.Dan Roth.
1998.
Learning to resolve natural languageambiguities: A unified approach.
In Proceedings ofAAAI-98 and IAAI-98, pages 806?813, Madison, Wis-consin.Erel Segal.
1997.
Morphological analyzer for unvocal-ized Hebrew words.
Unpublished work.Erel Segal.
1999.
Hebrew morphological analyzer forHebrew undotted texts.
Master?s thesis, Technion, Is-rael Institute of Technology, Haifa, October.
In He-brew.Shlomo Yona and Shuly Wintner.
2007.
A finite-statemorphological grammar of Hebrew.
Natural Lan-guage Engineering.
To appear.447
