Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 60?68,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsMorphology Induction from Limited Noisy DataUsing Approximate String MatchingBurcu Karagol-Ayan, David Doermann, and Amy WeinbergInstitute for Advanced Computer Studies (UMIACS)University of MarylandCollege Park, MD 20742{burcu,doermann,weinberg}@umiacs.umd.eduAbstractFor a language with limited resources, adictionary may be one of the few availableelectronic resources.
To make effectiveuse of the dictionary for translation, how-ever, users must be able to access it us-ing the root form of morphologically de-formed variant found in the text.
Stem-ming and data driven methods, however,are not suitable when data is sparse.
Wepresent algorithms for discovering mor-phemes from limited, noisy data obtainedby scanning a hard copy dictionary.
Ourapproach is based on the novel applica-tion of the longest common substring andstring edit distance metrics.
Results showthat these algorithms can in fact segmentwords into roots and affixes from the lim-ited data contained in a dictionary, and ex-tract affixes.
This in turn allows non na-tive speakers to perform multilingual tasksfor applications where response must berapid, and their knowledge is limited.
Inaddition, this analysis can feed other NLPtools requiring lexicons.1 IntroductionIn order to develop morphological analyzers for lan-guages that have limited resources (either in terms ofexperienced linguists, or electronic data), we mustmove beyond data intensive methods developed forrich resource languages that rely on large amountsof data for statistical methods.
New approaches thatcan deal with limited, and perhaps noisy, data arenecessary for these languages.Printed dictionaries often exist for languages be-fore large amounts of electronic text, and providea variety of information in a structured format.
Inthis paper, we propose Morphology Induction fromNoisy Data (MIND), a natural language morphologyinduction framework that operates on from informa-tion in dictionaries, specifically headwords and ex-amples of usage.
We use string searching algorithmsto morphologically segment words and identify pre-fixes, suffixes, circumfixes, and infixes in noisy andlimited data.
We present our preliminary results ontwo data sources (Cebuano and Turkish), give a de-tailed analysis of results, and compare them to astate-of-the-art morphology learner.
We employ theautomatically induced affixes in a simple word seg-mentation process, decreasing the error rate of in-correctly segmented words by 35.41%.The next section discusses prior work on mor-phology learning.
In Section 3 and 4, we describeour approach and MIND framework in detail.
Sec-tion 6 explains the experiments and presents results.We conclude with future work.2 Related WorkMuch of the previous work on morphology learninghas been reported on automatically acquiring affixlists.
Inspired by works of Harris (1955), Dejean(1998) attempted to find a list of frequent affixesfor several languages.
He used successor and pre-decessor frequencies of letters in a given sequenceof letters in identifying possible morpheme bound-60aries.
The morpheme boundaries are where the pre-dictability of the next letter in the letter sequence isthe lowest.Several researchers (Brent, 1993; Brent et al,1995; Goldsmith, 2001) used Minimum DescriptionLength (MDL) for morphology learning.
Snoverand Brent (2001) proposed a generative probabil-ity model to identify stems and suffixes.
Schoneand Jurafsky (2001) used latent semantic analysisto find affixes.
Baroni et al (2002) produced aranked list of morphologically related pairs froma corpus using orthographic or semantic similaritywith minimum edit distance and mutual informa-tion metrics.
Creutz and Lagus (2002) proposedtwo unsupervised methods for word segmentation,one based on maximum description length, and onebased on maximum likelihood.
In their model,words consisted of lengthy sequences of segmentsand there is no distinction between stems and af-fixes.
The Whole Word Morphologizer (Neuvel andFulop, 2002) uses a POS-tagged lexicon as input, in-duces morphological relationships without attempt-ing to discover or identify morphemes.
It is also ca-pable of generating new words beyond the learningsample.Mystem (Segalovich, 2003) uses a dictionary forunknown word guessing in a morphological analysisalgorithm for web search engines.
Using a very sim-ple idea of morphological similarity, unknown wordmorphology is taken from all the closest words inthe dictionary, where the closeness is the number ofletters on its end.The WordFrame model (Wicentowski, 2004) usesinflection-root pairs, where unseen inflections aretransformed into their corresponding root forms.The model works with imperfect data, and can han-dle prefixes, suffixes, stem-internal vowel shifts, andpoint-of-affixation stem changes.
The WordFramemodel can be used for co-training with low-accuracyunsupervised algorithms.Monson (2004) concentrated on languages withlimited resources.
The proposed language-independent framework used a corpus of full wordforms.
Candidate suffixes are grouped into candi-date inflection classes, which are then arranged in alattice structure.A recent work (Goldsmith et al, 2005) proposedto use string edit distance algorithm as a bootstrap-ping heuristic to analyze languages with rich mor-phologies.
String edit distance is used for rank-ing and quantifying the robustness of morphologicalgeneralizations in a set of clean data.All these methods require clean and most of thetime large amounts of data, which may not existfor languages with limited electronic resources.
Forsuch languages, the morphology induction is still aproblem.
The work in this paper is applicable tonoisy and limited data.
String searching algorithmsare used with information found in dictionaries toextract the affixes.3 ApproachDictionary entries contain headwords, and the exam-ples of how these words are used in context (i.e.
ex-amples of usage).
Our algorithm assumes that eachexample of usage will contain at least one instanceof the headword, either in its root form, or as oneof its morphological variants.
For each headword?example of usage pair, we find the headword occur-rence in the example of usage, and extract the affixif the headword is in one of its morphological vari-ants.
We should note that we do not require the datato be perfect.
It may have noise such as OCR errors,and our approach successfully identifies the affixesin such noisy data.4 FrameworkOur framework has two stages, exact match and ap-proximate match, and uses three string distance met-rics, the longest common substring (LCS), approx-imate string matching with k differences (k-DIFF),and string edit distance (SED).
We differentiate be-tween exact and approximate matches and assigntwo counts for each identified affix, exact countand approximate count.
We require that each affixshould have a positive exact count in order to be inthe final affix list.
Although approximate match canbe used to find exact matches to identify prefixes,suffixes, and circumfixes, it is not possible to differ-entiate between infixes and OCR errors.
For thesereasons, we process the two cases separately.First we briefly describe the three metrics we useand the adaptations we made to find the edit opera-tions in SED, and then we explain how we use thesemetrics in our framework.614.1 String Searching AlgorithmsLongest Common Substring (LCS) Given twostrings p = p1...pn and q = q1...qm, LCS finds thelongest contiguous sequence appearing in p and q.The longest common substring is not same as thelongest common subsequence because the longestcommon subsequence need not be contiguous.There is a dynamic programming solution forLCS1 that finds the longest common substring fortwo strings with length n and m in O(nm).String Edit Distance (SED) Given two strings pand q, SED is the minimum number of edit opera-tions which transforms p to q.
The edit operations al-lowed are insertions, deletions, and substitutions.
Inour algorithm, we set the cost of each edit operationto 1.
A solution based on dynamic programmingcomputes the distance between strings in O(mn),where m and n are the lengths of the strings (Wag-ner and Fischer, 1974).Approximate string matching with k differ-ences (k-DIFF) Given two strings p and q, the prob-lem of approximate string matching with k differ-ences is finding all the substrings of q which areat a distance less than or equal to a given value kfrom p. Insertions, deletions and substitutions areall allowed.
A dynamic programming solution tothis problem is the same as the classical string editdistance solution with one difference: the values ofthe first row of the table are initialized to 0 (Sellers,1980).
This initialization means that the cost of in-sertions of letters of q at the beginning of p is zero.The solutions are all the values of the last row of ta-ble which are less or equal to k. Consequently, theminimum value on the last row gives us the distanceof the closest occurrence of the pattern.String Edit Distance with Edit Operations(SED-path) In our framework, we are also inter-ested in tracing back the editing operations per-formed in achieving the minimum cost alignment.In order to obtain the sequence of edit operations,we can work backwards from the complete distancematrix.
For two strings p and q with lengths n andm respectively, the cell L[n,m] of the distance ma-trix L gives us the SED between p and q.
To getto the cell L[n,m], we had to come from one of 1)L[n ?
1,m] (insertion), 2) L[n,m ?
1] (deletion),1http://www.ics.uci.edu/ dan/class/161/notes/6/Dynamic.htmlor 3) L[n ?
1,m ?
1] (substitution).
Which of thethree options was chosen can be reconstructed giventhese costs, edit operation costs, and the charactersp[n], q[m] of the strings.
By working backwards,we can trace the entire path and thus reconstruct thealignment.
However, there are ambiguous cases; thesame minimum cost may be obtained by a numberof edit operation sequences.
We adapted the trace ofthe path for our purposes as explained below.Let path be the list of editing operations to obtainminimum distance, and SED-path be the SED algo-rithm that also returns a path.
The length of the pathis max(n,m), and path[j] contains the edit oper-ation to change q[j] (or p[j] if n > m).
Path cancontain four different types of operations: Match(M), substitution (S), insertion (I), and deletion (D).Our goal is finding affixes and in case of ambiguity,we employed the following heuristics for finding theSED operations leading the minimum distance:Case 1: If one string is longer than the other, chooseI for extra charactersCase 2: Until an M is found, choose I in case ofambiguityCase 3: If an M is found previously, choose M/S incase of ambiguityCase 4: If there is an M between two I?s, switch thiswith the last ICase 1 ensures that if one word has more charac-ters than the other, an insertion operation is selectedfor those characters.If there is an ambiguity, and an M/S or I oper-ation have the same minimum cost, Case 2 givespriority to the insertion operation until a matchcase is encountered, while Case 3 gives priority tomatch/substitution operations if a match case wasseen previously.Below example shows how Case 4 helps usto localize all the insertion operations.
For theheadword?candidate example word pair abirids ?makaabir?
?ds, the path changes from (1) to (2) usingCase 4, and correct prefix is identified as we explainin the next section.
(1) I M I I I M M M S M M?
Prefix m-(2) I I I I M M M M S M M?
Prefix maka-625 Morphology Induction from Noisy Data(MIND)The MIND framework consists of two stages.
In theexact match stage, MIND framework checks if theheadword occurs without any changes or errors (i.e.if headword occurs exactly in the example of us-age).
If no such occurrence is found an approximatematch search is performed in second stage.
Belowwe describe these two stages in detail.5.1 Exact MatchGiven a list of (noisy) headword?example of usagepairs (w,E), the exact match first checks if the head-word occurs in E in its root form.2 If the headwordcannot be found in E in its root form, for each eiin E, the longest common substring, LCS(w, ei),is computed.3 Let el be the ei that has the longestcommon substring (l) with w.4 If w = l, and forsome suffix s and some prefix p one of the followingconditions is true, the affix is extracted.1.
el = ws (suffix) or2.
el = pw (prefix) or3.
el = pws (circumfix)The extracted affixes are added to the induced af-fix list, and their exact counts are incremented.
Inthe third case p?s is treated together as a circumfix.For the infixes, there is one further step.
If w =w?l and el = e?ll, we compute LCS(w?, e?l).
If e?l =w?s, for some suffix s, s is added as an infix to theinduced affix list.
(This means el = w?sl wherew =w?l.
)The following sample run illustrates how the ex-act match part identifies affixes.
Given the Ce-buano headword?example of usage pair (abtik) ?
(naabtikan sad ku sa ba?ta?
), the word naabtikan ismarked as the candidate that has the longest com-mon substring with headword abtik.
These twowords have the following alignment, and we ex-tract the circumfix na?an.
In the illustration below,2Headwords consisting of one character are not checked.3In order to reduce the search space, we do not check theexample words that are shorter than the headword.
Althoughthere are some languages, such as Russian, in which headwordsmay be longer than the inflected forms, such cases are not in thescope of this paper.4Note that the length of the longest common substring canbe at most the length of the headword, in which case the longestcommon substring is the headword itself.straight lines represent matches, and short lines end-ing in square boxes represent insertions.5.2 Approximate MatchWhen we cannot find an exact match, there may bean approximate match resulting from an error withOCR or morphophonemic rules5, and we deal withsuch cases separately in the second part of the al-gorithm.
For each ei in E, we compute the dif-ference between headword, and example word, k-DIFF(w, ei).
The example word that has the min-imum difference from the headword is selected asthe most likely candidate (ecand).
We then find thesequence of the edit operations performed in achiev-ing the minimum distance alignment to transformecand to w using SED-path algorithm we describedabove.6Let cnt(X) be the count of X operation in thecomputed path.
If cnt(I) = 0, this case is consid-ered as an approximate root form (with OCR errors).The following conditions are considered as possibleerrors and no further analysis is done for such cases:cnt(M) = 0 ||cnt(M) < max(cnt(S), cnt(D), cnt(I)) ||cnt(M) < cnt(S) + cnt(D) + cnt(I)Otherwise, we use the insertion operations at thebeginning and at the end of the path to identify thetype of the affix (prefix, suffix, or circumfix) and thelength of the suffix (number of insertion operations).The identified affix is added to the affix list, andits approximate count is incremented.
All the othercases are dismissed as errors.
In its current state, theinfix affixes are not handled in approximate matchcase.The following sample shows how approximatematch works with noisy data.
In the Cebuano input5At this initial version, MIND does not make any distinc-tions between noise in the data such as OCR errors, and mor-phophonemic rules.
Making this distinction will be one of ourfuture focuses6Computing k-difference, and the edit path can be done inparallel to reduce the computing time.63pair (ambihas) ?
(ambsha?sa pagbutang ang duhaka silya arun makakita?
ang maglingkud sa luyu), thefirst word in the example of usage has an OCR er-ror, i is misrecognized as s. Moreover, there is avowel change in the word caused by the affix.
Anexact match of the headword cannot be found in theexample of usage.
The k-DIFF algorithm returnsambsha?sa as the candidate example of usage word,with a distance 2.
Then, the SED-path algorithmreturns the path M M M S M S M I, and algorithmsuccessfully concludes that a is the suffix as shownbelow in illustration (dotted lines represent substitu-tions).6 Experiments6.1 DictionariesThe BRIDGE system (Ma et al, 2003) processesscanned and OCRed dictionaries to reproduce elec-tronic versions and extract information from dictio-nary entries.
We used the BRIDGE system to pro-cess two bilingual dictionaries, a Cebuano-English(CebEng) dictionary (Wolff, 1972) and a Turkish-English (TurEng) dictionary (Avery et al, 1974),and extract a list of headword-example of usagepairs for our experiments.
The extracted data is notperfect: it has mistagged information, i.e.
it may in-clude some information that is not the headword orexample of usage, or some useful information maybe missing, and OCR errors may occur.
OCR errorscan be in different forms: Two words can be mergedinto one, one word can be split into two, or charac-ters can be misrecognized.Dictionary # of # of # ofDictionary pages hw-ex pairs wordsCebuano-all 1163 27129 206149Turkish-all 1000 27487 111334Cebuano-20 20 562 4134Turkish-20 20 503 1849Table 1: Details of Data from Two Dictionaries Usedin ExperimentsAlong with the headword?example of usage pairsfrom more than 1000 pages, we randomly selected20 pages for detailed analysis.
Table 1 provides de-tails of the data from two dictionaries we use in ourexperiments.Both Cebuano and Turkish are morphologicallyrich.
Cebuano allows prefixes, suffixes, circumfixes,infixes, while Turkish is an agglunative language.The two dictionaries have different characteristics.The example of usages in CebEng are complete sen-tences given in italic font while TurEng has phrases,idioms, or complete sentences as examples of usagesindicated in bold font.6.2 ProtocolWe ran our algorithm first on all of the data and thenon a randomly selected 20 pages from each dictio-nary.
We manually extracted the affixes from eachof the 20 pages.
We then evaluated the MIND re-sults with this ground truth.
During the evaluation,even if the number of an affix in the ground truth andresult are same, if they were extracted from differentwords, this is counted as an error.
We also examinedthe cause of each error in this data.We then compare our results from the wholeTurEng data with the state-of-the-art Linguistica(Goldsmith, 2001) algorithm.
Finally, we used thesuffixes extracted by MIND and Linguistica to seg-ment words in a Turkish treebank.6.3 AnalysisDict.
Affix Sample wordsmu- galing/mugaling hiku?h??ku?/muhiku`h?
?ku`C nag- kisdum/nagkisdum kugkugl/nagkugkugE mi- iktin/miiktin k?
?rus/mika?rusB i- kunsuylu/ikunsuylu paz??ha/ipar?
?haU na- p??l/nap?
?l ulatl/nau?latA gi- buga/gibuga da?lit/gida?ditN gi-an labuk/gilabukan ?
?kug/giiku?ganO -un gihay/gihayun ga?yung/gayu?ngun-a pisar/pisara sirnpul/simpu?la-?
ad/ad?
ilac?/ilae?T -i heves/hevesi ilim/ilmiU -a saz/saza sonsuz/sonsuzaR -e deniz/denize zmim/mimeK -?na etraf/etraf?na kolay/kolay?naI -ya hasta/hastaya orta/ortayaS -u?
u?st/u?stu?
zyu?z/yu?zu?H -ini bel/belini zevk/zevkini-ine derin/derinine ic?/ic?ineTable 3: Sample Affixes Extracted from Two Dictio-nariesTable 2 shows result of MIND runs.
The totalnumber of affixes and number of different types of64Cebuano TurkishWhole dict.
20 pages Whole dict.
20 pagesTotal 26106 542 27314 502Root form 5727 180 18416 345Prefix (diff.
type) 10300 (180) 197 (26) 6 (6) 0 (0)Suffix (diff.
type) 1315 (253) 16 (8) 6983 (447) 128 (59)Infix (diff.
type) 25 (11) 0 (0) 1 (1) 0 (0)Circumfix (diff.
type) 717 (221) 18 (11) 9 (9) 0 (0)App.
Root form 1023 14 103 1App.
Prefix (diff.
type) 1697 (116) 23 (9) 8 (8) 1 (1)App.
Suffix (diff.
type) 2930 (199) 63 (19) 168 (100) 5 (5)App.
Circumfix (diff.
type) 1060 (207) 14 (5) 20 (20) 0 (0)Couldn?t decide 1159 13 765 15Table 2: Total Number and Different Types of Affixes Extracted from Two Dictionaries Using MINDaffixes (in parenthesis) are presented for two dictio-naries, CebEng and TurEng, and two data sets, thewhole dictionary and 20 randomly selected pages.The top part of the table gives the exact match resultsand the bottom part shows the approximate matchresults.
For Cebuano, approximate match part of theframework finds many more affixes than it does forTurkish.
This is due to the different structures inthe two dictionaries.
We should note that althoughMIND incorrectly finds a few prefixes, circumfixes,and infixes for Turkish, these all have count one.Table 3 contains some of the most frequent ex-tracted affixes along with their exact and approxi-mate counts, and samples of headword?example ofusage word pairs they were extracted from.
Eachword is segmented into one root and one suffix,therefore when a word takes multiple affixes, theyare all treated as a compound affix.Dictionary GT cnt.
Res.cnt.
Misses AdditionsCebuano 311 314 17 14Turkish 155 142 8 10Table 4: Detailed Analysis of Affixes from 20 PagesTable 4 shows the number of affixes in groundtruth and MIND results along with number ofmissed and incorrectly added affixes on 20 of thesepages of data.
MIND only missed 5% of the affixesin the ground truth in both data sets.We also examined the causes of each miss and ad-dition.
Table 5 presents the causes of errors in theoutput of MIND with an example for each cause.
Weshould emphasize that a valid affix such as Turkishsuffix -m?
is counted as an error since the suffix -?n?
should be extracted for that particular headword?example of usage pair.
An OCR error such as themisrecognition of a as d, causes both the miss of theprefix mag- and incorrect addition of mdg- for Ce-buano.
There are some cases that cannot be correctlyidentified by the framework.
These usually involvedropping the last vowel because of morphophone-mic rules.
For the Cebuano dictionary, merge andsplit caused several errors, while Turkish data doesnot have any such errors.
Main reason is the differ-ent structure and format of the original dictionaries.In the Cebuano dictionary, an italic font which mayresult in merge and split is used to indicate exampleof usages.For the Cebuano data, five invalid suffixes, threeinvalid prefixes, and two invalid circumfixes arefound, while one valid suffix and one valid circumfixare missed.
For the Turkish data, three invalid suf-fixes, one invalid prefix, and two valid suffixes arefound while two valid suffix are missed.
When welook at the invalid affixes in the data, most of them(six of the Cebuano, and all of the Turkish ones)have count one, and maximum count in an invalidaffix is five.
Therefore, if we use a low threshold,we can eliminate many of the invalid affixes.6.4 Comparison to LinguisticaWe compared our system with Linguistica, a pub-licly available unsupervised corpus-based morphol-ogy learner (Goldsmith, 2001).
Linguistica inducesparadigms in a noise-free corpus, while MINDmakes use of string searching algorithms and allowsone to deal with noise at the cost of correctness.MIND emphasize segmenting a word into its rootand affixes.
We trained Linguistica using two dif-ferent data sets from TurEng7: 1) Whole headword-7We would like to do the same comparison in Cebuano.
Forthe time being, we could not find a treebank and native speakers65Reason Cebuano TurkishOCR 8 M?lbi 11 ?n??m?
or ?mAlgorithm 8 (uluy, giuylan)?
7 (al?n, aln?nda)?not gi-an, -lan is found not -?nda, -da is foundMerge 9 ?
?mung gila?ug??
?munggila?ug 0 -Split 1 nag-ku?gus?nag- ku?gus 0 -Other 5 apr.
?april 0 -Headword is an abbreviationTable 5: The Distribution of the Causes of Errors in 20 Pages with Samplesexample of usage sentence pairs, and 2) Headword-candidate example words that our algorithm returns.In the first case (Ling-all), Linguistica uses moredata than our algorithm, so to avoid any biases re-sulting from this, we also trained Linguistica usingthe headword and candidate example word (Ling-cand).
We only used the suffixes, since Turkish is asuffix-based language.
The evaluation is done by anative speaker.Figure 1 presents the analysis of the suffix listsproduced by Linguistica using two sets of trainingdata, and MIND.
The suffix lists are composed ofsuffixes the systems return that have counts morethan a threshold.
The results are presented for sixthreshold values for all of the data.
We use thresh-olding to decrease the number of invalid affixescaused primarily by the noise in the data.
For theMIND results, the suffixes over threshold are theones that have positive exact counts and total counts(sum of exact and approximate counts) more thanthe threshold.
Although Linguistica is not designedfor thresholding, the data we use is noisy, and weexplored if suffixes with a corpus count more thana threshold will eliminate invalid suffixes.
The ta-ble on the left gives the total number of suffixes,the percentage of suffixes that have a count morethan a threshold value, the percentage of invalid suf-fixes, and percentage of missed suffixes that are dis-carded by thresholding for the whole TurEng dictio-nary.
The number of affixes MIND finds are muchmore than that of Linguistica.
Furthermore, numberof invalid affixes are lower.
On the other hand, thenumber of missed affixes is also higher for MINDsince, for this particular data, there are many affixeswith counts less than 5.
41% of the affixes have anexact count of 1.
The main reason for this is theagglunative nature of Turkish language.
The effectof thresholding can also be examined in the graphfor Cebuano.on the right in Figure1 which gives the percentageof valid suffixes as a function of threshold values.MIND takes advantage of thresholding, and percent-age of valid suffixes rapidly decrease for thresholdvalue 1.System Th.
Total Over Th.
Invalid MissedLing-cand 0 6 100.00 0.00 0.00Ling-all 0 4 100.00 0.00 0.00MIND 0 60 96.67 1.72 0.00Ling-cand 1 6 66.67 0.00 33.33Ling-all 1 4 100.00 0.00 0.00MIND 1 60 41.67 0.00 53.33Ling-cand 2 6 50.00 0.00 50.00Ling-all 2 4 75.00 0.00 25.00MIND 2 60 18.33 0.00 76.67Table 6: Total Number and Percentage of Over theThreshold, Invalid, and Missed Suffixes Found byLinguistica and MIND for Different Threshold Val-ues for 20 pages of Turkish DataTable 6 presents the same results for 20 pagesfrom TurEng for three threshold values.
MIND per-forms well even with very small data and finds manyvalid affixes.
Linguistica on the other hand findsvery few.6.5 StemmingTo test the utility of the results, we perform a sim-ple word segmentation, with the aim of stripping theinflectional suffixes, and find the bare form of theword.
A word segmenter takes a list of suffixes, andtheir counts from the morphology induction system(Linguistica or MIND), a headword list as a dictio-nary, a threshold value, and the words from a tree-bank.
For each word in the treebank, there is a rootform (rf ), and a usage form (uf ).
The suffixes witha count more than the threshold are indexed accord-ing to their last letters.
For each word in the tree-bank, we first check if uf is already in the dictio-nary, i.e.
in the headword list.
If we cannot find it66System Th.
Total % Over Th.
% Invalid % MissedLing-cand 0 116 100.00 18.10 0.00Ling-all 0 274 100.00 34.67 0.00MIND 0 499 89.58 13.20 3.61Ling-cand 1 116 98.28 17.54 0.86Ling-all 1 274 94.89 32.69 1.46MIND 1 499 50.50 4.37 33.07Ling-cand 2 116 92.24 16.82 5.17Ling-all 2 274 87.96 31.12 4.74MIND 2 499 38.48 4.17 44.49Ling-cand 3 116 91.38 16.98 6.03Ling-all 3 274 85.40 31.20 6.57MIND 3 499 28.86 2.78 53.31Ling-cand 4 116 81.03 12.77 11.21Ling-all 4 274 81.39 30.94 9.12MIND 4 499 25.65 3.13 56.51Ling-cand 5 116 80.17 12.90 12.07Ling-all 5 274 79.56 31.19 10.58MIND 5 499 23.25 2.59 58.72Figure 1: Total Number and Percentage of Over the Threshold, Invalid, Missed and Valid Suffixes Found byLinguistica and MIND for Different Threshold Valuesin the dictionary, we repeatedly attempt to find thelongest suffix that matches the end of uf , and checkthe dictionary again.
The process stops when a dic-tionary word is found or when no matching suffixescan be found at the end of the word.
If the word thesegmenter returns is same as rf in the treebank, weincrease the correct count.
Otherwise, this case iscounted as an error.In our stemming experiments we used METU-Sabanci Turkish Treebank8, a morphologically andsyntactically annotated treebank corpus of 7262grammatical sentences (Atalay et al, 2003; Oflazeret al, 2003).
We skipped the punctuation and mul-tiple parses,9 and ran our word segmentation on14950 unique words.
We also used the headwordlist extracted from TurEng as the dictionary.
Notethat, the headword list is not error-free, it has OCRerrors.
Therefore even if the word segmenter returnsthe correct root form, it may not be in the dictionaryand the word may be stripped further.The percentage of correctly segmented words arepresented in Figure 2.
We show results for sixthreshold values.
Suffixes with counts more than thethreshold are used in each case.
Again for MINDresults, we require that the exact match counts aremore than zero, and the total of exact match and ap-8http://www.ii.metu.edu.tr/ corpus/treebank.html9Multiple parses are the cases where a suffix is attached notto a single word, but to a group of words.
The suffix -ti in takipetti is attached to takip et.Figure 2: Percentage of Correctly Segmented Wordsby Different Systems for Different Threshold Valuesproximate match counts are more than the thresh-old.
For Linguistica, suffixes with a corpus countmore than the threshold are used.
For each thresh-old value, MIND did much better than Ling-cand.MIND outperformed Ling-all for thresholds 0 and1.
For the other values, the difference is small.
Weshould note that Ling-all uses much more trainingdata than MIND (503 vs. 1849 example of words),and even with this difference the performance ofMIND is close to Ling-all.
We believe the reasonfor the close performance of MIND and Ling-all insegmentation despite the huge difference in the num-ber of correct affixes they found due to the fact thataffixes Ling-all finds are shorter, and more frequent.In its current state, MIND does not segment com-pound affixes, and find several long and less fre-quent affixes.
These long affixes can be composed67by shorter affixes Linguistica finds.7 Conclusion and Future WorkWe presented a framework for morphology induc-tion from noisy data, that is especially useful for lan-guages which have limited electronic data.
We usethe information in dictionaries, specifically head-word and the corresponding example of usage sen-tences, to acquire affix lists of the language.
We pre-sented results on two data sets and demonstrated thatour framework successfully finds the prefixes, suf-fixes, circumfixes, and infixes.
We also used the ac-quired suffix list from one data set in a simple wordsegmentation process, and outperformed a state-of-the-art morphology learner using the same amountof training data.At this point we are only using headword andcorresponding example of usage pairs.
Dictionariesprovide much more information.
We plan to makeuse of other information, such as POS, to categorizethe acquired affixes.
We will also investigate howusing all the words in example of usages and split-ting the compound affixes in agglunative languagescan help us to increase the confidence of correct af-fixes, and decrease the number of invalid affixes.Finally we will work on identifying morphophone-mic rules (especially stem-interval vowel shifts andpoint-of-affixation stem changes).AcknowledgmentsThe partial support of this research under contractMDA-9040-2C-0406 is gratefully acknowledged.ReferencesNart B. Atalay, Kemal Oflazer, and Bilge Say.
2003.
The an-notation process in the Turkish Treebank.
In Proceedings ofthe EACL Workshop on Linguistically Interpreted Corpora?LINC, Budapest, Hungary, April.Robert Avery, Serap Bezmez, Anna G. Edmonds, and MehlikaYaylal?.
1974.
Redhouse ?Ingilizce-Tu?rkc?e So?zlu?k.
Red-house Yay?nevi.Marco Baroni, Johannes Matiasek, and Harald Trost.
2002.Unsupervised discovery of morphologically related wordsbased on orthographic and semantic similarity.
In Proceed-ings of the ACL-02 Workshop on Morphological and Phono-logical Learning, pages 48?57.Michael R. Brent, Sreerama K. Murthy, and Andrew Lundberg.1995.
Discovering morphemic suffixes: A case study inminimum description length induction.
In Proceedings ofthe 15th Annual Conference of the Cognitive Science Soci-ety, pages 28?36, Hillsdale, NJ.Michael R. Brent.
1993.
Minimal generative models: A mid-dle ground between neurons and triggers.
In Proceedings ofthe 5th International Workshop on Artificial Intelligence andStatistics, Ft. Laudersdale, FL.Mathias Creutz and Krista Lagus.
2002.
Unsupervised discov-ery of morphemes.
In Proceedings of the ACL-02 Workshopon Morphological and Phonological Learning.H.
Dejean.
1998.
Morphemes as necessary concepts for struc-tures: Discovery from untagged corpora.
In Workshop onParadigms and Grounding in Natural Language Learning,pages 295?299.John Goldsmith, Yu Hu, Irina Matveeva, and Colin Sprague.2005.
A heuristic for morpheme discovery based on stringedit distance.
Technical Report TR-2205-04, Department ofComputer Science, University of Chicago.John Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Linguistics,27(2):153?198.Zellig Harris.
1955.
From phoneme to morpheme.
Language,31:190?222.Huanfeng Ma, Burcu Karagol-Ayan, David Doermann, Dou-glas Oard, and Jianqiang Wang.
2003.
Parsing and tag-ging of bilingual dictionaries.
Traitement Automatique DesLangues, pages 125?150.Christian Monson.
2004.
A framework for unsupervised nat-ural language morphology induction.
In Proceedings of theStudent Research Workshop: ACL 2004, pages 67?72.Sylvain Neuvel and Sean A. Fulop.
2002.
Unsupervised learn-ing of morphology without morphemes.
In Proceedings ofthe ACL-02 Workshop on Morphological and PhonologicalLearning, pages 31?40.Kemal Oflazer, Bilge Say, Dilek Hakkani-Tu?r, and Go?khan Tu?r.2003.
Building a Turkish Treebank.
In Anne Abeille?, edi-tor, Building and Using Parsed Corpora.
Kluwer AcademicPublishers.Patrick Schone and Daniel Jurafsky.
2001.
Knowledge-freeinduction of inflectional morphologies.
In Second Meetingof the NAACL, pages 183?191.Ilya Segalovich.
2003.
A fast morphological algorithm withunknown word guessing induced by a dictionary for a websearch engine.
In Proceedings of MLMTA, Las Vegas, NV.P.H.
Sellers.
1980.
The theory and computation of evolution-ary distances: pattern recognition.
Journal of Algorithms,1:359?373.Matthew G. Snover and Michael R. Brent.
2001.
A bayesianmodel for morpheme and paradigm identification.
In Pro-ceedings of the 39th Annual Meeting of the ACL, pages 482?490.Robert A. Wagner and Michael J. Fischer.
1974.
The string-to-string correction problem.
Journal of the Association forComputing Machinery, 21(1):168?173.Richard Wicentowski.
2004.
Multilingual noise-robust super-vised morphological analysis using the wordframe model.In Proceedings of the 7th Meeting of the ACL Special In-terest Group in Computational Phonology, pages 70?77,Barcelona, Spain.John U. Wolff.
1972.
A Dictionary of Cebuano Visaya.
South-east Asia Program, Cornell University, Ithaca, New York.68
