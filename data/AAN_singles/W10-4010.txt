Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 61?69,Beijing, August 2010Towards multi-lingual summarization: A comparative analysis ofsentence extraction methods on English and Hebrew corporaMarina Litvak and Hagay Lipman and Assaf Ben Gur and Mark LastBen Gurion University of the Negev{litvakm, lipmanh, bengura, mlast}@bgu.ac.ilSlava Kisilevich and Daniel KeimUniversity of Konstanzslaks@dbvis.inf.uni-konstanz.deDaniel.Keim@uni-konstanz.deAbstractThe trend toward the growing multi-linguality of the Internet requires textsummarization techniques that workequally well in multiple languages.
Onlysome of the automated summarizationmethods proposed in the literature, how-ever, can be defined as ?language-independent?, as they are not based onany morphological analysis of the sum-marized text.
In this paper, we per-form an in-depth comparative analysis oflanguage-independent sentence scoringmethods for extractive single-documentsummarization.
We evaluate 15 pub-lished summarization methods proposedin the literature and 16 methods intro-duced in (Litvak et al, 2010).
The eval-uation is performed on English and He-brew corpora.
The results suggest thatthe performance ranking of the com-pared methods is quite similar in bothlanguages.
The top ten bilingual scoringmethods include six methods introducedin (Litvak et al, 2010).1 IntroductionAutomatically generated summaries can signif-icantly reduce the information overload on pro-fessionals in a variety of fields, could prove ben-eficial for the automated classification and fil-tering of documents, the search for informationover the Internet and applications that utilizelarge textual databases.Document summarization methodologies in-clude statistic-based, using either the classic vec-tor space model or a graph representation, andsemantic-based, using ontologies and language-specific knowledge (Mani & Maybury, 1999).Although the use of language-specific knowl-edge can potentially improve the quality of auto-mated summaries generated in a particular lan-guage, its language specificity ultimately re-stricts the use of such a summarizer to a sin-gle language.
Only systems that perform equallywell on different languages in the absence of anylanguage-specific knowledge can be consideredlanguage-independent summarizers.As the number of languages used on the In-ternet increases continiously (there are at least75 different languages according to a estimateperformed by A. Gulli and A. Signorini1 in theend of January 2005), there is a growing needfor language-independent statistical summariza-tion techniques that can be readily applied to textin any language without using language-specificmorphological tools.In this work, we perform an in-depth com-parative analysis of 16 methods for language-independent extractive summarization intro-duced in (Litvak et al, 2010) that utilize ei-ther vector or graph-based representations of textdocuments computed from word segmentationand 15 state-of-the art language-independentscoring methods.
The main goal of the eval-uation experiments, which focused on Englishand Hebrew corpora, is to find the most efficientlanguage-independent sentence scoring methods1http://www.cs.uiowa.edu/ asignori/web-size/61in terms of summarization accuracy and com-putational complexity across two different lan-guages.This paper is organized as follows.
Thenext section describes related work in extrac-tive summarization.
Section 3 reviews the evalu-ated language-independent sentence scoring ap-proaches.
Section 4 contains our experimentalresults on English and Hebrew corpora.
The lastsection comprises conclusions and future work.2 Related WorkExtractive summarization is aimed at the selec-tion of a subset of the most relevant fragments,which can be paragraphs, sentences, keyphrases,or keywords from a given source text.
The ex-tractive summarization process usually involvesranking, such that each fragment of a summa-rized text gets a relevance score, and extraction,during which the top-ranked fragments are ex-tracted and arranged in a summary in the sameorder they appeared in the original text.
Statisti-cal methods for calculating the relevance scoreof each fragment can rely on such informa-tion as: fragment position inside the document,its length, whether it contains keywords or titlewords.Research by Luhn (1958), in which the sig-nificance factor of a sentence is based on thefrequency and the relative position of significantwords within that sentence, is considered the firston automated text summarization.
Luhn?s workwas followed shortly thereafter by that of Ed-mundson (1969) and some time later by stud-ies from Radev et al (2001) and Saggion et al(2003), all of who applied linear combinationsof multiple statistical methods to rank sentencesusing the vector space model as a text representa-tion.
In (Litvak et al, 2010) we improve the sum-marization quality by identifying the best linearcombination of the metrics evaluated in this pa-per.Several information retrieval and machinelearning techniques have been proposed for de-termining sentence importance (Kupiec et al,1995; Wong et al, 2008).
Gong and Liu (2001)and Steinberger and Jezek (2004) showed thatsingular value decomposition (SVD) can be ap-plied to generate extracts.Among text representation models, graph-based text representations have gained popular-ity in automated summarization, as they enablethe model to be enriched with syntactic and se-mantic relations.
Salton et al (1997) wereamong the first to attempt graph-based rankingmethods for single document extractive summa-rization by generating similarity links betweendocument paragraphs.
The important paragraphsof a text were extracted using degree scores.Erkan and Radev (2004) and Mihalcea (2005) in-troduced approaches for unsupervised extractivesummarization that rely on the application of it-erative graph based ranking algorithms.
In theirapproaches, each document is represented as agraph of sentences interconnected by similarityrelations.3 Language-Independent ScoringMethods for Sentence ExtractionVarious language dependent and independentsentence scoring methods have been introducedin the literature.
We selected the 15 most promi-nent language independent methods for evalua-tion.
Most of them can be categorized as fre-quency, position, length, or title-based, and theyutilize vector representation.
TextRank (ML TR)is the only method that is based on graph repre-sentation, but there are also position and length-based methods that calculate scores using theoverall structure of a document.
We have alsoconsidered 16 methods proposed in (Litvak et al,2010), including 13 based on the graph-theoreticrepresentation (Section 3.1).Figure 1 (Litvak et al, 2010) shows the taxon-omy of the 31 methods considered in our work.All methods introduced in (Litvak et al, 2010)are denoted by an asterisk (*).
Methods requir-ing a threshold value t ?
[0, 1] that specifies theportion of the top rated terms considered signifi-cant are marked by a cross in Figure 1 and listedin Table 1 along with the optimal average thresh-old values obtained after evaluating the methods62Table 1: Selected thresholds for threshold-basedscoring methodsMethod ThresholdLUHN 0.9LUHN DEG 0.9LUHN PR 0.0KEY [0.8, 1.0]KEY DEG [0.8, 1.0]KEY PR [0.1, 1.0]COV 0.9COV DEG [0.7, 0.9]COV PR 0.1on English and Hebrew documents (Litvak et al,2010).The methods are divided into three main cat-egories: structure-, vector-, and graph-basedmethods, and each category also contains aninternal taxonomy.
Sections 3.2, 3.3, and3.4 present structure-, vector-, and graph-basedmethods, respectively.
With each description, areference to the original work where the methodwas proposed for extractive summarization is in-cluded.
We denote sentence by S and text docu-ment by D.3.1 Text Representation ModelsThe vector-based scoring methods listed belowuse tf or tf-idf term weights to evaluate sen-tence importance while that used by the graph-based methods (except for TextRank) is basedon the word-based graph representation modelpresented in Schenker et al (2004).
We repre-sent each document by a directed, labeled, un-weighted graph in which nodes represent uniqueterms (distinct normalized words) and edges rep-resent order-relationships between two terms.Each edge is labeled with the IDs of sentencesthat contain both words in the specified order.3.2 Structure-based Scoring MethodsIn this section, we describe the existingstructure-based methods for multilingual sen-tence scoring.
These methods do not require anytext representation and are based on its structure.?
Position (Baxendale, 1958):POS L Closeness to the end of the document:score(Si) = i, where i is a sequential number ofa sentence in a document;POS F Closeness to the beginning of the docu-ment: score(Si) = 1i ;POS B Closeness to the borders of the docu-ment: score(Si) = max(1i , 1n?i+1), where n isthe total number of sentences in D.?
Length (Satoshi et al, 2001):LEN W Number of words in a sentence;LEN CH Number of characters in a sentence.3.3 Vector-based Scoring MethodsIn this section, we describe the vector-basedmethods for multilingual sentence scoring, thatare based on the vector space model for text rep-resentation.?
Frequency-based:LUHN (Luhn, 1958)score(S) = maxci?
{clusters(S)}{csi}, whereclusters are portions of a sentence brack-eted by keywords2 and csi = |keywords(ci)|2|ci| .KEY (Edmundson, 1969) Sum of the keywordfrequencies: score(S) = ?i?
{keywords(S)} tfi,where tfi is term in-document frequency ofkeyword i.COV (Kallel et al, 2004) Ratio of keywordnumbers (Coverage): score(S) = |keywords(S)||keywords(D)|TF (Vanderwende et al, 2007) Average termfrequency for all sentence words:score(S) =?i?
{words(S)} tfi|S| .TFISF (Neto et al, 2000) Average termfrequency inverted sentence frequencyfor all sentence words: score(S) =?i?
{words(S)} tfi ?
isfi,where isfi = 1?
log(ni)log(n) , where n is the numberof sentences in a document and ni is the numberof sentences containing word i.SVD (Steinberger & Jezek, 2004) score(S)is equal to the length of a sentence vectorin ?2V T after computing the Singular ValueDecomposition of a term by sentence matrixA = U?V T?
Title (Edmundson, 1969) similarity3 to thetitle, score(S) = sim(S, T ):TITLE O using overlap similarity: |S?T |min{|S|,|T |}TITLE J using Jaccard similarity: |S?T ||S?T |2Luhn?s experiments suggest an optimal limit of 4 or 5non-significant words between keywords.3Due to multilingual focus of our work, exact wordmatching was used in all similarity-based methods.63Multilingual sentencescoringmethodsStructure-based Vector-based Graph-basedPosition Length Frequency Similarity Degree SimilarityPagerankTitle DocumentPOS_FPOS_LPOS_BLEN_WLEN_CHLUHN?KEY ?COV ?TFTFIISFSVDTITLE_OTITLE_JTITLE_CD_COV_O*D_COV_J*D_COV_C*LUHN_DEG ?
*KEY_DEG ?
*COV_DEG ?
*DEG*GRASE*LUHN_PR ?
*KEY_PR ?
*COV_PR ?
*PR*ML_TRTitle DocumentTITLE_E_O*TITLE_E_J* D_COV_E_O*D_COV_E_J*Figure 1: Taxonomy of statistical language-independent sentence scoring methods (Litvak et al,2010)TITLE C using cosine similarity:sim(~S, ~T ) = cos(~S, ~T ) = ~S?~T|~S|?|~T |?
Document Coverage (Litvak et al, 2010).These methods score a sentence according toits similarity to the rest of the sentences inthe document (D ?
S) based on the followingintuition: the more document content is coveredby a sentence, the more important the sentence isto a summary.
Redundant sentences containingrepetitive information are removed using asimilarity filter.
score(S) = sim(S,D ?
S):D COV O using Overlap similarity:|S?T |min{|S|,|D?S|}D COV J using Jaccard similarity: |S?T ||S?D?S|D COV C using Cosine similarity:cos(~S, ~D ?
S) = ~S?
~D?S|~S|?| ~D?S|3.4 Graph-based Scoring MethodsIn this section, we describe the methods for mul-tilingual sentence scoring using the graph textrepresentation based on sentence (ML TR) orword (all except ML TR) segmentation.ML TR Multilingual version of Tex-tRank (Mihalcea, 2005) without morphologicalanalysis.
Each document is represented as adirected graph of nodes that stand for sen-tences interconnected by similarity (overlap)relationship.
To each edge connecting twovertices the weight is assigned and equal tothe similarity value between the correspondingsentences.
We used backward links, as it wasthe most successful according to the reportedresults in (Mihalcea, 2005).
score(S) is equalto PageRank (Brin & Page, 1998) of its node,according to the formula adapted to the weightsassigned to edges.?
Degree-based (Litvak et al, 2010):4LUHN DEG A graph-based extension of theLUHN measure, in which a node degree isused instead of a word frequency: words areconsidered significant if they are representedby nodes of a higher degree than a predefinedthreshold (see Table 1).KEY DEG Graph-based extension of KEYmeasure.COV DEG Graph-based extension of COVmeasure.DEG Average degree for all sentence nodes:score(S) =?i?
{words(S)}Degi|S| .GRASE(GRaph-based Automated SentenceExtractor) Modification of Salton?s algo-rithm (Salton et al, 1997) using the graph4All proposed here degree-based methods, except forGRASE, use undirected graphs and degree of nodes as apredictive feature.
The methods based on the directed wordgraphs and distinguishing between in- and out-links wereoutperformed in our preliminary experiments by the undi-rected approach.64representation defined in Section 3.1 above.In our graph representation, all sentences arerepresented by paths, completely or partially.To identify the relevant sentences, we searchfor the bushy paths and extract from them thesentences that appear the most frequently.
Eachsentence in the bushy path gets a dominationscore that is the number of edges with its labelin the path normalized by the sentence length.The relevance score for a sentence is calculatedas a sum of its domination scores over all paths.?
PageRank-based:5LUHN PR A graph-based extension of theLUHN measure in which the node PageRankvalue is used instead of the word frequency:keywords are those words represented by nodeswith a PageRank score higher than a predefinedthreshold (see Table 1).KEY PR Graph-based extension of KEY mea-sure.COV PR Graph-based extension of COV mea-sure.PR Average PageRank for all sentence nodes:score(S) =?i?
{words(S)} PRi|S| .?
Similarity-based.
Edge matching techniquessimilar to those of Nastase and Szpakowicz(2006) are used.
Edge matching is an alternativeapproach to measure the similarity betweengraphs based on the number of common edges:TITLE E O Graph-based extension of TI-TLE O ?
Overlap-based edge matching betweentitle and sentence graphs.TITLE E J Graph-based extension of TITLE J?
Jaccard-based edge matching between titleand sentence graphs.D COV E O Graph-based extension ofD COV O ?
Overlap-based edge matchingbetween sentence and document complement(the rest of a document sentences) graphs.D COV E J Graph-based extension ofD COV J ?
Jaccard-based edge matching5Using undirected word graphs with PageRank does notmake sense, since for an undirected graph a node pagerankscore is known to be proportional to its degree.
Revers-ing links will result in hub scores instead authority.
Themethods distinguishing between authority and hub scoreswere outperformed in our preliminary experiments by thedegree-based approach.between sentence and document complementgraphs.4 Experiments4.1 OverviewThe quality of the above-mentioned sentenceranking methods was evaluated through a com-parative experiment on corpora of English andHebrew texts.
These two languages, whichbelong to different language families (Indo-European and Semitic languages, respectively),were intentionally chosen for this experiment toincrease the generality of our evaluation.
Themain difference between these languages, is thatHebrew morphology allows morphemes to becombined systematically into complex word-forms.
In different contexts, the same morphemecan appear as a separate word-form, while in oth-ers it appears agglutinated as a suffix or prefix toanother word-form (Adler, 2009).The goals of the experiment were as follows:- To evaluate the performance of different ap-proaches for extractive single-document summa-rization using graph and vector representations.- To compare the quality of the multilingual sum-marization methods proposed in our previouswork (Litvak et al, 2010) to the state-of-the-artapproaches.- To identify sentence ranking methods that workequally well on both languages.4.2 Text PreprocessingExtractive summarization relies critically onproper sentence segmentation to insure the qual-ity of the summarization results.
We used a sen-tence splitter provided with the MEAD summa-rizer (Radev et al, 2001) for English and a sim-ple splitter for Hebrew splitting the text at everyperiod, exclamation point, or question mark.64.3 Experimental DataFor English texts, we used the corpus of sum-marized documents provided for the single doc-6Although the same set of splitting rules may be usedfor both languages, separate splitters were used since theMEAD splitter is restricted to European languages.65ument summarization task at the DocumentUnderstanding Conference 2002 (DUC, 2002).This benchmark dataset contains 533 news arti-cles, each of which is at least ten sentences longand has two to three human-generated abstractsof approximately 100 words apiece.However, to the best of our knowledge, nosummarization benchmarks exist for the Hebrewlanguage texts.
To collect summarized texts inHebrew, we set up an experiment7 in which 50news articles of 250 to 830 words each from theHaaretz8 newspaper internet site were summa-rized by human assessors by extracting the mostsalient sentences.
In total, 70 undergraduate stu-dents from the Department of Information Sys-tems Engineering, Ben Gurion University of theNegev participated in the experiment.
Ten doc-uments were randomly assigned to each of the70 study participants who were instructed (1)To dedicate at least five minutes to each doc-ument, (2) To ignore dialogs and citations, (3)To read the whole document before starting sen-tence extraction, (4) To ignore redundant, repet-itive, or overly detailed information, (5) To obeythe minimal and maximal summary constraintsof 95 and 100 words, respectively.
Summarieswere assessed for quality by procedure describedin (Litvak et al, 2010).4.4 Experimental ResultsWe evaluated English and Hebrew summariesusing the ROUGE-1, 2, 3, 4, L, SU and W met-rics9, described in Lin (2004).
Our results werenot statistically distinguishable and matched theconclusion of Lin (2004).
However, becauseROUGE-1 showed the largest variation acrossthe methods, all results in the following com-parisons are presented in terms of ROUGE-1metric.
Similar to the approach describedin Dang (2006), we performed multiple com-parisons between the sentence scoring methods.The Friedman test was used to reject the null hy-7The software enabling easy selection and storage ofsentences to be included in the document extract, can beprovided upon request.8http://www.haaretz.co.il9ROUGE toolkit was adapted to Hebrew by specifying?token?
using Hebrew alphabetTable 2: English: Multiple comparisons of sen-tence ranking approaches using the Bonferroni-Dunn test of ROUGE-1 RecallApproach ROUGE-1COV DEG?
0.436 AKEY DEG?
0.433 A BKEY 0.429 A B CCOV PR?
0.428 A B C DCOV 0.428 A B C DD COV C?
0.428 A B C DD COV J?
0.425 B C D EKEY PR?
0.424 B C D ELUHN DEG?
0.422 C D E FPOS F 0.419 E F GLEN CH 0.418 C D E F GLUHN 0.418 D E F GLUHN PR?
0.418 E F G HLEN W 0.416 D E F G HML TR 0.414 E F G HTITLE E J?
0.413 F G H ITITLE E O?
0.413 F G H ID COV E J?
0.410 F G H ID COV O?
0.405 G H I JTFISF 0.405 G H I JDEG?
0.403 G H I JD COV E O?
0.401 H I J KPR?
0.400 G H I J KTITLE J 0.399 I J KTF 0.397 I J KTITLE O 0.396 J KSVD 0.395 I J KTITLE C 0.395 J KPOS B 0.392 K LGRASE?
0.372 LPOS L 0.339 Mpothesis (all methods perform the same) at the0.0001 significance level, after which we ran theBonferroni-Dunn test (Demsar, 2006) for pair-wise comparisons.
Tables 2 and 3 show the re-sults of multiple comparisons and are arrangedin descending order with the best approacheson top.
Methods not sharing any common let-ter were significantly different at the 95% confi-dence level.The Pearson correlation between methodsranking in English and Hebrew was 0.775, whichwas larger than zero at a significance level of0.0001.
In other words, most of the methodswere ranked in nearly the same relative positionsin both corpora, and the top ranked methods per-formed equally well in both languages.
The dif-ferences in ranking were caused by morphologi-cal differences between two languages.To determine which approaches performedbest in both languages, we analyzed the cluster-ing results of the methods in both corpora andfound the intersection of the top clusters fromthe two clustering results.
For each language,a document-method matrix of ROUGE scoreswas created with methods represented by vec-tors of their ROUGE scores for each documentin a corpora.
Since most scores are not normally66Table 3: Hebrew: Multiple comparisons of sen-tence ranking approaches using the Bonferroni-Dunn test of ROUGE-1 RecallApproach ROUGE-1D COV J?
0.574 AKEY 0.570 A BCOV DEG?
0.568 A BPOS F 0.567 A BCOV 0.567 A BTITLE J 0.567 A BPOS B 0.565 A BLUHN PR?
0.560 A B CLUHN DEG?
0.560 A B CD COV E J?
0.559 A B CLUHN 0.559 A B CTITLE E J?
0.556 A B CTITLE E O?
0.556 A B CKEY DEG?
0.555 A B CLEN W 0.555 A B CLEN CH 0.553 A B CKEY PR?
0.546 A B CCOV PR?
0.546 A B CTITLE O 0.545 A B CD COV C?
0.543 A B CTITLE C 0.541 A B CML TR 0.519 A B C DTFISF 0.514 A B C DD COV E O?
0.498 A B C DSVD 0.498 A B C DD COV O?
0.466 B C DTF 0.427 C D EDEG?
0.399 D E FPR?
0.331 E FGRASE?
0.243 FPOS L 0.237 FTable 4: English: Correlation between sentenceranking approaches using PearsonApproach Correlated WithPOS F (LUHN PR, 0.973), (TITLE E J, 0.902), (TITLE E O, 0.902)TITLE O (TITLE J, 0.950)LEN W (LEN CH, 0.909)KEY PR (COV PR, 0.944)TITLE E O (TITLE E J, 0.997)distributed, we chose the K-means algorithm,which does not assume normal distribution ofdata, for clustering.
We ran the algorithm withdifferent numbers of clusters (2 ?
K ?
10),and for each K, we measured two parameters:the minimal distance between neighboring clus-ters in the clustered data for each language andthe level of similarity between the clustering re-sults for the two languages.
For both param-eters, we used the regular Euclidean distance.For K ?
6, the clusters were highly similarfor each language, and the distance between En-glish and Hebrew clustering data was maximal.Based on the obtained results, we left resultsonly for 2 ?
K ?
5 for each corpus.
Then,we ordered the clusters by the average ROUGEscore of each cluster?s instances (methods) andidentified the methods appearing in the top clus-ters for all K values in both corpora.
Table 6shows the resulting top ten scoring methods withtheir rank in each corpus.
Six methods intro-Table 5: Hebrew: Correlation between sentenceranking approaches using PearsonApproach Correlated WithKEY (KEY DEG, 0.930)COV (D COV J, 0.911)POS F (POS B, 0.945), (LUHN DEG, 0.959), (LUHN PR, 0.958)POS B (LUHN DEG, 0.927), (LUHN PR, 0.925)TITLE O (TITLE E J, 0.920), (TITLE E O, 0.920)TITLE J (TITLE E J, 0.942), (TITLE E O, 0.942)LEN W (LEN CH, 0.954), (KEY PR, 0.912)LEN CH (KEY PR, 0.936), (KEY DEG, 0.915), (COV DEG, 0.901)LUHN DEG (LUHN PR, 0.998)KEY DEG (COV DEG, 0.904)Table 6: Ranking of the best bilingual scoresScoring Rank in Rank in Textmethod English corpus Hebrew corpus RepresentationKEY 3 2 vectorCOV 4 4 vectorKEY DEG 2 10 graphCOV DEG 1 3 graphKEY PR 6 12 graphCOV PR 4 12 graphD COV C 4 14 vectorD COV J 5 1 vectorLEN W 10 10 structureLEN CH 9 11 structureduced in this paper, such as Document Cover-age (D COV C/J) and graph adaptations of Cov-erage (COV DEG/PR) and Key (KEY DEG/PR),are among these top ten bilingual methods.Neither vector- nor graph-based text represen-tation models, however, can claim ultimate supe-riority, as methods based on both models promi-nently in the top-evaluated cluster.
Moreover,highly-correlated methods (see Tables 4 and 5for highly-correlated pairs of methods in Englishand Hebrew corpora, respectively) appear in thesame cluster in most cases.
As a result, somepairs from among the top ten methods are highly-correlated in at least one language, and only onefrom each pair can be considered.
For example,LEN W and LEN CH have high correlation coef-ficients (0.909 and 0.954 in English and Hebrew,respectively).
Since LEN CH is more appropri-ate for multilingual processing due to variationsin the rules of tokenization between languages(e.g., English vs. German), it may be considereda preferable multilingual metric.In terms of summarization quality and com-putational complexity, all scoring functions pre-sented in Table 6 can be considered to performequally well for bilingual extractive summariza-tion.
Assuming their efficient implementation,all methods have a linear computational com-plexity, O(n), relative to the total number ofwords in a document.
KEY PR and COV PR re-67quire additional O(c(|E|+|V |)) time for runningPageRank, where c is the number of iterations itneeds to converge, |E| is the number of edges,and |V | is the number of nodes (distinct words)in a document graph.
Since neither |E| nor |V | inour graph representation can be as large as n, thetotal computation time for KEY PR and COV PRmetrics is also linear relative to the documentsize.In terms of implementation complexity,LEN W and LEN CH are simpliest, since theyeven do not require any preprocessing and repre-sentation building; KEY and COV require key-words identification; D COV C, and D COV Jrequire vector space model building; KEY DEGand COV DEG need graphs building (order ofwords); whereas KEY PR and COV PR, in ad-dition, require PageRank implementation.5 Conclusion and Future ResearchIn this paper, we conducted in-depth, compar-ative evaluations of 31 existing (16 of whichare mostly graph-based modifications of exist-ing state-of-the-art methods, introduced in (Lit-vak et al, 2010)) scoring methods10 using En-glish and Hebrew language texts.The experimental results suggest that the rel-ative ranking of methods performance is quitesimilar in both languages.
We identified meth-ods that performed significantly better in onlyone of the languages and those that performedequally well in both languages.
Moreover, al-though vector and graph-based approaches wereamong the top ranked methods for bilingual ap-plication, no text representation model presenteditself as markedly superior to the other.Our future research will extend the evaluationsof language-independent sentence ranking met-rics to a range of other languages such as Ger-man, Arabic, Greek, and Russian.
We will adaptsimilarity-based metrics to multilingual applica-tion by implementing them via n-gram matchinginstead of exact word matching.
We will fur-ther improve the summarization quality by ap-10We will provide the code for our summarizer upon re-quest.plying machine learning on described features.We will use additional techniques for summaryevaluation and study the impact of morpholog-ical analysis on the top ranked bilingual scoresusing part-of-speech (POS) tagging11, anaphoraresolution, named entity recognition, and takingword sense into account.AcknowledgmentsWe are grateful to Michael Elhadad and GalinaVolk for providing the ROUGE toolkit adaptedto Hebrew alphabet.ReferencesAdler, M. (2009).
Hebrew morphologicaldisambiguation: An unsupervised stochas-tic word-based approach.
Dissertation.http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.Baxendale, P. (1958).
Machine-made index fortechnical literature-an experiment.
IBM Jour-nal of Research and Development, 2, 354?361.Brin, S., & Page, L. (1998).
The anatomy ofa large-scale hypertextual web search engine.Computer networks and ISDN systems, 30,107?117.Dang, H. T. (2006).
Overview of DUC 2006.Proceedings of the Document UnderstandingConference.Demsar, J.
(2006).
Statistical comparisons ofclassifiers over multiple data sets.
Journal ofMachine Learning Research, 7, 1?30.DUC (2002).
Document understanding confer-ence.
http://duc.nist.gov.Edmundson, H. P. (1969).
New methods in auto-matic extracting.
J. ACM, 16.Erkan, G., & Radev, D. R. (2004).
LexRank:Graph-based lexical centrality as salience intext summarization.
Journal of Artificial In-telligence Research, 22, 457?479.11Our experiments have shown that syntactic filters,which select only lexical units of a certain part of speech,do not significantly improve the performance of the evalu-ated bilingual scoring methods.68Gong, Y., & Liu, X.
(2001).
Generic text summa-rization using relevance measure and latent se-mantic analysis.
Proceedings of the 24th ACMSIGIR conference on Research and develop-ment in information retrieval (pp.
19?25).Kallel, F. J., Jaoua, M., Hadrich, L. B., &Hamadou, A.
B.
(2004).
Summarization atLARIS laboratory.
Proceedings of the Doc-ument Understanding Conference.Kupiec, J., Pedersen, J., & Chen, F. (1995).
Atrainable document summarizer.
Proceedingsof the 18th annual international ACM SIGIRconference (pp.
68?73).Lin, C.-Y.
(2004).
ROUGE: A package for au-tomatic evaluation of summaries.
Proceedingsof the ACL?04 Workshop: Text SummarizationBranches Out (pp.
74?81).Litvak, M., Last, M., & Friedman, M. (2010).
Anew approach to improving multilingual sum-marization using a genetic algorithm.
Pro-ceedings of the Association for ComputationalLinguistics (ACL) 2010.
Uppsala, Sweden.Luhn, H. P. (1958).
The automatic creation ofliterature abstracts.
IBM Journal of Researchand Development, 2, 159?165.Mani, I., & Maybury, M. (1999).
Advances inautomatic text summarization.Mihalcea, R. (2005).
Language independent ex-tractive summarization.
AAAI?05: Proceed-ings of the 20th national conference on Artifi-cial intelligence (pp.
1688?1689).Nastase, V., & Szpakowicz, S. (2006).
A studyof two graph algorithms in topic-driven sum-marization.
Proceedings of the Workshopon Graph-based Algorithms for Natural Lan-guage.Neto, J., Santos, A., Kaestner, C., & Freitas, A.(2000).
Generating text summaries throughthe relative importance of topics.
LectureNotes in Computer Science, 300?309.Radev, D., Blair-Goldensohn, S., & Zhang, Z.(2001).
Experiments in single and multidocu-ment summarization using MEAD.
First Doc-ument Understanding Conference.Saggion, H., Bontcheva, K., & Cunningham, H.(2003).
Robust generic and query-based sum-marisation.
EACL ?03: Proceedings of thetenth conference on European chapter of theAssociation for Computational Linguistics.Salton, G., Singhal, A., Mitra, M., & Buckley, C.(1997).
Automatic text structuring and sum-marization.
Information Processing and Man-agement, 33, 193?207.Satoshi, C. N., Satoshi, S., Murata, M., Uchi-moto, K., Utiyama, M., & Isahara, H. (2001).Sentence extraction system assembling mul-tiple evidence.
Proceedings of 2nd NTCIRWorkshop (pp.
319?324).Schenker, A., Bunke, H., Last, M., & Kandel,A.
(2004).
Classification of web documentsusing graph matching.
International Journalof Pattern Recognition and Artificial Intelli-gence, 18, 475?496.Steinberger, J., & Jezek, K. (2004).
Text sum-marization and singular value decomposition.Lecture Notes in Computer Science, 245?254.Vanderwende, L., Suzuki, H., Brockett, C., &Nenkova, A.
(2007).
Beyond SumBasic: Task-focused summarization with sentence simplifi-cation and lexical expansion.
Information pro-cessing and management, 43, 1606?1618.Wong, K., Wu, M., & Li, W. (2008).
Ex-tractive summarization using supervised andsemi-supervised learning.
Proceedings of the22nd International Conference on Computa-tional Linguistics (pp.
985?992).69
