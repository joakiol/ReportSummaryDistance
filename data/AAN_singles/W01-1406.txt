A best-first alignment algorithm for automatic extraction of transfermappings from bilingual corporaArul Menezes and Stephen D. RichardsonMicrosoft ResearchOne Microsoft WayRedmond, WA 98008, USAarulm@microsoft.comsteveri@microsoft.comAbstractTranslation systems that automaticallyextract transfer mappings (rules orexamples) from bilingual corpora havebeen hampered by the difficulty ofachieving accurate alignment andacquiring high quality mappings.
Wedescribe an algorithm that uses a best-first strategy and a small alignmentgrammar to significantly improve thequality of the transfer mappingsextracted.
For each mapping,frequencies are computed and sufficientcontext is retained to distinguishcompeting mappings during translation.Variants of the algorithm are runagainst a corpus containing 200Ksentence pairs and evaluated based onthe quality of resulting translations.1 IntroductionA machine translation system requires asubstantial amount of translation knowledgetypically embodied in bilingual dictionaries,transfer rules, example bases, or a statisticalmodel.
Over the last decade, research hasfocused on the automatic acquisition of thisknowledge from bilingual corpora.
Statisticalsystems build translation models from this datawithout linguistic analysis (Brown, 1993).Another class of systems, including our own,parses sentences in parallel sentence-alignedcorpora to extract transfer rules or examples(Kaji, 1992) (Meyers, 2000) (Watanabe, 2000).These systems typically obtain a predicate-argument or dependency structure for sourceand target sentences, which are then aligned,and from the resulting alignment, lexical andstructural translation correspondences areextracted, which are then represented as a set ofrules or an example-base for translation.However, before this method of knowledgeacquisition can be fully automated, a number ofissues remain to be addressed.
The alignmentand transfer-mapping acquisition proceduremust acquire rules with very high precision.
Itmust be robust against errors introduced byparsing and sentence-level alignment, errorsintrinsic to the corpus, as well as errors resultingfrom the alignment procedure itself.
Theprocedure must also produce transfer mappingsthat provide sufficient context to enable thetranslation system utilizing these mappings tochoose the appropriate translation for a givencontext.In this paper, we describe the alignment andtransfer-acquisition algorithm used in ourmachine translation system, which attempts toaddress the issues raised above.
This systemacquires transfer mappings by aligning pairs oflogical form structures (LFs) similar to thosedescribed by Jensen (1993).
These LFs areobtained by parsing sentence pairs from asentence-aligned bilingual corpus.
(The problemof aligning parallel corpora at the sentence levelhas been addressed by Meyers (1998b) Chen(1993) and others and is beyond the scope ofthis paper).We show that alignment using a best-firststrategy in conjunction with a small alignmentgrammar improves the alignment and the qualityof the acquired transfer mappings.hacerustedinformaci?nhiperv?nculo hiperv?nculoclicdirecci?nclickHyperlink_Informationyouaddresshyperlinkde deen enunderDsub DobjModDsubDobjFigure 1a: Lexical correspondences Figure 1b: Alignment Mappingshacerustedinformaci?nhiperv?nculo hiperv?nculoclicdirecci?nclickHyperlink_Informationyouaddresshyperlinkde deen enunderDsub DobjModDsubDobj2 Logical FormA Logical Form (LF) is an unordered graphrepresenting the relations among the mostmeaningful elements of a sentence.
Nodes areidentified by the lemma of a content word anddirected, labeled arcs indicate the underlyingsemantic relations.
Logical Forms are intendedto be as independent as possible of specificlanguages and their grammars.
In particular,Logical Forms from different languages use thesame relation types and provide similar analysesfor similar constructions.
The logical formabstracts away from such language-particularaspects of a sentence as constituent order,inflectional morphology, and certain functionwords.Figure 1a illustrates the LFs for thefollowing Spanish sentence and itscorresponding English translation, which we usein example below.En Informaci?n del hiperv?nculo, haga clic enla direcci?n del hiperv?nculo.Under Hyperlink Information, click thehyperlink address.3 AlignmentWe consider an alignment of two logical formsto be a set of mappings, such that each mappingis between a node or set of nodes (and therelations between them) in the source LF and anode or set of nodes (and the relations betweenthem) in the target LF, where no nodeparticipates in more than one such mapping.
Inother words, we allow one-to-one, one-to-many,many-to-one and many-to-many mappings butthe mappings do not overlap.Our alignment algorithm proceeds in twophases.
The first phase establishes tentativelexical correspondences between nodes in thesource and target LFs.
The second phase alignsnodes based on these lexical correspondences aswell as structural considerations.
The algorithmstarts from the nodes with the tightest lexicalcorrespondence (?best-first?)
and works outwardfrom these anchor points.We first present the algorithm, and thenillustrate how it applies to the sentence-pair inFigure-1.3.1 Finding tentative lexicalcorrespondencesWe use a bilingual lexicon that merges datafrom several sources (CUP, 1995), (SoftArt,1995), (Langenscheidt, 1997), and invertstarget-to-source dictionaries to improvecoverage.
Our Spanish-English lexicon contains88,500 translation pairs.
We augment this with19,762 translation correspondences acquiredusing statistical techniques described by Moore(2001).Like Watanabe (2000) and Meyers (2000),we use a lexicon to establish initial tentativeword correspondences.
However, we have foundthat even a relatively large bilingual dictionaryhas only moderately good coverage for ourpurposes.
Hence, we pursue an aggressivematching strategy for establishing tentativeword correspondences.
Using the bilingualdictionary together with the derivationalmorphology component in our system(Pentheroudakis, 1993), we find directtranslations, translations of morphological basesand derivations, and base and derived forms oftranslations.
Fuzzy string matching is also usedto identify possible correspondences.
We havefound that aggressive over-generation ofcorrespondences at this phase is balanced by themore conservative second phase and results inimproved overall alignment quality.We also look for matches betweencomponents of multi-word expressions andindividual words.
This allows us to align suchexpressions that may have been analyzed as asingle lexicalized entity in one language but asseparate words in the other.3.2 Aligning nodesOur alignment procedure uses the tentativelexical correspondences established above, aswell as structural cues, to create affirmativenode alignments.
A set of alignment grammarrules licenses only linguistically meaningfulalignments.
The rules are ordered to create themost unambiguous alignments (?best?)
first anduse these to disambiguate subsequentalignments.
The algorithm and the alignmentgrammar rules are intended to be applicableacross multiple languages.
The rules weredeveloped while working primarily with aSpanish-English corpus, but have also beenapplied to other language pairs such as French,German, and Japanese to/from English.The algorithm is as follows:1.
Initialize the set of unaligned source andtarget nodes to the set of all source andtarget nodes respectively.2.
Attempt to apply the alignment rules in thespecified order, to each unaligned node orset of nodes in source and target.
If a rulefails to apply to any unaligned node or set ofnodes, move to the next rule.3.
If all rules fail to apply to all nodes, exit.
Nomore alignment is possible.
(Note: somenodes may remain unaligned).4.
When a rule applies, mark the nodes or setsof nodes to which it applied as aligned toeach other and remove them from the listsof unaligned source and target nodesrespectively.
Go to step 2 and apply rulesagain, starting from the first rule.The alignment grammar currently consistsof 18 rules.
Below we provide the specificationfor some of the most important rules.1.
Bidirectionally unique translation: A set ofcontiguous source nodes S and a set ofcontiguous target nodes T such that everynode in S has a lexical correspondence withevery node in T and with no other targetnode, and every node in T has a lexicalcorrespondence with every node in S andwith no other source node.
Align S and T toeach other.2.
Translation + Children: A source node Sand a target node T that have a lexicalcorrespondence, such that each child of Sand T is already aligned to a child of theother.
Align S and T to each other.3.
Translation + Parent: A source node S anda target node T that have a lexicalcorrespondence, such that a parent Ps of Shas already been aligned to a parent Pt of T.Align S and T to each other.4.
Verb+Object to Verb: A verb V1 (fromeither source or target), that has child O thatis not a verb, but is already aligned to a verbV2, and either V2 has no unaligned parents,or V1 and V2 have children aligned to eachother.
Align V1+O to V2.5.
Parent + relationship: A source node S anda target node T, with the same part-of-speech, and no unaligned siblings, where aparent Ps of S is already aligned to a parentPt of T, and the relationship between Ps andS is the same as that between Pt and T.Align S and T to each other.6.
Child + relationship: Analogous to previousrule but based on previously alignedchildren instead of parents.Note that rules 4-6 do not exploit lexicalcorrespondence, relying solely on relationshipsbetween nodes being examined and previouslyaligned nodes.3.3 Alignment ExampleIn this section, we illustrate the application ofthe alignment procedure to the example inFigure 1.
In the first phase, using the bilinguallexicon, we identify the lexical correspondencesdepicted in Figure-1a as dotted lines.
Note thateach of the two instances of hiperv?nculo hastwo ambiguous correspondences, and that whilethe correspondence from Informaci?n toHyperlink Information is unique, the reverse isnot.
Note also that neither the monolingual norbilingual lexicons have been customized for thisdomain.
For example, there is no entry in eitherlexicon for Hyperlink_Information.
This unit hasbeen assembled by general-purpose "Captoid"grammar rules.
Similarly, lexicalcorrespondences established for this unit arebased on translations found for its individualcomponents, there being no lexicon entry for thecaptoid as a whole.In the next phase, the alignment rules applyto create alignment mappings depicted inFigure-1b as dotted lines.Rule-1: Bidirectionally unique translation,applies in three places, creating alignmentmappings between direcci?n and address,usted and you, and clic and click.
These arethe initial ?best?
alignments that provide theanchors from which we will work outwards toalign the rest of the structure.Rule-3: Translation + Parent, applies next toalign the instance of hiperv?nculo that is thechild of direcci?n to hyperlink, which is thechild of address.
We leverage a previouslycreated alignment (direcci?n to address) andthe structure of the logical form to resolve theambiguity present at the lexical level.Rule-1 now applies (where previously it did not)to create a many-to-one mapping betweeninformaci?n and hiperv?nculo toHyperlink_Information.
The uniquenesscondition in this rule is now met because theambiguous alternative was cleared away bythe prior application of Rule-3.Rule-4: Verb+Object to Verb applies to rolluphacer with its object clic, since the latter isalready aligned to a verb.
This produces themany-to-one alignment of hacer and clic toclick4 Acquiring Transfer MappingsFigure-2 shows the transfer mappings derivedfrom the alignment example in Figure-1.informaci?nhiperv?nculode Hyperlink_Informationdirecci?nhiperv?nculodeaddresshyperlinkModhacerPron) clicenDsub Dobj(Noun)hacerPron) clicenDsub Dobj DobjDsubclickdirecci?n address(Pron)informaci?nhiperv?nculodeHyperlink_Inform ationunder(Verb)en(Verb)direcci?n addresshiperv?nculo hyperlinkDobjDsubclick(Noun)(Pron)Figure-2 : Transfer mappings acquired4.1 Transfer mappings with contextEach mapping created during alignment formsthe core of a family of mappings emitted by thetransfer mapping acquisition procedure.
Thealignment mapping by itself represents aminimal transfer mapping with no context.
Inaddition, we emit multiple variants, each oneexpanding the core mapping with varying typesand amounts of local context.We use linguistic constructs such as nounand verb phrases to provide the boundaries forthe context we include.
For example, thetransfer mapping for an adjective is expanded toinclude the noun it modifies; the mapping for amodal verb is expanded to include the mainverb; the mapping for a main verb is expandedto include its object; mappings for collocationsof nouns are emitted individually and as awhole.
Mappings may include ?wild card?
orunder-specified nodes, with a part of speech, butno lemma, as shown in Figure 2.4.2 Alignment Post-processingAfter we have acquired transfer mappings fromour entire training corpus, we computefrequencies for all mappings.
We use these toresolve conflicting mappings, i.e.
mappingswhere the source sides of the mapping areidentical, but the target sides differ.
Currentlywe resolve the conflict by simply picking themost frequent mapping.
Note that this does notimply that we are committed to a singletranslation for every word across the corpus,since we emitted each mapping with differenttypes and amounts of context (see section 4.1).Ideally at least one of these contexts serves todisambiguate the translation.
The conflicts beingresolved here are those mappings where thenecessary context is not present.A drawback of this approach is that we arerelying on a priori linguistic heuristics to ensurethat we have the right context.
Our future workplans to address this by iteratively searching forthe context that serves to optimallydisambiguate (across the entire training corpus)between conflicting mappings.4.2.1 Frequency ThresholdDuring post-processing we also apply afrequency threshold, keeping only mappingsseen at least N times (where N is currently 2).This frequency threshold greatly improves thespeed of the runtime system, with negligibleimpact on translation quality (see section 5.6).5 Experiments and Results5.1 Evaluation methodologyIn the evaluation process, we found that variousevaluation metrics of alignment in isolation borevery little relationship to the quality of thetranslations produced by a system that used theresults of such alignment.
Since it is the overalltranslation quality that we care about, we use theoutput quality (as judged by humans) of the MTsystem incorporating the transfer mappingsproduced by an alignment algorithm (keeping allother aspects of the system constant) as themetric for that algorithm.5.2 Translation systemOur translation system (Richardson, 2001)begins by parsing an input sentence andobtaining a logical form.
We then search thetransfer mappings acquired during alignment,for mappings that match portions of the inputLF.
We prefer larger (more specific) mappingsto smaller (more general) mappings.
Amongmappings of equal size, we prefer higher-frequency mappings.
We allow overlappingmappings that do not conflict.
The lemmas inany portion of the LF not covered by a transfermapping are translated using the same bilingualdictionary employed during alignment, or by ahandful of hard-coded transfer rules (see Section5.7 for a discussion of the contribution made byeach of these components).
Target LF fragmentsfrom matched transfer mappings and defaultdictionary translations are stitched together toform an output LF.
From this, a rule-basedgeneration component produces an outputsentence.The system provides output for every inputsentence.
Sentences for which spanning parsesare not found are translated anyway, albeit withlower quality.5.3 Training corpusWe use a sentence-aligned Spanish-Englishtraining corpus consisting of 208,730 sentencepairs mostly from technical manuals.
The datawas already aligned at the sentence-level since itwas taken from sentence-level translationmemories created by human translators using acommercial translation-memory product.
Thisdata was parsed and aligned at the sub-sentencelevel by our system, using the techniquesdescribed in this paper.
Our parser produces aparse in every case, but in each languageroughly 15% of the parses produced are ?fitted?or non-spanning.
Since we have a relativelylarge training corpus, we apply a conservativeheuristic and only use in alignment thosesentence-pairs that produced spanning parses inboth languages.
In this corpus 161,606 pairs (or77.4% of the corpus) were used.
This is asubstantially larger training corpus than thoseused in previous work on learning transfermappings from parsed data.
Table-1 presentssome data on the mappings extracted from thiscorpus using Best-First.Total Sentence pairs 208,730Sentence pairs used 161,606Number of transfer mappings 1,202,828Transfer mappings per pair 7.48Num.
unique transfer mappings 437,479Num.
unique after elim.
conflicts 369,067Num.
unique with frequency > 1 58,314Time taken to align entire corpus(on a 800MHz PC)74 minutesAlignment speed 35.6 sent/sTable-1: Best-first alignment of training corpus5.4 ExperimentsIn each experiment we used 5 human evaluatorsin a blind evaluation, to compare the translationsproduced by the test system with those producedby a comparison system.
Evaluators werepresented, for each sentence, with a referencehuman translation and with the two machinetranslations in random order, but not the originalsource language sentence.
They were asked topick the better overall translation, taking intoaccount both content and fluency.
They wereallowed to choose ?Neither?
if they consideredboth translations equally good or equally bad.All the experiments were run with ourSpanish-English system.
The test sentences wererandomly chosen from unseen data from thesame domain.
Experiment-1 used 200 sentencesand each sentence was evaluated by all raters.Sentences were rated better for one system orthe other if a majority of the raters agreed.Experiments 2-4 used 500 sentences each, buteach sentence was rated by a single rater.In each experiment, the test system was thesystem described in section 5.2, loaded withtransfer mappings acquired using the techniquesdescribed in this paper (hereafter ?Best-First?
).5.5 Comparison systemsIn the first experiment the comparison system isa highly rated commercial system, Babelfish(http://world.altavista.com).Each of the next three experiments variessome key aspect of Best-First in order to explorethe properties of the algorithm.5.5.1 Bottom UpExperiment-2 compares Best-First to theprevious algorithm we employed, which used abottom-up approach, similar in spirit to that usedby Meyers (1998a).This algorithm follows the proceduredescribed in section 3.1 to establish tentativelexical correspondences.
However, it does notuse an alignment grammar, and relies on abottom-up rather than a best-first strategy.
Itstarts by aligning the leaf nodes and proceedsupwards, aligning nodes whose child nodes havealready aligned.
Nodes that do not align areskipped over, and later rolled-up with ancestornodes that have successfully aligned.5.5.2 No ContextExperiment-3 uses a comparison algorithm thatdiffers from Best First in that it retains nocontext (see section 4.1) when emitting transfermappings.5.5.3 No ThresholdThe comparison algorithm used in Experiment-4differs from Best First in that the frequencythreshold (see section 4.2.1) is not applied, i.e.all transfer mappings are retained.ComparisonSystemNum.
sentencesBest-First ratedbetterNum.
sentencescomparisonsystem rated betterNum.
sentencesneither rated betterNet percentageimprovementBabelfish 93 (46.5%) 73 (36.5%) 34 (17%) 10.0%Bottom-Up 224 (44.8%) 111 (22.2%) 165 (33%) 22.6%No-Context 187 (37.4%) 69 (13.8%) 244 (48.8%) 23.6%No-Threshold 112 (22.4%) 122 (24.4%) 266 (53.2%) -2.0%Table-2: Translation Quality5.6 DiscussionThe results of the four experiments arepresented in Table-2.Experiment-1 establishes that the algorithmpresented in this paper automatically acquirestranslation knowledge of sufficient quantity andquality as to enable translations that exceed thequality of a highly rated traditional MT system.Note however that Babelfish/Systran was notcustomized to this domain.Experiment-2 shows that Best-Firstproduces transfer mappings resulting insignificantly better translations than Bottom-Up.Using Best-First produced better translations fora net of 22.6% of the sentences.Experiment-3 shows that retaining sufficientcontext in transfer mappings is crucial totranslation quality, producing better translationsfor a net of 23.6% of the sentences.Experiment-4 shows that the frequencythreshold hurts translation quality slightly (a netloss of 2%), but as Table-3 shows it results in amuch smaller (approx.
6 times) and faster(approx 45 times) runtime system.NummappingsTranslation speed(500 sentences)Best-First 58,314 173s (0.34s/sent)No-Threshold 359,528 8059s (17s/sent)Table-3: Translation Speed (500 sentences)5.7 Transfer mapping coverageUsing end-to-end translation quality as a metricfor alignment leaves open the question of howmuch of the translation quality derives fromalignment versus other sources of translationknowledge in our system, such as the bilingualdictionary, or the 2 hand-coded transfer rules inour system.
To address this issue we measuredthe contribution of each using a 3264-sentencetest set.
Table-4 presents the results.
The firstcolumn indicates the total number of words ineach category.
The next four columns indicatethe percentage translated using each knowledgesource, and the percentage not translatedrespectively.As the table shows, the vast majority ofcontent words get translated using transfer-mappings obtained via alignment.Our alignment algorithm does not explicitlyattempt to learn transfer mappings for pronouns,but pronouns are sometimes included in transfermappings when they form part of the contextthat is included with each mapping (see section4.1).
The 31.89% of pronoun translations thatthe table indicates as coming from alignmentfall into this category.Our algorithm does try to learn transfermappings for prepositions and conjunctions,which are represented in the Logical Form aslabels on arcs (see Figure-1).
Mappings forprepositions and conjunctions always includethe nodes on both ends of this arc.
Thesemappings may translate a preposition in thesource language to a preposition in the targetlanguage, or to an entirely different relationship,such as direct object, indirect object, modifieretc.As the table shows, the system is currentlyless successful at learning transfer mappings forprepositions and conjunctions than it is forcontent words.As a temporary measure we have 2 hand-coded transfer rules that apply to prepositions,which account for 8.4% of such transfers.
Weintend for these to eventually be replaced bymappings learned from the data.Number ofinstancesAlignment Dictionary Rules NottranslatedContent words 21,245 93.50% 4.10% 0% 2.4%Pronouns 2,158 31.89% 68.20% 0% 0%Prepositions/Conjunctions 6,640 32.00% 59.70% 8.4% 0%Table-4: Coverage of transfer mappings, dictionary & rules6 Conclusions and Future WorkWe proposed an algorithm for automaticallyacquiring high-quality transfer mappings fromsentence-aligned bilingual corpora using analignment grammar and a best-first strategy.We reported the results of applying thealgorithm to a substantially larger trainingcorpus than that used in previously reportedwork on learning transfer mappings from parseddata.We showed that this approach producestransfer mappings that result in translationquality comparable to a commercial MT systemfor this domain.We also showed that a best-first, alignment-grammar based approach produced better resultsthan a bottom-up approach, and that retainingcontext in the acquired transfer mappings isessential to translation quality.We currently rely on a priori linguisticheuristics to try to provide the right context foreach transfer mapping.
In future work, we planto use machine-learning techniques to determinethe extent of the context that optimallydisambiguates between conflicting mappings.ReferencesPeter Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer, 1993.
?Themathematics of statistical machine translation?Computational Linguistics, 19:263-312Cambridge University Press (1995), McCarthy, M.ed., Cambridge Word SelectorStanley F. Chen, 1993.
?Aligning sentences inbilingual corpora using lexical information?Proceedings of ACL 1993Karen Jensen, 1993.
?PEGASUS: Deriving argumentstructures after syntax.?
In Natural LanguageProcessing: The PLNLP Approach.
KluwerAcademic Publishers, Boston, MA.Hiroyuki Kaji, Yuuko Kida, and YasutsuguMorimoto, 1992.
?Learning Translation Templatesfrom Bilingual Text?
Proceedings of COLING1992Langenscheidt Publishers 1997, The LangenscheidtPocket Spanish DictionaryAdam Meyers, Roman Yangarber, Ralph Grishman,Catherine Macleod, and Antonio Moreno-Sandoval, 1998a.
?Deriving transfer rules fromdominance-preserving alignments?, Proceedingsof COLING 1998Adam Meyers, Michiko Kosaka and RalphGrishman, 1998b.
?A multilingual procedure fordictionary-based sentence alignment?
Proceedingsof AMTA 98Adam Meyers, Michiko Kosaka and RalphGrishman, 2000.
?Chart-based transfer ruleapplication in machine translation?
Proceedings ofCOLING 2000Robert C. Moore 2001, ?Towards a Simple andAccurate Statistical Approach to LearningTranslation Relationships among Words?Proceedings of the Workshop on Data-DrivenMachine Translation, ACL 2001Joseph Pentheroudakis and Lucretia Vanderwende1993, ?Automatically identifying morphologicalrelations in machine-readable dictionaries?
NinthAnnual conference of the University of WaterlooCenter for the new OED and Text ResearchStephen D. Richardson, William Dolan, MonicaCorston-Oliver, and Arul Menezes 2001,?Overcoming the customization bottleneck usingexample-based MT?, Workshop on Data-DrivenMachine Translation, ACL 2001SoftArt Inc (1995) Soft-Art translation dictionary.Version 7Hideo Watanabe, Sado Kurohashi, and Eiji Aramaki,2000.
?Finding Structural Correspondences fromBilingual Parsed Corpus for Corpus-basedTranslation?
Proceedings of COLING 2000
