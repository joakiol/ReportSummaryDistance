AUTOMATICALLY  EXTRACTING AND REPRESENTINGCOLLOCATIONS FOR LANGUAGE GENERATION*Frank  A .
Smad ja  tandKath leen  R .
McKeownDepar tment  of  Computer  ScienceCo lumbia  Un ivers i tyNew York, NY  10027ABSTRACTCollocational knowledge is necessary for language gener-ation.
The problem is that collocations come in a largevariety of forms.
They can involve two, three or morewords, these words can be of different syntactic ate-gories and they can be involved in more or less rigidways.
This leads to two main difficulties: collocationalknowledge has to be acquired and it must be representedflexibly so that it can be used for language generation.We address both problems in this paper, focusing on theacquisition problem.
We describe a program, Xtract ,that automatically acquires a range of collocations fromlarge textual corpora and we describe how they can berepresented in a flexible lexicon using a unification basedformalism.1 INTRODUCTIONLanguage generation research on lexical choice has fo-cused on syntactic and semantic onstraints on wordchoice and word ordering.
Colloca~ional constraints,however, also play a role in how words can co-occur inthe same sentence.
Often, the use of one word in a par-ticular context of meaning will require the use of one ormore other words in the same sentence.
While phrasallexicons, in which lexical associations are pre-encoded(e.g., \[Kukich 83\], \[Jacobs 85\], \[Danlos 87\]), allow for thetreatment of certain types of collocations, they also haveproblems.
Phrasal entries must be compiled by handwhich is both expensive and incomplete.
Furthermore,phrasal entries tend to capture rather rigid, idiomaticexpressions.
In contrast, collocations vary tremendouslyin the number of words involved, in the syntactic at-egories of the words, in the syntactic relations betweenthe words, and in how rigidly the individual words areused together.
For example, in some cases, the words ofa collocation must be adjacent, while in others they canbe separated by a varying number of other words.
*The research reported in this paper was partially sup-ported by DARPA grant N00039-84-C-0165, by NSF grantIRT-84-51438 and by ONR grant N00014-89-J-1782.tMost of this work is also done in collaboration with BellCommunication Research, 445 South Street, Morristown, NJ07960-1910In this paper, we identify a range of collocations thatare necessary for language generation, including opencompounds of two or more words, predicative relations(e.g., subject-verb), and phrasal templates represent-ing more idiomatic expressions.
We then describe howXt rac t  automatically acquires the full range of colloca-tions using a two stage statistical analysis of large do-main specific corpora.
Finally, we show how collocationscan be efficiently represented in a flexible lexicon using aunification based formalism.
This is a word based lexiconthat has been macrocoded with collocational knowledge.Unlike a purely phrasal lexicon, we thus retain the flexi-bility of word based lexicons which allows for collocationsto be combined and merged in syntactically acceptableways with other words or phrases of the sentence.
Unlikepure word based lexicons, we gain the ability to deal witha variety of phrasal entries.
Furthermore, while there hasbeen work on the automatic retrieval of lexical informa-tion from text \[Garside 87\], \[Choueka 88\], \[Klavans 88\],\[Amsler 89\], \[Boguraev & Briscoe 89\], \[Church 89\], noneof these systems retrieves the entire range of collocationsthat we identify and no real effort has been made to usethis information for language generation \[Boguraev &Briscoe 89\].In the following sections, we describe the range of col-locations that we can handle, the fully implemented ac-quisition method, results obtained, and the representa-tion of collocations in Functional Unification Grammars(FUGs) \[Kay 79\].
Our application domain is the domainof stock market reports and the corpus on which our ex-pertise is based consists of more than 10 million wordstaken from the Associated Press news wire.S INGLE WORDS TO WHOLEPHRASES:  WHAT K IND OFLEX ICAL  UNITS  ARE NEEDED?Collocational knowledge indicates which members of aset of roughly synonymous words co-occur with otherwords and how they combine syntactically.
These affini-ties can not be predicted on the basis of semantic or syn-tactic rules, but can be observed with some regularity in?
text \[Cruse 86\].
We have found a range of collocationsfrom word pairs to whole phrases, and as we shall show,252this range will require a flexible method of representa-tion.3 THE ACQUIS IT ION METHOD:XtractOpen Compounds  .
Open compounds involve unin-terrupted sequences of words such as "stock mar-ket," "foreign ezchange," "New York Stock Ez-change," "The Dow Jones average of $0 indust~-als."
They can include nouns, adjectives, and closedclass words and are similar to the type of colloca-tions retrieved by \[Choueka 88\] or \[Amsler 89\].
Anopen compound generally functions as a single con-stituent of a sentence.
More open compound exam-ples are given in figure 1. xPred icat ive  Relat ions consist of two (or several)words repeatedly used together in a similar syn-tactic relation.
These lexical relations axe harderto identify since they often correspond to inter-rupted word sequences in the corpus.
They axe alsothe most flexible in their use.
This class of collocations is related to Mel'~uk's Lexical Functions\[Mel'~uk 81\], and Benson's L-type relations \[Ben-son 86\].
Within this class, X t rac t  retrieves ubject-verb, verb-object, noun-adjective, verb-adverb, verb-verb and verb-particle predicative relations.
Church\[Church 89\] also retrieves verb-particle associations.Such collocations require a representation that al-lows for a lexical function relating two or morewords.
Examples of such collocations axe given infigure 2.
2Phrasa l  templates :  consist of idiomatic phrases con-taining one, several or no empty slots.
They axeextremely rigid and long collocations.
These almostcomplete phrases are quite representative of a givendomain.
Due to their slightly idiosyncratic struc-ture, we propose representing and generating themby simple template filling.
Although some of thesecould be generated using a word based lexicon, ingeneral, their usage gives an impression of fluencythat cannot be equaled with compositional genera-tion alone.
X t rac t  has retrieved several dozens ofsuch templates from our stock market corpus, in-eluding:"The NYSE's composite indez of all its listed com-mon stocks rose*NUMBER* to *NUMBER*""On the American Stock Ezchange the market valueindez was up*NUMBER* at *NUMBER*""The Dow Jones average of 30 industrials fell*NUMBER* points to *NUMBER*""The closely watched indez had been down about*NUMBER* points inthe first hour of trading""The average finished the week with a net loss of*NUMBER *"I All the examples related to the stock market domain havebeen actually retrieved by Xtract.2In the examples, the "~" sign, represents a gap of zero,one or several words.
The "?
*" sign means that the twowords can be in any order.In order to produce sentences containing collocations, alanguage generation system must have knowledge aboutthe possible collocations that occur in a given domain.In previous language generation work \[Danlos 87\], \[Ior-danskaja 88\], \[Nirenburg 88\], collocations are identifiedand encoded by hand, sometimes using the help of lexi-cographers (e.g., Danlos' \[Daulos 87\] use of Gross' \[Gross75\] work).
This is an expensive and time-consuming pro-cess, and often incomplete.
In this section, we describehow Xt ract  can automatically produce the full range ofcollocations described above.Xt ract  has two main components, a concordancingcomponent, Xconcord,  and a statistical component,Xstat .
Given one or several words, Xconcord  locatesall sentences in the corpus containing them.
Xstat  isthe co-occurrence compiler.
Given Xconcord's  output,it makes statistical observations about these words andother words with which they appear.
Only statisticallysignificant word pairs are retained.
In \[Smadja 89a\], and\[Smadja 88\], we detail an earlier version of X t ract  andits output, and in \[Smadja 891)\] we compare our resultsboth qualitatively and quantitatively to the lexicon usedin \[Kukich 83\].
X t rac t  has also been used for informa-tion retrieval in \[Maarek & Smadja 89\].
In the updatedversion of X t rac t  we describe here, statistical signifi-cance is based on four parameters, instead of just one,and a second stage of processing has been added thatlooks for combinations of word pairs produced in thefirst stage, resulting in multiple word collocations.Stage one- In the first phase, Xconcord  is called for asingle open class word and its output is pipeIined toXstat  which then analyses the distribution of wordsin this sample.
The output of this first stage is a listof tuples (wx,w2, distance, strength, spread, height,type), where (wl, w2) is a lexical relation betweentwo open-class words (Wx and w2).
Some resultsare given in Table 1.
"Type" represents the syn-tactic categories of wl and w2.
3.
"Distance" is therelative distance between the two words, wl and w2(e.g., a distance of 1 means w~ occurs immediatelyafter wx and a distance of - i  means it occurs imme-diately before it).
A different uple is produced foreach statistically significant word pair and distance.Thus, ff the same two words occur equally often sep-arated by two different distances, they will appeartwice in the list.
"Strength" (also computed in theearlier version of Xt ract )  indicates how strongly thetwo words are related (see \[Smadja 89a\]).
"Spread"is the distribution of the relative distance betweenthe two words; thus, the larger the "spread" themore rigidly they are used in combination to oneanother.
"Height" combines the factors of "spread"3In order to get part of speech information we use astochastic word tagger developed at AT&T Bell Laborato-ries by Ken Church \[Church 88\]253wordlstockpresidenttradeTable 1: Some binary lexical relations.word2marketvicedeficitdistance-Istrength47.01840.649630.3384spread28.529.728.436111457.1107577358.87vre  r avmcm'am; , , ,Lo?,~,c--  i~ f f t~ , , , ,~ l  , i l l l l ( ; t?1  I~ . '
lg l~: l~ i  Ig~llI,~lt:..compositebluetotaledclosing-1 12.3874 29.0682 3139.89 indexchip -1-4-1-2-1-110.078sharespricestocksvolume20.781523.046527.35416.872419.331213.51845.43739listedtakeovertakeoverstakeovertakeovers3029.368225.941523.869629.728.107129.368225.7917totaledbidhostileo~er2721.065376.874615.484583.574464.894580.393497.671084.05I ll"i~.~ l ' _ll-~,'l I~ , \ [ l l l  J i l l  ' \[ Ib'\]l~$'l\[ TypeNNNNNNNNNNNNNJNJNJNVNVNVNVNNNJiNNI NVTable 2: Concordances for "average indus~rial"On Tuesday the Dow Jones industrial average rose 26.28 points to 2 304.69.The Dow... a selling spurt that sent the DowOn Wednesday the DowThe DowThe Dow... Thursday with the Dow... swelling the DowThe rise in the DowJones industrial averageJones industrial averageJones industrial averageJones industrial averageJones industrial averageJones industrial averageJones industrial averageJones industrial averagewent up 11.36 points today.down sharply in the first hour of trading.showed some strength as ...was down 17.33 points to 2,287.36 ...had the biggest one day gain of its history ...soaring a record 69.89 points to ...by more than 475 points in the process ...was the biggest since a 54.14 point jump on ...TableThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite indexThe NYSE s composite index3: Concordances for "composite indez"of all its listed common stocks fell 1.76 to 164.13.of all its listed common stocks fell 0.98 to 164.91.of all its listed common stocks fell 0.96 to 164.93.of all its listed common stocks fell 0.91 to 164.98.of all its listed common stocks rose 1.04 to 167.08.of all its listed common stocks rose 0.76of all its listed common stocks rose 0.50 to 166.54.of all its listed common stocks rose 0.69 to 166.73.of all its listed common stocks fell 0.33 to 170.63.254open compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundopen compoundqeading industrialized countries""the Dow Jones average of .90 industriais""bear/buil market""the Dow Jones industrial average""The NYSE s composite indez of all it8 listed common stocks""Advancing/winuing/losing/declluing issues""The NASDAQ composite indez for the over the counter market""stock market""central bank'qeveraged buyout""the gross national product"'q~lue chip stocks""White House spokesman Marlin Fitztoater""takeover speculation/strategist/target/threat/attempt""takeover bid /batt le /  defense/ efforts/ flght / law /proposal / rumor"Figure 1: Some examples of open compoundsnoun adjectivenoun adjectivenoun adjectivesubject verbsubject verbsubject verbverb adverbverb objectverb objectverb particleverb verbverb verbexamples"heavy/Hght D tradlng/smoker/traffic""hlgh/low ~ fertil ity/pressure/bounce""large/small Dcrowd/retailer/client""index ~ rose"stock ~ \[rose, fell, closed, jumped, continued, declined, crashed, ...\]""advancers D \[outnumbered, outpaced, overwhelmed, outstripped\]""trade ?~ actively," mix ?~ narrowly," use ?~ widely," "watch ?~ closely"~posted ~ gain'~momentum D \[pick up, build, carry over, gather, loose, gain\]""take ~ from," "raise ~ by," "mix D with""offer to \[acquire, buy"\]"agree to \[acquire, buy"\]Figure 2: Some examples of predicative collocationsand "strength" resulting in a ranking of the twowords for their "distances".
Church \[Church 89\]produces results similar to those presented in thetable using a different statistical method.
However,Church's method is mainly based on the computa-tion of the "strength" attribute, and it does not takeinto account "spread" and "height".
As we shallsee, these additional parameters are crucial for pro-ducing multiple word collocations and distinguish-ing between open compounds (words are adjacent)and predicative relations (words can be separatedby varying distance).Stage two: In the second phase, Xtraet  first uses thesame components but in a different way.
It startswith the pairwise lexical relations produced in Stageone to produce multiple word collocations, thenclassifies the collocations as one of three classes iden-tified above, end finally attempts to determine thesyntactic relations between the words of the collo-cation.
To do this, Xtract  studies the lexical re-lations in context, which is exactly what lexicogra-phers do.
For each entry of Table 1, Xtract  callsXconcord on the two words wl and w~ to pro-duce the concordances.
Tables 2 and 3 show theconcordances (output of Xconcord) for the inputpairs: "average-industrial" end "indez-composite".Xstat then compiles information on the words sur-rounding both wl and w2 in the corpus.
This stageallows us to filter out incorrect associations suchas "blue.stocks" or "advancing-market" and replacethem with the appropriate ones, "blue chip stocks,""the broader market in the NYSE advancing is.sues."
This stage also produces phrasal templatessuch as those given in the previous ection.
In short,stage two filters inapropriate results and combinesword pairs to produce multiple word combinations.To make the results directly usable for language gen-eration we are currently investigating the use of abottom-up arser in combination with stage two inorder to classify the collocations according to syn-tactic criteria.
For example if the lexical relationinvolves a noun and a verb it determines if it is asubject-verb or a verb-object collocation.
We planto do this using a deterministic bottom up parserdeveloped at Bell Communication Research \[Abney89\] to parse the concordances.
The parser wouldanalyse ach sentence of the concordances and theparse trees would then be passed to Xstat.Sample results of Stage two are shown in Fig-ures 1, 2 and 3.
Figure 3 shows phrasal templates andopen compounds.
Xstat notices that the words "com-posite and "indez" are used very rigidly throughout thecorpus.
They almost always appear in one of the two255lexical relationcomposite-indezcomposite-indezcollocation"The NYSE's composite indez of all its listed commonstocks fell *NUMBER* to *NUMBER*""the NYSE's composite indez of all its listed commonstocks rose *NUMBER* to *NUMBER*.
"\[ "close-industrial" "Five minutes before the close the Dow Jones average of 30 industrials~as up/down *NUMBER* to/from *NUMBER*""the Dow Jones industrial average."
"average industrial""advancing-market""block- trading""cable- television""the broader market in the NYSE advancing issues""Jack Baker head of block trading in Shearson Lehman Brothers Inc.""cable television"Figure 3: Example collocations output of stage two.sentences.
The lexical relation composite-indez thus pro-duces two phrasal templates.
For the lexical relationaverage-industrial X t rac t  produces an open compoundcollocation as illustrated in figure 3.
Stage two also con-firms pairwise relations.
Some examples are given infigure 2.
By examining the parsed concordances andextracting recurring patterns, Xs ta t  produces all threetypes of collocations.4 HOW TO REPRESENT THEMFOR LANGUAGE GENERATION?Such a wide variety of lexical associations would be dif-ficnlt to use with any of the existing lexicon formalisms.We need a flexible lexicon capable of using single wordentries, multiple word entries as well as phrasal tem-plates and a mechanism that would be able to gracefullymerge and combine them with other types of constraints.The idea of a flexible lexicon is not novel in itself.
Thelexical representation used in \[Jacobs 85\] and later re-fined in \[Desemer & Jabobs 87\] could also represent awide range of expressions.
However, in this language,collocational, syntactic and selectional constraints aremixed together into phrasal entries.
This makes the lex-icon both difficnlt to use and difficult to compile.
In thefollowing we briefly show how FUGs can be successfullyused as they offer a flexible declarative language as wellas a powerful mechanism for sentence generation.We have implemented a first version of Cook, a sur-face generator that uses a flexible lexicon for express-in~ co-occurrence onstraints.
Cook  uses FUF \[Elhadad90J, an extended implementation f PUGs, to uniformlyrepresent the lexicon and the syntax as originally sug-gested by Halliday \[Halliday 66\].
Generating a sentenceis equivalent o unifying a semantic structure (LogicalForm) with the grammar.
The grammar we use is di-vided into three zones, the "sentential," the "lezical"and "the syntactic zone."
Each zone contains constraintspertaining to a given domain and the input logical formis unified in turn with the three zones.
As it is, fullbacktracking across the three zones is allowed.?
The sentential zone contains the phrasal templatesagainst which the logical form is unified first.
Asententiai entry is a whole sentence that should beused in a given context.
This context is specified bysubparts of the logical form given as input.
Whenthere is a match at this point, unification succeedsand generation is reduced to simple template filling.?
The lezical zone contains the information used tolexicalize the input.
It contains collocational infor-mation along with the semantic ontext in whichto use it.
This zone contains predicative and opencompound collocations.
Its role is to trigger phrasesor words in the presence of other words or phrases.Figure 5 is a portion of the lexical grammar usedin Cook.
It illustrates the choice of the verb to beused when "advancers" is the subject.
(See belowfor more detail).?
The syniacgic zone contains the syntactic grammar.It is used last as it is the part of the grammar en-suring the correctness of the produced sentences.An example input logical form is given in Figure 4.
Inthis example, the logical form represents he fact that onthe New York stock exchange, the advancing issues (se-mant ic  representation r sere-R: c:winners) were ahead(predicate c:lead)of the losing ones (sem-R: c:losers)andthat there were 3 times more winning issues than losingones ratio).
In addition, it also says that this ratio isof degree 2.
A degree of 1 is considered as a slim leadwhereas a degree of 5 is a commanding margin.
Whenunified with the grammar, this logical form produces thesentences given in Figure 6.As an example of how Cook uses and merges co-occurrence information with other kind of knowledgeconsider Figure 5.
The figure is an edited portion ofthe lexical zone.
It only includes the parts that are rel-evant to the choice of the verb when "advancers" is thesubject.
The lex  and sem-R attributes pecify the lex-eme we are considering ("advancers") and its semanticrepresentation (c:winners).The semantic ontext (sere-context) which points tothe logical form and its features will then be used in order256l og ica l - fo rmpredicate-name = p : leadleaders  = \[ sem-R L ra t iot ra i le rs: c : w inners  \]J : 3sem-R : c : losers \]: ra t io  ---- Idegree  = 2Figure 4: LF: An example logical form used by Cooko , ,  ?
??
oool ex  = "advancer"sam-R = c:~oinnerssem-context = <logical- form>OO010eo ,osem-contextSV-co l locates  =predicate-name = p:  lead \]degree = 2l ex  ---- "o.u~nurn, ber" /l ex  = "lead"l ex  = "finish"lex = "hold"l ex  = "~eept'l ex  = "have", , ?sem-contextSV-col locates =predicate-name : p:lead= degree : 4lex : U?verp?~er" 1l ex  = "outstrip"lex : "hold"lex : "keel'?Figure 5: A portion of the lexical grammar showing the verbal collocates of "advancers".
"Advancers outnumbered declining issues by a margin of 3 4o 1.
""Advancers had a slim lead over losing issues wi~h a margin of 3 4o 1.
""Advancers kep~ a slim lead over decliners wi~h a margin of 3 ~o 1"Figure 6: Example sentences that can be generated with the logical form LF257to select among the alternatives classes of verbs.
In thefigure we only included two alternatives.
Both are rela-tive to the predicate p : lead but they axe used with dif-ferent values of the degree attribute.
When the degree is2 then the first alternative containing the verbs listed un-der SV-colloca~es (e.g.
"outnumber") will be selected.When the degree is 4 the second alternative contain-ing the verbs listed under SV-collocal;es (e.g.
"over-power") will be selected.
All the verbal collocates shownin this figure have actually been retrieved by Xtract  ata preceding stage.The unification of the logical form of Figure 4 withthe lexical grammar and then with the syntactic gram-mar will ultimately produce the sentences shown in Fig-ure 6 among others.
In this example, the sentencial zonewas not used since no phrasal template expresses itssemantics.
The verbs selected are all listed under theSV-collocates of the first alternative in Figure 5.We have been able to use Cook to generate severalsentences in the domain of stock maxket reports usingthis method.
However, this is still on-going reseaxch andthe scope of the system is currently limited.
We areworking on extending Cook's lexicon as well as on de-veloping extensions that will allow flexible interactionamong collocations.5 CONCLUSIONIn summary, we have shown in this paper that thereaxe many different ypes of collocations needed for lan-guage generation.
Collocations axe flexible and they caninvolve two, three or more words in vaxious ways.
Wehave described a fully implemented program, Xtract,that automatically acquires uch collocations from largetextual corpora and we have shown how they can berepresented in a flexible lexicon using FUF.
In FUF, co-occurrence constraints axe expressed uniformly with syn-tactic and semantic onstraints.
The grammax's functionis to satisfy these multiple constraints.
We are currentlyworking on extending Cook as well as developing a fullsized from Xtract 's  output.ACKNOWLEDGMENTSWe would like to thank Kaxen Kukich and the ComputerSystems Research Division at Bell Communication Re-search for their help on the acquisition part of this work.References\[Abney 89\] S. Abney, "Parsing by Chunks" in C.
Tenny~ed., The MIT Parsing Volume, 1989, to appeax.\[Amsler 89\] R. Amsler, "Research Towards the Devel-opment of a Lezical Knowledge Base for NaturalLanguage Processing" Proceedings of the 1989 SI-GIR Conference, Association for Computing Ma-\[Benson 86\] M. Benson, E. Benson and R. Ilson, Lezi-cographic Description of English.
John BenjaminsPublishing Company, Philadelphia, 1986.\[Boguraev & Briscoe 89\] B. Boguraev & T. Briscoe, inComputational Lezicography for natural languageprocessing.
B. Boguraev and T. Briscoe editors.Longmans, NY 1989.\[Choueka 88\] Y. Choueka, Looking for Needles in aHaystack.
In Proceedings of the RIAO, p:609-623,1988.\[Church 88\] K. Church, A Stochastic Par~s Program andNoun Phrase Parser for Unrestricted Tezt In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing, Austin, Texas, 1988.\[Church 89\] K. Church & K. Hanks, Word AssociationNorms, Mutual Information, and Lezicography.
InProceedings of the 27th meeting of the Associ-ation for Computational Linguistics, Vancouver,B.C, 1989.\[Cruse 86\] D.A.
Cruse, Lezical Semantics.
CambridgeUniversity Press, 1986.\[Danlos 87\] L. Danlos, The linguistic Basis of TeztGeneration.
Cambridge University Press, 1987.\[Desemer & Jabobs 87\] D. Desemer & P. Jacobs,FLUSH: A Flezible Lezicon Design.
In proceedingsof the 25th Annual Meeting of the ACL, StanfordUniversity, CA, 1987.\[Elhadad 90\] M. Elhadad, Types in Functional Unifica-tion Grammars, Proceedings of the 28th meetingof the Association for Computational Linguistics,Pittsburgh, PA, 1990.\[Gaxside 87\] R. Gaxside, G. Leech & G. Sampson, edi-tors, The computational Analysis of English, a cor-pus based approach.
Longmans, NY 1987.\[Gross 75\] M. Gross, Mdthodes en Syntaze.
Hermann,Paxis, France, 1975.\[Halliday 66\] M.A.K.
Halliday, Lezis as a LinguisticLevel.
In C.E.
Bazell, J.C. Catford, M.A.K Hal-liday and R.H. Robins (eds.
), In memory of J.R.Firth London: Longmans Linguistics \]la Libraxy,1966, pp: 148-162.\[Iordanskaja88\] L. Iordanskaja, R. Kittredge, A.Polguere, Lezical Selection and Paraphrase in aMeaning-Tezt Generation Model Presented at thefourth International Workshop on Language Gen-eration, Catalina Island, CA, 1988.\[Jacobs 85\] P. Jacobs, PHRED: a generator for natu-ral language interfaces, Computational Linguis-tics, volume 11-4, 1985\[Kay 79\] M. Kay, Functional Grammar, in Proceedingsof the 5th Meeting of the Berkeley Linguistic So-ciety, Berkeley Linguistic Society, 1979.\[Klavans 88\] J. Klavans, "COMPLEX: a computationallezicon for natural anguage systems."
In proceed-ing of the 12th International Conference on Corn-chinery.
Cambridge, Ma, June 1989.258putational Linguistics, Budapest, Hungary, 1988.\[Kukich 83\] K. Kukich, Knowledge-Based Report Gen-eration: A Technique for Automatically Gener-ating Natural Language Reports from Databases.Proceedings of the 6th International ACM SIGIRConference, Washington, DC, 1983.\[Maarek & Smadja 89\] Y.S Maarek & F.A.
Smadja, FullTezt Indezing Based on Lezical Relations, An Ap.plication: Software Libraries.
Proceedings of the12th International ACM SIGIR Conference, Cam-bridge, Ma, June 1989.\[Mel'~uk 81\] I.A Mel'euk, Meaning-Tezt Models: a Re-cent Trend in Soviet Linguistics.
The annual re-view of anthropology, 1981.\[Nirenburg 88\] S. Nirenburg et al, Lezicon building innatural language processing.
In program and ab-stracts of the 15 th International ALLC, Confer-ence of the Association for Literary and LinguisticComputing, Jerusalem, Israel, 1988.\[Smadja 88\] F.A.
Smadja, Lezical Co-occurrence: TheMissing link.
In program and abstracts of the15 th International ALLC, Conference of the As-sociation for Literary and Linguistic Computing,Jerusalem, Israel, 1988.
Also in the Journal forLiterary and Linguistic computing, Vol.
4, No.
3,1989, Oxford University Press.\[Smadja 89a\] F.A.
Smadja, Microcoding the Lezicon forLanguage Generation, First International Work-shop on Lexical Acquisition, IJCAI'89, Detroit,Mi, August 89.
Also in "Lezical Acquisition: Usingon-line resources to build a lezicon", MIT press,Uri Zeruik editor, to appear.\[Smadja 89b\] F.A.
Smadja, On the Use of Flezible Col-locations for Language Generation.
Columbia Uni-versity, technical report, TR# CUCS-507-89.259
