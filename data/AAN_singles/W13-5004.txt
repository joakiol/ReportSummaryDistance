Proceedings of the TextGraphs-8 Workshop, pages 20?28,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsReconstructing Big Semantic Similarity NetworksAi He, Shefali Sharma?Information Sciences InstituteUniversity of Southern California4676 Adminralty WayMarina del Rey, CA 90292{aihe|sharma}@isi.eduChun-Nan Hsu?,?
?Division of Biomedical InformaticsDepartment of MedicineUniversity of California, San DiegoLa Jolla, CA 92093chunnan@ucsd.eduAbstractDistance metric learning from high (thou-sands or more) dimensional data with hun-dreds or thousands of classes is intractable butin NLP and IR, high dimensionality is usu-ally required to represent data points, suchas in modeling semantic similarity.
This pa-per presents algorithms to scale up learningof a Mahalanobis distance metric from a largedata graph in a high dimensional space.
Ournovel contributions include random projectionthat reduces dimensionality and a new objec-tive function that regularizes intra-class andinter-class distances to handle a large numberof classes.
We show that the new objectivefunction is convex and can be efficiently op-timized by a stochastic-batch subgradient de-scent method.
We applied our algorithm totwo different domains; semantic similarity ofdocuments collected from the Web, and phe-notype descriptions in genomic data.
Exper-iments show that our algorithm can handlethe high-dimensional big data and outperformcompeting approximations in both domains.1 IntroductionAccording to Yang (2006), distance metric learninglearns a distance metric from data sets that consistsof pairs of points of the same or different classeswhile at the same time preserving the adjacency re-lations among the data points.
Usually, it is easierto let the user label whether a set of data is in thesame class than directly assign a distance betweeneach pair or classify whether a pair of data pointsis a match or not.
Learning a good distance met-ric in the feature space is essential in many real-world NLP and IR applications.
For example, Webnews article clustering applying hierarchical cluster-ing or k-means requires that the distance betweenthe two feature vectors extracted from the news ar-ticles faithfully reflect the semantic similarity be-tween them for these algorithms to perform well.Studies on distance metric learning over the pastfew years show that the learned metric can outper-form Euclidean distance metric.
The constraints oftraining examples in learning usually comes fromthe global or local adjacency information.
Datapoints with the same class labels are supposed to beconnected while those with different classes labelsdisconnected.
Supervised algorithms aim to learnthe distance metric to make the adjacency relation-ships in the training examples preserved.One of the most common approaches to distancemetric learning is to learn a Mahalanobis distancemetric.
Example algorithms to learn a Mahalanobisdistance metric include (Xing et al 2002; Gold-berger et al 2004; Shaw et al 2011).A common limitation shared by these algorithmsis that they fail to scale up to high dimensional datasets.
When those algorithms run on high dimen-sional data sets, they usually run out of memory.However, many NLP applications depend on tens ofthousands of features to perform well.
Dimension-ality reduction and approximation have been sug-gested, but they usually degrade performance.
Otherissues occur when the data sets consists of a largenumber of disjoint classes.
In this case, the learneddistance metric must map the data points to a spacewhere the data points cluster unevenly into a large20number of small groups, which makes the learningproblem harder and may require special treatments.In this paper, we present a new scalable approachto distance metric learning that addresses the scal-ability issues mentioned above.
To deal with highdimensionality, one approach is to factorize the met-ric matrix in the Mahalanobis distance metric intolow-rank matrix.
This reduces the number of pa-rameters that must be learned during the learningphase.
However, the learning problem becomes non-convex.
Different initializations may result in dras-tically different performance due to local optima.We solve this problem by introducing random pro-jection, which projects data points to a low dimen-sional space before learning a low dimensional full-rank metric matrix.
We show that this strategy notonly is more robust than the low-rank approxima-tion, but also outperforms the Principal ComponentAnalysis (PCA), a common approach to dimension-ality reduction.Another contribution that our approach offers isnew regularization terms in the objective functionof learning.
The new terms specify that one shouldlearn to minimize the distance for data points in thesame classes and maximize those in different ones.We found that minimization but not maximizationwould lead to the best performance, so we kept onlythe minimization term.We evaluated our new approach with data setsfrom two problem domains.
One domain is aboutlearning semantic similarity between Web pages.This domain was studied in (Shaw et al 2011) andinvolves moderately high dimensional data sets ofbag-of-words.
The other is about matching seman-tically related phenotype variables across differentgenome-wide association studies (GWAS) (Hsu etal., 2011).
This problem domain requires extremelyhigh dimensional data for a learner to perform well.Our experimental results show that our new algo-rithm consistently outperform the previous ones inboth the domains.2 Distance Metric LearningLet X ?
Rd?n be the feature matrix of input datapoints.
For any two data points xi, xj ?
Rd?1 inX,the (square of) the Mahalanobis distance between xiand xj is defined asDi,jM = (xi ?
xj)?M(xi ?
xj),where M ?
Rd?d is a metric matrix.
The distanceis always non-negative because M is required to bepositive semidefinite (PSD).Xing (2002) used semidefinite programming tolearn a Mahalanobis distance metric for clustering.It was a convex optimization problem, which al-lowed them to derive local optima free algorithms.Weinberger (2005) learned a Mahalanobis distancemetric for the k-nearest neighbor classifier by main-taining a margin between data points in differentclasses, i.e., enforcing the neighbors of the sameclass to be closer than all others.
As in the sup-port vector machines, the learning problem was re-duced to convex optimization based on hinge loss.Yang (2006) presented a comprehensive survey ondistance metric learning.Recently, Shaw et al(2011) followed the preced-ing approaches but reformulated the problem, as aninstance of the on-line learning algorithm PEGA-SOS (Shalev-Shwartz et al 2011), albeit a com-plex construction.
In a way, it scaled up the tradi-tional metric learning method to a larger amount ofdata points.
They also reformulated the margin men-tioned above as triplets over the data set and clarifythe derivation of the objective function.
Each train-ing example used here is a triplet (xi, xj , xk), con-sisting of a pair xi and xj in the same class and xkthat is in a different class.
Learning in this case en-sures that the learned distance between xi and xj isless than their distance to xk.
In comparison, onemay formulate the distance learning problem as bi-nary classification, where the objective is to mini-mize the match probability of data point pairs in dif-ferent classes and maximize those in the same ones.Since there will always be much more data pointpairs in different classes than those in the same ones,this formulation always lead to an unbalanced clas-sification problem.3 Strategies for Scaling Up LearningShaw et al(2011) suggested various strategies toscale up the algorithm for high dimensional multi-class data.
In this section, we will review thesestrategies and their weaknesses, and then, presentour approach that addresses these weaknesses.213.1 Dimensional ReductionFour strategies were suggested to handle the high di-mensional data sets:?
Diagonal Approximation,?
Principal Component Analysis (PCA),?
Low Rank Decomposition,?
Random Projection.Diagonal approximation requires the metric ma-trix M to be diagonal, thus it consists of only d pa-rameters to learn, instead of d?
d. It indeed shrinksthe number of parameters to learn, but also ignoresthe feature-feature interaction and might harm theexpressiveness of the model.Dimensionality reduction using PCA as a prepro-cessing step is a common way to deal with high di-mensional data.
However, it does not always worksatisfactorily, especially when the data set does nothave an apparent intrinsic low dimensional structure.It usually fails to perform well for those data sets.Shaw et al(2011) suggested a low-rank decom-position to scale up distance metric learning.
Theidea is to decompose M into L?L, where L is ar ?
d matrix and r  d is a predefined low-rankdegree.
This approach reduces computational costbut results in a non-convex problem that suffers fromlocal minima.
Previously, low-rank decompositionhas been proposed for other machine learning prob-lems.
For example, Rennie et al(2005) applied itto scale up Maximum Margin Matrix Factorization(MMMF) (Srebro et al 2004).
Originally, MMMFwas formulated as a convex semi-definite program-ming (SDP) problem and solved by a standard SDPsolver, but it is no longer applicable for the non-convex low-rank decomposition.
Therefore, theysolved the non-convex problem by Conjugate Gradi-ent (CG) (Fletcher and Reeves, 1964), but still thereis no guarantee that CG will converge at a globalminimum.Our choice is to use random projection L?RL,where L is a random r ?
d (r  d) matrix, withall of its element in (0, 1).
Random projection the-ory has been developed by Johnson and Linden-strauss(1984).
The theory shows that a set of npoints in a high dimensional (d) Euclidean spacecan be mapped into a low-dimensional (r  d) Eu-clidean space such that the distance between any twopoints will be well-persevered (i.e., changes by onlya tiny factor  if r is greater than a function of n and).
Let R be a r ?
r PSD matrix to be learned fromdata.
Distance between xi and xj becomesDi,jR = (xi ?
xj)?L?RL(xi ?
xj).There are two possible strategies to generate arandom projection matrix L. One is completely ran-dom projection, where all elements are generated in-dependently; the other one is orthonormal randomprojection, which requires that Lr?d be a matrixwith r orthonormal random column vectors.
TheGram-Schmidt process (Golub and Van Loan, 1996)generates such a matrix, in which one starts by gen-erating a random column and then the next columns,though generated randomly, too, must be orthonor-mal with regard to other columns.
In both strategies,all of the elements must be in (0, 1).Consider L to be a matrix which compresses xiwith dimension d by vi = Lxi with dimension r.Hence,Di,jR = (vi ?
vj)?R(vi ?
vj) (1)This distance metric can be learned by searching forR that minimizes the following objective function:F(R) =1|S|?
(i,j,k)?SZ+(Di,jR ?Di,kR + ?
), (2)where Z+(x) is the hinge loss function.
It can beshown that this new objective function is convex.
Asin Shaw et al the training examples are given as ahinge loss function over triplets.S = {(i, j, k)|Aij = 1,Aik = 0}is the set of all triplets where A is the adjacencymatrix of X. ?
is a predefined constant margin.The hinge loss function will penalize a candidate Rwhen the resulting distance between xi and xj plusthe margin is greater than the distance between xiand xk.3.2 Intra and Inter-Class DistanceOne challenge in distance metric learning is deal-ing with data sets that can be clustered into a large22number of distinct classes.
The hinge loss term inEq.
(2) does not consider this because it only keepsa margin of data points in one class against pointsin other classes.
In our approach, we would like tolearn a distance metric such that the entire set datapoints in the same class are close to each other andaway from those in other classes.
This idea is real-ized by adding additional regularization terms to theobjective function.
These new regularization termsare proved to be useful to handle problem domainswhere data points form a large number of mutually-exclusive classes.The formula for intra-class distance regularizationis given asI1(R) =1|S1|?
(i,j)?S1Di,jR , S1 = {(i, j)|Aij = 1},(3)while the formulation for inter-class distance regu-larization isI2(R) =1|S2|?
(i,k)?S2Di,kR , S2 = {(i, k)|Aik = 0}.
(4)3.3 AlgorithmCombining the regularization, the hinge loss, intra-class and the inter-class item, we get the completeobjective function as Eq.
(5).F(R) =?2?R?2F+1|S|?
(i,j,k)?SZ+(Di,jR ?Di,kR + ?)+1|S1|?
(i,j)?S1Di,jR ?1|S2|?
(i,k)?S2Di,kR(5)It is also possible to assign weights to terms above.According to Vandenberghe (1996), Eq.
(5) can beexpressed as a convex semidefinite programmingproblem.
By construction, Eq.
(5) can also be re-formulated as an instance of PEGASOS algorithm,which basically employs a sub-gradient descent al-gorithm to optimize with a stochastic batch selec-tion, and a smart step size selection.
The sub-gradient of F in terms of R is then:?F = ?R+1|S+|?(i,j,k)?S+LXC(i,j,k)X?L?+1|S1|?(i,j)?S)1LXC1(i,j)X?L??1|S2|?
(i,k)?S2LXC2(i,k)X?L?,S+ = {(i, j, k)|Di,jR + ?
?Di,kR > 0}(6)Here the sparse symmetric matrix C is defined suchthatC(i,j,k)jj = C(i,j,k)ik = C(i,j,k)ki = 1,C(i,j,k)ij = C(i,j,k)ji = C(i,j,k)kk = ?1,and zero elsewhere.
Similarly, C1 is given byC1(i,j)ii = C1(i,j)jj = 1,C1(i,j)ij = C1(i,j)ji = ?1,and zero elsewhere.
C2 isC2(i,k)ii = C2(i,k)kk = 1,C2(i,k)ik = C2(i,k)ki = ?1,and zero elsewhere.
It is easy to verify thattr(C(i,j,k)X?L?RLX) = Di,jR ?Di,kR .The derivation is from (Petersen and Pedersen,2006):?tr(C(i,j,k)X?L?RLX)?R= LXC(i,j,k)X?L?Using the same method for intra-class and inter-class terms, the subgradient ofF atR can be derivedas?F = ?R+ LX???(i,j,k)?S+C(i,j,k)+1|S1|?(i,j)?S1C1(i,j)?1|S2|?(i,k)?S2C2(i,k)??X?L?.
(7)According to the PEGASOS algorithm, insteadof using all elements in S, S1 and S2 to optimizeF(R), we randomly sample subsets of S, S1 and S2with size of B, B1 and B2 in each iteration.
Thefull detail of the algorithm is given as procedureLEARN METRIC.23procedure LEARN METRIC(A ?
Bn?n, X ?
Bn?d,?, S, S1, S2, B, B1, B2, T , ?)L?
rand(r, d)R1 ?
zeros(r, r)for t?
1?
T do?t ?
1?tC,C1,C2 ?
zeros(n, n)for b?
1 to B do(i, j, k)?
Sample from Sif Di,jR ?Di,kR + ?
?
0 thenCjj ?
Cjj + 1, Cik ?
Cik + 1Cki ?
Cki + 1, Cij ?
Cij + 1Cji ?
Cji + 1, Ckk ?
Ckk + 1end ifend forfor b?
1 to B1 do(i, j)?
Sample from S1C1,ii ?
C1,ii + 1, C1,jj ?
C1,jj + 1C1,ij ?
C1,ij ?
1, C1,ji ?
C1,ji ?
1end forfor b?
1 to B2 do(i, k)?
Sample from S2C2,ii ?
C2,ii + 1, C2,kk ?
C2,kk + 1C2,ik ?
C2,ik ?
1, C2,ki ?
C2,ki ?
1end for?t ?
?R+ LX(C+C1 ?C2)X?L?Rt+1 ?
Rt ?
?t?tRt+1 ?
[Rt+1]+ Optional PSD projectionend forreturn L, RTend procedure4 Experimental ResultsWe applied our approach in two different problemdomains.
One involves a small amount of datapoints with moderately high dimensions (more than1,000 and less than 10,000); the other involves alarge number of data points with very high dimen-sions (more than 10,000).
The results show that ourapproach can perform well in both cases.4.1 Wikipedia ArticlesIn this domain, the goal is to predict seman-tic distances between Wikipedia documents about?Search Engine?
and ?Philosophy Concept?.
Thedata sets are available from Shaw et al(2011).They manually labeled these pages to decide whichpages should be linked, and extracted bag-of-wordsfeatures from Web documents after preprocessingsteps.
Each data set forms a sub-network of all re-lated documents.The problem here is to learn the metric matri-ces L?RL according to the sub-network describedabove.
The random projection matrix L was ran-domly initialized in the beginning and R was ob-tained by optimization, as described in Section 3.Tables 1 shows the statistics about the Wikipediadocuments data sets.Table 1: Wikipedia pages datasetData set n m dSearch Engine 269 332 6695Philosophy Concept 303 921 6695In this table, n denotes the number of data points, m,the number of edges, and d, feature dimensionality.We split this data set 80/20 for training and test-ing, where 20% of the nodes are randomly chosenfor evaluations.
The remaining 80% of data wereused in a five-fold cross-validation to select the bestperforming hyper-parameters, e.g., ?
in Eq.
(5).
Wethen trained the model with these hyper-parameters.With the held-out evaluation data, we applied thelearned distance metric to compute the distance be-tween each pair of data points.
Then we ranked alldistances and measured the quality of ranking usingthe Receiver Operator Characteristic (ROC) curve.The area under the curve (AUC) was then used tocompare the performance of various algorithms.The list below shows the alternatives of the ob-jective functions used in the algorithms comparedin the experiment.
Note that HL denotes the hingeloss term, parameterized by either M, the full-rankmetric matrix, L, the low-rank approximation ma-trix, or R, dimensionally-reduced metric matrix af-ter random projection.
I1 denotes the intra-classdistances as given in Eq.
(3) and I2 inter-class dis-tances in Eq.
(4).
Algorithms A, B, C and D arediagonal approximation.
Algorithm E implementsthe low rank decomposition with reduced dimensionr = 300.
Algorithms F, G, H, and I are variations ofour approach with combinations of random projec-tion and/or intra/inter-class regularization.
The re-duced dimension of the new feature space was also300 and R is a full matrix.
Algorithm J uses PCAto reduce the dimension to 300 and M here is a fullmatrix.In all algorithms, elements in the random projec-24tion matrix L were generated at random indepen-dently except for algorithm L, where L was gener-ated such that its columns are orthonormal.A ?2 ?M?2F +HL(M)B ?2 ?M?2F +HL(M) + I1(M)C ?2 ?M?2F +HL(M)?
I2(M)D ?2 ?M?2F +HL(M) + I1(M)?
I2(M)E ?2 ?L?2F +HL(L)F ?2 ?R?2F +HL(R)G ?2 ?R?2F +HL(R) + I1(R)H ?2 ?R?2F +HL(R)?
I2(R)I ?2 ?R?2F +HL(R) + I1(R)?
I2(R)J ?2 ?M?2F +HL(M) (PCA dimension reduction)L ?2 ?R?2F +HL(R) + I1(R) (orthonormal projection)Table 2: Performance Comparison on Wikipedia Docu-mentsSearch Engine Philosophy ConceptA 0.7297 0.6935B 0.6169 0.5870C 0.5817 0.6808D 0.6198 0.6704E* 0.6952 0.4832F 0.6962 0.6849G 0.7909 0.7264H 0.5744 0.6903I 0.5984 0.6966J 0.3509 0.4525L 0.7939 0.7174* Algorithm E performed unstably in this experimenteven when all initial hyper-parameters were the same.The running time for combined training and eval-uation for A, B, C, D was around 15 seconds(s), forE about 300s, for F, G, H and I 200s and J 300s.All ran on a MacBook Pro notebook computer with8GB of memory.
Learning full-rank M is impossi-ble because it ran out of memory.Table 2 shows that algorithm G performs betterthan others in terms of the AUC metric, suggestingthat the distances of two points in the same class islikely to be small and similar to some degree whilethe ones in the different classes vary a lot, as wespeculated.
Algorithm A also performs well proba-bly because the bag-of-words features tend to be in-dependent of each other and lack for word-to-word(feature-to-feature) interactions.
As a result, diago-nal approximation is sufficient to model the seman-tic similarity here.
The low rank approximation al-gorithm E is unstable.
Different trial runs resultedin drastically different AUCs.
The AUC reportedhere is the best observed, but still not the best com-pared to other algorithms.
In contrast, random pro-jection algorithms rarely suffer this problem.
PCAwith dimensionality reducing to 300 hurt the perfor-mance significantly.
This might be because there istoo much information loss for such a low dimension-ality, compared to the original one.
However, usingrandom L to project the feature vectors to 300 di-mensions did not seem to have the same problem,according our results of algorithm F.Comparing different strategies to generate therandom projection matrix L, we found that algo-rithms F and L basically performs similarly, yet vari-ations of the performance of algorithm F in differenttrials are slightly higher than L, though the varia-tions for both algorithms are negligible.4.2 Phenotype SimilarityThe second dataset comes from a project supportedby NIH, with the objective of matching semanticallyrelated phenotype variables across studies.
For train-ing, phenotype variables that are semantically sim-ilar are categorized to be in the same class.
Thedata set that we used here contained annotated datafrom the harmonization effort in the CARe (the Can-didate Gene Association Resource) consortium, ledby Prof. Leslie Lange from the University of NorthCarolina.
This data set is comprised of 3,700 phe-notypes that were manually classified into 160 cate-gories.
We note that previous works in distance met-ric learning usually used test data sets with less than30 classes or categories.A phenotype variable is given as a tuple of a con-dition description and an expert-assigned category.For example251.
?
Forced Vital .
.
.. .
.
Capacity (Litres),Forced Expiratory Volume ?2.
?
MAX MID-EXPIR?Y FLOW RATE .
.
.. .
.
FR MAX CURVE,Forced Expiratory Volume ?3.
?
Age at natural menopause,Menopause ?If we regard condition descriptions as data pointsand expert-assigned categories as class labels, eachtuple above can be considered to be a node with alabel in a graph.
Hence, nodes with the same la-bels should be a match (be linked) to form a se-mantic network.
For example, phenotype 1 and 2forms a match because they have the same category?
?Forced Expiratory Volume.?
?Previously, we applied the Maximum Entropymodel (MaxEnt) to predict the match on pairs, whichwere labeled according to the annotated data by con-sidering variables in the same category as semanti-cally similar (i.e., a match), and those across cate-gories as dissimilar (Hsu et al 2011).
We extractedfeatures from each pair, such as whether a keywordappear in both phenotype descriptions, and trained aMaxEnt model to predict whether a given pair is amatch.
To evaluate the performance, we split vari-ables into training and test sets and paired variableswithin each set as the training and test data sets.After careful observation of the descriptions ofthe phenotype variables, we found that they are ingeneral short and non-standardized.
To standardizethem, and detect their semantic similarity, we ex-panded the descriptions using UMLS (Unified Med-ical Language System, http://www.nlm.nih.gov/research/umls/).
We search UMLS witha series of n-grams constructed from the variabledescriptions.
For example, for a variable descrip-tion: ?Age Diagnosed Coronary Bypass?.
The n-gram words will be ?age diagnosed coronary by-pass?, ?age diagnosed coronary?, ?diagnosed coro-nary bypass?, ?age diagnosed?, ?diagnosed coro-nary?, ?coronary bypass?.
We query UMLS for con-cepts corresponding to these n-grams.
The defini-tions thus returned are appended to the original vari-able description.
We reused the feature set describedin (Hsu et al 2008) for the BioCreative 2 GeneMention Tagging task, but removed all char-gramfeatures.
This features set was carefully designedand proved to be effective for the gene mention tag-ging task, but results in a very high dimensional dataset.
Note that this feature set is different from theone used in our previous work (Hsu et al 2011).To make the configurations of our experiment ex-actly comparable with what described in Sharma etal.
(2012), we modified the algorithms to be pair-wise oriented.
We randomly divided the annotatedphenotype variable description data into three foldsand created pairs for this experiment.
The statis-tics of these folds is shown in Table 3.
We trainedour models on connected and disconnected trainingpairs in two folds and then evaluated with the pairsin the other fold.
Again, after we used the trainedmodel to predict the distances of all test pairs, weranked them and compared the quality of the rank-ing using the ROC curves as described earlier.
Theevaluation metric is the average of their AUC scoresfor the three folds.Table 3: Phenotype datasetFold train+ train?
test+ test?Fold1 98,964 2,824,398 24,550 705,686Fold2 99,607 2,823,755 25,386 704,850Fold3 98,013 2,825,349 23,892 706,344a.
The number of data points is 2,484 and dimension hereis 18,919.b.
In this table, train+ denotes number of connected (pos-itive) pairs in training data, train?
number of discon-nected (negative) pairs in training data, test+ number ofconnected pairs in testing data, test?
number of discon-nected pairs in testing data.We compared algorithms A, E, G, H, I, J and Llisted earlier with the same configurations exceptthat the reduced dimensionality r = 500.
Perfor-mance of the MaxEnt algorithm was also reportedas K. Their AUC scores are shown in Table 4.Training and evaluation processes took each foldfor algorithm A around 500 seconds(s) , for algo-rithm E around 1400s , for algorithms F, G, H, I andL around 900s , for J 1400s, and for K 1200s.The experimental results shows that random pro-jection (algorithm F) outperformed diagonal approx-imation (algorithm A) by reserving feature-featureinteractions with merely 500 dimensions, compared26Table 4: Performance Comparison on Phenotype DataAUC1 AUC2 AUC3 AUC*A 0.7668 0.7642 0.7627 0.7646E* 0.9612 0.9617 0.9689 0.9639F 0.8825 0.8953 0.8684 0.8820G 0.9461 0.9545 0.9293 0.9433H 0.7149 0.7272 0.7299 0.7240I 0.8930 0.8888 0.8809 0.8876J 0.7582 0.7357 0.7481 0.7473K 0.8884 0.9107 0.8996 0.8996L 0.9505 0.9580 0.9527 0.9537AUC1,2,3 denote the AUC performance of Fold1,2,3 re-spectively.
AUC* is the average AUC score.
* Results of algorithm E varied wildly in each running.For example, using Fold1 with a fixed hyper-parametersetting, its AUC scores were 0.9612, 0.9301, 0.9017 and0.8920 in different trails, respectively.to the original dimensionality of nearly 19K.
Again,the best performer is algorithm G, which combinesrandom projection with intra-class regularization.The intra-class term was beneficial but the inter-class term was not.
One speculation is that the dis-tance between data points in the same class maybe confined in a small range while those in dif-ferent classes may vary widely.
Maximizing inter-class distance here might have distorted the learnedfeature space.
The low rank decomposition (algo-rithm E) behaved unstably in this data sets.
The re-sult shown here is the best-chosen one.
In fact, thenon-convex low-rank decomposition results in dras-tically different AUC in each trial run.
We speculatethat the best result observed here is an overfitting.Diagonal approximation (algorithm A) and PCA (al-gorithm J) performed similarly, unlike the results forthe Web documents.Finally, when comparing different strategies togenerate the random projection matrix L, we hadthe same conclusion as for the Wikipedia domainthat orthonormal random projection (algorithm L)has a slight advantage in terms of low variations indifferent trials over completely indepndent randomproject (algorithm F).5 Discussions and Future WorkWe have proposed a convex, input-size free op-timization algorithm for distance metric learning.This algorithm combines random projection andintra-class regularization that addresses the weak-nesses presenting in the previous works.
When thedimension d is in tens of thousands, as in many NLPand IR applications, M will be hundreds of millionsin size, too large and intractable to handle for anyexisting approaches.
Our approach addresses theseissues, making the learning not only scalable, butalso more accurate.In the future, we will investigate theoretical prop-erties that explain why the synergy of random pro-jection and intra-class regularization works well androbust.
We would also like to investigate how to useunlabeled data to regulate the learning to accomplishsemi-supervised learning.AcknowledgmentsResearch reported in this publication was supportedin part by the National Human Genome ResearchInstitute of the National Institutes of Health (NIH)under Award Number U01HG006894.
The contentis solely the responsibility of the authors and doesnot necessarily represent the official views of NIH.ReferencesJohn Blitzer, Kilian Q Weinberger, and Lawrence K Saul.2005.
Distance metric learning for large margin near-est neighbor classification.
In Advances in neural in-formation processing systems, pages 1473?1480.Reeves Fletcher and Colin M Reeves.
1964.
Functionminimization by conjugate gradients.
The computerjournal, 7(2):149?154.Jacob Goldberger, Sam Roweis, Geoff Hinton, and Rus-lan Salakhutdinov.
2004.
Neighbourhood componentsanalysis.G.H.
Golub and C.F.
Van Loan.
1996.
Matrix Compu-tations.
Johns Hopkins Studies in the MathematicalSciences.
Johns Hopkins University Press.Chun-Nan Hsu, Yu-Ming Chang, Cheng-Ju Kuo, Yu-Shi Lin, Han-Shen Huang, and I-Fang Chung.
2008.Integrating high dimensional bi-directional parsingmodels for gene mention tagging.
Bioinformatics,24(13):i286?i294.Chun-Nan Hsu, Cheng-Ju Kuo, Congxing Cai, SarahPendergrass, Marylyn Ritchie, and Jose Luis Ambite.272011.
Learning phenotype mapping for integratinglarge genetic data.
In Proceedings of BioNLP 2011Workshop, pages 19?27, Portland, Oregon, USA, June.Association for Computational Linguistics.William B Johnson and Joram Lindenstrauss.
1984.
Ex-tensions of lipschitz mappings into a hilbert space.Contemporary mathematics, 26(189-206):1.Kaare Brandt Petersen and Michael Syskind Pedersen.2006.
The matrix cookbook.Jasson DM Rennie and Nathan Srebro.
2005.
Fast maxi-mum margin matrix factorization for collaborative pre-diction.
In Proceedings of the 22nd international con-ference on Machine learning, pages 713?719.
ACM.Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, andAndrew Cotter.
2011.
Pegasos: Primal estimated sub-gradient solver for svm.
Mathematical Programming,127(1):3?30.Shefali Sharma, Leslie Lange, Jose Luis Ambite, YigalArens, and Chun-Nan Hsu.
2012.
Exploring label de-pendency in active learning for phenotype mapping.In BioNLP: Proceedings of the 2012 Workshop onBiomedical Natural Language Processing, pages 146?154, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Blake Shaw, Bert Huang, and Tony Jebara.
2011.
Learn-ing a distance metric from a network.
In Advances inNeural Information Processing Systems, pages 1899?1907.Nathan Srebro, Jason Rennie, and Tommi S Jaakkola.2004.
Maximum-margin matrix factorization.
InAdvances in neural information processing systems,pages 1329?1336.Lieven Vandenberghe and Stephen Boyd.
1996.Semidefinite programming.
SIAM review, 38(1):49?95.Eric P Xing, Michael I Jordan, Stuart Russell, and An-drew Ng.
2002.
Distance metric learning withapplication to clustering with side-information.
InAdvances in neural information processing systems,pages 505?512.Liu Yang and Rong Jin.
2006.
Distance metric learning:A comprehensive survey.
Michigan State Universiy,pages 1?51.28
