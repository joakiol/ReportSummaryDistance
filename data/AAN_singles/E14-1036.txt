Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338?347,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDeterministic Parsing using PCFGsMark-Jan Nederhof and Martin McCafferySchool of Computer ScienceUniversity of St Andrews, UKAbstractWe propose the design of deterministicconstituent parsers that choose parser ac-tions according to the probabilities ofparses of a given probabilistic context-freegrammar.
Several variants are presented.One of these deterministically constructs aparse structure while postponing commit-ment to labels.
We investigate theoreticaltime complexities and report experiments.1 IntroductionTransition-based dependency parsing (Yamadaand Matsumoto, 2003; Nivre, 2008) has attractedconsiderable attention, not only due to its high ac-curacy but also due to its small running time.
Thelatter is often realized through determinism, i.e.for each configuration a unique next action is cho-sen.
The action may be a shift of the next wordonto the stack, or it may be the addition of a de-pendency link between words.Because of the determinism, the running timeis often linear or close to linear; most of the timeand space resources are spent on deciding the nextparser action.
Generalizations that allow nonde-terminism, while maintaining polynomial runningtime, were proposed by (Huang and Sagae, 2010;Kuhlmann et al., 2011).This work has influenced, and has been in-fluenced by, similar developments in constituentparsing.
The challenge here is to deterministi-cally choose a shift or reduce action.
As in thecase of dependency parsing, solutions to this prob-lem are often expressed in terms of classifiers ofsome kind.
Common approaches involve maxi-mum entropy (Ratnaparkhi, 1997; Tsuruoka andTsujii, 2005), decision trees (Wong and Wu, 1999;Kalt, 2004), and support vector machines (Sagaeand Lavie, 2005).The programming-languages community rec-ognized early on that large classes of gram-mars allow deterministic, i.e.
linear-time, pars-ing, provided parsing decisions are postponed aslong as possible.
This has led to (deterministic)LR(k) parsing (Knuth, 1965; Sippu and Soisalon-Soininen, 1990), which is a form of shift-reduceparsing.
Here the parser needs to commit to agrammar rule only after all input covered by theright-hand side of that rule has been processed,while it may consult the next k symbols (thelookahead).
LR is the optimal, i.e.
most determin-istic, parsing strategy that has this property.
De-terministic LR parsing has also been consideredrelevant to psycholinguistics (Shieber, 1983).Nondeterministic variants of LR(k) parsing, foruse in natural language processing, have beenproposed as well, some using tabulation to en-sure polynomial running time in the length ofthe input string (Tomita, 1988; Billot and Lang,1989).
However, nondeterministic LR(k) pars-ing is potentially as expensive as, and possiblymore expensive than, traditional tabular parsingalgorithms such as CKY parsing (Younger, 1967;Aho and Ullman, 1972), as shown by for exam-ple (Shann, 1991); greater values of k make mat-ters worse (Lankhorst, 1991).
For this reason, LRparsing is sometimes enhanced by attaching prob-abilities to transitions (Briscoe and Carroll, 1993),which allows pruning of the search space (Lavieand Tomita, 1993).
This by itself is not uncon-troversial, for several reasons.
First, the space ofprobability distributions expressible by a LR au-tomaton is incomparable to that expressible by aCFG (Nederhof and Satta, 2004).
Second, becausean LR automaton may have many more transitionsthan rules, more training data may be needed toaccurately estimate all parameters.The approach we propose here retains some im-portant properties of the above work on LR pars-ing.
First, parser actions are delayed as long as338possible, under the constraint that a rule is com-mitted to no later than when the input covered byits right-hand side has been processed.
Second, theparser action that is performed at each step is themost likely one, given the left context, the looka-head, and a probability distribution over parsesgiven by a PCFG.There are two differences with traditional LRparsing however.
First, there is no explicit repre-sentation of LR states, and second, probabilities ofactions are computed dynamically from a PCFGrather than retrieved as part of static transitions.In particular, this is unlike some other early ap-proaches to probabilistic LR parsing such as (Ngand Tomita, 1991).The mathematical framework is reminiscent ofthat used to compute prefix probabilities (Jelinekand Lafferty, 1991; Stolcke, 1995).
One major dif-ference is that instead of a prefix string, we nowhave a stack, which does not need to be parsed.
Inthe first instance, this seems to make our problemeasier.
For our purposes however, we need to addnew mechanisms in order to take lookahead intoconsideration.It is known, e.g.
from (Cer et al., 2010; Canditoet al., 2010), that constituent parsing can be usedeffectively to achieve dependency parsing.
It istherefore to be expected that our algorithms can beused for dependency parsing as well.
The parsingsteps of shift-reduce parsing with a binary gram-mar are in fact very close to those of many depen-dency parsing models.
The major difference is,again, that instead of general-purpose classifiers todetermine the next step, we would rely directly ona PCFG.The emphasis of this paper is on deriving thenecessary equations to build several variants ofdeterministic shift-reduce parsers, all guided by aPCFG.
We also offer experimental results.2 Shift-reduce parsingIn this section, we summarize the theory of LRparsing.
As usual, a context-free grammar (CFG)is represented by a 4-tuple (?, N, S, P ), where?
and N are two disjoint finite sets of terminalsand nonterminals, respectively, S ?
N is the startsymbol, and P is a finite set of rules, each of theform A ?
?, where A ?
N and ?
?
(?
?
N)?.By grammar symbol we mean a terminal or non-terminal.
We use symbols A,B,C, .
.
.
for non-terminals, a, b, c, .
.
.
for terminals, v, w, x, .
.
.
forstrings of terminals, X for grammar symbols, and?, ?, ?, .
.
.
for strings of grammar symbols.
Fortechnical reasons, a CFG is often augmented byan additional rule S??
S$, where S?/?
N and$ /?
?.
The symbol $ acts as an end-of-sentencemarker.As usual, we have a (right-most) ?derives?
re-lation ?rm, ?
?rmdenotes derivation in zero ormore steps, and ?+rmdenotes derivation in oneor more steps.
If d is a string of rules pi1?
?
?pik,then ?d?rm?
means that ?
can be derived from?
by applying this list of rules in right-most order.A string ?
such that S ??rm?
is called a right-sentential form.The last rule A ?
?
used in a derivationS ?+rm?
together with the position of (the rel-evant occurrence of) ?
in ?
we call the han-dle of the derivation.
In more detail, such aderivation can be written as S = A0?rm?1A1?1??rm?1A1v1?rm?1?2A2?2v2??rm.
.
.
??rm?1?
?
??k?1Ak?1vk?1?
?
?
v1?rm?1?
?
??k?1?vk?1?
?
?
v1, where k ?
1, andAi?1?
?iAi?i(1 ?
i < k) andAk?1?
?
are inP .
The underlined symbols are those that are (re-cursively) rewritten to terminal strings within thefollowing relation?rmor??rm.
The handle hereis Ak?1?
?, together with the position of ?
inthe right-sentential form, just after ?1?
?
??k?1.
Aprefix of ?1?
?
??k?1?
is called a viable prefix inthe derivation.Given an input string w, a shift-reduce parserfinds a right-most derivation of w, but in reverseorder, identifying the last rules first.
It manipulatesconfigurations of the form (?, v$), where ?
is aviable prefix (in at least one derivation) and v isa suffix of w. The initial configuration is (?, w$),where ?
is the empty string.
The two allowablesteps are (?, av$) ` (?a, v$), which is called ashift, and (?
?, v$) ` (?A, v$) where A?
?
is inP , which is called a reduce.
Acceptance happensupon reaching a configuration (S, $).A 1-item has the form [A ?
?
?
?, a], whereA ?
??
is a rule.
The bullet separates the right-hand side into two parts, the first of which has beenmatched to processed input.
The symbol a ?
?
?
{$} is called the follower.In order to decide whether to apply ashift or reduce after reaching a configuration(X1?
?
?Xk, w), one may construct the sets I0, .
.
.
,Ik, inductively defined as follows, with 0 ?
i ?
k:?
if S ?
?
in P , then [S ?
?
?, $] ?
I0,339?
if [A ?
?
?
B?, a] ?
Ii, B ?
?
in P , and?
?
?rmx, then [B ?
?
?, b] ?
Ii, whereb = 1 : xa,?
if [A ?
?
?
Xi?, a] ?
Ii?1then [A ??Xi?
?, a] ?
Ii.
(The expression 1 : y denotes a if y = az, forsome a and z; we leave it undefined for y = ?.
)Exhaustive application of the second clause abovewill be referred to as the closure of a set of items.It is not difficult to show that if [A?
?
?, a] ?Ik, then ?
is of the form Xj+1?
?
?Xk, some j,and A ?
?
at position j + 1 is the handle of atleast one derivation S ??rmX1?
?
?Xkax, somex.
If furthermore a = 1 : w, where 1 : w iscalled the lookahead of the current configuration(X1?
?
?Xk, w), then this justifies a reduce withA ?
?, as a step that potentially leads to a com-plete derivation; this is only ?potentially?
becausethe actual remaining input w may be unlike ax,apart from the matching one-symbol lookahead.Similarly, if [A ?
?
?
a?, b] ?
Ik, then?
= Xj+1?
?
?Xk, some j, and if furthermorea = 1 : w, then a shift of symbol a is a justifiablestep.
Potentially, if a is followed by some x suchthat ?
?
?rmx, then we may eventually obtain astack X1?
?
?Xj?a?, which is a prefix of a right-sentential form, with the handle being A ?
?a?at position j + 1.For a fixed grammar, the collection of all pos-sible sets of 1-items that may arise in processingany viable prefix is a finite set.
The techniqueof LR(1) parsing relies on a precomputation of allsuch sets of items, each of which is turned into astate of the LR(1) automaton.
The initial state con-sists of closure({[S ?
?
?, $] | S ?
?
?
P}).The automaton has a transition labeled X fromI to J if goto(I,X) = J , where goto(I,X)= closure({[A ?
?X ?
?, a] | [A ?
?
?X?, a] ?
I}).
In the present study, we do not pre-compute all possible states of the LR(1) automa-ton, as this would require prohibitive amounts oftime and memory.
Instead, our parsers are bestunderstood as computing LR states dynamically,while furthermore attaching probabilities to indi-vidual items.In the sequel we will assume that all rules eitherhave the (lexical) form A ?
a, the (binary) formA ?
BC, or the (unary) form A ?
B. Thismeans that A ??rm?
is not possible for any A.The end-of-sentence marker is now introduced bytwo augmented rules S??
SS$and S$?
$.3 Probabilistic shift-reduce parsingA probabilistic CFG (PCFG) is a 5-tuple (?, N,S, P , p), where the extra element p maps rulesto probabilities.
The probability of a derivation?d?rm?, with d = pi1?
?
?pik, is defined to bep(d) =?ip(pii).
The probability p(w) of a stringw is defined to be the sum of p(d) for all d withSd?rmw.We assume properness, i.e.?pi=A?
?p(pi) =1 for all A, and consistency, i.e.
?wp(w) = 1.Properness and consistency together imply that foreach nonterminal A, the sum of p(d) for all d with?wAd?rmw equals 1.
We will further assume anaugmented PCFG with extra rules S??
SS$andS$?
$ both having probability 1.Consider a viable prefix A1?
?
?Akon the stackof a shift-reduce parser, and lookahead a. Eachright-most derivation in which the handle is A ?Ak?1Akat position k ?
1 must be of the formsketched in Figure 1.Because of properness and consistency, we mayassume that all possible subderivations generat-ing strings entirely to the right of the lookaheadhave probabilities summing to 1.
To compactlyexpress the remaining probabilities, we need addi-tional notation.
First we define:V(C,D) =?d : ?wCd?rmDwp(d)for any pair of nonterminals C and D. This willbe used later to ?factor out?
a common term in a(potentially infinite) sum of probabilities of sub-derivations; the w in the expression above corre-sponds to a substring of the unknown input beyondthe lookahead.
In order to compute such values,we fix an ordering of the nonterminals by N ={C1, .
.
.
, Cr}, with r = |N |.
We then constructa matrix M , such that Mi,j=?pi=Ci?Cj?p(pi).In words, we sum the probabilities of all rules thathave left-hand sideCiand a right-hand side begin-ning with Cj.A downward path in a parse tree from an oc-currence of C to an occurrence of D, restrictedto following always the first child, can be of anylength n, including n = 0 if C = D. This meanswe need to obtain the matrixM?=?0?nMn, andV(Ci, Cj) =M?i,jfor all i and j. Fortunately,M?i,jcan be effectively computed as (I ?M)?1, whereI is the identity matrix of size r and the superscriptdenotes matrix inversion.340We further define:U(C,D) =?d : Cd?rmDp(d)much as above, but restricting attention to unitrules.The expected number of times a handle A ?Ak?1Akat position k ?
1 occurs in a right-mostderivation with viable prefix A1?
?
?Akand looka-head a is now given by:E(A1?
?
?Ak, a, A?
Ak?1Ak) =?S?= E0, .
.
.
, Ek?2, F1, .
.
.
, Fk?1= A,F,E,B,B?,m : 0 ?
m < k ?
1?i: 1?i?mV(Ei?1, Fi) ?
p(Fi?
AiEi) ?V(Em, F ) ?
p(F ?
EB) ?
U(E,Fm+1) ?
?i: m<i<k?1p(Fi?
AiEi) ?
U(Ei, Fi+1) ?p(Fk?1?
Ak?1Ak) ?
V(B,B?)
?
p(B??
a)Note that the value above is not a probability andmay exceed 1.
This is because the same viableprefix may occur several times in a single right-most derivation.At first sight, the computation of E seems to re-quire an exponential number of steps in k. How-ever, we can use an idea similar to that commonlyused for computation of forward probabilities forHMMs (Rabiner, 1989).
We first define F :F(?, E) ={1 if E = S?0 otherwiseF(?A,E) =?E?,pi=F?AEF(?,E?)
?
V(E?, F ) ?
p(pi)This corresponds to the part of the definitionof E involving A1, .
.
.
, Am, E0, .
.
.
, EmandF1, .
.
.
, Fm.
We build on this by defining:G(?,E,B) =?E?,pi=F?EBF(?,E?)
?
V(E?, F ) ?
p(pi)One more recursive function is needed forwhat was Am+1, .
.
.
, Ak?2, Em+1, .
.
.
, Ek?2andFm+1, .
.
.
, Fk?2in the earlier definition of E :H(?, E,B)=G(?, E,B)H(?A,E,B)=?E?,pi=F?AEH(?,E?, B) ?
U(E?, F ) ?
p(pi)+ G(?A,E,B)E0F1A1E1Em?1FmAmEmFEFm+1Am+1Em+1Ek?2Fk?1Ak?1AkBB?aFigure 1: Right-most derivation leading toFk?1?
Ak?1Akin viable prefix A1?
?
?Akwithlookahead a.Finally, we can express E in terms of these re-cursive functions, considering the more generalcase of any rule pi = F ?
?:E(?
?, a, F ?
?)
=?E,BH(?,E,B) ?
U(E,F ) ?
p(pi) ?
L(B, a)E(?, a, F ?
?)
= 0 if ????
= ?
?where:L(B, a) =?pi=B??aV(B,B?)
?
p(pi)The expected number of times the handle is tobe found to the right of ?, with the stack being ?and the lookahead symbol being a, is:E(?, a, shift) =?BF(?,B) ?
L(B, a)The expected number of times we see a stack ?with lookahead a is:E(?, a) = E(?, a, shift) +?piE(?, a, pi)341The probability that a reduce with rule pi is thecorrect action when the stack is ?
and the looka-head is a is naturally E(?, a, pi)/E(?, a) and theprobability that a shift is the correct action isE(?, a, shift)/E(?, a).
For determining the mostlikely action we do not need to compute E(?, a);it suffices to identify the maximum value amongE(?, a, shift) and E(?, a, pi) for each rule pi.A deterministic shift-reduce parser can now beconstructed that always chooses the most likelynext action.
For a given input string, the numberof actions performed by this parser is linear in theinput length.A call of E may lead to a number of recursivecalls of F and H that is linear in the stack sizeand thereby in the input length.
Note however thatby remembering the values returned by these func-tion between parser actions, one can ensure thateach additional element pushed on the stack re-quires a bounded number of additional calls of theauxiliary functions.
Because only linearly manyelements are pushed on the stack, the time com-plexity becomes linear in the input length.Complexity analysis seems less favorable if weconsider the number of nonterminals.
The defi-nitions of G and H each involve four nontermi-nals excluding the stack symbol A, so that thetime complexity is O(|w| ?
|N |4), where |w| isthe length of the input w. A finer analysis givesO(|w| ?
(|N | ?
|P |+ |N |2?
?P?
)), where ?P?
isthe maximum for all A of the number of rulesof the form F ?
AE.
By splitting up G andH into smaller functions, we obtain complexityO(|w| ?
|N |3), which can still be prohibitive.Therefore we have implemented an alternativethat has a time complexity that is only quadraticin the size of the grammar, at the expense of aquadratic complexity in the length of the inputstring, as detailed in Appendix A.
This is stillbetter in practice if the number of nonterminals ismuch greater than the length of the input string, asin the case of the grammars we investigated.4 Structural determinismWe have assumed so far that a deterministic shift-reduce parser chooses a unique next action in eachconfiguration, an action being a shift or reduce.Implicit in this was that if the next action is a re-duce, then also a unique rule is chosen.
However,if we assume for now that all non-lexical rulesare binary, then we can easily generalize the pars-ing algorithm to consider all possible rules whoseright-hand sides match the top-most two stack el-ements, and postpone commitment to any of thenonterminals in the left-hand sides.
This requiresthat stack elements now contain sets of grammarsymbols.
Each of these is associated with theprobability of the most likely subderivation con-sistent with the relevant substring of the input.Each reduce with a binary rule is implicitly fol-lowed by zero or more reduces with unary rules.Similarly, each shift is implicitly followed by a re-duce with a lexical rule and zero or more reduceswith unary rules; see also (Graham et al., 1980).This uses a precompiled table similar to U , but us-ing maximization in place of summation, definedby:Umax(C,D) = maxd : Cd?rmDp(d)More concretely, configurations have the form(Z1.
.
.
Zk, v$), k ?
0, where each Zi(1 ?
i ?
k)is a set of pairs (A, p), where A is a nonterminaland p is a (non-zero) probability; each A occursat most once in Zi.
A shift turns (?, av$) into(?Z, v$), where Z consists of all pairs (E, p) suchthat p = maxFUmax(E,F ) ?
p(F ?
a).
A gen-eralized binary reduce now turns (?Z1Z2, v$) into(?Z, v$), where Z consists of all pairs (E, p) suchthat:p = maxpi = F ?
A1A2,(A1, p1) ?
Z1, (A2, p2) ?
Z2Umax(E,F ) ?
p(pi) ?
p1?
p2We characterize this parsing procedure as struc-turally deterministic, as an unlabeled structure isbuilt deterministically in the first instance.
Theexact choices of rules can be postponed until af-ter reaching the end of the sentence.
Then followsa straightforward process of ?backtracing?, whichbuilds the derivation that led to the computed prob-ability associated with the start symbol.The time complexity is now O(|w| ?
|N |5) inthe most straightforward implementation, but wecan reduce this to quadratic in the size of the gram-mar provided we allow an additional factor |w| asbefore.
For more details see Appendix B.5 Other variantsOne way to improve accuracy is to increase thesize of the lookahead, beyond the current 1, com-parable to the generalization from LR(1) to LR(k)parsing.
The formulas are given in Appendix C.342Yet another variant investigates only the top-most n stack symbols when choosing the nextparser action.
In combination with Appendix A,this brings the time complexity down again to lin-ear time in the length of the input string.
The re-quired changes to the formulas are given in Ap-pendix D. There is a slight similarity to (Schuler,2009), in that no stack elements beyond a boundeddepth are considered at each parsing step, but inour case the stack can still have arbitrary height.Whereas we have concentrated on determinismin this paper, one can also introduce a limited de-gree of nondeterminism and allow some of themost promising configurations at each input posi-tion to compete, applying techniques such as beamsearch (Roark, 2001; Zhang and Clark, 2009; Zhuet al., 2013), best-first search (Sagae and Lavie,2006), or A?search (Klein and Manning, 2003)in order to keep the running time low.
For com-paring different configurations, one would need tomultiply the values E(?, a) as in Section 3 by theprobabilities of the subderivations associated withoccurrences of grammar symbols in stack ?.Further variants are obtained by replacing theparsing strategy.
One obvious candidate is left-corner parsing (Rosenkrantz and Lewis II, 1970),which is considerably simpler than LR parsing.The resulting algorithm would be very differentfrom the left-corner models of e.g.
(Henderson,2003), which rely on neural networks instead ofPCFGs.6 ExperimentsWe used the WSJ treebank from OntoNotes 4.0(Hovy et al., 2006), with Sections 2-21 for train-ing and the 2228 sentences of up to 40 words fromSection 23 for testing.
Grammars with differentsizes, and in the required binary form, were ex-tracted by using the tools from the Berkeley parser(Petrov et al., 2006), with between 1 and 6 split-merge cycles.
These tools offer a framework forhandling unknown words, which we have adopted.The implementation of the parsing algorithmsis in C++, running on a desktop with four 3.1GHzIntel Core i5 CPUs.
The main algorithm is that ofAppendix C, with lookahead k between 1 and 3,also in combination with structural determinism(Appendix B), which is indicated here by sd.
Thevariant that consults the stack down to boundeddepth n (Appendix D) will only be reported fork = 1 and n = 5.Bracketing recall, precision and F-measure, arecomputed using evalb, with settings as in (Collins,1997), except that punctuation was deleted.1Ta-ble 1 reports results.A nonterminal B in the stack may occur in asmall number of rules of the form A ?
BC.
TheC of one such rule is needed next in order to al-low a reduction.
If future input does not deliverthis C, then parsing may fail.
This problem be-comes more severe as nonterminals become morespecific, which is what happens with an increase ofthe number of split-merge cycles.
Even more fail-ures are introduced by removing the ability to con-sult the complete stack, which explains the poorresults in the case of k = 1, n = 5; lower valuesof n lead to even more failures, and higher valuesfurther increase the running time.
That the runningtime exceeds that of k = 1 is explained by the factthat with the variant from Appendix D, every popor push requires a complete recomputation of allfunction values.Parse failures can be almost completely elimi-nated however by choosing higher values of k andby using structural determinism.
A combinationthereof leads to high accuracy, not far below thatof the Viterbi parses.
Note that one cannot expectthe accuracy of our deterministic parsers to exceedthat of Viterbi parses.
Both rely on the same model(a PCFG), but the first is forced to make local deci-sions without access to the input string that followsthe bounded lookahead.7 ConclusionsWe have shown that deterministic parsers can beconstructed from a given PCFG.
Much of the ac-curacy of the grammar can be retained by choosinga large lookahead in combination with ?structuraldeterminism?, which postpones commitment tononterminals until the end of the input is reached.Parsers of this nature potentially run in lineartime in the length of the input, but our parsers arebetter implemented to run in quadratic time.
Interms of the grammar size, the experiments sug-gest that the number of rules is the dominating fac-tor.
The size of the lookahead strongly affects run-ning time.
The extra time costs of structural deter-minism are compensated by an increase in accu-racy and a sharp decrease of the parse failures.1Evalb otherwise stumbles over e.g.
a part of speech con-sisting of two single quotes in the parsed file, against a partof speech ?POS?
in the gold file, for an input token consistingof a single quote.343Table 1: Total time required (seconds), number of parse failures, recall, precision, F-measure, for deter-ministic parsing, compared to the Viterbi parses as computed with the Berkeley parser.time fail R P F11-split-merge (12,059 rules)k = 1 43 11 67.20 66.67 66.94k = 2 99 0 70.74 71.01 70.88k = 3 199 0 71.41 71.85 71.63k = 1, sd 62 0 68.12 68.52 68.32k = 2, sd 135 0 70.98 71.72 71.35k = 3, sd 253 0 71.31 72.50 71.90k = 1, n = 5 56 170 66.19 65.67 65.93Viterbi 0 72.45 74.55 73.492-split-merge (32,994 rules)k = 1 120 33 72.65 70.50 71.56k = 2 275 1 78.44 77.26 77.84k = 3 568 0 79.81 79.27 79.54k = 1, sd 196 0 74.78 74.96 74.87k = 2, sd 439 0 79.96 80.40 80.18k = 3, sd 770 0 80.49 81.20 80.85k = 1, n = 5 146 247 72.27 70.34 71.29Viterbi 0 82.16 82.69 82.433-split-merge (95,647 rules)k = 1 305 75 74.39 72.33 73.35k = 2 770 3 81.32 80.35 80.83k = 3 1,596 0 82.78 82.35 82.56k = 1, sd 757 0 78.11 78.37 78.24k = 2, sd 1,531 0 82.85 83.39 83.12k = 3, sd 2,595 0 83.66 84.25 83.96k = 1, n = 5 404 401 74.52 72.39 73.44Viterbi 0 85.38 86.03 85.71time fail R P F14-split-merge (269,162 rules)k = 1 870 115 75.69 73.30 74.48k = 2 2,257 1 83.48 82.35 82.91k = 3 4,380 1 84.95 84.06 84.51k = 1, sd 2,336 1 80.82 80.65 80.74k = 2, sd 4,747 0 85.52 85.64 85.58k = 3, sd 7,728 0 86.62 86.82 86.72k = 1, n = 5 1,152 508 76.21 73.92 75.05Viterbi 0 87.95 88.10 88.025-split-merge (716,575 rules)k = 1 3,166 172 76.17 73.44 74.78k = 2 7,476 2 84.14 82.80 83.46k = 3 14,231 1 86.05 85.24 85.64k = 1, sd 7,427 1 81.99 81.44 81.72k = 2, sd 14,587 0 86.89 87.00 86.95k = 3, sd 24,553 0 87.67 87.82 87.74k = 1, n = 5 4,572 559 77.65 75.13 76.37Viterbi 0 88.65 89.00 88.836-split-merge (1,947,915 rules)k = 1 7,741 274 76.60 74.08 75.32k = 2 19,440 5 84.60 83.17 83.88k = 3 35,712 0 86.02 85.07 85.54k = 1, sd 19,530 1 82.64 81.95 82.29k = 2, sd 39,615 0 87.36 87.20 87.28k = 3, sd 64,906 0 88.16 88.26 88.21k = 1, n = 5 10,897 652 77.89 75.57 76.71Viterbi 0 88.69 88.99 88.84There are many advantages over other ap-proaches to deterministic parsing that rely ongeneral-purpose classifiers.
First, some state-of-the-art language models are readily available asPCFGs.
Second, most classifiers require tree-banks, whereas our algorithms are also applicableto PCFGs that were obtained in any other way, forexample through intersection of language models.Lastly, our algorithms fit within well understoodautomata theory.Acknowledgments We thank the reviewers.A Formulas for quadratic timecomplexityThe following are the formulas that correspondto the first implemented variant.
Relative to Sec-tion 3, some auxiliary functions are broken up, andassociating the lookahead a with an appropriatenonterminal B is now done in G:F(?, E) ={1 if E = S?0 otherwiseF(?A,E) =?pi=F?AEF?
(?, F ) ?
p(pi)F?
(?, F ) =?EF(?,E) ?
V(E,F )G(?,E, a) =?FF?
(?, F ) ?
G?
(F,E, a)G?
(F,E, a) =?pi=F?EBp(pi) ?
L(B, a)H(?, E, a) = G(?, E, a)H(?A,E, a) =?pi=F?AEH?
(?, F, a) ?
p(pi)+ G(?A,E, a)H?
(?, F, a) =?EH(?,E, a) ?
U(E,F )344E(?
?, a, F ?
?)
= H?
(?, F, a) ?
p(F ?
?
)E(?, a, F ?
?)
= 0 if ????
= ?
?E(?A, a, shift) = G(?,A, a)E(?, a, shift) = L(S?, a)These equations correspond to a time complex-ity of O(|w|2?
|N |2+ |w| ?
|P |).
Each definitionexcept that of G?involves one stack (of linear size)and, at most, one terminal plus two arbitrary non-terminals.
The full grammar is only consideredonce for every input position, in the definition ofG?.The values are stored as vectors and matrices.For example, for each distinct lookahead symbola, there is a (sparse) matrix containing the value ofG?
(F,E, a) at a row and a column uniquely iden-tified by F and E, respectively.B Formulas for structural determinismFor the variant from Section 4, we need to changeonly two definitions of auxiliary functions:F(?Z,E) =?(A,p)?Z,pi=F?AEF?
(?, F ) ?
p(pi) ?
pH(?Z,E, a) =?(A,p)?Z,pi=F?AEH?
(?, F, a) ?
p(pi) ?
p+ G(?Z,E, a)The only actions are shift and generalized bi-nary reduce red .
The definition of E becomes:E(?Z1Z2, a, red)=?(A1,p1)?Z1,(A2,p2)?Z2pi=F?A1A2H?
(?, F, a) ?
p(pi) ?
p1?
p2E(?Z, a, shift) =?
(A,p)?ZG(?,A, a) ?
pThe time complexity now increases toO(|w|2?
(|N |2+ |P |)) due to the newH.C Formulas for larger lookaheadIn order to handle k symbols of lookahead (Sec-tion 5) some technical problems are best avoidedby having k copies of the end-of-sentence markerappended behind the input string, with a corre-sponding augmentation of the grammar.
We gen-eralize L(B, v) to be the sum of p(d) for all dsuch that Bd?rmvx, some x.
We let I(B, v)be the sum of p(d) for all d such that Bd?rmv.If I is given for all prefixes of a fixed lookaheadstring of length k (this requires cubic time in k),we can compute L in linear time for all suffixes ofthe same string:L(B, v) =?B?V(B,B?)
?
L?
(B?, v)L?
(B, v) =?pi=B?B1B2,v1,v2:v=v1v2,1?|v1|,1?|v2|p(pi) ?
I(B1, v1) ?
L(B2, v2)if |v| > 1L?
(B, a) =?pi=B?ap(pi)The function H is generalized straightforwardlyby letting it pass on a string v (1 ?
|v| ?
k) in-stead of a single terminal a.
The same holds for E .The function G requires a slightly bigger modifica-tion, leading back to H if not all of the lookaheadhas been matched yet:G(?,E, v) =?FF?
(?, F ) ?
G?
(F,E, v) +?F,v1,v2:v=v1v2,|v2|>0H?
(?, F, v2) ?
G??
(F,E, v1)G?
(F,E, v) =?pi=F?EBp(pi) ?
L(B, v)G??
(F,E, v) =?pi=F?EBp(pi) ?
I(B, v)The time complexity is nowO(k ?
|w|2?
|N |2+k3?
|w| ?
|P |).D Investigation of top-most n stacksymbols onlyAs discussed in Section 5, we want to predict thenext parser action without consulting any symbolsin ?, when the current stack is ?
?, with |?| =n.
This is achieved by approximating F(?,E) bythe outside value of E, that is, the sum of p(d)for all d such that ??,wSd?rm?Ew.
Similarly,H?
(?, F, v) is approximated by?EG(?,E, v) ?W(E,F ) where:W(C,D) =?d : ?
?Cd?rm?Dp(d)The time complexity (with lookahead k) is nowO(k ?
n ?
|w| ?
|N |2+ k3?
|w| ?
|P |).345ReferencesA.V.
Aho and J.D.
Ullman.
1972.
Parsing, volume 1 ofThe Theory of Parsing, Translation and Compiling.Prentice-Hall, Englewood Cliffs, N.J.S.
Billot and B. Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In 27th An-nual Meeting of the ACL, Proceedings of the Confer-ence, pages 143?151, Vancouver, British Columbia,Canada, June.T.
Briscoe and J. Carroll.
1993.
Generalized prob-abilistic LR parsing of natural language (corpora)with unification-based grammars.
ComputationalLinguistics, 19(1):25?59.M.
Candito, J. Nivre, P. Denis, and E. Henestroza An-guiano.
2010.
Benchmarking of statistical de-pendency parsers for French.
In The 23rd Inter-national Conference on Computational Linguistics,pages 108?116, Beijing, China, August.D.
Cer, M.-C. de Marneffe, D. Jurafsky, and C. Man-ning.
2010.
Parsing to Stanford dependen-cies: Trade-offs between speed and accuracy.
InLREC 2010: Seventh International Conference onLanguage Resources and Evaluation, Proceedings,pages 1628?1632, Valletta , Malta, May.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In 35th Annual Meeting of theACL, Proceedings of the Conference, pages 16?23,Madrid, Spain, July.S.L.
Graham, M.A.
Harrison, and W.L.
Ruzzo.
1980.An improved context-free recognizer.
ACM Trans-actions on Programming Languages and Systems,2:415?462.J.
Henderson.
2003.
Generative versus discrimina-tive models for statistical left-corner parsing.
In8th International Workshop on Parsing Technolo-gies, pages 115?126, LORIA, Nancy, France, April.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% solu-tion.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Main Conference,pages 57?60, New York, USA, June.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proceedingsof the 48th Annual Meeting of the ACL, pages 1077?1086, Uppsala, Sweden, July.F.
Jelinek and J.D.
Lafferty.
1991.
Computationof the probability of initial substring generation bystochastic context-free grammars.
ComputationalLinguistics, 17(3):315?323.T.
Kalt.
2004.
Induction of greedy controllers for de-terministic treebank parsers.
In Conference on Em-pirical Methods in Natural Language Processing,pages 17?24, Barcelona, Spain, July.D.
Klein and C.D.
Manning.
2003.
A?parsing: Fastexact Viterbi parse selection.
In Proceedings of the2003 Human Language Technology Conference ofthe North American Chapter of the ACL, pages 40?47, Edmonton, Canada, May?June.D.E.
Knuth.
1965.
On the translation of languagesfrom left to right.
Information and Control, 8:607?639.M.
Kuhlmann, C.
G?omez-Rodr?
?guez, and G. Satta.2011.
Dynamic programming algorithms fortransition-based dependency parsers.
In 49th An-nual Meeting of the ACL, Proceedings of the Con-ference, pages 673?682, Portland, Oregon, June.M.
Lankhorst.
1991.
An empirical comparison of gen-eralized LR tables.
In R. Heemels, A. Nijholt, andK.
Sikkel, editors, Tomita?s Algorithm: Extensionsand Applications, Proc.
of the first Twente Work-shop on Language Technology, pages 87?93.
Uni-versity of Twente, September.A.
Lavie and M. Tomita.
1993.
GLR??
an effi-cient noise-skipping parsing algorithm for contextfree grammars.
In Third International Workshop onParsing Technologies, pages 123?134, Tilburg (TheNetherlands) and Durbuy (Belgium), August.M.-J.
Nederhof and G. Satta.
2004.
An alternativemethod of training probabilistic LR parsers.
In 42ndAnnual Meeting of the ACL, Proceedings of the Con-ference, pages 551?558, Barcelona, Spain, July.S.-K. Ng and M. Tomita.
1991.
Probabilistic LR pars-ing for general context-free grammars.
In Proc.
ofthe Second International Workshop on Parsing Tech-nologies, pages 154?163, Cancun, Mexico, Febru-ary.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4):513?553.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the ACL, pages 433?440, Sydney,Australia, July.L.R.
Rabiner.
1989.
A tutorial on hidden Markov mod-els and selected applications in speech recognition.Proceedings of the IEEE, 77(2):257?286, February.A.
Ratnaparkhi.
1997.
A linear observed time statis-tical parser based on maximum entropy models.
InProceedings of the Second Conference on EmpiricalMethods in Natural Language Processing, pages 1?10, Providence, Rhode Island, USA, August.B.
Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.346D.J.
Rosenkrantz and P.M. Lewis II.
1970.
Determin-istic left corner parsing.
In IEEE Conference Recordof the 11th Annual Symposium on Switching and Au-tomata Theory, pages 139?152.K.
Sagae and A. Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnologies, pages 125?132, Vancouver, BritishColumbia, Canada, October.K.
Sagae and A. Lavie.
2006.
A best-first probabilisticshift-reduce parser.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand 44th Annual Meeting of the ACL, pages 691?698, Sydney, Australia, July.W.
Schuler.
2009.
Positive results for parsing witha bounded stack using a model-based right-cornertransform.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the ACL, pages 344?352, Boulder, Colorado, May?June.P.
Shann.
1991.
Experiments with GLR and chart pars-ing.
In M. Tomita, editor, Generalized LR Parsing,chapter 2, pages 17?34.
Kluwer Academic Publish-ers.S.M.
Shieber.
1983.
Sentence disambiguation bya shift-reduce parsing technique.
In 21st AnnualMeeting of the ACL, Proceedings of the Conference,pages 113?118, Cambridge, Massachusetts, July.S.
Sippu and E. Soisalon-Soininen.
1990.
Parsing The-ory, Vol.
II: LR(k) and LL(k) Parsing, volume 20 ofEATCS Monographs on Theoretical Computer Sci-ence.
Springer-Verlag.A.
Stolcke.
1995.
An efficient probabilistic context-free parsing algorithm that computes prefix proba-bilities.
Computational Linguistics, 21(2):167?201.M.
Tomita.
1988.
Graph-structured stack and natu-ral language parsing.
In 26th Annual Meeting ofthe ACL, Proceedings of the Conference, pages 249?257, Buffalo, New York, June.Y.
Tsuruoka and J. Tsujii.
2005.
Chunk parsing re-visited.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies, pages 133?140,Vancouver, British Columbia, Canada, October.A.
Wong and D. Wu.
1999.
Learning a lightweightrobust deterministic parser.
In Sixth European Con-ference on Speech Communication and Technology,pages 2047?2050.H.
Yamada and Y. Matsumoto.
2003.
Statistical de-pendency analysis with support vector machines.
In8th International Workshop on Parsing Technolo-gies, pages 195?206, LORIA, Nancy, France, April.D.H.
Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10:189?208.Y.
Zhang and S. Clark.
2009.
Transition-based pars-ing of the Chinese treebank using a global discrimi-native model.
In Proceedings of the 11th Interna-tional Conference on Parsing Technologies, pages162?171, Paris, France, October.M.
Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu.2013.
Fast and accurate shift-reduce constituentparsing.
In 51st Annual Meeting of the ACL, Pro-ceedings of the Conference, volume 1, pages 434?443, Sofia, Bulgaria, August.347
