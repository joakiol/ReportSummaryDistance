Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152?160,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsNon-scorable Response Detection for Automated Speaking ProficiencyAssessmentSu-Youn Yoon, Keelan Evanini, Klaus ZechnerEducational Testing Service660 Rosedale Road, Princeton, NJ, USA{syoon,kevanini,kzechner}@ets.orgAbstractWe present a method that filters out non-scorable (NS) responses, such as responseswith a technical difficulty, in an automatedspeaking proficiency assessment system.
Theassessment system described in this study firstfilters out the non-scorable responses and thenpredicts a proficiency score using a scoringmodel for the remaining responses.The data were collected from non-nativespeakers in two different countries, using twodifferent item types in the proficiency assess-ment: items that elicit spontaneous speech anditems that elicit recited speech.
Since the pro-portion of NS responses and the features avail-able to the model differ according to the itemtype, an item type specific model was trainedfor each item type.
The accuracy of the mod-els ranged between 75% and 79% in spon-taneous speech items and between 95% and97% in recited speech items.Two different groups of features, signal pro-cessing based features and automatic speechrecognition (ASR) based features, were im-plemented.
The ASR based models achievedhigher accuracy than the non-ASR based mod-els.1 IntroductionWe developed a method that filters out non-scorable(NS) responses as a supplementary module to anautomated speech proficiency assessment system.In this study, the method was developed for atelephony-based assessment of English proficiencyfor non-native speakers.
The examinees?
responseswere collected from several different environmen-tal conditions, and many of the utterances containbackground noise from diverse sources.
In ad-dition to the presence of noise, many responseshave other sub-optimal characteristics.
For exam-ple, some responses contain uncooperative behav-ior from the speakers, such as non-English speech,whispered speech, and non-responses.
These typesof responses make it difficult to provide a valid as-sessment of a speaker?s English proficiency.
There-fore, in order to address the diverse types of causesfor these problematic responses, we used a two stepapproach: first, these problematic responses werefiltered out by a ?filtering model,?
and only the re-maining responses were scored using the automatedscoring model.The overall architecture of our method, includ-ing the automated speech proficiency scoring sys-tem, is as follows: for a given spoken response,the system performs speech recognition, yielding aword hypothesis and time stamps.
In addition toword recognition, the system computes pitch andpower to generate prosodic features; the system cal-culates descriptive statistics such as the mean andstandard deviation of pitch and power at both theword level and response level.
Given the word hy-potheses and pitch/power features, it derives featuresfor automated proficiency scoring.
Next, the non-ASR based features are calculated separately usingsignal processing techniques.
Finally, given bothsets of features, the filtering model identifies NS re-sponses.This paper will proceed as follows: we will re-view previous studies (Section 2), present the data152(Section 3), and then describe the structure of thefiltering model (Section 4).
Next, the results willbe presented (Section 5), followed by a discussion(Section 6), and we will conclude with a summaryof the importance of the findings (Section 7).2 Previous WorkHiggins et al (2011) developed a ?filtering model?that is conceptually similar to the one in this pa-per.
The model was trained and tested on a corpuscontaining responses from non-native speakers to anEnglish proficiency assessment.
This system useda regression model based on four features whichwere originally designed for automated speech pro-ficiency scoring: the number of distinct words in thespeech recognition output, the average speech rec-ognizer confidence score, the average power of thespeech signal, and the mean absolute deviation ofthe speech signal power.
This model was able toidentify responses which were also identified as NSresponses by human raters with an approximately98% accuracy when a false positive rate (the propor-tion of responses without technical difficulties thatwere incorrectly flagged as problematic) was lowerthan 1%.Although there are few other studies which are di-rectly related to the task of filtering out non-scorableresponses in the domain of automated speech profi-ciency assessment, several signal processing studiesare related to this work.
Traditionally, the Signal toNoise Ratio (SNR) has been used to detect speechwith a large amount of background noise.
Thismethod measures the ratio between the total energyof the speech signal and the total energy of the noise;if the SNR is low, then the speech contains loudbackground noise.
A low SNR results in lower in-telligibility and increases the difficulty for both hu-man and automated scoring.
Furthermore, spectralcharacteristics can be also applied to detect speechwith loud background noise, since noise has differ-ent spectral characteristics than speech (noise tendsto have no or few peaks in the spectral domain).If a response contains loud background noise, thenthe spectral characteristics of the speech may be ob-scured by noise and it may have similar character-istics with the noise.
These differences in spectralcharacteristics have been used in audio informationretrieval Lu and Hankinson (1998).Secondly, responses without valid speech can beidentified using Voice Activity Detection (VAD).VAD is a technique which distinguishes humanspeech from non-speech.
When speech is clean,VAD can be calculated by simply computing thezero-crossing rate which signals the existence ofcyclic waves such as vowels.
However, if the re-sponse also contains loud background noises, moresophisticated methods are required.
In order to re-move the influence of noise, Chang and Kim (2003),Chang et al (2006), Shin et al (2005) and Sohn etal.
(1999) estimated the characteristics of the noisespectrum and the distribution of noise, and compen-sated for them when speech is identified.
The perfor-mance of these systems is heavily-influenced by theaccuracy of estimating characteristics of the back-ground noise.In this study, we used a set of ASR based fea-tures and non-ASR based features.
ASR based fea-tures were similar to the ones used by Zechner et al(2009).
In addition to the features based on ASRhypotheses, the ASR based feature set contained ba-sic pitch and power related features since the ASRsystem in this study also produced pitch and powermeasurements in order to generate prosodic features.The non-ASR based features were comprised of fourgroups of features based on signal processing tech-niques such as SNR, VAD, and pitch and power.Features related to pitch and power were included inboth the ASR based features and the non-ASR basedfeatures.
Since the non-ASR based features wereoriginally implemented as an independent modulefrom the ASR-based system (it was implemented forthe case where the appropriate recognizer is unavail-able), there is some degree of overlap between thetwo feature sets.3 DataThe data for this experiment were drawn from a pro-totype of a telephony-based English language as-sessment.
Non-native speakers of English each re-sponded to 40 test items designed to evaluate theirlevel of English proficiency.
The test was composedof items that elicited both spontaneous speech (here-after SS) and recited speech (hereafter RS).
In thisstudy, 8 items (four SS and four RS) were used for153each speaker.Participants used either a cell phone or a landline to complete the assessment, and the participantswere compensated for their time.
The motivationlevel of the participants was thus lower than in thecase of an actual high stakes assessment, where aparticipant?s performance could have a substantialimpact on their future.
In addition, the data collec-tion procedure was less controlled than in an op-erational testing environment; for example, somerecordings exhibited higher levels of ambient noisethan others.
These two facts led to the quality ofsome of the responses being lower than would beexpected in an operational assessment.The data for this study were collected from partic-ipants in two countries: India and China.
For India,4900 responses from 638 speakers were collected.For China, 5565 responses from 702 speakers werecollected (some of the participants did not provideresponses to all 8 test items).
Each response is ap-proximately 45 sec in duration.After the data was collected, all of the responseswere given scores on a three-point scale by trainedraters.
The raters also labeled responses as ?non-scorable?
(NS), when appropriate.
NS responses areones that could not be given a score according to therubrics of the three-point scale.
These were due toeither a technical difficulty obscuring the content ofthe response or an inappropriate response from theparticipant.The proportion of NS responses differs markedlybetween the two countries.
852 of the responses inthe India data set (17% of the total) were labeled asNS, compared to 1548 responses (28%) in the Chinadata set.Table 1 provides the different types of NS re-sponses that were annotated by the raters, along withthe relative frequency of each NS category com-pared to the others.Excluding the category ?Other?, backgroundnoise, non-responses, and unrelated topic were themost frequent types of NS response for both datasets.
However, the relative proportions of each typediffered somewhat between the two countries.
Forexample, the most frequent NS type in India wasbackground noise; 33% of NS responses were of thistype, 1.7 times higher than in China.The proportion of unrelated topic responses wasNS Type India (%) China (%)Background noise 33.2 19.6Other 25.0 15.4Unrelated response 18.9 40.1Non-response 10.6 8.8Non-English speech 4.9 6.4Too soft 2.8 1.0Background speech 2.0 1.9Missing samples 1.5 4.0Too loud 0.8 0.1Cheating 0.3 2.7Table 1: Different types of NS responses and their relativefrequency, in % of all NS for each country (ranked byfrequency of occurrence in India)Data PartitionIndia China# of re-sponsesNS(%)# of re-sponsesNS(%)SS-train 1114 31.6 1382 32.2SS-eval 1271 27.5 1391 33.8RS-train 1253 8.0 1392 22.4RS-eval 1275 4.8 1400 22.9Table 2: Item-type specific training and evaluation dataalso high in both countries, but it was much higherin the China data set: it was 19% in the responsesfrom India and 40% for China (more than twice ashigh as in India).
All responses which were not di-rectly related to the prompt fell into this category.For SS items, the majority included responses abouta different topic.
For RS items, responses in whichthe speakers read different prompts were classifiedinto this category.The responses were divided into training and test-ing for NS response detection.
Due to the significantdifference in the proportion of NS responses and rel-ative frequencies of NS types in the two data sets, fil-tering models were trained separately for each coun-try.
In addition, since the proportions of NS re-sponses and the available features varied accordingto the item type, training and testing data were fur-ther classified by item types.
The proportions of NSresponses and the sizes of the partitions, along withthe percent of NS responses in each item type, areshown in Table 2.154The partitions for testing the filtering model wereselected to maximize the number of speakers withcomplete sets of responses; however, this constraintwas not able to be met for the training partitions inthe India data set (due to insufficient data).
This ex-plains the lower proportion of NS responses in theIndia test partitions, since speakers with completesets of responses were less likely to provide bad re-sponses.
As Table 2 shows, NS responses were morefrequent among SS items than RS items: the pro-portion of NS responses in SS items was four timeshigher than in RS items in India and 1.5 times inChina.4 Method4.1 OverviewIn this study, two different sets of features were usedin the model training process; ASR-based featuresand non-ASR based features.
For each item-type,an item-type-specific filtering model was developedusing these two sets of features.4.2 Feature generation4.2.1 ASR based featuresFor this feature set, we used the features froman automated speech proficiency scoring system.This scoring system used an ASR engine containingword-internal triphone acoustic models and item-type-specific language models.
Separate acousticmodels were trained for the data sets from the twocountries.
The acoustic training data for the twomodels consisted of 45.5 hours of speech from In-dia and 123.1 hours of speech from China.
In addi-tion, separate language models were trained for theSS and RS items for each country; for the RS items,the language models also incorporated the texts ofthe prompts.A total of 61 features were available.
Amongthese features, many features were conceptuallysimilar but based on different normalization meth-ods.
These features showed a strong intercorrela-tion.
For this study, 30 features were selected andclassified into four groups according to their char-acteristics: basic features, fluency features, ASR-confidence features, and Word Error Rate (WER)features.The basic features are related to power and pitch,and they capture the overall distribution of pitch andpower values in a speaker?s response using mean andvariance calculations.
These features are relevantsince NS responses may have an abnormal distribu-tion in energy.
For instance, non-responses containvery low energy.
In order to detect these abnormal-ities in speech signal, pitch and power related fea-tures were calculated.The fluency features measure the length of a re-sponse in terms of duration and number of words.In addition, this group contains features related tospeaking rate and silences, such as mean durationand number of silence.
In particular, these featuresare effective in identifying Non-responses whichcontain zero or only a few words.The ASR-confidence group contains features pre-dicting the performance of the speech recognizer.Low speech recognition accuracy may be indicatedby low confidence scores.Finally, the WER group provides features esti-mating the similarity between the prompts and therecognition output.
In addition to the conventionalword error rate (WER), term error rate (TER) wasalso implemented for the filtering model.
TER isa metric commonly used in spoken information re-trieval, and it only accounts for errors in contentwords.
This measure may be more effective in iden-tifying NS responses than conventional WER; for in-stance, the overlap in function words between off-topic responses and prompts can be correctly ig-nored.
TER was calculated according to the follow-ing formula:dif(Wc) ={0 ifCref (Wc) < Chyp(Wc)Cref (Wc)?
Chyp(Wc) otherwiseTER =?c?WCdif(Wc)?c?WCCref (Wc)(1)where Cref (Wc) is the number of occurrences ofthe word Wc in reference, Chyp(Wc) is the numberof occurrences of the word Wc in hypothesis, andWC is the set of content words in reference.Formula 1 differs from the conventional method155Group List of featuresBasic mean/standard deviation/minimum/maximum of power, difference between maxi-mum and minimum in power, mean/standard deviation/minimum/maximum of pitch,difference between maximum and minimum in pitchFluency duration of whole speech part, number of words, speaking rate (word per sec),mean/standard deviation of silence duration, number of silences, silences per sec andsilences per wordASR score mean of confidence score, normalized Acoustic Model score by word length, normal-ized Language Model score by number of wordsWord Error Rate the word accuracy between prompt and ASR word hypothesis, correct words perminute, term error rateTable 3: List of ASR based featuresof calculating TER in two ways.
Firstly, contentwords which occurred only in the word hypothesisare ignored in the formula.
Secondly, if a word oc-curred in the word hypothesis more frequently thanin the reference, the difference is ignored.
Thesemodifications were made to address characteristicsof the responses in the data.
On the one hand, speak-ers occasionally inserted a few words such as ?toodifficult?
at the end of a response.
In addition, a fewspeakers repeated words contained in the promptmultiple times.
The two modifications to TER ad-dress both of these issues.All features from the four groups are summarizedin Table 3.4.2.2 Non-ASR based featuresA total of 12 features from four different groupswere implemented using non-ASR based methodssuch as VAD and SNR.
These features are listed inTable 4.Feature Category FeatureVAD proportion of voicedframes in response, num-ber and total duration ofvoiced regionsSyllable number of syllablesAmplitude maximum, minimum,mean, standard deviationSNR SNR, speech peakTable 4: List of non-ASR based featuresVAD related features were implemented using theESPS speech analysis program.
For every 10 mil-lisecond interval, the voice frame detector deter-mined whether the interval was voiced or not.
Threefeatures were implemented using this voiced intervalinformation: the number of voiced intervals, ratio ofvoiced intervals in the entire response, and the totalduration of voiced intervals.In addition, the number of syllables was estimatedbased on the flow of energy.
The energy of the syl-lable tends to reach its peak in the nucleus and thedip in the boundaries.
By counting the number ofsuch fluctuations in energy measurements, the num-ber of syllables can be estimated.
The Praat scriptfrom De Jong and Wempe (2009) was used for thispurpose.In order to detect the abnormalities in energy, am-plitude based features were calculated.
These fea-tures were similar to the basic features in ASR basedfeatures.Finally, if a response contains loud backgroundnoise, the ratio of speech to noise is low.
SNR, themean noise level, and the peak speech level werecomputed using the NIST audio quality assurancepackage (NIST, 2009).The VAD and syllable feature groups were de-signed to estimate the number of syllables, the pro-portion of speech to non-speech, and the total dura-tion of speech intervals.
These features were similarto the number of words and duration of speech fea-tures in the ASR-based feature set.
Despite the con-ceptual similarity, these features were implementedsince the two types of features were calculated us-ing different characteristics of the spoken response.156The VAD and syllable features are based on the flowof energy and the zero crossing rate and the ASR-based features are based on the speech recognition.In particular, the speech recognizer tends to gener-ate word hypotheses even for responses that containno speech input, but VAD does not have such a ten-dency.
Due to this difference, VAD based featuresmay be more robust in the responses with no validspeech.4.3 Model buildingFor each response, both ASR features and non-ASRfeatures were calculated.
In contrast to non-ASRfeatures, which were available for all responses,ASR features (except the Basic group) were un-available for some responses, namely, responses forwhich the ASR system did not generate any wordhypotheses because no tokens received scores abovethe rejection threshold.
This causes a missing valueproblem; about 7% of the responses did not have acomplete set of attributes.Missing values are a common problem in machinelearning.
One of the popular approaches is to replacea missing value with a unique value such as the at-tribute?s mean.
Ding and Simonoff (2008) proposeda method that replaces a missing value with an arbi-trary unique value.
This method is preferable whenmissing of a value depends on the target value andthis relationship holds in both training and test data.In this study, the missing values were replacedwith unique values due to the relationship betweenthe missing values and the target label; if the speechrecognizer did not produce any word hypotheses, theresponse was highly likely to be a NS response.
63%of the responses where the speech recognizer failedto generate word hypotheses were NS responses.Since all ASR-based features were continuous val-ues, we used two real values: 0.0 for fluency featuresand ASR features and 100.0 for word error rate fea-tures.
The fluency features and ASR features tend tobe 0.0 while the word error rate features tend to be100.0 when the responses are NS responses.A total of 42 features were used in the modelbuilding.
The only exception was WER; since WERfeatures were only available for the model basedon recited speech, they were calculated only for RSitems.
Decision tree models were trained using theJ48 algorithm (WEKA implementation of C4.5) ofWEKA machine learning toolkit (Hall et al, 2009).5 ResultsFor each item-type, three models were built to in-vestigate the impact of each feature group: a modelusing non-ASR features, a model using ASR fea-tures, and a model using both features (the ?Com-bined?
model).
Tables 5 and 6 present the accuracyof the SS models and Tables 7 and 8 present the ac-curacy of the RS models.
In all tables, the base-line was calculated using majority voting, and rep-resented a system in which no responses were clas-sified as NS; since the majority class was scorable,the baseline using the majority voting did not predictany response as non-scorable response.
Therefore,precision, recall, F-score are all 0 in this case.Model Acc.
Pre.
Rec.
F-scoreBaseline 72.5 0 0 0Non-ASR 77.0 0.645 0.364 0.465ASR 79.0 0.683 0.444 0.538Combined 78.6 0.657 0.461 0.542Table 5: Performance of the SS model in IndiaModel Acc.
Pre.
Rec.
F-scoreBaseline 66.2 0 0 0Non-ASR 68.9 0.601 0.240 0.343ASR 72.9 0.718 0.326 0.448Combined 72.9 0.720 0.323 0.446Table 6: Performance of the SS model in ChinaModel Acc.
Pre.
Rec.
F-scoreBaseline 94.8 0 0 0Non-ASR 95.7 0.684 0.210 0.321ASR 97.2 0.882 0.484 0.625Combined 96.8 0.769 0.484 0.594Table 7: Performance of the RS model in IndiaIn both item-types, the models using ASR-basedfeatures achieved the best performance.
The SSmodel achieved 79% accuracy in India and 73% ac-curacy in China, representing improvements of ap-proximately 7% over the baseline.
In both data sets,the RS model achieved high accuracies: 97% accu-racy in India and 96% accuracy in China.
In India,157Model Acc.
Pre.
Rec.
F-scoreBaseline 77.1 0 0 0Non-ASR 78.3 0.555 0.268 0.361ASR 95.6 0.942 0.860 0.899Combined 95.1 0.912 0.872 0.892Table 8: Performance of the RS model in Chinathis represents a 2.4% improvement over the base-line.
Although the absolute value of this error re-duction is not very large, the relative error reduc-tion is 46%.
In China, the improvement was moresalient; there was 18% improvement over baseline,corresponding to a relative error reduction of 78%.Additional experiments were conducted to deter-mine the robustness of the filtering models to evalu-ation data from a country not included in the train-ing data.
The evaluation sets from both item types(SS and RS) in both countries (India and China)were processed using three different models: 1) amodel trained using the ASR-based features for theresponses from the same country (the ?Same?
con-dition, whose results are identical to the ?ASR?
re-sults in Tables 5 - 8), 2) a model trained using theASR-based features for the responses from the othercountry (the ?Different?
condition), and 3) a modeltrained using the ASR-based features for the re-sponses from both countries (the ?Both?
condition).Table 9 presents the accuracy results for these foursets of experiments.ModelIndia ChinaSS RS SS RSSame 79.0 97.2 72.9 95.6Different 80.1 95.4 73.5 93.8Both 80.0 96.5 74.0 95.9Table 9: Accuracy results using training and evaluationdata from different countriesThese results show that the models are quite ro-bust to evaluation data from a different country.
Inall cases, there is at most a small decline in perfor-mance when training data from the other country isused (in the case of the SS responses, there is even aslight increase in performance).
Table 9 also showsthat the RS models performed worse in the DifferentCountry condition (compared to the Same Countrycondition) than the SS models.
This difference islikely due to the difference in the number of NS re-sponses among the RS data in the two countries (asshown in Table 2).
However, the decline is still rel-atively small, suggesting that it would be reasonableto extend the filtering models to responses from ad-ditional countries that were not seen in the trainingdata.6 DiscussionApproximately 40 features were available for themodel building, but not all features had a signifi-cant impact on the detection of NS responses.
Foreach item-type, the importance of features were fur-ther investigated using a logistic regression analysis.The training data of India and China were combined,and a stepwise logistic regression analysis was per-formed using the SPSS statistical analysis program.For each item-type, the top 3 features are pre-sented in Table 10; the features are presented in thesame order selected in the models.Model RS SSASR TER,speakingrate, s.d.
ofpitchmean of confi-dence scores,speaking rate,s.d.
of powerNon-ASR number ofsyllables,numberand du-ration ofvoicedregionsnumber of sylla-bles, s.d.
andmean of ampli-tudeCombined TER,speakingrate, s.s.dd.pitchmean of confi-dence scores,speaking rate,number ofvoiced regionsTable 10: Top 3 features in stepwise logistic regressionmodelFor the RS items, TER was the best feature and itwas the top feature for both the ASR feature basedmodel and the combined model.
The top 3 featuresin the combined model were the same as the ASRfeature based model, and non-ASR features were not158selected.
In non-ASR based features, the number ofsyllables was the best feature, followed by the VADbased features.For the SS items, the top 2 features were the samein both the ASR feature based model and the com-bined model.
The combined model selected onenon-ASR based feature, namely, a VAD based fea-ture.
As with the RS items, the number of syllableswas the best feature, followed by the energy relatedfeature.These results show the importance of WER fea-tures.
Most of the current features are designed forsignal level abnormalities such as responses withlarge background noise or non-responses.
For in-stance, fluency features and VAD features are effec-tive for non-response detection, since they can deter-mine whether the responses contain valid speech ornot.
SNR and pitch/power related features are use-ful for identifying responses with large backgroundnoise.
However, no features except the WER groupcan identify content-level abnormalities such as un-related topic and non-English responses.
The highproportion of these two types of responses (24%in India and 46% in China) may be the major ex-planation for the lower accuracy of the model forSS responses than for RS responses.
In the future,content-related features should also be developed forspontaneous speech.The features selected the first time in the logis-tic model differed according to item-types.
The re-sults support the item-type-specific model approachadopted in this paper; item-type-specific models canassign strong weights to the item-type-specific fea-tures that are most important.As shown in Tables 5 - 8, the combination of non-ASR and ASR features could not achieve any fur-ther improvement over the model consisting only ofASR based features.
However, in all cases, the non-ASR based model did lead to some improvementover the baseline.
The magnitude of this improve-ment was greater in SS items than RS items; in par-ticular, it was greatest among the SS items in theIndia data set.
This difference may be due to the dif-ferent distributions of the NS types among the datasets.
The non-ASR based features can cover onlylimited types of NS responses such as non-responsesand responses with background noise, and the pro-portion of these types is much higher among the SSresponses from India.In addition, in RS items, the poor performance ofthe combined model may be related to the high per-formance of TER.
The stepwise regression analysisshowed that the combined model did not select anyof non-ASR based features.7 ConclusionIn this study, filtering models were implemented as asupplementary module to an automated proficiencyscoring system.
Due to the difference in the avail-able features and proportion of NS responses, item-type specific models were trained.The item-types heavily influenced the overallcharacteristics of the filtering models.
First, the pro-portion of NS responses was significantly differentaccording to item-type; it was much higher in spon-taneous speech items than recited speech items.
Sec-ondly, the word error rate feature group was onlyavailable for recited speech.
Although the word er-ror rate feature group contained three features, theyimproved the performance of the filtering model sig-nificantly.ASR feature based models outperformed non-ASR feature based models, but non-ASR based fea-tures may be useful for new tests.
Finally, experi-ments demonstrated that the country-specific mod-els using the ASR-based features are relatively ro-bust to responses from a different country.
This re-sult suggests that this approach can generalize wellto speakers from different countries.In this study, large numbers of features (42 for RSitems and 39 for SS items) were used in the modeltraining, but some features were conceptually simi-lar and not all of them were significantly important;the logistic regression analsysis using traning datashowed that there was no significant improvementafter selecting 5 features for RS items and 13 fea-tures for SS items.
Use of non-significant featuresin the model training may result in the overfittingproblems.
In future research, the features will beclassified into subgroups based on their conceptualsimilarities; groups of features with high intercorre-lations will be reduced to include only the best per-forming feature in each group.
Thus, based on care-ful pre-selection procedures, only high performingfeatures will be selected, and the model will be re-159trained.In addition, many different types of NS responseswere lumped into one big category (NS); this mayincrease the confusion between scorable and non-scorable responses and decrease the model?s perfor-mance.
Some of NS types have very different char-acteristics compared to other NS types and this factcaused critical differences in the feature values.
Forinstance, non-responses contained zero or close tozero words, whereas non-English responses and off-topic responses typically had a word count similar toscorable responses.
This difference may reduce theeffectiveness of this feature.
In order to avoid thistype of problem, we will classify NS types into smallnumbers of subgroups and build a seperate model foreach subgroup.ReferencesJoon-Hyuk Chang and Nam Soo Kim.
2003.
Voice ac-tivity detection based on complex Laplacian model.Electronics Letters, 39(7):632?634.Joon-Hyuk Chang, Nam Soo Kim, and Sanjit K. Mitra.2006.
Voice activity detection based on multiple sta-tistical models.
IEEE Transactions on Signal Process-ing, 54(6):1965?1976.Nivja H. De Jong and Ton Wempe.
2009.
Praat scriptto detect syllable nuclei and measure speech rate au-tomatically.
Behavior research methods, 41(2):385?390.Yufeng Ding and Jeffrey S. Simonoff.
2008.
An investi-gation of missing data methods for classification trees.Statistics Working Papers Series.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA Data Mining Software: An Update.In SIGKDD Explorations, volume 11.Derrick Higgins, Xiaoming Xi, Klaus Zechner, andDavid Williamson.
2011.
A three-stage approachto the automated scoring of spontaneous spoken re-sponses.
Computer Speech and Language, 25:282?306, April.Guojun Lu and Templar Hankinson.
1998.
A techniquetowards automatic audio classification and retrieval.In Proceedings of the 4th International Conference onSignal Processing, volume 2, pages 1142?1145.NIST.
2009.
The NIST SPeech Quality Assurance(SPQA) Package Version 2.3. from http://www.nist.gov/speech/tools/index.htm.Jong Won Shin, Hyuk Jin Kwon, Suk Ho Jin, andNam Soo Kim.
2005.
Voice activity detection basedon generalized gamma distribution.
In Proceedingsof the IEEE International Conference on Acoustics,Speech, and Signal Processing, pages 781?784.Jongseo Sohn, Nam Soo Kim, and Wonyong Sung.1999.
A statistical model-based voice activity detec-tion.
IEEE Signal Processing Letter, 6(1):1?3.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken En-glish.
Speech Communication, 51(10):883?895.160
