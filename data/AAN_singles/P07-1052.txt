Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408?415,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsAn Ensemble Method for Selection of High Quality ParsesRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractWhile the average performance of statisti-cal parsers gradually improves, they still at-tach to many sentences annotations of ratherlow quality.
The number of such sentencesgrows when the training and test data aretaken from different domains, which is thecase for major web applications such as in-formation retrieval and question answering.In this paper we present a Sample Ensem-ble Parse Assessment (SEPA) algorithm fordetecting parse quality.
We use a functionof the agreement among several copies ofa parser, each of which trained on a differ-ent sample from the training data, to assessparse quality.
We experimented with bothgenerative and reranking parsers (Collins,Charniak and Johnson respectively).
Weshow superior results over several baselines,both when the training and test data are fromthe same domain and when they are fromdifferent domains.
For a test setting used byprevious work, we show an error reductionof 31% as opposed to their 20%.1 IntroductionMany algorithms for major NLP applications suchas information extraction (IE) and question answer-ing (QA) utilize the output of statistical parsers(see (Yates et al, 2006)).
While the average per-formance of statistical parsers gradually improves,the quality of many of the parses they produce istoo low for applications.
When the training and testdata are taken from different domains (the parseradaptation scenario) the ratio of such low qualityparses becomes even higher.
Figure 1 demonstratesthese phenomena for two leading models, Collins(1999) model 2, a generative model, and Charniakand Johnson (2005), a reranking model.
The parseradaptation scenario is the rule rather than the excep-tion for QA and IE systems, because these usuallyoperate over the highly variable Web, making it verydifficult to create a representative corpus for manualannotation.
Medium quality parses may seriouslyharm the performance of such systems.In this paper we address the problem of assess-ing parse quality, using a Sample Ensemble ParseAssessment (SEPA) algorithm.
We use the level ofagreement among several copies of a parser, each ofwhich trained on a different sample from the trainingdata, to predict the quality of a parse.
The algorithmdoes not assume uniformity of training and test data,and is thus suitable to web-based applications suchas QA and IE.Generative statistical parsers compute a probabil-ity p(a, s) for each sentence annotation, so the im-mediate technique that comes to mind for assess-ing parse quality is to simply use p(a, s).
Anotherseemingly trivial method is to assume that shortersentences would be parsed better than longer ones.However, these techniques produce results that arefar from optimal.
In Section 5 we show the superi-ority of our method over these and other baselines.Surprisingly, as far as we know there is only oneprevious work explicitly addressing this problem(Yates et al, 2006).
Their WOODWARD algorithmfilters out high quality parses by performing seman-40880 85 90 95 1000.20.40.60.81F scoreFractionofparsesCollins, IDCollins, Adap.Charniak, IDCharniak,Adap.Figure 1: F-score vs. the fraction of parses whosef-score is at least that f-score.
For the in-domainscenario, the parsers are tested on sec 23 of the WSJPenn Treebank.
For the parser adaptation scenario,they are tested on the Brown test section.
In bothcases they are trained on sections 2-21 of WSJ.tic analysis.
The present paper provides a detailedcomparison between the two algorithms, showingboth that SEPA produces superior results and thatit operates under less restrictive conditions.We experiment with both the generative parsingmodel number 2 of Collins (1999) and the rerankingparser of Charniak and Johnson (2005), both whenthe training and test data belong to the same domain(the in-domain scenario) and in the parser adapta-tion scenario.
In all four cases, we show substantialimprovement over the baselines.
The present paperis the first to use a reranking parser and the first toaddress the adaptation scenario for this problem.Section 2 discusses relevant previous work, Sec-tion 3 describes the SEPA algorithm, Sections 4 and5 present the experimental setup and results, andSection 6 discusses certain aspects of these resultsand compares SEPA to WOODWARD.2 Related WorkThe only previous work we are aware of that explic-itly addressed the problem of detecting high qualityparses in the output of statistical parsers is (Yates etal., 2006).
Based on the observation that incorrectparses often result in implausible semantic interpre-tations of sentences, they designed the WOODWARDfiltering system.
It first maps the parse produced bythe parser to a logic-based representation (relationalconjunction (RC)) and then employs four methodsfor semantically analyzing whether a conjunct in theRC is likely to be reasonable.
The filters use seman-tic information obtained from the Web.
Measuringerrors using filter f-score (see Section 3) and usingthe Collins generative model, WOODWARD reduceserrors by 67% on a set of TREC questions and by20% on a set of a 100 WSJ sentences.
Section 5provides a detailed comparison with our algorithm.Reranking algorithms (Koo and Collins, 2005;Charniak and Johnson, 2005) search the list of bestparses output by a generative parser to find a parse ofhigher quality than the parse selected by the genera-tive parser.
Thus, these algorithms in effect assessparse quality using syntactic and lexical features.The SEPA algorithm does not use such features, andis successful in detecting high quality parses evenwhen working on the output of a reranker.
Rerank-ing and SEPA are thus relatively independent.Bagging (Breiman, 1996) uses an ensemble of in-stances of a model, each trained on a sample of thetraining data1.
Bagging was suggested in order toenhance classifiers; the classification outcome wasdetermined using a majority vote among the mod-els.
In NLP, bagging was used for active learningfor text classification (Argamon-Engelson and Da-gan, 1999; McCallum and Nigam, 1998).
Specif-ically in parsing, (Henderson and Brill, 2000) ap-plied a constituent level voting scheme to an en-semble of bagged models to increase parser perfor-mance, and (Becker and Osborne, 2005) suggestedan active learning technique in which the agreementamong an ensemble of bagged parsers is used to pre-dict examples valuable for human annotation.
Theyreported experiments with small training sets only(up to 5,000 sentences), and their agreement func-tion is very different from ours.
Both works experi-mented with generative parsing models only.Ngai and Yarowsky (2000) used an ensemblebased on bagging and partitioning for active learningfor base NP chunking.
They select top items with-out any graded assessment, and their f-complementfunction, which slightly resembles our MF (see thenext section), is applied to the output of a classifier,while our function is applied to structured output.A survey of several papers dealing with mapping1Each sample is created by sampling, with replacement, Lexamples from the training pool, where L is the size of the train-ing pool.
Conversely, each of our samples is smaller than thetraining set, and is created by sampling without replacement.See Section 3 (?regarding S?)
for a discussion of this issue.409predictors in classifiers?
output to posterior proba-bilities is given in (Caruana and Niculescu-Mizil,2006).
As far as we know, the application of a sam-ple based parser ensemble for assessing parse qual-ity is novel.Many IE and QA systems rely on the output ofparsers (Kwok et al, 2001; Attardi et al, 2001;Moldovan et al, 2003).
The latter tries to addressincorrect parses using complex relaxation methods.Knowing the quality of a parse could greatly im-prove the performance of such systems.3 The Sample Ensemble Parse Assessment(SEPA) AlgorithmIn this section we detail our parse assessment algo-rithm.
Its input consists of a parsing algorithm A, anannotated training set TR, and an unannotated testset TE.
The output provides, for each test sentence,the parse generated for it by A when trained on thefull training set, and a grade assessing the parse?squality, on a continuous scale between 0 to 100.
Ap-plications are then free to select a sentence subsetthat suits their needs using our grades, e.g.
by keep-ing only high-quality parses, or by removing low-quality parses and keeping the rest.
The algorithmhas the following stages:1.
Choose N random samples of size S from thetraining set TR.
Each sample is selected with-out replacement.2.
Train N copies of the parsing algorithm A,each with one of the samples.3.
Parse the test set with each of the N models.4.
For each test sentence, compute the value of anagreement function F between the models.5.
Sort the test set according to F ?s value.The algorithm uses the level of agreement amongseveral copies of a parser, each trained on a differentsample from the training data, to predict the qual-ity of a parse.
The higher the agreement, the higherthe quality of the parse.
Our approach assumes thatif the parameters of the model are well designed toannotate a sentence with a high quality parse, thenit is likely that the model will output the same (ora highly similar) parse even if the training data issomewhat changed.
In other words, we rely on thestability of the parameters of statistical parsers.
Al-though this is not always the case, our results con-firm that strong correlation between agreement andparse quality does exist.We explored several agreement functions.
Theone that showed the best results is Mean F-score(MF)2, defined as follows.
Denote the models bym1 .
.
.mN , and the parse provided by mi for sen-tence s as mi(s).
We randomly choose a model ml,and computeMF (s) = 1N ?
1?i?
[1...N ],i6=lfscore(mi, ml) (1)We use two measures to evaluate the quality ofSEPA grades.
Both measures are defined using athreshold parameter T , addressing only sentenceswhose SEPA grades are not smaller than T .
We referto these sentences as T-sentences.The first measure is the average f-score of theparses of T-sentences.
Note that we compute thef-score of each of the selected sentences and thenaverage the results.
This stands in contrast to theway f-score is ordinarily calculated, by computingthe labeled precision and recall of the constituentsin the whole set and using these as the arguments ofthe f-score equation.
The ordinary f-score is com-puted that way mostly in order to overcome the factthat sentences differ in length.
However, for appli-cations such as IE and QA, which work at the singlesentence level and which might reach erroneous de-cision due to an inaccurate parse, normalizing oversentence lengths is less of a factor.
For this reason,in this paper we present detailed graphs for the aver-age f-score.
For completeness, Table 4 also providessome of the results using the ordinary f-score.The second measure is a generalization of the fil-ter f-score measure suggested by Yates et al (2006).They define filter precision as the ratio of correctlyparsed sentences in the filtered set (the set the algo-rithm choose) to total sentences in the filtered set andfilter recall as the ratio of correctly parsed sentencesin the filtered set to correctly parsed sentences in the2Recall that sentence f-score is defined as: f = 2?P?RP+R ,where P and R are the labeled precision and recall of the con-stituents in the sentence relative to another parse.410whole set of sentences parsed by the parser (unfil-tered set or test set).
Correctly parsed sentences aresentences whose parse got f-score of 100%.Since requiring a 100% may be too restrictive, wegeneralize this measure to filter f-score with param-eter k. In our measure, the filter recall and precisionare calculated with regard to sentences that get anf-score of k or more, rather than to correctly parsedsentences.
Filtered f-score is thus a special case ofour filtered f-score, with parameter 100.We now discuss the effect of the number of mod-els N and the sample size S. The discussion is basedon experiments (using development data, see Sec-tion 4) in which all the parameters are fixed exceptfor the parameter in question, using our developmentsections.Regarding N (see Figure 2): As the number ofmodels increases, the number of T-sentences se-lected by SEPA decreases and their quality im-proves, in terms of both average f-score and filterf-score (with k = 100).
The fact that more mod-els trained on different samples of the training dataagree on the syntactic annotation of a sentence im-plies that this syntactic pattern is less sensitive toperturbations in the training data.
The number ofsuch sentences is small and it is likely the parser willcorrectly annotate them.
The smaller T-set size leadsto a decrease in filter recall, while the better qualityleads to an increase in filter precision.
Since the in-crease in filter precision is sharper than the decreasein filter recall, filter f-score increases with the num-ber of models N .Regarding S3: As the sample size increases, thenumber of T-sentences increases, and their qual-ity degrades in terms of average f-score but im-proves in terms of filter f-score (again, with param-eter k = 100).
The overlap among smaller sam-ples is small and the data they supply is sparse.
Ifseveral models trained on such samples attach to asentence the same parse, this syntactic pattern mustbe very prominent in the training data.
The num-ber of such sentences is small and it is likely thatthe parser will correctly annotate them.
Thereforesmaller sample size leads to smaller T-sets with highaverage f-score.
As the sample size increases, the T-set becomes larger but the average f-score of a parse3Graphs are not shown due to lack of space.5 10 15 209091929394AveragefscoreNumber of models ?
N5456586062Filterfscore,k=1000 5 10 15 20657075808590Filterrecall,k=100354045505560Filterprecision,k=100Number of models ?
NFigure 2: The effect of the number of models N onSEPA (Collins?
model).
The scenario is in-domain,sample size S = 33, 000 and T = 100.
We see:average f-score of T-sentences (left, solid curve andleft y-axis), filter f-score with k = 100 (left, dashedcurve and right y-axis), filter recall with k = 100(right, solid curve and left y-axis), and filter preci-sion with k = 100 (right, dashed curve and righty-axis).decreases.
The larger T-set size leads to increase infilter recall, while the lower average quality leadsto decrease in filter precision.
Since the increase infilter recall is sharper than the decrease in filter pre-cision, the result is that filter f-score increases withthe sample size S.This discussion demonstrates the importance ofusing both average f-score and filter f-score, sincethe two measures reflect characteristics of the se-lected sample that are not necessarily highly (or pos-itively) correlated.4 Experimental SetupWe performed experiments with two parsing mod-els, the Collins (1999) generative model number2 and the Charniak and Johnson (2005) rerankingmodel.
For the first we used a reimplementation(?).
We performed experiments with each modelin two scenarios, in-domain and parser adaptation.In both experiments the training data are sections02-21 of the WSJ PennTreebank (about 40K sen-tences).
In the in-domain experiment the test datais section 23 (2416 sentences) of WSJ and in theparser adaptation scenario the test data is Brown testsection (2424 sentences).
Development sections areWSJ section 00 for the in-domain scenario (1981sentences) and Brown development section for theadaptation scenario (2424 sentences).
Following411(Gildea, 2001), the Brown test and development sec-tions consist of 10% of Brown sentences (the 9th and10th of each 10 consecutive sentences in the devel-opment and test sections respectively).We performed experiments with many configu-rations of the parameters N (number of models),S (sample size) and F (agreement function).
Dueto space limitations we describe only experimentswhere the values of the parameters N, S and F arefixed (F is MF , N and S are given in Section 5)and the threshold parameter T is changed.5 ResultsWe first explore the quality of the selected set interms of average f-score.
In Section 3 we reportedthat the quality of a selected T-set of parses increasesas the number of models N increases and samplesize S decreases.
We therefore show the results forrelatively high N (20) and relatively low S (13,000,which is about a third of the training set).
Denotethe cardinality of the set selected by SEPA by n (itis actually a function of T but we omit the T in orderto simplify notations).We use several baseline models.
The first, confi-dence baseline (CB), contains the n sentences hav-ing the highest parser assigned probability (whentrained on the whole training set).
The second, min-imum length (ML), contains the n shortest sentencesin the test set.
Since many times it is easier to parseshort sentences, a trivial way to increase the aver-age f-score measure of a set is simply to select shortsentences.
The third, following (Yates et al, 2006),is maximum recall (MR).
MR simply predicts that alltest set sentences should be contained in the selectedT-set.
The output set of this model gets filter recall of1 for any k value, but its precision is lower.
The MRbaseline is not relevant to the average f-score mea-sure, because it selects all of the sentences in a set,which leads to the same average as a random selec-tion (see below).
In order to minimize visual clutter,for the filter f-score measure we use the maximumrecall (MR) baseline rather than the minimum length(ML) baseline, since the former outperforms the lat-ter.
Thus, ML is only shown for the average f-scoremeasure.
We have also experimented with a randombaseline model (containing n randomly selected testsentences), whose results are the worst and which isshown for reference.Readers of this section may get confused betweenthe agreement threshold parameter T and the param-eter k of the filter f-score measure.
Please note: as toT , SEPA sorts the test set by the values of the agree-ment function.
One can then select only sentenceswhose agreement score is at least T .
T ?s values areon a continuous scale from 0 to 100.
As to k, the fil-ter f-score measure gives a grade.
This grade com-bines three values: (1) the number of sentences inthe set (selected by an algorithm) whose f-score rel-ative to the gold standard parse is at least k, (2) thesize of the selected set, and (3) the total number ofsentences with such a parse in the whole test set.
Wedid not introduce separate notations for these values.Figure 3 (top) shows average f-score results whereSEPA is applied to Collins?
generative model in thein-domain (left) and adaptation (middle) scenarios.SEPA outperforms the baselines for all values of theagreement threshold parameter T .
Furthermore, asT increases, not only does the SEPA set quality in-crease, but the quality differences between this setand the baseline sets increases as well.
The graphson the right show the number of sentences in the setsselected by SEPA for each T value.
As expected,this number decreases as T increases.Figure 3 (bottom) shows the same pattern of re-sults for the Charniak reranking parser in the in-domain (left) and adaptation (middle) scenarios.
Wesee that the effects of the reranker and SEPA are rel-atively independent.
Even after some of the errors ofthe generative model were corrected by the rerankerby selecting parses of higher quality among the 50-best, SEPA can detect parses of high quality fromthe set of parsed sentences.To explore the quality of the selected set in termsof filter f-score, we recall that the quality of a se-lected set of parses increases as both the number ofmodels N and the sample size S increase, and withT .
Therefore, for k = 85 .
.
.
100 we show the valueof filter f-score with parameter k when the parame-ters configuration is a relatively high N (20), rela-tively high S (33,000, which are about 80% of thetraining set), and the highest T (100).Figure 4 (top) shows filter f-score results forCollins?
generative model in the in-domain (left)and adaptation (middle) scenarios.
As these graphsshow, SEPA outperforms CB and random for all val-412ues of the filter f-score parameter k, and outper-forms the MR baseline where the value of k is 95 ormore.
Although for small k values MR gets a higherf-score than SEPA, the filter precision of SEPA ismuch higher (right, shown for adaptation.
The in-domain pattern is similar and not shown).
This stemsfrom the definition of the MR baseline, which sim-ply predicts any sentence to be in the selected set.Furthermore, since the selected set is meant to bethe input for systems that require high quality parses,what matters most is that SEPA outperforms the MRbaseline at the high k ranges.Figure 4 (bottom) shows the same pattern of re-sults for the Charniak reranking parser in the in-domain (left) and adaptation (middle) scenarios.
Asfor the average f-score measure, it demonstrates thatthe effects of the reranker and SEPA algorithm arerelatively independent.Tables 1 and 2 show the error reduction achievedby SEPA for the filter f-score measure with param-eters k = 95, 97, 100 (Table 1) and for the aver-age f-score measure with several SEPA agreementthreshold (T ) values (Table 2) .
The error reductionsachieved by SEPA for both measures are substantial.Table 3 compares SEPA and WOODWARD on theexact same test set used by (Yates et al, 2006)(taken from WSJ sec 23).
SEPA achieves error re-duction of 31% over the MR baseline on this set,compared to only 20% achieved by WOODWARD.Not shown in the table, in terms of ordinary f-scoreWOODWARD achieves error reduction of 37% whileSEPA achieves 43%.
These numbers were the onlyones reported in (Yates et al, 2006).For completeness of reference, Table 4 shows thesuperiority of SEPA over CB in terms of the usual f-score measure used by the parsing community (num-bers are counted for constituents first).
Results forother baselines are even more impressive.
The con-figuration is similar to that of Figure 3.6 DiscussionIn this paper we introduced SEPA, a novel algorithmfor assessing parse quality in the output of a statis-tical parser.
SEPA is the first algorithm shown tobe successful when a reranking parser is considered,even though such models use a reranker to detectand fix some of the errors made by the base gener-Filter f-scoreIn-domain Adaptationk value 95 97 100 95 97 100Coll.
MR 3.5 20.1 29.2 22.8 29.8 33.6Coll.
CB 11.6 11.7 3.4 14.2 9.9 7.4Char.
MR 1.35 13.6 23.44 21.9 30 32.5Char.
CB 21.9 16.8 11.9 25 20.2 16.2Table 1: Error reduction in the filter f-score mea-sure obtained by SEPA with Collins?
(top two lines)and Charniak?s (bottom two lines) model, in thetwo scenarios (in-domain and adaptation), vs. themaximum recall (MR lines 1 and 3) and confi-dence (CB, lines 2 and 4) baselines, using N =20, T = 100 and S = 33, 000.
Shown are pa-rameter values k = 95, 97, 100.
Error reductionnumbers were computed by 100?(fscoreSEPA?fscorebaseline)/(1?
fscorebaseline).Average f-scoreIn-domain AdaptationT 95 97 100 95 97 100Coll.
ML 32.6 37.2 60.8 46.8 52.7 70.7Coll.
CB 26.5 31.4 53.9 46.9 53.6 70Char.
ML 25.1 33.2 58.5 46.9 58.4 77.1Char.
CB 20.4 30 52 44.4 55.5 73.5Table 2: Error reduction in the average f-score mea-sure obtained by SEPA with Collins (top two lines)and Charniak (bottom two lines) model, in the twoscenarios (in-domain and adaptation), vs. the min-imum length (ML lines 1 and 3) and confidence(CB, lines 2 and 4) baselines, using N = 20 andS = 13, 000.
Shown are agreement threhsold pa-rameter values T = 95, 97, 100.
Error reductionnumbers were computed by 100?(fscoreSEPA?fscorebaseline)/(1?
fscorebaseline).SEPA WOODWARD CBER 31% 20% -31%Table 3: Error reduction compared to the MR base-line, measured by filter f-score with parameter 100.The data is the WSJ sec 23 test set usd by (Yateset al, 2006).
All three methods use Collins?
model.SEPA uses N = 20, S = 33, 000, T = 100.ative model.
WOODWARD, the only previously sug-gested algorithm for this problem, was tested withCollins?
generative model only.
Furthermore, this isthe first time that an algorithm for this problem suc-ceeds in a domain adaptation scenario, regardless of41385 90 95 100889092949698Agreement thresholdAveragefscoreSEPACBMLRand.85 90 95 10080859095100Agreement thresholdAveragefscoreSEPACBMLRand.85 90 95 10005001000150020002500Agreement thresholdNumber of sentencesIn domainAdaptation85 90 95 10092939495969798Agreement thresholdAveragefscoreSEPACBMLRand.85 90 95 100859095100Agreement thresholdAveragefscoreSEPACBMLRand.85 90 95 1005001000150020002500Agreement thresholdNumber of sentencesIn domainAdaptationFigure 3: Agreement threshold T vs. average f-score (left and middle) and number of sentences in the se-lected set (right), for SEPA with Collins?
generative model (top) and the Charniak reranking model (bottom).SEPA parameters are S = 13, 000, N = 20.
In both rows, SEPA results for the in-domain (left) and adap-tation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines.
Thegraphs on the right show the number of sentences in the selected set for both scenarios.85 90 95 1000.30.40.50.60.70.8KFilter fscorewithparameterkSEPACBMRRand.
85 90 95 1000.40.50.60.70.80.9KFilter fscorewithparameterkSEPACBMRRand.85 90 95 1000.20.40.60.81KFilter precisionwithparameterkSEPACBMRRand.85 90 95 1000.40.50.60.70.80.9KFilter fscorewithparameterkSEPACBMRRand.85 90 95 1000.40.50.60.70.80.91KFilter fscorewithparameterkSEPACBMRRand.85 90 95 1000.20.40.60.81KFilter precisionwithparameterkSEPA CB MR Rand.Figure 4: Parameter k vs. filter f-score (left and middle) and filter precision (right) with that parameter, forSEPA with Collins?
generative model (top) and the Charniak reranking model (bottom).
SEPA parametersare S = 33, 000, N = 20, T = 100.
In both rows, results for the in-domain (left) and adaptation (middle)scenarios.
In two leftmost graphs, the performance of the algorithm is compared to the confidence baseline(CB) and maximum recall (MR).
The graphs on the right compare the filter precision of SEPA with that ofthe MR and CB baselines.414the parsing model.
In the Web environment this isthe common situation.The WSJ and Brown experiments performed withSEPA are much broader than those performed withWOODWARD, considering all sentences of WSJ sec23 and Brown test section rather than a subsetof carefully selected sentences from WSJ sec 23.However, we did not perform a TREC experiment,as (Yates et al, 2006) did.
Our WSJ and Brownresults outperformed several baselines.
Moreover,WSJ (or Brown) sentences that contain conjunctionswere avoided in the experiments of (Yates et al,2006).
We have verified that our algorithm showssubstantial error reduction over the baselines for thistype of sentences (in the ranges 13 ?
46% for thefilter f-score with k = 100, and 30 ?
60% for theaverage f-score).As Table 3 shows, on a WSJ sec 23 test set similarto that used by (Yates et al, 2006), SEPA achieves31% error reduction compared to 20% of WOOD-WARD.WOODWARD works under several assumptions.Specifically, it requires a corpus whose content over-laps at least in part with the content of the parsedsentences.
This corpus is used to extract semanti-cally related statistics for its filters.
Furthermore, thefilters of this algorithm (except of the QA filter) arefocused on verb and preposition relations.
Thus, itis more natural for it to deal with mistakes containedin such relations.
This is reflected in the WSJ basedtest set on which it is tested.
SEPA does not makeany of these assumptions.
It does not use any exter-nal information source and is shown to select highquality parses from diverse sets.In-domain AdaptationF ER F ERSEPA Collins 97.09 44.36% 95.38 66.38%CB Collins 94.77 ?
86.3 ?SEPA Char-niak97.21 35.69% 96.3 54.66%CB Charniak 95.6 ?
91.84 ?Table 4: SEPA error reduction vs. the CB base-line in the in-domain and adaptation scenarios, us-ing the traditional f-score of the parsing literature.N = 20, S = 13, 000, T = 100.For future work, integrating SEPA into the rerank-ing process seems a promising direction for enhanc-ing overall parser performance.Acknowledgement.
We would like to thank DanRoth for his constructive comments on this paper.ReferencesShlomo Argamon-Engelson and Ido Dagan, 1996.committee-based sample selection for probabilisticclassifiers.
Journal of Artificial Intelligence Research,11:335?360.Giuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi and Alessandro Tommasi, 2001.PiQASso: Pisa question answering system.
TREC?01.Markus Becker and Miles Osborne, 2005.
A two-stagemethod for active learning of statistical grammars.
IJ-CAI ?05.Daniel Bikel, 2004.
Code developed at University ofPennsylvania.
http://www.cis.upenn.edu.bikel.Leo Breiman, 1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Rich Caruana and Alexandru Niculescu-Mizil, 2006.An empirical comparison of supervised learning algo-rithms.
ICML ?06.Eugene Charniak and Mark Johnson, 2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
ACL ?05.Michael Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Daniel Gildea, 2001.
Corpus variation and parser perfor-mance.
EMNLP ?01.John C. Henderson and Eric Brill, 2000.
Bagging andboosting a treebank parser.
NAACL ?00.Terry Koo and Michael Collins, 2005.
Hidden-variablemodels for discriminative reranking.
EMNLP ?05.Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.Scaling question answering to the web.
WWW ?01.Andrew McCallum and Kamal Nigam, 1998.
EmployingEM and pool-based active learning for text classifica-tion.
ICML ?98.Dan Moldovan, Christine Clark, Sanda Harabagiu andSteve Maiorano, 2003.
Cogex: A logic prover forquestion answering.
HLT-NAACL ?03.Grace Ngai and David Yarowsky, 2000.
Rule writing orannotation: cost-efficient resource usage for base nounphrase chunking.
ACL ?00.Alexander Yates, Stefan Schoenmackers and Oren Et-zioni, 2006.
Detecting parser errors using web-basedsemantic filters.
EMNLP ?06.415
