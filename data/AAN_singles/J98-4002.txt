Selective Sampling for Example-basedWord Sense DisambiguationAtsushi  Fujii*University of Library and InformationScienceTakenobu Tokunaga *Tokyo Institute of TechnologyKentaro Inui  tKyushu Institute of TechnologyHozumi  Tanaka ~Tokyo Institute of TechnologyThis paper proposes an efficient example sampling method for example-based word sense disam-biguation systems.
To construct a database of practical size, a considerable overhead for manualsense disambiguation (overhead for supervision) is required.
In addition, the time complexity ofsearching a large-sized atabase poses a considerable problem (overhead for search).
To counterthese problems, our method selectively samples a smaller-sized effective subset from a given ex-ample set for use in word sense disambiguation.
Our method is characterized by the reliance onthe notion of training utility: the degree to which each example is informative for future examplesampling when used for the training of the system.
The system progressively collects examplesby selecting those with greatest utility.
The paper eports the effectiveness of our method throughexperiments on about one thousand sentences.
Compared to experiments with other examplesampling methods, our method reduced both the overhead for supervision and the overhead forsearch, without he degeneration fthe performance ofthe system.1.
IntroductionWord sense disambiguation is a potentially crucial task in many NLP applications, uchas machine translation (Brown, Della Pietra, and Della Pietra 1991), parsing (Lytinen1986; Nagao 1994) and text retrieval (Krovets and Croft 1992; Voorhees 1993).
Variouscorpus-based approaches to word sense disambiguation have been proposed (Bruceand Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al 1996; Hearst 1991;Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky1995).
The use of corpus-based approaches has grown with the use of machine-readabletext, because unlike conventional rule-based approaches relying on hand-crafted selec-tional rules (some of which are reviewed, for example, by Hirst \[1987\]), corpus-basedapproaches release us from the task of generalizing observed phenomena through a setof rules.
Our verb sense disambiguation system is based on such an approach, that is,an example-based approach.
A preliminary experiment showed that our system per-forms well when compared with systems based on other approaches, and motivated* Department of Library and Information Science, University of Library and Information Science, 1-2Kasuga, Tsukuba, 305-8550, Japant Department of Artificial Intelligence, Faculty of Computer Science and Systems Engineering, KyushuInstitute of Technology, 680-4, Kawazu, Iizuka, Fukuoka 820-0067, Japan~t Department of Computer Science, Tokyo Institute of Technology, 2-12-10ookayama Meguroku Tokyo152-8552, Japan(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 4us to further explore the example-based approach (we elaborate on this experimentin Section 2.3).
At the same time, we concede that other approaches for word sensedisambiguation are worth further exploration, and while we focus on example-basedapproach in this paper, we do not wish to draw any premature conclusions regardingtlhe relative merits of different generalized approaches.As with most example-based systems (Fujii et al 1996; Kurohashi and Nagao1994; Li, Szpakowicz, and Matwin 1995; Uramoto 1994b), our system uses an exampledatabase (database, hereafter) that contains example sentences associated with eachverb sense.
Given an input sentence containing a polysemous verb, the system choosesthe most plausible verb sense from predefined candidates.
In this process, the systemcomputes a scored similarity between the input and examples in the database, andchoses the verb sense associated with the example that maximizes the score.
To realizethis, we have to manually disambiguate polysemous verbs appearing in examples,prior to their use by the system.
We shall call these examples upervised examples.A preliminary experiment on eleven polysemous Japanese verbs showed that (a) themore supervised examples we provided to the system, the better it performed, and(b) in order to achieve a reasonable r sult (say over 80% accuracy), the system neededa hundred-order supervised example set for each verb.
Therefore, in order to build anoperational system, the following problems have to be taken into account1:given human resource limitations, it is not reasonable to supervise veryexample in large corpora ("overhead for supervision"),given the fact that example-based systems, including our system, searchthe database for the examples most similar to the input, thecomputational cost becomes prohibitive if one works with a very largedatabase size ("overhead for search").These problems uggest a different approach, namely to select a small number of opti-mally informative xamples from given corpora.
Hereafter we will call these examplessamples.Our example sampling method, based on the utility maximization principle, de-cides on the preference for including a given example in the database.
This decisionprocedure is usually called selective sampling (Cohn, Atlas, and Ladner 1994).
Theoverall control flow of selective sampling systems can be depicted as in Figure 1, where"system" refers to our verb sense disambiguation system, and "examples" refers to anunsupervised example set.
The sampling process basically cycles between the wordsense disambiguation (WSD) and training phases.
During the WSD phase, the systemgenerates an interpretation for each polysemous verb contained in the input exam-ple ("WSD outputs" of Figure 1).
This phase is equivalent to normal word sensedisambiguation execution.
During the training phase, the system selects amples fortraining from the previously produced outputs.
During this phase, a human expertsupervises samples, that is, provides the correct interpretation for the verbs appearingin the samples.
Thereafter, samples are simply incorporated into the database withoutany computational overhead (as would be associated with globally reestimating pa-rameters in statistics-based systems), meaning that the system can be trained on theremaining examples (the "residue") for the next iteration.
Iterating between these two1 Note that these problems are associated with corpus-based approaches in general, and have beenidentified by a number of researchers (Engelson and Dagan 1996; Lewis and Gale 1994; Uramoto 1994a;Yarowsky 1995).574Fujii, Inui, Tokunaga, and Tanaka Selective Samplingsampling~WSD~sD out u t ~ ~  : ( ~ ~Figure 1Flow of control of the example sampling system.phases, the system progressively enhances the database.
Note that the selective sam-piing procedure gives us an optimally informative database of a given size irrespectiveof the stage at which processing is terminated.Several researchers have proposed this type of approach for NLP applications.Engelson and Dagan (1996) proposed a committee-based sampling method, whichis currently applied to HMM training for part-of-speech tagging.
This method setsseveral models (the committee) taken from a given supervised ata set, and selectssamples based on the degree of disagreement among the committee members as tothe output.
This method is implemented for statistics-based models.
How to formalizeand map the concept of selective sampling into example-based approaches has yet tobe explored.Lewis and Gale (1994) proposed an uncertainty sampling method for statistics-based text classification.
In this method, the system always samples outputs with anuncertain level of correctness.
In an example-based approach, we should also takeinto account the training effect a given example has on other unsupervised examples.This is introduced as training utility in our method.
We devote Section 4 to furthercomparison of our approach and other related works.With respect to the problem of overhead for search, possible solutions would in-clude the generalization of similar examples (Kaji, Kida, and Morimoto 1992; Nomi-yama 1993) or the reconstruction f the database using a small portion of useful in-stances elected from a given supervised example set (Aha, Kibler, and Albert 1991;Smyth and Keane 1995).
However, such approaches imply a significant overhead forsupervision of each example prior to the system's execution.
This shortcoming is pre-cisely what our approach aims to avoid: we aim to reduce the overhead for supervisionas well as the overhead for search.Section 2 describes the basis of our verb sense disambiguation system and pre-liminary experiment, in which we compared our method with other disambiguationmethods.
Section 3 then elaborates on our example sampling method.
Section 4 reportson the results of our experiments hrough comparison with other proposed selectivesampling methods, and discusses theoretical differences between those methods.2.
Example-based Verb Sense Disambiguation System2.1 The Basic IdeaOur verb sense disambiguation system is based on the method proposed by Kurohashiand Nagao (1994) and later enhanced by Fujii et al (1996).
The system uses a databasecontaining examples of collocations for each verb sense and its associated case frame(s).575Computational Linguistics Volume 24, Number 4I kane (money) } {suri(pickpocket)} sa0eu (wallet) kanojo (she) ga otoko (man) wo toru (to take/steal) ani (brother) urea (horse) aidea (idea) I kare (he) menkyoshou (license) kanojo(she)}ga {shikaku (qualification)}wotoru(toattain) gakusei (student) biza (visa)kare (he) } {shinbun(newspaper)} chichi (father) ga zasshi (journal) wo toru (to subscribe) kyaku (client){kare (he)  {kippu(t icket)} dantai (group) ga heya (room) wo toru (to reserve) ryokoukyaku (passenger) joshu (assistant) hikouki (airplane)Figure 2A fragment of the database, and the entry associated with the Japanese verb toru.Figure 2 shows a fragment of the entry associated with the Japanese verb toru.
Theverb toru has multiple senses, a sample of which are 'to take/steal,' 'to attain,' tosubscribe,' and 'to reserve.'
The database specifies the case frame(s) associated witheach verb sense.
In Japanese, a complement of a verb consists of a noun phrase (casefiller) and its case marker suffix, for example ga (nominative) or wo (accusative).
Thedatabase lists several case filler examples for each case.
The task of the system is to"interpret" he verbs occurring in the input text, i.e., to choose one sense from among aset of candidates.
2 All verb senses we use are defined in IPAL (information-technologyPromotion Agency, 1987), a machine-readable dictionary.
IPAL also contains examplecase fillers as shown in Figure 2.
Given an input, which is currently limited to a simplesentence, the system identifies the verb sense on the basis of the scored similaritybetween the input and the examples given for each verb sense.
Let us take the sentencebelow as an example input:hisho ga shindaisha wo toru.
(secretary-NOM) (sleeping car-ACC) (?
)In this example, one may consider hisho ('secretary') and shindaisha ('sleeping car')to be semantically similar to joshu ('assistant') and hikouki ('airplane') respectively, andsince both collocate with the 'to reserve' sense of toru, one could infer that toru shouldbe interpreted as 'to reserve.'
This resolution originates from the analogy principle(Nagao 1984), and can be called nearest neighbor esolution because the verb in theinput is disambiguated bysuperimposing the sense of the verb appearing in the exam-ple of highest similarity.
3 The similarity between an input and an example is estimatedbased on the similarity between case lers marked with the same case.Furthermore, since the restrictions imposed by the case fillers in choosing the verbsense are not equally selective, Fujii et al (1996) proposed aweighted case contributionto disambiguation (CCD) of the verb senses.
This CCD factor is taken into account2 Note that unlike the automatic acquisition of word sense definitions (Fukumoto and Tsujii 1994;Pustejovsky and Boguraev 1993; Utsuro 1996; Zernik 1989), the task of the system is to identify the bestmatched category with a given input, from predefined candidates.3 In this paper, we use "example-based systems" to refer to systems based on nearest neighbor esolution.576Fujii, Inui, Tokunaga, and Tanaka Selective Sampling?nominative accusativeFigure 3The semantic ranges of the nominative and accusative for the verb toru.databasenq-mc~nc nc3-rnllc3 v (?
)Cs~,c~ G,c2 &,;3 - -  v(s~)l- -  ~s3,c2 ~s3,C3 - - -  V (S3) JFigure 4An input and the database.when computing the score for each sense of the verb in question.
Consider again thecase of toru in  Figure 2.
Since the semantic range of nouns collocating with the verbin the nominative does not seem to have a strong delinearization i a semantic sense(in Figure 2, the nominative of each verb sense displays the same general concept, i.e.,HUMAN), it would be difficult, or even risky, to properly interpret the verb sense basedon similarity in the nominative.
In contrast, since the semantic ranges are disparate intile accusative, it would be feasible to rely more strongly on similarity here.This argument can be illustrated as in Figure 3, in which the symbols el and e2denote xample case fillers of different case frames, and an input sentence includes twocase fillers denoted by x and y.
The figure shows the distribution of example case fillersfor the respective case frames, denoted in a semantic space.
The semantic similaritybetween two given case fillers is represented by the physical distance between thetwo symbols.
In the nominative, since x happens to be much closer to an e2 than anyel, x may be estimated to belong to the range of e2's, although x actually belongs toboth sets of el's and e2's.
In the accusative, however, y would be properly estimatedto belong to the set of el's due to the disjunction of the two accusative case filler sets,even though examples do not fully cover each of the ranges of el's and e2's.
Notethat this difference would be critical if example data were sparse.
We will explain themethod used to compute CCD in Section 2.2.2.2 MethodologyTo illustrate the overall algorithm, we will consider an abstract specification of bothan input and the database (Figure 4).
Let the input be {no1 - reel, nc2 - mc2, nc3 - rnc3, v},where nci denotes the case filler for the case ci, and mci denotes the case marker for ci,and assume that the interpretation candidates for v are derived from the database assl, s2 and s3.
The database also contains a set Gi,cj of case filler examples for each casecj of each sense si (" - -"  indicates that the corresponding case is not allowed).During the verb sense disambiguation process, the system first discards thosecandidates whose case frame does not fit the input.
In the case of Figure 4, s3 isdiscarded because the case frame of v (s3) does not subcategorize for the case cl.577Computational Linguistics Volume 24, Number 4Table 1The relation between the length of the pathbetween two nouns nl and n2 in theBunruigoihyo thesaurus (len(nl, n2)), andtheir relative similarity (sire(n1, n2)).fen(n1, n2) 0 2 4 6 8 10 12sire(n1, n2) 11 10 9 8 7 5 0In the next step the system computes the score of the remaining candidates andchooses as the most plausible interpretation the one with the highest score.
The scoreof an interpretation is computed by considering the weighted average of the similaritydegrees of the input case fillers with respect o each of the example case lers (in thecorresponding case) listed in the database for the sense under evaluation.
Formally,this is expressed by Equation (1), where Score(s) is the score of sense s of the inputverb, and SIM(nc, G,c) is the maximum similarity degree between the input case filler ncand the corresponding case fillers in the database xample set ~s,c (calculated throughEquation (2)).
CCD(c) is the weight factor of case c, which we will explain later in thissection.Score(s) = ~c SIM(n?, &,c)' CCD(c) (1)CCD(c)SIM(nc, &,c) = max sim(nc, e) (2)eC G,cWith regard to the computation of the similarity between two different case fillers(sim(n~, e) in Equation (1)), we experimentally used two alternative approaches.
Thefirst approach uses semantic resources, that is, hand-crafted thesauri (such as the Ro-ger's thesaurus \[Chapman 1984\] or WordNet \[Miller et al 1993\] in the case of English,and Bunruigoihyo \[National Language Research Institute 1964\] or EDR \[Japan ElectronicDictionary Research Institute 1995\] in the case of Japanese), based on the intuitivelyfeasible assumption that words located near each other within the structure of a the-saurus have similar meaning.
Therefore, the similarity between two given words is:represented by the length of the path between them in the thesaurus tructure (Fujiiet al 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Uramoto1994b).
4 We used the similarity function empirically identified by Kurohashi and Na-gao in which the relation between the length of the path in the Bunruigoihyo thesaurusand the similarity between words is defined as shown in Table 1.
In this thesaurus,each entry is assigned a seven-digit class code.
In other words, this thesaurus can beconsidered as a tree, seven levels in depth, with each leaf as a set of words.
Figure 5shows a fragment of the Bunruigoihyo thesaurus including some of the nouns in bothFigure 2 and the input sentence above.The second approach is based on statistical modeling.
We adopted one typicalimplementation called the "vector space model" (VSM) (Frakes and Baeza-Yates 1992;Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which hasa long history of application in information retrieval (IR) and text categorization (TC)tasks.
In the case of IR/TC, VSM is used to compute the similarity between documents,which is represented by a vector comprising statistical factors of content words in adocument.
Similarly, in our case, each noun is represented by a vector comprising4 Different ypes of application ofhand-crafted thesauri to word sense disambiguation have beenproposed, for example, by Yarowsky (1992).578Fujii, Inui, Tokunaga, and Tanaka Selective SamplingI Ikare kanojo(he) (she)I I I I Iotoko joshu hisho kane heya kippu uma(man) (assistant) (secretary)(money) (room) (ticket) (horse)Figure 5A fragment of the Bunruigoihyo thesaurus.statistical factors, although statistical factors are calculated in terms of the predicateargument structure in which each noun appears.
Predicate argument structures, whichconsist of complements (case filler nouns and case markers) and verbs, have alsobeen used in the task of noun classification (Hindle 1990).
This can be expressed byEquation (3), where ff is the vector for the noun in question, and items ti represent thestatistics for predicate argument structures including n.ff = (h, t2, .
.
.
,  ti .
.
.
.  )
(3)In regard to ti, we used the notion of TF.
IDF (Salton and McGill 1983).
TF (termfrequency) gives each context (a case marker/verb pair) importance proportional tothe number of times it occurs with a given noun.
The rationale behind IDF (inversedocument frequency) is that contexts that rarely occur over collections of nouns arevaluable, and that therefore the IDF of a context is inversely proportional to the numberof noun types that appear in that context.
This notion is expressed by Equation (4),where f ( (n ,  c, v)) is the frequency of the tuple (n, c, v), nf((c, v)) is the number of nountypes which collocate with verb v in the case c, and N is the number of noun typeswithin the overall co-occurrence data.Nti =d((n, c, v)).
log nf((c, v)) (4)We compute the similarity between ouns nt and n2 by the cosine of the angle betweenthe two vectors t~ and n2.
This is realized by Equation (5)./ I /1  " n2sire(n1, n2) - ~ \]\]~21 (5)We extracted co-occurrence data from the RWC text base RWC-DB-TEXT-95-1 (RealWorld Computing Partnership 1995).
This text base consists of four years worth ofMainichi Shimbun newspaper articles (Mainichi Shimbun 1991-1994), which have beenautomatically annotated with morphological tags.
The total morpheme content is aboutone hundred million.
Since full parsing is usually expensive, a simple heuristic rulewas used to obtain collocations of nouns, case markers, and verbs in the form of tuples(n, c, v).
This rule systematically associates each sequence of noun and case marker tothe verb of highest proximity, and produced 419,132 tuples.
This co-occurrence datawas used in the preliminary experiment described in Section 2.3. s5 Note that each verb in co-occurrence data should ideally be annotated with its verb sense.
However,there is no existing Japanese text base with sufficient volume of word sense tags.579Computational Linguistics Volume 24, Number 4In Equation (1), CCD(c) expresses the weight factor of the contribution of case c to(current) verb sense disambiguation.
Intuitively, preference should be given to casesdisplaying case fillers that are classified in semantic ategories of greater disjunction.Thus, c's contribution to the sense disambiguation of a given verb, CCD(c), is likelyto be higher if the example case filler sets {gsi,c I i = 1,. .
.
,  n} share fewer elements, asin Equation (6).C' 1 r~- I  _ _  ~ CCD(c) = ~,7~ ~ ~ Igs''?l + \]?s,,cl 2\[?s~,c r"l ?s,,~lj=i+l I&,,I 7 I'G*I ) (6)Here, o~ is a constant for pararneterizing the extent to which CCD influences verbsense disambiguation.
The larger oe is, the stronger is CCD's influence on the systemoutput.
To avoid data sparseness, we smooth each element (noun example) in gsi,c.
Inpractice, this involves generalizing each example noun into a five-digit class based onthe Bunruigoihyo thesaurus, as has been commonly used for smoothing.2.3 Preliminary ExperimentationWe estimated the performance of our verb sense disambiguation method through anexperiment, in which we compared the following five methods:?
lower bound (LB), in which the system systematically chooses the mostfrequently appearing verb sense in the database (Gale, Church, andYarowsky 1992),?
rule-based method (RB), in which the system uses a thesaurus to(automatically) identify appropriate semantic lasses as selectionalrestrictions for each verb complement,?
Naive-Bayes method (NB), in which the system interprets a given verbbased on the probability that it takes each verb sense,?
example-based method using the vector space model (VSM), in whichthe system uses the above mentioned co-occurrence data extracted fromthe RWC text base,?
example-based method using the Bunruigoihyo thesaurus (BGH), inwhich the system uses Table 1 for the similarity computation.In the rule-based method, selectional restrictions are represented by thesaurusclasses, and allow only those nouns dominated by the given class in the thesaurusstructure as verb complements.
In order to identify appropriate thesaurus classes,we used the association measure proposed by Resnik (1993), which computes theinformation-theoretic association degree between case fillers and thesaurus classes,for each verb sense (Equation (7)).
6P(rls, c) A(s,c,r) = P(rls, c ) ?
log p(rlc) (7)6 Note that previous research as applied this technique to tasks other than verb sense disambiguation,such as syntactic disambiguation (Resnik 1993) and disambiguation f case filler noun senses (Ribas1995).580Fujii, Inui, Tokunaga, and Tanaka Selective SamplingHere, A(s, c, r) is the association degree between verb sense s and class r (selectionalrestriction candidate) with respect o case c. P(rls, c) is the conditional probability thata case filler example associated with case c of sense s is dominated by class r in thethesaurus.
P(rlc ) is the conditional probability that a case filler example for case c(disregarding verb sense) is dominated by class r. Each probability is estimated basedon training data.
We used the semantic lasses defined in the Bunruigoihyo thesaurus.In practice, every r whose association degree is above a certain threshold is chosenas a selectional restriction (Resnik 1993; Ribas 1995).
By decreasing the value of thethreshold, system coverage can be broadened, but this opens the way for irrelevant(noisy) selectional rules.The Naive-Bayes method assumes that each case filler included in a given input isconditionally independent of other case fillers: the system approximates the probabilitythat an input x takes a verb sense s (P(slx)), simply by computing the product of theprobability that each verb sense s takes nc as a case filler for case c. The verb sensewith maximal probability is then selected as the interpretation (Equation (8)).
7arg msax P(slx) P(s)  .
P(xls)  = arg msax P(x)= argn~axP(s).
P(xls)argmaxP(s) I I  P(ncls)c(8)Here, P(ncls) is the probability that a case filler associated with sense s for case c in thetraining data is nc.
We estimated P(s) based on the distribution of the verb senses in thetraining data.
In practice, data sparseness leads to not all case fillers nc appearing inthe database, so we generalize ach nc into a semantic lass defined in the Bunruigoihyothesaurus.All methods except he lower bound method involve a parametric onstant: thethreshold value for the association degree (RB), a generalization level for case fillernouns (NB), and a in Equation (6) (VSM and BGH).
For these parameters, we con-ducted several trials prior to the actual comparative xperiment, o determine theoptimal parameter values over a range of data sets.
For our method, we set a ex-tremely large, which is equivalent to relying almost solely on the SIM of the case withthe greatest CCD.
However, note that when the SIM of the case with the greatest CCDis equal for multiple verb senses, the system computes the SIM of the case with thesecond highest CCD.
This process is repeated until only one verb sense remains.
Whenmore than one verb sense is selected for any given method (or none of them remains,for the rule-based method), the system simply selects the verb sense that appears mostfrequently in the database, sIn the experiment, we conducted sixfold cross-validation, that is, we divided thetraining/test data into six equal parts, and conducted six trials in which a different7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for wordsense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997).8 One may argue that this goes against he basis of the rule-based method, in that, given a properthreshold value for the association degree, the system could improve on accuracy (potentiallysacrificing coverage), and that the trade-off between coverage and accuracy is therefore a moreappropriate valuation criterion.
However, our trials on the rule-based method with different hresholdvalues did not show significant correlation between the improvement of accuracy and the degenerationof coverage.581Computational Linguistics Volume 24, Number 4Table 2The verbs contained in the corpus used, and the accuracy of the different verb sensedisambiguation methods (LB: lower bound, RB: rule-based method, NB: Naive-Bayesmethod, VSM: vector space model, BGH: the Bunruigoihyo thesaurus).Verb# of # of Accuracy (%)English Gloss Sentences Senses LB RB NB VSM BGHataeru give 136 4 66.9 62.1 75.8 84.1 86.0kakeru hang 160 29 25.6 24.6 67.6 73.4 76.2kuwaeru add 167 5 53.9 65.6 82.2 84.0 86.8motomeru require 204 4 85.3 82.4 87.0 85.5 85.5noru ride 126 10 45.2 52.8 81.4 80.5 85.3osameru govern 108 8 30.6 45.6 66.0 72.0 74.5tsukuru make 126 15 25.4 24.9 59.1 56.5 69.9toru take 84 29 26.2 16.2 56.1 71.2 75.9umu bear offspring 90 2 83.3 94.7 95.5 92.0 99.4wakaru understand 60 5 48.3 40.6 71.4 62.5 70.7yameru stop 54 2 59.3 89.9 92.3 96.2 96.3total - -  1,315 - -  51.4 54.8 76.6 78.6 82.3part was used as test data each time, and the rest as training data (the database).
9 Weevaluated the performance of each method according to its accuracy, that is, the ratioof the number  of correct outputs compared to the total number  of inputs.
The train-ing/test  data used in the experiment contained about one thousand simple Japanesesentences collected from news articles.
Each sentence in the training/test data con-tained one or more complement(s) fol lowed by one of the eleven verbs described inTable 2.
In Table 2, the column "English Gloss" describes typical English translationsof the Japanese verbs.
The column "# of Sentences" denotes the number  of sentencesin the corpus, and "# of Senses" denotes the number  of verb senses contained in IPAL.The column "accuracy" shows the accuracy of each method.Looking at Table 2, one can see that our example-based method performed bet-ter than the other methods (irrespective of the similarity computation), although theNaive-Bayes method is relatively comparable in performance.
Surprisingly, despite therelatively ad hoc similarity definition used (see Table 1), the Bunruigoihyo thesaurusled to a greater accuracy gain than the vector space model.
In order to estimate theupper  bound (limitation) of the disambiguation task, that is, to what extent a humanexpert makes errors in disambiguation (Gale, Church, and Yarowsky 1992), we ana-lyzed incorrect outputs and found that roughly 30% of the system errors using theBunruigoihyo thesaurus fell into this category.
It should be noted that while the vectorspace model  requires computational cost ( t ime/memory)  of an order proportional tothe size of the vector, determination of paths in the Bunruigoihyo thesaurus comprisesa trivial cost.We also investigated errors made by the rule-based method to find a rational ex-planation for its inferiority.
We found that the association measure in Equation (7)tends to give a greater value to less frequently appearing verb senses and lower level9 Ideally speaking, training and test data should be drawn from different sources, to simulate a realapplication.
However, the sentences were already scrambled when provided to us, and therefore wecould not identify the original source corresponding toeach sentence.582Fujii, Inui, Tokunaga, and Tanaka Selective Sampling(more specified) classes, and therefore chosen rules are generally overspecified.
1?
Con-sequently, frequently appearing verb senses are likely to be rejected.
On the otherhand, when attempting to enhance the rule set by setting a smaller threshold valuefor the association score, overgeneralization can be a problem.
We also note that oneof the theoretical differences between the rule-based and example-based methods isthat the former statically generalizes examples (prior to system usage), while the lat-ter does so dynamically.
Static generalization would appear to be relatively risky forsparse training data.Although comparison of different approaches toword sense disambiguation shouldbe further investigated, this experimental result gives us good motivation to exploreexample-based verb sense disambiguation approaches, i.e., to introduce the notion ofselective sampling into them.2.4 Enhancement of Verb Sense DisambiguationLet us discuss how further enhancements o our example-based verb sense disam-biguation system could be made.
First, since inputs are simple sentences, informationfor word sense disambiguation is inadequate in some cases.
External information suchas the discourse or domain dependency of each word sense (Guthrie et al 1991; Na-sukawa 1993; Yarowsky 1995) is expected to lead to system improvement.
Second,some idiomatic expressions represent highly restricted collocations, and overgener-alizing them semantically through the use of a thesaurus can cause further errors.Possible solutions would include one proposed by Uramoto, in which idiomatic ex-pressions are described separately in the database so that the system can control theirovergeneralization (Uramoto 1994b).
Third, a number of existing NLP tools such asJUMAN (a morphological nalyzer) (Matsumoto et al 1993) and QJP (a morphologicaland syntactic analyzer) (Kameda 1996) could broaden the coverage of our system, asinputs are currently limited to simple, morphologically analyzed sentences.
Finally, itshould be noted that in Japanese, case markers can be omitted or topicalized (for exam-ple, marked with postposition wa), an issue which our framework does not currentlyconsider.3.
Example Sampling Algorithm3.1 OverviewLet us look again at Figure 1 in Section 1.
In this figure, "WSD outputs" refers to acorpus in which each sentence is assigned an expected verb interpretation during theWSD phase.
In the training phase, the system stores supervised samples (with eachinterpretation simply checked or appropriately corrected by a human) in the database,to be used in a later WSD phase.
In this section, we turn to the problem of whichexamples hould be selected as samples.Lewis and Gale (1994) proposed the notion of uncertainty sampling for the trainingof statistics-based text classifiers.
Their method selects those examples that the systemclassifies with minimum certainty, based on the assumption that there is no need forteaching the system the correct answer when it has answered with sufficiently highcertainty.
However, we should take into account he training effect a given example hason other remaining (unsupervised) examples.
In other words, we would like to selectsamples o as to be able to correctly disambiguate as many examples as possible in thenext iteration.
If this is successfully done, the number of examples to be supervised will10 This problem has also been identified by Charniak (1993).583Computational Linguistics Volume 24, Number 4el: seito ga (student-NOM) shitsumon wo (question-ACC) yameru (sl)e2: ani ga (brother-NOM) kaisha wo (company-ACC) yameru (s2)Xl: shain ga (employee-NOM) eigyou wo (sales-ACC) yameru (?
)x2: shouten ga (store-NOM) eigyou wo (sales-ACC) yameru (?
)x3: koujou ga (factory-NOM) sougyou wo (operation-ACC) yameru (?
)x4: shisetsu ga (facility-NOM) unten wo (operation-ACC) yameru (?
)xs: senshu ga (athlete-NOM) renshuu wo (pracfice-ACC) yameru (?
)x6: musuko ga (son-NOM) kaisha wo (company-ACC) yameru (?
)x7: kangofu ga (nurse-NOM) byouin wo (hospital-ACC) yameru (?
)x8: hikoku ga (defendant-NOM) giin wo (congressman-ACC) yameru (?
)xg: chichi ga (father-NOM) kyoushi wo (teacher-ACC) yameru (?
)Figure 6Example of a given corpus associated with the verb yameru.decrease.
We consider maximization of this effect by means of a training utility functionaimed at ensuring that the most useful example at a given point in time is the examplewith the greatest training utility factor.
Intuitively speaking, the training utility of anexample is greater when we can expect greater increase in the interpretation certaintyof the remaining examples after training using that example.To explain this notion intuitively, let us take Figure 6 as an example corpus.
Inthis corpus, all sentences contain the verb yameru, which has two senses accordingto IPAL, sl ('to stop (something)') and s2 ('to quit (occupation)').
In this figure, sen-tences el and e2 are supervised examples associated with senses Sl and s2, respectively,and xi's are unsupervised examples.
For the sake of enhanced readability, the exam-ples xi's are partitioned according to their verb senses, that is, xl to x5 correspondto sense Sl, and x6 to x9 correspond to sense s2.
In addition, note that examples inthe corpus can be readily categorized based on case similarity, that is, into clusters{Xl, X2, X3, X4} ('someone/something stops service'), {Ca, X6, X7} ('someone leaves orga-nization'), {Xs, X9} ('someone quits occupation'), {el}, and {Xs}.
Let us simulate thesampling procedure with this example corpus.
In the initial stage with {el, e2} in thedatabase, x6 and x7 can be interpreted as s2 with greater certainty than for the otherxi's, because these two examples are similar to e2.
Therefore, uncertainty samplingselects any example xcept x6 and x7 as the sample.
However, any one of examples Xlto x4 is more desirable because by way of incorporating one of these examples, we canobtain more xi's with greater certainty.
Assuming that Xl is selected as the sample andincorporated into the database with sense Sl, either of x8 and x9 will be more highlydesirable than other unsupervised xi's in the next stage.Let S be a set of sentences, i.e., a given corpus, and D be the subset of supervisedexamples tored in the database.
Further, let X be the set of unsupervised examples,realizing Equation (9).S = D u X (9)The example sampling procedure can be illustrated as:1.
WSD(D,  X)2. e ~ arg maxx~x TU(x)3.
D ~--D U {e}, X ~-- X n {e}4. goto 1where WSD(D,  X) is the verb sense disambiguation process on input X using D asthe database.
In this disambiguation process, the system outputs the following for584Fujii, Inui, Tokunaga, and Tanaka Selective SamplingX Xsense I(x e xl  x f sense2  (:ex)x , , .
.
.
_yX XA"sense IIx  d sense 2e t 'x) x )X(a) (b)Figure 7The concept of interpretation certainty.
The case where the interpretation certainty of theenclosed x's is great is shown in (a).
The case where the interpretation certainty of the x'scontained in the intersection of senses 1 and 2 is small is shown in (b).each input: (a) a set of verb sense candidates with interpretation scores, and (b) aninterpretation certainty.
These factors are used for the computation of TU(x), newlyintroduced in our method.
TU(x) computes the training utility factor for an examplex.
The sampling algorithm gives preference to examples of maximum utility.We will explain in the following sections how TU(x) is estimated, based on theestimation of the interpretation certainty.3.2 Interpretation CertaintyLewis and Gale (1994) estimate certainty of an interpretation asthe ratio between theprobability of the most plausible text category and the probability of any other textcategory, excluding the most probable one.
Similarly, in our verb sense disambiguationsystem, we introduce the notion of interpretation certainty of examples based on thefollowing preference conditions:.2.the highest interpretation score is greater,the difference between the highest and second highest interpretationscores is greater.The rationale for these conditions is given below.
Consider Figure 7, where each sym-bol denotes an example in a given corpus, with symbols x as unsupervised examplesand symbols e as supervised examples.
The curved lines delimit the semantic vicini-ties (extents) of the two verb senses 1 and 2, respectively.
11 The semantic similaritybetween two examples is graphically portrayed by the physical distance between thetwo symbols representing them.
In Figure 7(a), x's located inside a semantic vicinityare expected to be interpreted as being similar to the appropriate xample with highcertainty, a fact which is in line with condition 1 above.
However, in Figure 7(b), thedegree of certainty for the interpretation of any x located inside the intersection ofthe two semantic vicinities cannot be great.
This occurs when the case fillers associ-11 Note that this method can easily be extended for a verb with more than two senses.
In Section 4, wedescribe an experiment using multiply polysemous verbs.585Computational Linguistics Volume 24, Number 4100959085' ' ~5  ?
JO - - - -X  .
.
.
.80 i60 70 80 90coverage (%)100Figure 8The relation between coverage and accuracy with different A's.ated with two or more verb senses are not selective nough to allow for a clear-cutdelineation between them.
This situation is explicitly rejected by condition 2.Based on the above two conditions, we compute interpretation certainties usingEquation (10), where C(x) is the interpretation certainty of an example x, Scorel(x) andScore2(x) are the highest and second highest scores for x, respectively, and ,~, whichranges from 0 to 1, is a parametric constant used to control the degree to which eachcondition affects the computation of C(x).C(x)=A.
Scorel(x)+(1 - ~).
(Scorel(x) - Score2(x)) (10)Through a preliminary experiment, we estimated the validity of the notion ofinterpretation certainty, by the trade-off between accuracy and coverage of the system.Note that in this experiment, accuracy is the ratio of the number of correct outputsand the number of cases where the interpretation certainty of the output is above acertain threshold.
Coverage is the ratio of the number of cases where the interpretationcertainty of the output is above a certain threshold and the number of inputs.
Byraising the value of the threshold, accuracy also increases (at least theoretically), whilecoverage decreases.The system used the Bunruigoihyo thesaurus for the similarity computation, andwas evaluated by way of sixfold cross-validation using the same corpus as that usedfor the experiment described in Section 2.3.
Figure 8 shows the result of the experimentwith several values of ,~, from which the optimal )~ value seems to be in the rangearound 0.5.
It can be seen that, as we assumed, both of the above conditions areessential for the estimation of interpretation certainty.586Fujii, Inui, Tokunaga, and Tanaka Selective Samplingcertaintyx x x a x x x (( x b x(a)AXcertaintyA" X X (I ,~" ,?
X (( x x x e x b x x x(b)Figure 9The concept of training utility.
The case where the training utility of a is greater than that of bbecause ahas more unsupervised neighbors i  shown in (a); (b) shows the case where thetraining utility of a is greater than that of b because bclosely neighbors e, contained in thedatabase.3.3 Training UtilityThe training utility of an example a is greater than that of another example bwhen thetotal interpretation certainty of unsupervised examples increases more after trainingwith example a than with example b.
Let us consider Figure 9, in which the x-axismono-dimensionally denotes the semantic similarity between two unsupervised ex-amples, and the y-axis denotes the interpretation certainty of each example.
Let uscompare the training utility of the examples a and b in Figure 9(a).
Note that in thisfigure, whichever example we use for training, the interpretation certainty for eachunsupervised example (x) neighboring the chosen example increases based on its sim-ilarity to the supervised example.
Since the increase in the interpretation certainty ofa given x becomes maller as the similarity to a or b diminishes, the training utilityof the two examples can be represented by the shaded areas.
The training utility ofa is greater as it has more neighbors than b.
On the other hand, in Figure 9(b), b hasmore neighbors than a.
However, since b is semantically similar to e, which is alreadycontained in the database, the total increase in interpretation certainty of its neighbors,i.e.
the training utility of b, is smaller than that of a.Let AC(x = s,y) be the difference in the interpretation certainty of y c X aftertraining with x c X, taken with the sense s. TU(x = s), which is the training utilityfunction for x taken with sense s, can be computed by Equation (11).TU(x = s) = E AC(x = s,y) (11)yEXIt should be noted that in Equation (11), we can replace X with a subset of X thatconsists of neighbors of x.
However, in order to facilitate this, an efficient algorithmto search for neighbors of an example is required.
We will discuss this problem inSection 3.5.Since there is no guarantee that x will be supervised with any given sense s, it canbe risky to rely solely on TU(x = s) for the computation of TU(x).
We estimate TU(x),by the expected value of x, calculating the average of each TU(x = s), weighted by theprobability that x takes sense s. This can be realized by Equation (12), where P(slx ) isthe probability that x takes the sense s.TU(x) = E P(slx) " TU(x = s) (12)sGiven the fact that (a) P(sIx ) is difficult to estimate in the current formulation, and (b)the cost of computation for each TU(x = s) is not trivial, we temporarily approximate587Computational Linguistics Volume 24, Number 4TU(x) as in Equation (13), where K is a set of the k-best verb sense(s) of x with respectto the interpretation score in the current state.1 E TU(x = s) (13) TU(x) ;sEK3.4 Enhancement of ComputationIn this section, we discuss how to enhance the computation associated with our ex-ample sampling algorithm.First, we note that computation of TU(x = s) in Equation (11) above becomes timeconsuming because the system is required to search the whole set of unsupervisedexamples for examples whose interpretation certainty will increase after x is used fortraining.
To avoid this problem, we could apply a method used in efficient databasesearch techniques, by which the system can search for neighbor examples of x withoptimal time complexity (Utsuro et al 1994).
However, in this section, we will explainanother efficient algorithm to identify neighbors of x, in which neighbors of case fillersare considered to be given directly by the thesaurus structure.
12The basic idea is thefollowing: the system searches for neighbors of each case filler of x instead of x as awhole, and merges them as a set of neighbors of x.
Note that by dividing examplesalong the lines of each case filler, we can retrieve neighbors based on the structure ofthe Bunruigoihyo thesaurus (instead of the conceptual semantic space as in Figure 7).Let Nx=s,c be a subset of unsupervised neighbors of x whose interpretation certaintywill increase after x is used for training, considering only case c of sense s. The actualneighbor set of x with sense s (Nx=s) is then defined as in Equation (14).Nx=s=UNx:s,c (14)?Figure 10 shows a fragment of the thesaurus, in which the x and the y's are unsu-pervised case filler examples.
Symbols el and e2 are case filler examples tored in thedatabase taken as senses l and s2, respectively.
The triangles represent subtrees of thestructure, and the labels ni represent nodes.
In this figure, it can be seen that the inter-pretation score of Sl never changes for examples other than the children of n4, after xis used for training with sense Sl.
In addition, incorporating x into the database withsense sl never changes the score of examples y for other sense candidates.
Therefore,Nx=sl,c includes only examples dominated by n4, in other words, examples that arecloser to x than el in the thesaurus structure.
Since, during the WSD phase, the systemdetermines el as the supervised neighbor of x for sense Sl, identifying Nx=sl,c doesnot require any extra computational overhead.
We should point out that the techniquepresented here is not applicable when the vector space model (see Section 2.2) is usedfor the similarity computation.
However, automatic lustering algorithms, which as-sign a hierarchy to a set of words based on the similarity between them (such as theone proposed by Tokunaga, Iwayama, and Tanaka \[1995\]), could potentially facilitatethe application of this retrieval method to the vector space model.Second, sample size at each iteration should ideally be one, so as to avoid thesupervision of similar examples.
On the other hand, a small sampling size generatesa considerable computation overhead for each iteration of the sampling procedure.This can be a critical problem for statistics-based approaches, as the reconstruction12 Utsuro's method requires the constiuction f large-scale similarity templates prior to similaritycomputation (Utsuro et al 1994), and this is what we would like to avoid.588Fujii, Inui, Tokunaga, nd Tanaka Selective SamplingJ?7,2 n5  L 7~3 7~4el y y y x y e2 y yFigure 10A fragment of the thesaurus including neighbors of x associated with case c.of statistic lassifiers is expensive.
However, example-based systems fortunately donot require reconstruction, and examples imply have to be stored in the database.Furthermore, in each disambiguation phase, our example-based system needs onlyto compute the similarity between each newly stored example and its unsupervisedneighbors, rather than between every example in the database and every unsupervisedexample.
Let us reconsider Figure 10.
As mentioned above, when x is stored in thedatabase with sense sl, only the interpretation score of y's dominated by n4, i.e., Nx=sl,c,will be changed with respect o sense sl.
This algorithm reduces the time complexityof each iteration from O(N 2) to O(N), given that N is the total number of examples ina given corpus.3.5 Discussion3.5.1 Sense Ambiguity of Case Fillers in Selective Sampling.
The semantic ambi-guity of case fillers (nouns) should be taken into account during selective sampling.Figure 11, which uses the same basic notation as Figure 7, illustrates one possibleproblem caused by case filler ambiguity.
Let xl be a sense of a case filler x, and Yl andy2 be different senses of a case filler y.
On the basis of Equation (10), the interpretationcertainty of x and y is small in Figures 11(a) and 11(b), respectively.
However, in thesituation shown in Figure 11(b), since (a) the task of distinguishing between the verbsenses 1 and 2 is easier, and (b) instances where the sense ambiguity of case fillerscorresponds to distinct verb senses will be rare, training using either yl or y2 will beless effective than using a case filler of the type of x.
It should also be noted that sinceBunruigoihyo is a relatively small-sized thesaurus with limited word sense coverage,this problem is not critical in our case.
However, given other existing thesauri likethe EDR electronic dictionary (Japan Electronic Dictionary Research Institute 1995) orWordNet (Miller et al 1993), these two situations hould be strictly differentiated.3.5.2 A Limitation of our Selective Sampling Method.
Figure 12, where the basicnotation is the same as in Figure 7, exemplifies a limitation of our sampling method.In this figure, the only supervised examples contained in the database are el and e2,and x represents an unsupervised example belonging to sense 2.
Given this scenario,x is informative because (a) it clearly evidences the semantic vicinity of sense 2, and(b) without x as sense 2 in the database, the system may misinterpret o her examplesneighboring x.However, in our current implementation, the training utility of x wouldbe small because itwould be mistakenly interpreted as sense I with great certainty dueto its relatively close semantic proximity to el.
Even if x has a number of unsupervisedneighbors, the total increment of their interpretation certainty cannot be expected to belarge.
This shortcoming often presents itself when the semantic vicinities of different589Computational Linguistics Volume 24, Number 4sense 1/ (.~sense 2sense 1 sense 2(a) (b)Figure 11Two separate scenarios in which the interpretation certainty of x is small.
In (a), interpretationcertainty of x is small because x lies in the intersection of distinct verb senses; in (b),interpretation certainty of y is small because y is semantically ambiguous.sense ~ sense 2Figure 12The case where informative xample x is not selected.verb senses are closely aligned or their semantic ranges are not disjunctive.
Here, letus consider Figure 3 again, in which the nominative case would parallel the semanticspace shown in Figure 12 more closely than the accusative.
Relying more on thesimilarity in the accusative (the case with greater CCD) as is done in our system, weaim to map the semantic space in such a way as to achieve higher semantic disparityand minimize this shortcoming.4.
Evaluation4.1 Comparative Experimentation~n order to investigate the effectiveness of our example sampling method, we con-ducted an experiment in which we compared the following four sampling methods:?
a control (random), in which a certain proportion of a given corpus israndomly selected for training,?
uncertainty sampling (US), in which examples with minimuminterpretation certainty are selected (Lewis and Gale 1994),?
committee-based sampling (CBS) (Engelson and Dagan 1996),?
our method based on the notion of training utility (TU).590Fujii, Inui, Tokunaga, and Tanaka Selective SamplingWe elaborate on uncertainty sampling and committee-based sampling in Section 4.2.We compared these sampling methods by evaluating the relation between the num-ber of training examples ampled and the performance of the system.
We conductedsixfold cross-validation and carried out sampling on the training set.
With regard tothe training/test data set, we used the same corpus as that used for the experimentdescribed in Section 2.3.
Each sampling method uses examples from IPAL to initializethe system, with the number of example case fillers for each case being an averageof about 3.7.
For each sampling method, the system uses the Bunruigoihyo thesaurusfor the similarity computation.
In Table 2 (in Section 2.3), the column of "accuracy"for "BGH" denotes the accuracy of the system with the entire set of training datacontained in the database.
Each of the four sampling methods achieved this figure atthe conclusion of training.We evaluated each system performance according to its accuracy, that is the ratioof the number of correct outputs, compared to the total number of inputs.
For thepurpose of this experiment, we set the sample size to 1 for each iteration, A = 0.5for Equation (10), and k = 1 for Equation (13).
Based on a preliminary experiment,increasing the value of k either did not improve the performance over that for k = 1,or lowered the overall performance.
Figure 13 shows the relation between the numberof training data sampled and the accuracy of the system.
In Figure 13, zero on thex-axis represents the system using only the examples provided by 1PAL.
Looking atFigure 13 one can see that compared with random sampling and committee-basedsampling, our sampling method reduced the number of the training data required toachieve any given accuracy.
For example, to achieve an accuracy of 80%, the numberof training data required for our method was roughly one-third of that for randomsampling.
Although the accuracy of our method was surpassed by that of uncertaintysampling for larger sizes of training data, this minimal difference for larger data sizesis overshadowed by the considerable performance gain attained by our method forsmaller data sizes.Since IPAL has, in a sense, been manually selectively sampled in an attempt omodel the maximum verb sense coverage, the performance of each method is biasedby the initial contents of the database.
To counter this effect, we also conducted anexperiment involving the construction of the database from scratch, without using ex-amples from IPAL.
During the initial phase, the system randomly selected one examplefor each verb sense from the training set, and a human expert provided the correctinterpretation to initialize the system.
Figure 14 shows the performance of the variousmethods, from which the same general tendency as seen in Figure 13 is observable.However, in this case, our method was generally superior to other methods.
Throughthese comparative experiments, we can conclude that our example sampling methodis able to decrease the number of training data, i.e., the overhead for both supervisionand searching, without degrading the system performance.4.2 Related Work4.2.1 Uncertainty Sampling.
The procedure for uncertainty sampling (Lewis and Gale1994) is as follows, where C(x) represents the interpretation certainty for an examplex (see our sampling procedure in Section 3.1 for comparison):1.
WSD(D,X)2. e ~ argminxcx C(x)591Computational Linguistics Volume 24, Number 485 I I I I I80{i65~ _ _ .
_  ~---?
... .
~----Z;:;; =S .
.
.
.
~ .
.
.
.
.
~.
.
.
.
.
.
~_ .
.
.
.
.
~ .
.
.
.
2I~.:.:.
: '''''=f ,< ; I  ........... ~ .
.
.
./ : "  ,,~" ~ .
.
.
.
I~ ....... .El ........... .El-".... ...............
-/o(., ( .g .
?,1,'7 ~" ,?
..".~/ /,'7?111 TU ,US - - - -~  .
.
.
.'
CBS .... ~ .
.
.
.
J l'!/ r a n d o m  ........ ~ ........ rI I I I I0 200 400 600 800 1000 1200no.
o f  t ra in ing data sampledFigure 13The relation between the number of training data sampled and the accuracy of the system.3.
D ~ DU {e}, X +- XN {e}4. goto iLet us discuss the theoretical difference between this and our method.
ConsideringFigure 9 again, one can see that the concept of training utility is supported by thefollowing properties:.2.an example that neighbors more unsupervised examples is moreinformative (Figure 9(a)),an example less similar to one already existing in the database is moreinformative (Figure 9(b)).Uncertainty sampling directly addresses the second property but ignores the first.
Itdiffers from our method more crucially when more unsupervised examples remain,because these unsupervised examples have a greater influence on the computationof training utility.
This can be seen in the comparative xperiments in Section 4, inwhich our method outperformed uncertainty sampling to the highest degree in earlystages.4.2.2 Committee-based Sampling.
In committee-based sampling (Engelson and Da-gan 1996), which follows the "query by committee" principle (Seung, Opper, and592Fujii, Inui, Tokunaga, nd Tanaka Selective Sampling858O75605550450i i i i/ .
...... .El .........~.Q .......#TU ,US - - - -~  .
.
.
.CBS .... ~ ....random ........ ~ .......I I I I200 400  600  800no.
o f  training data sampled1000Figure 14The relation between the number of training data sampled and the accuracy of the systemwithout using examples from IPAL.Sompolinsky 1992), the system selects amples based on the degree of disagreementbetween models randomly taken from a given training set (these models are called"committee members").
This is achieved by iteratively repeating the steps given below,in which the number of committee members i given as two without loss of generality:?2..draw two models randomly,classify unsupervised example x according to each model, producingclassifications C1 and C2,if C1 # C2 (the committee members disagree), select x for the training ofthe system.Figure 15 shows a typical disparity evident between committee-based samplingand our sampling method.
The basic notation in this figure is the same as in Figure 7,and both x and y denote unsupervised examples, or more formally D = {el, e2}, andX = {x, y}.
Assume a pair of committee members {el} and {e2 } have been selected fromthe database D. In this case, the committee members disagree as to the interpretationsof both x and y, and consequently, either example can potentially be selected as asample for the next iteration.
In fact, committee-based sampling tends to require anumber of similar examples (similar to el and y) in the database, otherwise committeemembers taken from the database will never agree.
This is in contrast to our method, inwhich similar examples are less informative.
In our method, therefore, x is preferred toy as a sample.
This contrast can also correlate to the fact that committee-based amplingis currently applied to statistics-based language models (HMM classifiers), in otherwords, statistical models generally require that the distribution of the training data593Computational Linguistics Volume 24, Number 4_sense 1s sense 2Figure 15A case where either x or y can be selected in committee-based sampling.reflects that of the overall text.
Through this argument, one can assume that committee-based sampling is better suited to statistics-based systems, while our method is moresuitable for example-based systems.Engelson and Dagan (1996) criticized uncertainty sampling (Lewis and Gale 1994),which they call a "single model" approach, as distinct from their "multiple model"approach:sufficient statistics may yield an accurate 0.51 probability estimate for a class c ina given example, making it certain that c is the appropriate classificationJ 3However, the certainty that c is the correct classification is low, since there is a0.49 chance that c is the wrong class for the example.
A single model can be usedto estimate only the second type of uncertainty, which does not correlate directlywith the utility of additional training.
(p. 325)We note that this criticism cannot be applied to our sampling method, despitethe fact that our method falls into the category of a single model approach.
In oursampling method, given sufficient statistics, the increment of the certainty degree forunsupervised examples, i.e., the training utility of additional supervised examples,becomes small (theoretically, for both example-based and statistics-based systems).Thus, the utility factor can be considered to correlate directly with additional training,for our method.5.
ConclusionCorpus-based approaches have recently pointed the way to a promising trend in wordsense disambiguation.
However, these approaches tend to require a considerable over-head for supervision in constructing a large-sized atabase, additionally resulting ina computational overhead to search the database.
To overcome these problems, ourmethod, which is currently applied to an example-based verb sense disambiguationsystem, selectively samples a smaller-sized subset from a given example set.
Thismethod is expected to be applicable to other example-based systems.
Applicability forother types of systems needs to be further explored.The process basically iterates through two phases: (normal) word sense disam-biguation and a training phase.
During the disambiguation phase, the system is pro-vided with sentences containing a polysemous verb, and searches the database for the13 By appropriate classification, Engelson and Dagan mean the classification given by a perfectly trainedmodel.594Fujii, Inui, Tokunaga, nd Tanaka Selective Samplingmost semantically similar example to the input (nearest neighbor esolution).
There-after, the verb is disambiguated by superimposing the sense of the verb appearing inthe supervised example.
The similarity between the input and an example, or moreprecisely the similarity between the case fillers included in them, is computed based onan existing thesaurus.
In the training phase, a sample is then selected from the systemoutputs and provided with the correct interpretation by a human expert.
Throughthese two phases, the system iteratively accumulates supervised examples into thedatabase.
The critical issue in this process is to decide which example should beselected as a sample in each iteration.
To resolve this problem, we considered thefollowing properties: (a) an example that neighbors more unsupervised examples ismore influential for subsequent training, and therefore more informative, and (b) sinceour verb sense disambiguation is based on nearest neighbor esolution, an examplesimilar to one already existing in the database is redundant.
Motivated by these prop-erties, we introduced and formalized the concept of training utility as the criterionfor example selection.
Our sampling method always gives preference to that examplewhich maximizes training utility.We reported on the performance of our sampling method by way of experimentsin which we compared our method with random sampling, uncertainty sampling(Lewis and Gale 1994), and committee-based sampling (Engelson and Dagan 1996).The result of the experiments showed that our method reduced both the overheadfor supervision and the overhead for searching the database to a larger degree thanany of the above three methods, without degrading the performance of verb sensedisambiguation.
Through the experiment and discussion, we claim that uncertaintysampling considers property (b) mentioned above, but lacks property (a).
We alsoclaim that committee-based sampling differs from our sampling method in terms ofits suitability to statistics-based systems as compared to example-based systems.AcknowledgmentsThe authors would like to thank ManabuOkumura (JAIST, Japan), Timothy Baldwin(TITECH, Japan), Michael Zock (LIMSI,France), Dan Tufts (Romanian Academy,Romania) and anonymous reviewers fortheir comments on an earlier version of thispaper.
This research ispartially supportedby a Research Fellowship of the JapanSociety for the Promotion of Science forYoung Scientists.ReferencesAha, David W., Dennis Kibler, and Marc K.Albert.
1991.
Instance-based l arningalgorithms.
Machine Learning, 6(1):37-66.Brown, Peter E, Stephen A. Della Pietra, andVincent J. Della Pietra.
1991.
Word-sensedisambiguation using statistical methods.In Proceedings ofthe 29th Annual Meeting,pages 264-270, Association forComputational Linguistics.Bruce, Rebecca nd Janyce Wiebe.
1994.Word-sense disambiguation usingdecomposable models.
In Proceedings ofthe32nd Annual Meeting, pages 139-146,Association for ComputationalLinguistics.Chapman, Robert L. 1984.
Roget'sInternational Thesaurus.
Fourth Edition.Harper and Row.Charniak, Eugene.
1993.
Statistical LanguageLearning.
MIT Press, Cambridge, MA.Cohn, David, Les Atlas, and RichardLadner.
1994.
Improving eneralizationwith active learning.
Machine Learning,15(2):201-221.Dagan, Ido and Alon Itai.
1994.
Word sensedisambiguation using a second languagemonolingual corpus.
ComputationalLinguistics, 20(4):563-596.Engelson, Sean P. and Ido Dagan.
1996.Minimizing manual annotation cost insupervised training from corpora.
InProceedings ofthe 34th Annual Meeting,pages 319-326, Association forComputational Linguistics.Frakes, William B. and Ricardo Baeza-Yates.1992.
Information Retrieval: Data Structure &Algorithms.
PTR Prentice-Hall.Fujii, Atsushi, Kentaro Inui, TakenobuTokunaga, nd Hozumi Tanaka.
1996.
Towhat extent does case contribute to verbsense disambiguation?
In Proceedings ofthe16th International Conference on595Computational Linguistics Volume 24, Number 4Computational Linguistics, pages 59-64.Fukumoto, Fumiyo and Jun'ichi Tsujii.
1994.Automatic recognition of verbalpolysemy.
In Proceedings ofthe 15thInternational Conference on ComputationalLinguistics, pages 764-768.Gale, William, Kenneth Ward Church, andDavid Yarowsky.
1992.
Estimating upperand lower bounds on the performance ofword-sense disambiguation programs.
InProceedings ofthe 30th Annual Meeting,pages 249-256, Association forComputational Linguistics.Gale, William, Kenneth Ward Church, andDavid Yarowsky.
1993.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities,26:415-439.Guthrie, Joe A., Louise Guthrie, YorickWilks, and Homa Aidinejad.
1991.Subject-dependent co-occurrence andword sense disambiguation.
I  Proceedingsof the 29th Annual Meeting, pages 146-152,Association for ComputationalLinguistics.Hearst, Marti A.
1991.
Noun homographdisambiguation using local context inlarge text corpora.
In Proceedings ofthe 7thAnnual Conference ofthe University ofWaterloo Centre for the New OED and TextResearch, pages 1-22.Hindle, Donald.
1990.
Noun classificationfrom predicate-argument structures.
InProceedings ofthe 28th Annual Meeting,pages 268-275, Association forComputational Linguistics.Hirst, Graeme.
1987.
Semantic Interpretationand the Resolution of Ambiguity.
CambridgeUniversity Press.Information-technology Promotion Agency.1987.
IPAL Japanese dictionary forcomputers (basic verbs) (in Japanese).Japan Electronic Dictionary ResearchInstitute.
1995.
EDR electronic dictionarytechnical guide (in Japanese).Kaji, Hiroyuki, Yuuko Kida, and YasutsuguMorimoto.
1992.
Learning translationtemplates from bilingual text.
InProceedings ofthe 14th InternationalConference on Computational Linguistics,pages 672-678.Kameda, Masayuki.
1996.
A portable &quick Japanese parser: QJP.
In Proceedingsof the 16th International Conference onComputational Linguistics, pages 616-621.Karov, Yael and Shimon Edelman.
1996.Learning similarity-based word sensedisambiguation.
In Proceedings ofthe 4thWorkshop on Very Large Corpora, pages42-55.Krovets, Robert and W. Bruce Croft.
1992.Lexical ambiguity and informationretrieval.
ACM Transactions on InformationSystems, 10(2):115-141.Kurohashi, Sadao and Makoto Nagao.
1994.A method of case structure analysis forJapanese sentences based on examples incase frame dictionary.
IEICE Transactionson Information and Systems,E77-D(2):227-239.Leacock, Claudia, Geoffrey Towell, andEllen Voorhees.
1993.
Corpus-basedstatistical sense resolution.
In Proceedingsof ARPA Human Language TechnologyWorkshop, ages 260-265.Lewis, David D. and William Gale.
1994.
Asequential algorithm for training textclassifiers.
In Proceedings ofthe 17th AnnualInternational ACM SIGIR Conference onResearch and Development i  InformationRetrieval, pages 3-12.Li, Xiaobin, Stan Szpakowicz, and StanMatwin.
1995.
A WordNet-basedalgorithm for word sense disambiguation.In Proceedings ofthe 14th International JointConference on Artificial Intelligence, pages1368-1374.Lytinen, Steven L. 1986.
Dynamicallycombining syntax and semantics innatural anguage processing.
InProceedings ofAAAI-86, pages 574-578.Mainichi Shimbun.
1991-1994.
Mainichishimbun CD-ROM '91-'94 (in Japanese).Matsumoto, Yuji, Sadao Kurohashi, TakehitoUtsuro, Yutaka Myoki, and MakotoNagao, 1993.
JUMAN Users Manual (inJapanese).
Kyoto University and NaraInstitute of Science and Technology.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross,Katherine Miller, and Randee Tengi.
1993.Five papers on WordNet.
TechnicalReport CLS-Rep-43, Cognitive ScienceLaboratory, Princeton University.Mooney, Raymond J.
1996.
Comparativeexperiments on disambiguating wordsenses: An illustration of the role of biasin machine learning.
In Proceedings oftheConference on Empirical Methods in NaturalLanguage Processing, pages 82-91.Nagao, Katashi.
1994.
A preferentialconstraint satisfaction technique fornatural anguage analysis.
IEICETransactions on Information and Systems,E77-D(2):161-170.Nagao, Makoto.
1984.
A framework of amechanical translation between Japaneseand English by analogy principle.ArtiJicial and Human Intelligence, pages173-180.Nasukawa, Tetsuya.
1993.
Discourseconstraint in computer manuals.
In596Fujii, Inui, Tokunaga, and Tanaka Selective SamplingProceedings ofthe 5th International Conferenceon Theoretical nd Methodological Issues inMachine Translation, pages 183-194.National Language Research Institute.
1964.Bunruigoihyo (in Japanese).
Shueipublisher.Ng, Hwee Tou.
1997.
Exemplar-based wordsense disambiguation: Some recentimprovements.
In Proceedings ofthe 2ndConference on Empirical Methods in NaturalLanguage Processing, pages 208-213.Ng, Hwee Tou and Hian Beng Lee.
1996.Integrating multiple knowledge sourcesto disambiguate word sense: Anexemplar-based approach.
In Proceedingsof the 34th Annual Meeting, pages 40-47,Association for ComputationalLinguistics.Niwa, Yoshiki and Yoshihiko Nitta.
1994.Co-occurrence vectors from corpora vs.distance vectors from dictionaries.
InProceedings ofthe 15th InternationalConference on Computational Linguistics,pages 304-309.Nomiyama, Hiroshi.
1993.
Machinetranslation by case generalization (inJapanese).
Transactions ofInformationProcessing Society of Japan, 34(5):905-912.Pedersen, Ted, Rebecca Bruce, and JanyceWiebe.
1997.
Sequential model selectionfor word sense disambiguation.
IProceedings ofthe 5th Conference on AppliedNatural Language Processing, pages388-395.Pustejovsky, James and Branimir Boguraev.1993.
Lexical knowledge representationand natural anguage processing.
ArtificialIntelligence, 63(1-2):193-223.Real World Computing Partnership.
1995.RWC text database (in Japanese).Resnik, Philip.
1993.
Selection andInformation: A Class-Based Approach toLexical Relationships.
Ph.D. thesis,University of Pennsylvania.Ribas, Francesc.
1995.
On learning moreappropriate selectional restrictions.
InProceedings ofthe 7th Conference oftheEuropean Chapter of the Association forComputational Linguistics, pages 112-118.Salton, Gerard and Michael J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill.Schiitze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings ofSupercomputing,pages 787-796.Seung, H. S., M. Opper, andH.
Sompolinsky.
1992.
Query bycommittee.
In Proceedings ofthe 5th AnnualACM Workshop on Computational LearningTheory, pages 287-294.Smyth, Barry and Mark T. Keane.
1995.Remembering to forget: Acompetence-preserving case deletionpolicy for case-based reasoning systems.In Proceedings ofthe 14th International JointConference on Artificial Intelligence, pages377-382.Tokunaga, Takenobu, Makoto Iwayama, andHozumi Tanaka.
1995.
Automaticthesaurus construction based ongrammatical relations.
In Proceedings ofthe14th International Joint Conference onArtificial Intelligence, pages 1308-1313.Uramoto, Naohiko.
1994a.
A best-matchalgorithm for broad-coverageexample-based disambiguation, hiProceedings ofthe 15th InternationalConference on Computational Linguistics,pages 717-721.Uramoto, Naohiko.
1994b.
Example-basedword-sense disambiguation.
IEICETransactions on Information and Systems,E77-D(2):240-246.Utsuro, Takehito.
1996.
Sense classificationof verbal polysemy based on bilingualclass/class association.
In Proceedings ofthe16th International Conference onComputational Linguistics, pages 968-973.Utsuro, Takehito, Kiyotaka Uchimoto,Mitsutaka Matsumoto, and MakotoNagao.
1994.
Thesaurus-based fficientexample retrieval by generating retrievalqueries from similarities.
In Proceedings ofthe 15th International Conference onComputational Linguistics, pages 1044-1048.Voorhees, Ellen M. 1993.
Using WordNet todisambiguate word senses for textretrieval.
In Proceedings ofthe 16th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 171-180.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics, pages 454-460.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting, pages 189-196, Association forComputational Linguistics.Zernik, Uri.
1989.
Lexicon acquisition:Learning from corpus by capitalizing onlexical categories.
In Proceedings ofthe 11thInternational Joint Conference on ArtificialIntelligence, pages 1556-1562.597
