Generation that Exploits Corpus-Based Statistical KnowledgeI rene  Langk i lde  and Kev in  Kn ightInformation Sciences Inst i tuteUniversity of Southern CaliforniaMarina del Rey, CA 90292i langkil@isi,  edu and knight@isi, eduAbst ractWe describe novel aspects of a new natural lan-guage generator called Nitrogen.
This generator hasa highly flexible input representation that allows aspectrum of input from syntactic to semantic depth,and shifts' the burden of many linguistic decisionsto the statistical post-processor.
The generation al-gorithm is compositional, making it efficient, yet italso handles non-compositional aspects of language.Nitrogen's design makes it robust and scalable, op-erating with lexicons and knowledge bases of onehundred thousand entities.1 In t roduct ionLanguage generation is an important subtaskof applications like machine translation, human-computer dialogue, explanation, and summariza-tion.
The recurring need for generation suggests theusefulness of a general-purpose, domain-independentnatural anguage generator (NLG).
However, "plug-in" generators available today, such as FUF/SURGE(Elhadad and Robin, 1998), MUMBLE (Meteer etal., 1987), KPML (Bateman, 1996), and CoGen-Tex's RealPro (Lavoie and Rambow, 1997), requireinputs with a daunting amount of linguistic detail.As a result, many client applications resort insteadto simpler template-based methods.An important advantage of templates i that theysidestep linguistic decision-making, and avoid theneed for large complex knowledge resources and pro-cessing.
For example, the following structure couldbe a typical result from a database query on the typeof food a venue serves:( ( :ob j - type venue)(:obj-name Top_of_the_Mark)( :a t t r ibute  food- type) ( :at t r ib -va lue  American))By using a template like<obj-name> 's  <attr ibute> is <attr ib-value>.the structure could produce the sentence, "Top ofthe Mark's food type is American.
".Templates avoid the need for detailed linguisticinformation about lexical items, part-of-speech tags,number, gender, definiteness, tense, sentence organi-zation, sub-categorization structure, semantic rela-?
tions, etc., that more general NLG methods need tohave specified in the input (or supply defaults for).Such information is usually not readily inferrablefrom an application's database, nor is it always read-ily available from other sources, with the breadth ofcoverage or level of detail that is needed.
Thus, usinga general-purpose g nerator can be formidable (Re-iter, 1995).
However, templates only work in verycontrolled or limited situations.
They cannot pro-vide the expressiveness, flexibility or scalability thatmany real domains need.A desirable solution is a generator that abstractsaway from templates enough to provide the neededflexibility and scalability, and yet still requires onlyminimal semantic input (and maintains reasonableefficiency).
This generator would take on the re-sponsibility of finding an appropriate linguistic re-alization for an underspecified semantic input.
Thissolution is especially important in the context of ma-chine translation, where the surface syntactic orga-nization of the source text is usually different fromthat of the target language, and the deep semanticsare often difficult to obtain or represent completelyas well.
In Japanese to English translation, for ex-ample, it is often hard to determine from a Japanesetext the number or gender of a noun phrase, the En-glish equivalent of a verb tense, or the deep semanticmeaning of sentential arguments.
There are manyother obvious yntactic divergences as well.Thus, shifting such linguistic decisions to the gen-erator is significantly helpful for client applications.However, at the same time, it imposes enormousneeds for knowledge on the generator program.
Tra-ditional large-scale NLG already requires immenseamounts of knowledge, as does any large-scale AIenterprise.
NLG operating on a scale of 200,000 en-tities (concepts, relations, and words) requires largeand sophisticated lexicons, grammars, ontologies,collocation lists, and morphological tables.
Acquir-ing and applying accurate, detailed knowledge ofthis breadth poses difficult problems.
(Knight and Hatzivassiloglou, 1995) suggested704meaningsymbolic generator \]word lattice ofpossible renderings~- lexicon+- grammarI statistical extractor \] <--- corpusEnglish stringFigure 1: Combining Symbolic and Statisti-cal Knowledge in a Natural Language Generator(Knight and Hatzivassiloglou, 1995).overcoming this knowledge acquisition bottleneck inNLG by tapping the vast knowledge inherent in En-glish text corpora.
Experiments showed that corpus-based knowledge greatly reduced the need for deep,hand-crafted knowledge.
This knowledge, in theform of n-gram (word-pair) frequencies, could be ap-plied to a set of semantically related sentences tohelp sort good ones from bad ones.
A corpus-basedstatistical ranker takes a set of sentences packed ef-ficiently into a word lattice, (a state transition di-agram with links labeled by English words), andextracts the best path from the lattice as output,preferring fluent sentences over contorted ones.
Agenerator can take advantage ofthis by producing alattice that encodes various alternative possibilitieswhen the information eeded to make a linguisticdecision is not available.Such a system organization shown in Figure 1, isrobust against underspecified and even ambiguousinput meaning structures.
Traditionally, underspec-ification is handled with rigid defaults (e.g., assumepresent tense, use the alphabetically-first synonyms,use nominal arguments, etc.).
However, the wordlattice structure permits all the different possibili-ties to be encoded as different phrasings, and thecorpus-based statistical extractor can select a goodsentence from these possibilities.The questions that still remain are: What kind ofinput representation is minimally necessary?
Whatkinds of linguistic decisions can the statistics reliablymake, and which instead need to be made symbol-ically?
How should symbolic knowledge be appliedto the input to efficiently produce word lattices fromthe input?This paper describes Nitrogen, a generation sys-tem that computes word lattices from a meaning rep-resentation to take advantage of corpus-based sta-tistical knowledge.
Nitrogen performs entence real-ization and some components ofsentence planning--namely, mapping domain concepts to content words,and to some extent, mapping semantic relations togrammatical ones.
It contributes:?
A flexible input representation based on concep-tual meanings and the relations between them.?
A new grammar formalism for defining the map-ping of meanings onto word lattices.?
A new efficient algorithm to do this mapping.?
A large grammar, lexicon, and morphology ofEnglish, addressing linguistic phenomena suchas knowledge acquisition bottlenecks and under-specified/ambiguous input.This paper is organized as follows.
First, we de-scribe our Abstract Meaning Representation lan-guage (AMR).
Then we outline the generation algo-rithm and describe how various knowledge sourcesapply to render an AMR into English, includinglexical, morphological, and grammatical knowledgebases.
We describe the structure of these knowl-edge bases and give examples.
We also present atechnique that adds powerful flexibility to the gram-mar formalism.
We finish with a discussion of thestrengths and weaknesses of our generation system.2 Abst rac t  Mean ing  Representat ionThe AMR language is composed of concepts fromthe SENSUS knowledge base (Knight and Luk,1994}, including all of WordNet 1.5 (Miller, 1990),and keywords relating these concepts to each other)An AMR is a labeled directed graph, or featurestructure, derived from the PENMAN Sentence PlanLanguage (Penman, 1989).
The most basic AMR isof the form (label / concept), e.g.
: ~(ml / \[dog<canidl)The slash is shorthand for a type (or instance) fea-ture, and in logic notation this AMR might be writ-ten as instance(m1, dog).
This AMR can represent"the dog," "the dogs," "a dog," or "dog," etc.A concept can be modified using keywords:(,,2 I \[dog<canid\[: quant plural)IStrings can be used in place of concepts.
If the stringis not a recognized word/phrase, then the generator will addthis ambiguity to the word lattice for the statistical extrac-tor to resolve by proposing all possible part-of-speech tags.We prefer to use concepts because they make the AMR morelanguage-lndependent, and enable semantic reasoning andinference.2Concept names appear between vertical bars.
We use aset of short, unique concept names derived from the struc-ture of WordNet by Jonathan Graehl, and available fromhttp://www.isi.edu/natural-language/GAZELLE.html705This narrows the meaning to "the dogs," or "dogs.
"Concepts can be associated with each other ina nested fashion to form more complex mean-ings.
These relations between conceptual mean-ings are also expressed through keywords.
It isthrough them that our formalism exhibits an ap-pealing flexibility.
A client has the freedom to ex-press the relations at various semantic and syn-tactic levels, using whichever level of representa-tion is most convenient.
3 We have currently im-plemented shallow semantic versions of roles suchas :agent, :patient, :sayer, :sensor, etc., as well asdeep syntactic roles such as :obliquel, :oblique2, and:oblique3 (which correspond to deep subject, ob-ject, and indirect object respectively, and serve asan abstraction for passive versus active voice) andthe straightforward syntactic roles :subject, :direct-object, :indirect-object, etc.
We explain further howthis is implemented later in the paper.Below is an example of a slightly more complexmeaning.
The root concept is eating, and it has anagent and a patient, which are dogs and a bone (orbones), respectively.
(m3 / lear , take  inJ:agent  (m4 / Idog<canidJ:quant  p lu ra l ):patient (mS / \[os,boneJ))Possible output includes "The dogs ate the bone,""Dogs will eat a bone," "The dogs eat bones," "Dogseat bone," and "The bones were eaten by dogs.
"3 Lex ica l  KnowledgeThe Sensus concept ontology is mapped to an En-glish lexicon that is consulted to find words for ex-pressing the concepts in an AMR.
The lexicon is alist of 110,000 tuples of the form:(<word> <part-of-speech> <rank> <concept>)Examples:(("eat" VERB I feat,take in\[)("eat" VERB 2 Jeat>eat lunch\[)?
.
*  )The <rank> field orders the concepts by sense fre-quency for the given word, with a lower number sig-nifying a more frequent sense.Like other types of knowledge used in Nitro-gen, the lexicon is very simple.
It contains no3This flexibility has another advantage from a researchpoint of view.
We consider the appropriate l vel of abstrac-tion an important problem in interlingua-style machine trans-lation.
The flexibility of this representation allows us to ex-per iment  with various levels of abstraction without changingthe underlying system.
Further, it has opened up to us thepossibility of implementing interlingua-based semantic trans-fer, where the interlingua serves as the transfer mechanism,rather than being a single, fixed peak point of abstraction.information about features like transitivity, sub-categorization, gradability (for adjectives), or count-ability (for nouns), etc.
Such features are neededin other generators to produce correct grammaticalconstructions.
Our statistical post-processor insteadmore softly (and robustly) ranks different grammat-ical realizations according to their likelihood.At the lexical level, several important issues inword choice arise.
WordNet maps a concept o oneor more synonyms.
However, some words may beless appropriate than others, or may actually bemisleading in certain contexts.
An example is theconcept \[sell<cozen\[ to which the lexicon maps thewords "betray" and "sell."
However, it is not verycommon to use the word "sell" in the sense of "Atraitor sells out on his friends."
In the sentence "Icannot \[sell<cozen\[ their trust" the word "sell" ismisleading, or at least sounds very strange; "betray"is more appropriate.This word choice problem occurs frequently, andwe deal with it by taking advantage of the word-sense rankings that the lexicon offers.
According tothe lexicon, the concept \[sell<cozen\[ expresses thesecond most frequent sense of the word "betray,"but only the sixth most frequent sense of the word"sell."
To minimize the lexical choice problem, wehave adopted a policy of rejecting words whose pri-mary sense is not the given concept when betterwords are available.
4Another issue in word choice relates to the broaderissue of preserving ambiguities in MT.
In sourcelanguage analysis, it is often difficult to determinewhich concept is intended by a certain word.
TheAMR allows several concepts to be listed togetherin a disjunction.
For example,(m6 / (*OR* \[sell<cozen\[ \[cheat on\[ \[bewray\[Jbetray,failJ Jrat onJ))The lexical lookup will attempt o preserve theambiguity of this *0R*.
If it happens that several orall of the concepts in a disjunction can be expressedusing the same word, then the lookup will returnonly that word or words in preference to the otherpossibilities.
For the example above, the lookup re-turns only the word "betray."
This also reduces thecomplexity of the final sentence lattices.4 Morpho log ica l  KnowledgeThe lexicon contains words in their root form,so morphological inflections must be generated.The system also performs derivational morphol-ogy, such as adjective-*noun and noun--*verb (ex:4A better "soft" technique would be to accept all wordsreturned by the lexicon for a given concept, but associatewith each word a preference score using a method such asBayes' Rule and probabilities computed from a corpus suchas SEMCOR, allowing the statistical extractor to choose thebest alternative.
We plan to implement this in the future.706"translation"-~"translate") to give the generatormore syntactic flexibility in expressing complexAMK's.
This flexibility ensures that the generatorcan find a way to express a complex meaning repre-sented by nested AMRs, but is also useful for solvingproblems of syntactic divergence in MT.Both kinds of morphology are handled the sameway.
Rules and exception tables are merged into asingle, concise knowledge base.
Here, for example,is a portion of the table for pluralizing nouns:("-chi ld" "chi ldren")("-person" "people .... persons")("-a" "as" "ae") ; formulas / formulae("-x .... xes .... xen") ; boxes/oxen("-man .... roans .... men") ; humans/ footmen("-Co" "os" "oes")The last line means: if a noun ends in a conso-nant followed by "-o," then we compute two pluralforms, one ending in "-os" and one ending in "-oes,"and put both possibilities in the word lattice forthe post-generation statistical extractor to choosebetween later.
Deciding between these usually re-quires a large word list.
However, the statisticalextractor already has a strong preference for "pho-tos" and "potatoes" over "photoes" and "potatos,"so we do not need to create such a list.
Here againcorpus-based statistical knowledge greatly simplifiesthe task of symbolic generation.Derivational morphology raises the issue of mean-ing shift between different part-of-speech forms(such as "depart"-~ "departure"/"department").Errors of this kind are infrequent, and are correctedin the morphology tables.5 Generat ion  A lgor i thmAn AMR is transformed into word lattices bykeyword-based grammar ules described in Section7.
By contrast, other generators organize theirgrammar rules around syntactic categories.
Akeyword-based organization helps achieve simplicityin the input specification, since syntactic informa-tion is not required from a client.
This simplificationcan make Nitrogen more readily usable by client ap-plications that are not inherently linguistically ori-ented.
The decisions about how to syntactically re-alize a given meaning can be left largely up to thegenerator.The top-level keywords of an AMR are used tomatch it with a rule (or rules).
The algorithmis compositional, avoiding a combinatorial explo-sion in the number of rules needed for the variouskeyword combinations.
A matching rule splits theAMR apart, associating a sub-AMR with each key-word, and lumping the relations left over into a sub-AMR under the :rest role using the same root as theoriginal AMR.
Each sub-AMR is itself recursivelymatched against he keyword rules, until the recur-sion bottoms out at a basic AMR which matcheswith the instance rule.Lexical and morphological knowledge is used tobuild the initial word lattices associated with a con-cept when the recursion bottoms out.
Then the in-stance rule builds basic noun and verb groups fromthese, as well as basic word lattices for other syn-tactic categories.
As the algorithm climbs out of therecursion, each rule concatenates together latticesfor each of the sub-AMR's to form longer phrases.The rhs specifies the needed syntactic ategory foreach sub-lattice and the surface order of the concate-nation, as well as the syntactic ategory for the newresulting lattice.
Concatenation is performed by at-taching the end state of one sub-lattice to the startstate of the next.
Upon emerging from the top-levelrule, the lattice with the desired syntactic ategory,by default S (sentence), is selected and handed tothe statistical extractor for ranking.The next sections describe further how lexical andmorphological knowledge are used to build the initialword lattices, how underspecification is handled, andhow the grammar is encoded.6 The  Ins tance  Ru leThe instance rule is the most basic rule since it is ap-plied to every concept in the AMR.
This rule buildsthe initial word lattices for each lexical item andfor basic noun and verb groups.
Each concept inthe AMR is eventually handed to the instance rule,where word lattices are constructed for all availableparts of speech.The relational keywords that apply at the instancelevel are :polarity, :quant, :tense, and :modal.
Incases where a meaning is underspecified and doesnot include these keywords, the instance rule uses arecasting mechanism (described below) to add someof them.
If not specified, the system assumes posi-tive polarity, both singular and plural quantities, allpossible time frames, and no modality.Japanese nouns are often ambiguous with respectto number, so generating both singular and pluralpossibilities and allowing the statistical extractor tochoose the best one results in better translation qual-ity than rigidly choosing a single default as tradi-tional generation systems do.
Allowing number tobe unspecified in the input is also useful for gen-eral English generation as well.
There are many in-stances when the number of a noun is dictated moreby usage convention or grammatical constraint thanby semantic ontent.
For example, "The companyhas (a plan/plans) to establish itself in February," or"This child won't eat any carrots," ("carrots" mustbe plural by grammatical constraint).
It is easierfor a client program if the input is not required tospecify number in these cases, but is allowed to rely707on the statistical extractor to supply the best one.In translation, there is frequently no direct corre-spondence between tenses of different languages, soin Nitrogen, tense can be coarsely specified as eitherpast, present, or future, but need not be specifiedat all.
If not specified, Nitrogen generates latticesfor the most common English tenses, and allows thestatistical extractor to choose the most likely one.The instance rule is factored into several sub-instance rules with three main categories: nouns,verbs, and miscellaneous.
The noun instance rulesare further subdivided into two rules, one for plu-ral noun phrases, and the other for singular.
Theverb instance rules are factored into two categoriesrelating to modality and tense.Polarity can apply across all three main instancecategories (noun, verb, and other), but only affectsthe level it appears in.
When applied to nouns or ad-jectives, the result is "non-" prepended to the word,which conveys the general intention, but is not usu-ally very grammatical.
Negative polarity is usuallymost fluently expressed in the verb rules with theword "not," e.g., "does not eat.
''57 Grammar  Formal i smThe grammatical specifications in the keyword rulesconstitute the main formalism of the generation sys-tem.
The rules map semantic and syntactic roles togrammatical word lattices.
These roles include::agent, :patient, :domain, :range, :source,: dest inat ion, : spat ial-locat ing,:temporal-locating, : accompanier ;:obliquel, :oblique2, :oblique3;:subject, :object, :mod, etc.A simplified version of the rule that applies to anAMR with :agent and :patient roles is:((Xl :agent)(x2 :pat ient )(x3 :rest)->(s (seq (xl np nom-pro) (x3 v-tensed)(x2 np acc-pro) ))(s (seq (x2 np nom-pro) (x3 v-passive)(wrd "by") (xl np acc-pro)))(np (seq (x3 np acc-pro nora-pro) (wrd "of")(x2 np ace-pro) (wrd "by") (xl np acc-pro)))(s-ger (seq ...))(inf (seq ...)))The left-hand side is used to match an AMR withagent and patient roles at the top level.
The :restkeyword serves as a catch-all for other roles that ap-pear at the top level.
Note that the rule specifiestwo ways to build a sentence, one an active voiceSWe plan to generate more fluent expressions for negativepolarity on nouns and adjectives, for example, "unhappy"instead of "non-happy.
"version and the other passive.
Since at this level theinput may be underspecified regarding which voiceto use, the statistical extractor is expected to chooselater the most fluent version.
Note also that this rulebuilds lattices for other parts of speech, in additionto sentences (ex: "the consumption of the bone bythe dogs").
In this way the generation algorithmworks bottom-up, building lattices for the leaves (in-nermost nested levels of the input) first, to be com-bined at outer levels according the relations betweenthe leaves.
For example, the AMR below will matchthis rule:(m7 / \ [eat , take  inl: time present:agent (d / \[dog,canid~: quant plural):patient (b / ~os,bonel: quant sing))Below are some sample lattices that result fromapplying the rule above to this AMR: 6(S (or (seq (or (wrd "the")  (wrd "*empty*"))(wrd "dog") (wrd "+plural")(wrd "may") (wrd "eat")(or (wrd "the") (wrd "a")(wrd "an") (wrd "*empty*"))(wrd "bone") )(seq (or (wrd "the") (wrd "a")(wrd "an") (wrd "*empty*"))(wrd "bone") (wrd "may") (wrd "be")(or (wrd "being") (wrd "*empty*"))(wrd "eat") (wrd "+pastp") (wrd "by")(or (wrd "the") (wrd "*empty*"))(wrd "dog") (wrd "+plural")) ) )(NP (seq (or (wrd "the") (wrd "a")(wrd "an") (wrd "*empty*"))(wrd "possibility") (wrd "of")(or (wrd "the") (wrd "a")(wrd "an") (wrd "*empty*"))(wrd "consumption") (wrd "of")(or (wrd "the") (wrd "a")(wrd "an") (wrd "*empty*"))(wrd "bone") (wrd "by")(or (wrd "the") (wrd "*empty*") )(wrd "dog") (wrd "+plural") )) )(S-GER .
.
.
)( INF  .
.
.
)Note the variety of symbolic output hat is pro-duced with these xcessively simple rules.
Each re-lation is mapped not to one but to many differentrealizations, covering regular and irregular behav-ior exhibited in natural anguage.
Purposeful over-generation becomes a strength.6The grammar rules can insert the special token *empty*,here indicating an option for the null determiner.
Before run-ning, the statistical extractor removes all *empty* transitionsby determinizing the word lattice.
Note also the insertion ofmorphological tokens like +plural.
Inflectional morphologyrules also apply during this determinizing stage.708The : res t  keyword in the rule head providesa handy mechanism for decoupling the possiblekeyword combinations.
By means of this mecha-nism, keywords which generate relatively indepen-dent word lattices can be organized into separaterules, avoiding combinatorial explosion in the num-ber of rules which need to be written.7.1 Recast ing  Mechan ismThe recasting mechanism that is used in the gram-mar formalism gives it unique power and flexibil-ity.
The recasting mechanism enables the generatorto transform one semantic representation i to an-other one (such as deep to shallow, or instance tosub-instance) and to accept as input a specificationanywhere along this spectrum, permitting meaningto be encoded at whatever level is most convenient.The recasting mechanism also makes it possible tohandle non-compositional aspects of language.One area in which we use this mechanism is in the:domain rule.
Take for example the sentence, "It isnecessary that the dog eat."
It is sometimes mostconvenient to represent this as:(m8 / \[obligatory<necessaryi:domain (m9 / \[eat,take inl:agent (ml0 / Idog,canidl)))and at other times as:(mll / \[have the quality of beingl:domain (m12 / lear,take inl:agent (d / \]dog,canidl)):range (m13 / \[obligatory<necessaryl))but we can define them to be semantically equiva-lent.
In our system, both are accepted, and the firstis automatically transformed into the second.Other ways to say this sentence include "The dogis required to eat," or "The dog must eat."
How-ever, the grammar formalism cannot express this,because it would require inserting the word latticefor \]obligatory<necessary\[ within the lattice for m9or m12--but the formalism can only concatenate lat-tices.
The recasting mechanism solves this problem,by recasting the above AMR as:(m14 / feat , take inl:modal (m15 / lobl igatory<necessaryl):agent (m16 / \[dog,canidl))which makes it possible to form these sentences.
Thesyntax for recasting the first AMR to the second is:((xl : rest)(x2 :domain)->(?
(xl (:new (/ Ihave the qual i ty  of being\])(:domain x2) (:range xl))  ?
))and for recasting the second into the third:((xl : rest)(x2 :domain)(x3 :range)->(7 (x2 (:add (:modal (x3 (:add ( :extra  x l ) ) ) ) )  7))(s (seq (x2 np nom-pro) (xt v-tensed)(x3 adj np acc-pro)))(s (seq (wrd " i t" )  (xl v-tensed)(x3 adj np acc-pro) (wrd "that") (x2 s)))o.o)The :new and :add keywords ignal an AMR re-cast.
The list after the keyword contains the in-structions for doing the recast.
In the first case,the :new keyword means: build an AMR with anew root, Ihave the qua l i ty  of be ing l ,  and tworoles, one labeled :domain and assigned sub-AMRx2; the other labeled :range and assigned sub-AMRxl.
The question mark causes a direct splice of theresults from the recast.In the second case, the :add keyword means: in-sert into the sub-AMR of x2 a role labeled :modaland assign to it the sub-AMR of x3 which itself isrecast o include the roles in the sub-AMR of x l  butnot its root.
(This is in case there are other rolessuch as polarity or time which need to be includedin the new AMR.
)In fact, recasting makes it possible to nest modalswithin modals to any desired depth, and even to at-tach polarity and tense at any level.
For example,"It is not possible that it is required that you are per-mitted to go," can be also (more concisely) statedas "It cannot be required that you be permitted togo," or "It is not possible that you must be permit-ted to go," or "You cannot have to be permittedto go."
This is done by a grammar ule express-ing the most nested modal concept as a modal verband the remaining modal concepts as a combinationof regular verbs or adjective phrases.
Our grammarincludes a fairly complete model of obligation, pos-sibility, permission, negation, tense, and all of theirpossible interactions.8 DiscussionWe have presented a new generation grammar for-malism capable of mapping meanings onto word lat-tices.
It includes novel mechanisms for construct-ing and combining word lattices, and for re-writingmeaning representations to handle a broad range oflinguistic phenomena.
The grammar accepts inputsalong a continuum of semantic depth, requiring onlya minimal amount of syntactic detail, making it at-tractive for a variety of purposes.Nitrogen's grammar is organized around seman-tic input patterns rather than the syntax of English.This distinguishes it from both unification grammar(Elhadad, 1993a; Shieber et al, 1989) and systemic-network grammar (Penman, 1989).
Meanings can709be expressed irectly, or else be recast and recy-cled back through the generator.
This recycling ul-timately allows syntactic onstraints o be localized,even though the grammar is not organized aroundEnglish syntax.Nitrogen's algorithm operates bottom-up, effi-ciently encoding multiple analyses in a lattice datastructure to allow structure sharing, analogous tothe way a chart is used in bottom-up arsing.
Incontrast, traditional generation control mechanismswork top-down, either deterministically (Meteer etal., 1987; Penman, 1989) or by backtracking to pre-vious choice points (Elhadad, 1993b).
This unnec-essarily duplicates work at run time, unless sophis-ticated control directives are included in the searchengine (Elhadad and Robin, 1992).
Recently, (Kay,1996) has explored abottom-up approach to genera-tion as well, using a chart rather than a word lattice.Nitrogen's generation is robust and scalable.
Itcan generate output even for unexpected or incom-plete input, and is designed for broad coverage.It does not require the detailed, difficult-to-obtainknowledge bases that other NLG systems require,since it relies instead on corpus-based statistics tomake a wide variety of linguistic decisions.
Cur-rently the quality of the output is limited by the useof only word bigram statistical information, whichcannot handle long-distance agreement, or distin-guish likely collocations from unlikely grammaticalstructure.
However, we plan to remedy these prob-lems by using statistical information extracted fromthe Penn Treebank corpus (Marcus et al, 1994) torank tagged lattices and parse forests.Nitrogen's rule matching is much less expensivethan graph unification, and lattices generated forsub-AMIRs are cached and reused in subsequent ref-erences.
The semantic roles used in the grammarformalism cover most common syntactic phenomena,though our grammar does not yet generate ques-tions, or infer pronouns from explicit coreference.Nitrogen has been used extensively as part ofa semantics-based Japanese-English MT system(Knight et al, 1995).
Japanese analysis providesAMR's, which Nitrogen transforms into word lat-tices on the order of hundreds of nodes and thou-sands of arcs.
These lattices compactly encode anumber of syntactic variants that usually reach intothe trillions and beyond.
Most of these are some-what ungrammatical or awkward, yet the statisticalextractor rather successfully narrows them down tothe top N best paths.
An online demo is available athttp://www.isi .edu/natural-language/mt/nitrogen/Re ferencesJ.
Bateman.
1996.
KPML development environ-ment - -  multilingual linguistic resource devel-opment and sentence generation.
Technical re-port, German Centre for Information Technology(GMD).M.
Elhadad and J. Robin.
1992.
Controlling contentrealization with functional unification grammars.In R. Dale, E. Hovy, D. Roesner, and O. Stock,editors, Aspects of Automated Natural LanguageGeneration.
Springier Verlag.M.
Elhadad and J. Robin.
1998.
Surge: acomprehensive plug-in syntactic realiza-tion component for text generation.
Inhttp ://www.
es.
bgu.
ac.
il/researeh /projects /surge/.M.
Elhadad.
1993a.
FUF: The universal unifier--user manual, version 5.2.
Technical ReportCUCS-038-91, Columbia University.M.
Elhadad.
1993b.
Using Argumentation to Con-trol Lexieal Choice: A Unification-Based Imple-mentation.
Ph.D. thesis, Columbia University.M.
Kay.
1996.
Chart generation.
In Proc.
ACL.K.
Knight and V. Hatzivassiloglou.
1995.
Two-level,many-paths generation.
In Proc.
ACL.K.
Knight and S. Luk.
1994.
Building a large-scaleknowledge base for machine translation.
In Proc.AAAI.K.
Knight, I. Chander, M. Haines, V. Hatzivas-siloglou, E. Hovy, M. Iida, S. K. Luk, R. Whitney,and K. Yamada.
1995.
Filling knowledge gaps ina broad-coverage MT system.
In Proc.
IJCAI.Benoit Lavoie and Owen Rambow.
1997.
RealPro -a fast, portable sentence realizer.
In ANLP'97.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIn-tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-berger.
1994.
The Penn treebank: Annotatingpredicate argument structure.
In ARPA HumanLanguage Technology Workshop.M.
Meteer, D. McDonald, S. Anderson, D. Forster,L.
Gay, A. Iluettner, and P. Sibun.
1987.Mumble-86: Design and implementation.
Tech-nical Report COINS 87-87, U. of Massachussetsat Amherst, Amherst, MA.G.
Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4).
(Special Issue).Penman.
1989.
The Penman documentation.
Tech-nical report, USC/Information Sciences Institute.Ehud Reiter.
1995.
NLG vs. templates.
In Proc.ENLGW '95.S.
Shieber, G. van Noord, R. Moore, and F. Pereira.1989.
A semantic-head-driven g eration algo-rithm for unification based formalisms.
In Proc.ACL.710
