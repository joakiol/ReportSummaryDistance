Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54?58,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsGenerating Patient Problem Lists from the ShARe Corpus usingSNOMED CT/SNOMED CT CORE Problem ListDanielle MoweryJanyce WiebeUniversity of PittsburghPittsburgh, PAdlm31@pitt.eduwiebe@cs.pitt.eduMindy RossUniversity of CaliforniaSan DiegoLa Jolla, CAmkross@ucsd.eduSumithra VelupillaiStockholm UniversityStockholm, SEsumithra@dsv.su.seStephane MeystreWendy W ChapmanUniversity of UtahSalt Lake City, UTstephane.meystre,wendy.chapman@utah.eduAbstractAn up-to-date problem list is useful forassessing a patient?s current clinical sta-tus.
Natural language processing can helpmaintain an accurate problem list.
For in-stance, a patient problem list from a clin-ical document can be derived from indi-vidual problem mentions within the clin-ical document once these mentions aremapped to a standard vocabulary.
Inorder to develop and evaluate accuratedocument-level inference engines for thistask, a patient problem list could be gen-erated using a standard vocabulary.
Ad-equate coverage by standard vocabulariesis important for supporting a clear rep-resentation of the patient problem con-cepts described in the texts and for interop-erability between clinical systems withinand outside the care facilities.
In thispilot study, we report the reliability ofdomain expert generation of a patientproblem list from a variety of clinicaltexts and evaluate the coverage of anno-tated patient problems against SNOMEDCT and SNOMED Clinical ObservationRecording and Encoding (CORE) Prob-lem List.
Across report types, we learnedthat patient problems can be annotatedwith agreement ranging from 77.1% to89.6% F1-score and mapped to the COREwith moderate coverage ranging from45%-67% of patient problems.1 IntroductionIn the late 1960?s, Lawrence Weed publishedabout the importance of problem-oriented medi-cal records and the utilization of a problem listto facilitate care provider?s clinical reasoning byreducing the cognitive burden of tracking cur-rent, active problems from past, inactive problemsfrom the patient health record (Weed, 1970).
Al-though electronic health records (EHR) can helpachieve better documentation of problem-specificinformation, in most cases, the problem list ismanually created and updated by care providers.Thus, the problem list can be out-of-date con-taining resolved problems or missing new prob-lems.
Providing care providers with problem listupdate suggestions generated from clinical docu-ments can improve the completeness and timeli-ness of the problem list (Meystre and Haug, 2008).In recent years, national incentive and standardprograms have endorsed the use of problem listsin the EHR for tracking patient diagnoses overtime.
For example, as part of the Electronic HealthRecord Incentive Program, the Center for Medi-care and Medicaid Services defined demonstra-tion of Meaningful Use of adopted health infor-mation technology in the Core Measure 3 objec-tive as ?maintaining an up-to-date problem list ofcurrent and active diagnoses in addition to histor-ical diagnoses relevant to the patients care?
(Cen-ter for Medicare and Medicaid Services, 2013).More recently, the Systematized Nomenclature ofMedicine Clinical Terms (SNOMED CT) has be-come the standard vocabulary for representing anddocumenting patient problems within the clinicalrecord.
Since 2008, this list is iteratively refinedfour times each year to produce a subset of gen-eralizable clinical problems called the SNOMEDCT CORE Problem List.
This CORE list repre-sents the most frequent problem terms and con-cepts across eight major healthcare institutions inthe United States and is designed to support in-teroperability between regional healthcare institu-tions (National Library of Medicine, 2009).In practice, there are several methodologies ap-plied to generate a patient problem list from clin-ical text.
Problem lists can be generated fromcoded diagnoses such as the International Statis-tical Classification of Disease (ICD-9 codes) or54concept labels such as Unified Medical LanguageSystem concept unique identifiers (UMLS CUIs).For example, Meystre and Haug (2005) defined 80of the most frequent problem concepts from codeddiagnoses for cardiac patients.
This list was gen-erated by a physician and later validated by twophysicians independently.
Coverage of coded pa-tient problems were evaluated against the ICD-9-CM vocabulary.
Solti et al.
(2008) extended thework of Meystre and Haug (2005) by not limit-ing the types of patient problems from any listor vocabulary to generate the patient problem list.They observed 154 unique problem concepts intheir reference standard.
Although both studiesdemonstrate valid methods for developing a pa-tient problem list reference standard, neither studyleverages a standard vocabulary designed specifi-cally for generating problem lists.The goals of this study are 1) determine howreliably two domain experts can generate a pa-tient problem list leveraging SNOMED CT froma variety of clinical texts and 2) assess the cover-age of annotated patient problems from this corpusagainst the CORE Problem List.2 MethodsIn this IRB-approved study, we obtained theShared Annotated Resource (ShARe) corpusoriginally generated from the Beth Israel Dea-coness Medical Center (Elhadad et al., un-der review) and stored in the MultiparameterIntelligent Monitoring in Intensive Care, ver-sion 2.5 (MIMIC II) database (Saeed et al.,2002).
This corpus consists of discharge sum-maries (DS), radiology (RAD), electrocardiogram(ECG), and echocardiogram (ECHO) reports fromthe Intensive Care Unit (ICU).
The ShARe cor-pus was selected because it 1) contains a variety ofclinical text sources, 2) links to additional patientstructured data that can be leveraged for furthersystem development and evaluation, and 3) has en-coded individual problem mentions with semanticannotations within each clinical document that canbe leveraged to develop and test document-levelinference engines.
We elected to study ICU pa-tients because they represent a sensitive cohort thatrequires up-to-date summaries of their clinical sta-tus for providing timely and effective care.2.1 Annotation StudyFor this annotation study, two annotators - a physi-cian and nurse - were provided independent train-ing to annotate clinically relevant problems e.g.,signs, symptoms, diseases, and disorders, at thedocument-level for 20 reports.
The annotatorswere given feedback based on errors over two it-erations.
For each patient problem in the remain-ing set, the physician was instructed to review thefull text, span the a problem mention, and map theproblem to a CUI from SNOMED-CT using theextensible Human Oracle Suite of Tools (eHOST)annotation tool (South et al., 2012).
If a CUI didnot exist in the vocabulary for the problem, thephysician was instructed to assign a ?CUI-less?
la-bel.
Finally, the physician then assigned one offive possible status labels - Active, Inactive, Re-solved, Proposed, and Other - based on our pre-vious study (Mowery et al., 2013) to the men-tion representing its last status change at the con-clusion of the care encounter.
Patient problemswere not annotated as Negated since patient prob-lem concepts are assumed absent at a document-level (Meystre and Haug, 2005).
If the patientwas healthy, the physician assigned ?Healthy - noproblems?
to the text.
To reduce the cognitive bur-den of annotation and create a more robust refer-ence standard, these annotations were then pro-vided to a nurse for review.
The nurse was in-structed to add missing, modify existing, or deletespurious patient problems based on the guidelines.We assessed how reliably annotators agreedwith each other?s patient problem lists using inter-annotator agreement (IAA) at the document-level.We evaluated IAA in two ways: 1) by problemCUI and 2) by problem CUI and status.
Sincethe number of problems not annotated (i.e., truenegatives (TN)) are very large, we calculated F1-score as a surrogate for kappa (Hripcsak and Roth-schild, 2005).
F1-score is the harmonic mean ofrecall and precision, calculated from true posi-tive, false positive, and false negative annotations,which were defined as follows:true positive (TP) = the physician and nurse prob-lem annotation was assigned the same CUI(and status)false positive (FP) = the physician problem anno-tation (and status) did not exist among thenurse problem annotations55false negative (FN) = the nurse problem anno-tation (and status) did not exist among thephysician problem annotationsRecall =TP(TP + FN)(1)Precision =TP(TP + FP )(2)F1-score =2(Recall ?
Precision)(Recall + Precision)(3)We sampled 50% of the corpus and determinedthe most common errors.
These errors withexamples were programmatically adjudicatedwith the following solutions:Spurious problems: proceduressolution: exclude non-problems via guidelinesProblem specificity: CUI specificity differencessolution: select most general CUIsConflicting status: negated vs. resolvedsolution: select second reviewer?s statusCUI/CUI-less: C0031039 vs. CUI-lesssolution: select CUI since clinically usefulWe split the dataset into about two-thirds train-ing and one-third test for each report type.
The re-maining data analysis was performed on the train-ing set.2.2 Coverage StudyWe characterized the composition of the referencestandard patient problem lists against two stan-dard vocabularies SNOMED-CT and SNOMED-CT CORE Problem List.
We evaluated the cover-age of patient problems against the SNOMED CTCORE Problem List since the list was developedto support encoding clinical observations such asfindings, diseases, and disorders for generating pa-tient summaries like problem lists.
We evaluatedthe coverage of patient problems from the corpusagainst the SNOMED-CT January 2012 Releasewhich leverages the UMLS version 2011AB.
Weassessed recall (Eq 1), defining a TP as a patientproblem CUI occurring in the vocabulary and aFN as a patient problem CUI not occurring in thevocabulary.3 ResultsWe report the results of our annotation study onthe full set and vocabulary coverage study on thetraining set.3.1 Annotation StudyThe full dataset is comprised of 298 clinical doc-uments - 136 (45.6%) DS, 54 (18.1%) ECHO,54 (18.1%) RAD, and 54 (18.1%) ECG.
Seventy-four percent (221) of the corpus was annotated byboth annotators.
Table 1 shows agreement overalland by report, matching problem CUI and prob-lem CUI with status.
Inter-annotator agreementfor problem with status was slightly lower for allreport types with the largest agreement drop forDS at 15% (11.6 points).Report Type CUI CUI + StatusDS 77.1 65.5ECHO 83.9 82.8RAD 84.7 82.8ECG 89.6 84.8Table 1: Document-level IAA by report type for problem(CUI) and problem with status (CUI + status)We report the most common errors by frequencyin Table 2.
By report type, the most common er-rors for ECHO, RAD, and ECG were CUI/CUI-less, and DS was Spurious Concepts.Errors DS ECHO RAD ECGSP 423 (42%) 26 (23%) 30 (35%) 8 (18%)PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)Table 2: Error types by frequency - Spurious Problems (SP),Problem Specificity (PS), Conflicting status (CS), CUI/CUI-less (CC)3.2 Coverage StudyIn the training set, there were 203 clinical docu-ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.The average number of problems were 22?10 DS,10?4 ECHO, 6?2 RAD, and 4?1 ECG.
Thereare 5843 total current problems in SNOMED-CTCORE Problem List.
We observed a range ofunique SNOMED-CT problem concept frequen-cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG56by report type.
The prevalence of covered prob-lem concepts by CORE is 461 (59%) DS, 36(57%) ECHO, 71 (63%) RAD, and 16 (44%)ECG.
In Table 3, we report coverage of patientproblems for each vocabulary.
No reports wereannotated as ?Healthy - no problems?.
All reportshave SNOMED CT coverage of problem mentionsabove 80%.
After mapping problem mentions toCORE, we observed coverage drops for all reporttypes, 24 to 36 points.Report Patient Annotated with Mapped toType Problems SNOMED CT COREDS 2000 1813 (91%) 1335 (67%)ECHO 349 300 (86%) 173 (50%)RAD 190 156 (82%) 110 (58%)ECG 95 77(81%) 43 (45%)Table 3: Patient problem coverage by SNOMED-CT andSNOMED-CT CORE4 DiscussionIn this feasibility study, we evaluated how reliablytwo domain experts can generate a patient problemlist and assessed the coverage of annotated patientproblems against two standard clinical vocabular-ies.4.1 Annotation StudyOverall, we demonstrated that problems can be re-liably annotated with moderate to high agreementbetween domain experts (Table 1).
For DS, agree-ment scores were lowest and dropped most whenconsidering the problem status in the match crite-ria.
The most prevalent disagreement for DS wasSpurious problems (Table 2).
Spurious problemsincluded additional events (e.g., C2939181: Mo-tor vehicle accident), procedures (e.g., C0199470:Mechanical ventilation), and modes of administra-tion (e.g., C0041281: Tube feeding of patient) thatwere outside our patient problem list inclusion cri-teria.
Some pertinent findings were also missed.These findings are not surprising given on averagemore problems occur in DS and the length of DSdocuments are much longer than other documenttypes.
Indeed, annotators are more likely to missa problem as the number of patient problems in-crease.Also, status differences can be attributed to mul-tiple status change descriptions using expressionsof time e.g., ?cough improved then?
and modal-ity ?rule out pneumonia?, which are harder totrack and interpret over a longer document.
Themost prevalent disagreements for all other doc-ument types were CUI/CUI-less in which iden-tifying a CUI representative of a clinical obser-vation proved more difficult.
An example ofOther disagreement was a sidedness mismatchor redundant patient problem annotation.
Forexample, C0344911: Left ventricular dilatationvs.
C0344893: Right ventricular dilatation orC0032285: Pneumonia was recorded twice.4.2 Coverage StudyWe observed that DS and RAD reports have highercounts and coverage of unique patient problemconcepts.
We suspect this might be because otherdocument types like ECG reports are more likelyto have laboratory observations, which may beless prevalent findings in CORE.
Across documenttypes, coverage of patient problems in the corpusby SNOMED CT were high ranging from 81%to 91% (Table 3).
However, coverage of patientproblems by CORE dropped to moderate cover-ages ranging from 45% to 67%.
This suggests thatthe CORE Problem List is more restrictive andmay not be as useful for capturing patient prob-lems from these document types.
A similar reportof moderate problem coverage with a more restric-tive concept list was also reported by Meystre andHaug (2005).5 LimitationsOur study has limitations.
We did not apply a tra-ditional adjudication review between domain ex-perts.
In addition, we selected the ShARe corpusfrom an ICU database in which vocabulary cover-age of patient problems could be very different forother domains and specialties.6 ConclusionBased on this feasibility study, we conclude thatwe can generate a reliable patient problem listreference standard for the ShARe corpus andSNOMED CT provides better coverage of patientproblems than the CORE Problem List.
In fu-ture work, we plan to evaluate from each ShARereport type, how well these patient problem listscan be derived and visualized from the individ-ual disease/disorder problem mentions leveragingtemporality and modality attributes using natu-ral language processing and machine learning ap-proaches.57AcknowledgmentsThis work was partially funded by NLM(5T15LM007059 and 1R01LM010964), ShARe(R01GM090187), Swedish Research Council(350-2012-6658), and Swedish Fulbright Com-mission.ReferencesCenter for Medicare and Medicaid Services.
2013.EHR Incentive Programs-Maintain ProblemList.
http://www.cms.gov/Regulations-and-Guidance/Legislation/EHRIncentivePrograms/downloads/3 Maintain Problem ListEP.pdf.Noemie Elhadad, Wendy Chapman, Tim OGorman,Martha Palmer, and Guergana.
Under ReviewSavova.
under review.
The ShARe Schema forthe Syntactic and Semantic Annotation of ClinicalTexts.George Hripcsak and Adam S. Rothschild.
2005.Agreement, the F-measure, and Reliability in In-formation Retrieval.
J Am Med Inform Assoc,12(3):296?298.Stephane Meystre and Peter Haug.
2005.
Automationof a Problem List using Natural Language Process-ing.
BMC Medical Informatics and Decision Mak-ing, 5(30).Stephane M. Meystre and Peter J. Haug.
2008.
Ran-domized Controlled Trial of an Automated ProblemList with Improved Sensitivity.
International Jour-nal of Medical Informatics, 77:602?12.Danielle L. Mowery, Pamela W. Jordan, Janyce M.Wiebe, Henk Harkema, John Dowling, andWendy W. Chapman.
2013.
Semantic Annotationof Clinical Events for Generating a Problem List.
InAMIA Annu Symp Proc, pages 1032?1041.National Library of Medicine.
2009.
TheCORE Problem List Subset of SNOMED-CT. Unified Medical Language System 2011.http://www.nlm.nih.gov/research/umls/SNOMED-CT/core subset.html.Mohammed Saeed, C. Lieu, G. Raber, and Roger G.Mark.
2002.
MIMIC II: a massive temporal ICUpatient database to support research in intelligent pa-tient monitoring.
Comput Cardiol, 29.Imre Solti, Barry Aaronson, Grant Fletcher, MagdolnaSolti, John H. Gennari, Melissa Cooper, and ThomasPayne.
2008.
Building an Automated Problem Listbased on Natural Language Processing: LessonsLearned in the Early Phase of Development.
pages687?691.Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.Forbush, Scott L. DuVall, and Wendy W. Chapman.2012.
A prototype tool set to support machine-assisted annotation.
In Proceedings of the 2012Workshop on Biomedical Natural Language Pro-cessing, BioNLP ?12, pages 130?139.
Associationfor Computational Linguistics.Lawrence Weed.
1970.
Medical Records, Med-ical Education and Patient Care: The Problem-Oriented Record as a Basic Tool.
Medical Pub-lishers: Press of Case Western Reserve University,Cleveland: Year Book.58
