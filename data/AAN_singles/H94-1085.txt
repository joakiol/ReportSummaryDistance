USE OF LEX ICAL  AND SYNTACTIC  TECHNIQUESIN RECOGNIZ ING HANDWRITTEN TEXTRohini K. SrihariCenter for Document Analysis and Recognition (CEDAR)SUNY at BuffaloBuffalo, NY 14228-2567ABSTRACTThe output of handwritten word recognizers (Wit) tends tobe very noisy due to various factors.
In order to compensatefor this behaviour, several choices of the WR must be ini-tially considered.
In the case of handwritten sentence/phraserecognition, linguistic constraints may be applied in orderto improve the results of the Wit.
This paper discusses twostatistical methods of applying linguistic constraints to theoutput of an Wi t  on input consisting of sentences/phrases.The first is based on collocations and can be used to prOmotelower ranked word choices or to propose new words.
Thesecond is a Markov model of syntax and is based on syn-tactic categories (tags) associated with words.
In each case,we show the improvement in the word recognition rate as aresult of applying these constraints.1.
INTRODUCTIONThis paper focuses on the use of human language modelsin performing handwriting recognition.
Systems that recog-nize handwriting are referred to as off-line or on-line sys-tems, depending on whether ordinary handwriting on paperis scanned and digitized or a special stylus and a pressure-sensitive tablet are used.
The central component of a hand-written text recognizer is a word recognizer (Wit) which takesas input, a word signal and a lexicon.
Its output consists ofan ordered list of the best n words in the lexicon which matchthe word signal.
Due to wide variability in writing, WRs of-ten do not return the correct word as the top choice and getworse as the lexicon size increases.
Furthermore, the correctword may not even be present in the top n choices.
This isi l lustrated in Figure 1 which shows the output of an actualword recognizer (offiine) on isolated word images.imately 200 words on the average.
In the second stage, theword-image is segmented into several components; physicalfeatures of each component lead to a set of character choicesfor each segment hus resulting in a set of candidate words.All candidate words which are in the lexicon are returned asthe direct recognition output of the Wit.
In case none of thewords are found in the lexicon (,~ 62% of the time), stringmatching (the third stage) is performed.Since the training phase (of the language module) requiresthe processing of several thousand sentences, the computa-tionally expensive procedure of digitizing followed by recog-nition is avoided by employing a program which simulates theoutput of an actual WR.
Based on the intermediate resultsof the actual word recognizer, we have computed statisticswhich model the behavi'our of the second stage 1.
These in-clude substitution, splitting and merging statistics.
Givenan input (ASCII) word, and the above statistics, candidate(corrupted) words are generated based on simulating and pro-pogating each of the above three types of errors at each char-acter position.
The string matching algorithm used in thesimulator is the same as that used in the actual WR.Figure 2 illustrates the entire model for recognizing handwrit-ten text.
The ultimate goal of language models is to providefeedback to the word recognizer as indicated by the dashedlines in Figure 2.
There are two types of feedback provided:(i) feedback information to the Wit  post-processor in termsof eliminating syntactic categories from contention, or (ii)feedback to word recognition e.g., if syntactic analysis hasdetermined that a particular token must be alphabetic only(as opposed to mixed alphanumeric), this information couldbe incorporated in a second "reading" of the word image.This necessitates the use of linguistic constraints (which em-ploy phrase and sentence-level context) to achieve a perfor-mance level comparable to that of humans \[1, 2\].
We presenttwo techniques, (i) lexical analysis using collocations, and (ii)syntactic (n-gram) analysis using part-of-speech (POS) tags,both designed to improve the WR rate.2.
ISOLATED HANDWRITTENWORD RECOGNITION (WR)This research employs both off-line \[3\] and on-line word rec-ognizers \[4\].
The actual WK is implemented as a three-stageprocedure.
In the first stage, wholistic features of the wordare used to reduce the lexicon from 21,000 words to approx-*This work was supported in part by NSF grant IRI-93150063.
TRAINING CORPUS, LEXICONA database of representative t xt is crucial for this research.We are using an electronic orpus consisting of several thou-sand e-mail messages which is best categorized as intra-departmental communication (e.g., meeting notifications, re-quests for informa- tion, etc.).
The style of language usedin e-mail reflects that used in handwriting: informal, un-grammatical at times, relatively short sentences, etc.
Sucha training set has been collected and has being tagged usingthe Xerox POS tagger.
We employ a 21,000 word lexiconderived from this e-mail corpus which is represented as a t r ieto permit efficient access.1The simulator assumes perfect performance for the wholisticlexicon reduction stage; the actual module performs with betterthan 95% accuracy.427my alarm code soil rout wakedrcle raid hotshute risk listclock visit riotmail mostthtaMs .
having uprunningthis lovingFigure 1: Isolated Word Recognition Output.
Correct words are shown in bold; italicized lists indicate that correct word isnot among top choices4.
LEXICAL  ANALYSIS US INGCOLLOCATIONAL INFORMATIONThis module applies collocational information \[5\] in order tomodify word neighbourhoods generated by the WR.
Thesemodified neighbourhoods are then input to a statistical syn-tax analysis module which makes final word choices.
Collo-cations are word patterns that occur frequently in language;intuitively, if word A is present, there is a high probabil-ity that word B is also present.
We use Xtract to find col-locations in a 2.1 million word portion of the Wall StreetJournal corpus ~.
Collocations are categorized based on (i)the strength of their association (mutual information score,mis) and (ii) the mean and variance of the separation be-tween them.
At this point we are considering only fixedcollocations uch as compound nouns (e.g., "computer sci-entist", "letter of intent"), and lexico-syntactic collocations(e.g., "giving up ' )  which are categorized by low variance intheir separation.
In this training set, "significant" colloca-tions occur at the rate of approximately 2.6 per sentence,thus making it worthwhile to perform collocational analysis.Specifically, collocational analysis can result in the follow-ing actions (ranked from conservative to aggressive): (i) re-rank the word choices thereby promoting more likely words,(ii) eliminate word choices thereby reducing word neighbour-hoods, or (iii) propose new words (not in the top n choicesof the WR).
The first two actions are possible only if, foreach word in the multi-word collocation, the correct word isamong the top n choices of the WR.
The last action does nothave this restriction and constitutes a form of error detectionand correction.Based on the (i) the strength of the collocation that a wordchoice participates in, mis(xy), and (ii) the confidence givento this word by the WR wr_conf(x), a decision is madewhether to simply promote a word choice (i.e., increase itsrank) or to promote it to the top choice and eliminate allother word choices for each of the word neighbourhoods par-2Due to the  current ly  inadequate  size of the e-mai l  corpus, weare temporar i l y  conduct ing  our exper iments  on the WSJ  corpus.ticipating in the collocation.
We compute the lexically ad-justed score of the word las(z) = mis(xy) + wr_conf(x); if aword does not participate in any collocation with an adjacentword, its score remains the same.
The word choices are thenre-ranked based on any new scores.
There are two specialactions which are taken:1.
If one word in a collocation is promoted to top choice,the remaining words (if they are one of the top choices)are also promoted to the top choice.2.
If the confidences of word choices fall below a certainthreshold t (based on the difference between it and thetop choice), then they are ehminated from further con-sideration.This is il lustrated in Figure 3.Actual words: Wa l l  S t reetBEFORE AFTERrecal l  2.31 street 3.67 wall 6.87 streetrevolt  1.79 st ra i t  3.36 recall P.31 straitsmal l  1.75 str ict  3.22 .
.
.
.
.
.overal l  1.73 s t reak 3.14enrol l  1.71 s t rand  2.58wall 1.43 st ruck 2.369.113.36Figure 3: Collocational information used to re-rank anddelete words.
The words in italics (and those below) aredeleted from further consideration.Based on a test set of 1025 words from the WSJ, colloca-tional analysis improved the percentage correct in the topchoice from 67% to 72.5%.
We are experimenting with var-ious thresholds for deleting word choices which minimizesthe error.
We are in the process of extending collocationalanalysis by using one-sided information score.
For example,the word 'offended' is frequently followed by the word 'by',428lnpu~ Tokenization ~ WholtsticFilter LexiconImage I (word sep.) (intrinsic image (~ 30,000 words)features)reducedmodified wordnbds.Syntaetie Processing(n-gram odels) modHied syntactiecategories :, ..P..m~r_u_o _urn.,..~.~_ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?lsentence/phraseoutputFigure 2: Overall Model of Sentence/Phrase R cognitionL redueedword nbdl.ComMnmed ISyntadle, Semantic \[ Proceuing \](hybrid syntax models,,\[ semantic danes) \]but the word 'by'  may be preceded by virtually anything.Such analysis extends the utility of collocational analysis butcomes with a risk of promoting incorrect word choices.Action (iii), namely proposing new words, is based on thevisually similar neighbourhood (VSN) of a word choice.
TheVSN of a word is computed by the same process that is usedby the WR to reduce a lexicon based on wholistic propertiesof a word.
In cases where the reduced lexicon is still toolarge (over 200 words), more stringent constraints (such asword length) are applied in order to reduce the size evenfurther.
The VSN is computed automatically from the ASCIIrepresentation of a word.
For example, if the correct wordsaxe "nuclear power" and the set of word choices result in"nucleus power" and "mucus power", collocational analysisresults in the additional word choice "nuclear".
This is basedon the fact that (i) "nuclear" is in the VSN of "nucleus" and(ii) the words "nuclear power" constitute a strong collocation.This method is currently being attempted for only a small setof strong collocations.5.
SYNTACTIC  MODELS:  US ING POSTAGS TO REDUCE WORDNE IGHBOURHOODSThe performance of a WR system can be improved byincorporating statistical information at the word sequencelevel.
The performance improvement derives from selectionof lower-rank words from the WR output when the surround-ing context indicates uch selection makes the entire sentencemore probable.
Given a set of output words .X which emanatefrom a noisy channel (such as an WR), N-gram word mod-els \[6\] seek to determine the string of words VV which mostprobably gave rise to it.
This amounts to finding the stringITV for which the a posteriori probabilityP(W \[ .X) = P(VV) ?
P(.~ I 17?
)P(X)is maximum, where P (X  \] W) is the probability of observingX when W is the true word sequence, P(VV) is the a pri-ori probability of W and P(X) is the probability of stringX.
The values for each of the P(Xi \[ Wi) are known as thechannel (or confusion) probabilities and can be estimated em-pirically.
If we assume that words are generated by an nthorder Maxkov source, then the a priori probability P (W)  canbe estimated asP(W) = P(W~+i I W,~+l-,).
.
.
P(W1 \] Wo) * P(Wo)where P(Wn I Wh .
.
.
.
.
Wk-z) is called the nth-order tran-sitional probability.
The Viterbi algorithm \[7\] is a dynamicmethod of finding optimal solutions to the above quantity.The problem with such approaches is that as the numberof words grow in the vocabulary, estimating the parametersreliably becomes difficult.
More specifically, the number oflow or zero-valued entries in the transition matrix starts torise exponentially.
\[8\] reports that of the 6.799 X 10 l?
2-grams that could possibly occur in a 365,893,263 word corpus(consisting of 260,740 unique words), only 14,494,217 actuallyoccured, and of these, 8,045,024 occured only once.In n-gram class models, words axe mapped into syntactic \[9\]classes.
In this situation, p(wt I wt-1) becomes:p(w, I w,_l) = p(~,, I C(w,)) p(C(w,) I C(w,_,))where p(C(wt) I C(wt-1)) is the probability to get to theclass C(wt) following the class C(wt-1) and p(wt I C(wt)) isthe probability to get the word wt among the words of theclass C(wt).The research described here uses n-gram class models wherepaxt-of-speech (POS) tags are used to classify words.
We usethe notation A : B to indicate the case where word A hasbeen assigned the tag B.
For each sentence analyzed, we429form a word:tag lattice representing all possible sentences forthe set of word choices output by string matching (see figure4) 3.
The problem is to find the best path(s) through thislattice.
Computation of the best path requires the follow-ing information: (i) tag transition statistics, and (ii) wordprobabilities.Transition probabilities describe the likelihood of a tag fol-lowing some preceding (sequence of) tag(s).
These statisticsare calculated uring training as:e(tagBitagA) = #(tagA ~ tagB)#(tagA)Beginning- and end- of-sentence markers axe incorporated astags themselves to obtain necessary sentence-level informa-tion.Word probabilities are defined (and calculated uring train-ing) as:#(Word : Tag)P(Word \[ Tag) = #(AnyWord : Tag)The above statistics have been computed for the e-mail cor-pus.
The Xerox POS tagger \[10\] has been employed to tagthe corpus; the tagset used is the Penn treebank tagset.
Theadvantage of the Xerox tagger is the ability to train it on anuntagged corpus.The Viterbi algorithm is used to find the best Word:Tag se-quence through the lattice, i.e., the maximal value of thefollowing quantity:nH P(Word, \] Tag,)P(Tag, I Tag,-1)i=1over all possible tag sequences T = Tago, Taga .. .
.
Tag,+1where Tago and Tag,b+1 are the beginning-of-sentence andend-of-sentence tags respectively.
The Viterbi algorithm al-lows the best path to be selected without explicitly enumer-ating all possible tag sequences.
A modification to this algo-rithm produces the best n sequences.The lattice of Figure 4 demonstrates this procedure beingused to derive the correct ag sequence ven when the correctword ('the') was not output by the WR.
The chosen path isi l lustrated in boldface.
The values on the edges represent tagtransition probabilities and the node values represent wordprobabilities.
Analysis showed that the correct tag most fre-quently missing from the lattice was the DT (determiner)tag.
Thus, the DT tag is automatically included in the lat-tice in all cases of short words (< 4 characters) where it wasnot otherwise a candidate.A test set of 140 sentences from the e-mail corpus producedthe results shown in Figure 5.
The percentage of words cor-rectly recognized as the top choice increased from 51% to61% using this method; ceiling is 70% due to correct wordchoice being absent in WR output.
Furthermore, by elim-inating all word choices that were not part of the top 203the presence of the DT tag in the trellis is explained belowsequences output by the Viterbi, a reduction in the averageword neighbourhood of 56% (from 4.4 to 1.64 choices/word)was obtained with an error rate of only 3%.
The latter isuseful if a further language model is to be applied (e.g., se-mantic analysis) since fewer word choices, and therefore faxfewer sen- tence possibilities remain.While this method is effective in reducing word neighbour-hood sizes, it does not seem to be effective in determining thecorrect/best 4 sentence (the ultimate objective) or in provid-ing feedback.
We are investigating hybrid models (combiningsyntax and semantics) for achieving this.References1.
Rohini K. Srihaxi and Charlotte M. Baltus.
Incorpo-rating Syntactic Constraints in Recognizing Handwrit-ten Sentences.
In Proceedings of the International JointConference on Artificial Intelligence (1JCA1-93), pages1262-1267, 1993.2.
Rohini K. Srihaxi, Charlotte M. Baltus, Stayvis Ng, andJackie Kud.
Use of Language Models in On-line Recog-nition of Handwritten Sentences.
In Proceedings of theThird International Workshop on Frontiers in Handwrit-ing Recognition (1WFHR-3), pages 284-294, 1993.3.
John Favata and S.N.
Srihaxi.
Recognition of GeneralHandwritten Words Using a Hypothesis Generation andReduction Methodology.
In Proceedings of the UnitedStates Postal Service Advanced Technology Conference,pages 237-245, 1992.4.
Giovanni Seni, Nasser Nasrabadi, and Rohini K. Srihaxi.An On-Line Cursive Word Recognition System.
In Pro-ceedings of the conference on Computer Vision and Pat-tern Recognition (CVPR-94), to appear, 1994.5.
Frank Smadja.
Macrocoding the Lexicon with Co-Occurrence Knowledge.
In Uri Zernik, editor, LexicalAcquisition: Exploiting On-Line Resources to Build aLexicon, pages 165-189.
Lawrence Erlbaum Associates,Hillsdale, N J, 1991.6.
L.R.
Bahl, F. Jelinek, and R.L.
Mercer.
A MaximumLikelihood Approach to Continuous Speech Recognition.IEEE Transactions of Pattern Analysis and Machine In-telligence (PAMI), 5(2):179-190, 83.7.
G. E. Forney Jr.
The Viterbi Algorithm.
Proceedings ofIEEE, 61:268-278, 1973.8.
Averbuch et al Experiments with the tangora 20,000word speech recognizer.
In Proceedings of the IEEE In-ternational Conference on Acoustics, Speech and SignalProcessing, pages 701-704, 1987.9.
F.G. Keenan, L.J.
Evett, and R.J. Whitrow.
A largevocabulary stochastic syntax analyser for handwritingrecognition.
In Proceedings of the First InternationalConference on Document Analysis (ICDAR-91), pages794-802, 1991.10.
Doug Cutting, Julian Kupiec, Jan Pederson, and Pene-lope Sibun.
A Practical Part-of-Speech Tagger.
Techni-cal paper, Xerox Palo Alto Research Center, 1993.4The best sentence is one where all the words correctly recog-nized by the WR in the top 3 choices are selected; the sentencemay not be correct due to failure of the WR to recognize all words.430WORD LATTICEActual sentence:he/PPItWR word choices:haheWord/Tag lattice:Selected sentence:he/PPwiWMD slgn/VB the/DT letter/NNsign tie letterwider,h_ r-:--ZT:Li - - ' l  I \["~~_~'_~lm_ .__  - I  s ,gn~, ,F -  ~.1..~ ~ I"" II IIt,will/MD sign/VB ....jDT letter/NNFigure 4: Sample Word:Tag Latt ice For Analysis of WR choices./.Test set: 140 sentences from e-mall CorpusWR results: top choice 51.89% top 5 choices 69,45%CRITERIA% correct ags% correct words% correct Sentence% best sentence% reduction in word/tag nbd:er rorCorrect Word AddedNo.'
of Sequence ~hoices ' ,1 5 10 2091.34 95,79' 96.80 95:3"889.0 94.10 95.31 95.8632:14 51,43 i 55.71 58,5732.14 51.43 55.71 58.57-78.26 69.83 63.77 58.5010.0 5.90 4.69 4.14Correct Word Not AddedNo.
of Seq.uence ChoicesI 5 I0 2065.19 70.II  71.35 72.7561.13 64,85 66,25 67.164.29 5.71 5.7i 5.7140.0 57.14 63.57' 70,076.37 67.34 61.16 55.8911.87 6.63 4.61 3.30Average word/tag nbd.
size (correct words not added) before Method 1 : 4.40Avg.
word/tag nbd.
size if top 20 sequence choices axe taken: 1,6410 word sentence: 4.40 TM > 10 million sentences 1.64 l?
-- 140 sentencesFigure 5: Results from Syntact ic Class Markov Model431
