Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 770?778,Beijing, August 2010Recognising Entailment within DiscourseShachar Mirkin?, Jonathan Berant?, Ido Dagan?, Eyal Shnarch??
Computer Science Department, Bar-Ilan University?
The Blavatnik School of Computer Science, Tel-Aviv UniversityAbstractTexts are commonly interpreted based onthe entire discourse in which they are sit-uated.
Discourse processing has beenshown useful for inference-based applica-tion; yet, most systems for textual entail-ment ?
a generic paradigm for applied in-ference ?
have only addressed discourseconsiderations via off-the-shelf corefer-ence resolvers.
In this paper we explorevarious discourse aspects in entailment in-ference, suggest initial solutions for themand investigate their impact on entailmentperformance.
Our experiments suggestthat discourse provides useful informa-tion, which significantly improves entail-ment inference, and should be better ad-dressed by future entailment systems.1 IntroductionThis paper investigates the problem of recognisingtextual entailment within discourse.
Textual En-tailment (TE) is a generic framework for appliedsemantic inference (Dagan et al, 2009).
UnderTE, the relationship between a text (T) and a tex-tual assertion (hypothesis, H) is defined such thatT entails H if humans reading T would infer thatH is most likely true (Dagan et al, 2006).TE has been successfully applied to a variety ofnatural language processing applications, includ-ing information extraction (Romano et al, 2006)and question answering (Harabagiu and Hickl,2006).
Yet, most entailment systems have thusfar paid little attention to discourse aspects of in-ference.
In part, this is the result of the unavail-ability of adept tools for handling the kind of dis-course processing required for inference.
In addi-tion in the main TE benchmarks, the RecognisingTextual Entailment (RTE) challenges, discourseplayed little role.
This state of affairs has startedto change with the recent introduction of the RTEPilot ?Search?
task (Bentivogli et al, 2009b), inwhich assessed texts are situated within completedocuments.
In this setting, texts need to be inter-preted based on their entire discourse (Bentivogliet al, 2009a), hence attending to discourse issuesbecomes essential.
Consider the following exam-ple from the task?s dataset:(T) The seven men on board were said to haveas little as 24 hours of air.For the interpretation of T, e.g.
the identity andwhereabouts of the seven men, one must considerT?s discourse.
The preceding sentence T?, for in-stance, provides useful information to that aim:(T?)
The Russian navy worked desperately tosave a small military submarine.This example demonstrates a common situation intexts, and is also applicable to the RTE Searchtask?s setting.
Still, little was done by the task?sparticipants to consider discourse, and sentenceswere mostly processed independently.Analyzing the Search task?s development set,we identified several key discourse aspects that af-fect entailment in a discourse-dependent setting.First, we observed that the coverage of availablecoreference resolution tools is considerably lim-ited.
To partly address this problem, we extend theset of coreference relations to phrase pairs witha certain degree of lexical overlap, as long as nosemantic incompatibility is found between them.Second, many bridging relations (Clark, 1975) arerealized in the form of ?global information?
per-ceived as known for entire documents.
As bridg-ing falls completely out of the scope of availableresolvers, we address this phenomenon by iden-tifying and weighting prominent document termsand allowing their incorporation in inference even770when they are not explicitly mentioned in a sen-tence.
Finally, we observed a coherence-relateddiscourse phenomenon, namely inter-relations be-tween entailing sentences in the discourse, suchas the tendency of entailing sentences to be ad-jacent to one another.
To that end, we apply atwo-phase classification scheme, where a second-phase meta-classifier is applied, extracting dis-course and document-level features based on theclassification of each sentence on its own.Our results show that, even when simple so-lutions are employed, the reliance on discourse-based information is helpful and achieves a sig-nificant improvement of results.
We analyze thecontribution of each component and suggest somefuture work to better attend to discourse in entail-ment systems.
To our knowledge, this is the mostextensive effort thus far to empirically explore theeffect of discourse on entailment systems.2 BackgroundDiscourse plays a key role in text understandingapplications such as question answering or infor-mation extraction.
Yet, such applications typicallyonly handle a narrow aspect of discourse, address-ing coreference by term substitution (Dali et al,2009; Li et al, 2009).
The limited coverage andscope of existing tools for coreference resolutionand the unavailability of tools for addressing otherdiscourse aspects also contribute to this situation.For instance, VP anaphora and bridging relationsare usually not handled at all by such resolvers.
Asimilar situation is seen in the TE research field.The prominent benchmark for entailment sys-tems evaluation is the series of RTE challenges.The main task in these challenges has tradition-ally been to determine, given a text-hypothesispair (T,H), whether T entails H. Discourse playedno role in the first two RTE challenges asT?s were constructed of short simplified texts.In RTE-3 (Giampiccolo et al, 2007), wheresome paragraph-long texts were included, inter-sentential relations became relevant for correct in-ference.
Yet the texts in the task were manuallymodified to ensure they are self-contained.
Con-sequently, little effort was invested by the chal-lenges?
participants to address discourse issuesbeyond the standard substitution of coreferringnominal phrases, using publicly available toolssuch as JavaRap (Qiu et al, 2004) or OpenNLP1,e.g.
(Bar-Haim et al, 2008).A major step in the RTE challenges towards amore practical setting of text processing applica-tions occurred with the introduction of the Searchtask in the Fifth RTE challenge (RTE-5).
In thistask entailing sentences are situated within doc-uments and depend on other sentences for theircorrect interpretation.
Thus, discourse becomesa substantial factor impacting inference.
Surpris-ingly, discourse hardly received any treatment inthis task beyond the standard use of coreferenceresolution (Castillo, 2009; Litkowski, 2009), andan attempt to address globally-known informationby removing from H words that appear in docu-ment headlines (Clark and Harrison, 2009).3 The RTE Search TaskThe RTE-5 Search task was derived from theTAC Summarization task2.
The dataset consistsof several corpora, each comprised of news arti-cles concerning a specific topic, such as the im-pact of global warming on the Arctic or the Lon-don terrorist attacks in 2005.
Hypotheses weremanually generated based on Summary ContentUnits (Nenkova et al, 2007), clause-long state-ments taken from manual summaries of the cor-pora.
Texts are unmodified sentences in the arti-cles.
Given a topic and a hypothesis, entailmentsystems are required to identify all sentences inthe topic?s corpus that entail the hypothesis.Each sentence-hypothesis pair in both the de-velopment and test sets was annotated, judgingwhether the sentence entails the hypothesis.
Outof 20,104 annotations in the development set, only810 were judged as positive.
This small ratio (4%)of positive examples, in comparison to 50% in tra-ditional RTE tasks, better corresponds to the natu-ral distribution of entailing texts in a corpus, thusbetter simulates practical settings.The task may seem as a variant of informationretrieval (IR), as it requires finding specific textsin a corpus.
Yet, it is fundamentally different fromIR for two reasons.
First, the target output is a set1http://opennlp.sourceforge.net2http://www.nist.gov/tac/2009/Summarization/771of sentences, each evaluated independently, ratherthan a set of documents.
Second, the decision cri-terion is entailment rather than relevance.Despite the above, apparently, IR techniquesprovided hard-to-beat baselines for the RTESearch task (MacKinlay and Baldwin, 2009), out-performing every other system that relied on in-ference without IR-based pre-filtering.
At the cur-rent state of performance of entailment systems, itseems that lexical coverage largely overshadowsany other approach in this task.
Still, most (6 outof 8) participants in the challenge applied their en-tailment systems to the entire dataset without aprior retrieval of candidate sentences.
F1 scoresfor such systems vary between 10% and 33%, incomparison to over 40% of the IR-based methods.4 The Baseline RTE SystemIn this work we used BIUTEE, Bar-Ilan Univer-sity Textual Entailment Engine (Bar-Haim et al,2008; Bar-Haim et al, 2009), a state of the artRTE system, as a baseline and as a basis for ourdiscourse-based enhancements.
This section de-scribes this system?s architecture; the methods bywhich it was augmented to address discourse arepresented in Section 5.To determine entailment, BIUTEE performs thefollowing main steps:Preprocessing First, all documents are parsedand processed with standard tools for named en-tity recognition (Finkel et al, 2005) and corefer-ence resolution.
For the latter purpose, we useOpenNLP and enable the substitution of corefer-ring terms.
This is the only way by which BIUTEEaddresses discourse, representing the state of theart in entailment systems.Entailment-based transformations Given aT-H pair (both represented as dependencyparse trees), the system applies a sequence ofknowledge-based entailment transformations overT, generating a set of texts which are entailed byit.
The goal is to obtain consequent texts whichare more similar to H. Based on preliminary re-sults on the development set, in our experiments(Section 6) we use WordNet (Fellbaum, 1998) asthe system?s only knowledge resource, using itssynonymy, hyponymy and derivation relations.Classification A supervised classifier, trainedon the development set, is applied to determineentailment of each pair based on a set of syntacticand lexical syntactic features assessing the degreeby which T and its consequents cover H.5 Addressing DiscourseIn the following subsections we describe theprominent discourse phenomena that affect infer-ence, which we have identified in an analysis ofthe development set and addressed in our imple-mentation.
As mentioned, these phenomena arepoorly addressed by available reference resolversor fall completely out of their scope.5.1 Augmented coreference setA large number of coreference relations are com-prised of terms which share lexical elements, (e.g.
?airliners?s first flight?
and ?Airbus A380?s firstflight?).
Although common in coreference rela-tions, standard resolvers miss many of these cases.For the purpose of identifying additional corefer-ring terms, we consider two noun phrases in thesame document as coreferring if: (i) their headsare identical and (ii) no semantic incompatibil-ity is found between their modifiers.
The typesof incompatibility we handle are: (a) mismatch-ing numbers, (b) antonymy and (c) co-hyponymy(coordinate terms), as specified by WordNet.
Forexample, two nodes of the noun distance wouldbe considered incompatible if one is modified byshort and the second by its antonym long.
Simi-larly, two modifier co-hyponyms of distance, suchas walking and running would also result suchan incompatibility.
Adding more incompatibilitytypes (e.g.
first vs. second flight) may further im-prove the precision of this method.5.2 Global informationKey terms or prominent pieces of information thatappear in the document, typically at the title or thefirst few sentences, are many times perceived as?globally?
known throughout the document.
Forexample, the geographic location of the documenttheme, mentioned at the beginning of the docu-ment, is assumed to be known from that point on,and will often not be mentioned explicitly in fur-ther sentences.
This is a bridging phenomenon772that is typically not addressed by available dis-course processing tools.
To compensate for that,we identify key terms for each document basedon tf-idf scores and consider them as global in-formation for that document.
For example, globalterms for the topic discussing the ice melting inthe Arctic, typically contain a location such asArctic or Antarctica and terms referring to ice, likepermafrost or iceshelf.We use a variant of tf-idf, where term frequencyis computed as follows: tf(ti,j) = ni,j+~?> ?
~fi,j .Here, ni,j is the frequency of term i in document j(ti,j), which is incremented by additional positiveweights (~?)
for a set of features ( ~fi,j) of the term.Based on our analysis, we defined the followingfeatures, which correlated mostly with global in-formation: (i) does the term appear in the title?
(ii) is it a proper name?
(iii) is it a location?
Theweights for these features are set empirically.The document?s top-n global terms are addedto each of its sentences.
As a result, a global termthat occurs in the hypothesis is matched in eachsentence of the document, regardless of whetherthe term explicitly appears in the sentence.Considering the previous sentence Anothermethod for addressing missing coreference andbridging relations is based on the assumption thatadjacent sentences often refer to the same entitiesand events.
Thus, when extracting classificationfeatures for a given sentence, in addition to thefeatures extracted from the parse tree of the sen-tence itself, we extract the same set of featuresfrom the current and previous sentences together.Recall the example presented in Section 1.
T isannotated as entailing the hypothesis ?The AS-28mini-submarine was trapped underwater?, but theword submarine, e.g., appears only in its preced-ing sentence T?.
Thus, considering both sentencestogether when classifying T increases its coverageof the hypothesis.
Indeed, a bridging reference re-lates on board in T with submarine in T?, justify-ing our assumption in this case.5.3 Document-level classificationBeyond discourse references addressed above,further information concerning discourse and doc-ument structure is available in the Search settingand may contribute to entailment classification.We observed that entailing sentences tend to comein bulks.
This reflects a common coherence as-pect, where the discussion of a specific topic istypically continuous rather than scattered acrossthe entire document.
This locality phenomenonmay be useful for entailment classification sinceknowing that a sentence entails the hypothesis in-creases the probability that adjacent sentences en-tail the hypothesis as well.To capture this phenomenon, we use a two-phase meta-classification scheme, in which ameta-classifier utilizes entailment classificationsof the first classification phase to extract meta-features and determine the final classification de-cision.
This scheme also provides a convenientway to combine scores from multiple classifiersused in the first classification phase.
We referto these as base-classifiers.
This scheme and themeta-features we used are detailed hereunder.Let us write (s, h) for a sentence-hypothesispair.
We denote the set of pairs in the development(training) set asD and in the test set as T .
We splitD into two halves, D1 and D2.
We make use of nbase-classifiers, C1, .
.
.
, Cn, among which C?
isa designated classifier with additional roles in theprocess, as described below.
Classifiers may dif-fer, for example, in their classification algorithm.An additional meta-classifier is denoted CM .
Theclassification scheme is shown as Algorithm 1.Algorithm 1 Meta-classificationTraining1: Extract features for every (s, h) in D2: Train C1, .
.
.
, Cn on D13: Classify D2, using C1, .
.
.
, Cn4: Extract meta-features for D2 using theclassification of C1, .
.
.
, Cn5: Train CM on D2Classification6: Extract features for every (s, h) in T7: Classify T using C1, .
.
.
, Cn8: Extract meta-features for T9: Classify T using CMAt Step 1, features are extracted for every (s, h)pair in the training set, as in the baseline system.773In Steps 2 and 3 we split the training set into twohalves (taking half of each topic), train n differentclassifiers on the first half and then classify thesecond half using each of the n classifiers.
Giventhe classification scores of the n base-classifiersto the (s, h) pairs in the second half of the train-ing set, D2, we add in Step 4 the meta-featuresdescribed in Section 5.3.1.After adding the meta-features, we train(Step 5) a meta-classifier on this new set of fea-tures.
Test sentences then go through the sameprocess: features are extracted for them and theyare classified by the already trained n classifiers(Steps 6 and 7), meta-features are extracted inStep 8, and a final classification decision is madeby the meta-classifier in Step 9.A retrieval step may precede the actual en-tailment classification, allowing the processing offewer and potentially ?better?
candidates.5.3.1 Meta-featuresThe following features are extracted in ourmeta-classification scheme:Classification scores The classification score ofeach of the n base-classifiers.Title entailment In many texts, and in news ar-ticles in particular, the title and the first few sen-tences often represent the entire document?s con-tent.
Thus, knowing whether these sentences en-tail the hypothesis may be an indicator to the gen-eral potential of the document to include entailingsentences.
Two binary features are added accord-ing to the classification of C?
indicating whetherthe title entails the hypothesis and whether the firstsentence entails it.Second-closest entailment Considering the lo-cality phenomenon described above, we add a fea-ture assigning higher scores to sentences in thevicinity of an entailment environment.
This fea-ture is computed as the distance to the second-closest entailing sentence in the document (count-ing the sentence itself as well), according to theclassification ofC?.
Formally, let i be the index ofthe current sentence and J be the set of indices ofentailing sentences in the document according toC?.
For each j ?
J we compute di,j = |i?j|, andchoose the second smallest di,j as di.
The idea isEnt?# 1110987654321NO NOYESYESYESYESNONONONONOd2nd closest8887 or 96 or 87777772 3111123456dClosest8887 or 96 or 87666661 2111112345Figure 1: Comparison of the closest and second-closestschemes when applied to a bulk of entailing sentences (inwhite) situated within a non-entailing environment (in gray).Unlike the closest one, the second-closest scheme assignslarger distance values to non-entailing sentences located onthe ?edge?
of the bulk (5 and 10) than to entailing ones.that if entailing sentences indeed always come inbulks, then di = 1 for all entailing sentences, butdi > 1 for all non-entailing ones.
Figure 1 illus-trates such a case, comparing the second-closestdistance with the distance to the closest entailingsentence.
In the closest scheme we do not countthe sentence as closest to itself since it would dis-regard the environment of the sentence altogether,eliminating the desired effect.
We scale the dis-tance and add the feature score: ?
log(di).Smoothed entailment This feature addressedthe locality phenomenon by smoothing theclassification score of sentence i with the scoresof adjacent sentences, weighted by their distancefrom the current sentence i.
Let s(i) be thescore assigned by C?
to sentence i.
We add theSmoothed Entailment feature score:SE(i) =?w(b|w|?s(i+w))?w(b|w|)where 0 < b < 1 is the decay parameter and w isan integer bounded between?N and N , denotingthe distance from sentence i.1st sentence entailing title Bensley and Hickl(2008) showed that the first sentence in a news ar-ticle typically entails the article?s title.
We there-fore assume that in each document, s1 ?
s0,where s1 and s0 are the document?s first sentenceand title respectively.
Hence, under entailmenttransitivity, if s0 ?
h then s1 ?
h. The cor-responding binary feature states whether the sen-tence being classified is the document?s first sen-tence and the title entails h according to C?.774P (%) R (%) F1 (%)BIU-BL 14.53 55.25 23.00BIU-DISC 20.82 57.25 30.53BIU-BL3 14.86 59.00 23.74BIU-DISCno?loc 22.35 57.12 32.13All-yes baseline 4.6 100.0 8.9Table 1: Micro-average results.Note that the above locality-based features relyon high accuracy of the base classifier C?.
Oth-erwise, it will provide misleading information tothe features computation.
We analyze the effectof this accuracy in Section 6.6 Results and AnalysisUsing the RTE-5 Search data, we compareBIUTEE in its baseline configuration (cf.
Sec-tion 4), denoted BIU-BL, with its discourse-awareenhancement (BIU-DISC) which uses all the com-ponents described in Section 5.
To alleviate thestrong IR effect described in Section 3, both sys-tems are applied to the complete datasets (bothtraining and test), without candidates pre-filtering.BIU-DISC uses three base-classifiers (n = 3):SVMperf (Joachims, 2006), and Na?
?ve Bayes andLogistic Regression from the WEKA package(Witten and Frank, 2005).
The first among theseis set as our designated classifier C?, which isused for the computation of the document-levelfeatures.
SVMperf is also used for the meta-classifier.
For the smoothed entailment score (cf.Section 5.3), we used b = 0.9 and N = 3.
Globalinformation is added by enriching each sentencewith the highest-ranking term in the document, ac-cording to tf-idf scores (cf.
Section 5.2), wheredocument frequencies were computed based onabout half a million documents from the TIP-STER corpus (Harman, 1992).
The set of weights~?
equals {2, 1, 4} for title terms, proper names andlocations, respectively.
All parameters were tunedbased on a 10-fold cross-validation on the devel-opment set, optimizing the micro-averaged F1.The results are presented in Table 1.
As can beseen in the table, BIU-DISC outperforms BIU-BL inevery measure, showing the impact of addressingdiscourse in this setting.
To rule out the option thatthe improvement is simply due to the fact that weuse three classifiers for BIU-DISC and a single oneP (%) R (%) F1 (%)By TopicBIU-BL 16.54 55.62 25.50BIU-DISC 22.69 57.96 32.62All-yes baseline 4.85 100.00 9.25By HypothesisBIU-BL 22.87 59.62 33.06BIU-DISC 27.81 61.97 38.39All-yes baseline 4.96 100.00 9.46Table 2: Macro-average results.for BIU-BL, we show (BIU-BL3) the results whenthe baseline system is applied in the same meta-classification configuration as BIU-DISC, with thesame three classifiers.
Apparently, without thediscourse information this configuration?s contri-bution is limited.As mentioned in Section 5.3, the benefit fromthe locality features rely directly on the perfor-mance of the base classifiers.
Hence, consideringthe low precision scores obtained here, we appliedBIU-DISC to the data in the meta-classificationscheme, but with locality features removed.
Theresults, shown as BIU-DISCno?loc in the Table, in-dicate that indeed performance increases withoutthese features.
The last line of the table shows theresults obtained by a na?
?ve baseline where all test-set pairs are considered entailing.For completeness, Table 2 shows the macro-averaged results, when averaged over the topics orover the hypotheses.
Although we tuned our sys-tem to maximize micro-averaged F1, these figurescomply with the ones shown in Table 1.Analysis of locality As discussed in Section 5,determining whether a sentence entails a hypothe-sis should take into account whether adjacent sen-tences also entail the hypothesis.
In the above ex-periment we were unable to show the contributionof our system?s component that attempts to cap-ture this information; on the contrary, the resultsshow it had a negative impact on performance.Still, we claim that this information can be use-ful when used within a more accurate system.
Wetry to validate this conjecture by understandinghow performance of the locality features varies asthe systems becomes more accurate.
We do so viathe following simulation.When classifying a certain sentence, the classi-7752025303540 0.40.450.50.550.60.650.70.750.80.850.90.951pF1Figure 2: F1 performance of BIU-DISC as a function ofthe accuracy in classifying adjacent sentences.fications of its adjacent sentences are given by anoracle classifier that provides the correct answerwith probability p. The system is applied usingtwo locality features: the 1st sentence entailingtitle feature and a close variant of the smoothedentailment feature, which calculates the weightedaverage of adjacent sentences, but disregards thescore of the currently evaluated sentence.3 Thuswe supply information about adjacent sentencesand test whether overall performance increaseswith the accuracy of this information.We performed this experiment for p in a rangeof [0.5-1.0].
Figure 2 shows the results of this sim-ulation, based on the average F1 of five runs foreach p. Since performance, from a certain point,increases with the accuracy of the oracle classi-fier, we can conclude that indeed precise infor-mation about adjacent sentences improves perfor-mance on the current sentence, and that locality isa true phenomenon in the data.
We note, however,that performance improves only when accuracy isvery high, suggesting the currently limited prac-tical potential of this information, at least in theway locality was represented in this work.Ablation tests Table 3 presents the results of theablation tests performed to evaluate the contribu-tion of each component.
Based on the result re-ported in Table 1 and the above discussion, thetests were performed relative to BIU-DISCno?loc,the optimal configuration.
As seen in the table,the removal of each component causes a dropin results.
For global information we see a mi-3The second-closest entailment feature was not used as itconsiders the oracle?s decision for the current sentence, whilewe wish to use only information about adjacent sentences.Component removed F1 (%) ?F1 (%)Previous sent.
features 28.55 3.58Augmented coref.
26.73 5.40Global information 31.76 0.37Table 3: Results of ablation tests relative toBIU-DISCno?loc.
The columns specify the compo-nent removed, the micro-averaged F1 score achieved withoutit, and the marginal contribution of the component.nor difference, which is not surprising consideringthe conservative approach we took, using a sin-gle global term for each sentence.
Possibly, thisinformation is also included in the other compo-nents, thus proving no marginal contribution rel-ative to them.
Under the conditions of an over-whelming majority of negative examples, this isa risky method to use, and should be consideredwhen the ratio of positive examples is higher.
Forfuture work, we intend to use this information viaclassification features (e.g.
the coverage obtainedwith and without global information), rather thanthe crude addition of the term to the sentence.Analysis of augmented coreferences We an-alyzed the performance of the component foraugmenting coreference relations relative to theOpenNLP resolver.
Recall that our componentworks on top of the resolver?s output and can addor remove coreference relations.
As a completeannotation of coreference chains in the dataset isunavailable, we performed the following evalua-tion.
Recall is computed based on the numberof identified pairs from a sample of 100 intra-document coreference and bridging relations fromthe annotated dataset described in (Mirkin et al,2010).
Precision is computed based on 50 pairssampled from the output of each method, equallydistributed over topics.
The results, shown in Ta-ble 4, indicate the much higher recall obtainedby our component at some cost in precision.
Al-though rather simple, the ablation test of this com-ponent shows its usefulness.
Still, both methodsachieve low absolute recall, suggesting the needfor more robust tools for this task.P (%) R (%) F1 (%)OpenNLP 74 16 26.3Augmented coref.
60 28 38.2Table 4: Performance of coreference methods.77605101520253035404550 0102030405060708090100kF1BIU-BLBIU-DISCLuceneFigure 3: F1 performance as a function of the number ofretrieved candidates.Candidate retrieval setting As mentioned inSection 3, best performance of RTE systems in thetask was obtained when applying a first step of IR-based candidate filtering.
We therefore comparethe performance of BIU-DISC with that of BIU-BLunder this setting as well.4 For candidate retrievalwe used Lucene, a state of the art search engine5,in a range of top-k retrieved candidates.
The re-sults are shown in Figure 3.
For reference, the fig-ure also shows the performance along this rangeof Lucene as-is, when no further inference is ap-plied to the retrieved candidates.While BIU-DISC does not outperform BIU-BL atevery point, the area under the curve is clearlylarger for BIU-DISC.
The figure also indicates thatBIU-DISC is far more robust, maintaining a stableF1 and enabling a stable tradeoff between recalland precision along the whole range (recall rangesbetween 42% and 55% for k ?
[15 ?
100], withcorresponding precision range of 51% to 33%).Finally, Table 5 shows the results of the bestsystems as determined in our first experiment.We performed a single experiment to compareBIU-DISCno?loc and BIU-BL3 under a candidate re-trieval setting, using k = 20, where both systemshighly perform.
We compare these results to thehighest score obtained by Lucene, as well as to thetwo best submissions to the RTE-5 Search task6.BIU-DISCno?loc outperforms all other methods andits result is significantly better than BIU-BL3 withp < 0.01 according to McNemar?s test.4This time, for global information, the document?s threehighest ranking terms were added to each sentence.5http://lucene.apache.org6The best one is an earlier version of this work (Mirkin etal., 2009); the second is MacKinlay and Baldwin?s (2009).P (%) R (%) F1 (%)BIU-DISCno?loc 50.77 45.12 47.78BIU-BL3 51.68 40.38 45.33Lucene, top-15 35.93 52.50 42.66RTE-5 best 40.98 51.38 45.59RTE-5 second-best 42.94 38.00 40.32Table 5: Performance of best configurations.7 ConclusionsWhile it is generally assumed that discourse inter-acts with semantic entailment inference, the con-crete impacts of discourse on such inference havebeen hardly explored.
This paper presented a firstempirical investigation of discourse processingaspects related to entailment.
We argue that avail-able discourse processing tools should be substan-tially improved towards this end, both in terms ofthe phenomena they address today, namely nom-inal coreference, and with respect to the cover-ing of additional phenomena, such as bridginganaphora.
Our experiments show that even rathersimple methods for addressing discourse can havea substantial positive impact on the performanceof entailment inference.
Concerning the local-ity phenomenon stemming from discourse coher-ence, we learned that it does carry potentially use-ful information, which might become beneficialin the future when better-performing entailmentsystems become available.
Until then, integratingthis information with entailment confidence maybe useful.
Overall, we suggest that entailment sys-tems should extensively incorporate discourse in-formation, while developing sound algorithms foraddressing various discourse phenomena, includ-ing the ones described in this paper.AcknowledgementsThe authors are thankful to Asher Stern and IlyaKogan for their help in implementing and evalu-ating the augmented coreference component, andto Roy Bar-Haim for useful advice concerningthis paper.
This work was partially supportedby the Israel Science Foundation grant 1112/08and the PASCAL-2 Network of Excellence of theEuropean Community FP7-ICT-2007-1-216886.Jonathan Berant is grateful to the Azrieli Foun-dation for the award of an Azrieli Fellowship.777ReferencesBar-Haim, Roy, Jonathan Berant, Ido Dagan, IddoGreental, Shachar Mirkin, Eyal Shnarch, and IdanSzpektor.
2008.
Efficient semantic deduction andapproximate matching over compact parse forests.In Proc.
of Text Analysis Conference (TAC).Bar-Haim, Roy, Jonathan Berant, and Ido Dagan.2009.
A compact forest for scalable inferenceover entailment and paraphrase rules.
In Proc.
ofEMNLP.Bensley, Jeremy and Andrew Hickl.
2008.
Unsuper-vised resource creation for textual inference appli-cations.
In Proc.
of LREC.Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, Medea Lo Leggio, and BernardoMagnini.
2009a.
Considering discourse refer-ences in textual entailment annotation.
In Proc.
ofthe 5th International Conference on Generative Ap-proaches to the Lexicon (GL2009).Bentivogli, Luisa, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009b.
Thefifth PASCAL recognizing textual entailment chal-lenge.
In Proc.
of TAC.Castillo, Julio J.
2009.
Sagan in TAC2009: Usingsupport vector machines in recognizing textual en-tailment and TE search pilot task.
In Proc.
of TAC.Clark, Peter and Phil Harrison.
2009.
An inference-based approach to recognizing entailment.
In Proc.of TAC.Clark, Herbert H. 1975.
Bridging.
In Schank, R. C.and B. L. Nash-Webber, editors, Theoretical issuesin natural language processing, pages 169?174.
As-sociation of Computing Machinery.Dagan, Ido, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Machine Learning Challenges, vol-ume 3944 of Lecture Notes in Computer Science,pages 177?190.
Springer.Dagan, Ido, Bill Dolan, Bernardo Magnini, and DanRoth.
2009.
Recognizing textual entailment: Ratio-nal, evaluation and approaches.
Natural LanguageEngineering, pages 15(4):1?17.Dali, Lorand, Delia Rusu, Blaz Fortuna, DunjaMladenic, and Marko Grobelnik.
2009.
Questionanswering based on semantic graphs.
In Proc.
of theWorkshop on Semantic Search (SemSearch 2009).Fellbaum, Christiane, editor.
1998.
WordNet: AnElectronic Lexical Database (Language, Speech,and Communication).
The MIT Press.Finkel, Jenny Rose, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proc.
of ACL.Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third PASCAL recog-nizing textual entailment challenge.
In Proc.
of theACL-PASCAL Workshop on Textual Entailment andParaphrasing.Harabagiu, Sanda and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain ques-tion answering.
In Proc.
of ACL.Harman, Donna.
1992.
The DARPA TIPSTERproject.
SIGIR Forum, 26(2):26?28.Joachims, Thorsten.
2006.
Training linear SVMs inlinear time.
In Proc.
of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Li, Fangtao, Yang Tang, Minlie Huang, and XiaoyanZhu.
2009.
Answering opinion questions with ran-dom walks on graphs.
In Proc.
of ACL-IJCNLP.Litkowski, Ken.
2009.
Overlap analysis in textual en-tailment recognition.
In Proc.
of TAC.MacKinlay, Andrew and Timothy Baldwin.
2009.
Abaseline approach to the RTE5 search pilot.
In Proc.of TAC.Mirkin, Shachar, Roy Bar-Haim, Jonathan Berant, IdoDagan Eyal Shnarch, Asher Stern, and Idan Szpek-tor.
2009.
Addressing discourse and documentstructure in the RTE search task.
In Proc.
of TAC.Mirkin, Shachar, Ido Dagan, and Sebastian Pado?.2010.
Assessing the role of discourse references inentailment inference.
In Proc.
of ACL.Nenkova, Ani, Rebecca Passonneau, and KathleenMckeown.
2007.
The pyramid method: incorpo-rating human content selection variation in summa-rization evaluation.
In ACM Transactions on Speechand Language Processing.Qiu, Long, Min-Yen Kan, and Tat-Seng Chua.
2004.A public reference implementation of the RAPanaphora resolution algorithm.
In Proc.
of LREC.Romano, Lorenza, Milen Kouylekov, Idan Szpektor,and Ido Dagan.
2006.
Investigating a genericparaphrase-based approach for relation extraction.In Proc.
of EACL.Witten, Ian H. and Eibe Frank.
2005.
Data Mining:Practical machine learning tools and techniques,2nd Edition.
Morgan Kaufmann, San Francisco.778
