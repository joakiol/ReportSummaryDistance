Annotating Opinions in the World PressTheresa WilsonIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260, USAtwilson@cs.pitt.eduJanyce WiebeDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260, USAwiebe@cs.pitt.eduAbstractIn this paper we present a detailedscheme for annotating expressions ofopinions, beliefs, emotions, sentimentand speculation (private states) in thenews and other discourse.
We exploreinter-annotator agreement for individ-ual private state expressions, and showthat these low-level annotations are use-ful for producing higher-level subjec-tive sentence annotations.1 IntroductionIn this paper we present a detailed scheme forannotating expressions of opinions, beliefs, emo-tions, sentiment, speculation and other privatestates in newspaper articles.
Private state is ageneral term that covers mental and emotionalstates, which cannot be directly observed or ver-ified (Quirk et al, 1985).
For example, we canobserve evidence of someone else being happy,but we cannot directly observe their happiness.In natural language, opinions, emotions and otherprivate states are expressed using subjective lan-guage (Banfield, 1982; Wiebe, 1994).Articles in the news are composed of a mix-ture of factual and subjective material.
Writersof editorials frequently include facts to supporttheir arguments, and news reports often mix seg-ments presenting objective facts with segmentspresenting opinions and verbal reactions (vanDijk, 1988).
However, natural language pro-cessing applications that retrieve or extract infor-mation from or that summarize or answer ques-tions about news and other discourse have fo-cused primarily on factual information and thuscould benefit from knowledge of subjective lan-guage.
Traditional information extraction and in-formation retrieval systems could learn to concen-trate on objectively presented factual information.Question answering systems could identify whenan answer is speculative rather than certain.
Inaddition, knowledge of how opinions and otherprivate states are realized in text would directlysupport new tasks, such as opinion-oriented in-formation extraction (Cardie et al, 2003).
Theability to extract opinions when they appear indocuments would benefit multi-document sum-marization systems seeking to summarize differ-ent opinions and perspectives, as well as multi-perspective question-answering systems trying toanswer opinion-based questions.The annotation scheme we present in this paperwas developed as part of a U.S. government-sponsored project (ARDA AQUAINT NRRC)1to investigate multiple perspectives in questionanswering (Wiebe et al, 2003).
We implementedthe scheme in GATE2, a General Architecturefor Text Engineering (Cunningham et al, 2002).General instructions for annotating opinions andspecific instructions for downloading and usingGATE to perform the annotations are available at1This work was performed in support of the NortheastRegional Research Center (NRRC) which is sponsored bythe Advanced Research and Development Activity in In-formation Technology (ARDA), a U.S. Government entitywhich sponsors and promotes research of import to the In-telligence Community which includes but is not limited tothe CIA, DIA, NSA, NIMA, and NRO.2GATE is freely available from the University ofSheffield at http://gate.ac.uk.http://www.cs.pitt.edu/ ?wiebe/pubs/ardasummer02.The annotated data will be available to U.S. gov-ernment contractors this summer.
We are workingto resolve copyright issues to make it available tothe wider research community.In developing this annotation scheme, we hadtwo goals.
The first was to develop a represen-tation for opinions and other private states thatwas built on work in linguistics and literary the-ory on subjectivity (please see (Banfield, 1982;Fludernik, 1993; Wiebe, 1994; Stein and Wright,1995) for references).
The study of subjectiv-ity in language focuses on how private states areexpressed linguistically in context.
Our secondgoal was to develop an annotation scheme thatwould be useful for corpus-based research onsubjective language and for the development ofapplications such as multi-perspective question-answering systems.
The annotation scheme thatresulted is more detailed and comprehensive thanprevious ones for subjective language.Our study of the annotations produced by theannotation scheme gives two important results.First, we find that trained annotators can consis-tently perform detailed opinion annotations withgood agreement (0.81 Kappa).
Second, the agree-ment results are better than in previous sentence-level annotation studies, suggesting that addingdetail can help the annotators perform more re-liably.In the sections that follow, we first review howopinions and other private states are expressed inlanguage (section 2) and give a brief overviewof previous work in subjectivity tagging (section3).
We then describe our annotation scheme forprivate state expressions (section 4) and give theresults of an annotation study (section 5).
Weconclude with a discussion of our findings fromthe annotation study and and future work (section??).
In the appendix, we give sample annotationsas well as a snapshot of the annotations in GATE.2 Expressing Private States in Text2.1 Private States, Speech Events, andExpressive Subjective ElementsThere are two main ways that private states areexpressed in language.
Private states may be ex-plicitly mentioned, or they may be expressed in-directly by the types of words and the style of lan-guage that a speaker or writer uses.
An exampleof an explicitly-mentioned private state is ?frus-trated?
in sentence (1).
(1) Western countries were left frus-trated and impotent after RobertMugabe formally declared that hehad overwhelmingly won Zimbabwe?spresidential election.Although most often verbs, it is interesting to notethat explicit mentions of private states may alsobe nouns, such as ?concern?
in ?international con-cern?
and ?will?
in ?will of the people.?
They mayeven be adjectives, such as ?fearful?
in ?fearfulpopulace.
?The second way that private states are generallyexpressed is indirectly using expressive subjectiveelements (Banfield, 1982).
For example, the pri-vate states in sentences (2) and (3) are expressedentirely by the words and the style of languagethat is used.
(2) The time has come, gentlemen, forSharon, the assassin, to realize that in-justice cannot last long.
(3) ?We foresaw electoral fraud but notdaylight robbery,?
Tsvangirai said.In (2), although the writer does not explicitlysay that he hates Sharon, his choice of wordsclearly demonstrates a negative attitude.
In sen-tence (3), describing the election as ?daylight rob-bery?
clearly reflects the anger being experiencedby the speaker, Tsvangirai.
As used in these sen-tences, the phrases ?The time has come,?
?gentle-men,?
?the assassin,?
?injustice cannot last long,??fraud,?
and ?daylight robbery?
are all expressivesubjective elements.
Expressive subjective ele-ments are used by people to express their frus-tration, anger, wonder, positive sentiment, mirth,etc., without explicitly stating that they are frus-trated, angry, etc.
Sarcasm and irony often in-volve expressive subjective elements.When looking for opinions and other privatestates in text, an annotator must consider speechevents as well as explicitly-mentioned privatestates.
In this work, we use speech event to referto any event of speaking or writing.
However, themere presence of a speech event does not indicatea private state.
Both sentences (3) above and (4)below contain speech events indicated by ?said.
?As mentioned previously, sentence (3) is opinion-ated, while in (4) the information is presented asfactual.
(4) Medical Department head DrHamid Saeed said the patient?s bloodhad been sent to the Institute forVirology in Johannesburg for analysis.For speech terms such as ?said,?
?added,?
?told,??announce,?
and ?report,?
an annotator deter-mines if there is a private state mainly by lookinginside the scope of the speech term for expressivesubjective elements.Occasionally, we also find private states thatare expressed by direct physical actions.
We callsuch actions private state actions.
Examples arebooing someone, sighing heavily, shaking onesfist angrily, waving ones hand dismissively, andfrowning.
?Applauding?
in sentence (5) is an ex-ample of a positive-evaluative private state action.
(5) As the long line of would-be votersmarched in, those near the front of thequeue began to spontaneously applaudthose who were far behind them.2.2 Nested SourcesAn important aspect of a private state or speechevent is its source.
The source of a speech eventis the speaker or writer.
The source of a privatestate is the experiencer of the private state, i.e.,the person whose opinion or emotion is being ex-pressed.
Obviously, the writer of an article is asource, because he wrote the sentences compos-ing the article, but the writer may also write aboutother people?s private states and speech events,leading to multiple sources in a single sentence.For example, each of the following sentences hastwo sources: the writer (because he wrote the sen-tences), and Sue (because she is the source of aspeech event in (6) and of private states in (7) and(8), namely thinking and being afraid).
(6) Sue said, ?The election was fair.?
(7) Sue thinks that the election was fair.
(8) Sue is afraid to go outside.Note, however, that we don?t really know whatSue says, thinks or feels.
All we know is what thewriter tells us.
Sentence (6), for example, doesnot directly present Sue?s speech event but ratherSue?s speech event according to the writer.
Thus,we have a natural nesting of sources in a sentence.The nesting of sources may be quite deep andcomplex.
For example, consider sentence (9).
(9) The Foreign Ministry said Thursdaythat it was ?surprised, to put it mildly?by the U.S. State Department?s criti-cism of Russia?s human rights recordand objected in particular to the ?odi-ous?
section on Chechnya.There are three sources in this sentence: thewriter, the Foreign Ministry, and the U.S. StateDepartment.
The writer is the source of the over-all sentence.
The remaining explicitly mentionedprivate states and speech events in (9) have thefollowing nested sources:said: (writer, Foreign Ministry)surprised, to put it mildly:(writer, Foreign Ministry, Foreign Ministry)criticism:(writer, Foreign Ministry, U.S. State Dept.
)objected: (writer, Foreign Ministry)Expressive subjective elements may also havenested sources.
In sentence (9), ?to put it mildly?and ?odious?
are expressive subjective elements,both with nested source (writer, Foreign Min-istry).
We might expect that an expressive subjec-tive element always has the same nested sourceas the immediately dominating private state orspeech term.
Although this is the case for ?odi-ous?
in (9) (the nested source of ?odious?
and?objected?
is the same), it is not the same for ?big-ger than Jesus?
in (10):(10) ?It is heresy,?
said Cao.
?The?Shouters?
claim they are bigger thanJesus.
?The nested source of the subjectivity expressedby ?bigger than Jesus?
is Cao, while the nestedsource of ?claim?
is (writer, Cao, Shouters).33(10) is an example of a de re rather than de dicto propo-sitional attitude report (Rapaport, 1986).3 Previous Work on SubjectivityTaggingIn previous work (Wiebe et al, 1999), a corpus ofsentences from the Wall Street Journal TreebankCorpus (Marcus et al, 1993) was manually anno-tated with subjectivity classifications by multiplejudges.
The judges were instructed to classify asentence as subjective if it contained any signif-icant expressions of subjectivity, attributed to ei-ther the writer or someone mentioned in the text,and to classify the sentence as objective, other-wise.
The judges rated the certainty of their an-swers on a scale from 0 to 3.Agreement in the study was summarized interms of Cohen?s Kappa (   ) (Cohen, 1960),which compares the total probability of agree-ment to that expected if the taggers?
classifica-tions were statistically independent (i.e., ?chanceagreement?).
After two rounds of tagging bythree judges, an average pairwise   value of 0.69was achieved on a test set.
On average, the judgesrated 15% of the sentences as very uncertain (rat-ing 0).
When these sentences are removed, theaverage pairwise   value is 0.79.
When sentenceswith uncertainty judgment 0 or 1 are removed (onaverage 30% of the sentences), the average pair-wise   is 0.88.4 An Annotation Scheme for PrivateStatesThe annotation scheme described in this sectionis more detailed and comprehensive the previ-ous ones for subjective language.
In (Wiebe etal., 1999), summary subjective/objective judg-ments were performed at the sentence level.
Forthis work, annotators are asked to mark withineach sentence the word spans that indicate speechevents or that are expressions of private states.For every span that an annotator marks, there area number of attributes the annotator may set tocharacterize the annotation.The annotation scheme has two main com-ponents.
The first is an annotation type forexplicitly-mentioned private states and speechevents.
The second is an annotation type for ex-pressive subjective elements.
Table 1 lists the at-tributes that may be assigned to these two typesof annotations.
In addition, there is an annotationExplicit private states/speech eventsnested-sourceonlyfactive: yes, nooverall-strength: low, medium, high, extremeon-strength: neutral, low, medium, high, extremeattitude-type: positive, negative, both (exploratory)attitude-toward (exploratory)is-implicitminorExpressive subjective elementsnested-sourcestrength: low, medium, high, extremeattitude-type: positive, negative, other (exploratory)Table 1: Attributes for the two main annotationtypes.
For attributes that take on one of a fixed setof values, the set of possible values are given.type, agent, that annotators may use to mark thenoun phrase (if one exists) of the source of a pri-vate state or speech event.4.1 Explicitly-mentioned Private State andSpeech Event AnnotationsAn important part of the annotation scheme isrepresented by the onlyfactive attribute.
This at-tribute is marked on every private state and speechevent annotation.
The onlyfactive attribute is usedto indicate whether the source of the private stateor speech event is indeed expressing an emo-tion, opinion or other private state.
By defini-tion, any expression that is an explicit private state(e.g., ?think?, ?believe,?
?hope,?
?want?)
or a pri-vate state mixed with speech (e.g., ?berate,?
?ob-ject,?
?praise?)
is onlyfactive=no.
On the otherhand, neutral speech events (e.g., ?said,?
?added,??told?)
may be either onlyfactive=yes or onlyfac-tive=no, depending on their contents.
For ex-ample, the annotation for ?said?
in sentence (3)would be marked onlyfactive=no, but the annota-tion for ?said?
in sentence (4) would be markedonlyfactive=yes (sentences in section 2).Note that even if onlyfactive=no, the sentencemay express something the nested source believesis factual.
Consider the sentence ?John criti-cized Mary for smoking.?
John expresses a privatestate (his negative evaluation of Mary?s smoking).However, this does not mean that John does notbelieve that Mary smokes.Like the onlyfactive attribute, the nested-sourceattribute is included on every private state andspeech event annotation.
The nested source (i.e.,(writer, Foreign Ministry, U.S. State Dept.))
istyped in by the annotator.When an annotation is marked onlyfactive=no,additional attributes are used to characterize theprivate state.
The overall-strength attribute isused to indicate the overall strength of the pri-vate state (considering the explicit private stateor speech event phrase as well as everything in-side its scope).
It?s value may range from lowto extreme.
The on-strength attribute is used tomeasure the contribution made specifically by theexplicit private state or speech event phrase.
Forexample, the on-strength of ?said?
is typicallyneutral, the on-strength of ?criticize?
is typicallymedium, and the on-strength of ?vehemently de-nied?
is typically high or extreme.
(As for all as-pects of this annotation scheme, the annotatorsare asked to make these judgments in context.
)A speech event that is onlyfactive=yes has on-strength=neutral and no overall-strength.
Thus,there is no need to include the overall-strengthand on-strength attributes for onlyfactive=yes an-notations.4.1.1 Implicit Speech Event AnnotationsImplicit speech events posed a problem whenwe developed the annotation scheme.
Implicitspeech events are speech events in the discoursefor which there is no explicit speech event phrase,and thus no obvious place to attach the anno-tation.
For example, most of the writer?s sen-tences do not include a phrase such as ?I say.
?Also, direct quotes are not always accompaniedby discourse parentheticals (such as ?, she said?
).Our solution was to add the is-implicit attribute tothe annotation type for private states and speechevents, which may then be used to mark implicitspeech event annotations.4.1.2 Minor Private States and SpeechEventsDepending on its goals, an application mayneed to identify all private state and speech eventexpressions in a document, or it may want to findonly those opinions and other private states thatare significant and real in the discourse.
By ?sig-nificant?, we mean that a significant portion of thecontents of the private state or speech event aregiven within the sentence where the annotationis marked.
By ?real?, we mean that the privatestate or speech event is presented as an existingevent within the domain of discourse, e.g., it isnot hypothetical.
We use the term minor for pri-vate states and speech events that are not signif-icant or not real.
Annotators mark minor privatestate and speech event annotations by includingthe minor attribute.The following sentences all contain one ormore minor private states or speech events (high-lighted in bold).
(11) Such wishful thinking risks mak-ing the US an accomplice in the de-struction of human rights.
(not signif-icant)(12) If the Europeans wish to influenceIsrael in the political arena... (in a con-ditional, so not real)(13) ?And we are seeking a declara-tion that the British government de-mands that Abbasi should not face trialin a military tribunal with the deathpenalty.?
(not real, i.e., the declarationof the demand is just being sought)(14) The official did not say how manyprisoners were on the flight.
(not realbecause the saying event did not occur)(15) No one who has ever studied realistpolitical science will find this surpris-ing.
(not real since a specific ?surprise?state is not referred to; note that thesubject noun phrase is attributive ratherthan referential (Donnellan, 1966))4.2 Expressive Subjective ElementAnnotationsAs with private state/speech event annotations,the nested-source attribute is included on everyexpressive subjective element annotation.
In ad-dition to marking the source of an expression, thenested-source is also functioning as a link.
Withina sentence, the nested-source chains together allthe pieces that together indicate the overall pri-vate state of a particular source.In addition to nested-source, the strength at-tribute is used to characterize expressive subjec-tive element annotations.
The strength of an ex-pressive subjective element may range from lowto extreme (see Table 1).4.3 Exploratory AttributesWe are exploring additional attributes that allowan annotator to further characterize the type ofattitude being expressed by a private state.
Anannotator may use the attitude-type attribute tomark an onlyfactive=no private state/speech eventannotation or an expressive subjective elementannotation as positive or negative.
An attitude-toward attribute may also be included on privatestate/speech event annotations to indicate the par-ticular target of an evaluation, emotion, etc.5 Annotation StudyThe data in our study consists of English-language versions of foreign news documentsfrom FBIS, the U.S. Foreign Broadcast Informa-tion Service.
The data is from a variety of publi-cations and countries.
To date, 252 articles havebeen annotated with the scheme described in sec-tion 4.To measure agreement on various aspects of theannotation scheme, three annotators (A, M, andS) independently annotated 13 documents with atotal of 210 sentences.
None of the annotators areauthors of this paper.
The articles are from a vari-ety of topics and were selected so that 1/3 of thesentences are from news articles reporting on ob-jective topics (objective articles), 1/3 of the sen-tences are from news articles reporting on opin-ionated topics (?hot-topic?
articles), and 1/3 ofthe sentences are from editorials.In the instructions to the annotators, we askedthem to rate the annotation difficulty of each arti-cle on a scale from 1 to 3, with 1 being the eas-iest and 3 being the most difficult.
The annota-tors were not told which articles were objectiveor which articles were editorials, only that theywere being given a variety of different articles toannotate.We hypothesized that the editorials would bethe hardest to annotate and that the objective ar-ticles would be the easiest.
The ratings that theannotators assigned to the articles support this hy-pothesis.
The annotators rated an average of 44%of the articles in the study as easy (rating 1) and26% as difficult (rating 3).
But, they rated an av-erage of 73% of the objective articles as easy, and89% of the editorials as difficult.It makes intuitive sense that ?hot-topic?
articleswould be more difficult to annotate than objectivearticles and that editorials would be more difficultstill.
Editorials and ?hot-topic?
articles containmany more expressions of private states, requir-ing an annotator to make more judgments thanthey would for objective articles.5.1 Agreement for Expressive SubjectiveElement AnnotationsFor annotations that involve marking spans oftext, such as expressive subjective element an-notations, it is not unusual for two annotators toidentify the same expression in the text, but todiffer in how they mark the boundaries.4 Forexample, both annotators A and M saw expres-sive subjectivity in the phrase, ?such a disadvan-tageous situation.?
But, while A marked the entirephrase as a single expressive subjective element,M marked the individual words, ?such?
and ?dis-advantageous.?
Because the annotators will iden-tify a different number of annotations, as well asdifferent (but hopefully strongly overlapping) setsof expressions, we need an agreement metric thatcan measure agreement between sets of objects.We use the   metric to measure agreementfor expressive subjective elements (and later forprivate state/speech event annotations).  is a directional measure of agreement.
Letand  be the sets of spans annotated by anno-tators   and 	 .
We compute the agreement of 	 to  as:  	This measure of agreement corresponds to the no-tion of precision and recall as used to evaluate, forexample, named entity recognition.
The   	metric corresponds to the recall if   is the gold-standard and 	 the system, and to precision, if theyare reversed.In the 210 sentences in the annotation study, theannotators A, M, and S respectively marked 311,352 and 249 expressive subjective elements.
Ta-ble 2 shows the pairwise agreement for these setsof annotations.
For example, M agrees with 76%of the expressive subjective elements marked by4In the coding instructions, we did not attempt to definerules to try to enforce boundary agreement.mother of terrorismif the world has to rid itself from this menace, the perpetrators across the border had to be dealt with firmlyindulging in blood-shed and their lunaticismultimately the demon they have reared will eat up their own vitalsTable 3: Extreme strength expressive subjective elements     	    averageA M 0.76 0.72A S 0.68 0.81M S 0.59 0.740.72Table 2: Inter-annotator Agreement: Expressivesubjective elementsA, and A agrees with 72% of the expressivesubjective elements marked by M. The averageagreement in Table 2 is the arithmetic mean of allsix   .We hypothesized that the stronger the expres-sion of subjectivity, the more likely the annota-tors are to agree.
To test this hypothesis, we mea-sure agreement for the expressive subjective ele-ments rated with a strength of medium or higherby at least one annotator.
This excludes on av-erage 29% of the expressive subjective elements.The average pairwise agreement rises to 0.80.When measuring agreement for the expressivesubjective elements rated high or extreme, this ex-cludes an average 65% of expressive subjectiveelements, and the average pairwise agreement in-creases to 0.88.
Thus, annotators are more likelyto agree when the expression of subjectivity isstrong.
Table 3 gives examples of expressive sub-jective elements that at least one annotator ratedas extreme.5.2 Agreement for Private State/SpeechEvent AnnotationsFor private state and speech event annotations, weagain use   to measure agreement between thesets of expressions identified by each annotator.The three annotators, A, M, and S, respectivelymarked 338, 285, and 315 explicit expressions ofprivate states and speech events.
Implicit speechevents for the writer of course are excluded.
Table4 shows the pairwise agreement for these sets ofannotations.The average pairwise agreement for explicitprivate state and speech event expressions is 0.82,     	    averageA M 0.75 0.91A S 0.80 0.85M S 0.86 0.750.82Table 4: Inter-annotator Agreement: Explicitly-mentioned private states and speech eventswhich indicates that they are easier to annotatethan expressive subjective elements.5.3 Agreement for AttributesIn this section, we focus on the annotators?
agree-ment for judgments that reflect whether or notan opinion, emotion, sentiment, speculation, orother private state is being expressed.
We con-sider these judgments to be at the core of the an-notation scheme.
Two attributes, onlyfactive andon-strength, carry information about whether aprivate state is being expressed.For onlyfactive judgments, we measure pair-wise agreement between annotators for the setof private state and speech event annotations thatboth annotators identified.
Because we are nowmeasuring agreement over the same set of objectsfor each annotator, we use Kappa (   ) to capturehow well the annotators agree.Table 5 shows the contingency table for the on-lyfactive judgments made by annotators A and M.The Kappa scores for all annotator pairs are givenin Table 7.
For their onlyfactive judgments, i.e.,whether or not an opinion or other private stateis being expressed, the annotators have an aver-age pairwise Kappa of 0.81.
Under Krippendorf?sscale (Krippendorf, 1980), this allows for definiteconclusions.With many judgments that characterize naturallanguage, one would expect that there are clearcases as well as borderline cases, which would bemore difficult to judge.
The agreement study in-dicates that this is certainly true for private states.In terms of our annotations, we define an explicitprivate state or speech event to be borderline-        fiffflffififffiTable 5: A & M: Agreement for onlyfactive judg-ments           flfifi"!Table 6: A & M: Agreement for onlyfactive judg-ments, borderline-onlyfactive cases removedonlyfactive if 1) at least one annotator marked theexpression onlyfactive=no, and 2) neither anno-tator characterized an overall-strength as beinggreater than low.
In Table 6 we give the contin-gency table for the onlyfactive judgments madeby annotators A and M, excluding borderline-onlyfactive expressions.
Note that removing suchexpressions removes agreements as well as dis-agreements.
Borderline-onlyfactive expressionson average comprise only 10% of the privatestate/speech event annotations.
When they areremoved, the average pairwise Kappa climbs to0.89.In addition to the onlyfactive judgment, us-ing on-strength we can measure if the annota-tors agree as to whether an explicit private stateor speech event phrase by itself expresses a pri-vate state.
Specifically, we measure if the an-notators agree that an expression is neutral, i.e.,does not indicate a private state.
Recall that only-factive=yes annotations are on-strength=neutral.Implicit annotations are excluded when measur-ing on-strength agreement.The pairwise agreement results for the anno-tators?
on-strength neutral judgments are givenin Table 8.
For on-strength neutral judgments,annotators have an average pairwise Kappa ofAll Expressions Borderline Removed# agree # agree % removedA & M 0.84 0.91 0.94 0.96 10A & S 0.84 0.92 0.90 0.95 8M & S 0.74 0.87 0.84 0.92 12Table 7: Pairwise Kappa scores and overall per-cent agreement for onlyfactive judgmentsAll Expressions Borderline Removed# agree # agree % removedA & M 0.81 0.91 0.93 0.97 22A & S 0.74 0.87 0.92 0.96 17M & S 0.67 0.83 0.90 0.95 18Table 8: Pairwise Kappa scores and overall per-cent agreement for on-strength neutral judgments0.74.
As with the onlyfactive judgments, thereare clearly borderline cases.
We define an expres-sion to be borderline-low if 1) at least one anno-tator marked the expression onlyfactive=no, and2) neither annotator characterized an on-strengthas being greater than low.
When borderline-lowexpressions are removed, the pairwise Kappa in-creases to 0.92.5.4 Agreement for SentencesTo compare our results to those of earlier workthat evaluated the agreement of sentence-levelsubjectivity annotations (Wiebe et al, 1999), wedefine sentence-level classifications in terms ofour lower-level annotations as follows.
First, weexclude explicit private state/speech event expres-sions that the annotators agree are minor.
Then, ifan annotator marked one or more onlyfactive=noexpressions in the sentence, we consider the an-notator to have judged the sentence to be subjec-tive.
Otherwise, we consider the annotator to havejudged the sentence to be objective.The pairwise agreement results for these de-rived sentence-level annotations are given in Ta-ble 9.
The average pairwise Kappa for sentence-level agreement is 0.77, 8 points higher than thesentence-level agreement reported in (Wiebe etal., 1999).
Our new results suggest that addingdetail to the annotation task can can help annota-tors perform more reliably.
Note that the agree-ment is lower than that for onlyfactive judgments(Table 7) because explicit private-state and speechevent expressions upon which the annotators didnot agree are now included.As with the onlyfactive and on-strength neutraljudgments, we again test agreement when border-line cases are removed.
We define a sentence tobe borderline if 1) at least one annotator markedat least one expression onlyfactive=no, and 2)neither annotator marked an overall-strength at-tribute as being greater than low.
When border-All Sentences Borderline Removed# agree # agree % removedA & M 0.75 0.89 0.87 0.95 11A & S 0.84 0.94 0.92 0.97 8M & S 0.72 0.88 0.83 0.93 13Table 9: Pairwise Kappa scores and overall per-cent agreement for derived sentence-level judg-mentsline sentences are removed, the average Kappa in-creases to 0.89.6 ConclusionsIn this paper, we presented a detailed scheme forthe annotation of opinions and other private statesin the news and other discourse.
For the aspectsof this annotation scheme that indicate whethera private state is expressed, our three annotatorshave strong pairwise agreement, as measured byCohen?s Kappa.One interesting area explored in this paper isthe effect of borderline cases on inter-annotatoragreement.
We created a number of objec-tive definitions of borderline cases, based on thestrengths indicated by the annotators, and foundthat removing these borderline cases always re-sults in high agreement values.
This shows thatthe annotators agree strongly about which are theclear cases of subjectivity.We have also shown that lower-level subjectiv-ity annotations, such as those presented in this pa-per, may be used to produce higher-level subjec-tive sentence annotations.
In current research, weare using these higher-level annotations to evalu-ate subjective sentence classifiers, which we hopewill be useful for enhancing natural language pro-cessing applications such as information extrac-tion, summarization, and question answering sys-tems.There are characteristics of private state expres-sions not yet included in our scheme that wouldbe useful for NLP applications.
We believe thescheme is extendable, and hope that other groupswill build on it.ReferencesA.
Banfield.
1982.
Unspeakable Sentences.
Rout-ledge and Kegan Paul, Boston.C.
Cardie, J. Wiebe, T. Wilson, and D. Litman.
2003.Combining low-level and summary representationsof opinions for multi-perspective question answer-ing.
In Working Notes - New Directions in QuestionAnswering (AAAI Spring Symposium Series).J.
Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Meas.,20:37?46.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE: Aframework and graphical development environmentfor robust nlp tools and applications.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics.Keith Donnellan.
1966.
Reference and definite de-scriptions.
Philosophical Review, 60:281?304.M.
Fludernik.
1993.
The Fictions of Language andthe Languages of Fiction.
Routledge, London.K.
Krippendorf.
1980.
Content Analysis: An In-troduction to its Methodology.
Sage Publications,Beverly Hills.M.
Marcus, Santorini, B., and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The penn treebank.
Computational Linguis-tics, 19(2):313?330.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.1985.
A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.William Rapaport.
1986.
Logical foundations for be-lief representation.
Cognitive Science, 10:371?422.D.
Stein and S. Wright, editors.
1995.
Subjectivityand Subjectivisation.
Cambridge University Press,Cambridge.T.A.
van Dijk.
1988.
News as Discourse.
LawrenceErlbaum, Hillsdale, NJ.J.
Wiebe, R. Bruce, and T. O?Hara.
1999.
Develop-ment and use of a gold standard data set for subjec-tivity classifications.
In Proc.
37th Annual Meetingof the Assoc.
for Computational Linguistics (ACL-99), pages 246?253, University of Maryland, June.ACL.J.
Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,B.
Fraser, D. Litman, D. Pierce, E. Riloff, T. Wil-son, D. Day, and M. Maybury.
2003.
Recogniz-ing and organizing opinions expressed in the worldpress.
In Working Notes - New Directions in Ques-tion Answering (AAAI Spring Symposium Series).J.
Wiebe.
1994.
Tracking point of view in narrative.Computational Linguistics, 20(2):233?287.Figure 1: Example of annotations in GATEA Sample AnnotationsThe following is the first sentence from an articleabout the 2002 presidential election in Zimbabwe.The article appeared on March 15, 2002 in thenewspaper, Dawn.Western countries were left frustratedand impotent after Robert Mugabeformally declared that he had over-whelmingly won Zimbabwe?s presiden-tial election.There are three private state/speech event an-notations and one expressive subjective elementannotation in this sentence.
The annotations,including their attributes, are listed below:Speech Event: implicitnested-source = (writer)onlyfactive = yesPrivate State: were left frustratednested-source = (writer, Western countries)onlyfactive = nooverall-strength = mediumon-strength = mediumSpeech Event: formally declared:nested-source = (writer, Mugabe)onlyfactive = nooverall-strength = mediumon-strength = neutralExpressive Subjective Element: overwhelm-ingly:nested-source = (writer, Mugabe)strength = mediumFigure 1 shows how these annotationsappear inside the GATE annotation tool.
Ad-ditional annotated examples can be foundwith the on-line GATE annotation instruc-tions, http://www.cs.pitt.edu/mpqa/opinion-annotations/gate-instructions.
