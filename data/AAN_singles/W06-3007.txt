Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 49?56,New York City, NY, USA.
June 2006. c?2006 Association for Computational LinguisticsUser-Centered Evaluation of Interactive Question Answering SystemsDiane Kelly1, Paul B. Kantor2, Emile L. Morse3, Jean Scholtz3 & Ying Sun2University of North Carolina Rutgers University National Institute of Standards & TechnologyChapel Hill, NC 27599 New Brunswick, NJ 08901 Gaithersburg, MDdianek@email.unc.edu kantor@scils.rutgers.eduysun@scils.rutgers.eduemile.morse@nist.govjean.scholtz@nist.govAbstractWe describe a large-scale evaluation offour interactive question answering sys-tem with real users.
The purpose of theevaluation was to develop evaluationmethods and metrics for interactive QAsystems.
We present our evaluationmethod as a case study, and discuss thedesign and administration of the evalua-tion components and the effectiveness ofseveral evaluation techniques with respectto their validity and discriminatory power.Our goal is to provide a roadmap to othersfor conducting evaluations of their ownsystems, and to put forward a researchagenda for interactive QA evaluation.1 IntroductionThere is substantial literature on the evaluationof systems in the context of real users and/or real-istic problems.
The overall design issues were pre-sented by Tague-Sutcliffe (1992) in a classic paper.Other authors who have contributed substantiallyto the discussion include Hersh and Over (2001).The basic change in viewpoint required, in thestudy of interactive systems with real users, is thatone cannot follow the Cranfield Model, in whichspecific items (whether documents, or snippets ofinformation) are known to be ?good,?
so thatmeasures can be based on the count of such items(e.g., precision and recall).
Instead, one must de-velop methods and metrics that are sensitive toindividual users, tasks and contexts, and robustenough to allow for valid and reliable comparisonsacross systems.Most evaluations of QA systems have been con-ducted as part of the QA Track at TREC.
They aresystem-oriented rather than user-oriented, with afocus on evaluating techniques for answer extrac-tion, rather than interaction and use (Voorhees,2003).
In this paper, we consider an interactivesystem to be a system that supports at least oneexchange between the user and system.
Further, aninteractive system is a system that allows the userfull or partial control over content and action.While factoid QA plays a role in analytical QA,analytical QA also includes other activities, such ascomparison and synthesis, and demands muchricher interactions between the system, the infor-mation, and the user.
Thus different evaluationmeasures are needed for analytical QA systemsthan for those supporting factoid QA.
Emergingwork in the QA community is addressing user in-teraction with factoid-based QA systems and othermore complex QA tasks (Diekema, et al, 2004;Liddy, et al, 2004), but developing robust evalua-tion methods and metrics for interactive, analyticalQA systems in realistic settings with target usersand tasks remains an unresolved research problem.We describe a large-scale evaluation of four in-teractive QA systems with target users, completingtarget tasks.
Here we present the evaluationmethod and design decisions for each aspect of thestudy as a case study.
The goal of this paper is toidentify key issues in the design of evaluations ofinteractive QA systems and help others constructtheir own evaluations.
While systems participatingin this evaluation received individual feedbackabout the performances of their systems, the pur-pose of the project was not to compare a series ofsystems and declare a ?winner.?
In this paper wefocus on the method and results of that method,rather than the performance of any one system.In section 2, we describe our evaluation ap-proach, the evaluation environment, systems stud-ied, subjects, corpus and scenarios, and49experimental design.
In Section 3 we report ourinstruments and other data collection techniques.In Section 4 we discuss our evaluation methods,and present key findings regarding the effective-ness of the various evaluation techniques.
Weconclude by considering future research directionsfor interactive QA evaluation.2 Evaluation ApproachThis evaluation was conducted as a two-weekworkshop.
The workshop mode gives analysts anopportunity to fully interact with all four systems,complete time-intensive tasks similar to their nor-mal work tasks and lets us evaluate a range ofmethods and metrics.The researchers spent approximately 3 weeksonsite preparing and administering the workshop.Intelligence analysts, the study participants, spent 2weeks onsite.
The evaluation employed 8 analysts,8 scenarios in the chemical/biological WMD do-main, and 4 systems ?
3 QA systems and aGoogle1 baseline system.
Each analyst used eachsystem to analyze 2 scenarios and wrote a pseudo-report containing enough structure and content forit to be judged by peer analysts.During the planning stage, we generated hy-potheses about interactive QA systems to guidedevelopment of methods and metrics for measuringsystem effectiveness.
Fifteen hypotheses were se-lected, of which 13 were operationalized.
Examplehypotheses are presented in Table 1.A good interactive QA system should ?1 Support information gathering with lower cognitiveworkload2 Assist analysts in exploring more paths/hypotheses3 Enable analysts to produce higher quality reports4 Provide useful suggestions to the analyst5 Provide analysts with more good surprises than badTable 1: Example hypotheses2.1 Evaluation EnvironmentThe experiment was done at the Pacific North-west National Laboratory (PNNL) in Richland,WA.
We used one room with support servers, fourrooms with two copies of one system in each and a1 Any mention of commercial products or companies is forinformation only and does not imply recommendation or en-dorsement by NIST.conference room seating 20, for general meetings,focus group discussions, meetings among observ-ers, meetings among developers, etc.2.2 QA SystemsThree end-to-end interactive QA systems and aGoogle baseline were used.
System developerswere assigned a room, and installed their systemson two workstations in the room.Before analysts used each system, they weretrained by the system developer.
Training includeda skills check test, and free experimentation.Methods of training included: a script with traineesreproducing steps on their own workstations, aslide presentation with scripted activities, a presen-tation from a printed manual, and a presentation,orally and with participation, guided by a checklist.The workstations used during the experimentwere Dell workstations configured with WindowsXP Professional with updated OS, Intel PentiumIV processor 3.40 Ghz 512 K/800 Mhz, 2 GBDDR 400 SD RAM, 120 GB SATA 7200 RPMhard drive with Data Burst Cache, video card,floppy drive, 16 DVD ROM, and 48/32/48 CDRW.2.3 SubjectsAnalysts who participated in the study werevolunteers serving their yearly two-week servicerequirement as U.S.
Naval Reservists.
Analystswere recruited by email solicitation of a large poolof potential volunteers.
The first 8 positive re-sponders were inducted into the study.We collected the following data from analysts:age, education level, job type, number of years inthe military, number of years conducting analysiswork, computer usage, computer expertise, andexperience with querying systems.
Data about ana-lysts characterizes them on several dimensions.With small samples, this step is critical, but it isalso important in studies with larger samples.
Thistype of data lets us describe participants in pub-lished reports and ask whether individual differ-ences affect study results.
For instance, one mightlook for a relationship between computer experi-ence and performance.2.4 ScenariosScenarios were developed by a team of analystsfrom the Air Force Rome Research Lab, and were50vetted to produce 14 appropriate to the collectionand target participants.
We found after the first twoscenarios that while scenario descriptions weresufficient in describing the content of the task, im-portant information regarding context of the de-scription and the format of the report, such ascustomer and length, was lacking.
This omissiongenerated ambiguity in report creation, and causedsome uncertainty for the analysts on how to pro-ceed with the task.
Thereafter, analysts met as agroup in the conference room to agree on addi-tional specifications for each scenario when it wasassigned.
In addition to this information, the pro-ject director and one analyst worked together todesign a template for the report, which establisheda uniform report structure, and specified formattingguidelines such as headings and length.
An exam-ple scenario is displayed in Figure 1.Scenario B: [country] Chemical Weapons ProgramBefore a U.S. military presence is reestablished in[country], a current, thorough study of [country]chemical weapons program must be developed.
Yourtask is to produce a report for the Secretary of theUnited States Navy regarding general information on[country] and the production of chemical weapons.Provide information regarding [country] access tochemical weapons research, their current capabilitiesto use and deploy chemical weapons, reported stock-piles, potential development for the next few years,any assistance they have received for their chemicalweapons program, and the impact that this informa-tion will have on the United States.
Please add anyother related information to your report.Customer: Secretary of U.S. NavyRole: Country desk ?
[country]What they want: General report on [country] andCW productionFigure 1.
Example Scenario2.5 CorpusUsing the live Web would make it impossible toreplicate the experiment, so we started with theAQUAINT corpus from the Center for Non-Proliferation Studies (CNS).
The CNS data con-sists of the January 2004 distribution of the Eye onProliferation CD, which has been "disaggregated"by CNS into about 40,000 documents.
Once theinitial 14 scenarios were delivered to NIST, theywere characterized with respect to how well theCNS corpus could support them.
Several scenarioshad less than 100 documents in the CNS corpus, soto increase the number of documents available foreach scenario we supplemented the corpus by min-ing the Web.Documents were collected from the Web bysemi-automated querying of Google and manualretrieval of the documents listed in the results.
Afew unusually large and useless items, like CD im-ages, pornography and word lists, were deleted.The approximate counts of different kinds of files,as determined by their file extensions, are summa-rized in Table 2.Source All Files Documents ImagesCNS 40192 39932 945Other 261590 48035 188729Table 2: Characteristics of corpus in bytes2.6 Experimental DesignThe evaluation workshop included four, two-dayblocks.
In each block, a pair of analysts was as-signed to each room, and a single observer wasassigned to the pair of analysts.
Analysts used thetwo machines in each room to work independentlyduring the block.
After each block, analysts andobservers rotated to different system rooms, so thatanalysts were paired together only once and ob-servers observed different analysts during eachblock.
The goal in using designed experiments isto minimize the second-order interactions, so thatestimates of the main effects can be obtained froma much smaller set of observations than is requiredfor a full factorial design.
For instance, one mightimagine potential interaction effects of system andscenario (some systems might be better for certainscenarios); system and analysts (some analystsmight adapt more quickly to a system); and analystand scenario (some analysts might be more expertfor certain scenarios).
To control these potentialinteractions, we used a modified Greco-Latin 4x4design.This design ensured that each analyst was ob-served by each of the four observers, and used eachof the four systems.
This design also ensured thateach system was, for some analyst, the first, sec-ond, third or last to be encountered, and that noanalyst did the same pair of scenarios twice.
Ana-lyst pairings were unique across blocks.
Followingstandard practice, analysts and scenarios were ran-51domly assigned codenames (e.g.
A1, and ScenarioA), and systems were randomly assigned to therows of Table 3.
Although observers were simplyrotated across the system rows, the assignment ofhuman individuals to code number was random.Dates Day 1 2 Day 3 4  Day 5 6 Day 7 8Scenarios A, B C, D E, F G, HO1 O2 O3 O4A1 A2 A3 A4System 1A5 A6 A7 A8O2 O1 O4 O3A4 A3 A2 A1System 2A7 A8 A5 A6O3 O4 O1 O2A2 A1 A4 A3System 3A8 A7 A6 A5O4 O3 O2 O1A3 A4 A1 A2System 4A6 A5 A8 A7Table 3.
Experimental design (O=observer;A=analyst)3 Data CollectionSystem logs and Glass Box (Hampson & Crow-ley, 2005) were the core logging methods provid-ing process data.
Post-scenario, post-session, post-system and cognitive workload questionnaires,interviews, focus groups, and other user-centeredmethods were applied to understand more aboutanalysts?
experiences and attitudes.
Finally, cross-evaluation (Sun & Kantor, 2006) was the primarymethod for evaluating reports produced.Each experimental block had two sessions, cor-responding to the two unique scenarios.
Methodsand instruments described below were either ad-ministered throughout the experimental block (e.g.,observation and logging); at the end of the session,in which case the analyst would complete two ofthese instruments during the block (e.g., a post-session questionnaire for each scenario); or once,at the end of the experimental block (e.g., a post-system questionnaire).
We added several data col-lection efforts at the end of the workshop to under-stand more about analysts?
overall experiences andto learn more about the study method.3.1 ObservationThroughout the experimental sessions, trainedobservers monitored analysts?
interactions withsystems.
Observers were stationed behind ana-lysts, to be minimally intrusive and to allow for anoptimal viewing position.
Observers used an Ob-servation Worksheet to record activities and behav-iors that were expected to be indicative of analysts?level of comfort, and feelings of satisfaction ordissatisfaction.
Observers noted analysts?
apparentpatterns of activities.
Finally, observers used theWorksheet to note behaviors about which to fol-low-up during subsequent session interviews.3.2 Spontaneous Self-ReportsDuring the evaluation, we were interested in ob-taining feedback from analyst in situ.
Analystswere asked to report their experiences spontane-ously during the experimental session in threeways: commenting into lapel microphones, usingthe ?SmiFro Console?
(described more fully be-low), and completing a three-item online StatusQuestionnaire at 30 minute intervals.The SmiFro Console provided analysts with apersistent tool for commenting on their experiencesusing the system.
It was rendered in a small displaywindow, and analysts were asked to leave thiswindow open on their desktops at all times.
It dis-played smile and frown faces, which analysts couldselect using radio buttons.
The Console also dis-played a text box, in which analysts could writeadditional comments.
The goal in using smiles andfrowns was to create a simple, recognizable, andquick way for analysts to provide feedback.The SmiFro Console contained links to theStatus Questionnaires which were designed to so-licit analysts?
opinions and feedback about theprogress of their work during the session.
Eachquestionnaire contained the same three questions,which were worded differently to reflect differentmoments in time.
There were four Status Ques-tionnaires, corresponding to 30-minute intervalsduring the session:  30, 60, 90, 120 minutes.3.3 NASA TLX QuestionnaireAfter completing each scenario, analysts com-pleted the NASA Task Load Index (TLX)2.
TheNASA TLX is a standard instrument used in avia-tion research to assess pilot workload and was usedin this study to assess analysts?
subjective cogni-tive workloads while completing each scenario.The NASA TLX assesses six factors:2 http://www.nrl.navy.mil/aic/ide/NASATLX.php521.
Mental demand: whether this searching taskaffects a user's attention, brain, and focus.2.
Physical demand: whether this searchingtask affects a user's health, makes a usertired, etc.3.
Temporal demand: whether this searchingtask takes a lot of time that can't be af-forded.4.
Performance: whether this searching task isheavy or light in terms of workload.5.
Frustration: whether this searching taskmakes a user unhappy or frustrated.6.
Effort: whether a user has spent a lot of ef-fort on this searching task.3.4 Post-Scenario QuestionnaireFollowing the NASA TLX, analysts completedthe six-item Scenario Questionnaire.
This Ques-tionnaire was used to assess dimensions of scenar-ios, such as their realism and difficulty.3.5 Post-Session QuestionnaireAfter completing the Post-Scenario Question-naire, analysts completed the fifteen-item Post-Session Questionnaire.
This Questionnaire wasused to assess analysts?
experiences using this par-ticular system to prepare a pseudo-report.
Eachquestion was mapped to one or more of our re-search hypotheses.
Observers examined these re-sponses and used them to construct follow-upquestions for subsequent Post-Session Interviews.3.6 Post-Session InterviewObservers used a Post-Session Interview Sched-ule to privately interview each analyst.
The Inter-view Schedule contained instructions to theobserver for conducting the interview, and alsoprovided a list of seven open-ended questions.
Oneof these questions required the observer to usenotes from the Observation Worksheet, while twocalled for the observer to use analysts?
responses toPost-Session Questionnaire items.3.7 NASA TLX Weighting InstrumentAfter using the system to complete two scenar-ios, analysts completed the NASA-TLX Weightinginstrument.
The NASA-TLX Weighting instru-ment was used to elicit a ranking from analystsabout the factors that were probed with the NASA-TLX instrument.
There are 15 pair-wise compari-sons of 6 factors and analysts were forced tochoose one in each pair as more important.
A sim-ple sum of ?wins?
is used to assign a weight toeach dimension, for the specific analyst.3.8 Post-System QuestionnaireAfter the NASA-TLX Weighting instrument,analysts completed a thirty-three item Post-SystemQuestionnaire, to assess their experiences using thespecific system used during the block.
As with thePost-Session Questionnaire, each question fromthis questionnaire was mapped to one or more ofour research hypotheses and observers asked fol-low-up questions about analysts?
responses to se-lect questions during the Post-System Interview.3.9 Post-System InterviewObservers used a Post-System Interview Sched-ule to privately interview each analyst at the end ofa block.
The Interview Schedule contained in-structions to the observer for conducting the inter-view, as well as six open-ended questions.
As inthe Post-Session Interview, observers were in-structed to construct content for two of these ques-tions from analysts?
responses to the Post-SystemQuestionnaire.3.10 Cross-EvaluationThe last component of each block was CrossEvaluation (Ying & Kantor, 2006).
Each analystreviewed (using a paper copy) all seven reportsprepared for each scenario in the block (14 totalreports).
Analysts used an online tool to rate eachreport according to 7 criteria using 5-point scales.After analysts completed independent ratings ofeach report according to the 7 criteria, they wereasked to sort the stack of reports into rank order,placing the best report at the top of the pile.
Ana-lysts were then asked to use a pen to write the ap-propriate rank number at the top of each report,and to use an online tool to enter their report rank-ings.
The criteria that the analysts used for evalu-ating reports were: (1) covers the importantground; (2) avoids the irrelevant materials; (3)avoids redundant information; (4) includes selec-tive information; (5) is well organized; (6) readsclearly and easily; and (7) overall rating.3.11 Cross-Evaluation Focus GroupsAfter the Cross Evaluation, focus groups of four53analysts were formed to discuss the results of theCross Evaluation.
These focus groups had twopurposes: to develop a consensus ranking of theseven reports for each scenario, and to elicit theaspects, or dimensions, which led each analyst torank a report high or low in overall quality.
Thesediscussions were taped and an observer took notesduring the discussion.3.12 System Logs and Glass BoxThroughout much of the evaluation, logging andGlass Box software captured analysts?
interactionswith systems.
The Glass Box software supportscapture of analyst workstation activities includingkeyboard/mouse data, window events, file openand save events, copy/paste events, and webbrowser activity.
The Glass Box uses a relationaldatabase to store time-stamped events and a hierar-chical file store where files and the content of webpages are stored.
The Glass Box copies every filethe analyst opens so that there is a complete recordof the evolution of documents.
Material on everyweb page analysts visit is explicitly stored so thateach web page can be later recreated by research-ers as it existed at the time it was accessed by ana-lysts; screen and audio capture are also available.The data captured by the Glass Box provides de-tails about analysts?
interaction with Microsoftdesktop components, such as MS Office and Inter-net Explorer.
User interaction with applicationsthat do not run in a browser and Java applicationsthat may run in a browser are opaque to Glass Box.Although limited information, e.g.
Window Title,application name, information copied to the systemClipboard, is captured, the quantity and quality ofthe data is not sufficient to serve as a complete logof user-system interaction.
Thus, a set of loggingrequirements was developed and implement byeach system.
These included: time stamp; set ofdocuments the user copied text from; number ofdocuments viewed; number of documents that thesystem said contained the answer; and analyst?squery/question.3.13 End-of-Workshop ActivitiesOn the final day of the workshop, analysts com-pleted a Scenario Difficulty Assessment task, pro-vided feedback to system developers andparticipated in two focus group interviews.
As partof the Scenario Difficulty Assessment, analystsrated each scenario on 12 dimensions, and alsorank-ordered the scenarios according to level ofdifficulty.
After the Scenario Difficulty Assess-ment, analysts visited each of the three experimen-tal system developers in turn, for a 40-minute freeform discussion to provide feedback about sys-tems.
As the last event in the workshop, analystsparticipated in two focus groups.
The first was toobtain additional feedback about analysts?
overallexperiences and the second was to obtain feedbackfrom analysts about the evaluation process.4 DiscussionIn this section, we present key findings with re-gard to the effectiveness of these data collectiontechniques in discriminating between systems.Corpus.
The corpus consisted of a specializedcollection of CNS and Web documents.
Althoughthis combination resulted in a larger, diverse cor-pus, this corpus was not identical to the kinds ofcorpora analysts use in their daily jobs.
In particu-lar, analysts search corpora of confidential gov-ernment documents.
Obviously, these corpora arenot readily available for QA system evaluation.Thus, creation of a realistic corpus with documentsthat analysts are used to is a significant challenge.Scenarios.
Scenarios were developed by twoconsultants from the Rome AFRL.
The develop-ment of appropriate and robust scenarios that mim-icked real-world tasks was a time intensiveprocess.
As noted earlier, we discovered that inspite of this process, scenarios were still missingimportant contextual details that govern reportgeneration.
Thus, creating scenarios involves morethan identifying the content and scope of the in-formation sought.
It also requires identifying in-formation such as customer, role and deadline.Analysts.
Analysts in this experiment were na-val reservists, recruited by email solicitation of alarge pool of potential volunteers; the first 8 posi-tive responders were inducted into the study.
Suchself-selection is virtually certain to produce a non-random sample.
However, this sample was fromthe target population which adds to the validity ofthe findings.
We recommend that decision makersevaluating systems expend substantial effort torecruit analysts typical of those who will be usingthe system and be aware that self selection biasesare likely to be present.
Care should be taken toensure that subjects have a working knowledge of54basic tasks and systems, such as using browsers,Microsoft Word, and possibly Microsoft Excel.Experimental Design.
We used a great deal ofrandomization in our experimental design; the pur-pose was to obtain more valid statistical results.All statistical results are conditioned by the state-ment ?if the analysts and tasks used are a randomsample from the universe of relevant analysts andtasks.?
Scenarios were not a random selectionamong possible scenarios; instead, they were tai-lored to the corpus.
Similarly, analysts were not arandom sample of all possible analysts, since theywere in fact self-selected from a smaller pool of allpossible analysts.
The randomization in the ex-perimental rotation allowed us to mitigate biasesintroduced by non-probability sampling techniquesacross system, as well as curtail any potential biasintroduced by observers.Data Collection.
We employed a wide varietyof data collection techniques.
Key findings withrespect to each technique are presented below.Questionnaires were powerful discriminatorsacross the range of hypotheses tested.
They werealso relatively economical to develop and analyze.Most analysts were comfortable completing ques-tionnaires, although with eight repetitions theysometimes became fatigued.
Questionnaires alsoprovided a useful opportunity to check the validityof experimental materials such as scenarios.The NASA TLX was sensitive in assessing ana-lysts?
workloads for each scenario.
It was cheap toadminister and analyze, and has established valid-ity and reliability as an instrument in a differentarena, where there are real time pressures to con-trol a mechanical system.Formative techniques, such as interviews andfocus groups, provided the most useful feedback,especially to system developers.
Interview and fo-cus group data usually provide researchers withimportant information that supplements, qualifiesor elaborates data obtained through questionnaires.With questionnaires, users are forced to quantifytheir attitudes using numeric values.
Data collec-tion methods designed to gather qualitative data,such as interviews, provide users with opportuni-ties to elaborate and qualify their attitudes andopinions.
One effective technique used in thisevaluation was to ask analysts to elaborate on someof their numeric ratings from questionnaires.
Thisallows us to understand more about why analystsused particular values to describe their attitudesand experiences.
It is important to note that analy-sis of qualitative data is costly ?
interviews weretranscribed and training is needed to analyze andinterpret data.
Training is also necessary to con-duct such interviews.
Because researchers are es-sentially the ?instrument?
it is important to learn tomoderate one?s own beliefs and behaviors whileinterviewing.
It is particularly important that inter-viewers not be seen by their interviewees as ?in-vested in?
any particular system; havingindividuals who are not system developers conductinterviews is essential.The SmiFro Console was not effective as im-plemented.
Capturing analysts?
in situ thoughtswith minimal disruption remains a challenge.
Al-though SmiFro Console was not particularly effec-tive, status report data was easy to obtain andsomewhat effective, but defied analysis.Cross evaluation of reports was a sensitive andreliable method for evaluating product.
Comple-menting questionnaires, it is a good method forassessing the quality of the analysts?
work prod-ucts.
The method is somewhat costly in terms ofanalysts?
time (contributing approximately 8% ofthe total time required from subjects), and analysisrequires skill in statistical methods.System logs answered several questions not ad-dressable with other methods including the GlassBox.
However, logging is expensive, rarely reus-able, and often unruly when extracting particularmeasures.
Development of a standard logging for-mat for interactive QA systems is advisable.
TheGlass Box provided data on user interaction acrossall systems at various levels of granularity.
Thecost of collection is low but the cost of analysis isprobably prohibitive in most cases.
NIST?s previ-ous experience using Glass Box allowed for morerapid extraction, analysis and interpretation of data,which remained a very time consuming and labori-ous process.
Other commercial tools are availablethat capture some of the same data and we recom-mend that research teams evaluate such tools fortheir own evaluations.Hypotheses.
We started this study with hy-potheses about the types of interactions that a goodQA system should support.
Of course, differentmethods were more or less appropriate for assess-ing different hypotheses.
Table 4 displays part ofour results with respect to the example hypothesespresented above in Table 1.
For each of the exam-ple hypotheses provided in Table 1, we show55which method was used.Ques.
NASATLXSmi-FroCross-Eval.Logs GlassBox1  X   X X2 X3 X   X4 X    X X5 X  XTable 4: Most effective methods for gatheringdata about example hypotheses (see Table 1).Although not reported here, we note that theperformance of each of the systems evaluated inthis study varied according to hypothesis; in par-ticular, some systems did well according to somehypotheses and poor according to others.Interaction.
Finally, while the purposes of thispaper were to present our evaluation method forinteractive question answering systems, our in-struments elicited interesting results about ana-lysts?
perceptions of interaction.
Foremost amongthem, users of interactive systems expect systemsto exhibit behaviors which can be characterized asunderstanding what the user is looking for, whatthe user has done and what the user knows.
Ana-lysts in this study expected interactive systems totrack their actions over time, both with the systemand with information.5 ConclusionsWe have sketched a method for evaluating in-teractive analytic question answering system, iden-tified key design decisions that developers mustmake in conducting their own evaluations, and de-scribed the effectiveness of some of our methods.Clearly, each evaluation situation is different, andit is difficult to develop one-size-fits-all evaluationstrategies, especially for interactive systems.However, there are many opportunities for devel-oping shared frameworks and an infrastructure forevaluation.
In particular, the development of sce-narios and corpora are expensive and should beshared.
The creation of sharable questionnaires andother instruments that are customizable to individ-ual systems can also contribute to an infrastructurefor interactive QA evaluation.We believe that important opportunities existthrough interactive QA evaluation for understand-ing more about the interactive QA process and de-veloping extensive theoretical and empiricalfoundations for research.
We encourage systemdevelopers to think beyond independent systemevaluation for narrow purposes, and conductevaluations that create and inform theoretical andempirical foundations for interactive question an-swering research that will outlive individual sys-tems.
Although we do not have space here to detailthe templates, instruments, and analytical schemasused in this study, we hope that the methods andmetrics developed in connection with our study area first step in this direction3.
We plan to publishthe full set of results from this study in the future.ReferencesDiekema, A. R., Yilmazel, O., Chen, J., Harwell, S., He,L., & Liddy, E. D. (2004).
Finding answers to com-plex questions.
In M. T. Maybury?s, New directionsin question answering.
MIT Press, MA., 141-152.Hampson, E., & Crowley, P. (2005).
Instrumenting theintelligence analysis process.
Proceedings of the2005 International Conference on Intelligence Analy-sis, McLean, VA.Hersh, W. & Over, P. (2001).
Introduction to a specialissue on interactivity at the Text Retrieval Confer-ence (TREC).
Information Processing & Manage-ment 37(3), 365-367.Liddy, E. D., Diekema, A. R., & Yilmazel, O.
(2004).Context-based question-answering evaluation.
Pro-ceedings of SIGIR ?04, Sheffield, UK, 508-509.Sun, Y., & Kantor, P. (2006).
Cross-evaluation: A newmodel for information system evaluation.
Journal ofAmerican Society for Information Science & Tech-nology.Tague-Sutcliffe, J.
(1992).
The pragmatics of informa-tion retrieval experimentation, revisted.
InformationProcessing & Management, 28(4), 467-490.Voorhees, E. M. (2003).
Evaluating the evaluation: Acase study using the TREC 2002 Question Answer-ing Task.
Proceedings of HLT-NAACL?03, 181-188.3 The NIST team maintains a password-protected website(http://control.nist.gov/amc/) for materials related to this pro-ject.
Send email to emile.morse@nist.gov.56
