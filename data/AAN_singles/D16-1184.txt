Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1787?1796,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSyntactic Parsing of Web QueriesXiangyan SunFudan UniversityHaixun WangFacebookYanghua Xiao?Fudan UniversityZhongyuan WangMicrosoft ResearchAbstractSyntactic parsing of web queries is impor-tant for query understanding.
However, webqueries usually do not observe the grammar ofa written language, and no labeled syntactictrees for web queries are available.
In this pa-per, we focus on a query?s clicked sentence,i.e., a well-formed sentence that i) containsall the tokens of the query, and ii) appears inthe query?s top clicked web pages.
We ar-gue such sentences are semantically consistentwith the query.
We introduce algorithms to de-rive a query?s syntactic structure from the de-pendency trees of its clicked sentences.
Thisgives us a web query treebank without manuallabeling.
We then train a dependency parseron the treebank.
Our model achieves muchbetter UAS (0.86) and LAS (0.80) scores thanstate-of-the-art parsers on web queries.1 IntroductionSyntactic analysis is important in understandinga sentence?s grammatical constituents, parts ofspeech, syntactic relations, and semantics.
In thispaper, we are concerned with the syntactic structureof a short text.
The challenge is that short texts, forexample, web queries, do not observe grammars ofwritten languages (e.g., users often overlook capital-ization, function words, and word order when creat-?
Correspondence author.
This paper was supportedby National Key Basic Reserach Program of China un-der No.2015CB358800, by National NSFC(No.61472085,61171132, 61033010, U1509213), by Shanghai Municipal Sci-ence and Technology Commission foundation key project underNo.15JC1400900.ing a web query), and applying parsers trained onstandard treebanks on queries leads to poor results.Syntactic structures are valuable for query under-standing.
Consider the following web queries andtheir syntactic structures we would like to construct:cover iphone 6 plusNN NN CD NNnn num amoddistance earth moonNN NN NNnnnnfaucet adapter femaleNN NN ADJnn nnThe syntactic structure of query cover iphone6 plus tells us that the head token is cover, in-dicating its intent is to shop for the cover of aniphone, instead of iphones.
With this knowledge,search engines show ads of iphone covers instead ofiphones.
For distance earth moon, the headis distance, indicating its intent is to find the dis-tance between the earth and the moon.
For faucetadapter female, the intent is to find a femalefaucet adapter.
In summary, correctly identifyingthe head of a query helps identify its intent, andcorrectly identifying the modifiers helps rewrite thequery (e.g., dropping non-essential modifiers).Syntactic parsing of web queries is challengingfor at least two reasons.
First, grammatical signalsfrom function words and word order are not avail-able.
Query distance earth moon is missingfunction words between (preposition), and (coordi-nator), and the (determiner) in conveying the intent1787distance between the earth and the moon.
Also, itis likely that queries {distance earth moon,earth moon distance, earth distancemoon, ?
?
? }
have the same intent, which means theyshould have the same syntactic structure.
Second,there is no labeled dependency trees (treebank) forweb queries, nor is there a standard for construct-ing such dependency trees.
It will take a tremendousamount of time and effort to come up with such astandard and a treebank for web queries.In this paper, we propose an end-to-end solutionfrom treebank construction to syntactic parsing forweb queries.
Our model achieves a UAS of 0.830and an LAS of 0.747 on web queries, which isdramatic improvement over state-of-the-art parserstrained from standard treebanks.2 Our ApproachThe biggest challenge of syntactic analysis of webqueries is that they do not contain sufficient gram-matical signals required for parsing.
Indeed, webqueries can be very ambiguious.
For example, kidstoys may mean either toys for kids or kidswith toys, for which the dependency relation-ships between toys and kids are totally opposite.kids with toysNN IN NNprep pobjtoys for kidsNN IN NNprep pobjIn view of this, why is syntactic parsing of webqueries a legitimate problem?
We have shown someexample syntactic structures for 3 queries in Section1.
How do we know they are the correct syntacticstructures for the queries?
We answer these ques-tions here.2.1 Derive syntax from semanticsIn many cases, humans can easily determine the syn-tax of a web query because its intent is easy to under-stand.
For example, for toys kids, we are prettysure as a web query, its intent is to look for toys forkids, instead of the other way around.
Thus, toysshould be the head of the query, and kids should beits modifier.
In other words, when the semantics of aquery is understood, we can often recover its syntax.We may then manually annotate web queries.Specifically, given a query, a human annotator formsa sentence that is consistent with the meaning hecomes up for the query.
Then, from the sentence?ssyntactic structure (which is well understood andcan be derived by a parser), the annotator derives thesyntactic structure of the query.
For example, forquery thai food houston, the annotator mayformulate the following sentence:... my favorite Thai food in Houston ...PRP$ JJ NNP NN IN NNpossamodnn prep pobjThen we may project the dependency tree of thesentence to the query:thai food houstonNNP NN NNnn nnThe above approach has two issues.
First, foodand houston are not directly connected in the de-pendency tree of the sentence.
We connected themin the query, but in general, it is not trivial to in-fer synatx of the query from sentences in a consis-tent way.
There is no linguistic standard for doingthis.
Second, annotation is very costly.
A treebankproject takes years to accomplish.2.2 Semantics of a web queryTo avoid human annotation, we derive syntactic un-derstanding of the query from semantic understand-ing of the query.
Our goal is to decide for any twotokens x, y ?
q, whether there is a dependency arcbetween x and y, and if yes, what the dependency is.Context-free signals.
One approach to determinethe dependency between x and y is to directly modelP (e|x, y), where e denotes the dependency (x ?
yor x ?
y).
It is context-free because we do notcondition on the query where x and y appear in.To acquire P (e|x, y), we may consider annotatedcorpora such as Google?s syntactic ngram (Goldbergand Orwant, 2013).
For any x and y, we count thenumber of times that x is a dependent of y in the cor-pus.
One disadvantage of this approach is that webqueries and normal text differ significantly in distri-bution.
Another approach (Wang et al, 2014) is touse search log to estimate P (e|x, y), where x andy are nouns.
Specifically, we find queries of pat-tern x PREP y, where PREP is a preposition {of, in,for, at, on, with, ?
?
?
}.
We have P (x ?
y|x, y) =1788nx,ynx,y+ny,x where nx,y denotes the number of timespattern x PREP y appears in the search log.
The dis-advantage is that the simple pattern only gives de-pendency between two nouns.Context-sensitive signals.
The context-free ap-proach has two major weaknesses: (1) It is risky todecide the dependency between two tokens withoutconsidering the context.
(2) Context-free signals donot reveal the type of dependency, that is, it does notreveal the linguistic relationship between the headand the modifier.To take context into consideration, which meansestimating P (e|x, y, q) for any two tokens x, y ?
q,we are looking at the problem of building a parserfor web queries.
This requires a training dataset (atreebank).
In this work, we propose to automati-cally create such a treebank.
The feasibility is cen-tered on the following assumption: The intent of qis contained in or consistent with the semantics ofits clicked sentences.
We call sentence s a clickedsentence of q if i) s appears in a top clicked page forq, and ii) s contains all tokens in q.
For instance, as-sume sentence s = ?...
my favorite Thaifood in Houston ...?
appears in one of themost frequently clicked pages for query q = thaifood houston, then s is a clicked sentence of q.It follows from the above assumption that the de-pendency between any two tokens in q are likely tobe the same as the dependency between their corre-sponding tokens in s. This allows to create a tree-bank if we can project the dependency from sen-tences to queries.
However, since x and y may notbe directly connected by a dependency edge in s,we need a method to derive the dependency betweenx, y ?
q from the (indirect) dependency betweenx, y ?
s. We propose such a method in Section 3.3 Treebank for Web QueriesWe create a web query treebank by projecting de-pendency from clicked sentences to queries.3.1 Inferring a dependency treeA query q may have multiple clicked sentences.
Wedescribe here how we project dependency to q fromsuch a sentence s. We describe how we aggregatedependencies from multiple sentences in Sec 3.2.Under our assumption, each token x ?
q mustappear in sentence s. But x may appear multipletimes in s (especially when x is a function word).As an example, for query apple watch stand,we may get the following sentence:Its apple watch charging stand is my favorite stand .PRP$ NN NN NN NN VBZ PRP$ JJ NN .possnn nn nnnsubjcoppossamod punctSentence s contains token stand twice, but onlyone subtree contains each token in q exactly once.apple watch charging standNN NN NN NNnn nn nnWe use the following heuristics to derive a depen-dency tree for query q from sentence s.1.
Let Ts denote all the subtrees of the depen-dency tree of s.2.
Find the minimum subtree t ?
Ts such thateach x ?
q has one and only one match x?
?
t.3.
Derive dependency tree tq,s for q from t as fol-lows.
For any two tokens x and y in q:(a) if there is an edge from x?
to y?
in t, wecreate a same edge from x to y in tq,s.
(b) if there is a path1 from x?
to y?
in t, wecreate an edge from x to y in tq,s, and labelit temporarily as dep.We note the following.
First, we argue that if thedependency tree of s has a subtree that contains eachtoken in q once and only once, then it is very likelythat the subtree expresses the same semantics as thequery.
On the other hand, if we cannot find such asubtree, it is an indication that we cannot derive rea-sonable dependency information from the sentence.Second, it?s possible x?
and y?
are not connecteddirectly in s but through one or more other tokens.Thus, we do not know the label of the derived edge.We will decide on the label in Sec 3.3.Third, we want to know whether it is meaningfulto connect x and y in q while x?
and y?
are not di-rectly connected in s. We evaluated a few hundreds1A path consists of edges of the same direction.1789of query-sentence pairs.
Among the cases where de-pendency trees for queries can be derived success-fully, we found that x?
and y?
are connected in 5possible ways (Table 1).
We describe them in de-tails next.directly connected 46%connected via function words 24%connected via modifiers 24%connected via a head noun 4%connected via a verb 2%Table 1: Dependency ProjectionDirectly connected.
In this case, we copy theedge and its label directly.
Consider query partysupplies cheap?s clicked sentence below:... selection of cheap party supplies is ...NN IN JJ NN NNS VBZprepamodnnpobjHere both (party, supplies) and (supplies,cheap) are directly connected.
The query inheritsthe dependencies, but note that tokens suppliesand cheap have different word orders in q and s:party supplies cheapNN NNS JJnn amodConnected via function words.
It is quite com-mon prepositions are omitted in a query.
Considerquery moon landing?s clicked sentence:... first soft landing on moon in 37 years .JJ JJ NN IN NN IN CD NNS .amodamod prep pobjprepnumpobjWe can derive the following dependency tree:moon landingNN NNdepFor query side effects b12, suppose wehave the following sentence:The side effects of vitamin b12 ...DT NN NNS IN NN JJdet nn prep nnpobjThe derived dependency tree should be:side effects b12NN NNS JJnn depFor these two cases, we need to introduce a de-rived edge for the query, which will be resolved laterto a specific dependency label.Connected via modifiers.
Many web queries arenoun compounds.
Their clicked sentences may havemore modifiers.
Depending on the bracketing, wemay or may not have direct dependencies.For offshore work and its clicked sentencebelow, missing drilling in the query does notcause any problem: offshore and work are stilldirectly connected in the dependency tree.... this offshore drilling work ...DT JJ NN NNamodnnBut not for crude price and its clicked sentence.Still, there is a path: crude?
oil?
price.... crude oil price is rousing ...JJ NN NN VBZ VBGamod nn dep ccompIn this case, we create a dependency betweencrude and oil in the query and give it a tempo-rary label dep.
We will resolve it to a specific labellater.crude priceNN NNdepConnected via a head noun.
In some cases, thehead of a noun compound is missing.
Considercountry singers and its clicked sentence:... singers in country music ...NNS IN NN NNprep nnpobjClearly they mean the same thing, but the head(music) of the noun compound is missing in thequery.
Still, a path exists from singers tocountry, and we create a dependency:1790country singersNN NNdepConnected via a verb.
One common case is theomission of copular verbs.
Consider plantspoisonous to goats and its clicked sentence:... many plants are poisonous to goats .JJ NNS VBP JJ TO NNS .amodnsubjcop prep pobjHere, the missing are does not cause any problem.But for query pain between breasts and itsclicked sentence:The pain that appears between the breasts ...DT NN WDT VBZ IN DT NNSdet nsubjrcmod prep detpobjwe need to introduce a derived edge, and it leads to:pain between breastsNN IN NNSprep pobj3.2 Inferring a unique dependency treeA query corresponds to multiple clicked sentences.From each sentence, we derive a dependency tree.These dependency trees may not be the same, be-cause i) dependency parsing for sentences is not per-fect; ii) queries are ambiguous; or iii) some queriesdo not have well-formed clicked sentences.To choose a unique dependency tree for a query q,we define a scoring function f to measure the qual-ity of a dependency tree tq derived from q?s clickedsentence s:f(tq, s) =?(x?y)?tq?
?dist(x, y) + log count(x?
y)count(x?
y)(1)where (x?
y) is an edge in the tree tq, count(x?y) is the occurrence count of the edge x ?
y inthe entire query dataset, dist(x, y) is the distanceof words x and y on the original sentence parsingtree, and ?
is a parameter to adjust the importancebetween the two measures (its value is empiricallydetermined).The first term of the scoring function measures thecompactness of the query tree.
Consider two clickedCorrect Wrong Query Sentenceside?
effects side?
effects 1110:1 11257:17benefits?
of benefits?
of 144:63 5228:0Full?Movie Full?Movie 128:5 1585:27coconut?
oil coconut?
oil 91:10 1507:46credit?
card credit?
card 96:2 4394:60Table 2: Examples of globally inconsistent headmodifier relationssentences for query deep learning:... learning how to deep fry chicken ...... JJ WRB NN IN NN IN ...acladvmodmarkadvmod dobj... enjoy deep learning ...... VBP JJ NN ...dobjamodIn the first sentence, deep and learning areindirectly connected through fry so the total dis-tance measure is 2.
In the second query, the distanceis 1.
Therefore, query aligned with the second sen-tence is better than the first sentence.The second term of the scoring function measuresthe global consistency among head modifier direc-tions.
For a word pair (x, y), if in the dataset, thenumber of edges x ?
y dominates the number ofedges x?
y, then the latter is likely to be incorrect.One important thing to note is word order.
Wordorder may influence the head-modifier relations be-tween two words.
For example, child of andof child should definitely have different head-modifier relations.
Therefore, we treat two wordsof different order as two different word pairs.Table 2 shows some examples of conflicting de-pendency edges and their corresponding occurrencecount in queries and sentences.3.3 Label refinementIn Section 3.1, some dependencies are derived witha placeholder label dep.
Before we use the datato train a parser, we must resolve dep to a truelabel, otherwise they introduce inconsistency inthe training data.
For example, consider a sim-ple query crude price.
From clicked sen-tences that contain crude oil price, we de-1791rive crude dep??
?price, but from those that containcrude price, we derive crude amod???
?price.To resolve dep, we resort to majority vote first.For any x dep???
y, we count the occurrence of x label??
?y in the training data for each concrete label.
If thefrequency of a certain label is dominating by a pre-determined threshold (10 times more frequent thanany other label), then we resolve dep to that label.With our training data, the above process is ableto resolve about 90% dependencies.
We can simplydiscard queries that contain unresolvable dependen-cies.
However, such queries still contain useful in-formation, for example, the direction of this edge,and the directions and labels of all the other edges.We develop a bootstrapping method to preserve suchuseful information.
First, we train a parser on datawithout dep labels.
This skips about 10% queries inour experiments.
Second, we use the parser to pre-dict the unknown label.
If the prediction is consis-tent with the annotation except for the dep label, weuse the predicted label.
Third, we add the resolvedqueries into the training data and train a final parser.Experiments show the bootstrapping approach im-proves the quality of the parser.4 Dependency ParsingWe train a parser from the web query treebankdata.
We also try to incorporate context-free head-modifier signals into parsing.
To make it easier toincorporate such signals, we adopt a neural networkapproach to train our POS tagger and parser.4.1 Neural network POS tagger and parserWe first train a neural network POS tagger for webqueries.
For each word in the sentence, we constructfeatures out of a fixed context window centered atthat word.
The features include the word itself, case(whether the first letter, any letter, or every letter inthe word, is in uppercase), prefix, and suffix (we rec-ognize a pre-defined set of prefixes and suffixes, forthe rest we use a special token ?UNK?).
For the wordfeature, we use pre-trained word2vec embeddings.For word case and prefix/suffix, we use random ini-tialization for the embeddings.
The accuracy of thetrained POS tagger is similar to that of (Ganchev etal., 2012), which outperforms POS taggers trainedon PTB data.Buffer featuresb1.wt, b2.wt, b3.wtStack featuress1.wt, s2.wt, s3.wtTree featureslc1(s1).wtl, lc2(s1).wtl, rc1(s1).wtl, rc2(s1).wtllc1(lc1(s1)).wtl, rc1(rc1(s1)).wtllc1(s2).wtl, lc2(s2).wtl, rc1(s2).wtl, rc2(s2).wtllc1(lc1(s2)).wtl, rc1(rc1(s2)).wtlTable 3: The feature templates.
si(i = 1, 2, ...) de-note the ith top element of the stack, bi(i = 1, 2, ...)denote the ith element on the buffer, lck(si) andrck(si) denote the kth leftmost and rightmost chil-dren of si, w denotes words, t denotes POS tag, ldenotes label.We use the arc standard transition based depen-dency parsing system (Nivre, 2004).
The architec-ture of the neural network dependency parser is sim-ilar to that of (Chen and Manning, 2014) designedfor parsing sentences.
The features used in parsingare shown in Table 3.4.2 Context free featuresIn Section 2.2, we discussed context-free signalsP (e|x, y) and context-sensitive signals P (e|x, y, q).Previous work (Wang et al, 2014) uses context-freesignals for syntactic analysis of a query.
Our ap-proach outperforms the context-free approach.An interesting question is, will context-free sig-nals further improve our approach?
The rationale isthat although context-sensitive signals P (e|x, y, q)are more accurate in predicting the dependency be-tween x and y, such signals are also very sparse.
Docontext-free signals P (e|x, y) provide backoff infor-mation in parsing?It is not straightforward to include P (e|x, y) inthe neural network model.
The head-modifier rela-tions P (e|x, y) may exist between any pair of tokensin the input query.
Essentially, it is a pairwise graph-ical model and it is difficult to directly incorporatethe signals in transition based dependency parsing.We treat context-free signals as prior knowledge.We train head-modifier embeddings for each to-ken, and use such embeddings as pre-trained embed-dings.
Specifically, we use an approach similar totraining word2vec embeddings but focusing on head1792modifier relationships instead of co-occurrence rela-tionships.
More specifically, we train an one hiddenlayer neural network classifier to determine whethertwo words have head-modifier relations.
The inputof the neural network is the concatenation of the em-beddings of two words.
The output is whether thetwo words form a proper head-modifier relationship.We obtain a large set of head-modifier data from textcorpus by mining ?h PREP m?
pattern in search logwhere h and m are nouns.
Then, for each knownhead modifier pair h and m, we use (h,m) as pos-itive example and (m,h) as negative example.
Foreach word, we also choose a few random words asnegative examples.
During the training process, thegradients are back propagated to the word embed-dings.
After training, the embeddings should con-tain sufficient information to recover head modifierrelations between any word pairs.But we did not observe improvement over the ex-isting neural network that are trained on context sen-sitive treebank data alone.
The head-modifier em-beddings has about 3% advantage in UAS over ran-domized embeddings.
However, using pretrainedword2vec embeddings, we also achieve 3% advan-tage.
Thus, it seems that context-sensitive signalsplus the generalizing power of embeddings containall the context-free signals already.5 ExperimentsIn this section, we start with some case studies.
Thenwe describe data and compare models.In experiments, we use the standard UAS (unla-beled attachment score) and LAS (labeled attach-ment score) score for measuring the quality of de-pendency parsing.
They are calculated as:UAS = # correct arc directions# total arcs (2)LAS = # correct arc directions and labels# total arcs (3)5.1 Case StudyWe compare dependency trees produced by ourQueryParser and Stanford Parser (Chen and Man-ning, 2014) for some web queries (Stanford Parseris trained from the standard PTB treebank).
Table 4shows that Stanford Parser heavily relies on gram-mar signals such as function words and word or-der, while QueryParser relies more on the seman-tics of the query.
For instance, in the 1st exam-ple, QueryParser identifies toys as the head, re-gardless of the word order, while Stanford parseralways assumes the last token as the head.
In the2nd example, the semantics of the query is a school(vanguard school) at a certain location (lakewales).
QueryParser captures the semantics andcorrectly identifies school as the head (root) of thequery, while Stanford parser treats the entire queryas a single noun compound (likely inferred from thePOS tags).5.2 Clicked SentencesFor training data, we use one-month Bing query log(between July 25, 2015 and August 24, 2015).
Fromthe log, we obtain web query q and its top clickedURLs {url1, url2, ..., urlm}.
From the urls, we re-trieve the clicked HTML document, and find sen-tences {s1, s2, ..., sn} that contain all words (regard-less to their order of occurrence) in q.
Then we ex-tract query-sentence tuples (q, s, count) to serve asour training data to generate a web query treebank.The size (# of distinct query-sentence pairs) of theraw clicked sentences is 390,225,806.5.3 Web Query TreebankWe evaluate the 3 steps of treebank generation.
Af-ter each step, we sample 100 queries from the resultand manually compute their UAS and LAS scores.We also count the number of total query instances ineach step.
The results are shown in Table 5.?
Inferring a dependency tree: For each (query,sentence) pair, we project dependency fromthe sentence to the query.
The number of in-stances shown in Table 5 are the input num-ber of (query, sentence) pairs.
It shows thatwe obtain dependency trees for only 31% of thequeries, while the rest do not satisfy our filter-ing criterion.
This however is not a concern.By sacrificing recall in this process, we ensurehigh precision.
Given that query log is large,precision is more important.?
Inferring a unique dependency tree: In thisstep, we group (query, sentence) pairs byunique queries.
Using the method in Section1793QueryParser Stanford parsertoys kidsNNS NNSnnkids toysNNS NNSnntoys kidsNNS NNSnnkids toysNNS NNSnnvanguard school lake walesNN NN NN NNSnn nnnnvanguard school lake walesNN NN NN NNSnnnnnnpretty little liars season 4 episode 6RB JJ NNS NN CD NN CDadvmodnnnnnumnnnumpretty little liars season 4 episode 6RB JJ NNS NN CD NN CDadvmodnnnnnumnn numinterview questions contract specialistNN NNS NN NNnn nnnncontract specialist interview questionNN NN NN NNnn nn nninterview questions contract specialistNN NNS NN NNnnnnnncontract specialist interview questionNN NN NN NNnnnnnnTable 4: Case study of parsers.3.2, each group produces one or zero depen-dency trees.
The number of instances in Table5 corresponds to the number of different querygroups.
The overall success rate is high.
Thisis expected as the filtering process uses major-ity voting, and we already have high precisionparsing trees after the first step.?
Label refinement: Dependency labels are re-fined using the methodology in Section 3.3.
Itshows that with majority voting and bootstrap-ing, we are able to keep all the input.5.4 Parser PerformanceWe compare QueryParser against three state-of-the-art parsers: Stanford parser, which is a transitionbased dependency parser based on neural network,MSTParser (McDonald et al, 2005), which is agraph based dependency parser based on minimumspanning tree algorithms, and LSTMParser (Dyer etal., 2015), which is a transition based dependencyparser based on stack long short-term memory cells.Here, QueryParser is trained from our web querytreebank, while Stanford Parser and MSTParser aretrained from standard PTB treebanks.For comparison, we manually labeled 1,000 webqueries to serve as a ground truth dataset2.
We pro-duce POS tags for the queries using our neural net-work POS tagger.
To specifically measure the abilityof QueryParser in parsing queries with no explicitsyntax structure, we split the entire dataset All intotwo parts: NoFunc and Func, which correspond toqueries without any function word, and queries withat least one function word.
The number of queries2https://github.com/wishstudio/queryparser1794Step Total Instances Produced Instances Success Rate UAS LASInferring a dependency tree 3986300 1229860 31% 0.906 0.851Inferring a unique tree 716261 680857 95% 0.910 0.851Label refinement 680857 680857 100% 0.917 0.855Table 5: Training dataset generation statisticsSystem All (n=1000) NoFunc (n=900) Func (n=100)UAS LAS UAS LAS UAS LASStanford 0.694 0.602 0.670 0.568 0.834 0.799MSTParser 0.699 0.616 0.683 0.691 0.799 0.766LSTMParser 0.700 0.608 0.679 0.578 0.827 0.790QueryParser + label refinement 0.829 0.769 0.824 0.761 0.858 0.818QueryParser + word2vec 0.843 0.788 0.843 0.784 0.838 0.812QueryParser + label refinement + word2vec 0.862 0.804 0.858 0.795 0.883 0.854Table 6: Parsing performance on web queriesof the two datasets are 900 and 100, respectively.Table 6 shows the results.
We use 3 versionsof QueryParser.
The first two use random wordembedding for initialization, and the first one doesnot use label refinement.
From the results, it canbe concluded that QueryParser consistently outper-formed competitors on query parsing task.
Pre-trained word2vec embeddings improve performanceby 3-5 percent, and the postprocess of label refine-ment also improves the performance by 1-2 percent.Table 6 also shows that conventional depencencyparsers trained on sentence dataset relies much moreon the syntactic signals in the input.
While Stanfordparser and MSTParser have similar performance toour parser on Func dataset, the performance dropssignificantly on All and NoFunc dataset, when themajority of input has no function words.6 Related WorkSome recent work (Ganchev et al, 2012; Barr et al,2008) investigated the problem of syntactic analysisfor web queries.
However, current study is mostlyat postag rather than dependency tree level.
Barr etal.
(2008) showed that applying taggers trained ontraditional corpora on web queries leads to poor re-sults.
Ganchev et al (2012) propose a simple, ef-ficient procedure in which part-of-speech tags aretransferred from retrieval-result snippets to queriesat training time.
But they do not reveal syntacticstructures of web queries.More work has focused on resolving simple re-lations or structures in queries or short texts, par-ticularly entity-concept relations (Shen et al, 2006;Wang et al, 2015; Hua et al, 2015), entity-attributerelations (Pasca and Van Durme, 2007; Lee et al,2013), head-modifier relations (Bendersky et al,2010; Wang et al, 2014).
Such relations are impor-tant but not enough.
The general dependency rela-tions we focus on is an important addition to queryunderstanding.On the other hand, there is extensive work on syn-tactic analysis of well-formed sentences (De Marn-effe et al, 2006).
Recently, a lot of work (Collobertet al, 2011; Vinyals et al, 2015; Chen and Manning,2014; Dyer et al, 2015) started using neural networkfor this purpose.
In this work, we use similar neuralnetwork architecture for web queries.7 ConclusionSyntactic analysis of web queries is extremely im-portant as it reveals actional signals to many down-stream applications, including search ranking, adsmatching, etc.
In this work, we first acquire well-formed sentences that contain the semantics of thequery, and then infer the syntax of the query fromthe sentences.
This essentially creates a treebank forweb queries.
We then train a neural network depen-dency parser from the treebank.
Our experimentsshow that we achieve significant improvement overtraditional parsers on web queries.1795ReferencesCory Barr, Rosie Jones, and Moira Regelson.
2008.
Thelinguistic structure of english web-search queries.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, EMNLP ?08, pages1021?1030, Stroudsburg, PA, USA.
Association forComputational Linguistics.Michael Bendersky, Donald Metzler, and W Bruce Croft.2010.
Learning concept importance using a weighteddependence model.
In Proceedings of the third ACMinternational conference on Web search and data min-ing, pages 31?40.
ACM.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, volume 6, pages 449?454.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependeny parsing with stack long short-termmemory.
In Proc.
ACL.Kuzman Ganchev, Keith Hall, Ryan McDonald, and SlavPetrov.
2012.
Using search-logs to improve query tag-ging.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, ACL?12, pages 238?242, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In Second Joint Conference on Lexi-cal and Computational Semantics (* SEM), volume 1,pages 241?247.Wen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng,and Xiaofang Zhou.
2015.
Short text understand-ing through lexical-semantic analysis.
In InternationalConference on Data Engineering (ICDE).Taesung Lee, Zhongyuan Wang, Haixun Wang, andSeung-won Hwang.
2013.
Attribute extraction andscoring: A probabilistic approach.
In InternationalConference on Data Engineering (ICDE).Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 523?530.
Association for Computational Lin-guistics.Joakim Nivre.
2004.
Incrementality in deterministic de-pendency parsing.
In Proceedings of the Workshop onIncremental Parsing: Bringing Engineering and Cog-nition Together, pages 50?57.
Association for Compu-tational Linguistics.Marius Pasca and Benjamin Van Durme.
2007.
Whatyou seek is what you get: Extraction of class attributesfrom query logs.
In IJCAI, volume 7, pages 2832?2837.Dou Shen, Jian-Tao Sun, Qiang Yang, and Zheng Chen.2006.
Building bridges for web query classification.In Proceedings of the 29th annual international ACMSIGIR conference on Research and development in in-formation retrieval, pages 131?138.
ACM.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Advances in Neural Informa-tion Processing Systems, pages 2755?2763.Zhongyuan Wang, Haixun Wang, and Zhirui Hu.
2014.Head, modifier, and constraint detection in short texts.In Data Engineering (ICDE), 2014 IEEE 30th Inter-national Conference on, pages 280?291.
IEEE.Zhongyuan Wang, Kejun Zhao, Haixun Wang, XiaofengMeng, and Ji-Rong Wen.
2015.
Query understand-ing through knowledge-based conceptualization.
InProceedings of the Twenty-Fourth International JointConference on Artificial Intelligence (IJCAI).1796
