Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1445?1454, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsUnified Dependency Parsing of Chinese Morphologicaland Syntactic StructuresZhongguo Li Guodong ZhouNatural Language Processing LaboratorySchool of Computer Science and TechnologySoochow University, Suzhou, Jiangsu Province 215006, China{lzg, gdzhou}@suda.edu.cnAbstractMost previous approaches to syntactic pars-ing of Chinese rely on a preprocessing stepof word segmentation, thereby assuming therewas a clearly defined boundary between mor-phology and syntax in Chinese.
We showhow this assumption can fail badly, leadingto many out-of-vocabulary words and incom-patible annotations.
Hence in practice thestrict separation of morphology and syntax inthe Chinese language proves to be untenable.We present a unified dependency parsing ap-proach for Chinese which takes unsegmentedsentences as input and outputs both morpho-logical and syntactic structures with a singlemodel and algorithm.
By removing the inter-mediate word segmentation, the unified parserno longer needs separate notions for wordsand phrases.
Evaluation proves the effective-ness of the unified model and algorithm inparsing structures of words, phrases and sen-tences simultaneously.
11 IntroductionThe formulation of the concept of words has baf-fled linguists from ancient to modern times (Hock-ett, 1969).
Things are even worse for Chinese, partlydue to the fact that its written form does not delimitwords explicitly.
While we have no doubt that thereare linguistic units which are definitely words (orphrases, for that matter), it?s a sad truth that in manycases we cannot manage to draw such a clear bound-ary between morphology and syntax, for which wenow give two arguments.1Corresponding author is Guodong Zhou.The first argument is that many sub-word linguis-tic units (such as suffixes and prefixes) are so pro-ductive that they can lead to a huge number of out-of-vocabulary words for natural language process-ing systems.
This phenomenon brings us into anawkward situation if we adhere to a rigid separa-tion of morphology and syntax.
Consider charac-ter ?
?someone?
as an example.
On the one hand,there is strong evidence that it?s not a word as it cannever be used alone.
On the other hand, taking it as amere suffix leads to many out-of-vocabulary wordsbecause of the productivity of such characters.
Forinstance, Penn Chinese Treebank (CTB6) contains???
?one that fails?
as a word but not ???
?one that succeeds?, even with the word ??
?suc-ceed?
appearing 207 times.
We call words like ???
?one that succeeds?
pseudo OOVs.
By defini-tion, pseudo OOVs are OOVs since they do not occurin the training corpus, though their components arefrequently-seen words.
Our estimation is that over60% of OOVs in Chinese are of this kind (Section 2).Of course, the way out of this dilemma is to parsethe internal structures of these words.
That is tosay, we can still regard characters like ?
as suf-fixes, taking into account the fact that they cannot beused alone.
Meanwhile, pseudo OOVs can be largelyeliminated through analyzing their structures, thusgreatly facilitating syntactic and semantic analysisof sentences.
In fact, previous studies have revealedother good reasons for parsing internal structures ofwords (Zhao, 2009; Li, 2011).The second argument is that in Chinese many lin-guistic units can form both words and phrases withexactly the same meaning and part-of-speech, which1445??
NN ??
NN ?
NNMOD MOD?
NN ?
NNMODFigure 1: Unified parsing of words and phrases.causes lots of incompatible annotations in currentlyavailable corpora.
Take character?
?law?
as an ex-ample.
It is head of both??
?criminal law?
and?????
?environmental protection law?, but CTBtreat it as a suffix in the former (with the annotationbeing ?
?_NN) and a word in the later (the anno-tation is??_NN??_NN?_NN).
These annota-tions are incompatible since in both cases the char-acter ?
?law?
bears exactly the same meaning andusage (e.g.
part-of-speech).
We examined severalwidely used corpora and found that about 90% ofaffixes were annotated incompatibly (Section 2).
In-compatibility can be avoided through parsing struc-tures of both words and phrases.
Figure 1 conveysthis idea.
A further benefit of unified parsing is toreduce data sparseness.
As an example, in CTB6??machine?
appears twice in phrases but 377 times inwords (e.g.
???
?accelerator?).
Word structuresin Chinese can be excellent guide for parsing phrasestructures, and vice versa, due to their similarity.The present paper makes two contributions inlight of these issues.
Firstly, in order to get rid ofpseudo OOVs and incompatible annotations, we haveannotated structures of words in CTB6, after whichstatistical models can learn structures of words aswell as phrases from the augmented treebank (Sec-tion 4).
Although previous authors have noticedthe importance of word-structure parsing (Li, 2011;Zhao, 2009), no detailed description about annota-tion of word structures has been provided in the liter-ature.
Secondly, we designed a unified dependencyparser whose input is unsegmented sentences andits output incorporates both morphological and syn-tactic structures with a single model and algorithm(Section 5).
By removing the intermediate step ofword segmentation, our unified parser no longer de-pends on the unsound notion that there is a clearboundary between words and phrases.
Evaluation(Section 6) shows that our unified parser achievessatisfactory accuracies in parsing both morphologi-cal and syntactic structures.corpus OOV pseudo percentCTB6 158 112 70.9MSR 1,783 1,307 73.3PKU 2,860 1,836 64.2AS 3,020 2,143 71.0CITYU 1,665 1,100 66.0Table 1: Statistics of pseudo OOVs for five corpora.2 Pseudo OOVs and IncompatibleAnnotationsIn this section we show the surprisingly pervasivenature of pseudo OOVs and incompatible annota-tions through analysis of five segmented corpora,which are CTB6 and corpus by MSR, PKU, ASand CITYU provided in SIGHAN word segmentationBakeoffs 2.First we use the standard split of training and test-ing data and extract all OOVs for each corpus, thencount the number of pseudo OOVs.
Table 1 gives theresult.
It?s amazing that for every corpus, over 60%of OOVs are pseudo, meaning they can be avoided iftheir internal structures were parsed.
Reduction ofOOVs at such a large scale can benefit greatly down-stream natural language processing systems.We then sample 200 word types containing a pro-ductive affix from each corpus, and check whetherthe affix also occurs somewhere else in a phrase,i.e, the affix is annotated as a word in the phrase.The results are in Table 2.
It?s clear and somewhatshocking that most affixes are annotated incompat-ibly.
We believe it is not the annotators to blame,rather the root cause lies deeply in the unique char-acteristics of the Chinese language.
This becomesobvious in comparison with English, where suffixlike ?-ism?
in ?capitalism?
cannot be used alone as aword in phrases.
3 Incompatible annotations canbe removed only through unified parsing of wordand phrase structures, as mentioned earlier and il-lustrated in Figure 1.2http://www.sighan.org/bakeoff2005/3Actually English allows examples like ?pre- and post-warimperialism?
where a prefix like ?pre?
can appear on its own aslong as the hyphen is present and it is in a coordination struc-ture.
Note that such examples are much rarer than what wediscuss in this paper for Chinese.
We thank the reviewer verymuch for pointing this out and providing this example for us.1446corpus incompatible percentCTB6 190 95MSR 178 89PKU 192 96AS 182 91CITYU 194 97Table 2: Statistics of incompatibly annotated affixes in200 sampled words for five segmented corpora.??
NR ?
NN ??
VV ??
NN ?
NNMOD MODOBJSUBJFigure 2: Example output of unified dependency parsingof Chinese morphological and syntactic structures.3 Unified Parsing DefinedGiven an unsegmented sentence ????????
?Gansu province attaches great importance to in-surance industry?, the output of unified dependencyparser is shown in Figure 2.
As can be seen, thisoutput contains information about word (such as?
?_VV) as well as phrase structures (such as ?
?_VV ?
?_NN ?_NN), which is what we meanby ?unified?
parsing.
Now, it?s no longer vital to dif-ferentiate between morphology and syntax for Chi-nese.
People could regard???
?insurance indus-try?
as a word or phrase, but either way, there will beno disagreements about its internal structure.
Fromthe perspective of the unified parser, linguistic unitsare given the same labels as long as they functionsimilarly (e.g, they have the same parts-of-speech).As a bonus, output of unified parsing incorpo-rates Chinese word segmentation, part-of-speechtagging and dependency parsing.
To achieve thesegoals, previous systems usually used a pipelinedapproach by combining several statistical models,which was further complicated by different decod-ing algorithms for each of these models.
The presentpaper shows that a single model does all these jobs.Besides being much simpler in engineering such aparser, this approach is also a lot more plausible formodeling human language understanding.4 Annotation of Word StructuresUnified parsing requires a corpus annotated withboth morphological and syntactic structures.
Sucha corpus can be built with the least effort if we be-gin with an existing treebank such as CTB6 alreadyannotated with syntactic structures.
It only remainsfor us to annotate internal structures of words in thistreebank.4.1 Scope of AnnotationIn order to get rid of pseudo OOVs and incompati-ble annotations, internal structures are annotated fortwo kinds of words.
The first kind contains wordswith a productive component such as suffix or pre-fix.
One example is???
?speaker?
whose suffixis the very productive?
?person?
(e.g, in CTB6 thereare about 400 words having this suffix).
The secondkind includes words with compositional semantics.Examples are ???
?Monday?
and ???
?Sun-day?.
Though ??
?week?
is not very productive,the meaning of words with this prefix is deduciblefrom semantics of their components.Other compound words such as ??
?research?have no productive components and are not a causeof pseudo OOVs.
They are universally consid-ered as words instead of phrases due to their non-compositional semantics.
Hence their structures arenot annotated in the present research.
Meanwhile,for single-morpheme words with no structures what-soever, like???
?Iraq?
and??
?bat?, annotationof internal structures is of course unnecessary either.Of all the 54, 214 word types in CTB6, 35% areannotated, while the percentage is 24% for the 782,901 word tokens.
Around 80% of sentences containwords whose structures need annotation.
Our anno-tations will be made publicly available for researchpurposes.4.2 From Part-of-speeches to ConstituentsOf all 33 part-of-speech tags in CTB, annotation ofword structures is needed for nine tags: NN, VV, JJ,CD, NT, NR, AD, VA and OD.
Since part-of-speechtags are preterminals and can only have one terminalword as its child, POS tags of words become con-stituent labels after annotation of word structures.The mapping rules from POS tags to constituent la-bels are listed in Table 3.
Readers should note that1447POS tags constituent labelNR, NN, NT NPJJ ADJPAD ADVPCD, OD QPVV, VA VPTable 3: Correspondence between POS tags and con-stituent labels after annotation.PP????P?NPNN????
PP????P?NP????NN?
?NN?Figure 3: Example annotation for the word ???
NNin CTB6: POS tag NN changes to constituent label NPafter annotation.such mapping is not arbitrary.
The constraint is thatin the treebank the POS tag must somewhere be theunique child of the constituent label.
Figure 3 de-picts an example annotation, in which we also havean example of NP having a tag NN as its only child.4.3 Recursive AnnotationSome words in CTB have very complex structures.Examples include ???????
?physicist ma-joring in nuclear physics?, ?????
?anti-trustlaws?
etc.
Structures of these words are anno-tated to their full possible depth.
Existence of suchwords are characteristic of the Chinese language,since they are further demonstrations of the blurredboundary between morphology and syntax.
A full-fledged parser is needed to analyze structures ofthese words, which incidentally provides us with an-other motivation for unified morphological and syn-tactic parsing of Chinese.5 Unified Dependency ParsingAll previous dependency parsers for Chinese take itfor granted that the input sentence is already seg-mented into words (Li et al 2011).
Most systemseven require words to be tagged with their part-of-speeches (Zhang and Nivre, 2011).
Hence currentoff-the-shelf algorithms are inadequate for parsingunsegmented sentences.
Instead, a new unified pars-ing algorithm is given in this section.5.1 TransitionsTo map a raw sentence directly to output shown inFigure 2, we define four transitions for the unifieddependency parser.
They act on a stack containingthe incremental parsing results, and a queue holdingthe incoming Chinese characters of the sentence:SHIFT: the first character in the queue is shifted intothe stack as the start of a new word.
The queueshould not be empty.LEFT: the top two words of the stack are connectedwith an arc, with the top one being the head.
Thereshould be at least two elements on the stack.RIGHT: the top two words of the stack are con-nected, but with the top word being the child.
Theprecondition is the same as that of LEFT.APPEND: the first character in the queue is appendedto the word at the top of the stack.
There are twopreconditions.
First, the queue should not be empty.Second, the top of the stack must be a word with noarcs connected to other words (i.e, up to now it hasgot neither children nor parent).We see that these transitions mimic the general arc-standard dependency parsing models.
The first threeof them were used, for example, by Yamada andMatsumoto (2003) to parse English sentences.
Theonly novel addition is APPEND, which is necessarybecause we are dealing with raw sentences.
Its solepurpose is to assemble characters into words withno internal structures, such as ???
?Seattle?.Thus this transition is the key for removing the needof Chinese word segmentation and parsing unseg-mented sentences directly.To also output part-of-speech tags and depen-dency labels, the transitions above can be aug-mented accordingly.
Hence we can change SHIFT toSHIFT?X where X represents a certain POS tag.
Also,LEFT and RIGHT should be augmented with appro-priate dependency relations, such as LEFT?SUBJ fora dependency between verb and subject.As a demonstration of the usage of these tran-sitions, consider sentence ??????
?I loveSeattle?.
Table 4 lists all steps of the parsing pro-cess.
Readers interested in implementing their own1448step stack queue action1 ??????
SHIFT?PN2 ?
PN ?????
SHIFT?VV3 ?
PN?
VV ????
APPEND4 ?
PN??
VV ???
LEFT?SUBJ5 ?
PN SUBJ??????
VV ???
SHIFT?NR6 ?
PN SUBJ??????
VV?
NR ??
APPEND7 ?
PN SUBJ??????
VV??
NR ?
APPEND8 ?
PN SUBJ??????
VV???
NR RIGHT?OBJ9 ?
PN SUBJ??????
VV OBJ??????
NR STOPTable 4: Parsing process of a short sentence with the four transitions defined above.unified dependency parsers are invited to study thisexample carefully.5.2 ModelDue to structural ambiguity, there might be quite alot of possibilities for parsing a given raw sentence.Hence at each step in the parsing process, all fourtransitions defined above may be applicable.
To re-solve ambiguities, each candidate parse is scoredwith a global linear model defined as follows.For an input sentence x, the parsing result F (x) isthe one with the highest score in all possible struc-tures for this x:F (x) = argmaxy?GEN(x)Score(y) (1)Here GEN(x) is a set of all possible parses for sen-tence x, and Score(y) is a real-valued linear func-tion:Score(y) = ?
(y) ?
~w (2)where ?
(y) is a global feature vector extracted fromparsing result y, and ~w is a vector of weighting pa-rameters.
Because of its linearity, Score(y) can becomputed incrementally, following the transition ofeach parsing step.
Parameter vector ~w is trainedwith the generalized perceptron algorithm of Collins(2002).
The early-update strategy of Collins andRoark (2004) is used so as to improve accuracy andspeed up the training.5.3 Feature TemplatesFor a particular parse y, we now describe the wayof computing its feature vector ?
(y) in the linearDescription Feature Templates1 top of S S0wt; S0w; S0t2 next top of S S1wt; S1w; S1t3 S0 and S1 S1wtS0wt; S1wtS0wS1wS0wt; S1wtS0tS1tS0wt; S1wS0w; S1tS0t4 char unigrams Q0; Q1; Q2; Q35 char bigrams Q0Q1; Q1Q2; Q2Q36 char trigrams Q0Q1Q2; Q1Q2Q37 ST+unigrams STwtQ0; STwQ0; STtQ08 ST+bigrams STwtQ0Q1; STwQ0Q1STtQ0Q19 ST+trigrams STwtQ0Q1Q2STwQ0Q1Q2; STtQ0Q1Q210 parent P of ST PtSTtQ0; PtSTtQ0Q1PtSTtQ0Q1Q211 leftmost child STtLCtQ0; STtLCtQ0Q1LC and STtLCtQ0Q1Q2rightmost STtRCtQ0; STtRCtQ0Q1child RC STtRCtQ0Q1Q2Table 5: Transition-based feature templates.
Q0 is thefirst character in Q, etc.
w = word, t = POS tag.model of Equation (2).
If S denotes the stack hold-ing the partial results, and Q the queue storing theincoming Chinese characters of a raw sentence, thentransition-based parsing features are extracted fromS and Q according to those feature templates in Ta-ble 5.Although we employ transition-based parsing,nothing prevents us from using graph-based fea-tures.
As shown by Zhang and Clark (2011), depen-1449Description Feature Templates1 parent word Pwt; Pw; Pt2 child word Cwt; Cw; Ct3 P and C PwtCwt; PwtCw; PwCwtPtCwt; PwCw; PtCtPwtCt4 neighbor word PtPLtCtCLt; PtPLtCtCRtof P and C PtPRtCtCLt; PtPRtCtCRtleft (L) or PtPLtCLt; PtPLtCRtright (R) PtPRtCLt; PtPRtCRtPLtCtCLt; PLtCtCRtPRtCtCLt; PRtCtCRtPtCtCLt; PtCtCRtPtPLtCt; PtPRtCt5 sibling(S) of C CwSw;CtSt; CwStCtSw; PtCtSt6 leftmost and PtCtCLCtrightmost child PtCtCRCt7 left (la) and Ptla; Ptraright (ra) Pwtla; Pwtraarity of P Pwla; PwraTable 6: Graph-based feature templates for the unifiedparser.
Most of these templates are adapted from thoseused by Zhang and Clark (2011).
w = word; t = POS tag.dency parsers using both transition-based and graph-based features tend to achieve higher accuracy thanparsers which only make use of one kind of features.Table 6 gives the graph-based feature templates usedin our parser.
All such templates are instantiated atthe earliest possible time, in order to reduce as muchas possible situations where correct parses fall out ofthe beam during decoding.5.4 Decoding AlgorithmWe use beam-search to find the best parse for a givenraw sentence (Algorithm 1).
This algorithm usesdouble beams.
The first beam contains unfinishedparsing results, while the second holds completedparses.
Double beams are necessary because thenumber of transitions might well be different for dif-ferent parses, and those parses that finished earlierare not necessarily better parses.
During the search-ing process, correct parse could fall off the beams,resulting in a search error.
However, in practicebeam search decoding algorithm works quite well.In addition, it?s not feasible to use dynamic program-ming because of the complicated features used in themodel.The B in Algorithm 1 is the width of the twobeams.
In our experiments we set B to 64.
Thisvalue of B was determined empirically by using thestandard development set of the data, with the goalof achieving the highest possible accuracy withinreasonable time.
Note that in line 20 of the algo-rithm, the beam for completed parsers are pruned ateach iteration of the parsing process.
The purposeof this action is to keep this beam from growing toobig, resulting in a waste of memory space.Algorithm 1 Beam Search Decoding1: candidates?
{STARTITEM()}2: agenda?
?3: completed?
?4: loop5: for all candidate in candidates do6: for all legal action of candidate do7: newc?
EXPAND(candidate, action)8: if COMPLETED(newc) then9: completed.INSERT(newc)10: else11: agenda.INSERT(newc)12: end if13: end for14: end for15: if EMPTY(agenda) then16: return TOP(completed)17: end if18: candidates?
TOPB(agenda, B)19: agenda?
?20: completed?
TOPB(completed, B)21: end loop6 Experiments and EvaluationWe describe the experiments carried out and ourmethod of evaluation of the unified dependencyparser.
We used Penn2Malt 4 to convert constituenttrees of CTB to dependency relations.
The head rulesfor this conversion was given by Zhang and Clark(2008).
In all experiments, we followed the stan-4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html1450P R Four method, labeled 78.54 80.93 79.72our method, unlabeled 81.01 83.77 82.37ZC2011, unlabeled N/A N/A 75.09Table 7: Evaluation results on the original CTB5.
N/Ameans the value is not available to us.
ZC2011 is Zhangand Clark (2011).dard split of the data into training, testing and devel-opment data (Zhang and Clark, 2011).
Though weannotated structures of words in CTB6, most previ-ously results were on CTB5, a subset of the formertreebank.
Hence we report our results of evaluationon CTB5 for better comparability.6.1 Dependency Parsing of Morphological andSyntactic StructuresIf we look back at the Figure 2, it?s clear that adependency relation is correctly parsed if and onlyif three conditions are met: Firstly, words at bothends of the dependency are correctly segmented.Secondly, part-of-speech tags are correct for bothwords.
Thirdly, the direction of the dependency re-lation are correct.
Of course, if labeled precision andrecall is to be measured, the label of the dependencyrelation should also be correctly recovered.
Let ncbe the number of dependencies correctly parsed withrespect to these criterion, no be the total number ofdependencies in the output, and nr the number ofdependencies in the reference.
Then precision is de-fined to be p = nc/no and recall is defined to ber = nc/nr.6.1.1 Results on the Original CTB5We first train our unified dependency parser withthe original treebank CTB5.
In this case, all wordsare considered to be flat, with no internal structures.The result are shown in Table 7.
Note that on ex-actly the same testing data, i.e, the original CTB5,unified parser performs much better than the resultof a pipelined approach reported by Zhang and Clark(2011).
There are about 30% of relative error reduc-tion for the unlabeled dependency parsing results.This is yet another evidence of the advantage of jointmodeling in natural language processing, details ofwhich will be discussed in Section 7.P R Foriginal dependencies 82.13 84.49 83.29in CTB5ZN2011 with Gold N/A N/A 84.40segmentation & POSoriginal dependencies 85.71 87.18 86.44plus word structuresTable 8: Evaluation results on CTB5 with word structuresannotated.
All results are labeled scores.6.1.2 Results on CTB with Structures of WordsAnnotatedThen we train the parser with CTB5 augmentedwith our annotations of internal structures of words.For purpose of better comparability, we report re-sults on both the original dependencies of CTB5 andon the dependencies of CTB5 plus those of the in-ternal structures of words.
The results are shownin Table 8.
First, note that compared to another re-sult by Zhang and Nivre (2011), whose input weresentences with gold standard word segmentation andPOS tags, our F-score is only slightly lower evenwith input of unsegmented sentences.
This is un-derstandable since gold-standard segmentation andPOS tags greatly reduced the uncertainty of parsingresults.For the unified parser, the improvement of F-scorefrom 79.72% to 83.29% is attributed to the fact thatwith internal structures of words annotated, parsingof syntactic structures is also improved due to thesimilarity of word and phrase structures mentionedin Section 1, and also due to the fact that manyphrase level dependencies are now facing a muchless severe problem of data sparsity.
The improve-ment of F-score from 83.29% to 86.44% is attributedto the annotation of word structures.
Internal struc-tures of words are be mostly local in comparisonwith phrase and sentence structures.
Therefore, withthe addition of word structures, the overall depen-dency parsing accuracy naturally can be improved.6.2 Chinese Word SegmentationFrom the example in Figure 2, it is clear that outputof unified parser contains Chinese word segmenta-tion information.
Therefore, we can get results ofword segmentation for each sentence in the test sets,1451P R FK2009 N/A N/A 97.87This Paper 97.63 97.38 97.50Table 9: Word segmentation results of our parser andthe best performance reported in literature on the samedataset.
K2009 is the result of Kruengkrai et al(2009).P R FK2009 N/A N/A 93.67ZC2011 N/A N/A 93.67This Paper 93.42 93.20 93.31Table 10: Joint word segmentation and POS taggingscores.
K2009 is result of Kruengkrai et al(2009).ZC2011 is result of Zhang and Clark (2011).and evaluate their accuracies.
For maximal compa-rability, we train the unified parser on the originalCTB5 data used by previous studies.
The result isin Table 9.
Despite the fact that the performanceof our unified parser does not exceed the best re-ported result so far, which probably might be causedby some minute implementation specific details, it?sfair to say that our parser performs at the level ofstate-of-the-art in Chinese word segmentation.6.3 Joint Word Segmentation and POS TaggingFrom Figure 2 we see that besides word segmenta-tion, output of the unified parser also includes part-of-speech tags.
Therefore, it?s natural that we evalu-ate the accuracy of joint Chinese word segmentationand part of speech tagging, as reported in previousliterature (Kruengkrai et al 2009).
The results arein Table 10, in which for ease of comparison, againwe train the unified parser with the vanilla versionof CTB5.
We can see that unified parser performs atvirtually the same level of accuracy compared withprevious best systems.7 Related WorkResearchers have noticed the necessity of parsingthe internal structures of words in Chinese.
Li(2011) gave an method that could take raw sentencesas input and output phrase structures and internalstructures of words.
This paper assumes that the in-put are unsegmented, too, and our output also in-cludes both word and phrase structures.
There are?
?
?
?
?
?
?
?Figure 4: Example output of Zhao?s parser.two key differences, though.
The first is we outputdependency relations instead of constituent struc-tures.
Although dependencies can be extracted fromthe constituent trees of Li (2011), the time complex-ity of their algorithm is O(n5) while our parser runsin linear time.
Secondly, we specify the details ofannotating structures of words, with the annotationsbeing made publicly available.Zhao (2009) presented a dependency parser whichregards each Chinese character as a word and thenanalyzes the dependency relations between charac-ters, using ordinary dependency parsing algorithms.Our parser is different in two important ways.
Thefirst is we output both part-of-speech tags and la-beled dependency relations, both of which were ab-sent in Zhao?s parser.
More importantly, the AP-PEND transition for handling flat words were unseenin previous studies as far as we know.
The differencecan best be described with an example: For the sen-tence in Section 3, Zhao?s parser output the result inFigure 4 while in contrast our output is Figure 2.In recent years, considerable efforts have beenmade in joint modeling and learning in natural lan-guage processing (Lee et al 2011; Sun, 2011; Li etal., 2011; Finkel and Manning, 2009; Kruengkrai etal., 2009; Jiang et al 2008; Goldberg and Tsarfaty,2008).
Joint modeling can improve the performanceof NLP systems due to the obvious reason of beingable to make use of various levels of information si-multaneously.
However, the thesis of this paper, i.e,unified parsing of Chinese word and phrase struc-tures, bears a deeper meaning.
As demonstrated inSection 1 and by Li (2011), structures of words andphrases usually have significant similarity, and thedistinction between them is very difficult to define,even for expert linguists.
But for real world applica-tions, such subtle matters can safely be ignored if wecould analyzed morphological and syntactic struc-tures in a unified framework.
What applications re-ally cares is structures instead of whether a linguisticunit is a word or phrase.1452Another notable line of research closely relatedto the present work is to annotate and parse the flatstructures of noun phrases (NP) (Vadas and Curran,2007; Vadas and Curran, 2011).
This paper dif-fers from those previous work on parsing NPs in atleast two significant ways.
First, we aim to parseall kinds of words (e.g, nouns, verbs, adverbs, ad-jectives etc) whose structures are not annotated byCTB, and whose presence could cause lots of pseudoOOVs and incompatible annotations.
Second, theproblem we are trying to solve is a crucial obser-vation specific to Chinese language, that is, in lotsof cases forcing a separation of words and phrasesleads to awkward situations for NLP systems.
Re-member that in Section 2 we demonstrated that allcorpora we examined had the problem of pseudoOOVs and incompatible annotations.
In comparison,the problem Vadas and Curran (2007) tried to solveis a lack of annotation for structures of NPs in cur-rently available treebanks, or to put it in another way,a problem more closely related to treebanks ratherthan certain languages.8 Discussion and ConclusionChinese word segmentation is an indispensable stepfor traditional approaches to syntactic parsing ofChinese.
The purpose of word segmentation is todecide what goes to words, with the remaining pro-cessing (e.g, parsing) left to higher level structuresof phrases and sentences.
This paper shows that itcould be very difficult to make such a distinctionbetween words and phrases.
This difficulty cannotbe left unheeded, as we have shown quantitativelythat in practice it causes lots of real troubles such astoo many OOVs and incompatible annotations.
Weshowed how these undesirable consequences can beresolved by annotation of the internal structures ofwords, and by unified parsing of morphological andsyntactic structures in Chinese.Unified parsing of morphological and syntacticstructures of Chinese can also be implemented witha pipelined approach, in which we first segment in-put sentences into words or affixes (i.e, with thefinest possible granularity), and then we do part-of-speech tagging followed by dependency (or con-stituent) parsing.
However, a unified parsing ap-proach using a single model as presented in thispaper offers several advantages over pipelined ap-proaches.
The first one is that joint modeling tendsto result in higher accuracy and suffer less from er-ror propagation than do pipelined methods.
Sec-ondly, both the unified model and the algorithmare conceptually much more simpler than pipelinedapproaches.
We only need one implementation ofthe model and algorithm, instead of several ones inpipelined approaches.
Thirdly, our model and al-gorithm might comes closer to modeling the pro-cess of human language understanding, because hu-man brain is more likely a parallel machine in un-derstanding languages than an alternative pipelinedprocessor.
Hence this work, together with previ-ous studies by other authors like Li (2011) and Zhao(2009), open up a possibly new direction for futureresearch efforts in parsing the Chinese language.AcknowledgmentsReviewers of this paper offered many detailed andhighly valuable suggestions for improvement in pre-sentation.
The authors are supported by NSFC underGrant No.
90920004 and National 863 Program un-der Grant No.
2012AA011102.ReferencesMichael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages326?334, Boulder, Colorado, June.
Association forComputational Linguistics.Yoav Goldberg and Reut Tsarfaty.
2008.
A single gener-ative model for joint morphological segmentation andsyntactic parsing.
In Proceedings of ACL-08: HLT,pages 371?379, Columbus, Ohio, June.
Associationfor Computational Linguistics.1453C.
F. Hockett.
1969.
A Course in Modern Linguistics.Macmillan.Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.2008.
A cascaded linear model for joint Chinese wordsegmentation and part-of-speech tagging.
In Proceed-ings of ACL-08: HLT, pages 897?904, Columbus,Ohio, June.
Association for Computational Linguis-tics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint Chinese word segmentation and POStagging.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP, pages 513?521, Suntec, Singapore,August.
Association for Computational Linguistics.John Lee, Jason Naradowsky, and David A. Smith.
2011.A discriminative model for joint morphological disam-biguation and dependency parsing.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 885?894, Portland, Oregon, USA, June.Association for Computational Linguistics.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu, Wen-liang Chen, and Haizhou Li.
2011.
Joint models forChinese POS tagging and dependency parsing.
In Pro-ceedings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing, pages 1180?1191, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Zhongguo Li.
2011.
Parsing the internal structure ofwords: A new paradigm for Chinese word segmenta-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1405?1414, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Weiwei Sun.
2011.
A stacked sub-word model forjoint chinese word segmentation and part-of-speechtagging.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies, pages 1385?1394, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.David Vadas and James Curran.
2007.
Adding nounphrase structure to the penn treebank.
In Proceed-ings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 240?247, Prague,Czech Republic, June.
Association for ComputationalLinguistics.David Vadas and James R. Curran.
2011.
Parsing nounphrases in the penn treebank.
Computational Linguis-tics, 37(4):753?809.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal dependency analysis with support vector machines.In Proceeding of the 8th International Workshop ofParsing Technologies (IWPT), pages 195?206, Nancy,France.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-based andtransition-based dependency parsing.
In Proceedingsof the 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 562?571, Honolulu,Hawaii, October.
Association for Computational Lin-guistics.Yue Zhang and Stephen Clark.
2011.
Syntactic process-ing using the generalized perceptron and beam search.Computational Linguistics, 37(1):105?151.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Hai Zhao.
2009.
Character-level dependencies in Chi-nese: Usefulness and learning.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), pages 879?887, Athens, Greece, March.Association for Computational Linguistics.1454
