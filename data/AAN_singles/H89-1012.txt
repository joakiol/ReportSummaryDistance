THE BBN SPOKEN LANGUAGE SYSTEMSean Boisen Yen-Lu Chow Andrew Haas Robert Ingria Salim RoukosBBN Systems and Technologies Corporation10 Moulton StreetMailstop 009Cambridge, MA 02238David StallardABSTRACTWe describe HARC, a system for speech understand-ing that integrates peech recognition techniques withnatural language processing.
The integrated systemuses statistical pattern recognition to build a lattice ofpotential words in the input speech.
This word latticeis passed to a unification parser to derive all possibleassociated syntactic structures for these words.
Theresulting parse structures are passed to a multi-levelsemantics component for interpretation.INTRODUCTIONHARC, the BBN Spoken Language System (Boisen,et al (1989)) is a system for speech understandingthat integrates speech recognition techniques withnatural language processing.
As our integrationmethodology, we use lallice parsing.
In this architec-ture, an acoustic processor produces a lattice of pos-sible words that is passed to a parser which producesall possible parses for all syntactically permissibleword sequences present in the lattice.
These parsetrees are then passed to a semantic interpretationcomponent, which produces the possible interpreta-tions of these parse structures, filtering out anomalousreadings where possible.THE ARCHITECTURE OF HARCIn this section, we present a more detailed outline ofthe general architecture of HARC:1.
An acoustic processor, which uses context-dependent Hidden Markov Models (HMMs)for acoustic modelling, produces a lattice ofpossible words in the input speech.2.
A chart parser uses a unification grammar toparse the word lattice and produces the setof all possible parses for all syntactically per-missible word sequences in the lattice.
Theresulting parses are ranked by acousticlikelihood score.3.
A multi-level semantics componentprocesses the parse trees?
This component1This architecture and the names of the associated language levelsare from the PHLIQA1 system (Bronnenberg et al 1980).uses 4 translation steps to derive the mean-ing of each parse.a.
The parse tree is converted to an expres-sion of EFL (English-oriented FormalLanguage); at this level, each word hasone EFL constant; this includes wordswithmultiple senses.b.
Each EFL expression is translated intoone or more expressions of WML (WorldModel Language).
Where possible, am-biguous constants from an EFL expres-sion are disambiguated and logicallyequivalent EFL expressions are col-lapsed.c.
Each WML expression is converted to anexpression in DBL (Data BaseLanguage), which contains one constantfor each file in the data base.d.
The value of each DBL expression iscomputed by evaluating the expressionagainst the database; this value is ex-pressed in CVL (Canonical ValueLanguage).For speech understanding, semantics identifies thehighest scoring "meaningful" sentence.
This sen-tence is the recognized spoken utterance and itsmeaning is the sytem's interpretation of the input.TRAINING AND TEST SETSTo measure the coverage of the syntactic and seman-tic components and the speech understanding perfor-mance of the integrated system, we use the DARPA1000-word Resource Management Database corpus.This corpus is divided into two sets of sentences: 2 atraining corpus of 791 sentences and a test corpus of2The DARPA database has well.defined training and test sets forthe speech data.
However, for natural language development work,there is no such such well-defined ivision.
For the purpose ofevaluating natural anguage work, we defined at 8BN a trainingcorpus of 791 sentences, based on 791 patterns, and a test corpus of200 sentences, based on an independently selected set of 200patterns.
We feel these two corpora re a reasonable interim solutionfor the language modelling problem in the DARPA ResoumeManagement domain.106200 sentences.
Syntax and semantics developmentwork is done on the basis of the training corpus~ thetest corpus is kept hidden from the systemdevelopers, to simulate novel utlerances that users ofthe system might make.
Periodically, the test corpusis run through the system, again without thedevelopers looking at any of the sentences.
However,statistics are collected on the percentage of sen-tences successfully processed; this number can becompared to the percentage of the training corpusprocessed, to see how well the system generalizesfrom the training (known) to test (unknown) corpus.Subsequent sections of this paper present coverageresults on both training and test sets for each com-ponent of the system.THE ACOUSTIC PROCESSORSince the speech understanding search in our systemis decoupled into two phases--speech acoustic scor-ing and language model scoring, to do this overallsearch most efficiently, we need to ensure that suf-ficient computing is performed in the first stage andenough information is saved so that optimality ispreserved in the later stages of processing.To this end, our lattice computation algorithm at-tempts, in principle, to compute acoustic likelihoodscores for all words in the vocabulary V for all timeintervals t I and t 2.
The acoustic data is typically asequence of analyzed and vector-quantized (VQ) in-put spectra sampled every 10 milliseconds (Chow, etal.
1987).
We model the input speech at the phoneticlevel using robust context-dependent HMMs of thephoneme.
The acoustic model for each word in thevocabulary is then derived from the concatenation ofthese phonetic HMMs.
Using these acoustic modelsof the word, one can compute the acoustic scores forall words in the input utterance using a time-synchronous dynamic time warping (DTW) procedurewith beam pruning.An integral part of the task of the acoustic processoris to produce a word lattice that can be processedefficiently by the lattice parser.
To do this, we reducethe lattice size through various lattice pruning tech-niques.
We have used three pruning techniques,which we describe here briefly.
(For full details, seeBoisen, et al (1989).
)Score Thresholding:Word hypotheses are pruned on the basis of theunit score: the hypothesis' acoustic score normal-ized by its duration; the goal is to keep only thoseacoustic theories with a unit score greater thansome predetermined threshold, and eliminate allothers.
In practice, we found it nearly impossibleto find a single threshold that works for all wordsand have adopted a strategy that uses dualthresholds---one for short, function words andanother for longer, multi-syllabic words.Subsumption Pruning:Subsumption pruning is designed to explicitlydeal with the problem of short, function words,which are acoustically unreliable and which areoften found throughout the speech signal, evenwithin longer words..
Since it is almost alwaysthe case that short words match parts of longwords, not vice versa, word theories that arefound completely inside another word theory, withunit score below some factor 13 of the parenttheory, are eliminated from the word lattice.Forward-Backward Pruning:Forward-backward pruning is based on thefamiliar forward-backward algorithm for estimat-ing the parameters of HMMs; it requires that a/Iacoustic theories must be part of a complete paththrough the lattice, and furthermore, must scorereasonably well.Rather then determining the optimal pruning tech-nique and using it alone, the system uses these tech-niques in tandem to try to produce the optimal wordlattice in terms of size and information content,THE SYNTACTIC COMPONENTTHE GRAMMAR FORMALISMHARC uses a grammar formalism based on an-notated phrase structure rules; this formalism is calledthe BBN ACFG (for Annotated Context FreeGrammar).
While it is in the general tradition of aug-mented phrase structure grammars, its immediate in-spiration is Definite Clause Grammars (DCGs)(Pereira & Warren (1980)).
In such grammars, rulesare made up of elements that are not atomiccategories but, rather, are complex symbols consist-ing of a category label and feature specifications.Features (also called arguments) may be eitherconstants---indicated by lists in the BBN ACFG--orvariables---indicated by symbols with a leading colon.Identity of variables in the different elements of a ruleis used to enforce agreement in the feature indicatedby the variable.
An example is (features to be dis-cussed are underlined):(s (oco~) :MOOD (Wa-) ...)(NP :NSUBCATFRAME :AGR :NPTYPE .
..)(W :AGR :NPTYPE :MOOD ...)(OPTSADJUNCT :AGR ...)where the variable :AGR enforces agreement be-tween the VP (ultimately, its head V) and the subjectNP; :NPT'Z'PE, agreement between the syntactic typeof the subject NP and that selected by the head V ofthe VP; and :MOOD, agreement between the mood ofthe S and that of the VP.107In the BBN ACFG, as in DCGs, each grammaticalcategory has a fixed number of obligatory, positionalarguments.
The essential difference between our for-malism and DCGs is a syntactic typing system,whereby each argument position is limited to a fixednumber of values.
We have found that this restriction,in conjunction with the obligatory and positional natureof arguments, to be of great assistance in developinga large grammar (currently over 800 rules).
By es-chewing more sophisticated mechanisms such as fea-ture disjunction, feature negation, metarules, optionalarguments, and the use of attribute-value pairsmasare found in other complex feature based grammars,such as GPSG (Gazdar, et al(1985)), LFG (Bresnan(1982)), and PATR-II (Shieber, at al.
(1983))--it isrelatively straightforward to have a simple syntacticchecker that ensures that all grammar rules are well-formed.
In a grammar as large as the BBN ACFG,having the ability to automatically make sure that allrules are well-formed is no small advantage.
Wehave so far found no need for most of the advancedfacets of other complex feature based formalisms,with the possible exception of disjunction, which willprobably be added in a restricted form.An additional difference between our work and stan-dard DCGs is a depth-boundedness restriction, whichis discussed in the next section.THE PARSING ALGORITHMThe BBN Spoken Language System uses a parsingalgorithm which is essentially that of Graham, Har-rison, and Ruzzo (1980), henceforth, GHR.
This algo-rithm, in turn, is based on the familiar Cocke-Kasami-Younger (CKY) algorithm for context-free grammars.The CKY algorithm is quite simple and powerful: itstarts with the terminal elements in a sentence andbuilds successively larger constituents that containthose already found and constructs all possibleparses of the input.
However, while the CKY algo-rithm requires that each rule introducing non-terminalsymbolsmessentially the parts of speech, as op-posed to the terminal symbols (lexical items andgrammatical formatives)--be in Chomsky NormalForm (i.e.
of the form ~ ~ B C, with exactly twonon-terminal symbols on the right hand side), theGHR algorithm uses several mechanisms, includingtables and "dotted rules", to get around this restric-tion.
Since the GHR algorithm, like the CKY algo-rithm, deals with context-free grammars, rather thancontext-free grammars annotated with features, theuse of the required feature substitutionmechanism--unification--is an extension to theGHR algorithm; see Haas (1987) for full details.One useful result of our work on extending the GHRalgorithm to handle annotated context free grammars(ACFGs) is the discovery that there is a class ofACFGs, depth-bounded ACFGs, for which the parsingalgorithm is guaranteed to find all parses and halt(Haas (1989)).
Depth-bounded ACFGs are charac-terized by the property that the depth of a parse treecannot grow unboundedly large unless the length ofthe string also increases.
In effect, such grammars donot permit rules in which a category derives only itselfand no other children; such rules do not seem to beneeded for the analysis of natural languages, so com-putational tractability is maintained without sacrificinglinguistic coverage.
The fact that the parsing algo-rithm for this class of ACFGs halts is a useful result,since parsers for complex feature based grammarscannot be guaranteed to halt, in the general case.
Byrestricting our grammars to those that satisy depth-boundedness, we can be sure that we can parse inpututterances bottom-up and find all parses without theparser going into an infinite loop.CONSTRAINING SYNTACTIC AIVIBIGUITYSince the BBN ACFG parser finds all the parses for agiven input, there is a potential problem regarding thenumber of parses that are found for each input ut-terance.
Our experience has been that while theaverage number of parses per sentence is usuallyquite reasonable (about 2), in cases of conjunction orellipsis the number of parses can grow wildly.
In or-der to obtain broad coverage without explosive am-biguity, we have experimented with a version of theparser in which rules are sorted into different levels ofgrammaticality.
In this version of the parser, parsesare ranked according to the rules utilized.
Initial ef-forts, in which ranks were assigned to rules by hand,are encouraging.
A version of the grammar whichincluded rules such as determiner ellipsis that in-creased ambiguity, had an average of 18 parses persentence and a mode of 2 parses.
However, whenonly first order parses were considered the averagewas 2.86 parses and the mode was 1.
The parserwithout the extra rules and without ranking has anaverage of 3.95 parses and a mode of 1.We have also experimented with utilizing statisticalmethods of assigning probabilities to rules, based onthe frequency of occurrence of the rules of the gram-mar in the training corpus.
Testing the results of thisautomatic assignment against a corpus of 48 sen-tences (from the training corpus) that were manuallyparsed, the parse assigned the top score by theprobabilistic parser was correct 77% of the time.Looking only at the top 6 parses, the correct parsewas present 96% of the time.
Since the success rateis 96% considering only the top 6 parses, while 50%of the sentences have 6 or more parses, this suggeststhat this probabalistic approach is on the right track.108SYNTACTIC COVERAGEThe current ACFG grammar contains 866 rules: ofthese, 424 introduce grammatical formatives (such asthe articles "a", "the", prepositions, etc).
The remain-ing rules handle the general syntactic constructions ofEnglish.
Coverage on the training corpus is currentlyg1% and coverage of the test corpus is 81% with thisgrammar.
The version of the grammar used by theparser that utilizes ranked rules contains 873 rules.Coverage with this version of the grammar is 94% ontraining and 88% on test.THE SEMANTIC COMPONENTAs a previous section noted, the semantic processingof an input query takes place in several stages.
First,the output of the parser, the parse tree, is passed tothe structural semantic module.
This produces anexpression of the logical language EFL, which may beambiguous.
The second stage of processing acceptsas input an expression of EFL and returns as outputzero or more expressions of the logical languageWML.
The EFL translation is concerned with struc-tural semantics--in other words, just the effect ofsyntactic structure on meaning.
The WML translationis concerned with lexical semantics--the meaning (ina given domain) of particular words.The third steps converts the WML expression to anexpression of DBL.
This translation step maps be-tween the logical structure most natural in describingan application domain and the actual structure ofdatabase files.
Finally, the answer to a databasequery in DBL is expressed in a formula of CVL.THE LOGICAL LANGUAGESEach of the logical languages just mentioned---EFL,WML, DBL and CVL---is derived from a common logi-cal language from which each differs only in the par-ticular constant symbols which are allowed and not inthe operators (the only .exception is CVL, whoseoperators are a subset of the operators of this com-mon language).This logical language has three main properties.First, it is higher-order, which means that it can quan-tify not only over individuals, but over sets, tuples andfunctions as well.
Second, it is intensional, whichmeans that the denotations of its expressions are as-signed relative to external indices of world and time,and it incorporates an "intension" operator whichdenotes the "sense" (with respect to these indices) ofan expression.
Third, the languge has a rich systemof types which are used to represent semantic selec.tional restrictions and so serve to delimit the set ofmeaningful expressions in the language.STRUCTURAL SEMANTICSThe structural semantic component uses a set ofstructural semantic rules, paired one for one with thesyntactic rules of the grammar.
An example of theserules is given below: 3S --~ NP  VP  OPTSADJUNCT(lambda (np vp oa)(oa (intension ( (q np) vp) ) ) )This is the top-level declarative clause rule given ear-lier, with its corresponding semantic rule.
Note thatthere are three variables bound in the lambcla--np,vp oa--corresponding to the three terms on the right-hand side of the syntactic rule---NP, VP, andOPTSADJUNCT.
During semantic interpretation, thesemantic translations of these right-hand terms aresubstituted in for the variables np, vp and oa to makethe interpretation of the whole clause.The effect of this rule is to construct a propositioncorresponding to the application of the predicate ofthe clause--the VP- - to  the subject of theclause---the NP.
This proposition is modified by theoptional sentential adjunct, whose semantic trans-lation is applied to it.
Examples of sentential adjunctsare phrases such as "during April", and "in Africa", aswell as adverbs and more complicated modifiers.LEXlCAL SEMANTICSThe lexical semantic component is concerned with thespecific meanings of a word in a subject domain, asopposed to the manner in which these meanings arecombined.
These specific meanings are representedby expressions of WML which are associated by ruleswith the constant symbols of EFL.A recursive-descent ranslation algorithm returns foreach node of an EFL expression a set of WML trans-lations.
The translations for a constant expression arejust those dictated by its WML translation rule.
Thetranslations for a complex expression are derived bycombining, in a cartesian product fashion, the trans-lations of the parts of the expression.
At each level,the set of possible translations is filtered to removeanomalous translations---those which involve com-binations of WML expressions with incompatiblesemantic types.Sentences which are semantically ambiguous (suchas "They went to the bank") will simply return multipleWML translations.
Sentences which have no non-anomalous WML translation (and are therefore con-sidered meaningless by the system) will return theempty set as the result of WML translation.3Feature specifications are omitted here for conciseness.109SEMANTIC COVERAGECurrently, the semantics is able to map 75% of thetraining sentences and 52% of the test sentences to aWML expression.
The corresponding figures for CVLare 44% for training and 32% for test.THE LATTICE PARSERThe basic approach we have taken for speech under-standing is to extend the text parser to deal withspoken input.
So instead of operating on typed input,where a single word appears unambiguously at eachposition, the parser must now deal with speech input,which is highly ambiguous: a set of words is possibleat every position with varying acoustic likelihoodscores.
While the data structure which is the input tothe text parser is relatively simple---a list ofwords---the input to the lattice parser is a lattice of allthe possible words which have been found.
As-sociated with each word in the lattice (and thereforeeach grammatical constituent) is a set of acousticmatch scores, with each score corresponding to par-ticular starting and ending times.Parsing now consists of building larger grammaticalconstituents from smaller ones and also keeping trackof the resulting acoustic scores as the new con-stituents span longer intervals.
The parser buildsthese larger constituents word synchronously in a waysimilar to the text parser.
Two parsing algorithmshave been implemented for the lattice parser: the firstis a relatively straightforward modification of the textparser's algorithm to deal with the input word lattice.The second is similar, but supplements the textparser's algorithm with a top-down predictive filteringcomponent.
This significantly reduces the computa-tional complexity of the lattice parsing algorithm; seeChow & Roukos (1989) for full details.Currently, in the integrated speech understanding sys-tem, just as in the text processing system, the seman-tics component is applied after all parsing is done andis not interleaved with the parsing.
While there arepossible disadvantages to this approachmmore com-putation may be done since semantic knowledge isnot applied until late in processingmwe have chosenthis method of integration as our first attempt sincethe integration is simple and clean.INTEGRATED SYSTEM PERFORMANCEIn this section, we present results for HARC on thestandard DARPA 1000-Word Resource Managementspeech database (Price, et al (1988)), with 600 sen-tences (about 30 minutes) of training speech to trainthe acoustic models for each speaker.
For these ex-periments, speech was sampled at 20 kHz, and 14MeI-Frequency cepstral coeffients (MFCC), theirderivatives (DMFCC), plus power (R0) and derivativeof power (DR0) were computed for each 10 ms, usinga 20 ms analysis window.
Three separate 8-bitcodebooks were created for each of the three sets ofparameters using K-means vector quantization (VQ).The experiments were conducted using the multi-codebook paradigm in the HMM models, where theoutput of vector quantizer, which consists of a vectorof 3 VQ codewords per 10 ms frame, is used as theinput observation sequence to the HMM.For the purpose of making computation tractable, weapplied the lattice pruning techniques describedabove to a full word lattice to reduce the averagelattice size from over 2000 word theories to about 604 .At this lattice size, the probability of having the correctword sequence in the lattice is about 98%, whichplaces an upperbound on subsequent system perfor-mance using the language models.Grammar PerplexityNone 1000Word Pair 60Syntax 700+Semantics NA%Word % SentenceError Error15.45 71.33.9 26.07.5 38.06.9 36.4Figure 1: Recognition Performance of HARCFigure 1 shows the results averaged across 7speakers, using a total of 109 utterances, under 4grammar conditions.
As shown, the grammars testedinclude: 1) no grammmar: all word sequences arepossible; 2) the word pair grammar, containing allpairs of words occuring in the set of sentences thatwas used to define the database; 3) the syntacticgrammar alone; and 4) semantic interpretation for aposteriori filtering on the output of lattice parsing.Note that the performance using the syntactic lan-guage model is 7.5% error.
At a perplexity of 700, itsperformance should be closer to the no grammarcase, which has a perplexity of 1000 and an error rateof about 15%.
We hypothesize that perplexity aloneis not adequate to predict the quality of a languagemodel.
In order to be more precise, one needs to lookat acoustic perplexity: a measure of how well a lan-guage model can selectively and appropriately limitacoustic confusability.
A linguistically motivated lan-guage model seems to do just that--at least in thislimited experiment.
Also, surprisingly, using seman-tics gave insignificant improvement in the overall per-formance.
One possible explanation for this is that460 word theories corresponds to about 4000 acoustic scores.i i0semantics gets to filter only a small number of thesentences accepted by syntax.
Out of the sentenceswhich receive semantic interpretations, syntax alonedetermined the correct sentence better than 60 per-cent of the time, leaving only about 20 sentences inwhich the semantics has a chance to correct the error.Unfortunately, of these errorful answers, most weresemantically meaningful, although there were someexceptions.
Pragmatic information may be a higherlevel knowledge source to constrain the possible wordsequences, and therefore improve performance.ACKNOWLEDGMENTSThe work reported here was supported by the Ad-vanced Research Projects Agency under Contract No.N00014-C-87-0085 monitored by the Office of NavalResearch.
The views and conclusions contained inthis document are those of the authors and should notbe interpreted as necessarily representing the officialpolicies, either expressed or implied, of the DefenseAdvanced Research Projects Agency of the UnitedStates Government.ReferencesBoisen S., Y. Chow, A. Haas, R. lngria,S.
Roucos, R. Scha, D. Stallard andM.
Vilain (1989) Integration of Speech andNatural Language: Final Report, Report No.6991, BBN Systems and Technologies Cor-poration, Cambridge, Massachusetts.Bresnan, Joan W. (1982) The Mental Represen-tation of Grammatical Relations, MIT Press,Cambridge, Massachusetts.Bronnenberg, W.J.H.J., Harry C. Bunt, S.P.
JanLandsbergen, Remko J .H.
Scha, W.J.Schoenmakers, and E.P.C.
van Utteren(1980) "The Question Answering SystemPHLIQAI", in Leonard Bolc, ed., NaturalLanguage Question Answering Systems,Hanser, Munich, pp.
217--305.Chow, Y.L., M.O.
Dunham, O.A.
Kimball, M.A.Krasner, G.F. Kubala, J. Makhoul, P.J.Price, S. Roucos, and R.M.
Schwartz (1987)"BYBLOS: The BBN Continuous SpeechRecognition System", IEEE Int.
Conf.Acoust., Speech, Signal Processing, Dallas,TX, April 1987, pp.
89--92, Paper No.
3.7.Chow, Yen-Lu and Salim Roukos (1989)"Speech Understanding Using a UnificationGrammar", in Proceedings of the/EEE Inter-national Conference on Acoustics, Speech,and Signal Processing, Glasgow, Scotland.Gazdar, Gerald, Ewan Klein, Geoffrey Pullum,Ivan Sag (1985) Generalized Phrase Struc-ture Grammar, Harvard University Press,Cambridge, Massachusetts.Graham, Susan t., Michael A. Harrison, andWalter L. Ruzzo (1980) "An ImprovedContext-free Recognizer", ACM Trans-actions on Programming Languages andSystems, 2.3, pp.
415--461.Haas, Andrew (1987) "Parallel Parsing forUnification Grammar", Proceedings of theTenth International Joint Conference on Ar-tificial Intelligence, pp.
615--618.Haas, Andrew (1989) "A Generalization of theOffline Parsable Grammars", 27th AnnualMeeting of the Association for Computa-tional Linguistics: Proceedings of theConference, Association for ComputationalLinguistics, Morristown, NJ.Pereira, Fernando C. N. and David H. D. Warren(1980) "Definite Clause Grammars for Lan-guage Analysis---A Survey of the For-malism and a Comparison with AugmentedTransition Networks", Artificial Intelligence13, pp.
231--278.Price, P., W.M.
Fisher, J. Bernstein and D.S.
Pal-lett (1988) "The DARPA 1000-WordResource Management Database for Con-tinuous Speech Recognition", IEEE Int.Conf.
Acoust., Speech, Signal Processing,New York, NY, April 1988, pp.
651 --654.Shieber, Stuart, Hans Uszkoreit, FernandoPereira, Jane Robinson, and Mabry Tyson(1083) "The Formalism and Implementationof PATR-II" in Grosz, Barbra J. and MarkE.
Stickel (1983) Research on InteractiveAcquisition and Use of Know/edge: FinalReport SRI Project 1894, SRI International,Menlo Park, California., pp.
39--79.i i i
