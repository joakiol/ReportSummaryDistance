Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45?51,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsResults of the WMT13 Metrics Shared TaskMatous?
Macha?c?ek and Ondr?ej BojarCharles University in Prague, Faculty of Mathematics and PhysicsInstitute of Formal and Applied Linguisticsmachacekmatous@gmail.com and bojar@ufal.mff.cuni.czAbstractThis paper presents the results of theWMT13 Metrics Shared Task.
We askedparticipants of this task to score theoutputs of the MT systems involved inWMT13 Shared Translation Task.
Wecollected scores of 16 metrics from 8 re-search groups.
In addition to that we com-puted scores of 5 standard metrics such asBLEU, WER, PER as baselines.
Collectedscores were evaluated in terms of systemlevel correlation (how well each metric?sscores correlate with WMT13 official hu-man scores) and in terms of segment levelcorrelation (how often a metric agrees withhumans in comparing two translations of aparticular sentence).1 IntroductionAutomatic machine translation metrics play a veryimportant role in the development of MT systemsand their evaluation.
There are many differentmetrics of diverse nature and one would like toassess their quality.
For this reason, the MetricsShared Task is held annually at the Workshop ofStatistical Machine Translation (Callison-Burch etal., 2012).
This year, the Metrics Task was runby different organizers but the only visible changeis hopefully that the results of the task are pre-sented in a separate paper instead of the mainWMT overview paper.In this task, we asked metrics developers toscore the outputs of WMT13 Shared TranslationTask (Bojar et al 2013).
We have collected thecomputed metrics?
scores and use them to evalu-ate quality of the metrics.The systems?
outputs, human judgements andevaluated metrics are described in Section 2.
Thequality of the metrics in terms of system level cor-relation is reported in Section 3.
Segment levelcorrelation is reported in Section 4.2 DataWe used the translations of MT systems involvedin WMT13 Shared Translation Task together withreference translations as the test set for the MetricsTask.
This dataset consists of 135 systems?
out-puts and 6 reference translations in 10 translationdirections (5 into English and 5 out of English).Each system?s output and the reference translationcontain 3000 sentences.
For more details pleasesee the WMT13 main overview paper (Bojar et al2013).2.1 Manual MT Quality JudgementsDuring the WMT13 Translation Task a large scalemanual annotation was conducted to compare thesystems.
We used these collected human judge-ments for evaluating the automatic metrics.The participants in the manual annotation wereasked to evaluate system outputs by ranking trans-lated sentences relative to each other.
For eachsource segment that was included in the procedure,the annotator was shown the outputs of five sys-tems to which he or she was supposed to assignranks.
Ties were allowed.
Only sentences with 30or less words were ranked by humans.These collected rank labels were then used toassign each system a score that reflects how highthat system was usually ranked by the annotators.Please see the WMT13 main overview paper fordetails on how this score is computed.
You canalso find inter- and intra-annotator agreement esti-mates there.2.2 Participants of the Shared TaskTable 1 lists the participants of WMT13 SharedMetrics Task, along with their metrics.
We havecollected 16 metrics from a total of 8 researchgroups.In addition to that we have computed the fol-lowing two groups of standard metrics as base-lines:45Metrics ParticipantMETEOR Carnegie Mellon University (Denkowski and Lavie, 2011)LEPOR, NLEPOR University of Macau (Han et al 2013)ACTA, ACTA5+6 Idiap Research Institute (Hajlaoui, 2013) (Hajlaoui and Popescu-Belis, 2013)DEPREF-{ALIGN,EXACT} Dublin City University (Wu et al 2013)SIMPBLEU-{RECALL,PREC} University of Shefield (Song et al 2013)MEANT, UMEANT Hong Kong University of Science and Technology (Lo and Wu, 2013)TERRORCAT German Research Center for Artificial Intelligence (Fishel, 2013)LOGREGFSS, LOGREGNORM DFKI (Avramidis and Popovic?, 2013)Table 1: Participants of WMT13 Metrics Shared Task?
Moses Scorer.
Metrics BLEU (Papineni etal., 2002), TER (Snover et al 2006), WER,PER and CDER (Leusch et al 2006) werecomputed using the Moses scorer which isused in Moses model optimization.
To tok-enize the sentences we used the standard tok-enizer script as available in Moses Toolkit.
Inthis paper we use the suffix *-MOSES to labelthese metrics.?
Mteval.
Metrics BLEU (Papineni etal., 2002) and NIST (Doddington,2002) were computed using the scriptmteval-v13a.pl 1 which is used inOpenMT Evaluation Campaign and includesits own tokenization.
We use *-MTEVALsuffix to label these metrics.
By default,mteval assumes the text is in ASCII,causing poor tokenization around curlyquotes.
We run mteval in both thedefault setting as well as with the flag--international-tokenization(marked *-INTL).We have normalized all metrics?
scores suchthat better translations get higher scores.3 System-Level Metric AnalysisWe measured the quality of system-level metrics?scores using the Spearman?s rank correlation coef-ficient ?.
For each direction of translation we con-verted the official human scores into ranks.
Foreach metric, we converted the metric?s scores ofsystems in a given direction into ranks.
Since therewere no ties in the rankings, we used the simplifiedformula to compute the Spearman?s ?:?
= 1?
6?d2in(n2 ?
1) (1)1http://www.itl.nist.gov/iad/mig//tools/where di is the difference between the human rankand metric?s rank for system i and n is number ofsystems.
The possible values of ?
range between1 (where all systems are ranked in the same order)and -1 (where the systems are ranked in the re-verse order).
A good metric produces rankings ofsystems similar to human rankings.
Since we havenormalized all metrics such that better translationsget higher score we consider metrics with valuesof Spearman?s ?
closer to 1 as better.We also computed empirical confidences ofSpearman?s ?
using bootstrap resampling.
Sincewe did not have direct access to participants?
met-rics (we received only metrics?
scores for the com-plete test sets without the ability to run them onnew sampled test sets), we varied the ?goldentruth?
by sampling from human judgments.
Wehave bootstrapped 1000 new sets and used 95 %confidence level to compute confidence intervals.The Spearman?s ?
correlation coefficient issometimes too harsh: If a metric disagrees withhumans in ranking two systems of a very similarquality, the ?
coefficient penalizes this equally asif the systems were very distant in their quality.Aware of how uncertain the golden ranks are ingeneral, we do not find the method very fair.
Wethus also computed three following correlation co-efficients besides the Spearman?s ?:?
Pearson?s correlation coefficient.
This co-efficient measures the strength of the linearrelationship between metric?s scores and hu-man scores.
In fact, Spearman?s ?
is Pear-son?s correlation coefficient applied to ranks.?
Correlation with systems?
clusters.
In theTranslation Task (Bojar et al 2013), themanual scores are also presented as clus-ters of systems that can no longer be signifi-cantly distinguished from one another giventhe available judgements.
(Please see theWMT13 Overview paper for more details).46We take this cluster information as a ?rankwith ties?
for each system and calculate itsPearson?s correlation coefficient with eachmetric?s scores.?
Correlation with systems?
fuzzy ranks.
Fora given system the fuzzy rank is computedas an average of ranks of all systems whichare not significantly better or worse than thegiven system.
The Pearson?s correlation co-efficient of a metric?s scores and systems?fuzzy ranks is then computed.You can find the system-level correlations fortranslations into English in Table 2 and for transla-tions out of English in Table 3.
Each row in the ta-bles contains correlations of a metric in each of theexamined translation directions.
The metrics aresorted by average Spearman?s ?
correlation acrosstranslation directions.
The best results in each di-rection are in bold.As in previous years, a lot of metrics outper-formed BLEU in system level correlation.
Themetric which has on average the strongest corre-lation in directions into English is METEOR.
Forthe out of English direction, SIMPBLEU-RECALLhas the highest system-level correlation.
TER-RORCAT achieved even a higher average corre-lation but it did not participate in all languagepairs.
The implementation of BLEU in mtevalis slightly better than the one in Moses scorer(BLEU-MOSES).
This confirms the known truththat tokenization and other minor implementationdetails can considerably influence a metric perfor-mance.4 Segment-Level Metric AnalysisWe measured the quality of metrics?
segment-level scores using Kendall?s ?
rank correlationcoefficient.
For this we did not use the officialWMT13 human scores but we worked with rawhuman judgements: For each translation directionwe extracted all pairwise comparisons where onesystem?s translation of a particular segment wasjudged to be (strictly) better than the other sys-tem?s translation.
Formally, this is a list of pairs(a, b) where a segment translation a was rankedbetter than translation b:Pairs := {(a, b) | r(a) < r(b)} (2)where r(?)
is human rank.
For a given metricm(?
),we then counted all concordant pairwise compar-isons and all discordant pairwise comparisons.
Aconcordant pair is a pair of two translations ofthe same segment in which the comparison of hu-man ranks agree with the comparison of the met-ric?s scores.
A discordant pair is a pair in whichthe comparison of human ranks disagrees with themetric?s comparison.
Note that we totally ignorepairs where human ranks or metric?s scores aretied.
Formally:Con := {(a, b) ?
Pairs | m(a) > m(b)} (3)Dis := {(a, b) ?
Pairs | m(a) < m(b)} (4)Finally the Kendall?s ?
is computed using the fol-lowing formula:?
= |Con| ?
|Dis||Con|+ |Dis| (5)The possible values of ?
range between -1 (a met-ric always predicted a different order than humansdid) and 1 (a metric always predicted the same or-der as humans).
Metrics with higher ?
are better.The final Kendall?s ?s are shown in Table 4for directions into English and in Table 5 for di-rections out of English.
Each row in the tablescontains correlations of a metric in given direc-tions.
The metrics are sorted by average corre-lation across the translation directions.
Metricswhich did not compute scores for systems in alldirections are at the bottom of the tables.You can see that in both categories, into and outof English, the strongest correlated segment-levelmetric is SIMPBLEU-RECALL.4.1 Details on Kendall?s ?The computation of Kendall?s ?
has slightlychanged this year.
In WMT12 Metrics Task(Callison-Burch et al 2012), the concordant pairswere defined exactly as we do (Equation 3) but thediscordant pairs were defined differently: pairs inwhich one system was ranked better by the humanannotator but in which the metric predicted a tiewere considered also as discordant:Dis := {(a, b) ?
Pairs | m(a) ?
m(b)} (6)We feel that for two translations a and b of a seg-ment, where a is ranked better by humans, a metricwhich produces equal scores for both translationsshould not be penalized as much as a metric which47CorrelationcoefficientSpearman?s?CorrelationCoefficientPearson?sClustersFuzzyRanksDirectionsfr-ende-enes-encs-enru-enAverageAverageAverageAverageConsideredsystems1222111017METEOR.984?.014.961?.020.979?.024.964?.027.789?.040.935?.012.950.924.936DEPREF-ALIGN.995?.011.966?.018.965?.031.964?.023.768?.041.931?.012.926.909.924UMEANT.989?.011.946?.018.958?.028.973?.032.775?.037.928?.012.909.903o.930MEANT.973?.014.926?.021.944?.038.973?.032.765?.038.916?.013.901.891.918SEMPOS.938?.014.919?.028.930?.031.955?.018.823?.037.913?.012o.934o.894.901DEPREF-EXACT.984?.011.961?.017.937?.038.936?.027.744?.046.912?.015o.924o.892.901SIMPBLEU-RECALL.978?.014.936?.020.923?.052.909?.027.798?.043.909?.017o.923.874.886BLEU-MTEVAL-INTL.989?.014.902?.017.895?.049.936?.032.695?.042.883?.015.866.843.874BLEU-MTEVAL.989?.014.895?.020.888?.045.936?.032.670?.041.876?.015.854.835.865BLEU-MOSES.993?.014.902?.017.879?.051.936?.036.651?.041.872?.016o.856.826.861CDER-MOSES.995?.014.877?.017.888?.049.927?.036.659?.045.869?.017o.877o.831.859SIMPBLEU-PREC.989?.008.846?.020.832?.059.918?.023.704?.042.858?.017o.871.815.847NLEPOR.945?.022.949?.025.825?.056.845?.041.705?.043.854?.018o.867.804o.853LEPORV3.100.945?.019.934?.027.748?.077.800?.036.779?.041.841?.020o.869.780o.850NIST-MTEVAL.951?.019.875?.022.769?.077.891?.027.649?.045.827?.020.852.774.824NIST-MTEVAL-INTL.951?.019.875?.022.762?.077.882?.032.658?.045.826?.021o.856.774o.826TER-MOSES.951?.019.833?.023.825?.077.800?.036.581?.045.798?.021.803.733.797WER-MOSES.951?.019.672?.026.797?.070.755?.041.591?.042.753?.020.785.682.749PER-MOSES.852?.027.858?.025.357?.091.697?.043.677?.040.688?.024.757.637.706TERRORCAT.984?.011.961?.023.972?.028n/an/a.972?.012.977.958.959Table2:System-levelcorrelationsofautomaticevaluationmetricsandtheofficialWMThumanscoreswhentranslatingintoEnglish.Thesymbol?o?indicateswheretheotheraveragesareoutofsequencecomparedtothemainSpearman?s?average.48CorrelationcoefficientSpearman?s?CorrelationCoefficientPearson?sClustersFuzzyRanksDirectionsen-fren-deen-esen-csen-ruAverageAverageAverageAverageConsideredsystems1414121112SIMPBLEU-RECALL.924?.022.925?.020.830?.047.867?.031.710?.053.851?.018.844.856.849LEPORV3.100.904?.034.900?.027.841?.049.748?.056.855?.048.850?.020o.854.833.844NIST-MTEVAL-INTL.929?.032.846?.029.797?.060.902?.045.771?.048.849?.020.808o.863o.845CDER-MOSES.921?.029.867?.029.857?.058.888?.024.701?.059.847?.019.796o.861.843NLEPOR.919?.028.904?.027.852?.049.818?.045.727?.064.844?.021o.849o.846.840NIST-MTEVAL.914?.034.825?.030.780?.066.916?.031.723?.048.832?.021.794o.851.828SIMPBLEU-PREC.909?.026.879?.025.780?.071.881?.035.697?.051.829?.020o.840o.852.827METEOR.924?.027.879?.030.780?.060.937?.024.569?.066.818?.022o.806.825.814BLEU-MTEVAL-INTL.917?.033.832?.030.764?.071.895?.028.657?.062.813?.022o.802.821.808BLEU-MTEVAL.895?.037.786?.034.764?.071.895?.028.631?.053.794?.022o.799.809.790TER-MOSES.912?.038.854?.032.753?.066.860?.059.538?.068.783?.023.746.806.778BLEU-MOSES.897?.034.786?.034.759?.078.895?.028.574?.057.782?.022o.802.792o.779WER-MOSES.914?.034.825?.034.714?.077.860?.056.552?.066.773?.024.737o.796.766PER-MOSES.873?.040.686?.045.775?.047.797?.049.591?.062.744?.024o.758.747.739TERRORCAT.929?.022.946?.018.912?.041n/an/a.929?.017.952.933.923SEMPOSn/an/an/a.699?.045n/a.699?.045.717.615.696ACTA5?6.809?.046-.526?.034n/an/an/a.141?.029.166.196.176ACTA.809?.046-.526?.034n/an/an/a.141?.029.166.196.176Table3:System-levelcorrelationsofautomaticevaluationmetricsandtheofficialWMThumanscoreswhentranslatingoutofEnglish.Thesymbol?o?indicateswheretheotheraveragesareoutofsequencecomparedtothemainSpearman?s?average.49Directions fr-en de-en es-en cs-en ru-en AverageExtracted pairs 80741 128668 67832 85469 151422SIMPBLEU-RECALL .193 .318 .279 .260 .234 .257METEOR .178 .293 .236 .265 .239 .242DEPREF-ALIGN .161 .267 .234 .228 .200 .218DEPREF-EXACT .167 .263 .228 .227 .195 .216SIMPBLEU-PREC .154 .236 .214 .208 .174 .197NLEPOR .149 .240 .204 .176 .172 .188SENTBLEU-MOSES .150 .218 .198 .197 .170 .187LEPOR V3.100 .149 .221 .161 .187 .177 .179UMEANT .101 .166 .144 .160 .108 .136MEANT .101 .160 .145 .164 .109 .136LOGREGFSS-33 n/a .272 n/a n/a n/a .272LOGREGFSS-24 n/a .270 n/a n/a n/a .270TERRORCAT .161 .298 .230 n/a n/a .230Table 4: Segment-level Kendall?s ?
correlations of automatic evaluation metrics and the official WMThuman judgements when translating into English.Directions en-fr en-de en-es en-cs en-ru AverageExtracted pairs 100783 77286 60464 102842 87323SIMPBLEU-RECALL .158 .085 .231 .065 .126 .133SIMPBLEU-PREC .138 .065 .187 .055 .095 .108METEOR .147 .049 .175 .058 .111 .108SENTBLEU-MOSES .133 .047 .171 .052 .095 .100LEPOR V3.100 .126 .058 .178 .023 .109 .099NLEPOR .124 .048 .163 .048 .097 .096LOGREGNORM-411 n/a n/a .136 n/a n/a .136TERRORCAT .116 .074 .186 n/a n/a .125LOGREGNORMSOFT-431 n/a n/a .033 n/a n/a .033Table 5: Segment-level Kendall?s ?
correlations of automatic evaluation metrics and the official WMThuman judgements when translating out of English.50strongly disagrees with humans.
The method weused this year does not harm metrics which oftenestimate two segments as equally good.5 ConclusionWe carried out WMT13 Metrics Shared Task inwhich we assessed the quality of various au-tomatic machine translation metrics.
We usedthe human judgements as collected for WMT13Translation Task to compute system-level andsegment-level correlations with human scores.While most of the metrics correlate very wellon the system-level, the segment-level correlationsare still rather poor.
It was shown again this yearthat a lot of metrics outperform BLEU, hopefullyone of them will attract a wider use at last.AcknowledgementsThis work was supported by the grantsP406/11/1499 of the Grant Agency of theCzech Republic and FP7-ICT-2011-7-288487(MosesCore) of the European Union.ReferencesEleftherios Avramidis and Maja Popovic?.
2013.
Ma-chine learning methods for comparative and time-oriented Quality Estimation of Machine Translationoutput.
In Proceedings of the Eight Workshop onStatistical Machine Translation.Ondr?ej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 workshopon statistical machine translation.
In Proceedings ofthe Eight Workshop on Statistical Machine Transla-tion.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montre?al, Canada, June.
Association forComputational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human LanguageTechnology Research, HLT ?02, pages 138?145, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Mark Fishel.
2013.
Ranking Translations using ErrorAnalysis and Quality Estimation.
In Proceedings ofthe Eight Workshop on Statistical Machine Transla-tion.Najeh Hajlaoui and Andrei Popescu-Belis.
2013.
As-sessing the accuracy of discourse connective transla-tions: Validation of an automatic metric.
In 14th In-ternational Conference on Intelligent Text Process-ing and Computational Linguistics, page 12.
Uni-versity of the Aegean, Springer, March.Najeh Hajlaoui.
2013.
Are ACT?s scores increasingwith better translation quality.
In Proceedings of theEight Workshop on Statistical Machine Translation.Aaron Li-Feng Han, Derek F. Wong, Lidia S. Chao,Yi Lu, Liangye He, Yiming Wang, and Jiaji Zhou.2013.
A Description of Tunable Machine Transla-tion Evaluation Systems in WMT13 Metrics Task.In Proceedings of the Eight Workshop on StatisticalMachine Translation.Gregor Leusch, Nicola Ueffing, and Hermann Ney.2006.
Cder: Efficient mt evaluation using blockmovements.
In In Proceedings of EACL, pages 241?248.Chi-Kiu Lo and Dekai Wu.
2013.
MEANT @WMT2013 metrics evaluation.
In Proceedings ofthe Eight Workshop on Statistical Machine Transla-tion.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
pages 311?318.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In In Proceedings of Association for MachineTranslation in the Americas, pages 223?231.Xingyi Song, Trevor Cohn, and Lucia Specia.
2013.BLEU deconstructed: Designing a better MT evalu-ation metric.
March.Xiaofeng Wu, Hui Yu, and Qun Liu.
2013.
DCU Par-ticipation in WMT2013 Metrics Task.
In Proceed-ings of the Eight Workshop on Statistical MachineTranslation.51
