Parser-Based Retraining for Domain Adaptation of Probabilistic GeneratorsDeirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van GenabithNational Centre for Language TechnologySchool of ComputingDublin City UniversityIreland{dhogan, jfoster, jwagner, josef}@computing.dcu.ieAbstractWhile the effect of domain variation on Penn-treebank-trained probabilistic parsers has beeninvestigated in previous work, we study its ef-fect on a Penn-Treebank-trained probabilisticgenerator.
We show that applying the gener-ator to data from the British National Corpusresults in a performance drop (from a BLEUscore of 0.66 on the standard WSJ test set to aBLEU score of 0.54 on our BNC test set).
Wedevelop a generator retraining method wherethe domain-specific training data is automat-ically produced using state-of-the-art parseroutput.
The retraining method recovers a sub-stantial portion of the performance drop, re-sulting in a generator which achieves a BLEUscore of 0.61 on our BNC test data.1 IntroductionGrammars extracted from the Wall Street Journal(WSJ) section of the Penn Treebank have been suc-cessfully applied to natural language parsing, andmore recently, to natural language generation.
It isclear that high-quality grammars can be extractedfor the WSJ domain but it is not so clear howthese grammars scale to other text genres.
Gildea(2001), for example, has shown that WSJ-trainedparsers suffer a drop in performance when appliedto the more varied sentences of the Brown Cor-pus.
We investigate the effect of domain variation intreebank-grammar-based generation by applying aWSJ-trained generator to sentences from the BritishNational Corpus (BNC).As with probabilistic parsing, probabilistic gener-ation aims to produce the most likely output(s) giventhe input.
We can distinguish three types of prob-abilistic generators, based on the type of probabil-ity model used to select the most likely sentence.The first type uses an n-gram language model, e.g.
(Langkilde, 2000), the second type uses a proba-bility model defined over trees or feature-structure-annotated trees, e.g.
(Cahill and van Genabith,2006), and the third type is a mixture of the firstand second type, employing n-gram and grammar-based features, e.g.
(Velldal and Oepen, 2005).
Thegenerator used in our experiments is an instance ofthe second type, using a probability model definedover Lexical Functional Grammar c-structure andf-structure annotations (Cahill and van Genabith,2006; Hogan et al, 2007).In an initial evaluation, we apply our probabilisticWSJ-trained generator to BNC material, and showthat the generator suffers a substantial performancedegradation, with a drop in BLEU score from 0.66to 0.54.
We then turn our attention to the problemof adapting the generator so that it can more accu-rately generate the 1,000 sentences in our BNC testset.
The problem of adapting any NLP system to adomain different from the domain upon which it hasbeen trained and for which no gold standard train-ing material is available is a very real one, and onewhich has been the focus of much recent research inparsing.
Some success has been achieved by traininga parser, not on gold standard hand-corrected trees,but on parser output trees.
These parser output treescan by produced by a second parser in a co-trainingscenario (Steedman et al, 2003), or by the sameparser with a reranking component in a type of self-training scenario (McClosky et al, 2006).
We tackle165the problem of domain adaptation in generation ina similar way, by training the generator on domainspecific parser output trees instead of manually cor-rected gold standard trees.
This experiment achievespromising results, with an increase in BLEU scorefrom 0.54 to 0.61.
The method is generic and can beapplied to other probabilistic generators (for whichsuitable training material can be automatically pro-duced).2 BackgroundThe natural language generator used in our experi-ments is the WSJ-trained system described in Cahilland van Genabith (2006) and Hogan et al (2007).Sentences are generated from Lexical FunctionalGrammar (LFG) f-structures (Kaplan and Bresnan,1982).
The f-structures are created automaticallyby annotating nodes in the gold standard WSJ treeswith LFG functional equations and then passingthese equations through a constraint solver (Cahillet al, 2004).
The generation algorithm is a chart-based one which works by finding the most proba-ble tree associated with the input f-structure.
Theyield of the most probable tree is the output sen-tence.
An annotated PCFG, in which the non-terminal symbols are decorated with functional in-formation, is used to generate the most probable treefrom an f-structure.
Cahill and van Genabith (2006)attain 98.2% coverage and a BLEU score of 0.6652on the standard WSJ test set (Section 23).
Hoganet al (2007) describe an extension to the systemwhich replaces the annotated PCFG selection modelwith a more sophisticated history-based probabilis-tic model.
Instead of conditioning the righthand sideof a rule on the lefthand non-terminal and its asso-ciated functional information alone, the new modelincludes non-local conditioning information in theform of functional information associated with an-cestor nodes of the lefthand side category.
This sys-tem achieves a BLEU score of 0.6724 and 99.9%coverage.Other WSJ-trained generation systems includeNakanishi et al (2005) and White et al (2007).Nakanishi et al (2005) describe a generator trainedon a HPSG grammar derived from the WSJ Sectionof the Penn Treebank.
On sentences of ?
20 wordsin length, their system attains coverage of 90.75%and a BLEU score of 0.7733.
White et al (2007)describe a CCG-based realisation system which hasbeen trained on logical forms derived from CCG-Bank (Hockenmaier and Steedman, 2005), achiev-ing 94.3% coverage and a BLEU score of 0.5768 onWSJ23 for all sentence lengths.
The input structuresupon which these systems are trained vary in formand specificity, but what the systems have in com-mon is that their various input structures are derivedfrom Penn Treebank trees.3 The BNC Test DataThe new English test set consists of 1,000 sentencestaken from the British National Corpus (Burnard,2000).
The BNC is a one hundred million word bal-anced corpus of British English from the late twenti-eth century.
Ninety per cent of it is written text, andthe remaining 10% consists of transcribed sponta-neous and scripted spoken language.
The BNC sen-tences in the test set are not chosen completely atrandom.
Each sentence in the test set has the prop-erty of containing a word which appears as a verbin the BNC but not in the usual training sections ofthe Wall Street Journal section of the Penn Treebank(WSJ02-21).
Sentences were chosen in this way sothat the resulting test set would be a difficult onefor WSJ-trained systems.
In order to produce in-put f-structures for the generator, the test sentenceswere manually parsed by one annotator, using asreferences the Penn Treebank trees themselves andthe Penn Treebank bracketing guidelines (Bies etal., 1995).
When the two references did not agree,the guidelines took precedence over the Penn Tree-bank trees.
Difficult parsing decisions were docu-mented.
Due to time constraints, the annotator didnot mark functional tags or traces.
The context-freegold standard parse trees were transformed into f-structures using the automatic procedure of Cahill etal.
(2004).4 ExperimentsExperimental Setup In our first experiment, weapply the original WSJ-trained generator to ourBNC test set.
The gold standard trees for our BNCtest set differ from the gold standard Wall StreetJournal trees, in that they do not contain Penn-IItraces or functional tags.
The process which pro-166duces f-structures from trees makes use of trace andfunctional tag information, if available.
Thus, to en-sure that the training and test input f-structures arecreated in the same way, we use a version of thegenerator which is trained using gold standard WSJtrees without functional tag or trace information.When we test this system on the WSJ23 f-structures(produced in the same way as the WSJ training ma-terial), the BLEU score decreases slightly from 0.67to 0.66.
This is our baseline system.In a further experiment, we attempt to adaptthe generator to BNC data by using BNC trees astraining material.
Because we lack gold standardBNC trees (apart from those in our test set), wetry instead to use parse trees produced by an accu-rate parser.
We choose the Charniak and Johnsonreranking parser because it is freely available andachieves state-of-the-art accuracy (a Parseval f-scoreof 91.3%) on the WSJ domain (Charniak and John-son, 2005).
It is, however, affected by domain vari-ation ?
Foster et al (2007) report that its f-scoredrops by approximately 8 percentage points whenapplied to the BNC domain.
Our training size is500,000 sentences.
We conduct two experiments:the first, in which 500,000 sentences are extractedrandomly from the BNC (minus the test set sen-tences), and the second in which only shorter sen-tences, of length ?
20 words, are chosen as trainingmaterial.
The rationale behind the second experi-ment is that shorter sentences are less likely to con-tain parser errors.We use the BLEU evaluation metric for our ex-periments.
We measure both coverage and full cov-erage.
Coverage measures the number of cases forwhich the generator produced some kind of out-put.
Full coverage measures the number of cases forwhich the generator produced a tree spanning all ofthe words in the input.Results The results of our experiments are shownin Fig.
1.
The first row shows the results we ob-tain when the baseline system is applied to the f-structures derived from the 1,000 BNC gold stan-dard parse trees.
The second row shows the resultson the same test set for a system trained on Charniakand Johnson parser output trees for 500,000 BNCsentences.
The results in the final row are obtainedby training the generator on Charniak and Johnsonparser output trees for 500,000 BNC sentences oflength ?
20 words in length.Discussion As expected, the performance of thebaseline system degrades when faced with out-of-domain test data.
The BLEU score drops from a0.66 score for WSJ test data to a 0.54 score forthe BNC test data, and full coverage drops from85.97% to 68.77%.
There is a substantial improve-ment, however, when the generator is trained onBNC data.
The BLEU score jumps from 0.5358to 0.6135.
There are at least two possible reasonswhy a BLEU score of 0.66 is not obtained: The firstis that the quality of the f-structure-annotated treesupon which the generator has been trained has de-graded.
For the baseline system, the generator istrained on f-structure-annotated trees derived fromgold trees.
The new system is trained on f-structure-annotated parser output trees, and the performanceof Charniak and Johnson?s parser degrades when ap-plied to BNC data (Foster et al, 2007).
The secondreason has been suggested by Gildea (2001): WSJdata is easier to learn than the more varied data in theBrown Corpus or BNC.
Perhaps even if gold stan-dard BNC parse trees were available for training, thesystem would not behave as well as it does for WSJmaterial.It is interesting to note that training on 500,000shorter sentences does not appear to help.
We hy-pothesized that it would improve results becauseshorter sentences are less likely to contain parsererrors.
The drop in full coverage from 86.69% to79.58% suggests that the number of short sentencesneeds to be increased so that the size of the trainingmaterial stays constant.5 ConclusionWe have investigated the effect of domain varia-tion on a LFG-based WSJ-trained generation sys-tem by testing the system?s performance on 1,000sentences from the British National Corpus.
Perfor-mance drops from a BLEU score of 0.66 onWSJ testdata to 0.54 on the BNC test set.
Encouragingly, wehave also shown that domain-specific training mate-rial produced by a parser can be used to claw backa significant portion of this performance degrada-tion.
Our method is general and could be appliedto other WSJ-trained generators (e.g.
(Nakanishi et167Train BLEU Coverage Full CoverageWSJ02-21 0.5358 99.1 68.77BNC(500k) 0.6135 99.1 86.69BNC(500k) ?
20 words 0.5834 99.1 79.58Figure 1: Results for 1,000 BNC Sentencesal., 2005; White et al, 2007)).
We intend to con-tinue this research by training our generator on parsetrees produced by a BNC-self-trained version of theCharniak and Johnson reranking parser (Foster et al,2007).
We also hope to extend the evaluation beyondthe BLEU metric by carrying out a human judge-ment evaluation.AcknowledgmentsThis research has been supported by the EnterpriseIreland Commercialisation Fund (CFTD/2007/229),Science Foundation Ireland (04/IN/I527) and theIRCSET Embark Initative (P/04/232).
We thank theIrish Centre for High End Computing for providingcomputing facilities.ReferencesAnn Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for treebankII style, Penn Treebank project.
Technical ReportTech Report MS-CIS-95-06, University of Pennsylva-nia, Philadelphia, PA.Lou Burnard.
2000.
User reference guide for the BritishNational Corpus.
Technical report, Oxford UniversityComputing Services.Aoife Cahill and Josef van Genabith.
2006.
RobustPCFG-based generation using automatically acquiredlfg approximations.
In Proceedings of the 21st COL-ING and the 44th Annual Meeting of the ACL, pages1033?1040, Sydney.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations.In Proceedings of the 42nd Meeting of the ACL, pages320?327, Barcelona.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting of theACL, pages 173?180, Ann Arbor.Jennifer Foster, Joachim Wagner, Djame?
Seddah, andJosef van Genabith.
2007.
Adapting WSJ-trainedparsers to the British National Corpus using in-domainself-training.
In Proceedings of the Tenth IWPT, pages33?35, Prague.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of EMNLP, Pittsburgh.Julia Hockenmaier and Mark Steedman.
2005.
Ccgbank:Users?
manual.
Technical report, Computer and Infor-mation Science, University of Pennsylvania.Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josefvan Genabith.
2007.
Exploiting multi-word units inhistory-based probabilistic generation.
In Proceedingsof the joint EMNLP/CoNLL, pages 267?276, Prague.Ron Kaplan and Joan Bresnan.
1982.
Lexical FunctionalGrammar: a Formal System for Grammatical Repre-sentation.
In Joan Bresnan, editor, The Mental Repre-sentation of Grammatical Relations, pages 173?281.MIT Press.Irene Langkilde.
2000.
Forest-based statistical sentencegeneration.
In Proceedings of NAACL, Seattle.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159, NewYork City.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation of anHPSG-based chart generator.
In Proceedings of theNinth IWPT, pages 93?102, Vancouver.Mark Steedman, Miles Osbourne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Boot-strapping statistical parsers from small datasets.
InProceedings of EACL, pages 331?338, Budapest.Erik Velldal and Stephan Oepen.
2005.
Maximum en-tropy models for realization ranking.
In Proceedingsof the MT-Summit, Phuket.Michael White, Rajakrishnan Rajkumar, and Scott Mar-tin.
2007.
Towards broad coverage surface realiza-tion with CCG.
In Proceedings of the Workshop onUsing Corpora for NLG: Language Generation andMachine Translation (UCNLG+MT), pages 267?276,Copenhagen.168
