Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 10?18,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsRecognizing Biomedical Named Entities using Skip-chain ConditionalRandom FieldsJingchen Liu Minlie Huang?
Xiaoyan ZhuDepartment of Computer Science and TechnologyTsinghua University, Beijing 100084, Chinaliu-jc04@mails.tsinghua.edu.cn{aihuang, zxy-dcs}@tsinghua.edu.cnAbstractLinear-chain Conditional Random Fields(CRF) has been applied to perform theNamed Entity Recognition (NER) task inmany biomedical text mining and infor-mation extraction systems.
However, thelinear-chain CRF cannot capture long dis-tance dependency, which is very commonin the biomedical literature.
In this pa-per, we propose a novel study of capturingsuch long distance dependency by defin-ing two principles of constructing skip-edges for a skip-chain CRF: linking sim-ilar words and linking words having typeddependencies.
The approach is applied torecognize gene/protein mentions in the lit-erature.
When tested on the BioCreAtIvEII Gene Mention dataset and GENIA cor-pus, the approach contributes significantimprovements over the linear-chain CRF.We also present in-depth error analysis oninconsistent labeling and study the influ-ence of the quality of skip edges on the la-beling performance.1 IntroductionNamed Entity Recognition (NER) is a key task inmost text mining and information extraction sys-tems.
The improvement in NER can benefit thefinal system performance.
NER is a challengingtask, particularly in the biomedical literature dueto the variety of biomedical terminologies and thecomplicated syntactic structures.Many studies have been devoted to biomedicalNER.
To evaluate biomedical NER systems, sev-eral challenge competitions had been held, suchas BioNLP/NLPBA in 20041, BioCreAtIvE I in?
Corresponding author1http://research.nii.ac.jp/?collier/workshops/JNLPBA04st.htm2004 and BioCreAtIvE II in 20062.
The overviewreports from these competitions, presenting state-of-the-art of biomedical NER studies, show thatlinear-chain Conditional Random Fields (CRF) isone of the most commonly used models and hasthe most competitive results (Yeh et al, 2005;Smith et al, 2008).
Linear-chain CRF has alsobeen successfully applied to other NLP tasks suchas POS-tagging (Lafferty et al, 2001) and sen-tence chunking (Sha and Pereira, 2003).
However,in most of these applications, only linear-chainCRF was fully exploited, assuming that only adja-cent words are inter-dependent.
The dependencybetween distant words, which occurs frequently inthe biomedical literature, is yet to be captured.In the biomedical literature, the repeated ap-pearance of same or similar words in one sentenceis a common type of long distance dependencies.This phenomenon is due to the complicated syn-tactic structures and the various biomedical termi-nologies in nature.
See the following example:?Both GH deficiency and impairedspinal growth may result in shortstature, whereas the occurrence of earlypuberty in association with GH defi-ciency reduces the time available forGH therapy.
?the mentions of GH are repeated three times.
Ifthe entity are referred by a pronoun, the meaningof the sentence will be confusing and unclear be-cause of the complex sentence structure.
In thissentence:?These 8-oxoguanine DNA glycosy-lases, hOgg1 (human) and mOgg1(murine) , are homologous to each otherand to yeast Ogg1.
?the words hOgg1, mOgg1 and Ogg1 are homolo-gous genes belonging to different species, having2http://www.biocreative.org/10very similar entity names.
Some other types oflong distance dependencies also occur frequentlyin the biomedical literature.
For example, in thissentence?Western immunoblot analysis detectedp55gag and its cleavage products p39and p27 in purified particles derived byexpression of gag and gag-pol, respec-tively.
?the words p55gag, p39 and p27 conjuncted byand, have similar semantic meanings but they areseparated by several tokens.
A human curatorcan easily recognize such long distance dependen-cies and annotate these words consistently.
How-ever, when applying the linear-chain CRF, incon-sistency errors in annotating these entities couldhappen due to the inability of representing longdistance dependency.In this paper, we present an approach of cap-turing long distance dependencies between words.We adopte the skip-chain CRF to improve the per-formance of gene mention recognition.
We de-fine two principles of connecting skip-edges forskip-chain CRF to capture long distance depen-dencies.
The efficacy of the principles is inves-tigated with extensive experiments.
We test ourmethod on two data sets and significant improve-ments are observed over the linear-chain CRF.
Wepresent in-depth error analysis on inconsistent la-beling.
We also investigat whether the quality ofconnected edges affect the labeling performance.The remainder of this paper is organized as fol-lows: We survey related studies in Section 2.
Weintroduce linear-chain CRF and skip-chain CRF inSection 3.
The method of connecting skip-chainedges is described in Section 4 .
In Section 5 wepresent our experiments and in-depth analysis.
Wesummarize our work in Section 6.2 Related workNER is a widely studied topic in text miningresearch, and many new challenges are seen indomain-specific applications, such as biomedicalNER (Zhou et al, 2004).
The dictionary basedmethod is a common technique as biomedical the-sauruses play a key role in understanding suchtext.
Most dictionary based NER systems fo-cused on: (1) integrating and normalizing differ-ent biomedical databases to improve the quality ofthe dictionary to be used; (2) improving matchingstrategies that are more suitable for biomedical ter-minologies; and (3) making filtering rules for post-processing to refine the matching results or to ad-just the boundary of entities, see (Fukuda et al,1998; Narayanaswamy et al, 2003; Yang et al,2008).
Many information extraction systems hada dictionary matching module to perform prelim-inary detection of named entities (Schuhmann etal., 2007; Kolarik et al, 2007; Wang et al, 2010).Applying machine learning techniques gener-ally obtains superior performance for the biomedi-cal NER task.
The automated learning process caninduce patterns for recognizing biomedical namesand rules for pre- and post-processing.
Gener-ally speaking, there are two categories of ma-chine learning based methods: one treats NER asa classification task, while the other treats NERas a sequence labeling task.
For the first cate-gory, Support Vector Machine (SVM) was a com-monly adopted model (Kazama et al, 2002; Zhouet al, 2004).
Lee et al (2004) proposed a two-step framework to perform biomedical NER usingSVM: firstly detecting the boundaries of namedentities using classifiers; secondly classifying eachnamed entity into predefined target types.
For thesecond category, a sentence was treated as a se-quence of tokens and the objective was to find theoptimal label sequence for these tokens.
The labelspace was often defined as {B,I,O}, where B in-dicates the beginning token of an entity, I denotesthe continuing token and O represents the tokenoutside an entity.
The sequence labeling task canbe approached by Hidden Markov Model (HMM),Conditional Random Field (CRF) , or a combina-tion of different models (Zhou et al, 2005; Tatarand Cicekli, 2009).Since proposed in (Lafferty et al, 2001), CRFhas been applied to many sequence labelingtasks, including recognizing gene mentions frombiomedical text (McDonald and Pereira, 2005).The Gene Mention Recognition task was includedin both BioCreAtIvE I and BioCreAtIvE II chal-lenges.
CRF had been used in most of top per-forming systems in the Gene Mention Recognitiontask of BioCreAtIvE II (Smith et al, 2008).
Somenovel use of linear-chain CRF was proposed.
Forexample, in (Kuo et al, 2007) labeling was per-formed in forward and backward directions on thesame sentence and results were combined fromthe two directions.
Huang et al (2007) com-bines a linear-chain CRF and two SVM models11to enhance the recall.
Finkel et al (2005) usedGibbs Sampling to add non-local dependenciesinto linear-chain CRF model for information ex-traction.
However, the CRF models used in thesesystems were all linear-chain CRFs.
To the best ofour knowledge, no previous work has been doneon using non-linear-chain CRF in the biomedicalNER task.Beyond the biomedical domain, skip-chainCRF has been used in several studies to modellong distance dependency.
In (Galley, 2006), skipedges were linked between sentences with non-local pragmatic dependencies to rank meetings.In (Ding et al, 2008), skip-chain CRF was usedto detect the context and answers from online fo-rums.
The most close work to ours was in (Sut-ton and McCallum, 2004), which used skip-chainCRF to extract information from email messagesannouncing seminars.
By linking the same wordswhose initial letter is capital, the method obtainedimprovements on extracting speakers?
name.
Ourwork is in the spirit of this idea, but we approachit in a different way.
We found that the problem ismuch more difficult in the biomedical NER task:that is why we systematically studied the princi-ples of linking skip edges and the quality of con-nected edges.3 linear-chain and skip-chain CRFConditional Random Field is a probabilisticgraphic model.
The model predicts the outputvariables y for each input variables in x by calcu-lating the conditional probability p(y|x) accord-ing to the graph structure that represents the de-pendencies between the y variables.
Formally,given a graph structure over y, the CRF model canbe written as:p(y|x) =1Z(x)?Cp???
?c?Cp?c(xc,yc; ?p) (1)Z(x) is a normalization factor.In this definition, the graph is partitioned into aset of cliques ?
= {C1, C2, .
.
.
Cp}, where eachCp is a clique template.
Each ?c, called a factor,is corresponding to one edge in the clique c, andcan be parameterized as:?c(xc,yc; ?p) = exp?k=1?pkfpk(xc,yc) (2)Each feature function fpk(xc,yc) represents onefeature of x and the ?pk is the feature weight.In the training phrase, the parameters is esti-mated using an optimization algorithm such aslimited memory BFGS etc.
In the testing phrase,CRF finds the most likely label sequence for anunseen instance by maximizing the probability de-fined in (1).In the NER task, one sentence is firstly tok-enized into a sequences of tokens and each tokencan be seen as one word.
Each node in the graph isusually corresponding to one word in a sentence.Each x variable represents a set of features for oneword, and each y is the variable for the label ofone word.
Note that when one edge is linked be-tween two words, the edge is actually linked be-tween their corresponding y variables.
The y labelis one of {B,I,O}, in which B means the beginningword of an entity, I means the inside word of anentity, and O means outside an entity.If we link each word with its immediate preced-ing words to form a linear structure for one sen-tence, we get a linear-chain CRF, defined as:p?
(y|x) =1Z(x)T?t=1?t(yt, yt?1,x) (3)This structure contains only one clique template.If we add an extra clique template that containssome skip edges between nonadjacent words, theCRF become a skip-chain CRF, formulated as fol-lows:p?
(y|x) =1Z(x)T?t=1?t(yt, yt?1,x)??(u,v)??
?uv(yu, yv,x) (4)?
is the edge set of the extra clique template con-taining skip edges.
An illustration of linear-chainand skip-chain CRF is given in Figure 1.
It isstraightforward to change a linear-chain CRF toa skip-chain CRF by simply linking some addi-tional skip edges.
However, it must be careful toadd such edges because different graph structuresrequire different inference algorithms.
Those in-ference algorithms may have quite different timecomplexity.
For example, for the linear-chainCRF, inference can be performed efficiently andexactly by a dynamic-programming algorithm.However, for the non-linear structure, approxi-mate inference algorithms must be used.
Solv-ing arbitrary CRF graph structures is NP-hard.
Inother word, we must be careful to link too many12Figure 1: The illustration of linear-chain CRF and skip-chain CRF.
The blue edges represent the linear-chain edges belonging to one clique template, while the red edges represent the skip edges belonging toanother clique template.skip edges to avoid making the model impracti-cal.
Therefore, it is absolutely necessary to studywhich kinds of edges will contribute to the perfor-mance while avoiding over-connected edges.3.1 FeaturesAs our interest is in modifying the CRF graphstructure rather than evaluating the effectivenessof features, we simply adopted features from thestate-of-the-art such as (McDonald and Pereira,2005) and (Kuo et al, 2007).?
Common Features: the original word, thestemmed word, the POS-tag of a word, theword length, is or not the beginning or end-ing word of the sentence etc.?
Regular Expression Features: a set of reg-ular expressions to extract orthographic fea-tures for the word.?
Dictionary Features: We use several lexi-cons.
For example, a protein name dictionarycompiled from SWISS-PROT, a species dic-tionary from NCBI Taxonomy, a drug namedictionary from DrugBank database, and adisease name dictionary from several Internetweb site.?
N-gram Features: For each token, we ex-tract the corresponding 2-4 grams into thefeature set.Each word will include the adjacent words?
fea-tures within {?2,?1, 0, 1, 2} offsets.
The featuresused in the linear-chain CRF and skip-chain CRFare all the same in our experiment.4 MethodAs the limitations discussed above, detectingthe necessary nodes to link should be the firststep in constructing a skip-chain CRF.
In thespeaker name extraction task (Sutton and Mc-Callum, 2004), only identical capitalized wordsare linked, because there is few variations in thespeaker?s name.
However, gene mentions ofteninvolve words without obvious orthographic fea-tures and such phenomena are common in thebiomedical literature such as RGC DNA sequenceand multisubunit TFIID protein.
If we link allthe words like DNA, sequence and protein, the ef-ficiency and performance will drop due to over-connected edges.
Therefore, the most importantstep of detecting gene mentions is to determinewhich edges should be connected.4.1 Detect keywords in gene mentionWe found that many gene mentions have at leastone important word for the identification of genementions.
For example, the word, Gal4, is such a13keyword in Gal4 protein and NS1A in NS1A pro-tein.
These words can distinguish gene mentionsfrom other common English words and phrases,and can distinguish different gene mentions aswell.
We define such words as the keyword ofa gene mention.
The skip edges are limited toonly connect these keywords.
We use a rule-basedmethod to detect keywords.
By examining the an-notated data, we defined keywords as those con-taining at least one capital letter or digit.
And atthe same time, keywords must conform to the fol-lowing rules:?
Keywords are not stop words, single letters,numbers, Greek letters, Roman numbers ornucleotide sequence such as ATTCCCTGG.?
Keywords are not in the form of an upper-case initial letter followed by lowercase let-ters, such as Comparison and Watson.
Thesewords have capital letters only because theyare the first word in the sentences, or they arethe names of people or other objects.
Thisrule will miss some correct candidates, butreduces noise.?
Keywords do not include some commonwords with capital letters such as DNA,cDNA, RNA, mRNA, tRNA etc.
and some fre-quently appearing non-gene names such asHIV and mmHg.
We defined a lexicon forsuch words on the training data.4.2 Link similar keywordsAfter keyword candidates are detected, we judgeeach pair of keywords in the same sentence to findsimilar word pairs.
Each word pair is examined bythese rules:?
They are exactly the same words.?
Words only differ in digit letters, such asCYP1 and CYP2.?
Words with the same prefix, such as IgA andIgG, or with the same suffix, such as ANF andpANF.The token pair will be linked by a skip edge if theymatch at least one rule.4.3 Link typed dependenciesSome long distance dependency cannot be de-tected simply by string similarity.
To capture suchdependency, we used stanford parser3 to parse sen-tences and extract typed dependencies from parsedresults.
The typed dependencies are a set of bi-nary relations belonging to 55 pre-defined types toprovide a description of the grammatical relation-ships in a sentence (Marneffe and Manning, 2008).Some examples of typed dependencies are listed inTable 1.Type Descriptionconj conjuncted by the conjunc-tion such as andprep prepositional modifiernn noun compound modifieramod adjectival modifierdep uncertain typesTable 1: Examples for typed dependencies.The output of the parser is pairs of dependentwords, along with typed dependencies betweentwo words in a pair.
For example, in the sentence:?.
.
.
and activate transcription of a setof genes that includes G1 cyclins CLN1,CLN2, and many DN, synthesis genes.
?a typed dependency nn(G1,CLN1) is extracted bythe parser, meaning the words G1 and CLN1 has atyped dependency of nn because they form a nounphrase under a dependency grammar: modifica-tion.
Similarly, in the sentence?Using the same approach we haveshown that hFIRE binds the stimula-tory proteins Sp1 and Sp3 in addition toCBF.
?the words Sp1 and Sp3 can be detected to have atyped dependency of conj and, and the two wordshave a typed denpendency of prep in addition towith CBF, respectively.
The most common typedependencies are conj and, nn and dep.
The key-words having typed dependencies will be linkedby a skip edge.5 ExperimentWe tested our method on two datasets: the GeneMention (GM) data in BioCreAtIvE II (BCIIGM)3http://nlp.stanford.edu/software/lex-parser.shtml144and GENIA corpus5.
The BCIIGM dataset wasused in the BioCreAtIvE II Gene Mention Recog-nition task in 2006.
It was built from the GENE-TAG corpus (Tanabe et al, 2005) with some mod-ification of the annotation.
The dataset contains15000 sentences for training and 5000 sentencesfor testing.
Two gold-standard sets, GENE andALTGENE, were provided for evaluation and anofficial evaluation procedure in Perl script wasprovided.
The ALTGENE set provides alternateforms for genes in the GENE set.
In the officialevaluation, each identified string will be looked upin both GENE and ALTGENE.
If the correspond-ing gene was found in either GENE or ALTGENE,the identified string will be counted as a correctanswer.The GENIA corpus is a widely used dataset inmany NER and information extraction tasks dueto its high quality annotation.
The GENIA corpuscontains 2000 abstracts from MEDLINE, with ap-proximately 18500 sentences.
The corpus was an-notated by biomedical experts according to a pre-defined GENIA ontology.
In this work, we onlyused the annotated entities that have a category ofprotein, DNA, or RNA.
These categories are re-lated to the definition of gene mention in BioCre-AtIvE II.
We only used strict matching evaluation(no alternate forms check) for the GENIA corpusas no ALTGENE-like annotation is available.The performance is measured by precision, re-call and F score.
Each identified string is countedas a true positive (TP) if it is matched by a gold-standard gene mention, otherwise the identifiedstring is a false positive (FP).
Each gold standardgene mention is counted as a false negative (FN) ifit is not identified by the approach.
Then the pre-cision, recall and their harmonic average F scoreis calculated as follows:precision =TPTP + FPrecall =TPTP + FNF =2 ?
precision ?
recallprecision+ recallTo implement both linear-chain CRF and skip-4http://sourceforge.net/projects/biocreative/files/5http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi?page=Technical+Term+Annotationchain CRF, we used the GRMM Java package6which is an extended version of MALLET.
Thepackage provides an implement of arbitrary struc-ture CRF.5.1 Result ComparisonWe evaluated our approach on the BCIIGMdataset and GENIA corpus.
For the BCIIGMdataset, two evaluation criteria were used: official- exactly the same as that used in the BioCreAtIvEII competition, with the official evaluation proce-dure; and strict - strict matching for each identi-fied string without checking its alternate forms inALTGENE.
The GENIA dataset were randomlydivided into 10 parts to perform a 10-fold crossvalidation.
However, we didn?t do cross validationon the BCIIGM dataset because the BioCreAtIvEII competition annotations and evaluation proce-dure were tailored to evaluating participating sys-tems.The comparative results are listed in Table 2.We compared the two edge linking principles,linking similar words and linking words havingtyped dependencies.
The F score from the skip-chain CRF is better than that from the linear-chainCRF.
Significance tests were performed to checkwhether these results have significant differences.Paired two-tail t-tests were conducted with respectto the F scores of linear-chain CRF vs. those of thetwo skip-chain CRFs, respectively.
The p-valuewas 1.989?10?7 for the skip-chain CRF linked bysimilar words vs. linear-chain CRF.
The p-valuewas 3.971 ?
10?5 for the skip-chain CRF linkedby typed dependencies vs. linear-chain CRF.
Thisshows that the improvement is significant.Note that we did not compare our results on theBCIIGM dataset to those submitted to the compe-tition.
There are two reasons for this: First, ourfocus is on comparing the skip-chain CRF withthe linear-chain CRF.
Second, in the competition,most participating systems that used CRF alsoapplied other algorithms, or sophisticated rulesfor adjusting detected boundaries or refining therecognized results, to achieve competitive perfor-mance.
By contrast, we did not employ any post-processing rule or algorithm to further improve theperformance.
In this sense, comparing our resultsto those has become unfair.6http://mallet.cs.umass.edu/grmm/index.php15Data Model Precision(%) Recall(%) F score(%)BCIIGM officiallinear-chain CRF 85.16 81.50 83.29skip-chain CRF linked by sim-words 86.68 82.75 84.67skip-chain CRF linked by typed-dep 86.73 82.36 84.49BCIIGM strictlinear-chain CRF 74.09 69.49 71.73skip-chain CRF linked by sim-words 76.26 71.53 73.82skip-chain CRF linked by typed-dep 75.99 70.49 73.14GENIAlinear-chain CRF 76.77 74.92 75.83skip-chain CRF linked by sim-words 78.57 77.12 77.82skip-chain CRF linked by typed-dep 78.18 76.87 77.52Table 2: The result comparison between the linear-chain CRF and skip-chain CRF.
BCIIGM is theBioCreAtIvE II Gene Mention Recognition dataset.
official means using the official provided evalua-tion procedure and strict means using strict matching to evaluate the results.
sim-words means similarwords and typed-dep means typed dependencies.
The results for GENIA are averaged over 10-fold crossvalidation.5.2 DiscussionWe provided in-depth analysis of our results on theBCIIGM dataset.
As one of our motivations forconnecting words with skip edges is to enhancethe consistency of labeling, we firstly examinedwhether the proposed approach can provide con-sistent labeling.
Let us start from two typical ex-amples.
In the first sentence?The response sequences were localizedbetween -67 and +30 in the simian cy-tomegalovirus IE94 promoter and up-stream of position +9 in the HCMV IE68promoter.
?the word IE94 is missed (not labeled) while itssimilar word IE68 is labeled correctly by thelinear-chain CRF.
In the second sentence?It is suggested that biliary secretion ofboth TBZ and FBZ and their metabolitesmay contribute to this recycling.
?the word TBZ is labeled as a gene mention in-correctly (false positive) while its similar wordFBZ is not labeled at all (true negative) by thelinear-chain CRF.
Both sentences are correctly la-beled by the skip-chain CRF.
Similar improve-ments are also made by the skip-chain CRF modellinked by typed dependencies.
To study label-ing consistency, we counted the statistics of in-consistency errors, as shown in Table 3.
Twokinds of inconsistency errors were counted: falsenegatives correctable by consistency (FNCC) andfalse positives correctable by consistency (FPCC).An FNCC means that a gold-standard mention ismissed by the system while its skip edge linkedgene mention is correctly labeled, which is simi-lar to the inconsistent miss in (Sutton and McCal-lum, 2004), as the IE94 in the first example.
AnFPCC means a non-gene mention is labeled as agene while its skip edge linked mention (also non-gene mention) is not recognized, as TBZ in the sec-ond example.
These two kinds of inconsistency er-rors lead to inconsistent false negatives (FN) andfalse positives (FP).
A good model should reduceas much inconsistency errors as possible.
The in-consistency errors are reduced substantially as weexpected, showing that the reduction of inconsis-tency errors is one reason for the performance im-provements.The skip-chain CRF linked by similar wordshad better performance than the skip-chain CRFlinked by typed dependencies.
This may infer thatthe quality of skip edges has impact on the per-formance.
In order to study this issue, the qual-ity of skip edges was examined.
The statistics ofskip edges in the BCIIGM dataset for the two skip-chain CRF models (linked by similar words and bytyped dependencies respectively) is shown in thefirst two rows of Table 4.
A skip edge is counted asa correct edge if the edge links two words that areboth gene mentions in the gold-standard annota-tion.
The statistics shows that the skip-chain CRFlinked by similar words has a higher precision thanthe model by typed dependencies.
To make thecomparison more evident, we built another skip-chain CRF whose skip edges were randomly con-nected.
The number of skip edges in this model16Skip edgeModel FPCC FNCCtypesim-wordslinear-chain 112 70skip-chain 48 20Percentage of reduction 57.14% 71.43%typed-deplinear-chain 32 29skip-chain 9 5Percentage of reduction 71.88% 82.76%Table 3: Statistics of inconsistency errors forthe linear-chain CRF and skip-chain CRF.
FPCCis false positives correctable by consistency andFNCC is false negatives correctable by consis-tency in the table.
The percentage is calculatedby dividing the reduction of errors by the errornumber of linear-chain CRF, for example (112 ?48)/48 = 57.14%.approximately equals to that in the skip-chain CRFlinked by similar words.
The percentage of cor-rect skip-edges in this model is small, as shownin the last row of Table 4.
We tested this skip-chain CRF model on the BCIIGM dataset underthe strict matching criterion.
The performance ofthe randomly linked skip-chain CRF is shown inTable 5.
As can be seen from the table, the perfor-mance of the randomly connected skip-chain CRFdroped remarkably, even worse than that of thelinear-chain CRF.
This confirms that the qualityof skip edges is a key factor for the performanceimprovement.Model EdgesCorrectPercentageedgessim-words 1912 1344 70.29%typed-dep 728 425 53.38%random 1906 41 2.15%Table 4: Statistics of skip edges and correctskip edges for the skip-chain CRF models.
sim-words means the skip-chain CRF linked by sim-ilar words, typed-dep means the CRF linked bytyped dependencies and random means the skip-chain CRF has randomly connected skip edges.The edges are counted in the BCIIGM testing data.From the above discussion, we summarize thissection as follows: (1) the skip-chain CRF withhigh quality skip edges can reduce inconsistent la-beling errors, and (2) the quality of skip edges iscrucial to the performance improvement.Model P (%) R(%) F(%)linear 74.09 69.49 71.73sim-words 76.26 71.53 73.82typed-dep 75.99 70.49 73.14random 73.66 69.13 71.32Table 5: Performance comparison between therandomly linked skip-chain CRF and other mod-els.
The result was tested on the BCIIGM datasetunder the strict matching criterion.
P, R and Fdenote the precision, recall and F score respec-tively.
linear denotes the linear-chain CRF.
sim-words denotes the skip-chain CRF linked by sim-ilar words.
typed-dep denotes the skip-chain CRFlinked by typed dependencies.
random denotesthe skip-chain CRF having randomly linked skipedges.6 ConclusionThis paper proposed a method to construct a skip-chain CRF to perform named entity recognition inthe biomedical literature.
We presented two prin-ciples to connect skip edges to address the issueof capturing long distance dependency: linkingsimilar keywords and linking words having typeddependencies.
We evaluated our method on theBioCreAtIvE II GM dataset and GENIA corpus.Significant improvements were observed.
More-over, we presented in-depth analysis on inconsis-tent labeling errors and the quality of skip edges.The study shows that the quality of linked edges isa key factor of the system performance.The quality of linked edges plays an importantrole in not only performance but also time effi-ciency.
Thus, we are planning to apply machinelearning techniques to automatically induce pat-terns for linking high-quality skip-edges.
Further-more, to refine the recognition results, we are plan-ning to employ post-processing algorithms or con-struct refinement rules.AcknowledgmentsThis work was partly supported by the Chi-nese Natural Science Foundation under grant No.60803075 and No.60973104, and partly carriedout with the aid of a grant from the InternationalDevelopment Research Center, Ottawa, CanadaIRCI project from the International Development.17ReferencesShilin Ding, Gao Cong, Chin-Yew Lin and XiaoyanZhu.
2008.
Using Conditional Random Fields toExtract Contexts and Answers of Questions from On-line Forums.
In Proceedings of 46th Annual Meet-ing of the Association for Computational Linguistics(ACL?08), pp 710-718.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
Proceedings of the 43rd Annual Meetingof the ACL, pages 363C370.K.
Fukuda, A. Tamura, T. Tsunoda and T. Takagi.1998.
Toward information extraction: identifyingprotein names from biological papers.
Pacific Sym-posium on Biocomputing.
1998.Michel Galley.
2006.
A Skip-Chain Conditional Ran-dom Field for Ranking Meeting Utterances by Im-portance.
Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing(EMNLP 2006), pages 364-372.Han-Shen Huang, Yu-Shi Lin, Kuan-Ting Lin, Cheng-Ju Kuo, Yu-Ming Chang, Bo-Hou Yang, I-FangChung and Chun-Nan Hsu.
2007.
High-recall genemention recognition by unification of multiple back-ward parsing models.
Proceedings of the SecondBioCreative Challenge Evaluation Workshop, pages109-111.Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta andJun?ichi Tsujii.
2002.
Tuning support vectormachines for biomedical named entity recognition.Proceedings of the ACL-02 workshop on Naturallanguage processing in the biomedical domain - Vol-ume 3.Corinna Kolarik, Martin Hofmann-Apitius, Marc Zim-mermann and Juliane Fluck.
2007.
Identification ofnew drug classification terms in textual resources.Bioinformatics 2007 23(13):i264-i272Cheng-Ju Kuo, Yu-Ming Chang, Han-Shen Huang,Kuan-Ting Lin, Bo-Hou Yang, Yu-Shi Lin, Chun-Nan Hsu and I-Fang Chung.
2007.
Rich FeatureSet, Unification of Bidirectional Parsing and Dictio-nary Filtering for High F-Score Gene Mention Tag-ging.
Proceedings of the Second BioCreative Chal-lenge Evaluation Workshop, pages 105-107.Ki-Joong Lee, Young-Sook Hwang, Seonho Kim andHae-Chang Rim.
2004.
Biomedical named entityrecognition using two-phase model based on SVMs.Journal of Biomedical Informatics, Volume 37, Issue6, December 2004, Pages 436-447.John Lafferty, Andrew McCallum and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
ICML-01, pages 282-289,2001.Ryan McDonald and Fernando Pereira.
2005.
Identi-fying gene and protein mentions in text using con-ditional random fields.
BMC Bioinformatics 2005,6(Suppl 1):S6.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.M.
Narayanaswamy, K.E.
Ravikumar and K. Vijay-Shanker.
2003.
A biological named entity recog-nizer.
Pacific Symposium on Biocomputing.
2003.Dietrich Rebholz-Schuhmann, Harald Kirsch, MiguelArregui, Sylvain Gaudan, Mark Riethoven and Pe-ter Stoehr.
2007.
EBIMed?text crunching to gatherfacts for proteins from Medline.
Bioinformatics2007 23(2):e237-e244Fei Sha and Fernando Pereira.
2003.
Shallow Pars-ing with Conditional Random Fields.
Proceedingsof HLT-NAACL 2003, Main Papers, pp.134-141Larry Smith, Lorraine K Tanabe, et al 2008.Overview of BioCreative II gene mention recogni-tion.
Genome Biology 2008, 9(Suppl 2):S2.Charles Sutton and Andrew McCallum.
2004.
Collec-tive Segmentation and Labeling of Distant Entitiesin Information Extraction.
ICML workshop on Sta-tistical Relational Learning, 2004.Lorraine Tanabe, Natalie Xie, Lynne H Thom, WayneMatten and W John Wilbur.
2005.
GENETAG: atagged corpus for gene/protein named entity recog-nition .
BMC Bioinformatics 2005, 6(Suppl 1):S3Serhan Tatar and Ilyas Cicekli.
2009.
Two learningapproaches for protein name extraction.
Journal ofBiomedical Informatics 42(2009) 1046-1055Xinglong Wang, Jun?ichi Tsujii and Sophia Anani-adou.
2010.
Disambiguating the species of biomed-ical named entities using natural language parsers.Bioinformatics 2010 26(5):661-667Zhihao Yang, Hongfei Lin and Yanpeng Li.
2008.
Ex-ploiting the performance of dictionary-based bio-entity name recognition in biomedical literature.Computational Biology and Chemistry 32(2008)287-291.Alexander Yeh, Alexander Morgan, Marc Colosimoand Lynette Hirschman.
2005.
BioCreAtIvE Task1A: gene mention finding evaluation.
BMC Bioin-formatics 2005, 6(Suppl 1):S2.GuoDong Zhou, Jie Zhang, Jian Su, Dan Shen,ChewLim Tan.
2004.
Recognizing names inbiomedical texts: a machine learning approach.Bioinformatics 2004, Vol.20(7),pp.1178C1190.GuoDong Zhou, Dan Shen, Jie Zhang, Jian Su1 andSoonHeng Tan.
2005.
Recognition of protein/genenames from text using an ensemble of classifiers.BMC Bioinformatics 2005, 6(Suppl 1):S7.18
