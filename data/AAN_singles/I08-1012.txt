Dependency Parsing with Short Dependency Relations in Unlabeled DataWenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi IsaharaComputational Linguistics GroupNational Institute of Information and Communications Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jpAbstractThis paper presents an effective dependencyparsing approach of incorporating short de-pendency information from unlabeled data.The unlabeled data is automatically parsedby a deterministic dependency parser, whichcan provide relatively high performance forshort dependencies between words.
We thentrain another parser which uses the informa-tion on short dependency relations extractedfrom the output of the first parser.
Our pro-posed approach achieves an unlabeled at-tachment score of 86.52, an absolute 1.24%improvement over the baseline system onthe data set of Chinese Treebank.1 IntroductionIn dependency parsing, we attempt to build thedependency links between words from a sen-tence.
Given sufficient labeled data, there are sev-eral supervised learning methods for training high-performance dependency parsers(Nivre et al, 2007).However, current statistical dependency parsers pro-vide worse results if the dependency length be-comes longer (McDonald and Nivre, 2007).
Herethe length of a dependency from word wiand wordwjis simply equal to |i ?
j|.
Figure 1 shows theF1score1 provided by a deterministic parser rela-tive to dependency length on our testing data.
From1precision represents the percentage of predicted arcs oflength d that are correct and recall measures the percentage ofgold standard arcs of length d that are correctly predicted.F1= 2?
precision?
recall/(precision + recall)the figure, we find that F1score decreases when de-pendency length increases as (McDonald and Nivre,2007) found.
We also notice that the parser pro-vides good results for short dependencies (94.57%for dependency length = 1 and 89.40% for depen-dency length = 2).
In this paper, short dependencyrefers to the dependencies whose length is 1 or 2.304050607080901000  5  10  15  20  25  30F1Dependency LengthbaselineFigure 1: F-score relative to dependency lengthLabeled data is expensive, while unlabeled datacan be obtained easily.
In this paper, we present anapproach of incorporating unlabeled data for depen-dency parsing.
First, all the sentences in unlabeleddata are parsed by a dependency parser, which canprovide state-of-the-art performance.
We then ex-tract information on short dependency relations fromthe parsed data, because the performance for shortdependencies is relatively higher than others.
Fi-nally, we train another parser by using the informa-tion as features.The proposed method can be regarded as a semi-supervised learning method.
Currently, most semi-88supervised methods seem to do well with artificiallyrestricted labeled data, but they are unable to outper-form the best supervised baseline when more labeleddata is added.
In our experiments, we show that ourapproach significantly outperforms a state-of-the-artparser, which is trained on full labeled data.2 Motivation and previous workThe goal in dependency parsing is to tag dependencylinks that show the head-modifier relations betweenwords.
A simple example is in Figure 2, where thelink between a and bird denotes that a is the depen-dent of the head bird.I    see    a    beautiful    bird    .Figure 2: Example dependency graph.We define that word distance of word wiand wordwjis equal to |i ?
j|.
Usually, the two words in ahead-dependent relation in one sentence can be adja-cent words (word distance = 1) or neighboring words(word distance = 2) in other sentences.
For exam-ple, ?a?
and ?bird?
has head-dependent relation inthe sentence at Figure 2.
They can also be adjacentwords in the sentence ?I see a bird.
?.Suppose that our task is Chinese dependencyparsing.
Here, the string ????JJ(Specialist-level)/?
?NN(working)/?
?NN(discussion)?should be tagged as the solution (a) in Figure3.
However, our current parser may choose thesolution (b) in Figure 3 without any additionalinformation.
The point is how to assign the head for????(Specialist-level)?.
Is it ???
(working)?or ???(discussion)?
?      (b)(a)Figure 3: Two solutions for ????(Specialist-level)/??(working)/??
(discussion)?As Figure 1 suggests, the current dependencyparser is good at tagging the relation between ad-jacent words.
Thus, we expect that dependenciesof adjacent words can provide useful informationfor parsing words, whose word distances are longer.When we search the string ????(Specialist-level)/??(discussion)?
at google.com, many rele-vant documents can be retrieved.
If we have a goodparser, we may assign the relations between the twowords in the retrieved documents as Figure 4 shows.We can find that ???(discussion)?
is the head of????(Specialist-level)?
in many cases.1)?525	26///,//?2)? ,///!
"#$2004%218?3)?&'()*+,-.
(")*/01234///5?n)?6789:///;<=>?@A?.
)?/?Figure 4: Parsing ????(Specialist-level)/??(discussion)?
in unlabeled dataNow, consider what a learning model could doto assign the appropriate relation between ????(Specialist-level)?
and ???(discussion)?
in thestring ????(Specialist-level)/??(working)/??(discussion)?.
In this case, we provide additionalinformation to ???(discussion)?
as the possiblehead of ????(Specialist-level)?
in the unlabeleddata.
In this way, the learning model may use thisinformation to make correct decision.Till now, we demonstrate how to use the depen-dency relation between adjacent words in unlabeleddata to help tag the relation between two wordswhose word distance is 2.
In the similar way, we canalso assign the relation between two words whoseword distance is longer by using the information.Based on the above observations, we propose anapproach of exploiting the information from a large-scale unlabeled data for dependency parsing.
Weuse a parser to parse the sentences in unlabeled data.Then another parser makes use of the information onshort dependency relations in the newly parsed datato improve performance.Our study is relative to incorporating unlabeled89data into a model for parsing.
There are several otherstudies relevant to ours as described below.A simple method is self-training in which the ex-isting model first labels unlabeled data and then thenewly labeled data is then treated as hand annotateddata for training a new model.
But it seems that self-training is not so effective.
(Steedman et al, 2003)reports minor improvement by using self-trainingfor syntactic parsing on small labeled data.
The rea-son may be that errors in the original model wouldbe amplified in the new model.
(McClosky et al,2006) presents a successful instance of parsing withself-training by using a re-ranker.
As Figure 1 sug-gests, the dependency parser performs bad for pars-ing the words with long distances.
In our approach,we choose partial reliable information which comesfrom short dependency relations for the dependencyparser.
(Smith and Eisner, 2006) presents an approach toimprove the accuracy of a dependency grammar in-duction models by EM from unlabeled data.
Theyobtain consistent improvements by penalizing de-pendencies between two words that are farther apartin the string.The study most relevant to ours is done by (Kawa-hara and Kurohashi, 2006).
They present an in-tegrated probabilistic model for Japanese parsing.They also use partial information after current parserparses the sentences.
Our work differs in that weconsider general dependency relations while theyonly consider case frames.
And we represent addi-tional information as the features for learning mod-els while they use the case frames as one componentfor a probabilistic model.3 Our ApproachIn this section, we describe our approach of exploit-ing reliable features from unlabeled data, which isparsed by a basic parser.
We then train anotherparser based on new feature space.3.1 Training a basic parserIn this paper, we implement a deterministic parserbased on the model described by (Nivre, 2003).This model is simple and works very well in theshared-tasks of CoNLL2006(Nivre et al, 2006) andCoNLL2007(Hall et al, 2007).
In fact, our approachcan also be applied to other parsers, such as (Ya-mada and Matsumoto, 2003)?s parser, (McDonald etal., 2006)?s parser, and so on.3.1.1 The parserThe parser predicts unlabeled directed dependen-cies between words in sentences.
The algorithm(Nivre, 2003) makes a dependency parsing tree inone left-to-right pass over the input, and uses a stackto store the processed tokens.
The behaviors of theparser are defined by four elementary actions (whereTOP is the token on top of the stack and NEXT is thenext token in the original input string):?
Left-Arc(LA): Add an arc from NEXT to TOP;pop the stack.?
Right-Arc(RA): Add an arc from TOP toNEXT; push NEXT onto the stack.?
Reduce(RE): Pop the stack.?
Shift(SH): Push NEXT onto the stack.The first two actions mean that there is a dependencyrelation between TOP and NEXT.More information about the parser can be avail-able in the paper(Nivre, 2003).
The parser uses aclassifier to produce a sequence of actions for a sen-tence.
In our experiments, we use the SVM modelas the classifier.
More specifically, our parser usesLIBSVM(Chang and Lin, 2001) with a polynomialkernel (degree = 3) and the built-in one-versus-allstrategy for multi-class classification.3.1.2 Basic featuresWe represent basic features extracted from thefields of data representation, including word andpart-of-speech(POS) tags.
The basic features usedin our parser are listed as follows:?
The features based on words: the words of TOPand NEXT, the word of the head of TOP, thewords of leftmost and rightmost dependent ofTOP, and the word of the token immediatelyafter NEXT in original input string.?
The features based on POS: the POS of TOPand NEXT, the POS of the token immediatelybelow TOP, the POS of leftmost and rightmostdependent of TOP, the POS of next three tokensafter NEXT, and the POS of the token immedi-ately before NEXT in original input string.90With these basic features, we can train a state-of-the-art supervised parser on labeled data.
In the fol-lowing content, we call this parser Basic Parser.3.2 Unlabeled data preprocessing and parsingThe input of our approach is unlabeled data, whichcan be obtained easily.
For the Basic Parser, the cor-pus should have part-of-speech (POS) tags.
There-fore, we should assign the POS tags using a POStagger.
For Chinese sentences, we should segmentthe sentences into words before POS tagging.
Af-ter data preprocessing, we have the word-segmentedsentences with POS tags.
We then use the BasicParser to parse all sentences in unlabeled data.3.3 Using short dependency relations asfeaturesThe Basic Parser can provide complete dependencyparsing trees for all sentences in unlabeled data.
AsFigure 1 shows, short dependencies are more reli-able.
To offer reliable information for the model, wepropose the features based on short dependency re-lations from the newly parsed data.3.3.1 Collecting reliable informationIn a parsed sentence, if the dependency lengthof two words is 1 or 2, we add this word pairinto a list named DepList and count its frequency.We consider the direction and length of the de-pendency.
D1 refers to the pairs with dependencylength 1, D2 refers to the pairs with dependencylength 2, R refers to right arc, and L refers to leftarc.
For example, ????(specialist-level)?
and???(discussion)?
are adjacent words in a sentence???(We)/??(held)/???(specialist-level)/??(discussion)/b?
and have a left dependency arcassigned by the Basic Parser.
We add a word pair????(specialist-level)-??(discussion)?
with?D1-L?
and its frequency into the DepList.According to frequency, we then group wordpairs into different buckets, with a bucket ONEfor frequency 1, a single bucket LOW for 2-7, asingle bucket MID for 8-14, and a single bucketHIGH for 15+.
We choose these threshold val-ues via testing on development data.
For example,the frequency of the pair ????(specialist-level)-??(discussion)?
with ?D1-L?
is 20.
Then it isgrouped into the bucket ?D1-L-HIGH?.Here, we do not use the frequencies as the weightof the features.
We derive the weights of the featuresby the SVM model from training data rather thanapproximating the weights from unlabeled data.3.3.2 New featuresBased on the DepList, we represent new featuresfor training or parsing current two words: TOP andNEXT.
We consider word pairs from the contextaround TOP and NEXT, and get the buckets of thepairs in the DepList.First, we represent the features based on D1.
Wename these features D1 features.
The D1 featuresare listed according to different word distances be-tween TOP and NEXT as follows:1.
Word distance is 1: (TN0) the bucket of theword pair of TOP and NEXT, and (TN1) thebucket of the word pair of TOP and next tokenafter NEXT.2.
Word distance is 2 or 3+: (TN0) the bucket ofthe word pair of TOP and NEXT, (TN1) thebucket of the word pair of TOP and next tokenafter NEXT, and (TN 1) the bucket of the wordpair of TOP and the token immediately beforeNEXT.In item 2), all features are in turn combined withtwo sets of distances: a set for distance 2 anda single set for distances 3+.
Thus, we have 8types of D1 features, including 2 types in item1) and 6 types in item 2).
The feature is format-ted as ?Position:WordDistance:PairBucket?.
Forexample, we have the string ????(specialist-level)/w1/w2/w3/?
?
(discussion)?, and ??
??(specialist-level)?
is TOP and ???
(discussion)?is NEXT.
Thus we can have the feature?TN0:3+:D1-L-HIGH?
for TOP and NEXT,because the word distance is 4(3+) and ????(specialist-level)-??(discussion)?
belongs tothe bucket ?D1-L-HIGH?.
Here, if a string belongsto two buckets, we use the most frequent bucket.Then, we represent the features based on D2.
Wename these features D2 features.
The D2 featuresare listed as follows:1.
Word distance is 1: (TN1) the bucket of theword pair of TOP and next token after NEXT.912.
Word distance is 2: (TN0) the bucket of theword pair of TOP and NEXT, and (TN1) thebucket of the word pair of TOP and next tokenafter NEXT.4 ExperimentsFor labeled data, we used the Chinese Treebank(CTB) version 4.02 in our experiments.
We used thesame rules for conversion and created the same datasplit as (Wang et al, 2007): files 1-270 and 400-931as training, 271-300 as testing and files 301-325 asdevelopment.
We used the gold standard segmenta-tion and POS tags in the CTB.For unlabeled data, we used the PFR corpus 3.It includes the documents from People?s Daily at1998 (12 months).
There are about 290 thousandsentences and 15 million words in the PFR corpus.To simplify, we used its segmentation.
And we dis-carded the POS tags because PFR and CTB used dif-ferent POS sets.
We used the package TNT (Brants,2000), a very efficient statistical part-of-speech tag-ger, to train a POS tagger4 on training data of theCTB.We measured the quality of the parser by the un-labeled attachment score (UAS), i.e., the percentageof tokens with correct HEAD.
We reported two typesof scores: ?UAS without p?
is the UAS score with-out all punctuation tokens and ?UAS with p?
is theone with all punctuation tokens.4.1 Experimental resultsIn the experiments, we trained the parsers on train-ing data and tuned the parameters on developmentdata.
In the following sessions, ?baseline?
refersto Basic Parser (the model with basic features), and?OURS?
refers to our proposed parser (the modelwith all features).4.1.1 Our approachTable 1 shows the results of the parser with differ-ent feature sets, where ?+D1?
refers to the parser2More detailed information can be found athttp://www.cis.upenn.edu/?chinese/.3More detailed information can be found athttp://www.icl.pku.edu.4To know whether our POS tagger is good, we also testedthe TNT package on the standard training and testing sets forfull parsing (Wang et al, 2006).
The TNT-based tagger pro-vided 91.52% accuracy, the comparative result with (Wang etal., 2006).with basic features and D1 features, and ?+D2?refers to the parser with all features(basic features,D1 features, and D2 features).
From the table, wefound a large improvement (1.12% for UAS with-out p and 1.23% for UAS with p) from adding D1features.
And D2 features provided minor improve-ment, 0.12% for UAS without p and 0.14% for UASwith p. This may be due to the information from de-pendency length 2 containing more noise.
Totally,we achieved 1.24% improvement for UAS with pand 1.37% for UAS without p. The improvementis significant in one-tail paired t-test (p < 10?5).Table 1: The results with different feature setsUAS without p UAS with pbaseline 85.28 83.79+D1 86.40 85.02+D2(OURS) 86.52 85.16We also attempted to discover the effect of dif-ferent numbers of unlabeled sentences to use.
Ta-ble 2 shows the results with different numbers ofsentences.
Here, we randomly chose different per-centages of sentences from unlabeled data.
Whenwe used 1% sentences of unlabeled data, the parserachieved a large improvement.
As we added moresentences, the parser obtained more benefit.Table 2: The results with different numbers of unla-beled sentencesSentences UAS without p UAS with p0%(baseline) 85.28 83.791% 85.68 84.402% 85.69 84.515% 85.78 84.5910% 85.97 84.6220% 86.25 84.8650% 86.34 84.92100%(OURS) 86.52 85.164.1.2 Comparison of other systemsFinally, we compare our parser to the state ofthe art.
We used the same testing data as (Wanget al, 2005) did, selecting the sentences length upto 40.
Table 3 shows the results achieved by ourmethod and other researchers (UAS with p), whereWang05 refers to (Wang et al, 2005), Wang07 refers92to (Wang et al, 2007), and McDonald&Pereira065refers to (McDonald and Pereira, 2006).
From thetable, we found that our parser performed best.Table 3: The results on the sentences length up to 40UAS with pWang05 79.9McDonald&Pereira06 82.5Wang07 86.6baseline 87.1OURS 88.45 Analysis5.1 Improvement relative to dependency lengthWe now look at the improvement relative to depen-dency length as Figure 5 shows.
From the figure, wefound that our method provided better performancewhen dependency lengths are less than 13.
Espe-cially, we had improvements 2.35% for dependencylength 4, 3.13% for length 5, 2.56% for length 6, and4.90% for length 7.
For longer ones, the parser cannot provide stable improvement.
The reason maybe that shorter dependencies are often modifier ofnouns such as determiners or adjectives or pronounsmodifying their direct neighbors, while longer de-pendencies typically represent modifiers of the rootor the main verb in a sentence(McDonald and Nivre,2007).
We did not provide new features for modi-fiers of the root.304050607080901000  5  10  15  20F1Dependency LengthbaselineOURSFigure 5: Improvement relative to dependencylength5(Wang, 2007) reported this result.JJ  NN  NNJJ     NN    NNJJ      NN   NNNN  NN  NNNN    NN   NNNN     NN   NNAD  VV VVAD    VV    VVAD    VV     VVJJ  NN  CC NNJJ  NN  CC NNJJ  NN  CC NNFigure 6: Ambiguities5.2 Cases study in neighborhoodIn Chinese dependency parsing, there are many am-biguities in neighborhood, such as ?JJ NN NN?,?AD VV VV?, ?NN NN NN?, ?JJ NN CC NN?.They have possible parsing trees as Figure 6 shows.For these ambiguities, our approach can provideadditional information for the parser.
For ex-ample, we have the following case in the dataset: ??
?JJ(friendly)/ ?
?NN(corporation)/ ??NN(relationship)/?.
We can provide additional in-formation about the relations of ???JJ(friendly)/??NN(corporation)?
and ??
?JJ(friendly)/ ??NN(relationship)/?
in unlabeled data to help theparser make the correct decision.Our approach can also work for the longer con-structions, such as ?JJ NN NN NN?
and ?NN NNNN NN?
in the similar way.For the construction ?JJ NN1 CC NN2?, wenow do not define special features to solvethe ambiguity.
However, based on the cur-rent DepList, we can also provide additionalinformation about the relations of JJ/NN1 andJJ/NN2.
For example, for the string ???
?JJ(further)/ ?
?NN(improvement)/ ?CC(and)/?
?NN(development)/?, the parser often assigns??
?(improvement)?
as the head of ??
??(further)?
instead of ???(development)?.
Thereis an entry ????(further)-??(development)?
inthe DepList.
Here, we need a coordination identifierto identify these constructions.
After that, we canprovide the information for the model.6 ConclusionThis paper presents an effective approach to improvedependency parsing by using unlabeled data.
We ex-tract the information on short dependency relations93in an automatically generated corpus parsed by a ba-sic parser.
We then train a new parser with the infor-mation.
The new parser achieves an absolute im-provement of 1.24% over the state-of-the-art parseron Chinese Treebank (from 85.28% to 86.52%).There are many ways in which this researchshould be continued.
First, feature representationneeds to be improved.
Here, we use a simple fea-ture representation on short dependency relations.We may use a combined representation to use the in-formation from long dependency relations even theyare not so reliable.
Second, we can try to select moreaccurately parsed sentences.
Then we may collectmore reliable information than the current one.ReferencesT.
Brants.
2000.
TnT?a statistical part-of-speech tagger.Proceedings of the 6th Conference on Applied NaturalLanguage Processing, pages 224?231.C.C.
Chang and C.J.
Lin.
2001.
LIBSVM: a libraryfor support vector machines.
Software available athttp://www.
csie.
ntu.
edu.
tw/cjlin/libsvm, 80:604?611.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryigit,Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single malt or blended?
a study in multilingualparser optimization.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages933?939.D.
Kawahara and S. Kurohashi.
2006.
A fully-lexicalized probabilistic model for Japanese syntacticand case structure analysis.
Proceedings of the mainconference on Human Language Technology Confer-ence of the North American Chapter of the Associationof Computational Linguistics, pages 176?183.D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe ACL, pages 337?344.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency parsingmodels.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 122?131.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.of the 11th Conf.
of the European Chapter of the ACL(EACL).Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proceedings of theTenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 216?220, NewYork City, June.
Association for Computational Lin-guistics.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit, and S Marinov.2006.
Labeled pseudo-projective dependency parsingwith support vector machines.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proc.
of theJoint Conf.
on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).J.
Nivre.
2003.
An efficient algorithm for projectivedependency parsing.
Proceedings of the 8th Inter-national Workshop on Parsing Technologies (IWPT),pages 149?160.Noah A. Smith and Jason Eisner.
2006.
Annealing struc-tural bias in multilingual weighted grammar induction.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 569?576, Sydney, Australia, July.
Associationfor Computational Linguistics.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
The Proceedings of the Annual Meeting ofthe European Chapter of the ACL, pages 331?338.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005.Strictly lexical dependency parsing.
In IWPT2005.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.2006.
A Fast, Accurate Deterministic Parser for Chi-nese.
In Coling-ACL2006.Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007.Simple training of dependency parsers via structuredboosting.
In IJCAI2007.Qin Iris Wang.
2007.
Learning structured classifiers forstatistical dependency parsing.
In NAACL-HLT 2007Doctoral Consortium.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.of the 8th Intern.
Workshop on Parsing Technologies(IWPT), pages 195?206.94
