Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 21?27,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsLearning non-concatenative morphologyMichelle A. FullwoodDept.
of Linguistics and PhilosophyMassachusetts Institute of Technologymaf@mit.eduTimothy J. O?DonnellDept.
of Brain and Cognitive SciencesMassachusetts Institute of Technologytimod@mit.eduAbstractRecent work in computational psycholin-guistics shows that morpheme lexica canbe acquired in an unsupervised man-ner from a corpus of words by select-ing the lexicon that best balances pro-ductivity and reuse (e.g.
Goldwater etal.
(2009) and others).
In this paper,we extend such work to the problem ofacquiring non-concatenative morphology,proposing a simple model of morphologythat can handle both concatenative andnon-concatenative morphology and apply-ing Bayesian inference on two datasets ofArabic and English verbs to acquire lex-ica.
We show that our approach success-fully extracts the non-contiguous triliteralroot from Arabic verb stems.1 IntroductionWhat are the basic structure-building operationsthat enable the creative use of language, and howdo children exposed to a language acquire the in-ventory of primitive units which are used to formnew expressions?
In the case of word forma-tion, recent work in computational psycholinguis-tics has shown how an inventory of morphemescan be acquired by selecting a lexicon that bestbalances the ability of individual sound sequencesto combine productively against the reusability ofthose sequences (e.g., Brent (1999), Goldwater etal.
(2009), Feldman et al(2009), O?Donnell et al(2011), Lee et al(2011).)
However, this workhas focused almost exclusively on one kind ofstructure-building operation: concatenation.
Thelanguages of the world, however, exhibit a varietyof other, non-concatenative word-formation pro-cesses (Spencer, 1991).Famously, the predominant mode of Semiticword formation is non-concatenative.
For exam-ple, the following Arabic words, all related tothe concept of writing, share no contiguous se-quences of segments (i.e., phones), but they doshare a discontinuous subsequence?ktb, whichhas been traditionally analyzed as an independentmorpheme, termed the ?root?.kataba ?he wrote?kutiba ?it was written?yaktubu ?he writes?ka:tib ?writer?kita:b ?book?kutub ?books?maktab ?office?Table 1: List of Arabic words with root?ktbMany Arabic words appear to be constructedvia a process of interleaving segments from dif-ferent morphemes, as opposed to concatenation.ConcatenativecookPASTc o o k e dNon-concatenativecookPASTT a b a x aFigure 1: Schematic of concatenative vs non-concatenative morphologySuch non-concatenative morphology is perva-sive in the world?s languages.
Even English,whose morphology is fundamentally concatena-tive, displays pockets of non-concatenative behav-ior, for example in the irregular past tenses (seeTable 2).In these words, the stem vowels undergo ablautchanging between tenses.
This cannot be han-dled in a purely concatenative framework unlesswe consider these words listed exceptions.
How-ever, such irregulars do show limited productiv-21bite /bajt/ bit /bIt/sing /sIN/ sang /s?N/give /gIv/ gave /gejv/feel /fil/ felt /fElt/Table 2: Examples of English irregular verbsity (see Albright and Hayes (2003), Prasada andPinker (1993), Bybee and Slobin (1982), Bybeeand Moder (1983), Ambridge (2010)), and in otherlanguages such stem changing processes are fullyproductive.In Semitic, it is clear that non-concatenativeword formation is productive.
Borrowings fromother languages are modified to fit the avail-able non-concatenative templates.
This has alsobeen tested psycholinguistically: Berman (2003),for instance, shows that Hebrew-speaking pre-schoolers can productively form novel verbs outof nouns and adjectives, a process that requires theability to extract roots and apply them to existingverbal templates.Any model of word formation, therefore, needsto be capable of generalizing to both concatenativeand non-concatenative morphological systems.
Inthis paper, we propose a computational model ofword formation which is capable of capturing bothtypes of morphology, and explore its ramificationsfor morphological segmentation.We apply Bayesian inference on a small cor-pus of Arabic and English words to learn the mor-phemes that comprise them, successfully learningthe Arabic root with great accuracy, but less suc-cessfully English verbal inflectional suffixes.
Wethen examine the shortcomings of the model andpropose further directions.2 Arabic Verbal MorphologyIn this paper, we focus on Arabic verbal stem mor-phology.
The Arabic verbal stem is built fromthe interleaving of a consonantal root and a vo-calism that conveys voice (active/passive) and as-pect (perfect/imperfect).
The stem can then un-dergo further derivational prefixation or infixation.To this stem inflectional affixes indicating the sub-ject?s person, number and gender are then added.In the present work, we focus on stem morphol-ogy, leaving inflectional morphology to future ex-tensions of the model.There are nine common forms of the Arabic ver-bal stem, also known by the Hebrew grammati-cal term binyan.
In Table 3,?fQl represents thetriconsonantal root.
Only the perfect forms aregiven.Form Active PassiveI faQal fuQilII faQQal fuQQilIII faaQal fuuQilIV PafQal PufQilV tafaQQal tufuQQilVI tafaaQal tufuuQilVII PinfaQal -VIII PiftaQal PiftiQilX PistafQal PistufQilTable 3: List of common Arabic verbal binyanimEach of these forms has traditionally been asso-ciated with a particular semantics.
For example,Form II verbs are generally causatives of Form Iverbs, as is kattab ?to cause to write?
(c.f.
katab?to write?).
However, as is commonly the casewith derivational morphology, these semantic as-sociations are not completely regular: many formshave been lexicalized with alternative or more spe-cific meanings.2.1 Theoretical accountsThe traditional Arab grammarians?
account of theArabic verb was as follows: each form was asso-ciated with a template with slots labelled C1, C2and C3, traditionally represented with the conso-nants?fQl, as described above.
The actual rootconsonants were slotted into these gaps.
Thus thetemplate of the Form VIII active perfect verb stemwas taC1aC2C2aC3.
This, combined with the tri-consonantal root, made up the verbal stem.Template t a C1 a C2 C2 a C3Root f Q lFigure 2: Traditional analysis of Arabic Form VverbThe first generative linguistic treatment of Ara-bic verbal morphology (McCarthy, 1979; Mc-Carthy, 1981) adopted the notion of the rootand template, but split off the derivational pre-fixes and infixes and vocalism from the template.Borrowing from the technology of autosegmentalphonology (Goldsmith, 1976), the template was22now comprised of C(onsonant) and V(owel) slots.Rules governing the spreading of segments en-sured that consonants and vowels appeared in thecorrect positions within a template.Under McCarthy?s model, the analysis for[tafaQQal] would be as follows:CV Template C V C V C C V CPrefixt Rootf Q lVocalismaFigure 3: McCarthy analysis of Arabic Form VverbWhile increasing the number of morphemes as-sociated with each verb, the McCarthy approacheconomized on the variety of such units in the lex-icon.
The inventory of CV templates was limited;there were three vocalisms corresponding to activeand passive voice intersecting with perfect and im-perfect aspect; and only four derivational prefixes(/P/,/n/,/t/,/st/), one of which became an infix viamorphophonological rule in Form VIII.1We adopt a middle ground between the tradi-tional Arab grammarians?
description of the ver-bal stem and McCarthy?s analysis as our startingpoint.
We describe this approach in the next sec-tion.3 The ApproachOur initial model of morphology adopts Mc-Carthy?s notion of an abstract template, but coa-lesces the prefixes and infixes with the vocalisminto what we term the ?residue.?
Each stem isthus composed of two morphemes: the root andthe residue, and their interleaving is dictated by atemplate with slots for root and residue segments.For example, Piktatab = - - - r - - r - r (template)+ ktb (root) + Pitaa (residue), where r indicates aroot segment and - a residue segment.The residue may be of length 0, effectively mak-ing the word consist of a single morpheme.
Con-catenative morphology may be modelled in this1Other theories of Arabic morphology that reject the ex-istence of the root are also extant in the literature; see e.g.
(Bat-El, 1994) for a stem modification and vowel overwritingapproach.framework by grouping all the root segments to-gether, for example cooked [kukt] = r r r - (tem-plate) + kuk (root) + t (residue).The template, root and residue are each drawnfrom a separate sub-lexicon, modeled using toolsfrom Bayesian non-parametric statistics (see Sec-tion 4).
These tools put a prior distribution on thelexica that biases them in favour of reusing exist-ing frequent forms and small lexica by promotingmaximal sharing of morphemes.When applied to data, we derive a segmentationfor each word into a root and a residue.4 ModelFollowing earlier work on Bayesian lexicon learn-ing (e.g.
Goldwater et al(2009), we use a distri-bution over lexical items known as the Pitman?YorProcess (PYP) (Pitman and Yor, 1995).
LetG be adistribution over primitive phonological elementsof the lexicon (e.g., words, roots, residues, tem-plates, morphemes, etc.).
The behavior of PYPprocess PYP(a, b,G) with base measure G and pa-rameters a and b can be described as follows.
Thefirst time we sample from PYP(a, b,G) a new lex-ical item will be sampled using G. On subsequentsamples from PYP(a, b,G), we either reuse an ex-isting lexical item i with probability ni?aN+b , whereN is the number of lexical items sampled so far, niis the number of times that lexical item i has beenused in the past, and 0 ?
a ?
1 and b > ?a areparameters of the model.
Alternatively, we samplea new lexical item with probability aK+bN+b , whereK is the number of times a new lexical item wassampled in the past from the underlying distribu-tionG.
Notice that this process induces a rich-get-richer scheme for sampling from the process.
Themore a particular lexical item has been reused, themore likely it is to be reused in the future.
ThePitman?Yor process also produces a bias towardssmaller, more compact lexica.In our model, we maintain three sublexica fortemplates (LTp), roots (LRt), and residues (LRs)each drawn from a Pitman?Yor process with itsown hyperparameters.LX ?
PYP(aX, bX , GX) (1)where X ?
{Tp,Rt,Rs} Words are drawn byfirst drawing a template, then drawing a root anda residue (of the appropriate length) and insertingthe segments from the root and residue in the ap-propriate positions in the word as indicated by the23template.
Our templates are strings in {Rt,Rs}?indicating for each position in a word whether thatposition is part of the word?s root (Rt) or residue(Rs).
These templates themselves are drawn froma base measure GTp which is defined as follows.To add a new template to the template lexicon firstdraw a length for that template, K, from a Poissondistribution.K ?
POISSON(5) (2)We then sample a template of length K bydrawing a Bernoulli random variable ti for eachposition i ?
1..K is a root or residue position.ti ?
BERNOULLI(?)
(3)The base measure over templates, GTp, is de-fined as the concatenation of the ti?s.The base distributions over roots and residues,GRt and GRs, are drawn in the following manner.Having drawn a template, T we know the lengthsof the root, KRt, and residue KRt.
For each posi-tion in the root or residue ri where i ?
1..KRt/Rs,we sample a phone from a uniform distributionover phones.ri ?
UNIFORM(|alphabet|) (4)5 InferenceInference was performed via Metropolis?Hastingssampling.
The sampler was initialized by assign-ing a random template to each word in the trainingcorpus.
The algorithm then sampled a new tem-plate, root, and residue for each word in the corpusin turn.
The proposal distribution over templatesfor our sampler considered all templates currentlyin use by another word, as well as a randomly gen-erated template from the prior.
Samples from thisproposal distribution were corrected into the truedistribution using the Metropolis?Hastings crite-rion.6 Related workThe approach of this paper builds on previ-ous work on Bayesian lexicon learning start-ing with Goldwater et al(2009).
However,to our knowledge, this approach has not beenapplied to non-concatenative morphological seg-mentation.
Where it has been applied to Arabic(e.g.
Lee et al(2011)), it has been applied to un-vowelled text, since standard Arabic orthographydrops short vowels.
However, this has the effect ofreducing the problem mostly to one of concatena-tive morphology.Non-concatenative morphology has been ap-proached computationally via other research,however.
Kataja and Koskenniemi (1988) firstshowed that Semitic roots and patterns could bedescribed using regular languages.
This insightwas subsequently computationally implementedusing finite state methods by Beesley (1991) andothers.
Roark and Sproat (2007) present a modelof both concatenative and non-concatenative mor-phology based on the operation of compositionthat is similar to the one we describe above.The narrower problem of isolating roots fromSemitic words, for instance as a precursor to in-formation retrieval, has also received much atten-tion.
Existing approaches appear to be mostlyrule-based or dictionary-based (see Al-Shawakfaet al(2010) for a recent survey).7 ExperimentsWe applied the morphological model and infer-ence procedure described in Sections 4 and 5 totwo datasets of Arabic and English.7.1 DataThe Arabic corpus for this experiment consistedof verbal stems taken from the verb concordanceof the Quranic Arabic Corpus (Dukes, 2011).
Allpossible active, passive, perfect and imperfectfully-vowelled verbal stems for Forms I?X, ex-cluding the relatively rare Form IX, were gener-ated.
We used this corpus rather than a lexicon asour starting point to obtain a list of relatively highfrequency verbs.This list of stems was then filtered in two ways:first, only triconsonantal ?strong?
roots were con-sidered.
The so-called ?weak?
roots of Arabic ei-ther include a vowel or semi-vowel, or a doubledconsonant.
These undergo segmental changes invarious environments, which cannot be handled byour current generative model.Secondly, the list was filtered through the Buck-walter stem lexicon (Buckwalter, 2002) to obtainonly stems that were licit according to the Buck-walter morphological analyzer.This process yielded 1563 verbal stems, com-prising 427 unique roots, 26 residues, and 9 tem-plates.
The stems were supplied to the sampler inthe Buckwalter transliteration.24The English corpus was constructed along sim-ilar lines.
All verb forms related to the 299 mostfrequent lemmas in the Penn Treebank (Marcus etal., 1999) were used, excluding auxiliaries such asmight or should.
Each lemma thus had up to fiveverbal forms associated with it: the bare form (for-get), the third person singular present (forgets), thegerund (forgetting), past tense (forgot), and pastparticiple (forgotten).This resulted in 1549 verbal forms, compris-ing 295 unique roots, 108 residues, and 55 tem-plates.
CELEX (Baayen et al 1995) pronuncia-tions for these words were supplied to the samplerin CELEX?s DISC transliteration.Deriving a gold standard analysis for Englishverbs was less straightforward than in the Arabiccase.
The following convention was used: Theroot was any subsequence of segments shared byall the forms related to the same lemma.
Thus, forthe example lemma of forget, the correct template,root and residue were deemed to be:forget f@gEt r r r - r f@gt Eforgets f@gEts r r r - r - f@gt Esforgot f@gQt r r r - r f@gt Qforgetting f@gEtIN r r r - r - - f@gt EINforgotten f@gQtH r r r - r - f@gt QHTable 4: Correct analyses under the root/residuemodel for the lemma forget37 templates were concatenative, and 18 non-concatenative.
The latter were necessary to ac-commodate 46 irregular lemmas associated with254 forms.7.2 Results and DiscussionWe ran 10 instances of the sampler for 200 sweepsthrough the data.
For the Arabic training set, thisnumber of sweeps typically resulted in the sam-pler finding a local mode of the posterior, makingfew further changes to the state during longer runs.An identical experimental set-up was used for En-glish.
Evaluation was performed on the final stateof each sampler instance.The correctness of the sampler?s output wasmeasured in terms of the accuracy of the tem-plates it predicted for each word.
The word-levelaccuracy indicates the number of words that hadtheir entire template correctly sampled, while thesegment-level accuracy metric gives partial creditby considering the average number of correct bitsr -r -rr -- r- rr -r -- rr -- -r -r- -r -r -r- -r -r -- r- -r -- -r -r- -- -r -r -r- -- -- -r -r -r020406080100AccuracyFigure 4: Unweighted accuracy with which eachtemplate was sampled(r versus -) in each sampled template.Table 5 shows the average accuracy of the 10samples, weighted by each sample?s joint proba-bility.Accuracy Word-level Segment-levelArabic 92.3% 98.2%English 43.9% 85.3%Table 5: Average weighted accuracy of samplesArabic Analyses Figure 4 shows the averageunweighted accuracy with which each of the 9Arabic templates was sampled.Figure 4 reveals an effect of both the rarity andthe length of each template.
For instance, the per-formance on template r - - r - r (second bar fromleft) is exceptionally low, but this is the resultof there being only one instance of this templatein the training set: Euwqib, the passive form ofthe Form III verb of root Eqb, in the Buckwaltertransliteration.2 In addition, the longer the word,2This is an artifact of Arabic orthography and the Buck-walter transliteration, which puts the active form EAqab withtemplate r - r - r in correspondence with the passive templater - - r - r.25the poorer the performance of the model.
This islikely the result of the difficulty of searching overthe space of templates for longer forms.
Since thenumber of potential templates increases exponen-tially with the length of the form, finding the cor-rect template becomes increasingly difficult.
Thisproblem can likely be addressed in future mod-els by adopting an analysis similar to McCarthy?swhereby the residue is further subdivided into vo-calism, prefixes and infixes.
Note that even in suchlong forms, however, the letters belonging to theroot were generally isolated in one of the two mor-phemes.English Analyses The English experimentyielded poorer results than the Arabic dataset.The statistics of the datasets reveal the cause of thefailure of the English model: the English datasethad several times more residues and templatesthan the Arabic dataset did, thus lacking as muchuniform structure.
Nevertheless, the relativelyhigh segment-level accuracy shows that the modeltended to find templates that were only incorrectin 1 or 2 positions.The dominant pattern of errors was in the di-rection of overgeneralization of the concatenativetemplates to the irregular forms.
Out of the 254words related to a lemma with an irregular pastform, 241 received incorrect templates, 232 ofwhich were concatenative, often correctly splittingoff the regular suffix where there was one.
Forexample, sing and singing were parsed as sing+?and sing+ing, while sung was parsed as a separateroot.
Note that under an analysis of English ir-regulars as separate memorized lexical items, thesampler behaved correctly in such cases.However, out of 1295 words related to perfectlyregular lemmas, the sampler determined 628 tem-plates incorrectly.
Out of these, 325 were givenconcatenative templates, but with too much or toolittle segmental material allocated to the suffix.For example, the word invert was analyzed as in-ver+t, with its other forms following suit as in-ver+ted, inver+ting and inver+ts.
This is likelydue to subregularities in the word corpus: withmany words ending with -t, this analysis becomesmore attractive.The remaining 303 regular verbs were givennon-concatenative templates.
For instance, iden-tify was split up into dfy and ienti.
No consistentpattern could be discerned from these cases.8 ConclusionWe have proposed a model of morpheme-lexiconlearning that is capable of handling concatena-tive and non-concatenative morphology up to thelevel of two morphemes.
We have seen thatBayesian inference on this model with an Ara-bic dataset of verbal stems successfully learns thenon-contiguous root and residue as morphemes.In future work, we intend to extend our sim-plified model of morphology to McCarthy?s com-plete model by adding concatenative prefixationand suffixation processes and segment-spreadingrules.
Besides being capable of handling the in-flectional aspects of Arabic morphology, we an-ticipate that this extension will improve the per-formance of the model on Arabic verbal stemsas well, since the number of non-concatenativetemplates that have to be learned will decrease.For example, the template for the Form V verb[tafaQQal] can be reduced to that for the Form IIverb [faQQal] plus an additional prefix.We also anticipate that the performance on En-glish will be vastly improved, since the dominantmode of word formation in English is concate-native, while the small number of irregular pasttenses and plurals that undergo ablaut can be han-dled using the non-concatenative architecture ofthe model.
This would also be more in line withnative speakers?
intuitions and linguistic analysesof English morphology.AcknowledgmentsParts of the sampler code were written by Pe-ter Graff.
We would also like to thank AdamAlbright and audiences at the MIT PhonologyCircle and the Northeast Computational Phonol-ogy Workshop (NECPhon) for feedback on thisproject.
This material is based upon work sup-ported by the National Science Foundation Gradu-ate Research Fellowship Program under Grant No.1122374.ReferencesEmad Al-Shawakfa, Amer Al-Badarneh, Safwan Shat-nawi, Khaleel Al-Rabab?ah, and Basel Bani-Ismail.2010.
A comparison study of some Arabic root find-ing algorithms.
Journal of the American Society forInformation Science and Technology, 61(5):1015?1024.Adam Albright and Bruce Hayes.
2003.
Rulesvs.
analogy in English past tenses: A computa-26tional/experimental study.
Cognition, 90(2):119?161.Ben Ambridge.
2010.
Children?s judgments of regularand irregular novel past?tense forms: New data onthe English past?tense debate.
Developmental Psy-chology, In Press.Harald R. Baayen, Richard Piepenbrock, and Leon Gu-likers.
1995.
The CELEX Lexical Database.
Re-lease 2 (CD-ROM).
Linguistic Data Consortium,University of Pennsylvania, Philadelphia, Pennsyl-vania.Outi Bat-El.
1994.
Stem modification and clustertransfer in Modern Hebrew.
Natural Language andLinguistic Theory, 12:571?593.Kenneth R. Beesley.
1991.
Computer analysis ofArabic morphology: A two-level approach with de-tours.
In Bernard Comrie and Mushira Eid, editors,Perspectives on Arabic Linguistics III: Papers fromthe Third Annual Symposium on Arabic Linguistics,pages 155?172.
John Benjamins.
Read originally atthe Third Annual Symposium on Arabic Linguistics,University of Utah, Salt Lake City, Utah, 3-4 March1989.Ruth A. Berman.
2003.
Children?s lexical innova-tions.
In Joseph Shimron, editor, Language Process-ing and Acquisition in Languages of Semitic, Root-based, Morphology, pages 243?292.
John Ben-jamins.Michael R. Brent.
1999.
Speech segmentationand word discovery: A computational perspective.Trends in Cognitive Sciences, 3(8):294?301, Au-gust.Tim Buckwalter.
2002.
Buckwalter Arabic mor-phological analyzer version 1.0.
Technical ReportLDC2002L49, Linguistic Data Consortium.Joan L. Bybee and Carol Lynn Moder.
1983.
Mor-phological classes as natural categories.
Language,59(2):251?270, June.Joan L. Bybee and Daniel I. Slobin.
1982.
Rules andschemas in the development and use of the Englishpast tense.
Language, 58(2):265?289.Kais Dukes.
2011.
Quranic Arabic Corpus.http://corpus.quran.com/.Naomi H. Feldman, Thomas L. Griffiths, and James L.Morgan.
2009.
Learning phonetic categories bylearning a lexicon.
In Proceedings of the 31st An-nual Meeting of the Cognitive Science Society.John Anton Goldsmith.
1976.
Autosegmental Phonol-ogy.
Ph.D. thesis, Massachusetts Institute of Tech-nology.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112:21?54.Laura Kataja and Kimmo Koskenniemi.
1988.
Finite-state description of Semitic morphology: a casestudy of Ancient Akkadian.
In Proceedings of the12th conference on Computational linguistics - Vol-ume 1, COLING ?88, pages 313?315, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Yoong Keok Lee, Aria Haghighi, and Regina Barzi-lay.
2011.
Modeling syntactic context improvesmorphological segmentation.
In Proceedings of theConference on Natural Language Learning.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank3 technical report.
Technical report, Linguistic DataConsortium, Philadelphia.John J. McCarthy.
1979.
Formal Problems in SemiticPhonology and Morphology.
Ph.D. thesis, Mas-sachusetts Institute of Technology.John J. McCarthy.
1981.
A prosodic theory of noncon-catenative morphology.
Linguistic Inquiry, 12:373?418.Timothy J. O?Donnell, Jesse Snedeker, Joshua B.Tenenbaum, and Noah D. Goodman.
2011.
Pro-ductivity and reuse in language.
In Proceedings ofthe 33rd Annual Conference of the Cognitive ScienceSociety.Jim Pitman and Marc Yor.
1995.
The two-parameterPoisson?Dirichlet distribution derived from a sta-ble subordinator.
Technical report, Department ofStatistics University of California, Berkeley.Sandeep Prasada and Steven Pinker.
1993.
Generalisa-tion of regular and irregular morphological patterns.Language and Cognitive Processes, 8(1):1?56.Brian Roark and Richard Sproat.
2007.
Computa-tional Approaches to Morphology and Syntax.
Ox-ford University Press.Andrew Spencer.
1991.
Morphological Theory.Blackwell.27
