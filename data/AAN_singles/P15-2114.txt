Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 694?699,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Hybrid Representations to RetrieveSemantically Equivalent QuestionsC?
?cero dos Santos1, Luciano Barbosa1, Dasha Bogdanova2, Bianca Zadrozny11IBM Research, 138/146 Av.
Pasteur, Rio de Janeiro, Brazil{cicerons,lucianoa,biancaz}@br.ibm.com2ADAPT centre, School of Computing, Dublin City University, Dublin, Irelanddbogdanova@computing.dcu.ieAbstractRetrieving similar questions in onlineQ&A community sites is a difficult taskbecause different users may formulate thesame question in a variety of ways, us-ing different vocabulary and structure.In this work, we propose a new neuralnetwork architecture to perform the taskof semantically equivalent question re-trieval.
The proposed architecture, whichwe call BOW-CNN, combines a bag-of-words (BOW) representation with a dis-tributed vector representation created by aconvolutional neural network (CNN).
Weperform experiments using data collectedfrom two Stack Exchange communities.Our experimental results evidence that: (1)BOW-CNN is more effective than BOWbased information retrieval methods suchas TFIDF; (2) BOW-CNN is more robustthan the pure CNN for long texts.1 IntroductionMost Question-answering (Q&A) communitysites advise users before posting a new questionto search for similar questions.
This is not alwaysan easy task because different users may formulatethe same question in a variety of ways.We define two questions as semantically equiv-alent if they can be adequately answered by theexact same answer.
Here is an example of a pairof such questions from Ask Ubuntu community,which is part of the Stack Exchange Q&A com-munity site: (q1)?I have downloaded ISO files re-cently.
How do I burn it to a CD or DVD or mountit??
and (q2)?I need to copy the iso file for Ubuntu12.04 to a CD-R in Win8.
How do I do so?
?.Retrieving semantically equivalent questions is achallenging task due to two main factors: (1) thesame question can be rephrased in many differentways; and (2) two questions may be different butmay refer implicitly to a common problem withthe same answer.
Therefore, traditional similaritymeasures based on word overlap such as shinglingand Jaccard coefficient (Broder, 1997) and its vari-ations (Wu et al., 2011) are not able to capturemany cases of semantic equivalence.
To capturethe semantic relationship between pair of ques-tions, different strategies have been used such asmachine translation (Jeon et al., 2005; Xue et al.,2008), knowledge graphs (Zhou et al., 2013) andtopic modelling (Cai et al., 2011; Ji et al., 2012).Recent papers (Kim, 2014; Hu et al., 2014; Yihet al., 2014; dos Santos and Gatti, 2014; Shen etal., 2014) have shown the effectiveness of convo-lutional neural networks (CNN) for sentence-levelanalysis of short texts in a variety of different nat-ural language processing and information retrievaltasks.
This motivated us to investigate CNNs forthe task of semantically equivalent question re-trieval.
However, given the fact that the size of aquestion in an online community may vary from asingle sentence to a detailed problem descriptionwith several sentences, it was not clear that theCNN representation would be the most adequate.In this paper, we propose a hybrid neural net-work architecture, which we call BOW-CNN.
Itcombines a traditional bag-of-words (BOW) rep-resentation with a distributed vector representa-tion created by a CNN, to retrieve semanticallyequivalent questions.
Using a ranking loss func-tion in the training, BOW-CNN learns to representquestions while learning to rank them according totheir semantic similarity.
We evaluate BOW-CNNover two different Q&A communities in the StackExchange site, comparing it against CNN and 6well-established information retrieval algorithmsbased on BOW.
The results show that our proposedsolution outperforms BOW-based information re-trieval methods such as the term frequency - in-verse document frequency (TFIDF) in all evalu-694Figure 1: Representing and scoring questions withweighted bag-of-words.ated scenarios.
Moreover, we were able to showthat for short texts (title of the questions), an ap-proach using only CNN obtains the best results,whereas for long texts (title and body of the ques-tions), our hybrid approach (BOW-CNN) is moreeffective.2 BOW-CNN2.1 Feed Forward ProcessingThe goal of the feed forward processing is to cal-culate the similarity between a pair of questions(q1, q2).
To perform this task, each questionq follows two parallel paths (BOW and CNN),each one producing a distinct vector representa-tions of q.
The BOW path produces a weightedbag-of-words representation of the question, rbowq,where the weight of each word in the vocabu-lary V is learned by the neural network.
TheCNN path, uses a convolutional approach to con-struct a distributed vector representations, rconvq,of the question.
After producing the BOW andCNN representations for the two input questions,the BOW-CNN computes two partial similarityscores sbow(q1, q2), for the CNN representations,and sconv(q1, q2), for the BOW representations.Finally, it combines the two partial scores to createthe final score s(q1, q2).2.2 BOW PathThe generation of the bag-of-words representationfor a given question q is quite straightforward.
Asdetailed in Figure 1, we first create a sparse vec-tor qbow?
R|V |that contains the frequency in q ofeach word of the vocabulary.
Next, we computethe weighted bag-of-words representation by per-Figure 2: Representing and scoring questions witha convolutional approach.forming the element-wise vector multiplication:rbowq= qbow?
t (1)where the vector t ?
R|V |, contains a weight foreach word in the vocabulary V .
The vector t is aparameter to be learned by the network.
This isclosely related to the TFIDF text representation.In fact, if we fix t to the vector of IDFs, this corre-sponds to the exact TFIDF representation.2.3 CNN PathAs detailed in Figure 2, the first layer of theCNN path transforms words into representationsthat capture syntactic and semantic informationabout the words.
Given a question consisting ofN words q = {w1, ..., wN}, every word wnisconverted into a real-valued vector rwn.
There-fore, for each question, the input to the next NNlayer is a sequence of real-valued vectors qemb={rw1, ..., rwN}.
Word representations are encodedby column vectors in an embedding matrix W0?Rd?|V |, where V is a fixed-sized vocabulary.The next step in the CNN path consists in cre-ating distributed vector representations rconvq1andrconvq2from the word embedding sequencies qemb1and qemb2.
We perform this by using a convolu-tional layer in the same way as used in (dos Santosand Gatti, 2014) to create sentence-level represen-tations.More specifically, given a question q1, the con-volutional layer applies a matrix-vector operationto each window of size k of successive windows695in qemb1= {rw1, ..., rwN}.
Let us define the vectorzn?
Rdkas the concatenation of a sequence of kword embeddings, centralized in the n-th word:zn= (rwn?
(k?1)/2, ..., rwn+(k?1)/2)TThe convolutional layer computes the j-th ele-ment of the vector rconvq1?
Rcluas follows:[rconvq1]j= f(max1<n<N[W1zn+ b1]j)(2)where W1?
Rclu?dkis the weight matrix of theconvolutional layer and f is the hyperbolic tangentfunction.
Matrices W0and W1, and the vector b1are parameters to be learned.
The word embeddingsize d, the number of convolutional units clu, andthe size of the word context window k are hyper-parameters to be chosen by the user.2.4 Question Pair ScoringAfter the bag-of-words and convolutional-basedrepresentations are generated for the input pair (q1,q2), the partial scores are computed as the cosinesimilarity between the respective vectors:sbow(q1, q2) =rbowq1.rbowq2?rbowq1?
?rbowq2?sconv(q1, q2) =rconvq1.rconvq2?rconvq1?
?rconvq2?The final score for the input questions (q1, q2) isgiven by the following linear combinations(q1, q2) = ?1?
sbow(q1, q2) + ?2?
sconv(q1, q2)where ?1and ?2are parameters to be learned.2.5 Training ProcedureOur network is trained by minimizing a rankingloss function over the training set D. The input ineach round is two pairs of questions (q1, q2)+and(q1, qx)?where the questions in the first pair aresemantically equivalent (positive example), andthe ones in the second pair are not (negative ex-ample).
Let ?
be the difference of their similarityscores, ?
= s?
(q1, q2) ?
s?
(q1, qx), generated bythe network with parameter set ?.
As in (Yih et al.,2011), we use a logistic loss over ?L(?, ?)
= log(1 + exp(???
))where ?
is a scaling factor that magnifies ?
from[-2,2] (in the case of using cosine similarity) to alarger range.
This helps to penalize more on theprediction errors.
Following (Yih et al., 2011), inour experiments we set ?
to 10.Sampling informative negative examples canhave a significant impact in the effectiveness of thelearned model.
In our experiments, before train-ing, we create 20 pairs of negative examples foreach positive pair (q1,q2)+.
To create a negativeexample we (1) randomly sample a question qxthat is not semantically equivalent to q1or q2; (2)then create negative pairs (q1,qx)?and (q2,qx)?.During training, at each iteration we only use thenegative example x that produces the smallest dif-ferent s?
(q1, q2)+?
s?
(q1, qx)?.
Using this strat-egy, we select more representative negative exam-ples.We use stochastic gradient descent (SGD) tominimize the loss function with respect to ?.The backpropagation algorithm is used to com-pute the gradients of the network.
In our exper-iments, BOW-CNN architecture is implementedusing Theano (Bergstra et al., 2010).3 Experimental Setup3.1 DataA well-structured source of semantically equiv-alent questions is the Stack Exchange site.
Itis composed by multiple Q&A communities,whereby users can ask and answer questions, andvote up and down both questions and answers.Questions are composed by a title and a body.Moderators can mark questions as duplicates, andeventually a question can have multiple duplicates.For this evaluation, we chose two highly-accessed Q&A communities: Ask Ubuntu and En-glish.
They differ in terms of content and size.Whereas Ask Ubuntu has 29510 duplicated ques-tions, English has 6621.
We performed exper-iments using only the title of the questions aswell as title + body, which we call all for therest of this section.
The average size of a titleis very small (about 10 words), which is at least10 times smaller than the average size of all forboth datasets.
The data was tokenized using thetokenizer available with the Stanford POS Tag-ger (Toutanova et al., 2003), and all links were re-placed by a unique string.
For Ask Ubuntu, wedid not consider the content inside the tag code,which contains some specific Linux commands orprogramming code.For each community, we created training, vali-696Community Training Validation TestAsk Ubuntu 9802 1991 3800English 2235 428 816Table 1: Partition of training, validation and testsets for the experiments.dation and test sets.
In Table 1, we inform the sizeof each set.
The number of instances in the train-ing set corresponds to the number of positive pairsof semantically equivalent questions.
The numberof instances in the validation and the test sets cor-respond to the number of questions which are usedas queries.
All questions in the validation and testset contain at least one duplicated question in theset of all questions.
In our experiments, given aquery question q, all questions in the Q&A com-munity are evaluated when searching for a dupli-cate of q.3.2 Baselines and Neural Network SetupIn order to verify the impact of jointly usingBOW and CNN representations, we perform ex-periments with two NN architectures: the BOW-CNN and the CNN alone, which consists in us-ing only the CNN path of BOW-CNN and, con-sequently, computing the score for a pair of ques-tions using s(q1, q2) = sconv(q1, q2).Additionally, we compare BOW-CNN with sixwell-established IR algorithms available on theLucene package (Hatcher et al., 2004).
Here weprovide a brief overview of them.
For further de-tails, we refer the reader to the citation associatedwith the algorithm.?
TFIDF (Manning et al., 2008) uses the tradi-tional Vector Space Model to represent docu-ments as vectors in a high-dimensional space.Each position in the vector represents a wordand the weight of words are calculated usingTFIDF.?
BM25 (Robertson and Walker, 1994) is aprobabilistic weighting method that takesinto consideration term frequency, inversedocument frequency and document length.Its has two free parameters: k1 to tune term-frequency saturation; and b to calibrate thedocument-length normalization.?
IB (Clinchant and Gaussier, 2010) usesinformation-based models to capture the im-portance of a term by measuring how muchParam.
Name BOW-CNN CNNWord Emb.
Size 200 200Context Winow Size 3 3Convol.
Units 400 1000Learning Rate 0.01 0.05Table 2: Neural Network Hyper-Parametersits behavior in a document deviates from itsbehavior in the whole collection.?
DFR (Amati and Van Rijsbergen, 2002) isbased on divergence from randomness frame-work.
The relevance of a term is measured bythe divergence between its actual distributionand the distribution from a random process.?
LMDirichlet and LMJelinekMercer applyprobabilistic language model approaches forretrieval (Zhai and Lafferty, 2004).
They dif-fer in the smoothing method: LMDirichletuses Dirichlet priors and LMJelinekMerceruses the Jelinek-Mercer method.The word embeddings used in our experimentsare initialized by means of unsupervised pre-training.
We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) avail-able in the word2vec tool.
We use the En-glish Wikipedia to train word embeddings forexperiments with the English dataset.
For theAskUbuntu dataset, we use all available Ask-Ubuntu community data to train word embed-dings.The hyper-parameters of the neural networksand the baselines are tuned using the developmentsets.
In Table 2, we show the selected hyper-parameter values.
In our experiments, we initializeeach element [t]iof the bag-of-word weight vectort with the IDF of i?th word wicomputed over therespective set of questions Q as follows[t]i= IDF (wi, Q) = log|Q||q ?
Q : wi?
q|4 Experimental ResultsComparison with Baselines.
In Tables 3 and4, we present the question retrieval performance(Accuracy@k) of different algorithms over theAskUbuntu and English datasets for the title andall settings, respectively.
For both datasets, BOW-CNN outperforms the six IR algorithms for bothtitle and all settings.
For the AskUbuntu all,BOW-CNN is four absolute points larger than the697AskUbuntu EnglishAlgorithm @1 @5 @10 @1 @5 @10TFIDF 8.3 17.5 22.5 10.0 18.1 21.6BM25 7.3 17.1 21.8 10.0 18.9 23.2IB 8.1 18.1 22.6 10.1 18.4 22.7DFR 7.7 17.8 22.4 10.5 19.0 23.0LMD 5.6 14.1 19.0 10.9 20.1 24.2LMJ 8.3 17.5 22.5 10.3 18.5 22.1CNN 11.5 24.8 31.4 11.6 23.0 26.9BOW-CNN 10.9 22.6 28.7 11.3 21.4 26.0Table 3: Question title retrieval performance (Ac-curacy@k) for different algorithms.AskUbuntu EnglishAlgorithm @1 @5 @10 @1 @5 @10TFIDF 16.9 31.3 38.3 25.9 42.0 48.1BM25 18.2 33.1 39.8 29.4 45.7 52.5IB 14.9 28.2 34.8 25.4 42.3 48.0DFR 18.0 32.6 39.2 28.6 45.4 52.5LMD 13.7 26.8 34.4 23.0 40.2 46.0LMJ 18.3 33.4 40.7 28.5 45.7 52.3CNN 20.0 33.8 40.1 17.2 29.6 33.8BOW-CNN 22.3 39.7 46.4 30.8 47.7 54.9Table 4: Question title + body (all) retrieval per-formance for different algorithms.best IR baseline (LMJ) in terms of Accuracy@1,which represents an improvement of 21.9%.
Sincethe BOW representation we use is closely relatedto TFIDF, an important comparison is the perfor-mance of BOW-CNN vs. TFIDF.
In Tables 3 and4, we can see that BOW-CNN consistently outper-forms the TFIDF model in the two datasets forboth cases title and all.
These findings suggestthat BOW-CNN is indeed combining the strongsemantic representation power conveyed by theconvolutional-based representation to, jointly withthe BOW representation, construct a more effec-tive model.Another interesting finding is that CNN out-performs BOW-CNN for short texts (Table 3)and, conversely, BOW-CNN outperforms CNN forlong texts (Table 4).
This demonstrates that, whendealing with large input texts, BOW-CNN is aneffective approach to combine the strengths ofconvolutional-based representation and BOW.Impact of Initialization of BOW Weights.
Inthe BOW-CNN experiments whose results are pre-sented in tables 3 and 4 we initialize the elementsof the BOW weight vector t with the IDF of eachword in V computed over the question set Q. Inthis section we show some experimental resultsthat indicate the contribution of this initialization.In Table 5, we present the performance ofBOW-CNN for the English dataset when differ-ent configurations of the BOW weight vector t areused.
The first column of Table 5 indicates thetype of initialization, where ones means that t isinitialized with the value 1 (one) in all positions.The second column informs whether t is allowedto be updated (Yes) by the network or not (No).The numbers suggest that letting BOW weightsfree to be updated by the network produces betterresults than fixing them to IDF values.
In addition,using IDF to initialize the BOW weight vector isbetter than using the same weight (ones) to initial-ize it.
This is expected, since we are injecting aprior knowledge known to be helpful in IR tasks.Title Allt initial t updated @1 @10 @1 @10IDF Yes 11.3 26.0 30.8 54.9IDF No 10.6 25.3 29.7 54.9Ones Yes 10.7 24.2 26.3 51.2Table 5: BOW-CNN performance using differentmethods to initialize the BOW weight vector t.5 ConclusionsIn this paper, we propose a hybrid neural networkarchitecture, BOW-CNN, that combines bag-of-words with distributed vector representations cre-ated by a CNN, to retrieve semantically equivalentquestions.
Our experimental evaluation showedthat: our approach outperforms traditional bow ap-proaches; for short texts, a pure CNN obtains thebest results, whereas for long texts, BOW-CNN ismore effective; and initializing the BOW weightvector with IDF values is beneficial.AcknowledgmentsDasha Bogdanova?s contributions were madeduring an internship at IBM Research.
Herwork was partially supported by Science Foun-dation Ireland through the CNGL Programme(Grant 12/CE/I2267) in the ADAPT Centre(www.adaptcentre.ie) at DCU.ReferencesGianni Amati and Cornelis Joost Van Rijsbergen.2002.
Probabilistic models of information retrievalbased on measuring the divergence from random-ness.
ACM Transactions on Information Systems(TOIS), 20(4):357?389.James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,Pascal Lamblin, Razvan Pascanu, Guillaume Des-698jardins, Joseph Turian, David Warde-Farley, andYoshua Bengio.
2010.
Theano: a CPU and GPUmath expression compiler.
In Proceedings of thePython for Scientific Computing Conference.A.
Broder.
1997.
On the resemblance and containmentof documents.
In Proceedings of the Compressionand Complexity of Sequences 1997, SEQUENCES?97, pages 21?, Washington, DC, USA.
IEEE Com-puter Society.Li Cai, Guangyou Zhou, Kang Liu, and Jun Zhao.2011.
Learning the latent topics for question re-trieval in community qa.
In IJCNLP, volume 11,pages 273?281.St?ephane Clinchant and Eric Gaussier.
2010.Information-based models for ad hoc ir.
In Proceed-ings of the 33rd international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 234?241.
ACM.C?
?cero Nogueira dos Santos and Ma?
?ra Gatti.
2014.Deep convolutional neural networks for sentimentanalysis of short texts.
In Proceedings of the 25th In-ternational Conference on Computational Linguis-tics (COLING), Dublin, Ireland.Erik Hatcher, Otis Gospodnetic, and Michael McCan-dless.
2004.
Lucene in action.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.
InAdvances in Neural Information Processing Systems27: Annual Conference on Neural Information Pro-cessing Systems 2014, December 8-13 2014, Mon-treal, Quebec, Canada, pages 2042?2050.Jiwoon Jeon, W Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowl-edge management, pages 84?90.Zongcheng Ji, Fei Xu, Bin Wang, and Ben He.
2012.Question-answer topic model for question retrievalin community question answering.
In Proceedingsof the 21st ACM international conference on Infor-mation and knowledge management, pages 2471?2474.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In Proceedings of the 2014Conference on Empirical Methods for Natural Lan-guage Processing, pages 1746?1751.Christopher D Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to informa-tion retrieval, volume 1.
Cambridge university pressCambridge.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In In Proceedings of Work-shop at ICLR.Stephen E Robertson and Steve Walker.
1994.
Somesimple effective approximations to the 2-poissonmodel for probabilistic weighted retrieval.
In Pro-ceedings of the 17th annual international confer-ence on Research and development in informationretrieval, pages 232?241.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,and Gr?egoire Mesnil.
2014.
A latent semanticmodel with convolutional-pooling structure for in-formation retrieval.
In Proceedings of the 23rd ACMInternational Conference on Conference on Infor-mation and Knowledge Management, pages 101?110.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology, pages 173?180.Yan Wu, Qi Zhang, and Xuanjing Huang.
2011.
Ef-ficient near-duplicate detection for q&a forum.
InProceedings of 5th International Joint Conferenceon Natural Language Processing, pages 1001?1009,Chiang Mai, Thailand, November.
Asian Federationof Natural Language Processing.Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft.
2008.Retrieval models for question and answer archives.In Proceedings of the 31st Annual InternationalACM SIGIR Conference on Research and Devel-opment in Information Retrieval, SIGIR ?08, pages475?482.Wen-tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning discrimina-tive projections for text similarity measures.
InProceedings of the Fifteenth Conference on Com-putational Natural Language Learning, CoNLL?11,pages 247?256.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation ques-tion answering.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 643?648.Association for Computational Linguistics.Chengxiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied toinformation retrieval.
ACM Transactions on Infor-mation Systems (TOIS), 22(2):179?214.Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng,and Jun Zhao.
2013.
Improving question retrieval incommunity question answering using world knowl-edge.
In Proceedings of the Twenty-Third inter-national joint conference on Artificial Intelligence,pages 2239?2245.699
