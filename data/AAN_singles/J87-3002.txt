LARGE LEXICONS FOR NATURAL LANGUAGE PROCESSING:UTILISING THE GRAMMAR CODING SYSTEM OF LDOCEBran BoguraevUniversity of Cambridge Computer LaboratoryCorn Exchange StreetCambridge, CB2 3QG, EnglandandTed BriscoeDepartment of Linguistics, University of LancasterBailrigg, Lancaster LA1 4YT, EnglandThis article focusses on the derivation of large lexicons for natural language processing.
We describe thedevelopment of a dictionary support environment linking a restructured version of the LongmanDictionary of Contemporary English to natural language processing systems.
The process of restruc-turing the information in the machine readable version of the dictionary is discussed.
The Longmangrammar code system is used to construct 'theory neutral' lexical entries.
We demonstrate how suchlexical entries can be put to practical use by linking up the system described here with the experimentalPATR-II grammar development environment.
Finally, we offer an evaluation of the utility of thegrammar coding system for use by automatic natural language parsing systems.1 INTRODUCTIONThe grammar coding system employed by the LongmanDictionary of Contemporary English (henceforthLDOCE) is the most comprehensive description ofgrammatical properties of words to be found in anypublished dictionary available in machine readableform.
This paper describes the extraction of this, andother, information from LDOCE and discusses theutility of the coding system for automated naturallanguage processing.Recent developments in linguistics, and especially ongrammatical theory - -  for example, Generalised PhraseStructure Grammar (GPSG) (Gazdar et al, 1985), Lex-ical Functional Grammar (LFG) (Kaplan and Bresnan,1982) - -  and on natural language parsing frameworksfor example, Functional Unification Grammar (FUG)(Kay, 1984a), PATR-II (Shieber, 1984) - -  make itfeasible to consider the implementation of efficientsystems for the syntactic analysis of substantial frag-ments of natural anguage.
These developments alsoemphasise that if natural anguage processing systemsare to be able to handle the grammatical nd semanticidiosyncracies of individual lexical items elegantly andefficiently, then the lexicon must be a central compo-nent of the parsing system.
Real-time parsing imposesstringent requirements on a dictionary support environ-ment; at the very least it must allow frequent and rapidaccess to the information in the dictionary via thedictionary head words.
The research described below istaking place in the context of three collaborativeprojects (Boguraev, 1987; Russell et al, 1986; Phillipsand Thompson, 1986) to develop a general-purpose,wide coverage morphological nd syntactic analyser forEnglish.
One motivation for our interest in machinereadable dictionaries i to attempt to provide a substan-tial lexicon with lexical entries containing rammaticalinformation compatible with the grammatical frame-work employed by the analyser.The idea of using the machine readable source of apublished ictionary has occurred to a wide range ofresearchers, for spelling correction, lexical analysis,thesaurus construction, and machine translation, toname but a few applications.
Most of the work onautomated dictionaries has concentrated on extractinglexical or other information, essentially by batch pro-cessing (eg.
Amsler, 1981 ;Walker and Amsler, 1986), orCopyright 1987 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material is granted providedthat he copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, orto republish, requires afee and/or specific permission.0362-613X/87/030203-218503.00Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 203Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingon developing dictionary servers for office automationsystems (Kay, 1984b).
Few established parsing systemshave substantial lexicons and even those which employvery comprehensive grammars (eg.
Robinson, 1982;Bobrow, 1978) consult relatively small lexicons, typi-cally generated by hand.
Two exceptions to this gener-alisation are the Linguistic String Project (Sager, 1981)and the IBM CRITIQUE (formerly EPISTLE) Project(Heidorn et al, 1982; Byrd, 1983); the former employsa dictionary of approximately 10,000 words, most ofwhich are specialist medical terms, the latter has wellover 100,000 entries, gathered from machine readablesources.
In addition, there are a number of projectsunder way to develop substantial lexicons from machinereadable sources (see Boguraev, 1986 for details).
How-ever, as yet few results have been published concerningthe utility of electronic versions of published ictionar-ies as sources for such lexicons.
In this paper weprovide an evaluation of the LDOCE grammar codesystem from this perspective.We chose to employ LDOCE as the machine read-able source to aid the development of a substantiallexicon because this dictionary has several propertieswhich make it uniquely appropriate for use as the coreknowledge base of a natural anguage processing sys-tem.
Most prominent among these are the rich gram-matical subcategorisations of the 60,000 entries, thelarge amount of information concerning phrasal verbs,noun compounds and idioms, the individual subject,collocational nd semantic odes for the entries and theconsistent use of a controlled 'core' vocabulary indefining the words throughout the dictionary.
(Michiels(1982) contains further description and discussion ofLDOCE.)
In this paper we focus on the exploitation ofthe LDOCE grammar coding system; Alshawi et al(1985) and Alshawi (1987) describe further esearch inCambridge utilising different types of information avail-able in LDOCE.The information available in the dictionary is bothvery rich and diverse, but also typically only semi-formalised, as it is intended for human, rather thanmachine, interpetation.
As a consequence the programswe are developing, both to restructure and to exploitthis information, need to undergo constant revision asthey are being used.
The system we describe is notintended for off-line use, where one might attempt toderive, completely automatically, a lexicon for naturallanguage analysis.
Rather than trying to batch processthe electronic source, lexicon development from theLDOCE tape is more incremental nd interactive.
Oursystem is designed as an integral part of a largergrammar (and lexicon) development environment,where new lexical entries are automatically generatedfrom the on-line version of the dictionary, checked forcorrectness and consistency and only then added to the'final' lexicon.The problem of utilising LDOCE in natural languageprocessing falls into two areas.
Firstly, we must providean environment in which the machine readable source islinked to the development environment in an appropri-ate fashion and secondly, we must restructure theinformation in the dictionary, using the developmentenvironment, in such a way that natural language pro-cessing systems are able to utilise it effectively.
As anexample, we demonstrate how the LDOCE grammarcodes can be put to practical use by linking up thesystem with the experimental PATR-II parsing system.Finally, we offer an evaluation of the utility of theLDOCE grammar coding system from the perspectiveof natural language processing.2 TIlE ACCESS ENVIRONMENTThere is a well recognised problem with providingcomputational support for machine readable dictionar-ies, in particular where issues of access are concerned.On the one hand, dictionaries exhibit far too muchstructure for conventional techniques for managing'flat' text to apply to them.
On the other hand, theequally large amounts of free text in dictionary entries,as well as the implicitly marked relationships commonlyused to encode linguistic information, makes a dictio-nary difficult o represent as a structured atabase of astandard, eg.
relational, type.
In addition, in order tolink the machine readable version of LDOCE to ourdevelopment environment, and eventually to our natu-ral language processing systems, we need to providefast access from Lisp to data held in secondary storage.Lisp is not particularly well suited for interfacing tocomplex, structured objects, and it was not our inten-tion to embark on a major effort involving the develop-ment of a formal model of a dictionary (of the styledescribed in, eg., Tompa 1986); on the other hand amethod of access was clearly required, which wasflexible enough to support a range of applications in-tending to make use of the LDOCE tape.The requirement for having the dictionary entries in aform convenient for symbolic manipulation from withinLisp was furthermore augmented by the constraint thatall the information present in the typesetting tape shouldbe carried over to the on-line version of LDOCE, sinceit is impossible to say in advance which records andfields of an entry would, or would not, be of potentialuse to a natural anguage processing program.
Finally,the complexity of the data structures tored on discshould not be constrained in any way by the method ofaccess, as we do not have a very clear idea what formthe restructured dictionary may eventually take.Given that we were targeting all envisaged accessroutes from LDOCE to systems implemented in Lisp,and since the natural data structure for Lisp is thes-expression, we adopted the approach of convertingthe tape source into a set of list structures, one perentry.
Our task was made possible by the fact that whilefar from being a database in the accepted sense of theword, the LDOCE typesetting tape is the only trulycomputerised dictionary of English (Michiels, 1983).204 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language ProcessingThe logical structure of a dictionary entry is reflected onthe tape as a sequence of typed records (see Figure 1),each with additional internal segmentation, where rec-ords and fields correspond to separate units in an entry,such as headword, pronunciation, grammar code, wordsenses, and so forth.
(Record-type homograph(Seq-number E-code I-code))(Record-type headword (Serial-no Main-entry))(Record-type pronunciation (Phonetic))(Record-type variant (Spelling Pronunciation))(Record-type part-of-speech (Category Inflection))(Record-type grammar-code (G-code Label))(Record-type def-code(Seq-number G-code Subj-code Box-codes))(Record-type ntry-text(Phrase Label Definition Examples X-ref))(Record-type def-word (Basic-word MorphologyHomograph Word-sense))(Record-type cross-reference (Type Pointers))(Record-type word-sense (Def-code Def-text))(Record-type Usage (Text X-ref))Figure 1The "lispification" of the typesetting tape was car-fled out in a series of batch jobs, via a program writtenin a general text editing facility.
The need to carry outthe conversion without any loss of information meantthat special attention had to be paid to the large numberof non-printing characters which appear on the tape.Most of these signal changes in the typographic appear-ance of the printed dictionary, where crucial informa-tion about the structure of an entry is represented bychanges of typeface and font size.
All control characterswere translated into atoms of the form *AB, where Aand B correspond to the hexadecimal digits of theASCII character code.
Information was thus preserved,and readily available to any program which needed toparse the implicit structure of a dictionary entry or field,and the lispified source was made suitable for transport-ing between different software configurations and oper-ating systems.
Figure 2 illustrates part of an entry as itappears in the published ictionary, on the typesettingtape and after lispification.Note that as a result of the lispification, bracketshave been inserted at suitable points, both to delimitentries and indicate their internal structure; in additioncharacters pecial to Lisp have been appropriatelyescaped.
Thus an individual dictionary entry can nowbe made available to a client program by a single call toa generic read function, once the Lisp reader has beenproperly positioned and 'aligned' with the beginning ofrivet 2 u 1 \[TI;X9\] to cause to fasten with RIVETst:...2828980t<R0154300<rivet28289902<02< <28290005<v<28290107<0100<TI;Xg<NAZV< H XS28290208<to cause to fasten with28290318<{*CA}RIVET{*CB){'46}s{*44}{*8A}:((rivet)(1 R0154300 !
< rivet)(22T< !<)(5v  !<)(7 100 I<  T1 !
; X9 !< NAZV f< .... H---XS)(8 tO cause to fasten wi th*CA RIVET *CB *46 s *44  *8A : ........ ))Figure 2the s-expression encoding the required entry.
In thelispified entry in Figure 2 the numbers at the head ofeach sublist indicate the type of information stored ineach field within the overall entry.
For example, "5"  isthe part of speech field, and "8"  is the word sensedefinition.The 60,000 or so complete ntries of the processeddictionary require of the order of 20 MBytes to store.The problem of access, from Lisp, to the dictionaryentry s-expressions held on secondary storage cannotbe resolved by ad hoc solutions, such as sequentialscanning of files on disc or extracting subsets of suchfiles which will fit in main memory, as these are notadequate as an efficient interface to a parser.
(Exactlythe same problem would occur if our natural anguagesystems were implemented in Prolog, since the Prolog'database facility' refers to the knowledge base thatProlog maintains in main memory.)
In principle, giventhat the dictionary is now in a Lisp-readable format, apowerful virtual memory system might be able to man-age access to the internal Lisp structures resulting fromreading the entire dictionary; we have, however,adopted an alternative solution as outlined below.We have mounted LDOCE on-line under two dif-ferent hardware configurations.
In both cases the samelispified form of the dictionary has been converted intoa random access file, paired together with an indexingfile from which the disc addresses of dictionary entriesfor words and compounds can be computed.A series of systems in Cambridge are implemented inLisp running under Unix TM.
They all make use of anefficient dictionary access system which services re-quests for s-expression entries made by client pro-grams.
A dictionary access process is fired off, whichdynamically constructs a search tree and navigatesthrough it from a given homograph directly to the offsetin the lispified file from where all the associated infor-mation can be retrieved.
As Alshawi (1987) points out,given that no situations were envisaged where theinformation from the tape would be altered once in-stalled in secondary storage, this simple and conven-Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 205Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingtional access trategy is perfectly adequate.
The use ofsuch standard atabase indexing techniques makes itpossible for an active dictionary process to be veryundemanding with respect o main memory utilisation.For reasons of efficiency and flexibility of customisa-tion, namely the use of LDOCE by different clientprograms and from different Lisp and/or Prolog sys-tems, the dictionary access system is implemented inthe programming language C and makes use of theinter-process communication facilities provided by theUnix operating system.
To the Lisp programmer, thecreation of a dictionary process and subsequent re-quests for information from the dictionary appear sim-ply as Lisp function calls.Most of the recent work with the dictionary, and inparticular the decompacting and analysis of the gram-mar codes has been carried out in Interlisp-D on Xerox1100 series workstations.
The same lispified form of thedictionary was used.
Originally it was installed on asingle workstation and only available locally.
Instead ofa separate process building a search tree, the accessmethod relies on a precompiled, multilevel indexingstructure which allows direct hashing into the on-linesource.
In addition, the powerful Interlisp-D virtualmemory allows the access system to be significantlyenhanced by caching most of the working subset of thedictionary at any given turn in main memory.
It turnsout that for a single user workstation, specially tunedfor Lisp and operations optimised at the microcodelevel for random file access and s-expression I/O, thisstrategy offers remarkably good results.More recently, a dictionary server, of the kind de-scribed by Kay (1984b), was implemented and installedas a background process on a Xerox workstation et-worked together with the rest of the equipment dedi-cated to natural anguage processing applications (Bo-guraev et al, 1987).
Again, the same lispified form of themachine readable source of LDOCE was used.
Fromthe point of view of providing a centralised service tomore than one client, efficiently over a packet switchingnetwork, disc space on the server processor was not anissue.
This made it possible to construct a larger, butmore comprehensive, index for the dictionary, whichnow allows the recovery of a word in guaranteed time(typically less than a second).The main access route into LDOCE for most of ourcurrent applications is via the homograph fields (seeFigure 1).
Options exist in the access software tospecify which particular homograph (or homographs)for a lexical item is required.
The early process oflispification was designed to bring together in a singlegroup all dictionary entries corresponding not only todifferent homographs, but also to lexicalised com-pounds for which the argument word appears as thehead of the compound.
Thus, the primary index forblow allows access to two different verb homographs(eg.
b low 3) , two different noun homographs (eg.
blow2),10 compounds (eg.
blow offand blow-by-blow), or all 14of the dictionary entries (not necessarily to be found insubsequent positions in the dictionary) related to blow.While no application currently makes use of this facil-ity, the motivation for such an approach to dictionaryaccess comes from envisaging a parser which willoperate on the basis of the on-line LDOCE; and anyserious parser must be able to recognise compoundsbefore it segments its input into separate words.From the master LDOCE file, we have computedalternative indexing information, which allows accessinto the dictionary via different routes.
In addition toheadwords, dictionary search through the pronuncia-tion field is available; Carter (1987) has merged infor-mation from the pronunciation and hyphenation fields,creating an enhanced phonological representationwhich allows access to entries by broad phonetic lassand syllable structure (Huttenlocher and Zue, 1983).
Inaddition, a fully flexible access system allows the re-trieval of dictionary entries on the basis of constraintsspecifying any combination ofphonetic, lexical, syntac-tic, and semantic information (Boguraev et al, 1987).Independently, random selection of dictionary entries isalso provided to allow the testing of software on anunbiased sample.3 THE FORMAT OF THE GRAMMAR CODESThe lispified LDOCE file retains the broad structure ofthe typesetting tape and divides each entry into anumber of fields - -  head word, pronunciation, grammarcodes, definitions, examples, and so forth.
However,each of these fields requires further decoding and re-structuring to provide client programs with easy accessto the information they require (see Calzolari (1984) forfurther discussion).
For this purpose the formattingcodes on the typesetting tape are crucial since theyprovide clues to the correct structure of this informa-tion.
For example, word senses are largely defined interms of the 2000 word core vocabulary, however, insome cases other words (themselves defined elsewherein terms of this vocabulary) are used.
These wordsalways appear in small capitals and can therefore berecognised because they will be preceded by a fontchange control character.
In Figure 1 above the defini-tion of rivet as verb includes the noun definition of"RIVET 1'', as signalled by the font change and thenumerical superscript which indicates that it is the first(i.e.
noun entry) homograph; additional notation existsfor word senses within homographs.
On the typesettingtape, font control characters are indicated by hexadeci-mal numbers within curly brackets.
In addition, there isa further complication because this sense is used in theplural and the plural morpheme must be removed beforeRIVET can be associated with a dictionary entry.However, the restructuring program can achieve thisbecause such morphology is always italicised, so theprogram knows that, in the context of non-core vocab-ulary items, the italic font control character signals the206 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing((pair)(1 P0008800 < pair)(21 <<)(3 peER)(7 200 < C9 I, esp !.
*46 of < CD-- < .... J---Y)(8 *45 a *44 2 things that are alike or of the samekind !, and are usu !.
used together : *46 a pair ofshoes T Ia beautiful pair of legs *44 *63 compare*CA COUPLE *CB *8B *45 b *44 2 playing cards ofthe same value but of different *CA SUIT *CB *46s *8A *44 (3) : *46 a pair of kings)(7 300 < GC < .... < --S-U---Y)(8 *45 a *44 2 people closely connected : *46 a pairof dancers *45 b *CA COUPLE *CB *8B *44 (2)(esp \[.
in the phr !.
*45 the happy pair *44) *45 c*46 sl *44 2 people closely connected who causeannoyance or displeasure : *46 You!
're a fine paircoming as late as this \[\[ ........ )(Word-sense (Number 2)((Su b-deft nition(Item a) (Label NIL}(Deft nition 2 things that are alike or of thesame kind t, and are usually used together)((Example NIL (a pair of shoes))(Example NIL (a beautifu/ pair of legs)))(Cross-referencecompare-with(Ldoce-entry (Lexical COUPLE)(Morphology NIL )(Homograph-number 2)(Word-sense-number NIL)))(Sub-definition(Item b) (Label NIL)(Definition 2 playing cards of the same valuebut of different(Ldoce-entry (SUIT)(Morphology s)(Homograph-number 1)(Word-sense-number 3))((Exam pie NIL (a pair of kings))))))(Word-sense (Number 3)((Sub-definition(Item a) (Label NIL)(Definition 2 people closely connected)((Example NIL (a pair of dancers))))(Sub-definition(Item b) (Label NIL)(Definition(Ldoce-entry (Lexical COUPLE )(Morphology NIL)(Homograph-number 2)(Word-sense-number 2))(Gloss:especially in the phrase the happy pair )))(Sub-definition(Item c) (Label slang)(Definition 2 people closely connected whocause annoyance or displeasure)((Example NIL( You / 're a fine pair coming as late as this/))))))Figure 3occurrence of a morphological variant of a LDOCEhead entry.A suite of programs to unscramble and restructure allthe fields in LDOCE entries has been written which iscapable of decoding all the fields except hose providingcross-reference and usage information for completehomographs.
Figure 3 illustrates a simple lexical entrybefore and after the application of these programs.
Thedevelopment of the restructuring programs was a non-trivial task because the organisation of information onthe typesetting tape presupposes its visual presentation,and the ability of human users to apply common sense,utilise basic morphological knowledge, ignore minornotational inconsistencies, and so forth.
To provide atest-bed for these programs we have implemented aninteractive dictionary browser capable of displaying therestructured information in a variety of ways and rep-resenting it in perspicuous and expanded form.In what follows we will discuss the format of thegrammar codes in some detail as they are the focus ofthe current paper, however, the reader should bear inmind that they represent only one comparatively con-strained field of an LDOCE entry and therefore, a smallproportion of the overall restructuring task.
Figure 4illustrates the grammar code field for the third wordsense of the verb believe as it appears in the publisheddictionary, on the typesetting tape and after re-structuring.be l ieve  v ... B \[TSa,b;V3;X(to be)l, (to be)7\](7 300 !< TSa l ,  b !
; V3 !
; X (*46 to  be*44) 1 !,  (*46 to  be *44) 7 !< )sense-no  3 head: TSahead: T5bhead: V3head: X1 r ight  opt iona l  ( to  be)head: X7 r ight  opt iona l  ( to  be)Figure 4LDOCE provides considerably more syntactic infor-mation than a traditional dictionary.
The Longmanlexicographers have developed a grammar coding sys-tem capable of representing in compact form a non-trivial amount of information, usually to be found onlyin large descriptive grammars of English (such as Quirket al, 1985).
A grammar code describes a particularpattern of behaviour of a word.
Patterns are descriptive,and are used to convey a range of information: eg.distinctions between count and mass nouns (dog vs.desire), predicative, postpositive and attributive adjec-tives (asleep vs. elect vs. jokular), noun complementa-tion (fondness, fact) and, most importantly, verb com-plementation a d valency.Grammar codes typically contain a capital letter,followed by a number and, occasionally, a small letter,for example \[T5a\] or \[V3\].
The capital etters encodeinformation "about he way a word works in a sentenceComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 207Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingor about the position it can fill" (Procter, 1978: xxviii);the numbers "give information about he way the rest ofa phrase or clause is made up in relation to the worddescribed" (ibid.).
For example, "T"  denotes a transi-tive verb with one object, while "5"  specifies that whatfollows the verb must be a sentential complementintroduced by that.
(The small letters, eg.
"a"  in thecase above, provide further information typically re-lated to the status of various complementisers, adverbsand prepositions in compound verb constructions: eg.
"a"  indicates that the word that can be left out betweena verb and the following clause.)
As another example,"V3" introduces a verb followed by one NP object anda verb form (V) which must be an infinitive with to (3).In addition, codes can be qualified with words orphrases which provide further information concerningthe linguistic context in which the described item islikely, and able, to occur; for example \[Dl(to)\] or \[L(tobe)l\].
Sets of codes, separated by semicolons, areassociated with individual word senses in the lexicalentry for a particular item, as Figure 5 illustrates.
Thesesets are elided and abbreviated in the code field associ-ated with the word sense to save space.
Partial codessharing an initial letter can be separated by commas, forexample \[T1,5a\].
Word qualifiers relating to a completesequence of codes can occur at the end of a code field,delimited by a colon, for example \[T1 ;I0: (DOWN)\].Codes which are relevant o all the word senses in anentry often occur in a separate field after the head wordand occasionally codes are elided from this field downinto code fields associated with each word sense as, forexample, in Figure 6.
Decompacting and restructuringgrammar code entries into a format more suitable forfurther automated analysis can be done with knowledgeof the syntax of the grammar code system and thesignificance of punctuation and font changes.
However,discovering the syntax of the system is difficult since noexplicit description is available from Longman and thecode is geared more towards visual presentation thanformal precision; for example, words which qualifycodes, such as "to be" in Figure 4, appear in italics andtherefore, will be preceded by the font control character*45.
But sometimes the thin space control character *64also appears; the insertion of this code is based solelyon visual criteria, rather than the informational struc-ture of the dictionary.
Similarly, choice of font can bevaried for reasons of appearance and occasionally in-feel I ~ 1 \[T1,6\] to get the knowledge of by touching with thefingers: ... 2 \[Wv6;T1\] to experience (the touch or move-ment of something): ... $ \[LT\] to experience (a conditionof the mind or body); be consciously: ... 4 \[L1\] to seem tooneself to be: ... 5 \[T1,5;V3\] to believe, esp.
for the moment6 \[LT\] to give (a sensation): ... 7 \[Wv6;I0\] to (be able to)experience sensations: ... 8 \[Wv6;T1\] to suffer because of(a state or event): ... 9 \[L9 (after, \]or)\] to search with thefingers rather than with the eyes: ...Figure 5.see off v oA.
IT1\] 1 \[(at)\] to go to the airport, station, etc.,with (someone who is beginning a trip): saw h/s )'r/end oHat the bus #tat/on 2 to remain unharmed until (something orsomeone dangerous) has ceased to be active; WITHSTAND:They maw off $ enemy attacks within $ dayeFigure 6formation ormally associated with one field of an entryis shifted into another to create a more compact orelegant printed entry.In addition to the 'noise' generated by the fact thatwe are working with a typesetting tape geared to visualpresentation, rather than a database, there are errorsand inconsistencies in the use of the grammar codesystem.
Examples of errors, illustrated in Figure 7,include the code for the noun promise which contains amisplaced comma, that for the verb scream, in which acolon delimiter occurs before the end of the field, andthat for the verb like where a grammatical label occursinside a code field.p,o , - i .e ,  ... X \[C(of),C3.S;scream v ... 3 \[T1,5; (OUT); I0\]l i ke  v ... 2 \ [T3,4;  ne9.\]Figure 7In addition, inconsistencies occur in the applicationof the code system by different lexicographers.
Forexample, when codes containing "to be" are elidedthey mostly occur as illustrated in Figure 4 above.However, sometimes this is represented as \[L(tobe)l,9\].
Presumably this kind of inconsistency arosebecause one member of the team of lexicographersrealised that this form of elision saved more space.This type of error and inconsistency arises becausegrammatical codes are constructed by hand and noautomatic hecking procedure is attempted (see Mi-chiels, 1982, for further comment).
One approach to thisproblem is that taken by the ASCOT project (Akkermanet al, 1985; Akkerman, 1986).
In this project, a newlexicon is being manually derived from LDOCE.
Thecoding system for the new lexicon is a slightly modifiedand simplified version of the LDOCE scheme, withoutany loss of generalisation a d expressive power.
Moreimportantly, the assignment of codes for problematic orerroneously labelled words is being corrected in anattempt to make the resulting lexicon more appropriatefor automated analysis.
In the medium term this ap-proach, though time consuming, will be of some utilityfor producing more reliable lexicons for natural lan-guage processing.208 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language ProcessingHowever, in the short term, the necessity to copewith such errors provides much of the motivation forour interactive approach to lexicon development, sincethis allows the restructuring programs to be progres-sively refined as these problems emerge.
Any attempt atbatch processing without extensive initial testing of thiskind would inevitably result in an incomplete and pos-sibly inaccurate l xicon.4 THE CONTENT OF THE GRAMMAR CODESOnce the grammar codes have been restructured, it stillremains to be shown that the information they encode isgoing to be of some utility for natural language process-ing.
The grammar code system used in LDOCE is basedquite closely on the descriptive grammatical frameworkof Quirk et al (1972, 1985).
The codes are doublyarticulated; capital letters represent he grammaticalrelations which hold between a verb and its argumentsand numbers represent subcategorisation frames whicha verb can appear in.
Most of the subcategorisationframes are specified by syntactic ategory, but some arevery ill-specified; for instance, 9 is defined as "needs adescriptive word or phrase".
In practice many adver-bial and predicative complements will satisfy this code,when attached to a verb; for example, put \[xg\] wherethe code marks a locative adverbial prepositional phrasevs.
make under sense 14 (hereafter written make(14)) iscoded IX9\] where it marks a predicative noun phrase orprepositional phrase.The criteria for assignment of capital etters to verbsis not made explicit, but is influenced by the syntacticand semantic relations which hold between the verb andits arguments; for example, I5, L5 and T5 can all beassigned to verbs which take a NP subject and asentential complement, but L5 will only be assigned ifthere is a fairly close semantic link between the twoarguments and T5 will be used in preference to I5 if theverb is felt to be semantically two place rather than oneplace, such as know versus appear.
On the other hand,both believe and promise are assigned V3 which meansthey take a NP object and infinitival complement, yetthere is a similar semantic distinction to be madebetween the two verbs; so the criteria for the assign-ment of the V code seem to be purely syntactic.Michiels (1982) and Akkerman et al (1985) provide amore detailed analysis of the information encoded bythe LDOCE grammar codes and discuss their efficacy asa system of linguistic description.
Ingria (1984) compre-hensively compares different approaches to comple-mentation within grammatical theory providing atouch-stone against which the LDOCE scheme can beevaluated.Most automated parsing systems employ grammarswhich carefully distinguish syntactic and semantic in-formation, therefore, if the information provided by theLongman grammar code system is to be of use, we needto be able to separate out this information and map itinto a representation scheme compatible with the typeof lexicon used by such parsing systems.The program which transforms the LDOCE grammarcodes into lexical entries utilisable by a parser takes asinput the decompacted codes and produces a relativelytheory neutral representation f the lexical entry for aparticular word, in the sense that this representationcould be further transformed into a format suitable formost current parsing systems.
For example, if the inputwere the third sense of believe, as in Figure 4, theprogram would generate the (partial) entry shown inFigure 8 below.
The four parts correspond to differentsyntactic realisations of the third sense of the verbbelieve.
Takes indicates the syntactic ategory of thesubject and complements required for a particular rea-lisation.
Type indicates aspects of logical semanticsdiscussed below.
((Takes NP SBar) (Type 2))((Takes NP NP Inf) (Type 2 Ogaisin8))(or ((Takes NP NP NP) (Type 2 Ogaisin8))((Takes NP NP AuxInf) (Type 2 ORaising)))(or ((Takes NP NP AP) (Type 20Raising))((Takes NP NP AuxInf) (Type 20Raislng)))Figure 8At the time of writing, rules for producing adequateentries to drive a parsing system have only been devel-oped for verb codes.
In what follows we will describethe overall transformation strategy and the particularrules we have developed for the verb codes.
Extendingthe system to handle nouns, adjectives and adverbswould present no problems of principle.
However, theLDOCE coding of verbs is more comprehensive thanelsewhere, so verbs are the obvious place to start in anevaluation of the usefulness of the coding system.
Noattempt has been made to map any closed class entriesfrom LDOCE, as a 3,000 word lexicon containing mostclosed class items has been developed independently bone of the groups collaborating with us to develop thegeneral purpose morphological nd syntactic analyser(see the Introduction and Russell et al, 1986).Initially the transformation f the LDOCE codes wasperformed on a code-by-code basis, within a code fieldassociated with each individual word sense.
This ap-proach is adequate if all that is required is an indicationof the subcategorisation frames relevant o any partic-ular sense.
In the main, the code numbers determine aunique subcategorisation.
Thus the entries can be usedto select the appropriate VP rules from the grammar(assuming a GPSG-style approach to subcategorisation)Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 209Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingand the relevant word senses of a verb in a particulargrammatical context can be determined.
However, ifthe parsing system is intended to produce a representa-tion of the predicate-argument structure for input sen-tences, then this simple approach is inadequate becausethe individual codes only give partial indications of thesemantic nature of the relevant sense of the verb.The solution we have adopted is to derive a semanticclassification of the particular sense of the verb underconsideration on the basis of the complete set of codesassigned to that sense.
In any subcategorisation framewhich involves a predicate complement there will be anon-transparent relationship between the superficialsyntactic form and the underlying logical relations in thesentence.
In these situations the parser can use thesemantic type of the verb to compute this relationship.Expanding on a suggestion of Michiels (1982), weclassify verbs as Subject Equi, Object Equi, SubjectRaising or Object Raising for each sense which has apredicate complement code associated with it.
Theseterms, which derive from Transformational Grammar,are used as convenient labels for what we regard as asemantic distinction; the actual output of the program isa specification of the mapping from superficial syntacticform to an underlying logical representation.
For exam-ple, labelling believe(3) (Type 20Raising) indicates thatthis is a two place predicate and that, if believe(3)occurs with a syntactic direct object, as in(1) John believes the Earth to be roundit will function as the logical subject of the predicatecomplement.
Michiels proposed rules for doing this forinfinitive complement codes; however there seems to beno principled reason not to extend this approach tocomputing the underlying relations in other types of VPas well as in cases of NP, AP and PP predication (seeWilliams (1980), for further discussion).The five rules which are applied to the grammarcodes associated with a verb sense are ordered in a waywhich reflects the filtering of the verb sense through aseries of syntactic tests.
Verb senses with an \[it + 15\]code are classified as Subject Raising.
Next, verb senseswhich contain a \[V\] or \[X\] code and one of \[D5\], \[D5a\],\[D6\] or \[D6a\] codes are classified as Object Equi.
Then,verb senses which contain a \[V\] or \[X\] code and a IT5\]or \[T5a\] code in the associated grammar code field, (butnone of the D codes mentioned above), are classified asObject Raising.
Verb senses with a \[V\] or IX(to be)\]code, (but no IT5\] or \[T5a\] codes), are classified asObject Equi.
Finally, verb senses containing a \[T2\], \[T3\]or IT4\] code, or an \[I2\], \[13\] or \[14\] code are classified asSubject Equi.
Figure 9 gives examples of each type.The Object Raising and Object Equi rules attempt oexploit the variation in transformational potential be-tween Raising and Equi verbs; thus, in the paradigmcase, Object Raising verbs take a sentential complementand Object Equi verbs do not, as examples (2) and (3)illustrate.happen(S)warn( l )ass,l ine(I)decline(S)\[WvS;/t+15\](Type 1 SRaising)\[Wv4;IO;Tl:( of, against) ifa;D5a;V3\](Type 30Equi)\[Wv4;T1,Sa,b;X(to be)l,7\](Type 20Raising)\[T1,3;I0\](Type 2 SEqui)Figure 9(2) John believes that the Earth is round.
(3) *John forces that the Earth is round.Secondly, if a verb takes a direct object and asentential complement, it will be an Equi verb, asexamples in (4) and (5) illustrate.
(4) John persuaded Mary that the Earth is round.
(5) *John believed Mary that the Earth is round.Clearly, there are other syntactic and semantic testsfor this distinction, (see eg.
Perlmutter and Soames,1979:472), but these are the only ones which are explicitin the LDOCE coding system.Once the semantic type for a verb sense has beendetermined, the sequence of codes in the associatedcode field is translated, as before, on a code-by-codebasis.
However, when a predicate complement code isencountered, the semantic type is used to determine thetype assignment, asillustrated in Figures 4 and 8 above.Where no predicate complement is involved, the lettercode is usually sufficient to determine the logical prop-erties of the verb involved.
For example, T codes nearlyalways translate into two-place predicates as Figure 10illustrates.In some cases important syntactic information isconveyed by the word qualifiers associated with partic-ular grammar codes and the translation system is there-fore sensitive to these correlations.
For example, theSubject Raising rule above makes reference to the lefthate 2 e ... 1 \[T1,3,4; V3,4\] to have a great dislike of(hate((Sense 1)((Takes NP NP) (Type 2))((Takes NP Inf) (Type 2 SEqui))((Takes NP Ing) (Type 2 SEqui))((Takes NP NP Inf) (Type 30Equi))((Takes NP NP Ing) (Type 30Equi))Figure 10210 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingcontext qualifier " it".
Another example where wordqualifiers can be utilised straightforwardly is with di-transitive verbs such as give and donate.
Give is codedas \[Dl(to)\] which allows us to recover the informationthat this verb permits dative movement and requires aprepositional phrase headed by "to":(Takes NP NP ToPP) and (Takes NP NP NP).On the other hand, donate is coded \[T1 (to)\], whichtells us that it does not undergo dative movement butdoes require a prepositional phrase headed by "to":(Takes NP NP ToPP).There are many more distinctions which are con-veyed by the conjunction of grammar codes and wordqualifiers (see Michiels, 1982, for further details).
How-ever, exploiting this information to the full would be anon-trivial task, because it would require accessing therelevant knowledge about the words contained in thequalifier fields from their LDOCE entries.5 LEXICAL ENTRIES FOR PATR-IIThe output of the transformation program can be usedto derive entries which are appropriate for particulargrammatical formalisms.
To demonstrate hat this ispossible we have implemented a system which con-structs dictionary entries for the PATR-II system(Shieber, 1984 and references therein).
PATR-II waschosen because it has been reimplemented in Cam-bridge and was therefore, available; however, the taskwould be nearly identical if we were constructing en-tries for a system based on GPSG, FUG or LFG.
Weword storm:w Jense  ~ <head t rans  sense-no> = 1V TakesNP Dyadicworddag storm:\ [cat:  vhead: \[aux: fa l set rans:  \[pred: stormsense-no:  1arg l :  <DGIS> - \[\]arg2: <DG16> = \ [ \ ] \ ] \ ]syncat : \ [ f i r s t :  \ [cat:  NPhead: \ [ trans:  <DG15>\]\]res t :  \ [ f i r s t :  \ [cat:  NPhead:\ [ trans:  <DC16>\]\]res t :  \ [ f i r s t :  lambda\]\] \ ] \ ]Figure 11intend to use the LDOCE source in the same way toderive most of the lexicon for the general purpose,morphological nd syntactic parser we are developing.The latter employs a grammatical formalism based onGPSG; the comparatively theory neutral lexical entriesthat we construct from LDOCE should translatestraightforwardly into this framework as well.The PATR-II parsing system operates by unifyingdirected graphs (DGs); the completed parse for a sen-tence will be the result of successively unifying the DGsassociated with the words and constituents of the sen-tence according to the rules of the grammar.
The DG fora lexical item is constructed from its lexical entrywhichcontains a set of templates for each syntacticallydistinct variant.
Templates are themselves abbrevia-tions for unifications which define the DG.
For example,the basic entry and associated DG for the verb storm areillustrated in Figure 11.The template Dyadic defines the way in which thesyntactic arguments othe verb contribute to the logicalstructure of the sentence, while the template TakesNPdefines what syntactic arguments storm requires; thus,the information that storm is transitive and that it islogically a two-place predicate is kept distinct.
Conse-quently, the system can represent the fact that someverbs which take two syntactic arguments are neverthe-less one-place predicates.The modified version of PATR-II that we have im-plemented contains only a small dictionary and con-structs entries automatically from restructured LDOCEentries for most verbs that it encounters.
As well ascarrying over the grammar codes, the PATR-II lexiconsystem has been modified to include word senses num-bers, which are derived from LDOCE.
Thus, the anal-ysis of a sentence by the PATR-II system now repre-sents its syntactic and logical structure and theparticular senses of the words (as defined in LDOCE)which are relevant in the grammatical context.
Figures12 and 13 illustrate the dictionary entries for marry andpersuade constructed by the system from LDOCE.In Figure 14 we show one of the two analysesproduced by PATR-II for a sentence containing thesetwo verbs.
The other analysis is syntactically andlogically identical but incorporates sense two of marry.Thus, the output from this version of PATR-II repre-sents the information that further semantic analysisneed only consider sense two of persuade and sense oneand two of marry; this rules out one further sense ofeach, as defined in LDOCE.6 EVALUATIONThe utility of the work reported above rests ultimatelyon the accuracy of the lexical entries which can bederived from the LDOCE tape.
We have not attempteda systematic analysis of the entries which would resultif the decompacting and grammar code transformationprograms were applied to the entire dictionary.
InSection 3 we outlined some of the errors in the grammarComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 211Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingmarry v 1 \[T1; I0\] to take (a person) inmarriage: He married late in lifs / never  marrleKt (fig.)
She marr/ed money (= a rich man) 2TI\] (of a priest or official) to perform theceremony of marriage for (2 people): An o/dIt/end marr/ed them 3 IT1 (to)\] to cause to takein marriage: She want8 to marry her dAzw~er toa Hch man(marry((Sense 1)((Takes NP NP) (Type 2))((Takes NP) (Type i)))((Sense 2)((Takes NP NP) (Type 2)))((Sense 3)((Takes NP NP PP) (Type 3) ) ) )word marry:w_sense =~<head trane sense-no> = lV TakesNP Dyadicw_sense<head trans sense-no> = iV Takes IntraneNP Monadicw_sense<head trans sense-no> = 2V TakesNP Dyadicw_sense =~<head trane sense-no> = 3V TakesNPPP Tr iadicFigure 12codes which are problematic for the decompactingstage.
However, mistakes or omissions in the assign-ment of grammar codes represent a more serious prob-lem.
While inconsistencies or errors in the applicationof the grammar coding system in some cases can berectified by the gradual refinement of the decompactingprogram, it is not possible to correct errors of omissionor assignment automatically.
On the basis of unsyste-matic evaluation, using the programs to dynamicallyproduce entries for the PATR-II parsing system, anumber of errors of this type have emerged.For example, the LDOCE definitions and associatedcode fields in Figure 15, demonstrate hat upset(3) needsit + D5 which would correspond to its use with a nounphrase and a sentential complement; suppose(2) is miss-ing optional "to be" for the X1 and X7 codes listed;help(l) needs a T3 code since it does not always requirea direct object as well as an infinitive complement; anddetest needs a V4 code because it can take a directobject as well as a gerund complement.It is difficult o quantify the extent of this problem onthe the basis of enumeration of examples of this type.Therefore, we have undertaken a limited test of both theaccuracy of the assignment of the LDOCE codes in thesource dictionary and the reliability of the more ambi-tious (and potentially controversial) aspects of thegrammar code transformation rules.
It is not clear, inparticular, that the rules for computing semantic typesfor verbs are well enough motivated linguistically orthat the LDOCE lexicographers were sensitive noughto the different transformational potential of the variousclasses of verbs to make a rule such as our one forObject Raising viable.We tested the classification of verbs into semantictypes using a verb list of 139 pre-classified items drawnfrom the lists published in Rosenbaum (1967) and Stock-well et al (1973).
Figure 16 gives the number of verbsclassified under each category by these authors and thenumber successfully classified into the same categoriesby the system.The overall error rate of the system was 14%; how-ever, as the table illustrates, the rules discussed aboveclassify verbs into Subject Raising, Subject Equi andpersuade v I \[TI (of); D5\] to cause to feelcertain; CONVINCE:  She waa not persuadedo,f the truth o.f hi~ ~ement  = \[Tl(into, out o~;V3\] to cause to do something by reasoning,arguing, begging, etc.
: try to  persuade him tolet .a go with him.
l No~.~ wo.ld pers,zo~s him.. .
.
.
.
.
.
.
(persuade( (Sense  1)((Takes NP NP) (Type 2))((Takes NP NP SBar)(Type 3)))( (Sense  2)((Takes NP NP) (Type 2))((Takes NP NP Inf)(Type  3 0bjectEqui))))word persuade:w_sense =~<head t rans  sense-no> = IV TakesNP Dyadicw_sense =~<head t rans  sense-no> = IV TakeeNPSBar  Tr iadicw_sense =~<head t rane  sense-no> = 2V TakesNP Dyadicw_sense<head trans sense-no> = 2V TakesNPInfObjectControl TriadicFigure 13212 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingparse> uther might persuade gwen to marry cornwall\[cat: SENTENCEhead: \[form: finiteagr: \[per: p3 num: sg\]aux: truetrana: \[pred: possiblesense-no: Iargl: \[pred: persuadesense-no: 2argl: \[ref: uther sense-no: I\]arg2: \[ref: gwen sense-no: I\]arg3: \[pred: marrysense-no: 2argl: \[ref: gwen sense-no: i\]arg2: \[ref: cornwallsense-no :  1\]\]\]\]\]\]Figure 14Object Equi very successfully.
The two Subject Raisingverbs which were not so classified by the system werecome about  and turn out.
Come about  is coded 15 inLDOCE, but is not given any word qualifier; turn out  isnot given any 15 code.
These are clear examples ofomissions on the part of the Longman lexicographers,rather than of failure of the rule.
Similarly, trust is notrecognised as an Object Equi verb, because its dictio-nary entry does not contain a V3 code; this must be anomission, given the grammaticality of(6) I trust h im to do the job .P re fer  is misclassified as Object Raising, rather thanas Object Equi, because the relevant code field containsa T5 code, as well as a V3 code.
The T5 code is markedas 'rare', and the occurrence of prefer  with a tensedsentential complement, asopposed to with an infinitive,is certainly marginal:upset  ... $ \[T1\] to cause to worry, not to be calm,etc.
: ........suppose ... 2 \[TSa,b; V3 often pasta.
; X1,7,9\] to be-lieve: I suppose that's true.
\] I supposed him to be a work-man, but he was in/act a thief.
\[ He was ~ommonly supposed(to be) looti, h ........help ...
I \[T1; I0; V3, (eep arn~ 2\] to do part of thework (for someone); be of use to (someone in doingsomething); AID, ASSIST: Could ~lou help me up (thea~,o)~ I T~ a,'~ he~ps h~,.
(to) ,~k, I Yo,,, o~uhelps a lot.
I Can I help ( ,~  yo,,, wo~k)~ ........detest ... \[T1,4\] to hate with very strong feeling: Ideter people who decelse and tell lies.
I dn, .
~ishootir~ and k~lin?
.........(7) I prefer  that he come on Monday .
(8) ?I pre fer  that he marr ies Jul ie.This example also highlights a deficiency in theLDOCE coding system since pre fer  occurs much morenaturally with a sentential complement if it collocateswith a modal such as "would".
This deficiency isrectified in the verb classification system employed byJackendoff and Grimshaw (1985) in the Brandeis verbcatalogue.The main source of error comes from the misclassi-fication of Object Raising into Object Equi verbs.
Ar-guably, these errors also derive mostly from errors inthe dictionary, rather than a defect of the rule.
66% ofthe Object Raising verbs were misclassified as ObjectEqui verbs, because the cooccurrence of the T5 and V(2, 3, or 4) codes in the same code fields, as predicted bythe Object Raising rule above, was not confirmed byLDOCE.
All the 14 verbs misclassified contain V codesand 10 of these also contain T5 codes.
However, theLongman lexicographers typically define two differentword senses, one of which is marked (perhaps amongother codes) T5 and the other with a V code.
Analysis ofPublished Derived %lists fromLDOCESEqui 31OEqui 58SRaising 7ORaising 4231565281oo%97%71%67%Figure 15 Figure 16Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 213Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingacknowledge ...
I \[T1,4,5 (to) to agreeto the t ruth of; recognise the fact or ex-istence (of): I o ,?~ie~e the h '~ of uoartheU wer~ de/rated I They ~zowlcdCcd ha~/~?been d~y~t0d 2 \[T1 (o); X (to be) 1,7\] toreco~ise, accept, or admit (as): He w~acknowlod~d to be th~ b~ Ida~r.
J He wasaeknowlod~d am their hinter.
\[ ~ admowl-~d th~rn~d~ (to be) d~y~atat ........hear ...e I \[We6; T I ;  V2,4; I0\] to r~ceive and understand (sounds) by usingthe ears: I mn~ hear very wall.
J I heard himaa/t 8o.
\[ I can hear aomeone knock/nf 2 \[Wv6;Tl,Sa\] to be told or informed: I heardthat he w~, dl ~ compare HEAR ABOUT,HEAR FROM, HEAR OF ........Figure 17these word senses uggests that this approach is justi-fied in three cases, but unmotivated in five; for example,hear (1),(2) (justified) vs. acknowledge (1),(2) (unjus-tified) (see Figure 17).
The other four cases we inter-preted as unmotivated were show, suspect ,  know, con-fess  and in the case of consider(2), (Figure 18) there is aclear omission of a T5 code, as demonstrated by thegrammaticality of(9) I consider that it is a great  honour to be here.Similarly, expect  is not given a V3 code under sense1 (Figure 19), however the grammaticality of(10) I expect  him to pass  the examwith the relevant interpretation suggests that it shouldbe assigned a V3 code.
Alternatively, sense 5, which isassigned a V3 code, seems uspiciously similar to sense1.The four verbs which are misclassified as ObjectEqui and which do not have T5 codes anywhere in theirentries are elect, love, represent  and require.
None ofthese verbs take sentential complements and thereforethey appear to be counterexamples to our Object Rais-ing rule.
In addition, Moulin et al (1985) note that ourObject Raising rule would assign mean to this categoryincorrectly.
Mean is assigned both a V3 and a T5category in the code field associated with sense 2 (i.e.
"intend"), however, when it is used in this sense it mustbe treated as an Object Equi verb.This small experiment demonstrates a number ofpoints.
Firstly, it seems reasonable to conclude that theassignment of individual codes to verbs is on the wholerelatively accurate in LDOCE.
Of the 139 verbs tested,we only found code omissions in 10 cases.
Secondlythough, when we consider the interaction between theassignments of codes and word sense classification,LDOCE appears less reliable.
This is the primarysource of error in the case of the Object Raising rule.Thirdly, it seems clear that the Object Raising rule isstraining the limits of what can be reliably extractedfrom the LDOCE coding system.
Ideally, to distinguishbetween raising and equi verbs, a number of syntacticcriteria should be employed (Perlmutter and Soames,1979:460ff.).
However, only two of these criteria areexplicit in the coding system.On the basis of the results obtained, we explored thepossibility of modifying the Object Raising rule to takeaccount of the cooccurrence ofT5 and T5a codes and Vor X codes within a homograph, rather than within aword sense.
An exhaustive search of the dictionaryproduced 24 verbs coded in this fashion.
Ten of thesewere listed as Object Raising verbs in the published listsused in the above experiment.
Five more verbs wereclassified as Equi in the published lists.
Of the remainingnine verbs which did not appear in the published lists,three were clearly Object Raising, one was clearly Equi,a further two were probably Object Raising, and the lastthree were very difficult o classify.
This demonstratesthat modifying the Object Raising rule in this fashionwould result in the misclassification f some Equi verbs.In fact, the list is sufficiently small that this set of verbsis probably best coded by hand.As a final test, we ran the rules for determining thesemantic type of verbs over all the 7,965 verb entries inLDOCE.
There are 719 verb senses which are coded inthe dictionary as having the potential for predicatecomplementation.
Of these 5 were classified as SubjectRaising, 53 as Object Raising, 377 as Subject Equi, and326 as Object Equi by our rules.
42 of the Equi verbs areambiguous between Subject and Object Equi under thesame sense; in the transformation program this ambigu-ity is resolved by selecting the type appropriate for eachindividual code.
For example, a code which translatesconsider ... 2 \[WvS, X (to be) 1,7; V3\] to regard as; thinkof in a stated way: I ?on~der ~ a 1o0/(= I regard you as afool).
\[ Icou'dor/t a~honour tobe~ ~UoutodoU.
\[Hee.id ~ c o ~  me (to beJ ~o ~ to bB a ~ wor~.
\[ T~5~t l~ Ida~ are ~ l tV  oo~dc~d a part o!
Bootb~ .........expect  ... l \ [T3,5a,b\]  to think (that something will hap-pen): 1 ~ (t~t) heql p~s the ezra/nut/on.
I He ~ to/dthe ~rdnat io~ \[ "Wdl she oome wonf" " /~p~ *o. ?
........ S\[V3\] to believe, hope and think (that someone will do some-thing): The o~?er ezl~cfcd h~ ram to do thdr duty in the ?om/ngba~/s .......Figure 18 Figure 19214 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingas (Takes NP Inf) would select Subject Equi, while(Takes NP NP Inf) would select Object Equi.
These setsof verbs together with the relevant LDOCE sensenumber are listed in the appendix.
An exhaustive anal-ysis of the 54 verbs classified as Object Raising revealedtwo further errors of inclusion in this set; order(6)should be Object Equi and promise(l) should be SubjectEqui.
The 42 verbs which the transformation programtreats as ambiguous Equi verbs appear to be somewhatheterogeneous; some, such as want(1) and ask(2), arecases of 'Super-Equi' control verbs where the controlrelations are determined contextually, whilst others,particularly the phrasal verbs, appear to be betterclassified as Object Raising.
Allow(l) and permit(l)appear here incorrectly because they are coded \[T4\] tocapture xamples uch as(11) They do not al low/permit smoking in their house.In this example the subject of the progressive comple-ment is not controlled by the matrix subject.
Again,since the list is small, this set of verbs should probablybe coded by hand.7 CONCLUSIONMost applications for natural anguage processing sys-tems will require vocabularies substantially arger thanthose typically developed for theoretical or demonstra-tion purposes and it is often not practical, and certainlynever desirable, to generate these by hand.
The evalu-ation of the LDOCE grammar coding system suggeststhat it is sufficiently detailed and accurate (for verbs) tomake the on-line production of the syntactic omponentof lexical entries both viable and labour saving.
How-ever, the success rate of the programs described abovein producing useful exical entries for a parsing systemdepends directly on the accuracy of the code assign-ments in the source dictionary.
Correcting the mistakesand omissions in these assignments would be a non-trivial exercise.
This is part of the motivation foradopting the interactive, rather than batch mode, ap-proach to using the tape for lexicon development.
Weenvisage ventually using the system to generate l xicalentries in a semi-automatic fashion, allowing the user tointervene and correct errors during the actual process ofconstructing lexical entries, so that gradually a reliableand relatively error-free large lexicon for automatednatural language processing systems containing detailedgrammatical information can be constructed fromLDOCE.Clearly, there is much more work to be done withLDOCE in the extension of the use of grammar codesand the improvement of the word sense classificationsystem.
Similarly, there is a considerable amount ofinformation in LDOCE which we have not exploitedsystematically as yet; for example, the box codes,which contain selection restrictions for verbs or thesubject codes, which classify word senses according tothe Merriam-Webster codes for subject matter (seeWalker and Amsler (1983) for a suggested use for these).The large amount of semi-formalised information con-cerning the interpretation of noun compounds and idi-oms also represents a rich and potentially very usefulsource of information for natural anguage processingsystems.
In particular, we intend to investigate theautomatic generation of phrasal analysis rules from theinformation on idiomatic word usage.In the longer term, it is clear that neither the contentsnor form of any existing published ictionary meet althe requirements of a natural anguage processing sys-tem.
A substantial component of the research reportedabove has been devoted to restructuring LDOCE tomake it more suitable for automatic analysis.
However,even after this process much of the information inLDOCE remains difficult to access, essentially becauseit is aimed at a human reader, as opposed to a computersystem.
This suggests that the automatic construction ofdictionaries from published sources intended for otherpurposes will have a limited life unless lexicography isheavily influenced by the requirements of automatednatural language analysis.
In the longer term, therefore,the automatic onstruction of dictionaries for naturallanguage processing systems may need to be based ontechniques for the automatic analysis of large corpora(eg.
Leech et al, 1983).
However, in the short term, theapproach outlined in this paper will allow us to producea relatively sophisticated and useful dictionary rapidly.8 ACKNOWLEDGEMENTSWe would like to thank the Longman Group Limited forkindly allowing us access to the LDOCE typesettingtape for research purposes.
We also thank Steve Pul-man, Graham Russell and Karen Sparck Jones for theircomments on the first draft, which substantially im-proved this paper.
Part of the research reported herewas funded by the UK Science and Engineering Re-search Council (Grant No.
GR/D/05554) under the Al-vey Programme.
This paper is a substantially revisedand updated version of an earlier presentation at theSecond Conference of the European Chapter of theACL.REFERENCESAkkerman, Erik; Masereeuw, Pieter; and Meijs, Willem.
1985 De-signing a Computerised Lexicon for Linguistic Purposes.
ASCOTReport No.
l, CIP-Gegevens Koninklijke Bibliotheek, Den Haag,Netherlands.Akkerman, Erik.
1986 A Critical Assessment of the LDOCE CodingSystem.
To appear in: Akkerman, E.; Masereew, P.; and Meijs,W., Eds., ASCOT Report No 2, CIP-Gegevens Koninklijke Biblio-theek, The Hague, Netherlands.Alshawi, Hiyan; Boguraev, Branimir; and Briscoe, Ted.
1985Towards aLexicon Support Environment for Real Time Parsing.In Proceedings ofthe Second Conference ofthe European Chap-ter of the Association for Computational Linguistics, Geneva,Switzerland: 171-178.Alshawi, Hiyan.
1987 Processing Dictionary Definitions with PhrasalPattern Hierarchies.
In this issue.Boguraev, Branimir.
1986 (and forthcoming) Machine Readable Dic-tionaries and Research inComputational Linguistics.
InProceed-ings of a Workshop on Automating the Lexicon, Grosseto, Italy (toComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 215Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processingbe published as Walker,D.
and Zampolli,A., Eds., Automating theLexicon in a Multilingual Environment, Cambridge UniversityPress, Cambridge, UK).Boguraev, Branimir.
1987 A Natural Language Toolkit: ReconcilingTheory with Practice.
In Proceedings of a Workshop on WordOrder and Parsing in Unification Grammars, Friedenweiler, Ger-many (to be published as Reyle,U.
and Rohrer,C., Eds., "WordOrders, Parsing, and Unification Grammars" D. Reidel, Dor-drecht, Holland).Boguraev, Branimir; Carter, David and Briscoe, Ted.
1987 A Multi-Purpose Interface to an On-line Dictionary.
In Proceedings of theThird Conference of the European Chapter of the Association forComputational Linguistics, Copenhagen, Denmark: 63-69.Byrd, Roy.
1983 Word Formation in Natural Language ProcessingSystems.
In Proceedings of the Eighth International Joint Confer-ence on Artificial Intelligence, Karlsrnhe, Germany: 704-706.Calzolari, Nicoletta.
1984 Machine-Readable Dictionaries, LexicalData Bases and the Lexical System.
In Proceedings of the lOthInternational Congress on Computational Linguistics, Stanford,California: 460-461.Carter, David.
1987 An Information Theoretic Analysis of PhoneticDictionary Access, Computer Speech and Language, 2:1-11.Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan.
1985Generalized Phrase Structure Grammar.
Blackwell, Oxford, UK.Heidorn, George et al 1982 The EPISTLE Text-Critiquing System.IBM Systems Journal, 21(3): 305-326.Huttenlocher, Daniel and Zue, Victor.
1983 Phonotactic and LexicalConstraints in Speech Recognition, In Proceedings of the NationalConference on Artificial Intelligence, Washington, D.C.: 172-176.Ingria, Robert.
1984 Complement Types in English.
Report No.
5684,Bolt Beranek and Newman Inc., Cambridge, Mass.Jackendoff, Ray and Jane Grimshaw.
1985 A Key to the BrandeisVerb Catalog.
Unpublished mimeo, under NSF Grant IST-84-20073, "Information Structure of a Natural Language Lexicon",Program in Linguistics and Cognitive Science, Brandeis Univer-sity, Waltham, Mass.Kaplan, Ronald and Bresnan, Joan.
1982 Lexical-Functional Gram-mar: A Formal System for Grammatical Representation.
In:J.Bresnan, Ed., The Mental Representation of Grammatical Re-lations.
The MIT Press, Cambridge, Mass: 173-281.Kay, Martin.
1984a Functional Unification Grammar: A Formalism forMachine Translation.
In Proceedings of the lOth International Con-gress on Computational Linguistics, Stanford, California: 75-79.Kay, Martin.
1984b The Dictionary Server.
In Proceedings of the lOthInternational Congress on Computational Linguistics, Stanford,California, 461-462.Leech, Geoffrey; Garside, Roger; and Atwell, Erik.
1983 The Auto-matic Grammatical Tagging of the LOB Corpus.
Bulletin of the1.
Subject Raising verbs (total number 5)International Computer Archive of Modern English, NorwegianComputing Centre for the Humanities, Bergen, Norway.Michiels, Archibal.
1982 Exploiting a Large Dictionary Data Base.PhD Thesis, Universit6 de Liege, Liege, Belgium.Michiels, Archibal.
1983 Automatic Analysis of Texts.
In Informatics7, Proceedings of a Conference of the ASLIB Informatics Groupand the Information Retrieval Group of the British ComputerSociety, Cambridge, UK: 103-120.Moulin, A.; Jansen, J; and Michiels, A.
1985 Computer Exploitationof LDOCE's Grammatical Codes, paper presented at a Confer-ence on Survey of English Language, Lund.Perlmutter, D.M.
and Soames, S. 1979 Syntactic Argumentation andthe Structure of English.
University of California Press, Berkeley,California.Phillips, John and Thompson, Henry.
1986 A Parser for GeneralisedPhrase Structure Grammars.
To apper in Klein, E. and Haddock,N., Eds., Edinburgh Working Papers in Cognitive Science, Uni-versity of Edinburgh, Edinburgh, Scotland.Procter, Paul.
1978 Longman Dictionary of Contemporary English.Longman Group Limited, Harlow and London, England.Quirk, Randolph; Greenbaum, Sidney; Leech, Geoffrey; and Svart-vik, Jan. 1972 A Grammar of Contemporary English, LongmanGroup Limited, Harlow and London, England.Quirk, Randolph; Greenbaum, Sidney; Leech, Geoffrey; and Svart-vik, Jan. 1985 A Comprehensive Grammar of English, LongmanGroup Limited, Harlow and London, England.Robinson, Jane.
1982 DIAGRAM: A Grammar for Dialogues.
Com-munications of the ACM, 25(1): 27-47.Rosenbaum, P.S.
1967 The Grammar of English Predicate Comple-ment Constructions.
MIT Press, Cambridge, Mass.Russell, Graham; Pulman, Steve; Ritchie, Graeme; and Black, Alan.1986 A Dictionary and Morphological Analyser for English.
InProceedings of the Eleventh International Congress on Computa-tional Linguistics, Bonn, Germany: 277-279.Sager, N. 1981 Natural Language Information Processing, Addison-Wesley, Reading, Mass.Shieber, S. 1984 The Design of a Computer Language for LinguisticInformation, In Proceedings of the lOth International Congress onComputational Linguistics, Stanford, California: 362-366.Stockwell, R.P.
; Schachter, P.; and Partee, B.H.
1973 The MajorSyntactic Structures of English.
Holt, Rinehart and Winston, NewYork, New York.Tompa, Frank.
1986 Database Design for a Dictionary of the Future.Preliminary Report, Centre for the New Oxford English Dictio-nary, University of Waterloo, Waterloo, Ontario.Walker, D. and Amsler, A.
1986 The Use of Machine-ReadableDictionaries in Sublanguage Analysis.
In: R. Grishman and R.Kittredge, Eds., Analysing Language in Restricted Domains,Lawrence Erlbaum Associates, Hillsdale, New Jersey.Williams, E.S., 1980 Predication.
Linguistic Inquiry, 11(2): 203-238.APPENDIXappear (3) chance (1) happen (3) seem (2) transpire (2)2.
Object Raising verbs (total number 53)adjudge (1) admit (3) allow (5) argue (3) assert (1)assume (1) avow (1) believe (3) betray (3) certify (2)declare (2) deem (1) deny (1) determine (1) discover (2)engage (4) feel (5) find (8) foreordain (1) guess (1)hold (9) judge (3) maintain (5) make out (5) mean (2)mind (2) notice (1) observe (1) order (6) perceive (1)predicate (1) prefer (1) preordain (1) presume (1) presume (2)proclaim (1) pronounce (2) pronounce (3) prove (1) recognize (3)remember (1) report (1) reveal (2) see (2) smell (2)smell (3) suppose (1) suppose (2) tell (6) think (2)understand (3) understand (4) warrant (2)216 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing3.
Subject Equi verbs ( to ta l  number  335)abide (1) account for (2) ache (2) acknowledge (1) adore (3)advocate (1) affect (1) afford (2) agree (2) aim (2)aim at (1) allude to (1) anticipate (1) appear (2) arrange (2)aspire (1) assent (1) attach to (3) attempt (1) avoid (1)awake (1) bear (5) bear (9) begin (1) beg (3)begrudge (1) bid fair (1) blanch (2) blink at (1) blush (2)bother (3) break off (1) burn (6) burst (3) burst out (1)bust out (3) care (1) cease (1) chance (1) choose (2)claim (4) clamour (2) clog (1) close (3) cloud (3)come (1) come (7) come before (1) come down to (1) come out against (1)come into (1) come on (1) come to (1) commence (1) compare with (1)compete (1) conceal (1) conceive of (1) concur (2) condescend (1)conduce to (1) confess (1) confess (2) confide (1) connive (1)consent (1) consider (1) consist in (1) conspire (1) conspire (2)contemplate (2) continue (1) continue (3) contract (1) contrive (1)contrive (3) could (1) covenant (1) cut out (4) cry out against (1)dare (1) dare (2) decide (2) decide on (1) declare against (1)declare for (1) decline (3) defend (3) defy (3) deign (1)delay (1) delight (2) delight in (1) demand (1) depose (2)deride (1) descend to (1) deserve (1) detest (1) disclaim (1)discontinue (1) discourage (2) disdain (2) dislike (1) do with (1,2)dread (1) duck out of (1) elect (2) endeavour (1) endure (1)enjoy (1) envisage (1) escape (3) essay (1) evade (2)excuse (1) expect (1) exult (1) exult over (2) fail (1,3)fall to (1) finish (1) fix (2) fix on (1) flick (2)forbear (1) forbid (2) forget (1) forget about (1) forswear (I)frown on (1) funk (1) get (3,11) get around to (1) get away with (1)get down to (1) get out of (1) get round to (1) give up (1) go (5)go about (2) go in for (2) go on (5) go with (3) go without (1)grow (5) grow out of (2) grow out of (3) grudge (1) guarantee (2)guard against (1) happen (2) hasten (2) hate (3) hesitate (1)hinge on (1) hit on (1) hope (1) incline (4) include (1)indulge in (1) inveigh against (1) involve (2) itch (3) jib at (1)justify (1) keep (11) keep from (2) keep on at (1) kick against (1)knock off (2) know about (1) lament (1) lead to (1) learn (1)leave (7) like (2) live by (1) loathe (1) long (1)look forward to (1) make (18) make up for (1) manage (2) mean (5)merit (1) militate magainst (1) miss (1,2,5) necessitate (1) need (1)neglect (1) neglect (2) negotiate (1) offer (3) omit (2)operate (2) own to (1) pant (4) pay for (1) pertain to (1)petition (2) pine (3) plan (1) play (3) play at (1)play at (2) pledge (1) plot (5) plump for (1) pooh-pooh (1)postpone (1) practise (4) practise (5) prate about (1) pray (1)preclude (1) prepare (3) prepare for (1) presume (4) pretend (1)pretend (2) pretend (4) proceed (1) profess (2) profit by (1)prohibit (1) promise (3) propose (1) propose (2) provide for (2)provide for (3) purport (1) purpose (1) put off (1) quit (1)recall (1) reckon on (2) recollect (1) refuse (1) regret (1)rejoice (1) relish (1) remember (2) repent (1) require (1)resent (1) resist (1) resist (2) resolve (1) resolve (2)resort to (1) result from (1) resume (1) revel in (1) revert to (1)rise above (1) risk (2) rue (1) say (5) scheme (1)scorn (1) scramble (2) scream (4) scruple (1) seek (3)seem (1) see (7) see about (1) see to (1) send (4)send away (2) send off (3) serve (5) set about (1) set out (2)shirk (1) should (1) shrink from (1) shudder (1) shun (1)sicken of (1) smile (2) stand (8) stand (12) stand for (2)start (1) stem from (1) stick (8) stoop (3) stoop to (1)stop (1) strive (1) subscribe to (1) suggest (2) swear (1)swear by (1) swear off (1) swear to (1) take to (2) take up (1)tend (2) think of (1) think of (5) threaten (2) train (3)tremble (3) trouble (3) try (1) try (2) try (3)undertake (2) unite (2) use (1) venture (2) venture (4)volunteer (1) volunteer (2) vote (1) vouchsafe (1) vow (1)wait (1) want (3) want (4) warrant (1) watch (3)witness to (1) wriggle out of (1) write (4) write back (1) yearn (1)Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 217Bran Boguraev and Ted Briscoe Large Lexicons for Natural Language Processing4.
Object Equi verbs (total number 284)acknowledge (2) adjure (1) advise (1) aid (1) allow (2)allure (1) appoint (1) arrange for (1) ask (4) assign (4)assist (1) attribute to (1) authorize (1) badger (1) bargain for (1)beckon (1) behove (1) beseech (1) bestir (1) bid (2)bill (2) bludgeon into (1) bluff into (1) bribe (I) bring (2)bring (5) bring in (3) bully (1) buzz (3) cable (1)call on (2) catch (3) cause (1) caution (1) challenge (1)challenge (4) challenge (5) charge (5) charge with (1) charge with (2)come down on (1) command (1) commission (1) compel (1) condemn (3)condemn (4) condition (3) confess (3) conjure (1) connive at (1)consider (2) constrain (1) cop (1) counsel (1) couple with (1)cozen into (1) credit with (1) dare (5) debar from (1) decide (4)dedicate to (1) defy (2) delegate (2) depend on (1) depute (1)deputize (2) design (2) designate (2) detail (1) direct (3)doom (1) dragoon into (1) draw on (3) drive (8) egg on (1)embolden (1) employ (1) employ (2) employ in (1) empower (1)enable (1) enable (2) encourage (1) end in (1) engage (1)enggae in (1) entice (1) entitle (2) entreat (1) equip (2)esteem (2) excite (2) exhort (1) expect (5) fancy (1)fancy (3) figure on (1) find (1) find (6) fit (5)forbid (1) force (I) frighten into (1) frighten out of (2) get (4)get (8) give (17) give over to (1) goad into (1) groom (4)habituate to (1) hail as (1) harden to (1) hark at (1) hear (1)help (1) help (2) hunger (1) impel (1) implore (I)importune (1) impute to (l) incite (1) incline (3) induce (1)influence (1) inhibit from (1) inspire (1) instigate (2) instruct (2)instruct (3) intend (2) introduce to (1) inure to (1) inveigle into (1)invite (2) invite (3) itch for (1) join with in (1) keep (10)keep from (1) know (4) lead (2) lead on (1) legislate against (1)legislate for (1) let (1) let (2) let (3) let (4)let off (1) long for (I) look at (1) look to (2) lower (3)make (3) make (5) make (6) make (7) mean (4)motion (2) motion to (1) motivate (1) move (11) name (3)nominate (2,4) notify (1) obligate (1) oblige (1) order (1)organize (1) overhear (I) persuade (2) pester (1) petition (1)phone (1) pick (1) pick on (1) plead with (1) pledge (2)plume upon (1) pray (3) preclude from (1) predestinate (1) predestine (1)predetermine (1,3) predispose (1) preen on (1) prepare (1) prepare (5)prepare for (3) press (9) pressure (I) pressurize (1) prevail upon (1)prevent (1) prevent from (1) pride on (1) profess (3) program (1)programme (1) promise (1) prompt (i) prove (3) provoke (2)provoke into (1) push (3) push on (2) put down as (1) put down to (1)put off (1) put up to (1) reckon (1) reckon on (1) reduce to (4)reeducate (1) regard as (1) rely on (2) remember as (1) remind (1)represent (1.2) represent as (1) request (1) require (2) result in (1)schedule (1) school (1) seduce (2) select (1) send (1)send (2) send (3) set (4) set (8) shape (1)show (1) show (9) signal (2) sign (2) slate (2)spur (2) spy (3) steel (I) stop (2) suffer (4)summons (1) summon (I) supplicate (I) suppose (3) suspect (2)take (18) talk into (1) talk out of (1) tax with (1) teach (1)telegraph (1) telephone (1) telex (I) tell (2) tell (3)tell (5) tell off (2) tempt (1) tempt (2) thank (2)timetable (1) time (1) tip (1) tip off (2) train (2)trouble (2) unfit for (1) urge (2) want (2) warn (1)watch (1) watch (5) watch for (1) wean from (1) worry at (1)yearn for (1)5.
Equi verbs (total number 42)allow (1) allow for (1) approve of (1) ask (2) bank on (1)beg (2) calculate on (1) chance (2) choose (1) compensate for (1)countenance (1) count on (1) culminate in (1) desire (1) enjoin (1)hate (1) hate (2) hear about (1) hear of (1) help (4)imagine (1) intend (1) like (1) like (3) love (2)nag (1) need (1) pay (1) permit (1) prepare (4)qualify (1) race (3) recommend (2) rely on (1) save (4)see (6) sign (3) sign up (1) start (3) ' visualize (1)want (1) wish (5)218 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
