Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1024?1033, Prague, June 2007. c?2007 Association for Computational LinguisticsA Topic Model for Word Sense DisambiguationJordan Boyd-GraberComputer SciencePrinceton UniversityPrinceton, NJ 08540jbg@princeton.eduDavid BleiComputer SciencePrinceton UniversityPrinceton, NJ 08540blei@cs.princeton.eduXiaojin ZhuComputer ScienceUniversity of WisconsinMadison, WI 53706jerryzhu@cs.wisc.eduAbstractWe develop latent Dirichlet alocation withWORDNET (LDAWN), an unsupervisedprobabilistic topic model that includes wordsense as a hidden variable.
We develop aprobabilistic posterior inference algorithmfor simultaneously disambiguating a corpusand learning the domains in which to con-sider each word.
Using the WORDNET hi-erarchy, we embed the construction of Ab-ney and Light (1999) in the topic model andshow that automatically learned domainsimprove WSD accuracy compared to alter-native contexts.1 IntroductionWord sense disambiguation (WSD) is the task ofdetermining the meaning of an ambiguous word inits context.
It is an important problem in naturallanguage processing (NLP) because effective WSDcan improve systems for tasks such as informationretrieval, machine translation, and summarization.In this paper, we develop latent Dirichlet aloca-tion with WORDNET (LDAWN), a generative prob-abilistic topic model for WSD where the sense ofthe word is a hidden random variable that is inferredfrom data.There are two central advantages to this approach.First, with LDAWN we automatically learn the con-text in which a word is disambiguated.
Ratherthan disambiguating at the sentence-level or thedocument-level, our model uses the other words thatshare the same hidden topic across many documents.Second, LDAWN is a fully-fledged generativemodel.
Generative models are modular and can beeasily combined and composed to form more com-plicated models.
(As a canonical example, the ubiq-uitous hidden Markov model is a series of mixturemodels chained together.)
Thus, developing a gen-erative model for WSD gives other generative NLPalgorithms a natural way to take advantage of thehidden senses of words.In general, topic models are statistical models oftext that posit a hidden space of topics in which thecorpus is embedded (Blei et al, 2003).
Given acorpus, posterior inference in topic models amountsto automatically discovering the underlying themesthat permeate the collection.
Topic models have re-cently been applied to information retrieval (Wei andCroft, 2006), text classification (Blei et al, 2003),and dialogue segmentation (Purver et al, 2006).While topic models capture the polysemous useof words, they do not carry the explicit notion ofsense that is necessary for WSD.
LDAWN extendsthe topic modeling framework to include a hiddenmeaning in the word generation process.
In thiscase, posterior inference discovers both the topicsof the corpus and the meanings assigned to each ofits words.After introducing a disambiguation scheme basedon probabilistic walks over the WORDNET hierar-chy (Section 2), we embed the WORDNET-WALKin a topic model, where each topic is associated withwalks that prefer different neighborhoods of WORD-NET (Section 2.1).
Then, we describe a Gibbs sam-pling algorithm for approximate posterior inferencethat learns the senses and topics that best explain acorpus (Section 3).
Finally, we evaluate our systemon real-world WSD data, discuss the properties ofthe topics and disambiguation accuracy results, anddraw connections to other WSD algorithms from theresearch literature.10241740entity 19303122object2084615024animal 13049461305277artifact male2354808 2354559foalcolt3042424colt4040311revolverSynset IDWordsix-gunsix-shooter0.00 0.250.580.00 0.040.02 0.010.160.050.040.690.000.000.381.000.42 0.000.000.571.000.380.07Figure 1: The possible paths to reach the word ?colt?in WORDNET.
Dashed lines represent omitted links.All words in the synset containing ?revolver?
areshown, but only one word from other synsets isshown.
Edge labels are probabilities of transitioningfrom synset i to synset j.
Note how this favors fre-quent terms, such as ?revolver,?
over ones like ?six-shooter.
?2 Topic models and WordNetThe WORDNET-WALK is a probabilistic process ofword generation that is based on the hyponomy re-lationship in WORDNET (Miller, 1990).
WORD-NET, a lexical resource designed by psychologistsand lexicographers to mimic the semantic organiza-tion in the human mind, links ?synsets?
(short forsynonym sets) with myriad connections.
The spe-cific relation we?re interested in, hyponomy, pointsfrom general concepts to more specific ones and issometimes called the ?is-a?
relationship.As first described by Abney and Light (1999), weimagine an agent who starts at synset [entity],which points to every noun in WORDNET 2.1 bysome sequence of hyponomy relations, and thenchooses the next node in its random walk from thehyponyms of its current position.
The agent repeatsthis process until it reaches a leaf node, which corre-sponds to a single word (each of the synset?s wordsare unique leaves of a synset in our construction).For an example of all the paths that might gener-ate the word ?colt?
see Figure 1.
The WORDNET-WALK is parameterized by a set of distributions overchildren for each synset s in WORDNET, ?s.Symbol MeaningK number of topics?k,s multinomial probability vector overthe successors of synset s in topic kS scalar that, when multiplied by ?sgives the prior for ?k,s?s normalized vector whose ith entry,when multiplied by S, gives the priorprobability for going from s to i?d multinomial probability vector overthe topics that generate document d?
prior for ?z assignment of a word to a topic?
a path assignment throughWORDNET ending at a word.
?i,j one link in a path ?
going from syn-set i to synset j.Table 1: A summary of the notation used in the pa-per.
Bold vectors correspond to collections of vari-ables (i.e.
zu refers to a topic of a single word, butz1:D are the topics assignments of words in docu-ment 1 through D).2.1 A topic model for WSDThe WORDNET-WALK has two important proper-ties.
First, it describes a random process for wordgeneration.
Thus, it is a distribution over wordsand thus can be integrated into any generative modelof text, such as topic models.
Second, the synsetthat produces each word is a hidden random vari-able.
Given a word assumed to be generated by aWORDNET-WALK, we can use posterior inferenceto predict which synset produced the word.These properties allow us to develop LDAWN,which is a fusion of these WORDNET-WALKs andlatent Dirichlet alocation (LDA) (Blei et al, 2003),a probabilistic model of documents that is an im-provement to pLSI (Hofmann, 1999).
LDA assumesthat there are K ?topics,?
multinomial distributionsover words, which describe a collection.
Each docu-ment exhibits multiple topics, and each word in eachdocument is associated with one of them.Although the term ?topic?
evokes a collection ofideas that share a common theme and although thetopics derived by LDA seem to possess semanticcoherence, there is no reason to believe this would1025be true of the most likely multinomial distributionsthat could have created the corpus given the assumedgenerative model.
That semantically similar wordsare likely to occur together is a byproduct of howlanguage is actually used.In LDAWN, we replace the multinomial topic dis-tributions with a WORDNET-WALK, as describedabove.
LDAWN assumes a corpus is generated bythe following process (for an overview of the nota-tion used in this paper, see Table 1).1.
For each topic, k ?
{1, .
.
.
,K}(a) For each synset s, randomly choose transition prob-abilities ?k,s ?
Dir(S?s).2.
For each document d ?
{1, .
.
.
, D}(a) Select a topic distribution ?d ?
Dir(?
)(b) For each word n ?
{1, .
.
.
, Nd}i.
Select a topic z ?
Mult(1, ?d)ii.
Create a path ?d,n starting with ?0 as the rootnode.iii.
From children of ?i:A.
Choose the next node in the walk ?i+1 ?Mult(1, ?z,?i)B.
If ?i+1 is a leaf node, generate the associ-ated word.
Otherwise, repeat.Every element of this process, including thesynsets, is hidden except for the words of the doc-uments.
Thus, given a collection of documents, ourgoal is to perform posterior inference, which is thetask of determining the conditional distribution ofthe hidden variables given the observations.
In thecase of LDAWN, the hidden variables are the param-eters of the K WORDNET-WALKs, the topic assign-ments of each word in the collection, and the synsetpath of each word.
In a sense, posterior inferencereverses the process described above.Specifically, given a document collection w1:D,the full posterior isp(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?)
?
(?Kk=1 p(?k |S?
)?Dd=1 p(?d | ?
)?Ndn=1 p(?d,n |?1:K)p(wd,n |?d,n)), (1)where the constant of proportionality is the marginallikelihood of the observed data.Note that by encoding the synset paths as a hid-den variable, we have posed the WSD problem asa question of posterior probabilistic inference.
Fur-ther note that we have developed an unsupervisedmodel.
No labeled data is needed to disambiguate acorpus.
Learning the posterior distribution amountsto simultaneously decomposing a corpus into topicsand its words into their synsets.The intuition behind LDAWN is that the wordsin a topic will have similar meanings and thus sharepaths within WORDNET.
For example, WORDNEThas two senses for the word ?colt;?
one referring to ayoung male horse and the other to a type of handgun(see Figure 1).Although we have no a priori way of know-ing which of the two paths to favor for adocument, we assume that similar conceptswill also appear in the document.
Documentswith unambiguous nouns such as ?six-shooter?and ?smoothbore?
would make paths that passthrough the synset [firearm, piece,small-arm] more likely than those go-ing through [animal, animate being,beast, brute, creature, fauna].
Inpractice, we hope to see a WORDNET-WALK thatlooks like Figure 2, which points to the right senseof cancer for a medical context.LDAWN is a Bayesian framework, as each vari-able has a prior distribution.
In particular, theDirichlet prior for ?s, specified by a scaling factorS and a normalized vector ?s fulfills two functions.First, as the overall strength of S increases, we placea greater emphasis on the prior.
This is equivalent tothe need for balancing as noted by Abney and Light(1999).The other function that the Dirichlet prior servesis to enable us to encode any information we haveabout how we suspect the transitions to childrennodes will be distributed.
For instance, we might ex-pect that the words associated with a synset will beproduced in a way roughly similar to the token prob-ability in a corpus.
For example, even though ?meal?might refer to both ground cereals or food eaten ata single sitting and ?repast?
exclusively to the lat-ter, the synset [meal, repast, food eatenat a single sitting] still prefers to transi-tion to ?meal?
over ?repast?
given the overall corpuscounts (see Figure 1, which shows prior transitionprobabilities for ?revolver?
).By setting ?s,i, the prior probability of transition-ing from synset s to node i, proportional to the to-tal number of observed tokens in the children of i,1026we introduce a probabilistic variation on informa-tion content (Resnik, 1995).
As in Resnik?s defini-tion, this value for non-word nodes is equal to thesum of all the frequencies of hyponym words.
Un-like Resnik, we do not divide frequency among allsenses of a word; each sense of a word contributesits full frequency to ?.3 Posterior Inference with Gibbs SamplingAs described above, the problem of WSD corre-sponds to posterior inference: determining the prob-ability distribution of the hidden variables given ob-served words and then selecting the synsets of themost likely paths as the correct sense.
Directly com-puting this posterior distribution, however, is nottractable because of the difficulty of calculating thenormalizing constant in Equation 1.To approximate the posterior, we use Gibbs sam-pling, which has proven to be a successful approx-imate inference technique for LDA (Griffiths andSteyvers, 2004).
In Gibbs sampling, like all Markovchain Monte Carlo methods, we repeatedly samplefrom aMarkov chain whose stationary distribution isthe posterior of interest (Robert and Casella, 2004).Even though we don?t know the full posterior, thesamples can be used to form an empirical estimateof the target distribution.
In LDAWN, the samplescontain a configuration of the latent semantic statesof the system, revealing the hidden topics and pathsthat likely led to the observed data.Gibbs sampling reproduces the posterior distri-bution by repeatedly sampling each hidden variableconditioned on the current state of the other hiddenvariables and observations.
More precisely, the stateis given by a set of assignments where each wordis assigned to a path through one of K WORDNET-WALK topics: uth word wu has a topic assignmentzu and a path assignment ?u.
We use z?u and ?
?uto represent the topic and path assignments of allwords except for u, respectively.Sampling a new topic for the word wu requiresus to consider all of the paths that wu can take ineach topic and the topics of the other words in thedocument u is in.
The probability of wu taking ontopic i is proportional top(zu = i |z?u)??
p(?
|?
?u)1[wu ?
?
], (2)which is the probability of selecting z from ?d timesthe probability of a path generating wu from a pathin the ith WORDNET-WALK.The first term, the topic probability of the uthword, is based on the assignments to the K topicsfor words other than u in this document,p(zu = i|z?u) =n(d)?u,i + ?i?j n(d)?u,j +?Kj=1 ?j, (3)where n(d)?u,j is the number of words other than u intopic j for the document d that u appears in.The second term in Equation 2 is a sum over theprobabilities of every path that could have generatedthe word wu.
In practice, this sum can be com-puted using a dynamic program for all nodes thathave unique parent (i.e.
those that can?t be reachedby more than one path).
Although the probability ofa path is specific to the topic, as the transition prob-abilities for a synset are different across topics, wewill omit the topic index in the equation,p(?u = ?|?
?u, ) =?l?1i=1 ??u?i,?i+1.
(4)3.1 Transition ProbabilitiesComputing the probability of a path requires us totake a product over our estimate of the probabilityfrom transitioning from i to j for all nodes i and j inthe path ?.
The other path assignments within thistopic, however, play an important role in shaping thetransition probabilities.From the perspective of a single node i, only pathsthat pass through that node affect the probability ofu also passing through that node.
It?s convenient tohave an explicit count of all of the paths that tran-sition from i to j in this topic?s WORDNET-WALK,so we use T?ui,j to represent all of the paths that gofrom i to j in a topic other than the path currentlyassigned to u.Given the assignment of all other words to paths,calculating the probability of transitioning from i toj with word u requires us to consider the prior ?
andthe observations Ti,j in our estimate of the expectedvalue of the probability of transitioning from i to j,?
?ui,j =T?ui,j + Si?i,jSi +?k T?ui,k.
(5)1027As mentioned in Section 2.1, we paramaterize theprior for synset i as a vector ?i, which sums to one,and a scale parameter S.The next step, once we?ve selected a topic, is toselect a path within that topic.
This requires thecomputation of the path probabilities as specified inEquation 4 for all of the paths wu can take in thesampled topic and then sampling from the path prob-abilities.The Gibbs sampler is essentially a randomizedhill climbing algorithm on the posterior likelihood asa function of the configuration of hidden variables.The numerator of Equation 1 is proportional to thatposterior and thus allows us to track the sampler?sprogress.
We assess convergence to a local mode ofthe posterior by monitoring this quantity.4 ExperimentsIn this section, we describe the properties of thetopics induced by running the previously describedGibbs sampling method on corpora and how thesetopics improve WSD accuracy.Of the two data sets used during the course ofour evaluation, the primary dataset was SEMCOR(Miller et al, 1993), which is a subset of the Browncorpus with many nouns manually labeled with thecorrect WORDNET sense.
The words in this datasetare lemmatized, and multi-word expressions that arepresent in WORDNET are identified.
Only the wordsin SEMCOR were used in the Gibbs sampling pro-cedure; the synset assignments were only used forassessing the accuracy of the final predictions.We also used the British National Corpus, whichis not lemmatized and which does not have multi-word expressions.
The text was first run througha lemmatizer, and then sequences of words whichmatched a multi-word expression in WORDNETwere joined together into a single word.
We tooknouns that appeared in SEMCOR twice or in theBNC at least 25 times and used the BNC to com-pute the information-content analog ?
for individ-ual nouns (For example, the probabilities in Figure 1correspond to ?
).4.1 TopicsLike the topics created by structures such as LDA,the topics in Table 2 coalesce around reasonablethemes.
The word list was compiled by summingover all of the possible leaves that could have gen-erated each of the words and sorting the words bydecreasing probability.
In the vast majority of cases,a single synset?s high probability is responsible forthe words?
positions on the list.Reassuringly, many of the top senses for thepresent words correspond to the most frequent sensein SEMCOR.
For example, in Topic 4, the senses for?space?
and ?function?
correspond to the top sensesin SEMCOR, and while the top sense for ?set?
corre-sponds to ?an abstract collection of numbers or sym-bols?
rather than ?a group of the same kind that be-long together and are so used,?
it makes sense giventhe math-based words in the topic.
?Point,?
however,corresponds to the sense used in the phrase ?I got tothe point of boiling the water,?
which is neither thetop SEMCOR sense nor a sense which makes sensegiven the other words in the topic.While the topics presented in Table 2 resemblethe topics one would obtain through models likeLDA (Blei et al, 2003), they are not identical.
Be-cause of the lengthy process of Gibbs sampling, weinitially thought that using LDA assignments as aninitial state would converge faster than a random ini-tial assignment.
While this was the case, it con-verged to a state that less probable than the randomlyinitialized state and no better at sense disambigua-tion (and sometimes worse).
The topics presentedin 2 represent words both that co-occur together ina corpus and co-occur on paths through WORDNET.Because topics created through LDA only have thefirst property, they usually do worse in terms of bothtotal probability and disambiguation accuracy (seeFigure 3).Another interesting property of topics in LDAWNis that, with higher levels of smoothing, words thatdon?t appear in a corpus (or appear rarely) but arein similar parts of WORDNET might have relativelyhigh probability in a topic.
For example, ?maturity?in topic two in Table 2 is sandwiched between ?foot?and ?center,?
both of which occur about five timesmore than ?maturity.?
This might improve LDA-based information retrieval schemes (Wei and Croft,2006) .1028174019300.23 0.763122 0.420.0122360.10 0.000.000.000.007626someone0.0096097110.00912031617438240.00cancer7998922 genus0.040.048564599star_sign0.0685655800.06cancer0.59100327cancer1constellation0.010.01cancer0.5crab0.5138754080.58 0.1914049094 14046733tumor0.97140509580.00malignancy0.060.94140514510.90cancer0.96Synset IDTransition ProbWord19578881.0Figure 2: The possible paths to reach the word ?cancer?
in WORDNET along with transition probabilitiesfrom the medically-themed Topic 2 in Table 2, with the most probable path highlighted.
The dashed linesrepresent multiple links that have been consolidated, and synsets are represented by their offsets withinWORDNET 2.1.
Some words for immediate hypernyms have also been included to give context.
In all othertopics, the person, animal, or constellation senses were preferred.Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7president growth material point water plant musicparty age object number house change filmcity treatment color value road month workelection feed form function area worker lifeadministration day subject set city report timeofficial period part square land mercer worldoffice head self space home requirement groupbill portion picture polynomial farm bank audienceyesterday length artist operator spring farmer playcourt level art component bridge production thingmeet foot patient corner pool medium stylepolice maturity communication direction site petitioner yearservice center movement curve interest relationship showTable 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topicmodel trained on the words in SEMCOR.
These are summed over all of the possible synsets that generatethe words.
However, the vast majority of the contributions come from a single synset.10290.275 0.280.285 0.290.295 0.30.305010002000300040005000600070008000900010000AccuracyIterationUnseededSeeded withLDA-96000-94000-92000-90000-88000-86000-84000-82000-80000010002000300040005000600070008000900010000Model ProbabilityIterationUnseededSeeded withLDAFigure 3: Topics seeded with LDA initially havea higher disambiguation accuracy, but are quicklymatched by unseeded topics.
The probability for theseeded topics starts lower and remains lower.4.2 Topics and the Weight of the PriorBecause the Dirichlet smoothing factor in partdetermines the topics, it also affects the disam-biguation.
Figure 4 shows the modal disambigua-tion achieved for each of the settings of S ={0.1, 1, 5, 10, 15, 20}.
Each line is one setting of Kand each point on the line is a setting of S. Eachdata point is a run for the Gibbs sampler for 10,000iterations.
The disambiguation, taken at the mode,improved with moderate settings of S, which sug-gests that the data are still sparse for many of thewalks, although the improvement vanishes if S dom-inates with much larger values.
This makes sense,as each walk has over 100,000 parameters, there arefewer than 100,000 words in SEMCOR, and each0.240.260.28 0.30.320.340.360.38S=20S=15S=10S=5S=1S=0.1AccuracySmoothing Factor64 topics32 topics16 topics 8 topics4 topics2 topics1 topic RandomFigure 4: Each line represents experiments with a setnumber of topics and variable amounts of smooth-ing on the SEMCOR corpus.
The random baselineis at the bottom of the graph, and adding topics im-proves accuracy.
As smoothing increases, the prior(based on token frequency) becomes stronger.
Ac-curacy is the percentage of correctly disambiguatedpolysemous words in SEMCOR at the mode.word only serves as evidence to at most 19 parame-ters (the length of the longest path in WORDNET).Generally, a greater number of topics increasedthe accuracy of the mode, but after around sixteentopics, gains became much smaller.
The effect of ?is also related to the number of topics, as a value of Sfor a very large number of topics might overwhelmthe observed data, while the same value of S mightbe the perfect balance for a smaller number of topics.For comparison, the method of using a WORDNET-WALK applied to smaller contexts such as sentencesor documents achieves an accuracy of between 26%and 30%, depending on the level of smoothing.5 Error AnalysisThis method works well in cases where the delin-eation can be readily determined from the over-all topic of the document.
Words such as ?kid,??may,?
?shear,?
?coach,?
?incident,?
?fence,?
?bee,?and (previously used as an example) ?colt?
wereall perfectly disambiguated by this method.
Figure2 shows the WORDNET-WALK corresponding to amedical topic that correctly disambiguates ?cancer.
?Problems arose, however, with highly frequent1030words, such as ?man?
and ?time?
that have manysenses and can occur in many types of documents.For example, ?man?
can be associated with manypossible meanings: island, game equipment, ser-vant, husband, a specific mammal, etc.Although we know that the ?adult male?
senseshould be preferred, the alternative meanings willalso be likely if they can be assigned to a topicthat shares common paths in WORDNET; the doc-uments contain, however, many other places, jobs,and animals which are reasonable explanations (toLDAWN) of how ?man?
was generated.
Unfortu-nately, ?man?
is such a ubiquitous term that top-ics, which are derived from the frequency of wordswithin an entire document, are ultimately uninfor-mative about its usage.While mistakes on these highly frequent termssignificantly hurt our accuracy, errors associatedwith less frequent terms reveal that WORDNET?sstructure is not easily transformed into a probabilis-tic graph.
For instance, there are two senses ofthe word ?quarterback,?
a player in American foot-ball.
One is position itself and the other is a per-son playing that position.
While one would expectco-occurrence in sentences such as ?quarterback is aeasy position, so our quarterback is happy,?
the pathsto both terms share only the root node, thus makingit highly unlikely a topic would cover both senses.Because of WORDNET?s breadth, rare sensesalso impact disambiguation.
For example, themetonymical use of ?door?
to represent a wholebuilding as in the phrase ?girl next door?
is un-der the same parent as sixty other synsets contain-ing ?bridge,?
?balcony,?
?body,?
?arch,?
?floor,?
and?corner.?
Surrounded by such common terms thatare also likely to co-occur with the more conven-tional meanings of door, this very rare sense be-comes the preferred disambiguation of ?door.
?6 Related WorkAbney and Light?s initial probabilistic WSD ap-proach (1999) was further developed into a Bayesiannetwork model by Ciaramita and Johnson (2000),who likewise used the appearance of monosemousterms close to ambiguous ones to ?explain away?
theusage of ambiguous terms in selectional restrictions.We have adapted these approaches and put them intothe context of a topic model.Recently, other approaches have created ad hocconnections between synsets in WORDNET and thenconsidered walks through the newly created graph.Given the difficulties of using existing connectionsin WORDNET, Mihalcea (2005) proposed creatinglinks between adjacent synsets that might comprisea sentence, initially setting weights to be equal tothe Lesk overlap between the pairs, and then usingthe PageRank algorithm to determine the stationarydistribution over synsets.6.1 Topics and DomainsYarowsky was one of the first to contend that ?thereis one sense for discourse?
(1992).
This has leadto the approaches like that of Magnini (Magnini etal., 2001) that attempt to find the category of a text,select the most appropriate synset, and then assignthe selected sense using domain annotation attachedto WORDNET.LDAWN is different in that the categories are notan a priori concept that must be painstakingly anno-tated within WORDNET and require no augmenta-tion of WORDNET.
This technique could indeed beused with any hierarchy.
Our concepts are the onesthat best partition the space of documents and do thebest job of describing the distinctions of diction thatseparate documents from different domains.6.2 Similarity MeasuresOur approach gives a probabilistic method of us-ing information content (Resnik, 1995) as a start-ing point that can be adjusted to cluster words ina given topic together; this is similar to the Jiang-Conrath similarity measure (1997), which has beenused in many applications in addition to disambigua-tion.
Patwardhan (2003) offers a broad evaluation ofsimilarity measures for WSD.Our technique for combining the cues of topicsand distance in WORDNET is adjusted in a way sim-ilar in spirit to Buitelaar and Sacaleanu (2001), butwe consider the appearance of a single term to beevidence for not just that sense and its immediateneighbors in the hyponomy tree but for all of thesense?s children and ancestors.Like McCarthy (2004), our unsupervised systemacquires a single predominant sense for a domainbased on a synthesis of information derived from a1031textual corpus, topics, and WORDNET-derived sim-ilarity, a probabilistic information content measure.By adding syntactic information from a thesaurusderived from syntactic features (taken from Lin?s au-tomatically generated thesaurus (1998)), McCarthyachieved 48% accuracy in a similar evaluation onSEMCOR; LDAWN is thus substantially less effec-tive in disambiguation compared to state-of-the-artmethods.
This suggests, however, that other meth-ods might be improved by adding topics and that ourmethod might be improved by using more informa-tion than word counts.7 Conclusion and Future WorkThe LDAWN model presented here makes two con-tributions to research in automatic word sense dis-ambiguation.
First, we demonstrate a method for au-tomatically partitioning a document into topics thatincludes explicit semantic information.
Second, weshow that, at least for one simple model of WSD,embedding a document in probabilistic latent struc-ture, i.e., a ?topic,?
can improve WSD.There are two avenues of research with LDAWNthat we will explore.
First, the statistical nature ofthis approach allows LDAWN to be used as a com-ponent in larger models for other language tasks.Other probabilistic models of language could in-sert the ability to query synsets or paths of WORD-NET.
Similarly, any topic based information re-trieval scheme could employ topics that include se-mantically relevant (but perhaps unobserved) terms.Incorporating this model in a larger syntactically-aware model, which could benefit from the localcontext as well as the document level context, is animportant component of future research.Second, the results presented here show a markedimprovement in accuracy as more topics are addedto the baseline model, although the final result is notcomparable to state-of-the-art techniques.
As mosterrors were attributable to the hyponomy structureof WORDNET, incorporating the novel use of topicmodeling presented here with a more mature unsu-pervised WSD algorithm to replace the underlyingWORDNET-WALK could lead to advances in state-of-the-art unsupervised WSD accuracy.ReferencesSteven Abney and Marc Light.
1999.
Hiding a semantichierarchy in a markov model.
In Proceedings of theWorkshop on Unsupervised Learning in Natural Lan-guage Processing, pages 1?8.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Paul Buitelaar and Bogdan Sacaleanu.
2001.
Rankingand selecting synsets by domain relevance.
In Pro-ceedings of WordNet and Other Lexical Resources:Applications, Extensions and Customizations.
NAACL2001.
Association for Computational Linguistics.Massimiliano Ciaramita and Mark Johnson.
2000.
Ex-plaining away ambiguity: Learning verb selectionalpreference with bayesian networks.
In COLING-00,pages 187?193.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In HLT?91: Proceedings of the workshop on Speech and Nat-ural Language, pages 233?237.
Association for Com-putational Linguistics.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, pages 5228?5235.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
Proceedings of the Twenty-Second AnnualInternational SIGIR Conference.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical taxon-omy.
In Proceedings on International Conference onResearch in Computational Linguistics, Taiwan.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proc.
15th International Conf.
on Ma-chine Learning, pages 296?304.Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo.
2001.
Using domain informationfor word sense disambiguation.
In In Proceedings of2nd International Workshop on Evaluating Word SenseDisambiguation Systems, Toulouse, France.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word senses inuntagged text.
In In 42nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 280?287.Rada Mihalcea.
2005.
Large vocabulary unsupervisedword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedings ofthe Joint Human Language Technology and EmpiricalMethods in Natural Language Processing Conference,pages 411?418.1032George Miller, Claudia Leacock, Randee Tengi, and RossBunker.
1993.
A semantic concordance.
In 3rdDARPA Workshop on Human Language Technology,pages 303?308.George A. Miller.
1990.
Nouns in WordNet: A lexicalinheritance system.
International Journal of Lexicog-raphy, 3(4):245?264.Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-ersen.
2003.
Using Measures of Semantic Related-ness for Word Sense Disambiguation.
In Proceedingsof the Fourth International Conference on IntelligentText Processing and Computational Linguistics, pages241?257.Matthew Purver, Konrad Ko?rding, Thomas Griffiths, andJoshua Tenenbaum.
2006.
Unsupervised topic mod-elling for multi-party spoken discourse.
In Proceed-ings of COLING-ACL.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Inter-national Joint Conferences on Artificial Intelligence,pages 448?453.Christian Robert and George Casella.
2004.
MonteCarlo Statistical Methods.
Springer Texts in Statistics.Springer-Verlag, New York, NY.Xing Wei and Bruce Croft.
2006.
LDA-based docu-ment models for ad-hoc retrieval.
In Proceedings ofthe Twenty-Ninth Annual International SIGIR Confer-ence.1033
