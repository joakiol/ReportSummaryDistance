Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 732?741,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsN-Best Rescoring Based on Pitch-accent PatternsJe Hun Jeon1 Wen Wang2 Yang Liu11Department of Computer Science, The University of Texas at Dallas, USA2Speech Technology and Research Laboratory, SRI International, USA{jhjeon,yangl}@hlt.utdallas.edu, wwang@speech.sri.comAbstractIn this paper, we adopt an n-best rescoringscheme using pitch-accent patterns to improveautomatic speech recognition (ASR) perfor-mance.
The pitch-accent model is decoupledfrom the main ASR system, thus allowing usto develop it independently.
N-best hypothe-ses from recognizers are rescored by addi-tional scores that measure the correlation ofthe pitch-accent patterns between the acousticsignal and lexical cues.
To test the robustnessof our algorithm, we use two different datasets and recognition setups: the first one is En-glish radio news data that has pitch accent la-bels, but the recognizer is trained from a smallamount of data and has high error rate; the sec-ond one is English broadcast news data usinga state-of-the-art SRI recognizer.
Our experi-mental results demonstrate that our approachis able to reduce word error rate relatively byabout 3%.
This gain is consistent across thetwo different tests, showing promising futuredirections of incorporating prosodic informa-tion to improve speech recognition.1 IntroductionProsody refers to the suprasegmental features of nat-ural speech, such as rhythm and intonation, sinceit normally extends over more than one phonemesegment.
Speakers use prosody to convey paralin-guistic information such as emphasis, intention, atti-tude, and emotion.
Humans listening to speech withnatural prosody are able to understand the contentwith low cognitive load and high accuracy.
How-ever, most modern ASR systems only use an acous-tic model and a language model.
Acoustic informa-tion in ASR is represented by spectral features thatare usually extracted over a window length of a fewtens of milliseconds.
They miss useful informationcontained in the prosody of the speech that may helprecognition.Recently a lot of research has been done in au-tomatic annotation of prosodic events (Wightmanand Ostendorf, 1994; Sridhar et al, 2008; Anan-thakrishnan and Narayanan, 2008; Jeon and Liu,2009).
They used acoustic and lexical-syntacticcues to annotate prosodic events with a variety ofmachine learning approaches and achieved goodperformance.
There are also many studies us-ing prosodic information for various spoken lan-guage understanding tasks.
However, research usingprosodic knowledge for speech recognition is stillquite limited.
In this study, we investigate leverag-ing prosodic information for recognition in an n-bestrescoring framework.Previous studies showed that prosodic events,such as pitch-accent, are closely related with acous-tic prosodic cues and lexical structure of utterance.The pitch-accent pattern given acoustic signal isstrongly correlated with lexical items, such as syl-lable identity and canonical stress pattern.
There-fore as a first study, we focus on pitch-accent in thispaper.
We develop two separate pitch-accent de-tection models, using acoustic (observation model)and lexical information (expectation model) respec-tively, and propose a scoring method for the cor-relation of pitch-accent patterns between the twomodels for recognition hypotheses.
The n-best listis rescored using the pitch-accent matching scores732combined with the other scores from the ASR sys-tem (acoustic and language model scores).
We showthat our method yields a word error rate (WER) re-duction of about 3.64% and 2.07% relatively on twobaseline ASR systems, one being a state-of-the-artrecognizer for the broadcast news domain.
The factthat it holds across different baseline systems sug-gests the possibility that prosody can be used to helpimprove speech recognition performance.The remainder of this paper is organized as fol-lows.
In the next section, we review previous workbriefly.
Section 3 explains the models and featuresfor pitch-accent detection.
We provide details of ourn-best rescoring approach in Section 4.
Section 5describes our corpus and baseline ASR setup.
Sec-tion 6 presents our experiments and results.
The lastsection gives a brief summary along with future di-rections.2 Previous WorkProsody is of interest to speech researchers be-cause it plays an important role in comprehensionof spoken language by human listeners.
The useof prosody in speech understanding applications hasbeen quite extensive.
A variety of applicationshave been explored, such as sentence and topic seg-mentation (Shriberg et al, 2000; Rosenberg andHirschberg, 2006), word error detection (Litman etal., 2000), dialog act detection (Sridhar et al, 2009),speaker recognition (Shriberg et al, 2005), and emo-tion recognition (Benus et al, 2007), just to name afew.Incorporating prosodic knowledge is expectedto improve the performance of speech recogni-tion.
However, how to effectively integrate prosodywithin the traditional ASR framework is a difficultproblem, since prosodic features are not well de-fined and they come from a longer region, which isdifferent from spectral features used in current ASRsystems.
Various research has been conducted try-ing to incorporate prosodic information in ASR.
Oneway is to directly integrate prosodic features intothe ASR framework (Vergyri et al, 2003; Ostendorfet al, 2003; Chen and Hasegawa-Johnson, 2006).Such efforts include prosody dependent acoustic andpronunciation model (allophones were distinguishedaccording to different prosodic phenomenon), lan-guage model (words were augmented by prosodyevents), and duration modeling (different prosodicevents were modeled separately and combined withconventional HMM).
This kind of integration hasadvantages in that spectral and prosodic features aremore tightly coupled and jointly modeled.
Alterna-tively, prosody was modeled independently from theacoustic and language models of ASR and used torescore recognition hypotheses in the second pass.This approach makes it possible to independentlymodel and optimize the prosodic knowledge and tocombine with ASR hypotheses without any modi-fication of the conventional ASR modules.
In or-der to improve the rescoring performance, variousprosodic knowledge was studied.
(Ananthakrishnanand Narayanan, 2007) used acoustic pitch-accentpattern and its sequential information given lexi-cal cues to rescore n-best hypotheses.
(Kalinli andNarayanan, 2009) used acoustic prosodic cues suchas pitch and duration along with other knowledgeto choose a proper word among several candidatesin confusion networks.
Prosodic boundaries basedon acoustic cues were used in (Szaszak and Vicsi,2007).We take a similar approach in this study as thesecond approach above in that we develop prosodicmodels separately and use them in a rescoringframework.
Our proposed method differs from pre-vious work in the way that the prosody model is usedto help ASR.
In our approach, we explicitly modelthe symbolic prosodic events based on acoustic andlexical information.
We then capture the correla-tion of pitch-accent patterns between the two differ-ent cues, and use that to improve recognition perfor-mance in an n-best rescoring paradigm.3 Prosodic ModelAmong all the prosodic events, we use only pitch-accent pattern in this study, because previous stud-ies have shown that acoustic pitch-accent is stronglycorrelated with lexical items, such as canonicalstress pattern and syllable identity that can be eas-ily acquired from the output of conventional ASRand pronunciation dictionary.
We treat pitch-accentdetection as a binary classification task, that is, aclassifier is used to determine whether the base unitis prominent or not.
Since pitch-accent is usually733carried by syllables, we use syllables as our units,and the syllable definition of each word is basedon CMU pronunciation dictionary which has lexi-cal stress and syllable boundary marks (Bartlett etal., 2009).
We separately develop acoustic-prosodicand lexical-prosodic models and use the correlationbetween the two models for each syllable to rescorethe n-best hypotheses of baseline ASR systems.3.1 Acoustic-prosodic FeaturesSimilar to most previous work, the prosodic featureswe use include pitch, energy, and duration.
We alsoadd delta features of pitch and energy.
Duration in-formation for syllables is derived from the speechwaveform and phone-level forced alignment of thetranscriptions.
In order to reduce the effect by bothinter-speaker and intra-speaker variation, both pitchand energy values are normalized (z-value) with ut-terance specific means and variances.
For pitch, en-ergy, and their delta values, we apply several cate-gories of 12 functions to generate derived features.?
Statistics (7): minimum, maximum, range,mean, standard deviation, skewness and kurto-sis value.
These are used widely in prosodicevent detection and emotion detection.?
Contour (5): This is approximated by taking5 leading terms in the Legendre polynomialexpansion.
The approximation of the contourusing the Legendre polynomial expansion hasbeen successfully applied in quantitative pho-netics (Grabe et al, 2003) and in engineeringapplications (Dehak et al, 2007).
Each termmodels a particular aspect of the contour, suchas the slope, and information about the curva-ture.We use 6 duration features, that is, raw, normal-ized, and relative durations (ms) of the syllable andvowel.
Normalization (z-value) is performed basedon statistics for each syllable and vowel.
The rela-tive value is the difference between the normalizedcurrent duration and the following one.In the above description, we assumed that theevent of a syllable is only dependent on its observa-tions, and did not consider contextual effect.
To al-leviate this restriction, we expand the features by in-corporating information about the neighboring sylla-bles.
Based on the study in (Jeon and Liu, 2010) thatevaluated using left and right contexts, we choose touse one previous and one following context in thefeatures.
The total number of features used in thisstudy is 162.3.2 Lexical-prosodic FeaturesThere is a very strong correlation between pitch-accent in an utterance and its lexical information.Previous studies have shown that the lexical fea-tures perform well for pitch-accent prediction.
Thedetailed features for training the lexical-prosodicmodel are as follows.?
Syllable identity: We kept syllables that appearmore than 5 times in the training corpus.
Theother syllables that occur less are collapsed intoone syllable representation.?
Vowel phone identity: We used vowel phoneidentity as a feature.?
Lexical stress: This is a binary feature to rep-resent if the syllable corresponds to a lexicalstress based on the pronunciation dictionary.?
Boundary information: This is a binary featureto indicate if there is a word boundary beforethe syllable.For lexical features, based on the study in (Jeonand Liu, 2010), we added two previous and two fol-lowing contexts in the final features.3.3 Prosodic Model TrainingWe choose to use a support vector machine (SVM)classifier1 for the prosodic model based on previouswork on prosody labeling study in (Jeon and Liu,2010).
We use RBF kernel for the acoustic model,and 3-order polynomial kernel for the lexical model.In our experiments, we investigate two kindsof training methods for prosodic modeling.
Thefirst one is a supervised method where models aretrained using all the labeled data.
The second isa semi-supervised method using co-training algo-rithm (Blum and Mitchell, 1998), described in Algo-rithm 1.
Given a set L of labeled data and a set U ofunlabeled data with two views, it then iterates in the1LIBSVM ?
A Library for Support Vector Machines, loca-tion: http://www.csie.ntu.edu.tw/?cjlin/libsvm/734Algorithm 1 Co-training algorithm.Given:- L: labeled examples; U: unlabeled examples- there are two views V1 and V2 on an example xInitialize:- L1=L, samples used to train classifiers h1- L2=L, samples used to train classifiers h2Loop for k iterations- create a small pool U?
choosing from U- use V1(L1) to train classifier h1and V2(L2) to train classifier h2- let h1 label/select examples Dh1 from U?- let h2 label/select examples Dh2 from U?- add self-labeled examples Dh1 to L2and Dh2 to L1- remove Dh1 and Dh2 from Ufollowing procedure.
The algorithm first creates asmaller pool U?
containing unlabeled data from U. Ituses Li (i = 1, 2) to train two distinct classifiers: theacoustic classifier h1, and the lexical classifier h2.We use function Vi (i = 1, 2) to represent that onlya single view is used for training h1 or h2.
These twoclassifiers are used to make predictions for the unla-beled setU?, and only when they agree on the predic-tion for a sample, their predicted class is used as thelabel for this sample.
Then among these self-labeledsamples, the most confident ones by one classifierare added to the data set Li for training the otherclassifier.
This iteration continues until reaching thedefined number of iterations.
In our experiment, thesize of the pool U?
is 5 times of the size of trainingdata Li, and the size of the added self-labeled ex-ample set, Dhi , is 5% of Li.
For the newly selectedDhi , the distribution of the positive and negative ex-amples is the same as that of the training data Li.This co-training method is expected to cope withtwo problems in prosodic model training.
The firstproblem is the different decision patterns betweenthe two classifiers: the acoustic model has relativelyhigher precision, while the lexical model has rela-tively higher recall.
The goal of the co-training al-gorithm is to learn from the difference of each clas-sifier, thus it can improve the performance as wellas reduce the mismatch of two classifiers.
The sec-ond problem is the mismatch of data used for modeltraining and testing, which often results in systemperformance degradation.
Using co-training, we canuse the unlabeled data from the domain that matchesthe test data, adapting the model towards test do-main.4 N-Best Rescoring SchemeIn order to leverage prosodic information for bet-ter speech recognition performance, we augment thestandard ASR equation to include prosodic informa-tion as following:W?
= argmaxWp(W |As, Ap)= argmaxWp(As, Ap|W )p(W ) (1)where As and Ap represent acoustic-spectral fea-tures and acoustic-prosodic features.
We can furtherassume that spectral and prosodic features are con-ditionally independent given a word sequence W ,therefore, Equation 1 can be rewritten as following:W?
?
argmaxWp(As|W )p(W )p(Ap|W ) (2)The first two terms stand for the acoustic and lan-guage models in the original ASR system, and thelast term means the prosody model we introduce.
In-stead of using the prosodic model in the first pass de-coding, we use it to rescore n-best candidates froma speech recognizer.
This allows us to train theprosody models independently and better optimizethe models.For p(Ap|W ), the prosody score for a word se-quence W , in this work we propose a method to es-timate it, also represented as scoreW?prosody(W ).The idea of scoring the prosody patterns is that thereis some expectation of pitch-accent patterns giventhe lexical sequence (W ), and the acoustic pitch-accent should match with this expectation.
For in-stance, in the case of a prominent syllable, bothacoustic and lexical evidence show pitch-accent, andvice versa.
In order to maximize the agreement be-tween the two sources, we measure how good theacoustic pitch-accent in speech signal matches thegiven lexical cues.
For each syllable Si in the n-bestlist, we use acoustic-prosodic cues (ai) to estimatethe posterior probability that the syllable is promi-nent (P), p(P |ai).
Similarly, we use lexical cues (li)735to determine the syllable?s pitch-accent probabilityp(P |li).
Then the prosody score for a syllable Si isestimated by the match of the pitch-accent patternsbetween acoustic and lexical information using thedifference of the posteriors from the two models:scoreS?prosody(Si) ?
1?
| p(P |ai) ?
p(P |li) | (3)Furthermore, we take into account the effect dueto varying durations for different syllables.
We no-tice that syllables without pitch-accent have muchshorter duration than the prominent ones, and theprosody scores for the short syllables tend to behigh.
This means that if a syllable is split into twoconsecutive non-prominent syllables, the agreementscore may be higher than a long prominent syllable.Therefore, we introduce a weighting factor based onsyllable duration (dur(i)).
For a candidate word se-quence (W) consisting of n syllables, its prosodicscore is the sum of the prosodic scores for all thesyllables in it weighted by their duration (measuredusing milliseconds), that is:scoreW?prosody(W ) ?n?i=1log(scoreS?prosody(Si)) ?
dur(i) (4)We then combine this prosody score with theoriginal acoustic and language model likelihood(P (As|W ) and P (W ) in Equation 2).
In practice,we need to weight them differently, therefore, thecombined score for a hypothesis W is:Score(W ) = ?
?
scoreW?prosody(W )+ scoreASR(W ) (5)where scoreASR(W ) is generated by ASR systems(composed of acoustic and language model scores)and ?
is optimized using held out data.5 Data and Baseline SystemsOur experiments are carried out using two differentdata sets and two different recognition systems aswell in order to test the robustness of our proposedmethod.The first data set is the Boston University RadioNews Corpus (BU) (Ostendorf et al, 1995), whichconsists of broadcast news style read speech.
TheBU corpus has about 3 hours of read speech from7 speakers (3 female, 4 male).
Part of the data hasbeen labeled with ToBI-style prosodic annotations.In fact, the reason that we use this corpus, instead ofother corpora typically used for ASR experiments,is because of its prosodic labels.
We divided theentire data corpus into a training set and a test set.There was no speaker overlap between training andtest sets.
The training set has 2 female speakers (f2and f3) and 3 male ones (m2, m3, m4).
The test set isfrom the other two speakers (f1 and m1).
We use 200utterances for the recognition experiments.
Each ut-terance in BU corpus consists of more than one sen-tences, so we segmented each utterance based onpause, resulting in a total number of 713 segmentsfor testing.
We divided the test set roughly equallyinto two sets, and used one for parameter tuning andthe other for rescoring test.
The recognizer used forthis data set was based on Sphinx-32.
The context-dependent triphone acoustic models with 32 Gaus-sian mixtures were trained using the training par-tition of the BU corpus described above, togetherwith the broadcast new data.
A standard back-off tri-gram language model with Kneser-Ney smoothingwas trained using the combined text from the train-ing partition of the BU, Wall Street Journal data, andpart of Gigaword corpus.
The vocabulary size wasabout 10K words and the out-of-vocabulary (OOV)rate on the test set was 2.1%.The second data set is from broadcast news (BN)speech used in the GALE program.
The recognitiontest set contains 1,001 utterances.
The n-best hy-potheses for this data set are generated by a state-of-the-art SRI speech recognizer, developed for broad-cast news speech (Stolcke et al, 2006; Zheng etal., 2007).
This system yields much better perfor-mance than the first one.
We also divided the testset roughly equally into two sets for parameter tun-ing and testing.
From the data used for training thespeech recognizer, we randomly selected 5.7 hoursof speech (4,234 utterances) for the co-training al-gorithm for the prosodic models.For prosodic models, we used a simple binaryrepresentation of pitch-accent in the form of pres-ence versus absence.
The reference labels are de-2CMU Sphinx - Speech Recognition Toolkit, location:http://www.speech.cs.cmu.edu/sphinx/tutorial.html736rived from the ToBI annotation in the BU corpus,and the ratio of pitch-accented syllables is about34%.
Acoustic-prosodic and lexical-prosodic mod-els were separately developed using the features de-scribed in Section 3.
Feature extraction was per-formed at the syllable level from force-aligned data.For the supervised approach, we used those utter-ances in the training data partition with ToBI labelsin the BU corpus (245 utterances, 14,767 syllables).For co-training, the labeled data from BU corpus isused as initial training, and the other unlabeled datafrom BU and BN are used as unlabeled data.6 Experimental Results6.1 Pitch-accent DetectionFirst we evaluate the performance of our acoustic-prosodic and lexical-prosodic models for pitch-accent detection.
For rescoring, not only the ac-curacies of the two individual prosodic models areimportant, but also the pitch-accent agreement scorebetween the two models (as shown in Equation 3)is critical, therefore, we present results using thesetwo metrics.
Table 1 shows the accuracy of eachmodel for pitch-accent detection, and also the av-erage prosody score of the two models (i.e., Equa-tion 3) for positive and negative classes (using ref-erence labels).
These results are based on the BUlabeled data in the test set.
To compare our pitch ac-cent detection performance with previous work, weinclude the result of (Jeon and Liu, 2009) as a ref-erence.
Compared to previous work, the acousticmodel achieved similar performance, while the per-formance of lexical model is a bit lower.
The lowerperformance of lexical model is mainly because wedo not use part-of-speech (POS) information in thefeatures, since we want to only use the word outputfrom the ASR system (without additional POS tag-ging).As shown in Table 1, when using the co-trainingalgorithm, as described in Section 3.3, the over-all accuracies improve slightly and therefore theprosody score is also increased.
We expect this im-proved model will be more beneficial for rescoring.6.2 N-Best RescoringFor the rescoring experiment, we use 100-best hy-potheses from the two different ASR systems, as de-Accuracy(%) Prosody scoreAcoustic Lexical Pos NegSupervised 83.97 84.48 0.747 0.852Co-training 84.54 84.99 0.771 0.867Reference 83.53 87.92 - -Table 1: Pitch accent detection results: performance ofindividual acoustic and lexical models, and the agreementbetween the twomodels (i.e., prosody score for a syllable,Equation 3) for positive and negative classes.
Also shownis the reference result for pitch accent detection from Jeonand Liu (2009).scribed in Section 5.
We apply the acoustic and lex-ical prosodic models to each hypothesis to obtain itsprosody score, and combine it with ASR scores tofind the top hypothesis.
The weights were optimizedusing one test set and applied to the other.
We reportthe average result of the two testings.Table 2 shows the rescoring results using the firstrecognition system on BU data, which was trainedwith a relatively small amount of data.
The 1-best baseline uses the first hypothesis that has thebest ASR score.
The oracle result is from the besthypothesis that gives the lowest WER by compar-ing all the candidates to the reference transcript.We used two prosodic models as described in Sec-tion 3.3.
The first one is the base prosodic model us-ing supervised training (S-model).
The second is theprosodic model with the co-training algorithm (C-model).
For these rescoring experiments, we tuned?
(in Equation 5) when combining the ASR acous-tic and language model scores with the additionalprosody score.
The value in parenthesis in Table 2means the relative WER reduction when comparedto the baseline result.
We show the WER results forboth the development and the test set.As shown in Table 2, we observe performanceimprovement using our rescoring method.
Usingthe base S-model yields reasonable improvement,and C-model further reduces WER.
Even though theprosodic event detection performance of these twoprosodic models is similar, the improved prosodyscore between the acoustic and lexical prosodicmodels using co-training helps rescoring.
Afterrescoring using prosodic knowledge, the WER is re-duced by 0.82% (3.64% relative).
Furthermore, wenotice that the difference between development and737WER (%)1-best baseline 22.64S-modelDev 21.93 (3.11%)Test 22.10 (2.39%)C-modelDev 21.76 (3.88%)Test 21.81 (3.64%)Oracle 15.58Table 2: WER of the baseline system and after rescoringusing prosodic models.
Results are based on the first ASRsystem.test data is smaller when using the C-model than S-model, which means that the prosodic model withco-training is more stable.
In fact, we found thatthe optimal value of ?
is 94 and 57 for the twofolds using S-model, and is 99 and 110 for the C-model.
These verify again that the prosodic scorescontribute more in the combination with ASR likeli-hood scores when using the C-model, and are morerobust across different tuning sets.
Ananthakrish-nan and Narayanan (2007) also used acoustic/lexicalprosodic models to estimate a prosody score and re-ported 0.3% recognition error reduction on BU datawhen rescoring 100-best list (their baseline WER is22.8%).
Although there is some difference in experi-mental setup (data, classifier, features) between oursand theirs, our S-model showed comparable perfor-mance gain and the result of C-model is significantlybetter than theirs.Next we test our n-best rescoring approach using astate-of-the-art SRI speech recognizer on BN data toverify if our approach can generalize to better ASRn-best lists.
This is often the concern that improve-ments observed on a poor ASR system do not holdfor better ASR systems.
The rescoring results areshown in Table 3.
We can see that the baseline per-formance of this recognizer is much better than thatof the first ASR system (even though the recogni-tion task is also harder).
Our rescoring approachstill yields performance gain even using this state-of-the-art system.
The WER is reduced by 0.29%(2.07% relative).
This error reduction is lower thanthat in the first ASR system.
There are several pos-sible reasons.
First, the baseline ASR performanceis higher, making further improvement hard; sec-ond, and more importantly, the prosody models donot match well to the test domain.
We trained theprosody model using the BU data.
Even though co-training is used to leverage unlabeled BN data to re-duce data mismatch, it is still not as good as usinglabeled in-domain data for model training.WER (%)1-best baseline 13.77S-modelDev 13.53 (1.78%)Test 13.55 (1.63%)C-modelDev 13.48 (2.16%)Test 13.49 (2.07%)Oracle 9.23Table 3: WER of the baseline system and after rescoringusing prosodic models.
Results are based on the secondASR system.6.3 Analysis and DiscussionWe also analyze what kinds of errors are reducedusing our rescoring approach.
Most of the error re-duction came from substitution and insertion errors.Deletion error rate did not change much or some-times even increased.
For a better understanding ofthe improvement using the prosody model, we ana-lyzed the pattern of corrections (the new hypothesisafter rescoring is correct while the original 1-best iswrong) and errors.
Table 4 shows some positive andnegative examples from rescoring results using thefirst ASR system.
In this table, each word is asso-ciated with some binary expressions inside a paren-thesis, which stand for pitch-accent markers.
Twobits are used for each syllable: the first one is forthe acoustic-prosodic model and the second one isfor the lexical-prosodic model.
For both bits, 1 rep-resents pitch-accent, and 0 indicates none.
Thesehard decisions are obtained by setting a threshold of0.5 for the posterior probabilities from the acousticor lexical models.
For example, when the acousticclassifier predicts a syllable as pitch-accented andthe lexical one as not accented, ?10?
marker is as-signed to the syllable.
The number of such pairs ofpitch-accent markers is the same as the number ofsyllables in a word.
The bold words indicate correctwords and italic means errors.
As shown in the pos-itive example of Table 4, we find that our prosodicmodel is effective at identifying an erroneous wordwhen it is split into two words, resulting in dif-ferent pitch-accent patterns.
Language models are738Positive example1-best : most of the massachusetts(11 ) (10) (00) (11 00 01 00)rescored : most other massachusetts(11 ) (11 00) (11 00 01 00)Negative example1-best : robbery and on a theft(11 00 00) (00) (10) (00) (11)rescored : robbery and lot of theft(11 00 00) (00) (11) (00) (11)Table 4: Examples of rescoring results.
Binary expressions inside the parenthesis below a word represent pitch-accentmarkers for the syllables in the word.not good at correcting this kind of errors since bothword sequences are plausible.
Our model also intro-duces some errors, as shown in the negative exam-ple, which is mainly due to the inaccurate prosodymodel.We conducted more prosody rescoring experi-ments in order to understand the model behavior.These analyses are based on the n-best list from thefirst ASR system for the entire test set.
In the firstexperiment, among the 100 hypotheses in n-best list,we gave a prosody score of 0 to the 100th hypothe-sis, and used automatically obtained prosodic scoresfor the other hypotheses.
A zero prosody scoremeans the perfect agreement given acoustic and lex-ical cues.
The original scores from the recognizerwere combined with the prosodic scores for rescor-ing.
This was to verify that the range of the weight-ing factor ?
estimated on the development data (us-ing the original, not the modified prosody scores forall candidates) was reasonable to choose proper hy-pothesis among all the candidates.
We noticed that27% of the times the last hypothesis on the list wasselected as the best hypothesis.
This hypothesis hasthe highest prosodic scores, but lowest ASR score.This result showed that if the prosodic models wereaccurate enough, the correct candidate could be cho-sen using our rescoring framework.In the second experiment, we put the referencetext together with the other candidates.
We use thesame ASR scores for all candidates, and generatedprosodic scores using our prosody model.
This wasto test that our model could pick up correct candi-date using only the prosodic score.
We found thatfor 26% of the utterances, the reference transcriptwas chosen as the best one.
This was significantlybetter than random selection (i.e., 1/100), suggest-ing the benefit of the prosody model; however, thispercentage is not very high, implying the limitationof prosodic information for ASR or the current im-perfect prosodic models.In the third experiment, we replaced the 100thcandidate with the reference transcript and kept itsASR score.
When using our prosody rescoring ap-proach, we obtained a relative error rate reductionof 6.27%.
This demonstrates again that our rescor-ing method works well ?
if the correct hypothesis ison the list, even though with a low ASR score, us-ing prosodic information can help identify the cor-rect candidate.Overall the performance improvement we ob-tained from rescoring by incorporating prosodic in-formation is very promising.
Our evaluation usingtwo different ASR systems shows that the improve-ment holds even when we use a state-of-the-art rec-ognizer and the training data for the prosody modeldoes not come from the same corpus.
We believethe consistent improvements we observed for differ-ent conditions show that this is a direction worthy offurther investigation.7 ConclusionIn this paper, we attempt to integrate prosodic infor-mation for ASR using an n-best rescoring scheme.This approach decouples the prosodic model fromthe main ASR system, thus the prosodic model canbe built independently.
The prosodic scores that weuse for n-best rescoring are based on the matchingof pitch-accent patterns by acoustic and lexical fea-tures.
Our rescoring method achieved a WER reduc-tion of 3.64% and 2.07% relatively using two differ-ent ASR systems.
The fact that the gain holds acrossdifferent baseline systems (including a state-of-the-739art speech recognizer) suggests the possibility thatprosody can be used to improve speech recognitionperformance.As suggested by our experiments, better prosodicmodels can result in more WER reduction.
The per-formance of our prosodic model was improved withco-training, but there are still problems, such as theimbalance of the two classifiers?
prediction, as wellas for the two events.
In order to address these prob-lems, we plan to improve the labeling and selec-tion method in the co-training algorithm, and alsoexplore other training algorithms to reduce domainmismatch.
Furthermore, we are also interested inevaluating our approach on the spontaneous speechdomain, which is quite different from the data weused in this study.In this study, we used n-best rather than latticerescoring.
Since the prosodic features we use in-clude cross-word contextual information, it is notstraightforward to apply it directly to lattices.
Inour future work, we will develop models with onlywithin-word context, and thus allowing us to explorelattice rescoring, which we expect will yield moreperformance gain.ReferencesSankaranarayanan Ananthakrishnan and ShrikanthNarayanan.
2007.
Improved speech recognition usingacoustic and lexical correlated of pitch accent in an-best rescoring framework.
Proc.
of ICASSP, pages65?68.Sankaranarayanan Ananthakrishnan and ShrikanthNarayanan.
2008.
Automatic prosodic event detec-tion using acoustic, lexical and syntactic evidence.IEEE Transactions on Audio, Speech, and LanguageProcessing, 16(1):216?228.Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.2009.
On the syllabification of phonemes.
Proc.
ofNAACL-HLT, pages 308?316.Stefan Benus, Agust?
?n Gravano, and Julia Hirschberg.2007.
Prosody, emotions, and whatever.
Proc.
of In-terspeech, pages 2629?2632.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
Proc.
of theWorkshop on Computational Learning Theory, pages92?100.Ken Chen and Mark Hasegawa-Johnson.
2006.
Prosodydependent speech recognition on radio news corpusof American English.
IEEE Transactions on Audio,Speech, and Language Processing, 14(1):232?
245.Najim Dehak, Pierre Dumouchel, and Patrick Kenny.2007.
Modeling prosodic features with joint fac-tor analysis for speaker verification.
IEEE Transac-tions on Audio, Speech, and Language Processing,15(7):2095?2103.Esther Grabe, Greg Kochanski, and John Coleman.
2003.Quantitative modelling of intonational variation.
Proc.of SASRTLM, pages 45?57.Je Hun Jeon and Yang Liu.
2009.
Automatic prosodicevents detection suing syllable-based acoustic and syn-tactic features.
Proc.
of ICASSP, pages 4565?4568.Je Hun Jeon and Yang Liu.
2010.
Syllable-level promi-nence detection with acoustic evidence.
Proc.
of Inter-speech, pages 1772?1775.Ozlem Kalinli and Shrikanth Narayanan.
2009.
Contin-uous speech recognition using attention shift decodingwith soft decision.
Proc.
of Interspeech, pages 1927?1930.Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.2000.
Predicting automatic speech recognition perfor-mance using prosodic cues.
Proc.
of NAACL, pages218?225.Mari Ostendorf, Patti Price, and Stefanie Shattuck-Hufnagel.
1995.
The Boston University radio newscorpus.
Linguistic Data Consortium.Mari Ostendorf, Izhak Shafran, and Rebecca Bates.2003.
Prosody models for conversational speechrecognition.
Proc.
of the 2nd Plenary Meeting andSymposium on Prosody and Speech Processing, pages147?154.Andrew Rosenberg and Julia Hirschberg.
2006.
Storysegmentation of broadcast news in English, Mandarinand Arabic.
Proc.
of HLT-NAACL, pages 125?128.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,and Go?khan Tu?r.
2000.
Prosody-based automatic seg-mentation of speech into sentences and topics.
SpeechCommunication, 32(1-2):127?154.Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar,Anand Venkataraman, and Andreas Stolcke.
2005.Modeling prosodic feature sequences for speakerrecognition.
Speech Communication, 46(3-4):455?472.Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,and Shrikanth S. Narayanan.
2008.
Exploiting acous-tic and syntactic features for automatic prosody label-ing in a maximum entropy framework.
IEEE Trans-actions on Audio, Speech, and Language Processing,16(4):797?811.Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,and Shrikanth Narayanan.
2009.
Combining lexi-cal, syntactic and prosodic cues for improved online740dialog act tagging.
Computer Speech and Language,23(4):407?422.Andreas Stolcke, Barry Chen, Horacio Franco, VenkataRamana Rao Gadde, Martin Graciarena, Mei-YuhHwang, Katrin Kirchhoff, Arindam Mandal, NelsonMorgan, Xin Lin, Tim Ng, Mari Ostendorf, KemalSo?nmez, Anand Venkataraman, Dimitra Vergyri, WenWang, Jing Zheng, and Qifeng Zhu.
2006.
Recent in-novations in speech-to-text transcription at SRI-ICSI-UW.
IEEE Transactions on Audio, Speech and Lan-guage Processing, 14(5):1729?1744.
Special Issue onProgress in Rich Transcription.Gyorgy Szaszak and Klara Vicsi.
2007.
Speech recogni-tion supported by prosodic information for fixed stresslanguages.
Proc.
of TSD Conference, pages 262?269.Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,Luciana Ferrer, and Elizabeth Shriberg.
2003.Prosodic knowledge sources for automatic speechrecognition.
Proc.
of ICASSP, pages 208?211.Colin W. Wightman and Mari Ostendorf.
1994.
Auto-matic labeling of prosodic patterns.
IEEE Transactionon Speech and Auido Processing, 2(4):469?481.Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, An-dreas Stolcke, and Nelson Morgan.
2007.
Combin-ing discriminative feature, transform, and model train-ing for large vocabulary speech recognition.
Proc.
ofICASSP, pages 633?636.741
