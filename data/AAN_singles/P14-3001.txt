Proceedings of the ACL 2014 Student Research Workshop, pages 1?9,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsBayesian Kernel Methods for Natural Language ProcessingDaniel BeckDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdomdebeck1@sheffield.ac.ukAbstractKernel methods are heavily used in Natu-ral Language Processing (NLP).
Frequen-tist approaches like Support Vector Ma-chines are the state-of-the-art in manytasks.
However, these approaches lackefficient procedures for model selection,which hinders the usage of more advancedkernels.
In this work, we propose theuse of a Bayesian approach for kernelmethods, Gaussian Processes, which alloweasy model fitting even for complex kernelcombinations.
Our goal is to employ thisapproach to improve results in a number ofregression and classification tasks in NLP.1 IntroductionIn the last years, kernel methods have been suc-cessfully employed in many Natural LanguageProcessing tasks.
These methods allow the build-ing of non-parametric models which make less as-sumptions about the underlying pattern in the data.Another advantage of kernels is that they can bedefined in arbitrary structures like strings or trees,which greatly reduce the need for careful featureengineering in these structures.The properties cited above make kernel meth-ods ideal for problems where we do not havemuch prior knowledge about how the data be-haves.
This is a common setting in NLP, wherethey have been mostly applied in the form of Sup-port Vector Machines (SVMs).
Systems based onSVMs have been the state-of-the-art in classifica-tion tasks like Text Categorization (Lodhi et al,2002), Sentiment Analysis (Johansson and Mos-chitti, 2013; P?erez-Rosas and Mihalcea, 2013) andQuestion Classification (Moschitti, 2006; Croce etal., 2011).
Recently, they were also employed inregression settings like Machine Translation Qual-ity Estimation (Specia and Farzindar, 2010; Bojaret al, 2013) and structured prediction (Chang etal., 2013).SVMs are a frequentist method: they aim to findan approximation to the exact latent function thatexplains the data.
This is in contrast to Bayesiansettings, which define a prior distribution on thisfunction and perform inference by marginalizingover all its possible values.
Although there is somediscussion about which approach is better (Mur-phy, 2012, Sec.
6.6.4), Bayesian methods offermany useful theoretical properties.
In fact, theyhave been used before in NLP, especially in gram-mar induction (Cohn et al, 2010) and word seg-mentation (Goldwater et al, 2009).
However, onlyvery recently kernel methods have been applied inNLP using the Bayesian approach.Gaussian Processes (GPs) are the Bayesiancounterpart of kernel methods and are widely con-sidered the state-of-the-art for inference on func-tions (Hensman et al, 2013).
They have a numberof advantages which are very useful in NLP:?
Kernels in general can be combined and pa-rameterized in many ways.
This parame-terization lead to the problem of model se-lection, which is difficult in frequentist ap-proaches (mainly based on cross validation).The Bayesian formulation of GPs let themdeal with model selection in a much moremore efficient and elegant way: by maximiz-ing the likelihood on the training data.
Thisopens the door for the use of heavily param-eterized kernel combinations, like multi-taskkernels for example.?
Being a probabilistic framework, they areable to naturally encode uncertainty in thepredictions, which can be propagated if thetask is part of a larger system pipeline.Besides these properties, GPs have also beenapplied sucessfully in many Machine Learning1tasks.
Examples include Robotics (Ko et al,2007), Bioinformatics (Chu et al, 2005; Polaj-nar et al, 2011), Geolocation (Schwaighofer etal., 2004) and Computer Vision (Sinz et al, 2004;Riihim?aki et al, 2013).
In NLP, GPs have beenused only very recently and focused on regressiontasks (Cohn and Specia, 2013; Preotiuc-Pietro andCohn, 2013).
In this work, we propose to combineGPs with recent kernel developments to advancethe state-of-the-art in a number of NLP tasks.2 Gaussian ProcessesIn this Section, we follow closely the definition ofRasmussen and Williams (2006).
Consider a ma-chine learning setting, where we have a datasetX = {(x1, y1), (x2, y2), .
.
.
, (xn, yn)} and ourgoal is to infer the underlying function f(x) thatbest explains the data.
A GP model assumes aprior stochastic process over this function:f(x) ?
GP(?
(x), k(x,x?
)), (1)where ?
(x) is the mean function, which is usu-ally the 0 constant, and k(x,x?)
is the kernel orcovariance function.
In this sense, they are analo-gous to Gaussian distributions, which are also de-fined in terms of a mean and a variance values, orin the case of multivariate Gaussians, a mean vec-tor and a covariance matrix.
In fact, a GP can beinterpreted as an infinite-dimensional multivariateGaussian distribution.The full model uses Bayes?
rule to define a pos-terior over f , combining the GP prior with the datalikelihood:p(f |X,y) =p(y|X, f)p(f)p(y|X), (2)where X and y are the training inputs and outputs,respectively.
The posterior is then used to predictthe label for an unseen input x?by marginalizingover all possible latent functions:p(y?|x?,X,y) =?fp(y?|x?,X, f)p(f |X,y)df.
(3)where y?is the predicted output.
The choice ofthe likelihood distribution depends if the task is re-gression, classification or other prediction setting.2.1 GP RegressionIn a regression setting, we assume that the outputvalues are equal to noisy latent function evalua-tions, i.e., yi= f(xi) + ?, where ?
?
N (0, ?2n) isthe added white noise.
We also usually assume aGaussian likelihood, because this able us to solvethe integral in Equation 3 analytically.
Substitut-ing the likelihood and the prior in both Equations2 and 3 and manipulating the result, we computethe posterior also as a Gaussian distribution:y??
N (k?
(K + ?nI)?1yT, (4)k(x?,x?)?
kT?
(K + ?nI)?1k?
).where K is the Gram matrix corre-sponding to the training inputs andk?= [?x1,x?
?, ?x2,x?
?, .
.
.
, ?xn,x??]
is thevector of kernel evaluations between the test inputand each training input.2.2 GP ClassificationConsider binary classification using ?1 and +1as labels1.
The model in this case use the ac-tual, noiseless latent function evaluations f and?squash?
them through the [?1,+1] interval to ob-tain the outputs.
The posterior over the outputs isthen defined as:p(y?= +1|x?,X,y) =?f??(f?
)p(f?|x?,X,y)df?, (5)where ?(f?)
is a squashing function.
Two com-mon choices are the logistic function and the pro-bit function.
The distribution over the latent valuesf?is obtained by integrating out the latent func-tion:p(f?|x?,X,y) =?fp(f?|x?,X, f)p(f |X,y)df.
(6)Because the likelihood is not Gaussian, the re-sulting posterior integral is not analytically avail-able anymore.
The most common solution to thisproblem is to approximate the posterior p(f |X,y)with a Gaussian q(f |X,y).
Two such approxi-mation algorithms are the Laplace approximation(Williams and Barber, 1998) and the ExpectationPropagation (Minka, 2001).
Another option is touse Markov Chain Monte Carlo sampling methodson the true posterior (Neal, 1998).2.3 Hyperparameter OptimizationThe GP prior used in the models described aboveusually have a number of hyperparameters.
The1Extensions to multi-class settings are possible.2most important ones are the kernel ones but theycan also include others like the white noise vari-ance ?2nused in regression.
A key property of GPsis their ability to easily fit these hyperparametersto the data by maximizing the marginal likelihood:p(y|X,?)
=?fp(y|X,?, f)p(f), (7)where ?
represents the full set of hyperparameters(which was suppressed from all conditionals untilnow for brevity).
Optimization involves derivingthe gradients of the marginal log likelihood w.r.t.the hyperparameters and then employ a gradientascent procedure.
Gradients can be found ana-litically for regression and by approximations forclassification, using methods similar to the onesused for prediction.2.4 Sparse Approximations for GPsSVMs are naturally sparse models which use onlya subset of data points to make predictions.
Thisresults in important speed-ups which is one ofthe reasons for their success.
On the other hand,canonical GPs are not sparse, making use of alldata points.
This results in a training complexityof O(n3) (due to the Gram matrix inversion) andO(n) for predictions.Sparse GPs tackle this problem by approximat-ing the Gram matrix using only a subset of m in-ducing inputs.
Without loss of generalization, con-sider these m inputs as the first ones in the train-ing data and (n ?
m) the remaining ones.
Thenwe can partition the Gram matrix in the followingway (Rasmussen and Williams, 2006, Sec.
8.1):K =[KmmKm(n?m)K(n?m)mK(n?m)(n?m)],where each block corresponds to a matrix of ker-nel evaluations between two sets of inputs.
Forbrevity, we will refer Km(n?m)as Kmnand itstranspose as Knm.
The block structure of K formsthe base of the so-called Nystr?om approximation:?K = KnmK?1mmKmn.
(8)which result in the following predictive posterior:y??
N (kTm?
?G?1Kmny, (9)k(x?,x?)?
kTm?K?1mmkm?+?2nkTm??G?1km?
),where?G = ?2nKmm+ KmnKnmand km?is thevector of kernel evaluations between test input x?and the m inducing inputs.
The resulting com-plexities for training and prediction are O(m2n)and O(m), respectively.The remaining question is how to choose the in-ducing inputs.
Seeger et al (2003) use an iterativemethod that starts with some random data pointsand adds new ones based on a greedy procedure,in an active learning fashion.
Snelson and Ghahra-mani (2006) use a different approach: it defines afixed m a priori and use pseudo-inputs which canbe optimized as regular hyperparameters.
Later,Titsias (2009) also used pseudo-inputs but per-form optimization using a variational method in-stead.
Recently, Hensman et al (2013) modifiedthis method to allow Stochastic Variational Infer-ence (Hoffman et al, 2013), which reduces thetraining complexity to O(m3).3 KernelsThe core of a GP model is the kernel function.
Akernel k(x,x?)
is a symmetric and positive semi-definite function which returns a similarity scorebetween two inputs in some feature space (Shawe-Taylor and Cristianini, 2004).
Probably the mostused kernel in general is the Radial Basis Func-tion (RBF) kernel, which is defined over two real-valued vectors.
Our focus in this work is on twodifferent types of kernels which can be applied forNLP settings and allow richer parameterizations.3.1 Kernels for Discrete StructuresIn NLP, discrete structures like strings or trees arecommon in training data.
To apply a vectorialkernel like the RBF, one can always extract real-valued features from these structures.
However,kernels can be defined directly on these structures,potentially reducing the need for feature engineer-ing.
The string and tree kernels we define hereare based on the theory of Convolution kernelsof Haussler (1999), which calculate the similar-ity between two structures based on the numberof substructures they have in common.
Other ap-proaches include random walk kernels (G?artner etal., 2003; Vishwanathan et al, 2010) and Fisherkernels (Jaakkola et al, 2000).3.1.1 String KernelsConsider a function ?s(x) that counts the numberof times a substring s appears in x.
A string kernel3is defined as:k(x, x?)
=?s???ws?s(x)?s(x?
), (10)where wsis a non-negative weight for substring sand ?
?is the set of all possible strings over thesymbol alphabet ?.Usually in NLP, each word is considered a sym-bol, although some previous work also consideredcharacters as symbols (Lodhi et al, 2002).
If werestrict s to be only single words we end up hav-ing a bag-of-words (BOW) representation.
Allow-ing longer substrings lead us to the Word SequenceKernels of Cancedda et al (2003), which also al-low gaps between words.One extension of these kernels is to allow softmatching between substrings.
This is done bydefining a similarity matrix S, which encode sym-bol similarities.
This matrix can be defined by ex-ternal resources, like WordNet, or be inferred fromdata using Latent Semantic Analysis (Deerwesteret al, 1990) for example.3.1.2 Tree KernelsCollins and Duffy (2001) first introduced TreeKernels, which measure the similarity betweentwo trees by counting the number of fragmentsthey share, in a very similar way to string kernels.Consider two trees T1and T2.
We define the setof nodes in these two trees as N1and N2respec-tively.
Consider also F the full set of possible treefragments (similar to ?
?in the case of strings).
Wedefine Ii(n) as an indicator function that returns 1if fragment fi?
F has root n and 0 otherwise.
ATree Kernel can then be defined as:k(T1, T2) =?n1?N1?n2?N2?
(n1, n2),where:?
(n1, n2) =|F|?i=1?size(i)Ii(n1)Ii(n2).Here, 0 < ?
< 1 is a decay factor that penalizescontributions from larger fragments cf.
smallerones.Again, we can put restrictions on the type oftree fragment considered for comparison.
Collinsand Duffy (2001) defined Subtree kernels, whichconsidered only subtrees as fragments, and SubsetTree Kernels (SSTK), where fragments can havenon-terminals as leaves.
Later, Moschitti (2006)introduced the Partial Tree Kernels (PTK), by al-lowing fragments with partial rule expansions.Tree kernels were used in a variety of tasks, in-cluding Relation Extraction (Bloehdorn and Mos-chitti, 2007; Plank and Moschitti, 2013), Ques-tion Classification (Moschitti, 2006; Croce et al,2011) and Quality Estimation (Hardmeier, 2011;Hardmeier et al, 2012).
Furthermore, soft match-ing approaches were also used by Bloehdorn andMoschitti (2007) and Croce et al (2011).3.2 Multi-task KernelsKernels can also be extended to deal with set-tings where we want to predict a vector of val-ues (?Alvarez et al, 2012).
These settings are use-ful in multi-task and domain adaptation problems.Kernels for vector-valued functions are known ascoregionalization kernels in the literature.
Herewe are going to refer them as multi-task kernels.One of the simplest ways to define a kernel fora multi-task setting is the Intrinsic Coregionaliza-tion Model (ICM):K(x,x?)
= B?
k(x,x?
).where ?
denotes the Kronecker product and Bis the coregionalization matrix, encoding task co-variances.
We also denote the resulting kernelfunction as K(x,x?)
to stress out that its result isnow a matrix instead of a scalar.Cohn and Specia (2013) used the ICM to modelannotator bias in Quality Estimation datasets.They parameterize B in a number of differ-ent ways and get significant improvements oversingle-task baselines, especially in post-editingtime prediction.
They also point out that the wellknown EasyAdapt method (Daum?e III, 2007) fordomain adaptation can be modeled by the ICM us-ing B = 1+I, i.e., a coregionalization matrix withits diagonal elements equal to 2 and remaining el-ements equal to 1.An extension of the ICM is the Linear Model ofCoregionalization (LMC), which assume a sum ofkernels with different coregionalization matrices:K(x,x?)
=?kp?PBp?
kp(x,x?
).where P is the set of different kernels employed.
?Alvarez et al (2012) argue that the LMC is muchmore flexible than the ICM because the latter as-sumes that each kernel contributes equally to thetask covariances.44 Planned WorkOur goal in this proposal is to employ GPs andthe kernels introduced in Section 3 to advancethe state-of-the-art in regression and classificationNLP tasks.
It would be unfeasible though, at leastfor a single thesis, to address all possible tasks sowe are going to focus on three of them where ker-nel methods were already successfully applied.4.1 Quality EstimationThe purpose of Machine Translation Quality Esti-mation is to provide a quality prediction for new,unseen machine translated texts, without relyingon reference translations (Blatz et al, 2004; Bojaret al, 2013).
A common use of quality predictionsis the decision between post-editing a given ma-chine translated sentence and translating its sourcefrom scratch.GP regression models were recently success-fully employed for post-editing time (Cohn andSpecia, 2013) and HTER2prediction (Beck et al,2013).
Both used RBF kernels as the covariancefunction so a natural extension is to apply thestructured kernels of Section 3.1.
This was alreadybeen done with tree kernels by Hardmeier (2011)in the context of SVMs.Multi-task kernels can also be applied for Qual-ity Estimation in several ways.
The model usedby Cohn and Specia (2013) for modelling annota-tor bias can be further extended for settings withdozens or even hundreds of annotators.
This is acommon setting in crowdsourcing platforms likeAmazon?s Mechanical Turk3.Another plan is to use multi-task kernels tocombine different datasets.
Quality annotation isusually expensive, requiring post-editing or sub-jective scoring.
Possibilities include combiningdatasets from different language pairs or differentmachine translation systems.
Available datasetsinclude those used in the WMT12 and WMT13QE shared tasks (Callison-burch et al, 2012; Bo-jar et al, 2013) and others (Specia et al, 2009;Specia, 2011; Koponen et al, 2012).4.2 Question ClassificationA Question Classifier is a module that aims to re-strict the answer hypotheses generated by a Ques-tion Answering system by applying a label to theinput question (Li and Roth, 2002; Li and Roth,2Human Translation Error Rate (Snover et al, 2006).3www.mturk.com2005).
This task can be seen as an instance of textclassification, where the inputs are usually com-posed of only one sentence.Much of previous work in Question Classifica-tion largely used SVMs combined with structuredkernels.
Zhang and Lee (2003) compares StringKernels based on BOW and n-gram representa-tions with the Subset Tree Kernel on constituenttrees.
Moschitti (2006) show improved results byusing the Partial Tree Kernel and dependency treesinstead of constituency ones.
Bloehdorn and Mos-chitti (2007) combines a SSTK with different softmatching approaches to encode lexical similarityon tree leaves.
The same soft matching idea isused by Croce et al (2011), but applied to PTKsinstead and permitting soft matches between anynodes in each tree (which is sensible when usingkernels on dependency trees).Our work proposes to address this task by em-ploying tree kernels and GPs.
Unlike Quality Esti-mation, this is a classification setting and our pur-pose is to find if this combination can also improvethe state-of-the-art for tasks of this kind.
We willuse the TREC dataset provided by Li and Roth(2002), which assigns 6000 questions with both acoarse and a fine-grained label.4.3 Multi-domain Sentiment AnalysisSentiment Analysis is defined as ?the computa-tional treatment of opinion, sentiment and subjec-tivity in text?
(Pang and Lee, 2008).
In this pro-posal, we focus on the specific task of polarity de-tection, where the goal is to label a text as hav-ing positive or negative sentiment.
State-of-the-artmethods for this task use SVMs as the learning al-gorithm and vary between the feature sets used.Polarity predictions can be heavily biased onthe text domain.
Consider the example showedby Turney (2002): the word ?unpredictable?
usu-ally has a positive meaning in a movie review buta negative one when applied to an automotive re-view (in a phrase like ?unpredictable steering?, forinstance).
One of the first methods to tackle thisissue is the Structural Correspondence Learningof Blitzer et al (2007).
Their method uses pivotwords shared between domains to find correspon-dencies in words that are not shared.A previous work that used structured kernels inSentiment Analysis is the approach of Wu et al(2009).
Their method uses tree kernels on phrasedependency trees and outperforms bag-of-words5and word dependency approaches.
They also showgood results in cross-domain experiments.We propose to apply GPs with a combinationof structured and multi-task kernels for this task.The results showed by Wu et al (2009) suggestthat tree kernels on dependency trees are a goodapproach but we also plan to employ string ker-nels on this task.
This is because string kernelshave demonstrated promising results for text cate-gorization in past work.
Also, considering modelselection is easily dealt by GPs, we can combineall those kernels in complex and heavily param-eterized ways, an unfeasible setting for SVMs.We will use the Multi-Domain Sentiment Dataset(Blitzer et al, 2007), composed of Amazon prod-uct reviews in different categories.4.4 Research DirectionsIn Section 2.3 we saw how the Bayesian formu-lation of GPs let us do model selection by maxi-mizing the marginal likelihood.
In fact, one of ourmain research directions in this proposal revolvesaround this crucial point: because we can easily fithyperparameters to the data we have much morefreedom to use richer kernel parameterizations andkernel combinations.
Multi-task kernels are oneexample where we usually have a large number ofhyperparameters because we need to fit all the el-ements of the coregionalization matrix.
This num-ber can get even larger if we have a LMC model,with multiple coregionalization matrices.
Struc-tured kernels can also be redefined in a richer way:tree kernels between constituency trees could havemultiple decay hyperparameters, one for each POStag.
A more extreme example would be to treatall weights in a string kernel as hyperparameters.Thus, we plan to investigate these possibilities inthe context of the three tasks detailed before.As another research direction we also want toaddress the issue of scalability.
Although GPs al-ready showed promising results they can be slowwhen compared to other well established meth-ods like SVM.
Fortunately there has been a lotof advancements in the field of sparse GPs in thelast years and we plan to employ them in ourwork.
A key question is how to combine sparseGPs with the structured kernels we presented be-fore.
Although it is perfectly possible to select in-ducing points using greedy methods, it would bemuch more interesting to use the pseudo-inputsapproach.
However, it is not clear how to do thatin conjunction with non-vectorial inputs, like theones we plan to use in structured kernels, and thisis a key direction that we also plan to investigate.4.5 GP ToolkitsAvailable toolkits for GP modelling includeGPML4(Rasmussen and Williams, 2006) and GP-stuff5(Vanhatalo et al, 2013), which are writtenin Matlab.
Our experiments will mainly use GPy6,an open source toolkit written in Python.
It imple-ments models for regression and binary classifi-cation, including sparse approximations and manyvectorial kernels.
We plan to contribute to GPyby implementing the structured kernels of Section3.1, effectively extending it to a GP framework forNLP.5 Conclusions and Future WorkIn this work we showed a proposal for advancingthe state-of-the-art in a number of NLP tasks bycombining Gaussian Process with structured andmulti-task kernels.
Our hypothesis is that highlyparameterized kernel combinations allied with thefitting methods provided by GPs will result in bet-ter models for these tasks.
We also detailed thefuture plans for experiments, including availabledatasets and toolkits.Further research directions that can be exploredby this proposal include the use of GPs in differentlearning settings.
Models for ordinal regression(Chu and Ghahramani, 2005) and structured pre-diction (Altun et al, 2004; Brati`eres et al, 2013)were already proposed in the GP literature and anatural extension is to apply these models for theircorresponding NLP tasks.
Another extension is toemploy other kinds of kernels.
The literature onthat subject is quite vast, with many approachesshowing promising results.AcknowledgementsThis work was supported by funding fromCNPq/Brazil (No.
237999/2012-9) and from theEU FP7-ICT QTLaunchPad project (No.
296347).The author would also like to thank Yahoo for thefinancial support and the anonymous reviewers fortheir excellent comments.4www.gaussianprocess.org/gpml/code/matlab5becs.aalto.fi/en/research/bayes/gpstuff6github.com/SheffieldML/GPy6ReferencesYasemin Altun, Thomas Hofmann, and Alexander J.Smola.
2004.
Gaussian Process Classification forSegmenting and Annotating Sequences.
In Proceed-ings of ICML, page 8, New York, New York, USA.ACM Press.Mauricio A.?Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2012.
Kernels for Vector-Valued Func-tions: a Review.
Foundations and Trends in Ma-chine Learning, pages 1?37.Daniel Beck, Kashif Shah, Trevor Cohn, and LuciaSpecia.
2013.
SHEF-Lite : When Less is More forTranslation Quality Estimation.
In Proceedings ofWMT13, pages 337?342.John Blatz, Erin Fitzgerald, and George Foster.
2004.Confidence estimation for machine translation.
InProceedings of the 20th Conference on Computa-tional Linguistics, pages 315?321.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
In Proceedings of ACL.Stephan Bloehdorn and Alessandro Moschitti.
2007.Exploiting Structure and Semantics for ExpressiveText Kernels.
In Proceedings of CIKM.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proceedingsof WMT13, pages 1?44.S?ebastien Brati`eres, Novi Quadrianto, and ZoubinGhahramani.
2013.
Bayesian Structured Predictionusing Gaussian Processes.
arXiv:1307.3846, pages1?17.Chris Callison-burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of 7th Workshopon Statistical Machine Translation.Nicola Cancedda, Eric Gaussier, Cyril Goutte, andJean-Michel Renders.
2003.
Word-Sequence Ker-nels.
The Journal of Machine Learning Research,3:1059?1082.Kai-Wei Chang, Vivek Srikumar, and Dan Roth.
2013.Multi-core Structural SVM Training.
In Proceed-ings of ECML-PHDD.Wei Chu and Zoubin Ghahramani.
2005.
GaussianProcesses for Ordinal Regression.
Journal of Ma-chine Learning Research, 6:1019?1041.Wei Chu, Zoubin Ghahramani, Francesco Falciani, andDavid L Wild.
2005.
Biomarker discovery in mi-croarray gene expression data with Gaussian pro-cesses.
Bioinformatics, 21(16):3385?93, August.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of ACL.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
TheJournal of Machine Learning, 11:3053?3096.Michael Collins and Nigel Duffy.
2001.
ConvolutionKernels for Natural Language.
In Advances in Neu-ral Information Processing Systems.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured Lexical Similarity via Con-volution Kernels on Dependency Trees.
In Proc.
ofEMNLP.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of ACL.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal ofthe American Society For Information Science, 41.Thomas G?artner, Peter Flach, and Stefan Wrobel.2003.
On Graph Kernels: Hardness Results and Ef-ficient Alternatives.
LNAI, 2777:129?143.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54, July.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Tree Kernels for Machine Transla-tion Quality Estimation.
In Proceedings of WMT12,number 2011, pages 109?113.Christian Hardmeier.
2011.
Improving MachineTranslation Quality Prediction with Syntactic TreeKernels.
In Proceedings of EAMT, number May.David Haussler.
1999.
Convolution Kernels on Dis-crete Structures.
Technical report.James Hensman, Nicol`o Fusi, and Neil D. Lawrence.2013.
Gaussian Processes for Big Data.
In Pro-ceedings of UAI.Matt Hoffman, David M. Blei, Chong Wang, and JohnPaisley.
2013.
Stochastic Variational Inference.
TheJournal of Machine Learning Research.Tommi Jaakkola, Mark Diekhans, and David Haussler.2000.
A discriminative framework for detecting re-mote protein homologies.
Journal of ComputationalBiology, 7:95?114.Richard Johansson and Alessandro Moschitti.
2013.Relational Features in Fine-Grained Opinion Analy-sis.
Computational Linguistics, 39(3):473?509.7Jonathan Ko, Daniel J. Klein, Dieter Fox, and DirkHaehnel.
2007.
Gaussian Processes and Reinforce-ment Learning for Identification and Control of anAutonomous Blimp.
In Proceedings of IEEE Inter-national Conference on Robotics and Automation,pages 742?747.
Ieee, April.Maarit Koponen, Wilker Aziz, Luciana Ramos, andLucia Specia.
2012.
Post-editing time as a measureof cognitive effort.
In Proceedings of WPTP.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In Proceedings of COLING, volume 1, pages1?7.Xin Li and Dan Roth.
2005.
Learning Question Clas-sifiers: the Role of Semantic Information.
NaturalLanguage Engineering, 1(1).Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
TextClassification using String Kernels.
The Journal ofMachine Learning Research, 2:419?444.Thomas P. Minka.
2001.
A family of algorithms forapproximate Bayesian inference.
Ph.D. thesis.Alessandro Moschitti.
2006.
Efficient ConvolutionKernels for Dependency and Constituent SyntacticTrees.
In Proceedings of ECML.Kevin P. Murphy.
2012.
Machine Learning: a Proba-bilistic Perspective.Radford M. Neal.
1998.
Regression and ClassificationUsing Gaussian Process Priors.
Bayesian Statistics,6.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Ver?onica P?erez-Rosas and Rada Mihalcea.
2013.
Sen-timent Analysis of Online Spoken Reviews.
In Pro-ceedings of Interspeech.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding Semantic Similarity in Tree Kernels for Do-main Adaptation of Relation Extraction.
In Pro-ceedings of ACL, pages 1498?1507.Tamara Polajnar, Simon Rogers, and Mark Girolami.2011.
Protein interaction detection in sentences viaGaussian Processes: a preliminary evaluation.
Inter-national Journal of Data Mining and Bioinformat-ics, 5(1):52?72, January.Daniel Preotiuc-Pietro and Trevor Cohn.
2013.
A tem-poral model of text periodicities using Gaussian Pro-cesses.
In Proceedings of EMNLP.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian processes for machinelearning, volume 1.
MIT Press Cambridge.Jaakko Riihim?aki, Pasi Jyl?anki, and Aki Vehtari.
2013.Nested Expectation Propagation for Gaussian Pro-cess Classification with a Multinomial Probit Like-lihood.
Journal of Machine Learning Research,14:75?109.Anton Schwaighofer, Marian Grigoras, Volker Tresp,and Clemens Hoffmann.
2004.
GPPS: A GaussianProcess Positioning System for Cellular Networks.In Proceedings of NIPS.Matthias Seeger, Christopher K. I. Williams, andNeil D. Lawrence.
2003.
Fast Forward Selectionto Speed Up Sparse Gaussian Process Regression.In Proceedings of AISTATS.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel methods for pattern analysis.
Cambridge.Fabian H. Sinz, Joaquin Qui?nonero Candela,G?okhan H. Bak?r, Carl E. Rasmussen, andMatthias O. Franz.
2004.
Learning Depth fromStereo.
Pattern Recognition, pages 1?8.Edward Snelson and Zoubin Ghahramani.
2006.Sparse Gaussian Processes using Pseudo-inputs.
InProceedings of NIPS.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA.Lucia Specia and Atefeh Farzindar.
2010.
Estimatingmachine translation post-editing effort with HTER.In Proceedings of AMTA Workshop Bringing MT tothe User: MT Research and the Translation Indus-try.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proceedings of EAMT, pages 28?35.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of EAMT.Michalis K. Titsias.
2009.
Variational Learning of In-ducing Variables in Sparse Gaussian Processes.
InProceedings of AISTATS, volume 5, pages 567?574.Peter D. Turney.
2002.
Thumbs Up or ThumbsDown?
: Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
In Proceedings ofACL, number July, pages 417?424.Jarno Vanhatalo, Jaakko Riihim?aki, Jouni Hartikainen,Pasi Jyl?anki, Ville Tolvanen, and Aki Vehtari.
2013.GPstuff: Bayesian Modeling with Gaussian Pro-cesses.
The Journal of Machine Learning Research,14:1175?1179.S.
V. N. Vishwanathan, Nicol N. Schraudolph, RisiKondor, and Karsten M. Borgwardt.
2010.
GraphKernels.
Journal of Machine Learning Research,11:1201?1242.8Christopher K. I. Williams and David Barber.
1998.Bayesian Classification with Gaussian Processes.IEEE Transactions on Pattern Analysis and MachineIntelligence, 20(12):1342?1351.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2009.
Phrase Dependency Parsing for Opinion Min-ing.
In Proceedings of EMNLP, pages 1533?1541.Dell Zhang and Wee Sun Lee.
2003.
Question classi-fication using support vector machines.
In Proceed-ings of SIGIR, New York, New York, USA.
ACMPress.9
