Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 309?312,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving Blog Polarity Classification via Topic Analysis and AdaptiveMethodsFeifan LiuUniversity of Wisconsin, Milwaukeeliuf@uwm.eduDong Wang, Bin Li, Yang LiuThe University of Texas at Dallasdongwang,yangl@hlt.utdallas.eduAbstractIn this paper we examine different linguistic featuresfor sentimental polarity classification, and perform acomparative study on this task between blog and re-view data.
We found that results on blog are muchworse than reviews and investigated two methodsto improve the performance on blogs.
First we ex-plored information retrieval based topic analysis toextract relevant sentences to the given topics for po-larity classification.
Second, we adopted an adaptivemethod where we train classifiers from review dataand incorporate their hypothesis as features.
Bothmethods yielded performance gain for polarity clas-sification on blog data.1 IntroductionSentimental analysis is a task of text categorization thatfocuses on recognizing and classifying opinionated texttowards a given subject.
Different levels of sentimentalanalysis has been performed in prior work, from binaryclasses to more fine grained categories.
Pang et al (2002)defined this task as a binary classification task and ap-plied it to movie reviews.
More sentiment classes, suchas document objectivity and subjectivity as well as dif-ferent rating scales on the subjectivity, have also beentaken into consideration (Pang and Lee, 2005; Boiy etal., 2007).
In terms of granularity, this task has beeninvestigated from building word level sentiment lexicon(Turney, 2002; Moilanen and Pulman, 2008) to detectingphrase-level (Wilson et al, 2005; Agarwal et al, 2009)and sentence-level (Riloff and Wiebe, 2003; Hu and Liu,2004) sentiment orientation.
However, most previouswork has mainly focused on reviews (Pang et al, 2002;Hu and Liu, 2004), news resources (Wilson et al, 2005),and multi-domain adaptation (Blitzer et al, 2007; Man-sour et al, 2008).
Sentiment analysis on blogs (Chesleyet al, 2005; Kim et al, 2009) is still at its early stage.In this paper we investigate binary polarity classifica-tion (positive vs. negative).
We evaluate the genre effectbetween blogs and review data and show the difference offeature effectiveness.
We demonstrate improved polarityclassification performance in blogs using two methods:(a) integrating topic relevance analysis to perform topicspecific polarity classification; (b) adopting an adaptivemethod by incorporating multiple classifiers?
hypothesesfrom different review domains as features.
Our manualanalysis also points out some challenges and directionsfor further study in blog domain.2 Features for Polarity ClassificationFor the binary polarity classification task, we use a super-vised learning framework to determine whether a docu-ment is positive or negative.
We used a subjective lex-icon, containing 2304 positive words and 4145 negativewords respectively, based on (Wilson et al, 2005).
Thefeatures we explored are listed below.
(i) Lexical features (LF)We use the bag of words for the lexical features as theyhave been shown very useful in previous work.
(ii) Polarized lexical features (PL)We tagged each sentiment word in our data set with itspolarity tag based on the sentiment lexicon (?POS?
forpositive, and ?NEG?
for negative), along with its part-of-speech tag.
For example, in the sentence ?It is good,and I like it?, ?good?
is tagged as ?POS/ADJ?, ?like?
istagged as ?POS/VRB?.
Then we encode the number ofthe polarized tags in a document as features.
(iii) Polarized bigram features (PB)Contextual information around the polarized wordscan be useful for sentimental analysis.
A word mayflip the polarity of its neighboring sentiment words eventhough this word itself is not necessarily a negative word.For example, in ?Given her sudden celebrity with thoseon the left...?
(a sentence in a political blog), ?sudden?preceding ?celebrity?
implies the author?s negative atti-tude towards ?her?.
We combine the sentiment word?spolarized tag and its following and preceding word orits part-of-speech to comprise different bigram featuresto represent this kind of contextual information.
For ex-309ample, in ?I recommend this.
?, ?recommend?
is a posi-tive verb, denoted as ?POS/VRB?, and the bigram fea-tures including this tag and its previous word ?I?
are?I POS/VRB?
and ?pron POS/VRB?.
(iv) Transition word features (T)Transition words, such as ?although?, ?even though?,serve as function words that may change the literal opin-ion polarity in the current sentence.
This information hasnot been widely explored for sentiment analysis.
In thisstudy, we compiled a transition word list containing 31words.
We use the co-occurring feature between a transi-tion word and its nearby content words (noun, verb, ad-jective and adverb) or polarized tags of sentiment wordswithin the same sentence, but not spanning over othertransition words.
For example, in ?Although it is good?,we use features like ?although is?,?although good?
and?although POS/ADJ?, where ?POS/ADJ?
is the PL fea-ture for word ?good?.3 Feature Effectiveness on Blogs andReviewsThe blog data we used is from the TREC Blog Track eval-uation in 2006 and 2007.
The annotation was conductedfor the 100 topics used in the evaluation (blogs are rele-vant to a given topic and also opinionated).
We use 6,896positive and 5,300 negative blogs.
For the review data,we combined multiple review data sets from (Pang et al,2002; Blitzer et al, 2007) together.
It contains reviewsfrom movies and four product domains (kitchen, elec-tronics, books, and dvd), each of which has 1000 neg-ative and 1000 positive samples.
For the data withoutsentence information (e.g., blog data, some review data),we generated sentences using the maximum entropy sen-tence boundary detection tool1.
We used TnT tagger toobtain the part-of-speech tags for these data sets.For classification, we use the maximum entropy clas-sifier2 with a Gaussian prior of 1 and 100 iterations inmodel training.
For all the experiments below, we usea 10-fold cross validation setup and report the averageclassification accuracy.
Table 1 shows classification re-sults using various feature sets on blogs and review data.We keep the lexical feature (LF) as a base feature, andinvestigate the effectiveness of adding more different fea-tures.
We used Wilcox signed test for statistical signifi-cance test.
Symbols ???
and ???
in the table indicate thesignificant level of 0.05 and 0.1 respectively, compared tothe baseline performance using LF feature setup.For the review domain, most of the feature sets can sig-nificantly improve the classification performance over thebaseline of using ?LF?
features.
?PB?
features yieldedmore significant improvement than ?PL?
or ?T?
featurecategories.
Combining ?PL?
and ?T?
features resulted insome slight further improvement, achieving the best ac-1http://stp.ling.uu.se/?gustav/java/classes/MXTERMINATOR.html2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.htmlFeature Set Blogs ReviewsLF 72.07 81.67LF+PL 70.93 81.93LF+PB 72.44 83.62?LF+T 72.17 81.76LF+PL+PB 70.81 83.61?LF+PL+T 72.74 82.13?LF+PB+T 72.29 83.73?LF+PL+PB+T 71.85 83.94?Table 1: Polarity classification results (accuracy in %) usingdifferent features for blogs and reviews.curacy of 83.94%.
We notice that incorporating our pro-posed transition feature (T) always achieves some gain ondifferent feature settings, suggesting that those transitionfeatures are useful for sentimental analysis on reviews.From Table 1, we can see that overall the performanceon blogs is worse than on the review data.
We hypoth-esize this may be due to the large variation in terms ofcontents and styles in blogs.
Regarding the feature ef-fectiveness, we also observe some differences betweenblogs and reviews.
Adding the polarized bigram featureand transition feature (PB and T) individually can yieldsome improvement; however, adding both of them didnot result in any further improvement ?
performance de-grades compared to LF+PB.
Interestingly, although ?PL?feature alone does not seem to help, by adding ?PL?
and?T?
together, the performance achieved the best accuracyof 72.74%.
We also found that adding all the featurestogether hurts the performance, suggesting that differentfeatures interact with each other and some do not com-bine well (e.g., PB and T features).
In addition, all theimprovements here are not statistically significant.Note that for the blog data, we randomly split them forthe cross validation experiments regardless of the queries.In order to better understand whether the poor results onblog data is due to the effect of different queries, we per-formed another experiment where for each query, we ran-domly divided the corresponding blogs into training andtest splits.
Only 66 queries were kept for this experi-ments ?
we did not include those queries that have fewerthan 10 relevant blogs.
The results for the query balancedsplit on blogs are shown in Figure 1.
We also include re-sults for the five individual review data sets in order to seethe topic effect.
We present results using four represen-tative feature sets chosen according to Table 1.
For thereview data, we notice some difference across differentdata sets, suggesting their inherent difference in terms oftask difficulty.
We observe slight performance increasefor some feature sets using the query balanced setup forblog data, but overall it is still much worse than the reviewdata.
This shows that the query unbalanced training/testsplit does not explain the performance gap between blogsand reviews.
This is consistent with (Zhang et al, 2007)that found that a query-independent classifier performseven better than query-dependent one.
We expect that the310query unbalanced setup is more realistic, therefore, in thefollowing experiments, we continue with this setup.717375777981838587Accuracy(%)blog_datablog_data_query_balancedreview_booksreview_dvdreview_kitchenreview_moviereview_elecFigure 1: Polarity classification results on query balanced blogdata and five individual review data sets.4 Improving Blog Polarity ClassificationTo improve the performance of polarity classification onblogs, we propose two methods: (a) extract only topic-relevant segments from blogs for sentiment analysis; (b)apply adaptive methods to leverage review data.4.1 Using topic-relevant blog contextGenerally a review is written towards one product or onekind of service, but a blog may cover several topics withpossibly different opinions towards each topic.
The blogdata we used is annotated based on some specific topicsin the TREC Blog Track evaluation.
Take topic 870 inthe data as an example, ?Find opinions on alleged useof steroids by baseball player Barry?.
There is one blogthat talks about 5 different baseball players in issues ofusing steroids.
Since the reference opinion tag of a blogis determined by polarity towards the given query topic, itmight be confusing for the classifier if we use the wholeblog to derive features.
Recently topic analysis has beenused for polarity classification (Zhang et al, 2007; Titovand McDonald, 2008; Wiegand and Klakow, 2009).
Wetake a different approach in this study.In order to obtain a topic-relevant context, we retrievedthe top 10 relevant sentences corresponding to the giventopic using the Lemur toolkit3.
Then we used these sen-tences and their immediate previous and following sen-tences for feature extraction in the same way as whatwe did on the whole blog.
In addition to using all thewords in the relevant context, we also investigated usingonly content words since those are more topic indicativethan function words.
We extracted content words (nouns,verbs, adjectives and adverbs) from each blog in theiroriginal order and apply the same feature extraction pro-cess as for using all the words.3http://www.lemurproject.org/lemur/Table 2 shows the blog polarity classification resultsusing the whole blog vs. relevant context composed ofall the words or only content words.
For the significancetest, the comparison was done for using relevant contextwith all the words vs. using the whole blog; and us-ing content words only vs. using all the words in rele-vant context.
Each comparison was with respect to thesame feature setup.
We observe improved polarity classi-fication performance when using sentence retrieval basedtopic analysis to extract relevant context.
Using all thewords in the topic relevant context, all the improvementscompared to using the original entire blog are statisticallysignificant at the level of 0.01.
We also notice that un-like on the entire blog document, the ?PL?
features con-tribute positively when combined with ?LF?.
All the fea-ture settings with ?PL?
perform very well.
The best ac-curacy of 75.32% is achieved using feature combinationof ?LF+PL?
or ?LF+PL+T?.
This suggests that polarizedlexical features suffered from the off-topic content whenusing the entire blog and are more useful within contextsof certain topics.When using content words only, we observe consistentgain across all the feature sets.
Three feature settings,?LF+PB?,?LF+T?
and ?LF+PL+PB+T?, achieve statisti-cally significant further improvement (compared to usingall the words of relevant contexts).
The best accuracy(75.6%) is achieved by using the ?LF+PB?
features.Feature Set Whole Relevant ContextBlog All Words Content WordsLF 72.07 74.92?
75.14LF+PL 70.93 75.32?
75.34LF+PB 72.44 75.03?
75.6?LF+T 72.17 75.01?
75.35?LF+PL+PB 70.81 75.27?
75.35LF+PL+T 72.74 75.32?
75.41LF+PB+T 72.29 75.17?
75.42LF+PL+PB+T 71.85 75.21?
75.45?Table 2: Blog polarity classification results (accuracy in %) us-ing topic relevant context composed of all the words or onlycontent words.4.2 Adaptive methods using review dataDomain adaptation has been studied in some previouswork (e.g., (Blitzer et al, 2007; Mansour et al, 2008)).In this paper, we evaluate two adaptive approaches in or-der to leverage review data to improve blog polarity clas-sification.
In the first approach, in each of the 10-foldcross-validation training, we pool the blog training data(90% of the entire blog data) together with all the reviewdata from 5 different domains.
In the second method, weaugment features with hypotheses obtained from classi-fiers trained using other domain data.
Specifically, wefirst trained 5 classifiers from 5 review domain data setsrespectively, and encoded the hypotheses from differentclassifiers as features for blog training (together with theoriginal features of the blog data).
Results of these twoapproaches are shown in Table 3.
We use the topic rele-311vant context with content words only in this experiment,and present results for different feature combinations (ex-cept the baseline ?LF?
setting).
The significance test isconducted in comparison to the results using only blogdata for training, for the same feature setting.We find that the first approach does not yield any gain,even though the added data is about the same size asthe blog data.
It indicates that due to the large differ-ence between the two genres, simply combining blogsand reviews in training is not effective.
However, wecan see that using augmented features in training signifi-cantly improved the performance across different featuresets.
The best result is achieved using ?LF+T?
features,76.84% compared with the best accuracy of 75.6% whenusing the blog data only (?LF+PB?
features).Feature Set Only Blog Pool Data Augment FeaturesLF+PL 75.34 75.05 76.12?LF+PB 75.6 74.35 76.28?LF+T 75.35 74.47 76.84?LF+PL+PB 75.35 74.94 76.7?LF+PL+T 75.41 74.85 76.32?LF+PB+T 75.42 74.46 76.3?LF+PL+PB+T 75.45 74.96 76.53?Table 3: Results (accuracy in %) of blog polarity classificationusing two methods leveraging review data.4.3 Error analysisNotice that after achieving some improvements the per-formance on blogs is still much worse than on reviewdata.
Thus we performed a manual error analysis for abetter understanding of the difficulties of sentiment anal-ysis on blog data, and identified the following challenges.
(a) Idiomatic expressions.
Compared to reviews, blog-gers seem to use more idioms.
For example, ?Of coursehe has me over the barrel...?
expresses negative opinion,however, there are no superficially indicative features.
(b) Ironic writing style.
Some bloggers prefer ironicstyle especially when speaking against something orsomebody, whereas opinions are often expressed usingplain writing style in reviews.
Simply using the surfaceword level features is not able to model these properly.
(c) Background knowledge.
In some political blogs,the polarized expressions are implicit.
Correctly recog-nizing them requires background knowledge and deeperlanguage analysis techniques.5 Conclusions and Future WorkIn this paper, we have evaluated various features and thedomain effect on sentimental polarity classification.
Ourexperiments on blog and review data demonstrated dif-ferent feature effectiveness and the overall poorer perfor-mance on blogs than reviews.
We found that the polarizedfeatures and the transition word features we introducedare useful for polarity classification.
We also show thatby extracting topic-relevant context and considering onlycontent words, the system can achieve significantly betterperformance on blogs.
Furthermore, an adaptive methodusing augmented features can effectively leverage datafrom other domains, and yield improvement comparedto using in-domain training or training on combined datafrom different domains.
For our future work, we planto investigate other adaption methods, and try to addresssome of the problems identified in our error analysis.6 AcknowledgmentThe authors thank the three anonymous reviewers fortheir suggestions.ReferencesApoorv Agarwal, Fadi Biadsy, and Kathleen McKeown.
2009.Contextual phrase-level polarity analysis using lexical affectscoring and syntactic n-grams.
In Proc.
of EACL.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.
Bi-ographies, bollywood, boom-boxes and blenders: Domainadaptation for sentiment classification.
In Proc.
of ACL.Erik Boiy, Pieter Hens, Koen Deschacht, and Marie-FrancineMoens.
2007.
Automatic sentiment analysis in on-line text.In Proc.
of ELPUB.Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Srihari.2005.
Using verbs and adjectives to automatically classifyblog sentiment.
In Proc.
of AAAI.Minqing Hu and Bing Liu.
2004.
Mining and summarizingcustomer reviews.
In Proc.
of ACM SIGKDD.Jungi Kim, Jin-Ji Li, and Jong-Hyeok Lee.
2009.
Discoveringthe discriminative views: Measuring term weights for senti-ment analysis.
In Proc.
of ACL-IJCNLP.Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.2008.
Domain adaptation with multiple sources.
In Proc.of NIPS.Karo Moilanen and Stephen Pulman.
2008.
The good, the bad,and the unknown: Morphosyllabic sentiment tagging of un-seen words.
In Proc.
of ACL.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploiting classrelationships for sentiment categorization with respect to rat-ing scales.
In Proc.
of ACL.Bo Pang, Lilian Lee, and Shrivakumar Vaithyanathan.
2002.Thumbs up?
sentiment classification using machine learningtechniques.
In Proc.
of EMNLP.Ellen Riloff and Janyce Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In Proc.
of EMNLP.Ivan Titov and Ryan McDonald.
2008.
Modeling online re-views with multi-grain topic models.
In Proc.
of WWW.Peter D. Turney.
2002.
Thumbs up or thumbs down?
semanticorientation applied to unsupervised classification of reviews.In Proc.
of ACL.Michael Wiegand and Dietrich Klakow.
2009.
Topic-Relatedpolarity classification of blog sentences.
In Proc.
of the 14thPortuguese Conference on Artificial Intelligence: Progressin Artificial Intelligence, pages 658?669.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In Proc.
of HLT-EMNLP.Wei Zhang, Clement Yu, and Weiyi Meng.
2007.
Opinion re-trieval from blogs.
In Proc.
of CIKM, pages 831?840.312
