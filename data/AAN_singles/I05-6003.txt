A Study of Applying BTM Model on the Chinese Chunk BracketingJia-Lin TsaiTung Nan Institute of Technology, Department of Information ManagementTaipei 222, Taiwan, R.O.C.tsaijl@mail.tnit.edu.twAbstractThe purpose of this paper is to auto-matically generate Chinese chunkbracketing by a bottom-to-top map-ping (BTM) model with a BTM data-set.
The BTM model is designed as asupporting model with parsers.
We de-fine a word-layer matrix to generatethe BTM dataset from Chinese Tree-bank.
Our model matches auto-learnedpatterns and templates against seg-mented and POS-tagged Chinese sen-tences.
A sentence that can be matchedwith some patterns or templates iscalled a matching sentence.
The ex-perimental results have shown that thechunk bracketing of the BTM modelon the matching sentences is high andstable.
By applying the BTM model tothe matching sentences and the N-gram model to the non-matching sen-tences, the experiment results showthe F-measure of an N-gram modelcan be improved.1 IntroductionThe definition of chunk, which has been repre-sented as groups of words between squarebrackets, was first raised by (Abney, 1991).
Achunker is to divide sentences into non-overlapping phrases by starting with finding cor-related chunks of words.
Text chunking has beenshown a useful pre-processing step for languageparsing (Sang and Buchholz, 2000).
Among thechunk types, NP chunking is the first to receivethe attention (Ramshaw and Marcus, 1995), thanother chunk types, such as VP and PP chunking(Veenstra, 1999).
For English (Sang andBuchholz, 2000) and Chinese (Li et al, 2004)languages, the top 3 most frequent chunk typesare NP, VP and PP chunks.
Meanwhile, the threechunk types cover about 80% of chunking prob-lems.
In many natural language processing (NLP)applications, such as information retrieval,knowledge discovery, example-based machinetranslation (EBMT) and text summarization, canbenefit with chunks (Le et al, 2003; Munoz etal., 1999; Oliver, 2001; Zhou and Su, 2003).As per the reports (Menzel, 1995; Sang andBuchholz, 2000; Basili and Zanzotto, 2002;Knutsson et al, 2003; Li et al, 2004; Xu et al,2004; Johnny et al, 2005), there are three im-portant trends in the study of Chinese textchunking and parsing.
These important trendsare: (1) Treebank-Derived Approaches for auto-constructing useful patterns and templates fromTreebank (TB) as rules combined with statisticallanguage models (SLM), such as N-gram mod-els and support vector machines (SVMs), etc.
; (2)Robust Chunkers against Treebank sparsenessand perfect/actual input.
Here the perfect inputmeans the word-segmentation and Part-of-Speech (POS) tags all are correct.
The actualinput means the word-segmentation and POStags all are generated by a selected segmenterand a POS tagger; and (3) High PerformanceChunk Bracketing has been reported that thekey issue of Chinese parsing (Li et al, 2004).
Tosum up these trends, one of critical issues fordeveloping a high performance Chinese chunkeris to find methods to achieve high performanceof chunk bracketing against training size, perfectand actual input.Following these trends of Chinese chunkingand parsing, the goals of this paper are:(1) Define a Word-Layer Matrix and generatethe Bottom-to-Top Mapping (BTM) datasetto auto-derive useful patterns and templateswith probabilities from Chinese Treebank21(CTB) as rules for chunking;(2) Develop a BTM model with the BTM data-set to identify the chunks (i.e.
phrase bounda-ries) for a given segmented and POS-taggedChinese sentence;(3) Show the chunk bracketing performance ofthe BTM model is high and stable againsttraining corpus size, perfect and actual input;(4) Show the BTM model can improve the per-formance (F-measure) of N-gram models onchunk bracketing.The remainder of this paper is arranged asfollows.
In Section 2, we present the BTMmodel for identifying chunks for each seg-mented and POS-tagged Chinese sentence.
Ex-perimental results and analyses of the BTMmodel are presented in Section 3.
Finally, inSection 4, we present our conclusions and dis-cuss the direction of future research.2 Development of the BTM model2.1 Introduction of Chinese TreebankA Chinese Treebank (CTB) is a segmented,POS-tagged and fully bracketed Chinese corpuswith morphological, syntactic, semantic and dis-course structures.
The CKIP (Chinese Knowl-edge Information Processing) Chinese-Treebank(CCTB) and the Penn Chinese Treebank (PCTB)are two of most important Chinese Treebankresources for Treebank-derived NLP tasks inChinese (CKIP, 1995; Xia et al, 2000; Xu et al,2000; Li et al, 2004).
The brief introductions ofthe CCTB and the PCTB are given as below(Table 1 is a brief comparison between theCCTB and the PCTB):(1) CCTB: the CCTB is developed in traditionalChinese texts (BIG5 encoded) taken from theAcademia Sinica Balanced Corpus 3.0 (ASBC3)at the Academia Sinica, Taiwan (Chen et al,1996; Chen et al, 1999; Huang et al, 2000,Chen et al, 2003; Chen et al, 2004).
The CCTBuses Information-based Case Grammar (ICG) asthe language framework to express both syntac-tic and semantic descriptions (Chen and Huang,1996).
The structural frame of CCTB is basedon the Head-Driven Principle: it means a sen-tence or phrase is composed of a core Head andits arguments, or adjuncts (Chen and Hsieh,2004).
The Head defines its phrasal categoryand relations with other constituents.
The pre-sent version CCTB2.1 (CCTB Version 2.1) in-cludes 54,902 sentences (i.e.
trees) and 290,144words that are bracketed and post-edited by hu-mans, based on the computer parsed results(CKIP, 1995).
There are 1,000 CCTB trees opento the public for researchers to download on theCCTB portal.
The details of supplementaryprinciples, symbol illustrations, semantic roles,phrasal structures and applications of the CCTBcan be found in (CCTB portal; Chen et al, 2003;Chen and Hsieh, 2004; You and Chen, 2004).Table 1.
A brief comparison between CCTB2.1 andPCTB4 (The number in () is the word frequency andthe English word in [] is the English Translation forthe corresponding Chinese word)CCTB2.1         PCTB4Developer  CKIP          UPennContent type  Balanced          Newswirecorpus          sourcesLanguage framework ICG          HPSGWord standard  Taiwan          China(CKIP, 1996)        (Liu et al,1993)POS-tagging system type hierarchical    non-(5 layer)          hierarchicalStructure frame  Head-driven     Head-drivenCode   BIG5          GBNo.
of sentences  54,902          15,162No.
of distinct POS tags 302          47No.
of words in CTB 290,144          404,156Top 3 one-char words ?
(19,212)       ?
(15,080)[of]          [of]?
(4,608)         ?
(4,055)[is/are]          [at]?
(4,235)         ?
(2,965)[at]          [is/are]Top 3 two-char words ??
(1,057)     ??
(2,097)[we]          [China]??
(675)        ??
(1,015)[a/an/one]         [Economy]??
(564)        ??
(989)[they]          [business](2) PCTB: the PCTB is developed in simplifiedChinese texts (GB encoded) taken from thenewswire sources (consists of Xinhua newswire,Hong Kong news and Sinorama news magazine,Taiwan) at the Department of Computer andInformation Science, University of Pennsylvania(UPenn).
The PCTB uses Head-driven PhraseStructure Grammar (HPSG) to create Chinesetexts with syntactic bracketing (Xia et al 2000;Xue et al 2002).
Meanwhile, the semantic anno-tation of PCTB mainly deals with the predicate-argument structure of Chinese verbs in PennChinese Proposition Bank (Xue and Palmer, 2003;22Xue and Palmer, 2005).
The present versionPCTB5 (PCTB Version 5), contains 18,782 sen-tences, 507,222 words, 824,983 Hanzi and 890data files.
The PCTB was created by two passapproach.
The first pass was done by one anno-tator, and the resulting files were checked by asecond annotator (the second pass).
The detailsand applications of PCTB can be found in(PCTB portal; Xia et al 2000; Chiou et al 2001;Xue et al 2002; Xue et al 2005).Overall, from Table 1, the four major differ-ences between the CCTB and the PCTB are con-tent type, language framework, word standardand POS-tagging system type.
The CCTB isnatural to be a balanced CTB because its contentis taken from the Academia Sinica BalancedCorpus (CKIP, 1995).
On the other hand, sincethe content type of PCTB is newswire sources, itis natural to be a newswire-based CTB and not abalanced CTB.2.2 Generating the BTM DatasetFirstly, we use CCTB2.1 as an example to de-scribe how to generate a BTM dataset from theCCTB with the word-layer matrix.
Then, wedefine two types of conditional probabilitiesused in this study for constructing the BTMmodel.
Finally, the algorithm of our BTM modelis given in Section 2.3.Figure 1.
The tree structure of CCTB2.1 for the Chinesesentence ???(movie)?(of)??(picture)??(colorful)??(interesting)?
(Note that the content ofthe nodes between the root and the words is [The-matic role : Syntactic category])(1) Generation of BTM dataset from CCTB2.1:Figure 1 shows the tree structure of CCTB2.1for the Chinese sentence ???(movie)?(of)??
(picture)??
(colorful)??
(interesting).
?The content of the nodes between the root layerand the words layer (leaves) is comprised of the-matic roles and syntactic categories.
The the-matic roles can be annotated as a Theme,Property, etc., while the syntactic categoriescan be annotated as a POS-tag (such as Nac) ora phrasal category (such as NP).
The details ofCCTB syntactic and thematic annotations can befound in (Chen et al, 2003).Table 2.
The word-layer matrix extracted fromCCTB2.1 for the Chinese sentence ???(movie)?(of)??(picture)??(colorful)??
(interesting)?Word1st layer(Top)2nd layer3rd layer(Bottom)??
Head:Nac?
Property:NP.?
Head:DE?
?Theme:NPHead:Nac Head:Nac??
Head:H11??
Head:VH11 Head:VH11 Head:H11Table 3.
The BTM dataset for the CCTB2.1 tree ofthe Chinese sentence ???
(movie)?
(of)??(picture)??(colorful)??
(interesting)?Type  ContentBL Word pattern <??\?\??\??\?
?>TL Word pattern   <??:?:??+??:?
?>BL POS pattern <Nac\DE\Nac\VH11\VH11>TL POS pattern <Nac:DE:Nac+VH11:VH11>TL POS template  <Na%Na+VH11:VH11>PC pattern <NP+VH11>For each tree structure of CCTB2.1 (asshown in Fig.1), we first translate it into a word-layer matrix as shown in Table 2.
In a word-layer matrix, the left, first row is the word layer(with words) and the other rows are the firstlayer to the last layer (with thematic roles andsyntactic categories).
For each word-layer ma-trix, the first layer and last layer are called theTop Layer (TL) and the Bottom Layer (BL),respectively.
According to the TL and the BL ofa word-layer matrix (see Table 2), we can trans-late a CCTB tree into a BTM dataset as shownin Table 3.
Each BTM dataset includes twotypes of BTM content.
One is the BL and TLword patterns expressed by Chinese words.
Theother one is the BL and TL POS patterns ex-pressed by POS tags.
Furthermore, for each TL23POS pattern, we also generate its correspondingTL POS template (with POS tags) and phrasalcategory (PC) pattern (with POS tags andphrasal categories).In the Table 3:BL stands for the bottom layer (the last layer ofa word-layer matrix);TL stands for the top layer (the first layer of aword-layer matrix);PC stands for the phrasal category in a TL;?\?
indicates the word boundary in a BL;?+?
indicates word/phrase boundaries in a TL;?:?
indicates next to; for example,?Nac:DE?
means ?Nac?
next to ?DE?;?%?
indicates near by; for example,?Nac%Nac?
means ?Nac?near by ?Nac?;?<?
indicates the begin of a sentence; and?>?
indicates the end of a sentence.The CKIP POS tagging is a hierarchical system.The first layer of CKIP POS tagging includeeight main syntactic categories, i.e.
N (noun), V(verb), D (adverb), A (adjective), C (conjunc-tion), I (interjection), T (particles) and P (prepo-sition).
As per the CKIP technical reports (CKIP,1995; CKIP, 1996), the maximum layer numberof CKIP POS tagging is 5.
Take the CKIP POStag ?Ndabe?
as an example, we define its POStags with POS layer numbers 1, 2, 3, 4 and 5 as?N?, ?Nd?, ?Nda?, ?Ndab?
and ?Ndabe?, re-spectively.
Thus, if the POS layer of BTMmodel is set to 2 (called 2 POS-layer mode), theBL POS pattern ?<Nac\DE\Nac\VH11\VH11>?in Table 3 will become ?<Na\DE\Na\VH\VH>?,and so forth.Table 4.
The BTM dataset for the PCTB4 tree of theChinese sentence ???
(both)??
(major)??(agent)??
(appear)?Type   ContentBL Word pattern  <??\??\??\?
?>TL Word pattern     <??:??:??+?
?>BL POS pattern       <PN\JJ\NN\VV>TL POS pattern  <PN:JJ:NN+VV>TL POS template    <PN%NN+VV>PC pattern  <NP-SBJ+VP>By the word-layer matrix, the BTM dataset ofPCTB can also be generated.
Table 4 shows anexample BTM dataset for the PCTB4 tree of theChinese sentence ???(both)??(major)??(agent)??(appear).?
Since the POS tagging ofPCTB is not a hierarchical system, there is noPOS layer mode can be set to the BTM datasetof PCTB.
(2) Definitions of Two Types of Probabilities:In this study, two conditional probabilities wereused in the BTM model.
The Type I conditionalprobability is used to perform full TL POS pat-tern matching.
The Type II conditional prob-ability is used to perform full TL POS templatematching.
Details of these probabilities aregiven below.Type I. Pr(a given TL POS pattern | the BL POSpattern of the given TL POS pattern) =(# of the given TL POS pattern found in thetraining BTM dataset) /(# of the BL POS patterns of the given TL POSpattern found in the training BTM dataset).Take the BL POS pattern ?Cb\Nc\DE\Na?
as anexample.
There are:one TL POS pattern ?Cb+Nc:DE:Na?four TL POS pattern ?Cb+Nc:DE+Na?
andfive BL POS pattern ?Cb\Nc\DE\Na?
in theCCTB2.1 BTM dataset.
Thus,the Pr(Cb+Nc:DE:Na|Cb+Nc+DE+Na) =  1/5 =0.2; andthe Pr(Cb+Nc:DE+Na|Cb+Nc+DE+Na) =  4/5 =0.8.Table 5a.
Top 5 most frequent TL POS patternswhose number of POS tags is 5 for 2 POS-layer mo-de (training size is 45,000 CCTB2.1 trees)TL POS pattern (Type I  pro.
)V_+DM:VH:DE:Na (19/19 = 100%)(Eg.
?
[is]+??
[a]:??
[small]:?
[of]:??
[village])Nb:A:Na:Nb+VE (11/11 = 100%)(Eg.
??
[taiyuan]:??
[inference reader]:??[manager]:???[gao-zi-neng]+??
[point out])Nc:Nb+VH+Nc:Nb  (10/10 = 100%)(Eg.
??[Philadelphia]:????
[76-people team]+?[win]+???[Washington]:???
[bullet team]))VC+Di+Na:DE:Na (9/9 = 100%)(Eg.
?
[attach]+?
[to]+??
:?
:??
[bridge offriendship])Nh+VA+P2:Na:Nc (8/8 = 100%)(Eg.
??[they]+?[sit]+?:???:?
[at the aero-space plane])24Table 5a gives the Top 5 most frequent TLPOS patterns whose number of POS tags is 5while the POS layer number is 2.Type II.
Pr(matching patterns | a given TL POStemplate) =(# of matching TL POS patterns of the givenTL POS template found in the training BTMdataset) / (# of matching BL POS pattern of thegiven TL POS template found in the trainingBTM dataset).Take the TL POS template ?P3%Na+VA?
as anexample.
In the CCTB2.1 BTM dataset, thereare four matching BL POS patterns and twomatching TL POS pattern for the template?P3%Na+VA?, namely: (Note that ?%?
means?near by?)?P3\Dd\VA\DE\Na\VA??P3:Dd:VA:DE:Na+VA?(matching)?P3\Na\P2\VC\Nb\Nc\DE\Na\VA??P3:Na+P2:VC:Nb:Nc:DE:Na+VA?
(no matching)?P3\Na\VA??P3:Na+VA?(matching)?P3\Na\VC\Na\VA??P3:Na+VC+Na+VA?
(no matching)Thus, the Pr(matching pattern|P3%Na+VA) =2/4 = 0.5.Table 5b gives 5 randomly selected TL POStemplates where their POS number is 5 whilethe POS layer number is 2.Table 5b.
Five randomly selected TL POS templateswhere their POS number is 5 for 2 POS-layer mode(training size = 45,000)TL POS template  | Type II  pro.Na+VF+Nh+VA%Na  | 100% (1/1)P1%Nc+VC+Nc%Na  |   50% (1/2)Ne+Dd+V_+VH%Na  | 100% (3/3)DM%Na+VE+Nc%Na   | 100% (1/1)DM%Na+VK+VC%Na     | 100% (1/1)2.3 Algorithm of the BTM ModelFollowing is the algorithm of our BTM modeluses Types I (full TL POS pattern matching) andType II (full TL POS template matching) condi-tional probabilities to determine the chunks for agiven segmented and POS-tagged Chinese sen-tence.
We use BTM (value1, value2, value3) toexpress the function of our BTM model, wherevalue1 is the BTM threshold value, value2 is thePOS layer number and value3 is the BTM train-ing size.
Table 6 is a step by step example todemonstrate the detailed processes and outputsof our BTM model.Table 6.
A step by step example of the application ofBTM (0.5; 2; 45,000) for the given BL POS pattern?Na\Na\DE\Nb(??[boy]\??[age]\?[of]\???
[schweitzer])?Step    Output1 Na\Na\DE\Nb(??[boy]\??[age]\?[of]\???
[schweitzer])2       NULL; Goto Step 43       -4       Pr(Na%DE+Nb) = 66.7% (Type II);  and usethe selected TL POS template ?Na%De+Nb?
totranslate ?Na\Na\DE\Nb?
into ?Na:Na:DE+Nb?5 TL POS pattern = Na:Na:DE+Nb, andMatching sentence = ??:??:?+??
?Chunks =  ???????
and ????
?Step 1.
Give the value1 (BTM threshold value),value2 (POS layer number) and value3(training size), as well as the segmentedand POS-tagged sentence.
In the follow-ing steps, the POS tagging sequence ofthe given sentence is called the BL POSpattern, such as the ?Na\Na\DE\Nb?
inTable 6.Step 2.
According to the BL POS pattern in Step1, find all matched TL POS patternswhose corresponding Type I probabili-ties are greater than or equal to the BTMthreshold value.
If the number ofmatched TL POS patterns is zero, thengo to Step 4.Step 3.
Using the matched TL POS patternsfrom Step 2, select the TL POS patternthat has the maximum Type I probabil-ity as the output.
If there are two ormore TL POS patterns with the samemaximum Type I probability, randomlyselect one as the output.
Go to Step 5.Step 4.
According to the BL POS pattern, findall matched TL POS templates whosecorresponding Type II probabilities aregreater than or equal to the BTM thresh-old value.
Select the TL POS templatethat has the maximum Type II probabil-25ity to generate the output (see Table 6,Step 4).
If there are two or more TLPOS templates with the same maximumType II probability, randomly selectone to generate the output.
If the numberof matched TL POS patterns is zero,then a NULL output will be given.Step 5.
Stop.
If a NULL TL POS pattern outputis given, this input sentence is a non-matching sentence.
Otherwise, it is amatching sentence.3 Experiment ResultsTo conduct the following experiments in ten-folds, we randomly select 50,000 trees ofCCTB2.1 and separate them into the followingtwo sets:(1) Training Set consists of 45,000 CCTB2.1trees; and(2) Open Testing Set consists of the other 5,000CCTB2.1 trees.In our computation, 66% of CCTB2.1 BL POSpatterns in the open testing set are not found inthe training set.
This means the ratio of unseenCCTB2.1 BL POS patterns in the open testingset is 66%.
The PCTB4 BTM dataset was notused in this study by two reasons: the first one isthat the PCTB is not a balanced CTB; the sec-ond one is that the POS tagging system of PCTBis not a hierarchical system.We conducted four experiments in this study.The first three experiments are designed to showthe relationships between the chunk bracketingperformance of the BTM model on the matchingsentences and the three BTM parameters: POSlayer number; BTM threshold value; and BTMtraining size.
To avoid the error propagation ofword segmentation and POS tagging, the firstthree experiments only consider open testingsentences with correct word segmentations andPOS tags provided in CCTB2.1 as perfect input.The fourth experiment is to show the BTMmodel is able to improve the performance (F-measure) of N-gram models on Chinese chunkbracketing for both perfect input and actual in-put.
Here, the actual input means the wordsegmentations and POS tags of the testing sen-tences were all generated by a forward maxi-mum matching (FMM) segmenter and a bigram-based POS tagger, respectively.To evaluate the performance of our BTMmodel, we use recall (R), precision (P), and F-measure (F) (Manning and Schuetze, 1999),which are defined as follows:Recall (R) = (# of correctly identified chunkbrackets) / ( # of chunk brackets)        (1)Precision (P) = (# of correctly identified chunkbrackets) / ( # of identified chunk brackets)   (2)F-measure (F) = (2 ?
recall ?
precision) / (re-call + precision)           (3)In addition, we use coverage ratio (CR) to repre-sent the size of matching sentences (or say,matching set) of our BTM model.
The CR isdefined as:Coverage Ratio (CR) = (# of not NULL outputsentences) / (# of total testing sentences)        (4)3.1 Relationship between POS layer numberand BTM performanceIn the 1st experiment, the BTM threshold valueis set to 1 and the BTM training size is set to45,000.
Table 7 is the first experimental resultsof BTM performance (P, R, F) and CR for thePOS layer numbers are 1, 2, 3, 4 or 5.
From Ta-ble 7, it shows the POS layer number is posi-tively related to the F-measure.
Since the BTMmodel with POS layer number 2 is able toachieve more than 96% F-measure, we use POSlayer number 2 to conduct the following ex-periments.
This experimental result seems toindicate that the CCTB2.1 dataset with POSlayer number 2 (including 57 distinct POS tags)can provide sufficient information for the BTMmodel to achieve an F-measure of more than96% and a maximum CR of 46.88%.Table 7.
The first experimental results of BTM (1;1/2/3/4/5; 45,000)POS Layer #    P(%) R(%) F(%) | CR(%)1  86.32 85.57 85.94 | 33.432     97.03 95.82 96.42 | 46.883     99.04 98.86 98.95 | 34.074     99.07 98.88 98.97 | 31.925     99.07 98.87 98.97 | 31.843.2 Relationship between BTM thresholdvalue and BTM performanceIn the 2nd experiment, the POS layer number isset to 2 and the BTM training size is set to2645,000.
Table 8 is the second experimental re-sults of BTM performance and CR when theBTM threshold value is 1.0, 0.9, 0.8, 0.7, 0.6 or0.5.
From Table 8, it shows the BTM thresholdvalue is positively related to the F-measure.
Be-sides, the F-measure difference between thresh-old values 1.0 and 0.5 is only 1.37%.
This resultindicates that the BTM model can robustlymaintain an F-measure of more than 95% and aCR of more than 46% while the POS layer num-ber is set to 2, BTM training size is set to 45,000and the BTM threshold value is t 0.5.Table 8.
The second experimental results of BTM(1/0.9/0.8/0.7/0.6/0.5; 2; 45,000)ThresholdValue  P(%) R(%) F(%) | CR(%)1.0  97.03 95.82 96.42 | 46.880.9  96.99 95.71 96.35 | 47.840.8  96.95 95.54 96.24 | 49.420.7  96.94 95.49 96.21 | 50.660.6  96.86 95.26 96.05 | 51.920.5  96.34 93.80 95.05 | 53.723.3 Relationship between BTM training sizeand BTM performanceIn the 3rd experiment, the BTM threshold valueis set to 0.5 and POS layer number is set to 2.Table 9 is the third experimental results of BTMperformance and CR when the BTM trainingsize is 5000, 10000, 15000, 20000, 25000,30000, 35000, 40000 or 45000.
From Table 9, itseems to indicate that the F-measure of the BTMmodel is independent of the training size be-cause the maximum difference between theserespective F-measures is only 0.88%.Table 9.
The third experimental results of BTM (0.5,2, 5,000/ 10,000/ 15,000/ 20,000/ 25,000 /30,000/35,000 / 40,000/ 45,000)Training size P(%) R(%) F(%) | CR(%)5,000  95.82 91.33 94.61 | 30.2310,000  96.28 92.16 94.64 | 35.1315,000  96.29 92.48 94.54 | 40.3220,000  96.07 92.59 94.44 | 43.7325,000  96.19 92.74 94.43 | 46.7030,000  96.17 92.78 94.30 | 48.4635,000  96.22 92.92 94.35 | 50.5140,000  96.28  93.06 94.17 | 52.2945,000  96.34 93.80 95.05 | 53.72To sum up the above three experimental results(Tables 7-9), it shows that the F-measure (over-all performance) of our BTM model with POSlayer number (t 2) is apparently not sensitive toBTM threshold value (t 0.5) and BTM trainingsize (t 5,000) on the matching set with perfectinput.
Since the CR of our BTM model is posi-tively related to BTM training size, it indicatesour BTM model should be able to maintain thehigh performance chunk bracketing (more than95% F-measure on the matching set with perfectinput) and increase the CR only by enlarging theBTM training size.3.4 Comparative study of the N-gram modeland the BTM model on perfect/actual inputTo conduct the 4th experiment, we develop N-gram models (NGM) by the SRILM (StanfordResearch Institute Language Modeling) toolkit(Stolcke, 2002) as the baseline model.
SRILM isa freely available collection of C++ libraries,executable programs, and helper scripts de-signed to allow both production of, and experi-mentation with, statistical language models forspeech recognition and other NLP applications(Stolcke, 2002).
In this experiment, the TL POSpatterns (such as ?<Na:DE:Na+VH:VH>?)
oftraining set were used as the data for SIRLM tobuild N-gram models.
Then, use these N-grammodels to determine the chunks for each BLPOS pattern in the testing set.
Note that these N-gram models were trained by the TL POS pat-terns only, not by each layer?s POS patterns.Figure 2 shows the distribution of n-gram pat-terns of N-gram models (N is from 2 to 44)trained by the training set.???????????????????????????????????????????????
??
??
??
??
??????????????????????????
?Fig.2 The n-gram distribution of N-gram models (Nis 2 to 44) trained by the 45,000 CCTB2.1 TL POSpatterns of training set27Tables 10, 11, 12 13 and 14 are the results of thefourth experiment.
The explanations of the fivetables are given below.Table 10.
The fourth experimental results of NGM(2/3/4/5/6/N; 45,000) for perfect inputN-gram P(%) R(%) F(%)2  77.62 78.98 78.303  80.16 81.83 80.994  80.36 81.91 81.135  79.58 81.38 80.476  78.98 80.35 79.91N (=44) 78.51 80.44 79.46Table 11.
The comparative experimental results ofP/R/F and CR between BTM (0.5, 2, 45,000) and a 4-gram model for perfect inputSet     BTM(0.5, 2, 45k)    4-gram  | CR(%)matching     96.3/93.8/95.1         89.3/89.7/89.5    | 53.6no matching    -                   73.5/75.7/74.6    | 46.4Table 12.
The comparative experimental results ofP/R/F and CR between BTM (0.5, 2, 45,000) and a 4-gram model for actual inputSet     BTM(0.5, 2, 45k)     4-gram           | CR(%)matching    97.4/97.3/97.3          95.1/96.7/95.9    | 19.2no matching  -                   69.1/68.8/68.9    | 80.8Table 13.
The comparative experimental results ofP/R/F of a 4-gram model and a 4-gram with BTM(0.5, 2, 45,000) model for perfect input and actualinputModel  Perfect(P/R/F) Actual(P/R/F)4-gram  80.4/81.9/81.1 72.63/72.23/72.43BTM+4-gram 83.8/83.4/83.6 74.70/73.03/73.42From Table 10, it shows the maximum preci-sion, recall and F-measure of N-gram models alloccur at the 4-gram model for perfect input.Thus, we use the 4-gram model as the baselinemodel in this experiment.
Tables 11 and 12 arethe comparative experimental results of thebaseline model and the BTM model on thematching sets of perfect input and actual input,respectively.
From Table 11, it shows the per-formance (95.1% F-measure) of a BTM (0.5, 2,45,000) is 5.6% greater than that of a 4-grammodel (89.5% F-measure) for the matching setwith perfect input.
From Table 12, it shows theperformance (97.3% F-measure) of a BTM (0.5,2, 45,000) is 1.4% greater than that of a 4-grammodel (95.9% F-measure) for the matching setwith actual input.
Table 13 is the experimentalresults of applying the BTM model to thematching set and the 4-gram model to the non-matching set.
From Table 13, it shows the F-measure of a 4-gram model can be improved bythe BTM model for both perfect input (2.5%increasing) and actual input (1% increasing).According to all the four experimental results,we have: (1) the BTM model can achieve betterF-measure performance than N-gram models onthe matching sets for both perfect input and ac-tual input; and (2) the chunk bracketing per-formance of the BTM model for the matchingsets should be high and stable against trainingsize, perfect and actual input while POS layernumber t 2 and BTM threshold value t 0.5.4 Conclusion and Future DirectionsIn this paper, we define a word-layer matrix thatcan be used to translate the CKIP Treebank andthe Penn Chinese Treebank into correspondingBTM datasets.
By the BTM dataset, we devel-oped a BTM model, adopting two types of con-ditional probabilities and using full TL POSpattern matching and full TL POS templatematching to identify the chunks for each seg-mented and POS-tagged Chinese sentence.Our experiment results show that the BTMmodel can effectively achieve precision and re-call optimization on the matching sets for bothperfect input and actual input.
The experimentalresults also demonstrate that:(1) The BTM threshold value is positively re-lated to the BTM F-measure;(2) The POS layer number is positively relatedto the BTM F-measure;(3) The F-measure of our BTM model for thematching set should be not sensitive to twoBTM parameters: BTM threshold value andBTM training size;(4) The chunk bracketing of our BTM model onthe matching set should be high and stable (orsay, robust) against training size, perfect andactual input while POS layer number is t 2 andBTM threshold value is t 0.5;(5) The BTM model can provide a matching setwith high and stable performance (more than95% F-measure) for improving N-gram-likemodels without trial-and-error, or say, a tuningprocess.
For most statistical language models,such N-gram models, need tuning to improvetheir performance and large-scale corpus to28overcome corpus sparseness problem (Manninget al, 1999; Gao et al, 2002; Le et al, 2003).Furthermore, it is difficult for them to identifytheir ?matching set?
with high and stable per-formance, whereas our BTM model has the abil-ity to support chunkers and parsers forimproving chunking performance.
According tothe fourth experiment results, when applying aBTM (0.5, 2, 45,000) model on the matching setand a 4-gram model on the non-matching set,the combined system can improve the F-measure of 4-gram model 2.5% for perfect inputand 1.0% for actual input.
Among the chunkingand parsing models, Cascaded Markov Modelsshould be the first one to construct the parse treelayer by layer with each layer?s Markov Model.As per (Brants, 1999), each layer?s chunk brack-eting of Cascaded Markov Models is dependentbecause the output of a lower layer is passed asinput to the next higher layer.
On the contrast,our BTM model can independently generate thechunks for top layer without the results of lowerlayer chunk bracketing; and(6) Since the F-measures of the BTM model forthe matching sets of perfect and actual inputboth are greater than 95%, we believe our BTMmodel can be used not only to improve the F-measure of existing shallow parsing or chunkingsystems, but also to help select valuable sen-tences from the non-matching set for effectivelyextending the CR of our BTM model.In the future, we shall study how to combineour BTM model with more conventional statisti-cal approaches, such as Bayesian Networks,Maximum Entropy and Cascaded Markov Mod-els, etc.
Meanwhile, we will also apply our BTMmodel to the Penn English Treebank as a com-parative study.AcknowledgementSome work of this paper was done while theauthor was a visitor at the Institute of Informa-tion Science (Academia Sinica in Taiwan).
Wethank Prof. Wei-Lian Hsu and Cheng-Wei Shihfor their kind help.
We also thank the ChineseKnowledge Information Processing group inTaiwan for providing us the CKIP Treebank.ReferencesAbney, S. Parsing by chunks.
In Principle-BasedParsing.
Kluwer Academic Publishers,Dordrecht: pp.257?278.
1991.Basili, R. and Zanzotto, F. M. Parsing engineeringand empirical robustness.
Natural LanguageEngineering, 8(2?3):pp.97?120.
2002.Brants, T. Cascaded Markov Models.
Proceedings ofEACL '99.
pp.118 ?
125.
1999.Chen, K.-J.
and Huang C.-R. Information-based CaseGrammar: A Unification-based Formalism forParsing Chinese.
In Huang et al (Eds.
): pp.23-46.
1996.Chen, K.-J., Huang C.-R., Chang, L.-P. and Hsu, H.-L. Sinica Corpus: Design Methodology for Bal-anced Corpra.?
Proceedings of the 11th PacificAsia Conference on Language, Information,and Computation (PACLIC II), SeoulKorea,pp.167-176.
1996.Chen, K.-J., Luo, C.-C., Gao, Z.-M., Chang, M.-C.,Chen, F.-Y., and Chen, C.-J.
The CKIP ChineseTreebank.
In Journ ees ATALA sur les Corpusannot es pour la syntaxe, Talana, Paris VII:pp.85-96.
1999.Chen, K.-J.
et al Building and Using Parsed Corpora.
(Anne Abeill?
ed.
s) KLUWER, Dordrecht.2003.Chen, K.-J and Hsieh Y.-M. Chinese Treebanks andGrammar Extraction.
Proceedings of IJCNLP-2004: pp.560-565.
2004.Chiou, F.-D., Chiang, D. and Palmer, M. FacilitatingTreebank Annotation with a Statistical Parser.Proceedings of the Human Language Technol-ogy Conference (HLT 2001), San Diego, Cali-fornia, 2001.CKIP  portal.http://rocling.iis.sinica.edu.tw/CKIP/treebank.htmCKIP (Chinese Knowledge Information ProcessingGroup).
Technical Report no.
95-02, the contentand illustration of Sinica corpus of AcademiaSinica.
Institute of Information Science, Aca-demia Sinica.
1995.CKIP (Chinese Knowledge Information ProcessingGroup).
A study of Chinese Word Boundariesand Segmentation Standard for Informationprocessing (in Chinese).
Technical Report, Tai-wan, Taipei, Academia Sinica.
1996.Gao, J.-F., Goodman, J., Li, M.-J., and Lee, K.-F.Toward a Unified Approach to Statistical Lan-guage Modeling for Chinese, ACM Transac-tions on Asian language Information processing.1(1): pp.23-33.
2002.29Huang, Chu-Ren, Chen, K.-J., Chen, F.-Y., Gao, Z.-M. and Chen, K.-Y.
Sinica Treebank: DesignCriteria, Annotation Guidelines, and On-line In-terface.
Proceedings of 2nd Chinese LanguageProcessing Workshop (Held in conjunction withthe 38th Annual Meeting of the Association forComputational Linguistics, ACL-2000).
Octo-ber 7, 2000, Hong Kong: pp.29-37.
2000.Johnny Bigert, Jonas Sj?bergh, Ola Knutsson andMagnus Sahlgren.
Unsupervised Evaluation ofParser Robustness.
In Proc.
of CICLing 2005.Mexico City, Mexico.
2005.Knutsson, O., Bigert, J. and Kann, V. A Robust Shal-low Parser for Swedish.
In: Proc.
ofNodalida'03.
Reykavik, Iceland.
2003.Le, Zhang, Lu X.ue-qiang, Shen Yan-na and YaoTian-shun.
A Statistical Approach to ExtractChinese Chunk candidates from Large Corpora.In: Proc.
of ICCPOL-2003.
ShengYang:pp.109-117.
2003.Li, Hongqiao, Huang, C.-N., Gao, Jianfeng and Fan,Xiaozhong.
Chinese chunking with anothertype of spec.
In SIGHAN-2004.
Barcelona: pp.41-48.
2004.Liu Y., Q. Tan, and X. Shen.
Segmentation Standardfor Modern Chinese Information Processingand Automatic Segmentation Methodology.1993.Jorn Veenstra.
Memory-Based Text Chunking.
In:Nikos Fakotakis (ed), Machine learning in hu-man language technology, workshop at ACAI99.
Chania, Greece.
1999.Lance A. Ramshaw and Mitchell P. Marcus.
TextChunking Using Transformation-Based Learn-ing.
In: Proc.
of the Third ACL Workshop onVery Large Corpora.
Cambridge MA, USA:pp.82-94.
1995.Manning, C. D. and Schuetze, H. Fundations of Sta-tistical Natural Language Processing, MITPress: pp.191-220., 1999.Menzel, W. Robust processing of natural language.
InProc.
19th Annual German Conference on Arti-ficial Intelligence, Berlin.
Springer: pp.19?34.1995.Munoz, M., V. Punyakanok, D. Roth, and D. Zimak.A learning approach to shallow parsing.
Tech-nical Report UIUCDCS-R-99-2087, UIUCComputer Science Department.
1999.Oliver Streiter.
Memory-based Parsing: EnhancingRecursive Top-down Fuzzy Match with Bot-tom-up Chunking.
ICCPOL 2001, Seoul.
2001.PCTB  portal.http://www.cis.upenn.edu/~chinese/ctb.htmlRuifeng Xu, Qin Lu, Yin Li and Wanyin Li.
TheConstruction of a Chinese Shallow Treebank.
In:Proc.
of 3rd ACL SIGHAN Workshop.
Barce-lona: pp.94-101.
2004.Sang, Erik F. Tjong Kim and Buchholz, Sabine.
In-troduction to the CoNLL-2000 Shared Task:Chunking.
In: Proc.
of CoNLL-2000 and LLL-2000.
Lisbon, Portugal: pp.127-132.
2000.Stolcke, A. SRILM - An Extensible Lan-guage Mod-eling Toolkit, in Proc.
Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.
2003.Xia, Fei, Palmer, M., Xue, N., Okurowski, M.E.,Kovarik, J., Chiou, F.-D., Huang, S., Kroch, T.and Marcus, M. Developing Guidelines and En-suring Consistency for Chinese Text Annotation.In: Proc.
of LREC-2000.
Greece.
2000.Xue, N., Chiou, F. and M. Palmer.
Building a Large-Scale Annotated Chinese Corpus, In: Proc.
ofCOLING-2002.
Taipei, Taiwan.
2002.Xue, N. and Palmer, M. Annotating Propositions inthe Penn Chinese Treebank, In Proceedings ofthe 2nd SIGHAN Workshop on Chinese Lan-guage Processing, in conjunction with ACL'03.Sapporo, Japan.
2003.Xue., N. and M. Palmer.
Automatic Semantic RoleLabeling for Chinese Verbs, in Proceedings ofthe 19th International Joint Conference on Arti-ficial Intelligence.
Edinburgh, Scotland.
2005Xue, N., Xia, F., Chiou F.-D. and M. Palmer.
ThePenn Chinese TreeBank: Phrase Structure An-notation of a Large Corpus.
Natural LanguageEngineering, 11(2)-207.
2005.You Jia-Ming and Chen, K.-J.
Automatic SemanticRole Assignment for a Tree Structure.
Proceed-ings of SIGHAN workshop.
pp.109-115.
2004.Zhou, G.-D. and Su, J.
A Chinese Efficient AnalyserIntegrating Word Segmentation, Part-Of-SpeechTagging, Partial Parsing and Full Parsing.
ACLSecond SIGHAN Workshop on Chinese Lan-guage Processing.
pp.78-83.
2003.30
