Coling 2008: Companion volume ?
Posters and Demonstrations, pages 19?22Manchester, August 2008Phrasal Segmentation Models for Statistical Machine TranslationGraeme Blackwood, Adri`a de Gispert, William ByrneMachine Intelligence LaboratoryDepartment of Engineering, Cambridge UniversityTrumpington Street, Cambridge, CB2 1PZ, U.K.{gwb24|ad465|wjb31}@cam.ac.ukAbstractPhrasal segmentation models define amapping from the words of a sentenceto sequences of translatable phrases.
Wediscuss the estimation of these modelsfrom large quantities of monolingual train-ing text and describe their realization asweighted finite state transducers for incor-poration into phrase-based statistical ma-chine translation systems.
Results are re-ported on the NIST Arabic-English trans-lation tasks showing significant comple-mentary gains in BLEU score with large5-gram and 6-gram language models.1 IntroductionIn phrase-based statistical machine transla-tion (Koehn et al, 2003) phrases extracted fromword-aligned parallel data are the fundamentalunit of translation.
Each phrase is a sequenceof contiguous translatable words and there is noexplicit model of syntax or structure.Our focus is the process by which a string ofwords is segmented as a sequence of such phrases.Ideally, the segmentation process captures two as-pects of natural language.
Firstly, segmentationsshould reflect the underlying grammatical sentencestructure.
Secondly, common sequences of wordsshould be grouped as phrases in order to preservecontext and respect collocations.
Although theseaspects of translation are not evaluated explicitly,phrases have been found very useful in transla-tion.
They have the advantage that, within phrases,words appear as they were found in fluent text.However, reordering of phrases in translation canlead to disfluencies.
By defining a distribution overpossible segmentations, we hope to address suchc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.disfluencies.
A strength of our approach is that itexploits abundantly available monolingual corporathat are usually only used for training word lan-guage models.Most prior work on phrase-based statistical lan-guage models concerns the problem of identifyinguseful phrasal units.
In (Ries et al, 1996) an iter-ative algorithm selectively merges pairs of wordsas phrases with the goal of minimising perplex-ity.
Several criteria including word pair frequen-cies, unigram and bigram log likelihoods, and acorrelation coefficient related to mutual informa-tion are compared in (Kuo and Reichl, 1999).
Themain difference between these approaches and thework described here is that we already have a defi-nition of the phrases of interest (i.e.
the phrases ofthe phrase table extracted from parallel text) andwe focus instead on estimating a distribution overthe set of possible alternative segmentations of thesentence.2 Phrasal Segmentation ModelsUnder the generative model of phrase-based statis-tical machine translation, a source sentence sI1gen-erates sequences uK1= u1, .
.
.
, uKof source lan-guage phrases that are to be translated.
Sentencescannot be segmented into phrases arbitrarily: thespace of possible segmentations is constrained bythe contents of the phrase table which consists ofphrases found with translations in the parallel text.We start initially with a distribution in which seg-mentations assume the following dependencies:P (uK1,K|sI1) = P (uK1|K, sI1)P (K|I).
(1)The distribution over the number of phrases K ischosen to be uniform, i.e.
P (K|I) = 1/I, K ?
{1, 2, .
.
.
, I}, and all segmentations are consideredequally likely.
The probability of a particular seg-mentation is thereforeP (uK1|K, sI1) ={C(K, sI1) if uK1= sI10 otherwise(2)19where C(K, sI1) is chosen to ensure normalisationand the phrases u1, .
.
.
, uKare found in the phrasetable.
This simple model of segmentation has beenfound useful in practice (Kumar et al, 2006).Our goal is to improve upon the uniform seg-mentation of equation (2) by estimating the phrasalsegmentation model parameters from naturally oc-curing phrase sequences in a large monolingualtraining corpus.
An order-n phrasal segmentationmodel assigns a probability to a phrase sequenceuK1according toP (uK1|K, sI1) =K?k=1P (uk|uk?11,K, sI1) ?
{C(K, sI1)?Kk=1P (uk|uk?1k?n+1) if uK1= sI10 otherwise(3)where the approximation is due to the Markov as-sumption that only the most recent n ?
1 phrasesare useful when predicting the next phrase.
Again,each ukmust be a phrase with a known transla-tion.
For a fixed sentence sI1, the normalisationterm C(K, sI1) can be calculated.
In translation,however, calculating this quantity becomes hardersince the sI1are not fixed.
We therefore ignorethe normalisation and use the unnormalised like-lihoods as scores.2.1 Parameter EstimationWe focus on first-order phrasal segmentation mod-els.
Although we have experimented with higher-order models we have not yet found them to yieldimproved translation.Let f(uk?1, uk) be the frequency of occurrenceof a string of words wjiin a very large trainingcorpus that can be split at position x such thati < x ?
j and the substrings wx?1iand wjxmatchprecisely the words of two phrases uk?1and ukinthe phrase table.
The maximum likelihood proba-bility estimate for phrase bigrams is then their rel-ative frequency:?P (uk|uk?1) =f(uk?1, uk)f(uk?1).
(4)These maximum likelihood estimates are dis-counted and smoothed with context-dependentbackoff such thatP (uk|uk?1) ={?
(uk?1, uk)?P (uk|uk?1) if f(uk?1, uk) > 0?
(uk?1)P (uk) otherwise(5)where ?
(uk?1, uk) discounts the maximum like-lihood estimates and the context-specific backoffweights ?
(uk?1) are chosen to ensure normalisa-tion.3 The Transducer Translation ModelThe Transducer Translation Model (TTM) (Kumarand Byrne, 2005; Kumar et al, 2006) is a gener-ative model of translation that applies a series oftransformations specified by conditional probabil-ity distributions and encoded as Weighted FiniteState Transducers (Mohri et al, 2002).The generation of a target language sentencetJ1starts with the generation of a source lan-guage sentence sI1by the source language modelPG(sI1).
Next, the source language sentence issegmented according to the uniform phrasal seg-mentation model distribution PW(uK1,K|sI1) ofequation (2).
The phrase translation and reorder-ing model P?
(vR1|uK1) generates the reordered se-quence of target language phrases vR1.
Finally,the reordered target language phrases are trans-formed to word sequences tJ1under the targetsegmentation model P?(tJ1|vR1).
These compo-nent distributions together form a joint distribu-tion over the source and target language sentencesand their possible intermediate phrase sequencesas P (tJ1, vR1, uK1, sI1).In translation under the generative model, westart with the target sentence tJ1in the foreign lan-guage and search for the best source sentence s?I1.Encoding each distribution as a WFST leads to amodel of translation as a series of compositionsL = G ?W ?
?
?
?
?
T (6)in which T is an acceptor for the target languagesentence and L is the word lattice of translationsobtained during decoding.
The most likely trans-lation s?I1is the path in L with least cost.The above approach generates a word lattice Lunder the unweighted phrasal segmentation modelof equation (2).
In the initial experiments reportedhere, we apply the weighted phrasal segmentationmodel via lattice rescoring.
We take the word lat-tice L and compose it with the unweighted trans-ducer W to obtain a lattice of phrases L ?W ; thislattice contains phrase sequences and translationscores consistent with the initial translation.
Wealso extract the complete list of phrases relevant toeach translation.20We then wish to apply the phrasal segmentationmodel distribution of equation (3) to this phraselattice.
The conditional probabilities and backoffstructure defined in equation (5) can be encodedas a weighted finite state acceptor (Allauzen et al,2003).
In this acceptor, ?, states encode historiesand arcs define the bigram and backed-off unigramphrase probabilities.
We note that the raw countsof equation (4) are collected prior to translationand the first-order probabilities are estimated onlyfor phrases found in the lattice.The phrasal segmentation model is composedwith the phrase lattice and projected on the in-put to obtain the rescored word lattice L?=(L ?W ) ??.
The most likely translation after ap-plying the phrasal segmentation model is found asthe path in L?with least cost.
Apart from likeli-hood pruning when generating the original wordlattice, the model scores are included correctly intranslation search.4 System DevelopmentWe describe experiments on the NIST Arabic-English machine translation task and apply phrasalsegmentation models in lattice rescoring.The development set mt02-05-tune is formedfrom the odd numbered sentences of the NISTMT02?MT05 evaluation sets; the even numberedsentences form the validation set mt02-05-test.Test performance is evaluated using the NIST sub-sets from the MT06 evaluation: mt06-nist-nw fornewswire data and mt06-nist-ng for newsgroupdata.
Results are also reported for the MT08 evalu-ation.
Each set contains four references and BLEUscores are computed for lower-case translations.The uniformly segmented TTM baseline systemis trained using all of the available Arabic-Englishdata for the NIST MT08 evaluation1.
In first-passtranslation, decoding proceeds with a 4-gram lan-guage model estimated over the parallel text and a965 million word subset of monolingual data fromthe English Gigaword Third Edition.
Minimumerror training (Och, 2003) under BLEU optimisesthe decoder feature weights using the developmentset mt02-05-tune.
In the second pass, 5-gram and6-gram zero-cutoff stupid-backoff (Brants et al,2007) language models estimated using 4.7 billionwords of English newswire text are used to gener-ate lattices for phrasal segmentation model rescor-ing.
The phrasal segmentation model parameters1http://www.nist.gov/speech/tests/mt/2008/mt02-05-tune mt02-05-testTTM+MET 48.9 48.6+6g 51.9 51.7+6g+PSM 52.7 52.7Table 2: BLEU scores for phrasal segmentationmodel rescoring of 6-gram rescored lattices.are trained using a 1.8 billion word subset of thesame monolingual training data used to build thesecond-pass word language model.
A phrasal seg-mentation model scale factor and phrase insertionpenalty are tuned using the development set.5 Results and AnalysisFirst-pass TTM translation lattices generated witha uniform segmentation obtain baseline BLEUscores of 48.9 for mt02-05-tune and 48.6 formt02-05-test.
In our experiments we demon-strate that phrasal segmentation models continueto improve translation even for second-pass lat-tices rescored with very large zero-cutoff higher-order language models.
Table 1 shows phrasal seg-mentation model rescoring of 5-gram lattices.
Thephrasal segmentation models consistently improvethe BLEU score: +1.1 for both the developmentand validation sets, and +1.4 and +0.4 for the in-domain newswire and out-of-domain newsgrouptest sets.
Rescoring MT08 gives gains of +0.9 onmt08-nist-nw and +0.3 on mt08-nist-ng.For a limited quantity of training data it is notalways possible to improve translation quality sim-ply by increasing the order of the language model.Comparing tables 1 and 2 shows that the gains inmoving from a 5-gram to a 6-gram are small.
Evensetting aside the practical difficulty of estimatingand applying such higher-order language models,it is doubtful that further gains could be had simplyby increasing the order.
That the phrasal segmenta-tion models continue to improve upon the 6-gramlattice scores suggests they capture more than justa longer context and that they are complementaryto word-based language models.The role of the phrase insertion penalty is toencourage longer phrases in translation.
Table 3shows the effect of tuning this parameter.
Theupper part of the table shows the BLEU score,brevity penalty and individual n-gram precisions.The lower part shows the total number of wordsin the output, the number of words translated asa phrase of the specified length, and the averagenumber of words per phrase.
When the insertion21mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08-nist-nw mt08-nist-ngTTM+MET 48.9 48.6 46.1 35.2 48.4 33.7+5g 51.5 51.5 48.4 36.7 49.1 36.4+5g+PSM 52.6 52.6 49.8 37.1 50.0 36.7Table 1: BLEU scores for phrasal segmentation model rescoring of 5-gram rescored lattices.PIP -4.0 -2.0 0.0 2.0 4.0BLEU 48.6 50.1 51.1 49.9 48.7BP 0.000 0.000 0.000 -0.034 -0.0721g 82.0 83.7 84.9 85.7 86.22g 57.3 58.9 59.9 60.5 61.13g 40.8 42.2 43.1 43.6 44.24g 29.1 30.3 31.1 31.5 32.0words 70550 66964 63505 60847 586761 58840 46936 25040 15439 117442 7606 12388 18890 19978 188863 2691 4890 11532 13920 142954 860 1820 5016 6940 80085 240 450 1820 2860 35006+ 313 480 1207 1710 2243w/p 1.10 1.21 1.58 1.86 2.02Table 3: Effect of phrase insertion penalty (PIP)on BLEU score, brevity penalty (BP), individualn-gram precisions, phrase length distribution, andaverage words per phrase (w/p) for mt02-05-tune.penalty is too low, single word phrases dominatethe output and any benefits from longer context orphrase-internal fluency are lost.
As the phrase in-sertion penalty increases, there are large gains inprecision at each order and many longer phrasesappear in the output.
At the optimal phrase in-sertion penalty, the average phrase length is 1.58words and over 60% of the translation output isgenerated from multi-word phrases.6 DiscussionWe have defined a simple model of the phrasal seg-mentation process for phrase-based SMT and esti-mated the model parameters from naturally occur-ring phrase sequence examples in a large trainingcorpus.
Applying first-order models to the NISTArabic-English machine translation task, we havedemonstrated complementary improved transla-tion quality through exploitation of the same abun-dantly available monolingual data used for trainingregular word-based language models.Comparing the in-domain newswire and out-of-domain newsgroup test set performance showsthe importance of choosing appropriate data fortraining the phrasal segmentation model param-eters.
When in-domain data is of limited avail-ability, count mixing or other adaptation strategiesmay lead to improved performance.AcknowledgementsThis work was supported in part under theGALE program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0011-06-C-0022.ReferencesAllauzen, Cyril, Mehryar Mohri, and Brian Roark.2003.
Generalized algorithms for constructing sta-tistical language models.
In Proceedings of the 41stMeeting of the Association for Computational Lin-guistics, pages 557?564.Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large languagemodels in machine translation.
In Proceedings ofthe 2007 Joint Conference on EMNLP and CoNLL,pages 858?867.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference for ComputationalLinguistics on Human Language Technology, pages48?54, Morristown, NJ, USA.Kumar, Shankar and William Byrne.
2005.
Lo-cal phrase reordering models for statistical machinetranslation.
In Proceedings of the conference on HLTand EMNLP, pages 161?168.Kumar, Shankar, Yonggang Deng, and William Byrne.2006.
A weighted finite state transducer translationtemplate model for statistical machine translation.Natural Language Engineering, 12(1):35?75.Kuo, Hong-Kwang Jeff and Wolfgang Reichl.
1999.Phrase-based language models for speech recogni-tion.
In Sixth European Conference on Speech Com-munication and Technology, pages 1595?1598.Mohri, Mehryar, Fernando Pereira, and Michael Riley.2002.
Weighted finite-state transducers in speechrecognition.
In Computer Speech and Language,volume 16, pages 69?88.Och, Franz Josef.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, pages 160?167, Morristown, NJ, USA.Ries, Klaus, Finn Dag Bu, and Alex Waibel.
1996.Class phrase models for language modeling.
In Pro-ceedings of the 4th International Conference on Spo-ken Language Processing.22
