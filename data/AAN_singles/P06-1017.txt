Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 129?136,Sydney, July 2006. c?2006 Association for Computational LinguisticsRelation Extraction Using Label Propagation Based Semi-supervisedLearningJinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu11Institute for Infocomm Research 2Department of Computer Science21 Heng Mui Keng Terrace National University of Singapore119613 Singapore 117543 Singapore{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sgAbstractShortage of manually labeled data is anobstacle to supervised relation extractionmethods.
In this paper we investigate agraph based semi-supervised learning al-gorithm, a label propagation (LP) algo-rithm, for relation extraction.
It representslabeled and unlabeled examples and theirdistances as the nodes and the weights ofedges of a graph, and tries to obtain a la-beling function to satisfy two constraints:1) it should be fixed on the labeled nodes,2) it should be smooth on the whole graph.Experiment results on the ACE corpusshowed that this LP algorithm achievesbetter performance than SVM when onlyvery few labeled examples are available,and it also performs better than bootstrap-ping for the relation extraction task.1 IntroductionRelation extraction is the task of detecting andclassifying relationships between two entities fromtext.
Many machine learning methods have beenproposed to address this problem, e.g., supervisedlearning algorithms (Miller et al, 2000; Zelenko etal., 2002; Culotta and Soresen, 2004; Kambhatla,2004; Zhou et al, 2005), semi-supervised learn-ing algorithms (Brin, 1998; Agichtein and Gravano,2000; Zhang, 2004), and unsupervised learning al-gorithms (Hasegawa et al, 2004).Supervised methods for relation extraction per-form well on the ACE Data, but they require a largeamount of manually labeled relation instances.
Un-supervised methods do not need the definition ofrelation types and manually labeled data, but theycannot detect relations between entity pairs and itsresult cannot be directly used in many NLP taskssince there is no relation type label attached toeach instance in clustering result.
Considering boththe availability of a large amount of untagged cor-pora and direct usage of extracted relations, semi-supervised learning methods has received great at-tention.DIPRE (Dual Iterative Pattern Relation Expan-sion) (Brin, 1998) is a bootstrapping-based sys-tem that used a pattern matching system as clas-sifier to exploit the duality between sets of pat-terns and relations.
Snowball (Agichtein and Gra-vano, 2000) is another system that used bootstrap-ping techniques for extracting relations from un-structured text.
Snowball shares much in commonwith DIPRE, including the employment of the boot-strapping framework as well as the use of patternmatching to extract new candidate relations.
Thethird system approaches relation classification prob-lem with bootstrapping on top of SVM, proposed byZhang (2004).
This system focuses on the ACE sub-problem, RDC, and extracts various lexical and syn-tactic features for the classification task.
However,Zhang (2004)?s method doesn?t actually ?detect?
re-laitons but only performs relation classification be-tween two entities given that they are known to berelated.Bootstrapping works by iteratively classifying un-labeled examples and adding confidently classifiedexamples into labeled data using a model learnedfrom augmented labeled data in previous iteration.
It129can be found that the affinity information among un-labeled examples is not fully explored in this boot-strapping process.Recently a promising family of semi-supervisedlearning algorithm is introduced, which can effec-tively combine unlabeled data with labeled data inlearning process by exploiting manifold structure(cluster structure) in data (Belkin and Niyogi, 2002;Blum and Chawla, 2001; Blum et al, 2004; Zhuand Ghahramani, 2002; Zhu et al, 2003).
Thesegraph-based semi-supervised methods usually de-fine a graph where the nodes represent labeled andunlabeled examples in a dataset, and edges (may beweighted) reflect the similarity of examples.
Thenone wants a labeling function to satisfy two con-straints at the same time: 1) it should be close to thegiven labels on the labeled nodes, and 2) it should besmooth on the whole graph.
This can be expressedin a regularization framework where the first termis a loss function, and the second term is a regu-larizer.
These methods differ from traditional semi-supervised learning methods in that they use graphstructure to smooth the labeling function.To the best of our knowledge, no work has beendone on using graph based semi-supervised learningalgorithms for relation extraction.
Here we inves-tigate a label propagation algorithm (LP) (Zhu andGhahramani, 2002) for relation extraction task.
Thisalgorithm works by representing labeled and unla-beled examples as vertices in a connected graph,then propagating the label information from any ver-tex to nearby vertices through weighted edges itera-tively, finally inferring the labels of unlabeled exam-ples after the propagation process converges.
In thispaper we focus on the ACE RDC task1.The rest of this paper is organized as follows.
Sec-tion 2 presents related work.
Section 3 formulatesrelation extraction problem in the context of semi-supervised learning and describes our proposed ap-proach.
Then we provide experimental results of ourproposed method and compare with a popular su-pervised learning algorithm (SVM) and bootstrap-ping algorithm in Section 4.
Finally we concludeour work in section 5.1 http://www.ldc.upenn.edu/Projects/ACE/, Three tasks ofACE program: Entity Detection and Tracking (EDT), Rela-tion Detection and Characterization (RDC), and Event Detec-tion and Characterization (EDC)2 The Proposed Method2.1 Problem DefinitionThe problem of relation extraction is to assign an ap-propriate relation type to an occurrence of two entitypairs in a given context.
It can be represented as fol-lows:R ?
(Cpre, e1, Cmid, e2, Cpost) (1)where e1 and e2 denote the entity mentions, andCpre,Cmid,and Cpost are the contexts before, be-tween and after the entity mention pairs.
In this pa-per, we set the mid-context window as the words be-tween the two entity mentions and the pre- and post-context as up to two words before and after the cor-responding entity mention.Let X = {xi}ni=1 be a set of contexts of occur-rences of all the entity mention pairs, where xi rep-resents the contexts of the i-th occurrence, and n isthe total number of occurrences.
The first l exam-ples (or contexts) are labeled as yg ( yg ?
{rj}Rj=1,rj denotes relation type and R is the total number ofrelation types).
The remaining u(u = n ?
l) exam-ples are unlabeled.Intuitively, if two occurrences of entity mentionpairs have the similarity context, they tend to holdthe same relation type.
Based on the assumption, wedefine a graph where the vertices represent the con-texts of labeled and unlabeled occurrences of entitymention pairs, and the edge between any two ver-tices xi and xj is weighted so that the closer the ver-tices in some distance measure, the larger the weightassociated with this edge.
Hence, the weights are de-fined as follows:Wij = exp(?s2ij?2 ) (2)where sij is the similarity between xi and xj calcu-lated by some similarity measures, e.g., cosine sim-ilarity, and ?
is used to scale the weights.
In thispaper, we set ?
as the average similarity between la-beled examples from different classes.2.2 A Label Propagation AlgorithmIn the LP algorithm, the label information of anyvertex in a graph is propagated to nearby verticesthrough weighted edges until a global stable stage isachieved.
Larger edge weights allow labels to travel130through easier.
Thus the closer the examples are, themore likely they have similar labels.We define soft label as a vector that is a proba-bilistic distribution over all the classes.
In the la-bel propagation process, the soft label of each initiallabeled example is clamped in each iteration to re-plenish label sources from these labeled data.
Thusthe labeled data act like sources to push out labelsthrough unlabeled data.
With this push from la-beled examples, the class boundaries will be pushedthrough edges with large weights and settle in gapsalong edges with small weights.
Hopefully, the val-ues of Wij across different classes would be as smallas possible and the values of Wij within the sameclass would be as large as possible.
This will makelabel propagation to stay within the same class.
Thislabel propagation process will make the labelingfunction smooth on the graph.Define an n?
n probabilistic transition matrix TTij = P (j ?
i) = wij?nk=1 wkj(3)where Tij is the probability to jump from vertex xjto vertex xi.
We define a n ?
R label matrix Y ,where Yij representing the probabilities of vertex yito have the label rj .Then the label propagation algorithm consists thefollowing main steps:Step1 : Initialization?
Set the iteration index t = 0;?
Let Y 0 be the initial soft labels attached toeach vertex, where Y 0ij = 1 if yi is label rjand 0 otherwise.?
Let Y 0L be the top l rows of Y 0 and Y 0Ube the remaining u rows.
Y 0L is consistentwith the labeling in labeled data and theinitialization of Y 0U can be arbitrary.Step 2 : Propagate the labels of any vertex tonearby vertices by Y t+1 = TY t , whereT is the row-normalized matrix of T , i.e.Tij = Tij/?k Tik, which can maintain theclass probability interpretation.Step 3 : Clamp the labeled data, that is, replace thetop l row of Y t+1 with Y 0L .Step 4 : Repeat from step 2 until Y converges.Step 5 : Assign xh(l + 1 ?
h ?
n) with a label:yh = argmaxjYhj .The above algorithm can ensure that the labeleddata YL never changes since it is clamped in Step 3.Actually we are interested in only YU .
This algo-rithm has been shown to converge to a unique solu-tion Y?U = limt??
Y tU = (I ?
T?uu)?1T?ulY 0L (Zhuand Ghahramani, 2002).
Here, T?uu and T?ul are ac-quired by splitting matrix T?
after the l-th row andthe l-th column into 4 sub-matrices.
And I is u?
uidentity matrix.
We can see that the initialization ofY 0U in this solution is not important, since Y 0U doesnot affect the estimation of Y?U .3 Experiments and Results3.1 Feature SetFollowing (Zhang, 2004), we used lexical and syn-tactic features in the contexts of entity pairs, whichare extracted and computed from the parse trees de-rived from Charniak Parser (Charniak, 1999) and theChunklink script 2 written by Sabine Buchholz fromTilburg University.Words: Surface tokens of the two entities andwords in the three contexts.Entity Type: the entity type of both entity men-tions, which can be PERSON, ORGANIZA-TION, FACILITY, LOCATION and GPE.POS features: Part-Of-Speech tags correspondingto all tokens in the two entities and words inthe three contexts.Chunking features: This category of features areextracted from the chunklink representation,which includes:?
Chunk tag information of the two enti-ties and words in the three contexts.
The?0?
tag means that the word is not in anychunk.
The ?I-XP?
tag means that thisword is inside an XP chunk.
The ?B-XP?by default means that the word is at thebeginning of an XP chunk.?
Grammatical function of the two enti-ties and words in the three contexts.
The2Software available at http://ilk.uvt.nl/?sabine/chunklink/131last word in each chunk is its head, andthe function of the head is the function ofthe whole chunk.
?NP-SBJ?
means a NPchunk as the subject of the sentence.
Theother words in a chunk that are not thehead have ?NOFUNC?
as their function.?
IOB-chains of the heads of the two enti-ties.
So-called IOB-chain, noting the syn-tactic categories of all the constituents onthe path from the root node to this leafnode of tree.The position information is also specified in thedescription of each feature above.
For example,word features with position information include:1) WE1 (WE2): all words in e1 (e2)2) WHE1 (WHE2): head word of e1 (e2)3) WMNULL: no words in Cmid4) WMFL: the only word in Cmid5) WMF, WML, WM2, WM3, ...: first word, lastword, second word, third word, ...in Cmid when atleast two words in Cmid6) WEL1, WEL2, ...: first word, second word, ...before e17) WER1, WER2, ...: first word, second word, ...after e2We combine the above lexical and syntactic featureswith their position information in the contexts toform context vectors.
Before that, we filter out lowfrequency features which appeared only once in thedataset.3.2 Similarity MeasuresThe similarity sij between two occurrences of entitypairs is important to the performance of the LP al-gorithm.
In this paper, we investigated two similar-ity measures, cosine similarity measure and Jensen-Shannon (JS) divergence (Lin, 1991).
Cosine sim-ilarity is commonly used semantic distance, whichmeasures the angle between two feature vectors.
JSdivergence has ever been used as distance measurefor document clustering, which outperforms cosinesimilarity based document clustering (Slonim et al,2002).
JS divergence measures the distance betweentwo probability distributions if feature vector is con-sidered as probability distribution over features.
JSdivergence is defined as follows:Table 1: Frequency of Relation SubTypes in the ACE trainingand devtest corpus.Type SubType Training DevtestROLE General-Staff 550 149Management 677 122Citizen-Of 127 24Founder 11 5Owner 146 15Affiliate-Partner 111 15Member 460 145Client 67 13Other 15 7PART Part-Of 490 103Subsidiary 85 19Other 2 1AT Located 975 192Based-In 187 64Residence 154 54SOC Other-Professional 195 25Other-Personal 60 10Parent 68 24Spouse 21 4Associate 49 7Other-Relative 23 10Sibling 7 4GrandParent 6 1NEAR Relative-Location 88 32JS(q, r) = 12 [DKL(q?p?)
+DKL(r?p?)]
(4)DKL(q?p?)
=?yq(y)(log q(y)p?
(y) ) (5)DKL(r?p?)
=?yr(y)(log r(y)p?
(y) ) (6)where p?
= 12(q + r) and JS(q, r) represents JSdivergence between probability distribution q(y) andr(y) (y is a random variable), which is defined interms of KL-divergence.3.3 Experimental Evaluation3.3.1 Experiment SetupWe evaluated this label propagation based rela-tion extraction method for relation subtype detectionand characterization task on the official ACE 2003corpus.
It contains 519 files from sources includingbroadcast, newswire, and newspaper.
We dealt withonly intra-sentence explicit relations and assumedthat all entities have been detected beforehand in theEDT sub-task of ACE.
Table 1 lists the types andsubtypes of relations for the ACE Relation Detectionand Characterization (RDC) task, along with their132Table 2: The Performance of SVM and LP algorithm with different sizes of labeled data for relation detection on relation subtypes.The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.SVM LPCosine LPJSPercentage P R F P R F P R F1% 35.9 32.6 34.4 58.3 56.1 57.1 58.5 58.7 58.510% 51.3 41.5 45.9 64.5 57.5 60.7 64.6 62.0 63.225% 67.1 52.9 59.1 68.7 59.0 63.4 68.9 63.7 66.150% 74.0 57.8 64.9 69.9 61.8 65.6 70.1 64.1 66.975% 77.6 59.4 67.2 71.8 63.4 67.3 72.4 64.8 68.3100% 79.8 62.9 70.3 73.9 66.9 70.2 74.2 68.2 71.1Table 3: The performance of SVM and LP algorithm with different sizes of labeled data for relation detection and classificationon relation subtypes.
The LP algorithm is run with two similarity measures: cosine similarity and JS divergence.SVM LPCosine LPJSPercentage P R F P R F P R F1% 31.6 26.1 28.6 39.6 37.5 38.5 40.1 38.0 39.010% 39.1 32.7 35.6 45.9 39.6 42.5 46.2 41.6 43.725% 49.8 35.0 41.1 51.0 44.5 47.3 52.3 46.0 48.950% 52.5 41.3 46.2 54.1 48.6 51.2 54.9 50.8 52.775% 58.7 46.7 52.0 56.0 52.0 53.9 56.1 52.6 54.3100% 60.8 48.9 54.2 56.2 52.3 54.1 56.3 52.9 54.6frequency of occurrence in the ACE training set andtest set.
We constructed labeled data by randomlysampling some examples from ACE training dataand additionally sampling examples with the samesize from the pool of unrelated entity pairs for the?NONE?
class.
We used the remaining examples inthe ACE training set and the whole ACE test set asunlabeled data.
The testing set was used for finalevaluation.3.3.2 LP vs. SVMSupport Vector Machine (SVM) is a state of theart technique for relation extraction task.
In this ex-periment, we use LIBSVM tool 3 with linear kernelfunction.For comparison between SVM and LP, we ranSVM and LP with different sizes of labeled dataand evaluate their performance on unlabeled datausing precision, recall and F-measure.
Firstly, weran SVM or LP algorithm to detect possible rela-tions from unlabeled data.
If an entity mention pairis classified not to the ?NONE?
class but to the other24 subtype classes, then it has a relation.
Then con-struct labeled datasets with different sampling setsize l, including 1%?Ntrain, 10%?Ntrain, 25%?Ntrain, 50%?Ntrain, 75%?Ntrain, 100%?Ntrain(Ntrain is the number of examples in the ACE train-3LIBSVM : a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.ing set).
If any relation subtype was absent from thesampled labeled set, we redid the sampling.
For eachsize, we performed 20 trials and calculated averagescores on test set over these 20 random trials.Table 2 reports the performance of SVM and LPwith different sizes of labled data for relation detec-tion task.
We used the same sampled labeled data inLP as the training data for SVM model.From Table 2, we see that both LPCosine andLPJS achieve higher Recall than SVM.
Specifically,with small labeled dataset (percentage of labeleddata ?
25%), the performance improvement by LPis significant.
When the percentage of labeled dataincreases from 50% to 100%, LPCosine is still com-parable to SVM in F-measure while LPJS achievesslightly better F-measure than SVM.
On the otherhand, LPJS consistently outperforms LPCosine.Table 3 reports the performance of relation clas-sification by using SVM and LP with different sizesof labled data.
And the performance describes theaverage values of Precision, Recall and F-measureover major relation subtypes.From Table 3, we see that LPCosine and LPJS out-perform SVM by F-measure in almost all settingsof labeled data, which is due to the increase of Re-call.
With smaller labeled dataset (percentage of la-beled data ?
50%), the gap between LP and SVMis larger.
When the percentage of labeled data in-1330.250.30.350.40.450.50.550.61% 10% 25% 50% 75% 100%Percentage of Labeled ExamplesF-measure SVMLP_CosineLP_JSFigure 1: Comparison of the performance of SVMand LP with different sizes of labeled datacreases from 75% to 100%, the performance of LPalgorithm is still comparable to SVM.
On the otherhand, the LP algorithm based on JS divergence con-sistently outperforms the LP algorithm based on Co-sine similarity.
Figure 1 visualizes the accuracy ofthree algorithms.As shown in Figure 1, the gap between SVMcurve and LPJS curves is large when the percentageof labeled data is relatively low.3.3.3 An ExampleIn Figure 2, we selected 25 instances in train-ing set and 15 instances in test set from the ACEcorpus,which covered five relation types.
UsingIsomap tool 4, the 40 instances with 229 feature di-mensions are visualized in a two-dimensional spaceas the figure.
We randomly sampled only one la-beled example for each relation type from the 25training examples as labeled data.
Figure 2(a) and2(b) show the initial state and ground truth result re-spectively.
Figure 2(c) reports the classification re-sult on test set by SVM (accuracy = 415 = 26.7%),and Figure 2(d) gives the classification result on bothtraining set and test set by LP (accuracy = 1115 =73.3%).Comparing Figure 2(b) and Figure 2(c), we findthat many examples are misclassified from class ?to other class symbols.
This may be caused thatSVMs method ignores the intrinsic structure in data.For Figure 2(d), the labels of unlabeled examplesare determined not only by nearby labeled examples,but also by nearby unlabeled examples, so using LP4The tool is available at http://isomap.stanford.edu/.                    Figure 2: An example: comparison of SVM and LPalgorithm on a data set from ACE corpus.
?
and4 denote the unlabeled examples in training set andtest set respectively, and other symbols (?,?,2,+and 5) represent the labeled examples with respec-tive relation type sampled from training set.strategy achieves better performance than the localconsistency based SVM strategy when the size oflabeled data is quite small.3.3.4 LP vs. BootstrappingIn (Zhang, 2004), they perform relation classifi-cation on ACE corpus with bootstrapping on top ofSVM.
To compare with their proposed BootstrappedSVM algorithm, we use the same feature stream set-ting and randomly selected 100 instances from thetraining data as the size of initial labeled data.Table 4 lists the performance of the bootstrappedSVM method from (Zhang, 2004) and LP methodwith 100 seed labeled examples for relation typeclassification task.
We can see that LP algorithmoutperforms the bootstrapped SVM algorithm onfour relation type classification tasks, and performcomparably on the relation ?SOC?
classificationtask.4 DiscussionIn this paper,we have investigated a graph-basedsemi-supervised learning approach for relation ex-traction problem.
Experimental results showed thatthe LP algorithm performs better than SVM and134Table 4: Comparison of the performance of the bootstrapped SVM method from (Zhang, 2004) and LP method with 100 seedlabeled examples for relation type classification task.Bootstrapping LPJSRelation type P R F P R FROLE 78.5 69.7 73.8 81.0 74.7 77.7PART 65.6 34.1 44.9 70.1 41.6 52.2AT 61.0 84.8 70.9 74.2 79.1 76.6SOC 47.0 57.4 51.7 45.0 59.1 51.0NEAR ?
?
?
13.7 12.5 13.0Table 5: Comparison of the performance of previous methods on ACE RDC task.Relation Dectection Relation Detection and Classificationon Types on SubtypesMethod P R F P R F P R FCulotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -Kambhatla (2004) Feature based, Maxi-mum Entropy- - - - - - 63.5 45.2 52.8Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5bootstrapping.
We have some findings from theseresults:The LP based relation extraction method can usethe graph structure to smooth the labels of unlabeledexamples.
Therefore, the labels of unlabeled exam-ples are determined not only by the nearby labeledexamples, but also by nearby unlabeled examples.For supervised methods, e.g., SVM, very few la-beled examples are not enough to reveal the struc-ture of each class.
Therefore they can not performwell, since the classification hyperplane was learnedonly from few labeled data and the coherent struc-ture in unlabeled data was not explored when in-ferring class boundary.
Hence, our LP-based semi-supervised method achieves better performance onboth relation detection and classification when onlyfew labeled data is available.
BootstrappingCurrently most of works on the RDC task ofACE focused on supervised learning methods Cu-lotta and Soresen (2004; Kambhatla (2004; Zhouet al (2005).
Table 5 lists a comparison on re-lation detection and classification of these meth-ods.
Zhou et al (2005) reported the best result as63.1%/49.5%/55.5% in Precision/Recall/F-measureon the relation subtype classification using featurebased method, which outperforms tree kernel basedmethod by Culotta and Soresen (2004).
Comparedwith Zhou et al?s method, the performance of LP al-gorithm is slightly lower.
It may be due to that weused a much simpler feature set.
Our work in thispaper focuses on the investigation of a graph basedsemi-supervised learning algorithm for relation ex-traction.
In the future, we would like to use more ef-fective feature sets Zhou et al (2005) or kernel basedsimilarity measure with LP for relation extraction.5 Conclusion and Future WorkThis paper approaches the problem of semi-supervised relation extraction using a label propaga-tion algorithm.
It represents labeled and unlabeledexamples and their distances as the nodes and theweights of edges of a graph, and tries to obtain alabeling function to satisfy two constraints: 1) itshould be fixed on the labeled nodes, 2) it shouldbe smooth on the whole graph.
In the classifica-tion process, the labels of unlabeled examples aredetermined not only by nearby labeled examples,but also by nearby unlabeled examples.
Our exper-imental results demonstrated that this graph basedalgorithm can achieve better performance than SVMwhen only very few labeled examples are available,and also outperforms the bootstrapping method forrelation extraction task.In the future, we would like to investigate moreeffective feature set or use feature selection to im-prove the performance of this graph-based semi-supervised relation extraction method.135ReferencesAgichtein E. and Gravano L.. 2000.
Snowball: Ex-tracting Relations from large Plain-Text Collections,In Proceedings of the 5th ACM International Confer-ence on Digital Libraries (ACMDL?00).Belkin M. and Niyogi P.. 2002.
Using Manifold Struc-ture for Partially Labeled Classification.
Advances inNeural Infomation Processing Systems 15.Blum A. and Chawla S. 2001.
Learning from Labeledand Unlabeled Data Using Graph Mincuts.
In Pro-ceedings of the 18th International Conference on Ma-chine Learning.Blum A., Lafferty J., Rwebangira R. and Reddy R. 2004.Semi-Supervised Learning Using Randomized Min-cuts.
In Proceedings of the 21th International Confer-ence on Machine Learning..Brin Sergey.
1998.
Extracting patterns and relationsfrom world wide web.
In Proceedings of WebDB Work-shop at 6th International Conference on ExtendingDatabase Technology (WebDB?98).
pages 172-183.Charniak E. 1999.
A Maximum-entropy-inspired parser.Technical Report CS-99-12.
Computer Science De-partment, Brown University.Culotta A. and Soresen J.
2004.
Dependency tree kernelsfor relation extraction, In Proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.
21-26 July 2004.
Barcelona, Spain.Hasegawa T., Sekine S. and Grishman R. 2004.
Dis-covering Relations among Named Entities from LargeCorpora, In Proceeding of Conference ACL2004.Barcelona, Spain.Kambhatla N. 2004.
Combining lexical, syntactic andsemantic features with Maximum Entropy Models forextracting relations, In Proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.. 21-26 July 2004.
Barcelona, Spain.Lin J.
1991.
Divergence Measures Based on the Shan-non Entropy.
IEEE Transactions on Information The-ory.
Vol 37, No.1, 145-150.Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.A novel use of statistical parsing to extract informationfrom text.
In Proceedings of 6th Applied Natural Lan-guage Processing Conference 29 April-4 may 2000,Seattle USA.Slonim, N., Friedman, N., and Tishby, N. 2002.
Un-supervised Document Classification Using SequentialInformation Maximization.
In Proceedings of the 25thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval.Yarowsky D. 1995.
Unsupervised Word Sense Disam-biguation Rivaling Supervised Methods.
In Proceed-ings of the 33rd Annual Meeting of the Association forComputational Linguistics.
pp.189-196.Zelenko D., Aone C. and Richardella A.
2002.
Ker-nel Methods for Relation Extraction, Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).
Philadelphia.Zhang Zhu.
2004.
Weakly-supervised relation classifi-cation for Information Extraction, In Proceedings ofACM 13th conference on Information and KnowledgeManagement (CIKM?2004).
8-13 Nov 2004.
Wash-ington D.C.,USA.Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.2005.
Exploring Various Knowledge in Relation Ex-traction.
In Proceedings of 43th Annual Meeting of theAssociation for Computational Linguistics.
USA.Zhu Xiaojin and Ghahramani Zoubin.
2002.
Learningfrom Labeled and Unlabeled Data with Label Propa-gation.
CMU CALD tech report CMU-CALD-02-107.Zhu Xiaojin, Ghahramani Zoubin, and Lafferty J.
2003.Semi-Supervised Learning Using Gaussian Fields andHarmonic Functions.
In Proceedings of the 20th Inter-national Conference on Machine Learning.136
