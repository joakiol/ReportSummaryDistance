Proceedings of NAACL HLT 2009: Tutorials, pages 7?8,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsExtracting World and Linguistic Knowledge fromWikipediaSimone Paolo PonzettoDept.
of Computational LinguisticsUniversity of HeidelbergHeidelberg, Germanyhttp://www.cl.uni-heidelberg.de/?ponzettoMichael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 33Heidelberg, Germanyhttp://www.eml-research.de/?strubeOverviewMany research efforts have been devoted to develop robust statistical modeling techniques for many NLPtasks.
Our field is now moving towards more complex tasks (e.g.
RTE, QA), which require to complementthese methods with a semantically rich representation based on world and linguistic knowledge (i.e.
anno-tated linguistic data).
In this tutorial we show several approaches to extract this knowledge from Wikipedia.This resource has attracted the attention of much work in the AI community, mainly because it providessemi-structured information and a large amount of manual annotations.
The purpose of this tutorial is to in-troduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchersboth from a scientific and a practical (i.e.
data acquisition and processing issues) perspective.OutlineThe tutorial is divided into three main parts:1.
Extracting world knowledge from Wikipedia.
We review methods aiming at extracting fully struc-tured world knowledge from the content of the online encyclopedia.
We show how to take categories,hyperlinks and infoboxes as building blocks for a semantic network with unlabeled relations betweenthe concepts.
The task of taxonomy induction then boils down to labeling the relations between theseconcepts, e.g.
with isa, part-of, instance-of, located-in, etc.
relations.2.
Leveraging linguistic knowledge from Wikipedia.
Wikipedia provides shallow markup annotationswhich can be interpreted as manual annotations of linguistic phenomena.
These ?annotations?
includeword boundaries, word senses, named entities, translations of concepts in many languages.
Further-more, Wikipedia can be used as a multilingual comparable corpus.3.
Future directions.
Knowledge derived from Wikipedia has the potential to become a resource asimportant for NLP as WordNet.
Also the Wikipedia edit history provides a repository of linguisticknowledge which is to be exploited.
Potential applications of the knowledge implicitly encoded in theedit history include spelling corrections, natural language generation, text summarization, etc.Target audienceThis tutorial is designed for students and researchers in Computer Science and Computational Linguistics.No prior knowledge of information extraction topics is assumed.7Speakers?
biosSimone Paolo Ponzetto is an assistant professor at the Computational Linguistics Department of the Univer-sity of Heidelberg, Germany.
His main research interests lie in the area of information extraction, knowledgeacquisition and engineering, lexical semantics, and their application to discourse-based phenomena.Michael Strube is group leader of the NLP group at EML Research, a privately funded research institutein Heidelberg, Germany.
The NLP group focuses on the areas of semantics, pragmatics and discourse andapplications like summarization and information extraction.8
