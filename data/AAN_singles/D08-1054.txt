Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 514?522,Honolulu, October 2008. c?2008 Association for Computational LinguisticsHTM: A Topic Model for HypertextsCongkai Sun?Department of Computer ScienceShanghai Jiaotong UniversityShanghai, P. R. Chinamartinsck@hotmail.comBin GaoMicrosoft Research AsiaNo.49 Zhichun RoadBeijing, P. R. Chinabingao@microsoft.comZhenfu CaoDepartment of Computer ScienceShanghai Jiaotong UniversityShanghai, P. R. Chinazfcao@cs.sjtu.edu.cnHang LiMicrosoft Research AsiaNo.49 Zhichun RoadBeijing, P. R. Chinahangli@microsoft.comAbstractPreviously topic models such as PLSI (Prob-abilistic Latent Semantic Indexing) and LDA(Latent Dirichlet Allocation) were developedfor modeling the contents of plain texts.
Re-cently, topic models for processing hyper-texts such as web pages were also proposed.The proposed hypertext models are generativemodels giving rise to both words and hyper-links.
This paper points out that to better rep-resent the contents of hypertexts it is more es-sential to assume that the hyperlinks are fixedand to define the topic model as that of gen-erating words only.
The paper then proposesa new topic model for hypertext processing,referred to as Hypertext Topic Model (HTM).HTM defines the distribution of words in adocument (i.e., the content of the document)as a mixture over latent topics in the documentitself and latent topics in the documents whichthe document cites.
The topics are furthercharacterized as distributions of words, as inthe conventional topic models.
This paper fur-ther proposes a method for learning the HTMmodel.
Experimental results show that HTMoutperforms the baselines on topic discoveryand document classification in three datasets.1 IntroductionTopic models are probabilistic and generative mod-els representing contents of documents.
Examplesof topic models include PLSI (Hofmann, 1999) andLDA (Blei et al, 2003).
The key idea in topic mod-eling is to represent topics as distributions of words* This work was conducted when the first author visitedMicrosoft Research Asia as an intern.and define the distribution of words in document(i.e., the content of document) as a mixture over hid-den topics.
Topic modeling technologies have beenapplied to natural language processing, text min-ing, and information retrieval, and their effective-ness have been verified.In this paper, we study the problem of topic mod-eling for hypertexts.
There is no doubt that this isan important research issue, given the fact that moreand more documents are available as hypertexts cur-rently (such as web pages).
Traditional work mainlyfocused on development of topic models for plaintexts.
It is only recently several topic models for pro-cessing hypertexts were proposed, including Link-LDA and Link-PLSA-LDA (Cohn and Hofmann,2001; Erosheva et al, 2004; Nallapati and Cohen,2008).We point out that existing models for hypertextsmay not be suitable for characterizing contents ofhypertext documents.
This is because all the modelsare assumed to generate both words and hyperlinks(outlinks) of documents.
The generation of the lattertype of data, however, may not be necessary for thetasks related to contents of documents.In this paper, we propose a new topic model forhypertexts called HTM (Hypertext Topic Model),within the Bayesian learning approach (it is simi-lar to LDA in that sense).
In HTM, the hyperlinksof hypertext documents are supposed to be given.Each document is associated with one topic distribu-tion.
The word distribution of a document is definedas a mixture of latent topics of the document itselfand latent topics of documents which the documentcites.
The topics are further defined as distributions514of words.
That means the content (topic distribu-tions for words) of a hypertext document is not onlydetermined by the topics of itself but also the top-ics of documents it cites.
It is easy to see that HTMcontains LDA as a special case.
Although the idea ofHTM is simple and straightforward, it appears thatthis is the first work which studies the model.We further provide methods for learning and in-ference of HTM.
Our experimental results on threeweb datasets show that HTM outperforms the base-line models of LDA, Link-LDA, and Link-PLSA-LDA, in the tasks of topic discovery and documentclassification.The rest of the paper is organized as follows.
Sec-tion 2 introduces related work.
Section 3 describesthe proposed HTM model and its learning and infer-ence methods.
Experimental results are presented inSection 4.
Conclusions are made in the last section.2 Related WorkThere has been much work on topic modeling.
Manymodels have been proposed including PLSI (Hof-mann, 1999), LDA (Blei et al, 2003), and theirextensions (Griffiths et al, 2005; Blei and Lafferty,2006; Chemudugunta et al, 2007).
Inference andlearning methods have been developed, such as vari-ational inference (Jordan et al, 1999; Wainwrightand Jordan, 2003), expectation propagation (Minkaand Lafferty, 2002), and Gibbs sampling (Griffithsand Steyvers, 2004).
Topic models have been uti-lized in topic discovery (Blei et al, 2003), documentretrieval (Xing Wei and Bruce Croft, 2006), docu-ment classification (Blei et al, 2003), citation analy-sis (Dietz et al, 2007), social network analysis (Meiet al, 2008), and so on.
Most of the existing modelsare for processing plain texts.
There are also modelsfor processing hypertexts, for example, (Cohn andHofmann, 2001; Nallapati and Cohen, 2008; Gru-ber et al, 2008; Dietz et al, 2007), which are mostrelevant to our work.Cohn and Hofmann (2001) introduced a topicmodel for hypertexts within the framework of PLSI.The model, which is a combination of PLSI andPHITS (Cohn and Chang, 2000), gives rise to boththe words and hyperlinks (outlinks) of the documentin the generative process.
The model is useful whenthe goal is to understand the distribution of linksas well as the distribution of words.
Erosheva etal (2004) modified the model by replacing PLSI withLDA.
We refer to the modified mode as Link-LDAand take it as a baseline in this paper.
Note that theabove two models do not directly associate the top-ics of the citing document with the topics of the citeddocuments.Nallapati and Cohn (2008) proposed an extensionof Link-LDA called Link-PLSA-LDA, which is an-other baseline in this paper.
Assuming that the cit-ing and cited documents share similar topics, theyexplicitly model the information flow from the cit-ing documents to the cited documents.
In Link-PLSA-LDA, the link graph is converted into a bi-partite graph in which links are connected from cit-ing documents to cited documents.
If a documenthas both inlinks and outlinks, it will be duplicatedon both sides of the bipartite graph.
The generativeprocess for the citing documents is similar to that ofLink-LDA, while the cited documents have a differ-ent generative process.Dietz et al(2007) proposed a topic model for ci-tation analysis.
Their goal is to find topical influ-ence of publications in research communities.
Theyconvert the citation graph (created from the publica-tions) into a bipartite graph as in Link-PLSA-LDA.The content of a citing document is assumed to begenerated by a mixture over the topic distributionof the citing document and the topic distributions ofthe cited documents.
The differences between thetopic distributions of citing and cited documents aremeasured, and the cited documents which have thestrongest influence on the citing document are iden-tified.Note that in most existing models described abovethe hyperlinks are assumed to be generated and linkprediction is an important task, while in the HTMmodel in this paper, the hyperlinks are assumed tobe given in advance, and the key task is topic iden-tification.
In the existing models for hypertexts, thecontent of a document (the word distribution of thedocument) are not decided by the other documents.In contrast, in HTM, the content of a document isdetermined by itself as well as its cited documents.Furthermore, HTM is a generative model which cangenerate the contents of all the hypertexts in a col-lection, given the link structure of the collection.Therefore, if the goal is to accurately learn and pre-515Table 1: Notations and explanations.T Number of topicsD Documents in corpusD Number of documents??
, ??
Hyperparameters for ?
and ??
Hyperparameter to control the weight betweenthe citing document and the cited documents?
Topic distributions for all documents?
Word distribution for topicb, c, z Hidden variables for generating wordd document (index)wd Word sequence in document dNd Number of words in document dLd Number of documents cited by document dId Set of cited documents for document didl Index of lth cited document of document d?d Distribution on cited documents of document d?d Topic distribution associated with document dbdn Decision on way of generating nth word in doc-ument dcdn Cited document that generates nth word in doc-ument dzdn Topic of nth word in document ddict contents of documents, the use of HTM seemsmore reasonable.3 Hypertext Topic Model3.1 ModelIn topic modeling, a probability distribution ofwords is employed for a given document.
Specifi-cally, the probability distribution is defined as a mix-ture over latent topics, while each topic is futurecharacterized by a distribution of words (Hofmann,1999; Blei et al, 2003).
In this paper, we introducean extension of LDA model for hypertexts.
Table 1gives the major notations and their explanations.The graphic representation of conventional LDAis given in Figure 1(a).
The generative process ofLDA has three steps.
Specifically, in each documenta topic distribution is sampled from a prior distribu-tion defined as Dirichlet distribution.
Next, a topic issampled from the topic distribution of the document,which is a multinominal distribution.
Finally, a wordis sampled according to the word distribution of thetopic, which also forms a multinormal distribution.The graphic representation of HTM is given inFigure 1(b).
The generative process of HTM is de-scribed in Algorithm 1.
First, a topic distributionis sampled for each document according to Dirich-let distribution.
Next, for generating a word in adocument, it is decided whether to use the currentAlgorithm 1 Generative Process of HTMfor each document d doDraw ?d ?
Dir(??
).end forfor each word wdn doif Ld > 0 thenDraw bdn ?
Ber(?
)Draw cdn ?
Uni(?d)if bdn = 1 thenDraw zdn ?
Multi(?d)elseDraw zdn ?
Multi(?Idcdn )end ifelseDraw a topic zdn ?
Multi(?d)end ifDraw a word wdn ?
P (wdn | zdn, ?
)end fordocument or documents which the document cites.
(The weight between the citing document and citeddocuments is controlled by an adjustable hyper-parameter ?.)
It is also determined which cited doc-ument to use (if it is to use cited documents).
Then, atopic is sampled from the topic distribution of the se-lected document.
Finally, a word is sampled accord-ing to the word distribution of the topic.
HTM natu-rally mimics the process of writing a hypertext docu-ment by humans (repeating the processes of writingnative texts and anchor texts).The formal definition of HTM is given be-low.
Hypertext document d has Nd wordswd = wd1 ?
?
?wdNd and Ld cited documents Id ={id1, .
.
.
, idLd}.
The topic distribution of d is ?dand topic distributions of the cited documents are?i, i ?
Id.
Given ?, ?, and ?, the conditional proba-bility distribution of wd is defined as:p(wd|?, ?, ?)
=Nd?n=1?bdnp(bdn|?
)?cdnp(cdn|?d)?zdnp(zdn|?d)bdnp(zdn|?idcdn )1?bdnp(wdn|zdn, ?
).Here ?d, bdn, cdn, and zdn are hidden vari-ables.
When generating a word wdn, bdn determineswhether it is from the citing document or the citeddocuments.
cdn determines which cited document it516is when bdn = 0.
In this paper, for simplicity we as-sume that the cited documents are equally likely tobe selected, i.e., ?di = 1Ld .Note that ?
represents the topic distributions ofall the documents.
For any d, its word distributionis affected by both ?d and ?i, i ?
Id.
There is apropagation of topics from the cited documents tothe citing document through the use of ?i, i ?
Id.For a hypertext document d that does not havecited documents.
The conditional probability dis-tribution degenerates to LDA:p(wd|?d, ?)
=Nd?n=1?zdnp(zdn|?d)p(wdn|zdn, ?
).By taking the product of the marginal probabil-ities of hypertext documents, we obtain the condi-tional probability of the corpus D given the hyper-parameters ?, ?
?, ?,p(D|?, ?
?, ?)
=?
D?d=1p(?d|??)Nd?n=1?bdnp(bdn|?
)?cdnp(cdn|?d)?zdnp(zdn|?d)bdnp(zdn|?Idcdn )1?bdnp(wdn|zdn, ?)d?.
(1)Note that the probability function (1) also covers thespecial cases in which documents do not have citeddocuments.In HTM, the content of a document is decided bythe topics of the document as well as the topics ofthe documents which the document cites.
As a resultcontents of documents can be ?propagated?
along thehyperlinks.
For example, suppose web page A citespage B and page B cites page C, then the content ofpage A is influenced by that of page B, and the con-tent of page B is further influenced by the contentof page C. Therefore, HTM is able to more accu-rately represent the contents of hypertexts, and thusis more useful for text processing such as topic dis-covery and document classification.3.2 Inference and LearningAn exact inference of the posterior probability ofHTM may be intractable, we employ the mean fieldvariational inference method (Wainwright and Jor-dan, 2003; Jordan et al, 1999) to conduct approxi-mation.
Let I[?]
be an indicator function.
We firstdefine the following factorized variational posteriordistribution q with respect to the corpus:q =D?d=1q(?d|?d)Nd?n=1(q(xdn|?dn)(q(cdn|?dn))I[Ld>0]q(zdn|?dn) ,where ?, ?, ?, and ?
denote free variational parame-ters.
Parameter ?
is the posterior Dirichlet parametercorresponding to the representations of documentsin the topic simplex.
Parameters ?, ?, and ?
cor-respond to the posterior distributions of their asso-ciated random variables.
We then minimize the KLdivergence between q and the true posterior proba-bility of the corpus by taking derivatives of the lossfunction with respect to variational parameters.
Thesolution is listed as below.Let ?iv be p(wvdn = 1|zi = 1) for the word v. IfLd > 0, we haveE-step:?di = ?
?i +Nd?n=1?dn?dni +D?d?=1Ld?
?l=1I [id?l = d]Nd??n=1(1?
?d?n)?d?nl?d?ni .
?dni ?
?iv exp{?dnEq [log (?di) |?d]+ (1?
?dn)Ld?l=1?dnlEq [log (?Idli) |?Idl ]} .
?dn =(1 +(exp{k?i=1((?dniEq[log(?di)|?d]?Ld?l=1?dnl?dniEq[log(?Idli)|?Idl ])+ log ??
log (1?
?)})?1)?1.517zw?????
?T Nd D(a) LDAw????
?
?D TdzDNd?c?bId(b) HTMzw?????
?T NdDzd?
?Ld(c) Link-LDAzw?????
?TNdz?Ldpizwd?N MCited Documents Citing Documentsd(d) Link-PLSA-LDAFigure 1: Graphical model representations?dnl ?
?dl exp{(1?
?dn)k?i=1?dniEq[log(?Idli)|?Idl ]}.Otherwise,?di = ?
?i +Nd?n=1?dni +D?d?=1Ld?
?l=1I [id?l = d]Nd??n=1(1?
?d?n)?d?nl?d?ni .
?dni ?
?iv exp{Eq [log (?di) |?d]}.From the first two equations we can see that thecited documents and the citing document jointly af-fect the distribution of the words in the citing docu-ment.M-step:?ij ?D?d=1Nd?n=1?dniwjdn.In order to cope with the data sparseness problemdue to large vocabulary, we employ the same tech-nique as that in (Blei et al, 2003).
To be specific,we treat ?
as a K ?V random matrix, with each rowbeing independently drawn from a Dirichlet distri-bution ?i ?
Dir(??)
.
Variational inference ismodified appropriately.4 Experimental ResultsWe compared the performances of HTM and threebaseline models: LDA, Link-LDA, and Link-PLSA-LDA in topic discovery and document classification.Note that LDA does not consider the use of link in-formation; we included it here for reference.4.1 DatasetsWe made use of three datasets.
The documents in thedatasets were processed by using the Lemur Tookkit (http://www.lemurproject.org), and the low fre-quency words in the datasets were removed.The first dataset WebKB (available athttp://www.cs.cmu.edu/?webkb) contains sixsubjects (categories).
There are 3,921 documentsand 7,359 links.
The vocabulary size is 5,019.518The second dataset Wikipedia (available athttp://www.mpi-inf.mpg.de/?angelova) containsfour subjects (categories): Biology, Physics, Chem-istry, and Mathematics.
There are 2,970 documentsand 45,818 links.
The vocabulary size is 3,287.The third dataset is ODP composed of homepagesof researchers and their first level outlinked pages(cited documents).
We randomly selected five sub-jects from the ODP archive.
They are CognitiveScience (CogSci), Theory, NeuralNetwork (NN),Robotics, and Statistics.
There are 3,679 pages and2,872 links.
The vocabulary size is 3,529.WebKB and Wikipedia are public datasets widelyused in topic model studies.
ODP was collected byus in this work.4.2 Topic DiscoveryWe created four topic models HTM, LDA, Link-LDA, and Link-PLSA-LDA using all the data ineach of the three datasets, and evaluated the top-ics obtained in the models.
We heuristically set thenumbers of topics as 10 for ODP, 12 for WebKB,and 8 for Wikipedia (i.e., two times of the numberof true subjects).
We found that overall HTM canconstruct more understandable topics than the othermodels.
Figure 2 shows the topics related to thesubjects created by the four models from the ODPdataset.
HTM model can more accurately extractthe three topics: Theory, Statistic, and NN than theother models.
Both LDA and Link-LDA had mixedtopics, labeled as ?Mixed?
in Figure 2.
Link-PLSA-LDA missed the topic of Statistics.
Interestingly, allthe four models split Cognitive Science into two top-ics (showed as CogSci-1 and CogSci-2), probablybecause the topic itself is diverse.4.3 Document ClassificationWe applied the four models in the three datasets todocument classification.
Specifically, we used theword distributions of documents created by the mod-els as feature vectors of the documents and used thesubjects in the datasets as categories.
We furtherrandomly divided each dataset into three parts (train-ing, validation, and test) and conducted 3-fold cross-validation experiments.
In each trial, we trained anSVM classifier with the training data, chose param-eters with the validation data, and conducted evalu-ation on classification with the test data.
For HTM,Table 2: Classification accuracies in 3-fold cross-validation.LDA HTM Link-LDA Link-PLSA-LDAODP 0.640 0.698 0.535 0.581WebKB 0.786 0.795 0.775 0.774Wikipedia 0.845 0.866 0.853 0.855Table 3: Sign-test results between HTM and the threebaseline models.LDA Link-LDA Link-PLSA-LDAODP 0.0237 2.15e-05 0.000287WebKB 0.0235 0.0114 0.00903Wikipedia 1.79e-05 0.00341 0.00424we chose the best ?
value with the validation set ineach trial.
Table 2 shows the classification accura-cies.
We can see that HTM performs better than theother models in all three datasets.We conducted sign-tests on all the results of thedatasets.
In most cases HTM performs statisticallysignificantly better than LDA, Link-LDA, and Link-PLSA-LDA (p-value < 0.05).
The test results areshown in Table 3.4.4 DiscussionWe conducted analysis on the results to see whyHTM can work better.
Figure 3 shows an examplehomepage from the ODP dataset, where superscriptsdenote the indexes of outlinked pages.
The home-page contains several topics, including Theory, Neu-ral network, Statistics, and others, while the citedpages contain detailed information about the topics.Table 4 shows the topics identified by the four mod-els for the homepage.
We can see that HTM canreally more accurately identify topics than the othermodels.The major reason for the better performance byHTM seems to be that it can fully leverage the infor-Table 4: Comparison of topics identified by the four mod-els for the example homepage.
Only topics with proba-bilities > 0.1 and related to the subjects are shown.Model Topics ProbabilitiesLDA Mixed 0.537HTM Theory 0.229NN 0.278Statistics 0.241Link-LDA Statistics 0.281Link-PLSA-LDA Theory 0.527CogSci-2 0.175519(a) LDAMixed NN Robot CogSci-1 CogSci-2statistic learn robot visual consciouscompute conference project model psychologyalgorithm system file experiment languagetheory neural software change cognitivecomplex network code function experiencemathematics model program response brainmodel international data process theoryscience compute motor data philosophycomputation ieee read move scienceproblem proceedings start observe onlinerandom process build perception mindanalysis computation comment effect conceptpaper machine post figure physicalmethod science line temporal problemjournal artificial include sensory content(b) HTMTheory Statistics NN Robot CogSci-1 CogSci-2compute model learn robot conscious memoryscience statistic system project visual psychologyalgorithm data network software experience languagetheory experiment neural motor change sciencecomplex sample conference sensor perception cognitivecomputation process model code move brainmathematics method compute program theory humanpaper analysis ieee build online neuroscienceproblem response international line physical journallecture figure proceedings board concept societyrandom result machine read problem traumajournal temporal process power philosophy pressbound probable computation type object learngraph observe artificial comment content abuseproceedings test intelligence post view associate(c) Link-LDAStatistics Mixed Robot CogSci-1 CogSci-2statistic compute robot visual consciousmodel conference project model psychologydata system software experiment cognitiveanalysis learn file change languagemethod network motor function brainlearn computation robotics vision sciencesample proceedings informatik process memoryalgorithm neural program perception theoryprocess ieee build move philosophybayesian algorithm board response pressapplication international sensor temporal onlinerandom science power object neurosciencedistribution complex code observe journalsimulate theory format sensory humanmathematics journal control figure mind(d) Link-PLSA-LDATheory NN Robot CogSci-1 CogSci-2compute conference robot conscious modelalgorithm learn code experience processcomputation science project language visualtheory international typeof book datacomplex system motor change experimentscience compute control make functionmathematics network system problem learnnetwork artificial serve brain neuralpaper ieee power world systemjournal intelligence program read perceptionproceedings robot software case representrandom technology file than visionsystem proceedings build mind responseproblem machine pagetracker theory objectlecture neural robotics content abstractFigure 2: Topics identified by four modelsRadford M.NealProfessor, Dept.
of Statistics and Dept.
of Computer Science, University of TorontoI?m currently highlighting the following :?
A new R function for performing univariate slice sampling.1?
A workshop paper on Computing Likelihood Functions for High-Energy PhysicsExperiments when Distributions are Defined by Simulators with Nuisance Parameters.2?
Slides from a talk at the Third Workshop on Monte Carlo Methods on?Short-Cut MCMC: An Alternative to Adaptation?, May 2007: Postscript, PDF.Courses I?m teaching in Fall 2008 :?
STA 437: Methods for Multivariate Data3?
STA 3000: Advanced Theory of Statistics4You can also find information on courses I?ve taught in the past.5You can also get to information on :?
Research interests6 (with pointers to publications)?
Current and former graduate students7?
Current and former postdocs8?
Curriculum Vitae: PostScript, or PDF.?
Full publications list9?
How to contact me10?
Links to various places11If you know what you want already,you may wish to go directly to :?
Software available on-line12?
Papers available on-line13?
Slides from talks14?
Miscellaneous other stuff15Information in this hierarchy was last updated 2008-06-20.Figure 3: An example homepage: http://www.cs.utoronto.ca/?
radford/520Table 5: Word assignment in the example homepage.Word bdn cdn Topic Probabilitymcmc 0.544 2 Stat 0.949experiment 0.546 2 Stat 0.956neal 0.547 8 NN 0.985likelihood 0.550 2 Stat 0.905sample 0.557 2 Stat 0.946statistic 0.559 2 Stat 0.888parameter 0.563 2 Stat 0.917perform 0.565 2 Stat 0.908carlo 0.568 2 Stat 0.813monte 0.570 2 Stat 0.802toronto 0.572 8 NN 0.969distribution 0.578 2 Stat 0.888slice 0.581 2 Stat 0.957energy 0.581 13 NN 0.866adaptation 0.591 7 Stat 0.541teach 0.999 11 Other 0.612current 0.999 11 Other 0.646curriculum 0.999 11 Other 0.698want 0.999 11 Other 0.706highlight 0.999 10 Other 0.786professor 0.999 11 Other 0.764academic 0.999 11 Other 0.810student 0.999 11 Other 0.817contact 0.999 11 Other 0.887graduate 0.999 11 Other 0.901Table 6: Most salient topics in cited pages.URL Topic Probability2 Stat 0.6907 Stat 0.4678 NN 0.78613 NN 0.7760.60.650.70.750.80.850.90.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5ODPWebkbWikiAccuracy?Figure 4: Classification accuracies on three datasets withdifferent ?
values.
The cross marks on the curves cor-respond to the average values of ?
in the 3-fold cross-validation experiments.mation from the cited documents.
We can see thatthe content of the example homepage is diverse andnot very rich.
It might be hard for the other base-line models to identify topics accurately.
In con-trast, HTM can accurately learn topics by the helpof the cited documents.
Specifically, if the content ofa document is diverse, then words in the documentare likely to be assigned into wrong topics by theexisting approaches.
In contrast, in HTM with prop-agation of topic distributions from cited documents,the words of a document can be more accurately as-signed into topics.
Table 5 shows the first 15 wordsand the last 10 words for the homepage given byHTM, in ascending order of bdn, which measuresthe degree of influence from the cited documents onthe words (the smaller the stronger).
The table alsogives the values of cdn, indicating which cited docu-ments have the strongest influence.
Furthermore, thetopics having the largest posterior probabilities forthe words are also shown.
We can see that the words?experiment?, ?sample?, ?parameter?, ?perform?, and?energy?
are accurately classified.
Table 6 gives themost salient topics of cited documents.
It also showsthe probabilities of the topics given by HTM.
We cansee that there is a large agreement between the mostsalient topics in the cited documents and the topicswhich are affected the most in the citing document.Parameter ?
is the only parameter in HTM whichneeds to be tuned.
We found that the performance ofHTM is not very sensitive to the values of ?, whichreflects the degree of influence from the cited doc-uments to the citing document.
HTM can performwell with different ?
values.
Figure 4 shows the clas-sification accuracies of HTM with respect to differ-ent ?
values for the three datasets.
We can see thatHTM works better than the other models in most ofthe cases (cf., Table 2).5 ConclusionIn this paper, we have proposed a novel topicmodel for hypertexts called HTM.
Existing modelsfor processing hypertexts were developed based onthe assumption that both words and hyperlinks arestochastically generated by the model.
The gener-ation of latter type of data is actually unnecessaryfor representing contents of hypertexts.
In the HTMmodel, it is assumed that the hyperlinks of hyper-521texts are given and only the words of the hypertextsare stochastically generated.
Furthermore, the worddistribution of a document is determined not onlyby the topics of the document in question but alsofrom the topics of the documents which the doc-ument cites.
It can be regarded as ?propagation?of topics reversely along hyperlinks in hypertexts,which can lead to more accurate representations thanthe existing models.
HTM can naturally mimic hu-man?s process of creating a document (i.e., by con-sidering using the topics of the document and at thesame time the topics of the documents it cites).
Wealso developed methods for learning and inferringan HTM model within the same framework as LDA(Latent Dirichlet Allocation).
Experimental resultsshow that the proposed HTM model outperformsthe existing models of LDA, Link-LDA, and Link-PLSA-LDA on three datasets for topic discovery anddocument classification.As future work, we plan to compare the HTMmodel with other existing models, to develop learn-ing and inference methods for handling extremelylarge-scale data sets, and to combine the currentmethod with a keyphrase extraction method for ex-tracting keyphrases from web pages.6 AcknowledgementWe thank Eric Xing for his valuable comments onthis work.ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval.
ACM Press / Addison-Wesley.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of machine LearningResearch, 3:993?1022.David Blei and John Lafferty.
2005.
Correlated TopicModels.
In Advances in Neural Information Process-ing Systems 12.David Blei and John Lafferty.
2006.
Dynamic topicmodels.
In Proceedings of the 23rd international con-ference on Machine learning.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2007.
Modeling General and Specific As-pects of Documents with a Probabilistic Topic Model.In Advances in Neural Information Processing Sys-tems 19.David Cohn and Huan Chang.
2000.
Learning to Proba-bilistically Identify Authoritative Documents.
In Pro-ceedings of the 17rd international conference on Ma-chine learning.David Cohn and Thomas Hofmann.
2001.
The missinglink - a probabilistic model of document content andhypertext connectivity.
In Neural Information Pro-cessing Systems 13.Laura Dietz, Steffen Bickel and Tobias Scheffer.
2007.Unsupervised prediction of citation influences.
In Pro-ceedings of the 24th international conference on Ma-chine learning.Elena Erosheva, Stephen Fienberg, and John Lafferty.2004.
Mixed-membership models of scientific pub-lications.
In Proceedings of the National Academy ofSciences, 101:5220?5227.Thomas Griffiths and Mark Steyvers.
2004.
FindingScientific Topics.
In Proceedings of the NationalAcademy of Sciences, 101 (suppl.
1) .Thomas Griffiths, Mark Steyvers, David Blei, and JoshuaTenenbaum.
2005.
Integrating Topics and Syntax.
InAdvances in Neural Information Processing Systems,17.Amit Gruber, Michal Rosen-Zvi, and Yair Weiss.
2008.Latent Topic Models for Hypertext.
In Proceedings ofthe 24th Conference on Uncertainty in Artificial Intel-ligence.Thomas Hofmann.
1999.
Probabilistic Latent SemanticAnalysis.
In Proceedings of the 15th Conference onUncertainty in Artificial Intelligence.Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola,and Lawrence Saul.
1999.
An Introduction to Varia-tional Methods for Graphical Models.
Machine Learn-ing, 37(2):183?233.QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiangZhai.
2008.
Topic Modeling with Network Regular-ization.
In Proceeding of the 17th international con-ference on World Wide Web.Thomas Minka and John Lafferty.
2002.
Expectation-Propagation for the Generative Aspect Model.
In Pro-ceedings of the 18th Conference in Uncertainty in Ar-tificial Intelligence.Ramesh Nallapati and William Cohen.
2008.
Link-PLSA-LDA: A new unsupervised model for topics andinfluence of blogs.
In International Conference forWebblogs and Social Media.Martin Wainwright, and Michael Jordan.
2003.
Graph-ical models, exponential families, and variational in-ference.
In UC Berkeley, Dept.
of Statistics, TechnicalReport, 2003.Xing Wei and Bruce Croft.
2006.
LDA-based documentmodels for ad-hoc retrieval.
In Proceedings of the 29thannual international ACM SIGIR conference on Re-search and development in information retrieval.522
