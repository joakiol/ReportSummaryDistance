Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 816?823,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsResolving It, This, and That in Unrestricted Multi-Party DialogChristoph Mu?llerEML Research gGmbHVilla BoschSchlo?-Wolfsbrunnenweg 3369118 Heidelberg, Germanychristoph.mueller@eml-research.deAbstractWe present an implemented system for theresolution of it, this, and that in tran-scribed multi-party dialog.
The system han-dles NP-anaphoric as well as discourse-deictic anaphors, i.e.
pronouns with VP an-tecedents.
Selectional preferences for NP orVP antecedents are determined on the basisof corpus counts.
Our results show that thesystem performs significantly better than arecency-based baseline.1 IntroductionThis paper describes a fully automatic system forresolving the pronouns it, this, and that in unre-stricted multi-party dialog.
The system processesmanual transcriptions from the ICSI Meeting Cor-pus (Janin et al, 2003).
The following is a shortfragment from one of these transcripts.
The lettersFN in the speaker tag mean that the speaker is a fe-male non-native speaker of English.
The bracketsand subscript numbers are not part of the originaltranscript.FN083: Maybe you can also read through the - all the textwhich is on the web pages cuz I?d like to change the texta bit cuz sometimes [it]1?s too long, sometimes [it]2?s tooshort, inbreath maybe the English is not that good, so in-breath um, but anyways - So I tried to do [this]3 todayand if you could do [it]4 afterwards [it]5 would be reallynice cuz I?m quite sure that I can?t find every, like, ortho-graphic mistake in [it]6 or something.
(Bns003)For each of the six 3rd-person pronouns in the exam-ple, the task is to automatically identify its referent,i.e.
the entity (if any) to which the speaker makesreference.
Once a referent has been identified, thepronoun is resolved by linking it to one of its an-tecedents, i.e.
one of the referent?s earlier mentions.For humans, identification of a pronoun?s referentis often easy: it1, it2, and it6 are probably used torefer to the text on the web pages, while it4 is prob-ably used to refer to reading this text.
Humans alsohave no problem determining that it5 is not a normalpronoun at all.
In other cases, resolving a pronounis difficult even for humans: this3 could be used torefer to either reading or changing the text on theweb pages.
The pronoun is ambiguous because evi-dence for more than one interpretation can be found.Ambiguous pronouns are common in spoken dialog(Poesio & Artstein, 2005), a fact that has to be takeninto account when building a spoken dialog pronounresolution system.
Our system is intended as a com-ponent in an extractive dialog summarization sys-tem.
There are several ways in which coreference in-formation can be integrated into extractive summa-rization.
Kabadjov et al (2005) e.g.
obtained theirbest extraction results by specifying for each sen-tence whether it contained a mention of a particularanaphoric chain.
Apart from improving the extrac-tion itself, coreference information can also be usedto substitute anaphors with their antecedents, thusimproving the readability of a summary by minimiz-ing the number of dangling anaphors, i.e.
anaphorswhose antecedents occur in utterances that are notpart of the summary.
The paper is structured as fol-lows: Section 2 outlines the most important chal-lenges and the state of the art in spoken dialog pro-noun resolution.
Section 3 describes our annotationexperiments, and Section 4 describes the automatic816dialog preprocessing.
Resolution experiments andresults can be found in Section 5.2 Pronoun Resolution in Spoken DialogSpoken language poses some challenges for pro-noun resolution.
Some of these arise from nonrefer-ential resp.
nonresolvable pronouns, which are im-portant to identify because failure to do so can harmpronoun resolution precision.
One common typeof nonreferential pronoun is pleonastic it.
Anothercause of nonreferentiality that only applies to spokenlanguage is that the pronoun is discarded, i.e.
it ispart of an incomplete or abandoned utterance.
Dis-carded pronouns occur in utterances that are aban-doned altogether.ME010: Yeah.
Yeah.
No, no.
There was a whole co- Therewas a little contract signed.
It was - Yeah.
(Bed017)If the utterance contains a speech repair (Heeman &Allen, 1999), a pronoun in the reparandum part isalso treated as discarded because it is not part of thefinal utterance.ME10: That?s - that?s - so that?s a - that?s a very good question,then - now that it - I understand it.
(Bro004)In the corpus of task-oriented TRAINS dialogs de-scribed in Byron (2004), the rate of discarded pro-nouns is 7 out of 57 (12.3%) for it and 7 out of100 (7.0%) for that.
Schiffman (1985) reports thatin her corpus of career-counseling interviews, 164out of 838 (19.57%) instances of it and 80 out of582 (13.75%) instances of that occur in abandonedutterances.There is a third class of pronouns which is referen-tial but nonetheless unresolvable: vague pronouns(Eckert & Strube, 2000) are characterized by havingno clearly defined textual antecedent.
Rather, vaguepronouns are often used to refer to the topic of thecurrent (sub-)dialog as a whole.Finally, in spoken language the pronouns it, this, andthat are often discourse deictic (Webber, 1991), i.e.they are used to refer to an abstract object (Asher,1993).
We treat as abstract objects all referents ofVP antecedents, and do not distinguish between VPand S antecedents.ME013: Well, I mean there?s this Cyber Transcriber service,right?ME025: Yeah, that?s true, that?s true.
(Bmr001)Discourse deixis is very frequent in spoken dialog:The rate of discourse deictic expressions reported inEckert & Strube (2000) is 11.8% for pronouns andas much as 70.9% for demonstratives.2.1 State of the ArtPronoun resolution in spoken dialog has not receivedmuch attention yet, and a major limitation of the fewimplemented systems is that they are not fully au-tomatic.
Instead, they depend on manual removalof unresolvable pronouns like pleonastic it and dis-carded and vague pronouns, which are thus pre-vented from triggering a resolution attempt.
Thiseliminates a major source of error, but it renders thesystems inapplicable in a real-world setting whereno such manual preprocessing is feasible.One of the earliest empirically based works adress-ing (discourse deictic) pronoun resolution in spo-ken dialog is Eckert & Strube (2000).
The au-thors outline two algorithms for identifying the an-tecedents of personal and demonstrative pronouns intwo-party telephone conversations from the Switch-board corpus.
The algorithms depend on two non-trivial types of information: the incompatibility ofa given pronoun with either concrete or abstract an-tecedents, and the structure of the dialog in terms ofdialog acts.
The algorithms are not implemented,and Eckert & Strube (2000) report results of themanual application to a set of three dialogs (199 ex-pressions, including other pronouns than it, this, andthat).
Precision and recall are 66.2 resp.
68.2 forpronouns and 63.6 resp.
70.0 for demonstratives.An implemented system for resolving personal anddemonstrative pronouns in task-oriented TRAINSdialogs is described in Byron (2004).
The systemuses an explicit representation of domain-dependentsemantic category restrictions for predicate argu-ment positions, and achieves a precision of 75.0 anda recall of 65.0 for it (50 instances) and a precisionof 67.0 and a recall of 62.0 for that (93 instances)if all available restrictions are used.
Precision dropsto 52.0 for it and 43.0 for that when only domain-independent restrictions are used.To our knowledge, there is only one implementedsystem so far that resolves normal and discourse de-ictic pronouns in unrestricted spoken dialog (Strube& Mu?ller, 2003).
The system runs on dialogs fromthe Switchboard portion of the Penn Treebank.
For817it, this and that, the authors report 40.41 precisionand 12.64 recall.
The recall does not reflect the ac-tual pronoun resolution performance as it is calcu-lated against all coreferential links in the corpus, notjust those with pronominal anaphors.
The systemdraws some non-trivial information from the PennTreebank, including correct NP chunks, grammati-cal function tags (subject, object, etc.)
and discardedpronouns (based on the -UNF-tag).
The treebankinformation is also used for determining the acces-sibility of potential candidates for discourse deicticpronouns.In contrast to these approaches, the work describedin the following is fully automatic, using only infor-mation from the raw, transcribed corpus.
No manualpreprocessing is performed, so that during testing,the system is exposed to the full range of discarded,pleonastic, and other unresolvable pronouns.3 Data CollectionThe ICSI Meeting Corpus (Janin et al, 2003) isa collection of 75 manually transcribed group dis-cussions of about one hour each, involving threeto ten speakers.
A considerable number of partic-ipants are non-native speakers of English, whoseproficiency is sometimes poor, resulting in disflu-ent or incomprehensible speech.
The discussions arereal, unstaged meetings on various, technical topics.Most of the discussions are regular weekly meet-ings of a quite informal conversational style, con-taining many interrupts, asides, and jokes (Janin,2002).
The corpus features a semi-automaticallygenerated segmentation in which each segment is as-sociated with a speaker tag and a start and end timestamp.
Time stamps on the word level are not avail-able.
The transcription contains capitalization andpunctuation, and it also explicitly records interrup-tion points and word fragments (Heeman & Allen,1999), but not the extent of the related disfluencies.3.1 AnnotationThe annotation was done by naive project-externalannotators, two non-native and two native speak-ers of English, with the annotation tool MMAX21on five randomly selected dialogs2.
The annotation1http://mmax.eml-research.de2Bed017, Bmr001, Bns003, Bro004, and Bro005.instructions were deliberately kept simple, explain-ing and illustrating the basic notions of anaphoraand discourse deixis, and describing how markableswere to be created and linked in the annotation tool.This practice of using a higher number of naive ?rather than fewer, highly trained ?
annotators wasmotivated by our intention to elicit as many plau-sible interpretations as possible in the presence ofambiguity.
It was inspired by the annotation ex-periments of Poesio & Artstein (2005) and Artstein& Poesio (2006).
Their experiments employed upto 20 annotators, and they allowed for the explicitannotation of ambiguity.
In contrast, our annota-tors were instructed to choose the single most plau-sible interpretation in case of perceived ambigu-ity.
The annotation covered the pronouns it, this,and that only.
Markables for these tokens werecreated automatically.
From among the pronomi-nal3 instances, the annotators then identified normal,vague, and nonreferential pronouns.
For normal pro-nouns, they also marked the most recent antecedentusing the annotation tool?s coreference annotationfunction.
Markables for antecedents other than it,this, and that had to be created by the annotatorsby dragging the mouse over the respective wordsin the tool?s GUI.
Nominal antecedents could be ei-ther noun phrases (NP) or pronouns (PRO).
VP an-tecedents (for discourse deictic pronouns) spannedonly the verb phrase head, i.e.
the verb, not the en-tire phrase.
By this, we tried to reduce the numberof disagreements caused by differing markable de-marcations.
The annotation of discourse deixis waslimited to cases where the antecedent was a finite orinfinite verb phrase expressing a proposition, eventtype, etc.43.2 ReliabilityInter-annotator agreement was checked by comput-ing the variant of Krippendorff?s ?
described in Pas-sonneau (2004).
This metric requires all annotationsto contain the same set of markables, a conditionthat is not met in our case.
Therefore, we report?
values computed on the intersection of the com-3The automatically created markables included all instancesof this and that, i.e.
also relative pronouns, determiners, com-plementizers, etc.4Arbitrary spans of text could not serve as antecedents fordiscourse deictic pronouns.
The respective pronouns were to betreated as vague, due to lack of a well-defined antecedent.818pared annotations, i.e.
on those markables that canbe found in all four annotations.
Only a subset ofthe markables in each annotation is relevant for thedetermination of inter-annotator agreement: all non-pronominal markables, i.e.
all antecedent markablesmanually created by the annotators, and all referen-tial instances of it, this, and that.
The second columnin Table 1 contains the cardinality of the union ofall four annotators?
markables, i.e.
the number of alldistinct relevant markables in all four annotations.The third and fourth column contain the cardinalityand the relative size of the intersection of these fourmarkable sets.
The fifth column contains ?
calcu-lated on the markables in the intersection only.
Thefour annotators only agreed in the identification ofmarkables in approx.
28% of cases.
?
in the fivedialogs ranges from .43 to .52.| 1 ?
2 ?
3 ?
4 | | 1 ?
2 ?
3 ?
4 | ?Bed017 397 109 27.46 % .47Bmr001 619 195 31.50 % .43Bns003 529 131 24.76 % .45Bro004 703 142 20.20 % .45Bro005 530 132 24.91 % .52Table 1: Krippendorff?s ?
for four annotators.3.3 Data SubsetsIn view of the subjectivity of the annotation task,which is partly reflected in the low agreement evenon markable identification, the manual creation of aconsensus-based gold standard data set did not seemfeasible.
Instead, we created core data sets fromall four annotations by means of majority decisions.The core data sets were generated by automaticallycollecting in each dialog those anaphor-antecedentpairs that at least three annotators identified indepen-dently of each other.
The rationale for this approachwas that an anaphoric link is the more plausible themore annotators identify it.
Such a data set certainlycontains some spurious or dubious links, while lack-ing some correct but more difficult ones.
However,we argue that it constitutes a plausible subset ofanaphoric links that are useful to resolve.Table 2 shows the number and lengths of anaphoricchains in the core data set, broken down accord-ing to the type of the chain-initial antecedent.
Therare type OTHER mainly contains adjectival an-tecedents.
More than 75% of all chains consist oftwo elements only.
More than 33% begin with apronoun.
From the perspective of extractive sum-marization, the resolution of these latter chains is nothelpful since there is no non-pronominal antecedentthat it can be linked to or substituted with.length 2 3 4 5 6 > 6 totalBed017NP 17 3 2 - 1 - 23PRO 14 - 2 - - - 16VP 6 1 - - - - 7OTHER - - - - - - -all 37 4 4 - 1 - 4680.44%Bmr001NP 14 4 1 1 1 2 23PRO 19 9 2 2 1 1 34VP 9 5 - - - - 14OTHER - - - - - - -all 42 18 3 3 2 3 7159.16%Bns003NP 18 3 3 1 - - 25PRO 18 1 1 - - - 20VP 14 4 - - - - 18OTHER - - - - - - -all 50 8 4 1 - - 6379.37%Bro004NP 38 5 3 1 - - 47PRO 21 4 - 1 - - 26VP 8 1 1 - - - 10OTHER 2 1 - - - - 3all 69 11 4 2 - - 8680.23%Bro005NP 37 7 1 - - - 45PRO 15 3 1 - - - 19VP 8 1 - 1 - - 10OTHER 3 - - - - - 3all 63 11 2 1 - - 7781.82%?NP 124 22 10 3 2 2 163PRO 87 17 6 3 1 1 115VP 45 12 1 1 - - 59OTHER 5 1 - - - - 6all 261 52 17 7 3 3 34376.01%Table 2: Anaphoric chains in core data set.4 Automatic PreprocessingData preprocessing was done fully automatically,using only information from the manual tran-scription.
Punctuation signs and some heuristicswere used to split each dialog into a sequenceof graphemic sentences.
Then, a shallow disflu-ency detection and removal method was applied,which removed direct repetitions, nonlexicalizedfilled pauses like uh, um, interruption points, andword fragments.
Each sentence was then matchedagainst a list of potential discourse markers (actu-ally, like, you know, I mean, etc.)
If a sentencecontained one or more matches, string variants werecreated in which the respective words were deleted.Each of these variants was then submitted to a parsertrained on written text (Charniak, 2000).
The vari-ant with the highest probability (as determined bythe parser) was chosen.
NP chunk markables werecreated for all non-recursive NP constituents identi-819fied by the parser.
Then, VP chunk markables werecreated.
Complex verbal constructions like MD +INFINITIVE were modelled by creating markablesfor the individual expressions, and attaching themto each other with labelled relations like INFINI-TIVE COMP.
NP chunks were also attached, usingrelations like SUBJECT, OBJECT, etc.5 Automatic Pronoun ResolutionWe model pronoun resolution as binary classifica-tion, i.e.
as the mapping of anaphoric mentions toprevious mentions of the same referent.
This methodis not incremental, i.e.
it cannot take into accountearlier resolution decisions or any other informationbeyond that which is conveyed by the two mentions.Since more than 75% of the anaphoric chains in ourdata set would not benefit from incremental process-ing because they contain one anaphor only, we seethis limitation as acceptable.
In addition, incremen-tal processing bears the risk of system degradationdue to error propagation.5.1 FeaturesIn the binary classification model, a pronoun is re-solved by creating a set of candidate antecedents andsearching this set for a matching one.
This searchprocess is mainly influenced by two factors: ex-clusion of candidates due to constraints, and selec-tion of candidates due to preferences (Mitkov, 2002).Our features encode information relevant to thesetwo factors, plus more generally descriptive factorslike distance etc.
Computation of all features wasfully automatic.Shallow constraints for nominal antecedents includenumber, gender and person incompatibility, embed-ding of the anaphor into the antecedent, and coar-gumenthood (i.e.
the antecedent and anaphor mustnot be governed by the same verb).
For VP an-tecedents, a common shallow constraint is that theanaphor must not be governed by the VP antecedent(so-called argumenthood).
Preferences, on the otherhand, define conditions under which a candidateprobably is the correct antecedent for a given pro-noun.
A common shallow preference for nomi-nal antecedents is the parallel function preference,which states that a pronoun with a particular gram-matical function (i.e.
subject or object) preferablyhas an antecedent with a similar function.
The sub-ject preference, in contrast, states that subject an-tecedents are generally preferred over those withless salient functions, independent of the grammat-ical function of the anaphor.
Some of our featuresencode this functional and structural parallelism, in-cluding identity of form (for PRO antecedents) andidentity of grammatical function or governing verb.A more sophisticated constraint on NP an-tecedents is what Eckert & Strube (2000) call I-Incompatibility, i.e.
the semantic incompatibility ofa pronoun with an individual (i.e.
NP) antecedent.As Eckert & Strube (2000) note, subject pronounsin copula constructions with adjectives that can onlymodify abstract entities (like e.g.
true, correct, right)are incompatible with concrete antecedents like car.We postulate that the preference of an adjective tomodify an abstract entity (in the sense of Eckert &Strube (2000)) can be operationalized as the condi-tional probability of the adjective to appear with ato-infinitive resp.
a that-sentence complement, andintroduce two features which calculate the respec-tive preference on the basis of corpus5 counts.
Forthe first feature, the following query is used:# it (?s|is|was|were) ADJ to# it (?s|is|was|were) ADJAccording to Eckert & Strube (2000), pronouns thatare objects of verbs which mainly take sentencecomplements (like assume, say) exhibit a similarincompatibility with NP antecedents, and we cap-ture this with a similar feature.
Constraints forVPs include the following: VPs are inaccessible fordiscourse deictic reference if they fail to meet theright frontier condition (Webber, 1991).
We usea feature which is similar to that used by Strube& Mu?ller (2003) in that it approximates the rightfrontier on the basis of syntactic (rather than dis-course structural) relations.
Another constraint isA-Incompatibility, i.e.
the incompatibility of a pro-noun with an abstract (i.e.
VP) antecedent.
Accord-ing to Eckert & Strube (2000), subject pronouns incopula constructions with adjectives that can onlymodify concrete entities (like e.g.
expensive, tasty)are incompatible with abstract antecedents, i.e.
they5Based on the approx.
250,000,000 word TIPSTER corpus(Harman & Liberman, 1994).820cannot be discourse deictic.
The function of thisconstraint is already covered by the two corpus-based features described above in the context of I-Incompatibility.
Another feature, based on Yanget al (2005), encodes the semantic compatibilityof anaphor and NP antecedent.
We operationalizethe concept of semantic compatibility by substitut-ing the anaphor with the antecedent head and per-forming corpus queries.
E.g., if the anaphor is ob-ject, the following query6 is used:# (V|Vs|Ved|Ving) (?|a|an|the|this|that) ANTE+# (V|Vs|Ved|Ving) (?|the|these|those) ANTES# (ANTE|ANTES)If the anaphor is the subject in an adjective cop-ula construction, we use the following corpus countto quantify the compatibility between the predi-cated adjective and the NP antecedent (Lapata et al,1999):# ADJ (ANTE|ANTES) + # ANTE (is|was) ADJ+# ANTES (are|were) ADJ# ADJA third class of more general properties of the po-tential anaphor-antecedent pair includes the type ofanaphor (personal vs. demonstrative), type of an-tecedent (definite vs. indefinite noun phrase, pro-noun, finite vs. infinite verb phrase, etc.).
Specialfeatures for the identification of discarded expres-sions include the distance (in words) to the closestpreceeding resp.
following disfluency (indicated inthe transcription as an interruption point, word frag-ment, or uh resp.
um).
The relation between po-tential anaphor and (any type of) antecedent is de-scribed in terms of distance in seconds7 and words.For VP antecedents, the distance is calculated fromthe last word in the entire phrase, not from thephrase head.
Another feature which is relevant fordialog encodes whether both expressions are utteredby the same speaker.6V is the verb governing the anaphor.
Correct inflectedforms were also generated for irregular verbs.
ANTE resp.ANTES is the singular resp.
plural head of the antecedent.7Since the data does not contain word-level time stamps, thisdistance is determined on the basis of a simple forced align-ment.
For this, we estimated the number of syllables in eachword on the basis of its vowel clusters, and simply distributedthe known duration of the segment evenly on all words it con-tains.5.2 Data Representation and GenerationMachine learning data for training and testing wascreated by pairing each anaphor with each of itscompatible potential antecedents within a certaintemporal distance (9 seconds for NP and 7 secondsfor VP antecedents), and labelling the resulting datainstance as positive resp.
negative.
VP antecedentcandidates were created only if the anaphor was ei-ther that8 or the object of a form of do.Our core data set does not contain any nonreferen-tial pronouns, though the classifier is exposed to thefull range of pronouns, including discarded and oth-erwise nonreferential ones, during testing.
We tryto make the classifier robust against nonreferentialpronouns in the following way: From the manualannotations, we select instances of it, this, and thatthat at least three annotators identified as nonrefer-ential.
For each of these, we add the full range ofall-negative instances to the training data, applyingthe constraints mentioned above.5.3 Evaluation MeasureAs Bagga & Baldwin (1998) point out, in anapplication-oriented setting, not all anaphoric linksare equally important: If a pronoun is resolved toan anaphoric chain that contains only pronouns, thisresolution can be treated as neutral because it hasno application-level effect.
The common corefer-ence evaluation measure described in Vilain et al(1995) is inappropriate in this setting.
We calculateprecision, recall and F-measure on the basis of thefollowing definitions: A pronoun is resolved cor-rectly resp.
incorrectly only if it is linked (directlyor transitively) to the correct resp.
incorrect non-pronominal antecedent.
Likewise, the number ofmaximally resolvable pronouns in the core data set(i.e.
the evaluation key) is determined by consider-ing only pronouns in those chains that do not beginwith a pronoun.
Note that our definition of precisionis stricter (and yields lower figures) than that ap-plied in the ACE context, as the latter ignores incor-rect links between two expressions in the response8It is a common observation that demonstratives (in partic-ular that) are preferred over it for discourse deictic reference(Schiffman, 1985; Webber, 1991; Asher, 1993; Eckert & Strube,2000; Byron, 2004; Poesio & Artstein, 2005).
This preferencecan also be observed in our core data set: 44 out of 59 VP an-tecedents (69.49%) are anaphorically referred to by that.821if these expressions happen to be unannotated in thekey, while we treat them as precision errors unlessthe antecedent is a pronoun.
The same is true forlinks in the response that were identified by less thanthree annotators in the key.
While it is practical totreat those links as wrong, it is also simplistic be-cause it does not do justice to ambiguous pronouns(cf.
Section 6).5.4 Experiments and ResultsOur best machine learning results were obtainedwith the Weka9 Logistic Regression classifier.10 Allexperiments were performed with dialog-wise cross-validation.
For each run, training data was createdfrom the manually annotated markables in four di-alogs from the core data set, while testing was per-formed on the automatically detected chunks in theremaining fifth dialog.
For training and testing, theperson, number11, gender, and (co-)argument con-straints were used.
If an anaphor gave rise to a pos-itive instance, no negative training instances werecreated beyond that instance.
If a referential anaphordid not give rise to a positive training instance (be-cause its antecedent fell outside the search scopeor because it was removed by a constraint), no in-stances were created for that anaphor.
Instances fornonreferential pronouns were added to the trainingdata as described in Section 5.2.During testing, we select for each potential anaphorthe positive antecedent with the highest overall con-fidence.
Testing parameters include it-filter,which switches on and off the module for the detec-tion of nonreferential it described in Mu?ller (2006).When evaluated alone, this module yields a preci-sion of 80.0 and a recall of 60.9 for the detectionof pleonastic and discarded it in the five ICSI di-alogs.
For training, this module was always on.We also vary the parameter tipster, which con-trols whether or not the corpus frequency featuresare used.
If tipster is off, we ignore the corpusfrequency features both during training and testing.We first ran a simple baseline system which re-solved pronouns to their most recent compatible an-tecedent, applying the same settings and constraints9http://www.cs.waikato.ac.nz/ml/weka/10The full set of experiments is described in Mu?ller (2007).11The number constraint applies to it only, as this and thatcan have both singular and plural antecedents (Byron, 2004).as for testing (cf.
above).
The results can be foundin the first part of Table 3.
Precision, recall and F-measure are provided for ALL and for NP and VPantecedents individually.
The parameter tipsteris not available for the baseline system.
The bestbaseline performance is precision 4.88, recall 20.06and F-measure 7.85 in the setting with it-filteron.
As expected, this filter yields an increase in pre-cision and a decrease in recall.
The negative effectis outweighed by the positive effect, leading to asmall but insignificant12 increase in F-measure forall types of antecedents.Baseline Logistic RegressionSetting Ante P R F P R F-it-filter-tipsterNP 4.62 27.12 7.90 18.53 20.34 19.39?VP 1.72 2.63 2.08 13.79 10.53 11.94ALL 4.40 20.69 7.25 17.67 17.56 17.61?+tipsterNP - - - 19.33 22.03 20.59??
?VP - - - 13.43 11.84 12.59ALL - - - 18.16 19.12 18.63?
?+it-filter-tipsterNP 5.18 26.27 8.65 17.87 17.80 17.83?VP 1.77 2.63 2.12 13.12 10.53 11.68ALL 4.88 20.06 7.85 16.89 15.67 16.26?+tipsterNP - - - 20.82 21.61 21.21?
?VP - - - 11.27 10.53 10.88ALL - - - 18.67 18.50 18.58?
?Table 3: Resolution results.The second part of Table 3 shows the results of theLogistic Regression classifier.
When compared tothe best baseline, the F-measures are consistentlybetter for NP, VP, and ALL.
The improvement is(sometimes highly) significant for NP and ALL, butnever for VP.
The best F-measure for ALL is 18.63,yielded by the setting with it-filter off andtipster on.
This setting also yields the best F-measure for VP and the second best for NP.
Thecontribution of the it-filter is disappointing: In bothtipster settings, the it-filter causes F-measure forALL to go down.
The contribution of the corpusfeatures, on the other hand, is somewhat inconclu-sive: In both it-filter settings, they cause an in-crease in F-measure for ALL.
In the first setting, thisincrease is accompanied by an increase in F-measurefor VP, while in the second setting, F-measure forVP goes down.
It has to be noted, however, thatnone of the improvements brought about by the it-filter or the tipster corpus features is statistically sig-nificant.
This also confirms some of the findings ofKehler et al (2004), who found features similar to12Significance of improvement in F-measure is tested usinga paired one-tailed t-test and p <= 0.05 (?
), p <= 0.01 (??
),and p <= 0.005 (???
).822our tipster corpus features not to be significant forNP-anaphoric pronoun resolution in written text.6 Conclusions and Future WorkThe system described in this paper is ?
to our knowl-edge ?
the first attempt towards fully automatic res-olution of NP-anaphoric and discourse deictic pro-nouns (it, this, and that) in multi-party dialog.
Un-like other implemented systems, it is usable in a re-alistic setting because it does not depend on manualpronoun preselection or non-trivial discourse struc-ture or domain knowledge.
The downside is that,at least in our strict evaluation scheme, the perfor-mance is rather low, especially when compared tothat of state-of-the-art systems for pronoun resolu-tion in written text.
In future work, it might beworthwhile to consider less rigorous and thus moreappropriate evaluation schemes in which links areweighted according to how many annotators identi-fied them.In its current state, the system only processes man-ual dialog transcripts, but it also needs to be eval-uated on the output of an automatic speech recog-nizer.
While this will add more noise, it will alsogive access to useful prosodic features like stress.Finally, the system also needs to be evaluated extrin-sically, i.e.
with respect to its contribution to dialogsummarization.
It might turn out that our system al-ready has a positive effect on extractive summariza-tion, even though its performance is low in absoluteterms.Acknowledgments.
This work has been fundedby the Deutsche Forschungsgemeinschaft as part ofthe DIANA-Summ project (STR-545/2-1,2) and bythe Klaus Tschira Foundation.
We are grateful to theanonymous ACL reviewers for helpful commentsand suggestions.
We also thank Ron Artstein forhelp with significance testing.ReferencesArtstein, R. & M. Poesio (2006).
Identifying reference to ab-stract objects in dialogue.
In Proc.
of BranDial-06, pp.56?63.Asher, N. (1993).
Reference to Abstract Objects in Discourse.Dordrecht, The Netherlands: Kluwer.Bagga, A.
& B. Baldwin (1998).
Algorithms for scoring coref-erence chains.
In Proc.
of LREC-98, pp.
79?85.Byron, D. K. (2004).
Resolving pronominal reference to ab-stract entities., (Ph.D. thesis).
University of Rochester.Charniak, E. (2000).
A maximum-entropy-inspired parser.
InProc.
of NAACL-00, pp.
132?139.Eckert, M. & M. Strube (2000).
Dialogue acts, synchronis-ing units and anaphora resolution.
Journal of Semantics,17(1):51?89.Harman, D. & M. Liberman (1994).
TIPSTER CompleteLDC93T3A.
3 CD-ROMS.
Linguistic Data Consortium,Philadelphia, Penn., USA.Heeman, P. & J. Allen (1999).
Speech repairs, intonationalphrases, and discourse markers: Modeling speakers?
ut-terances in spoken dialogue.
Computational Linguistics,25(4):527?571.Janin, A.
(2002).
Meeting recorder.
In Proceedings of theApplied Voice Input/Output Society Conference (AVIOS),San Jose, California, USA, May 2002.Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke &C. Wooters (2003).
The ICSI Meeting Corpus.
In Pro-ceedings of the IEEE International Conference on Acous-tics, Speech and Signal Processing, Hong Kong, pp.
364?367.Kabadjov, M. A., M. Poesio & J. Steinberger (2005).
Task-based evaluation of anaphora resolution: The case ofsummarization.
In Proceedings of the RANLP Workshopon Crossing Barriers in Text Summarization Research,Borovets, Bulgaria.Kehler, A., D. Appelt, L. Taylor & A. Simma (2004).
The(non)utility of predicate-argument frequencies for pro-noun interpretation.
In Proc.
of HLT-NAACL-04, pp.
289?296.Lapata, M., S. McDonald & F. Keller (1999).
Determinantsof adjective-noun plausibility.
In Proc.
of EACL-99, pp.30?36.Mitkov, R. (2002).
Anaphora Resolution.
London, UK: Long-man.Mu?ller, C. (2006).
Automatic detection of nonreferential it inspoken multi-party dialog.
In Proc.
of EACL-06, pp.
49?56.Mu?ller, C. (2007).
Fully automatic resolution of it, this, andthat in unrestricted multi-party dialog., (Ph.D. thesis).Eberhard Karls Universita?t Tu?bingen, Germany.
To ap-pear.Passonneau, R. J.
(2004).
Computing reliability for co-reference annotation.
In Proc.
of LREC-04.Poesio, M. & R. Artstein (2005).
The reliability of anaphoricannotation, reconsidered: Taking ambiguity into account.In Proceedings of the ACL Workshop on Frontiers in Cor-pus Annotation II: Pie in the Sky, pp.
76?83.Schiffman, R. J.
(1985).
Discourse constraints on ?it?
and?that?
: A Study of Language Use in Career CounselingInterviews., (Ph.D. thesis).
University of Chicago.Strube, M. & C. Mu?ller (2003).
A machine learning approach topronoun resolution in spoken dialogue.
In Proc.
of ACL-03, pp.
168?175.Vilain, M., J. Burger, J. Aberdeen, D. Connolly & L. Hirschman(1995).
A model-theoretic coreference scoring scheme.In Proc.
of MUC-6, pp.
45?52.Webber, B. L. (1991).
Structure and ostension in the interpre-tation of discourse deixis.
Language and Cognitive Pro-cesses, 6(2):107?135.Yang, X., J. Su & C. L. Tan (2005).
Improving pronoun reso-lution using statistics-based semantic compatibility infor-mation.
In Proc.
of ACL-05, pp.
165?172.823
