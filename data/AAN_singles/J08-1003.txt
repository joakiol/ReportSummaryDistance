Wide-Coverage Deep Statistical ParsingUsing Automatic DependencyStructure AnnotationAoife Cahill?Dublin City UniversityMichael Burke?
?,?Dublin City UniversityIBM Center for Advanced StudiesRuth O?Donovan?
?Dublin City UniversityStefan Riezler?Palo Alto Research CenterJosef van Genabith?
?,?Dublin City UniversityIBM Center for Advanced StudiesAndy Way?
?,?Dublin City UniversityIBM Center for Advanced StudiesA number of researchers have recently conducted experiments comparing ?deep?
hand-craftedwide-coverage with ?shallow?
treebank- and machine-learning-based parsers at the level ofdependencies, using simple and automatic methods to convert tree output generated by theshallow parsers into dependencies.
In this article, we revisit such experiments, this time usingsophisticated automatic LFG f-structure annotation methodologies with surprising results.
Wecompare various PCFG and history-based parsers to find a baseline parsing system that fitsbest into our automatic dependency structure annotation technique.
This combined system ofsyntactic parser and dependency structure annotation is compared to two hand-crafted, deepconstraint-based parsers, RASP and XLE.
We evaluate using dependency-based gold standards?
Now at the Institut fu?r Maschinelle Sprachverarbeitung, Universita?t Stuttgart, Germany.
E-mail: aoife.cahill@ims.uni-stuttgart.de.??
National Centre for Language Technology, Dublin City University, Dublin 9, Ireland.?
IBM Dublin Center for Advanced Studies (CAS), Dublin 15, Ireland.?
Now at Google Inc., Mountain View, CA.Submission received: 24 August 2005; revised submission received: 20 March 2007; accepted for publication:2 June 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 1and use the Approximate Randomization Test to test the statistical significance of the results.Our experiments show that machine-learning-based shallow grammars augmented with so-phisticated automatic dependency annotation technology outperform hand-crafted, deep, wide-coverage constraint grammars.
Currently our best system achieves an f-score of 82.73% againstthe PARC 700 Dependency Bank, a statistically significant improvement of 2.18% over themost recent results of 80.55% for the hand-crafted LFG grammar and XLE parsing systemand an f-score of 80.23% against the CBS 500 Dependency Bank, a statistically significant3.66% improvement over the 76.57% achieved by the hand-crafted RASP grammar and parsingsystem.1.
IntroductionWide-coverage parsers are often evaluated against gold-standard CFG trees (e.g.,Penn-II WSJ Section 23 trees) reporting traditional PARSEVALmetrics (Black et al 1991)of labeled and unlabeled bracketing precision, recall and f-score measures, number ofcrossing brackets, complete matches, and so forth.
Although tree-based parser evalua-tion provides valuable insights into the performance of grammars and parsing systems,it is subject to a number of (related) drawbacks:1.
Bracketed trees do not always provide NLP applications with enoughinformation to carry out the required tasks: Many applications involvea deeper analysis of the input in the form of semantically motivatedinformation such as deep dependency relations, predicate?argumentstructures, or simple logical forms.2.
A number of alternative, but equally valid tree representations canpotentially be given for the same input.
To give just a few examples: InEnglish, VPs containing modals and auxiliaries can be analyzed using(predominantly) binary branching rules (Penn-II [Marcus et al 1994]), oremploy flatter analyses where modals and auxiliaries are sisters of themain verb (AP treebank [Leech and Garside 1991]), or indeed do withouta designated VP constituent at all (SUSANNE [Sampson 1995]).
Treebankbracketing guidelines can use ?traditional?
CFG categories such as S, NP,and so on (Penn-II) or a maximal projection-inspired analysis with IPsand DPs (Chinese Penn Treebank [Xue et al 2004]).3.
Because a tree-based gold standard for parser evaluation must adopt aparticular style of linguistic analysis (reflected in the geometry andnomenclature of the nodes in the trees), evaluation of statistical parsersand grammars that are derived from particular treebank resources (aswell as hand-crafted grammars/parsers) can suffer unduly if the goldstandard deviates systematically from the (possibly) equally valid styleof linguistic analysis provided by the parser.Problems such as these have motivated research on more abstract, dependency-based parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Carrollet al 2002; Clark and Hockenmaier 2002; King et al 2003; Preiss 2003; Kaplan et al2004; Miyao and Tsujii 2004).
Dependency-based linguistic representations are ap-proximations of abstract predicate-argument-adjunct (or more basic head-dependent)82Cahill et al Statistical Parsing Using Automatic Dependency Structuresstructures, providing a more normalized representation abstracting away from theparticulars of surface realization or CFG-tree representation, which enables meaningfulcross-parser evaluation.A related contrast holds between shallow and deep grammars and parsers.1 Inaddition to defining a language (as a set of strings), deep grammars relate strings to in-formation/meaning, often in the form of predicate?argument structure, dependency re-lations,2 or logical forms.
By contrast, a shallow grammar simply defines a language andmay associate syntactic (e.g., CFG tree) representations with strings.
Natural languagesdo not always interpret linguistic material locally where the material is encounteredin the string (or tree).
In order to obtain accurate and complete predicate?argument,dependency, or logical form representations, a hallmark of deep grammars is that theyusually involve a long-distance dependency (LDD) resolution mechanism.Traditionally, deep grammars are hand-crafted (cf.
the ALVEY Natural LanguageTools [Briscoe et al 1987], the Core Language Engine [Alshawi and Pulman 1992], theAlpino Dutch dependency parser [Bouma, van Noord, andMalouf 2000], the Xerox Lin-guistic Environment [Butt et al 2002], the RASP dependency parser [Carroll and Briscoe2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al 2004]).Wide-coverage, deep-grammar development, particularly in rich formalisms such asLFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollardand Sag 1994), is knowledge-intensive, time-consuming and expensive, constitutingan instance of the (in-)famous ?knowledge acquisition bottleneck?
familiar from otherareas in traditional, rule-based AI and NLP.
Very few hand-crafted deep grammars(Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al 2002)have, in fact, been successfully scaled to unrestricted input.The last 15 years have seen extensive efforts on treebank-based automatic gram-mar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995;Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Kleinand Manning 2003).
These grammars are wide-coverage and robust and in contrastto manual grammar development, machine-learning-based grammar acquisition in-curs relatively low development cost.
With few notable exceptions,3 however, thesetreebank-induced wide-coverage grammars are shallow: They usually do not attemptto resolve LDDs nor do they associate strings with meaning representations.Over the last few years, addressing the knowledge acquisition bottleneck in deepconstraint-based grammar development, a growing body of research has emerged to au-tomatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii2003], LFG [Cahill et al 2002b, 2004]).
To a first approximation, these approaches canbe classified as ?conversion?- or ?annotation?-based.
TAG-based approaches convert1 Our use of the terms ?shallow?
and ?deep?
parsers/grammars follows Kaplan et al (2004) wherea ?shallow parser?
does not relate strings to meaning representations.
This deviates from a morecommon use of the terms where, for example, a ?shallow parser?
refers to (often finite-state-based)parsers (or chunkers) that may produce partial bracketings of input strings.2 By dependency relations we mean deep, fine-grained, labeled dependencies that encode long-distancedependencies and passive information, for example.
These differ from the types of unlabeleddependency relations in other work such as (McDonald and Pereira 2006).3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces.
CollinsModel 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide rangeof LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displacedmaterial with where it is to be interpreted semantically.
The work of Dienes and Dubey (2003) andLevy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top ofCFG-based parsers.
None of them map strings into dependencies.83Computational Linguistics Volume 34, Number 1treebank trees into (lexicalized) elementary or adjunct trees.
CCG-based approachesconvert trees into CCG derivations fromwhich CCG categories can be extracted.
HPSG-and LFG-based grammar induction methods automatically annotate treebank treeswith (typed) attribute-value structure information for the extraction of constraint-basedgrammars and lexical resources.Two recent papers (Preiss 2003; Kaplan et al 2004) have started tying togetherthe research strands just sketched: They use dependency-based parser evaluation tocompare wide-coverage parsing systems using hand-crafted, deep, constraint-basedgrammars with systems based on a simple version of treebank-based deep grammaracquisition technology in the conversion paradigm.
In the experiments, tree outputgenerated by Collins?s Model 1, 2, and 3 (1999) and Charniak?s (2000) parsers, forexample, are automatically translated into dependency structures and evaluated againstgold-standard dependency banks.Preiss (2003) uses the grammatical relations and the CBS 500 Dependency Bankdescribed in Carroll, Briscoe, and Sanfilippo (1998) to compare a number of parsingsystems (Briscoe and Carroll 1993; Collins?s 1997 models 1 and 2; and Charniak 2000)using a simple version of the conversion-based deep grammar acquisition process (i.e.,reading off grammatical relations fromCFG parse trees produced by the treebank-basedshallow parsers).
The article also reports on a task-based evaluation experiment to rankthe parsers using the grammatical relations as input to an anaphora resolution system.Preiss concluded that parser ranking using grammatical relations reflected the absoluteranking (between treebank-induced parsers) using traditional tree-based metrics, butthat the difference between the performance of the parsing algorithms narrowed whenthey carried out the anaphora resolution task.
Her results show that the hand-crafteddeep unification parser (Briscoe and Carroll 1993) outperforms the machine-learnedparsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precisionand recall on grammatical relations.4 Kaplan et al (2004) compare their deep, hand-crafted, LFG-based XLE parsing system (Riezler et al 2002) with Collins?s (1999) model3 using a simple conversion-based approach, capturing dependencies from the treeoutput of the machine-learned parser, and evaluating both parsers against the PARC700 Dependency Bank (King et al 2003).
They conclude that the hand-crafted, deepgrammar outperforms the state-of-the-art treebank-based shallow parser on the level ofdependency representation, at the price of a small decrease in parsing speed.Both Preiss (2003) and Kaplan et al (2004) emphasize that they use rather basicversions of the conversion-based deep grammar acquisition technology outlined herein.In this article we revisit the experiments carried out by Preiss and Kaplan et al, this timeusing the sophisticated and fine-grained treebank- and annotation-based, deep, probabilis-tic LFG grammar acquisitionmethodology developed in Cahill et al (2002b), Cahill et al(2004), O?Donovan et al (2004), and Burke (2006) with a number of surprising results:1.
Evaluating against the PARC 700 Dependency Bank (King et al 2003)using a retrained version of Bikel?s (2002) parser, the best automaticallyinduced, deep LFG resources achieve an f-score of 82.73%.
This is animprovement of 3.13 percentage points over the previously best publishedresults established by Kaplan et al (2004) who use a hand-crafted,wide-coverage, deep LFG and the XLE parsing system.
This is also a4 The numbers given are difficult to compare as the results for the Briscoe and Carroll (1993) parser werecaptured for a richer set of grammatical relations than those for Collins (1997) and Charniak (2000).84Cahill et al Statistical Parsing Using Automatic Dependency Structuresstatistically significant improvement of 2.18 percentage points over themost recent improved results presented in this article for the XLE system.2.
Evaluating against the Carroll, Briscoe, and Sanfilippo (1998) CBS 500gold-standard dependency bank using a retrained version of Bikel?s (2002)parser, the best Penn-II treebank-based, automatically acquired, deep LFGresources achieve an f-score of 80.23%.
This is a statistically significantimprovement of 3.66 percentage points over Carroll and Briscoe (2002),who use a hand-crafted, wide-coverage, deep, unification grammar andthe RASP parsing system.Evaluation results on a reannotated version (Briscoe and Carroll 2006) of the PARC700 Dependency Bank were recently published in Clark and Curran (2007), reportingf-scores of 81.9% for the CCG parser, and 76.3% for RASP.
As Briscoe and Carroll(2006) point out, these evaluations are not directly comparable with the Kaplan et al(2004) style evaluation against the original PARC 700 Dependency Bank, because theannotation schemes are different.The article is structured as follows: In Section 2, we outline the automatic LFGf-structure annotation algorithm and the pipeline parsing architecture of Cahill et al(2002b), Cahill et al (2004), and Burke (2006).
In Section 3, we present our experimentdesign.
In Section 4, using the DCU 105 Dependency Bank as our development set, weevaluate a number of treebank-induced LFG parsing systems against the automaticallygenerated Penn-II WSJ Section 22 Dependency Bank test set.
We use the ApproximateRandomization Test (Noreen 1989) to test for statistical significance and choose the bestparsing system for the evaluations against the wide-coverage, hand-crafted RASP andLFG grammars of Carroll and Briscoe (2002) and Kaplan et al (2004) using the CBS500 and PARC 700 Dependency Banks in Section 5.
In Section 6, we discuss results andissues raised by our methodology, outline related and future research and conclude inSection 7.2.
MethodologyIn this section, we briefly outline LFG and present our automatic f-structure annotationalgorithm and parsing architecture.
The parsing architecture enables us to integratePCFG- and history-based parsers, which allows us to compare these parsers at the levelof dependency structures, rather than just trees.2.1 Lexical Functional GrammarLexical Functional Grammar (LFG) (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple2001) is a constraint-based theory of grammar.
It (minimally) posits two levels ofrepresentation, c(onstituent)-structure and f(unctional)-structure.
C-structure is rep-resented by context-free phrase-structure trees, and captures surface grammaticalconfigurations such as word order.
The nodes in the trees are annotated with functionalequations (attribute-value structure constraints, for example (?OBJ)=?)
which areresolved (in the case of well-formed strings) to produce an f-structure.
F-structuresare recursive attribute-value matrices, representing abstract syntactic functions, which85Computational Linguistics Volume 34, Number 1Figure 1C- and f-structures for the sentence U.N. signs treaty.approximate to basic predicate-argument-adjunct structures or dependency relations.5Figure 1 shows the c- and f-structures for the string U.N. signs treaty.
Each node in thec-structure is annotated with f-structure equations, for example (?
SUBJ)= ?.
Theuparrows (?)
point to the f-structure associated with the mother node, downarrows(?)
to that of the local node.
In a complete parse tree, these ?
and ?
meta variables areinstantiated to unique tree node identifiers and a set of constraints (a set of terms in anequality logic) is generated which (if satisfiable) generates an f-structure.2.2 Automatic F-Structure Annotation AlgorithmDeep grammars can be induced from treebank resources if the treebank encodesenough information to support the derivation of deep grammatical information, suchas predicate?argument structures, deep dependency relations, or logical forms.
Manysecond generation treebanks such as Penn-II provide information to support the compi-lation of meaning representations, for example in the form of traces relating displacedlinguistic material to where it should be interpreted semantically.
The f-structure anno-tation algorithm exploits configurational and categorial information, as well as tracesand the Penn-II functional tag annotations (Table 1) to automatically associate Penn-IICFG trees with LFG f-structure information.Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traversethe tree and deterministically add f-structure equations to the phrasal and leaf nodesof the tree, resulting in an f-structure annotated version of the tree.
The annotations arethen collected and passed on to a constraint solver which generates an f-structure (if theconstraints are satisfiable).
We use a simple graph-unification-based constraint solver(Eisele and Do?rre 1986), extended to handle path, set-valued, disjunctive, and existentialconstraints.
Given parser output without Penn-II style annotations and traces, the samealgorithm is used to assign annotations to each node in the tree, whereas a separatemodule is applied at the level of f-structure to resolve any long-distance dependencies(see Section 2.3).5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms(QLFs), and Underspecified Discourse Representation Structures (UDRSs).86Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 1A complete list of the Penn-II functional labels.Tag DescriptionForm/function discrepancies-ADV clausal and NP adverbials-NOM non NPs that function as NPsGrammatical role-DTV dative-LGS logical subjects in passives-PRD non VP predicates-PUT locative complement of put-SBJ surface subject-TPC topicalized and fronted constituents-VOC vocativesAdverbials-BNF benefactive-DIR direction and trajectory-EXT extent-LOC location-MNR manner-PRP purpose and reason-TMP temporal phrasesMiscellaneous-CLR closely related to verb-CLF true clefts-HLN headlines and datelines-TTL titlesThe f-structure annotation algorithm is described in detail in Cahill et al (2002a),McCarthy (2003), Cahill et al (2004), and Burke (2006).
In brief, the algorithm is modularwith four components (Figure 3), taking Penn-II trees as input and automatically addingLFG f-structure equations to each node in the tree.Lexical Information.
Lexical information is generated automatically by macros for eachof the POS classes in Penn-II.
To give a simple example, third-person plural nounPenn-II POS-word sequences of the form NNS word are automatically associated withthe equations (?PRED) = word?, (?NUM) = pl and (?PERS) = 3rd, where word?
is thelemmatized word.Left?Right Context Annotation.
The Left?Right context annotation component identifiesthe heads of Penn-II trees using a modified version of the head finding rules ofMagerman (1994).
This partitions each local subtree (of depth one) into a local head, aleft context (left sisters), and a right context (right sisters).
The contexts together withinformation about the local mother and daughter categories and (if present) Penn-II87Computational Linguistics Volume 34, Number 1Figure 2Trees for the sentence U.N. signs treaty, the headline said before and after automatic f-structureannotation, with the f-structure automatically produced.Figure 3F-structure annotation algorithm modules.88Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 2Sample from an NP Annotation matrix.Left context Head Right contextDT: (?SPEC DET)=?
NN, NNS, NNP, NNPS, NP: RRC, SBAR: (?RELMOD)=?CD: (?SPEC QUANT)=?
?=?
PP: ??
(?ADJUNCT)ADJP, JJ, NN, NNP: ??
(?ADJUNCT) NP: ??
(?APP)functional tag labels (Table 1) are used by the f-structure annotation algorithm.
For eachPenn-II mother (i.e., phrasal) category an Annotation matrix expresses generalizationsabout how to annotate immediate daughters dominated by the mother category relativeto their location in relation to the local head.
To give a (much simplified) example,the head finding rules for NPs state that the rightmost nominal (NN, NNS, NNP, .
.
.
)not preceded by a comma or ?-?6 is likely to be the local head.
The Annotation ma-trix for NPs states (inter alia) that heads are annotated ?=?, that DTs (determiners)to the left of the head are annotated (?
SPEC DET) = ?, NPs to the right of the head as??(?
APP) (appositions).
Table 2 provides a sample extract from the NP Annotationmatrix.
Figure 4 provides an example of the application of the NP and PP Annotationmatrices to a simple tree.For each phrasal category, Annotation matrices are constructed by inspecting themost frequent Penn-II rule types expanding the category such that the token occurrencesof these rule types cover more than 85% of all occurrences of expansions of that categoryin Penn-II.
For NP rules, for example, this means that we analyze the most frequent102 rule types expanding NP, rather than the complete set of more than 6,500 Penn-IINP rule types, in order to populate the NP Annotation matrix.
Annotation matricesgeneralize to unseen rule types as, in the case of NPs, these may also feature DTs tothe left of the local head and NPs to the right and similarly for rule types expandingother categories.Coordination.
In order to support the modularity, maintainability, and extendability ofthe annotation algorithm, the Left?Right Annotation matrices apply only to local treesof depth one, which do not feature coordination.
This keeps the statement of Annotationmatrices perspicuous and compact.
The Penn-II treatment of coordination is (inten-tionally) flat.
The annotation algorithm has modules for like- and unlike-constituentcoordination.
Coordinated constituents are elements of a COORD set and annotated ??(?
COORD).
The Coordination module reuses the Left?Right context Annotation ma-trices to annotate any remaining nodes in a local subtree containing a coordinatingconjunction.
Figure 5 provides a VP-coordination example (with right-node-raising).Catch-All and Clean-Up.
The Catch-All and Clean-Up module provides defaults to cap-ture remaining unannotated nodes (Catch-All) and corrects (Clean-Up) overgeneraliza-tions resulting from the application of the Left?Right context Annotation matrices.
TheLeft?Right Annotation matrices are allowed a certain amount of overgeneralization asthis facilitates the perspicuous statement of generalizations and a separate statement ofexceptions, supporting the modularity and maintainability of the annotation algorithm.PPs under VPs are a case in point.
The VP Annotation matrix analyses PPs to the rightof the local VP head as adjuncts: ?
?
(?ADJUNCT).
The Catch-All and Clean-Up module6 If the rightmost nominal is preceded by a comma or ?-?, it is likely to be an apposition to the head.89Computational Linguistics Volume 34, Number 1Figure 4Automatically annotated Penn-II tree (fragment) and f-structure (simplified) for Gerry Purdy,director of marketing.uses Penn-II functional tag (Table 1) information (if present), for example -CLR (closelyrelated to local head), to replace the original adjunct analysis by an oblique argumentanalysis: (?OBL)=?.
An example of this is provided by the PP-CLR in the left VP-conjunctin Figure 5.
In other cases, argument?adjunct distinctions are encoded configurationallyin Penn-II (without the use of -CLR tags).
To give a simple example, the NP Anno-tation matrix indiscriminately associates SBARs to the right of the local head with(?
RELMOD) = ?.
However, some of these SBARs are actually arguments of the localNP head and, unlike SBAR relative clauses which are Chomsky-adjoined to NP (i.e.,relative clauses are daughters of an NP mother and sisters of a phrasal NP head), SBARarguments are sisters of non-phrasal NP heads.7 In such cases, the Catch-All and Clean-Up module rewrites the original relative clause analysis into the correct complementargument analysis (?COMP)=?.
Figure 6 shows the COMP f-structure analyses for anexample NP containing an internal SBAR argument (rather than relative clause) node.Traces.
The Traces module translates traces and coindexed material in Penn-II treesrepresenting long-distance dependencies into corresponding reentrancies at f-structure.Penn-II provides a rich arsenal of trace types to relate ?displaced?
material to where it7 Structural information of this kind is not encoded in the Annotation matrices; compare Table 2.90Cahill et al Statistical Parsing Using Automatic Dependency StructuresFigure 5Automatically annotated Penn-II tree (fragment) and resulting f-structure for asked for andreceived refunds.should be interpreted semantically.
The f-structure annotation algorithm coverswh- andwh-less relative clause constructions, interrogatives, control and raising constructions,right-node-raising, and general ICH (interpret constituent here) traces.
Figure 5 gives anexample that shows the interplay between coordination, right-node-raising traces andthe corresponding automatically generated reentrancies at f-structure.2.3 Parsing ArchitectureThe pipeline parsing architecture of Cahill et al (2004) and Cahill (2004) for parsing rawtext into LFG f-structures is shown in Figure 7.
In this model, PCFGs or history-basedlexicalized parsers are extracted from the unannotated treebank and used to parse rawtext into trees.
The resulting parse trees are then passed to the automatic f-structureannotation algorithm to generate f-structures.8Compared to full Penn-II treebank trees, the output of standard probabilisticparsers is impoverished: Parsers do not normally output Penn-II functional tag an-notations (Table 1) nor do they indicate/resolve long-distance dependencies, recorded8 In the integratedmodel (Cahill et al 2004; Cahill 2004), we extract f-structure annotated PCFGs(A-PCFGs) from the f-structure annotated treebank, where each non-terminal symbol in the grammarhas been augmented with LFG functional equations, such as NP[?OBJ=?]
?
DT[?SPEC=?]
NN[?=?
].We treat a non-terminal symbol followed by annotations as a monadic category for grammar extractionand parsing.
Parsing with A-PCFGs results in annotated parse trees, from which an f-structure can begenerated.
In this article we only use the pipeline parsing architecture.91Computational Linguistics Volume 34, Number 1Figure 6Automatically annotated Penn-II tree (fragment) and f-structure for signs that managersexpect declines.in terms of a fine-grained system of empty productions (traces) and coindexation inthe full Penn-II treebank trees.
The f-structure annotation algorithm, as described inSection 2.2, makes use of Penn-II functional tag information (if present) and relies ontraces and coindexation to capture LDDs in terms of corresponding reentrancies atf-structure.Penn-II functional labels are used by the annotation algorithm to discriminatebetween adjuncts and (oblique) arguments.
PP-sisters to a head verb are analyzed asarguments iff they are labeled -CLR, -PUT, -DTV or -BNF, for example.
Conversely,functional labels (e.g., -TMP) are also used to analyze certain NPs as adjuncts, and-LGS labels help to identify logical subjects in passive constructions.
In the absence offunctional labels, the annotation algorithm will default to decisions based on simplestructural, configurational, and CFG-category information (and, for example, conserva-tively analyze a PP sister to a head verb as an adjunct, rather than as an argument).In Sections 3 and 4 we present a number of treebank-based parsers (in particular thePCFGs and a version of Bikel?s history-based, lexicalized generative parser) trained tooutput CFG categories with Penn-II functional tags.
We achieve this through a simple92Cahill et al Statistical Parsing Using Automatic Dependency StructuresFigure 7Treebank-based LFG parsing architecture.masking and un-masking operation where functional tags are joined with their localCFG category label to form a new (larger) set of (monadic) CFG category labels (e.g.,PP-CLR goes to PP CLR) for training and parsing (for Bikel, the parser head-finding rulesare also adjusted to the expanded set of categories).
After parsing, the Penn-II functionaltags are unmasked and available to the f-structure annotation algorithm.The Traces component in the f-structure annotation algorithm (Figure 3) translatesLDDs represented in terms of traces and coindexation in the original Penn-II treebanktrees into corresponding reentrancies at f-structure.
Most probabilistic treebank-basedparsers, however, do not indicate/resolve LDDs, and the Traces component of the an-notation algorithm does not apply.
Initially, the f-structures produced for parser outputtrees in the architecture in Figure 7 are therefore LDD-unresolved: They are incomplete(or proto) f-structures, where displaced material (e.g., the values of FOCUS, TOPIC, andTOPICREL attributes [wh- and wh-less relative clauses, topicalization, and interrogativeconstructions] at f-structure) is not yet linked to the appropriate argument grammati-cal functions (or elements of adjunct sets) for the governing local PRED.
A dedicatedLDD Resolution component in the architecture in Figure 7 turns parser output proto-f-structures into fully LDD-resolved proper f-structures, without traces and coindexa-tion in parse trees.Consider the following fragment of a proper Penn-II treebank tree (Figure 8), wherethe LDD between the WHNP in the relative clause and the embedded direct objectposition of the verb reward is indicated in terms of the trace *T*-3 and its coindexationwith the antecedent WHNP-3.
Note further that the control relation between the subjectof the verbs wanted and reward is similarly expressed in terms of traces (*T*-2) andcoindexation (NP-SBJ-2).
From the treebank tree, the f-structure annotation algorithmis able to derive a fully resolved f-structure where the LDD and the control relation arecaptured in terms of corresponding reentrancies (Figure 9).93Computational Linguistics Volume 34, Number 1Figure 8Penn-II treebank tree with LDD indicated in terms of traces (empty productions) andcoindexation and f-structure annotations generated by the annotation algorithm.Now consider the corresponding ?impoverished?
(but otherwise correct) parseroutput tree (Figure 10) for the same string: The parser output does not explicitly recordthe control relation nor the LDD.Given this parser output tree, prior to the LDD resolution component in the parsingarchitecture (Figure 7), the f-structure annotation algorithmwould initially construct thepartial (proto-) f-structure in Figure 11, where the LDD indicated by the TOPICREL func-tion is unresolved (i.e., the value of TOPICREL is not coindexedwith the OBJ grammaticalfunction of the embedded verb reward).
The control relation (shared subject betweenthe two verbs in the relative clause) is in fact captured by the annotation algorithm interms of a default annotation (?
SUBJ) = (?
SUBJ) on sole argument VPs to the right ofhead verbs (as often, even in the full Penn-II treebank trees, control relations are notconsistently captured through explicit argument traces).In LFG, LDD resolution operates at the level of f-structure, using functional un-certainty equations (regular expressions over paths in f-structure [Kaplan and Zaenen1989] relating f-structure components in different parts of an f-structure), obviatingtraces and coindexation in c-structure trees.
For the example in Figure 10, a functionaluncertainty equation of the form (?TOPICREL) = (?[COMP|XCOMP]?
[SUBJ|OBJ]) wouldbe associated with the WHNP daughter node of the SBAR relative clause.
The equationstates that the value of the TOPICREL attribute is token-identical (re-entrant) with thevalue of a SUBJ or OBJ function, reached through a path along any number (including94Cahill et al Statistical Parsing Using Automatic Dependency StructuresFigure 9Fully LDD-resolved f-structure.Figure 10Impoverished parser output tree: LDDs not captured.zero) of COMP or XCOMP attributes.
This equation, together with subcategorizationframes (LFG semantic forms) for the local PREDs and the usual LFG completeness andcoherence conditions, resolve the partial proto-f-structure in Figure 11 into the fullyLDD-resolved proper f-structure in Figure 9.95Computational Linguistics Volume 34, Number 1Figure 11Proto-f-structure: LDDs not captured.Following Cahill et al (2004), in our parsing architecture (Figure 7) we modelLFG LDD resolution using automatically induced finite approximations of functional-uncertainty equations and subcategorization frames from the f-structure-annotatedPenn-II treebank (O?Donovan et al 2004) in an LDD resolution component.
From thefully LDD-resolved f-structures from the Penn-II training section treebank trees welearn probabilistic LDD resolution paths (reentrancies in f-structure), conditional onLDD type (Table 3), and subcategorization frames, conditional on lemma (and voice)(Table 4).
Table 3 lists the eight most probable TOPICREL paths (out of a total of 37TOPICREL paths acquired).
The totality of these paths constitutes a finite subset of thereference language definde by the full functional uncertainty equation (?TOPICREL) =(?[COMP|XCOMP]?
[SUBJ|OBJ]).
Given an unresolved LDD type (such as TOPICREL inthe parser output for the relative clause example in Figure 11), admissible LDD res-olutions assert a reentrancy between the value of the LDD trigger (here, TOPICREL)and a grammatical function (or adjunct set element) of an embedded local predicate,subject to the conditions that (i) the local predicate can be reached from the LDD triggerusing the LDD path; (ii) the grammatical function terminates the LDD path; (iii) thegrammatical function is not already present (at the relevant level of embedding inthe local f-structure); and (vi) the local predicate subcategorizes for the grammaticalfunction in question.9 Solutions satisfying (i)?
(iv) are ranked using the product ofLDD path and subcategorization frame probabilities and the highest ranked solution(possibly involving multiple interacting LDDs for a single f-structure) is returned bythe algorithm (for details and comparison against alternative LDD resolution methods,see Cahill et al 2004).10For our example (Figure 11), the highest ranked LDD resolution is for LDD path(?TOPICREL) = (?
XCOMP OBJ) and the local subcat frame REWARD??
SUBJ, ?
OBJ?.
This9 Conditions (i)?
(iv) are suitably adapted for LDD resolutions terminating in adjunct sets.10 In our experiments we do not use the limited LDD resolution for wh-phrases provided by Collins?s Model3 parser as better results are achieved using the purely f-structure-based LDD resolution as shown inCahill et al (2004).96Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 3Most frequent wh-TOPICREL paths.wh-TOPICREL Probability wh-TOPICREL Probabilitysubj .7583 xcomp .0830obj .0458 xcomp:obj .0338xcomp:xcomp .0168 xcomp:subj .0109comp .0097 comp:subj .0073Table 4Most frequent semantic forms for active and passive (p) occurrences of the verb want andreward.Semantic form Probabilitywant([subj,xcomp]) .6208want([subj,obj]) .2496want([subj,obj,xcomp]) .1008want([subj]) .0096want([subj,obj,obl]) .0048want([subj,obj,part]),p) .5000want([subj,obl]),p) .1667want([subj,part]),p) .1667want([subj]),p) .1667reward([subj,obj]) .8000reward([subj,obj,obl]) .2000reward([subj]),p) 1.0000(together with the subject control equation described previously) turns the parser-output proto-f-structure (in Figure 11) into the fully LDD resolved f-structure in(Figure 9).The full pipeline parsing architecture with the LDD resolution (rather than theTraces component for LDD resolved Penn-II treebank trees) component (and the LDDpath and subcategorization frame extraction) is given in Figure 7.The pipeline architecture supports flexible integration of treebank-based PCFGsor state-of-the-art, history-based, and lexicalized parsers (Collins 1999; Charniak 2000;Bikel 2002) and enables dependency-based evaluation of such parsers.3.
Experiment DesignIn our experiments we compare four history-based parsers for integration into thepipeline parsing architecture described in Section 2.3: Collins?s 1999 Models 311 Charniak?s 2000 maximum-entropy inspired parser1211 Downloaded from ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz.12 Downloaded from ftp://ftp.cs.brown.edu/pub/nlparser/.97Computational Linguistics Volume 34, Number 1 Bikel?s 2002 emulation of Collins Model 213 a retrained version of Bikel?s (2002) parser which retains Penn-II functionaltagsInput for Collins?s and Bikel?s parsers was pre-tagged using the MXPOST POS tag-ger (Ratnaparkhi 1996).
Charniak?s parser provides its own POS tagger.
The combinedsystem of best history-based parser and automatic f-structure annotation is comparedto two probabilistic parsing systems based on hand-crafted, wide-coverage, constraint-based, deep grammars: the RASP parsing system (Carroll and Briscoe 2002) the XLE parsing system (Riezler et al 2002; Kaplan et al 2004)Both hand-crafted grammars perform their own POS tagging, resolve LDDs, andassociate strings with dependency relations (in the form of grammatical relations orLFG f-structures).We evaluate the parsers against a number of gold-standard dependency banks.We use the DCU 105 Dependency Bank (Cahill et al 2002a) as our development setfor the treebank-based LFG parsers.
We use the f-structure annotation algorithm toautomatically generate a gold-standard test set from the original Section 22 treebanktrees (the f-structure annotated WSJ Section 22 Dependency Bank) to choose the besttreebank-based LFG parsing systems for the PARC 700 and CBS 500 experiments.Following the experimental setup in Kaplan et al (2004), we use the Penn-II Section 23-based PARC 700 Dependency Bank (King et al 2003) to evaluate the treebank-inducedLFG resources against the hand-crafted XLE grammar and parsing system of Riezleret al (2002) and Kaplan et al Following Preiss (2003), we use the SUSANNE Based CBS500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998) to evaluate the treebank-induced LFG resources against the hand-crafted RASP grammar and parsing system(Carroll and Briscoe 2002) as well as against the XLE system (Riezler et al 2002).For each gold standard, our experiment design is as follows: We parse automati-cally tagged input14 sentences with the treebank- and machine-learning-based parserstrained on WSJ Sections 02?21 in the pipeline architecture, pass the resulting parsetrees to our automatic f-structure annotation algorithm, collect the f-structure equations,pass them to a constraint-solver which generates an f-structure, resolve long-distancedependencies at f-structure following Cahill et al (2004) and convert the resulting LDD-resolved f-structures into dependency representations using the formats and softwareof Crouch et al (2002) (for the DCU 105, PARC 700, and WSJ Section 22 evaluations)and the formats and software of Carroll, Briscoe, and Sanfilippo (1998) (for the CBS500 evaluation).
In the experiments we did not use any additional annotations such as-A (for argument) that can be generated by some of the history-based parsers (Collins1999) as the f-structure annotation algorithm is designed for Penn-II trees (which donot contain such annotations).
We also did not use the limited LDD resolution for wh-relative clauses provided by Collins?s Model 3 as better results are achieved by LDD13 This was developed at the University of Pennsylvania by Dan Bikel and is freely available to downloadfrom http://www.cis.upenn.edu/?dbikel/software.html.14 Tags were automatically assigned either by the parsers themselves or by the MXPOST tagger(Ratnaparkhi 1996).98Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 5Results of tree-based evaluation on all sentences WSJ section 23, Penn-II.Parser Labeledf-score (%)PCFG 73.03Parent-PCFG 78.05Collins M3 88.33Charniak 89.73Bikel 88.32Bikel+Tags 87.53resolution on f-structure (Cahill et al 2004).
A complete set of parameter settings for theparsers is provided in the Appendix.In order to evaluate the treebank-induced LFG resources against the PARC 700and the CBS 500 dependency banks, a certain amount of automatic mapping is re-quired to account for systematic differences in linguistic analysis, feature geometry,and nomenclature at the level of dependencies.
This is discussed in Sections 5.1 and5.2.
Throughout, we use the Approximate Randomization Test (Noreen 1989) to test thestatistical significance of the results.4.
Choosing a Treebank-Based LFG Parsing SystemIn this section, we choose the best treebank-based LFG parsing system for the compar-isons with the hand-crafted XLE and RASP resources in Section 5.
We use the DCU105 Dependency Bank as our development set and carry out comparative evaluationand statistical significance testing on the larger, automatically generated WSJ Section 22Dependency Bank as a test set.
The system based on Bikel?s (2002) parser retrained toretain Penn-II functional tags (Table 1) achieves overall best results.4.1 Tree-Based Evaluation against WSJ Section 23For reference, we include the traditional CFG-tree-based comparison for treebank-induced parsers.
The parsers are trained on Sections 02 to 21 of the Penn-II Treebank andtested on Section 23.
The published results15 on these experiments for the history-basedparsers are given in Table 5.
We also include figures for a PCFG and a Parent-PCFG (aPCFG which has undergone the parent transformation [Johnson 1999]).
These PCFGsare induced following standard treebank preprocessing steps, including elimination ofempty nodes, but following Cahill et al (2004), they do include Penn-II functional tags(Table 1), as these tags contain valuable information for the automatic f-structure anno-tation algorithm (Section 2.2).
These tags are removed for the tree-based evaluation.The results show that the history-based parsers produce considerably better treesthan the more basic PCFGs (with and without parent transformations).
Charniak?s(2000) parser scores best with an f-score of 89.73% on all sentences in Section 23.
The15 Where there were no published results available for Section 23, we calculated them using thedownloadable versions of the parsers.99Computational Linguistics Volume 34, Number 1vanilla PCFG achieves the lowest f-score of 73.03%, a difference of 16.7 percentagepoints.
The hand-crafted XLE and RASP grammars achieve around 80% coverage(measured in terms of complete spanning parse) on Section 23 and use a variety of(longest) fragments combining techniques to generate dependency representations forthe remaining 20% of Section 23 strings.
By contrast, the treebank-induced PCFGs andhistory-based parsers all achieve coverage of over 99.9%.
Given that the history-basedparsers score considerably better than PCFGs on trees, we would also expect them toproduce dependency structures of substantially higher quality.4.2 Using DCU 105 as a Development SetThe DCU 105 (Cahill et al 2002a) is a hand-crafted gold-standard dependency bankfor 105 sentences, randomly chosen from Section 23 of the Penn-II Treebank.16 This is arelatively small gold standard, initially developed to evaluate the automatic f-structureannotation algorithm.
We parse the 105 tagged sentences into LFG f-structures witheach of the treebank-induced parsers in the pipeline parsing and f-structure annotationarchitecture.
The f-structures of the gold standard and the f-structures returned by theparsing systems are converted into dependency triples following Crouch et al (2002)and Riezler et al (2002) and we also use their software for evaluation.
The followingdependency triples are produced by the f-structure in Figure 1:subj(sign?0,U.N.?1)obj(sign?0,treaty?2)num(U.N.?1,sg)pers(U.N.
?1,3)num(treaty?2,sg)pers(treaty?3,3)tense(sign?0,present)We evaluate preds-only f-structures (i.e., where paths in f-structures end in a PREDvalue: the predicate-argument-adjunct structure skeleton) and all grammatical func-tions (GFs) including number, tense, person, and so on.
The results are given in Table 6.With one main exception, Tables 5 and 6 confirm the general expectation thatthe better the trees produced by the parsers, the better the f-structures automaticallygenerated for those trees.
The exception is Bikel+Tags.
The automatic f-structure an-notation algorithm will exploit Penn-II functional tag information if present to generateappropriate f-structure equations (see Section 2.2).
It will default to possibly less reliableconfigurational and categorial information if Penn-II tags are not present in the trees.In order to test whether the retention of Penn-II functional labels in the history-based parser output will improve LFG f-structure-based dependency results, we useBikel?s (2002) training software,17 and retrain the parser on a version of the Penn-IItreebank (Sections 02 to 21) with the Penn-II functional tag labels (Table 1) annotatedin such a way that the resulting history-based parser will retain them (Section 2.3).
Theretrained parser (Bikel+Tags) then produces CFG-trees with Penn-II functional labelsand these are used by the f-structure annotation algorithm.
We evaluate the f-structuredependencies against the DCU 105 (Table 6) and achieve an f-score of 82.92% preds-only16 It is publicly available for download from: http://nclt.computing.dcu.ie/gold105.txt.17 We use Bikel?s software rather than Charniak?s for this experiment as the former proved more stableduring the retraining phase.100Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 6Treebank-induced parsers: results of dependency-based evaluation against DCU 105.Parser Preds only All GFsf-score (%) f-score (%)PCFG 70.24 79.90Parent-PCFG 75.84 83.58Collins M3 77.84 85.08Charniak 79.61 85.66Bikel 79.39 86.56Bikel+Tags 82.92 88.30Table 7Treebank induced parsers: breakdown by dependency relation of preds-only evaluation againstDCU 105.Dep.
Percent of total F-score (%)Parent-PCFG Collins M3 Charniak Bikel Bikel+TagsADJUNCT 33.73 71 72 76 73 79APP 0.68 61 0 55 70 65COMP 2.31 60 61 66 61 73COORD 5.73 64 73 77 67 76DET 9.58 91 93 96 96 96FOCUS 0.04 100 100 0 100 100OBJ 16.42 82 84 86 85 90OBJ2 0.07 80 57 50 57 50OBL 2.17 58 24 27 23 63OBL2 0.07 50 0 0 0 67OBL AG 0.43 40 96 96 92 92POSS 2.88 80 83 82 82 79QUANT 1.85 70 67 69 70 70RELMOD 1.78 50 78 67 78 73SUBJ 14.74 80 81 83 85 85TOPIC 0.46 85 87 96 96 89TOPICREL 1.85 61 80 80 79 74XCOMP 5.20 90 92 79 93 93and 88.3% all GFs.
A detailed breakdown by dependency is given in Table 7.
The systembased on the retrained parser is nowmuch better able to identify oblique arguments andoverall preds-only accuracy has improved by 3.53% over the original Bikel experimentand 3.31% over Charniak?s parser, even though Charniak?s parser performs more than2% better on the tree-based scores in Table 5 and even though the retrained parser drops0.79% against the original Bikel parser on the tree-based scores.18Inspection of the results broken down by grammatical function (Table 7) for thepreds-only evaluation against the DCU 105 shows that just over one third of all depen-dency triples in the gold standard are adjuncts.
SUBJ(ects) and OBJ(ects) together makeup a further 30%.18 The figures suggest that retraining Charniak?s parser to retain Penn-II functional tags is likely to produceeven better dependency scores than those achieved by Bikel?s retrained parser.101Computational Linguistics Volume 34, Number 1Table 7 shows that the treebank-based LFG system using Collins?s Models 3 isunable to identify APP(osition).
This is due to Collins?s treatment of punctuation andthe fact that punctuation is often required to reliably identify apposition.19 None ofthe original history-based parsers produced trees which enabled the annotation algo-rithm to identify second oblique dependencies (OBL2), and they generally performedconsiderably worse than Parent-PCFG when identifying OBL(ique) dependencies.
Thisis because the automatic f-structure annotation algorithm is cautious to the point ofundergeneralization when identifying oblique arguments.
In many cases, the algorithmrelies on the presence of, for example, a -CLR Penn-II functional label (indicating that thephrase is closely related to the verb), and the history-based (Collins M3, Charniak, andBikel) parsers do not produce these labels, whereas Parent-PCFG (as well as PCFG) aretrained to retain Penn-II functional labels.
Parent-PCFG, by contrast, performs poorlyfor oblique agents (OBL AG, agentive by-phrases in passive constructions), whereas thehistory-based parsers are able to identify these with considerable accuracy.
This is be-cause Parent-PCFG often erroneously finds oblique agents, even when the prepositionis not by, as it never has enough context in which to distinguish by prepositional phrasesfrom other PPs.
The history-based parsers produce trees from which the automaticf-structure annotation algorithm can better identify RELMOD and TOPICREL dependen-cies than Parent-PCFG.
This, in turn, leads to improved long distance dependencyresolution which improves overall accuracy.The DCU 105 development set is too small to support reliable statistical significancetesting of the performance ranking of the six treebank-based LFG parsing systems.
Inorder to carry out significance testing to select the best treebank-based LFG parsingsystem for comparative evaluation against the hand-crafted deep XLE and RASP re-sources, we move to a larger dependency-based evaluation data set: the gold-standarddependency bank automatically generated fromWSJ Section 22.4.3 Evaluation against WSJ Section 22 DependenciesIn an experimental setup similar to that of Hockenmaier and Steedman (2002),20 weevaluate each parser against a large automatically generated gold standard.
The gold-standard dependency bank is automatically generated by annotating the original 1,700treebank trees from WSJ Section 22 of the Penn-II Treebank with our f-structure an-notation algorithm.
We then evaluate the f-structures generated from the tree outputof the six parsers trained on Sections 02 to 21 resulting from parsing the Section 22strings against the automatically produced f-structures for the original Section 22 Penn-IItreebank trees.
The results are given in Table 8.Compared to Table 6 for the DCU 105 gold standard, most scores are up, particularlyso for the history-based parsers.
This trend is possibly due to the fact that the WSJ19 The annotation algorithm relies on Penn-II-style punctuation patterns where an NP apposition follows anominal head separated by a comma ([NP [NP Bush ] , [NP the president ] ]), all three sisters of the samemother node, while the trees produced by Collins?s parser attach the comma low in the tree ([NP [NPBush,] [NP the president ] ]).
Although it would be trivial to carry out a tree transformation on the Collinsoutput to raise the punctuation to the expected level, we have not done this here.20 This corresponds to experiments where the original Penn-II Section 23 treebank trees are automaticallyconverted into CCG derivations, which are then used as a gold standard to evaluate the CCG parsertrained on Sections 02?21.
A similar methodology is used for the evaluation of treebank-based HPSGresources (Miyao, Ninomiya, and Tsujii 2003) where Penn-II treebank trees are automatically annotatedwith HPSG typed-feature structure information.102Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 8Results of dependency-based evaluation against the automatically generated gold standard forWSJ Section 22.Parser Preds only All GFsf-score (%) f-score (%)PCFG 70.76 80.44Parent-PCFG 74.92 83.04Collins M3 79.30 86.00Charniak 81.35 86.96Bikel 81.40 87.00Bikel+Tags 83.06 87.63Section 22 gold standard is generated automatically from the original ?perfect?
Penn-IItreebank trees using the automatic f-structure annotation algorithm, whereas the DCU105 has been created manually without regard as to whether or not the f-structureannotation algorithm could ever generate the f-structures, even given the ?perfect?trees.The LFG system based on Bikel?s retrained parser achieves the highest f-score of83.06% preds-only and 87.63% all GFs.
Parent-PCFG achieves an f-score of 74.92%preds-only and 83.04% all GFs.
Table 9 provides a breakdown by feature of the preds-only evaluation.Table 9 shows that, once again, the automatic f-structure annotation algorithm isnot able to identify any cases of apposition from the output of Collins?s Model 3 parser.Apart from Bikel?s retrained parser, none of the history-based parsers are able to identifyTable 9Breakdown by dependency of results of preds-only evaluation against the automaticallygenerated Section 22 gold standard.Dep.
Percent of total F-score (%)Parent-PCFG Collins M3 Charniak Bikel Bikel+TagsADJUNCT 33.77 70 75 78 78 80APP 0.74 61 0 77 77 71COMP 1.35 60 72 70 70 80COORD 5.11 74 78 82 82 81DET 10.72 88 91 92 92 91FOCUS 0.02 27 67 88 88 71OBJ 16.17 80 85 87 87 88OBJ2 0.07 15 30 32 32 71OBL 1.92 50 19 21 21 73OBL2 0.07 47 3 3 3 69OBL AG 0.31 50 90 89 89 85POSS 2.47 86 91 91 91 91QUANT 2.12 89 89 93 93 92RELMOD 1.84 51 71 72 72 69SUBJ 15.45 75 80 81 81 82TOPIC 0.44 81 85 84 84 76TOPICREL 1.82 61 72 74 75 69XCOMP 5.62 81 87 81 81 88103Computational Linguistics Volume 34, Number 1Figure 12Approximate Randomization Test for statistical significance testing.OBJ2, OBL or OBL2 dependencies very well, although Parent-PCFG is able to producetrees from which it is easier to identify obliques (OBL), because of the Penn-II functional-CLR label.
The automatic annotation algorithm is unable to identify RELMOD depen-dencies satisfactorily from the trees produced by parsing with Parent-PCFG, althoughthe history-based parsers score reasonably well for this function.
Whereas Charniak?sparser is able to identify some dependencies better than Bikel?s retrained parser, overallthe system based on Bikel?s retrained parser performs better when evaluating againstthe dependencies in WSJ Section 22.In order to determine whether the results are statistically significant, we use the Ap-proximate Randomization Test (Noreen 1989).21 This test is an example of a computer-intensive statistical hypothesis test.
Such tests are designed to assess result differenceswith respect to a test statistic in cases where the sampling distribution of the test statisticis unknown.
Comparative evaluations of outputs of parsing systems according to teststatistics, such as differences in f-score, are examples of this situation.
The test statisticsare computed by accumulating certain count variables over the sentences in the testset.
In the case of f-score, variable tuples consisting of the number of dependency-relations in the parse for the system translation, the number of dependency-relationsin the parse for the reference translation, and the number of matching dependency-relations between system and reference parse, are accumulated over the test set.Under the null hypothesis, the compared systems are not different, thus any vari-able tuple produced by one of the systems could just as likely have been produced bythe other system.
So shuffling the variable tuples between the two systems with equalprobability, and recomputing the test statistic, creates an approximate distribution ofthe test statistic under the null hypothesis.
For a test set of S sentences there are 2Sdifferent ways to shuffle the variable tuples between the two systems.
Approximaterandomization produces shuffles by random assignments instead of evaluating all 2Spossible assignments.
Significance levels are computed as the percentage of trials wherethe pseudo statistic, that is the test statistic computed on the shuffled data, is greaterthan or equal to the actual statistic, that is the test statistic computed on the test data.
Asketch of an algorithm for approximate randomization testing is given in Figure 12.21 Applications of this test to natural language processing problems can be found in Chinchor et al (1993)and Yeh (2000).104Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 10Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values forapproximate randomization test for 10,000,000 randomizations.PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+TagsPCFG - - - - - -Parent-PCFG <.0001 - - - - -Collins M3 <.0001 <.0001 - - - -Charniak <.0001 <.0001 <.0001 - - -Bikel <.0001 <.0001 <.0001 .0003 - -Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 -Table 10 gives the p-values (the smallest fixed level at which the null hypothesis canbe rejected) for comparing each parser against all of the other parsers.
We test for sig-nificance at the 95% level.
Because we are doing a pairwise comparison of six systems,giving 15 comparisons, the p-value needs to be below .0034 for there to be a significantdifference at the 95% level.22 For each parser, the values in the row corresponding tothat parser represent the p-values for those parsers that achieve a lower f-score thanthat parser.
This shows that the system based on Bikel?s retrained parser is significantlybetter than those based on the other parsers with a statistical significance of >95%.
Forthe XLE and RASP comparisons, we will use the f-structure-annotation algorithm andBikel retrained-based LFG system.5.
Cross-Formalism Comparison of Treebank-Induced and Hand-Crafted GrammarsFrom the experiments in Section 4, we choose the treebank-based LFG system usingthe retrained version of Bikel?s parser (which retains Penn-II functional tag labels) tocompare against parsing systems using deep, hand-crafted, constraint-based grammarsat the level of dependencies.
We report on two experiments.
In the first experiment(Section 5.1), we evaluate the f-structure annotation algorithm and Bikel retrainedparser-based LFG system against the hand-crafted, wide-coverage LFG and XLE pars-ing system (Riezler et al 2002; Kaplan et al 2004) on the PARC 700 Dependency Bank(King et al 2003).
In the second experiment (Section 5.2), we evaluate against the hand-crafted, wide-coverage unification grammar and RASP parsing system of Carroll andBriscoe (2002) on the CBS 500 Dependency Bank (Carroll, Briscoe, and Sanfilippo 1998).5.1 Evaluation against PARC 700The PARC 700 Dependency Bank (King et al 2003) provides dependency relations(including LDD relations) for 700 sentences randomly selected from WSJ Section 23 ofthe Penn-II Treebank.
In order to evaluate the parsers, we follow the experimental setupof Kaplan et al (2004) with a split of 560 dependency structures for the test set and 140for the development set.
The set of features (Table 12, later in this article) evaluatedin the experiment form a proper superset of preds-only, but a proper subset of all22 Based on Cohen (1995, p. 190): ?e ?
1 ?
(1 ?
?c )m, where m is the number of pairwise comparisons, ?e isthe experiment-wise error, and ?c is the per-comparison error.105Computational Linguistics Volume 34, Number 1Figure 13PARC 700 conversion software.grammatical functions (preds-only ?
PARC ?
all GFs).
This feature set was selected inKaplan et al because the features carry important semantic information.
There are sys-tematic differences between the PARC 700 dependencies and the f-structures generatedin our approach as regards feature geometry, feature nomenclature, and the treatmentof named entities.
In order to evaluate against the PARC 700 test set, we automaticallymap the f-structures produced by our parsers to a format similar to that of the PARC700 Dependency Bank.
This is done with conversion software in a post-processing stageon the f-structure annotated trees (Figure 13).The conversion software is developed on the 140-sentence development set of thePARC 700, except for the Multi-Word Expressions section.
Following the experimentalsetup of Kaplan et al (2004), we mark up multi-word expression predicates based onthe gold-standard PARC 700 Dependency Bank.Multi-Word Expressions The f-structure annotation algorithm analyzes the internalstructure of all noun phrases fully.
In Figure 14, for example, BT is analyzed asan ADJUNCT modifier of the head securities, whereas PARC 700 analyzes this andother (more complex) named entities as multi-word expression predicates.
Theconversion software transforms the output of the f-structure annotation algorithminto the multi-word expression predicate format.Feature Geometry In constructions such as Figure 2, the f-structure annotation algo-rithm analyzes say as the main PRED with what is said as the value of a COMPargument.
In the PARC 700, these constructions are analyzed in such a way thatwhat is said/reported provides the top level f-structure whereas other material(who reported, etc.)
is analyzed in terms of ADJUNCTs modifying the top levelf-structure.
A further systematic structural divergence is provided by the analysisFigure 14Named entity and OBL AG feature geometry mapping.106Cahill et al Statistical Parsing Using Automatic Dependency Structuresof passive oblique agent constructions (Figure 14): The f-structure annotationalgorithm generates a complex internal analysis of the oblique agent PP, whereasthe PARC analysis encodes a flat representation.
The conversion software adjuststhe output of the f-structure annotation algorithm to the PARC-style encoding oflinguistic information.Feature Nomenclature There are a number of systematic differences between featurenames used by the automatic annotation algorithm and PARC 700: For example,DET is DET FORM in the PARC 700, COORD is CONJ, FOCUS is FOCUS INT.
Nomen-clature differences are treated in terms of a simple relabeling by the conversionsoftware.Additional Features A number of features in the PARC 700 are not produced by the au-tomatic annotation algorithm.
These include: AQUANT for adjectival quantifiers,MOD for NP-internal modifiers, and STMT TYPE for statement type (declarative,interrogative, etc.).
Additional features (and their values) are automatically gen-erated by the mapping software, using categorial, configurational, and alreadyproduced f-structure annotation information, extending the original annotationalgorithm.XCOMP Flattening The automatic annotation algorithm treats both auxiliary andmodal verb constructions in terms of hierarchically cascading XCOMPs, whereasin PARC 700 the temporal and aspectual information expressed by auxiliary verbsis represented in terms of a flat analysis and features (Figure 15).
The conversionsoftware automatically flattens the f-structures produced by the automatic anno-tation algorithm into the PARC-style encoding.For full details of the mapping, see Burke et al (2004).In our parsing experiments, we used the most up-to-date version of the hand-crafted, wide-coverage, deep LFG resources and XLE parsing system with improvedresults over those reported in Kaplan et al (2004): This latest version achieves 80.55%f-score, a 0.95 percentage point improvement on the previous 79.6%.
The XLE parsingsystem combines a large-scale, hand-crafted LFG for English and a statistical disam-biguation component to choose the most likely analysis among those returned bythe symbolic parser.
The statistical component is a log-linear model trained on 10,000partially labeled structures from the WSJ.
The results of the parsing experiments arepresented in Table 11.
We also include a figure for the upper bound of each system.23Using Bikel?s retrained parser, the treebank-based LFG system achieves an f-score of82.73%, and the hand-crafted grammar and XLE-based system achieves an f-scoreof 80.55%.
The approximate randomization test produced a p-value of .0054 for thispairwise comparison, showing that this result difference is statistically significant atthe 95% level.
Evaluation results on a reannotated version (Briscoe and Carroll 2006) ofthe PARC 700 Dependency Bank were recently published in Clark and Curran (2007),reporting f-scores of 81.9% for the CCG parser, and 76.3% for RASP.
As Briscoe and23 The upper bound for the treebank-based LFG system is determined by taking the original Penn-II WSJSection 23 trees corresponding to the PARC 700 strings, automatically annotating them with thef-structure annotation algorithm, and evaluating the f-structures against the PARC 700 dependencies.
Theupper bound for the XLE system is determined by selecting the XLE parse that scores best against thePARC 700 dependencies for each of the PARC 700 strings.
It is interesting to note that the upper boundfor the treebank-based system is only 1.18 percentage points higher than that for the XLE system.
Apartfrom the two different methods for establishing the upper bounds, this is most likely due to the fact thatthe mapping required for evaluating the treebank-based LFG system against PARC 700 is lossy (cf.
thediscussion in Section 6).107Computational Linguistics Volume 34, Number 1Figure 15DCU 105 and PARC 700 analyses for the sentence Unlike 1987, interest rates have been fallingthis year.Carroll point out, these evaluations are not directly comparable with the Kaplan et al(2004) style evaluation against the original PARC 700 Dependency Bank, because theannotation schemes are different.
Kaplan et al and our experiments use a fine-grainedfeature set of 34 features (Table 12), while the Briscoe and Carroll scheme uses 17features.A breakdown by dependency relation for each system is given in Table 12.
Thetreebank-induced grammar system can better identify DET FORM, SUBORD FORM, andTable 11Results of evaluation against the PARC 700 Dependency Bank following the experimental setupof Kaplan et al (2004).Bikel+Tags XLE p-ValueF-score 82.73 80.55 .0054Upper bound 86.83 85.65 -108Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 12Breakdown by dependency relation of results of evaluation against PARC 700.Dep.
Percent of deps.
F-score (%)Bikel+Tags XLEADEGREE 6.17 80 82ADJUNCT 14.32 68 66AQUANT 0.06 78 61COMP 1.23 80 74CONJ 2.64 73 69COORD FORM 1.20 83 90DET FORM 4.61 97 91FOCUS 0.02 0 36MOD 2.74 74 67NUM 19.82 91 89NUMBER 1.42 89 83NUMBER TYPE 2.10 94 86OBJ 8.92 87 78OBJ THETA 0.05 43 31OBL 0.83 55 69OBL AG 0.22 82 76OBL COMPAR 0.07 38 56PASSIVE 1.14 80 88PCASE 0.25 79 68PERF 0.41 89 90POSS 0.98 88 80PRECOORD FORM 0.03 0 91PROG 0.97 89 81PRON FORM 2.54 92 94PRON INT 0.03 0 33PRON REL 0.57 74 72PROPER 3.56 83 93PRT FORM 0.22 80 41QUANT 0.34 77 80STMT TYPE 5.23 87 80SUBJ 8.51 78 78SUBORD FORM 0.93 47 42TENSE 5.02 95 90TOPIC REL 0.57 56 73XCOMP 2.29 80 78PRT FORM dependencies24 and achieves higher f-scores for OBJ and POSS.
However, thehand-crafted parsing system can better identify FOCUS, OBL(ique) arguments, PRECO-ORD FORM and TOPICREL relations.5.2 Evaluation against CBS 500We also compare the hand-crafted, deep, probabilistic unification grammar-based RASPparsing system of Carroll and Briscoe (2002) to our treebank- and retrained Bikel24 DET FORM, SUBORD FORM, and PRT FORM (and in general X FORM) dependencies record (semanticallyrelevant) surface forms in f-structure for X-type closed class categories.109Computational Linguistics Volume 34, Number 1Figure 16CBS 500 conversion software.parser-based LFG system.
The RASP parsing system is a domain-independent, robuststatistical parsing system for English, based on a hand-written, feature-based unificationgrammar.
A probabilistic parse selection model conditioned on the structural parsecontext, degree of support for a subanalysis in the parse forest, and lexical informa-tion (when available) chooses the most likely parses.
For this experiment, we evaluateagainst the CBS 500,25 developed by Carroll, Briscoe, and Sanfilippo (1998) in order toevaluate a precursor of the RASP parsing resources.
The CBS 500 contains dependencystructures (including some long distance dependencies26) for 500 sentences chosen atrandom from the SUSANNE corpus (Sampson 1995), but subject to the constraint thatthey are parsable by the parser in Carroll, Briscoe, and Sanfilippo.
Aswith the PARC 700,there are systematic differences between the f-structures produced by our methodologyand the dependency structures of the CBS 500.
In order to be able to evaluate againstthe CBS 500, we automatically map our f-structures into a format similar to theirs.
Wedid not split the data into a heldout and a test set when developing the mapping, sothat a comparison could be made with other systems that report evaluations againstthe CBS 500.
The following CBS 500-style grammatical relations are produced from thef-structure in Figure 1:(ncsubj sign U.N.)(dobj sign treaty)Some mapping is carried out (as in the evaluation against the PARC 700) on the f-structure annotated trees, and the remaining mapping is carried out on the f-structures(Figure 16).
As with the PARC 700 mapping, all mappings are carried out automatically.The following phenomena were dealt with on the f-structure annotated trees:Auxiliary verbs (xcomp flattening) XCOMPS were flattened to promote the main verbto the top level, while maintaining a list of auxiliary and modal verbs and theirrelation to one another.Treatment of topicalized sentences The predicate of the topicalized sentence becamethe main predicate and any other top level material became an adjunct.Multi-word expressions Multi-word expressions (such as according to) were notmarked up in the parser input, but captured in the annotated trees and theannotations adjusted accordingly.Treatment of the verbs be and become Our automatic annotation algorithm does nottreat the verbs be and become differently from any other verbs when they are usedtransitively.
This analysis conflicted with the CBS 500 analysis, so was changed tomatch theirs.25 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.26 The long distance dependencies include passive, wh-less relative clauses, control verbs, and so forth.110Cahill et al Statistical Parsing Using Automatic Dependency StructuresTable 13Results of dependency evaluation against the CBS 500 (Carroll, Briscoe, and Sanfillipo 1998).Bikel+Tags RASP p-ValueF-score 80.23 76.57 <.0001The following are the main mappings carried out on the f-structures:Encoding of Passive We treat passive as a feature in our automatic f-structure annota-tion algorithm, whereas the CBS 500 triples encode this information indirectly.Objects of Prepositional Phrases No dependency was generated for these objects, asthere was no corresponding dependency in the CBS 500 analyses.Nomenclature Differences There were some trivial mappings to account for differ-ences in nomenclature, for example OBL in our analyses became IOBJ in themapped dependencies.Encoding of wh-less relative clauses These are encoded by means of reentrancies inf-structure, but were encoded in a more indirect way in the mapped dependenciesto match the CBS 500 annotation format.To carry out the experiments, we POS-tagged the tokenized CBS 500 sentences withthe MXPOST tagger (Ratnaparkhi 1996) and parsed the tag sequences with our Penn-IIand Bikel retrained-based LFG system.
We use the evaluation software of Carroll,Briscoe, and Sanfilippo (1998)27 to evaluate the grammatical relations produced by eachparser.
The results are given in Table 13.Our LFG system based on Bikel?s retrained parser achieves an f-score of 80.23%,whereas the hand-crafted RASP grammar and parser achieves an f-score of 76.57%.Crouch et al (2002) report that their XLE system achieves an f-score of 76.1% for thesame experiment.
A detailed breakdown by dependency is given in Table 14.
TheLFG system based on Bikel?s retrained parser is able to better identify MOD(ifier) de-pendency relations, ARG MOD (the relation between a head and a semantic argumentwhich is syntactically realized as a modifier, for example by-phrases), IOBJ (indirectobject) and AUXiliary relations.
RASP is able to better identify XSUBJ (clausal subjectscontrolled from without), CSUBJ (clausal subjects), and COMP (clausal complement)relations.
Again we use the Approximate Randomization Test to test the parsing resultsfor statistical significance.
The p-value for the test comparing our system using Bikel?sretrained parser against RASP is <.0001.
The treebank-based LFG system using Bikel?sretrained parser is significantly better than the hand-crafted, deep, unification grammar-based RASP parsing system with a statistical significance of >95%.6.
Discussion and Related WorkAt the moment, we can only speculate as to why our treebank-based LFG resourcesoutperform the hand-crafted XLE and RASP grammars.In Section 4, we observed that the treebank-induced LFG resources have consid-erably wider coverage (>99.9% measured in terms of complete spanning parse) than27 This was downloaded from http://www.informatics.susx.ac.uk/research/nlp/carroll/greval.html.111Computational Linguistics Volume 34, Number 1Table 14Breakdown by grammatical relation for results of evaluation against CBS 500.Dep.
Percent of deps.
F-score (%)Bikel+Tags RASPDEPENDANT 100.00 80.23 76.57MOD 48.96 80.30 75.29NCMOD 30.42 85.37 72.98XMOD 1.60 70.05 55.88CMOD 2.61 75.60 53.08DETMOD 14.06 95.85 91.97ARG MOD 0.51 80.00 64.52ARG 43.70 78.28 77.57SUBJ 13.10 79.84 83.57NCSUBJ 12.99 87.84 84.32XSUBJ 0.06 0.00 88.89CSUBJ 0.04 0.00 22.22SUBJ OR DOBJ 18.22 81.21 83.84COMP 12.38 76.73 71.87OBJ 7.34 76.05 69.53DOBJ 5.11 84.55 84.57OBJ2 0.25 48.00 43.84IOBJ 1.98 59.04 47.60CLAUSAL 5.04 77.74 75.37XCOMP 4.03 80.00 84.11CCOMP 1.01 69.61 75.14AUX 4.76 94.94 88.27CONJ 2.06 68.84 69.09the hand-crafted grammars (?80% for XLE and RASP grammars on unseen treebanktext).
Both XLE and RASP use a number of (largest) fragment-combining techniquesto achieve full coverage.
If coverage is a significant component in the performancedifference observed between the hand-crafted and treebank-induced resources, thenit is reasonable to expect that the performance difference is more pronounced withincreasing sentence length (with shorter sentences being simpler and more likely tobe within the coverage of the hand-crafted grammars).
In other words, we expect thehand-crafted, deep, precision grammars to do better on shorter sentences (more likely tobe within their coverage), whereas the treebank-induced grammars should show betterperformance on longer strings (less likely to be within the coverage of the hand-craftedgrammars).In order to test this hypothesis, we carried out a number of experiments:First, we plotted the sentence length distribution for the 560 PARC 700 test sen-tences and the 500 CBS 500 sentences (Figures 17 and 18).
Both gold standards areapproximately normally distributed, with the CBS 500 distribution possibly showingthe effects of being chosen subject to the constraint that the strings are parsable by theparser in Carroll, Briscoe, and Sanfilippo (1998).
For each case we use the mean, ?, andtwo standard deviations, 2?, to the left and right of themean to exclude sentence lengthsnot supported by sufficient observations: For PARC 700, ?
= 23.27, ??
2?
= 2.17, and?+ 2?
= 44.36; for CBS 500, ?
= 17.27, ??
2?
= 1.59, and ?+ 2?
= 32.96.
Both thePARC 700 and the CB 500 distributions are positively skewed.
For the PARC 700, ??
2?is actually outside the observed data range, whereas for CB 500, ??
2?
almost coincides112Cahill et al Statistical Parsing Using Automatic Dependency StructuresFigure 17Distribution of sentence frequency by sentence length in the PARC 700 test set with Bezierinterpolation.
Vertical lines mark two standard deviations from the mean.with the left border.
It is therefore useful to further constrain the ?2?
range by asentence count threshold of ?
5.28 This results in a sentence length range of 4?41 forPARC 700 and 4?32 for CBS 500.Second, in order to test whether fragment parses increase with sentence length, weplotted the percentage of fragment parses over sentence length for the XLE parses of the560-sentence test set of the PARC 700 (we did not do this for the CBS 500 as its stringsare selected to be fully parsable by RASP).
Figure 19 shows that the number of fragmentparses tends to increase with sentence length.Third, we plotted the average dependency f-score for the hand-crafted and thetreebank-induced resources against sentence lengths.
Figure 20 shows the results forPARC 700, Figure 21 for CBS 500.In both cases, contrary to our (perhaps somewhat naive) assumption, the graphsshow that the treebank-induced resources outperform the hand-crafted resourceswithin (most of) the 4?41 and 4?32 sentence length bounds, with the results for the veryshort and the very long strings outside those bounds not being supported by sufficientdata points.In the parsing literature, results are often also provided for strings with lengths?40.
Below we give those results and statistical significance testing for the PARC 700and CBS 500 (Tables 15 and 16).
The results show that the Bikel retrained?based LFGsystem achieves a higher dependency f-score on sentences of length ?40 than on allsentences, whereas the XLE system achieves a slightly lower score on sentences oflength ?40.
The Bikel-retrained system achieves an f-score of 83.18%, a statistically28 Note that because the distributions in Figures 17 and 18 are Bezier interpolated, the constraint does notguarantee that every sentence length within the range occurs five or more times.113Computational Linguistics Volume 34, Number 1Figure 18Distribution of sentence frequency by sentence length in the CB 500 test set with Bezierinterpolation.
Vertical lines mark two standard deviations from the mean.Figure 19Percentage of fragment sentences for XLE parsing system per sentence length with Bezierinterpolation.significant improvement of 2.67 percentage points over the XLE system on sentences oflength?40.
Against the CBS 500, Bikel?s retrained system achieves a weighted f-score of82.58%, a statistically significant improvement of 3.87 percentage points over the RASPsystem which achieves a weighted f-score of 78.81% on sentences of length ?40.114Cahill et al Statistical Parsing Using Automatic Dependency StructuresFigure 20Average f-score by sentence length for PARC 700 test set with Bezier interpolation.Figure 21Average f-score by sentence length for CB 500 test set with Bezier interpolation.Finally, we test whether the strict preds-only dependency evaluation has an ef-fect on the results for the PARC 700 experiments.
Recall that following Kaplan et al(2004) for the PARC 700 evaluation we used a set of semantically relevant grammaticalfunctions that is a superset of preds-only and a subset of all-GFs.
A preds-only basedevaluation is ?stricter?
and tends to produce lower scores as it directly reflects the effects115Computational Linguistics Volume 34, Number 1Table 15Evaluation and significance testing of sentences length ?40 against the PARC 700.All sentence lengths Length ?40f-score f-scoreBikel + Tags 82.73 83.18XLE 80.55 80.51p-value .0054 .0010Table 16Evaluation and significance testing of sentences length ?40 against the CBS 500.All sentence lengths Length ?40f-score f-scoreBikel + Tags 80.23 82.58RASP 76.57 78.81p-value <.0001 <.0001Table 17Preds-only evaluation against the PARC 700 Dependency Bank.All GFs Preds onlyf-score f-scoreBikel + Tags 82.73 77.40XLE 80.55 74.31of predicate?argument/adjunct misattachments in the resulting dependency represen-tations (while local functions such as NUM(ber), for example, can score properly evenif the local predicate is misattached).
Table 1729 below gives the results for preds-onlyevaluation30 on the PARC 700 for all sentence lengths.
The results show that the Bikel-retrained treebank-based LFG resource achieves an f-score of 77.40%, 5.33 percentagepoints lower than the score for all the PARC dependencies.
The XLE system achievesan f-score of 74.31%, 6.24 percentage points lower than the score for all the PARCdependencies and a 3.09 percentage point drop against the results obtained by theBikel-retrained treebank-based LFG resources.
The performance of the Bikel retrained?based LFG system suffers less than the XLE system when preds-only dependencies areevaluated.It is important to note that in our f-structure annotation algorithm and treebank-based LFG parsing architectures, we do not claim to provide fully adequate statisticalmodels.
It is well known (Abney 1997) that PCFG- or history-based parser approxima-tions to general constraint-based grammars can yield inconsistent probability models29 We do not include a p-value here as the breakdown by function per sentence was not available to us forthe XLE data.30 The dependency relations we include in preds-only evaluation are: ADJUNCT, AQUANT, COMP, CONJ,COORD FORM, DET FORM, FOCUS INT, MOD, NUMBER, OBJ, OBJ THETA, OBL, OBL AG, OBL COMPAR,POSS, PRON INT, PRON REL, PRT FORM, QUANT, SUBJ, SUBORD FORM, TOPIC REL, XCOMP.116Cahill et al Statistical Parsing Using Automatic Dependency Structuresdue to loss of probability mass: The parser successfully returns the highest rankedparse tree but the constraint solver cannot resolve the f-structure equations and theprobability mass associated with that tree is lost.
Research on adequate probabilitymodels for constraint-based grammars is important (Bouma, van Noord, and Malouf2000; Miyao and Tsujii 2002; Riezler et al 2002; Clark and Curran 2004).
In this context,it is interesting to compare parser performance against upper bounds.
For the PARC700 evaluation, the upper bound for the XLE-based resources is 85.65%, against 86.83%for the treebank-based LFG resources.
XLE-based parsing currently achieves 94.05%(f-score 80.55%) of its upper bound using a discriminative disambiguation method,whereas the treebank-based LFG resource achieves 95.28% (f-score 82.73%) of its upperbound.Although this seems to indicate that the two disambiguationmodels achieve similarresults, the figures are actually very difficult to compare.
In the case of the XLE, theupper bound is established by unpacking all parses for a string and scoring the bestmatch against the gold standard (rather than letting the probability model select aparse).
By contrast, in the case of the treebank-based LFG resources, we use the original?perfect?
Penn-II treebank trees (rather than the trees produced by the parser), auto-matically annotate those trees with the f-structure annotation algorithm, and score theresults against the PARC 700 (it is not feasible to generate all parses for a string, thereare simply too many for treebank-induced resources).
The upper bound computed inthis fashion for the treebank-based LFG resource (86.83%) is relatively low.
The mainreason is that the automatic mapping required to relate the f-structures generated by thetreebank-based LFG resources to the PARC 700 dependencies is lossy.
This is indicatedby comparing the upper bound for the treebank-based LFG resources for the PARC 700against the upper bound for the DCU 105 gold standard, where little or no mapping(apart from the feature-structure to dependency-triple conversion) is required: Scoringthe f-structure annotations for the original treebank trees results in 86.83% against PARC700 versus 96.80% against DCU 105.Our discussion shows how delicate it can be to compare parsing systems andtheir disambiguation models.
Ultimately what is required is an evaluation strategy thatseparates out and clearly distinguishes between the grammar, parsing algorithm, anddisambiguation model and is capable of assessing different combinations of these corecomponents.
Of course, this will not always be possible andmoving towards it is part ofa much more extended research agenda, well beyond the scope of the research reportedin the present article.
Our approach, and previous approaches, evaluate systems atthe highest level of granularity, that of the complete package: the combined grammar-parser-disambiguation model.
The results show that machine-learning-based resourcescan outperform deep, state-of-the-art hand-crafted resources with respect to the qualityof dependencies generated.Treebank-based, deep and wide-coverage constraint-based grammar acquisitionhas become an important research topic: Starting with the seminal TAG-based workof Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches.
Miyao,Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present re-search on inducing Penn-II treebank-based HPSGs with log-linear probability models.Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-inducedCCG-basedmodels including LDD resolution.
It would be interesting to conduct a com-parative evaluation involving treebank-based HPSG, CCG, and LFG resources againstthe PARC 700 and CBS 500 Dependency Banks to enable more comprehensive compar-ison of machine-learning-based with hand-crafted, deep, wide-coverage resources suchas those used in the XLE or RASP parsing systems.117Computational Linguistics Volume 34, Number 1Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank(Kingsbury, Palmer, andMarcus 2002)-based evaluations of their automatically inducedCCG and HPSG resources.
PropBank-based evaluations provide valuable informationto rank parsing systems.
Currently, however, PropBank-based evaluations are some-what partial: They only represent (and hence score) verbal arguments and disregard araft of other semantically important dependencies (e.g., temporal and aspectual infor-mation, dependencies within nominal clusters, etc.)
as captured in the PARC 700 or CBS500 Dependency Banks.317.
ConclusionsParser comparison is a non-trivial and time-consuming exercise.
We extensively eval-uated four machine-learning-based shallow parsers and two hand-crafted, wide-coverage deep probabilistic parsers involving four gold-standard dependency banks,using the Approximate Randomization Test (Noreen 1989) to test for statistical signifi-cance.
We used a sophisticated method for automatically producing deep dependencyrelations from Penn-II-style CFG-trees (Cahill et al 2002b, 2004) to compare shallowparser output at the level of dependency relation and revisit experiments carried out byPreiss (2003) and Kaplan et al (2004).Our main findings are twofold:1.
Using our CFG-tree-to-dependency annotation technology, together with treebank- andmachine-learning-based parsers, we can outperform hand-crafted, wide-coverage, deep,probabilistic, constraint-based grammars and parsers.This result is surprising for two reasons.
First, it is established against two externally-provided dependency banks (the PARC 700 and the CBS 500 gold standards), originallydesigned using the hand-crafted, wide-coverage, probabilistic grammars for the XLE(Riezler et al 2002) and RASP (Carroll and Briscoe 2002) parsing systems to evaluatethose systems.
What is more, the SUSANNE corpus-based CBS 500 constitutes aninstance of domain variation for the Penn-II-trained LFG resources, likely to adverselyaffect scores.
Second, the treebank- and machine-learning-based LFG resources requireautomatic mapping to relate f-structure output of the treebank-based parsing systemsto the representation format in the PARC 700 and CBS 500 Dependency Banks.
Thesemappings are partial and lossy: That is, they do not cover all of the systematic dif-ferences between f-structure and dependency bank representations and introduce acertain amount of error in what they are designed to capture, that is they both over-and undergeneralize, again adversely affecting scores.
Improvements of the mappingsshould lead to a further improvement in the dependency scores.2.
Parser evaluation at the level of dependency representation still requires non-trivialmappings between different dependency representation formats.31 In a sense, PropBank does not yet provide a single agreed upon gold standard: Role information isprovided indirectly and an evaluation gold-standard has to be computed from this.
In doing so, choiceshave to be made as regards the representation of shared arguments, the analysis of coordinate structures,and so forth, and it is not clear that the same choices are currently made for evaluations carried out bydifferent research groups.118Cahill et al Statistical Parsing Using Automatic Dependency StructuresEarlier we criticized tree-based parser evaluation on the grounds that equally validdifferent tree-typologies can be associated with strings, and identified this as a majorobstacle to fair and unbiased parser evaluation.
Dependency-based parser evaluation,as it turns out, is not entirely free of this criticism either: There are significant systematicdifferences between the PARC 700 dependency and the CBS 500 dependency represen-tations; there are significant systematic differences between the LFG f-structures gener-ated by the hand-crafted, wide-coverage grammars of Riezler et al (2002) and Kaplanet al (2004) and those of the treebank-induced and f-structure annotation algorithmbased resources of Cahill et al (2004).
These differences require careful implementationof mappings if parsers are not to be unduly penalized for systematic and motivateddifferences at the level of dependency representation.
By and large, these differencesare, however, less pronounced than differences on CFG tree representations, makingdependency-based parser evaluation a worthwhile and rewarding exercise.Summarizing our results, we find that against the DCU 105 development set, thetreebank- and f-structure annotation algorithm-based LFG system using Bikel?s parserretrained to retain Penn-II functional tag labels performs best, achieving f-scores of82.92% preds-only and 88.3% all grammatical functions.
Against the automaticallygenerated WSJ Section 22 Dependency Bank, the system using Bikel?s retrained parserachieves the highest results, achieving f-scores of 83.06% preds-only and 87.861% allGFs.
This is statistically significantly better than all other parsers.
In order to evaluateagainst the PARC 700 and CBS 500 gold standards, we automaticallymap the dependen-cies produced by our treebank-based LFG system into a format compatible with the goldstandards.
Against the PARC 700 Dependency Bank, the treebank-based LFG systemusing Bikel?s retrained parser achieves an f-score of 82.73%, a statistically significantimprovement of 2.18% against the most up-to-date results of the hand-crafted XLE-based parsing resources.
Against the CBS 500, the treebank-based LFG system usingBikel?s retrained parser achieved the highest f-score of 80.23%, a statistically significantimprovement of 3.66 percentage points on the highest previously published resultsfor the same experiment with the hand-crafted RASP resources in Carroll and Briscoe(2002).Appendix A. Parser Parameter SettingsThis section provides a complete list of the parameter settings used for each of theparsers described in this article.Parser ParametersCollins Model 3 We used the Collins parser with its default settings of abeam size of 10,000 and where the values of the followingflags are set to 1: punctuation-flag, distaflag, distvflag,and npbflag.
Input was pre-tagged using the MXPOSTPOS tagger (Ratnaparkhi 1996).
We parse the input filewith all three models and use the scripts provided tomerge the outputs into the final parser output file.
Notethat this file has been cleaned of all -A functional tags andtrace nodes.Charniak We used the parser dated August 2005 and ran theparser using the data provided in the download on pre-tokenized sentences of length ?200.
Input was automati-cally tagged by the parser.119Computational Linguistics Volume 34, Number 1Bikel Emulation of We used version 0.9.9b of the parser trained on the fileCollins Model 2 of observed events made available on the downloadspage.
We used the collins.properties file and a maximumheap size of 1,500 MB.
Input was pre-tagged using theMXPOST POS tagger (Ratnaparkhi 1996).Bikel + Functional Tags We used version 0.9.9b of the parser trained on a versionof Sections 02?21 of the Penn-II treebank where functionaltags were not ignored by the parser.
We updated the de-fault head-finding rules to deal with the new categories.We also trained on all sentences from the training set(the default collins.properties file is set to ignore trees ofmore than 500 tokens).
Input was pre-tagged using theMXPOST POS tagger (Ratnaparkhi 1996) and the maxi-mum heap size was 1,500 MB.RASP Weused a file of parser output provided through personalcommunication with John Carroll.
(Tagging is carried outautomatically by the parser.
)XLE We used a file of parser output provided by the NaturalLanguage Theory and Technology group at the Palo AltoResearch Center.
(Tagging is carried out automatically bythe parser.
)AcknowledgmentsWe are grateful to our anonymous reviewerswhose insightful comments have improvedthe article significantly.
We would like tothank John Carroll for discussion and helpwith reproducing the RASP parsing results;Michael Collins, Eugene Charniak, DanBikel, and Helmut Schmid for making theirparsing engines available; and Ron Kaplanand the team at PARC for discussion,feedback, and support.
Part of the researchpresented here has been supported byScience Foundation Ireland grants04/BR/CS0370 and 04/IN/I527, EnterpriseIreland Basic Research Grant SC/2001/0186,an Irish Research Council for Science,Engineering and Technology Ph.D.studentship, an IBM Ph.D. studentship andsupport from IBM?s Centre for AdvancedStudies (CAS) in Dublin.ReferencesAbney, Stephen.
1997.
Stochasticattribute-value grammars.
ComputationalLinguistics, 23(4):597?618.Alshawi, Hiyan and Stephen Pulman, 1992.Ellipsis, Comparatives, and Generation,chapter 13.
The MIT Press, Cambridge,MA.Baldwin, Timothy, Emily Bender, DanFlickinger, Ara Kim, and Stephan Oepen.2004.
Road-testing the English ResourceGrammar over the British NationalCorpus.
In Proceedings of the FourthInternational Conference on LanguageResources and Evaluation (LREC 2004),pages 2047?2050, Lisbon, Portugal.Bikel, Dan.
2002.
Design of a multi-lingual,parallel-processing statistical parsingengine.
In Proceedings of HLT 2002,pages 24?27, San Diego, CA.Black, Ezra, Steven Abney, Dan Flickenger,Claudia Gdaniec, Ralph Grishman, PhilipHarrison, Donald Hindle, Robert Ingria,Fred Jelineck, Judith Klavans, MarkLiberman, Mitchell Marcus, Salim Roukos,Beatrice Santorini, and TomekStrzalkowski.
1991.
A procedure forquantitatively comparing the syntacticcoverage of english grammars.
InProceedings of the Speech and NaturalLanguage Workshop, pages 306?311, PacificGrove, CA.Bod, Rens.
2003.
An efficient implementationof a new DOP model.
In Proceedings of theTenth Conference of the European Chapter ofthe Association for Computational Linguistics(EACL?03), pages 19?26, Budapest,Hungary.Bouma, Gosse, Gertjan van Noord, andRobert Malouf.
2000.
Alpino:Wide-coverage computational analysis ofdutch.
In Walter Daelemans, KhalilSima?an, Jorn Veenstra, and Jakub Zavrel,editors, Computational Linguistics in TheNetherlands 2000.
Rodopi, Amsterdam,pages 45?59.120Cahill et al Statistical Parsing Using Automatic Dependency StructuresBresnan, Joan.
2001.
Lexical-Functional Syntax.Blackwell, Oxford, England.Briscoe, Edward and John Carroll.
1993.Generalized probabilistic LR parsing ofnatural language (corpora) withunification-based grammars.
ComputationalLinguistics, 19(1):25?59.Briscoe, Ted and John Carroll.
2006.Evaluating the accuracy of anunlexicalized statistical parser on thePARC DepBank.
In Proceedings of theCOLING/ACL 2006 Main Conference PosterSessions, pages 41?48, Sydney, Australia.Briscoe, Edward, Claire Grover, BranBoguraev, and John Carroll.
1987.
Aformalism and environment for thedevelopment of a large grammar ofEnglish.
In Proceedings of the 10thInternational Joint Conference on AI,pages 703?708, Milan, Italy.Burke, Michael.
2006.
Automatic TreebankAnnotation for the Acquisition of LFGResources.
Ph.D. thesis, School ofComputing, Dublin City University,Dublin, Ireland.Burke, Michael, Aoife Cahill, RuthO?Donovan, Josef van Genabith, and AndyWay.
2004.
Evaluation of an automaticannotation algorithm against the PARC700 Dependency Bank.
In Proceedings of theNinth International Conference on LFG,pages 101?121, Christchurch, New Zealand.Butt, Miriam, Helge Dyvik, Tracy HollowayKing, Hiroshi Masuichi, and ChristianRohrer.
2002.
The Parallel GrammarProject.
In Proceedings of COLING 2002,Workshop on Grammar Engineering andEvaluation, pages 1?7, Taipei, Taiwan.Cahill, Aoife.
2004.
Parsing with AutomaticallyAcquired, Wide-Coverage, Robust,Probabilistic LFG Approximations.
Ph.D.thesis, School of Computing, Dublin CityUniversity, Dublin, Ireland.Cahill, Aoife, Michael Burke, RuthO?Donovan, Josef van Genabith, and AndyWay.
2004.
Long-distance dependencyresolution in automatically acquiredwide-coverage PCFG-based LFGapproximations.
In Proceedings of the 42ndAnnual Meeting of the Association forComputational Linguistics, pages 320?327,Barcelona, Spain.Cahill, Aoife, Maire?ad McCarthy, Josef vanGenabith, and Andy Way.
2002a.Automatic annotation of the PennTreebank with LFG f-structureinformation.
In Proceedings of the LRECWorkshop on Linguistic KnowledgeAcquisition and Representation: BootstrappingAnnotated Language Data, pages 8?15, LasPalmas, Canary Islands, Spain.Cahill, Aoife, Maire?ad McCarthy, Josef vanGenabith, and Andy Way.
2002b.
Parsingwith PCFGs and automatic f-structureannotation.
In Proceedings of the SeventhInternational Conference on LFG,pages 76?95, Palo Alto, CA.Carroll, John and Edward Briscoe.
2002.High precision extraction of grammaticalrelations.
In Proceedings of the 19thInternational Conference on ComputationalLinguistics (COLING), pages 134?140,Taipei, Taiwan.Carroll, John, Edward Briscoe, and AntonioSanfilippo.
1998.
Parser evaluation: Asurvey and new proposal.
In Proceedings ofthe International Conference on LanguageResources and Evaluation, pages 447?454,Granada, Spain.Carroll, John, Anette Frank, Dekang Lin,Detlef Prescher, and Hans Uszkoreit,editors.
2002.
HLT Workhop: ?BeyondPARSEVAL ?
Towards improved evaluationmeasures for parsing systems?, Las Palmas,Canary Islands, Spain.Charniak, Eugene.
1996.
Tree-bankgrammars.
In Proceedings of theThirteenth National Conference onArtificial Intelligence, pages 1031?1036,Menlo Park, CA.Charniak, Eugene.
2000.
A maximumentropy inspired parser.
In Proceedingsof the First Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL 2000),pages 132?139, Seattle, WA.Chinchor, Nancy, Lynette Hirschman, andDavid D. Lewis.
1993.
Evaluating messageunderstanding systems: An analysis of theThird Message Understanding Conference(MUC-3).
Computational Linguistics,19(3):409?449.Clark, Stephen and James Curran.
2004.Parsing the WSJ using CCG and log-linearmodels.
In Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics (ACL-04), pages 104?111,Barcelona, Spain.Clark, Stephen and James Curran.
2007.Formalism-independent parser evaluationwith CCG and DepBank.
In Proceedings ofthe 45th Annual Meeting of the Associationfor Computational Linguistics (ACL 2007),pages 248?255, Prague, Czech Republichttp://www.aclweb-org/anthology/P/P07/P07-1032.Clark, Stephen and Julia Hockenmaier.
2002.Evaluating a wide-coverage CCG parser.121Computational Linguistics Volume 34, Number 1In Proceedings of the LREC 2002 BeyondParseval Workshop, pages 60?66, LasPalmas, Canary Islands, Spain.Cohen, Paul R. 1995.
Empirical Methods forArtificial Intelligence.
The MIT Press,Cambridge, MA.Collins, Michael.
1997.
Three generative,lexicalized models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics,pages 16?23, Madrid, Spain.Collins, Michael.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania,Philadelphia, PA.Crouch, Richard, Ron Kaplan, Tracy HollowayKing, and Stefan Riezler.
2002.
Acomparison of evaluation metrics for abroad coverage parser.
In Proceedings ofthe LREC Workshop: Beyond PARSEVAL?Towards Improved Evaluation Measures forParsing Systems, pages 67?74, Las Palmas,Canary Islands, Spain.Dalrymple, Mary.
2001.
Lexical-FunctionalGrammar.
London, Academic Press.Dienes, Pe?ter and Amit Dubey.
2003.Antecedent recovery: Experiments with atrace tagger.
In Proceedings of the 2003Conference on Empirical Methods in NaturalLanguage Processing, pages 33?40, Sapporo,Japan.Eisele, Andreas and Jochen Do?rre.
1986.
Alexical functional grammar system inProlog.
In Proceedings of the 11th InternationalConference on Computational Linguistics(COLING 1986), pages 551?553, Bonn.Flickinger, Dan.
2000.
On building a moreefficient grammar by exploiting types.Natural Language Engineering, 6(1):15?28.Gaizauskas, Rob.
1995.
Investigations intothe grammar underlying the PennTreebank II.
Research MemorandumCS-95-25, Department of ComputerScience, Univeristy of Sheffield, UK.Gildea, Daniel and Julia Hockenmaier.2003.
Identifying semantic roles usingcombinatory categorial grammar.In Proceedings of the 2003 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 57?64,Sapporo, Japan.Hockenmaier, Julia.
2003.
Parsing withgenerative models of predicate?argumentstructure.
In Proceedings of the 41st AnnualConference of the Association forComputational Linguistics, pages 359?366,Sapporo, Japan.Hockenmaier, Julia and Mark Steedman.2002.
Generative models for statisticalparsing with combinatory categorialgrammar.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics, pages 335?342,Philadelphia, PA.Johnson, Mark.
1999.
PCFG models oflinguistic tree representations.Computational Linguistics, 24(4):613?632.Johnson, Mark.
2002.
A simplepattern-matching algorithm forrecovering empty nodes and theirantecedents.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics, pages 136?143,Philadelphia, PA.Kaplan, Ron and Joan Bresnan.
1982.Lexical functional grammar, a formalsystem for grammatical representation.In Joan Bresnan, editor, The MentalRepresentation of Grammatical Relations.MIT Press, Cambridge, MA,pages 173?281.Kaplan, Ron, Stefan Riezler, Tracy HollowayKing, John T. Maxwell, AlexanderVasserman, and Richard Crouch.
2004.Speed and accuracy in shallow and deepstochastic parsing.
In Proceedings of theHuman Language Technology Conference andthe 4th Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (HLT-NAACL?04),pages 97?104, Boston, MA.Kaplan, Ronald and Annie Zaenen.
1989.Long-distance dependencies, constituentstructure and functional uncertainty.
InMark Baltin and Anthony Kroch, editors,Alternative Conceptions of Phrase Structure,pages 17?42, University of Chicago Press,Chicago.King, Tracy Holloway, Richard Crouch,Stefan Riezler, Mary Dalrymple, and RonKaplan.
2003.
The PARC 700 dependencybank.
In Proceedings of the EACL03: 4thInternational Workshop on LinguisticallyInterpreted Corpora (LINC-03), pages 1?8,Budapest, Hungary.Kingsbury, Paul, Martha Palmer, andMitch Marcus.
2002.
Adding semanticannotation to the Penn TreeBank.
InProceedings of the Human LanguageTechnology Conference, pages 252?256,San Diego, CA.Klein, Dan and Christopher D. Manning.2003.
Accurate unlexicalized parsing.
InProceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics,pages 423?430, Sapporo, Japan.Leech, Geoffrey and Roger Garside.
1991.Running a grammar factory: On the122Cahill et al Statistical Parsing Using Automatic Dependency Structurescompilation of parsed corpora, or?treebanks?.
In Stig Johansson andAnna-Brita Stenstro?m, editors, EnglishComputer Corpora: Selected Papers.
Moutonde Gruyter, Berlin, pages 15?32.Levy, Roger and Christopher D. Manning.2004.
Deep dependencies from context-freestatistical parsers: Correcting the surfacedependency approximation.
In Proceedingsof the 42nd Annual Meeting of the Associationfor Computational Linguistics (ACL 2004),pages 328?335, Barcelona, Spain.Lin, Dekang.
1995.
A dependency-basedmethod for evaluating broad-coverageparsers.
In Proceedings of the InternationalJoint Conference on AI, pages 1420?1427,Montre?al, Canada.Magerman, David.
1994.
Natural LanguageParsing as Statistical Pattern Recognition.Ph.D.
thesis, Department of ComputerScience, Stanford University, CA.Marcus, Mitchell, Grace Kim, Mary AnnMarcinkiewicz, Robert MacIntyre,Ann Bies, Mark Ferguson, Karen Katz,and Britta Schasberger.
1994.
ThePenn Treebank: Annotating predicateargument structure.
In Proceedingsof the ARPA Workshop on HumanLanguage Technology, pages 110?115,Princeton, NJ.McCarthy, Maire?ad.
2003.
Design andEvaluation of the Linguistic Basis of anAutomatic F-Structure Annotation Algorithmfor the Penn-II Treebank.
Master?s thesis,School of Computing, Dublin CityUniversity, Dublin, Ireland.McDonald, Ryan and Fernando Pereira.
2006.Online learning of approximatedependency parsing algorithms.
InProceedings of the 11th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 81?88,Trento, Italy.Miyao, Yusuke, Takashi Ninomiya, andJun?ichi Tsujii.
2003.
Probabilistic modelingof argument structures including non-localdependencies.
In Proceedings of theConference on Recent Advances in NaturalLanguage Processing (RANLP),pages 285?291, Borovets, Bulgaria.Miyao, Yusuke and Jun?ichi Tsujii.
2002.Maximum entropy estimation for featureforests.
In Proceedings of Human LanguageTechnology Conference (HLT 2002),pages 292?297, San Diego, CA.Miyao, Yusuke and Jun?ichi Tsujii.2004.
Deep linguistic analysisfor the accurate identification ofpredicate?argument relations.
InProceedings of the 18th InternationalConference on Computational Linguistics(COLING 2004), pages 1392?1397,Geneva, Switzerland.Noreen, Eric W. 1989.
Computer IntensiveMethods for Testing Hypotheses: AnIntroduction.
Wiley, New York.O?Donovan, Ruth, Michael Burke, AoifeCahill, Josef van Genabith, and AndyWay.
2004.
Large-scale induction andevaluation of lexical resources from thePenn-II treebank.
In Proceedings of the 42ndAnnual Meeting of the Association forComputational Linguistics, pages 368?375,Barcelona, Spain.Pollard, Carl and Ivan Sag.
1994.
Head-drivenPhrase Structure Grammar.
CSLIPublications, Stanford, CA.Preiss, Judita.
2003.
Using grammaticalrelations to compare parsers.
In Proceedingsof the Tenth Conference of the EuropeanChapter of the Association for ComputationalLinguistics (EACL?03), pages 291?298,Budapest, Hungary.Ratnaparkhi, Adwait.
1996.
A maximumentropy part-of-speech tagger.
InProceedings of the Empirical Methods inNatural Language Processing Conference,pages 133?142, Philadelphia, PA.Riezler, Stefan, Tracy King, Ronald Kaplan,Richard Crouch, John T. Maxwell,and Mark Johnson.
2002.
ParsingtheWall Street Journal using alexical-functional grammar anddiscriminative estimation techniques.In Proceedings of the 40th AnnualConference of the Association forComputational Linguistics (ACL-02),pages 271?278, Philadelphia, PA.Sampson, Geoffrey.
1995.
English for theComputer: The SUSANNE Corpus andAnalytic Scheme.
Clarendon Press,Oxford, England.Tsuruoka, Yoshimasa, Yusuke Miyao,and Jun?ichi Tsujii.
2004.
Towardsefficient probabilistic HPSG parsing:Integrating semantic and syntacticpreference to guide the parsing.In Proceedings of IJCNLP-04 Workshop:Beyond shallow analyses?Formalismsand statistical modeling for deepanalyses, Hainan Island, China.
[Nopage numbers].van Genabith, Josef and Richard Crouch.1996.
Direct and underspecifiedinterpretations of LFG f-structures.
In 16thInternational Conference on ComputationalLinguistics (COLING 96), pages 262?267,Copenhagen, Denmark.123Computational Linguistics Volume 34, Number 1van Genabith, Josef and RichardCrouch.
1997.
On interpretingf-structures as UDRSs.
In Proceedingsof ACL-EACL-97, pages 402?409,Madrid, Spain.Xia, Fei.
1999.
Extracting tree adjoininggrammars from bracketed corpora.
InProceedings of the 5th Natural LanguageProcessing Pacific Rim Symposium(NLPRS-99), pages 398?403, Beijing,China.Xue, Nianwen, Fei Xia, Fu-Dong Chiou, andMartha Palmer.
2004.
The Penn Chinesetreebank: Phrase structure annotation of alarge corpus.
Natural Language Engineering,10(4):1?30.Yeh, Alexander.
2000.
More accurate testsfor the statistical significance of resultdifferences.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING, 2000), pages 947?953,Saarbru?cken, Germany.124
