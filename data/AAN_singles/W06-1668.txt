Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576?584,Sydney, July 2006. c?2006 Association for Computational LinguisticsCompetitive generative models with structure learning for NLPclassification tasksKristina ToutanovaMicrosoft ResearchRedmond, WAkristout@microsoft.comAbstractIn this paper we show that generativemodels are competitive with and some-times superior to discriminative models,when both kinds of models are allowed tolearn structures that are optimal for dis-crimination.
In particular, we compareBayesian Networks and Conditional log-linear models on two NLP tasks.
We ob-serve that when the structure of the gen-erative model encodes very strong inde-pendence assumptions (a la Naive Bayes),a discriminative model is superior, butwhen the generative model is allowed toweaken these independence assumptionsvia learning a more complex structure, itcan achieve very similar or better perfor-mance than a corresponding discrimina-tive model.
In addition, as structure learn-ing for generative models is far more ef-ficient, they may be preferable for sometasks.1 IntroductionDiscriminative models have become the modelsof choice for NLP tasks, because of their abilityto easily incorporate non-independent features andto more directly optimize classification accuracy.State of the art models for many NLP tasks are ei-ther fully discriminative or trained using discrim-inative reranking (Collins, 2000).
These includemodels for part-of-speech tagging (Toutanova etal., 2003), semantic-role labeling (Punyakanok etal., 2005; Pradhan et al, 2005b) and Penn Tree-bank parsing (Charniak and Johnson, 2005).The superiority of discriminative models hasbeen shown on many tasks when the discrimina-tive and generative models use exactly the samemodel structure (Klein and Manning, 2002).
How-ever, the advantage of the discriminative mod-els can be very slight (Johnson, 2001) and forsmall training set sizes generative models canbe better because they need fewer training sam-ples to converge to the optimal parameter setting(Ng and Jordan, 2002).
Additionally, many dis-criminative models use a generative model as abase model and add discriminative features withreranking (Collins, 2000; Charniak and Johnson,2005; Roark et al, 2004), or train discriminativelya small set of weights for features which are gener-atively estimated probabilities (Raina et al, 2004;Och and Ney, 2002).
Therefore it is important tostudy generative models and to find ways of mak-ing them better even when they are used only ascomponents of discriminative models.Generative models may often perform poorlydue to making strong independence assumptionsabout the joint distribution of features and classes.To avoid this problem, generative models forNLP tasks have often been manually designedto achieve an appropriate representation of thejoint distribution, such as in the parsing models of(Collins, 1997; Charniak, 2000).
This shows thatwhen the generative models have a good modelstructure, they can perform quite well.In this paper, we look differently at compar-ing generative and discriminative models.
We askthe question: given the same set of input features,what is the best a generative model can do if it isallowed to learn an optimal structure for the jointdistribution, and what is the best a discriminativemodel can do if it is also allowed to learn an op-timal structure.
That is, we do not impose any in-dependence assumptions on the generative or dis-criminative models and let them learn the best rep-resentation of the data they can.Structure learning is very efficient for genera-tive models in the form of directed graphical mod-els (Bayesian Networks (Pearl, 1988)), since theoptimal parameters for such models can be esti-mated in closed form.
We compare Bayesian Net-576works with structure learning to their closely re-lated discriminative counterpart ?
conditional log-linear models with structure learning.
Our condi-tional log-linear models can also be seen as Con-ditional Random Fields (Lafferty et al, 2001), ex-cept we do not have a structure on the labels, butwant to learn a structure on the features.We compare the two kinds of models on twoNLP classification tasks ?
prepositional phrase at-tachment and semantic role labelling.
Our re-sults show that the generative models are compet-itive with or better than the discriminative mod-els.
When a small set of interpolation parame-ters for the conditional probability tables are fitdiscriminatively, the resulting hybrid generative-discriminative models perform better than the gen-erative only models and sometimes better than thediscriminative models.In Section 2, we describe in detail the form ofthe generative and discriminative models we studyand our structure search methodology.
In Section3 we present the results of our empirical study.2 Model Classes and Methodology2.1 Generative ModelsIn classification tasks, given a training set of in-stances D = {[xi, yi]}, where xi are the inputfeatures for the i-th instance, and yi is its label,the task is to learn a classifier that predicts the la-bels of new examples.
If X is the space of inputsand Y is the space of labels, a classifier is a func-tion f : X ?
Y .
A generative model is one thatmodels the joint probability of inputs and labelsPD(x, y) through a distribution P?
(x, y), depen-dent on some parameter vector ?.
The classifierbased on this generative model chooses the mostlikely label given an input according to the con-ditionalized estimated joint distribution.
The pa-rameters ?
of the fitted distribution are usually es-timated using the maximum joint likelihood esti-mate, possibly with a prior.We study generative models represented asBayesian Networks (Pearl, 1988), because theirparameters can be estimated extremely fast as themaximizer of the joint likelihood is the closedform relative frequency estimate.
A Bayesian Net-work is an acyclic directed graph over a set ofnodes.
For every variable Z, let Pa(Z) denote theset of parents of Z.
The structure of the BayesianNetwork encodes the following set of indepen-YX1 X2 Xm......Figure 1: Naive Bayes Bayesian Networkdence assumptions: every variable is conditionallyindependent of its non-descendants given its par-ents.
For example, the structure of the BayesianNetwork model in Figure 1 encodes the indepen-dence assumption that the input features are con-ditionally independent given the class label.Let the input be represented as a vector of mnominal features.
We define Bayesian Networksover the m input variables X1, X2, .
.
.
, Xm andthe class variable Y .
In all networks, we add linksfrom the class variable Y to all input features.In this way we have generative models whichestimate class-specific distributions over featuresP (X|Y ) and a prior over labels P (Y ).
Figure 1shows a simple Bayesian Network of this form,which is the well-known Naive Bayes model.A specific joint distribution for a given BayesianNetwork (BN) is given by a set of condi-tional probability tables (CPTs) which spec-ify the distribution over each variable given itsparents P (Z|Pa(Z)).
The joint distributionP (Z1, Z2, .
.
.
, Zm) is given by:P (Z1, Z2, .
.
.
, Zm) =?i=1...mP (Zi|Pa(Zi))The parameters of a Bayesian Network modelgiven its graph structure are the values ofthe conditional probabilities P (Zi|Pa(Zi)).
Ifthe model is trained through maximizing thejoint likelihood of the data, the optimal pa-rameters are the relative frequency estimates:P?
(Zi = v|Pa(Zi) = ~u) = count(Zi=v,Pa(Zi)=~u)count(Pa(Zi)=~u) Herev denotes a value of Zi and ~u denotes a vector ofvalues for the parents of Zi.Most often smoothing is applied to avoid zeroprobability estimates.
A simple form of smooth-ing is add-?
smoothing which is equivalent to aDirichlet prior.
For NLP tasks it has been shownthat other smoothing methods are far superior toadd-?
smoothing ?
see, for example, Goodman577(2001).
In particular, it is important to incorpo-rate lower-order information based on subsets ofthe conditioning information.
Therefore we as-sume a structural form of the conditional proba-bility tables which implements a more sophisti-cated type of smoothing ?
interpolated Witten-Bell(Witten and Bell, 1991).
This kind of smooth-ing has also been used in the generative parser of(Collins, 1997) and has been shown to have a rel-atively good performance for language modeling(Goodman, 2001).To describe the form of the conditional proba-bility tables, we introduce some notation.
Let Zdenote a variable in the BN and Z1, Z2, .
.
.
, Zkdenote the set of its parents.
The probabil-ity P (Z = z|Z1 = z1, Z2 = z2, .
.
.
, Zk = zk) is estimatedusing Witten-Bell smoothing as follows: (belowthe tuple of values z1, z2, .
.
.
, zk is denoted byz1k).PWB(z|z1k) = ?
(z1k) ?
P?
(z|z1k) + (1 ?
?
(z1k)) ?
PWB(z|z1k?1)In the above equation, P?
is the relative fre-quency estimator.
The recursion is ended by inter-polating with a uniform distribution 1Vz , where Vzis the vocabulary of values for the prediction vari-able Z.
We determine the interpolation back-offorder by looking at the number of values of eachvariable.
We apply the following rule: the variablewith the highest number of values observed in thetraining set is backed off first, then the variablewith the next highest number of values, and so on.Typically, the class variable will be backed-off lastaccording to this rule.In Witten-Bell smoothing, the values of the in-terpolation coefficients are as follows: ?
(z1k) =count(z1k)count(z1k)+d?|z:count(z,z1k)>0| .
The weight of therelative frequency estimate based on a given con-text increases if the context has been seen moreoften in the training data and decreases if the con-text has been seen with more different values forthe predicted variable z.Looking at the form of our conditional proba-bility tables, we can see that the major parame-ters are estimated directly based on the counts ofthe events in the training data.
In addition, thereare interpolation parameters (denoted by d above),which participate in computing the interpolationweights ?.
The d parameters are hyper-parametersand we learn them on a development set of sam-ples.
We experimented with learning a single dparameter which is shared by all CPTs and learn-ing multiple d parameters ?
one for every type ofconditioning context in every CPT ?
i.e., each CPThas as many d parameters as there are back-off lev-els.We place some restrictions on the Bayesian Net-works learned, for closer correspondence with thediscriminative models and for tractability: Everyinput variable node has the label node as a parent,and at most three parents per variable are allowed.2.1.1 Structure Search MethodologyOur structure search method differs slightlyfrom previously proposed methods in the literature(Heckerman, 1999; Pernkopf and Bilmes, 2005).The search space is defined as follows.
We startwith a Bayesian Network containing only the classvariable.
We denote by CHOSEN the set of vari-ables already in the network and by REMAININGthe set of unplaced variables.
Initially, only theclass variable Y is in CHOSEN and all other vari-ables are in REMAINING.
Starting from the cur-rent BN, the set of next candidate structures is de-fined as follows: For every unplaced variable Rin REMAINING, and for every subset Sub of sizeat most two from the already placed variables inCHOSEN, consider adding R with parents Sub?Yto the current BN.
Thus the number of candidatestructures for extending a current BN is on the or-der of m3, where m is the number of variables.We perform a greedy search.
At each step, if thebest variable B with the best set of parents Pa(B)improves the evaluation criterion, move B fromREMAINING to CHOSEN, and continue the searchuntil there are no variables in REMAINING or theevaluation criterion can not be improved.The evaluation criterion for BNs we use is clas-sification accuracy on a development set of sam-ples.
Thus our structure search method is dis-criminative, in the terminology of (Grossman andDomingos, 2004; Pernkopf and Bilmes, 2005).
Itis very easy to evaluate candidate BN structures.The main parameters in the CPTs are estimatedvia the relative frequency estimator on the trainingset, as discussed in the previous section.
We do notfit the hyper-parameters d during structure search.We fit these parameters only after we have se-lected a final BN structure.
Throughout the struc-ture search, we use a fixed value of 1 for d for allCPTs and levels of back-off.
Therefore we are us-ing generative parameter estimation and discrimi-native structure search.
See Section 4 for discus-sion on how this method relates to previous work.578Notice that the optimal parameters of the con-ditional probability tables of variables already inthe current BN do not change at all when a newvariable is added, thus making update very ef-ficient.
After the stopping criterion is met, thehyper-parameters of the resulting BN are fit onthe development set.
As discussed in the previ-ous subsection, we fit either a single or multiplehyper-parameters d. The fitting criterion for thegenerative Bayesian Networks is joint likelihoodof the development set of samples with a Gaussianprior on the values log(d).
1Additionally, we explore fitting the hyper-parameters of the Bayesian Networks by opti-mizing the conditional likelihood of the develop-ment set of samples.
In this case we call theresulting models Hybrid Bayesian Network mod-els, since they incorporate a number of discrimi-natively trained parameters.
Hybrid models havebeen proposed before and shown to perform verycompetitively (Raina et al, 2004; Och and Ney,2002).
In Section 3.2 we compare generative andhybrid Bayesian Networks.2.2 Discriminative ModelsDiscriminative models learn a conditional distri-bution P?
(Y | ~X) or discriminant functions thatdiscriminate between classes.
Here we concen-trate on conditional log-linear models.
A sim-ple example of such model is logistic regression,which directly corresponds to Naive Bayes but istrained to maximize the conditional likelihood.
2To describe the form of models we study, let usintroduce some notation.
We represent a tuple ofnominal variables (X1,X2,.
.
.
,Xm) as a vector of0s and 1s in the following standard way: We mapthe tuple of values of nominal variables to a vectorspace with dimensionality the sum of possible val-ues of all variables.
There is a single dimension inthe vector space for every value of each input vari-able Xi.
The tuple (X1,X2,.
.
.
,Xm) is mapped toa vector which has 1s in m places, which are thecorresponding dimensions for the values of eachvariable Xi.
We denote this mapping by ?.In logistic regression, the probability of a labelY = y given input features ?
(X1, X2, .
.
.
, Xk) =1Since the d parameters are positive we convert the prob-lem to unconstrained optimization over parameters ?
suchthat d = e?
.2Logistic regression additionally does not have the sum toone constraint on weights but it can be shown that this doesnot increase the representational power of the model.~x is estimated as:P (y|~x) = exp ?
~wy, ~x??y?
exp ?
~wy?
, ~x?There is a parameter vector of feature weights~wy for each label y.
We fit the parameters of thelog-linear model by maximizing the conditionallikelihood of the training set including a gaussianprior on the parameters.
The prior has mean 0 andvariance ?2.
The variance is a hyper-parameter,which we optimize on a development set.In addition to this simple logistic regressionmodel, as for the generative models, we considermodels with much richer structure.
We considermore complex mappings ?, which incorporateconjunctions of combinations of input variables.We restrict the number of variables in the com-binations to three, which directly corresponds toour limit on number of parents in the BayesianNetwork structures.
This is similar to consider-ing polynomial kernels of up to degree three, butis more general, because, for example, we canadd only some and not all bigram conjunctionsof variables.
Structure search (or feature selec-tion) for log-linear models has been done beforee.g.
(Della Pietra et al, 1997; McCallum, 2003).We devise our structure search methodology in away that corresponds as closely as possible to ourstructure search for Bayesian Networks.
The ex-act hypothesis space considered is defined by thesearch procedure for an optimal structure we ap-ply, which we describe next.2.2.1 Structure Search MethodologyWe start with an initial empty feature set and acandidate feature set consisting of all input fea-tures: CANDIDATES={X1,X2,.
.
.
,Xm}.
In thecourse of the search, the set CANDIDATES maycontain feature conjunctions in addition to the ini-tial input features.
After a feature is selected fromthe candidates set and added to the model, the fea-ture is removed from CANDIDATES and all con-junctions of that feature with all input features areadded to CANDIDATES.
For example, if a fea-ture conjunction ?Xi1 ,Xi2 ,.
.
.,Xin?
is selected, allof its expansions of the form ?Xi1 ,Xi2 ,.
.
.,Xin ,Xi?,where Xi is not in the conjunction already, areadded to CANDIDATES.We perform a greedy search and at each stepselect the feature which maximizes the evaluationcriterion, add it to the model and extend the set579CANDIDATES as described above.
The evaluationcriterion for selecting features is classification ac-curacy on a development set of samples, as for theBayesian Network structure search.At each step, we evaluate all candidate fea-tures.
This is computationally expensive, becauseit requires iterative re-estimation.
In addition toestimating weights for the new features, we re-estimate the old parameters, since their optimalvalues change.
We did not preform search for thehyper-parameter ?
when evaluating models.
We fit?
by optimizing the development set accuracy af-ter a model was selected.
Note that our feature se-lection algorithm adds an input variable or a vari-able conjunction with all of its possible values in asingle step of the search.
Therefore we are addinghundreds or thousands of binary features at eachstep, as opposed to only one as in (Della Pietraet al, 1997).
This is why we can afford to per-form complete re-estimation of the parameters ofthe model at each step.3 Experiments3.1 Problems and DatasetsWe study two classification problems ?
preposi-tional phrase (PP) attachment, and semantic rolelabeling.Following most of the literature on preposi-tional phrase attachment (e.g., (Hindle and Rooth,1993; Collins and Brooks, 1995; Vanschoen-winkel and Manderick, 2003)), we focus on themost common configuration that leads to ambi-guities: V NP PP.
Here, we are given a verbphrase with a following noun phrase and a prepo-sitional phrase.
The goal is to determine if thePP should be attached to the verb or to the ob-ject noun phrase.
For example, in the sentence:Never [hang]V [a painting]NP [with a peg]PP , theprepositional phrase with a peg can either modifythe verb hang or the object noun phrase a painting.Here, clearly, with a peg modifies the verb hang.We follow the common practice in representingthe problem using only the head words of theseconstituents and of the NP inside the PP.
Thus theexample sentence is represented as the followingquadruple: [v:hang n1:painting p:with n2:peg].Thus for the PP attachment task we have binarylabels Att , and four input variables ?
v, n1, p, n2.We work with the standard dataset previouslyused for this task by other researchers (Ratna-Task Training Devset TestPP 20,801 4,039 3,097SRL 173,514 5,115 9,272Table 1: Data sizes for the PP attachment and SRLtasks.parkhi et al, 1994; Collins and Brooks, 1995).
It isextracted from the the Penn Treebank Wall StreetJournal data (Ratnaparkhi et al, 1994).
Table 1shows summary statistics for the dataset.The second task we concentrate on is semanticrole labeling in the context of PropBank (Palmeret al, 2005).
The PropBank corpus annotatesphrases which fill semantic roles for verbs on topof Penn Treebank parse trees.
The annotated rolesspecify agent, patient, direction, etc.
The labelsfor semantic roles are grouped into two groups,core argument labels and modifier argument la-bels, which correspond approximately to the tradi-tional distinction between arguments and adjuncts.There has been plenty of work on machinelearning models for semantic role labeling, start-ing with the work of Gildea and Jurafsky (2002),and including CoNLL shared tasks (Carreras andMa`rquez, 2005).
The most successful formulationhas been as learning to classify nodes in a syn-tactic parse tree.
The possible labels are NONE,meaning that the corresponding phrase has no se-mantic role and the set of core and modifier la-bels.
We concentrate on the subproblem of clas-sification for core argument nodes.
The problemis, given that a node has a core argument label, de-cide what the correct label is.
Other researchershave also looked at this subproblem (Gildea andJurafsky, 2002; Toutanova et al, 2005; Pradhan etal., 2005a; Xue and Palmer, 2004).Many features have been proposed for build-ing models for semantic role labeling.
Initially,7 features were proposed by (Gildea and Juraf-sky, 2002), and all following research has usedthese features and some additional ones.
Theseare the features we use as well.
Table 2 lists thefeatures.
State-of-the-art models for the subprob-lem of classification of core arguments addition-ally use other features of individual nodes (Xueand Palmer, 2004; Pradhan et al, 2005a), as wellas global features including the labels of othernodes in parse tree.
Nevertheless it is interestingto see how well we can do with these 7 featuresonly.We use the standard training, development, and580Feature Types (Gildea and Jurafsky, 2002)PHRASE TYPE: Syntactic Category of nodePREDICATE LEMMA: Stemmed VerbPATH: Path from node to predicatePOSITION: Before or after predicate?VOICE: Active or passive relative to predicateHEAD WORD OF PHRASESUB-CAT: CFG expansion of predicate?s parentTable 2: Features for Semantic Role Labeling.test sets from the February 2004 version of Prop-bank.
The training set consists of sections 2 to 21,the development set is from section 24, and the testset is from section 23.
The number of samples islisted in Table 1.
As we can see, the training setsize is much larger compared to the PP attachmenttraining set.3.2 ResultsIn line with previous work (Ng and Jordan, 2002;Klein and Manning, 2002), we first compare NaiveBayes and Logistic regression on the two NLPtasks.
This lets us see how they compare when thegenerative model is making strong independenceassumptions and when the two kinds of modelshave the same structure.
Then we compare thegenerative and discriminative models with learnedricher structures.Table 3 shows the Naive Bayes/Logistic re-gression results for PP attachment.
We list re-sults for several conditions of training the NaiveBayes classifier, depending on whether it is trainedas strictly generative or as a hybrid model, andwhether a single or multiple hyper-parameters dare trained.
In the table, we see results for gen-erative Naive Bayes, where the d parameters aretrained to maximize the joint likelihood of the de-velopment set, and for Hybrid Naive Bayes, wherethe hyper-parameters are trained to optimize theconditional likelihood.
The column H-Params (forhyper-parameters) indicates whether a single ormultiple d parameters are learned.Logistic regression is more fairly comparableto Naive Bayes trained using a single hyper-parameter, because it also uses a single hyper-parameter ?
trained on a development set.
How-ever, for the generative model it is very easy totrain multiple weights d since the likelihood of adevelopment set is differentiable with respect tothe parameters.
For logistic regression, we maywant to choose different variances for the differ-ent types of features but the search would be pro-Model H-params Test set accNaive Bayes 1 81.2Naive Bayes 9 81.2Logistic regression 1 82.6Hybrid Naive Bayes 1 81.2Hybrid Naive Bayes 9 81.5Table 3: Naive Bayes and Logistic regression PPattachment results.hibitively expensive.
Thus we think it is also fairto fit multiple interpolation weights for the gener-ative model and we show these results as well.As we can see from the table, logistic regressionoutperforms both Naive Bayes and Hybrid NaiveBayes.
The performance of Hybrid Naive Bayeswith multiple interpolation weights improves theaccuracy, but performance is still better for logis-tic regression.
This suggests that the strong in-dependence assumptions are hurting the classifier.According to McNemar?s test, logistic regressionis statistically significantly better than the NaiveBayes models and than Hybrid Naive Bayes with asingle interpolation weight (p < 0.025), but is notsignificantly better than Hybrid Naive Bayes withmultiple interpolation parameters at level 0.05.However, when both the generative and dis-criminative models are allowed to learn optimalstructures, the generative model outperforms thediscriminative model.
As seen from Table 4,the Bayesian Network with a single interpolationweight achieves an accuracy of 84.6%, whereasthe discriminative model performs at 83.8%.
Thehybrid model with a single interpolation weightdoes even better, achieving 85.0% accuracy.
Forcomparison, the model of Collins & Brooks hasaccuracy of 84.15% on this test set, and the high-est result obtained through a discriminative modelwith this feature set is 84.8%, using SVMs and apolynomial kernel with multiple hyper-parameters(Vanschoenwinkel and Manderick, 2003).
TheHybrid Bayes Nets are statistically significantlybetter than the Log-linear model (p < 0.05), andthe Bayes Nets are not significantly better than theLog-linear model.
All models from Table 4 aresignificantly better than all models in Table 3.For semantic role labelling classification of corearguments, the results are listed in Tables 5 and6.
We can see that the difference in performancebetween Naive Bayes with a single interpolationparameter d ?
83.3% and the performance of Lo-gistic regression ?
91.1%, is very large.
Thisshows that the independence assumptions are quite581Model H-params Test set accBayes Net 1 84.6Bayes Net 13 84.6Log-linear model 1 83.8Hybrid Bayes Net 1 85.0Hybrid Bayes Net 13 84.8Table 4: Bayesian Network and Conditional log-linear model PP attachment results.Model H-params Test set accNaive Bayes 1 83.3Naive Bayes 15 85.2Logistic regression 1 91.1Hybrid Naive Bayes 1 84.1Hybrid Naive Bayes 15 86.5Table 5: Naive Bayes and Logistic regression SRLclassificaion results.strong, and since many of the features are notsparse lexical features and training data for themis sufficient, the Naive Bayes model has no ad-vantage over the discriminative logistic regressionmodel.
The Hybrid Naive Bayes model with mul-tiple interpolation weights does better than NaiveBayes, performing at 86.5%.
All differences be-tween the classifiers in Table 5 are statistically sig-nificant at level 0.01.
Compared to the PP attach-ment task, here we are getting more benefit frommultiple hyper-parameters, perhaps due to the di-versity of the features for SRL: In SRL, we useboth sparse lexical features and non-sparse syntac-tic ones, whereas all features for PP attachment arelexical.From Table 6 we can see that when we com-pare general Bayesian Network structures to gen-eral log-linear models, the performance gap be-tween the generative and discriminative modelsis much smaller.
The Bayesian Network with asingle interpolation weight d has 93.5% accuracyand the log-linear model has 93.9% accuracy.
Thehybrid model with multiple interpolation weightsperforms at 93.7%.
All models in Table 6 are ina statistical tie according to McNemar?s test, andthus the log-linear model is not significantly bet-ter than the Bayes Net models.
We can see thatthe generative model was able to learn a structurewith a set of independence assumptions which arenot as strong as the ones the Naive Bayes modelmakes, thus resulting in a model with performancecompetitive with the discriminative model.Figures 2(a) and 2(b) show the Bayesian Net-works learned for PP Attachment and SemanticRole Labeling.
Table 7 shows the conjunctionsModel H-params Test set accBayes Net 1 93.5Bayes Net 20 93.6Log-linear model 1 93.9Hybrid Bayes Net 1 93.5Hybrid Bayes Net 20 93.7Table 6: Bayesian Network and Conditional log-linear model SRL classification results.PP Attachment Model?P?, ?P,V?, ?P,N1?, ?P,N2?
?N1?,?V?, ?P,N1,N2?SRL Model?PATH?, ?PATH,PLEMMA?,?SUB-CAT?,?PLEMMA??HW,PLEMMA?,?PATH,PLEMMA,VOICE?,?HW,PLEMMA,PTYPE?,?SUB-CAT,PLEMMA?
?SUB-CAT,PLEMMA,POS?,?HW?Table 7: Log-linear models learned for PP attach-ment and SRL.learned by the Log-linear models for PP attach-ment and SRL.We should note that it is much faster to dostructure search for the generative Bayesian Net-work model, as compared to structure search forthe log-linear model.
In our implementation, wedid not do any computation reuse between succes-sive steps of structure search for the Bayesian Net-work or log-linear models.
Structure search took 2hours for the Bayesian Network and 24 hours forthe log-linear model.To put our results in the context of previouswork, other results on core arguments using thesame input features have been reported, the bestbeing 91.4% for an SVM with a degree 2 poly-nomial kernel (Pradhan et al, 2005a).3 Thehighest reported result for independent classifica-tion of core arguments is 96.0% for a log-linearmodel using more than 20 additional basic features(Toutanova et al, 2005).
Therefore our resultingmodels with 93.5% and 93.9% accuracy comparefavorably to the SVM model with polynomial ker-nel and show the importance of structure learning.4 Comparison to Related WorkPrevious work has compared generative and dis-criminative models having the same structure,such as the Naive Bayes and Logistic regressionmodels (Ng and Jordan, 2002; Klein and Man-ning, 2002) and other models (Klein and Manning,2002; Johnson, 2001).3This result is on an older version of Propbank from July2002.582AttPN1N2V(a) Learned Bayesian Networkfor PP attachment.RoleSub-cat PathVoicePLem HWPos(b) Learned Bayesian Networkfor SRL.Figure 2: Learned Bayesian Network structuresfor PP attachment and SRL.Bayesian Networks with special structure of theCPTs ?
e.g.
decision trees, have been previouslystudied in e.g.
(Friedman and Goldszmidt, 1996),but not for NLP tasks and not in comparison to dis-criminative models.
Studies comparing generativeand discriminative models with structure learn-ing have been previously performed ((Pernkopfand Bilmes, 2005) and (Grossman and Domingos,2004)) for other, non-NLP domains.
There areseveral important algorithmic differences betweenour work and that of (Pernkopf and Bilmes, 2005;Grossman and Domingos, 2004).
We detail thedifferences here and perform an empirical evalua-tion of the impact of some of these differences.Form of the generative models.
The genera-tive models studied in that previous work do notemploy any special form of the conditional prob-ability tables.
Pernkopf and Bilmes (2005) use asimple smoothing method: fixing the probabilityof every event that has a zero relative frequencyestimate to a small fixed .
Thus the model doesnot take into account information from lower or-der distributions and has no hyper-parameters thatare being fit.
Grossman and Domingos (2004) donot employ a special form of the CPTs either anddo not mention any kind of smoothing used in thegenerative model learning.Form of the discriminative models.
Theworks (Pernkopf and Bilmes, 2005; Grossmanand Domingos, 2004) study Bayesian Networkswhose parameters are trained discriminatively (bymaximizing conditional likelihood), as represen-tatives of discriminative models.
We study moregeneral log-linear models, equivalent to MarkovRandom Fields.
Our models are more generalin that their parameters do not need to be inter-pretable as probabilities (sum to 1 and between 0and 1), and the structures do not need to corre-spond to Bayes Net structures.
For discriminativeclassifiers, it is not important that their compo-nent parameters be interpretable as probabilities;thus this restriction is probably unnecessary.
Likefor the generative models, another major differ-ence is in the smoothing algorithms.
We smooththe models both by fitting a gaussian prior hyper-parameter and by incorporating features of subsetsof cliques.
Smoothing in (Pernkopf and Bilmes,2005) is done by substituting zero-valued param-eters with a small fixed .
Grossman and Domin-gos (2004) employ early stopping using held-outdata which can achieve similar effects to smooth-ing with a gaussian prior.To evaluate the importance of the differencesbetween our algorithm and the ones presented inthese works, and to evaluate the importance of fit-ting hyper-parameters for smoothing, we imple-mented a modified version of our structure search.The modifications were as follows.
For BayesNet structure learning: (i) no Witten-Bell smooth-ing is employed in the CPTs, and (ii) no backoffsto lower-order distributions are considered.
Theonly smoothing remaining in the CPTs is an inter-polation with a uniform distribution with a fixedweight of ?
= .1.
For discriminative log-linearmodel structure learning: (i) the gaussian priorwas fixed to be very weak, serving only to keep theweights away from infinity (?
= 100) and (ii) theconjunction selection was restricted to correspondto a Bayes Net structure with no features for sub-sets of feature conjunctions.
Thus the only differ-ence between the class of our modified discrimina-tive log-linear models and the class of models con-sidered in (Pernkopf and Bilmes, 2005; Grossmanand Domingos, 2004) is that we do not restrict theparameters to be interpretable as probabilities.The results shown in Table 8 summarize the re-sults obtained by the modified algorithm on thetwo tasks.
Both the generative and discriminativelearners suffered a statistically significant (at level.01) loss in performance.
Notably, the log-linearmodel for PP attachment performs worse than lo-gistic regression with better smoothing.583PP Attachment ResultsModel H-params Test set accBayes Net 0 82.8Log-linear model 0 81.2SRL Classification ResultsModel H-params Test set accBayes Net 0 92.5Log-linear model 0 92.7Table 8: Bayesian Network and Conditional log-linear model: PP & SRL classification results us-ing minimal smoothing and no backoff to lowerorder distributions.In summary, our results showed that by learningthe structure for generative models, we can obtainmodels which are competitive with or better thancorresponding discriminative models.
We alsoshowed the importance of employing sophisti-cated smoothing techniques in structure search al-gorithms for natural language classification tasks.ReferencesXavier Carreras and Lu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.
InProceedings of CoNLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.
InProceedings of ACL.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL, pages 132?139.Michael Collins and James Brooks.
1995.
Prepositional at-tachment through a backed-off model.
In Proceedings ofthe Third Workshop on Very Large Corpora, pages 27?38.Michael Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In Proceedings of ACL, pages 16 ?23.Michael Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proceedings of ICML, pages 175?182.Stephen Della Pietra, Vincent J. Della Pietra, and John D.Lafferty.
1997.
Inducing features of random fields.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 19(4):380?393.Nir Friedman and Moises Goldszmidt.
1996.
LearningBayesian networks with local structure.
In Proceeding ofUAI, pages 252?262.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labelingof semantic roles.
Computational Linguistics, 28(3):245?288.Joshua T. Goodman.
2001.
A bit of progress in languagemodeling.
In MSR Technical Report MSR-TR-2001-72.Daniel Grossman and Pedro Domingos.
2004.
Learningbayesian network classifiers by maximizing conditionallikelihood.
In Proceedings of ICML, pages 361?368.David Heckerman.
1999.
A tutorial on learning with bayesiannetworks.
In Learning in Graphical Models.
MIT Press.Donald Hindle and Mats Rooth.
1993.
Structural ambi-guity and lexical relations.
Computational Linguistics,19(1):103?120.Mark Johnson.
2001.
Joint and conditional estimation oftagging and parsing models.
In Proceedings of ACL.Dan Klein and Christopher Manning.
2002.
Conditionalstructure versus conditional estimation in NLP models.
InProceedings of EMNLP.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proc.
18th In-ternational Conf.
on Machine Learning, pages 282?289.Morgan Kaufmann, San Francisco, CA.Andrew McCallum.
2003.
Efficiently inducing features ofconditional random fields.
In Proceedings of UAI.Andrew Ng and Michael Jordan.
2002.
On discriminative vs.generative classifiers: A comparison of logistic regressionand Naive Bayes.
In NIPS 14.Franz Josef Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statistical ma-chine translation.
In Proceedings of ACL, pages 295?302.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.
Theproposition bank: An annotated corpus of semantic roles.Computational Linguistics.Judea Pearl.
1988.
Probabilistic reasoning in intelligentsystems: Networks of plausible inference.
Morgan Kauf-mann.Franz Pernkopf and Jeff Bilmes.
2005.
Discriminative versusgenerative parameter and structure learning of bayesiannetwork classifiers.
In Proceedings of ICML.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, WayneWard, James Martin, and Dan Jurafsky.
2005a.
Supportvector learning for semantic argument classification.
Ma-chine Learning Journal.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Mar-tin, and Daniel Jurafsky.
2005b.
Semantic role labelingusing different syntactic views.
In Proceedings of ACL.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2005.
Thenecessity of syntactic parsing for semantic role labeling.In Proceedings of IJCAI.Rajat Raina, Yirong Shen, Andrew Y. Ng, and AndrewMcCallum.
2004.
Classification with hybrid genera-tive/discriminative models.
In Sebastian Thrun, LawrenceSaul, and Bernhard Scho?lkopf, editors, Advances in Neu-ral Information Processing Systems 16.
MIT Press, Cam-bridge, MA.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994.
Amaximum entropy model for prepositional phrase attach-ment.
In Workshop on Human Language Technology.Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative language modeling withconditional random fields and the perceptron algorithm.In Proceedings of ACL.Kristina Toutanova, Dan Klein, and Christopher D. Manning.2003.
Feature-rich part-of-speech tagging with a cyclicdependency network.
In Proceedings of HLT-NAACL.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
2005.
Joint learning improves semantic role label-ing.
In Proceedings of ACL.Bram Vanschoenwinkel and Bernard Manderick.
2003.
Aweighted polynomial information gain kernel for resolv-ing prepositional phrase attachment ambiguities with sup-port vector machines.
In IJCAI.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequencyproblem: Estimating the probabilities of novel events inadaptive text compression.
IEEE Transactions on Infor-mation Theory, 37,4:1085?1094.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP.584
