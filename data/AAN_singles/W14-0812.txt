Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 72?76,Gothenburg, Sweden, 26-27 April 2014.c?2014 Association for Computational LinguisticsExtraction of Nominal Multiword Expressions in FrenchMarie Dubremetz and Joakim NivreUppsala universityDepartment of Linguistics and PhilologyUppsala, SwedenAbstractMultiword expressions (MWEs) can beextracted automatically from large corporausing association measures, and tools likemwetoolkit allow researchers to generatetraining data for MWE extraction given atagged corpus and a lexicon.
We use mwe-toolkit on a sample of the French Europarlcorpus together with the French lexiconDela, and use Weka to train classifiers forMWE extraction on the generated trainingdata.
A manual evaluation shows that theclassifiers achieve 60?75% precision andthat about half of the MWEs found arenovel and not listed in the lexicon.
We alsoinvestigate the impact of the patterns usedto generate the training data and find thatthis can affect the trade-off between preci-sion and novelty.1 IntroductionIn alphabetic languages, words are delimited byspaces.
Some words can combine to create a newunit of meaning that we call a multiword expres-sion (MWE).
However, MWEs such as kick thebucket must be distinguished from free combina-tions of words such as kick the ball.
A sequence ofseveral words is an MWE if ?at least one of its syn-tactic, distributional or semantic properties cannotbe deduced from the properties of its component?
(Silberztein and L.A.D.L., 1990).
So how can weextract them?Statistical association measures have long beenused for MWE extraction (Pecina, 2010), and bytraining supervised classifiers that use associationmeasures as features we can further improve thequality of the extraction process.
However, super-vised machine learning requires annotated data,which creates a bottleneck in the absence of largecorpora annotated for MWEs.
In order to cir-cumvent this bottleneck, mwetoolkit (Ramisch etal., 2010b) generates training instances by first ex-tracting candidates that fit a certain part-of-speechpattern, such as Noun-Noun or Noun-Adjective,and then marking the candidates as positive ornegative instances depending on whether they canbe found in a given lexicon or not.
Such a train-ing set will presumably not contain any false pos-itives (that is, candidates marked as positive in-stances that are not real MWEs), but depending onthe coverage of the lexicon there will be a smalleror larger proportion of false negatives.
The ques-tion is what quality can be obtained using such anoisy training set.
To the best of our knowledge,we cannot find the answer for French in literature.Indeed, Ramisch et al.
(2012) compares the perfor-mance of mwetoolkit with another toolkit on En-glish and French corpora, but they never use thedata generated by mwetoolkit to train a model.
Incontrast, Zilio et al.
(2011) make a study involvingtraining a model but use it only on English and useextra lexical resources to complement the machinelearning method, so their study does not focus juston classifier evaluation.This paper presents the first evaluation of mwe-toolkit on French together with two resources verycommonly used by the French NLP community:the tagger TreeTagger (Schmid, 1994) and the dic-tionary Dela.1Training and test data are takenfrom the French Europarl corpus (Koehn, 2005)and classifiers are trained using the Weka machinelearning toolkit (Hall et al., 2009).
The primarygoal is to evaluate what level of precision can beachieved for nominal MWEs, using a manual eval-uation of MWEs extracted, and to what extent theMWEs extracted are novel and can be used to en-rich the lexicon.
In addition, we will investigatewhat effect the choice of part-of-speech patternsused to generate the training data has on precisionand novelty.
Our results indicate that classifiers1http://www-igm.univ-mlv.fr/?unitex/index.php?page=5&html=bibliography.html72achieve precision in the 60?75% range and thatabout half of the MWEs found are novel ones.
Inaddition, it seems that the choice of patterns usedto generate the training data can affect the trade-off between precision and novelty.2 Related Work2.1 Extraction TechniquesThere is no unique definition of MWEs (Ramisch,2012).
In the literature on the subject, we no-tice that manual MWE extraction often requiresseveral annotators native of the studied language.Nevertheless, some techniques exist for selectingautomatically candidates that are more likely to bethe true ones.
Candidates can be validated againstan external resource, such as a lexicon.
It is pos-sible also to check the frequency of candidates inanother corpus like the web.
Villaviciencio (2005),for example, uses number of hits on Google forvalidating the likelihood of particle verbs.However, as Ramisch (2012) states in hisintroduction, MWE is an institutionalised phe-nomenon.
This means that an MWE is fre-quently used and is part of the vocabulary of aspeaker as well as the simple words.
It meansalso that MWEs have specific statistical proper-ties that have been studied.
The results of thosestudies are statistical measures such as dice score,maximum likelihood estimate, pointwise mutualinformation, T-score.
As Islam et al.
(2012) re-mark in a study of Google Ngram, those measuresof association are language independent.
And itis demonstrated by Pecina (2008) that combiningdifferent collocation measures using standard sta-tistical classification methods improves over usinga single collocation measure.
However, nowadays,using only lexical association measures for extrac-tion and validation of MWE is not considered themost effective method.
The tendency these lastyears is to combine association measures with lin-guistic features (Ramisch et al., 2010a; Pecina,2008; Tsvetkov and Wintner, 2011).2.2 MwetoolkitAmong the tools developed for extracting MWEs,mwetoolkit is one of the most recent.
Developedby Ramisch et al.
(2010b) it aims not only at ex-tracting candidates for potential MWEs, but alsoat extracting their association measures.
Providedthat a lexicon of MWEs is available and provideda preprocessed corpus, mwetoolkit makes it pos-sible to train a machine learning system with theassociation measures as features with a minimumof implementation.Ramisch et al.
(2010b) provide experiments onPortuguese, English and Greek.
Zilio et al.
(2011)provide experiments with this tool as well.
In thelatter study, after having trained a machine on bi-gram MWEs, they try to extract full n-gram ex-pressions from the Europarl corpus.
They thenreuse the model obtained on bigrams for extractionof full n-gram MWEs.
Finally, they apply a secondfilter for getting back the false negatives by check-ing every MWE annotated as False by the algo-rithm against a online dictionary.
This method getsa very good precision (over 87%) and recall (over84%).
However, we do not really know if this re-sult is mostly due to the coverage of the dictionaryonline.
What is the contribution of machine learn-ing in itself?
Another question raised by this studyis the ability of a machine trained on one kind ofpattern (e.g., Noun-Adjective) to extract correctlyanother kind of MWE pattern (e.g., Noun-Noun).That is the reason why we will run three experi-ments close to the one of Zilio et al.
(2011) butwere the only changing parameter is the patternthat we train our classifiers on.3 Generating Training Data3.1 Choice of PatternsIn contrast to Zilio et al.
(2011) we run our ex-periment on French.
The choice of a differ-ent language requires an adaptation of the pat-terns.
French indeed, as a latin language, doesnot show the same characteristic patterns as En-glish.
We know that there is a strong recurrenceof the pattern Noun-Adjective in bigram MWEsin our lexicon (Silberztein and L.A.D.L., 1990,p.82), and the next most frequent pattern is Noun-Noun.
Therefore we extract only candidates thatcorrespond to these patterns.
And, since we havetwo patterns, we will run two extra experimentswhere our models will be trained only on one ofthe patterns.
In this way, we will discover howsensitive the method is to the choice of pattern.3.2 CorpusAs Ramisch et al.
(2012) we work on the FrenchEuroparl corpus.
We took the three first millionwords of Europarl and divided it into three equalparts (one million words each) for running our ex-periments.
The first part will be devoted at 80% to73training and 20% to development test set, whentraining classifiers on Noun-Adjective or Noun-Noun patterns, or both.
We use the second millionas a secondary development set that is not used inthis study.
The third million is used as a final testset and we will present results on this set.3.3 PreprocessingFor preprocessing we used the same processes asdescribed in Zilio et al.
(2011).
First we ran thesentence splitter and the tokenizer provided withthe Europarl corpus.
Then we ran TreeTagger(Schmid, 1994) to obtain the tags and the lemmas.3.4 Extracting Data and FeaturesThe mwetoolkit takes as input a preprocessed cor-pus plus a lexicon and gives two main outputs: anarff file which is a format adapted to the machinelearning framework Weka, and an XML file.
Atthe end of the process we obtain, for each candi-date, a binary classification as an MWE (True) ornot (False) depending on whether it is containedin the lexicon.
For each candidate, we also ob-tain the following features: maximum likelihoodestimate, pointwise mutual information, T-score,dice coefficient, log-likelihood ratio.
The machinelearning task is then to predict the class (True orFalse) given the features of a candidate.3.5 Choice of a Lexicon in FrenchThe evaluation part of mwetoolkit is furnishedwith an internal English lexicon as a gold stan-dard for evaluating bigram MWEs, but for Frenchit is necessary to provide an external resource.We used as our gold standard the French dictio-nary Dela (Silberztein and L.A.D.L., 1990), theMWE part of which is called Delac.
It is a gen-eral purpose dictionary for NLP and it includes100,000 MWE expressions, which is a reasonablesize for leading an experiment on the Europarlcorpus.
Also the technical documentation of theDelac (Silberztein and L.A.D.L., 1990, p.72) saysthat this dictionary has been constructed by lin-guists with reference to several dictionaries.
So itis a manually built resource that contains MWEsonly referenced in official lexicographical books.3.6 ProcessingThanks to mwetoolkit we extracted all the bi-grams that correspond to the patterns Noun-Adjective (NA), Noun-Noun (NN) and to bothNoun-Adjective and Noun-Noun (NANN) in ourthree data sets and let mwetoolkit make an auto-matic annotation by checking the presence of theMWE candidates in the Delac.
Note that the auto-matic annotation was used only for training.
Thefinal evaluation was done manually.4 Training ClassifiersFor finding the best model we think that we haveto favour the recall of the positive candidates.
In-deed, when an MWE candidate is annotated asTrue, it means that it is listed in the Dela, whichmeans that it is an officially listed MWE.
How-ever, if an MWE is not in the Dela, it does notmean that the candidate does not fulfil all the cri-teria for being an MWE.
For this reason, obtaininga good recall is much more difficult than getting agood precision, but it is also the most important ifwe stay on a lexicographical purpose.4.1 Training on NAWe tested several algorithms offered by Weka aswell as the training options suggested by Zilio etal.
(2011).
We also tried to remove some featuresand to keep only the most informative ones (MLE,T-score and log-likelihood according to informa-tion gain ratio) but we noticed each time a loss inthe recall.
At the end with all the features kept andfor the purpose of evaluating NA MWE candidatesthe best classification algorithm was the Bayesiannetwork.4.2 Training on NNWhen training a model on NN MWEs, our aimwas to keep as much as possible the same condi-tion for our three experiments.
However, the NNtraining set has definitely not the same propertiesas the NA and NANN ones.
The NN training setis twenty-four times smaller than NA training set.Most of the algorithms offered by Weka thereforeended up with a dummy systematic classificationto the majority class False.
The only exceptionswere ibk, ib1, hyperpipes, random trees and ran-dom forest.
We kept random forest because it gavethe best recall with a very good precision.
We triedseveral options and obtained the optimum resultswith 8 trees each constructed while considering 3random features, one seed, and unlimited depth oftrees.
As well as for NA we kept all features.4.3 Training on NA+NNFor the training on NANN candidates we tried thesame models as for NN and for NA candidates.74The best result was obtained with the same algo-rithm as for NA: Bayesian network.5 EvaluationThe data automatically annotated by mwetoolkitcould be used for training, but to properly evalu-ate the precision of MWE extraction on new dataand not penalize the system for ?false positives?that are due to lack of coverage of the lexicon, weneeded to perform a manual annotation.
To do so,we randomly picked 100 candidates annotated asTrue by each model (regardless if they were in theDelac or not).
We then annotated all such candi-dates as True if they were found in Delac (withoutfurther inspection) and otherwise classified themmanually following the definition of Silberzteinand L.A.D.L.
(1990) and the intuition of a nativeFrench speaker.
The results are in Table 1.Extracting NA NN NANNNANN model model modelIn Delac 40 ?9.4 18 ?7.2 28 ?8.6Not in Delac 34 ?9.0 41 ?9.2 38 ?9.3Precision 74 ?8.4 59 ?9.2 66 ?9.0Table 1: Performance of three different modelson the same corpus of Noun-Adjective and Noun-Noun candidates.
Percentages with 95% confi-dence intervals, sample size = 100.As we see in Table 1, the experiment reveals a pre-cision ranging from almost 60% up to 74%.
Theresults of our comparative manual annotation indi-cate that the model trained on NN candidates hasthe capacity to find more MWEs not listed in ourlexicon (41 out of 59) even if it is the least pre-cise model.
On the other hand, we notice that themodel based on Noun-Adjective patterns is moreprecise but at the same time extracts fewer MWEsthat are not already in the lexicon (34 out of 74).Our mixed model confirms these two tendencieswith a performance in between (38 new MWEs outof 66).
Thus, the method appears to be sensitive tothe patterns used for training.We notice during evaluation different kinds ofMWEs that are successfully extracted by modelsbut that are not listed in the Delac.
Most of themare the MWEs specific to Europarl (e.g., ?dimen-sion communautaire?, ?l?egislation europ?eenne?2).Another category are those MWEs that became2?community scale?, ?European legislation?popular in the French language after the years2000?s and therefore could not be included in theDelac, released in 1997.
Indeed by reading thefirst paragraph of the French version of Europarlwe notice that the texts have been written after1999.
Of course, they are not the majority of thesuccessfully extracted MWEs but we still manageto find up to 3 of them in a sample of 100 that wechecked (?d?eveloppement durable?, ?radiophonienum?erique?, ?site internet?3).
Furthermore the cor-pus in itself is already more than ten years old,so in a text of 2014 we can expect to find evenmore of them.
Finally, there are MWEs that arenot in French (e.g., ?Partido popular?
), these, how-ever, did not appear systematically in our samples.It is tricky to learn statistical properties ofMWEs when, actually, we do not have all the in-formation necessary for extracting the MWEs inthe corpus.
Indeed, for this purpose the corpusshould ideally be read and annotated by humans.However, we still managed to train models withdecent performance, even if it is likely that a lot ofcandidates pre-annotated as False in the trainingdata were probably perfect MWEs.
This meansthat the Delac has covered enough MWEs for thefeatures to not appear as completely meaninglessand arbitrary.
The final precision would never beas good as it is, if the coverage had been not suffi-cient enough.
This shows that the method of auto-matic annotation offered by mwetoolkit is reliablegiven a lexicon as large as Delac.6 ConclusionWe wanted to know if the method of automaticextraction and evaluation offered by mwetoolkitcould have a decent precision in French.
We an-notated automatically part of the Europarl corpusgiven the lexical resource Dela as a gold stan-dard and generated in this way annotated trainingsets.
Classifiers trained on this data using Wekaachieved a maximum precision of 74%, with abouthalf of the extracted MWEs being novel comparedto the lexicon.
In addition, we found that the fi-nal precision and novelty scores were sensitive tothe choice of patterns used to generate the trainingdata.3?sustainable development?, ?digital radio?,?website?75ReferencesMark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The WEKA data mining software: an update.SIGKDD Exploration Newsletter, 11(1):10?18.Aminul Islam, Evangelos E Milios, and Vlado Ke-selj.
2012.
Comparing Word Relatedness Mea-sures Based on Google n-grams.
In COLING, Inter-national Conference on Computational Linguistics(Posters), pages 495?506, Mumbai, India.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In The TenthMachine Translation Summit, pages 79?86, Phuket,Thailand.Pavel Pecina.
2008.
A Machine Learning Approachto Multiword Expression Extraction.
In Proceed-ings of the LREC 2008 Workshop Towards a SharedTask for Multiword Expressions, pages 54?57, Mar-rakech, Morocco.Pavel Pecina.
2010.
Lexical association measuresand collocation extraction.
Language Resources andEvaluation, 44(1-2):137?158.Carlos Ramisch, Helena de Medeiros Caseli, AlineVillavicencio, Andr?e Machado, and Maria Jos?e Fi-natto.
2010a.
A Hybrid Approach for MultiwordExpression Identification.
In Proceedings of the 9thInternational Conference on Computational Pro-cessing of Portuguese Language (PROPOR), pages65?74.Carlos Ramisch, Aline Villavicencio, and ChristianBoitet.
2010b.
Multiword Expressions in the wild?The mwetoolkit comes in handy.
In COLING, Inter-national Conference on Computational Linguistics(Demos), pages 57?60.Carlos Ramisch, Vitor De Araujo, and Aline Villavi-cencio.
2012.
A Broad Evaluation of Techniques forAutomatic Acquisition of Multiword Expressions.In Proceedings of ACL 2012 Student ResearchWork-shop, pages 1?6, Jeju Island, Korea.
Association forComputational Linguistics.Carlos Ramisch.
2012.
Une plate-forme g?en?erique etouverte pour l?acquisition des expressions polylexi-cales.
In Actes de la 14e Rencontres des?EtudiantsChercheurs en Informatique pour le Traitement Au-tomatique des Langues, pages 137?149, Grenoble,France.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, GreatBritain.Max Silberztein and L.A.D.L.
1990.
Le dictionnaire?electronique des mots compos?es.
Langue franc?aise,87(1):71?83.Yulia Tsvetkov and Shuly Wintner.
2011.
Identifica-tion of Multi-word Expressions by Combining Mul-tiple Linguistic Information Sources.
In Empiri-cal Methods in Natural Language Processing, pages836?845.Aline Villavicencio.
2005.
The availability ofverb?particle constructions in lexical resources:How much is enough?
Computer Speech & Lan-guage, 19(4):415?432.Leonardo Zilio, Luiz Svoboda, Luiz Henrique LonghiRossi, and Rafael Martins Feitosa.
2011.
Automaticextraction and evaluation of MWE.
In 8th BrazilianSymposium in Information and Human LanguageTechnology, pages 214?218, Cuiab?a, Brazil.76
