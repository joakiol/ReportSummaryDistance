Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 21?29,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsClustering dictionary definitions using Amazon Mechanical TurkGabriel Parent Maxine EskenaziLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue15213 Pittsburgh, USA{gparent,max}@cs.cmu.eduAbstractVocabulary tutors need word sense disambig-uation (WSD) in order to provide exercisesand assessments that match the sense of wordsbeing taught.
Using expert annotators to builda WSD training set for all the words supportedwould be too expensive.
Crowdsourcing thattask seems to be a good solution.
However, afirst required step is to define what the possi-ble sense labels to assign to word occurrenceare.
This can be viewed as a clustering taskon dictionary definitions.
This paper evaluatesthe possibility of using Amazon MechanicalTurk (MTurk) to carry out that prerequisitestep to WSD.
We propose two different ap-proaches to using a crowd to accomplish clus-tering: one where the worker has a globalview of the task, and one where only a localview is available.
We discuss how we canaggregate multiple workers?
clusters together,as well as pros and cons of our two approach-es.
We show that either approach has an inte-rannotator agreement with experts thatcorresponds to the agreement between ex-perts, and so using MTurk to cluster dictio-nary definitions appears to be a reliableapproach.1 IntroductionFor some applications it is useful to disambiguatethe meanings of a polysemous word.
For example,if we show a student a text containing a word like?bank?
and then automatically generate questionsabout the meaning of that word as it appeared inthe text (say as the bank of a river), we would liketo have the meaning of the word in the questionsmatch the text meaning.
Teachers do this each timethey assess a student on vocabulary knowledge.For intelligent tutoring systems, two options areavailable.
The first one is to ask a teacher to gothrough all the material and label each appearanceof a polysemous word with its sense.
This optionis used only if there is a relatively small quantity ofmaterial.
Beyond that, automatic processing,known as Word Sense Disambiguation (WSD) isessential.
Most approaches are supervised and needlarge amounts of data to train the classifier for eachand every word that is to be taught and assessed.Amazon Mechanical Turk (MTurk) has beenused for the purpose of word sense disambiguation(Snow et al 2008).
The results show that non-experts do very well (100% accuracy) when askedto identify the correct sense of a word out of a fi-nite set of labels created by an expert.
It is there-fore possible to use MTurk to build a trainingcorpus for WSD.
In order to extend the Snow et alcrowdsourced disambiguation to a large number ofwords, we need an efficient way to create the set ofsenses of a word.
Asking an expert to do this iscostly in time and money.
Thus it is necessary tohave an efficient Word Sense Induction (WSI) sys-tem.
A WSI system induces the different senses ofa word and provides the corresponding sense la-bels.
This is the first step to crowdsourcing WSDon a large scale.While many studies have shown that MTurkcan be used for labeling tasks (Snow et al 2008),to rate automatically constructed artifacts(Callison-Burch, 2009, Alonso et al 2008) and totranscribe speech (Ledlie et al 2009, Gruenstein etal, 2009), to our knowledge, there has not beenmuch work on evaluating the use of MTurk for21clustering tasks.
The goal of this paper is to inves-tigate different options available to crowdsource aclustering task and evaluate their efficiency in theconcrete application of word sense induction.2 Background2.1 WSD for vocabulary tutoringOur interest in the use of MTurk for disambigua-tion comes from work on a vocabulary tutor;REAP (Heilman et al 2006).
The tutor searches fordocuments from the Web that are appropriate for astudent to use to learn vocabulary from context(appropriate reading level, for example).
Since thesystem finds a large number of documents, makinga rich repository of learning material, it is impossi-ble to process all the documents manually.
When adocument for vocabulary learning is presented to astudent, the system should show the definition ofthe words to be learned (focus words).
In somecases a word has several meanings for the samepart of speech and thus it has several definitions.Hence the need for WSD to be included in vocabu-lary tutors.2.2 WSI and WSDThe identification of a list of senses for a givenword in a corpus of documents is called wordsense induction (WSI).
SemEval 2007 and 2010(SigLex, 2008) both evaluate WSI systems.
TheI2R system achieved the best results in 2007 withan F-score of 81.6% (I2R by Niu (2007)).
Snow etal (2007) have a good description of the inherentproblem of WSI where the appropriate granularityof the clusters varies for each application.
They tryto solve this problem by building hierarchical-likeword sense structures.
In our case, each dictionarydefinition for a word could be considered as aunique sense for that word.
Then, when usingMTurk as a platform for WSD, we could simplyask the workers to select which of the dictionarydefinitions best expresses the meaning of thewords in a document.
The problem here is thatmost dictionaries give quite several definitions fora word.
Defining one sense label per dictionarydefinition would result in too many labels, whichwould, in turn, make the MTurk WSD less effi-cient and our dataset sparser, thus decreasing thequality of the classifier.
Another option, investi-gated by Chklovski and Mihalcea (2003), is to useWordNet sense definitions as the possible labels.They obtained more than 100,000 labeled instancesfrom a crowd of volunteers.
They conclude thatWordNet senses are not coarse enough to providehigh interannotator agreement, and exploit workersdisagreement on the WSD task to derive coarsersenses.The granularity of the senses for each word is aparameter that is dependent on the application.
Inour case, we want to be able to assess a student onthe sense of a word that the student has just beentaught.
Learners have the ability to generalize thecontext in which a word is learned.
For example,if a student learns the meaning of the word ?bark?as the sound of a dog, they can generalize that thiscan also apply to human shouting.
Hence, there isno need for two separate senses here.
However, astudent could not generalize the meaning ?hardcover of a tree?
from that first meaning of ?bark?.This implies that students should be able to distin-guish coarse word senses.
(Kulkarni et al, 2007)have looked at automatic clustering of dictionarydefinitions.
They compared K-Means clusteringwith Spectral Clustering.
Various features wereinvestigated: raw, normalized word overlap withand without stop words.
The best combination re-sults in 74% of the clusters having no misclassifieddefinitions.
If those misclassified definitions endup being used to represent possible sense labels inWSD, wrong labels might decrease the quality ofthe disambiguation stage.
If a student is shown adefinition that does not match the sense of a wordin a particular context, they are likely to build thewrong conceptual link.
Our application requireshigher accuracy than that achieved by automaticapproaches, since students?
learning can be directlyaffected by the error rate.2.3 Clustering with MTurkThe possible interaction between users and cluster-ing algorithms has been explored in the past.Huang and Mitchell (2006) present an example ofhow user feedback can be used to improve cluster-ing results.
In this study, the users were not askedto provide clustering solutions.
Instead, they finetuned the automatically generated solution.With the advent of MTurk, we can use humanjudgment to build clustering solutions.
There aremultiple approaches for combining workforce: pa-rallel with aggregation (Snow et al 2008), iterative22(Little et al 2009) and collaboration betweenworkers (Horton, Turker Talk, 2009).
These strate-gies have been investigated for many applications,most of which are for labeling, a few for cluster-ing.
The Deneme blog presents an experimentwhere website clustering is carried out usingMTurk (Little, Website Clustering, 2009).
Theworkers?
judgments on the similarity between twowebsites are used to build a distance matrix for thedistance between websites.
Jagadeesan and others(2009) asked workers to identify similar objects ina pool of 3D CAD models.
They then used fre-quently co-occurring objects to build a distancematrix, upon which they then applied hierarchicalclustering.
Those two approaches are different: thefirst gives the worker only two items of the set (alocal view of the task), while the latter offers theworker a global view of the task.
In the next sec-tions we will measure the accuracy of these ap-proaches and their advantages and disadvantages.3 Obtaining clusters from a crowdREAP is used to teach English vocabulary and toconduct learning studies in a real setting, in a localESL school.
The vocabulary tutor provides instruc-tions for the 270 words on the school?s core voca-bulary list, which has been built using theAcademic Word List (Coxhead, 2000).
In order toinvestigate how WSI could be accomplished usingAmazon Mechanical Turk, 50 words were random-ly sampled from the 270, and their definitions wereextracted from the Longman Dictionary of Con-temporary English (LDOCE) and the CambridgeAdvanced Learner's Dictionary (CALD).
Therewas an average of 6.3 definitions per word.The problem of clustering dictionary definitionsinvolves solving two sub-problems: how manyclusters there are, and which definitions belong towhich clusters.
We could have asked workers tosolve both problems at the same time by havingthem dynamically change the number of clusters inour interface.
We decided not to do this due to thefact that some words have more than 12 defini-tions.
Since the worker already needs to keep trackof the semantics of each cluster, we felt that havingthem modify the number of sense boxes wouldincrease their cognitive load to the point that wewould see a decrease in the accuracy of the results.Thus the first task involved determining thenumber of general meanings (which in our casedetermines the number of clusters) that there are ina list of definitions.
The workers were shown theword and a list of its definitions, for example, forthe word ?clarify?
:to make something clearer and easier tounderstandto make something clear or easier to under-stand by giving more details or a simpler explana-tionto remove water and unwanted substancesfrom fat, such as butter, by heating itThey were then asked: ?How many generalmeanings of the word clarify are there in the fol-lowing definitions??
We gave a definition of whatwe meant by general versus specific meanings,along with several examples.
The worker wasasked to enter a number in a text box (in the aboveexample the majority answered 2).
This 2-centHIT was completed 13 times for every 50 words,for a total of 650 assignments and $13.00.
A ma-jority vote was used to aggregate the workers?
re-sults, giving us the number of clusters in which thedefinitions were grouped.
In case of a tie, the low-est number of clusters was retained, since our ap-plication requires coarse-grained senses.The number of ?general meanings?
we obtainedin this first HIT1 was then used in two differentHITs.
We use these two HITs to determine whichdefinitions should be clustered together.
In the firstsetup, which we called ?global-view?
the workershad a view of the entire task.
They were shown theword and all of its definitions.
They were thenprompted to drag-and-drop the definitions into dif-ferent sense boxes, making sure to group the defi-nitions that belong to the same general meaningtogether (Figure 3, Appendix).
Once again, an ex-plicit definition of what was expected for ?generalmeaning?
along with examples was given.
Also, aflash demo of how to use the interface was pro-vided.
The worker got 3 cents for this HIT.
It wascompleted 5 times for each of the 50 words, for atotal cost of $7.50.
We created another HIT wherethe workers were not given all of the definitions;we called this setup ?local-view?.
The worker wasasked to indicate if two definitions of a word wererelated to the same meaning or different meanings1 The code and data used for the different HITs are available athttp://www.cs.cmu.edu/~gparent/amt/wsi/23(Figure 4, Appendix).
For each word, we createdall possible pairs of definitions.
This accounts foran average of 21 pairs for all of the 50 words.
Foreach pair, 5 different workers voted on whether itcontained the same or different meanings, earning1 cent for each answer.
The total cost here was$52.50.
The agreement between workers was usedto build a distance matrix: if the 5 workers agreedthat the two definitions concerned the same sense,the distance was set to 0.
Otherwise, it was set tothe number of workers who thought they con-cerned different senses, up to a distance of 5.
Hie-rarchical clustering was then used to buildclustering solutions from the distance matrices.
Weused complete linkage clustering, with Ward?s cri-terion.4 Evaluation of global-view vs. local-viewapproachesIn order to evaluate our two approaches, wecreated a gold-standard (GS).
Since the task ofWSI is strongly influenced by an annotator?s grainsize preference for the senses, four expert annota-tors were asked to create the GS.
The literatureoffers many metrics to compare two annotators?clustering solutions (Purity and Entropy (Zhao andKarypis, 2001), clustering F-Measure (Fung et al,2003) and many others).
SemEval-2 includes aWSI task where V-Measure (Rosenberg and Hir-schberg, 2007) is used to evaluate the clusteringsolutions.
V-Measure involves two metrics, homo-geneity and completeness, that can be thought of asprecision and recall.
Perfect homogeneity is ob-tained if the solutions have clusters whose datapoints belong to a single cluster in the GS.
Perfectcompleteness is obtained if the clusters in the GScontain data points that belong to a single cluster inthe evaluated solution.
The V-Measure is a(weighted) harmonic mean of the homogeneity andof the completeness metrics.
Table 1 shows inter-annotator agreement (ITA) among four experts onthe test dataset, using the average V-Measure overall the 50 sense clusters.GS #1 GS #2 GS #3 GS #4GS #1 1,000 0,850 0,766 0,770GS #2 0,850 1,000 0,763 0,796GS #3 0,766 0,763 1,000 0,689GS #4 0,770 0,796 0,689 1,000Table 1 - ITA on WSI task for four annotatorsWe can obtain the agreement between one ex-pert and the three others by averaging the three V-Measures.
We finally obtain an ?Experts vs. Ex-perts?
ITA of 0.772 by averaging this value for allof our experts.
The standard deviation for this ITAis 0.031.To be considered reliable, non-expert clus-tering would have to agree with the 4 experts witha similar result.5 Aggregating clustering solutions frommultiple workersUsing a majority vote with the local-view HIT isan easy way of taking advantage of the ?wisdom ofcrowd?
principle.
In order to address clusteringfrom a local-view perspective, we need to build allpossible pairs of elements.
The number of thosepairs is O(n2) on the number of elements to cluster.Thus the cost grows quickly for large clusteringproblems.
For 100 elements to cluster there are4950 pairs of elements to show to workers.
Forlarge problems, a better approach would be to givethe problem to multiple workers through global-view, and then find a way to merge all of the clus-tering solutions to benefit from the wisdom ofcrowd.
Consensus clustering (Topchy et al 2005)has emerged as a way of combining multiple weakclusterings into a better one.
The cluster-based si-milarity partitioning algorithm (CSPA) (Strehl andGhosh, 2002) uses the idea that elements that arefrequently clustered together have high similarity.With MTurk, this involves asking multiple workersto provide full clusterings, and then, for each pairof elements, counting the number of times they co-occur in the same clusters.
This count is used as asimilarity measure between elements, which thenis used to build a distance matrix.
We can then useit to recluster elements.
The results from this tech-nique on our word sense induction problem areshown in the next section.24Another possibility is to determine which clus-tering solution is the centroid of the set of cluster-ings obtained from the worker.
Finding centroidclustering (Hu and Sung, 2006) requires a be-tween-cluster distance metric.
We decided to usethe entropy-based V-Measure for this purpose.
Forevery pair of workers?
solutions, we obtain theirrelative distance by calculating1-VMeasure(cluster #1,cluster #2).Then, for each candidate?s clusters, we average thedistance with every other candidate?s.
The candi-date with the lowest average distance, the centroid,is picked as the ?crowd solution?.
Results from thistechnique are also shown in the next section.6 ResultsFor the first HIT the goal was to determine thenumber of distinct senses in a list of definitions.The Pearson correlation between the four annota-tors on the number of clusters they used for the 50words was computed.
These correlations can beviewed as how much the different annotators hadthe same idea of the grain size to be used to definesenses.
While experts 1, 2 and 4 seem to agree ongrain size (correlation between 0.71 and 0.75), ex-pert 3 had a different opinion.
Correlations be-tween that expert and the three others are between0.53 and 0.58.
The average correlation betweenexperts is 0.63.
On the other hand, the crowd solu-tion does not agree as well with experts #1,#2 and#4 (Pearson correlation of 0.64, 0.68, 0.66), whileit better approaches expert 3, with a correlation of0.68.
The average correlation between the non-expert solution and the experts?
solutions is 0.67.Another way to analyze the agreement on grainsize of the word sense between annotators is tosum the absolute difference of number of clustersfor the 50 words (Table 3).
In this way, we canspecifically examine the results for the four anno-tators and for the non-expert crowd (N-E) solution,averaging that difference for each annotator versusall of the others (including the N-E solution).To determine how a clustering solution com-pared to our GS, we computed the V-Measure forall 50 words between the solution and each GS.By averaging the score on the four GSs, we get anaveraged ITA score between the clustering solutionand the experts.
For the sake of comparison, wefirst computed the score of a random solution,where definitions are randomly assigned to anyone cluster.
We also implemented K-means clus-tering using normalized word-overlap (Kulkarni etal., 2007), which has the best score on their test set.The resulting averaged ITA of our local-viewapproaches that of all 4 experts.
We did the samewith the global-view after applying CSPA and ourcentroid identification algorithm to the 5 clusteringsolutions the workers submitted.
Table 2 shows theagreement between each expert and those ap-proaches, as well as the averaged ITA.For the local-view and global-view ?centroid?,we looked at how the crowd size would affect theaccuracy.
We first computed the averaged ITA byconsidering the answers from the first worker.Then, step by step, we added the answers from thesecond, third, fourth and fifth workers, each timecomputing the averaged ITA.
Figure 1 shows theITA as a function of the workers.Random K-Means localglobalCSPAglobalcentroidGS #1 0,387 0,586 0,737 0,741 0,741GS #2 0,415 0,613 0,765 0,777 0,777GS #3 0,385 0,609 0,794 0,805 0,809GS #4 0,399 0,606 0,768 0,776 0,776Avg.
ITA 0.396 ?
0.014 0.603 ?
0.012 0.766 ?
0.023 0.775 ?
0.026 0.776 ?
0.028Table 2 - Interannotator agreement for our different approaches (bold numbers are within one standarddeviation of the Expert vs.
Expert ITA of 0.772 ?
0.031 described in section 4)GS #1 GS #2 GS #3 GS #4 N-EGS #1 0 24 26 29 26GS #2 24 0 30 27 26GS #3 26 30 0 37 20GS #4 29 27 37 0 27N-E 26 26 20 27 0Average 26.25 26.75 28.25 30 24.75Table 3 - Absolute difference of number of clustersbetween annotators257 DiscussionSince our two approaches are based on the result ofthe first HIT, which determines the number ofclusters, the accuracy of that first task is extremelyimportant.
It turns out that the correlation betweenthe crowd solution and the experts (0.67) is actual-ly higher than the average correlation between ex-perts (0.63).
One way to explain this is that of the 4experts, 3 had a similar opinion on what the grainsize should be, while the other one had a differentopinion.
The crowd picked a grain size that wasactually between those two opinions, thus resultingin a higher correlation.
This hypothesis is also sup-ported by Table 3.
The average difference in thenumber of clusters is lower for the N-E solutionthan for any expert solution.
The crowd of 13 wasable to come up with a grain size that could beseen as a good consensus of the four annotators?grain size.
This allows us to believe that using thecrowd to determine the number of clusters for ourtwo approaches is a reliable technique.As expected, Table 3 indicates that our two set-ups behave better than randomly assigning defini-tions to clusters.
This is a good indication that theworkers did not complete our tasks randomly.
Theautomatic approach (K-Means) clearly behavesbetter than the random baseline.
However, theclusters obtained with this approach agree less withthe experts than any of our crowdsourced ap-proaches.
This confirms the intuition that humansare better at distinguishing word senses than anautomatic approach like K-Means.Our first hypothesis was that global-view wouldgive us the best results: since the worker complet-ing a global-view HIT has an overall view of thetask, they should be able to provide a better solu-tion.
The results indicate that the local-view andglobal-view approaches give similar results interms of ITA.
Both of those approaches have clos-er agreement with the experts, than the expertshave with each other (all ITAs are around 77%).Here is an example of a solution that the crowdprovided through local-view for the verb ?tape?with the definitions;A.
To record something on tapeB.
To use strips of sticky material, especially to fixtwo things together or to fasten a parcelC.
To record sound or picture onto a tapeD.
To tie a bandage firmly around an injured part ofsomeone?s body, strapE.
To fasten a package, box etc with tapeThe crowd created two clusters: one by group-ing A and C to create a ?record audio/video?
sense,and another one by grouping B,D and E to create a?fasten?
sense.
This solution was also chosen bytwo of the four experts.
One of the other expertsgrouped definitions E with A and C, which isclearly an error since there is no shared meaning.The last expert created three clusters, by assigningD to a different cluster than B and E. This decisioncan be considered valid since there is a small se-mantic distinction between D and B/E from thefact that D is ?fasten?
for the specific case of in-jured body parts.
However, a student could gene-ralize D from B and E. So that expert?s grain sizedoes not correspond to our specifications.We investigated two different aggregation tech-niques for clustering solutions, CSPA and centroididentification.
In this application, both techniquesgive very similar results with only 2 clusters out of50 words differing between the two techniques.Centroid identification is easier to implement, anddoesn?t require reclustering the elements.
Figure 1shows the impact of adding more workers to thecrowd.
While it seems advantageous to use 3workers?
opinions rather than only 1, (gain of0.04), adding a fourth and fifth worker does notimprove the average ITA.Local-view is more tolerant to errors than glob-al-view.
If a chaotic worker randomly answers onepair of elements, the entire final clustering will notbe affected.
If a chaotic (or cheating) worker an-swers randomly in global-view, the entire cluster-ing solution will be random.
Thus, while a policyof using only one worker?s answer for a local-viewFigure 1 - Impact of the crowd size on the ITA of thelocal and global approaches26HIT could be adopted, the same policy might resultin poor clustering if used for the global-view HIT.However, global-view has the advantage overlocal-view of being cheaper.
Figure 2 shows thedistribution of the number of definitions extractedfrom both LDOCE and CALD per word (starting atword with more than 6 definitions).
Since the lo-cal-view cost increases in a quadratic manner asthe number of elements to cluster increases itwould cost more than $275,000 to group the defi-nitions of 30,000 words coming from the two dic-tionaries (using the parameters described in 3).
Itwould be possible to modify it to only ask workersfor the similarity of a subset of pairs of elementsand then reconstruct the incomplete distance ma-trix (Hathaway and Bezdek, 2002).
A better optionfor clustering a very large amount of elements is touse global-view.
For the same 30,000 words above,the cost of grouping definitions using this tech-nique would be around $4,500.
This would implythat worker would have to create clusters from setof over 22 definitions.
Keeping the cost constantwhile increasing the number of elements to clustermight decrease the workers?
motivation.
Thus scal-ing up a global-view HIT requires increasing thereward.
It also requires vigilance on how muchcognitive load the workers have to handle.
Cogni-tive load can be seen as a function of the numberof elements to cluster and of the number of clustersthat a new element can be assigned to.
If a workeronly has to decide if an element should be in A orB, the cognitive load is low.
But if the worker hasto decide among many more classes, the cognitiveload may increase to a point where the worker ishampered from providing a correct answer.8 ConclusionWe evaluated two different approaches for crowd-sourcing dictionary definition clustering as ameans of achieving WSI.
Global-view provides aninterface to the worker where all the elements tobe clustered are displayed, while local-view dis-plays only two elements at a time and prompts theworker for their similarity.
Both approaches showas much agreement with experts as the experts dowith one another.
Applying either CSPA or centro-id identification allows the solution to benefit fromthe wisdom of crowd effect, and shows similarresults.
While global-view is cheaper than local-view, it is also strongly affected by worker error,and sensitive to the effect of increased cognitiveload.It appears that the task of clustering definitionsto form word senses is a subjective one, due to dif-ferent ideas of what the grain size of the sensesshould be.
Thus, even though it seems that our twoapproaches provide results that are as good asthose of an expert, it would be interesting to trycrowdsourced clustering on a clustering problemwhere an objective ground truth exists.
For exam-ple, we could take several audio recordings fromeach of several different persons.
After mixing upthe recordings from the different speakers, wecould ask workers to clusters all the recordingsfrom the same person.
This would provide an evenstronger evaluation of local-view against global-view since we could compare them to the true so-lution, the real identity of the speaker.There are several interesting modifications thatcould also be attempted.
The local-view task couldask for similarity on a scale of 1 to 5, instead of abinary choice of same/different meaning.
Also,since using global-view with one large problemcauses high cognitive load, we could partition abigger problem, e.g., with 30 definitions, into 3problems including 10 definitions.
Using the sameinterface as global-view, the workers could clusterthe sub-problems.
We could then use CSPA tomerge local clusters into a final cluster with the 30definitions.In this paper we have examined clustering wordsense definitions.
Two approaches were studied,and their advantages and disadvantages were de-scribed.
We have shown that the use of humancomputation for WSI, with an appropriate crowdFigure 2- Distribution of the number of definitions02505007501000NumberofwordsNumber of definitions27size and mean of aggregation, is as reliable as us-ing expert judgments.AcknowledgementsFunding for this research is provided by the Na-tional Science Foundation, Grant Number SBE-0836012 to the Pittsburgh Science of LearningCenter (PSLC, http://www.learnlab.org).ReferencesAlonso, O., Rose, D. E., & Stewart, B.
(2008).
Crowd-sourcing for relevance evaluation.
ACM SIGIR Fo-rum , 42 (2), pp.
9-15.Callison-Burch, C. (2009).
Fast, Cheap, and Creative:Evaluating Translation Quality Using Amazon?s Me-chanical Turk.
Proceedings of EMNLP 2009.Coxhead, A.
(2000).
A new academic word list.
TESOLquarterly , 34 (2), 213-238.Chklovski, T. & Mihalcea, R. (2003).
Exploitingagreement and disagreement of human annotators forword sense disambiguation.
Proceedings of RANLP2003.Fung, B. C., Wang, K., & Ester, M. (2003).
Hierarchicaldocument clustering using frequent itemsets.
Proc.
ofthe SIAM International Conference on Data Mining.Gruenstein, A., McGraw, I., & Sutherland, A.
(2009).
"A self-transcribing speech corpus: collecting conti-nuous speech with an online educational game".SLaTE Workshop.Hathaway, R. J., & Bezdek, J. C. (2001).
Fuzzy c-meansclustering of incomplete data.
IEEE Transactions onSystems, Man, and Cybernetics, Part B , 31 (5), 735-744.Heilman, M. Collins-Thompson, K., Callan, J.
& Eske-nazi M. (2006).
Classroom success of an IntelligentTutoring System for lexical practice and readingcomprehension.
Proceedings of the Ninth Interna-tional Conference on Spoken Language.Horton, J.
(2009, 12 11).
Turker Talk.
Retrieved 012010, from Deneme:http://groups.csail.mit.edu/uid/deneme/?p=436Hu, T., & Sung, S. Y.
(2006).
Finding centroid cluster-ings with entropy-based criteria.
Knowledge and In-formation Systems , 10 (4), 505-514.Huang, Y., & Mitchell, T. M. (2006).
Text clusteringwith extended user feedback.
Proceedings of the 29thannual international ACM SIGIR conference on Re-search and development in information retrieval (p.420).
ACM.Jagadeesan, A., Lynn, A., Corney, J., Yan, X., Wenzel,J., Sherlock, A., et al (2009).
Geometric reasoningvia internet CrowdSourcing.
2009 SIAM/ACM JointConference on Geometric and Physical Modeling(pp.
313-318).
ACM.Kulkarni, A., Callan, J., & Eskenazi, M. (2007).
Dictio-nary Definitions: The Likes and the Unlikes.
Pro-ceedings of the SLaTE Workshop on Speech andLanguage Technology in Education.
Farmington, PA,USA.Ledlie, J., Odero, B., Minkow, E., Kiss, I., & Polifroni,J.
(2009).
Crowd Translator: On Building LocalizedSpeech Recognizers through Micropayments.
NokiaResearch Center.Little, G. (2009, 08 22).
Website Clustering.
Retrieved01 2010, from Deneme:http://groups.csail.mit.edu/uid/deneme/?p=244Little, G., Chilton, L. B., Goldman, M., & Miller, R. C.(2009).
TurKit: tools for iterative tasks on mechani-cal Turk.
Proceedings of the ACM SIGKDD Work-shop on Human Computation (pp.
29-30).
ACM.Niu, Z.-Y., Dong-Hong, J., & Chew-Lim, T. (2007).
I2r:Three systems for word sense discrimination, chineseword sense disambiguation, and english word sensedisambiguation.
Proceedings of the Fourth Interna-tional Workshop on Semantic Evaluations (SemEval-2007) (pp.
177-182).
Prague, Czech Republic: Asso-ciation for Computational Linguistics.Rosenberg, A., & Hirschberg, J.
(2007).
V-measure: Aconditional entropy-based external cluster evaluationmeasure.
Proceedings of the 2007 Joint ConferenceEMNLP-CoNLL, (pp.
410-420).SigLex, A.
(2008).
Retrieved 01 2010, from SemEval-2,Evaluation Exercises on Semantic Evaluation:http://semeval2.fbk.eu/semeval2.phpSnow, R., O'Connor, B., Jurafsky, D., & Ng, A.
Y.(2008).
Cheap and fast---but is it good?
Evaluatingnon-expert annotations for natural language tasks.Proceedings of the Conference on Empirical Methodsin Natural Language Processing (pp.
254-263).
Asso-ciation for Computational Linguistics.Snow, R., Prakash, S., Jurafsky, D., & Ng, A. Y.
(2007).Learning to Merge Word Senses.
Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL) (pp.
1005-1014).
Prague, Czech Republic: Association forComputational Linguistics.Strehl, A., & Ghosh, J.
(2003).
Cluster ensembles---aknowledge reuse framework for combining multiplepartitions.
The Journal of Machine Learning Re-search , 3, 583-617.Topchy, A., Jain, A. K., & Punch, W. (2005).
Clusteringensembles: Models of consensus and weak partitions.IEEE Transactions on Pattern Analysis and MachineIntelligence , 27 (12), 1866-1881.Zhao, Y., & Karypis, G. (2001).
Criterion functions fordocument clustering: Experiments and analysis.
Re-port TR 01?40, Department of Computer Science,University of Minnesota.28Figure 3: Example of a global-view HIT for the word ?code?
(not all of the instructions are shown)AppendixFigure 4: Example of a local-view HIT for the word ?aid?
(not all of the instructions are shown)29
