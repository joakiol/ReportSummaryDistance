Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 173?181,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsHierarchical Dirichlet Trees for Information RetrievalGholamreza HaffariSchool of Computing SciencesSimon Fraser Universityghaffar1@cs.sfu.caYee Whye TehGatsby Computational NeuroscienceUniversity College Londonywteh@gatsby.ucl.ac.ukAbstractWe propose a principled probabilisitc frame-work which uses trees over the vocabulary tocapture similarities among terms in an infor-mation retrieval setting.
This allows the re-trieval of documents based not just on occur-rences of specific query terms, but also on sim-ilarities between terms (an effect similar toquery expansion).
Additionally our principledgenerative model exhibits an effect similar toinverse document frequency.
We give encour-aging experimental evidence of the superiorityof the hierarchical Dirichlet tree compared tostandard baselines.1 IntroductionInformation retrieval (IR) is the task of retrieving,given a query, the documents relevant to the userfrom a large quantity of documents (Salton andMcGill, 1983).
IR has become very important inrecent years, with the proliferation of large quanti-ties of documents on the world wide web.
Many IRsystems are based on some relevance score functionR(j, q) which returns the relevance of document j toquery q.
Examples of such relevance score functionsinclude term frequency-inverse document frequency(tf-idf) and Okapi BM25 (Robertson et al, 1992).Besides the effect that documents containingmore query terms should be more relevant (term fre-quency), the main effect that many relevance scorestry to capture is that of inverse document frequency:the importance of a term is inversely related to thenumber of documents that it appears in, i.e.
thepopularity of the term.
This is because popularterms, e.g.
common and stop words, are often un-informative, while rare terms are often very infor-mative.
Another important effect is that related orco-occurring terms are often useful in determiningthe relevance of documents.
Because most relevancescores do not capture this effect, IR systems resort totechniques like query expansion which includes syn-onyms and other morphological forms of the origi-nal query terms in order to improve retrieval results;e.g.
(Riezler et al, 2007; Metzler and Croft, 2007).In this paper we explore a probabilistic model forIR that simultaneously handles both effects in a prin-cipled manner.
It builds upon the work of (Cow-ans, 2004) who proposed a hierarchical Dirichletdocument model.
In this model, each document ismodeled using a multinomial distribution (makingthe bag-of-words assumption) whose parameters aregiven Dirichlet priors.
The common mean of theDirichlet priors is itself assumed random and givena Dirichlet hyperprior.
(Cowans, 2004) showed thatthe shared mean parameter induces sharing of infor-mation across documents in the corpus, and leads toan inverse document frequency effect.We generalize the model of (Cowans, 2004) by re-placing the Dirichlet distributions with Dirichlet treedistributions (Minka, 2003), thus we call our modelthe hierarchical Dirichlet tree.
Related terms areplaced close by in the vocabulary tree, allowing themodel to take this knowledge into account when de-termining document relevance.
This makes it unnec-essary to use ad-hoc query expansion methods, as re-lated words such as synonyms will be taken into ac-count by the retrieval rule.
The structure of the treeis learned from data in an unsupervised fashion, us-173ing a variety of agglomerative clustering techniques.We review the hierarchical Dirichlet document(HDD) model in section 2, and present our proposedhierarchical Dirichlet tree (HDT) document modelin section 3.
We describe three algorithms for con-structing the vocabulary tree in section 4, and giveencouraging experimental evidence of the superi-ority of the hierarchical Dirichlet tree compared tostandard baselines in section 5.
We conclude the pa-per in section 6.2 Hierarchical Dirichlet Document ModelThe probabilistic approach to IR assumes that eachdocument in a collection can be modeled probabilis-tically.
Given a query q, it is further assumed thatrelevant documents j are those with highest gener-ative probability p(q|j) for the query.
Thus given qthe relevance score is R(j, q) = p(q|j) and the doc-uments with highest relevance are returned.Assume that each document is a bag of words,with document j modeled as a multinomial distri-bution over the words in j.
Let V be the terms inthe vocabulary, njw be the number of occurrencesof term w ?
V in document j, and ?flatjw be the proba-bility of w occurring in document j (the superscript?flat?
denotes a flat Dirichlet as opposed to our pro-posed Dirichlet tree).
(Cowans, 2004) assumes thefollowing hierarchical Bayesian model for the docu-ment collection:?flat0 = (?flat0w)w?V ?
Dirichlet(?u) (1)?flatj = (?flatjw)w?V ?
Dirichlet(?
?flat0 )nj = (njw)w?V ?
Multinomial(?flatj )In the above, bold face a = (aw)w?V means that ais a vector with |V | entries indexed by w ?
V , andu is a uniform distribution over V .
The generativeprocess is as follows (Figure 1(a)).
First a vector?flat0 is drawn from a symmetric Dirichlet distributionwith concentration parameter ?.
Then we draw theparameters ?flatj for each document j from a commonDirichlet distribution with mean ?flat0 and concentra-tion parameter ?.
Finally, the term frequencies ofthe document are drawn from a multinomial distri-bution with parameters ?flatj .The insight of (Cowans, 2004) is that becausethe common mean parameter ?flat0 is random, it in-duces dependencies across the document models inunjwnjJ?flatj?flat0 ??
?k0?kjJuk?k?k(a) (b)?flatkbFigure 1: (a) The graphical model representation of thehierarchical Dirichlet document model.
(b) The globaltree and local trees in hierarchical Dirichlet tree docu-ment model.
Triangles stand for trees with the samestructure, but different parameters at each node.
The gen-eration of words in each document is not shown.the collection, and this in turn is the mechanism forinformation sharing among documents.
(Cowans,2004) proposed a good estimate of ?flat0 :?flat0w = ?/|V |+ n0w?
+?w?V n0w(2)where n0w is simply the number of documents con-taining term w, i.e.
the document frequency.
Inte-grating out the document parameters ?flatj , we see thatthe probability of query q being generated from doc-ument j is:p(q|j) =?x?q?
?flat0x + njx?
+?w?V njw(3)= Const ?
?x?qConst + njx?/|V |+n0x?
+?w?V njwWhere Const are terms not depending on j.
We seethat njx is term frequency, its denominator ?/|V |+n0x is an inverse document frequency factor, and?
+ ?w?V njw normalizes for document length.The inverse document frequency factor is directlyrelated to the shared mean parameter, in that popularterms x will have high ?flat0x value, causing all docu-ments to assign higher probability to x, and downweighting the term frequency.
This effect will beinherited by our model in the next section.1743 Hierarchical Dirichlet TreesApart from the constraint that the parameters shouldsum to one, the Dirichlet priors in the HDD modeldo not impose any dependency among the param-eters of the resulting multinomial.
In other words,the document models cannot capture the notion thatrelated terms tend to co-occur together.
For exam-ple, this model cannot incorporate the knowledgethat if the word ?computer?
is seen in a document, itis likely to observe the word ?software?
in the samedocument.
We relax the independence assump-tion of the Dirichlet distribution by using Dirichlettree distributions (Minka, 2003), which can capturesome dependencies among the resulting parameters.This allows relationships among terms to be mod-eled, and we will see that it improves retrieval per-formance.3.1 ModelLet us assume that we have a tree over the vocab-ulary whose leaves correspond to vocabulary terms.Each internal node k of the tree has a multinomialdistribution over its children C(k).
Words are drawnby starting at the root of the tree, recursively pickinga child l ?
C(k) whenever we are in an internal nodek, until we reach a leaf of the tree which correspondsto a vocabulary term (see Figure 2(b)).
The Dirich-let tree distribution is the product of Dirichlet dis-tributions placed over the child probabilities of eachinternal node, and serves as a (dependent) prior overthe parameters of multinomial distributions over thevocabulary (the leaves).Our model generalizes the HDD model by replac-ing the Dirichlet distributions in (1) by Dirichlet treedistributions.
At each internal node k, define a hier-archical Dirichlet prior over the choice of the chil-dren:?0k = (?0l)l?C(k) ?
Dirichlet(?kuk) (4)?jk = (?jl)l?C(k) ?
Dirichlet(?k?0k)where uk is a uniform distribution over the childrenof node k, and each internal node has its own hy-perparameters ?k and ?k.
?jl is the probability ofchoosing child l if we are at internal node k. If thetree is degenerate with just one internal node (theroot) and all leaves are direct children of the root werecover the ?flat?
HDD model in the previous sec-tion.
We call our model the hierarchical Dirichlettree (HDT).3.2 Inference and LearningGiven a term, the path from the root to the corre-sponding leaf is unique.
Thus given the term fre-quencies nj of document j as defined in (1), thenumber of times njl child l ?
C(k) was picked atnode k is known and fixed.
The probability of allwords in document j, given the parameters, is thena product of multinomials probabilities over internalnodes k:p(nj |{?jk}) =?knjk!Ql?C(k) njl!
?l?C(k)?njljl (5)The probability of the documents, integrating out the?jk?s, is:p({nj}|{?0k}) = (6)?j?knjk !Ql?C(k) njl!?(?k)?(?k+njk)?l?C(k)?(?k?0l+njl)?
(?k?0l)The probability of a query q under document j, i.e.the relevance score, follows from (3):p(q|j) =?x?q?
(kl)?k?0l+njl?k+njk (7)where the second product is over pairs (kl) where kis a parent of l on the path from the root to x.The hierarchical Dirichlet tree model we pro-posed has a large number of parameters and hy-perparameters (even after integrating out the ?jk?s),since the vocabulary trees we will consider later typ-ically have large numbers of internal nodes.
Thisover flexibility might lead to overfitting or to param-eter regimes that do not aid in the actual task of IR.To avoid both issues, we constrain the hierarchicalDirichlet tree to be centered over the flat hierarchi-cal Dirichlet document model, and allow it to learnonly the ?k hyperparameters, integrating out the ?jkparameters.We set {?0k}, the hyperparameters of the globaltree, so that it induces the same distribution over vo-cabulary terms as ?flat0 :?0l = ?flat0l ?0k =?l?C(k)?0l (8)175The hyperparameters of the local trees ?k?s are es-timated using maximum a posteriori learning withlikelihood given by (6), and a gamma prior withinformative parameters.
The density function of aGamma(a, b) distribution isg(x; a, b) = xa?1bae?bx?
(a)where the mode happens at x = a?1b .
We set themode of the prior such that the hierarchical Dirichlettree reduces to the hierarchical Dirichlet documentmodel at these values:?flatl = ?
?flat0l ?flatk =?l?C(k)?flatl (9)?k ?
Gamma(b?flatk + 1, b)and b > 0 is an inverse scale hyperparameter to betuned, with large values giving a sharp peak around?flatk .
We tried a few values1 of b and have found thatthe results we report in the next section are not sen-sitive to b.
This prior is constructed such that if thereis insufficient information in (6) the MAP value willsimply default back to the hierarchical Dirichlet doc-ument model.We used LBFGS2 which is a gradient based opti-mization method to find the MAP values, where thegradient of the likelihood part of the objective func-tion (6) is:?
log p({nj}|{?0j})??k=?j?(?k)??
(?k + njk)+ ?l?C(k)?0l(?
(?k?0l + njl)??
(?k?0l))where ?
(x) := ?
log ?
(x)/?x is the digamma func-tion.
Because each ?k can be optimized separately,the optimization is very fast (approximately 15-30minutes in the experiments to follow on a Linux ma-chine with 1.8 GH CPU speed).4 Vocabulary Tree Structure LearningThe structure of the vocabulary tree plays an impor-tant role in the quality of the HDT document model,1Of the form 10i for i ?
{?2,?1, 0, 1}.2We used a C++ re-implementation of Jorge Nocedal?sLBFGS library (Nocedal, 1980) from the ALGLIB website:http://www.alglib.net.Algorithm 1 Greedy Agglomerative Clustering1: Place m words into m singleton clusters2: repeat3: Merge the two clusters with highest similarity, re-sulting in one less cluster4: If there still are unincluded words, pick one andplace it in a singleton cluster, resulting in one morecluster5: until all words have been included and there is onlyone cluster leftsince it encapsulates the similarities among wordscaptured by the model.
In this paper we exploredusing trees learned in an unsupervised fashion fromthe training corpus.The three methods are all agglomerative cluster-ing algorithms (Duda et al, 2000) with differentsimilarity functions.
Initially each vocabulary wordis placed in its own cluster; each iteration of the al-gorithm finds the pair of clusters with highest sim-ilarity and merges them, continuing until only onecluster is left.
The sequence of merges determines abinary tree with vocabulary words as its leaves.Using a heap data structure, this basic agglom-erative clustering algorithm requires O(n2 log(n) +sn2) computations where n is the size of the vocab-ulary and s is the amount of computation needed tocompute the similarity between two clusters.
Typi-cally the vocabulary size n is large; to speed up thealgorithm, we use a greedy version described in Al-gorithm 1 which restricts the number of cluster can-didates to at most m ?
n. This greedy version isfaster with complexity O(nm(logm + s)).
In theexperiments we used m = 500.Distributional clustering (Dcluster) (Pereira etal., 1993) measures similarity among words in termsof the similarity among their local contexts.
Eachword is represented by the frequencies of variouswords in a window around each occurrence of theword.
The similarity between two words is com-puted to be a symmetrized KL divergence betweenthe distributions over neighboring words associatedwith the two words.
For a cluster of words the neigh-boring words are the union of those associated witheach word in the cluster.
Dcluster has been usedextensively in text classification (Baker and McCal-lum, 1998).Probabilistic hierarchical clustering (Pcluster)176(Friedman, 2003).
Dcluster associates each wordwith its local context, as a result it captures bothsemantic and syntactic relationships among words.Pcluster captures more relevant semantic relation-ships by instead associating each word with the doc-uments in which it appears.
Specifically, each wordis associated with a binary vector indexed by doc-uments in the corpus, where a 1 means the wordappears in the corresponding document.
Pclustermodels a cluster of words probabilistically, with thebinary vectors being iid draws from a product ofBernoulli distributions.
The similarity of two clus-ters c1 and c2 of words is P (c1 ?
c2)/P (c1)P (c2),i.e.
two clusters of words are similar if their unioncan be effectively modeled using one cluster, rela-tive to modeling each separately.
Conjugate beta pri-ors are placed over the parameters of the Bernoullidistributions and integrated out so that the similarityscores are comparable.Brown?s algorithm (Bcluster) (Brown et al,1990) was originally proposed to build class-basedlanguage models.
In the 2-gram case, words areclustered such that the class of the previous wordis most predictive of the class of the current word.Thus the similarity between two clusters of wordsis defined to be the resulting mutual information be-tween adjacent classes corrresponding to a sequenceof words.4.1 Operations to Simplify TreesTrees constructed using the agglomerative hierarchi-cal clustering algorithms described in this sectionsuffer from a few drawbacks.
Firstly, because theyare binary trees they have large numbers of internalnodes.
Secondly, many internal nodes are simply notinformative in that the two clusters of words belowa node are indistinguishable.
Thirdly, Pcluster andDcluster tend to produce long chain-like brancheswhich significantly slows down the computation ofthe relevance score.To address these issues, we considered operationsto simplify trees by contracting internal edges of thetree while preserving as much of the word relation-ship information as possible.
Let L be the set of treeleaves and ?
(a) be the distance from node or edge ato the leaves:?
(a) := minl?L#{edges between a and l} (10)abFigure 2: ?
(root) = 2, while ?
(v) = 1 for shaded ver-tices v. Contracting a and b results in both child of bbeing direct children of a while b is removed.In the experiments we considered either contractingedges3 close to the leaves ?
(a) = 1 (thus remov-ing many of the long branches described above), oredges further up the tree ?
(a) ?
2 (preserving theinformative subtrees closer to the leaves while re-moving many internal nodes).
See Figure 2.
(Miller et al, 2004) cut the BCluster tree at a cer-tain depth k to simplify the tree, meaning every leafdescending from a particular internal node at levelk is made an immediate child of that node.
Theyuse the tree to get extra features for a discrimina-tive model to tackle the problem of sparsity?thefeatures obtained from the new tree do not sufferfrom sparsity since each node has several words asits leaves.
This technique did not work well for ourapplication so we will not report results using it inour experiments.5 ExperimentsIn this section we present experimental results ontwo IR datasets: Cranfield and Medline4.
The Cran-field dataset consists of 1,400 documents and 225queries; its vocabulary size after stemming and re-moving stop words is 4,227.
The Medline datasetcontains 1,033 documents and 30 queries with thevocabulary size of 8,800 after stemming and remov-ing stop words.
We compare HDT with the flatHDD model and Okapi BM25 (Robertson et al,1992).
Since one of our motivations has been to3Contracting an edge means removing the edge and the adja-cent child node and connecting the grandchildren to the parent.4Both datasets can be downloaded fromhttp://www.dcs.gla.ac.uk/idom/ir resources/test collections.177Depth Statistics PerformanceTree Cranfield Medline Cranfield Medlineavg / max total avg / max total avg-pr top10-pr avg-pr top10-prBCluster 16.7 / 24 4226 16.4 / 22 8799 0.2675 0.3218 0.2131 0.6433BC contract ?
?
2 6.2 / 16 3711 5.3 / 14 7473 0.2685 0.3147 0.2079 0.6533BC contract ?
= 1 16.1 / 23 3702 15.8 / 22 7672 0.2685 0.3204 0.1975 0.6400DCluster 41.2 / 194 4226 38.1 / 176 8799 0.2552 0.3120 0.1906 0.6300DC contract ?
?
2 2.3 / 8 2469 3.3 / 9 5091 0.2555 0.3156 0.1906 0.6167DC contract ?
= 1 40.9 / 194 3648 38.1 / 176 8799 0.2597 0.3129 0.1848 0.6300PCluster 50.2 / 345 4226 37.1 / 561 8799 0.2613 0.3231 0.1681 0.6633PC contract ?
?
2 35.2 / 318 3741 20.4 / 514 7280 0.2624 0.3213 0.1792 0.6767PC contract ?
= 1 33.6 / 345 2246 34.1 / 561 4209 0.2588 0.3240 0.1880 0.6633flat model 1 / 1 1 1 / 1 1 0.2506 0.3089 0.1381 0.6133BM25 ?
?
?
?
0.2566 0.3124 0.1804 0.6567BM25QueryExp ?
?
?
?
0.2097 0.3191 0.2121 0.7366Table 1: Average precision and Top-10 precision scores of HDT with different trees versus flat model and BM25.
Thestatistics for each tree shows its average/maximum depth of its leaf nodes as well as the number of its total internalnodes.
The bold numbers highlight the best results in the corresponding columns.get away from query expansion, we also compareagainst Okapi BM25 with query expansion.
Thenew terms to expand each query are chosen basedon Robertson-Sparck Jones weights (Robertson andSparck Jones, 1976) from the pseudo relevant docu-ments.
The comparison criteria are (i) top-10 preci-sion, and (ii) average precision.5.1 HDT vs BaselinesAll the hierarchical clustering algorithms mentionedin section 4 are used to generate trees, each of whichis further post-processed by tree simplification op-erators described in section 4.1.
We consider (i)contracting nodes at higher levels of the hierarchy(?
?
2), and (ii) contracting nodes right above theleaves (?
= 1).The statistics of the trees before and after post-processing are shown in Table 1.
Roughly, theDcluster and BCluster trees do not have long chainswith leaves hanging directly off them, which is whytheir average depths are reduced significantly by the?
?
2 simplification, but not by the ?
= 1 sim-plification.
The converse is true for Pcluster: thetrees have many chains with leaves hanging directlyoff them, which is why average depth is not reducedas much as the previous trees based on the ?
?
2simplification.
However the average depth is still re-duced significantly compared to the original trees.Table 1 presents the performance of HDT withdifferent trees against the baselines in terms of thetop-10 and average precision (we have bold facedthe performance values which are the maximumof each column).
HDT with every tree outper-forms significantly the flat model in both datasets.More specifically, HDT with (original) BCluster andPCluster trees significantly outperforms the threebaselines in terms of both performance measure forthe Cranfield.
Similar trends are observed on theMedline except here the baseline Okapi BM25 withquery expansion is pretty strong5, which is still out-performed by HDT with BCluster tree.To further highlight the differences among themethods, we have shown the precision at particularrecall points on Medline dataset in Figure 4 for HDTwith PCluster tree vs the baselines.
As the recallincreases, the precision of the PCluster tree signifi-cantly outperforms the flat model and BM25.
We at-tribute this to the ability of PCluster tree to give highscores to documents which have words relevant to aquery word (an effect similar to query expansion).5.2 AnalysisIt is interesting to contrast the learned ?k?s for eachof the clustering methods.
These ?k?s impose cor-5Note that we tuned the parameters of the baselines BM25with/without query expansion with respect to their performanceon the actual retrieval task, which in a sense makes them appearbetter than they should.17810?1 100 101 102 10310?1100101102103BCluster?0k ?parent(k)?k10?2 10?1 100 101 102 10310?210?1100101102103DCluster?0k ?parent(k)?k10?2 10?1 100 101 102 10310?210?1100101102103PCluster?0k ?parent(k)?kFigure 3: The plots showing the contribution of internal nodes in trees constructed by the three clustering algorithmsfor the Cranfield dataset.
In each plot, a point represent an internal node showing a positive exponent in the node?scontribution (i.e.
positive correlation among its children) if the point is below x = y line.
From left to the right plots,the fraction of nodes below the line is 0.9044, 0.7977, and 0.3344 for a total of 4,226 internal nodes.0 .1 .2 .3 .4 .5 .6 .7 .8 .90.20.30.40.50.60.70.80.91Precision at particular recall pointsPClusterFlat modelBM25BM25 Query ExpansionFigure 4: The precision of all methods at particular recallpoints for the Medline dataset.relations on the probabilities of the children under kin an interesting fashion.
In particular, if we com-pare ?k to ?0k?parent(k), then a larger value of ?kimplies that the probabilities of picking one of thechildren of k (from among all nodes) are positivelycorrelated, while a smaller value of ?k implies neg-ative correlation.
Roughly speaking, this is becausedrawn values of ?jl for l ?
C(k) are more likelyto be closer to uniform (relative to the flat Dirichlet)thus if we had picked one child of k we will likelypick another child of k.Figure 3 shows scatter plots of ?k values ver-sus ?0k?parent(k) for the internal nodes of the trees.Firstly, smaller values for both tend to be associ-ated with lower levels of the trees, while large val-ues are with higher levels of the trees.
Thus wesee that PCluster tend to have subtrees of vocabu-lary terms that are positively correlated with eachother?i.e.
they tend to co-occur in the same docu-ments.
The converse is true of DCluster and BClus-ter because they tend to put words with the samemeaning together, thus to express a particular con-cept it is enough to select one of the words and notto choose the rest.
Figure 5 show some fragmentsof the actual trees including the words they placedtogether and ?k parameters learned by HDT modelfor their internal nodes.
Moreover, visual inspectionof the trees shows that DCluster can easily misplacewords in the tree, which explains its lower perfor-mance compared to the other tree construction meth-ods.Secondly, we observed that for higher nodes ofthe tree (corresponding generally to larger values of?k and ?0k?parent(k)) PCluster ?k?s are smaller, thushigher levels of the tree exhibit negative correlation.This is reasonable, since if the subtrees capture pos-itively correlated words, then higher up the tree thedifferent subtrees correspond to clusters of wordsthat do not co-occur together, i.e.
negatively corre-lated.6 Conclusion and Future WorkWe presented a hierarchical Dirichlet tree model forinformation retrieval which can inject (semantical orsyntactical) word relationships as the domain knowl-edge into a probabilistic model for information re-trieval.
Using trees to capture word relationships,the model is highly efficient while making use ofboth prior information about words and their occur-rence statistics in the corpus.
Furthermore, we inves-tigated the effect of different tree construction algo-rithms on the model performance.On the Cranfield dataset, HDT achieves 26.85%for average-precision and 32.40% for top-10 preci-179Figure 5: Small parts of the trees learned by clustering algorithms for the Cranfield dataset where the learned ?k foreach internal node is written close to it.sion, and outperforms all baselines including BM25which gets 25.66% and 31.24% for these two mea-sures.
On the Medline dataset, HDT is competi-tive with BM25 with Query Expansion and outper-forms all other baselines.
These encouraging resultsshow the benefits of HDT as a principled probabilis-tic model for information retrieval.An interesting avenue of research is to constructthe vocabulary tree based on WordNet, as a way toinject independent prior knowledge into the model.However WordNet has a low coverage problem, i.e.there are some words in the data which do not ex-ist in it.
One solution to this low coverage problemis to combine trees generated by the clustering algo-rithms mentioned in this paper and WordNet, whichwe leave as a future work.ReferencesL.
Douglas Baker and Andrew Kachites McCallum.1998.
Distributional clustering of words for textclassification.
In SIGIR ?98: Proceedings of the21st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 96?103.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J. C. Lai,and R. L. Mercer.
1990.
Class-based n-gram modelsof natural language.
Computational Linguistics.P.
J. Cowans.
2004.
Information retrieval using hierar-chical dirichlet processes.
In Proceedings of the 27thAnnual International Conference on Research and De-velopment in Information Retrieval (SIGIR).R.
O. Duda, P. E. Hart, and D. G. Stork.
2000.
PatternClassification.
Wiley-Interscience Publication.N.
Friedman.
2003.
Pcluster: Probabilistic agglomera-tive clustering of gene expression profiles.
Availablefrom http://citeseer.ist.psu.edu/668029.html.Donald Metzler and W. Bruce Croft.
2007.
Latent con-cept expansion using markov random fields.
In Pro-ceedings of the 30th annual international ACM SIGIRconference on Research and development in informa-tion retrieval.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Nametagging with word clusters and discriminative training.In Proceedings of North American Chapter of the As-sociation for Computational Linguistics - Human Lan-guage Technologies conference (NAACL HLT).180T.
Minka.
2003.
The dirichlet-tree distribu-tion.
Available from http://research.microsoft.com/minka/papers/dirichlet/minka-dirtree.pdf.J.
Nocedal.
1980.
Updating quasi-newton matrices withlimited storage.
Mathematics of Computation, 35.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of english words.
In 31stAnnual Meeting of the Association for ComputationalLinguistics, pages 183?190.Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-taridis, Vibhu Mittal, and Yi Liu.
2007.
Statisticalmachine translation for query expansion in answer re-trieval.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics.S.
E. Robertson and K. Sparck Jones.
1976.
Relevanceweighting of search terms.
Journal of the AmericanSociety for Information Science, 27(3):129?146.S.
E. Robertson, S. Walker, M. Hancock-Beaulieu,A.
Gull, and M. Lau.
1992.
Okapi at trec.
In TextREtrieval Conference, pages 21?30.G.
Salton and M.J. McGill.
1983.
An Introduction toModern Information Retrieval.
McGraw-Hill, NewYork.181
