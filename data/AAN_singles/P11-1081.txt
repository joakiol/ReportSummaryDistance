Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804?813,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Cross-Lingual ILP Solution to Zero Anaphora ResolutionRyu IidaTokyo Institute of Technology2-12-1, ?Ookayama, Meguro,Tokyo 152-8552, Japanryu-i@cl.cs.titech.ac.jpMassimo PoesioUniversita` di Trento,Center for Mind / Brain SciencesUniversity of Essex,Language and Computation Groupmassimo.poesio@unitn.itAbstractWe present an ILP-based model of zeroanaphora detection and resolution that buildson the joint determination of anaphoricity andcoreference model proposed by Denis andBaldridge (2007), but revises it and extends itinto a three-way ILP problem also incorporat-ing subject detection.
We show that this newmodel outperforms several baselines and com-peting models, as well as a direct translation ofthe Denis / Baldridge model, for both Italianand Japanese zero anaphora.
We incorporateour model in complete anaphoric resolvers forboth Italian and Japanese, showing that ourapproach leads to improved performance alsowhen not used in isolation, provided that sep-arate classifiers are used for zeros and for ex-plicitly realized anaphors.1 IntroductionIn so-called ?pro-drop?
languages such as Japaneseand many romance languages including Italian,phonetic realization is not required for anaphoricreferences in contexts in which in English non-contrastive pronouns are used: e.g., the subjects ofItalian and Japanese translations of buy in (1b) and(1c) are not explicitly realized.
We call these non-realized mandatory arguments zero anaphors.
(1) a.
[EN] [John]iwent to visit some friends.
Onthe way, [he]ibought some wine.b.
[IT] [Giovanni]iando` a far visita a degli am-ici.
Per via, ?icompro` del vino.c.
[JA] [John]i-wa yujin-o houmon-sita.Tochu-de ?iwain-o ka-tta.The felicitousness of zero anaphoric referencedepends on the referred entity being sufficientlysalient, hence this type of data?particularly inJapanese and Italian?played a key role in earlywork in coreference resolution, e.g., in the devel-opment of Centering (Kameyama, 1985; Walker etal., 1994; Di Eugenio, 1998).
This research high-lighted both commonalities and differences betweenthe phenomenon in such languages.
Zero anaphoraresolution has remained a very active area of studyfor researchers working on Japanese, because of theprevalence of zeros in such languages1 (Seki et al,2002; Isozaki and Hirao, 2003; Iida et al, 2007a;Taira et al, 2008; Imamura et al, 2009; Sasano etal., 2009; Taira et al, 2010).
But now the availabil-ity of corpora annotated to study anaphora, includ-ing zero anaphora, in languages such as Italian (e.g.,Rodriguez et al (2010)), and their use in competi-tions such as SEMEVAL 2010 Task 1 on Multilin-gual Coreference (Recasens et al, 2010), is lead-ing to a renewed interest in zero anaphora resolu-tion, particularly at the light of the mediocre resultsobtained on zero anaphors by most systems partici-pating in SEMEVAL.Resolving zero anaphora requires the simulta-neous decision that one of the arguments of averb is phonetically unrealized (and which argu-ment exactly?in this paper, we will only be con-cerned with subject zeros as these are the onlytype to occur in Italian) and that a particular en-tity is its antecedent.
It is therefore natural toview zero anaphora resolution as a joint inference1As shown in Table 1, 64.3% of anaphors in the NAIST TextCorpus of Anaphora are zeros.804task, for which Integer Linear Programming (ILP)?introduced to NLP by Roth and Yih (2004) and suc-cessfully applied by Denis and Baldridge (2007) tothe task of jointly inferring anaphoricity and deter-mining the antecedent?would be appropriate.In this work we developed, starting from the ILPsystem proposed by Denis and Baldridge, an ILPapproach to zero anaphora detection and resolu-tion that integrates (revised) versions of Denis andBaldridge?s constraints with additional constraintsbetween the values of three distinct classifiers, oneof which is a novel one for subject prediction.
Wedemonstrate that treating zero anaphora resolutionas a three-way inference problem is successful forboth Italian and Japanese.
We integrate the zeroanaphora resolver with a coreference resolver anddemonstrate that the approach leads to improved re-sults for both Italian and Japanese.The rest of the paper is organized as follows.Section 2 briefly summarizes the approach proposedby Denis and Baldridge (2007).
We next present ournew ILP formulation in Section 3.
In Section 4 weshow the experimental results with zero anaphoraonly.
In Section 5 we discuss experiments testingthat adding our zero anaphora detector and resolverto a full coreference resolver would result in overallincrease in performance.
We conclude and discussfuture work in Section 7.2 Using ILP for joint anaphoricity andcoreference determinationInteger Linear Programming (ILP) is a method forconstraint-based inference aimed at finding the val-ues for a set of variables that maximize a (linear) ob-jective function while satisfying a number of con-straints.
Roth and Yih (2004) advocated ILP as ageneral solution for a number of NLP tasks that re-quire combining multiple classifiers and which thetraditional pipeline architecture is not appropriate,such as entity disambiguation and relation extrac-tion.Denis and Baldridge (2007) defined the followingobject function for the joint anaphoricity and coref-erence determination problem.min??i,j??PcC?i,j??
x?i,j?+ c?C?i,j??
(1 ?
x?i,j?)+?j?McAj?
yj+ c?Aj?
(1 ?
yj) (2)subject tox?i,j??
{0, 1} ?
?i, j?
?
Pyj?
{0, 1} ?j ?
MM stands for the set of mentions in the document,and P the set of possible coreference links over thesementions.
x?i,j?is an indicator variable that is set to1 if mentions i and j are coreferent, and 0 otherwise.yjis an indicator variable that is set to 1 if mentionj is anaphoric, and 0 otherwise.
The costs cC?i,j?=?log(P (COREF|i, j)) are (logs of) probabilitiesproduced by an antecedent identification classifierwith ?log, whereas cAj= ?log(P (ANAPH|j)),are the probabilities produced by an anaphoricity de-termination classifier with ?log.
In the Denis &Baldridge model, the search for a solution to an-tecedent identification and anaphoricity determina-tion is guided by the following three constraints.Resolve only anaphors: if a pair of mentions ?i, j?is coreferent (x?i,j?= 1), then mention j must beanaphoric (yj= 1).x?i,j??
yj?
?i, j?
?
P (3)Resolve anaphors: if a mention is anaphoric (yj=1), it must be coreferent with at least one antecedent.yj??i?Mjx?i,j?
?j ?
M (4)Do not resolve non-anaphors: if a mention is non-anaphoric (yj= 0), it should have no antecedents.yj?1|Mj|?i?Mjx?i,j?
?j ?
M (5)3 An ILP-based account of zero anaphoradetection and resolutionIn the corpora used in our experiments, zeroanaphora is annotated using as markable the firstverbal form (not necessarily the head) following theposition where the argument would have been real-ized, as in the following example.805(6) [Pahor]ie` nato a Trieste, allora porto princi-pale dell?Impero Austro-Ungarico.A sette anni [vide]il?incendio del Narodnidom,The proposal of Denis and Baldridge (2007) can beeasily turned into a proposal for the task of detectingand resolving zero anaphora in this type of data byreinterpreting the indicator variables as follows:?
yjis 1 if markable j (a verbal form) initiates averbal complex whose subject is unrealized, 0otherwise;?
x?i,j?is 1 if the empty mention realizing thesubject argument of markable j and markablei are mentions of the same entity, 0 otherwise.There are however a number of ways in which thisdirect adaptation can be modified and extended.
Wediscuss them in turn.3.1 Best FirstIn the context of zero anaphora resolution, the ?Donot resolve non-anaphors?
constraint (5) is too weak,as it allows the redundant choice of more than onecandidate antecedent.
We developed therefore thefollowing alternative, that blocks selection of morethan one antecedent.Best First (BF):yj??i?Mjx?i,j?
?j ?
M (7)3.2 A subject detection modelThe greatest difficulty in zero anaphora resolutionin comparison to, say, pronoun resolution, is zeroanaphora detection.
Simply relying for this on theparser is not enough: most dependency parsers arenot very accurate at identifying cases in which theverb does not have a subject on syntactic groundsonly.
Again, it seems reasonable to suppose thisis because zero anaphora detection requires a com-bination of syntactic information and informationabout the current context.
Within the ILP frame-work, this hypothesis can be implemented by turn-ing the zero anaphora resolution optimization prob-lem into one with three indicator variables, with theobjective function in (8).
The third variable, zj, en-codes the information provided by the parser: it is1 with cost cSj= ?log(P (SUBJ |j)) if the parserthinks that verb j has an explicit subject with proba-bility P (SUBJ |j), otherwise it is 0.min??i,j??PcC?i,j??
x?i,j?+ c?C?i,j??
(1 ?
x?i,j?)+?j?McAj?
yj+ c?Aj?
(1 ?
yj)+?j?McSj?
zj+ c?Sj?
(1 ?
zj) (8)subject tox?i,j??
{0, 1} ?
?i, j?
?
Pyj?
{0, 1} ?j ?
Mzj?
{0, 1} ?j ?
MThe crucial fact about the relation between zjandyjis that a verb has either a syntactically realized NPor a zero pronoun as a subject, but not both.
This isencoded by the following constraint.Resolve only non-subjects: if a predicate j syntac-tically depends on a subject (zj= 1), then the predi-cate j should have no antecedents of its subject zeropronoun.yj+ zj?
1 ?j ?
M (9)4 Experiment 1: zero anaphora resolutionIn a first round of experiments, we evaluated the per-formance of the model proposed in Section 3 on zeroanaphora only (i.e., not attempting to resolve othertypes of anaphoric expressions).4.1 Data setsWe use the two data sets summarized in Table 1.The table shows that NP anaphora occurs more fre-quently than zero-anaphora in Italian, whereas inJapanese the frequency of anaphoric zero-anaphors2is almost double the frequency of the remaininganaphoric expressions.Italian For Italian coreference, we used the anno-tated data set presented in Rodriguez et al (2010)and developed for the Semeval 2010 task ?Corefer-ence Resolution in Multiple Languages?
(Recasenset al, 2010), where both zero-anaphora and NP2In Japanese, like in Italian, zero anaphors are often usednon-anaphorically, to refer to situationally introduced entities,as in I went to John?s office, but they told me that he had left.806#instances (anaphoric/total)language type #docs #sentences #words zero-anaphors others allItalian train 97 3,294 98,304 1,093 / 1,160 6,747 / 27,187 7,840 / 28,347test 46 1,478 41,587 792 / 837 3,058 / 11,880 3,850 / 12,717Japanese train 1,753 24,263 651,986 18,526 / 29,544 10,206 / 161,124 28,732 / 190,668test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857In the 6th column we use the term ?anaphoric?
to indicate the number of zero anaphors that have an antecedent inthe text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / genericreference.Table 1: Italian and Japanese Data Setscoreference are annotated.
This dataset consistsof articles from Italian Wikipedia, tokenized, POS-tagged and morphologically analyzed using TextPro,a freely available Italian pipeline (Pianta et al,2008).
We parsed the corpus using the Italian ver-sion of the DESR dependency parser (Attardi et al,2007).In Italian, zero pronouns may only occur as omit-ted subjects of verbs.
Therefore, in the task ofzero-anaphora resolution all verbs appearing in atext are considered candidates for zero pronouns,and all gold mentions or system mentions preced-ing a candidate zero pronoun are considered as can-didate antecedents.
(In contrast, in the experimentson coreference resolution discussed in the followingsection, all mentions are considered as both candi-date anaphors and candidate antecedents.
To com-pare the results with gold mentions and with systemdetected mentions, we carried out an evaluation us-ing the mentions automatically detected by the Ital-ian version of the BART system (I-BART) (Poesioet al, 2010), which is freely downloadable.3Japanese For Japanese coreference we used theNAIST Text Corpus (Iida et al, 2007b) version1.4?, which contains the annotated data about NPcoreference and zero-anaphoric relations.
We alsoused the Kyoto University Text Corpus4 that pro-vides dependency relations information for the samearticles as the NAIST Text Corpus.
In addition, wealso used a Japanese named entity tagger, CaboCha5for automatically tagging named entity labels.
Inthe NAIST Text Corpus mention boundaries are notannotated, only the heads.
Thus, we considered3http://www.bart-coref.org/4http://www-lab25.kuee.kyoto-u.ac.jp/nl-resource/corpus.html5http://chasen.org?taku/software/cabocha/as pseudo-mentions all bunsetsu chunks (i.e.
basephrases in Japanese) whose head part-of-speech wasautomatically tagged by the Japanese morphologi-cal analyser Chasen6 as either ?noun?
or ?unknownword?
according to the NAIST-jdic dictionary.7For evaluation, articles published from January1st to January 11th and the editorials from Januaryto August were used for training and articles datedJanuary 14th to 17th and editorials dated Octoberto December are used for testing as done by Tairaet al (2008) and Imamura et al (2009).
Further-more, in the experiments we only considered subjectzero pronouns for a fair comparison to Italian zero-anaphora.4.2 ModelsIn these first experiments we compared the threeILP-based models discussed in Section 3: the directreimplementation of the Denis and Baldridge pro-posal (i.e., using the same constrains), a version re-placing Do-Not-Resolve-Not-Anaphors with Best-First, and a version with Subject Detection as well.As discussed by Iida et al (2007a) and Imamuraet al (2009), useful features in intra-sentential zero-anaphora are different from ones in inter-sententialzero-anaphora because in the former problem syn-tactic information between a zero pronoun and itscandidate antecedent is essential, while the lat-ter needs to capture the significance of saliencybased on Centering Theory (Grosz et al, 1995).To directly reflect this difference, we created twoantecedent identification models; one for intra-sentential zero-anaphora, induced using the traininginstances which a zero pronoun and its candidate an-tecedent appear in the same sentences, the other for6http://chasen-legacy.sourceforge.jp/7http://sourceforge.jp/projects/naist-jdic/807inter-sentential cases, induced from the remainingtraining instances.To estimate the feature weights of each classifier,we used MEGAM8, an implementation of the Max-imum Entropy model, with default parameter set-tings.
The ILP-based models were compared withthe following baselines.PAIRWISE: as in the work by Soon et al (2001),antecedent identification and anaphoricity determi-nation are simultaneously executed by a single clas-sifier.DS-CASCADE: the model first filters out non-anaphoric candidate anaphors using an anaphoric-ity determination model, then selects an antecedentfrom a set of candidate antecedents of anaphoriccandidate anaphors using an antecedent identifica-tion model.4.3 FeaturesThe feature sets for antecedent identification andanaphoricity determination are briefly summarizedin Table 2 and Table 3, respectively.
The agreementfeatures such as NUM AGREE and GEN AGREE areautomatically derived using TextPro.
Such agree-ment features are not available in Japanese becauseJapanese words do not contain such information.4.4 Creating subject detection modelsTo create a subject detection model for Italian, weused the TUT corpus9 (Bosco et al, 2010), whichcontains manually annotated dependency relationsand their labels, consisting of 80,878 tokens inCoNLL format.
We induced an maximum entropyclassifier by using as items all arcs of dependencyrelations, each of which is used as a positive instanceif its label is subject; otherwise it is used as a nega-tive instance.To train the Japanese subject detection model weused 1,753 articles contained both in the NAISTText Corpus and the Kyoto University Text Corpus.By merging these two corpora, we can obtain the an-notated data including which dependency arc is sub-ject10.
To create the training instances, any pair ofa predicate and its dependent are extracted, each of8http://www.cs.utah.edu/?hal/megam/9http://www.di.unito.it/?tutreeb/10Note that Iida et al (2007b) referred to this relation as?nominative?.feature descriptionSUBJ PRE 1 if subject is included in the precedingwords of ZERO in a sentence; otherwise 0.TOPIC PRE* 1 if topic case marker appears in the preced-ing words of ZERO in a sentence; otherwise0.NUM PRE(GEN PRE)1 if a candidate which agrees with ZEROwith regards to number (gender) is includedin the set of NP; otherwise 0.FIRST SENT 1 if ZERO appears in the first sentence of atext; otherwise 0.FIRST WORD 1 if the predicate which has ZERO is thefirst word in a sentence; otherwise 0.POS / LEMMA/ DEP LABELpart-of-speech / dependency label / lemmaof the predicate which has ZERO.D POS /D LEMMA /D DEP LABELpart-of-speech / dependency label / lemmaof the dependents of the predicate which hasZERO.PATH* dependency labels (functional words) ofwords intervening between a ZERO and thesentence headThe features marked with ?*?
used only in Japanese.Table 3: Features for anaphoricity determinationwhich is judged as positive if its relation is subject;as negative otherwise.As features for Italian, we used lemmas, PoS tagof a predicate and its dependents as well as theirmorphological information (i.e.
gender and num-ber) automatically computed by TextPro (Pianta etal., 2008).
For Japanese, the head lemmas of predi-cate and dependent chunks as well as the functionalwords involved with these two chunks were used asfeatures.
One case specially treated is when a de-pendent is placed as an adnominal constituent of apredicate, as in this case relation estimation of de-pendency arcs is difficult.
In such case we insteaduse the features shown in Table 2 for accurate esti-mation.4.5 Results with zero anaphora onlyIn zero anaphora resolution, we need to find all pred-icates that have anaphoric unrealized subjects (i.e.zero pronouns which have an antecedent in a text),and then identify an antecedent for each such argu-ment.The Italian and Japanese test data sets contain4,065 and 25,467 verbal predicates respectively.
Theperformance of each model at zero-anaphora detec-tion and resolution is shown in Table 4, using recall808feature descriptionHEAD LEMMA characters of the head lemma in NP.POS part-of-speech of NP.DEFINITE 1 if NP contains the article corresponding to DEFINITE ?the?
; otherwise 0.DEMONSTRATIVE 1 if NP contains the article corresponding to DEMONSTRATIVE such as ?that?
and ?this?
; otherwise 0.POSSESSIVE 1 if NP contains the article corresponding to POSSESSIVE such as ?his?
and ?their?
; otherwise 0.CASE MARKER** case marker followed by NP, such as ?wa (topic)?, ?ga (subject)?, ?o (object)?.DEP LABEL* dependency label of NP.COOC MI** the score of well-formedness model estimated from a large number of triplets ?
NP, Case, Predicate?.FIRST SENT 1 if NP appears in the first sentence of a text; otherwise 0.FIRST MENTION 1 if NP first appears in the set of candidate antecedents; otherwise 0.CL RANK** a rank of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)CL ORDER** a order of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)PATH dependency labels (functional words) of words intervening between a ZERO and NPNUM (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to number; otherwise 0.GEN (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to gender; otherwise 0.HEAD MATCH 1 if ANA and NP have the same head lemma; otherwise 0.REGEX MATCH 1 if the string of NP subsumes the string of ANA; otherwise 0.COMP MATCH 1 if ANA and NP have the same string; otherwise 0.NP, ANA and ZERO stand for a candidate antecedent, a candidate anaphor and a candidate zero pronoun respectively.
The featuresmarked with ?*?
are only used in Italian, while the features marked with ?**?
are only used in Japanese.Table 2: Features used for antecedent identificationItalian Japanesesystem mentions gold mentionsmodel R P F R P F R P FPAIRWISE 0.864 0.172 0.287 0.864 0.172 0.287 0.286 0.308 0.296DS-CASCADE 0.396 0.684 0.502 0.404 0.697 0.511 0.345 0.194 0.248ILP 0.905 0.034 0.065 0.929 0.028 0.055 0.379 0.238 0.293ILP +BF 0.803 0.375 0.511 0.834 0.369 0.511 0.353 0.256 0.297ILP +SUBJ 0.900 0.034 0.066 0.927 0.028 0.055 0.371 0.315 0.341ILP +BF +SUBJ 0.777 0.398 0.526 0.815 0.398 0.534 0.345 0.348 0.346Table 4: Results on zero pronouns/ precision / F over link detection as a metric (modeltheoretic metrics do not apply for this task as onlysubsets of coreference chains are considered).
Ascan be seen from Table 4, the ILP version with Do-Not-Resolve-Non-Anaphors performs no better thanthe baselines for either languages, but in both lan-guages replacing that constraint with Best-First re-sults in a performance above the baselines; addingSubject Detection results in further improvement forboth languages.
Notice also that the performance ofthe models on Italian is quite a bit higher than forJapanese although the dataset is much smaller, pos-sibly meaning that the task is easier in Italian.5 Experiment 2: coreference resolution forall anaphorsIn a second series of experiments we evaluated theperformance of our models together with a fullcoreference system resolving all anaphors, not justzeros.5.1 Separating vs combining classifiersDifferent types of nominal expressions display verydifferent anaphoric behavior: e.g., pronoun res-olution involves very different types of informa-tion from nominal expression resolution, depend-ing more on syntactic information and on the localcontext and less on commonsense knowledge.
Butthe most common approach to coreference resolu-809tion (Soon et al, 2001; Ng and Cardie, 2002, etc.
)is to use a single classifier to identify antecedents ofall anaphoric expressions, relying on the ability ofthe machine learning algorithm to learn these differ-ences.
These models, however, often fail to capturethe differences in anaphoric behavior between dif-ferent types of expressions?one of the reasons be-ing that the amount of training instances is often toosmall to learn such differences.11 Using differentmodels would appear to be key in the case of zero-anaphora resolution, which differs even more fromthe rest of anaphora resolution, e.g., in being partic-ularly sensitive to local salience, as amply discussedin the literature on Centering discussed earlier.To test the hypothesis that using what we willcall separated models for zero anaphora and every-thing else would work better than combined mod-els induced from all the learning instances, we man-ually split the training instances in terms of thesetwo anaphora types and then created two classifiersfor antecedent identification: one for zero-anaphora,the other for NP-anaphora, separately induced fromthe corresponding training instances.
Likewise,anaphoricity determination models were separatelyinduced with regards to these two anaphora types.5.2 Results with all anaphorsIn Table 5 and Table 6 we show the (MUC scorer)results obtained by adding the zero anaphoric reso-lution models proposed in this paper to both a com-bined and a separated classifier.
For the separatedclassifier, we use the ILP+BF model for explicitlyrealized NPs, and different ILP models for zeros.The results show that the separated classi-fier works systematically better than a combinedclassifier.
For both Italian and Japanese theILP+BF+SUBJ model works clearly better than thebaselines, whereas simply applying the original De-nis and Baldridge model unchanged to this case weobtain worse results than the baselines.
For Italianwe could also compare our results with those ob-tained on the same dataset by one of the two sys-tems that participated to the Italian section of SE-MEVAL, I-BART.
I-BART?s results are clearly bet-ter than those with both baselines, but also clearly in-11E.g., the entire MUC-6 corpus contains a grand total of 3reflexive pronouns.Japanesecombined separatedmodel R P F R P FPAIRWISE 0.345 0.236 0.280 0.427 0.240 0.308DS-CASCADE 0.207 0.592 0.307 0.291 0.488 0.365ILP 0.381 0.330 0.353 0.490 0.304 0.375ILP +BF 0.349 0.390 0.368 0.446 0.340 0.386ILP +SUBJ 0.376 0.366 0.371 0.484 0.353 0.408ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427Table 6: Results for overall coreference: Japanese (MUCscore)ferior to the results obtained with our models.
In par-ticular, the effect of introducing the separated modelwith ILP+BF+SUBJ is more significant when us-ing the system detected mentions; it obtained perfor-mance more than 13 points better than I-BART whenthe model referred to the system detected mentions.6 Related workWe are not aware of any previous machine learn-ing model for zero anaphora in Italian, but therehas been quite a lot of work on Japanese zero-anaphora (Iida et al, 2007a; Taira et al, 2008; Ima-mura et al, 2009; Taira et al, 2010; Sasano et al,2009).
In work such as Taira et al (2008) and Ima-mura et al (2009), zero-anaphora resolution is con-sidered as a sub-task of predicate argument structureanalysis, taking the NAIST text corpus as a targetdata set.
Taira et al (2008) and Taira et al (2010) ap-plied decision lists and transformation-based learn-ing respectively in order to manually analyze whichclues are important for each argument assignment.Imamura et al (2009) also tackled to the same prob-lem setting by applying a pairwise classifier for eachargument.
In their approach, a ?null?
argument is ex-plicitly added into the set of candidate argument tolearn the situation where an argument of a predicateis ?exophoric?.
They reported their model achievedbetter performance than the work by Taira et al(2008).Iida et al (2007a) also used the NAIST textcorpus.
They adopted the BACT learning algo-rithm (Kudo and Matsumoto, 2004) to effectivelylearn subtrees useful for both antecedent identifica-tion and zero pronoun detection.
Their model drasti-cally outperformed a simple pairwise model, but it isstill performed as a cascaded process.
Incorporating810Italiansystem mentions gold mentionscombined separated combined separatedmodel R P F R P F R P F R P FPAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 0.361 0.566 0.314 0.404DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 0.349 0.246 0.686 0.362I-BART 0.324 0.294 0.308 ?
?
?
0.532 0.441 0.482 ?
?
?ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.461 0.607 0.384 0.470ILP +BF 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 0.530 0.563 0.519 0.540ILP +SUBJ 0.537 0.325 0.405 0.534 0.318 0.399 0.611 0.372 0.463 0.606 0.387 0.473ILP +BF +SUBJ 0.464 0.410 0.435 0.478 0.418 0.446 0.538 0.527 0.533 0.559 0.536 0.547R: Recall, P: Precision, F: f -score, BF: best first constraint, SUBJ: subject detection model.Table 5: Results for overall coreference: Italian (MUC score)their model into the ILP formulation proposed herelooks like a promising further extension.Sasano et al (2009) obtained interesting experi-mental results about the relationship between zero-anaphora resolution and the scale of automaticallyacquired case frames.
In their work, their caseframes were acquired from a very large corpus con-sisting of 100 billion words.
They also proposeda probabilistic model to Japanese zero-anaphorain which an argument assignment score is esti-mated based on the automatically acquired caseframes.
They concluded that case frames acquiredfrom larger corpora lead to better f -score on zero-anaphora resolution.In contrast to these approaches in Japanese, theparticipants to Semeval 2010 task 1 (especially theItalian coreference task) simply solved the prob-lems using one coreference classifier, not distin-guishing zero-anaphora from the other types ofanaphora (Kobdani and Schu?tze, 2010; Poesio et al,2010).
On the other hand, our approach shows sep-arating problems contributes to improving perfor-mance in Italian zero-anaphora.
Although we usedgold mentions in our evaluations, mention detectionis also essential.
As a next step, we also need to takeinto account ways of incorporating a mention detec-tion model into the ILP formulation.7 ConclusionIn this paper, we developed a new ILP-based modelof zero anaphora detection and resolution that ex-tends the coreference resolution model proposed byDenis and Baldridge (2007) by introducing modi-fied constraints and a subject detection model.
Weevaluated this model both individually and as partof the overall coreference task for both Italian andJapanese zero anaphora, obtaining clear improve-ments in performance.One avenue for future research is motivated by theobservation that whereas introducing the subject de-tection model and the best-first constraint results inhigher precision maintaining the recall compared tothe baselines, that precision is still low.
One of themajor source of the errors is that zero pronouns arefrequently used in Italian and Japanese in contexts inwhich in English as so-called generic they would beused: ?I walked into the hotel and (they) said ..?.
Insuch case, the zero pronoun detection model is oftenincorrect.
We are considering adding a generic theydetection component.We also intend to experiment with introducingmore sophisticated antecedent identification modelsin the ILP framework.
In this paper, we used a verybasic pairwise classifier; however Yang et al (2008)and Iida et al (2003) showed that the relative com-parison of two candidate antecedents leads to obtain-ing better accuracy than the pairwise model.
How-ever, these approaches do not output absolute prob-abilities, but relative significance between two can-didates, and therefore cannot be directly integratedwith the ILP-framework.
We plan to examine waysof appropriately estimating an absolute score from aset of relative scores for further refinement.Finally, we would like to test our model withEnglish constructions which closely resemble zeroanaphora.
One example were studied in the Semeval2010 ?Linking Events and their Participants in Dis-course?
task, which provides data about null instan-811tiation, omitted arguments of predicates like ?Wearrived ?goal at 8pm.?.
(Unfortunately the datasetavailable for SEMEVAL was very small.)
Anotherinteresting area of application of these techniqueswould be VP ellipsis.AcknowledgmentsRyu Iida?s stay in Trento was supported by the Ex-cellent Young Researcher Overseas Visit Programof the Japan Society for the Promotion of Science(JSPS).
Massimo Poesio was supported in part bythe Provincia di Trento Grande Progetto LiveMem-ories, which also funded the creation of the Italiancorpus used in this study.
We also wish to thankFrancesca Delogu, Kepa Rodriguez, Olga Uryupinaand Yannick Versley for much help with the corpusand BART.ReferencesG.
Attardi, F. Dell?Orletta, M. Simi, A. Chanev, andM.
Ciaramita.
2007.
Multilingual dependency pars-ing and domain adaptation using desr.
In Proc.
of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,Prague.C.
Bosco, S. Montemagni, A. Mazzei, V. Lombardo,F.
Dell?Orletta, A. Lenci, L. Lesmo, G. Attardi,M.
Simi, A. Lavelli, J.
Hall, J. Nilsson, and J. Nivre.2010.
Comparing the influence of different treebankannotations on dependency parsing.
In Proceedings ofLREC, pages 1794?1801.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proc.
of HLT/NAACL, pages 236?243.B.
Di Eugenio.
1998.
Centering in Italian.
In M. A.Walker, A. K. Joshi, and E. F. Prince, editors, Cen-tering Theory in Discourse, chapter 7, pages 115?138.Oxford.B.
J. Grosz, A. K. Joshi, and S. Weinstein.
1995.
Center-ing: A framework for modeling the local coherence ofdiscourse.
Computational Linguistics, 21(2):203?226.R.
Iida, K. Inui, H. Takamura, and Y. Matsumoto.
2003.Incorporating contextual cues in trainable models forcoreference resolution.
In Proceedings of the 10thEACL Workshop on The Computational Treatment ofAnaphora, pages 23?30.R.
Iida, K. Inui, and Y. Matsumoto.
2007a.
Zero-anaphora resolution by learning rich syntactic patternfeatures.
ACM Transactions on Asian Language Infor-mation Processing (TALIP), 6(4).R.
Iida, M. Komachi, K. Inui, and Y. Matsumoto.
2007b.Annotating a Japanese text corpus with predicate-argument and coreference relations.
In Proceeding ofthe ACL Workshop ?Linguistic Annotation Workshop?,pages 132?139.K.
Imamura, K. Saito, and T. Izumi.
2009.
Discrimi-native approach to predicate-argument structure anal-ysis with zero-anaphora resolution.
In Proceedings ofACL-IJCNLP, Short Papers, pages 85?88.H.
Isozaki and T. Hirao.
2003.
Japanese zero pronounresolution based on ranking rules and machine learn-ing.
In Proceedings of EMNLP, pages 184?191.M.
Kameyama.
1985.
Zero Anaphora: The case ofJapanese.
Ph.D. thesis, Stanford University.H.
Kobdani and H. Schu?tze.
2010.
Sucre: A modularsystem for coreference resolution.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 92?95.T.
Kudo and Y. Matsumoto.
2004.
A boosting algorithmfor classification of semi-structured text.
In Proceed-ings of EMNLP, pages 301?308.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In Proceedingsof the 40th ACL, pages 104?111.E.
Pianta, C. Girardi, and R. Zanoli.
2008.
The TextProtool suite.
In In Proceedings of LREC, pages 28?30.M.
Poesio, O. Uryupina, and Y. Versley.
2010.
Creating acoreference resolution system for Italian.
In Proceed-ings of LREC.M.
Recasens, L. Ma`rquez, E. Sapena, M. A.
Mart??,M.
Taule?, V. Hoste, M. Poesio, and Y. Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 1?8.K-J.
Rodriguez, F. Delogu, Y. Versley, E. Stemle, andM.
Poesio.
2010.
Anaphoric annotation of wikipediaand blogs in the live memories corpus.
In Proc.
LREC.D.
Roth and W.-T. Yih.
2004.
A linear programmingformulation for global inference in natural languagetasks.
In Proc.
of CONLL.R.
Sasano, D. Kawahara, and S. Kurohashi.
2009.
Theeffect of corpus size on case frame acquisition for dis-course analysis.
In Proceedings of HLT/NAACL, pages521?529.K.
Seki, A. Fujii, and T. Ishikawa.
2002.
A probabilisticmethod for analyzing Japanese anaphora integratingzero pronoun detection and resolution.
In Proceedingsof the 19th COLING, pages 911?917.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics, 27(4):521?544.812H.
Taira, S. Fujita, and M. Nagata.
2008.
A Japanesepredicate argument structure analysis using decisionlists.
In Proceedings of EMNLP, pages 523?532.H.
Taira, S. Fujita, and M. Nagata.
2010.
Predicate ar-gument structure analysis using transformation basedlearning.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 162?167.M.
A. Walker, M. Iida, and S. Cote.
1994.
Japanesediscourse and the process of centering.
ComputationalLinguistics, 20(2):193?232.X.
Yang, J. Su, and C. L. Tan.
2008.
Twin-candidatemodel for learning-based anaphora resolution.
Com-putational Linguistics, 34(3):327?356.813
