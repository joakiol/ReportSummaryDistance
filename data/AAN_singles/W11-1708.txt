Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 61?69,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsA Link to the Past: Constructing Historical Social NetworksMatje van de CampTilburg Centre for Cognitionand CommunicationTilburg University, The NetherlandsM.M.v.d.Camp@uvt.nlAntal van den BoschTilburg Centre for Cognitionand CommunicationTilburg University, The NetherlandsAntal.vdnBosch@uvt.nlAbstractTo assist in the research of social networks inhistory, we develop machine-learning-basedtools for the identification and classificationof personal relationships.
Our case study fo-cuses on the Dutch social movement between1870 and 1940, and is based on biographicaltexts describing the lives of notable people inthis movement.
We treat the identification andthe labeling of relations between two personsinto positive, neutral, and negative both as asequence of two tasks and as a single task.
Weobserve that our machine-learning classifiers,support vector machines, produce better gen-eralization performance on the single task.
Weshow how a complete social network can bebuilt from these classifications, and provide aqualitative analysis of the induced network us-ing expert judgements on samples of the net-work.1 IntroductionThe rapid growth of Social Networking Servicessuch as Facebook, Myspace and Twitter over thelast few years has made it possible to gather data onhuman interactions on a large scale, causing an in-creased interest in the field of Social Network Anal-ysis and Extraction.
Although we are now more in-terconnected than ever before due to technologicaladvances, social networks have always been a vitalpart of human existence.
They are prerequisite to thedistribution of knowledge and beliefs among peopleand to the formation of larger entities such as orga-nizations and communities.
By applying the tech-nology of today to the heritage of our past, it may bepossible to uncover yet unknown patterns and pro-vide a better insight into our society?s development.In this paper we present a case study based onhistorical biographical information, so-called sec-ondary historical sources, describing people in aparticular domain, region and time frame: theDutch social movement between the mid-19th andmid-20th century.
?Social movement?
refers tothe social-political-economical complex of ideolo-gies, worker?s unions, political organizations, andart movements that arose from the ideas of KarlMarx (1818?1883) and followers.
In the Nether-lands, a network of persons unfolded over time withleader figures such as Ferdinand Domela Nieuwen-huis (1846?1919) and Pieter Jelles Troelstra (1860?1930).
Although this network is implicit in allthe primary and secondary historical writings doc-umenting the period, and partly explicit in the mindsof experts studying the domain, there is no explic-itly modeled social network of this group of persons.Yet, it would potentially benefit further research insocial history to have this in the form of a computa-tional model.In our study we focus on detecting and labelingrelations between two persons, where one of the per-sons, A, is the topic of a biographical article, andthe other person, B, is mentioned in that article.
Thegenre of biographical articles allows us to assumethat person A is topical throughout the text.
Whatremains is to determine whether the mention of per-son B signifies a relation between A and B, and if so,whether the relation in the direction of A to B can belabeled as positive, neutral, or negative.
Many morefine-grained labels are possible (as discussed later in61the paper), but the primary aim of our case study isto build a basic network out of robustly recognizedperson-to-person relations at the highest possible ac-curacy.
As our data only consists of several hun-dreds of articles describing an amount of people ofroughly the same order of magnitude, we are facingdata sparsity, and thus are limited in the granularityof the labels we wish to predict.This paper is structured as follows.
After a briefsurvey of related research in Section 2, we describeour method of research, our data, and our annota-tion scheme in Section 3.
In Section 4 we describehow we implement relation detection and classifica-tion as supervised machine learning tasks.
The out-comes of the experiments on our data are provided inSection 5.
We discuss our findings, formulate con-clusions, and identify points for future research inSection 6.2 Related ResearchOur research combines Social Network Extractionand Sentiment Analysis.
We briefly review relatedresearch in both areas.2.1 Social Network ExtractionA widely used method for determining the related-ness of two entities was first introduced by Kautz etal (1997).
They compute the relatedness betweentwo entities by normalizing their co-occurrencecount on the Web with their individual hit counts us-ing the Jaccard coefficient.
If the coefficient reachesa certain threshold, the entities are considered to berelated.
For disambiguation purposes, keywords areadded to the queries when obtaining the hit counts.Matsuo et al(2004) apply the same method to findconnections between members of a closed commu-nity of researchers.
They gather person names fromconference attendance lists to create the nodes of thenetwork.
The affiliations of each person are addedto the queries as a crude form of named entity dis-ambiguation.
When a connection is found, the re-lation is labeled by applying minimal rules, basedon the occurrence of manually selected keywords,to the contents of websites where both entities arementioned.A more elaborate approach to network min-ing is taken by Mika (2005) in his presentationof the Flink system.
In addition to Web co-occurrence counts of person names, the system usesdata mined from other?highly structured?sourcessuch as email headers, publication archives and so-called Friend-Of-A-Friend (FOAF) profiles.
Co-occurrence counts of a name and different intereststaken from a predefined set are used to determine aperson?s expertise and to enrich their profile.
Theseprofiles are then used to resolve named entity co-reference and to find new connections.Elson et al(2010) use quoted speech attribution toreconstruct the social networks of the characters ina novel.
Though this work is most related regardingthe type of data used, their method can be consid-ered complementary to ours: where they relate enti-ties based on their conversational interaction withoutfurther analysis of the content, we try to find connec-tions based solely on the words that occur in the text.Efforts in more general relation extraction fromtext have focused on finding recurring patterns andtransforming them into triples (RDF).
Relation typesand labels are then deduced from the most commonpatterns (Ravichandran and Hovy, 2002; Culotta etal, 2006).
These approaches work well for the in-duction and verification of straightforwardly verbal-ized factoids, but they are too restricted to capturethe multitude of aspects that surround human inter-action; a case in point is the kind of relationship be-tween two persons, which people can usually inferfrom the text, but is rarely explicitly described in asingle triple.2.2 Sentiment AnalysisSentiment analysis is concerned with locating andclassifying the subjective information contained in asource.
Subjectivity is inherently dependent on hu-man interpretation and emotion.
A machine can betaught to mimic these aspects, given enough exam-ples, but the interaction of the two is what makeshumans able to understand, for instance, that a sar-castic comment is not meant to be taken literally.Although the general distinction between negativeand positive is intuitive for humans to make, sub-jectivity and sentiment are very much domain andcontext dependent.
Depending on the domain andcontext, a single sentence can have opposite mean-ings (Pang and Lee, 2008).Many of the approaches to automatically solv-62ing tasks like these involve using lists of positivelyand negatively polarized words or phrases to calcu-late the overall sentiment of a clause, sentence ordocument (Pang et al 2002).
As shown by Kimand Hovy (2006), the order of the words poten-tially influences the interpretation of a text.
Panget al(2002) also found that the simple presence of aword is more important than the number of times itappears.Word sense disambiguation can be a useful tool indetermining polarity.
Turney (2002) proposed a sim-ple, but seemingly effective way to determine polar-ity at the word level.
He calculates the differencebetween the mutual information gain of a phrase andthe word ?excellent?
and of the same phrase and theword ?poor?.3 Method, Data, and Annotation3.1 MethodIn contrast to most previous work regarding socialnetwork extraction, we do not possess any explicitrecord of the network we are after.
Although thedocuments we work with are available online, thenumber of hyperlinks between them is minimal andall personal relations are expressed only in runningtext.
We aim to train a system able to extract theserelations and classify their polarity automatically us-ing as little information as possible that is not explic-itly included in the text, thus keeping the reliance onexternal resources as limited as possible.We take the same approach with regards to thesentiment analysis part of the task: no predefinedlists are supplied to the system and no word sensedisambiguation is performed.We take a supervised machine learning approachto solving the problem, by training support vectormachines on a limited number of preclassified exam-ples.
We chose to use SVMs as a baseline methodthat has been shown to be effective in text catego-rization tasks (Joachims, 1998).
We compare perfor-mance between joint learning, using one multi-classclassifier, and a pipeline, using a single class clas-sifier to judge whether an instance describes a rela-tion, and a second classifier to classify the relationsaccording to their polarity.3.2 DataWe use the Biographical Dictionary of Socialismand the Workers?
Movement in the Netherlands(BWSA) as input for our system.1 This digitalresource consists of 574 biographical articles, inDutch, relating to the most notable actors within thedomain.
The texts are accompanied by a databasethat holds such metadata as a person?s full nameand known aliases, dates of birth and death, and ashort description of the role they played within theWorkers?
Movement.
The articles were written byover 200 different authors, thus the use of vocabu-lary varies greatly across the texts.
The length of thebiographies also varies: the shortest text has 308 to-kens, the longest has 7,188 tokens.
The mean lengthis 1,546 tokens with a standard deviation of 784.A biography can be seen as a summary of the mostimportant events in a person?s life.
Therefore, thistype of data suits our purpose well: any person thatthe main character was closely related to, can be ex-pected to appear in his or her biography.In training our relation extraction system we lookonly at the relation from A to B and its associatedpolarity.
The assumption that we make here is thatby processing the BWSA in its entirety, making eachof the 574 main characters person A once and har-vesting all of their relations, we will get a full viewof the existing relations, including the relation fromB to A if A and B have a relation and B also has abiography in the BWSA.We create one data set focused on a particular per-son who is prevalent throughout the data, namelyFerdinand Domela Nieuwenhuis (FDN).
He startedhis career as a Lutheran priest, but lost his faith andpursued a career in socialist politics.
After a seriesof disappointments, however, he turned to anarchismand eventually withdrew himself from the politicalstage completely, though his ideas continued to in-spire others.
We expect that the turmoil of his lifewill be reflected in his social network and the vari-ety of relationships surrounding him.As a first step in recreating Domela Nieuwenhuis?network, we extract all sentences from the BWSAthat mention the name ?Domela?, by which he isgenerally known.
We exclude Domela?s own bi-ography from the search.
All but one of the ex-1http://www.iisg.nl/bwsa/63tracted sentences, 447 in total, actually refer to Fer-dinand Domela Nieuwenhuis.
This sentence is re-moved, resulting in a total of 446 sentences spreadover 153 biographies.
Each sentence with a men-tion is expanded with additional context, to capturemore clues than the sentence with the mention mighthold.
Preliminary tests showed that two sentencesof context before the mention, and two sentences ofcontext after the mention is sufficient.
Often thereis an introduction before a person is mentioned, andan elaboration on the relation after the mention.
Fig-ure 1 shows an example fragment.However, since Domela was a rather controver-sial and a-typical figure, his network might not bea good representation of the actual relations in thedata.
Therefore, we create a second data set by ran-domly extracting another 534 sentences with theirsurrounding context from the BWSA that contain anamed entity which is not the main entity of the bi-ography.
We aim to test which data set leads to bet-ter performance in finding and classifying relationsacross the entire community.3.3 AnnotationAll fragments in the Domela set were annotated bytwo human annotators, native speakers of Dutch, butunfamiliar with the domain of social history.
Theywere asked to judge whether the fragment does infact describe a relation between the two entities and,if so, whether the polarity of the relation from A to Bis negative, neutral, or positive; i.e.
whether personA has a negative, neutral or positive attitude towardsperson B.With regards to the existence of a relation, the an-notators reached an agreement of 74.9%.
For thenegative, neutral and positive classes they agreed on60.8%, 24.2%, and 66.5%, respectively.
All dis-agreements were resolved in discussion.
The classdistribution over the three polarities after resolutionis shown in Table 1.The generic set was annotated by only one of theannotators.
The class distribution of this set is alsoshown in Table 1.
It is roughly the same as the dis-tribution for the A to B polarities from the Domelaset.Class Generic set FDN setNo.
% No.
%negative 86 16.1 74 16.6neutral 134 25.1 87 19.5positive 238 44.6 215 48.2not related 76 14.2 70 15.7total 534 100 446 100Table 1: Class distribution4 Relation Extraction and ClassificationWe train our system using LibSVM (Chang andLin, 2001), an implementation of support vector ma-chines.
In training, the cost factor is set to 0.01 witha polynomial kernel type.4.1 PreprocessingFirst, all fragments and biographies are lemmatizedand POS-tagged using Frog, a morpho-syntacticanalyzer for Dutch (Van den Bosch et al 2007).In a next step, Named Entity Recognition is per-formed with a classifier-based sequence processingtool trained on biographical data.To identify the person to which a named entityrefers, the name is split up into chunks representingfirst name, initials, infix and surname.
These chunks,as far as they are included in the string, are thenmatched against the BWSA database.
If no matchis found, the name is added to the database as a newperson.
For now, however, we treat the network asa closed community by only extracting those frag-ments in which person B is one that already has a bi-ography in the BWSA.
At a later stage, biographiesof people from outside the BWSA can be gatheredand used to determine their position within the net-work.4.2 FeaturesCo-occurrence counts: We calculate an initial mea-sure of the relatedness of A to B using a method thatis similar to Kautz et al(1997).
The main differenceis that we do not get our co-occurrence counts onlyfrom the Web, but also from the data itself.
Since thedomain of the data is so specific, Web counts do notaccurately represent the actual distribution of peoplein the data.
More famous people are likely to receivemore attention on the Web than less famous people.64AnsingPER?A and Domela NieuwenhuisPER?B were in written contact with each other since August 1878.Domela Nieuwenhuis probably wrote uplifting words in his letter to Ansing, which was not preserved, afterreading Pekelharing?s report of the program convention of the ANWV in Vragen des Tijds, which was allbut flattering for Ansing.In this letter, Domela also offered his services to Ansing and his friends.Domela Nieuwenhuis used this opportunity to ask Ansing several questions about the conditions of theworkers, the same that he had already asked in a letter to the ANWV in 1877, which had been left unan-swered.Ansing answered the questions extensively.Figure 1: English translation of an example fragment from the FDN set.This is illustrated by Figure 2, where the number oftimes each person?s name is mentioned within theBWSA is compared to the number of times he orshe is mentioned on the Web.We collect all possible combinations of each per-son?s first names, initials and surnames (some areknown by multiple surnames) and their aliases fromthe database and get the number of hits, i.e.
the num-ber of articles or webpages that contain the name, byquerying the BWSA and Yahoo!.
For each we derive6 scores:?
A-B: the maximum hit count of all combina-tions of A ?
B divided by the maximum hitcount of A;?
A-B(25): the maximum hit count of all combi-nations of A ?
B within 25 words divided bythe maximum hit count of A;?
B-A: the maximum hit count of all combina-tions of A ?
B divided by the maximum hitcount of B;?
B-A(25): the maximum hit count of all combi-nations of A ?
B within 25 words divided bythe maximum hit count of B;?
AB: the maximum hit count of all combinationsof A ?
B divided by the maximum hit count ofA plus the maximum hit count of B;?
AB(25): the maximum hit count of all combina-tions of A ?
B within 25 words divided by themaximum hit count of A plus the maximum hitcount of B.0 100 200 300 400 500 60000.20.40.60.81PersonsOccurrenceBWSAYahoo!Figure 2: Fraction of maximum occurrence countfor all 574 persons in the BWSA and on Yahoo!.Set mention count: As an indication of the re-latedness more specific to the text fragment underconsideration, we add the number of times A or Bis mentioned in the 5-sentence-context of the frag-ment, and the number of sentences in which both Aand B are mentioned to the feature vector.Lexical features: Preliminary tests revealed thatkeeping lemmatized verbs and nouns provided thebest results, with mildly positive effects for prepo-sitions and person names.
All tokens outside thesecategories were not incorporated in the feature vec-tor.Person names are further processed in two ways:all mentions of person A and person B are replacedwith labels ?PER-A?
and ?PER-B?
; all names of otherpersons mentioned in the fragment are replaced withlabel ?PER-X?, where X is either the next available65letter in the alphabet (anonymous) or the person?sunique ID in the database (identified).We create four variants of both the generic dataset and the FDN data set: one that represents onlyverbs and nouns (VN), one that also includes prepo-sitions (VNPr), one that includes anonymous personnames (VNP-a) and a last one that includes identi-fied person names (VNP-i).
Each set is split into atraining and a test set of respectively 90% and 10%of the total size.
We test our system both with binaryfeatures and with tf.idf weighted features.5 Results and Evaluation5.1 Binary versus Tf.idfFigure 3 shows the 10-fold cross-validation accu-racy scores on the joint learning task for each of thetraining vector sets using binary and tf.idf weightedfeatures.
We take the majority class of the trainingset as our baseline.
In all cases we observe that un-weighted binary features outperform weighted fea-tures.
These results are in line with the findings ofPang et al(2002), who found that the occurrence ofa word is more important than its frequency in de-termining the sentiment of a text.Regarding the different feature sets, the additionof prepositions or person names, either anonymousor identified, does not have a significant effect on theresults.
Only for the VNP-a set the score is raisedfrom 47.86 % to 48.53 % by the inclusion of anony-mous person names.5.2 Co-occurrenceWe perform a second experiment to assess the influ-ence of adding any of the co-occurrence measuresto the feature vectors.
Figure 4 displays the resultsfor the VN set on its own and with inclusion of theset mention counts (M), the BWSA co-occurrencescores (B) and the Yahoo!
co-occurrence scores (Y).For the generic set, we observe in all cases thatthe co-occurrence measures have a negative effecton the overall score.
For the FDN set this is not al-ways the case.
The set mention counts slightly im-prove the score, though this is not significant.
Theremainder of the experiments is performed on thevectors without any co-occurrence scores.5.3 Joint Learning versus PipelineTable 2 lists the accuracy scores on the training setson both the joint learning task and the pipeline.
Onlyfor the FDN set does the system perform better onthe two-step task than on the single task.
In fact, theFDN set reaches an accuracy of 53.08 % in the two-step task, which is 6.55 % higher than the majorityclass baseline and the highest score so far.The system consistently performs better on thejoint learning task for the generic set.
Further in-vestigation into why the pipeline does not do wellon the generic set reveals that in the first step of thetask, where instances are classified on whether theydescribe a relation or not, all instances always getclassified as ?related?.
This immediately results in anerror rate of approximately 15%.
In the second step,when classifying relations into negative, neutral orpositive, we observe that in most cases the systemagain resorts to majority class voting and thus doesnot exceed the baseline.Even for the FDN set, where the pipeline doesoutperform the joint learning task, the difference inaccuracy between both tasks is minor (0.22-0.96 %).We conclude that it is preferable to approach ourclassification problem as a single, rather than a two-step task.
If the system already resorts to majorityclass voting in the first step, every occurrence of aname in a biography will be flagged as a relation,which is detrimental to the precision of the system.5.4 Generic versus FDNAlthough the classifiers trained on both sets do notperform particularly well, the FDN set provides agreater gain in accuracy over the baseline.
The sameis shown when we train the system on the trainingsets for both data sets and test them on the held outtest sets.
For the generic set, the VNP-a feature setprovides the best results.
It results in an accuracy of50% on the test set, with a baseline of 48.2%.For the FDN data set, none of the different fea-ture sets performs better than the others on the jointlearning task.
In testing, however, the VNP-a setproves to be most successful.
It results in an ac-curacy of 66.7%, which is a gain of 4.5% over thebaseline of 62.2%.To test how well each of the sets generalizes overthe entire community, we test both sets on each66baselineAccuracyGeneric set1020304050607044.17binaryVNVNPrVNP-aVNP-itf.idfVNVNPrVNP-aVNP-ibaselineFDN set1020304050607046.63binaryVNVNPrVNP-aVNP-itf.idfVNVNPrVNP-aVNP-iFigure 3: Binary versus weighted features.Generic set FDN setjoint pipeline baseline joint pipeline baselineVN 47.92 45.83 44.17 52.12 52.83 46.63VNPr 48.33 46.88 44.17 52.12 53.08 46.63VNP-a 48.54 46.88 44.17 52.12 52.34 46.63VNP-i 47.71 45.83 44.17 52.12 52.59 46.63Table 2: Accuracy scores on training sets (10-fold cross-validation) for both the joint learning task and thepipeline.other.
Training on the generic set and testing onthe FDN set results in an accuracy of 45.3% with abaseline of 48.2%.
Doing the same experiment viceversa results in an accuracy of 44.8% with a baselineof 44.6%.
Examining the output reveals that bothsystems resort to selecting the majority class (?posi-tive?)
in most cases.
The system that was trained onthe FDN set correctly selects the ?negative?
class in afew cases, but never classifies a fragment as ?neutral?or ?not related?.
The distribution of classes in theoutput of the generic system shows a bit more vari-ety: 0.2% is classified as ?negative?, 10.1% is classi-fied as ?neutral?
and 89.7% is classified as ?positive?.None of the fragments are classified as ?not related?.A possible explanation for this is the fact that the?not related?
fragments in the FDN set specificallydescribe situations where the main entity is not re-lated to Ferdinand Domela Nieuwenhuis; these frag-ments could still describe a relation from the mainentity to another person mentioned in the fragmentand therefore be miss-classified.5.5 EvaluationTo evaluate our system, we process the entireBWSA, extracting from each biography all frag-ments that mention a person from any of the otherbiographies.
We train the system on the best per-forming feature set of the generic data set, VNP-a.In order to filter out some of the errors, we removeall relations of which only one instance is found inthe BWSA.The resulting network is evaluated qualitativelyby a domain expert on a sample of the network.
Forthis we extracted the top-five friends and foes forfive persons.
Both rankings are based on the fre-quency of the relation in the system?s output.
Thelists of friends are judged to be mostly correct.
Thisis probably due to the fact that the positive relationis the majority class, to which the classifiers easilyrevert.The generated lists of foes are more controversial.Some of the lists contain names which are also in-cluded in the list of friends.
Of course, this is not67Accuracy102030405060701020304050607046.6344.17Generic setVNVN-MVN-BVN-YFDN setVNVN-MVN-BVN-YFigure 4: Comparison of co-occurrence features: M= set mention counts, B = BWSA co-occurrence, Y= Yahoo!
co-occurrence.necessarily a sign of bad system performance: wedo not count time as a factor in this experiment andrelationships are subject to change.
25% of the listedfoes are judged to be completely wrong by the expertjudge.
10% are not so much enemies of the mainentity, but did have known political disagreementswith them.
The remaining 65% are considered to beplausible as foes, though the expert would not haveplaced them in the top five.6 Discussion and Future ResearchOur case study has demonstrated that relations be-tween persons can be identified and labeled by theirpolarity at an above-baseline level, though the im-provements are minor.
Yet, the utility of the clas-sifications is visible in the higher-level task of con-structing a complete social network from all the clas-sified pairwise relations.
After filtering out relationswith only one attestation, a qualitative analysis by adomain expert on frequency-ranked top-five lists offriends and foes yielded mostly correct results on themajority class, ?positive?, and approximately 65%correct on the harder ?negative?
class.
If we wouldnot have used the classifier and guessed only the ma-jority ?positive?
class, we would not have been ableto build ranked lists of foes.In discussions with domain experts, several ex-tensions to our current annotation scheme have beenproposed, some of which may be learnable to someusable extent (i.e.
leading to qualitatively good la-belings in the overall social network) with machinelearning tools given sufficient annotated material.First, we plan to include more elaborate annotationsby domain experts that discriminate between typesof relationships, such as between family members,co-workers, or friends.
Second, relationships are ob-viously not static throughout time; their polarity andtype can change, and they have a beginning and anend.We aim at working with other machine learn-ing methods in future expansions of our experi-mental matrix, including the use of rule learningmethods because of their interpretable output.
An-other direction of research, related to the idea ofthe improved annotation levels, is the identifica-tion of sub-networks in the total social network.Arguably, certain sub-networks identify ideologi-cally like-minded people, and may correspond towhat eventually developed into organizations suchas workers unions or political organizations.
Whenwe are able to link automatically detected temporalexpressions to initializations, changes, and endingsof relationships, we may be able to have enough in-gredients for the automatic identification of large-scale events such as the emergence of a politicalmovement.ReferencesAntal van den Bosch, Bertjan Busser, Sander Canisiusand Walter Daelemans.
2007.
An efficient memory-based morphosyntactic tagger and parser for Dutch.Selected Papers of the 17th Computational Linguis-tics in the Netherlands Meeting, Leuven, Belgium, 99?114.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Aron Culotta, Andrew McCallum and Jonathan Betz.2006.
Integrating probabilistic extraction models anddata mining to discover relations and patterns in text.Proceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics (HLT-NAACL) 2006, 296?303.Walter Daelemans, Jakub Zavrel, Ko van der Sloot andAntal van den Bosch.
2010.
TiMBL: Tilburg Memory68Based Learner, version 6.3, Reference Guide.
ILKResearch Group Technical Report Series no.
10-01.David K. Elson, Nicholas Dames, Kathleen R. McKe-own.
2010.
Extracting social networks from literaryfiction.
Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics 2010, 138?147.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: Learning with many relevantfeatures.
Proceedings of ECML-98, 10th EuropeanConference on Machine Learning 1998, 137-142.Henry Kautz, Bart Selman and Mehul Shah.
1997.
Thehidden web.
AI Magazine, volume 18, number 2, 27?36.Soo-Min Kim and Eduard Hovy.
2006.
Automatic iden-tification of pro and con reasons in online reviews.Proceedings of the COLING/ACL Main ConferencePoster Sessions, 483?490.Yutaka Matsuo, Hironori Tomobe, Koiti Hasida and Mit-suru Ishizuka.
2004.
Finding social network for trustcalculation.
European Conference on Artificial Intel-ligence - ECAI 2004.Peter Mika.
2005.
Flink: Semantic web technology forthe extraction and analysis of social networks.
WebSemantics: Science, Services and Agents on the WorldWide Web, volume 3, number 2-3, 211?223.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), 79?86.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, vol.
2, number 1-2, 1?135.Deepak Ravichandran and Eduard Hovy.
2002.
LearningSurface Text Patterns for a Question Answering Sys-tem.
Proceedings of the 40th Annual Meeting on As-sociation for Computational Linguistics (ACL) 2002.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classifi-cation of reviews.
Proceedings of the Association forComputational Linguistics (ACL), 417-424.69
