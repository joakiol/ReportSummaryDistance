Proceedings of the SIGDIAL 2014 Conference, pages 22?31,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsSituated Language Understanding at 25 Miles per HourTeruhisa Misu, Antoine Raux?, Rakesh GuptaHonda Research Institute USA425 National AvenueMountain View, CA 94040tmisu@hra.comIan LaneCarnegie Mellon UniversityNASA Ames Research ParkMoffett Field, CA 93085AbstractIn this paper, we address issues in situ-ated language understanding in a rapidlychanging environment ?
a moving car.Specifically, we propose methods for un-derstanding user queries about specific tar-get buildings in their surroundings.
Unlikeprevious studies on physically situated in-teractions such as interaction with mobilerobots, the task is very sensitive to tim-ing because the spatial relation betweenthe car and the target is changing whilethe user is speaking.
We collected situatedutterances from drivers using our researchsystem, Townsurfer, which is embeddedin a real vehicle.
Based on this data, weanalyze the timing of user queries, spa-tial relationships between the car and tar-gets, head pose of the user, and linguis-tic cues.
Optimized on the data, our al-gorithms improved the target identificationrate by 24.1% absolute.1 IntroductionRecent advances in sensing technologies have en-abled researchers to explore applications that re-quire a clear awareness of the systems?
dynamiccontext and physical surroundings.
Such appli-cations include multi-participant conversation sys-tems (Bohus and Horvitz, 2009) and human-robotinteraction (Tellex et al., 2011; Sugiura et al.,2011).
The general problem of understanding andinteracting with human users in such environmentsis referred to as situated interaction.We address yet another environment, where sit-uated interactions takes place ?
a moving car.
Inthe previous work, we collected over 60 hours ofin-car human-human interactions, where driversinteract with an expert co-pilot sitting next to themin the vehicle (Cohen et al., 2014).
One of the?Currently with Lenovo.insights from the analysis on this corpus is thatdrivers frequently use referring expressions abouttheir surroundings.
(e.g.
What is that big buildingon the right?)
Based on this insight, we have de-veloped Townsurfer (Lane et al., 2012; Misu etal., 2013), a situated in-car intelligent assistant.Using geo-location information, the system cananswer user queries/questions that contain objectreferences about points-of-interest (POIs) in theirsurroundings.
We use driver (user) face orienta-tion to understand their queries and provide the re-quested information about the POI they are look-ing at.
We have previously demonstrated and eval-uated the system in a simulated environment (Laneet al., 2012).
In this paper, we evaluate its utilityin real driving situations.Compared to conventional situated dialog tasks,query understanding in our task is expected to bemore time sensitive, due to the rapidly changingenvironment while driving.
Typically, a car willmove 10 meters in one second while driving at 25mi/h.
So timing can be a crucial factor.
In addi-tion, it is not well understood what kind of linguis-tic cues are naturally provided by drivers, and theircontributions to situated language understandingin such an environment.
To the best of our knowl-edge, this is the first study that tackles the issue ofsituated language understanding in rapidly movingvehicles.In this paper, we first present an overview of theTownsurfer in-car spoken dialog system (Section2).
Based on our data collection using the sys-tem, we analyze user behavior while using the sys-tem focusing on language understanding (Section3).
Specifically, we answer the following researchquestions about the task and the system throughdata collection and analysis:1.
Is timing an important factor of situated lan-guage understanding?2.
Does head pose play an important role in lan-guage understanding?
Or is spatial distanceinformation enough?22Speech recognitionNatural language understandingGaze (Head-pose) estimation3) POI Posterior calculation            by Belief tracking1) (Candidate)     POI look-upMicrophoneDepth sensorSensors Sensor signal understanding POI identification (situated understanding)SpeechGaze2) POI score (prior) calculationUnderstanding result (POI with maximum posterior)Geo-location estimation Semantic         geo-spatial databaseGPSIMUGeo-locationFigure 1: System overview of TownsurferTable 1: Example dialog with TownsurferU1: What is that place.
(POI in gaze)S1: This is Specialty Cafe, a mid-scale coffeeshop that serves sandwiches.U2: What is its (POI in dialog history) rating.S2: The rating of Specialty Cafe is above av-erage.U3: How about that one on the left.
(POI located on the left)S3: This is Roger?s Deli, a low-priced restau-rant that serves American food.3.
What is the role of linguistic cues in this task?What kinds of linguistic cues do drivers nat-urally provide?Based on the hypothesis obtained from the analy-sis for these questions, we propose methods to im-prove situated language understanding (Section 4),and analyze their contributions based on the col-lected data (Sections 5 and 6).
We then clarify ourresearch contributions through discussion (Section7) and comparison with related studies (Section 8).2 Architecture and Hardware ofTownsurferThe system uses three main input modalities,speech, geo-location, and head pose.
Speech isthe main input modality of the system.
It is used totrigger interactions with the system.
User speechis recognized, then requested concepts/values areextracted.
Geo-location and head pose informa-tion are used to understand the target POI of theuser query.
An overview of the system with a pro-cess flow is illustrated in Figure 1 and an exam-ple dialog with the system is shown in Table 1.
Avideo of an example dialog is also attached.In this paper, we address issues in identify-ing user intended POI, which is a form of ref-erence resolution using multi-modal informationsources1.
The POI identification process consistsof the following three steps (cf.
Figure 1).
Thisis similar to but different from our previous workon landmark-based destination setting (Ma et al.,2012).1) The system lists candidate POIs based on geo-location at the timing of a driver query.
Rela-tive positions of POIs to the car are also cal-culated based on geo-location and the head-ing of the car.2).
Based on spatial linguistic cues in the userutterance (e.g.
to my right, on the left), a2D scoring function is selected to identify ar-eas where the target POI is likely to be.
Thisfunction takes into account the position of thePOI relative to the car, as well as driver headpose.
Scores for all candidate POIs are cal-culated.3) Posterior probabilities of each POI are cal-culated using the score of step 2 as prior,and non-spatial linguistic information (e.g.POI categories, building properties) as obser-vations.
This posterior calculation is com-puted using our Bayesian belief tracker calledDPOT (Raux and Ma, 2011).The details are explained in Section 4.System hardware consists of a 3D depth sen-sor (Primesense Carmine 1.09), a USB GPS (BU-353S4), an IMU sensor (3DM-GX3-25) and aclose talk microphone (plantronics Voyage Leg-1We do not deal with issues in language understandingrelated to dialog history and query type.
(e.g.
General infor-mation request such as U1 vs request about specific propertyof POI such as U2 in Table 1)23end UC).
These consumer grade sensors are in-stalled in our Honda Pilot experiment car.
Weuse Point Cloud Library (PCL) for the face direc-tion estimation.
Geo-location is estimated basedon Extended Kalman filter-based algorithm usingGPS and gyro information as input at 1.5 Hz.
Thesystem is implemented based on the Robot Oper-ating System ROS (Quigley et al., 2009).
Eachcomponent is implemented as a node of ROS, andcommunications between the nodes are performedusing the standard message passing mechanismsin ROS.3 Data Collection and Analysis3.1 Collection SettingWe collected data using a test route.
The routepasses through downtown Mountain View2andresidential area around Honda Research Institute.We manually constructed our database containing250 POIs (businesses such as restaurants, compa-nies) in this area.
Each database entry (POI) hasname, geo-location, category and property infor-mation explained in Section 3.4.
POI geo-locationis represented as a latitude-longitude pair (e.g.37.4010,-122.0539).
Size and shape of buildingsare not taken into account.
It takes about 30 min-utes to drive the route.
The major difference be-tween residential area and downtown is the POIdensity.
While each POI in downtown has on aver-age 7.2 other POIs within 50 meters, in residentialarea POIs have only 1.9 neighbors.
Speed limitsalso differ between the two (35 mi/h vs 25 mi/h).We collected data from 14 subjects.
They wereasked to drive the test route and make queriesabout surrounding businesses.
We showed a demovideo3of the system to the users before starting thedata collection.
We also told them that the objec-tive is a data collection for a situated spoken dia-log system, rather than the evaluation of the wholesystem.
We asked subjects to include the full de-scription of the target POI within a single utteranceto avoid queries whose understanding requires di-alog history information4.
Although the systemanswered based on the baseline strategy explainedin Section 4.1, we asked subjects to ignore the sys-tem responses.As a result, we collected 399 queries with avalid target POI.
Queries about businesses that do2We assumed that a POI is in downtown when it is locatedwithin the rectangle by geo-location coordinates (37.3902, -122.0827) and (37.3954, -122.0760).3not the attached one.4Understanding including dialog history information isour future work.POIxy ?face directiontargetdirectionHeadingdirectionFigure 2: Parameters used to calculate POI score(prior)?
:   rightX:   left+:   no cueDistance (m)yRightLeft Distance (m)xFigure 3: Target POI positionsnot exist on our database (typically a vacant store)were excluded.
The data contains 171 queries indowntown and 228 in residential area.
The querieswere transcribed and the user-intended POIs weremanually annotated by confirming the intendedtarget POI with the subjects after the data collec-tion based on a video taken during the drive.3.2 Analysis of Spatial Relation of POI andHead PoseWe first analyze the spatial relation between posi-tion cues (right/left) and the position of the user-intended target POIs Out of the collected 399queries, 237 (59.4%) of them contain either rightor left position cue (e.g.
What is that on the left?
).The relation between the position cues (cf.
Figure2) and POI positions at start-of-speech timing5isplotted in Figure 3.
The X-axis is a lateral distance(a distance in the direction orthogonal to the head-ing; a positive value means the right direction) andthe Y-axis is an axial distance (a distance in theheading direction; a negative value means the POIis in back of the car.
).
The most obvious findingfrom the scatter plot is that right and left are pow-5Specifically, the latest GPS and face direction informa-tion at that timing is used.24Table 2: Comparison of average and standard deviation of distance (in meter) of POI form the carASR result timing Start-of-speech timingPosition cue Site Ave dist.
Std dist.
Ave dist.
Std dist.Right/left Downtown 17.5 31.0 31.9 28.3Residential 22.0 36.3 45.2 36.5No right/left Downtown 17.4 27.8 31.1 26.5cue Residential 38.3 45.9 52.3 43.4Distance (m)y?angular difference (degree)Figure 4: Relation between POI positions andhead poseerful cues for the system to identify target POIs.We can also see that the POI position distributionhas a large standard deviation.
This is partly be-cause the route has multiple sites from downtownand residential area.
Interestingly, while the aver-age distance to the target POI in downtown is 37.0meters, that of residential area is 57.4 meters.We also analyze the relation between face di-rection and POI positions.
Figure 4 plots the re-lation between the axial distance and the angulardifference ?
(between the user face direction andthe target POI direction) (cf.
Figure 2).
The scat-ter plot suggests that the angular differences fordistant target POIs is often small.
For close targetPOIs the angular differences are larger and have alarge variance6.3.3 Analysis of TimingReferring expressions such as ?the building on theright?
must be resolved with respect to the contextin which the user intended.
However, in a movingcar, such a context (i.e.
the position of the car andthe situation in the surroundings) can be very dif-ferent between the time when the user starts speak-ing the sentence and the time they finish speakingit.
Therefore, situated understanding must be verytime sensitive.To confirm and investigate this issue, we ana-lyze the difference in the POI positions betweenthe time the ASR result is output vs the time theuser actually started speaking.
The hypothesis is6We will discuss the reason for this in Section 6.2.Table 3: User-provided linguistic cuesCategory of linguistic cue Percentageused (%)Relative position to the car (right/left) 59.4Business category (e.g.
restaurant, cafe) 31.8Color of the POI (e.g.
green, yellow) 12.8Cuisine (e.g.
Chinese, Japanese, Mexican) 8.3Equipments (e.g.
awning, outside seating) 7.2Relative position to the road (e.g.
corner) 6.5that the latter yields a more accurate context inwhich to interpret the user sentence.
In contrast,our baseline system uses the more straightforwardapproach of resolving expressions using the con-text at the time of resolution, i.e.
whenever theASR/NLU has finished processing an utterance(hereafter ?ASR results timing?
).Specifically, we compare the average axial dis-tance to the target POIs and its standard deviationbetween these two timings.
Table 2 lists these fig-ures broken down by position cue types and sites.The average axial distance from the car to the tar-get POIs is often small at the ASR result timing,but the standard deviation is generally small at thestart-of-speech timing.
This indicates that the tar-get POI positions at the start-of-speech timing ismore consistent across users and sentence lengthsthan that at the ASR result timing.
This result indi-cates the presence of a better POI likelihood func-tion using the context (i.e.
car position and orien-tation) at the start-of-speech timing than using theASR result timing.3.4 Analysis of Linguistic CuesWe then analyze the linguistic cues provided bythe users.
Here, we focus on objective and sta-ble cues.
We exclude subjective cues (e.g.
big,beautiful, colorful) and cues that might change ina short period of time (e.g.
with a woman dressedin green in front).
We have categorized the linguis-tic cues used to describe the target POIs.
Table 3lists the cue types and the percentage of user utter-ances containing each cue type.The cues that the users most often provided con-cern POI position related to the car (right and left).Nearly 60% of queries included this type of cueand every subject provided it at least once.
Thesecond most frequent cue is category of business,especially in downtown.
Users also provided col-25ors of POIs.
Other cues include cuisine, equip-ments, relative position to the road (e.g.
on thecorner).Another interesting finding from the analysis isthat the users provided more linguistic cues withincreasing candidate POIs in their field of view.Actually, the users provided 1.51 categories in av-erage per query in downtown, while they provided1.03 categories in residential area.
(cf.
POI den-sity in Section 3.2: 7.2 vs 1.9) This indicates thatusers provide cues considering environment com-plexity.4 Methods for Situated LanguageUnderstanding4.1 Baseline StrategyWe use our previous version (Misu et al., 2013)as the baseline system for situated language un-derstanding.
The baseline strategy consists of thefollowing three paragraphs, which correspond tothe process 1)-3) in Section 2 and Figure 1.The system makes a POI look-up based on thegeo-location information at the time ASR resultis obtained.
The search range of candidate POIsis within the range (relative geo-location of POIsagainst the car location) of -50 to 200 meters inthe travelling direction and 100 meters to the leftand 100 meters to the right in the lateral direction.The ASR result timing is also used to measure thedistances to the candidate POIs.POI priors are calculated based on the distancefrom the car (= axial distance) based on ?the closerto the car the likely?
principle.
We use a likelihoodfunction inversely proportional to the distance.
Weuse position cues simply to remove POIs from alist of candidates.
For example ?right?
positioncue is used to remove candidate POIs that are lo-cated on< 0 position in the lateral distance.
Whenno right/left cue is provided, POIs outside of 45degrees from the face direction are removed fromthe list of candidates.No linguistic cues except right/left are used tocalculate POI posterior probabilities.
So, the sys-tem selects the POI with the highest prior (POIscore) as the language understanding result.4.2 Strategies Toward Better SituatedLanguage UnderstandingTo achieve better situated language understanding(POI identification) based on the findings of theanalysis in Section 3, we modify steps 1)-3) as fol-lows:1.
Using start-of-speech timing for the POIprior calculationDistance (m)y?
:   rightX:   leftRightLeft Distance (m)xFigure 5: Example GMM fitting2.
Gaussian mixture model (GMM)-based POIprobability (prior) calculation3.
Linguistic cues for the posterior calculation.We use the start-of-speech timing instead of thetime ASR result is output.
Because the standarddeviations of the POI distances are small (cf.
Sec-tion 3.2), we expect that a better POI probabilityscore estimation with the POI positions at this tim-ing in the subsequent processes than the positionsat the ASR result timing.
The POI look-up rangeis the same as the baseline.We apply Gaussian mixture model (GMM) withdiagonal covariance matrices over the input pa-rameter space.
The POI probability (prior) is cal-culated based on these Gaussians.
We use two in-put parameters of the lateral and axial distances forqueries with right/left cue, and three parameters ofthe lateral and axial distances and the differencein degree between the target and head pose direc-tions for queries without right/left cue.
(The effectof the parameters is discussed later in Section 6.2.
)We empirically set the number of Gaussian com-ponents to 2.
An example GMM fitting to the POIpositions for queries with right and left cues is il-lustrated in Figures 5.
The center of ellipse is themean of the Gaussian.We use the five linguistic cue categories of Sec-tion 3.4 for the posterior calculation by the belieftracker.
In the following experiments, we use ei-ther 1 or 0 as a likelihood of natural language un-derstanding (NLU) observation.
The likelihoodfor the category value is 1 if a user query (NLUresult) contains the target value, otherwise 0.
Thiscorresponds to a strategy of simply removing can-didate POIs that do not have the category valuesspecified by the user.
Here, we assume a clean POIdatabase with all their properties annotated manu-ally.26Table 4: Comparison of POI identification rateMethod Successrate (%)right/left linguistic cues,the-closer-the-likely likelihood, 43.1ASR result timing) (Baseline)1) Start-of-speech timing 42.92) GMM-based likelihood 47.93) Linguistic cues 54.61) + 2) 50.61) + 3) 54.42) + 3) 62.21) + 2) + 3) 67.25 ExperimentsWe use manual transcriptions and natural languageunderstanding results of the user queries to focusour evaluations on the issues listed in Section 1.We evaluate the situated language understanding(POI identification) performance based on crossvalidation.
We use the data from 13 users to trainGMM parameters and to define a set of possiblelinguistic values, and the data from the remaininguser for evaluation.
We train the model parametersof the GMM using the EM algorithm.
Knowledgeabout the sites (downtown or residential area) isnot used in the training7.We do not set a threshold for the presentation.We judge the system successfully understands auser query when the posterior of the target (user-intended) POI is the highest.
The chance rate,given by the average of the inverse number of can-didate POIs in the POI look-up is 10.0%.6 Analysis of the ResultsWe first analyze the effect of our three methodsdescribed in Section 4.2.
The results are listed inTable 4.Simply using the POI positions at the start-of-speech timing instead of those of the ASR resulttiming did not lead to an improvement.
This re-sult is reasonable because the distances to targetPOIs are often smaller at the ASR result timingas we showed in Table 2.
However, we achieveda better improvement (7.5% over the baseline) bycombining it with the GMM-based likelihood cal-culation.
The results supports our Section 3.3 hy-pothesis that the POI position is less dependenton users/scenes at the start-of-speech timing.
Thelinguistic cues were the most powerful informa-7The performance was better when the knowledge was notused.ConfusionLinguistic cueLocalization errorUser errorFigure 6: Breakdown of error causestion for this task.
The improvement over the base-line was 11.5%.
By using these three methods to-gether, we obtained more than additive improve-ment of 24.1% in the POI identification rate overthe baseline8.
The success rates per site were60.8% in downtown and 71.9% in residential area.6.1 Error AnalysisTo analyze the causes of the remaining errors, wehave categorized the errors into the following fourcategories:1.
Ambiguous references: There were multi-ple POIs that matched the user query.
(e.g.another yellow building sat next to the target)2.
Linguistic cue: The driver used undefinedlinguistic cues such subjective expressions ordynamic references objects (e.g.
optometrist,across the street, colorful)3.
Localization error: Errors in estimatinggeo-location or heading of the car.4.
User error: There were errors in the userdescriptions (e.g.
user misunderstood theneighbor POI?s outside seating as the tar-get?s)The distribution of error causes is illustrated inFigure 6.
More than half of the errors are dueto reference ambiguity.
These errors are expectedto be resolved through clarification dialogs.
(e.g.asking user ?Did you mean the one in front orback??)
Linguistic errors might be partly resolvedby using a better database with detailed categoryinformation.
For dynamic references and subjec-tive cues, use of image processing techniques willhelp.
Localization errors can be solved by usinghigh-quality GPS and IMU sensors.
User errorswere rare and only made in downtown.6.2 Breakdown of Effect of the SpatialDistance and Head PoseWe then evaluate the features used for the POIprior calculation to investigate the effect of the in-put parameters of the lateral and axial distances8For reference, the performances of ?1) + 2) + 3)?
were62.9%, 67.2%, 66.1%, 67.2%, and 66.2% when the numberof Gaussian components were 1, 2, 3, 4, and 5.27Table 5: Relation between the parameters used forthe POI identification and success rates (%)query typeparameters used right/left no cuelateral (x) distance 58.6 51.2axial (y) distance 59.5 53.7face direction 43.3 44.4lateral + axial (x+ y) 73.8 54.3lateral (x) + face direction 57.8 48.1axial (y) + face direction 59.1 54.9lateral + axial + face 68.4 57.4and the difference in degree between the targetand user face direction angles.
Table 5 lists therelationship between the parameters used for theGMM-based likelihood calculation and the POIidentification performances9.The results indicate that the axial distance isthe most important parameter.
We got a slightimprovement by using the face direction informa-tion for the queries without right/left cue, but theimprovement was not significant.
On the otherhand, use of face direction information for theright/left queries clearly degraded the POI iden-tification performance.
We think this is becausethe users finished looking at the POI and returnedthe face to the front when they started speaking,thus they explicitly provided right/left informationto the system.
However, we believe that using along-term trajectory of the user face direction willcontribute to an improve in the POI identificationperformance.6.3 Breakdown of the Effect of LinguisticCuesWe then evaluate the effect of the linguistic cuesper category.
Table 6 lists the relationship betweenthe categories used for the posterior calculationand the success rates.
There is a strong correlationbetween the frequency of the cues used (cf.
Table3) and their contributions to the improvement insuccess rate.
For example, business category in-formation contributed the most, boosting the per-formance by 8.5%.Another point we note is that the contribution ofbusiness category and cuisine categories is large.Because other categories (e.g.
color) are not read-ily available in a public POI database (e.g.
GooglePlaces API, Yelp API), we can obtain reasonableperformance without using a special database or9Note that, we first determine the function to calculatePOI scores (priors) based on the position cues, then calculatescores with the selected function.Table 6: Effect of linguistic cueslinguistic cue Successcategory used rate (%)No linguistic cues (*) 50.6(*) + Business category (e.g.
cafe) 59.1(*) + Color of the POI (e.g.
green) 57.6(*) + Cuisine (e.g.
Chinese) 54.1(*) + Equipments (e.g.
awning) 53.9(*) + Relative position (e.g.
corner) 51.4image processing.We also found that linguistic cues were espe-cially effective in downtown.
Actually, while theimprovement10was 20.0% in downtown that forresidential area was 14.4%.
This mainly would bebecause the users provided more linguistic cues indowntown considering the difficulty of the task.6.4 Using Speech Recognition ResultsWe evaluate the degradation by using automaticspeech recognition (ASR) results.
We use GoogleASR11and Julius (Kawahara et al., 2004) speechrecognition system with a language model trainedfrom 38K example sentences generated from agrammar.
An acoustic model trained from theWSJ speech corpus is used.
Note that they arenot necessarily the best system for this domain.Google ASR uses a general language model fordictation and Julius uses a mismatched acousticmodel in terms of the noise condition.The query success rate was 56.3% for Julius and60.3% for Google ASR.
We got ASR accuraciesof 77.9% and 80.4% respectively.
We believe theperformance will improve when N-best hypothe-ses with confidence scores are used in the posteriorcalculating using the belief tracker.7 DiscussionThe main limitation of this work comes from thesmall amount of data that we were able to collect.It is not clear how the results obtained here wouldgeneralize to other sites, POI density, velocitiesand sensor performances.
Also, results might de-pend on experimental conditions, such as weather,hour, season.
Hyper-parameters such as the opti-mal number of Gaussian components might haveto be adapted to different situations.
We there-fore acknowledge that the scenes we experimentedare only a limited cases of daily driving activities.101) + 2) vs 1) + 2) + 3).11Although it is not realistic to use cloud-based speechrecognition system considering the current latency, we usethis as a reference system.28However, the methods we propose are general andour findings should be verifiable without loss ofgenerality by collecting more data and using moreinput parameters (e.g.
velocity) for the POI priorcalculation.In addition, much future work remains to realizea natural interaction with the system, such as tak-ing into account dialog history and selecting opti-mal system responses.
On the other hand, we be-lieve this is one of the best platform to investigatesituated interactions.
The major topics that we aregoing to tackle are:1.
Dialog strategy: Dialog strategy and systemprompt generation for situated environmentsare important research topics, especially toclarify the target when there is ambiguity asmentioned in Section 6.1.
The topic will in-clude an adaptation of system utterances (en-trainment) to the user (Hu et al., 2014).2.
Eye tracker: Although we believe head poseis good enough to estimate user intentions be-cause we are trained to move the head in driv-ing schools to look around to confirm safety,we would like to confirm the difference inthis task between face direction and eye-gaze.3.
POI identification using face direction trajec-tory: Our analysis showed that the use of facedirection sometimes degrades the POI identi-fication performance.
However, we believethat using a trajectory of face direction willchange the result.4.
Database: We assumed a clean and perfectdatabase but we are going to evaluate the per-formance when noisy database is used.
(e.g.A database based on image recognition re-sults or user dialog log.)5.
Feedback: Koller et al.
(2012) demonstratedreferential resolution is enhanced by givinggaze information feedback to the user.
Wewould like to analyze the effect of feedbackwith an automotive augmented reality envi-ronment using our 3D head-up display (Ng-Thow-Hing et al., 2013).8 Related WorkThe related studies include a landmark-based nav-igation that handles landmarks as information fora dialog.
Similar system concepts have beenprovided for pedestrian navigation situations (Ja-narthanam et al., 2013; Hu et al., 2014), they donot handle a rapidly changing environment.Several works have used timing to enhancenatural interaction with systems.
Rose andHorvitz (2003) and Raux and Eskenazi (2009)used timing information to detect user barge-ins.Studies on incremental speech understanding andgeneration (Skantze and Hjalmarsson, 2010; Deth-lefs et al., 2012) have proved that real-time feed-back actions have potential benefits for users.Komatani et al.
(2012) used user speech timingagainst user?s previous and system?s utterancesto understand the intentions of user utterances.While the above studies have handled timing fo-cusing on (para-)linguistic aspect, our work han-dles timing issues in relation to the user?s physicalsurroundings.Recent advancements in gaze and face directionestimation have led to better user behavior under-standing.
There are a number of studies that haveanalyzed relationship between gaze and user in-tention, such as user focus (Yonetani et al., 2010),preference (Kayama et al., 2010), and referenceexpression understanding (Koller et al., 2012), be-tween gaze and turn-taking (Jokinen et al., 2010;Kawahara, 2012).
Nakano et al.
(2013) used facedirection for addressee identification.
The previ-ous studies most related to ours are reference res-olution methods by Chai and Prasov (2010), Iidaet al.
(2011) and Kennington et al.
(2013).
Theyconfirmed that the system?s reference resolutionperformance is enhanced by taking the user?s eyefixation into account.
However, their results arenot directly applied to an interaction in a rapidlychanging environment while driving, where eyefixations are unusual activities.Marge and Rudnicky (2010) analyzed the effectof space and distance for spatial language under-standing for a human-robot communication.
Ourtask differs with this because we handle a rapidlychanging environment.
We believe we can im-prove our understanding performance based ontheir findings.9 ConclusionWe addressed situated language understanding ina moving car.
We focused on issues in understand-ing user language of timing, spatial distance, andlinguistic cues.
Based on the analysis of the col-lected user utterances, we proposed methods of us-ing start-of-speech timing for the POI prior calcu-lation, GMM-based POI probability (prior) calcu-lation, and linguistic cues for the posterior calcula-tion to improve the accuracy of situated languageunderstanding.
The effectiveness of the proposedmethods was confirmed by achieving a significantimprovement in a POI identification task.2910 AcknowledgmentsThe authors would like to thank Yi Ma at OhioState University for his contributions to the devel-opment of HRItk.ReferencesD.
Bohus and E. Horvitz.
2009.
Models for Multi-party Engagement in Open-World Dialog.
In Proc.SIGDIAL, pages 225?234.J.
Chai and Z. Prasov.
2010.
Fusing eye gaze withspeech recognition hypotheses to resolve exophoricreference in situated dialogue.
In Proc.
EMNLP.D.
Cohen, A. Chandrashekaran, I.
Lane, and A. Raux.2014.
The hri-cmu corpus of situated in-car interac-tions.
In Proc.
IWSDS, pages 201?212.N.
Dethlefs, H. Hastie, V. Rieser, and O.
Lemon.
2012.Optimising incremental dialogue decisions using in-formation density for interactive systems.
In Proc.EMNLP, pages 82?93.Z.
Hu, G. Halberg, C. Jimenez, and M. Walker.
2014.Entrainment in pedestrian direction giving: Howmany kinds of entrainment?
In Proc.
IWSDS, pages90?101.R.
Iida, M. Yasuhara, and T. Tokunaga.
2011.
Multi-modal reference resolution in situated dialogue byintegrating linguistic and extra-linguistic clues.
InProc.
IJCNLP, pages 84?92.S.
Janarthanam, O.
Lemon, X. Liu, P. Bartie, W. Mack-aness, and T. Dalmas.
2013.
A multithreaded con-versational interface for pedestrian navigation andquestion answering.
In Proc.
SIGDIAL, pages 151?153.K.
Jokinen, M. Nishida, and S. Yamamoto.
2010.
Oneye-gaze and turn-taking.
In Proc.
EGIHMI.T.
Kawahara, A. Lee, K. Takeda, K. Itou, andK.
Shikano.
2004.
Recent Progress of Open-SourceLVCSR Engine Julius and Japanese Model Reposi-tory.
In Proc.
ICSLP, volume IV.T.
Kawahara.
2012.
Multi-modal sensing and analysisof poster conversations toward smart posterboard.
InProc.
SIGDIAL.K.
Kayama, A. Kobayashi, E. Mizukami, T. Misu,H.
Kashioka, H. Kawai, and S. Nakamura.
2010.Spoken Dialog System on Plasma Display Panel Es-timating User?s Interest by Image Processing.
InProc.
1st International Workshop on Human-CentricInterfaces for Ambient Intelligence (HCIAmi).C.
Kennington, S. Kousidis, and D. Schlangen.
2013.Interpreting situated dialogue utterances: an updatemodel that uses speech, gaze, and gesture informa-tion.
In Proc.
SIGDIAL.A.
Koller, K. Garoufi, M. Staudte, and M. Crocker.2012.
Enhancing referential success by trackinghearer gaze.
In Proc.
SIGDIAL, pages 30?39.K.
Komatani, A. Hirano, and M. Nakano.
2012.
De-tecting system-directed utterances using dialogue-level features.
In Proc.
Interspeech.I.
Lane, Y. Ma, and A. Raux.
2012.
AIDAS - Immer-sive Interaction within Vehicles.
In Proc.
SLT.Y.
Ma, A. Raux, D. Ramachandran, and R. Gupta.2012.
Landmark-based location belief tracking ina spoken dialog system.
In Proc.
SIGDIAL, pages169?178.M.
Marge and A. Rudnicky.
2010.
Comparing Spo-ken Language Route Instructions for Robots acrossEnvironment Representations.
In Proc.
SIGDIAL,pages 157?164.T.
Misu, A. Raux, I.
Lane, J. Devassy, and R. Gupta.2013.
Situated multi-modal dialog system in vehi-cles.
In Proc.
Gaze in Multimodal Interaction, pages25?28.Y.
Nakano, N. Baba, H. Huang, and Y. Hayashi.2013.
Implementation and evaluation of a multi-modal addressee identification mechanism for mul-tiparty conversation systems.
In Proc.
ICMI, pages35?42.V.
Ng-Thow-Hing, K. Bark, L. Beckwith, C. Tran,R.
Bhandari, and S. Sridhar.
2013.
User-centeredperspectives for automotive augmented reality.
InProc.
ISMAR.M.
Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote,J.
Leibs, R. Wheeler, and A. Ng.
2009.
ROS:an open-source Robot Operating System.
In Proc.ICRA Workshop on Open Source Software.A.
Raux and M. Eskenazi.
2009.
A Finite-state Turn-taking Model for Spoken Dialog Systems.
In Proc.HLT/NAACL, pages 629?637.A.
Raux and Y. Ma.
2011.
Efficient probabilistic track-ing of user goal and dialog history for spoken dialogsystems.
In Proc.
Interspeech, pages 801?804.R.
Rose and H. Kim.
2003.
A hybrid barge-in proce-dure for more reliable turn-taking in human-machinedialog systems.
In Proc.
Automatic Speech Recog-nition and Understanding Workshop (ASRU), pages198?203.G.
Skantze and A. Hjalmarsson.
2010.
Towards incre-mental speech generation in dialogue systems.
InProc.
SIGDIAL, pages 1?8.K.
Sugiura, N. Iwahashi, H. Kawai, and S. Nakamura.2011.
Situated spoken dialogue with robots usingactive learning.
Advance Robotics, 25(17):2207?2232.30Table 7: Example user utterances- What is that blue restaurant on the right?- How about this building to my right with outside seating?- What is that Chinese restaurant on the left?- Orange building to my right.- What kind of the restaurant is that on the corner?- The building on my right at the corner of the street.- What about the building on my right with woman with a jacket in front- Do you know how good is this restaurant to the left?- Townsurfer, there is an interesting bakery what is that?- Is this restaurant on the right any good?S.
Tellex, T. Kollar, S. Dickerson, M. Walter, A. Baner-jee, S. Teller, and N. Roy.
2011.
Understanding nat-ural language commands for robotic navigation andmobile manipulation.
In Proc.
AAAI.R.
Yonetani, H. Kawashima, T. Hirayama, and T. Mat-suyama.
2010.
Gaze probing: Event-based estima-tion of objects being focused on.
In Proc.
ICPR,pages 101?104.11 AppendixTest route:https://www.google.com/maps/preview/dir/Honda+Research+Institute,+425+National+Ave+%23100,+Mountain+View,+CA+94043/37.4009909,-122.0518957/37.4052337,-122.0565795/37.3973374,-122.0595982/37.4004787,-122.0730021/Wells+Fargo/37.4001639,-122.0729708/37.3959193,-122.0539449/37.4009821,-122.0540093/@37.3999836,-122.0792529,14z/data=!4m21!4m20!1m5!1m1!1s0x808fb713c225003d:0xcf989a0bb230e5c0!2m2!1d-122.054006!2d37.401016!1m0!1m0!1m0!1m0!1m5!1m1!1s0x0:0x86ca9ba8a2f15150!2m2!1d-122.082546!2d37.388722!1m0!1m0!1m0!3e031
