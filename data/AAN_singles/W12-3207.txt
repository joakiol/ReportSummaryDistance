Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 66?75,Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational LinguisticsApplying Collocation Segmentation to the ACL Anthology Reference CorpusVidas Daudaravic?iusVytautas Magnus University / Vileikos 8, Lithuaniav.daudaravicius@if.vdu.ltAbstractCollocation is a well-known linguistic phe-nomenon which has a long history of researchand use.
In this study I employ collocationsegmentation to extract terms from the largeand complex ACL Anthology Reference Cor-pus, and also briefly research and describethe history of the ACL.
The results of thestudy show that until 1986, the most signifi-cant terms were related to formal/rule basedmethods.
Starting in 1987, terms related tostatistical methods became more important.For instance, language model, similarity mea-sure, text classification.
In 1990, the termsPenn Treebank, Mutual Information , statis-tical parsing, bilingual corpus, and depen-dency tree became the most important, show-ing that newly released language resources ap-peared together with many new research areasin computational linguistics.
Although PennTreebank was a significant term only tem-porarily in the early nineties, the corpus is stillused by researchers today.
The most recentsignificant terms are Bleu score and semanticrole labeling.
While machine translation as aterm is significant throughout the ACL ARCcorpus, it is not significant for any particu-lar time period.
This shows that some termscan be significant globally while remaining in-significant at a local level.1 IntroductionCollocation is a well-known linguistic phenomenonwhich has a long history of research and use.
Theimportance of the collocation paradigm shift israised in the most recent study on collocations (Sere-tan, 2011).
Collocations are a key issue for tasks likenatural language parsing and generation, as well asreal-life applications such as machine translation, in-formation extraction and retrieval.
Collocation phe-nomena are simple, but hard to employ in real tasks.In this study I introduce collocation segmentation asa language processing method, maintaining simplic-ity and clarity of use as per the n-gram approach.
Inthe beginning, I study the usage of the terms collo-cation and segmentation in the ACL Anthology Ref-erence Corpus (ARC), as well as other related termssuch as word, multi-word, and n-gram.
To evaluatethe ability of collocation segmentation to handle dif-ferent aspects of collocations, I extract the most sig-nificant collocation segments in the ACL ARC.
Inaddition, based on a ranking like that of TF -IDF ,I extract terms that are related to different phenom-ena of natural language analysis and processing.
Thedistribution of these terms in ACL ARC helps to un-derstand the main breakpoints of different researchareas across the years.
On the other hand, there wasno goal to make a thorough study of the methodsused by the ACL ARC, as such a task is complexand prohibitively extensive.2 ACL Anthology Reference CorpusThis study uses the ACL ARC version 20090501.The first step was to clean and preprocess the corpus.First of all, files that were unsuitable for the analysiswere removed.
These were texts containing charac-ters with no clear word boundaries, i.e., each charac-ter was separated from the next by whitespace.
Thisproblem is related to the extraction of text from .pdf66format files and is hard to solve.
Each file in theACL ARC represents a single printed page.
The filename encodes the document ID and page number,e.g., the file name C04-1001 0007.txt is made up offour parts: C is the publication type, (20)04 is theyear, 1001 is the document ID, and 0007 is the pagenumber.
The next step was to compile files of thesame paper into a single document.
Also, headersand footers that appear on each document page wereremoved, though they were not always easily rec-ognized and, therefore, some of them remained.
Afew simple rules were then applied to remove linebreaks, thus keeping each paragraph on a single line.Finally, documents that were smaller than 1 kB werealso removed.
The final corpus comprised 8,581files with a total of 51,881,537 tokens.3 Terms in the ACL ARC related tocollocationsThe list of terms related to the term collocationcould be prohibitively lengthy and could includemany aspects of what it is and how it is used.
Forsimplicitys sake, a short list of related terms, includ-ing word, collocation, multiword, token, unigram,bigram, trigram, collocation extraction and segmen-tation, was compiled.
Table 2 shows when theseterms were introduced in the ACL ARC: some termswere introduced early on, others more recently.
Theterm collocation was introduced nearly 50 years agoand has been in use ever since.
This is not unex-pected, as collocation phenomena were already be-ing studied by the ancient Greeks (Seretan, 2011).Table 2 presents the first use of terms, showing thatthe terms segmentation, collocation and multiwordare related to a similar concept of gathering consec-utive words together into one unit.Term Count Documents Introduced inword 218813 7725 1965segmentation 11458 1413 1965collocation 6046 786 1965multiword 1944 650 1969token 3841 760 1973trigram 3841 760 1973/87bigram 5812 995 1988unigram 2223 507 1989collocation extraction 214 57 1992Table 1: Term usage in ACL ARCWhile the term collocation has been used formany years, the first attempt to define what a col-location is could be related to the time period whenstatistics first began to be used in linguistics heavily.Until that time, collocation was used mostly in thesense of an expression produced by a particular syn-tactic rule.
The first definition of collocation in ACLARC is found in (Cumming, 1986).
(Cumming, 1986): By ?collocation?
I mean lex-ical restrictions (restrictions which are not pre-dictable from the syntactic or semantic properties ofthe items) on the modifiers of an item; for example,you can say answer the door but not answer thewindow.
The phenomenon which I?ve called col-location is of particular interest in the context of apaper on the lexicon in text generation because thisparticular type of idiom is something which a gener-ator needs to know about, while a parser may not.It is not the purpose of this paper to provide a def-inition of the term collocation, because at the mo-ment there is no definition that everybody wouldagree upon.
The introduction of unigrams, bigramsand trigrams in the eighties had a big influence onthe use of collocations in practice.
N -grams, asa substitute to collocations, started being used in-tensively and in many applications.
On the otherhand, n-grams are lacking in generalization capabil-ities and recent research tends to combine n-grams,syntax and semantics (Pecina, 2005) .The following sections introduce collocation seg-mentation and apply it to extracting the most signif-icant collocation segments to study the main break-points of different research areas in the ACL ARC.4 Collocation SegmentationThe ACL ARC contains many different segmenta-tion types: discourse segmentation (Levow, 2004),topic segmentation (Arguello and Rose, 2006), textsegmentation (Li and Yamanishi, 2000), Chinesetext segmentation (Feng et al, 2004), word segmen-tation (Andrew, 2006).
Segmentation is performedby detecting boundaries, which may also be of sev-eral different types: syllable boundaries (Mu?ller,2006), sentence boundaries (Liu et al, 2004), clauseboundaries (Sang and Dejean, 2001), phrase bound-aries (Bachenko and Fitzpatrick, 1990), prosodicboundaries (Collier et al, 1993), morpheme bound-67Term Source and Citationword (Culik, 1965) : 3.
Translation ?word by word?
.
?Of the same simplicity and uniqueness is the decomposition of the sentence S in itssingle words w1 , w2 , ..., wk separated by interspaces, so that it is possible to writes = (w1 w2 ... wk ) like at the text.
?A word is the result of a sentence decomposition.segmentation (Sakai, 1965): The statement ?x is transformed to y?
is a generalization of the originalfact, and this generalization is not always true.
The text should be checked before atransformational rule is applied to it.
Some separate steps for this purpose will savethe machine time.
(1) A text to be parsed must consist of segments specified by therule.
The correct segmentation can be done by finding the tree structure of the text.Therefore, the concatenation rules must be prepared so as to account for the structureof any acceptable string.Collocation (Tosh, 1965): We shall include features such as lexical collocation (agent-actionagreement) and transformations of semantic equivalence in a systematic descriptionof a higher order which presupposes a morpho-syntactic description for each lan-guage [8, pp.
66-71].
The following analogy might be drawn: just as strings ofalphabetic and other characters are taken as a body of data to be parsed and classifiedby a phrase structure grammar, we may regard the string of rule numbers generatedfrom a phrase structure analysis as a string of symbols to be parsed and classified in astill higher order grammar [11; 13, pp.
67-83], for which there is as yet no universallyaccepted nomenclature.multi-word (Yang, 1969): When title indices and catalogs, subject indices and catalogs, businesstelephone directories, scientific and technical dictionaries, lexicons and idiom-and-phrase dictionaries, and other descriptive multi-word information are desired, thefirst character of each non-trivial word may be selected in the original word sequenceto form a keyword.
For example, the rather lengthy title of this paper may have akeyword as SADSIRS.
Several known information systems are named exactly in thismanner such as SIR (Raphael?s Semantic Information Retrieval), SADSAM (Lind-say?s Sentence Appraiser and Diagrammer and Semantic Analyzing Machine), BIRS(Vinsonhaler?s Basic Indexing and Retrieval System), and CGC (Klein and Simmons?Computational Grammar Coder).token (Beebe, 1973): The type/token ratio is calculated by dividing the number of discreteentries by the total number of syntagms in the row.trigram (Knowles, 1973): sort of phoneme triples (trigrams), giving list of clusters and third-order information-theoretic values.
(D?Orta et al, 1987): Such a model it called trigram language model.
It is basedon a very simple idea and, for this reason, its statistics can be built very easily onlycounting all the sequences of three consecutive words present in the corpus.
On theother hand, its predictive power is very high.bigram (van Berkelt and Smedt, 1988): Bigrams are in general too short to contain anyuseful identifying information while tetragrams and larger n-gram are already closeto average word length.
(Church and Gale, 1989): Our goal is to develop a methodology for extending ann-gram model to an (n+l)-gram model.
We regard the model for unigrams as com-pletely fixed before beginning to study bigrams.unigram the same as bigram for (Church and Gale, 1989)collocationextraction(McKeown et al, 1992): Added syntactic parser to Xtract, a collocation extractionsystem, to further filter collocations produced, eliminating those that are not consis-tently used in the same syntactic relation.Table 2: Terms introductions in ACL ARC.aries (Monson et al, 2004), paragraph boundaries(Filippova and Strube, 2006), word boundaries (Ryt-ting, 2004), constituent boundaries (Kinyon, 2001),topic boundaries (Tur et al, 2001).Collocation segmentation is a new type of seg-mentation whose goal is to detect fixed word se-quences and to segment a text into word sequencescalled collocation segments.
I use the definition ofa sequence in the notion of one or more.
Thus, acollocation segment is a sequence of one or moreconsecutive words that collocates and have colloca-bility relations.
A collocation segment can be of any68Figure 1: The collocation segmentation of the sentence a collocation is a recurrent and conventional fixed expressionof words that holds syntactic and semantic relations .
(Xue et al, 2006).length (even a single word) and the length is not de-fined in advance.
This definition differs from othercollocation definitions that are usually based on n-gram lists (Tjong-Kim-Sang and S., 2000; Choueka,1988; Smadja, 1993).
Collocation segmentation isrelated to collocation extraction using syntactic rules(Lin, 1998).
The syntax-based approach allows theextraction of collocations that are easier to describe,and the process of collocation extraction is well-controlled.
On the other hand, the syntax-based ap-proach is not easily applied to languages with fewerresources.
Collocation segmentation is based on adiscrete signal of associativity values between twoconsecutive words, and boundaries that are used tochunk a sequence of words.The main differences of collocation segmentationfrom other methods are: (1) collocation segmenta-tion does not analyze nested collocations it takesthe longest one possible in a given context, while then-gram list-based approach cannot detect if a collo-cation is nested in another one, e.g., machine trans-lation system; (2) collocation segmentation is able toprocess long collocations quickly with the complex-ity of a bigram list size, while the n-gram list-basedapproach is usually limited to 3-word collocationsand has high processing complexity.There are many word associativity measures,such as Mutual Information (MI), T-score, Log-Likelihood, etc.
A detailed overview of associativ-ity measures can be found in (Pecina, 2010), andany of these measures can be applied to colloca-tion segmentation.
MI and Dice scores are almostsimilar in the sense of distribution of values (Dau-daravicius and Marcinkeviciene, 2004), but the Dicescore is always in the range between 0 and 1, whilethe range of the MI score depends on the corpussize.
Thus, the Dice score is preferable.
This scoreis used, for instance, in the collocation compilerXTract (Smadja, 1993) and in the lexicon extractionsystem Champollion (Smadja et al, 1996).
Dice isdefined as follows:D(xi?1;xi) =2 ?
f(xi?1;xi)f(xi?1) + f(xi)where f(xi?1;xi) is the number of co-occurrenceof xi?1 and xi, and f(xi?1) and f(xi) are the num-bers of occurrence of xi?1 and xi in the training cor-pus.
If xi?1 and xi tend to occur in conjunction,their Dice score will be high.
The Dice score issensitive to low-frequency word pairs.
If two con-secutive words are used only once and appear to-gether, there is a good chance that these two wordsare highly related and form some new concept, e.g.,a proper name.
A text is seen as a changing curve ofDice values between two adjacent words (see Figure1).
This curve of associativity values is used to de-tect the boundaries of collocation segments, whichcan be done using a threshold or by following cer-tain rules, as described in the following sections.69length unique segments segment count word count corpus coverage1 289,277 31,427,570 31,427,570 60.58%2 222,252 8,594,745 17,189,490 33.13%3 72,699 994,393 2,983,179 5.75%4 12,669 66,552 266,208 0.51%5 1075 2,839 14,195 0.03%6 57 141 846 0.00%7 3 7 49 0.00%Total 598,032 41,086,247 51,881,537 100%Table 3: The distribution of collocation segments2 word segments CTFIDF 3 word segments CTFIDFmachine translation 10777 in terms of 4099speech recognition 10524 total number of 3926training data 10401 th international conference 3649language model 10188 is used to 3614named entity 9006 one or more 3449error rate 8280 a set of 3439test set 8083 note that the 3346maximum entropy 7570 it is not 3320sense disambiguation 7546 is that the 3287training set 7515 associated with the 3211noun phrase 7509 large number of 3189our system 7352 there is a 3189question answering 7346 support vector machines 3111information retrieval 7338 are used to 3109the user 7198 extracted from the 3054word segmentation 7194 with the same 3030machine learning 7128 so that the 3008parse tree 6987 for a given 2915knowledge base 6792 it is a 2909information extraction 6675 fact that the 28764 word segments CTFIDF 5 word segments CTFIDFif there is a 1690 will not be able to 255human language technology conference 1174 only if there is a 212is defined as the 1064 would not be able to 207is used as the 836 may not be able to 169human language technology workshop 681 a list of all the 94could be used to 654 will also be able to 43has not yet been 514 lexical information from a large 30may be used to 508 should not be able to 23so that it can 480 so that it can also 23our results show that 476 so that it would not 23would you like to 469 was used for this task 23as well as an 420 indicate that a sentence is 17these results show that 388 a list of words or 16might be able to 379 because it can also be 16it can also be 346 before or after the predicate 16have not yet been 327 but it can also be 16not be able to 323 has not yet been performed 16are shown in table 320 if the system has a 16is that it can 311 is defined as an object 16if there is an 305 is given by an expression 16Table 4: Top 20 segments for the segment length of two to five words.704.1 Setting segment boundaries with aThresholdA boundary can be set between two adjacent wordsin a text when the Dice value is lower than a cer-tain threshold.
We use a dynamic threshold whichdefines the range between the minimum and the av-erage associativity values of a sentence.
Zero equalsthe minimum associativity value and 100 equals theaverage value of the sentence.
Thus, the thresholdvalue is expressed as a percentage between the min-imum and the average associativity values.
If thethreshold is set to 0, then no threshold filtering isused and no collocation segment boundaries are setusing the threshold.
The main purpose of using athreshold is to keep only strongly connected tokens.On the other hand, it is possible to set the thresh-old to the maximum value of associativity values.This would make no words combine into more thansingle word segments, i.e., collocation segmentationwould be equal to simple tokenization.
In general,the threshold makes it possible to move from onlysingle-word segments to whole-sentence segmentsby changing the threshold from the minimum to themaximum value of the sentence.
There is no reasonto use the maximum value threshold, but this helpsto understand how the threshold can be used.
(Dau-daravicius and Marcinkeviciene, 2004) uses a globalconstant threshold which produces very long collo-cation segments that are like the clichs used in le-gal documents and hardly related to collocations.
Adynamic threshold allows the problem of very longsegments to be reduced.
In this study I used a thresh-old level of 50 percent.
An example of threshold isshown in Figure 1.
In the example, if the thresholdis 50 percent then segmentation is as follows: a |collocation | is a | recurrent | and | conventional |fixed | expression | of words that | holds | syntactic| and | semantic relations | .
To reduce the problemof long segments even more, the Average MinimumLaw can also be used, as described in the followingsection.4.2 Setting segment boundaries with AverageMinimum Law(Daudaravicius, 2010) introduces the Average Min-imum Law (AML) for setting collocation segmen-tation boundaries.
AML is a simple rule which isapplied to three adjacent associativity values and isexpressed as follows:boundary(xi?2, xi?1) ==??
?TrueD(xi?3;xi?2) + D(xi?1;xi)2< D(xi?2;xi?1)False otherwiseThe boundary between two adjacent words in thetext is set where the Dice value is lower than the av-erage of the preceding and following Dice values.In order to apply AML to the first two or last twowords, I use sequence beginning and sequence end-ing as tokens and calculate the associativity betweenthe beginning of the sequence and the first word,and the last word and the end of the sequence asshown in Figure 1.
AML can be used together withThreshold or alone.
The recent study of (Daudar-avicius, 2012) shows that AML is able to producesegmentation that gives the best text categorizationresults, while the threshold degrades them.
On theother hand, AML can produce collocation segmentswhere the associativity values between two adjacentwords are very low (see Figure 1).
Thus, for lexiconextraction tasks, it is a good idea to use AML and athreshold together.5 Collocation segments from the ACLARCBefore the collocation segmentation, the ACL ARCwas preprocessed with lowercasing and tokeniza-tion.
No stop-word lists, taggers or parsers wereused, and all punctuation was kept.
Collocation seg-mentation is done on a separate line basis, i.e., foreach text line, which is usually a paragraph, the av-erage and the minimum combinability values are de-termined and the threshold is set at 50 percent, mid-way between the average and the minimum.
The Av-erage Minimum Law is applied in tandem.
The toolCoSegment for collocation segmentation is availableat (http://textmining.lt/).Table 3 presents the distribution of segments bylength, i.e., by the number of words.
The lengthof collocation segments varies from 1 to 7 words.In the ACL ARC there are 345,455 distinct tokens.After segmentation, the size of the segment list was598,032 segments, almost double the length of thesingle word list.
The length of the bigram list is714,484,358, which is more than 10 times the size ofthe word list and 7 times that of the collocation seg-ment list.
About 40 percent of the corpus comprisescollocation segments of two or more words, showingthe amount of fixed language present therein.
Thelongest collocation segment is described in section2 .
2 , which contains seven words (when punctu-ation is included as words).
This shows that collo-cation segmentation with a threshold of 50 percentand AML diverges to one-, two- or three-word seg-ments.
Despite that, the list size of collocation seg-ments is much shorter than the list size of bigrams,and shorter still than that of trigrams.After segmentation, it was of interest to find themost significant segments used in the ACL ARC.For this purpose I used a modified TF-IDF whichis defined as follows:CTFIDF (x) = TF (x)?ln(N ?D(x) + 1D(x) + 1)where TF (x) is the raw frequency of segment x inthe corpus, N is the total number of documents inthe corpus, and D(x) is the number of documentsin which the segment x occurs.
Table 4 presents thetop 20 collocation segments for two-, three-, four-and five-word segments of items that contain alpha-betic characters only.
The term machine transla-tion is the most significant in CTFIDF terms.
Thisshort list contains many of the main methods anddatasets used in daily computational linguistics re-search, such as: error rate, test set, maximum en-tropy, training set, parse tree, unknown words, wordalignment, Penn Treebank, language models, mutualinformation, translation model, etc.
These termsshow that computational linguistics has its own ter-minology, methods and tools to research many top-ics.Finally, 76 terms of two or more words in lengthwith the highest CTFIDF values were selected.
Thegoal was to try to find how significant terms wereused yearly in the ACL ARC.
The main part of theACL ARC was compiled using papers published af-ter 1995.
Therefore, for each selected term, the av-erage CTFIDF value of each document for each yearwas calculated.
This approach allows term usagethroughout the history of the ACL to be analysed,and reduces the influence of the unbalanced amountof published papers.
Only those terms whose aver-age CTFIDF in any year was higher than 20 werekept.
For instance, the term machine translation hadto be removed, as it was not significant throughoutall the years.
Each term was ranked by the yearin which its average CTFIDF value peaked.
Theranked terms are shown in Table 5.
For instance,the peak of the CTFIDF average of the term sta-tistical parsing occurred in 1990, of the term lan-guage model in 1987, and of the term bleu scorein 2006.
The results (see Table 5) show the mainresearch trends and time periods of the ACL com-munity.
Most of the terms with CTFIDF peaksprior to 1986 are related to formal/rule-based meth-ods.
Beginning in 1987, terms related to statisticalmethods become more important.
For instance, lan-guage model, similarity measure, and text classifi-cation.
The year 1990 stands out as a kind of break-through.
In this year, the terms Penn Treebank, Mu-tual Information, statistical parsing, bilingual cor-pus, and dependency tree became the most impor-tant terms, showing that newly released language re-sources were supporting many new research areasin computational linguistics.
Despite the fact thatPenn Treebank was only significant temporarily, thecorpus is still used by researchers today.
The mostrecent important terms are Bleu score and semanticrole labeling.This study shows that collocation segmentationcan help in term extraction from large and complexcorpora, which helps to speed up research and sim-plify the study of ACL history.6 ConclusionsThis study has shown that collocation segmentationcan help in term extraction from large and complexcorpora, which helps to speed up research and sim-plify the study of ACL history.
The results show thatthe most significant terms prior to 1986 are relatedto formal/rule based research methods.
Beginning in1987, terms related to statistical methods (e.g., lan-guage model, similarity measure, text classification)become more important.
In 1990, a major turningpoint appears, when the terms Penn Treebank, Mu-tual Information, statistical parsing, bilingual cor-pus, and dependency tree become the most impor-tant, showing that research into new areas of compu-72tational linguistics is supported by the publication ofnew language resources.
The Penn Treebank, whichwas only significant temporarily, it still used today.The most recent terms are Bleu score and semanticrole labeling.
While machine translation as a termis significant throughout the ACL ARC, it is not sig-nificant in any particular time period.
This showsthat some terms can be significant globally, but in-significant at a local level.ReferencesGalen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 465?472, Sydney, Australia, July.
Association for Compu-tational Linguistics.Jaime Arguello and Carolyn Rose.
2006.
Topic-segmentation of dialogue.
In Proceedings of the An-alyzing Conversations in Text and Speech, pages 42?49, New York City, New York, June.
Association forComputational Linguistics.J.
Bachenko and E. Fitzpatrick.
1990.
A computationalgrammar of discourse-neutral prosodic phrasing in en-glish.
Computational Linguistics, 16:155?170.Ralph D. Beebe.
1973.
The frequency distribution ofenglish syntagms.
In Proceedings of the InternationalConference on Computational Linguistics, COLING.Y.
Choueka.
1988.
Looking for needles in a haystack, orlocating interesting collocational expressions in largetextual databases.
In Proceedings of the RIAO Confer-ence on User-Oriented Content-Based Text and ImageHandling, pages 21?24, Cambridge, MA.Kenneth W. Church and William A. Gale.
1989.
En-hanced good-turing and cat.cal: Two new methods forestimating probabilities of english bigrams (abbrevi-ated version).
In Speech and Natural Language: Pro-ceedings of a Workshop Held at Cape Cod.Rene?
Collier, Jan Roelof de Pijper, and Angelien San-derman.
1993.
Perceived prosodic boundaries andtheir phonetic correlates.
In Proceedings of the work-shop on Human Language Technology, HLT ?93,pages 341?345, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Karel Culik.
1965.
Machine translation and connected-ness between phrases.
In International Conference onComputational Linguistics, COLING.Susanna Cumming.
1986.
The lexicon in text gener-ation.
In Strategic Computing - Natural LanguageWorkshop: Proceedings of a Workshop Held at Ma-rina del Rey.V.
Daudaravicius and R Marcinkeviciene.
2004.
Grav-ity counts for the boundaries of collocations.
Interna-tional Journal of Corpus Linguistics, 9(2):321?348.Vidas Daudaravicius.
2010.
The influence of colloca-tion segmentation and top 10 items to keyword assign-ment performance.
In Alexander F. Gelbukh, editor,CICLing, volume 6008 of Lecture Notes in ComputerScience, pages 648?660.
Springer.Vidas Daudaravicius.
2012.
Automatic multilingual an-notation of eu legislation with eurovoc descriptors.
InIn Proceedings of the 8th International Conference onLanguage Resources and Evaluation (LREC?2012).Paolo D?Orta, Marco Ferretti, Alessandro Martelli, andStefano Scarci.
1987.
An automatic speech recogni-tion system for the italian language.
In Third Confer-ence of the European Chapter of the Association forComputational Linguistics.Haodi Feng, Kang Chen, Xiaotie Deng, and WeiminZheng.
2004.
Accessor variety criteria for chineseword extraction.
Computational Linguistics, 30:75?93.Katja Filippova and Michael Strube.
2006.
Using lin-guistically motivated features for paragraph boundaryidentification.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, pages 267?274, Sydney, Australia, July.
Associa-tion for Computational Linguistics.Alexandra Kinyon.
2001.
A language independentshallow-parser compiler.
In Proceedings of 39th An-nual Meeting of the Association for ComputationalLinguistics, pages 330?337, Toulouse, France, July.Association for Computational Linguistics.F.
Knowles.
1973.
The quantitative syntagmatic anal-ysis of the russian and polish phonological systems.In Computational And Mathematical Linguistics: Pro-ceedings of the International Conference on Computa-tional Linguistics, COLING.Gina-Anne Levow.
2004.
Prosodic cues to discoursesegment boundaries in human-computer dialogue.
InMichael Strube and Candy Sidner, editors, Proceed-ings of the 5th SIGdial Workshop on Discourse andDialogue, pages 93?96, Cambridge, Massachusetts,USA, April 30 - May 1.
Association for ComputationalLinguistics.Hang Li and Kenji Yamanishi.
2000.
Topic analysisusing a finite mixture model.
In 2000 Joint SIGDATConference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora, pages 35?44, Hong Kong, China, October.
Association for Com-putational Linguistics.D.
Lin.
1998.
Extracting collocations from text cor-pora.
In First Workshop on Computational Terminol-ogy, Montreal.73Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and MaryHarper.
2004.
Comparing and combining generativeand posterior probability models: Some advances insentence boundary detection in speech.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 64?71, Barcelona, Spain, July.
Association forComputational Linguistics.Kathleen McKeown, Diane Litman, and Rebecca Passon-neau.
1992.
Extracting constraints on word usagefrom large text corpora.
In Speech and Natural Lan-guage: Proceedings of a Workshop Held at Harriman.Christian Monson, Alon Lavie, Jaime Carbonell, andLori Levin.
2004.
Unsupervised induction of naturallanguage morphology inflection classes.
In Proceed-ings of the Seventh Meeting of the ACL Special Inter-est Group in Computational Phonology, pages 52?61,Barcelona, Spain, July.
Association for ComputationalLinguistics.Karin Mu?ller.
2006.
Improving syllabification modelswith phonotactic knowledge.
In Proceedings of theEighth Meeting of the ACL Special Interest Group onComputational Phonology and Morphology at HLT-NAACL 2006, pages 11?20, New York City, USA,June.
Association for Computational Linguistics.Pavel Pecina.
2005.
An extensive empirical study ofcollocation extraction methods.
In Proceedings of theACL Student Research Workshop, pages 13?18, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Pavel Pecina.
2010.
Lexical association measures andcollocation extraction.
Language Resources and Eval-uation, 44(1-2):137?158.C.
Anton Rytting.
2004.
Segment predictability as a cuein word segmentation: Application to modern greek.In Proceedings of the Seventh Meeting of the ACLSpecial Interest Group in Computational Phonology,pages 78?85, Barcelona, Spain, July.
Association forComputational Linguistics.Itiroo Sakai.
1965.
Some mathematical aspects onsyntactic discription.
In International Conference onComputational Linguistics, COLING.Erik F. Tjong Kim Sang and Herve Dejean.
2001.
Intro-duction to the conll-2001 shared task: clause identifi-cation.
In Proceedings of the ACL 2001 Workshop onComputational Natural Language Learning, Toulouse,France, July.
Association for Computational Linguis-tics.Violeta Seretan.
2011.
Syntax-Based Collocation Ex-traction, volume 44 of Text, Speech and LanguageTechnology.
Springer.Frank Smadja, Vasileios Hatzivassiloglou, and Kath-leen R. McKeown.
1996.
Translating collocations forbilingual lexicons: A statistical approach.
Computa-tional Linguistics, 22:1?38.Frank Smadja.
1993.
Retrieving collocations from text:Xtract.
Computational Linguistics, 19:143?177.E.
Tjong-Kim-Sang and Buchholz S. 2000.
Introductionto the conll-2000 shared task: Chunking.
In Proc.
ofCoNLL-2000 and LLL-2000, pages 127?132, Lisbon,Portugal.L.
W. Tosh.
1965.
Data preparation for syntactic trans-lation.
In International Conference on ComputationalLinguistics, COLING.Gokhan Tur, Andreas Stolcke, Dilek Hakkani-Tur, andElizabeth Shriberg.
2001.
Integrating prosodic andlexical cues for automatic topic segmentation.
Com-putational Linguistics, 27:31?57.Brigitte van Berkelt and Koenraad De Smedt.
1988.
Tri-phone analysis: A combined method for the correctionof orthographical and typographical errors.
In Pro-ceedings of the Second Conference on Applied Natu-ral Language Processing, pages 77?83, Austin, Texas,USA, February.
Association for Computational Lin-guistics.Nianwen Xue, Jinying Chen, and Martha Palmer.
2006.Aligning features with sense distinction dimensions.In Proceedings of the COLING/ACL 2006 Main Con-ference Poster Sessions, pages 921?928, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Shou-Chuan Yang.
1969.
A search algorithm and datastructure for an efficient information system.
In In-ternational Conference on Computational Linguistics,COLING.7465676973757879808182838485868788899091929394959697989900010203040506parsingalgorithm25295131371010lexicalentry362141491311sourcelanguage1121415912wordsenses1031221261077191713699targetlanguage11152421851110131017browncorpus4361663021182120614929logicalform8211117132691812191516161714813811111010semanticrepresentation94321911multi-word22219referenceresolution478419301713161899231412languagemodel9341119141312187131112109textgeneration2417925251392919127spokenlanguage637232019211314speechrecognition1211331921191616similaritymeasure131333171510textclassification5523171116statisticalparsing30treeadjoininggrammars3142219291915131211mutualinformation121727121591115penntreebank266bilingualcorpus221119911916171211dependencytree1089118971021151211131611161511postagging2314101110101011139spontaneousspeech1683422204216171891310textcategorization212516915141210featureselection201151107101191214151010translationmodel28274917111518151410spellingcorrection10149717171619429272615101710131916editdistance37211316101411121512targetword421391291614201311speechsynthesis3811981418161129911111010searchengine273116122011910maximumentropy221222252712101091210lexicalrules12621051891136188244621181111annotationscheme151431211415111810coreferenceresolution1121343710162720221627textsummarization1536171413131312naivebayes32234314121323201217trigrammodel202016131314381013111310namedentity201214111431121019161011anaphoraresolution510915171029132111121211wordsegmentation20192616161310112031262430wordalignment101319201124242217semanticrolelabeling252528bleuscore121816142165676973757879808182838485868788899091929394959697989900010203040506Table5:ThelistofselectedtermsandtheyearlyimportanceintermsofCTFIDF.75
