Proceedings of NAACL HLT 2007, pages 404?411,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsImproved Inference for Unlexicalized ParsingSlav Petrov and Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{petrov,klein}@eecs.berkeley.eduAbstractWe present several improvements to unlexicalizedparsing with hierarchically state-split PCFGs.
First,we present a novel coarse-to-fine method in whicha grammar?s own hierarchical projections are usedfor incremental pruning, including a method for ef-ficiently computing projections of a grammar with-out a treebank.
In our experiments, hierarchicalpruning greatly accelerates parsing with no loss inempirical accuracy.
Second, we compare variousinference procedures for state-split PCFGs from thestandpoint of risk minimization, paying particularattention to their practical tradeoffs.
Finally, wepresent multilingual experiments which show thatparsing with hierarchical state-splitting is fast andaccurate in multiple languages and domains, evenwithout any language-specific tuning.1 IntroductionTreebank parsing comprises two problems: learn-ing, in which we must select a model given a tree-bank, and inference, in which we must select aparse for a sentence given the learned model.
Pre-vious work has shown that high-quality unlexical-ized PCFGs can be learned from a treebank, eitherby manual annotation (Klein and Manning, 2003)or automatic state splitting (Matsuzaki et al, 2005;Petrov et al, 2006).
In particular, we demon-strated in Petrov et al (2006) that a hierarchicallysplit PCFG could exceed the accuracy of lexical-ized PCFGs (Collins, 1999; Charniak and Johnson,2005).
However, many questions about inferencewith such split PCFGs remain open.
In this work,we present1.
an effective method for pruning in split PCFGs2.
a comparison of objective functions for infer-ence in split PCFGs,3.
experiments on automatic splitting for lan-guages other than English.In Sec.
3, we present a novel coarse-to-fine pro-cessing scheme for hierarchically split PCFGs.
Ourmethod considers the splitting history of the finalgrammar, projecting it onto its increasingly refinedprior stages.
For any projection of a grammar, wegive a new method for efficiently estimating the pro-jection?s parameters from the source PCFG itself(rather than a treebank), using techniques for infi-nite tree distributions (Corazza and Satta, 2006) anditerated fixpoint equations.
We then parse with eachrefinement, in sequence, much along the lines ofCharniak et al (2006), except with much more com-plex and automatically derived intermediate gram-mars.
Thresholds are automatically tuned on held-out data, and the final system parses up to 100 timesfaster than the baseline PCFG parser, with no loss intest set accuracy.In Sec.
4, we consider the well-known issue ofinference objectives in split PCFGs.
As in manymodel families (Steedman, 2000; Vijay-Shanker andJoshi, 1985), split PCFGs have a derivation / parsedistinction.
The split PCFG directly describes a gen-erative model over derivations, but evaluation is sen-sitive only to the coarser treebank symbols.
Whilethe most probable parse problem is NP-complete(Sima?an, 1992), several approximate methods exist,including n-best reranking by parse likelihood, thelabeled bracket alorithm of Goodman (1996), anda variational approximation introduced in Matsuzakiet al (2005).
We present experiments which explic-itly minimize various evaluation risks over a can-didate set using samples from the split PCFG, andrelate those conditions to the existing non-samplingalgorithms.
We demonstrate that n-best rerankingaccording to likelihood is superior for exact match,and that the non-reranking methods are superior formaximizing F1.
A specific contribution is to discussthe role of unary productions, which previous workhas glossed over, but which is important in under-standing why the various methods work as they do.404Finally, in Sec.
5, we learn state-split PCFGs forGerman and Chinese and examine out-of-domainperformance for English.
The learned grammars arecompact and parsing is very quick in our multi-stagescheme.
These grammars produce the highest testset parsing figures that we are aware of in each lan-guage, except for English for which non-local meth-ods such as feature-based discriminative rerankingare available (Charniak and Johnson, 2005).2 Hierarchically Split PCFGsWe consider PCFG grammars which are derivedfrom a raw treebank as in Petrov et al (2006): Asimple X-bar grammar is created by binarizing thetreebank trees.
We refer to this grammar as G0.From this starting point, we iteratively refine thegrammar in stages, as illustrated in Fig.
1.
In eachstage, all symbols are split in two, for example DTmight become DT-1 and DT-2.
The refined grammaris estimated using a variant of the forward-backwardalgorithm (Matsuzaki et al, 2005).
After a split-ting stage, many splits are rolled back based on (anapproximation to) their likelihood gain.
This pro-cedure gives an ontogeny of grammars Gi, whereG = Gn is the final grammar.
Empirically, thegains on the English Penn treebank level off after 6rounds.
In Petrov et al (2006), some simple smooth-ing is also shown to be effective.
It is interesting tonote that these grammars capture many of the ?struc-tural zeros?
described by Mohri and Roark (2006)and pruning rules with probability below e?10 re-duces the grammar size drastically without influenc-ing parsing performance.
Some of our methods andconclusions are relevant to all state-split grammars,such as Klein and Manning (2003) or Dreyer andEisner (2006), while others apply most directly tothe hierarchical case.3 SearchWhen working with large grammars, it is standard toprune the search space in some way.
In the case oflexicalized grammars, the unpruned chart often willnot even fit in memory for long sentences.
Severalproven techniques exist.
Collins (1999) combines apunctuation rule which eliminates many spans en-tirely, and then uses span-synchronous beams toprune in a bottom-up fashion.
Charniak et al (1998)G0G1G2G3G4G5G6X-bar =G =pi iDT:DT-1: DT-2:thethatthisthis0 1 2 3 4That5 6 7somesome8 9 10 11these12 13thethethe14 15The16aa17Figure 1: Hierarchical refinement proceeds top-down while pro-jection recovers coarser grammars.
The top word for the firstrefinements of the determiner tag (DT) is shown on the right.introduces best-first parsing, in which a figure-of-merit prioritizes agenda processing.
Most relevantto our work is Charniak and Johnson (2005) whichuses a pre-parse phase to rapidly parse with a verycoarse, unlexicalized treebank grammar.
Any itemX:[i, j] with sufficiently low posterior probability inthe pre-parse triggers the pruning of its lexical vari-ants in a subsequent full parse.3.1 Coarse-to-Fine ApproachesCharniak et al (2006) introduces multi-level coarse-to-fine parsing, which extends the basic pre-parsingidea by adding more rounds of pruning.
In theirwork, the extra pruning was with grammars evencoarser than the raw treebank grammar, such asa grammar in which all nonterminals are col-lapsed.
We propose a novel multi-stage coarse-to-fine method which is particularly natural for our hi-erarchically split grammar, but which is, in princi-ple, applicable to any grammar.
As in Charniak etal.
(2006), we construct a sequence of increasinglyrefined grammars, reparsing with each refinement.The contributions of our method are that we derivesequences of refinements in a new way (Sec.
3.2),we consider refinements which are themselves com-plex, and, because our full grammar is not impossi-ble to parse with, we automatically tune the pruningthresholds on held-out data.3.2 ProjectionIn our method, which we call hierarchical coarse-to-fine parsing, we consider a sequence of PCFGsG0, G1, .
.
.
Gn = G, where each Gi is a refinementof the preceding grammar Gi?1 and G is the fullgrammar of interest.
Each grammar Gi is related toG = Gn by a projection pin?i or pii for brevity.
A405projection is a map from the non-terminal (includingpre-terminal) symbols of G onto a reduced domain.A projection of grammar symbols induces a pro-jection of rules and therefore entire non-weightedgrammars (see Fig.
1).In our case, we also require the projections to besequentially compatible, so that pii?j =pik?j?pii?k.That is, each projection is itself a coarsening of theprevious projections.
In particular, we take the pro-jection pii?j to be the map that collapses split sym-bols in round i to their earlier identities in round j.It is straightforward to take a projection pi andmap a CFG G to its induced projection pi(G).
Whatis less obvious is how the probabilities associatedwith the rules of G should be mapped.
In the casewhere pi(G) is more coarse than the treebank orig-inally used to train G, and when that treebank isavailable, it is easy to project the treebank and di-rectly estimate, say, the maximum-likelihood pa-rameters for pi(G).
This is the approach taken byCharniak et al (2006), where they estimate what inour terms are projections of the raw treebank gram-mar from the treebank itself.However, treebank estimation has several limita-tions.
First, the treebank used to train G may notbe available.
Second, if the grammar G is heavilysmoothed or otherwise regularized, its own distri-bution over trees may be far from that of the tree-bank.
Third, the meanings of the split states can anddo drift between splitting stages.
Fourth, and mostimportantly, we may wish to project grammars forwhich treebank estimation is problematic, for exam-ple, grammars which are more refined than the ob-served treebank grammars.
Our method effectivelyavoids all of these problems by rebuilding and refit-ting the pruning grammars on the fly from the finalgrammar.3.2.1 Estimating Projected GrammarsFortunately, there is a well worked-out notion ofestimating a grammar from an infinite distributionover trees (Corazza and Satta, 2006).
In particular,we can estimate parameters for a projected grammarpi(G) from the tree distribution induced by G (whichcan itself be estimated in any manner).
The earli-est work that we are aware of on estimating modelsfrom models in this way is that of Nederhof (2005),who considers the case of learning language mod-els from other language models.
Corazza and Satta(2006) extend these methods to the case of PCFGsand tree distributions.The generalization of maximum likelihood esti-mation is to find the estimates for pi(G) with min-imum KL divergence from the tree distribution in-duced by G. Since pi(G) is a grammar over coarsersymbols, we fit pi(G) to the distribution G inducesover pi-projected trees: P (pi(T )|G).
The proofsof the general case are given in Corazza and Satta(2006), but the resulting procedure is quite intuitive.Given a (fully observed) treebank, the maximum-likelihood estimate for the probability of a rule X ?Y Z would simply be the ratio of the count of X tothe count of the configuration X ?
Y Z .
If we wishto find the estimate which has minimum divergenceto an infinite distribution P (T ), we use the same for-mula, but the counts become expected counts:P (X ?
Y Z) =EP (T )[X ?
Y Z]EP (T )[X]with unaries estimated similarly.
In our specificcase, X,Y, and Z are symbols in pi(G), and theexpectations are taken over G?s distribution of pi-projected trees, P (pi(T )|G).
We give two practicalmethods for obtaining these expectations below.3.2.2 Calculating Projected ExpectationsConcretely, we can now estimate the minimumdivergence parameters of pi(G) for any projectionpi and PCFG G if we can calculate the expecta-tions of the projected symbols and rules according toP (pi(T )|G).
The simplest option is to sample treesT from G, project the samples, and take averagecounts off of these samples.
In the limit, the countswill converge to the desired expectations, providedthe grammar is proper.
However, we can exploit thestructure of our projections to obtain the desired ex-pectations much more simply and efficiently.First, consider the problem of calculating the ex-pected counts of a symbol X in a tree distributiongiven by a grammar G, ignoring the issue of projec-tion.
These expected counts obey the following one-step equations (assuming a unique root symbol):c(root) = 1c(X) =?Y?
?X?P (?X?|Y )c(Y )406Here, ?, ?, or both can be empty, and a rule X ?
?appears in the sum once for each X it contains.
Inprinciple, this linear system can be solved in anyway.1 In our experiments, we solve this system it-eratively, with the following recurrences:c0(X)?
{1 if X = root0 otherwiseci+1(X)??Y?
?X?P (?X?|Y )ci(Y )Note that, as in other iterative fixpoint methods, suchas policy evaluation for Markov decision processes(Sutton and Barto, 1998), the quantities ck(X) havea useful interpretation as the expected counts ignor-ing nodes deeper than depth k (i.e.
the roots are allthe root symbol, so c0(root) = 1).
In our experi-ments this method converged within around 25 iter-ations; this is unsurprising, since the treebank con-tains few nodes deeper than 25 and our base gram-mar G seems to have captured this property.Once we have the expected counts of symbolsin G, the expected counts of their projectionsX ?
= pi(X) according to P (pi(T )|G) are given byc(X ?)
= ?X:pi(X)=X?
c(X).
Rules can be esti-mated directly using similar recurrences, or given byone-step equations:c(X ?
?)
= c(X)P (?|X)This process very rapidly computes the estimatesfor a projection of a grammar (i.e.
in a few secondsfor our largest grammars), and is done once duringinitialization of the parser.3.2.3 Hierarchical ProjectionsRecall that our final state-split grammars G come,by their construction process, with an ontogeny ofgrammars Gi where each grammar is a (partial)splitting of the preceding one.
This gives us a nat-ural chain of projections pii?j which projects back-wards along this ontogeny of grammars (see Fig.
1).Of course, training also gives us parameters forthe grammars, but only the chain of projections isneeded.
Note that the projected estimates need not1Whether or not the system has solutions depends on theparameters of the grammar.
In particular, G may be improper,though the results of Chi (1999) imply that G will be proper ifit is the maximum-likelihood estimate of a finite treebank.
(and in general will not) recover the original param-eters exactly, nor would we want them to.
Insteadthey take into account any smoothing, substate drift,and so on which occurred by the final grammar.Starting from the base grammar, we run the pro-jection process for each stage in the sequence, cal-culating pii (chained incremental projections wouldalso be possible).
For the remainder of the paper,except where noted otherwise, all coarser grammars?estimates are these reconstructions, rather than thoseoriginally learned.3.3 ExperimentsAs demonstrated by Charniak et al (2006) parsingtimes can be greatly reduced by pruning chart itemsthat have low posterior probability under a simplergrammar.
Charniak et al (2006) pre-parse with a se-quence of grammars which are coarser than (parent-annotated) treebank grammars.
However, we alsowork with grammars which are already heavily split,up to half as split as the final grammar, because wefound the computational cost for parsing with thesimple X-bar grammar to be insignificant comparedto the costs for parsing with more refined grammars.For a final grammar G = Gn, we compute esti-mates for the n projections Gn?1, .
.
.
, G0 =X-Bar,where Gi = pii(G) as described in the previous sec-tion.
Additionally we project to a grammar G?1 inwhich all nonterminals, except for the preterminals,have been collapsed.
During parsing, we start ofby exhaustively computing the inside/outside scoreswith G?1.
At each stage, chart items with low poste-rior probability are removed from the chart, and weproceed to compute inside/outside scores with thenext, more refined grammar, using the projectionspii?i?1 to map between symbols in Gi and Gi?1.
Ineach pass, we skip chart items whose projection intothe previous stage had a probability below a stage-specific threshold, until we reach G = Gn (afterseven passes in our case).
For G, we do not prunebut instead return the minimum risk tree, as will bedescribed in Sec.
4.Fig.
2 shows the (unlabeled) bracket posteriors af-ter each pass and demonstrates that most construc-tions can be ruled out by the simpler grammars,greatly reducing the amount of computation for thefollowing passes.
The pruning thresholds were em-pirically determined on a held out set by computing407Influentialmembers of theHouseWaysandMeansCommitteeintroducedlegislationthatwouldrestricthowthenews&lbailoutagencycanraisecapital ;creatinganotherpotentialobstacle to thegovernment?ssale ofsickthrifts .G?1 G0=X-bar G1G2 G3 G4G5(G6=G)OutputFigure 2: Bracket posterior probabilities (black = high) for thefirst sentence of our development set during coarse-to-finepruning.
Note that we compute the bracket posteriors at a muchfiner level but are showing the unlabeled posteriors for illustra-tion purposes.
No pruning is done at the finest level (G6 = G)but the minimum risk tree is returned instead.the most likely tree under G directly (without prun-ing) and then setting the highest pruning thresholdfor each stage that would not prune the optimal tree.This setting also caused no search errors on the testset.
We found our projected grammar estimates to beat least equally well suited for pruning as the orig-inal grammar estimates which were learned duringthe hierarchical training.
Tab.
1 shows the tremen-dous reduction in parsing time (all times are cumu-lative) and gives an overview over grammar sizesand parsing accuracies.
In particular, in our Java im-plementation on a 3GHz processor, it is possible toparse the 1578 development set sentences (of length40 or less) in less than 1200 seconds with an F1 of91.2% (no search errors), or, by pruning more, in680 seconds at 91.1%.
For comparison, the Feb.2006 release of the Charniak and Johnson (2005)parser runs in 1150 seconds on the same machinewith an F1 of 90.7%.4 Objective Functions for ParsingA split PCFG is a grammar G over symbols of theform X-k where X is an evaluation symbol (suchas NP) and k is some indicator of a subcategory,such as a parent annotation.
G induces a deriva-tion distribution P (T |G) over trees T labeled withsplit symbols.
This distribution in turn inducesa parse distribution P (T ?|G) = P (pi(T )|G) over(projected) trees with unsplit evaluation symbols,where P (T ?|G) = ?T :T ?=pi(T ) P (T |G).
We nowhave several choices of how to select a tree giventhese posterior distributions over trees.
In this sec-tion, we present experiments with the various op-tions and explicitly relate them to parse risk mini-mization (Titov and Henderson, 2006).G0 G2 G4 G6Nonterminals 98 219 498 1140Rules 3,700 19,600 126,100 531,200No pruning 52 min 99 min 288 min 1612 minX-bar pruning 8 min 14 min 30 min 111 minC-to-F (no loss) 6 min 12 min 16 min 20 minF1 for above 64.8 85.2 89.7 91.2C-to-F (lossy) 6 min 8 min 9 min 11 minF1 for above 64.3 84.7 89.4 91.1Table 1: Grammar sizes, parsing times and accuracies for hier-archically split PCFGs with and without hierarchical coarse-to-fine parsing on our development set (1578 sentences with 40 orless words from section 22 of the Penn Treebank).
For compar-ison the parser of Charniak and Johnson (2005) has an accuracyof F1=90.7 and runs in 19 min on this set.The decision-theoretic approach to parsing wouldbe to select the parse tree which minimizes our ex-pected loss according to our beliefs:T ?P = argminTP?TTP (TT |w,G)L(TP , TT )where TT and TP are ?true?
and predicted parsetrees.
Here, our loss is described by the function Lwhose first argument is the predicted parse tree andthe second is the gold parse tree.
Reasonable can-didates for L include zero-one loss (exact match),precision, recall, F1 (specifically EVALB here), andso on.
Of course, the naive version of this process isintractable: we have to loop over all (pairs of) pos-sible parses.
Additionally, it requires parse likeli-hoods P (TP |w,G), which are tractable, but not triv-ial, to compute for split models.
There are two op-tions: limit the predictions to a small candidate set orchoose methods for which dynamic programs exist.For arbitrary loss functions, we can approximatethe minimum-risk procedure by taking the min overonly a set of candidate parses TP .
In some cases,each parse?s expected risk can be evaluated in closed408Rule score: r(A?
B C, i, k, j) =?x?y?zPOUT(Ax, i, j)P(Ax ?
By Cz)PIN(By, i, k)PIN(Cy, k, j)VARIATIONAL: q(A?
B C, i, k, j) = r(A?
B C, i, k, j)Px POUT(Ax,i,j)PIN(Ax,i,j)TG = argmaxT?e?T q(e)MAX-RULE-SUM: q(A?
B C, i, k, j) = r(A?
B C, i, k, j)PIN(root,0,n) TG = argmaxT?e?T q(e)MAX-RULE-PRODUCT: q(A?
B C, i, k, j) = r(A?
B C, i, k, j)PIN(root,0,n) TG = argmaxT?e?T q(e)Figure 3: Different objectives for parsing with posteriors, yielding comparable results.
A, B, C are nonterminal symbols, x, y, zare latent annotations and i, j, k are between-word indices.
Hence (Ax, i, j) denotes a constituent labeled with Ax spanning fromi to j.
Furthermore, we write e = (A?
B C, i, j, k) for brevity.form.
Exact match (likelihood) has this property.
Ingeneral, however, we can approximate the expecta-tion with samples from P (T |w,G).
The method forsampling derivations of a PCFG is given in Finkelet al (2006) and Johnson et al (2007).
It requires asingle inside-outside computation per sentence andis then efficient per sample.
Note that for split gram-mars, a posterior parse sample can be drawn by sam-pling a derivation and projecting away the substates.Fig.
2 shows the results of the following exper-iment.
We constructed 10-best lists from the fullgrammar G in Sec.
2 using the parser of Petrov etal.
(2006).
We then took the same grammar and ex-tracted 500-sample lists using the method of Finkelet al (2006).
The minimum risk parse candidate wasselected for various loss functions.
As can be seen,in most cases, risk minimization reduces test-set lossof the relevant quantity.
Exact match is problematic,however, because 500 samples is often too few todraw a match when a sentence has a very flat poste-rior, and so there are many all-way ties.2 Since ex-act match permits a non-sampled calculation of theexpected risk, we show this option as well, whichis substantially superior.
This experiment highlightsthat the correct procedure for exact match is to findthe most probable parse.An alternative approach to reranking candidateparses is to work with inference criteria which ad-mit dynamic programming solutions.
Fig.
3 showsthree possible objective functions which use the eas-ily obtained posterior marginals of the parse tree dis-tribution.
Interestingly, while they have fairly differ-ent decision theoretic motivations, their closed-formsolutions are similar.25,000 samples do not improve the numbers appreciably.One option is to maximize likelihood in an ap-proximate distribution.
Matsuzaki et al (2005)present a VARIATIONAL approach, which approxi-mates the true posterior over parses by a cruder, buttractable sentence-specific one.
In this approximatedistribution there is no derivation / parse distinctionand one can therefore optimize exact match by se-lecting the most likely derivation.Instead of approximating the tree distribution wecan use an objective function that decomposes alongparse posteriors.
The labeled brackets algorithm ofGoodman (1996) has such an objective function.
Inits original formulation this algorithm maximizesthe number of expected correct nodes, but insteadwe can use it to maximize the number of correctrules (the MAX-RULE-SUM algorithm).
A worry-ing issue with this method is that it is ill-defined forgrammars which allow infinite unary chains: therewill be no finite minimum risk tree under recall loss(you can always reduce the risk by adding one morecycle).
We implement MAX-RULE-SUM in a CNF-like grammar family where above each binary splitis exactly one unary (possibly a self-loop).
Withthis limitation, unary chains are not a problem.
Asmight be expected, this criterion improves bracketmeasures at the expense of exact match.We found it optimal to use a third approach,in which rule posteriors are multiplied instead ofadded.
This corresponds to choosing the tree withgreatest chance of having all rules correct, underthe (incorrect) assumption that the rules correct-ness are independent.
This MAX-RULE-PRODUCTalgorithm does not need special treatment of infi-nite unary chains because it is optimizing a productrather than a sum.
While these three methods yield409Objective P R F1 EXBEST DERIVATIONViterbi Derivation 89.6 89.4 89.5 37.4RERANKINGRandom 87.6 87.7 87.7 16.4Precision (sampled) 91.1 88.1 89.6 21.4Recall (sampled) 88.2 91.3 89.7 21.5F1 (sampled) 90.2 89.3 89.8 27.2Exact (sampled) 89.5 89.5 89.5 25.8Exact (non-sampled) 90.8 90.8 90.8 41.7Exact/F1 (oracle) 95.3 94.4 95.0 63.9DYNAMIC PROGRAMMINGVARIATIONAL 90.7 90.9 90.8 41.4MAX-RULE-SUM 90.5 91.3 90.9 40.4MAX-RULE-PRODUCT 91.2 91.1 91.2 41.4Table 2: A 10-best list from our best G can be reordered as tomaximize a given objective either using samples or, under somerestricting assumptions, in closed form.very similar results (see Fig.
2), the MAX-RULE-PRODUCT algorithm consistently outperformed theother two.Overall, the closed-form options were superior tothe reranking ones, except on exact match, where thegains from correctly calculating the risk outweighthe losses from the truncation of the candidate set.5 Multilingual ParsingMost research on parsing has focused on Englishand parsing performance on other languages is gen-erally significantly lower.3 Recently, there havebeen some attempts to adapt parsers developed forEnglish to other languages (Levy and Manning,2003; Cowan and Collins, 2005).
Adapting lexi-calized parsers to other languages in not a trivialtask as it requires at least the specification of headrules, and has had limited success.
Adapting unlexi-calized parsers appears to be equally difficult: Levyand Manning (2003) adapt the unlexicalized parserof Klein and Manning (2003) to Chinese, but evenafter significant efforts on choosing category splits,only modest performance gains are reported.In contrast, automatically learned grammars likethe one of Matsuzaki et al (2005) and Petrov et al(2006) require a treebank for training but no addi-tional human input.
One has therefore reason to3Of course, cross-linguistic comparison of results is com-plicated by differences in corpus annotation schemes and sizes,and differences in linguistic characteristics.ENGLISH GERMAN CHINESE(Marcus et al, 1993) (Skut et al, 1997) (Xue et al, 2002)TrainSet Section 2-21 Sentences 1-18,602 Articles 26-270DevSet Section 22 18,603-19,602 Articles 1-25TestSet Section 23 19,603-20,602 Articles 271-300Table 3: Experimental setup.believe that their performance will generalize bet-ter across languages than the performance of parsersthat have been hand tailored to English.5.1 ExperimentsWe trained models for English, Chinese and Ger-man using the standard corpora and splits as shownin Tab.
3.
We applied our model directly to eachof the treebanks, without any language dependentmodifications.
Specifically, the same model hyper-parameters (merging percentage and smoothing fac-tor) were used in all experiments.Tab.
4 shows that automatically inducing latentstructure is a technique that generalizes well acrosslanguage boundaries and results in state of the artperformance for Chinese and German.
On English,the parser is outperformed only by the rerankingparser of Charniak and Johnson (2005), which hasaccess to a variety of features which cannot be cap-tured by a generative model.Space does not permit a thorough exposition ofour analysis, but as in the case of English (Petrovet al, 2006), the learned subcategories exhibit inter-esting linguistic interpretations.
In German, for ex-ample, the model learns subcategories for differentcases and genders.5.2 Corpus VariationRelated to cross language generalization is the gen-eralization across domains for the same language.It is well known that a model trained on the WallStreet Journal loses significantly in performancewhen evaluated on the Brown Corpus (see Gildea(2001) for more details and the exact setup of theirexperiment, which we duplicated here).
RecentlyMcClosky et al (2006) came to the conclusion thatthis performance drop is not due to overfitting theWSJ data.
Fig.
4 shows the performance on theBrown corpus during hierarchical training.
Whilethe F1 score on the WSJ is rising we observe a dropin performance after the 5th iteration, suggestingthat some overfitting is occurring.410?
40 words allParser LP LR LP LRENGLISHCharniak et al (2005) 90.1 90.1 89.5 89.6Petrov et al (2006) 90.3 90.0 89.8 89.6This Paper 90.7 90.5 90.2 89.9ENGLISH (reranked)Charniak et al (2005)4 92.4 91.6 91.8 91.0GERMANDubey (2005) F1 76.3 -This Paper 80.8 80.7 80.1 80.1CHINESE5Chiang et al (2002) 81.1 78.8 78.0 75.2This Paper 80.8 80.7 78.8 78.5Table 4: Our final test set parsing performance compared to thebest previous work on English, German and Chinese.7880828486Grammar SizeF 1Hierarchically Split PCFGsCharniak and Johnson (2005) generative parserCharniak and Johnson (2005) reranking parserG3G5 G6G4Figure 4: Parsing accuracy starts dropping after 5 training iter-ations on the Brown corpus, while it is improving on the WSJ,indicating overfitting.6 ConclusionsThe coarse-to-fine scheme presented here, in con-junction with the risk-appropriate parse selectionmethodology, allows fast, accurate parsing, in multi-ple languages and domains.
For training, one needsonly a raw context-free treebank and for decodingone needs only a final grammar, along with coars-ening maps.
The final parser is publicly available athttp://www.nlp.cs.berkeley.edu.Acknowledgments We would like to thank Eu-gene Charniak, Mark Johnson and Noah Smith forhelpful discussions and comments.ReferencesE.
Charniak and M. Johnson.
2005.
Coarse-to-Fine N-BestParsing and MaxEnt Discriminative Reranking.
In ACL?05.E.
Charniak, S. Goldwater, and M. Johnson.
1998.
Edge-basedbest-first chart parsing.
6th Wkshop on Very Large Corpora.4This is the performance of the updated reranking parseravailable at http://www.cog.brown.edu/mj/software.htm5Sun and Jurafsky (2004) report even better performance onthis dataset but since they assume gold POS tags their work isnot directly comparable (p.c.).E.
Charniak, M. Johnson, et al 2006.
Multi-level coarse-to-finePCFG Parsing.
In HLT-NAACL ?06.Z.
Chi.
1999.
Statistical properties of probabilistic context-freegrammars.
In Computational Linguistics.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, U. of Pennsylvania.A.
Corazza and G. Satta.
2006.
Cross-entropy and estimationof probabilistic context-free grammars.
In HLT-NAACL ?06.B.
Cowan and M. Collins.
2005.
Morphology and rerankingfor the statistical parsing of Spanish.
In HLT-EMNLP ?05.M.
Dreyer and J. Eisner.
2006.
Better informed training oflatent syntactic features.
In EMNLP ?06, pages 317?326.A.
Dubey.
2005.
What to do when lexicalization fails: parsingGerman with suffix analysis and smoothing.
In ACL ?05.J.
Finkel, C. Manning, and A. Ng.
2006.
Solving the prob-lem of cascading errors: approximate Bayesian inference forlingusitic annotation pipelines.
In EMNLP ?06.D.
Gildea.
2001.
Corpus variation and parser performance.EMNLP ?01, pages 167?202.J.
Goodman.
1996.
Parsing algorithms and metrics.
ACL ?96.M.
Johnson, T. Griffiths, and S. Goldwater.
2007.
Bayesianinference for PCFGs via Markov Chain Monte Carlo.
InHLT-NAACL ?07.D.
Klein and C. Manning.
2003.
Accurate unlexicalized pars-ing.
In ACL ?03, pages 423?430.R.
Levy and C. Manning.
2003.
Is it harder to parse Chinese,or the Chinese treebank?
In ACL ?03, pages 439?446.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.
Build-ing a large annotated corpus of English: The Penn Treebank.In Computational Linguistics.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilistic CFGwith latent annotations.
In ACL ?05, pages 75?82.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Rerankingand self-training for parser adaptation.
In COLING-ACL?06.M.
Mohri and B. Roark.
2006.
Probabilistic context-free gram-mar induction based on structural zeros.
In HLT-NAACL ?06.M.-J.
Nederhof.
2005.
A general technique to train languagemodels on language models.
In Computational Linguistics.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.
Learn-ing accurate, compact, and interpretable tree annotation.
InCOLING-ACL ?06, pages 443?440.K.
Sima?an.
1992.
Computatoinal complexity of probabilisticdisambiguation.
Grammars, 5:125?151.W.
Skut, B. Krenn, T. Brants, and H. Uszkoreit.
1997.
An anno-tation scheme for free word order languages.
In Conferenceon Applied Natural Language Processing.M.
Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, Massachusetts.H.
Sun and D. Jurafsky.
2004.
Shallow semantic parsing ofChinese.
In HLT-NAACL ?04, pages 249?256.R.
Sutton and A. Barto.
1998.
Reinforcement Learning: AnIntroduction.
MIT Press.I.
Titov and J. Henderson.
2006.
Loss minimization in parsereranking.
In EMNLP ?06, pages 560?567.K.
Vijay-Shanker and A. Joshi.
1985.
Some computationalproperties of Tree Adjoining Grammars.
In ACL ?85.N.
Xue, F.-D. Chiou, and M. Palmer.
2002.
Building a largescale annotated Chinese corpus.
In COLING ?02.411
