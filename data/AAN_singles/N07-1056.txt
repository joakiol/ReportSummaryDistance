Proceedings of NAACL HLT 2007, pages 444?451,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsRandomized Decoding for Selection-and-Ordering ProblemsPawan Deshpande, Regina Barzilay and David R. KargerComputer Science and Articial Intelligence LaboratoryMassachusetts Institute of Technology{pawand,regina,karger}@csail.mit.eduAbstractThe task of selecting and ordering infor-mation appears in multiple contexts in textgeneration and summarization.
For in-stance, methods for title generation con-struct a headline by selecting and order-ing words from the input text.
In this pa-per, we investigate decoding methods thatsimultaneously optimize selection and or-dering preferences.
We formalize decod-ing as a task of finding an acyclic pathin a directed weighted graph.
Since theproblem is NP-hard, finding an exact so-lution is challenging.
We describe a noveldecoding method based on a randomizedcolor-coding algorithm.
We prove boundson the number of color-coding iterationsnecessary to guarantee any desired likeli-hood of finding the correct solution.
Ourexperiments show that the randomized de-coder is an appealing alternative to a rangeof decoding algorithms for selection-and-ordering problems, including beam searchand Integer Linear Programming.1 IntroductionThe task of selecting and ordering information ap-pears in multiple contexts in text generation andsummarization.
For instance, a typical multidocu-ment summarization system creates a summary byselecting a subset of input sentences and orderingthem into a coherent text.
Selection and ordering atthe word level is commonly employed in lexical re-alization.
For instance, in the task of title generation,the headline is constructed by selecting and orderingwords from the input text.Decoding is an essential component of theselection-and-ordering process.
Given selection andordering preferences, the task is to find a sequence ofelements that maximally satisfies these preferences.One possible approach for finding such a solutionis to decompose it into two tasks: first, select a setof words based on individual selection preferences,and then order the selected units into a well-formedsequence.
Although the modularity of this approachis appealing, the decisions made in the selection stepcannot be retracted.
Therefore, we cannot guaranteethat selected units can be ordered in a meaningfulway, and we may end up with a suboptimal output.In this paper, we investigate decoding methodsthat simultaneously optimize selection and order-ing preferences.
We formalize decoding as find-ing a path in a directed weighted graph.1 Thevertices in the graph represent units with associ-ated selection scores, and the edges represent pair-wise ordering preferences.
The desired solution isthe highest-weighted acyclic path of a prespecifiedlength.
The requirement for acyclicity is essentialbecause in a typical selection-and-ordering problem,a well-formed output does not include any repeatedunits.
For instance, a summary of multiple docu-ments should not contain any repeated sentences.1We assume that the scoring function is local; that is, it iscomputed by combining pairwise scores.
In fact, the majorityof models that are used to guide ordering (i.e., bigrams) are localscoring functions.444Since the problem is NP-hard, finding an exactsolution is challenging.
We introduce a novel ran-domized decoding algorithm2 based on the idea ofcolor-coding (Alon et al, 1995).
Although the algo-rithm is not guaranteed to find the optimal solutionon any single run, by increasing the number of runsthe algorithm can guarantee an arbitrarily high prob-ability of success.
The paper provides a theoreticalanalysis that establishes the connection between therequired number of runs and the likelihood of find-ing the correct solution.Next, we show how to find an exact solution usingan integer linear programming (ILP) formulation.Although ILP is NP-hard, this method is guaranteedto compute the optimal solution.
This allows us toexperimentally investigate the trade-off between theaccuracy and the efficiency of decoding algorithmsconsidered in the paper.We evaluate the accuracy of the decoding algo-rithms on the task of title generation.
The decod-ing algorithms introduced in the paper are comparedagainst beam search, a heuristic search algorithmcommonly used for selection-and-ordering and othernatural language processing tasks.
Our experimentsshow that the randomized decoder is an appealing al-ternative to both beam search and ILP when appliedto selection-and-ordering problems.2 Problem FormulationIn this section, we formally define the decoding taskfor selection-and-ordering problems.
First, we intro-duce our graph representation and show an exampleof its construction for multidocument summariza-tion.
(An additional example of graph constructionfor title generation is given in Section 6.)
Then, wediscuss the complexity of this task and its connec-tion to classical NP-hard problems.2.1 Graph RepresentationWe represent the set of selection units as the set ofvertices V in a weighted directed graph G. Theset of edges E represents pairwise ordering scoresbetween all pairs of vertices in V .
We also add aspecial source vertex s and sink vertex t. For eachvertex v in V , we add an edge from s to v and an2The code is available athttp://people.csail.mit.edu/pawand/rand/edge from v to t. We then define the set of all ver-tices as V ?
= V ?
{s, t}, and the set of all edges asE?
= E ?
{(s, v) ?
v ?
V } ?
{(v, t) ?
v ?
V }.To simplify the representation, we remove all ver-tex weights in our graph structure and instead shiftthe weight for each vertex onto its incoming edges.For each pair of distinct vertices (v, u) ?
V , we setthe weight of edge ev,u to be the sum of the loga-rithms of the selection score for u and the pairwiseordering score of (v, u).We also enhance our graph representation bygrouping sets of vertices into equivalence classes.We introduce these classes to control for redundancyas required in many selection-and-ordering prob-lems.3 For instance, in title generation, an equiva-lence class may consist of morphological variants ofthe same stem (i.e., examine and examination).
Be-cause a typical title is unlikely to contain more thanone word with the same stem, we can only select asingle representative from each class.Our task is now to find the highest weightedacyclic path starting at s and ending at t with k ver-tices in between, such that no two vertices belong tothe same equivalence class.2.2 Example: Decoding for MultidocumentSummarizationIn multidocument summarization, the vertices in thedecoding graph represent sentences from input doc-uments.
The vertices may be organized into equiva-lence classes that correspond to clusters of sentencesconveying similar information.
The edges in thegraph represent the combination of the selection andthe ordering scores.
The selection scores encode thelikelihood of a sentence to be extracted, while pair-wise ordering scores capture coherence-based prece-dence likelihood.
The goal of the decoder is to findthe sequence of k non-redundant sentences that op-timize both the selection and the ordering scores.Finding an acyclic path with the highest weight willachieve this goal.3An alternative approach for redundancy control would beto represent all the members of an equivalence class as a sin-gle vertex in the graph.
However, such an approach does notallow us to select the best representative from the class.
For in-stance, one element in the equivalence class may have a highlyweighted incoming edge, while another may have a highlyweighted outgoing edge.4452.3 Relation to Classical ProblemsOur path-finding problem may seem to be simi-lar to the tractable shortest paths problem.
How-ever, the requirement that the path be long makes itmore similar to the the Traveling Salesman Problem(TSP).
More precisely, our problem is an instance ofthe prize collecting traveling salesman problem, inwhich the salesman is required to visit k vertices atbest cost (Balas, 1989; Awerbuch et al, 1995).Since our problem is NP-hard, we might be pes-simistic about finding an exact solution.
But ourproblem has an important feature: the length k ofthe path we want to find is small relative to the num-ber of vertices n. This feature distinguishes our taskfrom other decoding problems, such as decoding inmachine translation (Germann et al, 2001), that aremodeled using a standard TSP formulation.
In gen-eral, the connection between n and k opens up a newrange of solutions.
For example, if we wanted tofind the best length-2 path, we could simply try allsubsets of 2 vertices in the graph, in all 2 possibleorders.
This is a set of only O(n2) possibilities, sowe can check all to identify the best in polynomialtime.This approach is very limited, however: in gen-eral, its runtime of O(nk) for paths of length kmakes it prohibitive for all but the smallest valuesof k. We cannot really hope to avoid the exponentialdependence on k, because doing so would give usa fast solution to an NP-hard problem, but there ishope of making the dependence ?less exponential.
?This is captured by the definition of xed parametertractability (Downey and Fellows, 1995).
A prob-lem is fixed parameter tractable if we can make theexponential dependence on the parameter k indepen-dent of the polynomial dependence on the problemsize n. This is the case for our problem: as we willdescribe below, an algorithm of Alon et al can beused to achieve a running time of roughly O(2kn2).In other words, the path length k only exponentiatesa small constant, instead of the problem size n, whilethe dependence on n is in fact quadratic.3 Related WorkDecoding for selection-and-ordering problems iscommonly implemented using beam search (Bankoet al, 2000; Corston-Oliver et al, 2002; Jin andHauptmann, 2001).
Being heuristic in nature, thisalgorithm is not guaranteed to find an optimal so-lution.
However, its simplicity and time efficiencymake it a decoding algorithm of choice for a widerange of NLP applications.
In applications wherebeam decoding does not yield sufficient accuracy,researchers employ an alternative heuristic search,A* (Jelinek, 1969; Germann et al, 2001).
While insome cases A* is quite effective, in other cases itsrunning time and memory requirements may equalthat of an exhaustive search.
Time- and memory-bounded modifications of A* (i.e., IDA-A*) do notsuffer from this limitation, but they are not guaran-teed to find the exact solution.
Nor do they pro-vide bounds on the likelihood of finding the exactsolution.
Newly introduced methods based on lo-cal search can effectively examine large areas of asearch space (Eisner and Tromble, 2006), but theystill suffer from the same limitations.As an alternative to heuristic search algorithms,researchers also employ exact methods from com-binatorial optimization, in particular integer linearprogramming (Germann et al, 2001; Roth and Yih,2004).
While existing ILP solvers find the exact so-lution eventually, the running time may be too slowfor practical applications.Our randomized decoder represents an impor-tant departure from previous approaches to decod-ing selection-and-ordering problems.
The theoreti-cally established bounds on the performance of thisalgorithm enable us to explicitly control the trade-off between the quality and the efficiency of the de-coding process.
This property of our decoder sets itapart from existing heuristic algorithms that cannotguarantee an arbitrarily high probability of success.4 Randomized Decoding withColor-CodingOne might hope to solve decoding with a dynamicprogram (like that for shortest paths) that grows anoptimal path one vertex at a time.
The problem isthat this dynamic program may grow to include avertex already on the path, creating a cycle.
One wayto prevent this is to remember the vertices used oneach partial path, but this creates a dynamic programwith too many states to compute efficiently.Instead, we apply a color coding technique of446Alon et al(1995).
The basic step of the algo-rithm consists of randomly coloring the graph ver-tices with a set of colors of size r, and using dy-namic programming to find the optimum length-kpath without repeated colors.
(Later, we describehow to determine the number of colors r.) Forbid-ding repeated colors excludes cycles as required, butremembering only colors on the path requires lessstate than remembering precisely which vertices areon the path.
Since we color randomly, any single it-eration is not guaranteed to find the optimal path; ina given coloring, two vertices along the optimal pathmay be assigned the same color, in which case theoptimal path will never be selected.
Therefore, thewhole process is repeated multiple times, increasingthe likelihood of finding an optimal path.Our algorithm is a variant of the original color-coding algorithm (Alon et al, 1995), which was de-veloped to detect the existence of paths of length kin an unweighted graph.
We modify the original al-gorithm to find the highest weighted path and alsoto handle equivalence classes of vertices.
In addi-tion, we provide a method for determining the opti-mal number of colors to use for finding the highestweighted path of length k.We first describe the dynamic programming algo-rithm.
Next, we provide a probabilistic bound onthe likelihood of finding the optimal solution, andpresent a method for determining the optimal num-ber of colors for a given value of k.Dynamic Programming Recall that we beganwith a weighted directed graphG to which we addedartificial start and end vertices s and t. We now posita coloring of that graph that assigns a color cv toeach vertex v aside from s and t. Our dynamic pro-gram returns the maximum score path of length k+2(including the artificial vertices s and t) from s to twith no repeated colors.Our dynamic program grows colorful paths?paths with at most one vertex of each color.
Fora given colorful path, we define the spectrum ofa path to be the set of colors (each used exactlyonce) of nodes on the interior of the path?we ex-clude the starting vertex (which will always be s)and the ending vertex.
To implement the dynamicprogram, we maintain a table q[v, S] indexed by apath-ending vertex v and a spectrum S. For vertexv and spectrum S, entry q[v, S] contains the valueof the maximum-score colorful path that starts at s,terminates at v, and has spectrum S in its interior.We initialize the table with length-one paths:q[v, ?]
represents the path from s to v, whose spec-trum is the empty set since there are no interior ver-tices.
Its value is set to the score of edge (s, v).
Wethen iterate the dynamic program k times in orderto build paths of length k + 1 starting at s. We ob-serve that the optimum colorful path of length ` andspectrum S from s to v must consist of an optimumpath from s to u (which will already have been foundby the dynamic program) concatenated to the edge(u, v).
When we concatenate (u, v), vertex u be-comes an interior vertex of the path, and so its colormust not be in the preexisting path?s spectrum, butjoins the spectrum of the path we build.
It followsthatq[v, S] = max(u,v)?G,cu?S,cv /?Sq[u, S?
{cu}] +w(u, v)After k iterations, for each vertex v we will havea list of optimal paths from s to v of length k + 1with all possible spectra.
The optimum length-k+ 2colorful path from s to t must follow the optimumlength-k + 1 path of some spectrum to some penul-timate vertex v and then proceed to vertex t; we findit by iterating over all such possible spectra and allvertices v to determine argmaxv,Sq[v, S]+w(v, t).Amplification The algorithm of Alon et al, andthe variant we describe, are somewhat peculiar inthat the probability of finding the optimal solu-tion in one coloring iteration is quite small.
Butthis can easily be dealt with using a standard tech-nique called amplication (Motwani and Raghavan,1995).
Suppose that the algorithm succeeds withsmall probability p, but that we would like it to suc-ceed with probability 1 ?
?
where ?
is very small.We run the algorithm t = (1/p) ln 1/?
times.
Theprobability that the algorithm fails every single runis then (1 ?
p)t ?
e?pt = ?.
But if the algorithmsucceeds on even one run, then we will find the op-timum answer (by taking the best of the answers wesee).No matter how many times we run the algo-rithm, we cannot absolutely guarantee an optimalanswer.
However, the chance of failure can easily bedriven to negligible levels?achieving, say, a one-in-a-billion chance of failure requires only 20/p itera-447tions by the previous analysis.Determining the number of colors Suppose thatwe use r random colors and want to achieve a givenfailure probability ?.
The probability that the opti-mal path has no repeated colors is:1 ?
r ?
1r ?r ?
2r ?
?
?r ?
(k ?
1)r .By the amplification analysis, the number of trialsneeded to drive the failure probability to the desiredlevel will be inversely proportional to this quantity.At the same time, the dynamic programming tableat each vertex will have size 2r (indexing on a bitvector of colors used per path), and the runtime ofeach trial will be proportional to this.
Thus, the run-ning time for the necessary number of trials Tr willbe proportional to1 ?
rr ?
1 ?rr ?
2 ?
?
?rr ?
(k ?
1) ?
2rWhat r ?
k should we choose to minimize thisquantity?
To answer, let us consider the ratio:Tr+1Tr=(r + 1r)k?
r ?
(k ?
1)r + 1 ?
2= 2(1 + 1/r)k(1?
k/(r + 1))If this ratio is less than 1, then using r + 1 col-ors will be faster than using r; otherwise it will beslower.
When r is very close to k, the above equa-tion is tiny, indicating that one should increase r.When r  k, the above equation is huge, indicatingone should decrease r. Somewhere in between, theratio passes through 1, indicating the optimum pointwhere neither increasing nor decreasing r will help.If we write ?
= k/r, and consider large k, then Tr+1Trconverges to 2e?(1??).
Solving numerically to findwhere this is equal to 1, we find ?
?
.76804, whichyields a running time proportional to approximately(4.5)k.In practice, rather than using an (approximate)formula for the optimum r, one should simply plugall values of r in the range [k, 2k] into the running-time formula in order to determine the best; doingso takes negligible time.5 Decoding with Integer LinearProgrammingIn this section, we show how to formulate theselection-and-ordering problem in the ILP frame-work.
We represent each edge (i, j) from vertex ito vertex j with an indicator variable Ii,j that is setto 1 if the edge is selected for the optimal path and 0otherwise.
In addition, the associated weight of theedge is represented by a constant wi,j .The objective is then to maximize the followingsum:maxI?i?V?j?Vwi,jIi,j (1)This sum combines the weights of edges selected tobe on the optimal path.To ensure that the selected edges form a validacyclic path starting at s and ending at t, we intro-duce the following constraints:Source-Sink Constraints Exactly one edge orig-inating at source s is selected:?j?VIs,j = 1 (2)Exactly one edge ending at sink t is selected:?i?VIi,t = 1 (3)Length Constraint Exactly k + 1 edges are se-lected: ?i?V?j?VIi,j = k + 1 (4)The k + 1 selected edges connect k + 2 vertices in-cluding s and t.Balance Constraints Every vertex v ?
V has in-degree equal to its out-degree:?i?VIi,v =?i?VIv,j ?
v ?
V ?
(5)Note that with this constraint, a vertex can have atmost one outgoing and one incoming edge.Redundancy Constraints To control for redun-dancy, we require that at most one representativefrom each equivalence class is selected.
Let Z bea set of vertices that belong to the same equivalenceclass.
For every equivalence class Z, we force thetotal out-degree of all vertices in Z to be at most 1.448s tFigure 1: A subgraph that contains a cycle, whilesatisfying constraints 2 through 5.?i?Z?j?VIi,j ?
1 ?
Z ?
V (6)Acyclicity Constraints The constraints intro-duced above do not fully prohibit the presence ofcycles in the selected subgraph.
Figure 1 shows anexample of a selected subgraph that contains a cyclewhile satisfying all the above constraints.We force acyclicity with an additional set of vari-ables.
The variables fi,j are intended to number theedges on the path from 1 to k+ 1, with the first edgegetting number fi,j = k + 1, and the last gettingnumber fi,j = 1.
All other edges will get fi,j = 0.To enforce this, we start by ensuring that only theedges selected for the path (Ii,j = 1) get nonzerof -values:0 ?
fi,j ?
(k + 1) Ii,j ?
i, j ?
V (7)When Ii,j = 0, this constraint forces fi,j = 0.When Ii,j = 1, this allows 0 ?
fi,j ?
k+1.
Now weintroduce additional variables and constraints.
Weconstrain demand variables dv by:dv =?i?VIi,v ?
v ?
V ?
?
{s} (8)The right hand side sums the number of selectededges entering v, and will therefore be either 0 or 1.Next we add variables av and bv constrained by theequations:av =?i?Vfi,v (9)bv =?i?Vfv,i (10)Note that av sums over f values on all edges enter-ing v. However, by the previous constraints thosef -values can only be nonzero on the (at most one)selected edge entering v. So, av is simply the f -value on the selected edge entering v, if one exists,and 0 otherwise.
Similarly, bv is the f -value on the(at most one) selected edge leaving v.Finally, we add the constraintsav ?
bv = dv v 6= s (11)bs = k + 1 (12)at = 1 (13)These last constraints let us argue, by induction, thata path of length exactly k + 1 must run from s to t,as follows.
The previous constraints forced exactlyone edge leaving s, to some vertex v, to be selected.The constraint bs = k+ 1 means that the f -value onthis edge must be k + 1.
The balance constraint onv means some edge must be selected leaving v. Theconstraint av ?
bv = dv means this edge must havef -value k. The argument continues the same way,building up a path.
The balance constraints meanthat the path must terminate at t, and the constraintthat at = 1 forces that termination to happen afterexactly k + 1 edges.4For those familiar with max-flow, our programcan be understood as follows.
The variables I forcea flow, of value 1, from s to t. The variables f rep-resent a flow with supply k + 1 at s and demand dvat v, being forced to obey ?capacity constraints?
thatlet the flow travel only along edges with I = 1.6 Experimental Set-UpTask We applied our decoding algorithm to the taskof title generation.
This task has been extensivelystudied over the last six years (Banko et al, 2000; Jinand Hauptmann, 2001).
Title generation is a classicselection-and-ordering problem: during title realiza-tion, an algorithm has to take into account both thelikelihood of words appearing in the title and theirordering preferences.
In the previous approaches,beam search has been used for decoding.
Therefore,it is natural to explore more sophisticated decodingtechniques like the ones described in this paper.Our method for estimation of selection-and-ordering preferences is based on the technique de-scribed in (Banko et al, 2000).
We compute the4The network flow constraints allow us to remove the previ-ously placed length constraint.449likelihood of a word in the document appearing inthe title using a maximum entropy classifier.
Everystem is represented by commonly used positionaland distributional features, such as location of thefirst sentence that contains the stem and its TF*IDF.We estimate the ordering preferences using a bigramlanguage model with Good-Turing smoothing.In previous systems, the title length is either pro-vided to a decoder as a parameter, or heuristics areused to determine it.
Since exploration of theseheuristics is not the focus of our paper, we providethe decoder with the actual title length (as measuredby the number of content words).Graph Construction We construct a decodinggraph in the following fashion.
Every unique con-tent word comprises a vertex in the graph.
All themorphological variants of a stem belong to the sameequivalence class.
An edge (v, u) in the graph en-codes the selection preference of u and the likeli-hood of the transition from v to u.Note that the graph does not contain any auxiliarywords in its vertices.
We handle the insertion of aux-iliary words by inserting additional edges.
For everyauxiliary word x, we add one edge representing thetransition from v to u via x, and the selection pref-erence of u.
The auxiliary word set consists of 24prepositions and articles extracted from the corpus.Corpus Our corpus consists of 547 sections of acommonly used undergraduate algorithms textbook.The average section contains 609.2 words.
A title,on average, contains 3.7 words, among which 3.0 arecontent words; the shortest and longest titles have 1and 13 words respectively.
Our training set consistsof the first 382 sections, the remaining 165 sectionsare used for testing.
The bigram language model isestimated from the body text of all sections in thecorpus, consisting of 461,351 tokens.To assess the importance of the acyclicity con-straint, we compute the number of titles that haverepeated content words.
The empirical findings sup-port our assumption: 97.9% of the titles do not con-tain repeated words.Decoding Algorithms We consider three decod-ing algorithms: our color-coding algorithm, ILP, andbeam search.5 The beam search algorithm can only5The combination of the acyclicity and path length con-straints require an exponential number of states for A* sinceeach state has to preserve the history information.
This preventsconsider vertices which are not already in the path.6To solve the ILP formulations, we employ aMixed Integer Programming solver lp solve whichimplements the Branch-and-Bound algorithm.
Weimplemented the rest of the decoders in Python withthe Psyco speed-up module.
We put substantial ef-fort to optimize the performance of all of the al-gorithms.
The color-coding algorithm is imple-mented using parallelized computation of coloringiterations.7 ResultsTable 1 shows the performance of various decodingalgorithms considered in the paper.
We first evalu-ate each algorithm by the running times it requiresto find all the optimal solutions on the test set.
SinceILP is guaranteed to find the optimal solution, wecan use its output as a point of comparison.
Table 1lists both the average and the median running times.For some of the decoding algorithms, the differencebetween the two measurements is striking ?
6,536seconds versus 57.3 seconds for ILP.
This gap can beexplained by outliers which greatly increase the av-erage running time.
For instance, in the worst case,ILP takes an astounding 136 hours to find the opti-mal solution.
Therefore, we base our comparison onthe median running time.The color-coding algorithm requires a mediantime of 9.7 seconds to find an optimal solution com-pared to the 57.3 seconds taken by ILP.
Furthermore,as Figure 2 shows, the algorithm converges quickly:just eleven iterations are required to find an optimalsolution in 90% of the titles, and within 35 itera-tions all of the solutions are found.
An alternativemethod for finding optimal solutions is to employ abeam search with a large beam size.
We found thatfor our test set, the smallest beam size that satisfiesthis condition is 1345, making it twenty-three timesslower than the randomized decoder with respect tothe median running time.Does the decoding accuracy impact the quality ofthe generated titles?
We can always trade speed foraccuracy in heuristic search algorithms.
As an ex-treme, consider a beam search with a beam of size1: while it is very fast with a median running timeus from applying A* to this problem.6Similarly, we avoid redundancy by disallowing two verticesfrom the same equivalence class to belong to the same path.450Average (s) Median (s) ROUGE-L Optimal Solutions (%)Beam 1 0.6 0.4 0.0234 0.0Beam 80 28.4 19.3 0.2373 64.8Beam 1345 368.6 224.4 0.2556 100.0ILP 6,536.2 57.3 0.2556 100.0Color-coding 73.8 9.7 0.2556 100.0Table 1: Running times in seconds, ROUGE scores, and percentage of optimal solutions found for each ofthe decoding algorithms.0204060801000  5  10  15  20  25  30  35Exact Solutions(%)IterationsFigure 2: The proportion of exact solutions foundfor each iteration of the color coding algorithm.of less than one second, it is unable to find any ofthe optimal solutions.
The titles generated by thismethod have substantially lower scores than thoseproduced by the optimal decoder, yielding a 0.2322point difference in ROUGE scores.
Even a largerbeam size such as 80 (as used by Banko et al (2000))does not match the title quality of the optimal de-coder.8 ConclusionsIn this paper, we formalized the decoding task forselection-and-ordering as a problem of finding thehighest-weighted acyclic path in a directed graph.The presented decoding algorithm employs random-ized color-coding, and can closely approximate theILP performance, without blowing up the runningtime.
The algorithm has been tested on title genera-tion, but the decoder is not specific to this task andcan be applied to other generation and summariza-tion applications.9 AcknowledgementsThe authors acknowledge the support of the Na-tional Science Foundation (CAREER grant IIS-0448168 and grant IIS-0415865).
We also wouldlike to acknowledge the MIT NLP group and theanonymous reviewers for valuable comments.ReferencesN.
Alon, R. Yuster, U. Zwick.
1995.
Color-coding.
Jour-nal of the ACM (JACM), 42(4):844?856.B.
Awerbuch, Y. Azar, A. Blum, S. Vempala.
1995.Improved approximation guarantees for minimum-weight k-trees and prize-collecting salesmen.
In Pro-ceedings of the STOC, 277?283.E.
Balas.
1989.
The prize collecting traveling salesmanproblem.
Networks, 19:621?636.M.
Banko, V. O. Mittal, M. J. Witbrock.
2000.
Headlinegeneration based on statistical translation.
In Proceed-ings of the ACL, 318?325.S.
Corston-Oliver, M. Gamon, E. Ringger, R. Moore.2002.
An overview of amalgam: A machine-learnedgeneration module.
In Proceedings of INLG, 33?40.R.
G. Downey, M. R. Fellows.
1995.
Fixed-parametertractability and completeness II: On completeness forW [1].
Theoretical Computer Science, 141(1?2):109?131.J.
Eisner, R. W. Tromble.
2006.
Local search with verylarge-scale neighborhoods for optimal permutationsin machine translation.
In Proceedings of the HLT-NAACL Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Process-ing.U.
Germann, M. Jahr, K. Knight, D. Marcu, K. Yamada.2001.
Fast decoding and optimal decoding for ma-chine translation.
In Proceedings of the EACL/ACL,228?235.F.
Jelinek.
1969.
A fast sequential decoding algorithmusing a stack.
IBM Research Journal of Research andDevelopment.R.
Jin, A. G. Hauptmann.
2001.
Automatic title genera-tion for spoken broadcast news.
In Proceedings of theHLT, 1?3.R.
Motwani, P. Raghavan.
1995.
Randomized Algo-rithms.
Cambridge University Press, New York, NY.D.
Roth, W. Yih.
2004.
A linear programming formula-tion for global inference in natural language tasks.
InProceedings of the CONLL, 1?8.451
