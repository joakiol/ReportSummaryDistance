The Exploitation of Spatial Information in Narrative DiscourseBlake Stephen HowaldGeorgetown Universitybsh25@georgetown.eduE.
Graham KatzGeorgetown Universityegk7@georgetown.eduAbstractWe present the results of several machine learning tasks that exploit explicit spatial languageto classify rhetorical relations and the spatial information of narrative events.
Three corpora areannotated with figure and ground (granularity) relationships, mereotopologically classified verbsand prepositions, and frames of reference.
For rhetorical relations, Na?
?ve Bayesian models achieve84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION re-lations respectively (16% and 23% above baseline).
For the spatial information of narrative events,K* models achieve 55.68% average accuracy (12% above baseline) for all spatial information types.This result is boosted to 71.85% (28% above baseline) when inertial spatial reference and text se-quence information are considered.
Overall, spatial information is shown to be central to narrativediscourse structure and prediction tasks.1 IntroductionClauses in discourse are related to one another in a number of semantic and pragmatic ways.
Some of themost prominent are temporal relations that hold among the times of events and states described (Partee,1984; Pustejovsky et al, 2003) and the rhetorical relations that hold between a pair of clauses (Mannand Thompson, 1987; Asher and Lascarides, 2003).
For example, (1) illustrates the NARRATION relationwhich obtains between (1a-b) and between (1b-c).
(1) a. Klose was sitting with his teammates.b.
He walked to the sidelines.c.
Then he entered the game.Because of the temporal properties of NARRATION (Asher and Lascarides 2003, p. 462), the eventdescribed in (1a) is taken to precede that described in (1b) and (1b)?s event to precede (1c)?s.
As Asherand Lascarides show, there is a close tie between the rhetorical structure of a discourse and its temporalstructure.
In (2), for example, the fact that the clauses are related by ELABORATION entails that thetemporal relation between (2a) and (2b) is inclusion.
(2) a. Klose scored a goal.b.
He headed the ball into the upper corner.We observe that the spatial relations among the locations of the events described in these discoursesare also highly determined by the rhetorical relations between the clauses used to describe them.
Inthe NARRATION-related discourse (1), there is a spatial progression: Klose is located relative to histeammates (1a), he then moves from the bench to the sidelines (1b), and then he moves from the sidelinesinto the game (1c).
In the ELABORATION-related discourse (2), there is no such progression.In this paper, we investigate the degree to which the spatial structure of discourse and its rhetoricalstructure are co-determined.
Using supervised machine learning techniques (Witten and Frank, 2002),we evaluate two hypotheses: (a) spatial information encoded in adjacent clauses is highly predictive ofthe rhetorical relations that hold between them and (b) spatial information is highly predictable based onassociated spatial information within narrative event clauses.
To do this, we build a corpus of narrativetexts which are annotated both for spatial information (figure and ground (granularity) relationships,175mereotopologically classified verbs and prepositions, and frames of reference) and rhetorical relations (abinary NARRATION vs. ELABORATION/BACKGROUND distinction discussed in Section 3.2).
This corpusis then used to train two types of classifiers - one type that classifies the rhetorical relations holdingbetween clauses on the basis of spatial information, and another type that classifies spatial relationshipswithin clauses where the NARRATION relation holds.
The results support both hypotheses and indicatethe centrality of spatial information to narrative discourse structure and associated classification tasks.2 Background and Related Research2.1 Rhetorical RelationsRhetorical relations describe the role that one clause plays with respect to another in a text and contributesto a text?s coherence (Hobbs, 1985).
As such, these relations are pragmatic features of a text.
In NLPgenerally, classifying rhetorical relations has been an important area of research (Marcu, 2000; Sporlederand Lascarides, 2005) and has been shown to be useful for tasks such as text summarization (Marcu,1998).
The inventory of rhetorical relations in Segmented Discourse Representation Theory (SDRT)(Asher and Lascarides, 2003) is widely used in these applications.
This inventory includes the followingrelations, illustrated by example: NARRATION: Klose got up.
He entered the game.
ELABORATION:Klose pushed the Serbian midfielder.
He knew him from school.
BACKGROUND: Klose entered the game.The pitch was very wet.
EXPLANATION: Klose received a red card.
He pushed the Serbian midfielder.CONSEQUENCE: If Klose received a red card, then he pushed the Serbian midfielder.
RESULT: Klosepushed the Serbian midfielder.
He received a red card.
ALTERNATION: Klose received a red card or hereceived a yellow card.
CONTINUATION: Klose received a red card.
Ronaldo received a yellow card.In previous work, rhetorical relations have been predicted based on a range of features includingdiscourse connectives, relation location, clause length, part-of-speech, content and function words, andsyntactic features (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004).
These systems have a widerange of average accuracies for all relations sought to be predicted - e.g.
33.96% (Marcu and Echihabi,2002) to 70.70% (Lapata and Lascarides, 2004) - and individual relations - e.g.
RESULT - 16.21% andEXPLANATION - 75.39% (Marcu and Echihabi, 2002) and CONTRAST - 43.64% and CONTINUATION -83.35% (Sporleder and Lascarides, 2005).
Our focus is on the NARRATION, BACKGROUND and ELAB-ORATION relations, which account for over 90% of the discourses in our corpus.2.2 Spatial Language and DiscourseSpatial language has been discussed in a number of NLP contexts.
For example, linking natural languagewith physical locations via semantic mark-up (e.g.
SpatialML (MITRE, 2009)); spatial description andwayfinding tasks (e.g.
Anderson et al, 1991); and dialogue systems (e.g.
Coventry et al, 2009), justto name a very few.
Perspectives on spatial language are similarly varied in terms of their focus andtheoretical background (e.g.
cognitive, semantic and syntactic); however, common threads do emerge.First, all physical spatial references are reducible to figure and ground relationships (Talmy, 2000).
InEnglish, these are triggered by a deictic verb or adverb (e.g.
went, here) (3a); a spatial preposition (e.g.in, at) (3b); a particle verb (e.g.
put on, got out) (3c); or a motion verb (e.g.
drive, follow) (3d).
(3) a.
[Ronaldo]figure is [here]ground.b.
[Ronaldo]figure is in [the park]ground.c.
[Ronaldo]figure rolled over [?]ground.d.
[Ronaldo]figure ran to [the park]ground.Second, figure and ground relationships qualitatively vary by the type of verb and preposition cre-ating the relationship.
These differences can be modeled in mereotopology, which defines spatial re-lationships in terms of regions and connections (e.g.
RCC-8 (Randell et al, 1992)).
We follow Asherand Sablayrolles (1995) who classify prepositions based on the position (Position - at, Initial Direction- from, Medial Position - through, Final Position - to) and contact (Inner - in, Contact - against, Outer176- along, and Outer-Most - beyond) of two regions (figure and ground).
For verbs, Muller (2002) pro-poses six mereotopological classes: Reach, Leave, Internal, External, Hit, and Cross.
Pustejovsky andMoszkowicz (2008) mapped Muller?s classes to FrameNet and VerbNet and propose ten general classesof motion (Move, Move-External, Move-Internal, Leave, Reach, Detach, Hit, Follow, Deviate, Stay).Third, figure and ground relationships vary by the perspective used to describe the relationship.For this discussion, perspective takes two forms, granularity of spatial description (following Montello(1993)) and frames of reference (following Levinson (1996)).
Granularity refers to the level of detailin a given spatial description.
Montello (1993, p. 315) indicates four spatial granularities based on thecognitive organization of spatial knowledge (summarized in (4)).
(4) a. Ronaldo jumped on the ball.b.
Ronaldo is in the corner.c.
Ronaldo is running around the field.d.
Ronaldo is in Cape Town.
(4a) is a Figural granularity which describes space smaller than the human body.
(4b) is a Vista gran-ularity which describes space from a single point of view.
(4c) is an Environmental granularity whichdescribes space larger than the body with multiple (scanning) point(s) of view.
(4d) is a Geographicgranularity which describes space even larger than the body and is learned by symbolic representation.Frames of reference provide different ways of describing the same spatial relationships.
For example,given a static scene of Ronaldo sitting on a bench next to his coach, each utterance in (5) would be anaccurate spatial description.
(5) a. Deictic: Ronaldo is there.b.
Contiguity: Ronaldo is on the bench.c.
Named Location: Ronaldo is at the sideline.d.
Relative: Ronaldo is in front of me.e.
Intrinsic: Ronaldo is behind his coach.f.
Absolute: Ronaldo is north of his coach.
(5a-c) are non-coordinated as they relate just the figure and ground.
Coordinated information, relatingthe figure to an additional entity within the ground, occurs in (5d-f).
Frames of reference apply to bothstatic and dynamic relationships (Levinson, 1996, p. 360).In terms of attending to spatial information in discourse, Herman (2001) argues that spatial informa-tion patterns in narrative discourse carve out spatially defined domains that group narrative actions.
Inparticular, the emergence and change in different types of spatial reference to physical location (discoursecues) create maps of the narrative actions.
These discourse cues include figure, ground and path (motion)relationships (3); frames of reference (5); and deictic shifts - here vs. there.
Herman?s demonstration isbased on ghost story narratives that are rich in spatial reference.Howald (2010) showed in a corpus of serial killer first person narratives, also rich in spatial reference,that these spatial narrative domains, in the form of abstract Pre-Crime, Crime and Post-Crime events,were predicted to a 90% accuracy from three spatial features (figure, ground, and spatial verb) anddiscourse sequence.
Overall, research by Herman (2001) and Howald (2010) demonstrates some levelof dependency between spatial information and discourse structure.
The present research addresses thespecific question of whether there is a systematic relationship between spatial information and temporalinformation via rhetorical relations and the spatial architecture of narrative events.3 Data and Annotation3.1 DataThree corpora of narrative discourse were annotated with rhetorical and spatial information.
These cor-pora were then used to train and test machine learning systems.
Summarized in Table 1, the three dif-ferent narrative corpora selected for analysis were: (1) narratives from serial criminals (CRI) - oral and177written confession statements and guilty pleas; (2) American National Corpus Charlotte Narrative andConversation Collection (Ide and Suderman, 2007) (ANC) - oral narratives in conversations collected ina sociolinguistic interview format; and (3) The Degree Confluence Project (DEG) - this project, whichseeks to map all possible latitude-longitude intersections on Earth, requires that participants who visitthese intersections provide written narratives of the visit for inclusion on the project?s website.Table 1: Relation and Spatial Clause DistributionCorpus ANC (n=20) DEG (n=20) CRI (n=20) Total (N=60)Total Clauses 588 611 1,710 2,909Spatial Clauses 260 354 932 1,546Average 44.21 57.93 54.50 53.14Total Rhetorical 568 591 1,690 2,848Spatial Rhetorical 259 345 929 1,533Average 45.59 58.37 55.00 53.8220 narratives from each corpus were selected.
There was a total of 2,909 (independent) clauses with1,546 of those clauses containing spatial information - spatial clauses (53.14% on average).
There was atotal of 2,848 relations with 1,533 of those relations where both clauses contained spatial information -spatial rhetorical (53.82% on average).3.2 Spatial Information and Rhetorical Relation AnnotationWe developed a coding scheme for spatial information that consolidates the insights on spatial langaugediscussed in Section 2.2.?
FIGURE is an indication of grammatical person or a non-person entity (1 = I, my; 2 = you, your;3 = he, she, it, his, her; 4 = we, our; 5 = you, your; 6 = they, their; NP = the purse, a bench, threecars);?
VERB is one of the four mereotopological classes - a consolidation of Pustejovsky and Moszkow-icz?s (2008) ten classifications (State = was, stay, was sitting; Move = run, go, jump; Outside =follow, pass, track; Hit = attach, detach, strike);?
PREPOSITION is one of four mereotopological classes based on Asher and Sablayrolles (1995)(Positional = in, on; Initial = from ; Medial = through; Final = to);?
GROUND is one of four granularities (Figural, Environmental, Vista, Geographic) (see (4)above);?
FRAME is one of six frames of reference (Deictic, Contiguity, Named Location, Relative, In-trinsic, Absolute) (see (5) above).The three corpora were annotated by one of the authors.
Annotation occurred one narrative at atime and any information from that narrative could be used to resolve rhetorical relations and spatialinformation.
A reference sheet including several examples of each coding element was available tothe annotator.
The annotation happened in two phases.
First, each pair of clauses was annotated withan SDRT relation.
Second, each clause that contained a physical figure and ground relationship wasidentified.
The figure, ground, preposition and verb were annotated with a Figure, Verb, Preposition,Ground, and Frame.
We illustrate with (6) where the NARRATION relation obtains between (6a-b).
(6) a. Kaka kicked the ball into the goal.b.
Then he ran to the left side of the bench.178The spatial annotation of (6a) is: FIGURE = NP, the ball; VERB = Hit (H), kicked; PREPOSITION =Final (F), into; GROUND = Environmental (E), the goal; and FRAME = Contiguity (C).
The spatialannotation of (6b) is: FIGURE = 3, he; VERB = Move (M), ran; PREPOSITION = Final (F), to the leftside of; GROUND = Environmental (E), the bench; and FRAME = Intrinsic (INT).
The distribution ofspatial rhetorical relations is summarized in Table 2.Table 2: Spatial Rhetorical Relation Distribution per CorpusRelation ANC DEG CRI TotalNARRATION 133 124 654 911BACKGROUND 74 87 238 399ELABORATION 34 63 17 114CONTINUATION 14 27 10 51RESULT 3 22 0 25EXPLANATION 0 16 1 17ALTERNATION 0 0 9 9CONSEQUENCE 1 6 0 7Total 259 345 929 1,533An additional individual was queried for inter-rater reliability against the author annotation.
The raterwas given roughly one-third of the data (10 narratives (4 ANC, 4 DEG, 2 CRI) accounting for 510 spatialclause pairs), the same example sheet used by the author, and as much time as needed to complete thetask.
Average agreement and Cohen?s kappa statistics (Cohen, 1960) were computed between the inter-rater and the author for the spatial annotations and NARRATION, BACKGROUND, and ELABORATIONcodings.
Individually, BACKGROUND and ELABORATION have low interannotator agreement (?
= 32.92and 54.20 respectively), but these two relations were often confused (26% of BACKGROUND relationscoded as ELABORATION and 12% of ELABORATION relations coded as BACKGROUND).
As illustratedin (7-8), both BACKGROUND and ELABORATION add information to the surrounding state of affairs.
(7) a. Klose entered the game.b.
The pitch was very wet.
(8) a. Klose pushed the Serbian midfielder.b.
He knew him from school.As evidenced by the annotation confusions, the difference between these relations is difficult to distin-guish and the distinction made by Asher and Lascarides (2003) is subtle - BACKGROUND?s temporalconsequence is one of overlap and ELABORATION, a subordinating relation, is one of part-of.
Howevercollapsing these relations resulted in a fairly reliably distinguished category.
Average agreement andkappa statistics are summarized in Table 3.Table 3: Agreement and Kappa Statistics for Relation and Spatial CodingsCoding Agreement (%) Kappa (?
)All Rhetorical Relations 71.97 60.27NARRATION 86.32 74.36BACKGROUND / ELABORATION 73.40 62.20Figure 94.91 89.92Verb 90.90 81.80Preposition 78.35 56.70Granularity 87.87 75.74Frame 69.38 38.76179For rhetorical relations, the average agreement and kappa statistic are consistent with previously re-ported performances (e.g.
Agreement = 71.25 / ?
= 61.00 (Sporleder and Lascarides, 2005)).
We havenot been able to find previously reported performance accuracies for NARRATION, ELABORATION andBACKGROUND relations specifically.
However, ?
statistics from 60.00 to 75.00 and above are consideredacceptable (e.g.
Landis and Koch, 1977).
For the spatial codings, the average agreements are relativelyhigh with Preposition and Frame falling lowest.
There is no basis for direct comparison of these num-bers to other research as the coding scheme is novel.4 Machine Learning ExperimentsWe constructed two machine learning tasks to exploit the annotated spatial information to determine whatcontributions the information is making to narrative structure.
The first task evaluates the prediction ofNARRATION and BACKGROUND/ ELABORATION relations based on pairs of spatial clauses.
The secondtask evaluates the prediction of spatial information types, based on the other spatial information types inthat clause, in individual clauses where the NARRATION relation holds.4.1 Rhetorical Relation Prediction4.1.1 Methods and ResultsTask 1 builds a 2-way classifier for the NARRATION and BACKGROUND/ ELABORATION relations.Clause pairs were coded as vectors (n = 1,424) - for example, the vector for (6) is NP3, HM, FF,EE, CINT.
These vectors were used to train and test (10-fold cross-validation) a number of classifiers.The Na?
?ve Bayes classifier performed the best.
Results are reported in Table 4.Table 4: Na?
?ve Bayes Classification Accuracy and F-Measures for Task 1NARRATION Accuracy (% / baseline) Precision Recall F-ScoreANC 63.29 / 58 .676 .633 .654DEG 75.71 / 61 .803 .757 .779CRI 90.12 / 73 .822 .901 .860TOTAL 84.90 / 68 .808 .841 .824BACK/ ELAB Accuracy (% / baseline) Precision Recall F-ScoreANC 57.89 / 41 .532 .579 .555DEG 70.11 / 38 .642 .701 .670CRI 45.63 / 26 .624 .456 .527TOTAL 57.87 / 35 .622 .567 .593For all corpora combined, the majority class (?baseline?)
for NARRATION is 68% and 26% for BACK-GROUND / ELABORATION; the classifier performs 16% and 22% above baseline respectively.
The differ-ence between the NARRATION and BACKGROUND / ELABORATION relations and baselines is statisticallysignificant for each corpus and all corpora combined - ANC: ?2 = 25.64, d.f.
= 1, p ?
.001; DEG: ?2 =33.86, d.f.
= 1, p ?
.001; CRI: ?2 = 22.69, d.f.
= 1, p ?
.001; and TOTAL:?2 = 34.09, d.f.
= 1, p ?
.001.4.1.2 DiscussionAgain, we have not been able to find reported results for a direct comparison of NARRATION and BACK-GROUND/ ELABORATION.
However, the 84.90% and 57.87% (at 16% and 22% over baseline) perfor-mance of our Na?
?ve Bayesian model is consistent with results reported in similar tasks.
For example,Marcu and Echihabi (2002) report an average accuracy of 33.96% (5-way classifier) and 49.70% (6-wayclassifier) based on training with very large data sets.
Sporleder and Lascarides (2005) report a 57.55%average accuracy, based on training with large data sets, which is 20% over Marcu and Echihabi?s 5-way180classifier and almost 40% over a random 20% baseline.
Lapata and Lascarides (2004) report an averageaccuracy of 70.70% for inferring temporal relations based on training.We ran an additional set of experiments to determine the relative contribution of spatial features topredict NARRATION and BACKGROUND / ELABORATION relations.
As shown in Table 5, Figure andVerb outperform Ground, Preposition and Frame in accuracy.
Figure performs at a 71% averageaccuracy (85% for NARRATION and 40% for BACKGROUND/ ELABORATION) and Verb performs at a74% average accuracy (84% for NARRATION and 54% for BACKGROUND/ ELABORATION).
Figure andVerb appear to be most discriminating.
Note that we are not suggesting that subject and verb generallyare similarly discriminatory - Figure and Verb in this task are overtly spatial.
Despite the performanceof Figure and Verb, different subsets of spatial information worked better (we ran all permutations ofspatial features - the top five are listed in Table 5).
However, the difference in performance is negligible.For example, the best subset of Figure, Verb and Ground (85% and 58%) only performed 1% aboveNARRATION and BACKGROUND/ ELABORATION prediction based on all five features combined.Table 5: Single and Combined Spatial Feature PerformanceFeature NARRATION BACK/ ELAB Features NARRATION BACK/ ELABFigure (F) 85.58 40.33 FVG 85.24 58.33Verb (V) 84.59 54.97 VGP 84.34 58.33Prepostion (P) 97.34 1.00 FVGR 86.33 56.45Ground (G) 97.33 1.00 FV 86.56 56.90Frame (R) 98.02 2.00 VG 85.37 57.33These results tell us several things about the relationship between spatial information and rhetoricalstructure as it applies to narrative discourse.
First, spatial information predicts rhetorical structure asgood as non-spatial types of linguistic information reported in other investigations and with many fewerfeatures.
For example, Sporleder and Lascarides (2005) rely on 72 different features falling into nineclasses whereas we rely on 14 features in five classes.
This suggests that spatial information is not onlycentral to rhetorical stucture, like temporal components, but central to the task of prediction.
Second,while the type of spatial information that predicts rhetorical structure is based on the primary figure andground relationship, it is the qualitative semantic variations within these elements that is providing thediscrimination.
It is the organization of spatial relationships - (Verb and Preposition) and the perspectiveprovided by the narrator (Figure, Ground and Frame) combined - rather than any individual elements.4.2 Spatial Information Prediction4.2.1 Methods and ResultsTask 2 is a series of five experiments.
Each experiment builds a classifier for each type of spatial infor-mation: a 6-way classifier for Frame; a 5-way classifier for Figure (Figure types 2 and 5 did not occurin our corpus); and 4-way classifiers for Ground, Preposition and Verb.
Single clauses that contributeto the NARRATION relation were coded as vectors (n = 911) - for example, the single vectors for (6a)and (6b) are NP, H, F, E, C and 3, M, F, E, INT.
These vectors were used to train and test (10-foldcross-validation) a number of classifiers to predict one of the five spatial features given the remainingfour.
The K* classifier performed the best.
Results are reported in Table 6.
For all corpora combined, theK* classifier performs above baseline for all spatial information (Figure = 9%, Verb = 17%, Preposition= 9%, Ground = 19%, Frame = 8%) (?2 = 20.95, d.f.
= 4, p ?
.001).4.2.2 DiscussionEven though the accuracies of predicting spatial information are significantly above baseline, we soughtways to boost performance by considering implicit spatial information.
For those clauses without explicitspatial information, we extended the annotation of the previous clause?s coding based on the inertia of181Table 6: K* Classification Accuracy and F-Measures for Task 2Spatial Information Accuracy (% / baseline) Precision Recall F-ScoreFigure 47.97 / 38 .464 .480 .428Verb 67.32 / 50 .635 .673 .640Preposition 53.69 / 46 .492 .537 .499Ground 53.59 / 34 .530 .536 .519Frame 55.67 / 47 .507 .557 .511narrative texts.
Rapaport, et al (1994) discuss the temporal inertia of narrative texts - time moves forwardthrough narrative events.
In the absence of updating, information is maintained.
We suggest that inertiaapplies to spatial information as well.
For example, given the clauses - John entered the room.
He satdown.
- we make the assumption that John sat down in the room that he entered.
We illustrate with (9).
(9) a. Kaka kicked the ball into the goal.NP, H, F, E, C, .33b.
The goaltender yelled in frustration.NP, H, F, E, C, .66c.
Then Kaka ran to the left side of the bench.3, M, F, E, INT, 1No explicit spatial information exists in (9b).
We took the coding from the explicit spatial informationin (9a) and maintained it for (9b).
New explicit spatial information occurs in (9c) and the coding isupdated.
Further, we included explicit sequence information as a measure of a given clause?s proportionalposition within the text (.33, .66 and 1).
In the absence of overt temporal specification (occuring in only10% of the clauses in our corpus), the sequence information, a textual feature, parallels the temporalprogression (and inertia) of narrative events.
This added 560 additional vectors (n = 1,471).
The K*classifier still performed the best.
The results are summarized in Table 7.Table 7: K* Classification Accuracy and F-Measures for Task 2 Boosted VectorsSPATIAL INERTIA Accuracy (% / baseline) Precision Recall F-ScoreFigure 51.73 / 41 .509 .517 .473Verb 70.22 / 48 .673 .700 .679Preposition 57.30 / 47 .571 .573 .540Ground 62.61 / 35 .636 .626 .611Frame 59.82 / 44 .574 .598 .564SPATIAL INERTIA + SEQUENCE Accuracy (% / baseline) Precision Recall F-ScoreFigure 70.56 / 41 .702 .706 .699Verb 79.33 / 48 .789 .793 .790Preposition 67.91 / 47 .676 .679 .674Ground 72.39 / 35 .721 .724 .721Frame 69.06 / 44 .678 .691 .681Inclusion of the spatial inertia values improves performance of the K* classifier in all cases (?2 =40.59, d.f.
= 4, p ?
.001).
Inclusion of sequence information improves performance even further (?2= 102.36, d.f.
= 4, p ?
.001).
Note that, despite the increase in performance, sequencing informationalone does not do as well, indicating that spatial information still plays a discriminatory role.
Usingsequence information alone as a baseline (Figure = 47%, Verb = 52%, Preposition = 47%, Ground =44%, Frame = 48%;), the normalized performance values above sequence baseline become Figure =23%, Verb = 27%, Preposition = 28%, Ground = 20%, and Frame = 21%.The ability to predict spatial features appears to be dependent both on a patterned distribution of182the per-clause spatial information (increased by spatial inertia) and on the textual feature of sequence(temporal inertia).
This seems to hold despite the specific subject matter or spatial characteristics of agiven narrative.
Considering the complete spatiotemporal picture for narrative clauses yields the bestprediction results and suggests that the spatial information structure of narrative discourse representssome type of organization akin to what Herman (2001) and Howald (2010) have evaluated in spatially-rich narratives.
Based on the tasks presented here, this organization appears to be fundamental andrelative to formal temporally-informed discourse structure.5 ConclusionExploration of the spatial dimension in narrative discourse provides interesting and robust possibilitiesfor computational discourse analysis.
We have described two machine learning tasks which exploitspatial linguistic features.
In addition to improving on existing prediction systems, both tasks empiricallydemonstrate that, when available, certain types of spatial information are predictors of the rhetoricalstructure of narrative discourse and the spatial information of narrative event sequences.
Based on theseresults, we indicate that spatial structure is related to temporal structure in narrative discourse.The coding scheme proposed here models complex and interrelated properties of spatial relationshipsand perspectives and should be generalizeable to other non-narrative discourses.
Future research will fo-cus on different discourse corpora to determine how spatial information is related to rhetorical structure.Additional future research will also focus on automation of the annotation process.
The ambiguity ofspatial language makes automatic extraction of spatial features infeasible at the current state of the art.Fortunately, average agreement and kappa statistics for coding of the spatial information and rhetoricalrelations are within acceptable ranges.
The annotated spatial features are semantically deep and usefulfor not only computational discourse systems, but tasks that involve the semantic modeling of spatialrelations and spatial reasoning.AcknowledgmentsThank you to David Herman and James Pustejovsky for productive comments and discussion and toJerry Hobbs for suggesting the Degree Confluence Project as a source of spatially rich narratives.
Thankyou also to four anonymous reviewers for very helpful insights.References[1] Anne Anderson, Miles Bader, Ellen Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, StephenIsard, Jacqueline Kowtko, Jan McAllister, Jim Miller, Catherine Sotillo, Henry Thompson, and ReginaWeinert.
1991.
The HCRC Map Task Corpus.
Language and Speech, 34:351?366.
[2] Nicholas Asher and Alex Lascarides.
2003.
Logics of Conversation.
Cambridge University Press,Cambridge, UK.
[3] Nicholas Asher and Pierre Sablayrolles.
1995.
A Typology and Discourse Semantics for MotionVerbs and Spatial PPs in French.
Journal of Semantics, 12(2):163?209.
[4] Jacob Cohen.
1960.
A Coefficient of Agreement for Nominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46.
[5] Kenny Coventry, Thora Tenbrink, and John Bateman.
2009.
Spatial Language and Dialogue.
OxfordUniversity Press, Oxford, UK.
[6] David Herman.
2001.
Spatial Reference in Narrative Domains.
Text, 21(4):515?541.
[7] Jerry R. Hobbs.
1985.
On The Coherence and Structure of Discourse.
CSLI Technical Report, 85-37.183[8] Blake Howald.
2010.
Linguistic Spatial Classifications of Event Domains in Narratives of Crime.Journal of Spatial Information Science, 1.75?93.
[9] Nancy Ide and Keith Suderman.
2007.
The Open American National Corpus (OANC), available athttp://www.AmericanNationalCorpus.org/OANC.
[10] Richard Landis and Gary Koch.
1977.
The Measurement of Observer Agreement for CategoricalData.
Biometrics, 33(1):159?174.
[11] Mirella Lapata and Alex Lascarides.
2004.
Inferring sentence internal temporal relations.
In Pro-ceedings of NAACL-04, 153?160.
[12] Stephen C. Levinson.
1996.
Language and Space.
Annual Review of Anthropology, 25(1):353?382.
[13] William Mann and Sandra Thompson.
1987.
Rhetorical Structure Theory: A Framework for TheAnalysis of Texts.
International Pragmatics Association Papers in Pragmatics, 1:79?105.
[14] Daniel Marcu.
1998.
Improving Summarization Through Rhetorical Parsing Tuning.
In The 6thWorkshop on Very Large Corpora, 206?215.
[15] Daniel Marcu.
2000.
The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach.Computational Linguistics, 26(3):395?448.
[16] Daniel Marcu and Abdessamad Echihabi.
2002.
An Unsupervised Approach to Recognizing Dis-course Relations.
In Proceedings of ACL-02, 368?375.
[17] MITRE.
2009.
SpatialML: Annotation Scheme for Marking Spatial Expressions in Natural Lan-guage, Version 3.0.
April 3, 2009.
[18] Daniel R. Montello.
1993.
Scale and Multiple Psychologies of Space.
In A. Frank and I.
Campari(eds.
), Spatial Information Theory: A Theoretical Basis for GIS (LNCS 716), 312?321.
Springer-Verlag,Berlin.
[19] Philippe Muller.
2002.
Topological Spatio-temporal Reasoning and Representation.
ComputationalIntelligence, 18(3):420?450.
[20] Barbara Partee.
1984.
Nominal and Temporal Anaphora.
Linguistics and Philosophy, 7(3):243?286.
[21] James Pustejovsky and Jessica Moszkowicz.
2008.
Integrating motion predicate classes with spatialand temporal annotations.
COLING 2008:95?98.
[22] James Pustejovsky, Jose?
Castan?o, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer,and Graham Katz.
2003.
TimeML: Robust Specification of Event and Temporal Expressions in Text.
InProceedings of the IWCS-5, Fifth International Workshop on Computational Semantics.
[23] David Randell, Zhan Cui, and Anthony Cohn.
1992.
A Spatial Logic Based on Regions andConnection.
Proceedings of KR92, 394?398.
Los Altos, CA: Morgan Kaufmann.
[24] William Rapaport, Erwin Segal, Stuart Shapiro, David Zubin, Gail Bruder, Judith Duchan, MichaelAlmeida, Joyce Daniels, Mary Galbraith, Janyce Wiebe and Albert Yuhan.
1994.
Deictic Centers andthe Cognitive Structure of Narrative Comprehension.
Technical Report No.
89-01.
Buffalo, NY: SUNYBuffalo Department of Computer Science.
[25] Caroline Sporleder and Alex Lascarides.
2005.
Exploiting Linguistic Cues to Classify RhetoricalRelations.
Proceedings of Recent Advances in Natural Language Processing (RANLP-05), 532?539.
[26] Leonard Talmy.
2000.
Toward a Cognitive Semantics, Volume 2.
The MIT Press, Cambridge, MA.
[27] Ian Witten and Eibe Frank.
2002.
Data Mining Practical Machine Learning Tools and Techniqueswith Java Implementation.
Morgan Kaufmann.184
