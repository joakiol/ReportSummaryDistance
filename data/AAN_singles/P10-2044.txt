Proceedings of the ACL 2010 Conference Short Papers, pages 236?240,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsOptimizing Question Answering Accuracy by Maximizing Log-LikelihoodMatthias H. Heie, Edward W. D. Whittaker and Sadaoki FuruiDepartment of Computer ScienceTokyo Institute of TechnologyTokyo 152-8552, Japan{heie,edw,furui}@furui.cs.titech.ac.jpAbstractIn this paper we demonstrate that thereis a strong correlation between the Ques-tion Answering (QA) accuracy and thelog-likelihood of the answer typing com-ponent of our statistical QA model.
Weexploit this observation in a clustering al-gorithm which optimizes QA accuracy bymaximizing the log-likelihood of a set ofquestion-and-answer pairs.
Experimentalresults show that we achieve better QA ac-curacy using the resulting clusters than byusing manually derived clusters.1 IntroductionQuestion Answering (QA) distinguishes itselffrom other information retrieval tasks in that thesystem tries to return accurate answers to queriesposed in natural language.
Factoid QA limits it-self to questions that can usually be answered witha few words.
Typically factoid QA systems em-ploy some form of question type analysis, so thata question such as What is the capital of Japan?will be answered with a geographical term.
Whilemany QA systems use hand-crafted rules for thistask, such an approach is time-consuming anddoesn?t generalize well to other languages.
Ma-chine learning methods have been proposed, suchas question classification using support vector ma-chines (Zhang and Lee, 2003) and language mod-eling (Merkel and Klakow, 2007).
In these ap-proaches, question categories are predefined and aclassifier is trained on manually labeled data.
Thisis an example of supervised learning.
In this pa-per we present an unsupervised method, where weattempt to cluster question-and-answer (q-a) pairswithout any predefined question categories, henceno manually class-labeled questions are used.We use a statistical QA framework, described inSection 2, where the system is trained with clustersof q-a pairs.
This framework was used in severalTREC evaluations where it placed in the top 10of participating systems (Whittaker et al, 2006).In Section 3 we show that answer accuracy isstrongly correlated with the log-likelihood of theq-a pairs computed by this statistical model.
InSection 4 we propose an algorithm to cluster q-apairs by maximizing the log-likelihood of a dis-joint set of q-a pairs.
In Section 5 we evaluate theQA accuracy by training the QA system with theresulting clusters.2 QA systemIn our QA framework we choose to model onlythe probability of an answer A given a question Q,and assume that the answer A depends on two setsof features: W = W (Q) and X = X(Q):P (A|Q) = P (A|W,X), (1)where W represents a set of |W | features describ-ing the question-type part of Q such as who, when,where, which, etc., and X is a set of featureswhich describes the ?information-bearing?
part ofQ, i.e.
what the question is actually about andwhat it refers to.
For example, in the questionsWhere is Mount Fuji?
and How high is MountFuji?, the question type features W differ, whilethe information-bearing features X are identical.Finding the best answer A?
involves a search overall A for the one which maximizes the probabilityof the above model, i.e.:A?
= arg maxAP (A|W,X).
(2)Given the correct probability distribution, thiswill give us the optimal answer in a maximumlikelihood sense.
Using Bayes?
rule, assuminguniform P (A) and that W and X are indepen-dent of each other given A, in addition to ignoringP (W,X) since it is independent of A, enables usto rewrite Eq.
(2) as236A?
= arg maxAP (A | X)?
??
?retrievalmodel?
P (W | A)?
??
?filtermodel.
(3)2.1 Retrieval ModelThe retrieval model P (A|X) is essentially a lan-guage model which models the probability of ananswer sequence A given a set of information-bearing features X = {x1, .
.
.
, x|X|}.
This setis constructed by extracting single-word featuresfrom Q that are not present in a stop-list of high-frequency words.
The implementation of the re-trieval model used for the experiments describedin this paper, models the proximity of A to fea-tures in X .
It is not examined further here;see (Whittaker et al, 2005) for more details.2.2 Filter ModelThe question-type feature set W = {w1, .
.
.
, w|W |}is constructed by extracting n-tuples (n = 1, 2, .
.
.
)such as where, in what and when were from theinput question Q.
We limit ourselves to extractingsingle-word features.
The 2522 most frequentwords in a collection of example questions areconsidered in-vocabulary words; all other wordsare out-of-vocabulary words, and substituted with?UNK?.Modeling the complex relationship betweenW and A directly is non-trivial.
We there-fore introduce an intermediate variable CE ={c1, .
.
.
, c|CE |}, representing a set of classes ofexample q-a pairs.
In order to construct theseclasses, given a set E = {t1, .
.
.
, t|E|} of ex-ample q-a pairs, we define a mapping functionf : E 7?
CE which maps each example q-a pair tjfor j = 1 .
.
.
|E| into a particular class f(tj) = ce.Thus each class ce may be defined as the union ofall component q-a features from each tj satisfy-ing f(tj) = ce.
Hence each class ce constitutes acluster of q-a pairs.
Finally, to facilitate modelingwe say that W is conditionally independent of Agiven ce so that,P (W | A) =|CE |?e=1P (W | ceW ) ?
P (ceA | A), (4)where ceW and ceA refer to the subsets of question-type features and example answers for the class ce,respectively.P (W | ceW ) is implemented as trigram langu-age models with backoff smoothing using absolutediscounting (Huang et al, 2001).Due to data sparsity, our set of example q-apairs cannot be expected to cover all the possi-ble answers to questions that may ever be asked.We therefore employ answer class modeling ratherthan answer word modeling by expanding Eq.
(4)as follows:P (W | A) =|CE |?e=1P (W | ceW )?|KA|?a=1P (ceA | ka)P (ka | A),(5)where ka is a concrete class in the set of |KA|answer classes KA.
These classes are generatedusing the Kneser-Ney clustering algorithm, com-monly used for generating class definitions forclass language models (Kneser and Ney, 1993).In this paper we restrict ourselves to single-word answers; see (Whittaker et al, 2005) for themodeling of multi-word answers.
We estimateP (ceA | kA) asP (ceA | kA) =f(kA, ceA)|CE |?g=1f(kA, cgA), (6)wheref(kA, ceA) =??i:i?ceA?
(i ?
kA)|ceA|, (7)and ?(?)
is a discrete indicator function whichequals 1 if its argument evaluates true and 0 iffalse.P (ka | A) is estimated asP (ka | A) =1??j:j?Ka?
(A ?
j) .
(8)3 The Relationship between MeanReciprocal Rank and Log-LikelihoodWe use Mean Reciprocal Rank (MRR) as ourmetric when evaluating the QA accuracy on a setof questions G = {g1...g|G|}:MRR =?|G|i=1 1/Ri|G| , (9)2370.150.160.170.180.190.20.210.220.23-1.18 -1.16 -1.14 -1.12MRRLL?
= 0.86Figure 1: MRR vs. LL (average per q-a pair) for100 random cluster configurations.where Ri is the rank of the highest ranking correctcandidate answer for gi.Given a set D = (d1...d|D|) of q-a pairs disjointfrom the q-a pairs in CE , we can, using Eq.
(5),calculate the log-likelihood asLL =|D|?d=1logP (Wd|Ad)=|D|?d=1log|CE |?e=1P (Wd | ceW )?|KA|?a=1P (ceA | ka)P (ka | Ad).
(10)To examine the relationship between MRR andLL, we randomly generate configurations CE ,with a fixed cluster size of 4, and plot the result-ing MRR and LL, computed on the same data setD, as data points in a scatter plot, as seen in Fig-ure 1.
We find that LL and MRR are stronglycorrelated, with a correlation coefficient ?
= 0.86.This observation indicates that we should beable to improve the answer accuracy of the QAsystem by optimizing the LL of the filter modelin isolation, similar to how, in automatic speechrecognition, the LL of the language model canbe optimized in isolation to improve the speechrecognition accuracy (Huang et al, 2001).4 Clustering algorithmUsing the observation that LL is correlated withMRR on the same data set, we expect that opti-mizing LL on a development set (LLdev) will alsoimprove MRR on an evaluation set (MRReval).Hence we propose the following greedy algorithmto maximize LLdev:init: c1 ?
CE contains all training pairs |E|while improvement > threshold dobest LLdev ?
?
?for all j = 1...|E| dooriginal cluster = f(tj)Take tj out of f(tj)for e = ?1, 1...|CE |, |CE |+ 1 doPut tj in ceCalculate LLdevif LLdev > best LLdev thenbest LLdev ?
LLdevbest cluster ?
ebest pair ?
jend ifTake tj out of ceend forPut tj back in original clusterend forTake tbest pair out of f(tbest pair)Put tbest pair into cbest clusterend whileIn this algorithm, c?1 indicates the set of train-ing pairs outside the cluster configuration, thus ev-ery training pair will not necessarily be includedin the final configuration.
c|C|+1 refers to a new,empty cluster, hence this algorithm automaticallyfinds the optimal number of clusters as well as theoptimal configuration of them.5 Experiments5.1 Experimental SetupFor our data sets, we restrict ourselves to questionsthat start with who, when or where.
Furthermore,we only use q-a pairs which can be answered witha single word.
As training data we use questionsand answers from the Knowledge-Master collec-tion1.
Development/evaluation questions are thequestions from TREC QA evaluations from TREC2002 to TREC 2006, the answers to which are tobe retrieved from the AQUAINT corpus.
In totalwe have 2016 q-a pairs for training and 568 ques-tions for development/evaluation.
We are able toretrieve the correct answer for 317 of the devel-opment/evaluation questions, thus the theoreticalupper bound for our experiments is an answer ac-curacy of MRR = 0.558.Accuracy is evaluated using 5-fold (rotating)cross-validation, where in each fold the TRECQA data is partitioned into a development set of1http://www.greatauk.com/238Configuration LLeval MRReval #clustersmanual -1.18 0.262 3all-in-one -1.32 0.183 1one-in-each -0.87 0.263 2016automatic -0.24 0.281 4Table 1: LLeval (average per q-a pair) andMRReval (over all held-out TREC years), andnumber of clusters (median of the cross-evaluationfolds) for the various configurations.4 years?
data and an evaluation set of one year?sdata.
For each TREC question the top 50 doc-uments from the AQUAINT corpus are retrievedusing Lucene2.
We use the QA system describedin Section 2 for QA evaluation.
Our evaluationmetric is MRReval, and LLdev is our optimiza-tion criterion, as motivated in Section 3.Our baseline system uses manual clusters.These clusters are obtained by putting all who q-apairs in one cluster, all when pairs in a second andall where pairs in a third.
We compare this baselinewith using clusters resulting from the algorithmdescribed in Section 4.
We run this algorithm untilthere are no further improvements in LLdev.
Twoother cluster configurations are also investigated:all q-a pairs in one cluster (all-in-one), and each q-a pair in its own cluster (one-in-each).
The all-in-one configuration is equivalent to not using the fil-ter model, i.e.
answer candidates are ranked solelyby the retrieval model.
The one-in-each configura-tion was shown to perform well in the TREC 2006QA evaluation (Whittaker et al, 2006), where itranked 9th among 27 participants on the factoidQA task.5.2 ResultsIn Table 1, we see that the manual clusters (base-line) achieves an MRReval of 0.262, while theclusters resulting from the clustering algorithmgive an MRReval of 0.281, which is a relativeimprovement of 7%.
This improvement is sta-tistically significant at the 0.01 level using theWilcoxon signed-rank test.
The one-in-each clus-ter configuration achieves an MRReval of 0.263,which is not a statistically significant improvementover the baseline.
The all-in-one cluster configura-tion (i.e.
no filter model) has the lowest accuracy,with an MRReval of 0.183.2http://lucene.apache.org/-1.4-1.2-1-0.8-0.6-0.4-0.200  400  800  1200  1600  20000.160.180.20.220.240.260.280.30.32LL MRR# iterationsLLdevMRRdev(a) Development set, 4 year?s TREC.-1.4-1.2-1-0.8-0.6-0.4-0.200  400  800  1200  1600  20000.160.180.20.220.240.260.280.30.32LL MRR# iterationsLLevalMRReval(b) Evaluation set, 1 year?s TREC.Figure 2: MRR and LL (average per q-a pair)vs. number of algorithm iterations for one cross-validation fold.6 DiscussionManual inspection of the automatically derivedclusters showed that the algorithm had constructedconfigurations where typically who, when andwhere q-a pairs were put in separate clusters, as inthe manual configuration.
However, in some casesboth who and where q-a pairs occurred in the samecluster, so as to better answer questions like Whowon the World Cup?, where the answer could be acountry name.As can be seen from Table 1, there are only 4clusters in the automatic configuration, comparedto 2016 in the one-in-each configuration.
Sincethe computational complexity of the filter modeldescribed in Section 2.2 is linear in the number ofclusters, a beneficial side effect of our clusteringprocedure is a significant reduction in the compu-tational requirement of the filter model.In Figure 2 we plot LL and MRR for one ofthe cross-validation folds over multiple iterations(the while loop) of the clustering algorithm in Sec-239tion 4.
It can clearly be seen that the optimizationof LLdev leads to improvement in MRReval, andthat LLeval is also well correlated with MRReval.7 Conclusions and Future WorkIn this paper we have shown that the log-likelihoodof our statistical model is strongly correlated withanswer accuracy.
Using this information, we haveclustered training q-a pairs by maximizing log-likelihood on a disjoint development set of q-apairs.
The experiments show that with these clus-ters we achieve better QA accuracy than usingmanually clustered training q-a pairs.In future work we will extend the types of ques-tions that we consider, and also allow for multi-word answers.AcknowledgementsThe authors wish to thank Dietrich Klakow for hisdiscussion at the concept stage of this work.
Theanonymous reviewers are also thanked for theirconstructive feedback.References[Huang et al2001] Xuedong Huang, Alex Acero andHsiao-Wuen Hon.
2001.
Spoken Language Pro-cessing.
Prentice-Hall, Upper Saddle River, NJ,USA.
[Kneser and Ney1993] Reinhard Kneser and HermannNey.
1993.
Improved Clustering Techniques forClass-based Statistical Language Modelling.
Pro-ceedings of the European Conference on SpeechCommunication and Technology (EUROSPEECH).
[Merkel and Klakow2007] Andreas Merkel and Diet-rich Klakow.
2007.
Language Model Based QueryClassification.
Proceedings of the European Confer-ence on Information Retrieval (ECIR).
[Whittaker et al2005] Edward Whittaker, Sadaoki Fu-rui and Dietrich Klakow.
2005.
A Statistical Clas-sification Approach to Question Answering usingWeb Data.
Proceedings of the International Con-ference on Cyberworlds.
[Whittaker et al2006] Edward Whittaker, Josef Novak,Pierre Chatain and Sadaoki Furui.
2006.
TREC2006 Question Answering Experiments at Tokyo In-stitute of Technology.
Proceedings of The FifteenthText REtrieval Conference (TREC).
[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.2003.
Question Classification using Support Vec-tor Machines.
Proceedings of the Special InterestGroup on Information Retrieval (SIGIR).240
