Using Bag-of-Concepts to Improve the Performance of SupportVector Machines in Text CategorizationMagnus SahlgrenSICSBox 1263SE-164 29 KistaSwedenmange@sics.seRickard Co?sterSICSBox 1263SE-164 29 KistaSwedenrick@sics.seAbstractThis paper investigates the use of concept-based representations for text categoriza-tion.
We introduce a new approach to cre-ate concept-based text representations, andapply it to a standard text categorizationcollection.
The representations are used asinput to a Support Vector Machine classi-fier, and the results show that there are cer-tain categories for which concept-based rep-resentations constitute a viable supplementto word-based ones.
We also demonstratehow the performance of the Support Vec-tor Machine can be improved by combiningrepresentations.1 IntroductionText categorization is the task of assigning atext1 to one or more of a set of predefined cat-egories.
As with most other natural languageprocessing applications, representational factorsare decisive for the performance of the catego-rization.
The incomparably most common rep-resentational scheme in text categorization isthe Bag-of-Words (BoW) approach, in which atext is represented as a vector ~t of word weights,such that ~ti = (w1...wn) where wn are theweights (usually a tf?idf -value)2 of the wordsin the text.
The BoW representation ignores allsemantic or conceptual information; it simplylooks at the surface word forms.There have been attempts at deriving moresophisticated representations for text catego-rization, including the use of n-grams or phrases1In the remainder of this paper, we use the terms?text?
and ?document?
synonymously.2The tf?idf measure is a standard weighting scheme,where tf i is simply the frequency of word i in the doc-ument, and idf is the inverse document frequency, givenby Nni where N is the total number of documents in thedata, and ni is the number of documents in which wordi occurs.
The most common version of the tf?idf for-mula is wi = tf i?
log Nni (Baeza-Yates and Ribeiro-Neto,1999).
(Lewis, 1992; Dumais et al, 1998), or augment-ing the standard BoW approach with synonymclusters or latent dimensions (Baker and Mc-Callum, 1998; Cai and Hofmann, 2003).
How-ever, none of the more elaborate representationsmanage to significantly outperform the stan-dard BoW approach (Sebastiani, 2002).
In ad-dition to this, they are typically more expensiveto compute.What interests us in this paper is the differ-ence between using standard BoW and moreelaborate, concept-based representations.
Sincetext categorization is normally cast as a prob-lem concerning the content of the text (Du-mais et al, 1998), one might assume that look-ing beyond the mere surface word forms shouldbe beneficial for the text representations.
Webelieve that, even though BoW representationsare superior in most text categorization tasks,concept-based schemes do provide important in-formation, and that they can be used as a sup-plement to the BoW representations.
Our goalis therefore to investigate whether there are spe-cific categories in a standard text categorizationcollection for which using concept-based repre-sentations is more appropriate, and if combi-nations of word-based and concept-based repre-sentations can be used to improve the catego-rization performance.In order to do this, we introduce a newmethod for producing concept-based represen-tations for natural language data.
The methodis efficient, fast and scalable, and requires noexternal resources.
We use the method to cre-ate concept-based representations for a stan-dard text categorization problem, and we usethe representations as input to a Support VectorMachine classifier.
The categorization resultsare compared to those reached using standardBoW representations, and we also demonstratehow the performance of the Support Vector Ma-chine can be improved by combining represen-tations.2 Bag-of-ConceptsThe standard BoW representations are usuallyrefined before they are used as input to a clas-sification algorithm.
One refinement methodis to use feature selection, which means thatwords are removed from the representationsbased on statistical measures, such as documentfrequency, information gain, ?2, or mutual in-formation (Yang and Pedersen, 1997).
Anotherrefinement method is to use feature extraction,which means that ?artificial?
features are cre-ated from the original ones, either by using clus-tering methods, such as distributional clustering(Baker and McCallum, 1998), or by using factoranalytic methods such as singular value decom-position.Note that feature extraction methods alsohandle problems with synonymy, by groupingtogether words that mean similar things, or byrestructuring the data (i.e.
the number of fea-tures) according to a small number of salient di-mensions, so that similar words get similar rep-resentations.
Since these methods do not rep-resent texts merely as collections of the wordsthey contain, but rather as collections of theconcepts they contain ?
whether these be syn-onym sets or latent dimensions ?
a more fittinglabel for these representations would be Bag-of-Concepts (BoC).3 Random IndexingOne serious problem with BoC approaches isthat they tend to be computationally expensive.This is true at least for methods that use fac-tor analytic techniques.
Other BoC approachesthat use resources such as WordNet have limitedportability, and are normally not easily adapt-able to other domains and to other languages.To overcome these problems, we have devel-oped an alternative approach for producing BoCrepresentations.
The approach is based on Ran-dom Indexing (Kanerva et al, 2000; Karlgrenand Sahlgren, 2001), which is a vector spacemethodology for producing context vectors3 forwords based on cooccurrence data.
The contextvectors can be used to produce BoC represen-tations by combining the context vectors of thewords that occur in a text.In the traditional vector space model, con-text vectors are generated by representing the3Context vectors represent the distributional profileof words, making it possible to express distributionalsimilarity between words by standard vector similaritymeasures.data in a cooccurrence matrix F of order w?
c,such that the rows Fw represent the words,the columns Fc represent the contexts (typi-cally words or documents4), and the cells arethe (weighted and normalized) cooccurrencecounts of a given word in a given context.
Thepoint of this representation is that each row ofcooccurrence counts can be interpreted as a c-dimensional context vector ~w for a given word.In the Random Indexing approach, the cooc-currence matrix is replaced by a context ma-trix G of order w ?
k, where k  c. Eachrow Gi is the k-dimensional context vector forword i.
The context vectors are accumulatedby adding together k-dimensional index vectorsthat have been assigned to each context in thedata ?
whether document, paragraph, clause,window, or neighboring words.
The index vec-tors constitute a unique representation for eachcontext, and are sparse, high-dimensional, andternary, which means that their dimensionalityk typically is on the order of thousands and thatthey consist of a small number of randomly dis-tributed +1s and ?1s.
The k-dimensional indexvectors are used to accumulate k-dimensionalcontext vectors by the following procedure: ev-ery time a given word occurs in a context, thatcontext?s index vector is added (by vector addi-tion) to the context vector for the given word.Note that the same procedure will produce astandard cooccurrence matrix F of order w?c ifwe use unary index vectors of the same dimen-sionality c as the number of contexts.5 Math-ematically, the unary vectors are orthogonal,whereas the random index vectors are onlynearly orthogonal.
However, since there aremore nearly orthogonal than truly orthogonaldirections in a high-dimensional space, choos-ing random directions gets us sufficiently closeto orthogonality to provide an approximation ofthe unary vectors (Hecht-Nielsen, 1994).The Random Indexing approach is motivatedby the Johnson-Lindenstrauss Lemma (John-son and Lindenstrauss, 1984), which states thatif we project points into a randomly selectedsubspace of sufficiently high dimensionality, the4Words are used as contexts in e.g.
Hyperspace Ana-logue to Language (HAL) (Lund et al, 1995), whereasdocuments are used in e.g.
Latent Semantic Index-ing/Analysis (LSI/LSA) (Deerwester et al, 1990; Lan-dauer and Dumais, 1997).5These unary index vectors would have a single 1marking the place of the context in a list of all con-texts ?
the nth element of the index vector for the nthcontext would be 1.distances between the points are approximatelypreserved.
Thus, if we collect the random indexvectors into a random matrix R of order c?
k,whose row Ri is the k-dimensional index vectorfor context i, we find that the following relationholds:Gw?k = Fw?cRc?kThat is, the Random Indexing context matrixG contains the same information as we get bymultiplying the standard cooccurrence matrixF with the random matrix R, where RRT ap-proximates the identity matrix.3.1 Advantages of Random IndexingOne advantage of using Random Indexing isthat it is an incremental method, which meansthat we do not have to sample all the databefore we can start using the context vectors?
Random Indexing can provide intermediaryresults even after just a few vector additions.Other vector space models need to analyze theentire data before the context vectors are oper-ational.Another advantage is that Random Indexingavoids the ?huge matrix step?, since the di-mensionality k of the vectors is much smallerthan, and not directly dependent on, the num-ber of contexts c in the data.
Other vector spacemodels, including those that use dimension re-duction techniques such as singular value de-composition, depend on building the w ?
c co-occurrence matrix F .This ?huge matrix step?
is perhaps the mostserious deficiency of other models, since theircomplexity becomes dependent on the num-ber of contexts c in the data, which typicallyis a very large number.
Even methods thatare mathematically equivalent to Random In-dexing, such as random projection (Papadim-itriou et al, 1998) and random mapping (Kaski,1999), are not incremental, and require the ini-tial w ?
c cooccurrence matrix.Since dimension reduction is built into Ran-dom Indexing, we achieve a significant gainin processing time and memory consumption,compared to other models.
Furthermore, theapproach is scalable, since adding new contextsto the data set does not increase the dimension-ality of the context vectors.3.2 Bag-of-Context vectorsThe context vectors produced by Random In-dexing can be used to generate BoC representa-tions.
This is done by, for every text, summingthe (weighted) context vectors of the words thatoccur in the particular text.
Note that summingvectors result in tf -weighting, since a word?svector is added to the text?s vector as manytimes as the word occurs in the text.
The sameprocedure generates standard BoW representa-tions if we use unary index vectors of the samedimensionality as the number of words in thedata instead of context vectors, and weight thesummation of the unary index vectors with theidf -values of the words.64 Experiment SetupIn the following sections, we describe the setupfor our text categorization experiments.4.1 DataWe use the Reuters-21578 test collection, whichconsists of 21,578 news wire documents thathave been manually assigned to different cat-egories.
In these experiments, we use the?ModApte?
split, which divides the collectioninto 9,603 training documents and 3,299 testdocuments, assigned to 90 topic categories.
Af-ter lemmatization, stopword filtering based ondocument frequency, and frequency threshold-ing that excluded words with frequency < 3, thetraining data contains 8,887 unique word types.4.2 RepresentationsThe standard BoW representations for thissetup of Reuters-21578 are 8,887-dimensionaland very sparse.
To produce BoC represen-tations, a k-dimensional random index vectoris assigned to each training document.
Con-text vectors for the words are then producedby adding the index vectors of a document tothe context vector for a given word every timethe word occur in that document.7 The context6We can also use Random Indexing to produce re-duced BoW representations (i.e.
BoW representationswith reduced dimensionality), which we do by summingthe weighted random index vectors of the words thatoccur in the text.
We do not include any results fromusing reduced BoW representations in this paper, sincethey contain more noise than the standard BoW vec-tors.
However, they are useful in very high-dimensionalapplications where efficiency is an important factor.7We initially also used word-based contexts, whereindex vectors were assigned to each unique word, andcontext vectors were produced by adding the random in-dex vectors of the surrounding words to the context vec-tor of a given word every time the word ocurred in thetraining data.
However, the word-based BoC representa-tions consistently produced inferior results compared tothe document-based ones, so we decided not to pursuethe experiments with word-based BoC representationsfor this paper.vectors are then used to generate BoC represen-tations for the texts by summing the contextvectors of the words in each text, resulting ink-dimensional dense BoC vectors.4.3 Support Vector MachinesFor learning the categories, we use the Sup-port Vector Machine (SVM) (Vapnik, 1995) al-gorithm for binary classification.
SVM findsthe separating hyperplane that has maximummargin between the two classes.
Separating theexamples with a maximum margin hyperplaneis motivated by results from statistical learningtheory, which states that a learning algorithm,to achieve good generalisation, should minimizeboth the empirical error and also the ?capacity?of the functions that the learning algorithm im-plements.
By maximizing the margin, the ca-pacity or complexity of the function class (sep-arating hyperplanes) is minimized.
Finding thishyperplane is expressed as a mathematical op-timization problem.Let {(~x1, y1), .
.
.
, (~xl, yl)} where ~xi ?
Rn, yi ?
?1 be a set of training examples.
The SVM sep-arates these examples by a hyperplane definedby a weight vector ~w and a threshold b, see Fig-ure 1.
The weight vector ~w determines a direc-tion perpendicular to the hyperplane, while bdetermines the distance to the hyperplane fromthe origin.
A new example ~z is classified accord-ing to which side of the hyperplane it belongsto.
From the solution of the optimization prob-lem, the weight vector ~w has an expansion in asubset of the training examples, so classifying anew example ~z is:f(~z) = sgn(l?i=1?iyiK(~xi, ~z) + b)(1)where the ?i variables are determined by theoptimization procedure and K(~xi, ~z) is the innerproduct between the example vectors.The examples marked with grey circles in Fig-ure 1 are called Support Vectors.
These exam-ples uniquely define the hyperplane, so if thealgorithm is re-trained using only the supportvectors as training examples, the same separat-ing hyperplane is found.
When examples arenot linearly separable, the SVM algorithm al-lows for the use of slack variables for allowingclassification errors and the possibility to mapexamples to a (high-dimensional) feature space.In this feature space, a separating hyperplanecan be found such that, when mapped backto input space, describes a non-linear decisionx2x1bwFigure 1: A maximum margin hyperplane sepa-rating a set of examples in R2.
Support Vectorsare marked with circles.function.
The implicit mapping is performed bya kernel function that expresses the inner prod-uct between two examples in the desired fea-ture space.
This function replaces the functionK(~xi, ~z) in Equation 1.In our experiments, we use three standardkernel functions ?
the basic linear kernel, thepolynomial kernel, and the radial basis kernel:8?
Linear: K(~xi, ~z) = ~xi ?
~z?
Polynomial: K(~xi, ~z) = (~xi ?
~z)d?
Radial Basis: K(~xi, ~z = exp(??
?~xi ?
~z?2)For all experiments, we select d = 3 for thepolynomial kernel and ?
= 1.0 for the radialbasis kernel.
These parameters are selected asdefault values and are not optimized.5 Experiments and ResultsIn these experiments, we use a one-against-alllearning method, which means that we trainone classifier for each category (and represen-tation).
When using the classifiers to predictthe class of a test example, there are four pos-sible outcomes; true positive (TP), true nega-tive (TN), false positive (FP), and false nega-tive (FN).
Positive means that the documentwas classified as belonging to the category, neg-ative that it was not, whereas true means thatthe classification was correct and false that itwas not.
From these four outcomes, we candefine the standard evaluation metrics preci-sion P = TP/(TP + FP ) and recall R =8We use a modified version of SVM light that is avail-able at: http://svmlight.joachims.org/TP/(TP+FN).
We report our results as a com-bined score of precision and recall, the micro-averaged F1 score:9F1 =2 ?
P ?
RP +RThere are a number of parameters that need tobe optimized in this kind of experiment, includ-ing the weighting scheme, the kernel function,and the dimensionality of the BoC vectors.
Forease of exposition, we report the results of eachparameter set separately.
Since we do not ex-periment with feature selection in this investi-gation, our results will be somewhat lower thanother published results that use SVM with op-timized feature selection.
Our main focus is tocompare results produced with BoW and BoCrepresentations, and not to produce a top scorefor the Reuters-21578 collection.5.1 Weighting SchemeUsing appropriate word weighting functions isknown to improve the performance of text cate-gorization (Yang and Pedersen, 1997).
In or-der to investigate the impact of using differ-ent word weighting schemes for concept-basedrepresentations, we compare the performanceof the SVM using the following three weightingschemes: tf, idf, and tf?idf.The results are summarized in Table 1.
TheBoW run uses the linear kernel, while the BoCruns use the polynomial kernel.
The numbers inboldface are the best BoC runs for tf, idf, andtf?idf, respectively.tf idf tf?idfBoW 82.52 80.13 82.77BoC 500-dim 79.97 80.18 81.25BoC 1,000-dim 80.31 80.87 81.93BoC 1,500-dim 80.41 80.81 81.79BoC 2,000-dim 80.54 80.85 82.04BoC 2,500-dim 80.64 81.19 82.18BoC 3,000-dim 80.67 81.15 82.11BoC 4,000-dim 80.60 81.07 82.24BoC 5,000-dim 80.78 81.09 82.29BoC 6,000-dim 80.78 81.08 82.12Table 1: Micro-averaged F1 score for tf, idf andtf?idf using BoW and BoC representations.9Micro-averaging means that we sum the TP, TN,FP and FN over all categories and then compute the F1score.
In macro-averaging, the F1 score is computed foreach category, and then averaged.As expected, the best results for both BoWand BoC representations were produced usingtf?idf.
For the BoW vectors, tf consistentlyproduced better results than idf, and it was evenbetter than tf?idf using the polynomial and ra-dial basis kernels.
For the BoC vectors, the onlyconsistent difference between tf and idf is foundusing the polynomial kernel, where idf outper-forms tf.10 It is also interesting to note that foridf weighting, all BoC runs outperform BoW.5.2 Parameterizing RIIn theory, the quality of the context vectorsproduced with the Random Indexing processshould increase with their dimensionality.
Kaski(1999) show that the higher the dimensional-ity of the vectors, the closer the matrix RRTwill approximate the identity matrix, and Bing-ham and Mannila (2001) observe that the meansquared difference between RRT and the iden-tity matrix is about 1k , where k is the dimen-sionality of the vectors.
In order to evaluate theeffects of dimensionality in this application, wecompare the performance of the SVM with BoCrepresentations using 9 different dimensionali-ties of the vectors.
The index vectors consistof 4 to 60 non-zero elements (?
1% non-zeros),depending on their dimensionality.
The resultsfor all three kernels using tf?idf -weighting aredisplayed in Figure 2.0 1000 2000 3000 4000 5000 600079.58080.58181.58282.5DimensionalityF?scoreLinearPolyRBFFigure 2: Micro-averaged F1 score for three ker-nels using 9 dimensionalities of the BoC vectors.Figure 2 demonstrates that the quality ofthe concept-based representations increase withtheir dimensionality as expected, but that the10For the linear and radial basis kernels, the tendencyis that tf in most cases is better than idf.increase levels out when the dimensionality be-comes sufficiently large; there is hardly any dif-ference in performance when the dimensionalityof the vectors exceeds 2,500.
There is even aslight tendency that the performance decreaseswhen the dimensionality exceeds 5,000 dimen-sions; the best result is produced using 5,000-dimensional vectors with 50 non-zero elementsin the index vectors.There is a decrease in performance when thedimensionality of the vectors drops below 2,000.Still, the difference in F1 score between using500 and 5,000 dimensions with the polynomialkernel and tf?idf is only 1.04, which indicatesthat Random Indexing is very robust in com-parison to, e.g., singular value decomposition,where choosing appropriate dimensionality iscritical.5.3 Parameterizing SVMRegarding the different kernel functions, Fig-ure 2 clearly shows that the polynomial kernelproduces consistently better results for the BoCvectors than the other kernels, and that the lin-ear kernel consistently produces better resultsthan the radial basis kernel.
This could be ademonstration of the difficulties of parameterselection, especially for the ?
parameter in theradial basis kernel.
To further improve the re-sults, we can find better values of ?
for the radialbasis kernel and of d for the polynomial kernelby explicit parameter search.6 Comparing BoW and BoCIf we compare the best BoW run (using the lin-ear kernel and tf ?
idf -weighting) and the bestBoC run (using 5,000-dimensional vectors withthe polynomial kernel and tf ?
idf -weighting),we can see that the BoW representations barelyoutperform BoC: 82.77% versus 82.29%.
How-ever, if we only look at the results for the tenlargest categories in the Reuters-21578 collec-tion, the situation is reversed and the BoC rep-resentations outperform BoW.
The F1 measurefor the best BoC vectors for the ten largest cat-egories is 88.74% compared to 88.09% for thebest BoW vectors.
This suggests that BoC rep-resentations are more appropriate for large-sizecategories.The best BoC representations outperform thebest BoW representations in 16 categories, andare equal in 6.
Of the 16 categories where thebest BoC outperform the best BoW, 9 are bet-ter only in recall, 5 are better in both recall andprecision, while only 2 are better only in preci-sion.It is always the same set of 22 categorieswhere the BoC representations score betterthan, or equal to, BoW.11 These include the twolargest categories in Reuters-21578, ?earn?
and?acq?, consisting of 2,877 and 1,650 documents,respectively.
For these two categories, BoC rep-resentations outperform BoW with 95.57% ver-sus 95.36%, and 91.07% versus 90.16%, respec-tively.
The smallest of the ?BoC categories?
is?fuel?, which consists of 13 documents, and forwhich BoC outperforms BoW representationswith 33.33% versus 30.77%.
The largest per-formance difference for the ?BoC categories?
isfor category ?bop?, where BoC reaches 66.67%,while BoW only reaches 54.17%.
We also notethat it is the same set of categories that is prob-lematic for both types of representations; whereBoW score 0.0%, so does BoC.7 Combining RepresentationsThe above comparison suggests that we can im-prove the performance of the SVM by combin-ing the two types of representation.
The best F1score can be achieved by selecting the quadruple(TP, FP, TN,FN) for each individual categoryfrom either BoW or BoC so that it maximizesthe overall score.
There are 290 such combina-tions, but by expressing the F1 function in itsequivalent form F1 = (2 ?
TP )/(2 ?
TP +FP +FN), we can determine that for our two topruns there are only 17 categories such that weneed to perform an exhaustive search to findthe best combination.
For instance, if for onecategory both runs have the same TP but oneof the runs have higher FP and FN , the otherrun is selected for that category and we do notinclude that category in the exhaustive search.Combining the best BoW and BoC runs in-creases the results from 82.77% (the best BoWrun) to 83.91%.
For the top ten categories, thisincreases the score from 88.74% (the best BoCrun) to 88.99%.
Even though the difference isadmittedly small, the increase in performancewhen combining representations is not negligi-ble, and is consistent with the findings of previ-ous research (Cai and Hofmann, 2003).11The ?BoC categories?
are: veg-oil, heat, gold, soy-bean, housing, jobs, nat-gas, cocoa, wheat, rapeseed, live-stock, ship, fuel, trade, sugar, cpi, bop, lei, acq, crude,earn, money-fx.8 ConclusionsWe have introduced a new method for pro-ducing concept-based (BoC) text representa-tions, and we have compared the performance ofan SVM classifier on the Reuters-21578 collec-tion using both traditional word-based (BoW),and concept-based representations.
The re-sults show that BoC representations outperformBoW when only counting the ten largest cate-gories, and that a combination of BoW and BoCrepresentations improve the performance of theSVM over all categories.We conclude that concept-based representa-tions constitute a viable supplement to word-based ones, and that there are categories in theReuters-21578 collection that benefit from usingconcept-based representations.9 AcknowledgementsThis work has been funded by the Euro-pean Commission under contract IST-2000-29452 (DUMAS ?
Dynamic Universal Mobilityfor Adaptive Speech Interfaces).ReferencesR.
Baeza-Yates and B. Ribeiro-Neto.
1999.Modern Information Retrieval.
ACM Press /Addison-Wesley.D.
Baker and A. McCallum.
1998.
Distribu-tional clustering of words for text classifica-tion.
In SIGIR 1998, pages 96?103.Ella Bingham and Heikki Mannila.
2001.
Ran-dom projection in dimensionality reduction:applications to image and text data.
InKnowledge Discovery and Data Mining, pages245?250.Lijuan Cai and Thomas Hofmann.
2003.
Textcategorization by boosting automatically ex-tracted concepts.
In SIGIR 2003, pages 182?189.S.
Deerwester, S. Dumais, G. Furnas, T. Lan-dauer, and R. Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the Soci-ety for Information Science, 41(6):391?407.S.
Dumais, J. Platt, D. Heckerman, and M. Sa-hami.
1998.
Inductive learning algorithmsand representations for text categorization.In Proceedings of ACM-CIKM98, pages 148?155.R.
Hecht-Nielsen.
1994.
Context vectors: gen-eral purpose approximate meaning represen-tations self-organized from raw data.
In J.M.Zurada, R.J. Marks II, and C.J.
Robinson,editors, Computational Intelligence: Imitat-ing Life, pages 43?56.
IEEE Press.W.B.
Johnson and J. Lindenstrauss.
1984.Extensions of lipshitz mapping into hilbertspace.
Contemporary Mathematics, 26:189?206.P.
Kanerva, J. Kristofersson, and A. Holst.2000.
Random indexing of text samples forlatent semantic analysis.
In Proceedings ofthe 22nd Annual Conference of the CognitiveScience Society, page 1036.
Erlbaum.J.
Karlgren and M. Sahlgren.
2001.
From wordsto understanding.
In Y. Uesaka, P. Kan-erva, and H. Asoh, editors, Foundations ofReal-World Intelligence, pages 294?308.
CSLIPublications.S.
Kaski.
1999.
Dimensionality reduction byrandom mapping: Fast similarity computa-tion for clustering.
In Proceedings of theIJCNN?98, International Joint Conferenceon Neural Networks, pages 413?418.
IEEEService Center.T.
Landauer and S. Dumais.
1997.
A solutionto plato?s problem: The latent semantic anal-ysis theory of acquisition, induction and rep-resentation of knowledge.
Psychological Re-view, 104(2):211?240.D.
Lewis.
1992.
An evaluation of phrasal andclustered representations on a text catego-rization task.
In SIGIR 1992, pages 37?50.K.
Lund, C. Burgess, and R. A. Atchley.
1995.Semantic and associative priming in high-dimensional semantic space.
In Proceedingsof the 17th Annual Conference of the Cogni-tive Science Society, pages 660?665.
Erlbaum.C.
H. Papadimitriou, P. Raghavan, H. Tamaki,and S. Vempala.
1998.
Latent semantic in-dexing: A probabilistic analysis.
In Proceed-ings of the 17th ACM Symposium on thePrinciples of Database Systems, pages 159?168.
ACM Press.F.
Sebastiani.
2002.
Machine learning in auto-mated text categorization.
ACM ComputingSurveys, 34(1):1?47.V.
Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.Y.
Yang and J. Pedersen.
1997.
A compara-tive study on feature selection in text cate-gorization.
In Proceedings of ICML-97, 14thInternational Conference on Machine Learn-ing, pages 412?420.
