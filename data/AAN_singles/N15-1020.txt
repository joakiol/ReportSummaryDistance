Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 196?205,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Neural Network Approach toContext-Sensitive Generation of Conversational ResponsesAlessandro Sordoni1?
?Michel Galley2?Michael Auli3?Chris Brockett2Yangfeng Ji4?Margaret Mitchell2Jian-Yun Nie1?Jianfeng Gao2Bill Dolan21DIRO, Universit?e de Montr?eal, Montr?eal, QC, Canada2Microsoft Research, Redmond, WA, USA3Facebook AI Research, Menlo Park, CA, USA4Georgia Institute of Technology, Atlanta, GA, USAAbstractWe present a novel response generation sys-tem that can be trained end to end on largequantities of unstructured Twitter conversa-tions.
A neural network architecture is usedto address sparsity issues that arise when in-tegrating contextual information into classicstatistical models, allowing the system to takeinto account previous dialog utterances.
Ourdynamic-context generative models show con-sistent gains over both context-sensitive andnon-context-sensitive Machine Translation andInformation Retrieval baselines.1 IntroductionUntil recently, the goal of training open-domain con-versational systems that emulate human conversationhas seemed elusive.
However, the vast quantitiesof conversational exchanges now available on so-cial media websites such as Twitter and Reddit raisethe prospect of building data-driven models that canbegin to communicate conversationally.
The workof Ritter et al (2011), for example, demonstrates thata response generation system can be constructed fromTwitter conversations using statistical machine trans-lation techniques, where a status post by a Twitteruser is ?translated?
into a plausible looking response.However, an approach such as that presented in Rit-ter et al (2011) does not address the challenge of*The entirety of this work was conducted while at MicrosoftResearch.
?Corresponding authors: Alessandro Sordoni (sor-donia@iro.umontreal.ca) and Michel Galley (mgal-ley@microsoft.com).contextbecause of your game ?messageyeah i?m on myway nowresponseok good luck !Figure 1: Example of three consecutive utterances occur-ring between two Twitter users A and B.generating responses that are sensitive to the contextof the conversation.
Broadly speaking, context maybe linguistic or involve grounding in the physical orvirtual world, but we here focus on linguistic context.The ability to take into account previous utterancesis key to building dialog systems that can keep con-versations active and engaging.
Figure 1 illustratesa typical Twitter dialog where the contextual infor-mation is crucial: the phrase ?good luck?
is plainlymotivated by the reference to ?your game?
in the firstutterance.
In the MT model, such contextual sensitiv-ity is difficult to capture; moreover, naive injectionof context information would entail unmanageablegrowth of the phrase table at the cost of increasedsparsity, and skew towards rarely-seen context pairs.In most statistical approaches to machine translation,phrase pairs do not share statistical weights regard-less of their intrinsic semantic commonality.We propose to address the challenge of context-sensitive response generation by using continuousrepresentations or embeddings of words and phrasesto compactly encode semantic and syntactic simi-larity.
We argue that embedding-based models af-196ford flexibility to model the transitions between con-secutive utterances and to capture long-span depen-dencies in a domain where traditional word andphrase alignment is difficult (Ritter et al, 2011).
Tothis end, we present two simple, context-sensitiveresponse-generation models utilizing the RecurrentNeural Network Language Model (RLM) architec-ture of (Mikolov et al, 2010).
These models firstencode past information in a hidden continuous repre-sentation, which is then decoded by the RLM to pro-mote plausible responses that are simultaneously flu-ent and contextually relevant.
Unlike typical complextask-oriented multi-modular dialog systems (Young,2002; Stent and Bangalore, 2014), our architectureis completely data-driven and can easily be trainedend-to-end using unstructured data without requiringhuman annotation, scripting, or automatic parsing.This paper makes the following contributions.
Wepresent a neural network architecture for responsegeneration that is both context-sensitive and data-driven.
As such, it can be trained from end to end onmassive amounts of social media data.
To our knowl-edge, this is the first application of a neural-networkmodel to open-domain response generation, and webelieve that the present work will lay groundwork formore complex models to come.
We additionally in-troduce a novel multi-reference extraction techniquethat shows promise for automated evaluation.2 Related WorkOur work naturally lies in the path opened by Ritteret al (2011), but we generalize their approach byexploiting information from a larger context.
Rit-ter et al and our work represent a radical paradigmshift from other work in dialog.
More traditionaldialog systems typically tease apart dialog manage-ment (Young, 2002) from response generation (Stentand Bangalore, 2014), while our holistic approachcan be considered a first attempt to accomplish bothtasks jointly.
While there are previous uses of ma-chine learning for response generation (Walker et al,2003), dialog state tracking (Young et al, 2010), anduser modeling (Georgila et al, 2006), many compo-nents of typical dialog systems remain hand-coded:in particular, the labels and attributes defining dia-log states.
In contrast, the dialog state in our neuralnetwork model is completely latent and directly opti-mized towards end-to-end performance.
In this sense,we believe the framework of this paper is a signif-icant milestone towards more data-driven and lesshand-coded dialog processing.Continuous representations of words and phrasesestimated by neural network models have been ap-plied on a variety of tasks ranging from InformationRetrieval (IR) (Huang et al, 2013; Shen et al, 2014),Online Recommendation (Gao et al, 2014b), Ma-chine Translation (MT) (Auli et al, 2013; Cho et al,2014; Kalchbrenner and Blunsom, 2013; Sutskeveret al, 2014), and Language Modeling (LM) (Bengioet al, 2003; Collobert and Weston, 2008).
Gao etal.
(2014a) successfully use an embedding model torefine the estimation of rare phrase-translation prob-abilities, which is traditionally affected by sparsityproblems.
Robustness to sparsity is a crucial prop-erty of our method, as it allows us to capture contextinformation while avoiding unmanageable growth ofmodel parameters.Our work extends the Recurrent Neural NetworkLanguage Model (RLM) of (Mikolov et al, 2010),which uses continuous representations to estimate aprobability function over natural language sentences.We propose a set of conditional RLMs where contex-tual information (i.e., past utterances) is encoded ina continuous context vector to help generate the re-sponse.
Our models differ from most previous workin the way the context vector is constructed.
Forexample, Mikolov and Zweig (2012) and Auli et al(2013) use a pre-trained topic model.
In our models,the context vector is learned along with the condi-tional RLM that generates the response.
Additionally,the learned context encodings do not exclusively cap-ture contentful words.
Indeed, even ?stop words?
cancarry discriminative power in this task; for exam-ple, all words in the utterance ?how are you??
arecommonly characterized as stop words, yet this is acontentful dialog utterance.3 Recurrent Language ModelWe give a brief overview of the Recurrent LanguageModel (RLM) (Mikolov et al, 2010) architecture thatour models extend.
A RLM is a generative modelof sentences, i.e., given sentence s = s1, .
.
.
, sT, itestimates:p(s) =T?t=1p(st|s1, .
.
.
, st?1).
(1)197The model architecture is parameterized by threeweight matrices, ?RNN= ?Win,Wout,Whh?
: an in-put matrixWin, a recurrent matrixWhhand an outputmatrix Wout, which are usually initialized randomly.The rows of the input matrix Win?
RV?Kcontainthe K-dimensional embeddings for each word in thelanguage vocabulary of size V .
Let us denote by stboth the vocabulary token and its one-hot representa-tion, i.e., a zero vector of dimensionality V with a 1corresponding to the index of the sttoken.
The em-bedding for stis then obtained by s>tWin.
The recur-rent matrix Whh?
RK?Kkeeps a history of the sub-sequence that has already been processed.
The outputmatrix Wout?
RK?Vprojects the hidden state htinto the output layer ot, which has an entry for eachword in the vocabulary V .
This value is used to gen-erate a probability distribution for the next word inthe sequence.
Specifically, the forward pass proceedswith the following recurrence, for t = 1, .
.
.
, T :ht= ?
(s>tWin+ h>t?1Whh), ot= h>tWout(2)where ?
is a non-linear function applied element-wise, in our case the logistic sigmoid.
The recurrenceis seeded by setting h0= 0, the zero vector.
Theprobability distribution over the next word given theprevious history is obtained by applying the softmaxactivation function:P (st= w|s1, .
.
.
, st?1) =exp(otw)?Vv=1exp(otv).
(3)The RLM is trained to minimize the negative log-likelihood of the training sentence s:L(s) = ?T?t=1logP (st|s1, .
.
.
, st?1).
(4)The recurrence is unrolled backwards in time us-ing the back-propagation through time (BPTT) al-gorithm (Rumelhart et al, 1988), and gradients areaccumulated over multiple time-steps.4 Context-Sensitive ModelsWe distinguish three linguistic entities in a conver-sation between two users A and B: the context1c,1In this work, the context is purely linguistic, but future workmight integrate further contextual information, e.g., geographicallocation, time information, or other forms of grounding.WoutWinWhhhtotstWoutWinWhhhtotstot+1ht+1st+1Figure 2: Compact representation of an RLM (left) andunrolled representation for two time steps (right).the message m and response r. The context c rep-resents a sequence of past dialog exchanges of anylength; then B emits a message m to which A reactsby formulating its response r (see Figure 1).We use three context-based generation models toestimate a generation model of the response r, r =r1, .
.
.
, rT, conditioned on past information c and m:p(r|c,m) =T?t=1p(rt|r1, .
.
.
, rt?1, c,m).
(5)These three models differ in the manner in whichthey compose the context-message pair (c,m).4.1 Tripled Language ModelIn our first model, dubbed RLMT, we straightfor-wardly concatenate each utterance c, m, r into asingle sentence s and train the RLM to minimizeL(s).
Given c and m, we compute the probabilityof the response as follows: we perform the forwardpropagation over the known utterances c andm to ob-tain a hidden state encoding useful information aboutprevious utterances.
Subsequently, we compute thelikelihood of the response from that hidden state.An issue with this simple approach is that the con-catenated sentence s will be very long on average,especially if the context comprises multiple utter-ances.
Modelling such long-range dependencies withan RLM is difficult and is still considered an openproblem (Pascanu et al, 2013).
We will considerRLMT as an additional context-sensitive baseline forthe models we present next.4.2 Dynamic-Context Generative Model IThe above limitation of RLMT can be addressed bystrengthening the context bias.
In our second model(DCGM-I), the context and the message are encoded198DCGM-IDCGM-IIWinWoutWoutWinWhhWLfW1fWLfW1fWhhbcmsthtbcotbmothtstFigure 3: Compact representations of DCGM-I (left) andDCGM-II (right).
The decoder RLM receives a bias fromthe context encoder.
In DCGM-I, we encode the bag-of-words representation of both c and m in a single vectorbcm.
In DCGM-II, we concatenate the representations bcand bmon the first layer to preserve order information.into a fixed-length vector representation the is usedby the RLM to decode the response.
This is illus-trated in Figure 3 (left).
First, we consider c andm asa single sentence and compute a single bag-of-wordsrepresentation bcm?
RV.
Then, bcmis providedas input to a multilayered non-linear forward archi-tecture that produces a fixed-length representationthat is used to bias the recurrent state of the decoderRLM.
At training time, both the context encoder andthe RLM decoder are learned so as to minimize thenegative log-probability of the generated response.The parameters of the model are ?DCGM-I=?Win,Whh,Wout, {W`f}L`=1?, where {W`f}L`=1arethe weights for the L layers of the feed-forward con-text networks.
The fixed-length context vector kLisobtained by forward propagation of the network:k1= b>cmW1fk`= ?
(k>`?1W`f) for ` = 2, ?
?
?
, L(6)The rows of W1fcontain the embeddings of the vo-cabulary.2These are different from those employedin the RLM and play a crucial role in promoting thespecialization of the context encoder to a distincttask.
The hidden layer of the decoder RLM takes the2Notice that the first layer of the encoder network is linear.We found that this helps learning the embedding matrix as itreduces the vanishing gradient effect partially due to stacking ofsquashing non-linearities (Pascanu et al, 2013).following form:ht= ?
(h>t?1Whh+ kL+ s>tWin) (7a)ot= h>tWout(7b)p(st+1|s1, .
.
.
, st?1, c,m) = softmax(ot) (7c)This model conditions on the previous utterances viabiasing the hidden layer state on the context repre-sentation kL.
Note that the context representationdoes not change through time.
This is useful because:(a) it forces the context encoder to produce a repre-sentation general enough to be useful for generatingall words in the response and (b) it helps the RLMdecoder to remember context information when gen-erating long responses.4.3 Dynamic-Context Generative Model IIBecause DCGM-I does not distinguish between c andm, that model has the propensity to underestimatethe strong dependency that holds between m and r.Our third model (DCGM-II) addresses this issue byconcatenating the two linear mappings of the bag-of-words representations bcand bmin the input layer ofthe feed-forward network representing c and m (seeFigure 3 right).
Concatenating continuous representa-tions prior to deep architectures is a common strategyto obtain order-sensitive representations (Bengio etal., 2003; Devlin et al, 2014).The forward equations for the context encoder are:k1= [b>cW1f, b>mW1f],k`= ?
(k>`?1W`f) for ` = 2, ?
?
?
, L(8)where [x, y] denotes the concatenation of x and y vec-tors.
In DCGM-II, the bias on the recurrent hiddenstate and the probability distribution over the nexttoken are computed as described in Eq.
7.5 Experimental Setting5.1 Dataset ConstructionFor computational efficiency and to alleviate the bur-den of human evaluators, we restrict the context se-quence c to a single sentence.
Hence, our dataset iscomposed of ?triples?
?
?
(c?,m?, r?)
consisting ofthree sentences.
We mined 127M context-message-response triples from the Twitter FireHose, coveringthe 3-month period June 2012 through August 2012.199Corpus # Triples Avg # Ref [Min,Max] # RefTuning 2118 3.22 [1, 10]Test 2114 3.58 [1, 10]Table 1: Number of triples, average, minimum and maxi-mum number of references for tuning and test corpora.Only those triples where context and response weregenerated by the same user were extracted.
To mini-mize noise, we selected triples that contained at leastone frequent bigram that appeared more than 3 timesin the corpus.
This produced a corpus of 29M Twittertriples.
Additionally, we hired crowdsourced raters toevaluate approximately 33K candidate triples.
Judg-ments on a 5-point scale were obtained from 3 ratersapiece.
This yielded a set of 4232 triples with a meanscore of 4 or better that was then randomly binnedinto a tuning set of 2118 triples and a test set of 2114triples3.
The mean length of responses in these setswas approximately 11.5 tokens, after cleanup (e.g.,stripping of emoticons), including punctuation.5.2 Automatic EvaluationWe evaluate all systems using BLEU (Papineni et al,2002) and METEOR (Banerjee and Lavie, 2005), andsupplement these results with more targeted humanpairwise comparisons in Section 6.3.
A major chal-lenge in using these automated metrics for responsegeneration is that the set of reasonable responsesin our task is potentially vast and extremely diverse.The dataset construction method just described yieldsonly a single reference for each status.
Accordingly,we extend the set of references using an IR approachto mine potential responses, after which we have hu-man judges rate their appropriateness.
As we see inSection 6.3, it turns out that by optimizing systemstowards BLEU using mined multi-references, BLEUrankings align well with human judgments.
This laysgroundwork for interesting future correlation studies.Multi-reference extraction We use the followingalgorithm to better cover the space of reasonable re-sponses.
Given a test triple ?
?
(c?,m?, r?
), ourgoal is to mine other responses {r??}
that fit the con-text and message pair (c?,m?).
To this end, we firstselect a set of 15 candidate triples {??}
using an IR3The Twitter ids of the tuning and test sets along with thecode for the neural network models may be obtained fromhttp://research.microsoft.com/convo/system.
The IR system is calibrated in order to selectcandidate triples ??
for which both the message m?
?and the response r?
?are similar to the original mes-sage m?and response r?.
Formally, the score of acandidate triple is:s(??
, ?)
= d(m??,m?)
(?d(r?
?, r?)+(1??
)), (9)where d is the bag-of-words BM25 similarity func-tion (Robertson et al, 1995), ?
controls the impactof the similarity between the responses and  is asmoothing factor that avoids zero scores for candi-date responses that do not share any words with thereference response.
We found that this simple for-mula provided references that were both diverse andplausible.
Given a set of candidate triples {??
}, hu-man evaluators are asked to rate the quality of theresponse within the new triples {(c?,m?, r??)}.
Af-ter human evaluation, we retain the references forwhich the score is 4 or better on a 5 point scale, re-sulting in 3.58 references per example on average(Table 1).
The average lengths for the responses inthe multi-reference tuning and test sets are 8.75 and8.13 tokens respectively.5.3 Feature SetsThe response generation systems evaluated in this pa-per are parameterized as log-linear models in a frame-work typical of statistical machine translation (Ochand Ney, 2004).
These log-linear models comprisethe following feature sets:MT MT features are derived from a large responsegeneration system built along the lines of Ritter etal.
(2011), which is based on a phrase-based MT de-coder similar to Moses (Koehn et al, 2007).
OurMT feature set includes the following features thatare common in Moses: forward and backward maxi-mum likelihood ?translation?
probabilities, word andphrase penalties, linear distortion, and a modifiedKneser-Ney language model (Kneser and Ney, 1995)trained on Twitter responses.
For the translation prob-abilities, we built a very large phrase table of 160.7million entries by first filtering out Twitterisms (e.g.,long sequences of vowels, hashtags), and then se-lecting candidate phrase pairs using Fisher?s exacttest (Ritter et al, 2011).
We also included MT de-coder features specifically motivated by the responsegeneration task: Jaccard distance between source and200System BLEURANDOM 0.33MT 3.21HUMAN 6.08Table 2: Multi-reference corpus-level BLEU obtained byleaving one reference out at random.target phrase, Fisher?s exact probability, and a scorerelating the lengths of source and target phrases.IR We also use an IR feature built from an index oftriples, whose implementation roughly matches theIRstatusapproach described in Ritter et al (2011): Fora test triple ?
, we choose r?
?as the candidate responseiff ??
= arg max??d(m?,m??
).CMM Neither MT nor IR traditionally take into ac-count contextual information.
Therefore, we take intoconsideration context and message matches (CMM),i.e., exact matches between c, m and r. We define8 features as the [1-4]-gram matches between c andthe candidate reply r and the [1-4]-gram matchesbetween m and the candidate reply r. These exactmatches help capture and promote contextual infor-mation in the replies.RLMT, DCGM-I, DCGM-II We consider theRLM trained on the concatenated triples, denoted asRLMT (Section 4.1), to be a context-sensitive RLMbaseline.
Each neural network model contributes anadditional feature corresponding to the likelihood ofthe candidate response given context and message.5.4 Model TrainingThe proposed models are trained on a 4M subset ofthe triple data.
The vocabulary consists of the mostfrequent V = 50K words.
In order to speed up train-ing, we use the Noise-Contrastive Estimation (NCE)loss, which avoids repeated summations over V byapproximating the probability of the target word (Gut-mann and Hyv?arinen, 2010).
Parameter optimizationis done using Adagrad (Duchi et al, 2011) with amini-batch size of 100 and a learning rate ?
= 0.1,which we found to work well on held-out data.
Inorder to stabilize learning, we clip the gradients toa fixed range [?10, 10], as suggested in Mikolov etal.
(2010).
All the parameters of the neural modelsare sampled from a normal distribution N (0, 0.01)while the recurrent weight Whhis initialized as arandom orthogonal matrix and scaled by 0.01.
Toprevent over-fitting, we evaluate performance on aheld-out set during training and stop when the objec-tive increases.
The size of the RLM hidden layer isset to K = 512, where the context encoder is a 512,256, 512 multilayer network.
The bottleneck in themiddle compresses context information that leads tosimilar responses and thus achieves better generaliza-tion.
The last layer embeds the context vector intothe hidden space of the decoder RLM.5.5 Rescoring SetupWe evaluate the proposed models by rescoring then-best candidate responses obtained using the MTphrase-based decoder and the IR system.
In contrastto MT, the candidate responses provided by IR havebeen created by humans and are less affected by flu-ency issues.
The different n-best lists will providea comprehensive testbed for our experiments.
First,we augment the n-best list of the tuning set with thescores of the model of interest.
Then, we run an itera-tion of MERT (Och, 2003) to estimate the log-linearweights of the new features.
At test time, we rescorethe test n-best list with the new weights.6 Results6.1 Lower and Upper BoundsTable 2 shows the expected upper and lower boundsfor this task as suggested by BLEU scores for humanresponses and a random response baseline.
The RAN-DOM system comprises responses randomly extractedfrom the triples corpus.
HUMAN is computed bychoosing one reference amongst the multi-referenceset for each context-status pair.4Although the scoresare lower than those usually reported in SMT tasks,the ranking of the three systems is unambiguous.6.2 BLEU and METEORThe results of automatic evaluation using BLEU andMETEOR are presented in Table 3, where somebroad patterns emerge.
First, both metrics indi-cate that a phrase-based MT decoder outperformsa purely IR approach.
Second, adding CMM features4For the human score, we compute corpus-level BLEU witha sampling scheme that randomly leaves out one reference - thehuman sentence to score - for each reference set.
This samplingscheme (repeated with 100 trials) is also applied for the MT andRANDOM system so as to make BLEU scores comparable.201MT n-best BLEU (%) METEOR (%)MT9 feat.3.60 (-9.5%) 9.19 (-0.9%)CMM9 feat.3.33 (-16%) 9.34 (+0.7%) MT + CMM17 feat.3.98 (-) 9.28 (-)RLMT2 feat.4.13 (+3.7%) 9.54 (+2.7%)DCGM-I2 feat.4.26 (+7.0%) 9.55 (+2.9%)DCGM-II2 feat.4.11 (+3.3%) 9.45 (+1.8%)DCGM-I + CMM10 feat.4.44 (+11%) 9.60 (+3.5%)DCGM-II + CMM10 feat.4.38 (+10%) 9.62 (+3.5%)IR n-best BLEU (%) METEOR (%)IR2 feat.1.51 (-55%) 6.25 (-22%)CMM9 feat.3.39 (-0.6%) 8.20 (+0.6%) IR + CMM10 feat.3.41 (-) 8.04 (-)RLMT2 feat.2.85 (-16%) 7.38 (-8.2%)DCGM-I2 feat.3.36 (-1.5%) 7.84 (-2.5%)DCGM-II2 feat.3.37 (-1.1%) 8.22 (+2.3%)DCGM-I + CMM10 feat.4.07 (+19%) 8.67 (+7.8%)DCGM-II + CMM10 feat.4.24 (+24%) 8.61 (+7.1%)Table 3: Context-sensitive ranking results on both MT (left) and IR (right) n-best lists, n = 1000.
The subscriptfeat.indicates the number of features of the models.
The log-linear weights are estimated by running one iteration of MERT.We mark by (?%) the relative improvements with respect to the reference system ().to the baseline systems helps.
Third, the neural net-work models contribute measurably to improvement:RLMT and DCGM models outperform baselines, andDCGM models provide more consistent gains thanRLMT.MT vs. IR BLEU and METEOR scores indicatethat the phrase-based MT decoder outperforms apurely IR approach, despite the fact that IR proposesfluent human generated responses.
This may be be-cause the IR model only loosely captures importantpatterns between message and response: It rankscandidate responses solely by the similarity of theirmessage with the message of the test triple (?5.3).
Asa result, the top ranked response is likely to drift fromthe purpose of the original conversation.
The MT ap-proach, by contrast, more directly models statisticalpatterns between message and response.CMM MT+CMM, totaling 17 features (9 from MT+ 8 CMM), improves 0.38 BLEU points, a 9.5%relative improvement, over the baseline MT model.IR+CMM, with 10 features (IR + word penalty +8 CMM), benefits even more, attaining 1.8 BLEUpoints and 1.5 METEOR points over the IR base-line.
Figure 4 (a) and (b) plots the magnitude ofthe learned CMM feature weights for MT+CMMand IR+CMM.
CMM features help in both these hy-pothesis spaces and especially on the IR n-best list.Figure 4 (b) supports the hypothesis formulated in theprevious paragraph: Since IR solely captures inter-message similarities, the matches between messageand response are important, while context matcheshelp in providing additional gains.
The phrase-basedstatistical patterns captured by the MT system do a1-gram 2-gram 3-gram 4-gram0.00.10.20.30.40.50.6Feat.
weights(a) MT+CMM1-gram 2-gram 3-gram 4-gram0.00.10.20.30.40.50.6Feat.
weights(c) DCGMII+CMM on MT1-gram 2-gram 3-gram 4-gram0.00.10.20.30.40.50.6Feat.
weights(b) IR+CMMm matchesc matches1-gram 2-gram 3-gram 4-gram0.00.10.20.30.40.50.6Feat.
weights(d) DCGMII+CMM on IRFigure 4: Comparison of the weights of learned CMMfeatures for MT+CMM and IR+CMM systems (a) et (b)and DCGM-II+CMM on MT and IR (c) and (d).good job in explaining away 1-gram and 2-gram mes-sage matches (Figure 4 (a)) and the performance gainmainly comes from context matches.
On the otherhand, we observe that 4-gram matches may be impor-tant in selecting appropriate responses.
Inspection ofthe tuning set reveals instances where responses con-tain long subsequences of their corresponding mes-sages, e.g., m = ?good night best friend, I love you?,r = ?I love you too, good night best friend?.
Althoughinfrequent, such higher-order n-gram matches, whenthey occur, may provide a more robust signal of thequality of the response than 1- and 2-gram matches,given the highly conversational nature of our dataset.RLMT and DCGM Both RLMT and DCGMmodels outperform their respective MT and IR base-lines.
Both models also exhibit similar performanceand show improvements over the MT+CMM mod-els, albeit using a lower dimensional feature space.We believe that their similar performance is due tothe limited diversity of MT n-best list together withgains in fluency stemming from the strong language202System A System B Gain (%) CIHUMAN MT+CMM 13.6* [12.4,14.8]DCGM-II MT 1.9* [0.8, 2.9]DCGM-II+CMM MT 3.1* [2.0, 4.3]DCGM-II+CMM MT+CMM 1.5* [0.5, 2.5]DCGM-II IR 5.2* [4.0, 6.4]DCGM-II+CMM IR 5.3* [4.1, 6.6]DCGM-II+CMM IR+CMM 2.3* [1.2, 3.4]Table 4: Pairwise human evaluation scores between Sys-tem A and B.
The first (second) set of results refer to theMT (IR) hypothesis list.
The asterisk means agreementbetween human preference and BLEU rankings.model provided by the RLM.
In the case of IR mod-els, on the other hand, there is more headroom forimprovement and fluency is already guaranteed.
Anygains must come from context and message matches.Hence, RLMT underperforms with respect to bothDCGM and IR+CMM.
The DCGM models appear tohave better capacity to retain contextual informationand thus achieve similar performance to IR+CMMdespite their lack of exact n-gram match information.In the present experimental setting, no strikingperformance difference can be observed between thetwo versions of the DCGM architecture.
If multiplesequences were used as context, we expect that theDCGM-II model would likely benefit more owing tothe separate encoding of message and context.DCGM+CMM We also investigated whether mix-ing exact CMM n-gram overlap with semantic in-formation encoded by the DCGM models can bringadditional gains.
DCGM-{I-II}+CMM systems eachtotaling 10 features show increases of up to 0.48BLEU points over MT+CMM and up to 0.88 BLEUover the model based on Ritter et al (2011).
ME-TEOR improvements similarly align with BLEU im-provements both for MT and IR lists.
We take thisas evidence that CMM exact matches and DCGMsemantic matches interact positively, a finding thatcomports with Gao et al (2014a), who show thatsemantic relationships mined through phrase embed-dings correlate positively with classic co-occurrence-based estimations.
Analysis of CMM feature weightsin Figure 4 (c) and (d) suggests that 1-gram matchesare explained away by the DCGM model, but thathigher order matches are important.
It appears thatDCGM models might be improved by preservingword-order information in context and message en-codings.6.3 Human EvaluationHuman evaluation was conducted using crowd-sourced annotators.
Annotators were asked to com-pare the quality of system output responses pairwise(?Which is better??)
in relation to the context andmessage strings in the 2114 item test set.
Identicalstrings were held out, so that the annotators only sawthose outputs that differed.
Paired responses werepresented in random order to the annotators, and eachpair of responses was judged by 5 annotators.Table 4 summarizes the results of human evalua-tion, giving the difference in mean scores (pairwisepreference margin) between systems and 95% confi-dence intervals generated using Welch?s t-test.
Iden-tical strings not shown to raters are incorporated withan automatically assigned score of 0.5.
The pattern inthese results is clear and consistent: context-sensitivesystems (+CMM) outperform non-context-sensitivesystems, with preference gains as high as approxi-mately 5.3% in the case of DCGM-II+CMM versusIR, and about 3.1% in the case of DCGM-II+CMMversus MT.
Similarly, context-sensitive DCGM sys-tems outperform non-DCGM context-sensitive sys-tems by 1.5% (MT) and 2.3% (IR).
These results areconsistent with the automated BLEU rankings andconfirm that our best performing DCGM models out-perform both raw baseline and the context-sensitivebaseline using CMM features.6.4 DiscussionTable 5 provides examples of responses generated onthe tuning corpus by the MT-based DCGM-II+CMMsystem, our best system in terms of both BLEU andhuman evaluation.
Responses from this system are onaverage shorter (8.95 tokens) than the original humanresponses in the tuning set (11.5 tokens).
Overall, theoutputs tend to be generic or commonplace, but areoften reasonably plausible in the context as in ex-amples 1-3, especially where context and messagecontain common conversational elements.
Example 2illustrates the impact of context-sensitivity: the word?book?
in the response is not found in the message.Nonetheless, longer generated responses are apt todegrade both syntactically and in terms of content.We notice that longer responses are likely to present203# Context c Messagem Generated Response r1 noone can help this time its just i need a support system well im here to support you.
whenever youneed me.thank you, i appreciate it thank you2 nothin much, and how?s the book?!
its good but i?m only like halfway throughcuz i don?t feel like reading.
i?m so bored ...that?s good!
i have the book but i?mbored too.3 i know!
what have you been up to?
nothing really.
just enjoying summer andworking.
what about you?that?s good!
nothing much justlistening to music and you?4 thanks, your way better though i?m just good at expressing it that?s but trustme you are freakin amazing and waay betteri know, i don?t trust you, i love youtoo!
thank you5 thanks.
yeah i am good thanks!
good, but angry!
sorry you?re angry, been there, i hope it diesdown.i hope so too, i?m not angry!
!6 figures.
bass.
never ate bass.
bass is extremely mild with very few bones.my favorite after catfishthat?s one of the bones, chuck bass ismine too.7 today i will walk for miles.
clear sky clear mind.
love walking.
it?s too hot to walk this month.i haven?t moved.
frozen.
heat freeze.it?s too hot for you to get up to honeyif you have frozen yogurt to you.8 arresting people over ?nasty tweets??
please.
i?m notcondoning what the lad did but arresting him is a tadsilly.it appears he may have sent death threats toothers while all this was kicking off too.i did!
he was armed with nuclearthreats?
that?s what?s happening toyou.Table 5: Sample responses produced by the MT-based DCGM-II+CMM system.information that conflicts either internally within theresponse itself, or is at odds with the context, as in ex-amples 4-5.
This is not unsurprising, since our modellacks mechanisms both for reflecting agent intent inthe response and for maintaining consistency withrespect to sentiment polarity.
Longer context andmessage components may also result in responsesthat wander off-topic or lapse into incoherence as in6-8, especially when relatively low frequency uni-grams (?bass?, ?threat?)
are echoed in the response.In general, we expect that larger datasets and incorpo-ration of more extensive contexts into the model willhelp yield more coherent results in these cases.
Con-sistent representation of agent intent is outside thescope of this work, but will likely remain a significantchallenge.7 ConclusionWe have formulated a neural network architecturefor data-driven response generation trained from so-cial media conversations, in which generation ofresponses is conditioned on past dialog utterancesthat provide contextual information.
We have pro-posed a novel multi-reference extraction techniqueallowing for robust automated evaluation using stan-dard SMT metrics such as BLEU and METEOR.Our context-sensitive models consistently outper-form both context-independent and context-sensitivebaselines by up to 11% relative improvement inBLEU in the MT setting and 24% in the IR setting, al-beit using a minimal number of features.
As our mod-els are completely data-driven and self-contained,they hold the potential to improve fluency and con-textual relevance in other types of dialog systems.Our work suggests several directions for futureresearch.
We anticipate that there is much room forimprovement if we employ more complex neural net-work models that take into account word order withinthe message and context utterances.
Direct genera-tion from neural network models is an interesting andpotentially promising next step.
Future progress inthis area will also greatly benefit from thorough studyof automated evaluation metrics.AcknowledgmentsWe thank Alan Ritter, Ray Mooney, Chris Quirk,Lucy Vanderwende, Susan Hendrich and MouniReddy for helpful discussions, as well as the threeanonymous reviewers for their comments.ReferencesMichael Auli, Michel Galley, Chris Quirk, and GeoffreyZweig.
2013.
Joint language and translation modelingwith recurrent neural networks.
In Proc.
of EMNLP,pages 1044?1054.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improved204correlation with human judgments.
In Proc.
of ACLWorkshop on Intrinsic and Extrinsic Evaluation Mea-sures for Machine Translation and/or Summarization,pages 65?72, Ann Arbor, Jun.Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.2003.
A neural probabilistic language model.
Journ.Mach.
Learn.
Res., 3:1137?1155.Kyunghyun Cho, Bart van Merrienboer, C?aglar G?ulc?ehre,Fethi Bougares, Holger Schwenk, and Yoshua Ben-gio.
2014.
Learning phrase representations usingRNN encoder-decoder for statistical machine transla-tion.
Proc.
of EMNLP.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neuralnetworks with multitask learning.
In Proc.
of ICML,pages 160?167.
ACM.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for statisti-cal machine translation.
In Proc.
of ACL.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journ.
Mach.
Learn.
Res.,12:2121?2159.Jianfeng Gao, Xiaodong He, Wen tau Yih, and Li Deng.2014a.
Learning continuous phrase representations fortranslation modeling.
In Proc.
of ACL, pages 699?709.Jianfeng Gao, Patrick Pantel, Michael Gamon, XiaodongHe, and Li Deng.
2014b.
Modeling interestingnesswith deep neural networks.
In Proc.
of EMNLP, pages2?13.Kallirroi Georgila, James Henderson, and Oliver Lemon.2006.
User simulation for spoken dialogue sys-tems: Learning and evaluation.
In Proc.
of Inter-speech/ICSLP.Michael Gutmann and Aapo Hyv?arinen.
2010.
Noise-contrastive estimation: A new estimation principle forunnormalized statistical models.
In Proc.
of AISTATS,pages 297?304.Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck.
2013.
Learning deepstructured semantic models for web search using click-through data.
In Proc.
of CIKM, pages 2333?2338.Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrentcontinuous translation models.
Proc.
of EMNLP, pages1700?1709.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.
In Proc.of ICASSP, pages 181?184, May.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In Proc.
of ACLDemo and Poster Sessions, pages 177?180.Tomas Mikolov and Geoffrey Zweig.
2012.
Context De-pendent Recurrent Neural Network Language Model.Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cer-nock?y, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
In Proc.
of INTER-SPEECH, pages 1045?1048.Franz Josef Och and Hermann Ney.
2004.
The alignmenttemplate approach to machine translation.
Comput.Linguist., 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of ACL, pages311?318.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neuralnetworks.
Proc.
of ICML, pages 1310?1318.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProc.
of EMNLP, pages 583?593.Stephen E Robertson, Steve Walker, Susan Jones, et al1995.
Okapi at TREC-3.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1988.
Learning representations by back-propagating errors.
In James A. Anderson and EdwardRosenfeld, editors, Neurocomputing: Foundations ofResearch, pages 696?699.
MIT Press, Cambridge, MA,USA.Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, andGr?egoire Mesnil.
2014.
A latent semantic modelwith convolutional-pooling structure for informationretrieval.
In Proc.
of CIKM, pages 101?110.Amanda Stent and Srinivas Bangalore.
2014.
NaturalLanguage Generation in Interactive Systems.
Cam-bridge University Press.Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014.Sequence to sequence learning with neural networks.Proc.
of NIPS.Marilyn A. Walker, Rashmi Prasad, and Amanda Stent.2003.
A trainable generator for recommendations inmultimodal dialog.
In Proc.
of EUROSPEECH.Steve Young, Milica Ga?si?c, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, and KaiYu.
2010.
The hidden information state model: Apractical framework for pomdp-based spoken dialoguemanagement.
Comput.
Speech Lang., 24(2):150?174.Steve Young.
2002.
Talking to machines (statisticallyspeaking).
In Proc.
of INTERSPEECH.205
