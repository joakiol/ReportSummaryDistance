1DESCRIPTION OF THE TASC SYSTEM USED FOR MUC-7Terry Patten,  Beryl Hoffman,  Martin ThurnTASC13605 Dulles Technology DriveHerndon VA 20171tapatten@tasc.com(703) 793-3700 x2580INTRODUCTIONTASC has recently developed a technology for learning scenario-template-extraction grammars from examples provided by end-users with no special computational orlinguistic knowledge.
For straightforward scenario-template problems, complete extraction systemscan be learned from scratch in a few hours.
The learned systems are fast, robust, and as accurate ascarefully handcrafted systems.
For more complex extraction problems, such as the MUC-7 task, thegrammars can still be learned automatically, but some computational-linguistic work may berequired for complex template merging and inference.Although MUC evaluations do not test some of the strengths of the TASC extractiontechnology?in particular its speed and robustness?the evaluation of recall and precision wasvaluable.
Early anecdotal experience with this grammar-learning technology suggested that thelearned grammars performed practical extraction at least as well as manually constructed grammars.MUC-7 provided the opportunity for a more rigorous evaluation, and a comparison with otherscenario-template extraction technologies.SYSTEM OVERVIEWTASC?s research has focused on the problem of learning the grammatical knowledgerequired for scenario-template extraction.
Only the ST task was performed?off-the-shelf productsand well-known techniques were used to handle entity identification and anaphora, and only to theextent necessary for the ST task.
IsoQuest?s NetOwl?
was used to mark up entities as a pre-processing step.
NetOwl does a good job of recognizing many categories such as dates, locations,people, and organizations.
Although the names of rockets and satellites are not built into the product,they were easily added to its dictionary.For straightforward scenario-template extraction problems, a single extractor can easily betrained to handle all the slots.
For more complex problems it may be wise to conceptualize theproblem as several relatively independent extraction problems.
The MUC-7 task was divided intothree simpler problems: filling a template for launches, filling a template for payloads, and filling atemplate for missile events.
Each of these three extraction problems was addressed with a separateextractor that was independently trained.
The results of the three extractors were merged as the finalstep in the extraction process.
Reducing the problem in this fashion can be particularly beneficial forthe learning component, as the information about missile events (for instance), involves verydifferent language than that for rocket launches and payloads.
A shared slot helped merge templatesfor launches and payloads.
In cases where the shared slot was not clearly matched, mergingdecisions were made by a set of domain-specific heuristics.2Breaking up an extraction problem in this manner has proved to be a versatile and valuabletechnique.
Even apparently overwhelming extraction problems can often be broken down into anumber of manageable sub-problems that can successfully be trained independently.The speed of the system as a whole is impressive despite having to run several extractors onthe data.
The total amount of time taken to process the formal-run data (.39MB) was two minutes ona single-processor machine.
This measurement includes the time required to run NetOwl, to runeach of the three extractors one after the other, and to do all the merging and inference.LEARNING GRAMMARSThe grammars TASC uses to perform scenario-template extraction are a special variant ofsemantic grammars [1] that were developed for robustness in practical applications?they are highlytolerant of corrupted data, ungrammatical sentences, and other types of noise.
These grammars arelearned from scratch through a combination of statistical and search algorithms.
There is no basegrammar from which the learning starts?the system begins with no grammatical knowledge at allbecause it was designed to be able to learn extraction grammars for any natural language, or fortechnical sublanguages that may contain unusual constructions.
Once a grammar is learned, acorresponding extractor is generated automatically.The process of learning these extraction grammars begins by defining the template througha simple graphical user interface.
That interface allows the names of the slots to be entered, and thecategory of the corresponding filler to be chosen from a simple menu.
For instance, the interface canbe used to specify that there is a Payload Owner slot that should be filled by an Organization.Once the slots and fillers have been specified, an initial extractor is automaticallyconstructed.
Having no grammatical knowledge, it is willing to fill the slots with anything of theright category, producing poor results.
But these poor results give the end-user a tangible focus fortraining (not to mention motivation for training).A graphical display (see Figure 1) is used to show the results of the extraction run on agiven set of data.
Any mistakes that appear can easily be corrected by the end-user with two clicksof the mouse.
The interface displays a text passage with color-coded annotations corresponding tothe scenario-template slots.
Clicking on an incorrect item will produce a menu of possiblealternatives?other slots that could be filled by an item of that type.
For instance, if an organizationis mistakenly marked as the Manufacturer of a payload, the menu will contain other slots that can befilled by an organization?Owner will be in the menu, but Site will not.
The menu always containsan Other alternative to indicate that the item does not fill any slot.After several examples have been corrected, the user invokes the learning mechanism byclicking on the ?Learn Rules?
button.
Once the learning is complete, the new extractor can be runon additional training examples.
Initially, the system?with no grammatical knowledge?is merelyguessing at how to interpret a passage, and many mistakes are made.
With each learning iteration,fewer mistakes are made and the training becomes faster and easier.
This iterative bootstrappingprocess is much more efficient than simply marking up a corpus, because the system quickly starts toextract much of the information correctly, and the annotations must only be done for the ones it getswrong.
In most cases, the system will end up doing the majority of the actual mark-up, with the usersimply verifying the results.3Figure 1:  Training the system to extract information about payloadsAdditional training efficiency is gained by automatically sorting the training examples.Rather than simply presenting examples sequentially as they appear in the training corpus, the besttraining examples are displayed first.
Choosing the right training examples can allow the system toimprove faster in the early stages?and this reduces the human effort because of the bootstrappingprocess.
Learning faster in the early stages also ensures the best results if the user runs out of timeor patience and is not able to work through the entire training corpus.One technique for ordering training examples is to display the examples containing themost ambiguity first.
This allows the teacher to provide the most information (in the information-theoretic sense) in the least amount of time.
For instance, an organization in the payload extractorcould be the owner, the manufacturer, the recipient, or a spurious organization such as the parent ofthe manufacturer.
Obtaining feedback on examples that contain several organizations allows thesystem to quickly learn how to put organizations in the right slots.Another ordering technique is to avoid displaying examples if several similar exampleshave already been presented.
There is no point displaying hundreds of similar examples?thesystem will learn the relevant grammatical knowledge after the first few.
This technique cansignificantly reduce the required training time and allows the system to benefit from a larger varietyof constructions earlier in the training process.In summary, being able to learn extraction grammars from training examples is only usefulif the required training examples can be obtained cost-effectively in practice.
Annotating examplescan be prohibitively expensive, but the graphical user interface, bootstrapped training, and orderedexamples described above allow an end-user to perform the training quickly and easily.
The totalamount of human effort involved in creating the three extraction grammars for the MUC-7 scenario-template task was roughly one day.4DISCUSSIONPerformance vs. Other SystemsUsing grammars that were learned automatically from scratch, the TASC system receivedthe second-highest score on the scenario-template formal run and the highest score on the dry run.This clearly suggests that the grammar-learning technology is able to compete with the very bestmethods?both manual and automatic?for building grammars for extraction systems.
It isimportant to understand, however, that there were several issues that made these MUC-7 tasksparticularly ill-suited for the TASC technology.
This section will outline the reasons why thistechnology performs significantly better on many other scenario-template extraction problems.First, TASC did not participate in the NE evaluation, and used an off-the-shelf named-entity tool with a limited customization effort.
Given the importance of MUC-7 categories such asrockets, satellites, and military vehicles, none of which are built into NetOwl, the score was likelylowered by the limited effort in this area.
The MUC experiment is really not designed forparticipants who want to evaluate scenario-template technology independently of named-entityissues.
The combination of grammar learning and off-the-shelf name tagging should perform betteron tasks that were not specifically chosen to be challenging for entity-identification tools.Second, the discussion-oriented nature of the New York Times articles poses significantdifficulties for approaches based on grammar learning.
At first glance, the New York Times articleswould seem to be ideal for learning?they are free from spelling errors and contain only highlygrammatical text.
But discussion articles provide little consistency in how information is presented.Indeed, repeated use of similar constructions would be considered poor style.
This, together with thesmall amount of New York Times data provided, made finding generalizations extremely difficult.The grammar-learning techniques embodied in the TASC system produce better results when severalexamples of each construction can be provided during the training?as is normally the case in WallStreet Journal business briefs, technical documents, news briefs, and many other sources.Learning extraction grammars automatically has enormous benefits in terms of the time andeffort required to build a system, but there has always been a recall/precision penalty.
The recall andprecision scores of the TASC system demonstrate that automatically learned grammars can competewith manually constructed grammars, even for complex extraction problems, and even when thelearning takes place under difficult conditions.Finally, it is important to note that while the MUC-7 task required customized templatemerging by computational linguists, this new learning technology allows simpler template-extractionproblems to be successfully tackled by an end-user with no additional assistance.
For many practicalproblems, a financial or intelligence analyst can complete a scenario-template extractor for acompletely new problem in a few hours.
Applying extraction to spur-of-the-moment, time-criticalapplications has not been possible until now.Performance vs. HumansThe TASC extraction system, like all other MUC ST extraction systems to date, has recalland precision scores that are much lower than those published for the human annotators.
It shouldbe noted, however, that MUC evaluations are heavily biased against fast extraction systems and infavor of the human annotators because of the small amount of test data and the practically unlimitedamount of time allowed for the task.
In practice, time limitations have a significant impact onrecall?information cannot be extracted from material that is not read.
Many applications requirethe monitoring of a large number of online sources, and recall for humans can be very low becauseof time constraints.
In this type of situation, TASC?s fast extraction system will achieve recallscores that are several times better than the human scores, and hence combined recall and precisionscores that are significantly better than those for humans.5CONCLUSIONSBeing able to learn extraction grammars from examples provided by an end-user greatlyincreases the practicality of scenario-template-extraction technology.
Manually constructinggrammars is a slow, laborious, and ultimately expensive process.
Largely eliminating this costproduces a huge swing in the cost/benefit ratio?even if some effort by computational linguists isstill required for other aspects of the problem.The benefits of trainable systems can be lost, however, if the training itself is too expensiveor requires special skills.
TASC has addressed this issue by developing a training scheme thatrequires no special computational or linguistic knowledge of the teacher, involves an extremelysimple graphical interface, uses bootstrapping to automate the creation of training examples, andautomatically selects the best training examples.
An end-user can train this technology quickly andeasily, ensuring the benefits of learning grammars are maximized.The most significant result here is that automatically learned grammars do not entail arecall/precision sacrifice.
The scores achieved using the automatically learned grammars clearlyindicate that this new technology can successfully compete with handcrafted grammars, even whenfaced with complex problems and data ill-suited to learning.REFERENCES[1] Allen, J., Natural Language Understanding, Bejamin/Cummings, 1987, pp.
250-253.
