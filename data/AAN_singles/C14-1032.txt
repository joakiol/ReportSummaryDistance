Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 322?333, Dublin, Ireland, August 23-29 2014.Zipf?s Law and Statistical Data on Modern TibetanHuidan LiuInstitute of Software,Chinese Academy ofSciences, Beijing,China, 100190huidan@iscas.ac.cnMinghua NuoInstitute of Software,Chinese Academy ofSciences, Beijing,China, 100190minghua@iscas.ac.cnJian WuInstitute of Software,Chinese Academy ofSciences, Beijing,China, 100190wujian@iscas.ac.cnAbstractIn this paper, a large scale modern Tibetan text corpus is built, which includes about 190 thou-sands documents, 67.21 million words, 93.66 million syllables in total.
Based on the corpus,statistics are made in several language units in different granularities.
Statistical data show that: a syllable has 3.26 letters or 2.20 super characters in average, while a sentence has 75.40 let-ters or 63.14 super characters.
The top 10 super characters, syllables, words take up 66.3156%,16.5556%, 24.6415% of the corpus respectively.
Curves for the n-gram frequency-rank list ofsuper chars, syllables and words are plotted.
It shows that when all the n-gram phrases forn = 1, 2, .
.
.
, 5 are put together and sorted by frequency in descending order, the frequency-rankcurves in log-log axes can be fitted well by a straight line for the unit of syllable and word respec-tively.
But for the unit of super character, we didn?t find a curve that can be fitted well enough bya straight line even if we combine all the n-grams for n = 1, 2, .
.
.
, 10.1 IntroductionThe statistical property is the natural property of a language.
In recent tens of years, people made sta-tistical analysis on Tibetan characters or syllables.
But it?s difficult to make statistics on larger languageunits such as word and n-gram word phrases, especially on a large scale corpus.
There are two reasonsresulting in the difficulty.
First, as Tibetan is a resource poor language, it?s hard to build a large scaleTibetan text corpus.
Second, Tibetan word segmentation technology is not well developed even untilnow.In this paper, we report our word on the statistics on Tibetan based on the language units such ascharacter, syllable, word and their n-gram pairs on a large scale corpus.
The remainder of the paperis organized as follow.
Tibetan language units are introduced in Section 2.
We recall the related workin Section 3.
In Section 4, the methods which is used to build the corpus and to segment Tibetan textinto language units are described in detail.
We make statistics on the corpus and list the most frequencyTibetan language units and test Zipf?s law respectively in Section 5.
Section 6 concludes this paper.2 Language Units of TibetanGenerally speaking, Tibetan is a alphabetic writing system.
But there is a unit larger than letter butsmaller than syllable, which is different from other language such as English and Chinese.
Meanwhile,people have used different terms (in English) to express the same unit or the same term to expressdifferent units.
So we must make a clarification in this Section.2.1 Letter, Character and Super CharacterThere are 30 consonants and 4 vowel signs in modern Tibetan.
Several other consonants and vowel signsare also used in Tibetan text to transliterate Sanskrit script.
There are only 4 vowel signs (writing) forthe vowels (reading) /e/, /i/, /o/ and /u/, but there isn?t any signs for the vowel /a/, so every consonant hasThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/322!
"# =   + # +  !
+  " + $% + " + #!
"#$%& &  "$$%&& !!
"'%& & & !
"()% & & &  !
"*'%& &  !
"+'%& & &  !
"$$%&Figure 1: Tibetan encoding schema with smallunit used in ISO/IEC 10646.!
"# =   + !
+ " + #(0F56)  (F393)  (0F42)  (0F66)Figure 2: Tibetan encoding schema with large unitused in GB/T 20542.???????????
?PrefixRoot LetterTshegSubscribedLetterVowelSuperscribedLetterSuffixSecondarySuffixRootLetterVowelSuffixTshegFigure 3: Structure of a Tibetan word.!"!#!$%!&!'()!
*!+!,*!-.!&!/%!%0%"!1*!2!"!
#!
$%!&!
'()!
*!+ ,*!-.!&!
/%!
%0%"!
1*!
2Yesterday man rich this house expensive an bought did .Yesterday this rich man bought an expensive house.Figure 4: A Tibetan sentence.an inherent vowel /a/.
Other vowels can be indicated using a variety of diacritics which appear above orbelow the main letter.
Each of these consonants and vowel signs is called a ?letter?.In Tibetan encoding schema used in ISO/IEC 10646 and Unicode standard (Consortium, 2013), eachTibetan consonant has two or even more code points to denote its normal form or subjoined form, orother variant forms which are only used in very special context.
Each variant form is called a ?character?corresponding to a code point in ISO/IEC 10646.
In Figure 1, seven characters form a Tibetan syllable.Note that three consonants and a vowel sign are clustered.Different from the encoding schema with small unit used in ISO/IEC 10646, in Chinese notional stan-dard GB/T 20542 and GB/T 22238 on Tibetan coded character set and some legacy Tibetan encodings,another encoding schema is used.
In this schema, the cluster of consonants and vowel sign in Figure 1is assigned only one code point.
Figure 2 shows the schema.
The encoding unit shown in Figure 2 iscalled ???
?
(Zi Ding) in Chinese but the Chinese term doesn?t have an exact English translation, andwe call it ?super character?
or ?super char?
briefly in this paper.2.2 Syllable, Word and SentenceA syllable contains one or up to seven character(s).
Syllables are separated by a marker known as?tsheg?, which is simply a superscripted dot.
People sometime use the Chinese term ??
?
(Zi, exactly acharacter in Chinese script) to denote ?syllable?.
The term ??
?
is often translated to ?word?
in English.But ?word?
mainly used to express a larger language unit as an item in the vocabulary.
So we use theterm ?syllable?
in this paper, and take ?word?
as a larger language unit which is made up of one or moresyllables and has meanings.Note that in Tibetan ?tsheg?
is used as the delimiter between two syllables.
But there is no anotherdelimiter to mark the boundary between two words.
Thus there is a lack of word boundaries in Tibetan.Figure 3 shows the structure of a Tibetan word which is made up of two syllables and means ?show?
or?exhibition?.In Tibetan text, some monosyllable words, including ??
?, ??
?, ??
?, ???
?, ???
?, ???
(We call themabbreviation markers (AM) in this paper), can glue to the previous word without a syllable delimiter?tsheg?, which produce many abbreviated syllables.
For example, when the genitive case word ??
?follows the word ??????
(king), we don?t put a ?tsheg?
between them and get the fused form ????????(king[+genitive]).
The existence of abbreviated syllables contributes to the difficulty to segment Tibetansentence into words.Tibetan sentence contains one or more phrase(s), which contain one or more words.
Another markerknown as ?shed?
indicates the sentence boundary, which looks like a vertical pipe.
Figure 4 shows aTibetan sentence and its translation in English.3233 Related WorkIn the early 1930s, G. K. Zipf pointed out a statistical feature of large language corpora ( both writtentexts and speech streams ) which, remarkably, is observed in many languages, and for different authorsand styles (Zipf, 1935).
He noticed that the number of words w(n) which occur exactly n times in alanguage corpus varies with n as w(n) ?
1/n?, where the exponent is close to 2, which results in thewell known Zipf?s law.
The general form of Zipf?s law states that:y = f(r) =Cr?
(1)where ?
is a positive parameter close to 1.Zipf showed that, by and large, his law held for words, syllables and morphemes.
Consequently, itis natural to ask if the law also holds for pairs of words.
Egghe devised a mathematical argument thatit, in fact, does not, but that the exact relation can be approximated by a power law (Egghe, 1999).
Heextended his investigations to parts of words, namely to the study of N-grams (Egghe, 2000).Zipfs law was the source of a lively debate related to the structure of DNA.
It was claimed (Mantegnaet al., 1994) that Zipf?s law shows the difference between coding and non-coding DNA as non-coding(so-called junk) DNA fits Zipf?s law much better than coding DNA.
This would mean, according to theauthors, that non-coding regions of DNA may carry new biological information.
Yet, this does not meanthat junk DNA is a kind of language.
Other scientists (Chatzidimitriou-Dreismann et al., 1996), however,have shown that this distinction is not universal and lacks all biological basis.Zipf?s law has been tested on the Internet.
It turned out that popularity of Internet pages is describedaccording to Zipf?s law.
This fact can be used to design better cache tables (Masaki and Takahashi,1998; Breslau et al., 1998; Adamic and Huberman, 2002).
Zipf?s studies on city sizes still lead to newdevelopments in geographical and economical studies (Gabaix, 1999a; Gabaix, 1999b; Okuyama et al.,1999; Ioannides and Overman, 2003; Soo, 2005; Soo, 2007).Back to text, Li (1992) found that the distribution of word frequencies for randomly generated texts isvery similar to Zipf?s law observed in natural languages such as English (Li, 1992).
Ha et al.
(2002)investigated the law for two languages English and Mandarin and for n-gram word phrases as well as forsingle words.
The law for single words is shown to be valid only for high frequency words.
However,when single word and n-gram phrases are combined together in one list and put in order of frequency thecombined list follows Zipf?s law accurately for all words and phrases, down to the lowest frequencies inboth languages.
The Zipf curves for the two languages are then almost identical (Ha et al., 2002).In recent years, researchers also made statistics on Tibetan.
Jiang and Dong (1994) made statisticson the length and different structural mode of Tibetan syllables, and counted up the number of initialclusters and finals of Tibetan syllables, as well as the number of Tibetan letters at different positions insyllables (Jiang and Dong, 1994; Jiang and Dong, 1995).
In a further research Jiang (1998; Jiang andKong (2006; Jiang and Long (2010) , they made statistics on Tibetan letters, and found that the 1th orderand 2nd order entropy of Tibetan is 3.9913 bits and 1.2531 bits resplectively (Jiang, 1998), while onsuper character they are 4.82 and 3.12 (Jiang and Kong, 2006; Jiang and Long, 2010).
Wang and Chen(2004) made similar research to calculate the frequency and information entropy of Tibetan character andsyllable based on a corpus of 20, 000, 000 characters, and discovered that the most frequent 703 Tibetansyllables cover 90% of the corpus (Wang and Chen, 2004).
She also presented the research on thefrequency-rank relation of Tibetan super character and syllable, and found that the distributions followZipf?s law too (Wang, 2004).
But no further research is reported on whether she tests Zipf?s law on largerlanguage units of Tibetan.
Other researchers also made statistics on Tibetan syllable?s structural modebased a static corpus such as syllable list or dictionary (Gao and Gong, 2005; Ai et al., 2009).
Lu et al.
(2003) presented the theories and approaches to calculate the frequencies of Tibetan characters, pieces,syllables and words based on a large scale Tibetan corpus including about 40, 000, 000 syllables (Lu etal., 2003).
However, a large part of the corpus they used are Buddhist literatures and the work can?t bedone well without a pragmatic Tibetan word segmentation tool (Chen et al., 2003a; Chen et al., 2003b;Jiang, 2006; Jiang and Kong, 2006; Sun et al., 2009; Sun et al., 2010; Lu and Shi, 2011; Liu et al.,2012a).324At present, people already find methods to build a large scale corpus from Tibetan web sites with lowcost.
Liu et al.
(2012b) presented their method to extract the title, content, author and other useful infor-mation of articles from several news and broadcasting web sites (Liu et al., 2012b).
It?s not a difficultwork to implement a pragmatic Tibetan word segmentation tool based on the former researches (Chenet al., 2003a; Chen et al., 2003b; Sun et al., 2009; Sun et al., 2010; Liu et al., 2012a) So it?s time to makestatistics on the frequency distribution of larger language units such as word and n-gram word phrase forTibetan to see whether they follow Zipf?s law.4 Methods and CorpusWe present our methods to build the corpus and to segment Tibetan text into different units mentionedabove.4.1 Building a Large Scale Tibetan Text CorpusPreviously Liu et al.
(2012b) proposed an approach to build a large scale text corpus for Tibetan naturallanguage processing.
We adopt the method to build our corpus.
we crawled eight Tibetan websiteswhich mainly focus on news and broadcastings.
Topic pages but hub pages are selected with a rule basedmethod by checking the url.
We analysed the layout structure mode of each web site and built templatesto extract topic title, publishing date, author, topic content and some other topic related informations.Consequently, a large scale Tibetan text corpus is built, which includes about 190 thousands doc-uments, 67.21 million words, 93.66 million syllables and 265 million super characters in total.
Thesources and scales in different units are shown in Table 1.?
source ?document ?sentence ?word ?syllable ?super character1 http://tb.chinatibetnews.com 74,632 1,419,967 26,648,803 37,633,467 108,010,7152 http://tb.tibet.cn 13,348 331,022 4,288,187 5,872,524 16,388,2423 http://ti.gzznews.com 8,084 281,405 3,518,918 4,763,097 13,301,4084 http://ti.tibet3.com 26,631 725,669 9,186,980 12,634,804 35,595,3455 http://tibet.people.com.cn 29,797 833,221 9,323,838 12,908,542 35,328,4436 http://www.qhtb.cn 20,616 575,242 7,908,508 10,913,097 31,200,4657 http://www.tibetcnr.com 9,559 278,681 3,272,274 4,624,878 13,114,1308 http://xizang.news.cn 7,707 187,423 3,062,419 4,307,175 12,258,911Total 190,374 4,632,630 67,209,927 93,657,584 265,197,659Table 1: the sources and scales of the corpus.It?s a heavy task to manually classify those document into domains.
However, we still canget the domain information for a certain subsets of the corpus.
For some web sites listed above,we can get the domain information from the URL of each web page.
For instance, the URL?http://tb.chinatibetnews.com/xzmeishi/2011-12/05/content 831210.htm?
shows it belongs to a columncalled ?xzmeishi?.
so it must be a page about Tibetan foods, because ?xz?
is the abbreviated form ofChinese word ?xizang?
(??
), which means the Tibetan Autonomous Region, while ?meishi?
means?delicious food?.
So we can classify the documents in the corpus into domains.
Table 2 and 3 list thedomains of subsets of the documents from two web sites named ?China Tibet News?
and ?Tibetan?s webof China?
respectively.
Obviously, a large part of the documents in the corpus are news as expected,because nearly all of the 8 web sites are hold by news agencies or radio stations.4.2 Methods to Segment Tibetan TextAs described in section 2, there is a delimiter between two Tibetan syllables.
So we can segment thetext into syllables by adding segmentation mark after the delimiter.
The encoding schema can be used tosegment text into smaller language units.The challenge lies in the word segmentation.
With a similar method to those methods proposed byother researchers (Chen et al., 2003a; Chen et al., 2003b; Jiang and Kong, 2006; Sun et al., 2009; Sunet al., 2010; Liu et al., 2012a), we implemented a segmenter.
As mentioned in Section 2.2, some mono-syllable words can glue to the previous word without a syllable delimiter ?tsheg?, which produce many325Order Domain ?document (%) ?sentence (%) ?syllable (%)1 Art 3,240 4.76 112,642 8.71 1,265,914 4.402 Finance & Economy 712 1.05 12,477 0.96 314,698 1.093 History & Geometry 2,897 4.25 19,627 1.52 283,621 0.984 News 25,247 37.08 576,842 44.59 14,753,178 51.235 Picture 12,732 18.70 51,088 3.95 766,895 2.666 Politics & Law 3,230 4.74 63,437 4.90 1,708,839 5.937 Rural Life 2,402 3.53 35,535 2.75 871,406 3.038 Social Life 1,153 1.69 9,881 0.76 233,454 0.819 Special Issues 9,986 14.67 268,003 20.72 6,499,488 22.5710 Technology & Education 1,988 2.92 38,321 2.96 825,395 2.8711 Tibetan Buddhism 1,983 2.91 48,832 3.77 569,756 1.9812 Tibetan Food 215 0.32 2,963 0.23 35,365 0.1213 Tibetan Medicine 720 1.06 36,676 2.84 303,012 1.0514 Tour 1,588 2.33 17,296 1.34 367,226 1.2815 Total 68,093 100.00 1,293,620 100.00 28,798,247 100.00Total 68,093 100.00 1,293,620 100.00 28,798,247 100.00Table 2: Domains of a subset of the documents from ?China Tibet News?.Order Domain ?document (%) ?sentence (%) ?syllable (%)1 Art 92 0.35 3,021 0.45 44,727 0.432 Culture 885 3.40 109,749 16.18 980,554 9.323 Economy 78 0.30 7,749 1.14 124,101 1.184 Education 15 0.06 695 0.10 13,919 0.135 Music 323 1.24 3,169 0.47 31,791 0.306 News 24,055 92.45 519,576 76.61 8,783,626 83.507 Photo 80 0.31 2,548 0.38 35,982 0.348 Policy 116 0.45 7,062 1.04 121,930 1.169 Politics 124 0.48 7,668 1.13 137,538 1.3110 Tibetan Medicine 107 0.41 11,417 1.68 162,557 1.5511 Tour 145 0.56 5,563 0.82 82,443 0.78Total 26,020 100.00 678,217 100.00 10,519,168 100.00Table 3: Domains of a subset of the documents from ?Tibetan?s web of China?.abbreviated syllables.
So Tibetan has a significant number of complex words where the sounds havebeen synthesized due to internal sandhi something like Sanskrit.
As some of those abbreviated syllablescan also be used as normal syllables, they lead to considerable problem in Tibetan word segmentation.So in the first step, we analyse the structure of each syllable in the sentence, and break them into normalsyllables and abbreviated mark candidates and take them as the basic units (unbreakable units).
Then, inthe second step, some special case-auxiliary words ( which are all monosyllable words ) are used as sepa-rators to break the sentence into blocks.
Consequently, both the forward maximum matching method andbackward maximum matching method are used to segment each block into words.
Mean while, it detectsambiguities by bidirectional segmentation, and makes disambiguation with word frequency.
A previousresearch shows that the precision of this method reaches 96.98% (Liu et al., 2012a).
The followingexample shows the main procedure of the method.Input: ???????????????????????????????????????????????????????????????????????????
?Translation: We have always followed the principles of socialist public ownership and distributionaccording to work.Step 1: ??
(?
??)
??
????
???
????
??
??
??
????
(?
??)
???
????
???
???
????
???
???
??
??
???
????
????????
??
?Step 2: (??
?
??
??
????
???
????)
??
(??
??
????
?
??
???
????
???
???
????
???
???)
??
(??
???
?????????
???
??)
?Step 3: (??
?)
(??)
(??
????
???
????)
??
(??
??
????
?
??
???)
(????)
(???)
(???)
(????)
(???)
(???)
??(??
???)
(????
?????)
(???
??)
?Output: ???
??
?????????????
??
??????????????
????
???
???
????
???
???
??
?????
?????????
?????
?326Note that, in step 1, two abbreviated syllable candidates are found and given in parentheses.
In step 2,the two occurrences of the case-auxiliary word ??
break the sentence into several blocks, and each blockis segmented into words consequently in step 3.
In this paper, as we mainly focus on Tibetan text, so inthe segmentation, all Latin words, Latin numbers, Chinese phrases, Tibetan alphabetic numbers such as?
???????
?
(6331089) and so on are all replaced by place-holders.4.3 Counting and CalculationThe SRI Language Modelling Toolkit (SRILM) (Stolcke and others, 2002) is used to count the frequen-cies in our work.5 Statistical Data and AnalysisIn this section, we show the statistical data and check whether the frequency-rank on the units of supercharacter, syllable and word follows Zipf?s law respectively.
As there isn?t many enough items in thefrequency list, we won?t check it on the units of letter and character.
For the other units, the number ofoccurrences of each n-gram is listed in Table 4.unit super char syllable wordunigram 265,197,659 93,657,584 67,209,927bigram 260,565,029 89,024,954 62,577,297trigram 256,022,772 84,521,746 58,085,9304-gram 251,638,774 80,194,075 54,051,8775-gram 247,285,278 76,155,897 50,242,999Total 1,280,709,512 423,554,256 292,168,030Table 4: Number of occurrences of each Tibetan n-gram in different units in the corpus.5.1 Letter FrequencyIn total, Tibetan 83 letters are used in the corpus, of which 39 letters are consonants and 8 letters arevowel signs.
Letter ?
and ?
didn?t occur in the corpus, which means people might prefer to use twoletters to spell each of them.
The other are Tibetan punctuations and signs.
There are also 200 nonTibetan characters used in the corpus.
The 47 letters and the two delimiters are listed in Table 5.
Thecharacter ?P?, ?C?
and ?V?
in the table denote ?Punctuation?, ?Consonant?
and ?Vowel?
respectively.The ?theg?
shares 23.45% of the corpus while the ?thed?
shares 1.33%, which shows that a syllable has3.26 letters (not including the ?theg?
itself), while a sentence has 75.40 letters in average.
4 of the 8vowels occur frequently while the other 4 vowels are rarely used.
The 2 punctuations share 24.7762% ofthe corpus while 4 vowels in modern Tibetan share 16.0805%, and the 30 consonants in modern Tibetanshare 58.3918%.
All these 36 letters share 99.2485% of the corpus in total.
The other 4 vowels and 9consonants which is used to transliterate Sanskrit script are rarely used.
They share only 0.0437%.
OtherTibetan signs and non Tibetan characters share 0.7078%.5.2 Character FrequencyThere are 119 Tibetan characters used in the corpus in total, including Tibetan punctuations and signs,but Tibetan number is replaced with a place-holder.
As there are 83 letters as described in the formersubsection, the other 36 characters are the second or third forms of Tibetan consonants.
As the frequencyof Tibetan character is seldom a concerned issue, we don?t make any further remarks on it.5.3 Super Character FrequencyThere are 1,466 super characters used in the corpus in total.
The topmost frequently occurred supercharacters and n-gram super char phrases for n = 2, 3 are listed in Table 6.
As expected, the ?theg?
isthe most frequently occurred one when we take it as a super character, which shares 31.22%.
It indicatesthat a syllable is formed by 2.20 super characters in average.
The ?theg?
shares 1.5837%, which indicatesthat a sentence has 63.14 super characters in average.327?
letter ?
occur rate(%) cum.rate(%) ?
letter ?
occur rate(%) cum.rate(%)P01 ?
82,818,775 23.4500 23.4500 C20 ?
1,788,360 0.5064 96.3522C01 ?
25,967,545 7.3527 30.8027 C21 ?
1,688,121 0.4780 96.8302C02 ?
21,589,015 6.1129 36.9155 C22 ?
1,442,603 0.4085 97.2386C03 ?
18,211,934 5.1567 42.0722 C23 ?
1,193,175 0.3378 97.5765V01 ?
17,755,843 5.0275 47.0998 C24 ?
1,110,743 0.3145 97.8910V02 ?
17,663,843 5.0015 52.1012 C25 ?
1,106,366 0.3133 98.2043C04 ?
16,796,414 4.7559 56.8571 C26 ?
1,074,769 0.3043 98.5086C05 ?
14,320,083 4.0547 60.9118 C27 ?
962,770 0.2726 98.7812C06 ?
14,311,260 4.0522 64.9640 C28 ?
932,081 0.2639 99.0451C07 ?
13,969,128 3.9553 68.9194 C29 ?
411,338 0.1165 99.1616V03 ?
12,067,901 3.4170 72.3364 C30 ?
306,863 0.0869 99.2485C08 ?
11,706,716 3.3147 75.6511 V05 ?
112,528 0.0319 99.2803C09 ?
10,652,623 3.0163 78.6674 C31 ?
13,398 0.0038 99.2841C10 ?
10,252,221 2.9029 81.5703 C32 ?
11,482 0.0033 99.2874V04 ?
9,304,303 2.6345 84.2048 C33 ?
7,979 0.0023 99.2896C11 ?
8,003,478 2.2662 86.4709 C34 ?
6,617 0.0019 99.2915C12 ?
6,484,634 1.8361 88.3070 C35 ?
945 0.0003 99.2918C13 ?
4,788,018 1.3557 89.6628 V06 ?
689 0.0002 99.2920P02 ?
4,683,745 1.3262 90.9890 V07 ?
288 0.0001 99.2920C14 ?
3,621,446 1.0254 92.0144 C36 ?
159 0.0000 99.2921C15 ?
3,220,311 0.9118 92.9262 C37 ?
128 0.0000 99.2921C16 ?
3,082,972 0.8729 93.7991 V08 ?
64 0.0000 99.2921C17 ?
2,696,958 0.7636 94.5628 C38 ?
55 0.0000 99.2922C18 ?
2,307,810 0.6535 95.2162 C39 ?
38 0.0000 99.2922C19 ?
2,223,533 0.6296 95.8458 Total 353,171,951 100.00Table 5: Frequency of Tibetan letters used in the corpus.?
Unigram ?occur rate(%) Bigram ?occur rate(%) Trigram ?occur rate(%)1 ?
82,794,773 31.2200 ?
?
15,628,059 5.9978 ?
?
?
3,507,130 1.36992 ?
17,136,507 6.4618 ?
?
11,178,538 4.2901 ?
?
?
2,344,655 0.91583 ?
13,283,592 5.0089 ?
?
7,913,908 3.0372 ?
?
?
2,311,335 0.90284 ?
12,677,652 4.7805 ?
?
6,137,354 2.3554 ?
?
?
1,829,894 0.71475 ?
11,999,418 4.5247 ?
?
6,108,586 2.3444 ?
?
?
1,699,226 0.66376 ?
10,582,545 3.9904 ?
?
4,972,884 1.9085 ?
?
?
1,353,391 0.52867 ?
9,387,744 3.5399 ?
?
4,892,373 1.8776 ?
?
?
1,284,801 0.50188 ?
6,335,567 2.3890 ?
?
4,705,897 1.8060 ?
?
?
1,282,074 0.50089 ?
5,899,668 2.2246 ?
?
4,596,516 1.7641 ?
?
?
1,251,081 0.488710 ?
5,769,971 2.1757 ?
?
4,268,149 1.6380 ?
?
?
1,244,612 0.4861Total 175,867,437 66.3156 Total 70,402,264 27.0191 Total 18,108,199 7.0729Table 6: The topmost frequently occurred super characters and n-gram super char phrases.The frequency-rank curves of the n-gram for n = 1, 2, 3, 4, 5 are plotted with log-log axes in Figure5.
A straight line with slop = ?1.0 is also plotted in the figure.
It?s obvious that the curves don?t followZipf?s law so exactly.
The high frequency parts of the curves follow Zipf?s law at large, but as the rankincreases the curves have more rapid decreases than a linear curve with slop = ?1.0 when the rank> 100.
However, we still found that the curve becomes more straight when the n increases.Similar to Ha et al.
(2002), we also combine the frequency list of the n-grams for all n = 1, 2, .
.
.
, 5together in one list and put in order of frequency.
The frequency-rank curve is plotted in Figure 6.
Astraight line with slope= ?1.0 is also plotted in the figure, which shows that there are large gaps between328	     	  	 !"Figure 5: Frequency-rank of super chars and theirn-grams.	     	  Figure 6: Frequency-rank of combined super charn-gram list.	    	    	 !"Figure 7: Frequency-rank of syllables and syllablen-grams.	    	    Figure 8: Frequency-rank of combined syllable n-gram list.	    	    	 !"#Figure 9: Frequency-rank of words and word n-grams.	    	    	Figure 10: Frequency-rank of combined word n-gram list.the curve and the line.
Obviously it doesn?t follow Zipf?s law well.5.4 Syllable FrequencyThere are 27,546 syllables and 200 other characters occurred in the corpus in total.
The topmost fre-quently occurred syllables and n-gram syllable phrases for n = 2, 3 are listed in Table 7.
As expected,the ?thed?
is the most frequently used unigram when we take is as a syllable.
It shares 4.4843% of thecorpus.
Most of the top 15 unigrams are case auxiliary words (monosyllable word), including ??
, ??
,???
, ???
, ???
and ??
.
The conjunction ???
, the two nominalization markers ??
and ??
are also in the top10 list.
The top 10 syllables take up 16.5556% of the corpus.The frequency-rank curves of the n-gram for n = 1, 2, 3, 4, 5 are plotted with log-log axes in Figure 7.A straight line with slop = ?1.0 is also plotted in the figure , which shows that the curves don?t followZipf?s law very exactly.
The high frequency parts of the curves when n = 1, 2 follow Zipf?s law at large,but as the rank increases the curves have more rapid decreases than a linear curve with slop = ?1.0when the rank > 1000 and the rank > 10000 respectively.
The curve becomes more straight when the nincreases, and becomes almost straight lines when n = 3, 4, 5.We also combine the frequency list of the n-grams for all n = 1, 2, .
.
.
, 5 together in one list and329?
Unigram ?occur rate(%) Bigram ?occur rate(%) Trigram ?occur rate(%)1 ?
4,199,896 4.4843 ???
?
593,668 0.6669 ??
???
?
237,068 0.28052 ???
2,370,981 2.5315 ??
???
537,089 0.6033 ??
???
?
219,016 0.25913 ??
2,233,002 2.3842 ??
?
367,725 0.4131 ???
???
????
104,505 0.12364 ??
1,377,206 1.4705 ???
?
345,059 0.3876 ???
??
???
92,475 0.10945 ???
1,319,052 1.4084 ???
?
241,152 0.2709 ???
??
???
86,964 0.10296 ??
1,023,287 1.0926 ??
???
222,771 0.2502 ??
???
?
82,532 0.09767 ??
1,008,926 1.0772 ??
???
209,657 0.2355 ???
??
?
58,133 0.06888 ???
1,007,539 1.0758 ???
??
209,095 0.2349 ???
??
???
56,153 0.06649 ???
965,728 1.0311 ???
??
205,235 0.2305 ??
???
???
55,173 0.065310 ???
915,663 0.9777 ??
?
198,682 0.2232 ???
???
???
54,168 0.0641Total 15,505,617 16.5556 Total 2,931,451 3.2928 Total 992,019 1.1737Table 7: The topmost frequently occurred syllables and n-gram syllable phrases.put in order of frequency.
The frequency-rank curve is plotted in Figure 8.
A fitting straight line y =2?
107?x?0.963with R2= 0.9985 is also plotted in the figure, which shows that the curve can be wellfitted by the line.
Thus, it follows Zipf?s law.5.5 Word Frequency?
Unigram ?occur rate(%) Bigram ?occur rate(%) Trigram ?occur rate(%)1 ?
4,199,896 6.2489 ???
?
593,286 0.8258 ?????
???
?
83,110 0.12372 ??
3,580,891 5.3279 ???
?
319,479 0.4447 ??
??
??
36,087 0.05373 ???
2,241,125 3.3345 ???
?
232,155 0.3231 ?????
???
?
33,574 0.05004 ??
1,357,874 2.0203 ??
??
163,093 0.2270 ?????
???
?
28,562 0.04255 ??
982,161 1.4613 ??
??
145,563 0.2026 ???
??
?
28,387 0.04226 ??
977,484 1.4544 ?????
??
132,517 0.1845 ??????
?????????
?
27,707 0.04127 ???
949,750 1.4131 ???
??
131,176 0.1826 ?????
??
?
26,523 0.03958 ???
863,061 1.2841 ???
??
116,202 0.1617 ?????
???
?
25,829 0.03849 ??
754,281 1.1223 ???
???
110,635 0.1540 ????
????
??
25,418 0.037810 ??
654,987 0.9745 ?????
??
96,640 0.1345 ???
???
?
24,786 0.0369Total 16,561,510 24.6415 Total 2,040,746 2.8406 Total 339,983 0.5058Table 8: The topmost frequently occurred words and n-gram word phrases.There are 96,296 words( including Tibetan punctuations, signs) used in the corpus in total.
The top-most frequently occurred words and n-gram word phrases for n = 2, 3 are listed in Table 8.
As expected,the ?thed?
is the most frequently used unigram when we take is as a word.
It shares 6.2489% of thecorpus.
Almost all of the top 10 unigrams are auxiliary case words (monosyllable word), including ??
,??
, ??
, ??
, ???
, ???
, ??
and ??
.
The top 10 words take up 24.6415% of the corpus.The frequency-rank curves of the n-gram for n = 1, 2, 3, 4, 5 are plotted with log-log axes in Figure 9.A straight line with slop = ?1.0 is also plotted in the figure , which shows that the curves don?t followZipf?s law very exactly.
The high frequency part of the curve when n = 1 follows Zipf?s law at large,but as the rank increases the curve has more rapid decreases than a linear curve with slop = ?1.0 whenthe rank > 1000.
The curve becomes more straight when the n increases, and becomes almost straightlines when n = 3, 4, 5.We also combine the frequency list of the n-grams for all n = 1, 2, .
.
.
, 5 together in one list andput in order of frequency.
The frequency-rank curve is plotted in Figure 10.
A fitting straight line330y = 2?
107?
x?0.934with R2= 0.9966 is also plotted in the figure, which shows that the curve can bewell fitted by the line.
Thus, it follows Zipf?s law.5.6 Further Discussion	    	     !"
  #$ !"
  #$ !"
  #$ !"
  #$ !"
  #$ !"
  #$$%&&'(%)$%&&'(%)Figure 11: Frequency-rank of combined Tibetan word n-gram lists for n <= 5, 6, 7, 8, 9, 10.Comparing the curves in Figure 5, 7 and 9, we find that the curves with the same n for all n =1, 2, 3, 4, 5 become more straight when the granularity becomes larger.
It?s similar in the combined n-gram curves in Figure 6, 8 and 10.
As it?s shown that the two combined n-gram frequency lists for alln <= 5 on syllable and word follow Zipf?s law well.
So, the question is that whether we can find a largerM , which for the combined n-gram list for all all n < m, the frequency-rank curve in log-log axes isstraight enough.
To find the M , the frequency-rank curves for the combined n-gram super character listsfor m = 5, 6, 7, 8, 9, 10 are plotted respectively in Figure 11.
From the figure, we see that the head partsof the curves are overlapped, which correspond to the high frequency parts of the combined n-gram lists,while the tail parts of the curves are divergent.
As the m increases, the tail part of the curve becomescloser to the straight line y = 3.8 ?
107?
x?0.81.
This mainly results from that the frequency of then-gram decreases when the n increase, and the low frequency part of the combined n-gram list includesmore n-grams.
However, the two straight lines of y = 3.8 ?
107?
x?0.81and y = 2.3 ?
109?
x?1.18in the figure show that any one of those curves can?t be fitted well by a straight line.
The reason leadingto this somewhat unusual result is an issue to be made further research and analysis.6 ConclusionIn the former section, we make statistics on different Tibetan language units : letter, super character,syllable and word, and their n-gram phrases.
It shows that when we put all the n-gram phrases forn = 1, 2, .
.
.
, 5 together and sort all of them by frequency in descending order, then the frequency-rankcurves in log-log axes can be fitted well for the unit of syllable and word respectively.
But for the unitof super character, we didn?t find a curve which can be fitted well enough by a straight line when wecombine all the n-grams for n <= m even if m is up to 10.AcknowledgementsWe thank the reviewers for their critical and constructive comments and suggestions that helpedus improve the quality of the paper.
The research is partially supported by National Scienceand Technology Major Project (No.2012ZX01039-004), National Science Foundation (No.61202219,No.61202220, No.61303165), Major Science and Technology Projects in Press and Publishing(No.0610-1041BJNF2328/23, No.0610-1041BJNF2328/26), and Informationization Project of the Chi-nese Academy of Sciences (No.XXH12504-1-10).ReferencesLada A. Adamic and Bernardo A. Huberman.
2002.
Zipfs law and the internet.
Glottometrics, 3(1):143?150.331Jinyong Ai, Hongzhi Yu, and Yonghong Li.
2009.
Statistical analysis on tibetan shaped structure.
Journal ofComputer Applications, 29(7):2029?2031.MG Boroda and AA Polikarpov.
1988.
The zipf-mandelbrot law and units of different text levels.
Musikometrika,1:127?158.Lee Breslau, Pei Cao, Li Fan, Graham Phillips, and Scott Shenker.
1998.
On the implications of zipfs law for webcaching.
Technical report, Citeseer.CA Chatzidimitriou-Dreismann, RMF Streffer, and Dan Larhammar.
1996.
Lack of biological significance in thelinguistic features of noncoding dnaa quantitative analysis.
Nucleic acids research, 24(9):1676?1681.Yuzhong Chen, Baoli Li, and Shiwen Yu.
2003a.
The design and implementation of a tibetan word segmentationsystem.
Journal of Chinese Information Processing, 17(3):15?20.Yuzhong Chen, Baoli Li, Shiwen Yu, and Lancuoji.
2003b.
An automatic tibetan segmentation scheme based oncase auxiliary words and continuous features.
Applied Linguistics, 2003(01):75?82.Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ Newman.
2009.
Power-law distributions in empirical data.SIAM review, 51(4):661?703.The Unicode Consortium.
2013.
The Unicode Standard, Version 6.3.0.
The Unicode Consortium, ISBN 978-1-936213-08-5, Mountain View, CA.Leo Egghe.
1999.
On the law of zipf-mandelbrot for multi-world phrases.Leo Egghe.
2000.
The distribution of n-grams.
Scientometrics, 47(2):237?252.Xavier Gabaix.
1999a.
Zipf?s law and the growth of cities.
The American Economic Review, 89(2):129?132.Xavier Gabaix.
1999b.
Zipf?s law for cities: an explanation.
The Quarterly Journal of Economics, 114(3):739?767.Dingguo Gao and Yuchang Gong.
2005.
A statistically study on the qualities of all modern tibetan character set.Journal of Chinese Information Processing, 19(1):71?75.Le Quan Ha, Elvira I Sicilia-Garcia, Ji Ming, and F Jack Smith.
2002.
Extension of zipf?s law to words andphrases.
In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages1?6.
Association for Computational Linguistics.Yannis M Ioannides and Henry G Overman.
2003.
Zipfs law for cities: an empirical examination.
Regionalscience and urban economics, 33(2):127?137.Di Jiang and Yinghong Dong.
1994.
Statistical analysis on linear processing of tibetan clustered structures.Chinese Information Processing, (4):44?46.Di Jiang and Yinghong Dong.
1995.
Research on property of tibetan characters as information processing.
Journalof Chinese Information Processing, 9(2):37?44.Di Jiang and Jiangping Kong.
2006.
Advances on the Minority Language Processing of China.
Social SciencesAcademic Press, Beijing, China.Di Jiang and Congjun Long.
2010.
On Characters of Tibetan Writing System: Alpabetic Characters, Pronunci-ations, ISO Codes, Frequencies, Sorting Orders, Picture Symbols and Transliterations.
Social Sciences Aca-demic Press, Beijing, China.Di Jiang.
1998.
An entropy value of classical tibetan language and some other questions.
In Proceedings ofInternational Conference on Chinese Information Processing, pages 377?381.
Chinese Information ProcessingSociety of China.Di Jiang.
2006.
History and development of tibetan text information processing.
In Frontiers of Chinese Informa-tion Processing - Proceedings of the 25th Anniversary Conference of Chinese Information Processing Societyof China, pages 83?97.
Tsinghua University Press, Beijing, China.Wentian Li.
1992.
Random texts exhibit zipf?s-law-like word frequency distribution.
IEEE Transactions onInformation Theory, 38(6):1842?1845.332Huidan Liu, Minghua Nuo, Longlong Ma, and et al.
2011.
Tibetan Word Segmentation as Syllable Tagging UsingConditional Random Fields.
In Proceedings of the 25th Pacific Asia Conference on Language, Information andComputation (PACLIC 2011), pages 168?177.Huidan Liu, Minghua Nuo, Longlong Ma, and et al.
2012a.
SegT: A pragmatic tibetan word segmentation system.Journal of Chinese Information Processing, 26(1):97?103.Huidan Liu, Minghua Nuo, Jian Wu, and Yeping He.
2012b.
Building large scale text corpus for tibetan byextracting text from web pages.
In Proceedings of the 10th asian language resources at COLING 2012, pages8?17.Yajun Lu and Xiaodong Shi.
2011.
Random texts exhibit zipf?s-law-like word frequency distribution.
Journal ofChinese Information Processing, 25(4):54?56.Yajun Lu, Shaoping Ma, Min Zhang, and Guang Luo.
2003.
Researches of calculations of tibetan characters,pieces, syllables, vocabulary and universal frequency and its applications.
Journal of Northwest MinoritiesUniversity(Natural Science), 24(48):32?42.R.
N. Mantegna, S. V. Buldyrev, A. L. Goldberger, S. Havlin, C. K. Peng, M. Simons, and H. E. Stanley.
1994.Linguistic features of noncoding dna sequences.
Phys.
Rev.
Lett., 73:3169?3172, Dec.AIDA Masaki and Noriyuki Takahashi.
1998.
A proposal of dual zipfian model for describing http access trendsand its application to address cache design.
IEICE transactions on communications, 81(7):1475?1485.Marcelo A Montemurro and Damian H Zanette.
2002.
New perspectives on zipfs law in linguistics: from singletexts to large corpora.
Glottometrics, 4:87?99.S Naranan and VK Balasubrahmanyan.
1998.
Models for power law relations in linguistics and informationscience.
Journal of Quantitative Linguistics, 5(1-2):35?61.Kazumi Okuyama, Misako Takayasu, and Hideki Takayasu.
1999.
Zipf?s law in income distribution of companies.Physica A: Statistical Mechanics and its Applications, 269(1):125?131.Ronald Rousseau.
2002.
George kingsley zipf: life, ideas, his law and informetrics.
Glottometrics, 3:11?18.Herbert A Simon.
1955.
On a class of skew distribution functions.
Biometrika, 42(3/4):425?440.Kwok Tong Soo.
2005.
Zipf?s law for cities: a cross-country investigation.
Regional science and urban Eco-nomics, 35(3):239?263.Kwok Tong Soo.
2007.
Zipf?s law and urban growth in malaysia.
Urban Studies, 44(1):1?14.Andreas Stolcke et al.
2002.
Srilm-an extensible language modeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing, volume 2, pages 901?904.
Denver.Yuan Sun, Luosangqiangba, Rui Yang, and Xiaobing Zhao.
2009.
Design of a tibetan automatic segmentationscheme.
In the 12th Symposium on Chinese Minority Information Processing.Yuan Sun, Xiaodong Yan, , Xiaobing Zhao, and Guosheng Yang.
2010.
A resolution of overlapping ambiguityin tibetan word segmentation.
In Proceedings of the 3rd International Conference on Computer Science andInformation Technology, pages 222?225.Weilan Wang and Wanjun Chen.
2004.
The frequency and information entropy of tibetan character and syllabie.Terminology Standardization and Information Technology, (2):27?31.Weilan Wang.
2004.
The frequency-rank of language unit in modern tibetan.
Science Technology and Engineer-ing, 4(5):413?417.Dami?an Zanette and Marcelo Montemurro.
2005.
Dynamics of text generation with realistic zipf?s distribution.Journal of quantitative Linguistics, 12(1):29?40.Dami?aan H Zanette.
2006.
Zipf?s law and the creation of musical context.
Musicae Scientiae, 10(1):3?18.George Kingsley Zipf.
1935.
The psycho-biology of language.George Kingsley Zipf.
1949.
Human behavior and the principle of least effort.333
