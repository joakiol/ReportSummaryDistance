Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 476?485, Dublin, Ireland, August 23-29 2014.Measuring Lexical Cohesion: Beyond Word RepetitionAnna Kazantseva & Stan SzpakowiczSchool of Electrical Engineering and Computer ScienceUniversity of OttawaOttawa, Ontario, Canada{ankazant,szpak}@eecs.uottawa.caAbstractThis paper considers the problem of finding topical shifts in documents and in particular at whatinformation can be leveraged to identify them.
Recent research on topical segmentation usuallyassumes that topical shifts in discourse are signalled by changes in vocabulary.
This information,however, is not always a sufficient indicator of a topical shift, especially for certain genres.
Thispaper explores an additional source of information.
Our hypothesis is that the type of a referringexpression is an indicator of how accessible its antecedent is.
The shorter and less informativethe expression (e.g., a personal pronoun versus a lengthy post-modified noun phrase), the moreaccessible the antecedent is likely to be and the more likely it is that the topic under discussion hasremained constant between the two mentions.
We explore how this information can be used toaugment a lexically-based topical segmenter.
We test our hypothesis on two types of data, literarynarratives and lecture notes.
The results suggest that our similarity metric is useful: depending onthe settings it either slightly improves the performance or leaves it unchanged.
They also suggestthat certain types of referring expressions are more useful than others.1 IntroductionIn the past 10 years, research on topical segmentation has mostly centred on using surface vocabulary toidentify topical shifts.
The intuition is that if the vocabulary changes perceptibly, so does the topic underdiscussion.
One popular way to model this assumption is by probabilistic graphical models.
A documentmay be modelled as a sequence of strings (e.g., sentences) generated by a latent topic variable, wherethe topic variables correspond to distributions over a finite vocabulary.
Similarity-based methods arean alternative methodology.
The segmenter explicitly measures the amount of lexical similarity betweensentences.
Places where similarity is low are likely to indicate shifts of topic.
The common thread amongthese approaches is that they rely almost exclusively on the explicitly mentioned words.The idea that vocabulary shifts indicate topical shifts dates back to Youmans (1991).
Indeed, by andlarge, introducing new concepts almost necessarily requires that the concepts be named and described.How densely the concepts are explicitly mentioned and how often the mentions are repeated dependsto a large degree on the genre and on the cognitive complexity of the document.
In scientific papers orlegal documents clarity is paramount, so the author will endeavour to state things explicitly and avoidambiguity.
The less complicated the document, however, the less it is necessary to explicitly repeatterminology.
In literature, for example, word repetition is not only uncommon, but it is usually a signof bad writing.
In casual conversations, the topic can easily be never mentioned explicitly.
How can weidentify topical shifts in a document whose author does not ?hold the reader?s hand?
?It turns out that lexical cohesion (or, put simply, word repetition) is only one of several devices ofcohesion (Halliday and Hasan, 1976, p. 29) Other possibilities are reference, substitution, ellipsis andconjunction.
In this paper we mainly explore referential cohesion.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/476Figure 1: An example dialogue from the Moonstone corpus?What?s wrong now??
I said once more.
?Rosanna?s late again for dinner,?
says Nancy.
?And I?m sent to fetch her in.
All the hard work falls on my shoulders inthis house.
Let me alone, Mr.
Betteredge!
?The person here mentioned as Rosanna was our second housemaid.
?Where is she??
I inquired.
[.
.
.
]?At the sands, of course!?
says Nancy, with a toss of her head.
?She had another of her fainting fits this morning, and sheasked to go out and get a breath of fresh air.
I have no patience with her!?
?Go back to your dinner, my girl,?
I said.
?I have patience with her, and I?ll fetch her in.
?Figure 1 shows a snippet of a dialogue from the publicly available Moonstone corpus (Kazantseva andSzpakowicz, 2012).
The two speakers discuss a specific person, Rosanna, yet her name is mentionedexplicitly only twice.
In the remainder of the dialogue the author uses pronouns to refer to this person,whose identity is evident from the context.
Running an automatic segmenter on such a document wouldlikely be challenging since focal concepts ?
characters ?
are often referred to by pronouns or definitenoun phrases (NPs) instead of explicit repetition.The focal entity Rosanna is introduced once and then it is referred to by nominal and pronominalanaphora, not by explicit repetition.
Simplifying things somewhat, we can say that merely by the virtueof encountering a referring expression (e.g., she or the person), we know that it refers to something thatmust be clear from the context.
The type of the referring expression also contains information about theavailability of the antecedent.
A she implies that the ?she?
in question is rather obvious, that is to say,the antecedent is nearby and, more important for our purposes, the topical thread continues.
A moreverbose referring expression (e.g., the woman in red) is more likely in situations where the antecedent isless obvious and the reader needs additional information to disambiguate the expression.The idea that the type of referring expression tells a lot about the accessibility of its antecedent datesback to Giv?on (1981).
He postulated that the more informative the referring expression is, the less acces-sible the antecedent will be.
Figure 2 shows the list of expressions from the least to the most informative.Projecting this information onto our task, we can say that the more informative the expression is, the lesscontinuity there will be in the topic.The main contribution of this work is to show how such information can be used to improve thequality of text segmentation.
We extract NPs and classify them by informativeness.
This is achievedwith the help of a syntactic parser, but a lighter form of processing might do, perhaps even if it capturedpersonal pronouns.
Using this information, we augment and correct a matrix of lexical similaritiesbetween sentences, a structure frequently used as an input to a topical segmenter.The results of using coreferential similarity are evaluated on a dataset of manually segmented chaptersfrom a novel (Kazantseva and Szpakowicz, 2012) and on transcripts of lectures in Artificial Intelligence(Malioutov and Barzilay, 2006).
We try the new similarity matrix on two publicly available similarity-based segmenters APS (Kazantseva and Szpakowicz, 2011) and MinCutSeg (Malioutov and Barzilay,2006).
The results suggest that the new matrix never hurts, and in several case improves, the performanceof the segmenter, especially for the novel.
We also check whether this metric would still be useful ifinstead of the traditionally used lexical similarity we used a similarity metric which took synonymy intoaccount.
In this case, the margin of improvement is lower, but still the coreferential similarity metricnever hurts the performance and often improves it.Section 2 of the paper gives an overview of related work.
Section 3 describes our similarity metric andhow we compute it.
Section 4 shows the details of the experiments, while Section 5 discusses the results.We conclude in Section 6 with a discussion of how our metric can be improved and simplified.2 Background and related workMuch of research on topical segmentation of text is based on the idea that changes of topic are usuallyaccompanied by vocabulary changes.
Introduced by Youmans (1991), it has since formed the backbone477of research on topical segmentation.
We now briefly review recent work on text segmentation.
Sincethe focus of this research is on what information is useful for text segmentation, this review emphasizesrepresentations rather than algorithms.Perhaps the simplest way of estimating topical similarity between sentences is to measure cosine sim-ilarity between corresponding feature vectors.
It has been used extensively in text segmentation.
Hearst(1994; 1997) describes TextTiling, an algorithm which identifies topical shifts by sliding a windowthrough the document and measures cosine similarity between adjacent windows.
The drops in simi-larity signal shifts of topic.
More recently, Malioutov and Barzilay (2006) as well as Kazantseva andSzpakowicz (2011) use graph cuts and factor graph clustering for text segmentation.
Both systems relyon cosine similarity between bag-of-word vectors as an underlying representation.While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topicalsimilarity.
Several researchers have used lexical chains ?
first introduced by Halliday and Hasan (1976)?
to improve the performance of topical segmenters.1The intuition behind using lexical chains for textsegmentation is that the beginning and end of a chain tend to correspond to the beginning and end ofa topically cohesive segment.
One version of TextTiling (Hearst, 1997) uses lexical chains manuallyconstructed using Roget?s Thesaurus.
Okumura and Honda (1994) apply automatically created lexicalchains to segment a small set of documents in Japanese.
More recently, Marathe (2010) tried to buildlexical chains using distributional semantics and apply the method to text segmentation.Other proposals to move beyond word repetition in topical segmentation include the use of bigramoverlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauerand Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010).It should be noted that much of the recent work on topical segmentation revolves around generativemodels.
For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misraet al.
(2011) and Du et al.
(2013) use higher-order models.
We do not review this work in detail herebecause it centers on algorithms for text segmentation and not on the information supplied to those algo-rithms, which is the focus of this research.
Fundamentally, the text is modelled as a sequence of tokensgenerated by latent topic variables.
Although probabilistic segmenters can be extended to use addi-tional information (e.g., Eisenstein and Barzilay (2008) augment their segmenter with information aboutdiscourse markers), it is not trivial to change these models to include information such as synonymy,co-reference and so on.
That is why we do not review them in detail here.As this brief review shows, a number of approaches have been proposed to measure cohesion betweensentences, that is to say, to describe to what extent a pair of sentences is ?about the same thing?.
Mostof them have a common denominator: they use explicit lexical information, sometimes augmented bysemantic relations from thesauri or ontologies.Lexical resources, such as ontologies and knowledge-bases, may help improve the quality of segmen-tations, but such resources are not always available.
They also may cause problems with precision.
Moreimportant, however, they do not solve a more fundamental problem: a text may be highly cohesive andcoherent without being tightly bound by either lexical cohesion or synonymy.The main ideas developed in this work originate in (Giv?on, 1981).
The author looks at the functionaldomain of topical accessibility.
A number of coding devices affect this property.
They are listed inFigure 2, ordered from the devices used to mark the most continuous topics to those which mark the leastcontinuous topics.
The order in Figure 2 is governed by a simple principle: the more accessible the topicis, the less information is used to code it.
The author argues that the continuum is applicable in manylanguages.
He also mentions that while the exact values of the phenomenon in question are difficult topredict or even estimate, their relative order can be predicted with certainty, even if some devices areunavailable in some languages.In a similar spirit, Ariel (2014) groups non-initial NPs into expressions with low accessibility (definiteNPs and proper names), those with intermediate accessibility (personal and demonstrative pronouns) andthose with high accessibility (pronouns).In this work, we propose to leverage the presence and type of co-referential relations to improve1Very simply put, a lexical chain is a sequence of related words in a text.478the results of two recent similarity-based segmenters.
Instead of resolving anaphoric references, weassume that their mere presence often indicates topic continuity.
With this augmented model, we segmentfiction and spoken lecture transcripts, the two types of data where low rates of lexical cohesion precludeachieving segmentation of good quality using only surface information about token types.3 Estimating coreferential similarityIn order to see whether knowledge about types of referential expressions is useful for measuring topi-cal similarity, we incorporate this information into two publicly available similarity-based topical seg-menters, MCSeg (Malioutov and Barzilay, 2006) and APS (Kazantseva and Szpakowicz, 2011).
Nor-mally, both MCSeg and APS measure similarity between sentences by computing cosine similarity be-tween the vectors corresponding to bag-of-words representation for each sentence:sim(s1, s2) =s1?
s2||s1|| ?
||s2||(1)Each atomic unit of text is represented as a vector of features corresponding the occurrences of eachtoken type.
The vectors are weighted using tf.idf values for each token type.
Next, a segmenter measurescosine similarity between vectors according to Equation 1.
That is the fundamental representation in bothMCSeg and APS.
MCSeg identifies segment boundaries by creating a weighted cyclic graph and cuttingit so as to maximize the sum of edges within segments and to minimize the sum of severed edges.
APSsegments the sequence by finding segment centres ?
points which best capture the content of a segment?
and assigning data points to best segment centres so as to maximize net similarity.The proposed similarity metric relies on the following idea: in order to measure how many conceptstwo sentences share, we do not need to resolve anaphoric expressions in full, but only to map them ontosentences which contain their most recent antecedent (without actually naming the antecedents).
We dothat by parsing the documents with the Connexor parser (Tapanainen and J?arvinen, 1997) and extractingall NPs with their constituents.
Next, we attempt to classify the NPs into categories which would roughlycorrespond to those listed in Figure 2 and to those in (Ariel, 2014).A manual study by Brown (1983) suggests that the average referential distance for animate and inan-imate entities differs widely within the same document.2That is why it makes sense to distinguishbetween these two types.
In the end, then, we classify each identified NP into one of the categories listedin Figure 3.
The list is not exhaustive and in some cases an NP may belong to more than one type.
Inpractice, however, an NP is always assigned a single type dictated by the implementation.2Brown (1983, pp.
323-324) compares referring expressions which denote human and non-human entities.
She uses threemeasurements: average distance to the nearest antecedent, average ambiguity and persistence.
On all three counts, human andnon-human entities appear to have different distributions.Figure 2: Linguistic coding devices which signal topic accessibility (Giv?on, 1981)Most continuous (least surprising)1. zero anaphora2.
unstressed pronouns (e.g., He was speaking loudly.)3.
right-dislocated definite noun phrases (NPs) (e.g., It is no good, that book.)4.
neutral-ordered definite NPs (e.g., That book is no good.)5.
left-dislocated definite NPs (e.g., That book, it is no good.)6.
Y-moved NP?s (e.g., The book they read in turns.)7.
cleft/focus constructions (e.g., It was that book, that was on her mind for weeks.)8.
referential indefinite NPs (e.g., He picked up a book and left.
)Least continuous (most surprising)479Figure 3: Categories of noun phrases taken into account when computing coreferential similarity1.
animate personal pronouns (he, she, they)2. inanimate pronouns (it)3. demonstrative pronouns (that, those)4. animate proper names (John Hernecastle)5. inanimate proper names (London)6. animate definite noun phrases (the man)7. inanimate definite noun phrases (the jewel)8. animate indefinite noun phrases (a man)9. inanimate indefinite noun phrases (a jewel)Finally, coreferential similarity between sentences Siand Sjis measured as follows:coref sim(Si, Sj) = (?|T |t=0countSjt?
weightt|S1| ?
|S2|)(j?i?1)?decayFactor(2)T is the set of of all types of referring expressions which we consider ?
those given in Figure 3.countSjtis the number of times when an expression of type t appears in the most recent sentence, Sj.Note that we only consider the referring expressions in the most recent sentence, because a referringexpression, by its nature, must refer to something previously mentioned.
The ?tightness?
of the link iscontrolled by setting weighttfor each expression type t. weightteffectively specifies how likely it isthat the antecedent for an expression of a type t appears in sentence si.
The values of the weights areset experimentally on the holdout data.
They can almost certainly be further fine-tuned.
Intuitively, thesettings of the weights reflect the logic behind Giv?on?s theory.
Consider an example vector of weightsfor expressions, where a higher weight corresponds to a more accessible antecedent (for animate andinanimate entities respectively).<personal pronouns anim: 4, demonstr pronouns anim: 2, proper names anim: 1,def np anim: 0.5, indef np anim: 0, pronouns inanim: 2, demonstr pronouns inanim: 2,proper names inanim: 0, def np inanim: 0, indef np inanim: 0>The denominator of Equation 2 normalizes the value by the product of the lengths of sentences S1andS2.
The exponent (j ?
i ?
1) ?
decayFactor is responsible for decreasing similarity as the distancebetween sentence Siand Sjincreases.
The decay factor, 0 < decayFactor < 1, is set experimentally,and j ?
i is the distance between sentences Siand Sj, i < j.Figure 4 contains a walk-through example of computing referential similarity between two sentences.The coreferential similarity as defined by Equation 2 is rather limited.
The first limitation is the range:it can only measure similarity between nearby sentences or paragraphs, because it only makes sensebetween the closest occurrences of an antecedent and a subsequent referring expression.
For example,it does not make sense to measure coreferential similarity between sentences that are several paragraphsapart.
Even if they indeed talk about the same entities, the topic has most likely been re-introducedseveral times in between.
That is why we only compute coreferential similarity for sentences no morethan decayWindow sentences apart.
The value of decayWindow is usually between 2 and 6 and it isset experimentally on the holdout set for each corpus.The values of corefsim are usually quite small and the information used is rather one-sided.
Weuse it, therefore, in addition to, not instead of, lexical similarity.
In our experiments, we first computelexical similarity between sentences (or paragraphs) and then modify the lexical matrix by adding to itthe matrix of coreferential similarity.480Figure 4: An example of computing coreferential similaritycoref sim(Si, Sj) = (?|T |t=0countSjt?
weightt|S1| ?
|S2|)(j?i?1)?decayFactorS1: ?At the sands, of course!?
says Nancy, with a toss of her head.S2: ?She had another of her fainting fits this morning, and she asked to go out and get a breath of fresh air.
?Expression counts:personal pronouns anim: 2 (she, she)demonstr pronouns anim: 0proper names anim: 1def np anim: 0indef np anim: 0pronouns inanim: 0demonstr pronouns inanim: 1proper names inanim: 0def np inanim: 2 (this morning, fainting fits)indef np inanim: 1 (a breath)Weights:4210.5022000coref sim(S2, S1) =2?
4 + 1?
1 + 1?
221?
22(2?1?1)?0.5= 0.02344 Experimental resultsThe effectiveness of coreferential similarity metric has been tested in practice.
A set of experimentscompared how much the metric improves the quality of topical segmentations.
To this end, we ranAPS and MCSeg with and without adding coreferential similarity to lexical similarity, and comparedthe results.
We chose these segmenters for comparison because coreferential similarity can only benaturally incorporated into a similarity-based segmenter.Data.
In our experiments we used two publicly available datasets.
The first one is a set of lectures onArtificial Intelligence (Malioutov and Barzilay, 2006).
The dataset contains 22 documents which weremanually annotated for the presence of topical shifts.
The second dataset is the Moonstone dataset de-scribed in (Kazantseva and Szpakowicz, 2012).
It contains 20 chapters from Wilkie Collins?s novel, eachannotated by 4-6 people.
To reconcile these multiple reference annotations, we create a majority goldstandard.
It only contains segment breaks which were marked by at least 30% of the annotators.
Bothsegmenters are compared against this gold standard.
There is a fair amount of disagreement between theannotators of this dataset.
The average inter-annotator windowDiff is 0.38 (Kazantseva and Szpakow-icz, 2012, pp.
215-216), but if one takes into account near-hits, then at least 50% of the boundaries aremarked by more than two annotators.Both datasets are quite challenging.
The lecture dataset contain a lot of rather informal speech andthere is not as much lexical repetition as would be in a more formal text.
The Moonstone dataset is anexample of literary language, full of small digressions, dialogue and so on.The first dataset is annotated at the level of individual sentences.
The second dataset is annotated at thelevel of paragraphs.
We segment both datasets at the level of the gold-standard annotations (sentencesfor lectures, paragraphs for the novel).When working with paragraphs, coref sim is computed slightly differently:coref sim(pi, pj) = (?|T |t=0countpjt?
weightt|p1| ?
|p2|)(j?i?1)?decayFactor(3)In this case, countpjtrefers to the number of occurrences of expression of type t in the firstparagraphCutOff sentences of the paragraph pj, instead of the whole paragraph.
The rationale behindthis heuristic is that the referring expressions in the opening sentences of the paragraph are likely to refer481to entities from the previous paragraph, while expressions in the middle or the end of the paragraph arelikely to refer to entities introduced inside the paragraph.Segmenters and baselines.
We use two publicly available topical segmenters in our experiments:MCSeg and APS.
The default version of each segmenter computes a similarity matrix between sentencein the input document.
The values in the matrix correspond to cosine similarity (Equation 1) computedafter the removal of stop words and weighting the bag-of-word vectors by tf.idf .
The results obtainedusing these default matrices are our first baseline.In our experiments, we modify this matrix by adding to it the matrix of coreferential similarities.
Thevalues of coreferential similarities are rather small and most modifications are localized.
That is becausethe value of decayWindow is set between 2 and 6 (see Section 3).In addition to the matrices based on cosine similarity, we wanted to see if using a more intelligentmeasure of topical similarity improves the results.
We built one more flavour of similarity matrices usingthe DKPro Similarity framework (B?ar et al., 2013).
The framework contains a model of textual similaritywhich has been used by the winning system at the SemEval Textual Similarity 2012 shared evaluation.We use this model (further STS-2012) as a more competitive baseline for computing topical similarity.The STS-2012 baseline consists of a log-linear regression model trained on the SemEval 2012 train-ing data.
It combines an assortment of measures of textual similarity to come up with its judgments.The metrics include n-gram overlap, semantic similarity measures (based on both corpora and lexicalresources) and several measures of stylistic similarity.
We chose to use this relatively complicated met-ric because of its competitive performance at SemEval 2012.
The system, however, was not designedto measure topical similarity per se, especially between many sentences coming from the same sourcedocument.
By default, the STS-2012 baseline outputs values between 1 and 5.
These were normalized tobe between 0 and 1.Similarly to the experimental design with cosine similarity matrices, we try running the segmentersusing STS-2012 with and without adding coreferential similarity matrix to it.On both datasets we set the weights for various types of referential expressions using hold-out setsof two files.
When setting the weights, we were guided by the principle captured in Figure 2: personalpronouns suggest the tightest link, followed by demonstrative pronouns, proper names, and so on.It should be noted that because we had to modify the native representation of both segmenters by sup-plying a matrix computed using non-native code, we could not use the proper training scripts which comewith the segmenters.
In effect, the results are likely to be lower than they could have been.
Even so, this isacceptable for our purposes because we are interested in the improvement gained by using coreferentialsimilarity, not in obtaining the best possible segmentation via the setting of the best parameters.Processing.
We computed the underlying lexical similarity matrices using the same procedure asdescribed in (Malioutov and Barzilay, 2006; Kazantseva and Szpakowicz, 2011), but using our owncode.
In other words, we built a matrix of cosine similarities after removing stop words and weightingthe underlying vectors by tf.idf values.In order to compute coreferential similarity, all documents were parsed using the Connexor parser(Tapanainen and J?arvinen, 1997).
The parser was chosen because it produces high-quality partial parsesof long sentences often encountered in the Moonstone dataset.
We also tagged named entities and labelledNPs as animate or inanimate using the Stanford Core NLP suite.3Metrics.
We compare topical segmentations using the windowDiff metric:winDiff =1N ?
kN?k?i=1(|ref ?
hyp| 6= 0) (4)windowDiff slides a window of size k through the input sequence of length N .
At every position of thewindow, the metric compares the number of boundaries in the reference sequence and in the hypotheticalsequence.
The number of erroneous windows is normalized by the total number of windows to obtainthe final value.
windowDiff is a penalty metric: lower values correspond to better segmentations.3http://nlp.stanford.edu/software/corenlp.shtml482AI Lectures MoonstoneAPS 0.420 (?
0.014) 0.441 (?
0.075)APS-coref sim 0.411 (?
0.025) 0.391 (?
0.060)APS-STS 0.428 (?
0.049) 0.479 (?
0.041)APS-STS -coref sim 0.429 (?
0.020) 0.478 (?
0.035)MCSeg 0.431 (?
0.045) 0.470 (?
0.095)MCSeg-corefsim 0.410 (?
0.060) 0.413 (?
0.030)MCSeg-STS 0.451 (?
0.023) 0.441 (?
0.051)MCSeg-STS-coref sim 0.433 (?
0.070) 0.430 (?
0.025)Table 1: Results of comparing APS and MCSeg using four different matrix types (windowDiff values andstandard deviation)5 EvaluationTable 1 presents the results of running APS and MCSeg using four different input matrices each.
Thefirst column shows the combination of the name of the segmenter and the specific input matrix.
APS andMCSeg refer to the cases where both segmenters were run using simple cosine similarity matrices.
STSrefers to matrices computed using STS-2012 from the DKPro Similarity framework.
coref sim refers tocosine similarity matrices modified by adding a matrix with coreferential similarities.
STS?coref simare matrices computed using STS-2012 which had coreferential similarity added to them.In all experiments, we set the weights for different types of referring expressions on two hold-out files.The remainder of the data is divided into five folds.
Standard deviation reported in the tables is computedacross folds.Coreferential similarity improves the results of the cosine matrix for both segmenters, but the improve-ment on the AI dataset is rather small (1% for APS and 2% for MCSeg).It is interesting to see that in most cases using STS matrices slightly hurts the performance of thesegmenters compared to using simple cosine similarity matrices.
The only exception is running MCSegon the Moonstone dataset which improves the performance by 3%.Adding a matrix of coreferential similarities to STS matrices slightly improves the performance onthe Moonstone dataset and leaves it practically unchanged on the dataset of AI lectures.It is somewhat surprising that using STS-2012 for similarity computation does not improve, and occa-sionally worsens, the results compared to using simple cosine similarity.
Coreferential similarity, on theother hand, produces a small but consistent improvement.We have examined the vectors of weights used in these experiments (set using hold-out data).
Onthe Moonstone dataset, the results are the best when personal animate pronouns get the highest weight,followed by demonstrative animate pronouns, as well as inanimate pronouns, both regular and demon-strative.
Other expression types are assigned either a very small weight or the value 0, effectively makingthem inconsequential.
We hypothesize that this is due to the fact that the novel discusses people, theirrelations and interactions, making animate entities central for estimating topical links.The vectors used on the AI lecture dataset are similar, except that here the highest weights are givento demonstrative and regular inanimate pronouns.
These are followed by demonstrative and then regularanimate pronouns.
This distribution is likely due to the fact that the lecture dataset discusses abstractconcepts, while people are likely to be noted more tangentially.
We are not sure how to explain the factthat in this dataset demonstrative pronouns have a slightly higher weight than the regular ones.Identifying and categorizing noun phrases requires either high-quality NP-tagging or parsing.
On theother hand, most pronouns can be captured very easily, perhaps even using a list of words.
It is interestingto note that the most gain is due to these ?cheap?
types of referring expressions.
In the future, we plan toimplement a lighter version of the coreferential similarity metric which only considers pronouns.6 Conclusions and future workThis paper has presented a method for improving the quality of topical segmentations by using infor-mation about referential expressions in nearby sentences.
The method slightly improves the quality ofsegmentations and, what is even more important, seems never to worsen the results.483The necessity to perform complete parsing of the input document is a drawback of the current ap-proach.
We note in Section 5, however, that the only types of referential expressions which improveperformance are personal and demonstrative pronouns.
Those can be easily captured without parsing.
Inthe near future we plan to investigate such a light-weight version of coref sim metric.Another way to improve our current implementation would be a more objective method of setting theweights for different types of referring expressions.
At present, the expressions are set by hand on asmall hold-out set of documents.
This is far from ideal.
We plan to investigate if using logistic regressionor expectation maximization would make the system more robust.ReferencesMira Ariel.
2014.
Accessing Noun-Phrase Antecedents.
Routledge, London and New York.Daniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.
DKPro Similarity: An Open Source Framework for TextSimilarity.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: SystemDemonstrations, pages 121?126, Sofia, Bulgaria.David Blei and Pedro Moreno.
2001.
Topic segmentation with an aspect hidden Markov Model.
In Proceedings ofthe 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,pages 343?348.Elizabeth Brown.
1983.
Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:993?1022.Freddy Y. Y. Choi, Peter Wiemer-Hastings, and Johanna Moore.
2001.
Latent Semantic Analysis for Text Seg-mentation.
In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,pages 109?117, Pittsburgh, Pennsylvania.Lan Du, Wray Buntine, and Mark Johnson.
2013.
Topic Segmentation with a Structured Topic Model.
InProceedings of the 2013 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages 190?200, Atlanta, Georgia.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesian Unsupervised Topic Segmentation.
In Proceedings of the2008 Conference on Empirical Methods in Natural Language Processing, pages 334?343, Honolulu, Hawaii.Talmy Giv?on.
1981.
Typology and Functional Domains.
Studies in Language, 5(2):163?193.M.A.K Halliday and Ruqaiya Hasan.
1976.
Cohesion in English.
Longman, London and New York.Marti A. Hearst.
1994.
Multi-paragraph Segmentation of Expository Text.
In Proceedings of the 32nd AnnualMeeting of the Association for Computational Linguistics, ACL ?94, pages 9?16, Las Cruces, New Mexico.Marti A. Hearst.
1997.
TextTiling: segmenting text into multi-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Amanda C. Jobbins and Lindsay J. Evett.
1998.
Text Segmentation Using Reiteration and Collocation.
In Pro-ceedings of the 17th International Conference on Computational Linguistics - Volume 1, COLING ?98, pages614?618, Montr?eal, Qu?ebec.Anna Kazantseva and Stan Szpakowicz.
2011.
Linear Text Segmentation Using Affinity Propagation.
In Proceed-ings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284?293, Edinburgh,Scotland.Anna Kazantseva and Stan Szpakowicz.
2012.
Topical Segmentation: a Study of Human Performance and a NewMeasure of Quality.
In Proceedings of the 2012 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pages 211?220, Montr?eal, Canada.Thomas K Landauer and Susan T. Dumais.
1997.
A solution to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation of knowledge.
Psychological Review, pages 211?240.Igor Malioutov and Regina Barzilay.
2006.
Minimum Cut Model for Spoken Lecture Segmentation.
In Pro-ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages 25?32, Sydney, Australia.Meghana Marathe.
2010.
Lexical Chains Using Distributional Measures of Concept Distance.
Master?s thesis,University of Toronto.484Hemant Misra, Franc?ois Yvon, Olivier Capp?e, and Joemon M. Jose.
2011.
Text segmentation: A topic modelingperspective.
Information Processing and Management, 47(4):528?544.Manabu Okumura and Takeo Honda.
1994.
Word Sense Disambiguation and Text Segmentation Based On LexicalCohesion.
In COLING 1994 Volume 2: The 15th International Conference on Computational Linguistics, pages775?761, Kyoto, Japan.Andrew Olney and Zhiqiang Cai.
2005.
An Orthonormal Basis for Topic Segmentation in Tutorial Dialogue.
InProceedings of the Conference on Human Language Technology and Empirical Methods in Natural LanguageProcessing ?HLT ?05, pages 971?978, Vancouver, Canada.Jeffrey C. Reynar.
1999.
Statistical Models of Text Segmentation.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics, pages 357?364.Martin Scaiano, Diana Inkpen, Robert Lagani`ere, and Adele Reinhartz.
2010.
Automatic Text Segmentation forMovie Subtitles.
In Advances in Artificial Intelligence, volume 6085 of Lecture Notes in Computer Science,pages 295?298.
Springer.Pasi Tapanainen and Timo J?arvinen.
1997.
A non-projective dependency parser.
Proceedings of the 5th Confer-ence on Applied Natural Language Processing, pages 64?71.Gilbert Youmans.
1991.
A new tool for discourse analysis: The vocabulary-management profile.
Language,67(4):763?789.485
