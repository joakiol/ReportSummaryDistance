Spectral Clustering for German VerbsChris BrewDepartment of LinguisticsThe Ohio State UniversityColumbus, Ohio, USAcbrew@ling.osu.eduSabine Schulte im WaldeInstitut fu?r Maschinelle SprachverarbeitungUniversita?t StuttgartStuttgart, Germanyschulte@ims.uni-stuttgart.deAbstractWe describe and evaluate the application of aspectral clustering technique (Ng et al, 2002)to the unsupervised clustering of German verbs.Our previous work has shown that standardclustering techniques succeed in inducing Levin-style semantic classes from verb subcategorisa-tion information.
But clustering in the veryhigh dimensional spaces that we use is fraughtwith technical and conceptual difficulties.
Spec-tral clustering performs a dimensionality reduc-tion on the verb frame patterns, and provides arobustness and efficiency that standard cluster-ing methods do not display in direct use.
Theclustering results are evaluated according to thealignment (Christianini et al, 2002) betweenthe Gram matrix defined by the cluster outputand the corresponding matrix defined by a goldstandard.1 IntroductionStandard multivariate clustering technology(such as k-Means) can be applied to the problemof inferring verb classes from information aboutthe estimated prevalence of verb frame patterns(Schulte im Walde and Brew, 2002).
But oneof the problems with multivariate clustering isthat it is something of a black art when appliedto high-dimensional natural language data.
Thesearch space is very large, and the availabletechniques for searching this large space do notoffer guarantees of global optimality.In response to this insight, the present workapplies a spectral clustering technique (Ng etal., 2002) to the verb frame patterns.
At theheart of this approach is a transformation ofthe original input into a set of orthogonal eigen-vectors.
We work in the space defined by thefirst few eigenvectors, using standard clusteringtechniques in the reduced space.
The spectralclustering technique has been shown to han-dle difficult clustering problems in image pro-cessing, offers principled methods for initializ-ing cluster centers, and (in the version that weuse) has no random component.The clustering results are evaluated accord-ing to their alignment with a gold standard.Alignment is Pearson correlation between corre-sponding elements of the Gram matrices, whichhas been suggested as a measure of agreementbetween a clustering and a distance measure(Christianini et al, 2002).
We are also able touse this measure to quantify the fit between aclustering result and the distance matrix thatserves as input to clustering.
The evidence isthat the spectral technique is more effectivethan the methods that have previously beentried.2 Verb valency descriptionThe data in question come from a subcate-gorization lexicon induced from a large Ger-man newspaper corpus (Schulte im Walde,2002).
The verb valency information is pro-vided in form of probability distributions oververb frames for each verb.
There are two condi-tions: the first with 38 relatively coarse syntac-tic verb subcategorisation frames, the second amore delicate classification subdividing the verbframes of the first condition using prepositionalphrase information (case plus preposition), re-sulting in 171 possible frame types.The verb frame types contain at most threearguments.
Possible arguments in the framesare nominative (n), dative (d) and accusative(a) noun phrases, reflexive pronouns (r), prepo-sitional phrases (p), expletive es (x), non-finiteclauses (i), finite clauses (s-2 for verb secondclauses, s-dass for dass-clauses, s-ob for ob-clauses, s-w for indirect wh-questions), and cop-Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
117-124.Proceedings of the Conference on Empirical Methods in Naturalula constructions (k).
For example, subcate-gorising a direct (accusative case) object anda non-finite clause would be represented bynai.
Table 1 shows an example distributionfor the verb glauben ?to think/believe?.
Themore delicate version of subcategorisation framewas done by distributing the frequency mass ofprepositional phrase frame types (np, nap, ndp,npr, xp) over the prepositional phrases, accord-ing to their frequencies in the corpus.
Preposi-tional phrases are referred to by case and prepo-sition, such as ?Dat.mit?, ?Akk.fu?r?.
The presentwork uses the latter, more delicate, verb valencydescriptions.Frame Prob Frame Probns-dass 0.27945 npr 0.00261ns-2 0.27358 nds-dass 0.00253np 0.09951 ndi 0.00161n 0.08811 nrs-dass 0.00029na 0.08046 ndr 0.00029ni 0.05015 nrs-w 0.00029nd 0.03392 nir 0.00027nad 0.02325 nds-w 0.00024nds-2 0.01011 xd 0.00017nai 0.00894 ns-ob 0.00014ns-w 0.00859 nas-ob 0.00014nas-w 0.00681 nds-ob 0.00000nap 0.00594 nrs-ob 0.00000nr 0.00455 x 0.00000nar 0.00436 xa 0.00000nrs-2 0.00391 xp 0.00000ndp 0.00356 xr 0.00000nas-dass 0.00342 xs-dass 0.00000nas-2 0.00281 k 0.00000Table 1: Probability distribution for glauben3 Problems with standard clusteringOur previous work on the valency data ap-plied k-Means (a standard technique) to thetask of inducing semantic classes for Germanverbs (Schulte im Walde and Brew, 2002).
Wecompared the results of k-Means clustering witha gold standard set prepared according to theprinciples of verb classification advocated by(Levin, 1993), and reported on the sensitivityof the classes to linguistically motivated ?lesion-ing?
of the input verb frame.
The verb classeswe used are listed in Table 2.The verb classes are closely related to Levin?sEnglish classes.
They also agree with the Ger-man verb classification in (Schumacher, 1986)as far as the relevant verbs appear in his lessextensive semantic ?fields?.
The rough glossesand the references to Levin?s classes in the tableare primarily to aid the intuition of non-nativespeakers of German.Clustering can be thought of as a processthat finds a discrete approximation to a distancemeasure.
For any data set of n items over whicha distance measure is defined, the Gram matrixis the symmetric n-by-n matrix whose elementsMij are the distances between items i and j.The diagonal elements Mii of this matrix will allbe 0.
Every clustering corresponds to a block-diagonal Gram matrix.
Clustering n items intok classes corresponds to the choice of an order-ing for the labels on the axes of the Gram matrixand the choice of k ?
1 change points markingthe boundaries between the blocks.
Thus, thesearch space of clusters is very large.
The avail-able techniques for searching this large space donot (and probably cannot) offer guarantees ofglobal optimality.
Standard nostrums includetransformations of the underlying data, and thedeployment of different strategies for initializ-ing the cluster centers.
These may produce in-tuitively attractive clusters, but when we applythese ideas to our verb frame data many ques-tions remain, including the following:?
When the solutions found by clustering dif-fer from our intuitions, is this because offailures in the features used, the clusteringtechniques, or the intuitions??
How close are the local optima found by theclustering techniques to the best solutionsin the space defined by the data??
Is it even appropriate to use frequency in-formation for this problem?
Or would itsuffice to characterize verb classes by thepattern of frames that their members caninhabit, without regard to frequency??
Does the data support some clusters morestrongly than others?
Are all the distinc-tions made in classifications such as Levin?sof equal validity?In response to these questions, the presentpaper describes an application to the verb dataof a particular spectral clustering technique (Nget al, 2002).
At the heart of this approach is atransformation of the original verb frame datainto a set of orthogonal eigenvectors.
We workAspect 55.1 anfangen, aufho?ren, beenden, beginnen, endenstart, stop, bring to an end, begin, endPropositional Attitude 29.5 ahnen, denken, glauben, vermuten, wissensense, think, think, guess, knowTransfer of Possession 29.5 bekommen, erhalten, erlangen, kriegen(Obtaining) receive, obtain, acquire, getTransfer of Possession 11.1/3 bringen, liefern, schicken, vermitteln, zustellen(Supply) bring, deliver, send, procure, deliverManner of Motion 51.4.2 fahren, fliegen, rudern, segelndrive, fly, row, sailEmotion 31.1 a?rgern, freuenirritate, delightAnnouncement 37.7 anku?ndigen, bekanntgeben, ero?ffnen, verku?ndenannounce, make known, disclose, proclaimDescription 29.2 beschreiben, charakterisieren, darstellen, interpretierendescribe, characterise, present, interpretInsistence - beharren, bestehen, insistieren, pochenall mean insistPosition 50 liegen, sitzen, stehenlie, sit, standSupport - dienen, folgen, helfen, unterstu?tzenserve, follow, help, supportOpening 45.4 o?ffnen, schlie?enopen, closeConsumption 39.4 essen, konsumieren, lesen, saufen, trinkeneat, consume, read, drink (esp.
animals or drunkards), drinkWeather 57 blitzen, donnern, da?mmern, nieseln, regnen, schneienflash, thunder, dawn/grow dusky, drizzle, rain, snowTable 2: Levin-style verb classes for Germanin the space defined by the first few eigenvec-tors, using standard clustering techniques in thetransformed space.4 The spectral clustering algorithmThe spectral clustering algorithm takes as in-put a matrix formed from a pairwise similarityfunction over a set of data points.
In imagesegmentation two pixels might be declared sim-ilar if they have similar intensity, similar hue orsimilar location, or if a local edge-detection al-gorithm does not place an edge between them.The technique is generic, and as (Longuet-Higgins and Scott, 1990) point out, originatednot in computer science or AI but in molecu-lar physics.
Most of the authors nevertheless?adopt the terminology of image segmentation(i.e.
the data points are pixels and the set of pix-els is the image), keeping in mind that all theresults are also valid for similarity-based clus-tering?
(Meila?
and Shi, 2001).
Our natural lan-guage application of the technique uses straight-forward similarity measures based on verb framestatistics, but nothing in the algorithm hingeson this, and we plan in future work to elabo-rate our similarity measures.
Although thereare several roughly analogous spectral cluster-ing techniques in the recent literature (Meila?and Shi, 2001; Longuet-Higgins and Scott, 1990;Weiss, 1999), we use the algorithm from (Ng etal., 2002) because it is simple to implement andunderstand.Here are the key steps of that algorithm:Given a set of points S = {s1, .
.
.
, sn} in a highdimensional space.1.
Form a distance matrix D ?
R2.
For(Ng et al, 2002) this distance measure isEuclidean, but other measures also makesense.2.
Transform the distance matrix to an affin-ity matrix by Aij = exp(?D2ij?2 ) if i 6= j,0 if i = j.
The free parameter ?2 controlsthe rate at which affinity drops off with dis-tance.3.
Form the diagonal matrix D whose (i,i) el-ement is the sum of A?s ith row, and createthe matrix L = D?1/2AD?1/2.4.
Obtain the eigenvectors and eigenvalues ofL.5.
Form a new matrix from the vectors associ-ated with the k largest eigenvalues.
Choosek either by stipulation or by picking suffi-cient eigenvectors to cover 95% of the vari-ance1.6.
Each item now has a vector of k co-ordinates in the transformed space.
Nor-malize these vectors to unit length.7.
Cluster in k-dimensional space.
Following(Ng et al, 2002) we use k-Means for thispurpose, but any other algorithm that pro-duces tight clusters could fill the same role.In (Ng et al, 2002) an analysis demon-strates that there are likely to be k well-separated clusters.We carry out the whole procedure for a rangeof values of ?.
In our experiments ?
is searchedin steps of 0.001 from 0.01 to 0.059, since thatalways sufficed to find the best aligned set ofclusters.
If ?
is set too low no useful eigenvec-tors are returned, but this situation is easy todetect.
We take the solution with the best align-ment (see definition below) to the (original) dis-tance measure.
This is how (Christianini et al,2002) choose the best solution, while (Ng et al,2002) explain that they choose the solution withthe tightest clusters, without being specific onhow this is done.In general it matters how initialization ofcluster centers is done for algorithms like k-Means.
(Ng et al, 2002) provide a neat ini-tialization strategy, based on the expectationthat the clusters in their space will be orthog-onal.
They select the first cluster center to bea randomly chosen data point, then search theremaining data points for the one most orthog-onal to that.
For the third data point they lookfor one that is most orthogonal to the previ-ous two, and so on until sufficient have beenobtained.
We modify this strategy slightly, re-moving the random component by initializing ntimes, starting out at each data point in turn.This is fairly costly, but improves results, andis less expensive than the random initializationsand multiple runs often used with k-Means.1Srini Parthasarathy suggested this dodge for allow-ing the eigenvalues to select the appropriate number ofclusters.5 Experiments and evaluationWe clustered the verb frames data using our ver-sion of the algorithm in (Ng et al, 2002).
To cal-culate the distance d between two verbs v1 andv2 we used a range of measures: the cosine of theangle between the two vectors of frame proba-bilities, a flattened version of the cosine mea-sure in which all non-zero counts are replacedby 1.0 (labelled bcos, for binarized cosine, in Ta-ble 3), and skew divergence, recently shown asan effective measure for distributional similar-ity (Lee, 2001).
This last is defined in terms ofKL-divergence, and includes a free weight pa-rameter w, which we set to 0.9, following(Lee,2001), Skew-divergence is asymmetric in its ar-guments, but our technique needs a symmet-ric measure,so we calculate it in both directionsand use the larger value.Table 3 contains four results for each of threedistance measures (cos,bcos and skew).
The firstline of each set gives the results when the spec-tral algorithm is provided with the prior knowl-edge that k = 14.
The second line gives theresults when the standard k-Means algorithm isused, again with k = 14.
In the third line ofeach set, the value of k is determined from theeigenvalues, as described above.
For cos 12 clus-ters are chosen, for bcos the chosen value is 17,and for skew it is 16.
The final line of each setgives the results when the standard algorithm isused, but k is set to the value selected for thatdistance measure by the spectral method.For standard k-Means, the initializationstrategy from (Ng et al, 2002) does not ap-ply (and does not work well in any case), sowe used 100 random replications of the initial-ization, each time initializing the cluster centerswith k randomly chosen data points.
We reportthe result that had the highest alignment withthe distance measure (cf.
Section 5.1).(Meila?
and Shi, 2001) provide analysis in-dicating that their MNcut algorithm (anotherspectral clustering technique) will be exactwhen the eigenvectors used for clustering arepiecewise constant.
Figure 1 shows the top 16eigenvectors of a distance matrix based on skewdivergence, with the items sorted by the firsteigenvector.
Most of the eigenvectors appear tobe piecewise constant, suggesting that the con-ditions for good performance in clustering areindeed present in the language data.
Many ofevidence performanceAlgorithm k Support Confidence Quality Precision Recall F-MeasureCos (Ng) 14 0.80 0.83 0.81 0.30 0.43 0.35Cos (Direct) 14 - 0.78 0.74 0.21 0.44 0.28Cos (Ng) (12) - 0.79 0.81 0.26 0.40 0.32Cos (Direct) 12 - 0.72 0.79 0.20 0.45 0.28BCos (Ng) 14 0.86 0.86 0.86 0.21 0.23 0.22BCos (Direct) 14 - 0.81 0.78 0.16 0.21 0.18BCos (Ng) (17) - 0.86 0.83 0.28 0.23 0.25BCos (Direct) 17 - 0.87 0.80 0.13 0.11 0.12Skew (Ng) 14 0.84 0.85 0.85 0.37 0.47 0.41Skew (Direct) 14 - 0.84 0.78 0.22 0.34 0.27Skew (Ng) (16) - 0.86 0.88 0.49 0.47 0.48Skew (Direct) 16 - 0.84 0.84 0.35 0.41 0.37Table 3: Performance of the clustering algorithmsthe eigenvectors appear to correspond to a par-tition of the data into a small number of tightclusters.
Taken as a whole they induce the clus-terings reported in Table 3.5.1 Alignment as an evaluation toolPearson correlation between corresponding ele-ments of the Gram matrices has been suggestedas a measure of agreement between a cluster-ing and a distance measure (Christianini et al,2002).
Since we can convert a clustering into adistance measure, alignment can be used in anumber of ways, including comparison of clus-terings against each other.For evaluation, three alignment-based mea-sures are particularly relevant:?
The alignment between the gold standardand the distance measure reflects the pres-ence or absence in the distance measureof evidential support for the relationshipsthat the clustering algorithm is supposedto infer.
This is the column labelled ?Sup-port?
in Table 3.?
The alignment between the clusters in-ferred by the algorithm and the distancemeasure reflects the confidence that the al-gorithm has in the relationships that it haschosen.
This is the column labelled ?Con-fidence?
in Table 3.?
The alignment between the gold standardand the inferred clusters reflects the qualityof the result.
This is the column labelled?Quality?
in Table 3.We hope that when the algorithms are confi-dent they will also be right, and that when thedata strongly supports a distinction the algo-rithms will find it.5.2 ResultsTable 3 contains our data.
The columns basedon various forms of alignment have been dis-cussed above.
Clusterings are also sets of pairs,so, when the Gram matrices are discrete, wecan also provide the standard measures of pre-cision, recall and F-measure.
Usually it is ir-relevant whether we choose alignment or thestandard measures, but the latter can yield un-expected results for extreme clusterings (manysmall clusters or few very big clusters).
Theremaining columns provide these conventionalperformance measures.For all the evaluation methods and all thedistance measures that we have tried, the algo-rithm from (Ng et al, 2002) does better than di-rect clustering, usually finding a clustering thataligns better with the distance measure thandoes the gold standard.
Deficiencies in the re-sult are due to weaknesses in the distance mea-sures or the original count data, rather thansearch errors committed by the clustering al-gorithm.
Skew divergence is the best distancemeasure, cosine is less good and cosine on bina-rized data the worst.5.3 Which verbs and clusters are hard?All three alignment measures can be applied toa clustering as whole, as above, or restricted toa subset of the Gram matrix.
These can tellus how well each verb and each cluster matchesthe distance measure (or indeed the gold stan-dard).
To compute alignment for a verb we cal-?1.0?0.20.00.6?1.00.0?1.00.0?1.00.0?0.80.0?0.20.8?0.80.2?1.0?0.20.00.80.00.8?0.40.6?0.80.2?0.80.2?0.40.6?0.20.8Figure 1: The top 16 eigenvectors of the distance matrixculate Spearman correlation over its row of theGram matrix.
For a cluster we do the same, butover all the rows corresponding to the clustermembers.
The second column of Table 4, la-belled ?Support?, gives the contribution of thatverb to the alignment between the gold stan-dard clustering and the skew-divergence dis-tance measure (that is, the empirical supportthat the distance measure gives to the human-preferred placement of the verb).
The third col-umn, labelled ?Confidence?
contains the contri-bution of the verb to the alignment between theskew-divergence and the clustering inferred byour algorithm (this is the measure of the confi-dence that the clustering algorithm has in thecorrectness of its placement of the verb, andis what is maximized by Ng?s algorithm as wevary ?).
The fourth column, labelled ?Correct-ness?, measures the contribution of the verb tothe alignment between the inferred cluster andthe gold standard (this is the measure of howcorrectly the verb was placed).
To get a feelfor performance at the cluster level we mea-sured the alignment with the gold standard.We merged and ranked the lists proposed byskew divergence and binary cosine.
The figureof merit, labelled ?Score?
is the geometric meanof the alignments for the members of the clus-ter.
The second column, labelled ?Method?,indicates which distance measure or measuresproduced this cluster.
Table 5 shows this rank-ing.
Two highly ranked clusters (Emotion anda large subset of Weather) are selected by bothdistance measures.
The highest ranked clus-ter proposed only by binary cosine is a sub-Verb Support Confidence Correctnessfreuen 0.97 0.97 1.00a?rgern 0.95 0.95 1.00stehen 0.93 0.93 1.00sitzen 0.93 0.93 1.00liegen 0.92 0.92 1.00glauben 0.90 0.96 0.89dienen 0.90 0.94 0.96pochen 0.89 0.91 0.96beharren 0.89 0.91 0.96segeln 0.89 0.87 0.89. .
.nieseln 0.87 0.92 0.93. .
.da?mmern 0.82 0.87 0.93. .
.donnern 0.76 0.86 0.68unterstu?tzen 0.71 0.79 0.68beenden 0.68 0.80 0.65Table 4: Empirical support, confidence andalignment for skew-divergenceset of Position, but this is dominated by skew-divergence?s correct identification of the wholeclass (see Table 2 for a reminder of the defini-tions of these classes).
The systematic superi-ority of the probabilistic measure suggests thatthere is after all useful information about verbclasses in the non-categorical part of our verbframe data.6 Related workLevin?s (Levin, 1993) classification has pro-voked several studies that aim to acquire lex-ical semantic information from corpora usingcues pertaining to mainly syntactic structureScore Method Cluster1.0 both freuen a?rgern1.0 skew liegen sitzen stehen0.96 skew dienen folgen helfen0.96 skew beschreiben charakterisiereninterpretieren0.96 skew beharren insistieren, pochen0.96 bcos liegen stehen0.93 skew liefern vermitteln zustellen0.93 both da?mmern nieseln regnen schneien0.93 skew ahnen vermuten wissenTable 5: Cluster quality by origin(Merlo and Stevenson, 2001; Schulte im Walde,2000; Lapata, 1999; McCarthy, 2000; Lapataand Brew, 1999).
Other work has used Levin?slist of verbs (in conjunction with related lexicalresources) for the creation of dictionaries thatexploit the systematic correspondence betweensyntax and meaning (Dorr, 1997; Dang et al,1997; Dorr and Jones, 1996).Most statistical approaches, including ours,treat verbal meaning assignment as a semanticclustering or classification task.
The underly-ing question is the following: how can corpusinformation be exploited in deriving the seman-tic class for a given verb?
Despite the unify-ing theme of using corpora and corpus distri-butions for the acquisition task, the approachesdiffer in the inventory of classes they employ,in the methodology used for inferring semanticclasses and the specific assumptions concerningthe verbs to be classified (i.e., can they be pol-ysemous or not).
(Merlo and Stevenson, 2001) use grammati-cal features (acquired from corpora) to classifyverbs into three semantic classes: unergative,unaccusative, and object-drop.
These classesare abstractions of Levin?s (Levin, 1993) classesand as a result yield a coarser classification.The classifier used is a decision tree learner.
(Schulte im Walde, 2000) uses subcategoriza-tion information and selectional restrictions tocluster verbs into (Levin, 1993) compatible se-mantic classes.
Subcategorization frames are in-duced from the BNC using a robust statisticalparser (Carroll and Rooth, 1998).
The selec-tional restrictions are acquired using Resnik?s(Resnik, 1993) information-theoretic measure ofselectional association which combines distribu-tional and taxonomic information in order toformalise how well a predicate associates with agiven argument.7 ConclusionsWe have described the application to naturallanguage data of a spectral clustering technique(Ng et al, 2002) closely related to kernel PCA(Christianini et al, 2002).
We have presentedevidence that the dimensionality reduction in-volved in the clustering technique can give k-Means a robustness that it does not display indirect use.
The solutions found by the spec-tral clustering are always at least as well-alignedwith the distance measure as is the gold stan-dard measure produced by human intuition, butthis does not hold when k-Means is used directlyon the untransformed data.Since we work in a transformed space of lowdimensionality, we gain efficiency, and we nolonger have to sum and average data pointsin the original space associated with the verbframe data.
In principle, this gives us thefreedom to use, as is standardly done withSVMs (Christianini and Shawe-Taylor, 2000),extremely high dimensional representations forwhich it would not be convenient to use k-Meansdirectly.
We could for instance use featureswhich are derived not from the counts of a singleframe but of two or more.
This is linguisticallydesirable, since Levin?s verb classes are definedprimarily in terms of alternations rather than interms of single frames.
We plan to explore thispossibility in future work.It is also clearly against the spirit of (Levin,1993) to insist that verbs should belong to onlyone cluster, since, for example, both the Ger-man ?da?mmern?
and the English ?dawn?
areclearly related both to verbs associated withweather and natural phenomena (because of?Day dawns.?)
and to verbs of cognition (be-cause of ?It dawned on Kim that .
.
.
?).
In or-der to accommodate this, we are exploring theconsequences of replacing the k-Means step ofour algorithm with an appropriate soft cluster-ing technique.ReferencesGlenn Carroll and Mats Rooth.
1998.
Valenceinduction with a head-lexicalized PCFG.
InNancy Ide and Atro Voutilainen, editors, Pro-ceedings of the 3rd Conference on Empiri-cal Methods in Natural Language Processing,pages 36?45, Granada, Spain.Nello Christianini and John Shawe-Taylor.2000.
An Introduction to Support VectorMachines and other Kernel-based LearningMethods.
Cambridge University Press.Nello Christianini, John Shawe-Taylor, and JazKandola.
2002.
Spectral kernel methods forclustering.
In T. G. Dietterich, S. Becker, andZ.
Ghahramani, editors, Advances in Neu-ral Information Processing Systems 14, Cam-bridge, MA.
MIT Press.Hoa Trang Dang, Joseph Rosenzweig, andMartha Palmer.
1997.
Associating semanticcomponents with intersective Levin classes.In Proceedings of the 1st AMTA SIG-ILWorkshop on Interlinguas, pages 1?8, SanDiego, CA.Bonnie J. Dorr and Doug Jones.
1996.
Roleof word sense disambiguation in lexical ac-quisition: Predicting semantics from syntac-tic cues.
In Proceedings of the 16th Interna-tional Conference on Computational Linguis-tics, pages 322?327, Copenhagen, Denmark.Bonnie J. Dorr.
1997.
Large-scale dictionaryconstruction for foreign language tutoringand interlingual machine translation.
Ma-chine Translation, 12(4):371?322.Maria Lapata and Chris Brew.
1999.
Usingsubcategorization to resolve verb class am-biguity.
In Pascal Fung and Joe Zhou, edi-tors, Joint SIGDAT Conference on Empiri-cal Methods in NLP and Very Large Corpora,College Park, Maryland.Maria Lapata.
1999.
Acquiring lexical gener-alizations from corpora: A case study fordiathesis alternations.
In Proceedings of the37th Annual Meeting of the Association forComputational Linguistics, pages 397?404,College Park, MD.Lillian Lee.
2001.
On the effectiveness of theskew divergence for statistical language anal-ysis.
In Artificial Intelligence and Statistics2001, pages 65?72.Beth Levin.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.H.
Christopher Longuet-Higgins and Guy L.Scott.
1990.
Feature grouping by ?relocalisa-tion?
of eigenvectors of the proximity matrix.In Proceedings of the British Machine VisionConference., pages 103?8, Oxford, UK.Diana McCarthy.
2000.
Using semantic pref-erences to identify verbal participation inrole switching alternations.
In Proceedings ofthe 1st North American Annual Meeting ofthe Association for Computational Linguis-tics, pages 256?263, Seattle, WA.Marina Meila?
and Jianbo Shi.
2001.
A randomwalks view of spectral segmentation.
In Arti-ficial Intelligence and Statistics 2001.Paola Merlo and Susanne Stevenson.
2001.
Au-tomatic verb classification based on statisticaldistribution of argument structure.
Compu-tational Linguistics, 27(3):373?408.Andrew.
Y. Ng, Michael.
I. Jordan, and YairWeiss.
2002.
On spectral clustering: Anal-ysis and an algorithm.
In T. G. Dietterich,S.
Becker, and Z. Ghahramani, editors, Ad-vances in Neural Information Processing Sys-tems 14, Cambridge, MA.
MIT Press.Philip Stuart Resnik.
1993.
Selection and In-formation: A Class-Based Approach to Lexi-cal Relationships.
Ph.D. thesis, University ofPennsylvania.Sabine Schulte im Walde and Chris Brew.
2002.Inducing german semantic verb classes frompurely syntactic subcategorization informa-tion.
In Association for Computational Lin-guistics,40th Anniversary Meeting, Philadel-phia,Pa.
To appear.Sabine Schulte im Walde.
2000.
Clusteringverbs semantically according to their alter-nation behaviour.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics, pages 747?753, Saarbru?cken,Germany.Sabine Schulte im Walde.
2002.
A subcategori-sation lexicon for german verbs induced froma lexicalised PCFG.
In Proceedings of the 3rdConference on Language Resources and Eval-uation, Las Palmas, Spain.
To appear.Helmut Schumacher.
1986.
Verben in Feldern.de Gruyter, Berlin.Yair Weiss.
1999.
Segmentation using eigenvec-tors: A unifying view.
In ICCV (2), pages975?982.
