Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498?1507,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEmbedding Semantic Similarity in Tree Kernels for Domain Adaptationof Relation ExtractionBarbara Plank?Center for Language TechnologyUniversity of Copenhagen, Denmarkbplank@gmail.comAlessandro MoschittiQCRI - Qatar Foundation &DISI - University of Trento, Italyamoschitti@qf.org.qaAbstractRelation Extraction (RE) is the task ofextracting semantic relationships betweenentities in text.
Recent studies on rela-tion extraction are mostly supervised.
Theclear drawback of supervised methods isthe need of training data: labeled data isexpensive to obtain, and there is often amismatch between the training data andthe data the system will be applied to.This is the problem of domain adapta-tion.
In this paper, we propose to combine(i) term generalization approaches such asword clustering and latent semantic anal-ysis (LSA) and (ii) structured kernels toimprove the adaptability of relation ex-tractors to new text genres/domains.
Theempirical evaluation on ACE 2005 do-mains shows that a suitable combinationof syntax and lexical generalization is verypromising for domain adaptation.1 IntroductionRelation extraction is the task of extracting se-mantic relationships between entities in text, e.g.to detect an employment relationship between theperson Larry Page and the company Google inthe following text snippet: Google CEO LarryPage holds a press announcement at its headquar-ters in New York on May 21, 2012.
Recent stud-ies on relation extraction have shown that super-vised approaches based on either feature or ker-nel methods achieve state-of-the-art accuracy (Ze-lenko et al, 2002; Culotta and Sorensen, 2004;?
The first author was affiliated with the Department ofComputer Science and Information Engineering of the Uni-versity of Trento (Povo, Italy) during the design of the mod-els, experiments and writing of the paper.Zhang et al, 2005; Zhou et al, 2005; Zhang etal., 2006; Bunescu, 2007; Nguyen et al, 2009;Chan and Roth, 2010; Sun et al, 2011).
How-ever, the clear drawback of supervised methods isthe need of training data, which can slow downthe delivery of commercial applications in newdomains: labeled data is expensive to obtain, andthere is often a mismatch between the training dataand the data the system will be applied to.
Ap-proaches that can cope with domain changes areessential.
This is the problem of domain adapta-tion (DA) or transfer learning (TL).
Technically,domain adaptation addresses the problem of learn-ing when the assumption of independent and iden-tically distributed (i.i.d.)
samples is violated.
Do-main adaptation has been studied extensively dur-ing the last couple of years for various NLP tasks,e.g.
two shared tasks have been organized on do-main adaptation for dependency parsing (Nivre etal., 2007; Petrov and McDonald, 2012).
Resultswere mixed, thus it is still a very active researcharea.However, to the best of our knowledge, thereis almost no work on adapting relation extraction(RE) systems to new domains.1 There are someprior studies on the related tasks of multi-tasktransfer learning (Xu et al, 2008; Jiang, 2009)and distant supervision (Mintz et al, 2009), whichare clearly related but different: the former is theproblem of how to transfer knowledge from oldto new relation types, while distant supervisiontries to learn new relations from unlabeled textby exploiting weak-supervision in the form of aknowledge resource (e.g.
Freebase).
We assumethe same relation types but a shift in the underlying1Besides an unpublished manuscript of a student project,but it is not clear what data was used.
http://tinyurl.com/bn2hdwk1498data distribution.
Weak supervision is a promis-ing approach to improve a relation extraction sys-tem, especially to increase its coverage in terms oftypes of relations covered.
In this paper we ex-amine the related issue of changes in the underly-ing data distribution, while keeping the relationsfixed.
Even a weakly supervised system is ex-pected to perform well when applied to any kind oftext (other domain/genre), thus ideally, we believethat combining domain adaptation with weak su-pervision is the way to go in the future.
This studyis a first step towards this.We focus on unsupervised domain adaptation,i.e.
no labeled target data.
Moreover, we considera particular domain adaptation setting: single-system DA, i.e.
learning a single system able tocope with different but related domains.
Moststudies on DA so far have focused on buildinga specialized system for every specific target do-main, e.g.
Blitzer et al (2006).
In contrast, thegoal here is to build a single system that can ro-bustly handle several domains, which is in linewith the setup of the recent shared task on pars-ing the web (Petrov and McDonald, 2012).
Par-ticipants were asked to build a single system thatcan robustly parse all domains (reviews, weblogs,answers, emails, newsgroups), rather than to buildseveral domain-specific systems.
We consider thisas a shift in what was considered domain adapta-tion in the past (adapt from source to a specific tar-get) and what can be considered a somewhat dif-ferent recent view of DA, that became widespreadsince 2011/2012.
The latter assumes that the tar-get domain(s) is/are not really known in advance.In this setup, the domain adaptation problem boilsdown to finding a more robust system (S?gaardand Johannsen, 2012), i.e.
one wants to build asystem that can robustly handle any kind of data.We propose to combine (i) term generalizationapproaches and (ii) structured kernels to improvethe performance of a relation extractor on newdomains.
Previous studies have shown that lexi-cal and syntactic features are both very important(Zhang et al, 2006).
We combine structural fea-tures with lexical information generalized by clus-ters or similarity.
Given the complexity of featureengineering, we exploit kernel methods (Shawe-Taylor and Cristianini, 2004).
We encode wordclusters or similarity in tree kernels, which, inturn, produce spaces of tree fragments.
For ex-ample, ?president?, ?vice-president?
and ?Texas?,?US?, are terms indicating an employment rela-tion between a person and a location.
Rather thanonly matching the surface string of words, lexi-cal similarity enables soft matches between similarwords in convolution tree kernels.
In the empir-ical evaluation on Automatic Content Extraction(ACE) data, we evaluate the impact of convolu-tion tree kernels embedding lexical semantic sim-ilarities.
The latter is derived in two ways with:(a) Brown word clustering (Brown et al, 1992);and (b) Latent Semantic Analysis (LSA).
We firstshow that our system aligns well with the state ofthe art on the ACE 2004 benchmark.
Then, wetest our RE system on the ACE 2005 data, whichexploits kernels, structures and similarities for do-main adaptation.
The results show that combiningthe huge space of tree fragments generalized at thelexical level provides an effective model for adapt-ing RE systems to new domains.2 Semantic Syntactic Tree KernelsIn kernel-based methods, both learning and classi-fication only depend on the inner product betweeninstances.
Kernel functions can be efficiently andimplicitly computed by exploiting the dual formu-lation: ?i=1..l yi?i?(oi)?
(o) + b = 0, where oiand o are two objects, ?
is a mapping from an ob-ject to a feature vector ~xi and ?(oi)?
(o) =K(oi, o)is a kernel function implicitly defining such a map-ping.
In case of structural kernels, K determinesthe shape of the substructures describing the ob-jects.
Commonly used kernels in NLP are stringkernels (Lodhi et al, 2002) and tree kernels (Mos-chitti, 2006; Moschitti, 2008).NPPPNPE2NNPTexasINfromNPE1NNPgovernor?NPPPNPNPPPNPE1NPPPNPE1NNPgovernorE1NNPgovernor.
.
.NNPTexasFigure 1: Syntactic tree kernel (STK).Syntactic tree kernels (Collins and Duffy, 2001)compute the similarity between two trees T1and T2 by counting common sub-trees (cf.
Fig-ure 1), without enumerating the whole fragmentspace.
However, if two trees have similar sub-structures that employ different though related ter-minal nodes, they will not be matched.
This is1499clearly a limitation.
For instance, the fragmentscorresponding to governor from Texas andhead of Maryland are intuitively semanti-cally related and should obtain a higher matchwhen compared to mother of them.Semantic syntactic tree kernels (Bloehdornand Moschitti, 2007a; Bloehdorn and Moschitti,2007b; Croce et al, 2011) provide one way to ad-dress this problem by introducing similarity ?
thatallows soft matches between words and, conse-quently, between fragments containing them.
LetN1 and N2 be the set of nodes in T1 and T2, re-spectively.
Moreover, let Ii(n) be an indicatorvariable that is 1 if subtree i is rooted at n and0 otherwise.
The syntactic semantic convolutionkernel TK?
(Bloehdorn and Moschitti, 2007b)over T1 and T2 is computed as TK?
(T1, T2) =?n1?N1,n2?N2 ??
(n1, n2) where ??
(n1, n2) =?n1?N1?n2?N2?i Ii(n1)Ii(n2) is computed ef-ficiently using the following recursive defini-tion: i) If the nodes n1 and n2 are ei-ther different or have different number of chil-dren then ??
(n1, n2) = 0; else ii) Ifn1 and n2 are pre-terminals then ??
(n1, n2)= ?
?nc(n1)j=1 ??
(ch(n1, j), ch(n2, j)), where ?measures the similarity between the correspond-ing children of n1 and n2; iii) If n1 and n2 haveidentical children: ??
(n1, n2) = ?
?nc(n1)j=1 (1 +??
(ch(n1, j)), ch(n2, j)); else ??
(n1, n2) = 0.TK?
combines generalized lexical with structuralinformation: it allows matching tree fragmentsthat have the same syntactic structure but differ intheir terminals.
After introducing related work, wewill discuss computational structures for RE andtheir extension with semantic similarity.3 Related WorkSemantic syntactic tree kernels have been previ-ously used for question classification (Bloehdornand Moschitti, 2007a; Bloehdorn and Moschitti,2007b; Croce et al, 2011).
These kernels havenot yet been studied for either domain adaptationor RE.
Brown clusters were studied previously forfeature-based approaches to RE (Sun et al, 2011;Chan and Roth, 2010), but they were not yet eval-uated in kernels.
Thus, we present a novel applica-tion of semantic syntactic tree kernels and Brownclusters for domain adaptation of tree-kernel basedrelation extraction.Regarding domain adaptation, several meth-ods have been proposed, ranging from instanceweighting (Jiang and Zhai, 2007) to approachesthat change the feature representation (Daume?
III,2007) or try to exploit pivot features to finda generalized shared representation between do-mains (Blitzer et al, 2006).
The easy-adapt ap-proach presented in Daume?
III (2007) assumes thesupervised adaptation setting and is thus not ap-plicable here.
Structural correspondence learn-ing (Blitzer et al, 2006) exploits unlabeled datafrom both source and target domain to find cor-respondences among features from different do-mains.
These correspondences are then integratedas new features in the labeled data of the sourcedomain.
The key to SCL is to exploit pivot fea-tures to automatically identify feature correspon-dences, and as such is applicable to feature-basedapproaches but not in our case since we do not as-sume availability of target domain data.
Instead,we apply a similar idea where we exploit an en-tire unlabeled corpus as pivot, and compare ourapproach to instance weighting (Jiang and Zhai,2007).Instance weighting is a method for domainadaptation in which instance-dependent weightsare assigned to the loss function that is mini-mized during the training process.
Let l(x, y, ?
)be some loss function.
Then, as shown in Jiangand Zhai (2007), the loss function can be weightedby ?il(x, y, ?
), such that ?i = Pt(xi)Ps(xi) , where Psand Pt are the source and target distributions, re-spectively.
Huang et al (2007) present an appli-cation of instance weighting to support vector ma-chines by minimizing the following re-weightedfunction: min?,?
12 ||?||2 + C?mi=1 ?i?i.
Findinga good weight function is non-trivial (Jiang andZhai, 2007) and several approximations have beenevaluated in the past, e.g.
S?gaard and Haulrich(2011) use a bigram-based text classifier to dis-criminate between domains.
We will use a binaryclassifier trained on RE instance representations.4 Computational Structures for REA common way to represent a constituency-basedrelation instance is the PET (path-enclosed-tree),the smallest subtree including the two target enti-ties (Zhang et al, 2006).
This is basically the for-mer structure PAF2 (predicate argument feature)defined in Moschitti (2004) for the extraction ofpredicate argument relations.
The syntactic rep-2It is the smallest subtree enclosing the predicate and oneof its argument node.1500resentation used by Zhang et al (2006) (we willrefer to it as PET Zhang) is the PET with enrichedentity information: e.g.
E1-NAM-PER, includingentity type (PER, GPE, LOC, ORG) and mentiontype (NAM, NOM, PRO, PRE: name, nominal,pronominal or premodifier).
An alternative ker-nel that does not use syntactic information is theBag-of-Words (BOW) kernel, where a single rootnode is added above the terminals.
Note that inthis BOW kernel we actually mark target entitieswith E1/E2.
Therefore, our BOW kernel can beconsidered an enriched BOW model.
If we do notmark target entities, performance drops consider-ably, as discussed later.As shown by Zhang et al (2006), includ-ing gold-standard information on entity and men-tion type substantially improves relation extrac-tion performance.
We will use this gold infor-mation also in Section 6.1 to show that our sys-tem aligns well to the state of the art on the ACE2004 benchmark.
However, in a realistic settingthis information is not available or noisy.
In fact,as we discuss later, excluding gold entity informa-tion decreases system performance considerably.In the case of porting a system to new domainsentity information will be unreliable or missing.Therefore, in our domain adaptation experimentson the ACE 2005 data (Section 6.3) we will notrely on this gold information but rather train a sys-tem using PET (target mentions only marked withE1/E2 and no gold entity label).34.1 Syntactic Semantic StructuresCombining syntax with semantics has a clear ad-vantage: it generalizes lexical information encap-sulated in syntactic parse trees, while at the sametime syntax guides semantics in order to obtain aneffective semantic similarity.
In fact, lexical infor-mation is highly affected by data-sparseness, thustree kernels combined with semantic informationcreated from additional resources should providea way to obtain a more robust system.We exploit this idea here for domain adaptation(DA): if words are generalized by semantic simi-larity LS, then in a hypothetical world changingLS such that it reflects the target domain would3In a setup where gold label info is included, the impactof similarity-based methods is limited ?
gold informationseems to predominate.
We argue that whenever gold data isnot available, distributional semantics paired with kernels canbe useful to improve generalization and complement missinggold info.allow the system to perform better in the targetdomain.
The question remains how to establish alink between the semantic similarity in the sourceand target domain.
We propose to use an entireunlabeled corpus as pivot: this corpus must begeneral enough to encapsulate the source and tar-get domains of interest.
The idea is to (i) learnsemantic similarity between words on the pivotcorpus and (ii) use tree kernels embedding sucha similarity to learn a RE system on the source,which allows to generalize to the new target do-main.
This reasoning is related to Structural Cor-respondence Learning (SCL) (Blitzer et al, 2006).In SCL, a representation shared across domains islearned by exploiting pivot features, where a setof pivot features has to be selected (usually a fewthousands).
In our case pivots are words that co-occur with the target words in a large unlabeledcorpus and are thus implicitly represented in thesimilarity matrix.
Thus, in contrast to SCL, we donot need to select a set of pivot features but ratherrely on the distributional hypothesis to infer a se-mantic similarity from a large unlabeled corpus.Then, this similarity is incorporated into the treekernel that provides the necessary restriction foran effective semantic similarity calculation.
Onepeculiarity of our work is that we exploit a largeamount of general data, i.e.
data gathered from theweb, which is a different but also more challeng-ing scenario than the general unsupervised DA set-ting where domain specific data is available.
Westudy two ways for term generalization in tree ker-nels: Brown words clusters and Latent SemanticAnalysis (LSA), both briefly described next.a) replace posNPPPNPE21111100110Seoul10001110fromNPE11101100011officialsb) replace word..NPE2NNP1111100110c) above pos..NPE21111100110NNPSeoulFigure 2: Integrating Brown cluster informationThe Brown algorithm (Brown et al, 1992) isa hierarchical agglomerative hard-clustering algo-rithm.
The path from the root of the tree down toa leaf node is represented compactly as a bitstring.By cutting the hierarchy at different levels one canobtain different granularities of word clusters.
We1501evaluate different ways to integrate cluster infor-mation into tree kernels, some of which are illus-trated in Figure 2.For LSA, we compute term similarity functionsfollowing the distributional hypothesis (Harris,1964), i.e.
the meaning of a word can be describedby the set of textual contexts in which it appears.The original word-by-word context matrix M isdecomposed through Singular Value Decomposi-tion (SVD) (Golub and Kahan, 1965), where Mis approximated by UlSlV Tl .
This approxima-tion supplies a way to project a generic term wiinto the l-dimensional space using W = UlS1/2l ,where each row corresponds to the vectors ~wi.Given two words w1 and w2, the term similarityfunction ?
is estimated as the cosine similarity be-tween the corresponding projections ~w1, ~w2 andused in the kernel as described in Section 2.5 Experimental SetupWe treat relation extraction as a multi-class classi-fication problem and use SVM-light-TK4 to trainthe binary classifiers.
The output of the classifiersis combined using the one-vs-all approach.
Wemodified the SVM-light-TK package to includethe semantic tree kernels and instance weight-ing.
The entire software package is publicly avail-able.5 For the SVMs, we use the same parametersas Zhang et al (2006): ?
= 0.4, c = 2.4 using theCollins Kernel (Collins and Duffy, 2001).
The pre-cision/recall trade-off parameter for the none classwas found on held-out data: j = 0.2.
Evalua-tion metrics are standard micro average Precision,Recall and balanced Fscore (F1).
To compute sta-tistical significance, we use the approximate ran-domization test (Noreen, 1989).6 In all our exper-iments, we model argument order of the relationsexplicitly.
Thus, for instance for the 7 coarse ACE2004 relations, we build 14 coarse-grained classi-fiers (two for each coarse ACE 2004 relation typeexcept for PER-SOC, which is symmetric, and oneclassifier for the none relation).Data We use two datasets.
To compare ourmodel against the state of the art we use the ACE2004 data.
It contains 348 documents and 4,374positive relation instances.
To generate the train-ing data, we follow prior studies and extract aninstance for every pair of mentions in the same4http://disi.unitn.it/moschitti/Tree-Kernel.htm5http://disi.unitn.it/ikernels/RelationExtraction6http://www.nlpado.de/?sebastian/software/sigf.shtmlsentence, which are separated by no more thanthree other mentions (Zhang et al, 2006; Sun etal., 2011).
After data preprocessing, we obtained4,327 positive and 39,120 negative instances.ACE 2005 docs sents ASL relationsnw+bn 298 5029 18.8 3562bc 52 2267 16.3 1297cts 34 2696 15.3 603wl 114 1697 22.6 677Table 1: Overview of the ACE 2005 data.For the domain adaptation experiments we usethe ACE 2005 corpus.
An overview of the datais given in Table 1.
Note that this data is dif-ferent from ACE 2004: it covers different years(ACE 2004: texts from 2001-2002; ACE 2005:2003-2005).
Moreover, the annotation guidelineshave changed (for example, ACE 2005 contains nodiscourse relation, some relation (sub)types havechanged/moved, and care must be taken for differ-ences in SGM markup, etc.
).More importantly, the ACE 2005 corpus cov-ers additional domains: weblogs, telephone con-versation, usenet and broadcast conversation.
Inthe experiments, we use news (the union of nwand bn) as source domain, and weblogs (wl), tele-phone conversations (cts) and broadcast conversa-tion (bc) as target domains.7 We take half of bcas only target development set, and leave the re-maining data and domains for final testing (sincethey are already small, cf.
Table 1).
To get a feel-ing of how these domains differ, Figure 3 depictsthe distribution of relations in each domain and Ta-ble 2 provides the most frequent out-of-vocabularywords together with their percentage.Lexical Similarity and Clustering We appliedLSA to ukWaC (Baroni et al, 2009), a 2 billionword corpus constructed from the Web8 using thes-space toolkit.9 Dimensionality reduction wasperformed using SVD with 250 dimensions, fol-lowing (Croce et al, 2011).
The co-occurrencematrix was transformed by tfidf.
For the Brownword clusters, we used Percy Liang?s implemen-tation10 of the Brown clustering algorithm (Liang,2005).
We incorporate cluster information by us-7We did not consider the usenet subpart, since it is amongthe smaller domains and data-preprocessing was difficult.8http://wacky.sslmit.unibo.it/9http://code.google.com/p/airhead-research/10https://github.com/percyliang/brown-cluster1502nw_bn bc cts wlARTGEN?AFFORG?AFFPART?WHOLEPER?SOCPHYSDistribution of relations across domains (normalized)DomainProportion0.00.10.20.30.4Figure 3: Distribution of relations in ACE 2005.Dom Most frequent OOV wordsbc(24%)insurance, unintelligible, malprac-tice, ph, clip, colonel, crosstalkcts(34%)uh, Yeah, um, eh, mhm, uh-huh, ?,ah, mm, th, plo, topic, y, workplacewl(49%)title, Starbucks, Well, blog, !
!,werkheiser, undefeated, poor, shitTable 2: For each domain the percentage of targetdomain words (types) that are unseen in the sourcetogether with the most frequent OOV words.ing the 10-bit cluster prefix (Sun et al, 2011; Chanand Roth, 2010).
For the domain adaptation exper-iments, we use ukWaC corpus-induced clusters asbridge between domains.
We limited the vocabu-lary to that in ACE 2005, which are approximately16k words.
Following previous work, we left caseintact in the corpus and induced 1,000 word clus-ters from words appearing at least 100 times.11DA baseline We compare our approach to in-stance weighting (Jiang and Zhai, 2007).
We mod-ified SVM-light-TK such that it takes a parametervector ?i, .., ?m as input, where each ?i representsthe relative importance of example i with respectto the target domain (Huang et al, 2007; Wid-mer, 2008).
To estimate the importance weights,we train a binary classifier that distinguishes be-tween source and target domain instances.
Weconsider the union of the three target domains astarget data.
To train the classifier, the source in-stances are marked as negative and the target in-stances are marked as positive.
Then, this classi-11Clusters are available at http://disi.unitn.it/ikernels/RelationExtractionPrior Work: Type P R F1Zhang (2006), tree only K,yes 74.1 62.4 67.7Zhang (2006), linear K,yes 73.5 67.0 70.1Zhang (2006), poly K,yes 76.1 68.4 72.1Sun & Grishman (2011) F,yes 73.4 67.7 70.4Jiang & Zhai (2007) F,no 73.4 70.2 71.3Our re-implementation: Type P R F1Tree only (PET Zhang) K,yes 70.7 62.5 66.3Linear composite K,yes 71.3 66.6 68.9Polynomial composite K,yes 72.6 67.7 70.1Table 3: Comparison to previous work on the 7 re-lations of ACE 2004.
K: kernel-based; F: feature-based; yes/no: models argument order explicitly.fier is applied to the source data.
To obtain theweights ?i, we convert the SVM scores into pos-terior probabilities by training a sigmoid using themodified Platt algorithm (Lin et al, 2007).126 Results6.1 Alignment to Prior WorkAlthough most prior studies performed 5-foldcross-validation on ACE 2004, it is often not clearwhether the partitioning has been done on the in-stance or on the document level.
Moreover, it isoften not stated whether argument order is mod-eled explicitly, making it difficult to compare sys-tem performance.
Citing Wang (2008), ?We feelthat there is a sense of increasing confusion downthis line of research?.
To ease comparison for fu-ture research we use the same 5-fold split on thedocument level as Sun et al (2011)13 and makeour system publicly available (see Section 5).Table 3 shows that our system (bottom) alignswell with the state of the art.
Our best sys-tem (composite kernel with polynomial expan-sion) reaches an F1 of 70.1, which aligns well tothe 70.4 of Sun et al (2011) that use the same data-split.
This is slightly behind that of Zhang (2006);the reason might be threefold: i) different data par-titioning; ii) different pre-processing; iii) they in-corporate features from additional sources, i.e.
aphrase chunker, dependency parser and semanticresources (Zhou et al, 2005) (we have on aver-age 9 features/instance, they use 40).
Since wefocus on evaluating the impact of semantic simi-larity in tree kernels, we think our system is verycompetitive.
Removing gold entity and mention12Other weightings/normalizations (like LDA) didn?t im-prove the results; best was to take the posteriors and add c.13http://cs.nyu.edu/?asun/pub/ACL11_CVFileList.txt1503information results in a significant F1 drop from66.3% to 54.2%.
However, in a realistic settingwe do not have gold entity info available, espe-cially not in the case when we apply the systemto any kind of text.
Thus, in the domain adapta-tion setup we assume entity boundaries given butnot their label.
Clearly, evaluating the approach onpredicted mentions, e.g.
Giuliano et al (2007), isanother important dimension, however, out of thescope of the current paper.6.2 Tree Kernels with Brown Word ClustersTo evaluate the effectiveness of Brown word clus-ters in tree kernels, we evaluated different instancerepresentations (cf.
Figure 2) on the ACE 2005 de-velopment set.
Table 4 shows the results.bc-dev P R F1baseline 52.2 41.7 46.4replace word 49.7 38.6 43.4replace pos 56.3 41.9 48.0replace pos only mentions 55.3 41.6 47.5above word 54.5 42.2 47.6above pos 55.8 41.1 47.3Table 4: Brown clusters in tree kernels (cf.
Fig 2).To summarize, we found: i) it is generally a badidea to dismiss lexical information completely,i.e.
replacing or ignoring terminals harms perfor-mance; ii) the best way to incorporate Brown clus-ters is to replace the Pos tag with the cluster bit-string; iii) marking all words is generally betterthan only mentions; this is in contrast to Sun etal.
(2011) who found that in their feature-basedsystem it was better to add cluster informationto entity mentions only.
As we will discuss, thecombination of syntax and semantics exploited inthis novel kernel avoids the necessity of restrictingcluster information to mentions only.6.3 Semantic Tree Kernels for DATo evaluate the effectiveness of the proposed ker-nels across domains, we use the ACE 2005 dataas testbed.
Following standard practices on ACE2004, the newswire (nw) and broadcast news (bn)data from ACE 2005 are considered training data(labeled source domain).
The test data consistsof three targets: broadcast conversation, telephoneconversation, weblogs.
As we want to build a sin-gle system that is able to handle heterogeneousdata, we do not assume that there is further unla-beled domain-specific data, but we assume to havea large unlabeled corpus (ukWaC) at our disposalto improve the generalizability of our models.Table 5 presents the results.
In the first threerows we see the performance of the baselinemodels (PET, BOW and BOW without mark-ing).
In-domain (col 1): when evaluated on thesame domain the system was trained on (nw+bn,5-fold cross-validation).
Out-of-domain perfor-mance (cols 2-4): the system evaluated on thetargets, namely broadcast conversation (bc), tele-phone conversation (cts) and weblogs (wl).
Whilethe system achieves a performance of 46.0 F1within its own domain, the performance drops to45.3, 43.4 and 34.0 F1 on the target domains, re-spectively.
The BOW kernel that disregards syn-tax is often less effective (row 2).
We see alsothe effect of target entity marking: the BOW ker-nel without entity marking performs substantiallyworse (row 3).
For the remaining experiments weuse the BOW kernel with entity marking.Rows 4 and 5 of Table 5 show the effect ofusing instance weighting for the PET baseline.Two models are shown: they differ in whetherPET or BOW was used as instance representa-tion for training the discriminative classifier.
In-stance weighting shows mixed results: it helpsslightly on the weblogs domain, but does not helpon broadcast conversation and telephone conversa-tions.
Interestingly, the two models used to obtainthe weights perform similarly, despite the fact thattheir performance differs (F1: 70.5 BOW, 73.5PET); it turns out that the correlation between theweights is high (+0.82).The next part (rows 6-9) shows the effect of en-riching the syntactic structures with either Brownword clusters or LSA.
The Brown cluster ker-nel applied to PET (P WC) improves performanceover the baseline over all target domains.
Thesame holds also for the lexical semantic kernelbased on LSA (P LSA), however, to only two outof three domains.
This suggests that the two ker-nels capture different information and a combinedkernel might be effective.
More importantly, thetable shows the effect of adding Brown clusters orLSA semantics to the BOW kernel: it can actuallyhurt performance, sometimes to a small but othertimes to a considerably degree.
For instance, WCapplied to PET achieves an F1 of 47.0 (baseline:45.3) on the bc domain, while applied to BOW ithurts performance significantly, i.e.
it drops from1504nw+bn (in-dom.)
bc cts wlBaseline: P: R: F1: P: R: F1: P: R: F1: P: R: F1:PET 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.0BOW 55.1 37.3 44.5 57.2 37.1 45.0 57.5 31.8 41.0 41.1 27.2 32.7BOW no marking 49.6 34.6 40.7 51.5 34.7 41.4 54.6 30.7 39.3 37.6 25.7 30.6PET adapted: P: R: F: P: R: F: P: R: F: P: R: F:IW1 (using PET) 51.4 44.1 47.4 49.1 41.1 44.7 50.8 37.5 43.1 35.5 33.9 34.7IW2 (using BOW) 51.2 43.6 47.1 49.1 41.3 44.9 51.2 37.8 43.5 35.6 33.8 34.7With Similarity: P: R: F1: P: R: F1: P: R: F1: P: R: F1:P WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.0B WC 47.9 36.4 41.4 49.5 35.2 41.2 53.3 33.2 40.9 31.7 24.1 27.4P LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.3B LSA 53.7 37.8 44.4 55.1 33.8 41.9 54.9 32.3 40.7 39.2 28.6 33.0P+P WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.3P+P LSA 52.7 46.6 49.5 53.9 45.2 49.2 49.9 37.6 42.9 37.9 38.3 38.1P+P WC+P LSA 55.1 45.9 50.1 55.3 43.1 48.5?
53.1 37.0 43.6 39.9 35.8 37.8?Table 5: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005.PET and BOW are abbreviated by P and B, respectively.
If not specified BOW is marked.45.0 to 41.2.
This is also the case for LSA ap-plied to the BOW kernel, which drops to 41.9.
Onthe cts domain this is less pronounced.
Only onthe weblogs domain B LSA achieves a minor im-provement (from 32.7 to 33.0).
In general, dis-tributional semantics constrained by syntax (i.e.combined with PET) can be effectively exploited,while if applied ?blindly?
?
without the guide ofsyntax (i.e.
BOW) ?
performance might drop, of-ten considerably.
We believe that the semantic in-formation does not help the BOW kernel as there isno syntactic information that constrains the appli-cation of the noisy source, as opposed to the casewith the PET kernel.As the two semantically enriched kernels,PET LSA and PET WC, seem to capture differentinformation we use composite kernels (rows 10-11): the baseline kernel (PET) summed with thelexical semantic kernels.
As we can see, resultsimprove further: for instance on the bc test set,PET WC reaches an F1 of 47.0, while combinedwith PET (PET+PET WC) this improves to 48.3.Adding also PET LSA results in the best perfor-mance and our final system (last row): the com-posite kernel (PET+PET WC+PET LSA) reachesan F1 of 48.5, 43.6 and 37.8 on the target domains,respectively, i.e.
with an absolute improvement of:+3.2%, +0.2% and +3.8%, respectively.
Two outof three improvements are significant at p < 0.05(indicated by ?
in Table 5).
Moreover, the systemalso improved in its own domain (first column),therefore having achieved robustness.By performing an error analysis we found that,for instance, the Brown clusters help to general-ize locations and professions.
For example, thebaseline incorrectly considered ?Dutch filmmaker?in a PART-WHOLE relation, while our systemcorrectly predicted GEN-AFF(filmmaker,Dutch).?Filmmaker?
does not appear in the source, how-ever ?Dutch citizen?
does.
Both ?citizen?
and ?film-maker?
appear in the same cluster, thereby helpingthe system to recover the correct relation.bc cts wlRelation: BL SYS BL SYS BL SYSPART-WHOLE 37.8 43.1 59.3 52.3 30.5 36.3ORG-AFF 60.7 62.9 35.5 42.3 41.0 42.0PHYS 35.3 37.6 25.4 28.7 25.2 26.9ART 20.8 37.9 34.5 43.5 26.5 40.3GEN-AFF 30.1 33.0 16.8 18.6 21.6 28.1PER-SOC 74.1 74.2 66.3 63.1 42.6 48.0?
average 45.3 48.5 43.4 43.6 34.0 37.8Table 6: F1 per coarse relation type (ACE2005).
SYS is the final model, i.e.
last row(PET+PET WC+PET LSA) of Table 5.Furthermore, Table 6 provides the performancebreakdown per relation for the baseline (BL) andour best system (SYS).
The table shows that oursystem is able to improve F1 on all relations forthe broadcast and weblogs data.
On most rela-tions, this is also the case for the telephone (cts)data, although the overall improvement is not sig-nificant.
Most errors were made on the PER-SOC1505relation, which constitutes the largest portion ofcts (cf.
Figure 3).
As shown in the same figure,the relation distribution of the cts domain is alsorather different from the source.
This conversationdata is a very hard domain, with a lot of disflu-encies and spoken language patterns.
We believeit is more distant from the other domains, espe-cially from the unlabeled collection, thus other ap-proaches might be more appropriate, e.g.
domainidentification (Dredze et al, 2010).7 Conclusions and Future WorkWe proposed syntactic tree kernels enriched bylexical semantic similarity to tackle the portabil-ity of a relation extractor to different domains.The results of diverse kernels exploiting (i) Brownclustering and (ii) LSA show that a suitable com-bination of syntax and lexical generalization isvery promising for domain adaptation.
The pro-posed system is able to improve performance sig-nificantly on two out of three target domains (upto 8% relative improvement).
We compared it toinstance weighting, which gave only modest orno improvements.
Brown clusters remained un-explored for kernel-based approaches.
We sawthat adding cluster information blindly might ac-tually hurt performance.
In contrast, adding lex-ical information combined with syntax can helpto improve performance: the syntactic structureenriched with lexical information provides a fea-ture space where syntax constrains lexical similar-ity obtained from unlabeled data.
Thus, seman-tic syntactic tree kernels appear to be a suitablemechanism to adequately trade off the two kindsof information.
In future we plan to extend theevaluation to predicted mentions, which necessar-ily includes a careful evaluation of pre-processingcomponents, as well as evaluating the approach onother semantic tasks.AcknowledgmentsWe would like to thank Min Zhang for discus-sions on his prior work as well as the anony-mous reviewers for their valuable feedback.
Theresearch described in this paper has been sup-ported by the European Community?s SeventhFramework Programme (FP7/2007-2013) underthe grant #288024: LIMOSINE ?
LinguisticallyMotivated Semantic aggregation engiNes.ReferencesMarco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky Wide Web:A Collection of Very Large Linguistically ProcessedWeb-Crawled Corpora.
Language Resources andEvaluation, pages 209?226.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain Adaptation with Structural Corre-spondence Learning.
In Conference on EmpiricalMethods in Natural Language Processing, Sydney,Australia.Stephan Bloehdorn and Alessandro Moschitti.
2007a.Combined syntactic and semantic kernels for textclassification.
In ECIR, pages 307?318.Stephan Bloehdorn and Alessandro Moschitti.
2007b.Exploiting Structure and Semantics for ExpressiveText Kernels.
In Conference on Information Knowl-edge and Management, Lisbon, Portugal.Peter F. Brown, Peter V. deSouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-Based n-gram Models of Natural Language.Computational Linguistics, 18:467?479.Razvan C. Bunescu.
2007.
Learning to extract rela-tions from the web using minimal supervision.
InProceedings of ACL.Yee Seng Chan and Dan Roth.
2010.
Exploitingbackground knowledge for relation extraction.
InProceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages152?160, Beijing, China, August.
Coling 2010 Or-ganizing Committee.Michael Collins and Nigel Duffy.
2001.
Convolu-tion Kernels for Natural Language.
In Proceedingsof Neural Information Processing Systems (NIPS2001).Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Semantic convolution kernels overdependency trees: smoothed partial tree kernel.
InCIKM, pages 2013?2016.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof the 42nd Annual Meeting on ACL, Barcelona,Spain.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofACL, pages 256?263, Prague, Czech Republic, June.Mark Dredze, Tim Oates, and Christine Piatko.
2010.We?re not in kansas anymore: Detecting domainchanges in streams.
In Proceedings of EMNLP,pages 585?595, Cambridge, MA.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2007.
Relation extraction and the influenceof automatic named-entity recognition.
ACM Trans.Speech Lang.
Process., 5(1):2:1?2:26, December.1506G.
Golub and W. Kahan.
1965.
Calculating the singu-lar values and pseudo-inverse of a matrix.
Journal ofthe Society for Industrial and Applied Mathematics:Series B, Numerical Analysis, 2(2):pp.
205?224.Zellig Harris.
1964.
Distributional structure.
In Jer-rold J. Katz and Jerry A. Fodor, editors, The Philos-ophy of Linguistics.
Oxford University Press.Jiayuan Huang, Arthur Gretton, Bernhard Scho?lkopf,Alexander J. Smola, and Karsten M. Borgwardt.2007.
Correcting sample selection bias by unlabeleddata.
In In NIPS.
MIT Press.Jing Jiang and Chengxiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In In ACL2007, pages 264?271.Jing Jiang.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In Proceed-ings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th IJCNLP, pages1012?1020, Suntec, Singapore.Percy Liang.
2005.
Semi-Supervised Learning forNatural Language.
Master?s thesis, MassachusettsInstitute of Technology.Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.2007.
A note on platt?s probabilistic outputs forsupport vector machines.
Mach.
Learn., 68(3):267?276.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, pages 419?444.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings ofACL-IJCNLP, pages 1003?1011, Suntec, Singapore,August.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow semantic parsing.
In Proceed-ings of the 42nd Meeting of the ACL, Barcelona,Spain.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Proceedings of the 17th ECML, Berlin, Germany.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM, pages 253?262.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution kernels onconstituent, dependency and sequential structuresfor relation extraction.
In Proceedings of EMNLP?09, pages 1378?1387, Stroudsburg, PA, USA.J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nilsson,S.
Riedel, and D. Yuret.
2007.
The CoNLL 2007shared task on dependency parsing.
In Proceed-ings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
Notes ofthe First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Anders S?gaard and Martin Haulrich.
2011.Sentence-level instance-weighting for graph-basedand transition-based dependency parsing.
In Pro-ceedings of the 12th International Conference onParsing Technologies, IWPT ?11, pages 43?47,Stroudsburg, PA, USA.Anders S?gaard and Anders Johannsen.
2012.
Robustlearning in random subspaces: equipping NLP forOOV effects.
In Proceedings of Coling.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised relation extraction with large-scaleword clustering.
In Proceedings of ACL-HLT, pages521?529, Portland, Oregon, USA.Mengqiu Wang.
2008.
A re-examination of depen-dency path kernels for relation extraction.
In Pro-ceedings of the 3rd International Joint Conferenceon Natural Language Processing-IJCNLP.Christian Widmer.
2008.
Domain adaptation insequence analysis.
Diplomarbeit, University ofTu?bingen.Feiyu Xu, Hans Uszkoreit, Hond Li, and Niko Felger.2008.
Adaptation of relation extraction rules to newdomains.
In Proceedings of LREC?08, Marrakech,Morocco.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2002.
Kernel methods for relationextraction.
In Proceedings of EMNLP-ACL, pages181?201.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,and Chew Lim Tan.
2005.
Discovering relationsbetween named entities from a large raw corpus us-ing tree similarity-based clustering.
In Proceedingsof IJCNLP?2005, pages 378?389, Jeju Island, SouthKorea.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In Proceedings of COLING-ACL 2006, pages 825?832.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Annual Meetingof ACL), pages 427?434, Ann Arbor, Michigan.1507
