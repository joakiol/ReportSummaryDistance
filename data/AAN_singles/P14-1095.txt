Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1009?1019,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLanguage-Aware Truth Assessment of Fact CandidatesNdapandula NakasholeCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA, 15213ndapa@cs.cmu.eduTom M. MitchellCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA, 15213tom.mitchell@cs.cmu.eduAbstractThis paper introduces FactChecker,language-aware approach to truth-finding.FactChecker differs from prior approachesin that it does not rely on iterative peervoting, instead it leverages language toinfer believability of fact candidates.
Inparticular, FactChecker makes use of lin-guistic features to detect if a given sourceobjectively states facts or is speculativeand opinionated.
To ensure that factcandidates mentioned in similar sourceshave similar believability, FactCheckeraugments objectivity with a co-mentionscore to compute the overall believabilityscore of a fact candidate.
Our experimentson various datasets show that FactCheckeryields higher accuracy than existingapproaches.1 IntroductionTruth-finding algorithms aim to separate truestatements (facts) from false information.
Morespecifically, given a set of statements whose truth-fulness is unknown (fact candidates), the key goalof truth-finding algorithms is to generate a rankingsuch that true statements are ranked ahead of falseones.
Truth-finders have the potential to address amajor obstacle on the Web: the problem of sourcesspreading inaccurate and conflicting information.This problem continues to grow with the develop-ment of tools for easy Web authorship.
Blogs, fo-rums and social networking websites are not sub-ject to traditional journalistic standards.
Conse-quently, the accuracy of information reported bythese sources is often unclear.
Even more estab-lished newspapers and websites may sometimesreport false information as they race to break sto-ries.
Therefore, truth-finding is becoming an in-creasingly important problem.
Information extrac-tion projects aim to distill relational facts from nat-ural language text (Auer et al, 2007; Bollacker etal., 2008; Carlson et al, 2010; Fader et al, 2011;Nakashole et al, 2011; Del Corro and Gemulla,2013).
These projects have produced knowledgebases containing many millions of relational factsbetween entities.
However, despite these impres-sive advances, there are still major limitations re-garding precision.
Within the context of informa-tion extraction, fact extractors assign confidencescores to extracted facts.
However, such scoresare often tied to the extractor?s ability to read andunderstand natural language text.
This is differ-ent from a score that indicates the degree to whicha given fact candidate is believable.
Such a be-lievability score is sometimes also referred to asa credibility score or truthfulness score.
The be-lievability score reflects the likelihood that a givenstatement is true.
Truth-finding algorithms aim tocompute this score for each fact candidate.Prior truth-finding methods are mostly based oniterative voting, where votes are propagated fromsources to fact candidates and then back to sources(Yin et al, 2007; Galland et al, 2010; Paster-nack and Roth, 2010; Li et al, 2011; Yin andTan, 2011).
At the core of iterative voting is theassumption that candidates mentioned by manysources are more likely to be true.
However, ad-ditional aspects of a source influence its trustwor-thiness, besides external votes.Our goal is to accurately assess truthfulness offact candidates by taking into account the lan-guage of sources that mention them.
A Mechan-ical Turk study we carried out revealed that thereis a significant correlation between objectivity oflanguage and trustworthiness of sources.
Objec-tivity of language refers to the use of neutral,impartial language, which is not personal, judg-mental, or emotional.
Trustworthiness refers to1009a source of information being reliable and truth-ful.
We use linguistics features to detect if a givensource objectively states facts or is speculativeand opinionated.
Additionally, in order to ensurethat fact candidates mentioned in similar sourceshave similar believability scores, our believabilitycomputation model incorporates influence of co-mentions.
However, we must avoid falsely boost-ing co-mentioned fact candidates.
Our model ad-dresses potential false boosts in two ways: first, byensuring that co-mention influence is only propa-gated to related fact candidates; second, by ensur-ing that the degree of co-mention influence is de-termined by the trustworthiness of the sources inwhich co-mentions occur.The contribution of this paper is a language-aware truth-finding approach.
More precisely,we make the following contributions: (1) Al-ternative Fact Candidates: Truth-finders rank agiven fact candidate with respect to its alter-natives.
For example, alternative places whereBarack Obama could have been born.
Virtuallyall existing truth-finders assume that the alterna-tives are provided.
In contrast, we developed amethod for generating alternative fact candidates.
(2) Objectivity-Trustworthiness Correlation: Wehypothesize that objectivity of language and trust-worthiness of sources are positively correlated.
Totest this hypothesis, we designed a MechanicalTurk study.
The study showed that this correlationdoes in fact hold.
(3) Objectivity Classifier: Us-ing labeled data from the Mechanical Turk study,we developed and trained an objectivity classifierwhich performed better than prior proposed lexi-cons from literature.
(4) Believability Computa-tion: We developed FactChecker, a truth-findingmethod that linearly combines objectivity and co-mention influence.
Our experiments showed thatFactChecker outperforms prior methods.2 Fact CandidatesIn this section, we formally define what constitutesa fact candidate and describe how we go aboutunderstanding semantics of fact candidates.
Wethen present our approach for generating alterna-tive fact candidates.2.1 RepresentationThe triple format is the most common representa-tion of facts in knowledge bases.
A formal specifi-cation of the triple format is presented in the RDFprimer1.
In RDF, data is represented as subject-predicate-object (SPO) triples.
In this work, werestrict predicates to verbs (or verbal phrases suchas ?plays for?, ?graduated from?, etc.).
Litera-ture on automatic relation discovery (Fader et al,2011) has shown that verbal phrases uncover alarge fraction of binary predicates while reducingthe amount of noisy phrases that do not denote anyrelations.
Therefore, we define a fact candidate asfollows:Definition 1 (Fact Candidate) A fact candidatefiis an ?S?
V ?O?
triple; where S is the subject,V is a verbal phrase, and O is the object.
We aimto compute the truthfulness of fi, ?
(fi) ?
{T, F},where T and F stand for true and false, respec-tively.Note that in this paper we are interested in caseswhere ?
(fi) is either T or F .
That is, we assesstruthfulness of factual statements and not opinionswhose truthfulness is often both T and F to somedegree.
For example, the triples: ?Obama?
born in?Kenya?
and ?Obama?
graduated from ?Harvard?are valid fact candidates.
However, the triple:?Obama?
deserves ?Nobel Peace Prize?
is not.2.2 SemanticsBased on the SVO triple, the meaning of a factcandidate can be unclear and ambiguous.
There-fore, we first determine the semantics of a fact can-didate before computing its truthfulness.Entity Types.
We first determine the expectedtypes of the subject and object in the SVO.
For ex-ample, for the SVO ?Einstein?
died in ?Princeton?,the expected types are person ?
location.
We de-termine this by first computing the types of en-tities that are valid for each verb (verbal phrase)in a large SVO collection of 114m SVO triples(Talukdar et al, 2012).
Typing verbal phrasesis a once-off computation.
Our phrase typingmethod is similar to prior work on typing rela-tional phrases (Nakashole et al, 2012).
Exam-ples of typed phrases are: ?person?
died in ?year?,?person?
died in ?location?, and ?athlete?
plays for?team?.
Given a triple, we look up the types for thesubject and the object and then determine whichof the typed phrases are compatible with the cur-rent triple.
We look up entity types in a knowledge1http://www.w3.org/TR/rdf-primer/1010base containing entities and their types.
In partic-ular, we use the NELL entity typing API (Carlsonet al, 2010).
NELL?s entity typing method hashigh recall because when entities are not in theknowledge base, it performs on-the-fly type infer-ence using the Web.
This is not the case for otheroptions such as (Auer et al, 2007; Bollacker et al,2008; Hoffart et al, 2011).Relation Cardinality.
Next, we learn cardinali-ties of verbal phrases.
Cardinality refers to howarguments of a given relation relate to one anothernumerically.
We define the relation cardinality of averb Card(V ), as the average number of expectedarguments per given subject.
For example, for therelation ?died in?, 1 location is expected for eachsubject.
For other relations, the expected numberof arguments can be greater than 1 but less thann : n ?
R, n > 1.
We approximate n usingstatistics from the 114m SVO corpus based on theaverage number of arguments per given first argu-ment.
In a once-off computation, we generate car-dinality approximations per typed verbal phrase Vand its inverse V?1.
For example, we generatethe cardinality estimates for both: ?person?
died in?location?
and for ?location?
INVERSE-OF(diedin) ?person?.Synonymous Relations.
Natural language is di-verse.
Semantically similar phrases can be syntac-tically different.
Therefore, we learn other verbsthat can be used to substitute V in SVO.
Wepre-compute synonymous phrases from the 114mSVO corpus using distributional semantics in thesame spirit as (Lin and Pantel, 2001; Nakashole etal., 2012).Synonymous verbs, relation cardinalities, andentity types enable us to generate alternative factcandidates.2.3 Alternative Fact CandidatesTruth-finding methods rank firelative to alter-native candidates.
While prior methods assumethe alternatives are known apriori, we developeda method for generating alternative fact candi-dates.
For a given fi, we first identify the fixedargument.
The fixed argument is the argument ofthe SVO which when fixed, requires finding thefewest number of alternative candidates.
For ex-ample, for ?Einstein?
died in ?Princeton?, the so-lution is to fix the subject.
This is because the car-dinality of ?person?
died in ?location?
is one (1).On the other hand, the cardinality of ?INVERSE-OF(died in)?
is many(n).
In other words, the num-ber of places where a person can be born (one)is much fewer than the number of people thatcan die in a place (many).
In our example, al-ternatives are possible places, other than Prince-ton, where Einstein could have died.
For example:?Einstein?
died in ?Germany?
or ?Einstein?
died in?Switzerland?.
More generally, the fixed argumentof fact candidate fi, is defined as follows:Definition 2 (Fixed Argument) Let Card(V) bethe cardinality of V and Card(V?1) be the car-dinality of the inverse of V , if Card(V ) <Card(V?1), then the fixed argument is the sub-ject, Argfixed(fi) = S, else it is the object, O. IfCard(V ) == Card(V?1), then both argumentsare fixed, one at a time.We use the fixed argument to define a topic as thefixed argument plus the verb.
Therefore, for theSVO ?X?
died in ?Y?, the topic ?places where Xdied?, (Argfixed= S), is not the same as the topic?people who died in Y?
(Argfixed= O).To locate alternatives, we use the topic(Argfixed+ V ) as a query.
We search threesources to either locate relevant documents or rele-vant triples: the Google Web search API, the 114mSVO collection, and the NELL KB.
The SVO col-lection and the KB return triples, however, theWeb search API returns documents.
Therefore,we apply a triple extractor to the retrieved docu-ments.
For all potential alternative triples, we per-form type checking to ensure that the argumentsof the triples are type-compatible with fi.
Further-more, we generate an additional query for everysynonymous verb sVi, replacing V with sVi.
Ex-ample queries are: ?Einstein died in?, ?Einsteinpassed in?, etc.3 Objectivity and TrustworthinessThe principle of objective journalism, which isa significant part of journalistic ethics, aims topromote factual and fair reporting, undistorted byemotion or personal bias (Schudson, 1978; Ka-plan, 2002).
Objectivity is also required in refer-ence sources such as encyclopedias, scientific pub-lications, and textbooks.
For example, Wikipediaenforces a neutral point-of-view policy (NPOV)2.Articles violating the NPOV policy are marked2http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view1011to indicate potential bias.
While opinions, emo-tions, and speculations can also be expressed us-ing objective language, they are often stated usingsubjective language (Turney et al, 2002; Riloffand Wiebe, 2003; Yu and Hatzivassiloglou, 2003;Wiebe et al, 2004; Liu et al, 2005; Recasens etal., 2013).
For example, consider the followingpieces of text:(S) Well, I think Obama was born in Kenyabecause his grandma who lives in Kenya saidhe was born there.
(O) Theories allege that Obama?s publishedbirth certificate is a forgery, that his actualbirthplace is not Hawaii but Kenya.Text S is a snippet from Yahoo Answers andtext O is a snippet from the Wikipedia page ti-tled: ?Barack Obama Citizenship Conspiracy The-ories?.
S is subjective, expressing the opinion ofthe author.
On the other hand, O is objective, stat-ing only what has been alleged.
Literature on sen-timent analysis (Turney et al, 2002; Liu et al,2005), subjectivity detection (Riloff and Wiebe,2003; Wiebe et al, 2004), and bias detection (Yuand Hatzivassiloglou, 2003; Recasens et al, 2013)has developed lexicons for identifying subjectivelanguage.
Due to the principle of objective jour-nalism and the requirement of objectivity placedon reference sources, we hypothesize a link be-tween objectivity and trustworthiness as follows.Hypothesis 1 Objective sources are more trust-worthy than subjective sources.
Therefore, wecan assume that fact candidates stated in objec-tive sources are more likely to be true than thosestated in subjective sources.To test the validity of the hypothesis, we carriedout a study where we solicited human input.3.1 Mechanical Turk StudyWe deployed an annotation study on Amazon Me-chanical Turk (MTurk)3, a crowd-sourcing plat-form for tasks requiring human input.
Tasks onMTurk are small questionnaires consisting of a de-scription and a set of questions.
Our study con-sisted of two independent tasks.
The first task wastitled ?Trustworthiness of News Articles?, whereannotators were given a link to a news article and3http://www.mturk.comFigure 1: Summary of the results of the annotationstudy on objectivity and trustworthiness.asked to judge if they thought it was trustworthyor not.
The second task was titled ?Objectivityof News Articles?.
For this task, annotators wereasked to judge if a given article is objective or sub-jective.
For both tasks a third option of ?not sure?was provided.
We randomly selected 500 news ar-ticles from a corpus of about 300,000 news articlesobtained from Google News from the topics ofTop News, Business, Entertainment, and SciTech.For each task, every article was judged by threeannotators.
This produced a total of 3000 annota-tions.
When we analyzed the output, we accepteda label as valid for a given article if the label wasselected by the majority of the judges.
Based onthis criteria, we obtained a set of 420 articles thatwere both labeled for trustworthiness and objec-tivity.A summary of the outcome of the study isshown in Figure 1; 74% of the untrustworthyarticles were independently labeled as subjec-tive.
On the other hand, 64% of trustworthyarticles were independently labeled as objective.These results indicate a non-trivial positive cor-relation between objectivity and trustworthiness.We leverage this correlation in our believabilitycomputation model.
To incorporate objectivity inFactChecker, we require for a given source docu-ment, an objectivity score ?
[0, 1], where 0 meansthe source is subjective and 1 means it is objec-tive.
Next, describe our method for automaticallydetermining objectivity of sources.3.2 Automatic Objectivity DetectionWe trained a logistic regression classifier to pre-dict the objectivity of a document.
For trainingand testing data, we used the labeled data fromthe Mechanical Turk study.
We additionally usedlabeled text from prior work on subjectivity de-tection (Pang and Lee, 2004).
This resulted in atotal of 4, 600 documents, half subjective and theother half objective.
We used 4000 documents for1012# Feature1 Subjectivity lexicon of strong and weaksubjective words (Riloff and Wiebe,2003).2 Sentiment lexicon of positive and negativewords (Liu et al, 2005).3 Wikipedia-derived bias lexicon (Recasenset al, 2013).4 Part-of-speech (POS) tags5 Frequent bi-gramsTable 1: Features used for the objectivity detector.training, 2000 per label.
The rest of the documentswere split into a development set (380) and a testset (220).A summary of the features we used is shownin Table 1.
Features 1-3 refer to lexicons devel-oped by prior methods on subjectivity (Wiebe etal., 2004), sentiment analysis (Liu et al, 2005) andbias detection (Recasens et al, 2013).
Feature 4refers to part-of-speech tags of the terms found inthe document that are also in the lexicons.
Feature5 refers to bi-grams that frequently occur (men-tion frequency of > 10) in the 4, 600 documents.The most contributing features were the lexicons,features (1-3) and the frequent bi-grams, feature5.
We discovered that using frequent bi-gram fea-tures instead of uni-grams or bi-grams resulted inhigher precision.
The classifier was able to de-termine that for example bi-grams such as ?thinkthat?, ?so funny?
and ?you thought?
are negativefeatures for objectivity.
Evaluation results of ourobjectivity detector vs. baselines are shown in Ta-ble 2.
FactChecker?s objectivity detector has pre-cision of 0.7814 ?
0.0539, with a 0.9-confidenceWilson score interval (Brown et al, 2001) and thisoutperforms the baselines.
Next, we describe howwe leverage objectivity into FactChecker?s truth-fulness model.4 Believability Computation ModelFactChecker computes the believability score of afact candidate from its: i) objectivity score and(ii) co-mention score.
In this section we defineeach of these scores.The objectivity score reflects the trustworthi-ness of sources where a fact candidate is men-tioned.
Given a fact candidate fi, mentioned ina set of documents Di, where each document d ?Approach AccuracySentiment Lexicon 0.65?0.06Wikipedia bias Lexicon 0.69?0.06Subjectivity Lexicon 0.70?0.06FC-Objectivity Detector 0.78?0.05Table 2: Accuracy of the objectivity detector.Dihas objectivity O(d), fi?s objectivity score isdefined as follows:Definition 3 (Objectivity Score)O(fi) = log|Di|.
?dk?DiO(dk)|Di|(1)We do not use the sum of objectivity of sourcesas the objectivity score because this enables factcandidates mentioned in many low objectivitysources to have high aggregate objectivity.
Sim-ilarly, we avoid using average objectivity of thesources as it overestimates objectivity of candi-dates stated in few sources.
A candidate men-tioned in 10 sources with 0.9 objectivity shouldhave higher objectivity than a candidate stated in1 source of 0.9 objectivity.
In Equation 1, log|Di|addresses this issue.The co-mention score aims to ensure that factcandidates mentioned in similar sources have sim-ilar believability scores.
Suppose candidate fiismentioned in many highly objective sources, an-other candidate fjis stated in only one highly ob-jective source dkwhere fiis also mentioned.
Thenthe believability of fjshould be boosted by it be-ing co-mentioned with fi.
If on the other hand fiand fjwere co-mentioned in a subjective source,fjshould receive less boost from fi.
This leads usto the co-mention score ?
(fi) of a candidate.Definition 4 (Co-Mention Score)?
(fi) = ?
(fi) +?fj?Fwij?
(fj) (2)Where ?
(fi) is the normalized mention fre-quency of fi.
The propagation weight wijcontrolshow much boost is obtained from a co-mentionedcandidate.
We define propagation weight, wij, asthe average of the objectivity of the sources thatmention both candidates.wij= average O(dk) : dk?
(Di?Dj) (3)1013where O(dk) is the objectivity of document dk,Diand Djare the sets of documents that mentionfiand fj, respectively.
Notice that we could boostco-mentioned but not related candidates, therebycausing false boosts.
To remedy this, we only al-low wijto be greater than zero if the fact can-didates fiand fjare on the same topic.
Recallthat the topic is determined by the fixed argument(Definition 2) and the verb.
Allowing only factcandidates on the same topic to influence eachother is important considering that many trivialfacts are often repeated in sources of diverse qual-ity.To leverage the inter-dependencies among re-lated co-mentioned fact candidates, we model thesolution with a graph ranking method.
Each factcandidate is a node and there is an edge betweeneach pair of related fact candidate nodes fiandfj, with wijas the edge weight.
Thus, equation 2can be reformulated as ?
= M?, where ?
is theco-mention score vector and M is a Markov ma-trix which is stochastic, irreducible and aperiodic.Thus, a power method will converge to a solutionin a similar manner to PageRank.
Implementationconsists of iteratively applying Equation 2 untilthe change in the score is less than a threshold .The solution is the final co-mention scores of factcandidates.Finally, to compute the believability score of afact candidate, we linearly combine its objectivityscore with its co-mention as follows:Definition 5 (Believability Score)?
(fi) = ?O(fi) + (1?
?)?
(fi) (4)Where ?
is a weighting parameter ?
[0, 1]which controls the relative importance of the twoaspects of FactChecker.
As we show in our exper-iments, ?
can be robustly chosen within the rangeof 0.2 to 0.6.
In our experiments we used ?
= 0.6.The entire procedure of FactChecker is summa-rized in Algorithm 1.5 EvaluationWe evaluated FactChecker for accuracy.
We de-fine accuracy as the probability of a true fact can-didate having a higher believability score than afalse candidate.
Let ?
(fi) ?
{T, F} be the truth-fulness of a fact candidate fi, accuracy is definedas:Algorithm 1 FactCheckerInput: A set F of fact candidatesInput: KB K, SVO corpus C, WebWOutput: A set L of rankings ?fi?
FL = ?while F 6= ?
dopick fifrom FA= getAlternatives(fi,K,C,W)PriorityQueue Li= ?for all alternative fact candidates f?j?
A do?
(f?j) = getBelievabilityScore(f?j)Li.insert(f?j, ?
(f?j))end for?
(fi) = getBelievabilityScore(fi)Li.insert(fi, ?
(fi))L ?
LiRemove fifrom Fend whilereturn LAcc =?(?
(fi)=T :?
(fj)=F )(?
(fi) > ?(fj))|{?
(fi, fj) : ?
(fi) = T ?
?
(fj) = F}|Datasets.
We evaluated FactChecker on threedatasets: i) KB Fact Candidates: The first datasetconsists of fact candidates taken from the fact ex-traction pipeline of a state-of-the-art knowledgebase, NELL (Carlson et al, 2010).
The fact candi-dates span four different relation types: companyacquisitions, book authors, movie directors andathlete teams.
For each fact candidate, we appliedour alternative candidate generation method.
Weonly considered fact candidates with non-trivialalternative candidate sets; where the alternativecandidate set is greater than zero.
Since all ofthe baselines we compared against assume alter-natives are provided, we apply all methods to thesame set of alternative fact candidates discoveredby our method.
Details of this dataset are shownas rows starting with ?KB-?
in Table 3.ii) Wikipedia Fact Candidates: For the sec-ond dataset, we did not restrict the fact candidatesto specific topics from a knowledge base, insteadwe aimed to evaluate all fact candidates about agiven entity.
We selected entities from Wikipe-dia.
For this, we chose US politicians: all currentstate senators, all current state governors, and all44 presidents.
First, we extracted fact candidates1014#Candidates #AlternativesKB-Acquisitions 50 241KB-Authors 50 295KB-Directors 50 228KB-Teams 40 162WKP Politicians 54 219GK Quiz 18 72Table 3: Fact candidate datasets.from the infoboxes of the Wikipedia pages of theentities.
Second, we applied our alternative can-didate generation method to discover alternativesfrom the Web, SVO corpus, and NELL.
Details ofthe resulting dataset are shown in the row ?WKPPoliticians?
in Table 3.iii) General Knowledge Quiz: The thirddataset consists of questions from a generalknowledge quiz4.
We selected questions fromthe inventions category.
Questions are multiplechoice, with 4 options per question.
Thus, fromeach question, we created one fact candidate and3 alternative candidates.
Details of the resultingdataset are shown in the row ?KWP Quiz?
in Ta-ble 3.Baselines.
We compared FactChecker against fivebaselines: i) Vote counts the number of sourcesthat mention the fact candidate.
ii) TruthFinder isan iterative voting approach where votes are prop-agated from sources to fact candidates and thenback to sources.
Implemented as described in (Yinet al, 2007).
iii) Investment is also based on tran-sitive voting, however scores are updated differ-ently.
A source gets a vote of trust from eachcandidate it ?invests?
in, but the vote is weightedby the proportion of trust the source previously?invested?
in the candidate relative to other in-vestors.
Implemented as described in (Pasternackand Roth, 2010).
iv) PooledInvest is a variationof investment, we report both because in their pa-per, there was no clear winner among the two vari-ations.
v) 2-Estimates is a probabilistic modelwhich approximates error rates of sources and factcandidates (Galland et al, 2010).5.1 Accuracy on KB Fact CandidatesFigure 2 shows accuracy on KB fact candidates.FactChecker achieves accuracy between 70% and88% and is significantly more accurate than the4http://www.indiabix.com/general-knowledge/Figure 2: Accuracy of KB fact candidates.Figure 3: FactChecker variations.other approaches on all relations except com-pany acquisitions.
On book authors, movie di-rectors, and athlete teams, FactChecker outper-forms all other approaches by at least 10%, 9%,and 8% respectively.
On company acquisitions,the different methods achieve similar accuracy,with TruthFinder being the most accurate andFactChecker is 4% behind.
Company acquisitionsalso yield the lowest difference between Vote andthe highest performing method, of 6%.
For bookauthors, movie directors, and athlete teams, thedifference between majority Vote and the highestperforming method (FactChecker in this case) is13%, 12%, and 13% respectively.5.2 Accuracy of FactChecker VariationsTo quantify how various aspects of our approachaffect overall performance, we studied two varia-tions.
The first variation is FC-Objectivity whichonly uses objectivity to compute believability.Thus, ?
= 1 in Definition 5.
The second varia-tion is FC-CoMention which only uses co-mentionscores to compute believability, ?
= 0.
The1015Approach WKP Politicians GK QuizVote 0.85?0.09 0.82?0.15TruthFinder 0.85?0.09 0.82?0.152-Estimates 0.85?0.09 0.82?0.15Investment 0.86?0.08 0.82?0.15PooledInvest 0.85?0.09 0.82?0.15FC-Objectivity 0.88?0.08 0.87?0.12FC-CoMention 0.85?0.09 0.72?0.18FactChecker 0.90?0.07 0.87?0.12Table 4: Accuracy on politicians and quiz data setslast variation is the full FactChecker method us-ing both objectivity and co-mentions with ?
= 0.6From Figure 3, it is clear that both the objectiv-ity of sources and the influence of co-mentionscontribute to the overall accuracy of FactChecker.Full-fledged FactChecker performs better thanboth variations.
In most cases, FC-Objectivity per-forms better than FC-CoMention.5.3 Accuracy on Wikipedia Fact CandidatesTable 4, column ?WKP Politicians?, shows ac-curacy on Wikipedia fact candidates, with a 0.9-confidence Wilson score interval (Brown et al,2001).
For this dataset we again see FactCheckeroutperforming the other methods under compari-son.
On this dataset, FactChecker has a accuracyof 0.9 ?
0.07 and a 5% accuracy advantage overthe other methods.
The second best performancecomes from the FC-Objectivity variation, with ac-curacy of 0.88?
0.08.5.4 Accuracy on General Knowledge QuizTable 4, column ?GK Quiz ?, shows accuracy onthe general knowledge quiz fact candidates.
Onthis dataset, FactChecker and its objectivity-onlyvariation (FC-objectivity) have the highest accu-racy of 87%.
Notice that this dataset was the onlyone where we did not generate the alternative factcandidates.
Instead, we took the options of themultiple choice questions as alternatives.
Sincethe quiz is meant to be taken by humans, the alter-natives are often very close, plausible answers.
Yeteven in this difficult setting, we see FactCheckeroutperforming the baselines.Sample fact candidates, with ranked alternativesfrom all three datasets are shown in Table 5.Figure 4: Effect of ?
of FactChecker.5.5 Parameter SensitivityWe analyzed the effect of the selection of lambda?
(see Definition 5) on FactChecker?s perfor-mance.
The result of this analysis is shown in Fig-ure 4.
FactChecker is insensitive to this parame-ter when ?
is varied from 0.2 to 0.6.
Therefore,lambda can be robustly chosen within this range.5.6 DiscussionOverall, from these results we make the follow-ing observations: i) Majority vote is a competitivebaseline; ii) Iterative voting-based methods pro-vide slight improvements on majority vote.
Thisis due to the fact that at the core of iterative vot-ing is still the assumption that fact candidatesmentioned in many sources are more likely to betrue.
Therefore, for both majority vote and it-erative voting, when mention frequencies of var-ious alternatives are the same, accuracy suffers.Based on these observations, it is clear that truth-finding solutions need to incorporate fine-grainedcontent-aware features outside of external votes.FactChecker takes a step in this direction by incor-porating the document-level feature of objectivity.6 Related WorkThere is a fairly small body of work on truth-finding (Yin et al, 2007; Galland et al, 2010;Pasternack and Roth, 2010; Li et al, 2011; Yin andTan, 2011; Zhao et al, 2012; Pasternack and Roth,2013).
The method underlying most truth-findingalgorithms is iterative transitive voting (Yin et al,2007; Galland et al, 2010; Pasternack and Roth,2010; Li et al, 2011).
Fact candidates are ini-tialized with a score.
Trustworthiness of sourcesis then computed from the believability of the factcandidates they mention.
In return, believability ofcandidates is recomputed based on the trustworthi-1016Dataset Fact Candidate Alternatives & RankingWKP ?George W. Bush?
lived in ?Midland,TX?
1.Midland,TX2.Compton,CA3.Washington D.C.4.Venezuela*KB ?Dirk Kuyt?
plays for ?Liverpool?
1.
Liverpool2.Cardiff City*3.Netherlands4.Hungary*Quiz ?Bifocals?
invented by ?Benjamin Franklin?
1.
Benjamin Franklin2.
Rudolf Diesel*3.Thomas Alva Edison*4.Alfred B. Nobel*Table 5: Sample rankings by FactChecker, alternatives marked (*) are false.
The ranking of the candidatefrom the ?KB?
dataset is not completely accurate.ness of their sources.
This process is repeated overseveral iterations until convergence.
(Yin et al,2007) was the first to implement this idea, subse-quent work improved upon iterative voting in sev-eral directions.
(Dong et al, 2009) incorporatescopying-detection; giving high trust to sourcesthat are independently authored.
(Galland et al,2010) approximates error rates of sources and factcandidates.
(Pasternack and Roth, 2010) intro-duces prior knowledge in the form of linear pro-gramming constraints in order to ensure that thetruth discovered is consistent with what is alreadyknown.
(Yin and Tan, 2011) introduces supervi-sion by using ground truth facts so that sourcesthat disagree with the ground truth are penalized.
(Li et al, 2011) uses search engine APIs to gatheradditional evidence for believability of fact can-didates.
WikiTrust (Adler and Alfaro, 2007) isa content-aware but domain-specific method.
Itcomputes trustworthiness of wiki authors basedon the revision history of the articles they haveauthored.
Motivated by interpretability of prob-abilistic scores, two recent papers addressed thetruth-finding problem as a probabilistic inferenceproblem over the sources and the fact candidates(Zhao et al, 2012; Pasternack and Roth, 2013).Truth-finders based on textual entailment such asTruthTeller (Lotan et al, 2013) determine if a sen-tence states something or not.
The focus is on un-derstanding natural language, including the use ofnegation.
This is similar to the goal of fact ex-traction (Banko et al, 2007; Carlson et al, 2010;Fader et al, 2011; Nakashole et al, 2011; DelCorro and Gemulla, 2013).In a departure from prior work, our methodleverages language of sources in its believabilitycomputation model.
Furthermore, we introduceda co-mention score which is designed to avoid po-tential false boots among fact candidates.
Addi-tionally, we developed a method for generating al-ternative fact candidates.
Prior methods assumethese are readily available.
Only (Li et al, 2011)uses the Web to identify alternatives, however, thisis only done after manually specifying the fixed ar-gument.
In contrast, we introduced a method foridentifying the fixed argument based on relationcardinalities learned from SVO statistics.7 ConclusionIn this paper, we presented FactChecker, alanguage-aware approach to truth-finding.
In con-trast to prior approaches, which rely on externalvotes, FactChecker includes objectivity of sourcesin its believability computation model.FactChecker can be seen as a first step to-wards language-aware truth-finding.
Future di-rections include using more sentence-level fea-tures such the use of hedges, assertive verbs, andfactive verbs.
These types of words fall into aclass of words used to express certainties, spec-ulations or doubts ?
these are important cues thatFactChecker can leverage.AcknowledgmentsWe thank members of the NELL team at CMUfor their helpful comments.
This research wassupported by DARPA under contract numberFA8750-13-2-0005.1017ReferencesS.
Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-niak, Z.G.
Ives: DBpedia: A Nucleus for a Web ofOpen Data.
In Proceedings of the 6th InternationalSemantic Web Conference (ISWC), pages 722?735,Busan, Korea, 2007.B.
T. Adler, L. de Alfaro: A content-driven reputa-tion system for the wikipedia.
In Proceedings of the16th International Conference on World Wide Web(WWW), pages 261-270, 2007.M.
Banko, M. J. Cafarella, S. Soderland, M. Broad-head, O. Etzioni: Open Information Extraction fromthe Web.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI),pages 2670?2676, Hyderabad, India, 2007.K.
D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.Taylor: Freebase: a Collaboratively Created GraphDatabase for Structuring Human Knowledge.
InProceedings of the ACM SIGMOD InternationalConference on Management of Data (SIGMOD),pages, 1247-1250, Vancouver, BC, Canada, 2008.L.
D. Brown, T.T.
Cai, A. Dasgupta: Interval Estima-tion for a Binomial Proportion.
Statistical Science16: pages 101?133, 2001.E.
Cabrio, S. Villata: Combining Textual Entailmentand Argumentation Theory for Supporting OnlineDebates Interaction.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pp.
208-212, 2012.A.
Carlson, J. Betteridge, R.C.
Wang, E.R.
Hruschka,T.M.
Mitchell: Coupled Semi-supervised Learningfor Information Extraction.
In Proceedings of theThird International Conference on Web Search andWeb Data Mining (WSDM), pages 101?110, NewYork, NY, USA, 2010.L.
Del Corro, R. Gemulla: ClausIE: clause-basedopen information extraction.
In Proceedings of the22nd International Conference on World Wide Web(WWW), pages 355-366.
2013.X.
Dong, L. Berti-Equille, D. Srivastava: Truth discov-ery and copying detection in a dynamic world.
InProceedings of the VLDB Endowment PVLDB, 2(1),pp.
562-573, 2009.A.
Fader, S. Soderland, O. Etzioni: Identifying Rela-tions for Open Information Extraction.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1535?1545, Edinburgh, UK, 2011.A.
Galland, S. Abiteboul, A. Marian, P. Senellart: Cor-roborating information from disagreeing views.
InProceedings of the 3rd International Conference onWeb Search and Web Data Mining (WSDM), pages131-140, 2010.C.
Havasi, R. Speer, J. Alonso: ConceptNet 3: a Flex-ible, Multilingual Semantic Network for CommonSense Knowledge.
In Proceedings of the Recent Ad-vances in Natural Language Processing (RANLP),Borovets, Bulgaria, 2007.J.
Hoffart, F. Suchanek, K. Berberich, E. Lewis-Kelham, G. de Melo, G. Weikum: YAGO2: Ex-ploring and Querying World Knowledge in Time,Space, Context, and Many Languages.
In Proceed-ings of the 20th International Conference on WorldWide Web (WWW), pages 229?232, Hyderabad, In-dia.
2011.R.
Kaplan: Politics and the American Press: The Riseof Objectivity, pages 1865-1920, New York, Cam-bridge University Press, 2002.X.
Li and W. Meng, C. T. Yu: T-verifier: Verifyingtruthfulness of fact statements.
In Proceedings ofthe International Conference on Data Engineering(ICDE), pp.
63-74, 2011.D.
Lin, P. Pantel: DIRT: discovery of inference rulesfrom text.
KDD 2001B.
Liu, M. Hu, J. Cheng: Opinion Observer: analyzingand comparing opinions on the Web.
InProceedingsof the 14th International Conference on World WideWeb (WWW), pages 342351, 2005.A.
Lotan, A. Stern, I. Dagan TruthTeller: AnnotatingPredicate Truth.
In Proceedings of Human LanguageTechnologies: Conference of the North AmericanChapter of the Association of Computational Lin-guistics (HLT-NAACL), pp.
752-757, 2013.N.
Nakashole, M. Theobald, G. Weikum: ScalableKnowledge Harvesting with High Precision andHigh Recall.
In Proceedings of the 4th InternationalConference on Web Search and Web Data Mining(WSDM), pages 227?326, Hong Kong, China, 2011.N.
Nakashole, T. Tylenda, G. Weikum: Fine-grainedSemantic Typing of Emerging Entities.
In Proceed-ings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pp.
1488-1497, 2013.N.
Nakashole, G. Weikum, F. Suchanek: PATTY:A Taxonomy of Relational Patterns with Seman-tic Types.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 1135 -1145, Jeju, South Korea, 2012.V.
Nastase, M. Strube, B. Boerschinger, C. Zirn, A.Elghafari: WikiNet: A Very Large Scale Multi-Lingual Concept Network.
In Proceedings of the 7thInternational Conference on Language Resourcesand Evaluation(LREC), Malta, 2010.B.
Pang, L. Lee: A Sentimental Education: SentimentAnalysis Using Subjectivity Summarization Based1018on Minimum Cuts.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics (ACL), 271-278, 2004.J.
Pasternack, D. Roth: Knowing What to Believe.
InProceedings the International Conference on Com-putational Linguistics (COLING), pp.
877-885, Bei-jing, China.
2010.J.
Pasternack, D. Roth: Latent credibility analysis.
InProceedings of the 22nd International Conferenceon World Wide Web (WWW), pp.
1009-1020, 2013.E.
Riloff, J. Wiebe: Learning Learning extraction pat-terns for subjective expressions.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), pages 105112,2013.M.
Recasens, C. Danescu-Niculescu-Mizil, D. Juraf-sky: Linguistic Models for Analyzing and DetectingBiased Language.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL), pp.
1650-1659, 2013.F.
Niu, C. Zhang, C. Re, J. W. Shavlik: DeepDive:Web-scale Knowledge-base Construction using Sta-tistical Learning and Inference.
In the VLDS Work-shop, pages 25-28, 2012.M.
Schudson: Discovering the News: A Social Historyof American Newspapers.
New York: Basic Books.1978.F.
M. Suchanek, M. Sozio, G. Weikum: SOFIE: ASelf-organizing Framework for Information Extrac-tion.
InProceedings of the 18th International Con-ference on World Wide Web (WWW), pages 631?640,Madrid, Spain, 2009.P.
P. Talukdar, D. T. Wijaya, T.M.
Mitchell: Acquir-ing temporal constraints between relations.
In Pro-ceeding of the 21st ACM International Conferenceon Information and Knowledge Management, pages992-1001, CIKM 2012.P.
D. Turney: Thumbs up or thumbs down?
Seman-tic orientation applied to unsupervised classificationof reviews.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 417424.
2002.J.
Wiebe, T. Wilson, R. Bruce, M. Bell, M. Martin:Learning subjective language.
Computational Lin-guistics, 30(3):277308.
2004.X.
Yin, J. Han, P. S. Yu: Truth Discovery withMultiple Conflicting Information Providers on theWeb.
In Proceedings of the International Confer-ence on Knowledge Discovery in Databases (KDD), pages1048-1052.
2007.X.
Yin, W. Tan: Semi-supervised truth discover.
InProceedings of the 19th International Conference onWorld Wide Web (WWW), pp.
217-226, 2011.H.
Yu, V. Hatzivassiloglou: Towards Answering Opin-ion Questions: Separating Facts from Opinions andIdentifying the Polarity of Opinion Sentences.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages.
129-136, 2003.B.
Zhao, B. I. P. Rubinstein, J. Gemmell, J. Han: ABayesian approach to discovering truth from con-flicting sources for data integration.
In Proceedingsof the VLDB Endowment (PVLDB), 5(6):550-561,2012.1019
