Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 1?12,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCreating Speech and Language Data With Amazon?s Mechanical TurkChris Callison-Burch and Mark DredzeHuman Language Technology Center of Excellence& Center for Language and Speech ProcessingJohns Hopkins Universityccb,mdredze@cs.jhu.eduAbstractIn this paper we give an introduction to us-ing Amazon?s Mechanical Turk crowdsourc-ing platform for the purpose of collectingdata for human language technologies.
Wesurvey the papers published in the NAACL-2010 Workshop.
24 researchers participatedin the workshop?s shared task to create data forspeech and language applications with $100.1 IntroductionThis paper gives an overview of the NAACL-2010Workshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.
A number of re-cent papers have evaluated the effectiveness of us-ing Mechanical Turk to create annotated data fornatural language processing applications.
The lowcost, scalable workforce available through Mechan-ical Turk (MTurk) and other crowdsourcing sitesopens new possibilities for annotating speech andtext, and has the potential to dramatically changehow we create data for human language technolo-gies.
Open questions include: What kind of researchis possible when the cost of creating annotated train-ing data is dramatically reduced?
What new tasksshould we try to solve if we do not limit ourselves toreusing existing training and test sets?
Can complexannotation be done by untrained annotators?
Howcan we ensure high quality annotations from crowd-sourced contributors?To begin addressing these questions, we orga-nized an open-ended $100 shared task.
Researcherswere given $100 of credit on Amazon MechanicalTurk to spend on an annotation task of their choos-ing.
They were required to write a short paper de-scribing their experience, and to distribute the datathat they created.
They were encouraged to ad-dress the following questions: How did you conveythe task in terms that were simple enough for non-experts to understand?
Were non-experts as good asexperts?
What did you do to ensure quality?
Howquickly did the data get annotated?
What is the costper label?
Researchers submitted a 1 page proposalto the workshop organizers that described their in-tended experiments and expected outcomes.
Theorganizers selected proposals based on merit, andawarded $100 credits that were generously providedby Amazon Mechanical Turk.
In total, 35 creditswere awarded to researchers.Shared task participants were given 10 days to runexperiments between the distribution of the creditand the initial submission deadline.
30 papers weresubmitted to the shared task track, of which 24 wereaccepted.
14 papers were submitted to the generaltrack of which 10 were accepted, giving a 77% ac-ceptance rate and a total of 34 papers.
Shared taskparticipants were required to provide the data col-lected as part of their experiments.
All of the sharedtask data is available on the workshop website.2 Mechanical TurkAmazon?s Mechanical Turk1 is an online market-place for work.
Amazon?s tag line for Mechani-cal Turk is artificial artificial intelligence, and thename refers to a historical hoax from the 18th cen-1http://www.mturk.com/1< 11-22-44-88-2020-4040+< 1 HIT1-55-1010-2020-5050-100100-200200-500500-1k1k-5k5k+ HITs< $1$1-5$5-1010-2020-5050-100100-200$200+5%17%22%26%19%10%3%100%1%6%9%13%19%17%12%13%5%4%1%100%11%36%22%15%11%4%2%0%100%0%5%10%15%20%25%30%< 1 1-2 2-4 4-8 8-20 40+Hours spent on Mechanical Turk per week0%5%10%15%20%< 1 HIT 5-10 20-50 100-200 500-1k 5k+ HITsNumber of HITs completed per week0%10%20%30%40%< $1 $5-10 20-50 100-200Weekly income from Mechanical TurkFigure 1: Time spent, HITs completed, and amount earned from a survey of 1,000 Turkers by Ipeirotis (2010).tury where a chess-playing automaton appeared tobe able to beat human opponents using a mecha-nism, but was, in fact, controlled by a person hidinginside the machine.
These hint at the the primary fo-cus of the web service, which is to get people to per-form tasks that are simple for humans but difficultfor computers.
The basic unit of work on MTurk iseven called a Human Intelligence Task (HIT).Amazon?s web service provides an easy way topay people small amounts of money to performHITs.
Anyone with an Amazon account can eithersubmit HITs or work on HITs that were submittedby others.
Workers are referred to as ?Turkers?
andpeople designing the HITs are called ?Requesters.
?Requesters set the amount that they will pay for eachitem that is completed.
Payments are frequently aslow as $0.01.
Turkers are free to select whicheverHITs interest them.
], and to disregard HITs that theyfind uninteresting or which they deem pay too little.Because of its focus on tasks requiring human in-telligence, Mechanical Turk is obviously applicableto the field of natural language processing.
Snowet al (2008) used Mechanical Turk to inexpensivelycollect labels for several NLP tasks including wordsense disambiguation, word similarity, textual en-tailment, and temporal ordering of events.
Snow etal.
had two exciting findings.
First, they showed thata strong correlation between non-expert and expertannotators can be achieved by combining the judg-ments of multiple non-experts, for instance by vot-ing on each label using 10 different Turkers.
Cor-relation and accuracy of labeling could be furtherimproved by weighting each Turker?s vote by cal-ibrating them on a small amount of gold standarddata created by expert annotators.
Second, they col-lected a staggering number of labels for a very smallamount of money.
They collected 21,000 labels forjust over $25.
Turkers put in over 140+ hours worthWhy do you complete tasks in MTurk?
US IndiaTo spend free time fruitfully and getcash (e.g., instead of watching TV)70% 60%For ?primary?
income purposes (e.g.,gas, bills, groceries, credit cards)15% 27%For ?secondary?
income purposes,pocket change (for hobbies, gadgets)60% 37%To kill time 33% 5%The tasks are fun 40% 20%Currently unemployed or part time work 30% 27%Table 1: Motivations for participating on MechanicalTurk from a survey of 1,000 Turkers by Ipeirotis (2010).of human effort to generate the labels.
The amountof participation is surprisingly high, given the smallpayment.Turker demographicsGiven the amount of work that can get done for solittle, it is natural to ask: who would contribute somuch work for so little pay, and why?
The answersto these questions are often mysterious becauseAmazon does not provide any personal informa-tion about Turkers (each Turker is identifiable onlythrough a serial number like A23KO2TP7I4KK2).Ipeirotis (2010) elucidates some of the reasons bypresenting a demographic analysis of Turkers.
Hebuilt a profile of 1000 Turkers by posting a survey toMTurk and paying $0.10 for people to answer ques-tions about their reasons for participating on Me-chanical Turk, the amount that they earn each week,and how much time they spend, as well as demo-graphic information like country of origin, gender,age, education level, and household income.One suspicion that people often have when theyfirst hear about MTurk is that it is some sort of dig-ital sweatshop that exploits workers in third worldcountries.
However, Ipeirotis reports that nearly half2(47%) of the Turkers who answered his survey werefrom the United States, with the next largest group(34%) coming from India, and the remaining 19%spread between 66 other countries.Table 1 gives the survey results for questionsrelating to why people participate on MechanicalTurk.
It shows that most US-based workers use Me-chanical Turk for secondary income purposes (tohave spending money for hobbies or going out),but that the overwhelming majority of them useit to spend their time more fruitfully (i.e., insteadof watching TV).
The economic downturn mayhave increased participation, with 30% of the US-based Turkers reporting that they are unemployedor underemployed.
The public radio show Mar-ketplace recently interviewed unemployed Turkers(Rose, 2010).
It reports that they earn a little in-come, but that they do not earn enough to make aliving.
Figure 1 confirms this, giving a break downof how much time people spend on Mechanical Turkeach week, how many HITs they complete, and howmuch money they earn.
Most Turkers spend lessthan 8 hours per week on Mechanical Turk, and earnless than $10 per week through the site.3 Quality ControlIpeirotis (2010) reports that just over half of Turkershave a college education.
Despite being reasonablywell educated, it is important to keep in mind thatTurkers do not have training in specialized subjectslike NLP.
Because the Turkers are non-experts, andbecause the payments are generally so low, qualitycontrol is an important consideration when creatingdata with MTurk.Amazon provides three mechanisms to help en-sure quality:?
Requesters have the option of rejecting thework of individual Turkers, in which case theyare not paid.2 Turkers can also be blocked fromdoing future work for a requester.2Since the results are downloadable even if they are rejected,this could allow unscrupulous Requesters to abuse Turkers byrejecting all of their work, even if it was done well.
Turkers havemessage boards at http://www.turkernation.com/,where they discuss Requesters.
They even have a Firefox plu-gin called Turkopticon that lets them see ratings of how goodthe Requesters are in terms of communicating with Turkers, be-ing generous and fair, and paying promptly.?
Requesters can specify that each HIT shouldbe redundantly completed by several differentTurkers.
This allows higher quality labels tobe selected, for instance, by taking the majoritylabel.?
Requesters can require that all workers meeta particular set of qualifications, such as suffi-cient accuracy on a small test set or a minimumpercentage of previously accepted submissions.Amazon provides two qualifications that a Re-quester can use by default.
These are past HIT Ap-proval Rate and Location.
The location qualifica-tion allows the Requester to have HITs done only byresidents of a certain country (or to exclude Turk-ers from certain regions).
Additionally, Requesterscan design custom Qualification Tests that Turkersmust complete before working on a particular HIT.These can be created through the MTurk API, andcan either be graded manually or automatically.
Animportant qualification that isn?t among Amazon?sdefault qualifications is language skills.
One mightdesign a qualification test to determine a Turker?sability to speak Arabic or Farsi before allowing themto do part of speech tagging in those languages, forinstance.There are several reasons that poor quality datamight be generated.
The task may be too complex orthe instructions might not be clear enough for Turk-ers to follow.
The financial incentives may be toolow for Turkers to act conscientiously, and certainHIT designs may allow them to simply randomlyclick instead of thinking about the task.
Mason andWatts (2009) present a study of financial incentiveson Mechanical Turk and find, counterintuitively, thatincreasing the amount of compensation for a partic-ular task does not tend to improve the quality of theresults.
Anecdotally, we have observed that some-times there is an inverse relationship between theamount of payment and the quality of work, becauseit is more tempting to cheat on high-paying HITs ifyou don?t have the skills to complete them.
For ex-ample, a number of Turkers tried to cheat on an Urduto English translation HIT by cutting-and-pastingthe Urdu text into an online machine translation sys-tem (expressly forbidden in the instructions) becausewe were paying the comparatively high amount of$1.33.1 Designing HITs for quality controlWe suggest designing your HITs in a way that willdeter cheating or that will make cheating obvious.HIT design is part of the art of using MTurk.
Itcan?t be easily quantified, but it has a large impact onthe outcome.
For instance, we reduced cheating onour translation HIT by changing the design so thatwe displayed images of the Urdu sentences insteadof text, which made it impossible to copy-and-pasteinto an MT system for anyone who could not type inArabic script.Another suggestion is to include informationwithin the data that you upload to MTurk that willnot be displayed to the Turkers, but will be usefulto you when reviewing the HITs.
For example, weinclude machine translation output along with thesource sentences.
Although this is not displayed toTurkers, when we review the Turkers?
translationswe compare them to the MT output.
This allows usto reject translations that are identical to the MT, orwhich are just random sentences that are unrelated tothe original Urdu.
We also use a javascript3 to gatherthe IP addresses of the Turkers and do geolocationto look up their location.
Turkers in Pakistan requireless careful scrutiny since they are more likely to bebilingual Urdu speakers than those in Romania, forinstance.CrowdFlower4 provides an interface for design-ing HITs that includes a phase for the Requester toinput gold standard data with known labels.
Insert-ing items with known labels alongside items whichneed labels allows a Requester to see which Turkersare correctly replicating the gold standard labels andwhich are not.
This is an excellent idea.
If it is possi-ble to include positive and negative controls in yourHITs, then do so.
Turkers who fail the controls canbe blocked and their labels can be excluded from thefinal data set.
CrowdFlower-generated HITs evendisplay a score to the Turkers to give them feedbackon how well they are doing.
This provides trainingfor Turkers, and discourages cheating.3http://wiki.github.com/callison-burch/mechanical_turk_workshop/geolocation4http://crowdflower.com/3.2 Iterative improvements on MTurkAnother class of quality control on Mechanical Turkis through iterative HITs that build on the output ofprevious HITs.
This could be used to have Turkersjudge whether the results from a previous HIT con-formed to the instructions, and whether it is of highquality.
Alternately, the second set of Turkers couldbe used to improve the quality of what the first Turk-ers created.
For instance, in a translation task, a sec-ond set of US-based Turkers could edit the Englishproduced by non-native speakers.CastingWords,5 a transcription company that usesTurker labor, employs this strategy by having a first-pass transcription graded and iteratively improvedin subsequent passes.
Little et al (2009) even de-signed an API specifically for running iterative taskson MTurk.64 Recommended PracticesAlthough it is hard to define a set of ?best practices?that applies to all HITs, or even to all NLP HITs, werecommend the following guidelines to Requesters.First and foremost, it is critical to convey instruc-tions appropriately for non-experts.
The instructionsshould be clear and concise.
To calibrate whetherthe HIT is doable, you should first try the task your-self, and then have a friend from outside the field tryit.
This will help to ensure that the instructions areclear, and to calibrate how long each HIT will take(which ought to allow you to price the HITs fairly).If possible, you should insert positive and nega-tive controls so that you can quickly screen out badTurkers.
This is especially important for HITs thatonly require clicking buttons to complete.
If pos-sible, you should include a small amount of goldstandard data in each HIT.
This will allow you todetermine which Turkers are good, but will also al-low you weight the Turkers if you are combiningthe judgments of multiple Turkers.
If you are hav-ing Turkers evaluate the output of systems, then ran-domize the order that the systems are shown in.When publishing papers that use Mechanical Turkas a source of training data or to evaluate the outputof an NLP system, report how you ensured the qual-ity of your data.
You can do this by measuring the5http://castingwords.com/6http://groups.csail.mit.edu/uid/turkit/4inter-annotator agreement of the Turkers against ex-perts on small amounts of gold standard data, or bystating what controls you used and what criteria youused to block bad Turkers.
Finally, whenever possi-ble you should publish the data that you generate onMechanical Turk (and your analysis scripts and HITtemplates) alongside your paper so that other peoplecan verify it.5 Related workIn the past two years, several papers have publishedabout applying Mechanical Turk to a diverse set ofnatural language processing tasks, including: cre-ating question-answer sentence pairs (Kaisser andLowe, 2008), evaluating machine translation qual-ity and crowdsouring translations (Callison-Burch,2009), paraphrasing noun-noun compouds for Se-mEval (Butnariu et al, 2009), human evaluation oftopic models (Chang et al, 2009), and speech tran-scription (McGraw et al, 2010; Marge et al, 2010a;Novotney and Callison-Burch, 2010a).
Others haveused MTurk for novel research directions like non-simulated active learning for NLP tasks such as sen-timent classification (Hsueh et al, 2009) or doingquixotic things like doing human-in-the-loop min-imum error rate training for machine translation(Zaidan and Callison-Burch, 2009).Some projects have demonstrated the super-scalability of crowdsourced efforts.
Deng et al(2009) used MTurk to construct ImageNet, an anno-tated image database containing 3.2 million that arehierarchically categorized using the WordNet ontol-ogy (Fellbaum, 1998).
Because Mechanical Turkallows researchers to experiment with crowdsourc-ing by providing small incentives to Turkers, othersuccessful crowdsourcing efforts like Wikipedia orGames with a Purpose (von Ahn and Dabbish, 2008)also share something in common with MTurk.6 Shared TaskThe workshop included a shared task in which par-ticipants were provided with $100 to spend on Me-chanical Turk experiments.
Participants submitted a1 page proposal in advance describing their intendeduse of the funds.
Selected proposals were provided$100 seed money, to which many participants addedtheir own funds.
As part of their participation, eachteam submitted a workshop paper describing theirexperiments as well as the data collected and de-scribed in the paper.
Data for the shared papers isavailable at the workshop website.7This section describes the variety of data types ex-plored and collected in the shared task.
Of the 24participating teams, most did not exceed the $100that they were awarded by a significant amount.Therefore, the variety and extent of data described inthis section is the result of a minimal $2,400 invest-ment.
This achievement demonstrates the potentialfor MTurk?s impact on the creation and curation ofspeech and language corpora.6.1 Traditional NLP TasksAn established core set of computational linguistictasks have received considerable attention in the nat-ural language processing community.
These includeknowledge extraction, textual entailment and wordsense disambiguation.
Each of these tasks requires alarge and carefully curated annotated corpus to trainand evaluate statistical models.
Many of the sharedtask teams attempted to create new corpora for thesetasks at substantially reduced costs using MTurk.Parent and Eskenazi (2010) produce new corporafor the task of word sense disambiguation.
Thestudy used MTurk to create unique word definitionsfor 50 words, which Turkers then also mapped ontoexisting definitions.
Sentences containing these 50words were then assigned to unique definitions ac-cording to word sense.Madnani and Boyd-Graber (2010) measured theconcept of transitivity of verbs in the style of Hop-per and Thompson (1980), a theory that goes beyondsimple grammatical transitivity ?
whether verbs takeobjects (transitive) or not ?
to capture the amount ofaction indicated by a sentence.
Videos that portrayedverbs were shown to Turkers who described the ac-tions shown in the video.
Additionally, sentencescontaining the verbs were rated for aspect, affirma-tion, benefit, harm, kinesis, punctuality, and volition.The authors investigated several approaches for elic-iting descriptions of transitivity from Turkers.Two teams explored textual entailment tasks.Wang and Callison-Burch (2010) created data for7http://sites.google.com/site/amtworkshop2010/5recognizing textual entailment (RTE).
They submit-ted 600 text segments and asked Turkers to identifyfacts and counter-facts (unsupported facts and con-tradictions) given the provided text.
The resultingcollection includes 790 facts and 203 counter-facts.Negri and Mehdad (2010) created a bi-lingual en-tailment corpus using English and Spanish entail-ment pairs, where the hypothesis and text come fromdifferent languages.
The authors took a publiclyavailable English RTE data set (the PASCAL-RTE3dataset1) and created an English-Spanish equivalentby having Turkers translating the hypotheses intoSpanish.
The authors include a timeline of theirprogress, complete with total cost over the 10 daysthat they ran the experiments.In the area of natural language generation, Heil-man and Smith (2010) explored the potential ofMTurk for ranking of computer generated questionsabout provided texts.
These questions can be used totest reading comprehension and understanding.
60Wikipedia articles were selected, for each of which20 questions were generated.
Turkers provided 5 rat-ings for each of the 1,200 questions, creating a sig-nificant corpus of scored questions.Finally, Gordon et al (2010) relied on MTurk toevaluate the quality and accuracy of automaticallyextracted common sense knowledge (factoids) fromnews and Wikipedia articles.
Factoids were pro-vided by the KNEXT knowledge extraction system.6.2 Speech and VisionWhile MTurk naturally lends itself to text tasks,several teams explored annotation and collection ofspeech and image data.
We note that one of the pa-pers in the main track described tools for collectingsuch data (Lane et al, 2010).Two teams used MTurk to collect text annotationson speech data.
Marge et al (2010b) identified easyand hard sections of meeting speech to transcribeand focused data collection on difficult segments.Transcripts were collected on 48 audio clips from4 different speakers, as well as other types of an-notations.
Kunath and Weinberger (2010) collectedratings of accented English speech, in which non-native speakers were rated as either Arabic, Man-darin or Russian native speakers.
The authors ob-tained multiple annotations for each speech sample,and tracked the native language of each annotator,allowing for an analysis of rating accuracy betweennative English and non-native English annotators.Novotney and Callison-Burch (2010b) usedMTurk to elicit new speech samples.
As part of aneffort to increase the accessibility of public knowl-edge, such as Wikipedia, the team prompted Turkersto narrate Wikipedia articles.
This required Turkersto record audio files and upload them.
An additionalHIT was used to evaluate the quality of the narra-tions.A particularly creative data collection approachasked Turkers to create handwriting samples andthen to submit images of their writing (Tong et al,2010).
Turkers were asked to submit handwrittenshopping lists (large vocabulary) or weather descrip-tions (small vocabulary) in either Arabic or Spanish.Subsequent Turkers provided a transcription and atranslation.
The team collected 18 images per lan-guage, 2 transcripts per image and 1 translation pertranscript.6.3 Sentiment, Polarity and BiasTwo papers investigated the topics of sentiment, po-larity and bias.
Mellebeek et al (2010) used severalmethods to obtain polarity scores for Spanish sen-tences expressing opinions about automative topics.They evaluated three HITs for collecting such dataand compared results for quality and expressiveness.Yano et al (2010) evaluated the political bias of blogposts.
Annotators labeled 1000 sentences to deter-mine biased phrases in political blogs from the 2008election season.
Knowledge of the annotators ownbiases allowed the authors to study how bias differson the different ends of the political spectrum.6.4 Information RetrievalLarge scale evaluations requiring significant humanlabor for evaluation have a long history in the in-formation retrieval community (TREC).
Grady andLease (2010) study four factors that influence Turkerperformance on a document relevance search task.The authors present some negative results on howthese factors influence data collection.
For furtherwork on MTurk and information retrieval, readersare encouraged to see the SIGIR 2010 Workshop onCrowdsourcing for Search Evaluation.88http://www.ischool.utexas.edu/?cse2010/call.htm66.5 Information ExtractionInformation extraction (IE) seeks to identify specifictypes of information in natural languages.
The IEpapers in the shared tasks focused on new domainsand genres as well as new relation types.The goal of relation extraction is to identify rela-tions between entities or terms in a sentence, such asborn in or religion.
Gormley et al (2010) automat-ically generate potential relation pairs in sentencesby finding relation pairs appearing in news articlesas given by a knowledge base.
They ask Turkers ifa sentence supports a relation, does not support a re-lation, or whether the relation makes sense.
Theycollected close to 2500 annotations for 17 differentperson relation types.The other IE papers explored new genres and do-mains.
Finin et al (2010) obtained named entity an-notations (person, organization, geopolitical entity)for several hundred Twitter messages.
They con-ducted experiments using both MTurk and Crowd-Flower.
Yetisgen-Yildiz et al (2010) exploredmedical named entity recognition.
They selected100 clinical trial announcements from ClinicalTri-als.gov.
4 annotators for each of the 100 announce-ments identified 3 types of medical entities: medicalconditions, medications, and laboratory test.6.6 Machine TranslationThe most popular shared task topic was MachineTranslation (MT).
MT is a data hungry task that re-lies on huge corpora of parallel texts between twolanguages.
Performance of MT systems dependson the size of training corpora, so there is a con-stant search for new and larger data sets.
Such datasets are traditionally expensive to produce, requiringskilled translators.
One of the advantages to MTurkis the diversity of the Turker population, making itan especially attractive source of MT data.
Sharedtask papers in MT explored the full range of MTtasks, including alignments, parallel corpus creation,paraphrases and bilingual lexicons.Gao and Vogel (2010) create alignments in a 300sentence Chinese-English corpus (Chinese alignedto English).
Both Ambati and Vogel (2010) andBloodgood and Callison-Burch (2010) explore thepotential of MTurk in the creation of MT paral-lel corpora for evaluation and training.
Bloodgoodand Callison-Burch replicate the NIST 2009 Urdu-English test set of 1792 sentences, paying only $0.10a sentence, a substantially reduced price than thetypical annotator cost.
The result is a data set that isstill effective for comparing MT systems in an eval-uation.
Ambati and Vogel create corpora with 100sentences and 3 translations per sentence for all thelanguage pairs between English, Spanish, Urdu andTelugu.
This demonstrates the feasibility of creatingcheap corpora for high and low resource languages.Two papers focused on the creation and evalua-tion of paraphrases.
Denkowski et al (2010) gen-erated and evaluated 728 paraphrases for Arabic-English translation.
MTurk was used to identifycorrect and fix incorrect paraphrases.
Over 1200high quality paraphrases were created.
Buzek etal.
(2010) evaluated error driven paraphrases forMT.
In this setting, paraphrases are used to sim-plify potentially difficult to translate segments oftext.
Turkers identified 1780 error regions in 1006English/Chinese sentences.
Turkers provided 4821paraphrases for these regions.External resources can be an important part of anMT system.
Irvine and Klementiev (2010) createdlexicons for low resource languages.
They evaluatedtranslation candidates for 100 English words in 32languages and solicited translations for 10 additionallanguages.
Higgins et al (2010) expanded namelists in Arabic by soliciting common Arabic nick-names.
The 332 collected nicknames were primar-ily provided by Turkers in Arab speaking countries(35%), India (46%), and the United States (13%).Finally, Zaidan and Ganitkevitch (2010) exploredhow MTurk could be used to directly improve an MTgrammar.
Each rule in an Urdu to English transla-tion system was characterized by 12 features.
Turk-ers were provided examples for which their feed-back was used to rescore grammar productions di-rectly.
This approach shows the potential of finetuning an MT system with targeted feedback fromannotators.7 Future DirectionsLooking ahead, we can?t help but wonder what im-pact MTurk and crowdsourcing will have on thespeech and language research community.
Keep-ing in mind Niels Bohr?s famous exhortation ?Pre-7diction is very difficult, especially if it?s about thefuture,?
we attempt to draw some conclusions andpredict future directions and impact on the field.Some have predicted that access to low cost,highly scalable methods for creating language andspeech annotations means the end of work on un-supervised learning.
Many a researcher has advo-cated his or her unsupervised learning approach be-cause of annotation costs.
However, if 100 exam-ples for any task are obtainable for less than $100,why spend the time and effort developing often infe-rior unsupervised methods?
Such a radical change ishighly debatable, in fact, one of this paper?s authorsis a strong advocate of such a position while theother disagrees, perhaps because he himself workson unsupervised methods.
Certainly, we can agreethat the potential exists for a change in focus in anumber of ways.In natural language processing, data drives re-search.
The introduction of new large and widelyaccessible data sets creates whole new areas of re-search.
There are many examples of such impact,the most famous of which is the Penn Treebank(Marcus.
et al, 1994), which has 2910 citations inGoogle scholar and is the single most cited paperon the ACL anthology network (Radev et al, 2009).Other examples include the CoNLL named entitycorpus (Sang and Meulder (2003) with 348 citationson Google Scholar), the IMDB movie reviews senti-ment data (Pang et al (2002) with 894 citations) andthe Amazon sentiment multi-domain data (Blitzer etal.
(2007) with 109 citations) .
MTurk means thatcreating similar data sets is now much cheaper andeasier than ever before.
It is highly likely that newMTurk produced data sets will achieve prominenceand have significant impact.
Additionally, the cre-ation of shared data means more comparison andevaluation against previous work.
Progress is madewhen it can be demonstrated against previous ap-proaches on the same data.
The reduction of datacost and the rise of independent corpus producerslikely means more accessible data.More than a new source for cheap data, MTurk isa source for new types of data.
Several of the pa-pers in this workshop collected information aboutthe annotators in addition to their annotations.
Thiscreates potential for studying how different user de-mographics understand language and allow for tar-geting specific demographics in data creation.
Be-yond efficiencies in cost, MTurk provides access toa global user population far more diverse than thoseprovided by more professional annotation settings.This will have a significant impact on low resourcelanguages as corpora can be cheaply built for a muchwider array of languages.
As one example, Irvineand Klementiev (2010) collected data for 42 lan-guages without worrying about how to find speak-ers of such a wide variety of languages.
Addition-ally, the collection of Arabic nicknames requires adiverse and numerous Arabic speaking population(Higgins et al, 2010).
In addition to extending intonew languages, MTurk also allows for the creationof evaluation sets in new genres and domains, whichwas the focus of two papers in this workshop (Fininet al, 2010; Yetisgen-Yildiz et al, 2010).
We ex-pect to see new research emphasis on low resourcelanguages and new domains and genres.Another factor is the change of data type and itsimpact on machine learning algorithms.
With pro-fessional annotators, great time and care are paid toannotation guidelines and annotator training.
Theseare difficult tasks with MTurk, which favors simpleintuitive annotations and little training.
Many papersapplied creative methods of using simpler annota-tion tasks to create more complex data sets.
Thisprocess can impact machine learning in a numberof ways.
Rather than a single gold standard, anno-tations are now available for many users.
Learn-ing across multiple annotations may improve sys-tems (Dredze et al, 2009).
Additionally, even withefforts to clean up MTurk annotations, we can ex-pect an increase in noisy examples in data.
This willpush for new more robust learning algorithms thatare less sensitive to noise.
If we increase the sizeof the data ten-fold but also increase the noise, canlearning still be successful?
Another learning areaof great interest is active learning, which has longrelied on simulated user experiments.
New workevaluated active learning methods with real users us-ing MTurk (Baker et al, 2009; Ambati et al, 2010;Hsueh et al, 2009; ?).
Finally, the composition ofcomplex data set annotations from simple user in-puts can transform the method by which we learncomplex outputs.
Current approaches expect exam-ples of labels that exactly match the expectation ofthe system.
Can we instead provide lower level sim-8pler user annotations and teach systems how to learnfrom these to construct complex output?
This wouldopen more complex annotation tasks to MTurk.A general trend in research is that good ideascome from unexpected places.
Major transforma-tions in the field have come from creative new ap-proaches.
Consider the Penn Treebank, an ambitiousand difficult project of unknown potential.
Suchlarge changes can be uncommon since they are oftenassociated with high cost, as was the Penn Treebank.However, MTurk greatly reduces these costs, en-couraging researchers to try creative new tasks.
Forexample, in this workshop Tong et al (2010) col-lected handwriting samples in multiple languages.Their creative data collection may or may not havea significant impact, but it is unlikely that it wouldhave been tried had the cost been very high.Finally, while obtaining new data annotationsfrom MTurk is cheap, it is not trivial.
Workshop par-ticipants struggled with how to attract Turkers, howto price HITs, HIT design, instructions, cheating de-tection, etc.
No doubt that as work progresses, sowill a communal knowledge and experience of howto use MTurk.
There can be great benefit in newtoolkits for collecting language data using MTurk,and indeed some of these have already started toemerge (Lane et al, 2010)9.AcknowledgementsThanks to Sharon Chiarella of Amazon?s Mechan-ical Turk for providing $100 credits for the sharedtask, and to CrowdFlower for allowing free use oftheir tool to workshop participants.Research funding was provided by the NSF un-der grant IIS-0713448, by the European Commis-sion through the EuroMatrixPlus project, and bythe DARPA GALE program under Contract No.HR0011-06-2-0001.
The views and findings are theauthors?
alone.ReferencesCem Akkaya, Alexander Conrad, Janyce Wiebe, andRada Mihalcea.
2010.
Amazon Mechanical Turk forsubjectivity word sense disambiguation.
In NAACL9http://wiki.github.com/callison-burch/mechanical_turk_workshop/Workshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Vamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In NAACL Workshop on Creating Speech and Lan-guage Data With Amazon?s Mechanical Turk.Vamshi Ambati, Stephan Vogel, and Jamie Carbonell.2010.
Active learning and crowd-sourcing for ma-chine translation.
Language Resources and Evalua-tion (LREC).Kathy Baker, Steven Bethard, Michael Bloodgood, RalfBrown, Chris Callison-Burch, Glen Coppersmith,Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,Mike Kayser, Lori Levin, Justin Martineau, Jim May-field, Scott Miller, Aaron Phillips, Andrew Philpot,Christine Piatko, Lane Schwartz, and David Zajic.2009.
Semantically-informed machine translation.Technical Report 002, Johns Hopkins Human Lan-guage Technology Center of Excellence, SummerCamp for Applied Language Exploration, Johns Hop-kins University, Baltimore, MD.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
In As-sociation for Computational Linguistics (ACL).Michael Bloodgood and Chris Callison-Burch.
2010a.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In 48thAnnual Meeting of the Association for ComputationalLinguistics, Uppsala, Sweden.Michael Bloodgood and Chris Callison-Burch.
2010b.Using Mechanical Turk to build machine translationevaluation sets.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechan-ical Turk.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2009.
Semeval-2010 Task 9: The interpretation ofnoun compounds using paraphrasing verbs and prepo-sitions.
In Workshop on Semantic Evaluations.Olivia Buzek, Philip Resnik, and Ben Bederson.
2010.Error driven paraphrase annotation using MechanicalTurk.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using amazon?s mechan-ical turk.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2009), Singapore.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In NeuralInformation Processing Systems.9Jonathan Chang.
2010.
Not-so-Latent Dirichlet Allo-cation: Collapsed Gibbs sampling using human judg-ments.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,and Li Fei-Fei.
2009.
Imagenet: A large-scale hier-archical image database.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recogni-tion (CVPR, Miami Beach, Floriday.Michael Denkowski and Alon Lavie.
2010.
Explor-ing normalization techniques for human judgments ofmachine translation adequacy collected using AmazonMechanical Turk.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechani-cal Turk.Michael Denkowski, Hassan Al-Haj, and Alon Lavie.2010.
Turker-assisted paraphrasing for English-Arabic machine translation.
In NAACL Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk.Mark Dredze, Partha Pratim Talukdar, and Koby Cram-mer.
2009.
Sequence learning from data with multiplelabels.
In ECML/PKDD Workshop on Learning fromMulti-Label Data (MLD).Keelan Evanini, Derrick Higgins, and Klaus Zechner.2010.
Using Amazon Mechanical Turk for transcrip-tion of non-native speech.
In NAACL Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Tim Finin, William Murnane, Anand Karandikar,Nicholas Keller, Justin Martineau, and Mark Dredze.2010.
Touring PER, ORG and LOC on $100 a day.
InNAACL Workshop on Creating Speech and LanguageData With Amazon?s Mechanical Turk.Qin Gao and Stephan Vogel.
2010.
Semi-supervisedword alignment with Mechanical Turk.
In NAACLWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Dan Gillick and Yang Liu.
2010.
Non-expert evaluationof summarization systems is risky.
In NAACL Work-shop on Creating Speech and Language Data WithAmazon?s Mechanical Turk.Jonathan Gordon, Benjamin Van Durme, and LenhartSchubert.
2010.
Evaluation of commonsense knowl-edge with Mechanical Turk.
In NAACL Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk.Matthew R. Gormley, Adam Gerber, Mary Harper, andMark Dredze.
2010.
Non-expert correction of auto-matically generated relation annotations.
In NAACLWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Catherine Grady and Matthew Lease.
2010.
Crowd-sourcing document relevance assessment with Me-chanical Turk.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechan-ical Turk.Michael Heilman and Noah A. Smith.
2010.
Ratingcomputer-generated questions with Mechanical Turk.In NAACL Workshop on Creating Speech and Lan-guage Data With Amazon?s Mechanical Turk.Chiara Higgins, Elizabeth McGrath, and Laila Moretto.2010.
AMT crowdsourcing: A viable method for rapiddiscovery of Arabic nicknames?
In NAACL Workshopon Creating Speech and Language Data With Ama-zon?s Mechanical Turk.Paul J. Hopper and Sandra A. Thompson.
1980.
Transi-tivity in grammar and discourse.
Language, 56:251?299.Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.2009.
Data quality from crowdsourcing: A study ofannotation selection criteria.
In Proceedings of theNAACL HLT 2009 Workshop on Active Learning forNatural Language Processing, pages 27?35, Boulder,Colorado, June.
Association for Computational Lin-guistics.Panos Ipeirotis.
2010.
New demographics of MechanicalTurk.
http://behind-the-enemy-lines.blogspot.com/2010/03/new-demographics-of-mechanical-turk.html.Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechan-ical Turk.Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-thal, and Kathleen McKeown.
2010.
Corpus creationfor new genres: A crowdsourced approach to PP at-tachment.
In NAACL Workshop on Creating Speechand Language Data With Amazon?s Mechanical Turk.Michael Kaisser and John Lowe.
2008.
Creating a re-search collection of question answer sentence pairswith Amazons Mechanical Turk.
In Proceedings ofthe Sixth International Language Resources and Eval-uation (LREC?08), Marrakech, Morocco.Stephen Kunath and Steven Weinberger.
2010.
The wis-dom of the crowd?s ear: Speech accent rating and an-notation with Amazon Mechanical Turk.
In NAACLWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Ian Lane, Matthias Eck, Kay Rottmann, and AlexWaibel.
2010.
Tools for collecting speech corpora viaMechanical-Turk.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechani-cal Turk.10Nolan Lawson, Kevin Eustice, Mike Perkowitz, andMeliha Yetisgen Yildiz.
2010.
Annotating large emaildatasets for named entity recognition with MechanicalTurk.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-man.
2009.
Turkit: Tools for iterative tasks on me-chanical turk.
In Proceedings of the Workshop onHuman Computation at the International Conferenceon Knowledge Discovery and Data Mining (KDD-HCOMP ?09), Paris.Nitin Madnani and Jordan Boyd-Graber.
2010.
Measur-ing transitivity using untrained annotators.
In NAACLWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Mitch Marcus., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
Computational lin-guistics, 19(2):313?330.Matthew Marge, Satanjeev Banerjee, and AlexanderRudnicky.
2010a.
Using the Amazon MechanicalTurk for transcription of spoken language.
ICASSP,March.Matthew Marge, Satanjeev Banerjee, and AlexanderRudnicky.
2010b.
Using the Amazon MechanicalTurk to transcribe and annotate meeting speech for ex-tractive summarization.
In NAACL Workshop on Cre-ating Speech and Language Data With Amazon?s Me-chanical Turk.Winter Mason and Duncan J. Watts.
2009.
Financialincentives and the ?performance of crowds?.
In Pro-ceedings of the Workshop on Human Computation atthe International Conference on Knowledge Discoveryand Data Mining (KDD-HCOMP ?09), Paris.Ian McGraw, Chia ying Lee, Lee Hetherington, and JimGlass.
2010.
Collecting voices from the crowd.LREC, May.Bart Mellebeek, Francesc Benavent, Jens Grivolla, JoanCodina, Marta R. Costa-Jussa`, and Rafael Banchs.2010.
Opinion mining of spanish customer commentswith non-expert annotations on Mechanical Turk.
InNAACL Workshop on Creating Speech and LanguageData With Amazon?s Mechanical Turk.Robert Munro, Steven Bethard, Victor Kuperman,Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,Tyler Schnoebelen, and Harry Tily.
2010.
Crowd-sourcing and language studies: the new generationof linguistic data.
In NAACL Workshop on CreatingSpeech and Language Data With Amazon?s Mechani-cal Turk.Matteo Negri and Yashar Mehdad.
2010.
Creating abi-lingual entailment corpus through translations withMechanical Turk: $100 for a 10 days rush.
In NAACLWorkshop on Creating Speech and Language DataWith Amazon?s Mechanical Turk.Scott Novotney and Chris Callison-Burch.
2010a.Cheap, fast and good enough: Automatic speechrecognition with non-expert transcription.
NAACL,June.Scott Novotney and Chris Callison-Burch.
2010b.Crowdsourced accessibility: Elicitation of Wikipediaarticles.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Empirical Methods inNatural Language Processing (EMNLP).Gabriel Parent and Maxine Eskenazi.
2010.
Cluster-ing dictionary definitions using Amazon MechanicalTurk.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The acl anthology network.
In Pro-ceedings of the 2009 Workshop on Text and CitationAnalysis for Scholarly Digital Libraries, pages 54?61,Suntec City, Singapore, August.
Association for Com-putational Linguistics.Cyrus Rashtchian, Peter Young, Micah Hodosh, and JuliaHockenmaier.
2010.
Collecting image annotations us-ing Amazon?s Mechanical Turk.
In NAACL Workshopon Creating Speech and Language Data With Ama-zon?s Mechanical Turk.Joel Rose.
2010.
Some turn to ?Mechanical?
job search.http://marketplace.publicradio.org/display/web/2009/06/30/pm_turking/.Marketplace public radio.
Air date: June 30, 2009.Erik Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In CoNLL-2003.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP-2008), Honolulu, Hawaii.Audrey Tong, Jerome Ajot, Mark Przybocki, andStephanie Strassel.
2010.
Document image collectionusing Amazon?s Mechanical Turk.
In NAACL Work-shop on Creating Speech and Language Data WithAmazon?s Mechanical Turk.Luis von Ahn and Laura Dabbish.
2008.
General tech-niques for designing games with a purpose.
Commu-nications of the ACM.Rui Wang and Chris Callison-Burch.
2010.
Cheap factsand counter-facts.
In NAACL Workshop on Creating11Speech and Language Data With Amazon?s Mechani-cal Turk.Tae Yano, Philip Resnik, and Noah A Smith.
2010.Shedding (a thousand points of) light on biased lan-guage.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Meliha Yetisgen-Yildiz, Imre Solti, Scott Halgrim, andFei Xia.
2010.
Preliminary experiments with Ama-zon?s Mechanical Turk for annotating medical namedentities.
In NAACL Workshop on Creating Speech andLanguage Data With Amazon?s Mechanical Turk.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasi-bility of human-in-the-loop minimum error rate train-ing.
In Proceedings of EMNLP 2009, pages 52?61,August.Omar Zaidan and Juri Ganitkevitch.
2010.
An enrichedMT grammar for under $100.
In NAACL Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk.12
