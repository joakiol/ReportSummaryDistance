Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 41?48,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsA Cognitive Model for the Representation and Acquisitionof Verb Selectional PreferencesAfra AlishahiDepartment of Computer ScienceUniversity of Torontoafra@cs.toronto.eduSuzanne StevensonDepartment of Computer ScienceUniversity of Torontosuzanne@cs.toronto.eduAbstractWe present a cognitive model of inducingverb selectional preferences from individ-ual verb usages.
The selectional preferencesfor each verb argument are represented asa probability distribution over the set ofsemantic properties that the argument canpossess?a semantic profile.
The seman-tic profiles yield verb-specific conceptual-izations of the arguments associated with asyntactic position.
The proposed model canlearn appropriate verb profiles from a smallset of noisy training data, and can use themin simulating human plausibility judgmentsand analyzing implicit object alternation.1 IntroductionVerbs have preferences for the semantic propertiesof the arguments filling a particular role.
For ex-ample, the verb eat expects that the object receivingits theme role will have the property of being edi-ble, among others.
Learning verb selectional pref-erences is an important aspect of human languageacquisition, and the acquired preferences have beenshown to guide children?s expectations about miss-ing or upcoming arguments in language comprehen-sion (Nation et al, 2003).Resnik (1996) introduced a statistical approachto learning and use of verb selectional preferences.In this framework, a semantic class hierarchy forwords is used, together with statistical tools, to in-duce a verb?s selectional preferences for a particu-lar argument position in the form of a distributionover all the classes that can occur in that position.Resnik?s model was proposed as a model of humanlearning of selectional preferences that made min-imal representational assumptions; it showed howsuch preferences could be acquired from usage dataand an existing conceptual hierarchy.
However, hisand later computational models (see Section 2) haveproperties that do not match with certain cognitiveplausibility criteria for a child language acquisitionmodel.
All these models use the training data in?batch mode?, and most of them use informationtheoretic measures that rely on total counts from acorpus.
Therefore, it is not clear how the representa-tion of selectional preferences could be updated in-crementally in these models as the person receivesmore data.
Moreover, the assumption that childrenhave access to a full hierarchical representation ofsemantic classes may be too strict.
We propose analternative view in this paper which is more plausi-ble in the context of child language acquisition.In previous work (Alishahi and Stevenson, 2005),we have proposed a usage-based computationalmodel of early verb learning that uses Bayesian clus-tering and prediction to model language acquisitionand use.
Individual verb usages are incrementallygrouped to form emergent classes of linguistic con-structions that share semantic and syntactic proper-ties.
We have shown that our Bayesian model canincrementally acquire a general conception of thesemantic roles of predicates based only on expo-sure to individual verb usages (Alishahi and Steven-son, 2007).
The model forms probabilistic associa-tions between the semantic properties of arguments,their syntactic positions, and the semantic primitives41of verbs.
Our previous experiments demonstratedthat, initially, this probability distribution for an ar-gument position yields verb-specific conceptualiza-tions of the role associated with that position.
As themodel is exposed to more input, the verb-based rolesgradually transform into more abstract representa-tions that reflect the general properties of argumentsacross the observed verbs.A shortcoming of the model was that, becausethe prediction of the semantic roles was based onlyon the groupings of verbs, it could not make use ofverb-specific knowledge in generating expectationsabout a particular verb?s arguments.
That is, onceit was exposed to a range of verbs, it no longer hadaccess to the verb-specific information, only to gen-eralizations over clusters of verbs.In this paper, we propose a new version of ourmodel that, in addition to learning general seman-tic roles for constructions, can use its verb-specificknowledge to predict intuitive selectional prefer-ences for each verb argument position.
We introducea new notion, a verb semantic profile, as a prob-ability distribution over the semantic properties ofan argument for each verb.
A verb semantic pro-file is predicted from both the verb-based and theconstruction-based knowledge that the model haslearned through clustering, and reflects the prop-erties of the arguments that are observed for thatverb.
Our proposed prediction model makes appro-priate generalizations over the observed properties,and captures expectations about previously unseenarguments.As in other work on selectional preferences, thesemantic properties that we use in our representa-tion of arguments are drawn from a standard lex-ical ontology (WordNet; Miller, 1990), but we donot require knowledge of the hierarchical structureof the WordNet concepts.
From the computationalpoint of view, this makes use of an available re-source, while from the cognitive view, this avoidsad hoc assumptions about the representation of aconceptual hierarchy.
However, we do require someproperties to be more general (i.e., shared by morewords) than others, which eventually enables themodel to make appropriate generalizations.
Other-wise, the selected semantic properties are not fun-damental to the model, and could in the future bereplaced with an approach that is deemed more ap-propriate to child language acquisition.
Each argu-ment contributes to the semantic profile of the verbthrough its (potentially large) set of semantic prop-erties instead of its membership in a single class.
Asinput to our model, we use an automatically parsedcorpus, which is very noisy.
However, as a result ofour novel representation, the model can induce anduse selectional preferences using a relatively smallset of noisy training data.2 Related Computational ModelsA variety of computational models for verb selec-tional preferences have been proposed, which usedifferent statistical models to induce the preferencesof each verb from corpus data.
Most of thesemodels, however, use the same representation forverb selectional preferences: the preference can bethought of as a mapping, with respect to an argumentposition for a verb, of each class to a real number(Light and Greiff, 2002).
The induction of a verb?spreferences is, therefore, modeled as using a set oftraining data to estimate that number.Resnik (1996) defines the selectional preferencestrength of a verb as the divergence between twoprobability distributions: the prior probabilities ofthe classes, and the posterior probabilities of theclasses given that verb.
The selectional associationof a verb with a class is also defined as the contribu-tion of that class to the total selectional preferencestrength.
Resnik estimates the prior and posteriorprobabilities based on the frequencies of each verband its relevant argument in a corpus.Li and Abe (1998) model selectional preferencesof a verb (for an argument position) as a set of nodesin the semantic class hierarchy with a probabilitydistribution over them.
They use the Minimum De-scription Length (MDL) principle to find the best setfor each verb and argument based on the usages ofthat verb in the training data.
Clark and Weir (2002)also find an appropriate set of concept nodes to rep-resent the selectional preferences for a verb, but doso using a ?2 test over corpus frequencies mappedto concepts to determine when to generalize from anode to its parent.
Ciaramita and Johnson (2000)use a Bayesian network with the same topology asWordNet to estimate the probability distribution ofthe relevant set of nodes in the hierarchy.
Abney42and Light (1999) use a different representational ap-proach: they train a separate hidden Markov modelfor each verb, and the selectional preference is rep-resented as a probability distribution over words in-stead of semantic classes.3 The Bayesian Verb-Learning Model3.1 Overview of the ModelOur model learns the set of argument structureframes for each verb, and their grouping across verbsinto constructions.
An argument structure frame isa set of features of a verb usage that are both syn-tactic (the number of arguments, the syntactic pat-tern of the usage) and semantic (the semantic prop-erties of the verb, the semantic properties of eachargument).
The syntactic pattern indicates the wordorder of the verb and arguments.
A construction isa grouping of individual frames which probabilisti-cally share syntactic and semantic features, and formprobabilistic associations across verb semantic prop-erties, argument semantic properties, and syntacticpattern.
These groupings typically correspond togeneral constructions in the language such as tran-sitive, intransitive, and ditransitive.For each verb, the model associates an argumentposition with a probability distribution over a set ofsemantic properties?a semantic profile.
In doingso, the model uses the knowledge that it has learnedfor that verb, as well as the grouping of frames forthat verb into constructions.The semantic properties of words are taken fromWordNet (version 2.0) as follows.
We extract all thehypernyms (ancestors) for all the senses of the word,and add all the words in the hypernym synsets to thelist of the semantic properties.
Figure 1 shows an ex-ample of the hypernyms for dinner, and its resultingset of semantic properties.1The following sections review basic propertiesof the model from Alishahi and Stevenson (2005,2007), and introduce extensions that give the modelits ability to make verb-based predictions.3.2 Learning as Bayesian ClusteringEach argument structure frame for an observed verbusage is input to an incremental Bayesian clustering1We do not remove alternate spellings of a term in WordNet;this will be seen in the profiles in the results section.Sense 1dinner=> meal, repast=> nutriment, nourishment, nutrition, sustenance,aliment, alimentation, victuals=> food, nutrient=> substance, matter=> entitySense 2dinner, dinner party=> party=> social gathering, social affair=> gathering, assemblage=> social group=> group, groupingdinner: {meal, repast, nutriment, nourishment, nutrition, substance, aliment, alimentation,victuals, food, nutrient, substance, matter, entity, party, social gathering,social affair, gathering, assemblage, social group, group, grouping }Figure 1: Semantic properties for dinner from Word-Netprocess.
This process groups the new frame togetherwith an existing group of frames?a construction?that probabilistically has the most similar semanticand syntactic properties to it.
If no construction hassufficiently high probability for the new frame, thena new construction is created for it.
We use the prob-abilistic model of Alishahi and Stevenson (2007) forlearning constructions, which is itself an adaptationof a Bayesian model of human categorization pro-posed by Anderson (1991).
It is important to notethat the categories (i.e., constructions) are not prede-fined, but rather are created according to the patternsof similarity over observed frames.Grouping a frame F with other frames participat-ing in construction k is formulated as finding the kwith the maximum probability given F :BestConstruction(F ) = argmaxkP (k|F ) (1)where k ranges over the indices of all constructions,with index 0 representing recognition of a new con-struction.Using Bayes rule, and dropping P (F ) which isconstant for all k:P (k|F ) = P (k)P (F |k)P (F ) ?
P (k)P (F |k) (2)The prior probability, P (k), indicates the degree ofentrenchment of construction k, and is given by therelative frequency of its frames over all observedframes.
The posterior probability of a frame F isexpressed in terms of the individual probabilities ofits features, which we assume are independent, thusyielding a simple product of feature probabilities:43P (F |k) =?i?FrameFeaturesPi(j|k) (3)where j is the value of the ith feature of F , andPi(j|k) is the probability of displaying value j onfeature i within construction k. Given the focus hereon semantic profiles, we next focus on the calcula-tion of the probabilities of semantic properties.3.3 Probabilities of Semantic PropertiesThe probability in equation (3) of value j for featurei in construction k is estimated using a smoothedversion of this maximum likelihood formula:Pi(j|k) =countki (j)nk(4)where nk is the number of frames participating inconstruction k, and countki (j) is the number ofthose with value j for feature i.For most features, countki (j) is calculated bysimply counting those members of construction kwhose value for feature i exactly matches j. How-ever, for the semantic properties of words, countingonly the number of exact matches between the setsis too strict, since even highly similar words veryrarely have the exact same set of properties.
Weinstead use the following Jaccard similarity scoreto measure the overlap between the set of semanticproperties, SF , of a particular argument in the frameto be clustered, and the set of semantic properties,Sk, of the same argument in a member frame of aconstruction:sem score(SF , Sk) =|SF ?
Sk||SF ?
Sk|(5)For example, assume that the new frame F repre-sents a usage of John ate cake.
In the constructionthat we are considering for inclusion of F , one ofthe member frames represents a usage of Mom gotwater.
We must compare the semantic properties ofthe corresponding arguments cake and water:cake: {baked goods,food,solid,substance,matter,entity}water: {liquid,fluid,food,nutrient,substance,matter,entity}The intersection of the two sets is {food, substance,matter, entity}, yielding a sem score of 49 .In general, to calculate the conditional probabilityfor the set of semantic properties, we set countki (j)in equation (4) to the sum of the sem score?s forthe new frame and every member of construction k,and normalize the resulting probability over all pos-sible sets of semantic properties in our lexicon.3.4 Predicting Semantic Profiles for VerbsWe represent the selectional preferences of a verbfor an argument position as a semantic profile, whichis a probability distribution over all the semanticproperties.
To predict the profile of a verb v foran argument position arg , we need to estimate theprobability of each semantic property j separately:Parg (j|v) =?kParg(j, k|v) (6)?
?kP (k, v)Parg (j|k, v)Here, j ranges over all the possible semantic proper-ties that an argument can have, and k ranges over allconstructions.
The prior probability of having verb vin construction k, or P (k, v), takes into account twoimportant factors: the relative entrenchment of theconstruction k, and the (smoothed) frequency withwhich v participates in k.The posterior probability Parg (j|k, v) is calcu-lated analogously to Pi(j|k) in equation (4), but lim-iting the count of matching features to those framesin k that contain v:Parg (j|k, v) =verb countkarg (j, v)nkv(7)where nkv is the number of frames for v participat-ing in construction k, and verb countkarg(j, v) isthe number of those with semantic property j forargument arg .
We use a smoothed version of theabove formula, where the relative frequency of eachproperty j among all nouns is used as the smoothingfactor.3.5 Verb-Argument CompatibilityIn one of our experiments, we need to measure thecompatibility of a particular noun n for an argumentposition arg of some verb v. That is, we need to es-timate how much the semantic properties of n con-form to the acquired semantic profile of v for arg .We formulate the compatibility as the conditionalprobability of observing n as an argument arg of v:compatibility(v, n) = log(Parg (jn|v)) (8)44where jn is the set of the semantic properties forword n, and Parg (jn|v) is estimated as in equa-tion (7).
However, since jn here is a set of prop-erties (as opposed to j in equation (7) being asingle property), verb countkarg in equation (7)should be modified as described in Section 3.3:we set verb countkarg (jn, v) to the sum of thesem score?s (equation (5)) for jn and every frameof v that participates in construction k.4 Experimental ResultsIn the following sections, we first describe the train-ing data for our model.
In accordance with othercomputational models, we focus here on the verbpreferences for the direct object position.2 Next, weprovide a qualitative analysis of our model throughexamination of the semantic profiles for a numberof verbs.
We then evaluate our model through twotasks of simulating verb-argument plausibility judg-ment, and analyzing the implicit object alternation,following Resnik (1996).34.1 The Training DataIn earlier work (Alishahi and Stevenson, 2005,2007), we used a method to automatically generatetraining data with the same distributional propertiesas the input children receive.
However, this relies onmanually-compiled data about verbs and their argu-ment structure frames from the CHILDES database(MacWhinney, 1995).
To evaluate the new versionof our model for the task of learning selectional pref-erences, we need a wide selection of verbs and theirarguments that is impractical to compile by hand.The training data for our experiments here aregenerated as follows.
We use 20,000 sentencesrandomly selected from the British National Cor-pus (BNC),4 automatically parsed using the Collinsparser (Collins, 1999), and further processed withTGrep2,5 and an NP-head extraction software.6 For2To our knowledge, the only work that considers selectionalpreferences of subjects and prepositional phrases as well as di-rect objects is Brockmann and Lapata (2003).3Computational models of verb selectional preference havebeen evaluated through disambiguation tasks (Li and Abe,1998; Abney and Light, 1999; Ciaramita and Johnson, 2000;Clark and Weir, 2002), but for to evaluate our cognitive model,the experiments from Resnik (1996) are the most interesting.4http://www.natcorp.ox.ac.uk5http://tedlab.mit.edu/?dr/Tgrep26The software was provided to us by Eric Joanis, and Af-each verb usage in a sentence, we construct a frameby recording the verb in root form, the number ofthe arguments for that verb, and the syntactic patternof the verb usage (i.e., the word order of the verband the arguments).
We also record in the frame thesemantic properties of the verb and each of the ar-gument heads (each noun is also converted to rootform); these properties are extracted from WordNet(as discussed in Section 3.1 and illustrated in Fig-ure 1).
This process results in 16,300 frames whichserve as input data to our learning model.4.2 Formation of Semantic Profiles for VerbsAfter training our model on the above data, we useequation (7) to predict the semantic profile of the di-rect object position for a range of verbs.
Some ofthese verbs, such as write and sing, have strong se-lectional preferences, whereas others, such as wantand put, can take a wide range of nouns as directobject (as confirmed by Resnik?s (1996) estimatedstrength of selectional preference for these verbs).The semantic profiles for write and sing are dis-played in Figure 2, and the profiles for want and putare displayed in Figure 3.
(Due to limited space, weonly include the 25 properties that have the highestprobability in each profile.
)Because we extract the semantic properties ofwords from WordNet, which has a hierarchicalstructure, the properties that come from nodes inthe higher levels of the hierarchy (such as entity andabstraction) appear as the semantic property for avery large set of words, whereas the properties thatcome from the leaves in the hierarchy are specific toa small set of words.
Therefore, the general prop-erties are more likely to be associated with a higherprobability in the semantic profiles for most verbs.In fact, a closer look at the semantic profiles for wantand put reveals that the top portion of the semanticprofile for these verbs consists solely of such gen-eral properties that are shared among a large groupof words.
However, this is not the case for the morerestrictive verbs.
The semantic profiles for write andsing show that the specific properties that these verbsdemand from their direct object appear amongst thehighest-ranked properties, even though only a smallset of words share these properties (e.g., content,saneh Fazly helped us in using the above-mentioned tools forgenerating our input corpora.45write(0.024) abstraction(0.022) entity(0.021) location(0.020) substance(0.019) destination(0.018) relation(0.015) communication(0.015) social relation(0.013) content(0.011) message(0.011) subject matter(0.011) writtencommunication(0.011) writtenlanguage(0.010) object(0.010) physical object(0.010) writing(0.010) goal(0.010) unit(0.009) whole(0.009) whole thing(0.009) artifact(0.009) artefact(0.009) state(0.009) amount(0.009) measuresing(0.020) abstraction(0.015) relation(0.015) communication(0.015) social relation(0.013) act(0.013) human action(0.013) human activity(0.013) auditorycommunication(0.012) music(0.010) entity(0.010) piece(0.009) composition(0.009) musicalcomposition(0.009) opus(0.009) piece of music(0.009) psychologicalfeature(0.008) cognition(0.008) knowledge(0.008) noesis(0.008) activity(0.008) content(0.008) grouping(0.008) group(0.008) amount(0.008) measureFigure 2: Semantic profiles of write and sing for thedirect object position.message, written communication, written language,... for write, and auditory communication, music,musical composition, opus, ... for sing).The examination of the semantic profiles for fairlyfrequent verbs in the training data shows that ourmodel can use the verb usages to predict an appro-priate semantic profile for each verb.
When pre-sented with a novel verb (for which no verb-basedinformation is available), equation (7) predicts a se-mantic profile which reflects the relative frequenciesof the semantic properties among all words (due tothe smoothing factor added to equation (7)), modu-lated by the prior probability of each construction.The predicted profile is displayed in Figure 4.
Itshows similarities with the profiles for want and putin Figure 3, but the general properties in this profilehave an even higher probability.
Since the profile forthe novel verb is predicted in the absence of any evi-dence (i.e., verb usage) in the training data, we lateruse it as the base for estimating other verbs?
strengthof selectional preference.want(0.016) entity(0.015) object(0.015) physical object(0.014) abstraction(0.013) act(0.012) human action(0.012) human activity(0.012) relation(0.011) unit(0.011) whole(0.011) whole thing(0.011) artifact(0.011) artefact(0.008) communication(0.008) social relation(0.008) activity(0.007) cause(0.007) state(0.007) instrumentality(0.007) instrumentation(0.007) event(0.006) being(0.006) living thing(0.006) animate thing(0.006) organismput(0.015) entity(0.015) object(0.013) physical object(0.013) abstraction(0.011) unit(0.011) whole(0.011) whole thing(0.011) artifact(0.011) artefact(0.010) act(0.009) relation(0.008) human action(0.008) human activity(0.008) communication(0.008) social relation(0.007) substance(0.007) content(0.007) instrumentality(0.007) instrumentation(0.007) measure(0.006) amount(0.006) quantity(0.006) cause(0.006) causal agent(0.006) causal agencyFigure 3: Semantic profiles of want and put for thedirect object position.4.3 Verb-Argument Plausibility JudgmentsHolmes et al (1989) evaluate verb argument plau-sibility by asking human subjects to rate sentenceslike The mechanic warned the driver and The me-chanic warned the engine.
Resnik (1996) used thisdata to assess the performance of his model by com-paring its judgments of selectional fit against theplausibility ratings elicited from human subjects.
Heshowed that his selectional association measure fora verb and its direct object can be used to select themore plausible verb-noun pair among the two (e.g.,<warn,driver> vs. <warn,engine> in the previousexample).
That is, a higher selectional associationbetween the verb and one of the nouns compared tothe other noun indicates that the former is the moreplausible pair.
Resnik (1996) used the Brown corpusas training data, and showed that his model arrivesat the correct ordering of more and less plausible ar-guments in 11 of the 16 cases.We repeated this experiment, using the same 16pairs of verb-noun combinations.
For each pair of<v, n1> and <v, n2>, we calculate the compati-bility measure using equation (8); these values areshown in Figure 5.
(Note that because these are46A novel verb(0.021) entity(0.017) object(0.017) physical object(0.015) abstraction(0.010) act(0.010) human action(0.010) human activity(0.010) unit(0.009) whole(0.009) whole thing(0.009) artifact(0.009) artefact(0.009) being(0.009) living thing(0.009) animate thing(0.009) organism(0.008) cause(0.008) causal agent(0.008) causal agency(0.008) relation(0.008) person(0.008) individual(0.008) someone(0.008) somebody(0.008) mortalFigure 4: Semantic profile of a novel verb for thedirect object position.log-probabilities and therefore negative numbers,a lower absolute value of compatibility(v, n)shows a better compatibility between the verb vand the argument n.) For example, <see,friend>has a higher compatibility score (-30.50) than<see,method> (-32.14).
Similar to Resnik, ourmodel detects 11 plausible pairs out of 16.
How-ever, these results are reached with a much smallertraining corpus (around 500,000 words), comparedto the Brown corpus used by Resnik (1996) whichcontains one million words.
Moreover, whereas theBrown corpus is tagged and parsed manually, theportion of the BNC that we use is parsed automat-ically, and as a result our training data is very noisy.Nonetheless, the model achieves the same level ofaccuracy in distinguishing plausible verb-argumentpairs from implausible ones.4.4 Implicit Object AlternationsIn English, some inherently transitive verbs can ap-pear with or without their direct objects (e.g., Johnate his dinner as well as John ate), but others can-not (e.g., Mary made a cake but not *Mary made).It is argued that implicit object alternations involve aVerb Plausible Implausiblesee friend -30.50 method -32.14read article -32.76 fashion -33.33find label -32.05 fever -33.30hear story -32.11 issue -32.40write letter -31.37 market -32.46urge daughter -36.73 contrast -35.64warn driver -33.68 engine -34.42judge contest -39.05 climate -38.23teach language -45.64 distance -45.11show sample -31.75 travel -31.42expect visit -33.88 mouth -32.87answer request -31.89 tragedy -33.95recognize author -32.53 pocket -32.62repeat comment -33.80 journal -33.97understand concept -32.25 session -32.93remember reply -33.79 smoke -34.29Figure 5: Compatibility scores for plausible vs. im-plausible verb-noun pairs.particular relationship between the verb and its argu-ment.
In particular, for verbs that participate in theimplicit object alternation, the omitted object mustbe in some sense inferable or typical for that verb(Levin, 1993, among others).Resnik (1996) used his model of selectional pref-erences to analyze implicit object alternations, andshowed a relationship between his measure of se-lectional preference strength and the notion of typ-icality of an object.
He calculated this measurefor two groups of Alternating and Non-alternatingverbs, and showed that, on average, the Alternatingverbs have a higher strength of selectional prefer-ence for the direct object than the Non-alternatingverbs.
However, there was no threshold separatingthe two groups of verbs.To repeat Resnik?s experiment, we need a mea-sure of how ?strongly constraining?
a semantic pro-file is.
We can do this by measuring the similaritybetween the semantic profile we generate for the ob-ject of a particular verb and some ?default?
notion ofthe argument for that position across all verbs.
Weuse the semantic profile predicted for the object po-sition of a novel verb, shown earlier in Figure 4, asthe default profile for that argument position.
Be-cause this profile is predicted in the absence of anyevidence in the training data, it makes the minimumassumptions about the properties of the argumentand thus serves as a suitable default.
We then assumethat verbs with weaker selectional preferences havesemantic profiles more similar to the default profile47Alternating verbs Non-alternating verbswrite 0.61 hang 0.56sing 0.67 wear 0.71drink 0.67 say 0.75eat 0.74 catch 0.76play 0.74 show 0.77pour 0.76 make 0.78watch 0.77 hit 0.78pack 0.78 open 0.81steal 0.80 take 0.83push 0.80 see 0.87call 0.80 like 0.87pull 0.80 get 0.87explain 0.81 find 0.87read 0.82 give 0.88hear 0.87 bring 0.89want 0.89put 0.90Mean: 0.76 Mean: 0.81Figure 6: Similarity with the base profile for Alter-nating and Non-alternating verbs.than verbs with stronger preferences.
We use thecosine measure to estimate the similarity betweentwo profiles p and q:cosine(p, q) = p?
q||p|| ?
||q|| (9)The similarity values for the Alternating and Non-alternating verbs are shown in Figure 6.
The largervalues represent more similarity with the base pro-file, which means a weaker selectional preference.The means for the Alternating and Non-alternatingverbs were respectively 0.76 and 0.81, which con-firm the hypothesis that verbs participating in im-plicit object alternations select more strongly for thedirect objects than verbs that do not.
However, likeResnik (1996), we find that it is not possible to set athreshold that will distinguish the two sets of verbs.5 ConclusionsWe have proposed a cognitively plausible model forlearning selectional preferences from instances ofverb usage.
The model represents verb selectionalpreferences as a semantic profile, which is a prob-ability distribution over the semantic properties thatan argument can take.
One of the strengths of ourmodel is the incremental nature of its learning mech-anism, in contrast to other approaches which learnselectional preferences in batch mode.
Here we haveonly reported the results for the final stage of learn-ing, but the model allows us to monitor the semanticprofiles during the course of learning, and compareit with child data for different age groups, as we dowith semantic roles (Alishahi and Stevenson, 2007).We have shown that the model can predict appropri-ate semantic profiles for a variety of verbs, and usethese profiles to simulate human judgments of verb-argument plausibility, using a small and highly noisyset of training data.
The model can also use the pro-files to measure verb-argument compatibility, whichwas used in analyzing the implicit object alternation.ReferencesAbney, S. and Light, M. (1999).
Hiding a semantic hierarchyin a Markov model.
In Proc.
of the ACL Workshop on Unsu-pervised Learning in Natural Language Processing.Alishahi, A. and Stevenson, S. (2005).
A probabilistic model ofearly argument structure acquisition.
In Proc.
of the CogSci2005.Alishahi, A. and Stevenson, S. (2007).
A computational usage-based model for learning general properties of semanticroles.
In Proc.
of the EuroCogSci 2007.Anderson, J. R. (1991).
The adaptive nature of human catego-rization.
Psychological Review, 98(3):409?429.Brockmann, C. and Lapata, M. (2003).
Evaluating and com-bining approaches to selectional preference acquisition.
InProc.
of the EACL 2003.Ciaramita, M. and Johnson, M. (2000).
Explaining away am-biguity: Learning verb selectional preference with Bayesiannetworks.
In Proc.
of the COLING 2000.Clark, S. and Weir, D. (2002).
Class-based probability estima-tion using a semantic hierarchy.
Computational Linguistics,28(2):187?206.Collins, M. (1999).
Head-Driven Statistical Models for NaturalLanguage Parsing.
PhD thesis, University of Pennsylvania.Holmes, V. M., Stowe, L., and Cupples, L. (1989).
Lexicalexpectations in parsing complement-verb sentences.
Journalof Memory and Language, 28:668?689.Levin, B.
(1993).
English verb classes and alternations: A pre-liminary investigation.
The University of Chicago Press.Li, H. and Abe, N. (1998).
Generalizing case frames using athesaurus and the MDL principle.
Computational Linguis-tics, 24(2):217?244.Light, M. and Greiff, W. (2002).
Statistical models for the in-duction and use of selectional preferences.
Cognitive Sci-ence, 26(3):269?281.MacWhinney, B.
(1995).
The CHILDES project: Tools for an-alyzing talk.
Lawrence Erlbaum.Miller, G. (1990).
WordNet: An on-line lexical database.
Inter-national Journal of Lexicography, 17(3).Nation, K., Marshall, C. M., and Altmann, G. T. M. (2003).
In-vestigating individual differences in children?s real-time sen-tence comprehension using language-mediated eye move-ments.
J. of Experimental Child Psych., 86:314?329.Resnik, P. (1996).
Selectional constraints: An information-theoretic model and its computational realization.
Cognition,61:127?199.48
