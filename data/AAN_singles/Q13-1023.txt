Transactions of the Association for Computational Linguistics, 1 (2013) 279?290.
Action Editor: Lillian Lee.Submitted 11/2012; Revised 1/2013; Published 7/2013.
c?2013 Association for Computational Linguistics.Good, Great, Excellent:Global Inference of Semantic IntensitiesGerard de MeloICSI, Berkeleydemelo@icsi.berkeley.eduMohit BansalCS Division, UC Berkeleymbansal@cs.berkeley.eduAbstractAdjectives like good, great, and excellent aresimilar in meaning, but differ in intensity.
In-tensity order information is very useful forlanguage learners as well as in several NLPtasks, but is missing in most lexical resources(dictionaries, WordNet, and thesauri).
In thispaper, we present a primarily unsupervisedapproach that uses semantics from Web-scaledata (e.g., phrases like good but not excel-lent) to rank words by assigning them posi-tions on a continuous scale.
We rely on MixedInteger Linear Programming to jointly deter-mine the ranks, such that individual decisionsbenefit from global information.
When rank-ing English adjectives, our global algorithmachieves substantial improvements over pre-vious work on both pairwise and rank corre-lation metrics (specifically, 70% pairwise ac-curacy as compared to only 56% by previouswork).
Moreover, our approach can incorpo-rate external synonymy information (increas-ing its pairwise accuracy to 78%) and extendseasily to new languages.
We also make ourcode and data freely available.11 IntroductionCurrent lexical resources such as dictionaries andthesauri do not provide information about the in-tensity order of words.
For example, both WordNet(Miller, 1995) and Roget?s 21st Century Thesaurus(thesaurus.com) present acceptable, great, and su-perb as synonyms of the adjective good.
However,a native speaker knows that these words representvarying intensity and can in fact generally be rankedby intensity as acceptable< good< great< superb.Similarly, warm < hot < scorching are identified assynonyms in these resources.
Ranking information,1http://demelo.org/gdm/intensity/however, is crucial because it allows us to differen-tiate e.g.
between various intensities of an emotion,and is hence very useful for humans when learning alanguage or judging product reviews, as well as forautomatic text understanding and generation taskssuch as sentiment and subjectivity analysis, recog-nizing textual entailment, question answering, sum-marization, and coreference and discourse analysis.In this work, we attempt to automatically ranksets of related words by intensity, focusing in par-ticular on adjectives.
This is made possible by thevast amounts of world knowledge that are now avail-able.
We use lexico-semantic information extractedfrom a Web-scale corpus in conjunction with an al-gorithm based on a Mixed Integer Linear Program(MILP).
Linguistic analyses have identified phrasessuch as good but not great or hot and almost scorch-ing in a text corpus as sources of evidence about therelative intensities of words.
However, pure infor-mation extraction approaches often fail to provideenough coverage for real-world downstream appli-cations (Tandon and de Melo, 2010), unless someform of advanced inference is used (Snow et al2006; Suchanek et al 2009).In our work, we address this sparsity problem byrelying on Web-scale data and using an MILP modelthat extends the pairwise scores to a more com-plete joint ranking of words on a continuous scale,while maintaining global constraints such as transi-tivity and giving more weight to the order of wordpairs with higher corpus evidence scores.
Insteadof considering intensity ranking as a pairwise deci-sion process, we thus exploit the fact that individualdecisions may benefit from global information, e.g.about how two words relate to some third word.Previous work (Sheinman and Tokunaga, 2009;Schulam and Fellbaum, 2010; Sheinman et al2012) has also used lexico-semantic patterns to or-279der adjectives.
They mainly evaluate their algorithmon a set of pairwise decisions, but also present a par-titioning approach that attempts to form scales byplacing each adjective to the left or right of pivotwords.
Unfortunately, this approach often fails be-cause many pairs lack order-based evidence even onthe Web, as explained in more detail in Section 3.In contrast, our MILP jointly uses informationfrom all relevant word pairs and captures com-plex interactions and inferences to produce inten-sity scales.
We can thus obtain an order betweentwo adjectives even when there is no explicit evi-dence in the corpus (using evidence for related pairsand transitive inference).
Our global MILP is flex-ible and can also incorporate additional synonymyinformation if available (which helps the MILP findan even better ranking solution).
Our approach alsoextends easily to new languages.
We describe twoapproaches for this multilingual extension: patternprojection and cross-lingual MILPs.We evaluate our predicted intensity rankings us-ing both pairwise classification accuracy and rank-ing correlation coefficients, achieving strong results,significantly better than the previous approach bySheinman & Tokunaga (32% relative error reduc-tion) and quite close to human-level performance.2 MethodIn this section, we describe each step of our ap-proach to ordering adjectives on a single, relativescale.
Our method can also be applied to other wordclasses and to languages other than English.2.1 Web-based Scoring Model2.1.1 Intensity ScalesNear-synonyms may differ in intensity, e.g.
joyvs.
euphoria, or drizzle vs. rain.
This is particu-larly true of adjectives, which can represent differentdegrees of a given quality or attribute such as sizeor age.
Many adjectives are gradable and thus al-low for grading adverbial modifiers to express suchintensity degrees, e.g., a house can be very big orextremely big.
Often, however, completely differ-ent adjectives refer to varying degrees on the samescale, e.g., huge, gigantic, gargantuan.
Even adjec-tives like enormous (or superb, impossible) that areconsidered non-gradable from a syntactic perspec-tive can be placed on a such a scale.Weak-Strong Patterns Strong-Weak Patterns?
(,) but not ?
not ?
(,) just ??
(,) if not ?
not ?
(,) but just ??
(,) although not ?
not ?
(,) still ??
(,) though not ?
not ?
(,) but still ??
(,) (and/or) even ?
not ?
(,) although still ??
(,) (and/or) almost ?
not ?
(,) though still ?not only ?
but ?
?
(,) or very ?not just ?
but ?Table 1: Ranking patterns used in this work.
Among thepatterns represented by the regular expressions above, weuse only those that capture less than or equal to five words(to fit in the Google n-grams, see Section 2.1.2).
Articles(a, an, the) are allowed to appear before the wildcardswherever possible.2.1.2 Intensity PatternsLinguistic studies have found lexical patterns like??
but not ??
(e.g.
good but not great) to reveal orderinformation between a pair of adjectives (Sheinmanand Tokunaga, 2009).
We assume that we have twosets of lexical patterns that allow us to infer the mostlikely ordering between two words when encoun-tered in a corpus.
A first pattern set, Pws, containspatterns that reflect a weak-strong order between apair of word (the first word is weaker than the sec-ond), and a second pattern set, Psw, captures thestrong-weak order.
See Table 1 for the adjective pat-terns that we used in this work (and see Section 4.1for implementation details regarding our pattern col-lection).
Many of these patterns also apply to otherparts of speech (e.g.
?drizzle but not rain?, ?runningor even sprinting?
), with significant discriminationon the Web in the right direction.2.1.3 Pairwise ScoresGiven an input set of words to be placed on ascale, we first collect evidence of their intensity or-der by using the above-mentioned intensity patternsand a large, Web-scale text corpus.Previous work on information extraction fromlimited-sized raw text corpora revealed that cover-age is often limited (Hearst, 1992; Hatzivassiloglouand McKeown, 1993).
Some studies (Chklovskiand Pantel, 2004; Sheinman and Tokunaga, 2009)used hit counts from an online search engine, butthis is unstable and irreproducible (Kilgarriff, 2007).To avoid these issues, we use the largest available280(good, great) (great, good) (small, minute)good , but not great?
24492.0 not great , just good?
248.0 small , almost minute?
97.0good , if not great?
1912.0 great or very good?
89.0 small , even minute?
41.0good , though not great?
504.0 not great but still good?
47.0good , or even great?
338.0not just good but great?
181.0good , almost great?
156.0Table 2: Some examples from the Web-scale corpus of useful intensity-based phrases on adjective pairs.static corpus of counts, the Google n-grams corpus(Brants and Franz, 2006), which contains Englishn-grams (n = 1 to 5) and their observed frequencycounts, generated from nearly 1 trillion word tokensand 95 billion sentences.We consider each pair of words (a1, a2) in the in-put set in turn.
For each pattern p in the two patternsets (weak-strong Pws and strong-weak Psw), we in-sert the word pair into the pattern as p(a1, a2) to geta phrasal query like ?big but not huge?.
This is doneby replacing the two wildcards in the pattern by thetwo words in order.
Finally, we scan the Web n-grams corpus in a batch approach similar to Bansaland Klein (2011) and collect frequencies of all ourphrase queries.
Table 2 depicts some examples ofuseful intensity-based phrase queries and their fre-quencies in the Web-scale corpus.
We also collectfrequencies for the input word unigrams and the pat-terns for normalization purposes.
Given a word pair(a1, a2) and a corpus count function cnt, we defineW1 =1P1?p1?Pwscnt(p1(a1, a2))S1 =1P2?p2?Pswcnt(p2(a1, a2))W2 =1P1?p1?Pwscnt(p1(a2, a1))S2 =1P2?p2?Pswcnt(p2(a2, a1)) (1)withP1 =?p1?Pwscnt(p1)P2 =?p2?Pswcnt(p2), (2)such that the final overall weak-strong score isscore(a1, a2) =(W1 ?
S1)?
(W2 ?
S2)cnt(a1) ?
cnt(a2).
(3)Here W1 and S1 represent Web evidence of a1and a2 being in the weak-strong and strong-weakrelation, respectively.
W2 and S2 fit the reversepair (a2, a1) in the patterns and hence representthe strong-weak and weak-strong relations, respec-tively, in the opposite direction.
Hence, overall,(W1 ?
S1) ?
(W2 ?
S2) represents the total weak-strong score of the pair (a1, a2), i.e.
the score of a1being on the left of a2 on a relative intensity scale,such that score(a1, a2) = ?score(a2, a1).
The rawfrequencies in the score are divided by counts of thepatterns and by individual word unigram counts toobtain a pointwise mutual information (PMI) stylenormalization and hence avoid any bias in the scoredue to high-frequency patterns or word unigrams.22.2 Global Ordering with an MILP2.2.1 Objective and ConstraintsGiven pairwise scores, we now aim at producing aglobal ranking of the input words that is much moreinformative than the original pairwise scores.
Jointinference from multiple word pairs allows us to ben-efit from global information: Due to the sparsity ofthe pattern evidence, determining how two adjec-tives relate to each other can sometimes e.g.
onlybe inferred by observing how each of them relate tosome third adjective.We assume that we are given N input words A =a1, .
.
.
, aN that we wish to place on a linear scale,say [0, 1].
Thus each word ai is to be assigned aposition xi ?
[0, 1] based on the pairwise weak-strong weights score(ai, aj).
A positive value for2In preliminary experiments on a development set, we alsoevaluated other intuitive forms of normalization.281Figure 1: The input weak-strong data may contain oneor more cycles, e.g.
due to noisy patterns, so the finalranking will have to choose which input scores to honorand which to remove.score(ai, aj) means that ai is supposedly weakerthan aj and hence we would like to obtain xi < xj .A negative value for score(ai, aj) means that ai isassumed to be stronger than aj , so we would wantto obtain xi > xj .
Therefore, intuitively, our goalcorresponds to maximizing the objective?i,jsgn(xj ?
xi) ?
score(ai, aj) (4)Note that it is important to use the signum func-tion sgn() here, because we only care about the rel-ative order of xi and xj .
Maximizing?ij(xj?xi) ?score(ai, aj) would lead to all words being placedat the edges of the scale, because the highest scoreswould dominate over all other ones.
We do includethe score magnitudes in the objective, because theyhelp resolve contradictions in the pairwise scores(e.g., see Figure 1).
This is discussed in more de-tail in Section 2.2.2.In order to maximize this non-differentiable ob-jective, we use Mixed Integer Linear Programming(MILP), a variant of linear programming in whichsome but not all of the variables are constrained tobe integers.
Using an MILP formalization, we canfind a globally optimal solution in the joint deci-sion space, and unlike previous work, we jointly ex-ploit global information rather than just individuallocal (pairwise) scores.
To encode the objective in aMILP, we need to introduce additional variables dij ,wij , sij to capture the effect of the signum function,as explained below.We additionally also enable our MILP to makeuse of any external equivalence (synonymy) infor-mation E ?
{1, .
.
.
, N} ?
{1, .
.
.
, N} that may beavailable.
In this context, two words are consideredsynonymous if they are close enough in meaning tobe placed on (almost) the same position in the inten-sity scale.
If (i, j) ?
E, we can safely assume thatai, aj have near-equivalent intensity, so we shouldencourage xi, xj to remain close to each other.
TheMILP is defined as follows:maximize?
(i,j) 6?E(wij ?
sij) ?
score(ai, aj)??
(i,j)?E(wij + sij) Csubject todij = xj ?
xi ?i, j ?
{1, .
.
.
, N}dij ?
wijC ?
0 ?i, j ?
{1, .
.
.
, N}dij + (1?
wij)C > 0 ?i, j ?
{1, .
.
.
, N}dij + sijC ?
0 ?i, j ?
{1, .
.
.
, N}dij ?
(1?
sij)C < 0 ?i, j ?
{1, .
.
.
, N}xi ?
[0, 1] ?i ?
{1, .
.
.
, N}wij ?
{0, 1} ?i, j ?
{1, .
.
.
, N}sij ?
{0, 1} ?i, j ?
{1, .
.
.
, N}The difference variables dij simply capture differ-ences between xi, xj .
C is any very large constantgreater than ?i,j |score(ai, aj)|; the exact value isirrelevant.
The indicator variables wij and sij arejointly used to determine the value of the signumfunction sgn(dij) = sgn(xj ?
xi).
Variables wijbecome 1 if and only if dij > 0 and hence serveas indicator variables for weak-strong relationshipsin the output.
Variables sij become 1 if and only ifdij < 0 and hence serve as indicator variables fora strong-weak relationship in the output.
The ob-jective encourages wij = 1 for score(ai, aj) > 0and sij = 1 for score(ai, aj) < 0.3 When equiva-lence (synonymy) information is available, then for(i, j) ?
E both sij = 0 andwij = 0 are encouraged.2.2.2 DiscussionOur MILP uses intensity evidence of all inputpairs together and assimilates all the scores viaglobal transitivity constraints to determine the posi-tions of the input words on a continuous real-valuedscale.
Hence, our approach addresses drawbacks3In order to avoid numeric instability issues due to verysmall score(ai, aj) values after frequency normalization, inpractice we have found it necessary to rescale them by a fac-tor of 1 over the smallest |score(ai, aj)| > 0.282Figure 2: Equivalence Information: Knowing that am, a2are synonyms gives the MILP an indication of where toplace an on the scale with respect to a1, a2, a3of local or divide-and-conquer approaches, whereadjectives are scored with respect to selected pivotwords, and hence many adjectives that lack pairwiseevidence with the pivots are not properly classified,although they may have order evidence with somethird adjective that could help establish the ranking.Optional synonymy information can further help, asshown in Figure 2.Moreover, our MILP also gives higher weightto pairs with higher scores, which is useful whenbreaking global constraint cycles as in the simpleexample in Figure 1.
If we need to break a con-straint violating triangle or cycle, we would have tomake arbitrary choices if we were ranking based onsgn(score(a, b)) alone.
Instead, we can choose abetter ranking based on the magnitude of the pair-wise scores.
A stronger score between an adjectivepair doesn?t necessarily mean that they should befurther apart in the ranking.
It means that these twowords are attested together on the Web with respectto the intensity patterns more than with other candi-date words.
Therefore, we try to respect the order ofsuch word pairs more in the final ranking when weare breaking constraint-violating cycles.3 Related WorkHatzivassiloglou and McKeown (1993) presentedthe first step towards automatic identification of ad-jective scales, thoroughly discussing the backgroundof adjective semantics and a means of discoveringclusters of adjectives that belong on the same scale,thus providing one way of creating the input for ourranking algorithm.Inkpen and Hirst (2006) study near-synonyms andnuances of meaning differentiation (such as stylistic,attitudinal, etc.).
They attempt to automatically ac-quire a knowledge base of near-synonym differencesvia an unsupervised decision-list algorithm.
How-ever, their method depends on a special dictionaryof synonym differences to learn the extraction pat-terns, while we use only a raw Web-scale corpus.Mohammad et al(2013) proposed a method ofidentifying whether two adjectives are antonymous.This problem is related but distinct, because the de-gree of antonymy does not necessarily determinetheir position on an intensity scale.
Antonyms (e.g.,little, big) are not necessarily on the extreme ends ofscales.Sheinman and Tokunaga (2009) and Sheinman etal.
(2012) present the most closely related previouswork on adjective intensities.
They collect lexico-semantic patterns via bootstrapping from seed adjec-tive pairs to obtain pairwise intensities, albeit usingsearch engine ?hits?, which are unstable and prob-lematic (Kilgarriff, 2007).
While their approachis primarily evaluated in terms of a local pairwiseclassification task, they also suggest the possibil-ity of ordering adjectives on a scale using a pivot-based partitioning approach.
Although intuitive intheory, the extracted pairwise scores are frequentlytoo sparse for this to work.
Thus, many adjec-tives have no score with a particular headword.
Inour experiments, we reimplemented this approachand show that our MILP method improves over itby allowing individual pairwise decisions to benefitmore from global information.
Schulam and Fell-baum (2010) apply the approach of Sheinman andTokunaga (2009) to German adjectives.
Our methodextends easily to various foreign languages as de-scribed in Section 5.Another related task is the extraction of lexico-syntactic and lexico-semantic intensity-order pat-terns from large text corpora (Hearst, 1992;Chklovski and Pantel, 2004; Tandon and de Melo,2010).
Sheinman and Tokunaga (2009) followsDavidov and Rappoport (2008) to automaticallybootstrap adjective scaling patterns using seed ad-jectives and Web hits.
These methods thus can beused to provide the input patterns for our algorithm.VerbOcean by Chklovski and Pantel (2004) ex-tracts various fine-grained semantic relations (in-cluding the stronger-than relation) between pairs ofverbs, using lexico-syntactic patterns over the Web.283Our approach of jointly ranking a set of words usingpairwise evidence is also applicable to the VerbO-cean pairs, and should help address similar sparsityissues of local pairwise decisions.
Such scales willagain be quite useful for language learners and lan-guage understanding tools.de Marneffe et al(2010) infer yes-or-no answersto questions with responses involving scalar adjec-tives in a dialogue corpus.
They correlate adjectiveswith ratings in a movie review corpus to find thatgood appears in lower-rated reviews than excellent.Finally, there has been a lot of work on measuringthe general sentiment polarity of words (Hatzivas-siloglou and McKeown, 1997; Hatzivassiloglou andWiebe, 2000; Turney and Littman, 2003; Liu andSeneff, 2009; Taboada et al 2011; Yessenalina andCardie, 2011; Pang and Lee, 2008).
Our work in-stead aims at producing a large, unrestricted numberof individual intensity scales for different qualitiesand hence can help in fine-grained sentiment analy-sis with respect to very particular content aspects.4 Experiments4.1 DataInput Clusters In order to obtain input clusters forevaluation, we started out with the satellite cluster or?dumbbell?
structure of adjectives in WordNet 3.0,which consists of two direct antonyms as the polesand a number of other satellite adjectives that are se-mantically similar to each of the poles (Gross andMiller, 1990).
For each antonymy pair, we deter-mined an extended dumbbell set by looking up syn-onyms and words in related (satellite adjective and?see-also?)
synonym sets.
We cut such an extendeddumbbell into two antonymous halves and treatedeach of these halves as a potential input adjectivecluster.Most of these WordNet clusters are noisy for thepurpose of our task, i.e.
they contain adjectives thatappear unrelatable on a single scale due to polysemyand semantic drift, e.g.
violent with respect to super-natural and affected.
Motivated by Sheinman andTokunaga (2009), we split such hard-to-relate ad-jectives into smaller scale-specific subgroups usingthe corpus evidence4.
For this, we consider an undi-4Note that we do not use the WordNet dataset of Sheinmanand Tokunaga (2009) for evaluation, as it does not provide full438115 60 35 19 12 14 5 4 3 0 100200 300400 5002 3 4 5 6 7 8 9 10-14 15-17# of chainsLength of chainFigure 3: The histogram of cluster sizes after partitioning.412712 3 3 2 010203040503 4 5 6 7 8# ofchainsLength of chainFigure 4: The histogram of cluster sizes in the test set.rected edge between each pair of adjectives that hasa non-zero intensity score (based on the Web-scalescoring procedure described in Section 2.1.3).
Theresulting graph is then partitioned into connectedcomponents such that any adjectives in a subgraphare at least indirectly connected via some path andthus much more likely to belong to the same inten-sity scale.
While this does break up partitions when-ever there is no corpus evidence connecting them,ordering the adjectives within each such partition re-mains a challenging task.
This is because the Webevidence will still not necessarily directly relate alladjectives (in a partition) to each other.
Addition-ally, the Web evidence may still indicate the wrongdirection.
Figure 3 shows the size distribution of theresulting partitions.Patterns To construct our intensity pattern set, westarted with a couple of common rankable adjectiveseed pairs such as (good, great) and (hot, boiling)and used the Web-scale n-grams corpus (Brants andFranz, 2006) to collect the few most frequent pat-terns between and around these seed-pairs (in bothdirections).
Among these, we manually chose ascales.
Instead, their annotators only made pairwise compar-isons with select words, using a 5-way classification scheme(neutral, mild, very mild, intense, very intense).284small set of intuitive patterns that are linguisticallyuseful for ordering adjectives, several of which hadnot been discovered in previous work.
These areshown in Table 1.
Note that we only collected pat-terns that were not ambiguous in the two orders, forexample the pattern ??
, not ??
is ambiguous be-cause it can be used as both ?good, not great?
and?great, not good?.
Alternatively, one can easily alsouse fully-automatic bootstrapping techniques basedon seed word pairs (Hearst, 1992; Chklovski andPantel, 2004; Yang and Su, 2007; Turney, 2008;Davidov and Rappoport, 2008).
However, our semi-automatic approach is a simple and fast process thatextracts a small set of high-quality and very gen-eral adjective-scaling patterns.
This process canquickly be repeated from scratch in any other lan-guage.
Moreover, as described in Section 5.1, theEnglish patterns can also be projected automaticallyto patterns in other languages.Development and Test Sets Section 2.1 describesthe method for collecting the intensity scores for ad-jective pairs, using Web-scale n-grams (Brants andFranz, 2006).
We relied on a small developmentset to test the MILP structure and the pairwise scoresetup.
For this, we manually chose 5 representativeadjective clusters from the full set of clusters.The final test set, distinct from this developmentset, consists of 569 word pairs in 88 clusters, eachannotated by two native speakers of English.
Boththe gold test data (and our code) are freely avail-able.5 To arrive at this data, we randomly drew 30clusters each for cluster sizes 3, 4, and 5+ from thehistogram of partitioned adjective clusters in Fig-ure 3.
While labeling a cluster, annotators could ex-clude words that they deemed unsuitable to fit ona single shared intensity scale with the rest of thecluster.
Fortunately, the partitioning described ear-lier had already separated most such cases into dis-tinct clusters.
The annotators ordered the remainingwords on a scale.
Words that seemed indistinguish-able in strength could share positions in their anno-tation.As our goal is to compare scale formation algo-rithms, we did not include trivial clusters of size 2.On such trivial clusters, the Web evidence alone de-termines the output and hence all algorithms, includ-5http://demelo.org/gdm/intensity/ing the baseline, obtain the same pairwise accuracy(defined below) of 93.3% on a separate set of 30 ran-dom clusters of size 2.Figure 4 shows the distribution of cluster sizes inour main gold set.
The inter-annotator agreement interms of Cohen?s ?
(Cohen, 1960) on the pairwiseclassification task with 3 labels (weaker, stronger,or equal/unknown) was 0.64.
In terms of pairwiseaccuracy, the agreement was 78.0%.4.2 MetricsIn order to thoroughly evaluate the performance ofour adjective ordering procedure, we rely on bothpairwise and ranking-correlation evaluation metrics.Consider a set of input words A = {a1, a2, .
.
.
, an}and two rankings for this set ?
a gold-standard rank-ing rG(A) and a predicted ranking rP (A).4.2.1 Pairwise AccuracyFor a pair of words ai, aj , we may consider theclassification task of choosing one of three labels (<,>, =?)
for the case of ai being weaker, stronger, andequal (or unknown) in intensity, respectively, com-pared to a2:L(a1, a2) =??
?< if r(ai) < r(aj)> if r(ai) > r(aj)=?
if r(ai) = r(aj)For each pair (a1, a2), we compute gold-standardlabelsLG(a1, a2) and predicted labelsLP (a1, a2) asabove, and then the pairwise accuracy PW (A) fora particular ordering on A is simply the fraction ofpairs that are correctly classified, i.e.
for which thepredicted label is same as the gold-standard label:PW (A) =?i<j1{LG(ai, aj) = LP (ai, aj)}?i<j14.2.2 Ranking Correlation CoefficientsOur second type of evaluation assesses therank correlation between two ranking permutations(gold-standard and predicted).
Many studies useKendall?s tau (Kendall, 1938), which measures thetotal number of pairwise inversions, while othersprefer Spearman?s rho (Spearman, 1904), whichmeasures the L1 distance between ranks.285Kendall?s tau correlation coefficient We use the?b version of Kendall?s correlation metric, as it in-corporates a correction for ties (Kruskal, 1958; Douet al 2008):?b =P ?Q?
(P +Q+X0) ?
(P +Q+ Y0)where P is the number of concordant pairs, Q isthe number of discordant pairs, X0 is the numberof pairs tied in the first ranking, Y0 is the number ofpairs tied in the second ranking.
Given the two rank-ings of an adjective set A, the gold-standard rankingrG(A) and the predicted ranking rP (A), two wordsai, aj are:?
concordant iff both rankings have the same strictorder of the two elements, i.e., rG(ai) > rG(aj)and rP (ai) > rP (aj), or rG(ai) < rG(aj) andrP (ai) < rP (aj).?
discordant iff the two rankings have an invertedstrict order of the two elements, i.e., rG(ai) >rG(aj) and rP (ai) < rP (aj), or rG(ai) <rG(aj) and rP (ai) > rP (aj).?
tied iff rG(ai) = rG(aj) or rP (ai) = rP (aj).Spearman?s rho correlation coefficient For twon-sized ranked lists {xi} and {yi}, the Spearmancorrelation coefficient is defined as the Pearson cor-relation coefficient between the ranks of variables:?
=?i(xi ?
x?)
?
(yi ?
y?)?
?i(xi ?
x?
)2 ?
?i(yi ?
y?
)2Here, x?
and y?
denote the means of the values in therespective lists.
We use the standard procedure forhandling ties correctly.
Tied values are assigned theaverage of all ranks of items sharing the same valuein the ranked list sorted in ascending order of thevalues.Handling Inversions While annotating, we some-times observed that the ordering itself was very clearbut the annotators disagreed about which end of aparticular scale was to count as the strong one, e.g.when transitioning from soft to hard or from alphato beta.
We thus also report average absolute valuesof both correlation coefficients, as these properly ac-count for anticorrelations.
Our test set only containsclusters of size 3 or larger, so there is no need toaccount for inversions in clusters of size 2.4.3 ResultsIn Table 3, we use the evaluation metrics mentionedabove to compare several different approaches.Web Baseline The first baseline simply reflectsthe original pairwise Web-based intensity scores.We classify (with one of 3 labels) a given pair ofadjectives using the Web-based intensity scores (asdescribed in Section 2.1.3) as follows:Lbaseline(a1, a2) =??
?< if score(ai, aj) > 0> if score(ai, aj) < 0=?
if score(ai, aj) = 0Since score(ai, aj) represents the weak-strongscore of the two adjectives, a more positive valuemeans a higher likelihood of ai being weaker (<, onthe left) in intensity than aj .In Table 3, we observe that the (micro-averaged)pairwise accuracy, as defined earlier, for the origi-nal Web baseline is 48.2%, while the ranking mea-sures are undefined because the individual pairs donot lead to a coherent scale.Divide-and-Conquer The divide-and-conquerbaseline recursively splits a set of words into threesubgroups, placed to the left (weaker), on the sameposition (no evidence), or to the right (stronger) of agiven randomly chosen pivot word.While this approach shows only a minor improve-ment in terms of the pairwise accuracy (50.6%), itsmain benefit is that one obtains well-defined inten-sity scales rather than just a collection of pairwisescores.Sheinman and Tokunaga The approach bySheinman and Tokunaga (2009) involves a simi-lar divide-and-conquer based partitioning in the firstphase, except that their method makes use of syn-onymy information from WordNet and uses all syn-onyms in WordNet?s synset for the headword asneutral pivot elements (if the headword is not inWordNet, then the word with the maximal unigramfrequency is chosen).
In the second phase, theirmethod performs pairwise comparisons within themore intense and less intense subgroups.
We reim-plement their approach here, using the Google N-Grams dataset instead of online Web search enginehits.
We observe a small improvement over the Webbaseline in terms of pairwise accuracy.
Note that the286Method Pairwise Accuracy Avg.
?
Avg.
|?
| Avg.
?
Avg.
|?|Web Baseline 48.2% N/A N/A N/A N/ADivide-and-Conquer 50.6% 0.45 0.53 0.52 0.62Sheinman and Tokunaga (2009) 55.5% N/A N/A N/A N/AMILP 69.6% 0.57 0.65 0.64 0.73MILP with synonymy 78.2% 0.57 0.66 0.67 0.80Inter-Annotator Agreement 78.0% 0.67 0.76 0.75 0.86Table 3: Main test resultsPredicted ClassWeaker Tie StrongerTrue ClassWeaker 117 127 15Tie 5 42 15Stronger 11 122 115Table 4: Confusion matrix (Web baseline)rank correlation measure scores are undefined fortheir approach.
This is because in some cases theirmethod placed all words on the same position in thescale, which these measures cannot handle even intheir tie-corrected versions.
Overall, the Sheinmanand Tokunaga approach does not aggregate informa-tion sufficiently well at the global level and oftenfails to make use of transitive inference.MILP Our MILP exploits the same pairwisescores to induce significantly more accurate pair-wise labels with 69.6% accuracy, a 41% relativeerror reduction over the Web baseline, 38% overDivide-and-Conquer, and 32% over Sheinman andTokunaga (2009).
We further see that our MILPmethod is able to exploit external synonymy (equiv-alence) information (using synonyms marked by theannotators).
The accuracy of the pairwise scores aswell as the quality of the overall ranking increaseeven further to 78.2%, approaching the human inter-annotator agreement.
In terms of average correlationcoefficients, we observe similar improvement trendsfrom the MILP, but of different magnitudes, becausethese averages give small clusters the same weightas larger ones.4.4 AnalysisConfusion Matrices For a given approach, wecan study the confusion matrix obtained by cross-tabulating the gold classification with the predictedPredicted ClassWeaker Tie StrongerTrue ClassWeaker 177 29 53Tie 9 24 29Stronger 15 38 195Table 5: Confusion matrix (MILP)classification of every unique pair of adjectives inthe ground truth data.
Table 4 shows the confusionmatrix for the Web baseline.
We observe that due tothe sparsity of pairwise intensity order evidence, thebaseline method predicts too many ties.Table 5 provides the confusion matrix for theMILP (without external equivalence information)for comparison.
Although the middle column stillshows that the MILP predicts more ties than humansannotators, we find that a clear majority of all uniquepairs are now correctly placed along the diagonal.This confirms that our MILP successfully infers newordering decisions, although it uses the same input(corpus evidence) as the baseline.
The remainingties are mostly just the result of pairs for which theresimply is no evidence at all in the input Web counts.Note that this problem could for instance be circum-vented by relying on a crowdsourcing approach: Afew dispersed tie-breakers are enough to allow ourMILP to correct many other predictions.Predicted Examples Finally, in Table 6, we pro-vide a selection of real results obtained by our algo-rithm.
For instance, it correctly inferred that terri-fying is more intense than creepy or scary, althoughthe Web pattern counts did not provide any explicitinformation about these words pairs.
In some cases,however, the Web evidence did not suffice to drawthe right conclusions, or it was misleading due to is-sues like polysemy (as for the word funny).287Accuracy Prediction Gold StandardGoodhard< painful< hopelesshard< painful< hopelessfull< stuffed< (overflowing,overloaded)full< stuffed< overflowing< overloadedunusual< uncommon< rare< exceptional< extraordinaryuncommon< unusual< rare< extraordinary< exceptionalAverage creepy< scary< sinister< frightening< terrifyingcreepy< (scary, frightening)< terrifying< sinisterBad (awake, conscious)< alive< awarealive< awake< (aware, conscious)strange< (unusual, weird)< (funny, eerie)(strange, funny)< unusual< weird< eerieTable 6: Some examples (of bad, average and good accu-racy) of our MILP predictions (without synonymy infor-mation) and the corresponding gold-standard annotation.While we show results on gold-standard chainshere for evaluation purposes, in practice one can alsorecombine two [0, 1] chains for a pair of antonymicclusters to form a single scale from [?1, 1] that visu-alizes the full spectrum of available adjectives alonga dimension, from adjacent all the way to removed,or from black to glaring.5 Extension to Multilingual OrderingOur method for globally ordering words on a scalecan easily be applied to languages other than En-glish.
The entire process is language-independentas long as the required resources are available and asmall number of patterns are chosen.
For morpho-logically rich languages, the information extractionstep of course may require additional morphologi-cal analysis tools for stemming and aggregating fre-quencies across different forms.Alternatively, a cross-lingual projection approachis possible at multiple levels, utilizing informationfrom the English data and ranking.
As the first step,the set of words in the target language that we wishto rank can be projected from the English word set ifnecessary ?
e.g., as shown in de Melo and Weikum(2009).
Next, we outline two projection methods forthe ordering step.
The first method is based on pro-jection of the English intensity-ordering patterns tothe new language, and then using the same MILPas described in Section 2.2.
In the second method,we also change the MILP and add cross-lingual con-straints to better inform the target language?s ad-jective ranking.
A detailed empirical evaluation ofthese approaches remains future work.5.1 Cross-Lingual Pattern ProjectionInstead of creating new patterns, in many caseswe obtain quite adequate intensity patterns by us-ing cross-lingual projection.
We simply take sev-eral adjective pairs, instantiate the English patternswith them, and obtain new patterns using a machinetranslation system.
Filling the wildcards in a pat-tern, say ??
but not ?
?, with good/excellent results in?good but not excellent?.
This phrase is then trans-lated into the target language using the translationsystem, say into German ?gut aber nicht ausgezeich-net?.
Finally, put back the wildcards in the place ofthe translations of the adjective words, here gut andausgezeichnet, to get the corresponding German pat-tern ??
aber nicht ??.
Table 7 shows various Germanintensity patterns that we obtain by projecting fromthe English patterns as described.
The process is re-peated with multiple adjective pairs in case differentvariants are returned, e.g.
due to morphology.
Mostof these translations deliver useful results.Now that we have the target language adjectivesand the ranking patterns, we can compute the pair-wise intensity scores using large-scale data in thatlanguage.
We can use the Google n-grams cor-pora for 10 European languages (Brants and Franz,2009), and also for Chinese (LDC2010T02) andJapanese (LDC2009T08).
For other languages, onecan use available large raw-text corpora or Webcrawling tools.5.2 Crosslingual MILPTo improve the rankings for lesser-resourced lan-guages, we can further use a joint MILP approachfor the new language we want to transfer this pro-cess to.
Additional constraints between the English288Weak-Strong Patterns Strong-Weak PatternsEnglish German English German?
but not ?
?
aber nicht ?
not ?
just ?
nicht ?
gerade ??
if not ?
?
wenn nicht ?
not ?
but just ?
nicht ?
aber nur ??
and almost ?
?
und fast ?
not ?
though still ?
nicht ?
aber immer noch ?not just ?
but ?
nicht nur ?
sondern ?
?
or very ?
?
oder sehr ?Table 7: Examples of German intensity patterns projected (translated) directly from the English patterns.words and their corresponding target language trans-lations, in combination with the English ranking in-formation, allow the algorithm to obtain better rank-ings for the target words whenever the non-Englishtarget language corpus does not provide sufficientintensity order evidence.In this case, the input set A contains wordsin multiple languages.
The Web intensity scoresscore(ai, aj) should be set to zero when comparingwords across languages.
We instead link them usinga translation table T ?
{1, .
.
.
, N} ?
{1, .
.
.
, N}from a translation dictionary or phrase table.
Here,(i, j) ?
T signifies that ai is a translation of aj .
Wedo not require a bijective relationship between them(i.e., translations needn?t be unique).
The objectivefunction is augmented by adding the new term?
(i,j)?T(w?ij + s?ij)CT (5)for a constant CT > 0 that determines how muchweight we assign to translations as opposed to thecorpus count scores.
The MILP is extended byadding the following extra constraints.dij ?
w?ijCT < ?dmax ?i, j ?
{1, .
.
.
, N}dij + (1?
w?ij)CT ?
?dmax ?i, j ?
{1, .
.
.
, N}dij + s?ijCT > dmax ?i, j ?
{1, .
.
.
, N}dij ?
(1?
s?ij)CT ?
dmax ?i, j ?
{1, .
.
.
, N}w?ij ?
{0, 1} ?i, j ?
Ts?ij ?
{0, 1} ?i, j ?
TThe variables di,j , as before, encode distances be-tween positions of words on the scale, but now alsoinclude cross-lingual pairs of words in different lan-guages.
The new constraints encourage translationalequivalents to remain close to each other, preferablywithin a desired (but not strictly enforced) maximumdistance dmax.
The new variables w?ij , s?ij are sim-ilar to wij , sij in the standard MILP.
However, thew?ij become 1 if and only if dij ?
?dmax and the s?ijbecome 1 if and only if dij ?
dmax.
If both w?ij ands?ij are 1, then the two words have a small distance?dmax ?
dij ?
dmax.
The augmented objectivefunction explicitly encourages this for translationalequivalents.
Overall, this approach thus allows evi-dence from a language with more Web evidence toimprove the process of adjective ordering in lesser-resourced languages.6 ConclusionIn this work, we have presented an approach to thechallenging and little-studied task of ranking wordsin terms of their intensity on a continuous scale.
Weaddress the issue of sparsity of the intensity order ev-idence in two ways.
First, pairwise intensity scoresare computed using linguistically intuitive patternsin a very large, Web-scale corpus.
Next, a MixedInteger Linear Program (MILP) expands on this fur-ther by inferring new relative relationships.
Insteadof making ordering decisions about word pairs in-dependently, our MILP considers the joint decisionspace and factors in e.g.
how two adjectives relateto some third adjective, thus enforcing global con-straints such as transitivity.Our approach is general enough to allow addi-tional evidence such as synonymy in the MILP,and can straightforwardly be applied to other wordclasses (such as verbs), and to other languages(monolingually as well as cross-lingually).
Theoverall results across multiple metrics are substan-tially better than previous approaches, and fairlyclose to human agreement on this challenging task.AcknowledgmentsWe would like to thank the editor and the anony-mous reviewers for their helpful feedback.289ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale featuresfor full-scale parsing.
In Proceedings of ACL 2011.Thorsten Brants and Alex Franz.
2006.
The Google Web1T 5-gram corpus version 1.1.
LDC2006T13.Thorsten Brants and Alex Franz.
2009.
Web 1T 5-gram,10 European languages, version 1.
LDC2009T25.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of EMNLP 2004.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20(1):37?46.Dmitry Davidov and Ari Rappoport.
2008.
Unsuper-vised discovery of generic relationships using patternclusters and its evaluation by automatically generatedsat analogy questions.
In Proceedings of ACL 2008.Marie-Catherine de Marneffe, Christopher D. Manning,and Christopher Potts.
2010.
Was it good?
it wasprovocative.
learning the meaning of scalar adjectives.In Proceedings of ACL 2010.Gerard de Melo and Gerhard Weikum.
2009.
Towardsa universal wordnet by learning from combined evi-dence.
In Proceedings of CIKM 2009.Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-RongWen.
2008.
Are click-through data adequate for learn-ing web search rankings?
In Proc.
of CIKM 2008.Derek Gross and Katherine J. Miller.
1990.
Adjectivesin WordNet.
International Journal of Lexicography,3(4):265?277.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1993.
Towards the automatic identification of adjecti-val scales: Clustering adjectives according to meaning.In Proceedings of ACL 1993.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of ACL 1997.Vasileios Hatzivassiloglou and Janyce M. Wiebe.
2000.Effects of adjective orientation and gradability on sen-tence subjectivity.
In Proceedings of COLING 2000.Marti Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING1992.Diana Inkpen and Graeme Hirst.
2006.
Building andusing a lexical knowledge base of near-synonym dif-ferences.
Computational Linguistics, 32(2):223?262.Maurice G. Kendall.
1938.
A new measure of rank cor-relation.
Biometrika, 30(1/2):81?93.Adam Kilgarriff.
2007.
Googleology is bad science.Computational Linguistics, 33(1).William H. Kruskal.
1958.
Ordinal measures of associa-tion.
Journal of the American Statistical Association,53(284):814?861.Jingjing Liu and Stephanie Seneff.
2009. Review senti-ment scoring via a parse-and-paraphrase paradigm.
InProceedings of EMNLP 2009.George A. Miller.
1995.
WordNet: A lexical database forenglish.
Communications of the ACM, 38(11):39?41.Said M. Mohammad, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2013.
Computing lexical contrast.Computational Linguistics.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135, January.Peter F. Schulam and Christiane Fellbaum.
2010.
Au-tomatically determining the semantic gradation of ger-man adjectives.
In Proceedings of KONVENS 2010.Vera Sheinman and Takenobu Tokunaga.
2009.
AdjS-cales: Visualizing differences between adjectives forlanguage learners.
IEICE Transactions on Informationand Systems, 92(8):1542?1550.Vera Sheinman, Takenobu Tokunaga, I. Julien, P. Schu-lam, and C. Fellbaum.
2012.
Refining WordNet adjec-tive dumbbells using intensity relations.
In Proceed-ings of Global WordNet Conference 2012.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of COLING/ACL 2006.Charles Spearman.
1904.
The proof and measurement ofassociation between two things.
The American journalof psychology, 15(1):72?101.Fabian M. Suchanek, Mauro Sozio, and GerhardWeikum.
2009.
SOFIE: a self-organizing frameworkfor information extraction.
In Proceedings of WWW2009.Maite Taboada, Julian Brooke, Milan Tofiloskiy, andKimberly Vollz.
2011.
Lexicon-based methods forsentiment analysis.
Computational Linguistics.Niket Tandon and Gerard de Melo.
2010.
Informationextraction from web-scale n-gram data.
In Proceed-ings of the SIGIR 2010 Web N-gram Workshop.Peter D. Turney and Michael L. Littman.
2003.
Mea-suring praise and criticism: Inference of semanticorientation from association.
ACM Trans.
Inf.
Syst.,21(4):315?346, October.Peter D. Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of COLING 2008.Xiaofeng Yang and Jian Su.
2007.
Coreference resolu-tion using semantic relatedness information from auto-matically discovered patterns.
In Proceedings of ACL2007.Ainur Yessenalina and Claire Cardie.
2011.
Composi-tional matrix-space models for sentiment analysis.
InProceedings of EMNLP 2011.290
