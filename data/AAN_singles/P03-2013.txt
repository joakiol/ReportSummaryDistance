Approaches to Zero Adnominal RecognitionMitsuko Yamura-TakeiGraduate School of Information SciencesHiroshima City UniversityHiroshima, JAPANyamuram@nlp.its.hiroshima-cu.ac.jpAbstractThis paper describes our preliminary at-tempt to automatically recognize zero ad-nominals, a subgroup of zero pronouns, inJapanese discourse.
Based on the corpusstudy, we define and classify what we call?argument-taking nouns (ATNs),?
i.e.,nouns that can appear with zero adnomi-nals.
We propose an ATN recognition al-gorithm that consists of lexicon-basedheuristics, drawn from the observations ofour analysis.
We finally present the resultof the algorithm evaluation and discussfuture directions.1 Introduction(1) Zebras always need to watch out for lions.Therefore, even while eating grass, so that ableto see behind, eyes are placed at face-side.This is a surface-level English translation of anaturally occurring ?unambiguous?
Japanese dis-course.
By ?unambiguous,?
we mean that Japa-nese speakers find no difficulty in interpreting thisdiscourse segment, including whose eyes are beingtalked about.
Moreover, Japanese speakers findthis segment quite ?coherent,?
even though thereseems to be no surface level indication of who iseating or seeing, or whose eyes are being men-tioned in this four-clause discourse segment.
1However, this is not always the case with Japaneseas a Second Language (JSL) learners.2What constitutes ?coherence?
has been studiedby many researchers.
Reference is one of the lin-guistic devices that create textual unity, i.e., cohe-1 This was verified by an informal poll conducted on 15 nativespeakers of Japanese.2 Personal communication with a JSL teacher.sion (Halliday and Hasan, 1976).
Reference alsocontributes to the semantic continuity and contentconnectivity of a discourse, i.e., coherence.
Co-herence represents the natural and reasonable con-nections between utterances that make for easyunderstanding, and thus lower inferential load forhearers.The Japanese language uses ellipsis as its majortype of referential expression.
Certain elementsare ellipted when they are recoverable from a givencontext or from relevant knowledge.
These ellip-ses may include verbals and nominals; the missingnominals have been termed ?zero pronouns,?
?zeropronominals,?
?zero arguments,?
or simply ?zeros?by researchers.How many zeros are contained in (1), for ex-ample, largely depends on how zeros are defined.In the literature, zeros are usually defined as ele-ments recoverable from the valency requirementsof the predicate with which they occur.
However,does this cover all the zeros in Japanese?
Does thisexplain all the content connectivity created bynominal ellipsis in Japanese?In this paper, we introduce a subgroup of zeros,what we call ?zero adnominals,?
in contrast toother well-recognized ?zero arguments?
and inves-tigate possible approaches to recognizing thesenewly-defined zeros, in an attempt to incorporatethem in an automatic zero detecting tool for JSLteachers that aims to promote effective instructionof zeros.
In section 2, we provide the definition ofzero adnominals, and present the results of theirmanual identification in the corpus.
Section 3 de-scribes the theoretical and pedagogical motivationsfor this study.
Section 4 illustrates the syntac-tic/semantic classification of the zero adnominalexamples found in the corpus.
Based on the classi-fication results, we propose lexical information-based heuristics, and present a preliminary evalua-tion.
In the final two sections, we present relatedwork, and discuss possible future directions.2 Zero Adnominals2.1 DefinitionRecall the discourse segment in (1).
Its originalJapanese is analyzed in (2).
(2)  a. simauma-wa  raion ni   itumozebra-TOP     lion-DAT  alwayski-o-tuke-nakereba-narimasen.watch-out-for-need-to?Zebras always need to watch out for lions.?b.
desukara,  ?
kusa-o  tabete-ite-mo,so      ?-NOM grass-ACC eating-even-while?So even while (they) are eating grass,?c.
?
?
usiro-no-ho-made             mieru-yo-ni?-NOM ?-ADN-behind-even  see-can-for?so that (they) can see even what isbehind (them),?d.
?
me-ga                 ?
kao-no-yoko-ni?-ADN-eye-NOM ?-ADN-face-side LOCtuite-imasu.placed-be?
(their)eyes are on the sides of (their) faces.
?Zero arguments are unexpressed elements that arepredictable from the valency requirements of theirheads, i.e., a given predicate of the clause.
Zeronominatives in (2b) and (2c) are of this type.
Zeroadnominals, analogously, are missing elements thatcan be inferred from some features specified bytheir head nouns.
A noun for body-part, me ?eyes?in (2d) usually calls hearers?
attention to ?of-whom?
information and hearers recover that in-formation in the flow of discourse.
That missinginformation can be supplied by a noun phrase (NP)followed by an adnominal particle no, i.e., si-mauma-no ?zebras?
(= their)?
in the case of (2d)above.
Hence, as a first approximation, we definea zero adnominal as an unexpressed ?NP no?
in theNP no NP (a.k.a., A no B) construction.2.2 The CorpusBefore we proceed, we will briefly describe thecorpus that we investigated.
The corpus consistsof a collection of 83 written narrative texts takenfrom seven different JSL textbooks with levelsranging from beginning to intermediate.
Thus, it isa representative sample of naturally-occurring, butmaximally canonical, free-from-deviation, and co-herent narrative discourse.2.3 IdentificationOur primary goal is to identify relevant informa-tion for recognizing zero adnominals.
Since suchinformation is unavailable in the surface text, theidentification of missing adnominal elements andtheir referents in the corpus was based on the na-tive speaker intuitions and the linguistic expertiseof the author, who used the definition in 2.1, withoccasional consultation with a JSL teaching ex-pert/linguist.
As a result, we located a total of 320zero adnominals.
These adnominals serve as thezero adnominal samples on which our later analy-sis is based.3 Theoretical/Pedagogical Motivations3.1 Centering AnalysisOne discourse account that models the perceiveddegree of coherence of a given discourse in rela-tion to local focus of attention and the choice ofreferring expressions is centering (e.g., Grosz,Joshi and Weinstein, 1995).The investigation of zeros behavior in our cor-pus, within the centering framework, shows thatzero adnominals make a considerable contributionto center continuity in discourse by realizing thecentral entity in an utterance (called Cb) just aswell-acknowledged zero arguments do.Recall example (2).
Its center data structure isgiven in (3).
The Cf (forward-looking center) listis a set of discourse entities that appear in eachutterance (Ui).
The Cb (backward-looking center)is a special member of the Cf list, and is meant torepresent the entity that the utterance is most cen-trally about; it is the most highly ranked element ofthe Cf (Ui-1) that is realized in Ui.
(3) a. Cb: none   [Cf: zebra, lion]b.  Cb: zebra  [Cf: zebra, grass]c. Cb: zebra [Cf: zebra, what is behind]d.  Cb: zebra [Cf: zebra, eye, face-side]In (3b) and (3c), the Cb is realized as a zero nomi-native, and in (3d), it is realized by the same entity(zebra) as a zero adnominal, maintaining theCONTINUE transition that by definition is maxi-mally coherent.
This matches the intuitively per-ceived degree of coherence in the utterance.
Ourcorpus contains a total of 138 zero adnominals thatrefer to previously mentioned entities (15.56% ofall the zero Cbs), and realize the Cb of the utter-ance in which they occur, as in (3d=2d).Our corpus study shows that discourse coher-ence can be more accurately characterized, in thecentering account, by recognizing the role of zeroadnominals as a valid realization of Cbs (see Ya-mura-Takei et al, ms. for detailed discussion).This is our first motivation towards zero adnominalrecognition.3.2 Zero DetectorYamura-Takei et al (2002) developed an auto-matic zero identifying tool.
This program, ZeroDetector (henceforth, ZD) takes Japanese writtennarrative texts as input and provides the zero-specified texts and their underlying structures asoutput.
This aims to draw learners?
and teachers?attention to zeros, on the basis of a hypothesisabout ideal conditions for second language acquisi-tion, by making invisible zeros visible.
ZD regardsteachers as its primary users, and helps them pre-dict the difficulties with zeros that students mightencounter, by analyzing text in advance.
Such dif-ficulties often involve failure to recognize dis-course coherence created by invisible referentialdevices, i.e., the center continuity maintained bythe use of various types of zeros.As our centering analysis above indicates, in-clusion of zero adnominals into ZD?s detectingcapability enables a more comprehensive coverageof the zeros that contributes to discourse coherence.This is our project goal.4 Towards Zero Adnominal Recognition4.1 Semantic ClassificationUnexpressed elements need to be predicted fromother expressed elements.
Thus, we need to char-acterize B nouns (which are overt) in the (A no) Bconstruction, assuming that zero adnominals (A)are triggered by their head nouns (B) and that cer-tain types of NPs tend to take implicit (A) argu-ments.
Our first approach is to use an existing Ano B classification scheme.
We adopted, fromamong many A no B works, a classification mod-eled on Shimazu, Naito and Nomura (1985, 1986,and 1987) because it offers the most comprehen-sive classification (Fais and Yamura-Takei, ms).Table 1 below describes the five main groups thatwe used to categorize (A no) B phrases.4.2 ResultsWe classified our 320 ?
(A no) B?
examples intothe five groups described in the previous section.Group V comprised the vast majority, while ap-proximately the same percentage of examples wasincluded in Groups I, II and III.
There were noGroup IV examples.
The number and percentageof examples of each group are presented in Table 2.Group # of examplesI  33 (10.31%)II  23 (  7.19%)III  35 (10.94%)IV   0 (  0.00%)V 229 (71.56%)Total 320    (100%)Table 2: Distribution of semantic typesGroup # Definition Example from Shimazu et al (1986)I A: argument B: nominalized verbal elementkotoba no rikai?word-no-understanding?II A: noun denoting an entity B: abstract relational nounbiru no mae?building-no-front?III A: noun denoting an entity B: abstract attribute nounhasi no nagasa?bridge-no-length?IV A: nominalized verbal element B: argumentkenka no hutari?argument-no-two people?V A: noun expressing attribute B: noun denoting an entityningen no atama?human-no-head?Table 1: (A no) B classification schemeWe conjecture that certain nouns are morelikely to take zero adnominals than others, and thatthe head nouns which take zero adnominals, ex-tracted from our corpus, are representative samplesof this particular group of nouns.
We call them?argument-taking nouns (ATNs).?
ATNs syntacti-cally require arguments and are semantically de-pendent on their arguments.
We use the term ATNonly to refer to a particular group of nouns that cantake implicit arguments (i.e., zero adnominals).We closely examined the 127 different ATNtokens among the 320 cases of zero adnominalsand classified them into the four types that corre-spond to Groups I, II, III and V in Table 1.
Wethen listed their syntactic/semantic propertiesbased on the syntactic/semantic properties pre-sented in the Goi-Taikei Japanese Lexicon (hereaf-ter GT, Ikehara, Miyazaki, Shirai, Yokoo, Nakaiwa,Ogura, Oyama, and Hayashi, 1997).
GT is a se-mantic feature dictionary that defines 300,000nouns based on an ontological hierarchy of ap-proximately 2,800 semantic attributes.
It also usesnine part-of-speech codes for nouns.
Table 3 liststhe syntactic/semantic characterizations of thenouns in each type and the number of examples inthe corpus.
What bold means in the table will beexplained later in section 4.3.Type Syntactic properties Semantic properties # ExamplesHuman activity 21 zikosyokai ?self-introduction?
I Nominalized verbal, de-rived  (from verb) noun,common nounphenomenon 3 entyo ?extension?Location 13 mae ?front?
II formal noun, commonnoun Time 1 yokuzitu ?next day?Amount 9 sintyo ?height?Value 2 nedan ?price?Emotion 1 kimoti ?feeling?Material phenomenon 1 nioi ?smell?Name 1 namae ?name?III Derived (from verb/ad-jective) noun, suffixnoun, common nounOrder 1 ichiban ?first?Human (kinship) 14 haha ?mother?Animate (body-part) 14 atama ?head?Organization 7 kaisya ?company?Housing (part) 7 doa ?door?Human (profession) 4 sensei ?teacher?Human (role) 4 dokusya ?reader?Human (relationship) 3 dooryoo ?colleague?Clothing 3 kutu ?shoes?Tool 2 saihu ?purse?Human (biological feature) 2 zyosei ?woman?Man-made 2 kuruma ?car?Facility 1 byoin ?hospital?Building 1 niwa ?garden?Housing (body) 1 gareeji ?garage?Housing (attachment) 1 doa ?door?Creative work 1 sakuhin ?work?Substance 1 kuuki ?air?Language 1 nihongo ?Japanese?Document 1 pasupooto ?passport?Chart 1 chizu ?map?Animal 1 petto ?pet?V Common noun?
(unregistered) 2 hoomusutei ?homestay?Total 127Table 3: Subtypes of ATNsWhen we examine these four types, we see thatthey partially overlap with some particular types ofnouns studied theoretically in the literature.
Tera-mura (1991) subcategorizes locative relationalnouns like mae ?front?, naka ?inside?, and migi?right?
as ?incomplete nouns?
that require elementsto complete their meanings; these are a subset ofType II.
Iori (1997) argues that certain nouns arecategorized as ?one-place nouns,?
in which heseems to include Type I and some of Type V nouns.Kojima (1992) examines so-called ?low-independence nouns?
and categorizes them intothree types, according to their syntactic behaviorsin Japanese copula expressions.
These cover sub-sets of our Type I, II, III and V.   In computationalwork, Bond, Ogura, and Ikehara (1995) extracted205 ?trigger nouns?
from a corpus aligned withEnglish.
These nouns trigger the use of possessivepronouns when they are machine-translated intoEnglish.
They seem to correspond mostly to ourType V nouns.
Our result offers a comprehensivecoverage which subsumes all of the types of nounsdiscussed in these accounts.Next, let us more closely look at the propertiesexpressed by our samples.
The most prevalentATNs (21 in number) are nominalized verbals inthe semantic category of human activity.
The nextmost common are kinship nouns (14 in number)and body-part nouns (14), both in the commonnoun category; location nouns (13), either in thecommon noun or formal noun category; and nounsthat express amount (9) whose syntactic categoryis either common or de-adjectival.
The others in-clude some ?human?
subcategories, etc.The part-of-speech subcategory, ?nominalizedverbal?
(sahen-meishi) is a reasonably accurateindicator of Type 1 nouns.
So is ?formal noun?
(keishiki-meishi) for Type II, although this does notoffer a full coverage of this type.
Numeral nounand counter suffix noun compounds also representa major subset of Type III.Semantic properties, on the other hand, seemhelpful to extract certain groups such as location(Type II), amount (Type III), kinship, body-part,organization, and some human subcategories (TypeV).
But other low-frequency ATN samples areproblematic for determining an appropriate level ofcategorization in GT?s semantic hierarchy tree.4.3 AlgorithmOur goal is to build a system that can identify thepresence of zero adnominals.
In this section, wepropose an ATN (hence zero adnominal) recogni-tion algorithm.
The algorithm consists of a set oflexicon-based heuristics, drawn from the observa-tions in section 4.2.The algorithm takes morphologically-analyzedtext as input and provides ATN candidates as out-put.
The process consists of the following threephases: (i) bare noun extraction, (ii) syntactic cate-gory (part-of-speech) checking, and (iii) semanticcategory checking.Zero adnominals usually co-occur with ?barenouns.?
Bare nouns, in our definition, are nounswithout any pre-nominal modifiers, including de-monstratives, explicit adnominal phrases, relativeclauses, and adjectives.3 Bare nouns are often sim-plex as in (4a), and sometimes are compound (e.g.,numeral noun + counter suffix noun) as in (4b).These are immediately followed by case-marking,topic/focus-marking or other particles (e.g., ga, o,ni, wa, mo).
(4)  a. atama-ga   head-NOMb.
70-paasento-o  70-percent-ACCThe extracted nouns under this definition are initialcandidates for ATNs.Once bare nouns are identified, they arechecked against our syntactic-property- (i.e., part-of-speech, POS) based-, followed by semantic-attribute (SEM) based-heuristics.
For semanticfiltering, we decided to use the noun groups ofhigh frequency (more than two tokens categorizedin the same group; indicated in bold in Table 3above) to minimize a risk of over-generalization.The algorithm checks the following two condi-tions, for each bare noun, in this order:[1] If POS = [nominalized verval, derived noun,formal noun, numeral + counter suffix com-pound], label it as ATN.
[2] If SEM = [2610: location, 2585: amount,362: organization, 552: animate (part), 111: hu-man (relation), 224: human (profession), 72:3 Japanese do not use determiners for its nouns.human (kinship), 866: housing (part), 813: cloth-ing], label it as ATN.
4Therefore, nouns that pass condition [1] are labeledas ATNs, without checking their semantic proper-ties.
A noun that fails to pass condition [1] andpasses condition [2] is labeled as ATN.
A nounthat fails to match both [1] and [2] is labeled asnon-ATN.
Consider the noun sintyo ?height?
forexample.
Its POS code in GT is common noun, soit fails condition [1] and goes to [2].
This noun iscategorized in the ?2591: measures?
group whichis under the ?2585: amount?
node in the hierarchytree, so it is labeled as ATN.
In this way, the algo-rithm labels each bare noun as either ATN or non-ATN.4.4 EvaluationTo assess the performance of our algorithm, we ranit by hand on a sample text.5  The test corpus con-tains a total of 136 bare nouns.
We then matchedthe result against our manually-extracted ATNs (34in number).
The result is shown in Table 4 below,with recall and precision metrics.
As a baselinemeasurement, we give the accuracy for classifyingevery bare noun as ATN.
For comparison, we alsoprovide the results when only either POS-based orsemantic-based heuristics are applied.Recall PrecisionBaseline 34/34    (100%) 34/136 (25.00%)POS only 2/34 (  5.88%) 2/6 (33.33%)Semantic only 30/34 (88.23%) 30/35 (85.71%)POS/Semantic 32/34 (94.11%) 32/41 (78.04%)Table 4: Algorithm evaluationSemantic categories make a greater contributionto identifying ATNs than POS.
However, thePOS/Semantic algorithm achieved a higher recallbut a lower precision than the semantic-only algo-rithm did.
This is mainly because the former pro-duced more over-detected errors.
Closerexamination of those errors indicates that most ofthem (8 out of 9 cases) involve verbal idiomaticexpressions that contain ATN candidate nouns, asexample (5) shows.4 These numbers indicate the numbers assigned to each seman-tic category in Goi-Taikei Japanese Lexicon (GT).5 This is taken from the same genre as our corpus for the initialanalysis, i.e., another JSL textbook.
(5) me-o-samasu   eye-ACC-wake   ?wake up?Although me ?eye?
is a strong ATN candidate, as inexample (2) above, case (5) should be treated aspart of an idiomatic expression rather than as azero adnominal expression.6  Thus, we decided toadd another condition, [0] below, before we applythe POS/SEM checks.
The revised algorithm is asfollows:[0] If part of idiom in [idiom list],7 label it asnon-ATN.
[1] If POS = [nominalized verval, derived noun,formal noun, numeral + counter suffix com-pound], label it as ATN.
[2] If SEM = [2610: location, 2585: amount,362: organization, 552: animate (part), 111: hu-man (relation), 224: human (profession), 72:human (kinship), 866: housing (part), 813: cloth-ing], label it as ATN.When a noun matches condition [0], it will not bechecked against [1] and [2].
When this applies, theevaluation result is now as shown below.Recall PrecisionPOS only 2/34 (  5.88%) 2/4 (50.00%)Semantic only 30/34 (88.23%) 31/35 (88.57%)POS/Semantic 32/34 (94.11%) 32/33 (96.96%)Table 5: Revised-algorithm evaluationThe revised algorithm, with both syntac-tic/semantic heuristics and the additional idiom-filtering rule, achieved a precision of 96.96%.
Theresult still includes some over/under-detecting er-rors, which will require future attention.5 Related WorkAssociative anaphora (e.g., Poesio and Vieira,1998) and indirect anaphora (e.g., Murata and Na-gao, 2000) are virtually the same phenomena thatthis paper is concerned with, as illustrated in (6).6 Vieira and Poesio (2000) also list ?idiom?
as one use of defi-nite descriptions (English equivalent to Japanese bare nouns),along with same head/associative anaphora, etc.7 The list currently includes eight idiomatic samples from thetest data, but it should of course be expanded in the future.
(6) a. a house ?
the roofb.
ie ?house?
?
yane ?roof?c.
ie ?house?
?
(?-no) yane ?(?
?s) roof?We take a zero adnominal approach, as in (6c),because we assume, for our pedagogical purposediscussed in section 3.2, that zero adnominals, bymaking them visible, more effectively prompt peo-ple to notice referential links than lexical relations,such as meronymy in (6a) and (6b).However, insights from other approaches areworth attention.
There is a strong resemblancebetween bare nouns (that zero adnominals co-occurwith) in Japanese and definite descriptions in Eng-lish in their behaviors, especially in their referen-tial properties (Sakahara, 2000).
The task ofclassifying several different uses of definite de-scriptions (Vieira and Poesio, 2000; Bean andRiloff, 1999) is somewhat analogous to that forbare nouns.
Determining definiteness of Japanesenoun phrases (Heine, 1998; Bond et al, 1995; Mu-rata and Nagao, 1993)8 is also relevant to ATN(which is definite in nature) recognition.6 Future DirectionsWe have proposed an ATN (hence zero adnomi-nal) recognition algorithm, with lexicon-based heu-ristics that were inferred from our corpusinvestigation.
The evaluation result shows that thesyntactic/semantic feature-based generalization(using GT) is capable of identifying potentialATNs.
The evaluation on a larger corpus, ofcourse, is essential to verify this claim.
Implemen-tation of the algorithm is also in our future agenda.This approach has its limitations, too, as ispointed out by Kurohashi et al (1999).
One limi-tation is illustrated by a pair of Japanese nouns,sakusya ?author?
and sakka ?writer,?
which fall un-der the same GT semantic property group (at thedeepest level).9  These nouns have an intuitivelydifferent status for their valency requirements; theformer requires ?of-what work?
information, whilethe latter does not.
10   We risk over- or under-generation when we designate certain semanticproperties, no matter how fine-grained they might8 Their interests are in machine-translation of Japanese intolanguages that require determiners for their nouns.9 This example pair is taken from Iori (1997).10 This intuition was verified by an informal poll conducted onseven native speakers of Japanese.be.
We proposed the idiom-filtering rule to solveone case of over-detection.
A larger-scale evalua-tion of the algorithm and its error analysis mightlead to additional rules that refine extracted ATNcandidates.
Insights from the works presented inthe previous section could also be incorporated.Determining an appropriate level of generaliza-tion is a significant factor for this type of approach,and this was done, in this study, according to ourintrospective judgments.
More systematic methodsshould be explored.A related issue is the notoriously hard-to-defineargument-adjunct distinction for nouns, which isclosely related to the distinction between ATNsand non-ATNs.
We experimentally tested sevennative-Japanese-speaking subjects in distinguish-ing these two.
We presented 26 nouns in the sameGT semantic category (at the deepest level): ?per-sons who write.?
There were six nouns which allthe subjects agreed on categorizing as ATNs, in-cluding sakusha ?author.?
Five nouns, includingsakka ?writer,?
on the other hand, were judged asnon-ATNs by all the subjects.
For the remaining15 nouns, however, their judgments varied widely.As Somers (1984) suggests for verbs, binary dis-tinction does not work well for nouns, either.
Thisdistinction might largely depend on the context insome cases.
This is also something we will need toaddress.In this study, we focused on ?implicit argu-ment-taking nouns.?
There may be a line (al-though it may be very thin) between nouns whichtake explicit arguments and those which take im-plicit arguments.
This distinction also needs fur-ther investigation in the corpus.AcknowledgementsSome of the foundation work for this paper wasdone while the author was at NTT CommunicationScience Laboratories, NTT Corporation, Japan, asa research intern.
The author would like to thankLaurel Fais and Miho Fujiwara for their support,and anonymous reviewers for their insightfulcomments and suggestions that helped elaborate anearlier draft into this paper.ReferencesBean, David L. and Ellen Riloff.
1999.
Corpus-basedidentification of non-anaphoric noun phrases.
In Pro-ceedings of the 37th Annual Meeting of the ACL, 373-380.Bond, Francis, Kentaro Ogura, and Satoru Ikehara.
1995.Possessive pronouns as determiners in Japanese-to-English machine translation.
In Proceedings of the2nd Pacific Association for Computational Linguisticsconference.Bond, Francis, Kentaro Ogura, and Tsukasa Kawaoka.1995.
Noun phrase reference in Japanese-to-Englishmachine translation.
In Proceedings of the 6th Inter-national Conference on Theoretical and Methodo-logical Issues in Machine Translation, 1-14.Fais, Laurel and Mitsuko Yamura-Takei (under review).Salience ranking in centering: The case of a Japanesecomplex nominal.
ms.Grosz, Barbara J., Aravind Joshi, and Scott Weinstein.1995.
Centering: a framework for modeling the localcoherence of discourse.
Computational Linguistics,21(2), 203-225.Halliday, M.A.K.
and Ruqaiya Hasan.
1976.
Cohesionin English.
Longman, New York.Heine, Julia E. 1998.
Definiteness prediction for Japa-nese noun phrases.
In Proceedings of theCOLING/ACL?98, Quebec, 519-525.Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai,Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura, andYoshifumi Oyama, editors.
1997.
Goi-Taikei ?
Japa-nese Lexicon.
Iwanami Publishing, Tokyo.Iori, Isao.
1997.
Aspects of Cohesion in Japanese Texts.Unpublished PhD dissertation, Osaka University (inJapanese).Kojima, Sachiko.
1992.
Low-independence nouns andcopula expressions.
In IPA Technical Report No.
3-125, 175-198 (in Japanese).Kurohashi and Sakai.
1999.
Semantic analysis of Japa-nese noun phrases: A new approach to dictionary-based understanding.
In Proceedings of the 37th An-nual Meeting of the ACL, 481-488.Murata, Masaki and Makoto Nagao.
1993.
Determina-tion of referential property and number of nouns inJapanese sentences for machine translation into Eng-lish.
In Proceedings of the 5th International Confer-ence on Theoretical and Methodological Issues inMachine Translation, 218-225.Murata, Masaki and Makoto Nagao.
2000.
Indirect ref-erence in Japanese sentences.
In Botley, S. andMcEnerry, A.
(eds.)
Corpus-based and Computa-tional Approaches to Discourse Anaphora, 189-212.John Benjamins, Amsterdam/Philadelphia.Poesio, Massimo and Renata Vieira.
1998.
A corpus-based investigation of definite description use.
Com-putational Linguistics, 24(2): 183-216.Sakahara, Shigeru.
2000.
Advances in Cognitive Lin-guistics.
Hituzi Syobo Publishing, Tokyo, Japan (inJapanese).Shimazu, Akira, Shozo Naito, and Hirosato Nomura.1985.
Classification of semantic structures in Japa-nese sentences with special reference to the nounphrase (in Japanese).
In Information Processing So-ciety of Japan, Natural Language Special InterestGroup Technical Report, No.
47-4.Shimazu, Akira, Shozo Naito, and Hirosato Nomura.1986.
Analysis of semantic relations between nounsconnected by a Japanese particle ?no.?
MathematicalLinguistics, 15(7), 247-266 (in Japanese).Shimazu, Akira, Shozo Naito, and Hirosato Nomura.1987.
Semantic structure analysis of Japanese nounphrases with adnominal particles.
In Proceedings ofthe 25th Annual Meeting of the ACL, Stanford, 123-130.Somers, Harold L. 1984.
On the validity of the comple-ment-adjunct distinction in valency grammar.
Lin-guistics 22, 507-53.Teramura, Hideo.
1991.
Japanese Syntax and MeaningII.
Kurosio Publishers, Tokyo (in Japanese).Vieira, Renata and Massimo Poesio.
2000.
An empiri-cally based system for processing definite descrip-tions.
Computational Linguistics, 26(4): 525-579.Yamura-Takei, Mitsuko, Laurel Fais, Miho Fujiwaraand Teruaki Aizawa.
2003.
Forgotten referentiallinks in Japanese discourse and centering.
ms.Yamura-Takei, Mitsuko, Miho Fujiwara, Makoto Yo-shie, and Teruaki Aizawa.
2002.
Automatic linguis-tic analysis for language teachers: The case of zeros.In Proceedings of the 19th International Conferenceon Computational Linguistics (COLING), Taipei,1114-1120.
