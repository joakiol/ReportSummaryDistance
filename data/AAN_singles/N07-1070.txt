Proceedings of NAACL HLT 2007, pages 556?563,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTowards Robust Semantic Role LabelingSameer PradhanBBN TechnologiesCambridge, MA 02138pradhan@bbn.comWayne Ward, James H. MartinUniversity of ColoradoBoulder, CO 80303{whw,martin}@colorado.eduAbstractMost research on semantic role labeling(SRL) has been focused on training andevaluating on the same corpus in orderto develop the technology.
This strategy,while appropriate for initiating research,can lead to over-training to the particularcorpus.
The work presented in this pa-per focuses on analyzing the robustnessof an SRL system when trained on onegenre of data and used to label a differentgenre.
Our state-of-the-art semantic rolelabeling system, while performing well onWSJ test data, shows significant perfor-mance degradation when applied to datafrom the Brown corpus.
We present a se-ries of experiments designed to investigatethe source of this lack of portability.
Theseexperiments are based on comparisons ofperformance using PropBanked WSJ dataand PropBanked Brown corpus data.
Ourresults indicate that while syntactic parsesand argument identification port relativelywell to a new genre, argument classifica-tion does not.
Our analysis of the reasonsfor this is presented and generally pointto the nature of the more lexical/semanticfeatures dominating the classification taskand general structural features dominatingthe argument identification task.1 IntroductionAutomatic, accurate and wide-coverage techniquesthat can annotate naturally occurring text with se-mantic argument structure play a key role in NLPapplications such as Information Extraction (Sur-deanu et al, 2003; Harabagiu et al, 2005), QuestionAnswering (Narayanan and Harabagiu, 2004) andMachine Translation (Boas, 2002; Chen and Fung,2004).
Semantic Role Labeling (SRL) is the pro-cess of producing such a markup.
When presentedwith a sentence, a parser should, for each predicatein the sentence, identify and label the predicate?s se-mantic arguments.
In recent work, a number of re-searchers have cast this problem as a tagging prob-lem and have applied various supervised machinelearning techniques to it.
On the Wall Street Jour-nal (WSJ) data, using correct syntactic parses, it ispossible to achieve accuracies rivaling human inter-annotator agreement.
However, the performance gapwidens when information derived from automaticsyntactic parses is used.So far, most of the work on SRL systems has beenfocused on improving the labeling performance on atest set belonging to the same genre of text as thetraining set.
Both the Treebank on which the syntac-tic parser is trained and the PropBank on which theSRL systems are trained represent articles from theyear 1989 of the WSJ.
While all these systems per-form quite well on the WSJ test data, they show sig-nificant performance degradation (approximately 10point drop in F-score) when applied to label test datathat is different than the genre that WSJ represents(Pradhan et al, 2004; Carreras and Ma`rquez, 2005).556Surprisingly, it does not matter much whether thedata is from another newswire, or a completely dif-ferent type of text ?
as in the Brown corpus.
Theseresults indicate that the systems are being over-fit tothe specific genre of text.
Many performance im-provements on the WSJ PropBank corpus may re-flect tuning to the corpus.
For the technology tobe widely accepted and useful, it must be robustto change in genre of the data.
Until recently, datatagged with similar semantic argument structure wasnot available for multiple genres of text.
Recently,Palmer et al, (2005), have PropBanked a significantportion of the Treebanked Brown corpus which en-ables us to perform experiments to analyze the rea-sons behind the performance degradation, and sug-gest potential solutions.2 Semantic Annotation and CorporaIn the PropBank1 corpus (Palmer et al, 2005), pred-icate argument relations are marked for the verbsin the text.
PropBank was constructed by assign-ing semantic arguments to constituents of the hand-corrected Treebank parses.
The arguments of a verbare labeled ARG0 to ARG5, where ARG0 is thePROTO-AGENT (usually the subject of a transitiveverb) ARG1 is the PROTO-PATIENT (usually its di-rect object), etc.
In addition to these CORE ARGU-MENTS, 16 additional ADJUNCTIVE ARGUMENTS,referred to as ARGMs are also marked.More recently the PropBanking effort has beenextended to encompass multiple corpora.
In thisstudy we use PropBanked versions of the Wall StreetJournal (WSJ) part of the Penn Treebank (Marcus etal., 1994) and part of the Brown portion of the PennTreebank.The WSJ PropBank data comprise 24 sectionsof the WSJ, each section representing about 100documents.
PropBank release 1.0 contains about114,000 predicates instantiating about 250,000 argu-ments and covering about 3,200 verb lemmas.
Sec-tion 23, which is a standard test set and a test setin some of our experiments, comprises 5,400 predi-cates instantiating about 12,000 arguments.The Brown corpus is a Standard Corpus of Ameri-can English that consists of about one million wordsof English text printed in the calendar year 19611http://www.cis.upenn.edu/?ace/(Kuc?era and Francis, 1967).
The corpus containsabout 500 samples of 2000+ words each.
The ideabehind creating this corpus was to create a hetero-geneous sample of English text so that it would beuseful for comparative language studies.The Release 3 of the Penn Treebank contains thehand parsed syntactic trees of a subset of the BrownCorpus ?
sections F, G, K, L, M, N, P and R. Palmeret al, (2005) have recently PropBanked a signifi-cant portion of this Treebanked Brown corpus.
Inall, about 17,500 predicates are tagged with their se-mantic arguments.
For these experiments we used alimited release of PropBank dated September 2005.A small portion of the predicates ?
about 8,000 havealso been tagged with frame sense information.3 SRL System DescriptionWe formulate the labeling task as a classificationproblem as initiated by Gildea and Jurafsky (2002)and use Support Vector Machine (SVM) classi-fiers (2005).
We use TinySVM2 along with Yam-Cha3 (Kudo and Matsumoto, 2000) (Kudo and Mat-sumoto, 2001) as the SVM training and classifica-tion software.
The system uses a polynomial kernelwith degree 2; the cost per unit violation of the mar-gin, C=1; and, tolerance of the termination criterion,e=0.001.
More details of this system can be foundin Pradhan et al, (2005).
The performance of thissystem on section 23 of the WSJ when trained onsections 02-21 is shown in Table 1ALL ARGs Task P R F A(%) (%) (%)TREEBANK Id.
97.5 96.1 96.8Class.
- - - 93.0Id.
+ Class.
91.8 90.5 91.2AUTOMATIC Id.
86.9 84.2 85.5Class.
- - - 92.0Id.
+ Class.
82.1 77.9 79.9Table 1: Performance of the SRL system on WSJThe performance of the SRL system is reportedon three different tasks, all of which are with respectto a particular predicate: i) argument identification(ID), is the task of identifying the set of words (here,parse constituents) that represent a semantic role; ii)argument classification (Class.
), is the task of clas-sifying parse constituents known to represent some2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/557semantic role into one of the many semantic roletypes; and iii) argument identification and classifi-cation (ID + Class.
), which involves both the iden-tification of the parse constituents that represent se-mantic roles of the predicate and their classificationinto the respective semantic roles.
As usual, argu-ment classification is measured as percent accuracy(A), whereas ID and ID + Class.
are measured interms of precision (P), recall (R) and F-score (F)?
the harmonic mean of P and R. The first threerows of Table 1 report performance for the systemthat uses hand-corrected Treebank parses, and thenext three report performance for the SRL systemthat uses automatically generated ?
Charniak parser?
parses, both during training and testing.4 Robustness ExperimentsThis section describes experiments that we per-formed using the PropBanked Brown corpus in anattempt to analyze the factors affecting the portabil-ity of SRL systems.4.1 How does the SRL system trained on WSJperform on Brown?In order to test the robustness of the SRL system,we used a system trained on the PropBanked WSJcorpus to label data from the Brown corpus.
We usethe entire PropBanked Brown corpus (about 17,500predicates) as a test set for this experiment and usethe SRL system trained on WSJ sections 02-21 totag its arguments.Table 2 shows the performance for training andtesting on WSJ, and for training on WSJ and testingon Brown.
There is a significant reduction in per-formance when the system trained on WSJ is usedto label data from the Brown corpus.
The degrada-tion in the Identification task is small compared tothat of the combined Identification and Classifica-tion task.
A number of factors could be responsiblefor the loss of performance.
It is possible that theSRL models are tuned to the particular vocabularyand sense structure associated with the training data.Also, since the syntactic parser that is used for gen-erating the syntax parse trees (Charniak) is heavilylexicalized and is trained on WSJ, it could have de-creased accuracy on the Brown data resulting in re-duced accuracy for Semantic Role Labeling.
Sincethe SRL algorithm walks the syntax tree classifyingeach node, if no constituent node is present that cor-responds to the correct argument, the system cannotproduce a correct labeling for the argument.Train Test Id.
Id.
+ ClassF FWSJ WSJ 85.5 79.9WSJ Brown 82.4 65.1Table 2: Performance of the SRL system on Brown.In order to check the extent to which constituentnodes representing semantic arguments were deletedfrom the syntax tree due to parser error, we gener-ated the performance numbers which are shown inTable 3.
These numbers are for top one parse for theCharniak parser, and represent not all parser errors,but deletion of argument bearing constituent nodes.Total Misses %PropBank 12000 800 6.7Brown 45880 3692 8.1Table 3: Constituent deletions in WSJ and Brown.The parser misses 6.7% of the argument-bearingnodes in the PropBank test set and about 8.1% inthe Brown corpus.
This indicates that the errors insyntactic parsing account for a fairly small amountof the argument deletions and probably do not con-tributing significantly to the increased SRL errorrate.
Obviously, just the presence of a argument-bearing constituent does not necessarily guaranteethe correctness of the structural connections be-tween itself and the predicate.4.2 Identification vs Classification PerformanceDifferent features tend to dominate in the identifi-cation task vs the classification task.
For example,the path feature (representing the path in the syntaxtree from the argument to the predicate) is the sin-gle most salient feature for the ID task and is notvery important in the classification task.
In the nextexperiment we look at cross genre performance ofthe ID and Classification tasks.
We used gold stan-dard syntactic trees from the Treebank so there areno errors in generating the syntactic structure.
Inaddition to training on the WSJ and testing on WSJand Brown, we trained the SRL system on a Browntraining set and tested it on a test set alo from theBrown corpus.
In generating the Brown training and558SRL SRL Task P R F ATrain Test (%) (%) (%)WSJ WSJ Id.
97.5 96.1 96.8(104k) (5k) Class.
93.0Id.
+ Class.
91.8 90.5 91.2WSJ WSJ Id.
96.3 94.4 95.3(14k) (5k) Class.
86.1Id.
+ Class.
84.4 79.8 82.0BROWN BROWN Id.
95.7 94.9 95.2(14k) (1.6k) Class.
80.1Id.
+ Class.
79.9 77.0 78.4WSJ BROWN Id.
94.2 91.4 92.7(14k) (1.6k) Class.
72.0Id.
+ Class.
71.8 65.8 68.6Table 4: Performance of the SRL system using correct Treebank parses.test sets, we used stratified sampling, which is oftenused by the syntactic parsing community (Gildea,2001).
The test set was generated by selecting ev-ery 10th sentence in the Brown Corpus.
We alsoheld out the development set used by Bacchiani etal., (2006) to tune system parameters in the future.This procedure resulted in a training set of approxi-mately 14,000 predicates and a test set of about 1600predicates.
We did not perform any parameter tun-ing for any of the following experiments, and usedthe parameter settings from the best performing ver-sion of the SRL system as reported in Table1.
Wecompare the performance on this test set with thatobtained when the SRL system is trained using WSJsections 02-21 and use section 23 for testing.
Fora more balanced comparison, we retrained the SRLsystem on the same amount of data as used for train-ing on Brown, and tested it on section 23.
As usual,trace information, and function tag information fromthe Treebank is stripped out.Table 4 shows the results.
There is a fairly smalldifference in argument Identification performancewhen the SRL system is trained on 14,000 predi-cates vs 104,000 predicates from the WSJ (F-score95.3 vs 96.8).
However, there is a considerable dropin Classification accuracy (86.1% vs 93.0%).
Whenthe SRL system is trained and tested on Brown data,the argument Identification performance is not sig-nificantly different than that for the system trainedand tested on WSJ data (F-score 95.2 vs 95.3).
Thedrop in argument Classification accuracy is muchmore severe (86.1% vs 80.1%).This same trend between ID and Classification iseven more pronounced when training on WSJ andtesting on Brown.
For a system trained on WSJ,there is a fairly small drop in performance of theID task when tested on Brown vs tested on WSJ (F-score 92.7 vs 95.3).
However, in this same condi-tion, the Classification task has a very large drop inperformance (72.0% vs 86.1%).So argument ID is not very sensitive to amountof training data in a corpus, or to the genre of thecorpus, and ports well from WSJ to Brown.
This ex-periment supports the belief that there is no signifi-cant drop in the task of identifying the right syntacticconstituents that are arguments ?
and this is intuitivesince previous experiments have shown that the taskof argument identification is more dependent on thestructural features ?
one such feature being the pathin the syntax tree.Argument Classification seems to be the problem.It requires more training data within the WSJ corpus,does not perform as well when trained and tested onBrown as it does for WSJ and does not port wellfrom WSJ to Brown.
This suggests that the featuresit uses are being over-fit to the training data and aremore idiosyncratic to a given dataset.
In particular,the predicate whose arguments are being identified,and the head word of the syntactic constituent beingclassified are both important features in the task ofargument classification.As a generalization, the features used by the Iden-tification task reflect structure and port well.
Thefeatures used by the Classification task reflect spe-cific lexical usage and semantics, and tend to requiremore training data and are more subject to over-fitting.
Even when training and testing on Brown,Classification accuracy is considerably worse than559training and testing on WSJ (with comparable train-ing set size).
It is probably the case that the predi-cates and head words in a homogeneous corpus suchas the WSJ are used more consistently, and tend tohave single dominant word senses.
The Brown cor-pus probably has much more variety in its lexicalusage and word senses.4.3 How sensitive is semantic argumentprediction to the syntactic correctnessacross genre?This experiment examines the same cross-genre ef-fects as the last experiment, but uses automaticallygenerated syntactic parses rather than gold standardones.For this experiment, we used the same amount oftraining data from WSJ as available in the Browntraining set ?
that is about 14,000 predicates.
Theexamples from WSJ were selected randomly.
TheBrown test set is the same as used in the previousexperiment, and the WSJ test set is the entire section23.Recently there have been some improvements tothe Charniak parser, use n-best re-ranking as re-ported in (Charniak and Johnson, 2005) and self-training and re-ranking using data from the NorthAmerican News corpus (NANC) and adapts muchbetter to the Brown corpus (McClosky et al, 2006a;McClosky et al, 2006b).
The performance of theseparsers as reported in the respective literature areshown in Table 6 shows the performance (as re-ported in the literature) of the Charniak parser: whentrained and tested on WSJ, when trained on WSJ andtested on Brown, When trained and tested on Brown,and when trained on WSJ and adapted with NANC.Train Test FWSJ WSJ 91.0WSJ Brown 85.2Brown Brown 88.4WSJ+NANC Brown 87.9Table 6: Charniak parser performance.We describe the results of Semantic Role Label-ing under the following five conditions:1.
The SRL system is trained on features ex-tracted from automatically generated parses ofthe PropBanked WSJ sentences.
The syntacticparser ?
Charniak parser ?
is itself trained onthe WSJ training sections of the Treebank.
Thisis used for Semantic Role Labeling of section-23 of WSJ.2.
The SRL system is trained on features ex-tracted from automatically generated parses ofthe PropBanked WSJ sentences.
The syntac-tic parser ?
Charniak parser ?
is itself trainedon the WSJ training sections of the Treebank.This is used to classify the Brown test set.3.
The SRL system is trained on features ex-tracted from automatically generated parses ofthe PropBanked Brown corpus sentences.
Thesyntactic parser is trained using the WSJ por-tion of the Treebank.
This is used to classifythe Brown test set.4.
The SRL system is trained on features ex-tracted from automatically generated parses ofthe PropBanked Brown corpus sentences.
Thesyntactic parser is trained using the Browntraining portion of the Treebank.
This is usedto classify the Brown test set.5.
The SRL system is trained on features ex-tracted from automatically generated parses ofthe PropBanked Brown corpus sentences.
Thesyntactic parser is the version that is self-trained using 2,500,000 sentences from NANC,and where the starting version is trained onlyon WSJ data (McClosky et al, 2006b).
This isused to classify the Brown test set.Table 5 shows the results.
For simplicity of dis-cussion we have tagged the five conditions as 1.,2., 3., 4., and 5.
Comparing conditions 2. and 3.shows that when the features used to train the SRLsystem are extracted using a syntactic parser that istrained on WSJ it performs at almost the same levelon the task of Identification, regardless of whetherit is trained on the PropBanked Brown corpus orthe PropBanked WSJ corpus.
This, however, is sig-nificantly lower than when all the three ?
the syn-tactic parser training set, the SRL system trainingset, and the SRL system test set, are from the samegenre (6 F-score points lower than condition 1, and5 points lower than conditions 4 and 5).
In case ofthe combined task, the gap between the performancefor conditions 2 and 3 is about 10 points in F-score(59.1 vs 69.8).
Looking at the argument classifica-tion accuracies, we see that using the SRL system560Setup Parser SRL SRL Task P R F ATrain Train Test (%) (%) (%)1.
WSJ WSJ WSJ Id.
87.3 84.8 86.0(40k ?
sec:00-21) (14k) (5k) Class.
84.1Id.
+ Class.
77.5 69.7 73.42.
WSJ WSJ Brown Id.
81.7 78.3 79.9(40k ?
sec:00-21) (14k) (1.6k) Class.
72.1Id.
+ Class.
63.7 55.1 59.13.
WSJ Brown Brown Id.
81.7 78.3 80.0(40k ?
sec:00-21) (14k) (1.6k) Class.
79.2Id.
+ Class.
78.2 63.2 69.84.
Brown Brown Brown Id.
87.6 82.3 84.8(20k) (14k) (1.6k) Class.
78.9Id.
+ Class.
77.4 62.1 68.95.
WSJ+NANC Brown Brown Id.
87.7 82.5 85.0(2,500k) (14k) (1.6k) Class.
79.9Id.
+ Class.
77.2 64.4 70.0Table 5: Performance on WSJ and Brown using automatic syntactic parsestrained on WSJ to test Brown sentences give a 12point drop in F-score (84.1 vs 72.1).
Using the SRLsystem trained on Brown using WSJ trained syntac-tic parser shows a drop in accuracy by about 5 F-score points (84.1 to 79.2).
When the SRL system istrained on Brown using syntactic parser also trainedon Brown, we get a quite similar classification per-formance, which is again about 5 points lower thanwhat we get using all WSJ data.
This shows lexicalsemantic features might be very important to get abetter argument classification on Brown corpus.4.4 How much data is required to adapt to anew genre?We would like to know how much data from a newgenre we need to annotate and add to the trainingdata of an existing corpus to adapt the system suchthat it gives the same level of performance as whenit is trained on the new genre.One section of the Brown corpus ?
section CKhas about 8,200 predicates annotated.
We use sixdifferent conditions ?
two in which we use correctTreebank parses, and the four others in which weuse automatically generated parses using the varia-tions described before.
All training sets start withthe same number of examples as in the Brown train-ing set.
The part of this section used as a test set forthe CoNLL 2005 shared task is used as the test sethere.
It contains a total of about 800 predicates.Table 7 shows a comparison of these conditions.In all the six conditions, the performance on the taskof Identification and Classification improves gradu-ally until about 5625 examples of section CK whichis about 75% of the total added, above which theyimprove very little.
In fact, even 50% of the newdata accounts for 90% of the performance differ-ence.
Even when the syntactic parser is trained onWSJ and the SRL is trained on WSJ, adding 7,500instances of the new genres allows it to achieve al-most the same performance as when all three arefrom the same genre (67.2 vs 69.9).
Numbers for ar-gument identification aren?t shown because addingmore data does not have any statistically signifi-cant impact on its performance.
The system thatuses self-trained syntactic parser seems to performslightly better than the rest of the versions that useautomatically generated syntactic parses.
The preci-sion numbers are almost unaffected ?
except whenthe labeler is trained on WSJ PropBank data.4.5 How much does verb sense informationcontribute?In order to find out how important the verb senseinformation is in the process of genre transfer, weused the subset of PropBanked Brown corpus thatwas tagged with verb sense information, ran an ex-periment similar to that of Experiment 1.
We usedthe oracle sense information and correct syntactic in-formation for this experiment.Table 8 shows the results of this experiment.There is about 1 point F-score increase on usingoracle sense information on the overall data.
Welooked at predicates that had high perplexity in boththe training and test sets, and whose sense distribu-561Parser SRL Id.
+ Class Parser SRL Id.
+ ClassP R F P R FTrain Train (%) (%) (%) (%)WSJ WSJ (14k) WSJ Brown (14k)(Treebank parses) (Treebank parses)+0 ex.
from CK 74.1 66.5 70.1 (40k) +0 ex.
from CK 74.4 57.0 64.5+1875 ex.
from CK 77.6 71.3 74.3 +1875 ex.
from CK 75.1 58.7 65.9+3750 ex.
from CK 79.1 74.1 76.5 +3750 ex.
from CK 76.1 59.6 66.9+5625 ex.
from CK 80.4 76.1 78.1 +5625 ex.
from CK 76.9 60.5 67.7+7500 ex.
from CK 80.2 76.1 78.1 +7500 ex.
from CK 76.8 59.8 67.2Brown Brown (14k) Brown Brown (14k)(Treebank parses) (Treebank parses)+0 ex.
from CK 77.1 73.0 75.0 (20k) +0 ex.
from CK 76.0 59.2 66.5+1875 ex.
from CK 78.8 75.1 76.9 +1875 ex.
from CK 76.1 60.0 67.1+3750 ex.
from CK 80.4 76.9 78.6 +3750 ex.
from CK 77.7 62.4 69.2+5625 ex.
from CK 80.4 77.2 78.7 +5625 ex.
from CK 78.2 63.5 70.1+7500 ex.
from CK 81.2 78.1 79.6 +7500 ex.
from CK 78.2 63.2 69.9WSJ WSJ (14k) WSJ+NANC Brown (14k)(40k) +0 ex.
from CK 65.2 55.7 60.1 (2,500k) +0 ex.
from CK 74.4 60.1 66.5+1875 ex.
from CK 68.9 57.5 62.7 +1875 ex.
from CK 76.2 62.3 68.5+3750 ex.
from CK 71.8 59.3 64.9 +3750 ex.
from CK 76.8 63.6 69.6+5625 ex.
from CK 74.3 61.3 67.2 +5625 ex.
from CK 77.7 63.8 70.0+7500 ex.
from CK 74.8 61.0 67.2 +7500 ex.
from CK 78.2 64.9 70.9Table 7: Effect of incrementally adding data from a new genreTrain Test Without Sense With SenseId.
Id.F FWSJ Brown (All) 69.1 69.9WSJ Brown (predicate: go) 46.9 48.9Table 8: Influence of verb sense feature.tion was different.
One such predicate is ?go?.
Theimprovement on classifying the arguments of thispredicate was about 2 points (46.9 to 48.9), whichsuggests that verb sense is more important when thesense structure of the test corpus is more ambiguousand is different from the training.
Here we used ora-cle verb sense information, but one can train a clas-sifier as done by Girju et al, (2005) which achievesa disambiguation accuracy in the 80s for within theWSJ corpus.5 ConclusionsOur experimental results on robustness to change ingenre can be summarized as follows:?
There is a significant drop in performance whentraining and testing on different corpora ?
forboth Treebank and Charniak parses?
In this process the classification task is moredisrupted than the identification task.?
There is a performance drop in classificationeven when training and testing on Brown (com-pared to training and testing on WSJ)?
The syntactic parser error is not a large part ofthe degradation for the case of automaticallygenerated parses.An error analysis leads us to believe that somereasons for this behavior could be: i) lexical us-ages that are specific to WSJ, ii) variation in sub-categorization across corpora, iii) variation in wordsense distribution and iv) changes in topics and enti-ties.
Training and testing on the same corpora tendsto give a high weight to very specific semantic fea-tures.
Two possibilities remedies could be: i) usingless homogeneous corpora and ii) less specific fea-tures, for eg., proper names are replaced with thename entities that they represent.
This way the sys-tem could be forced to use the more general features.Both of these manipulations would most likely re-duce performance on the training set, and on testsets of the same genre as the training data.
But theywould be likely to generalize better.6 AcknowledgmentsWe are extremely grateful to Martha Palmer for pro-viding us with the PropBanked Brown corpus, andto David McClosky for providing us with hypothe-ses on the Brown test set as well as a cross-validated562version of the Brown training data for the variousmodels reported in his work reported at HLT 2006.This research was partially supported bythe ARDA AQUAINT program via contractOCG4423B and by the NSF via grants IS-9978025and ITR/HCI 0086132.ReferencesMichiel Bacchiani, Michael Riley, Brian Roark, andRichard Sproat.
2006.
MAP adaptation of stochas-tic grammars.
Computer Speech and Language,20(1):41?68.Hans Boas.
2002.
Bilingual framenet dictionaries formachine translation.
In Proceedings of LREC-2002.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: Semantic role label-ing.
In Proceedings of CoNLL-2005, pages 152?164,Ann Arbor, MI.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of ACL-2005, pages 173?180,Ann Arbor, MI.Benfeng Chen and Pascale Fung.
2004.
Automatic con-struction of an english-chinese bilingual framenet.
InProceedings of the HLT/NAACL-2004, Boston, MA.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In In Proceedings of EMNLP-2001.R.
Girju, D. Roth, and M. Sammons.
2005.
Token-level disambiguation of verbnet classes.
In Proceed-ings of the Interdisciplinary Workshop on the Identifi-cation and Representation of Verb Features and VerbClasses, K. Erk, A. Melinger, and S. Schulte im Walde(eds.
).Sanda Harabagiu, Cosmin Adrian Bejan, andPaul Morarescu.
2005.
Shallow semantics for relationextraction.
In IJCAI-2005, pages 1061?1067, Edin-burgh, Scotland.Henry Kuc?era and W. Nelson Francis.
1967.
Com-putational analysis of present-day American English.Brown University Press, Providence, RI.Taku Kudo and Yuji Matsumoto.
2000.
Use of supportvector learning for chunk identification.
In Proceed-ings of CoNLL-2000 and LLL-2000, pages 142?144.Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings of theNAACL-2001.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.David McClosky, Eugene Charniak, and Mark Johnson.2006a.
Effective self-training for parsing.
In Proceed-ings of HLT/NAACL-2006, pages 152?159, New YorkCity, USA.
Association for Computational Linguistics.David McClosky, Eugene Charniak, and Mark Johnson.2006b.
Rerankinng and self-training for parser adapta-tion.
In Proceedings of COLING/ACL-2006, Sydney,Australia.Srini Narayanan and Sanda Harabagiu.
2004.
Questionanswering based on semantic structures.
In Proceed-ings of COLING-2004), Geneva, Switzerland.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of HLT/NAACL-2004, Boston, MA.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2005.
Semantic role label-ing using different syntactic views.
In Proceedings ofACL-2005, Ann Arbor, MI.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofACL-2003, Sapporo, Japan.563
