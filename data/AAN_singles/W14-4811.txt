Proceedings of the 4th International Workshop on Computational Terminology, pages 86?93,Dublin, Ireland, August 23 2014.Evaluating Term Extraction Methods for InterpretersRan Xu, Serge SharoffCentre for Translation StudiesSchool of Modern Languages and CulturesUniversity of Leeds, UK,{sml3rx,s.sharoff}leeds.ac.ukAbstractThe study investigates term extraction methods using comparable corpora for interpreters.
Simul-taneous interpreting requires efficient use of highly specialised domain-specific terminology inthe working languages of an interpreter with limited time to prepare for new topics.
We evaluateseveral terminology extraction methods for Chinese and English using settings which replicatereal-life scenarios, concerning the task difficulty, the range of terms and the amount of mate-rials available, etc.
We also investigate interpreters?
perception on the usefulness of automatictermlists.
The results show the accuracy of the terminology extraction pipelines is not perfect,as their precision ranges from 27% on short texts to 83% on longer corpora for English, 24%to 31% on Chinese.
Nevertheless, the use of even small corpora for specialised topics greatlyfacilitates interpreters in their preparation.1 IntroductionThe study investigates term extraction methods using comparable corpora for interpreters.
Simultane-ous interpreting requires efficient use of highly specialised domain-specific terminology in the workinglanguages of the interpreter.
By necessity, interpreters often work in a wide range of domains and havelimited time to prepare for new topics.
To ensure the best possible simultaneous interpreting of spe-cialised conferences where a great number of domain-specific terms are used, interpreters need prepa-ration, usually under considerable time pressure.
They need to familiarise themselves with concepts,technical terms, and proper names in the interpreters?
working languages.However, there is little research into the use of modern terminology extraction tools and pipelines forthe task of simultaneous interpretation.
At the start of computer-assisted termbank development, Moser-Mercer (1992) overviewed the needs and workflow of practicing interpreters with respect to terminologyand offered some guidelines for developing term management tools specifically for the interpreters.
Thatstudy did review the functionalities of some termbanks and term management systems, yet there was nomention of corpus collection (a fairly new idea at the time) or automatic term extraction.A few previous studies mentioned the application of corpora as potential electronic tools for the in-terpreters.
Fantinuoli (2006) and Gorjanc (2009) discussed the functions of specific online crawlingtools and explored ways to extract specialised terminology from disposable web corpora for interpreters.Our work is most closely connected to Fantinuoli?s work on evaluation of termlists obtained from Web-derived corpora.
However, that study relied on a single method of corpus collection and term extraction,and did not include an investigation into integration of corpus research into practice of interpreter train-ing.R?tten (2003) suggested a conceptual software model for interpreters?
terminology management, inwhich termlists are expected to be extracted (semi-)automatically and then to be revised by their users, theinterpreters, who can concentrate on those terms which are relevant and important to remember.
Howeverthe study neither tested the functions of the term extraction tools nor further discussed interpreters?perception on the usefulness of the automatically lists in their preparation for interpreting tasks.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/86FR0 FR1 FR2 SM1 SM2En Zh En Zh En Zh En Zh En ZhTexts 1 1 9 9 81 86 9 12 74 84Size 774 1,641 42,006 30,174 206,197 129,350 20,533 40,545 166,499 116,235Table 1: Corpora used in this study (the size is in words for En, in characters for Zh)Based on R?tten model, this paper will further test the functions of several term extraction tools forEnglish and Chinese, and will discuss the interpreters?
perception on the usefulness of the automatically-generated lists in their preparation for interpreting tasks.In the remainder of this paper we will describe the pipelines for corpus collection and terminologyextraction (Section 2), present the results of their numeric evaluation (Section 3), and discuss options forfuture research, including the challenges for the term extraction pipelines in this setting (Section 4).2 Corpus collection and term extractionIn this section we describe pipelines for interpreters?
terminology preparation with the use of term ex-traction tools.
We compare several approaches to corpus compilation and processing for specialised textsas well as several pipelines for terminology extraction.2.1 Description of the procedureTwo specialized topics In this study MA student interpreters were invited to prepare for simultane-ous interpreting tasks on two specialised topics: fast reactors (FR) and Seabed minerals (SM).
Theywere provided with two monolingual specialised corpora in both English and Chinese for their advancepreparation on each of the topics (FR & SM).Three term extractors The students started with the FR topic, and were asked to manually generatetheir own lists from the provided corpora (FR1 En & Zh) before their simultaneous interpreting exerciseon the topic in both directions (English?Chinese and Chinese?English).
After their interpreting tasks,they were then asked to evaluate the relevance of two monolingual lists (En & Zh) which were automati-cally generated by one of the three tools (TTC TermSuite, Syllabs Tools and TeaBoat).
The purpose hereis to see which tool could extract more relevant terms for the needs of trainee interpreters.We collected and compared the annotation results from the students to select a single tool with com-paratively better performance.
We then invited the students to prepare for the other topic (SM) by usingautomatically-generated lists in the simultaneous interpreting preparation.2.2 Corpus compilationThere are two types of sources where comparable corpora are from:1.
Conference documents and relevant background documents provided by the conference organisers2.
Specialised corpora collected from the internet using WebBootCat (Baroni and Bernardini, 2004)Table 1 presents all the corpora we use in this study.
FR0/SM0 has been created from a single rele-vant document, representing the speech that the trainee interpreters were asked to interpret from in thisexperiment.
We also ran term extraction from this ?corpus?
since often a text of this length is the onlysource of information given to the interpreters in advance.
We tried to balance the terminological diffi-culty for both languages, even if this was not always possible.
After manual term selection, we foundthat FR0-Zh contains 147 terms per 591 seconds of delivery (15 terms per minute), FR0-En: 86 termsper 566 seconds (9 t/min), SM0-Zh: 157 terms per 604 seconds (16 t/min), SM0-En: 169 terms per 750seconds (14/min).11Counting the term density per unit of text is not straightforward, because of very different tokenisation rules in Chineseand English.87Seeds (En) Seeds (Zh)fast breeder reactor ???????
?fission ?
?decay heat ?
?uranium ?plutonium ?core damage ???
?Fukushima accident ???
?nuclear waste ??
?fuel cycle ???
?coolant ??
?Table 2: Parallel keyword seeds on Fast Reactors for FR2FR1 (En & Zh) and SM1 (En & Zh) are comparable corpora, which represent conference documentsand relevant background documents passed from the conference organisers, including speech outlines,research papers from experts and research institutes, reports from national and international authorities,as well as popular science articles, Wikipedia articles, specialised journal articles and interviews, etc.FR2 (En & Zh) and SM2 (En & Zh) are corpora collected by Web crawling using Bootcat(Baroni andBernardini, 2004).
For instance, to produce FR2 we started with a set of ten relevant keywords in Englishand Chinese as shown in Table 2, then used BootCat to retrieve online resources and generate two corpora(FR2 En & Zh).
All the keyword seeds are from the English speech-FR0 that the students were going tointerpret from, and are therefore considered very relevant and important terms.
The Chinese keywordsare the translations of the English ones.Preprocessing included webpage cleaning (Baroni et al., 2008), as well as basic linguistic processing.Lemmatisation and tagging for English was done using TreeTagger (Schmid, 1994), while for Chinesewe used ?Segmenter?, an automatic tokenisation tool (Liang et al., 2010) followed by TreeTagger forPOS tagging.
Lemmatisation is needed because the keywords in a glossary are expected to be in their dic-tionary form.
Lemmatisation also helps in reducing the nearly identical forms, e.g., sulphide deposit(s).However, lemmatisation also leads to imperfect terms, e.g., recognise type of marine resource, while theplurals and participles should be expected in a dictionary form (recognised type of marine resources).2.3 Automatic term extractionTTC TermSuite (Daille, 2012) is based on lexical patterns defined in terms of Part-of-Speech (POS)tags with frequency comparison against a reference corpus using specificity index (Ahmad et al., 1994),which extracts both single (SWT) and multi-word terms (MWT) outputs their lemmas, part of speech,lexical pattern, term variants (if any), etc.
The most important feature of the TTC TermSuite is the factthat term candidates can be output with their corresponding term variants.
Syllabs Tools (Blancafortet al., 2013) is a knowledge-poor tool, which is based on unsupervised detection of POS tags, followingthe procedure of (Clark, 2003), and on the Conditional Random Field framework for term extraction(Lafferty et al., 2001).
Teaboat (Sharoff, 2012) does term extraction by detecting noun phrases usingsimple POS patterns in IMS Corpus Workbench (Christ, 1994) and by applying log-likelihood statistics(Rayson and Garside, 2000) to rank terms by their relevance to the corpus in question against the Internetreference corpora for English and Chinese (Sharoff, 2006).3 Term extraction evaluationFantinuoli (2006) used five categories to find the level of specialisation and well-formedness of theautomatically-generated candidate termlist:1. specialised terms that were manually extracted by the terminologist (and are contained in the refer-ence term list);88FR-TTC FR-Teaboat FR-Syllabs SM-SyllabsEN ZH EN ZH EN ZH EN ZH0.541 0.500 0.166 0.435 0.181 0.662 0.117 0.221Table 3: Krippendorff?s ?
for different term lists2.
highly specialised terms that were not detected by the terminologist;3. non-specialised terms that are commonly used in the field of his study (medicine);4. general terms that are not specific to the medical field;5. ill-formed, incomplete expressions and fragments.Our annotation system extends Fantinuoli?s study because the purpose of annotation in this project isto give the interpreters possibility to extract relevant terms from all the candidate terms regardless of theirlevels of specialisation.
Our premise is that interpreters may need relevant terms, both highly specialisedand less specialised, in order to prepare themselves for a conference.
The annotators are the end usersof the list, i.e.
the trainee interpreters who participated in this research.
Since the interpreters are taskedwith translating speeches in the domain, they need themselves to decide what is likely to be relevantinstead of relying on the terminologists who describe the overall structure of the domain.
The followingis the five-category annotation system that we used in this research:R relevant terms (terms closely relevant to the topic), eg.
breed ratio, uranium-238, decay heat re-moval system;P potentially relevant terms (a category between ?I?
and ?R?
: they are terms; but annotators arenot sure whether they are closely relevant to the topic of their assignment), eg.
daughter nuclide,neutron poison, Western reactor;I irrelevant terms (terms not relevant to the topic), eg.
schematic diagram, milk crate;G general words (rather than terms), eg.
technical option, monthly donation, Google tag, discussionforum;IL ill-formed constructions (parts of terms or chunks of words), eg.
var, loss of cooling, separatesample container, first baseline data, control ranging.It only took several minutes to generate a termlist after uploading the designated corpus onto TTCTermSuite, Syllabs Tools and TeaBoat.
Each of them automatically generated corresponding monolin-gual termlists sorted by their term specificity scores.
For all the tools we set the threshold of obtaining500 terms (if possible), as a practical limit for all evaluation experiments.The trainee interpreters were asked to annotate the list by using the above annotation system.
Each ofthem reported that it took them about 60 minutes to annotate both lists (in EN& ZH) on each of the topics(FR & SM).
All the annotators were briefed about what counts as terms and the annotation system beforethey started their evaluation of term lists.
We aim for consistency, yet inter-annotator disagreement doesexist and there is a certain degree of subjectivity in annotation.
To measure the level of agreement weused Krippendorff?s ?
over the other measures, such as Fleiss?
?, because Krippendorff?s ?
offers anextension of such measures as Fleiss?
?
and Scott?s pi by introducing a distance metric for the pairwisedisagreements, thus making it possible to work with interval-scale ratings, e.g., considering disagreementbetween R and P as less severe than between R and I (Krippendorff, 2004).The values of Krippendorff?s ?
(see Table 3) are relatively low.
The most common cases of disagree-ment are between R and P (the boundary between them often depends on the amount of knowledge onthe side of the annotator), but also quite surprisingly between R and IL, when some annotators interpretill-formed sequences as a contribution to useful terms.With the disagreement taken into consideration, our evaluation on the number of relevant terms wasjudged by the agreement between at least two annotators among four to six annotators for the topic ofFR.
This established the gold standard lists reported in Table 4.The annotation results from Table 4 for English show that Syllabs generated more relevant terms thanthe other two tools from both FR0 and FR1.
Both Syllabs and Teaboat generated good numbers of89English:Tool FR0 FR1 FR2 SM1Syllabs 85/104(82%) 309/500 (62%) 400/500 (80%) 441/500 (88%)Teaboat 44/56(79%) 232/376 (62%) 413/499 (83%)TTC NA 136/500 (27%) 287/500 (57%)Chinese:Tool FR1 SM1Syllabs 156/500(31%) 130/500 (26%)Teaboat 141/450(31%)TTC 119/500(24%)Table 4: Number of relevant (R) terms against candidate termsrelevant terms from FR2.
In addition, Syllabs?
and TeaBoat?s English lists contain more specialisedterms in the domain of FR, such as defence-in-depth, once-through fuel cycle, suppression chamber ofthe containment, etc.
These specialised terms with relatively low frequency are not included in the TTC?slist.
The terms included in TTC?s list are more general terms, such as steam, energy, liquid, heat, leak,etc., which are likely to be already known by the trainee interpreters.The English termlists from all the tools contain a number of repetitions in the form of term variants,following Daille?s definition as ?an utterance which is semantically and conceptually related to an orig-inal term?
(Daille, 2005).
The automatically generated termlists contain the following types of termvariations, which are counted as individual term candidates scattered in the termlists:Morphological variation: bathymetry vs bathymetric (not different when translated into Chinese)Anaphoric variation: pollymetallic sulphide deposit vs depositPattern switching: meltdown of the core vs core meltdown; level of gamma radiation vs gamma radia-tion levelSynonymy in variation: deep sea mining vs deep seabed mining, seabed vs seafloor, ferromanganesecrust vs iron-manganese crustOne the one hand, these variations provide useful lexical information about the term, preparing theinterpreters for what is possible in their assignment; on the other hand, the term variants need to beexplicitly linked, which is possible only in the TTC TermSuite tool.The annotation results from Table 4 for Chinese show, both Syllabs?
and Teaboat?s lists offer obvi-ously less relevant terms from FR1 compared with the English lists.
When we further investigate thedistribution of the term classes in annotations in Table 5, Syllabs?
Chinese list on FR1 contains a largenumber of ill-formed constructions, including incomplete terms, eg.
??
?water reactor?, ?????
?Mile Island nuclear plant?
and longer chunks, eg.
????????
?, ?????????????????.
Teaboat?s list contains a number of general words, eg.
??
?development?,???production?
or??
?project?.
Both categories (G and IL) are frequent in the TTC?s Chinese list.On the basis of these results, we selected a single tool (Syllabs) with comparatively better performancein both languages to generate termlists on SM1 (En & Zh) and asked 12 annotators to select the relevantterms and learn the terms during their interpreting preparation.
Among the 500 candidate terms forEnglish, 441 terms were agreed as relevant by at least two annotators, 266 terms were agreed by fiveannotators.
Precision rates are 88.2% and 53.2% respectively.
On the other hand, only 130 terms wereagreed as relevant by two annotators from the 500 Chinese candidate terms.
The precision rate for theChinese list is 26%.
The results basically replicate the previous findings on FR1.The other pattern we observe from the current data is that the larger the corpus is, the more relevantterms the tools can generate.
If the corpus is of very limited size (eg.
FR0-en has only 774 words), theTTC TermSuite fails to generate any list for a ?corpus?
of only 774 words, while the Syllabs and Teaboattools produce shorter lists of 104 or 56 terms respectively.
The situation is similar to other studies whichused small (single-document) corpora, e.g., (Matsuo and Ishizuka, 2004).90FR1-en FR2-en FR1-zhSyllabs 500 500 500R 309 400 156P 90 53 73I 15 10 5G 56 16 46IL 30 21 220Teaboat 376 499 450R 232 413 141P 33 20 61I 19 5 7G 73 29 191IL 19 32 50TTC 500 500 500R 136 287 119P 48 1 32I 3 1 4G 310 205 209IL 3 6 136Table 5: Distribution of term annotation classes4 Conclusions and future workReliability of the three term extractors The results show the accuracy of the terminology extractionpipelines is not perfect, as its precision ranges from 27% on short texts to 83% on longer corpora forEnglish, 24% to 31% for Chinese.
Among the three term extractors (TTC TermSuite, Syllabs Toolsand Teaboat), Syllabs is more reliable in generating more relevant terms in English.
All the three toolsperform less satisfactory in generating relevant terms in Chinese.
We hypothesise that at least threefactors play an important role here:1.
Chinese is written without explicit word boundaries, while term extraction starts with already to-kenised texts.
Errors of the tokenisation process lead to difficulties in obtaining proper terms, e.g.,???
?primary loop?
becomes??
?once??
?road?, also???????
?and passive security?becomes??
?and not???
?active????
?security?, which reduces the chances of detecting??????
?passive security?
as a term.2.
Word ambiguity in Chinese is high.
This leads to POS tagging errors, for example, when nouns aretreated as verbs, and this breaks the POS patterns for term extraction, e.g.,???
?demonstrationreactor?
is treated as??/vn?/v.3.
Chinese exhibits more patterns than captured by the three term extraction tools we tested.
Forexample,????
?connect to the grid?
is potentially a useful term, which is correctly POS-taggedas??/v?
?/vn, but not captured by the patterns in all the tools.Two of the three causes of the results in Chinese concern text pre-processing.
.
Further investigationmight be helpful in finding out how the pre-processing steps affect the performance of the term extractorsand which terms are affected by each source of errors.Manual selection Vs Automatic extraction of terms For the interpreters, manually selecting termsfrom a single document of limited size (eg.
FR0-en=774 words) is possible.
However, when conferencedocuments amount to the size of FR1 (FR1-en=42,006 words), it took the trainee interpreters 9 hours onaverage to extract terms manually and to produce initial termlists, since they had to spend the majorityof their time on reading through fairly complex documents, copying the terms from the texts onto theirown termlists and searching for unfamiliar terms.91With the use of automatically-generated termlists on the same preparation task, students in the exper-iment group spent an average of 4 hours producing their initial bilingual termlists.
Therefore half of thetime spent on reading could be saved for the interpreters to get familiar with the concepts relevant to theterms and further activate the terms for their simultaneous interpreting tasks.Furthermore, if interpreters are given limited time for preparation, they would not be able to readthrough larger corpora of the size of FR2 (FR2-en=206,197 words) and to produce termlists from themmanually.
That is probably when such tools we discussed in this article may have obvious advantage overthe manual terms extraction by the interpreters.
Moreover, in other studies we also demonstrated that inaddition to providing an automatically-extracted termlist, it is also beneficial to link the terms to theiruses in the concordance lines of the corpus they have been extracted from.
This is expected to give theinterpreters an easy access to the context of the terms to see how they are used and get more backgroundknowledge about the domain.Feedback from students After doing annotation, the students offered their written feedback on thetermlists generated by the three term extractors.
They also commented on the usefulness of the Syllabs?lists for their interpreting preparation.They generally reported that the termlists provided many relevant terms on the two topics, and the useof the lists saved their precious preparation time.
Some of them found the lists ?unexpectedly accurateand complete?, and the presence of irrelevant words in the lists and the repetitions in the lists ?tolerable?
(even taking into account the 24% to 31% precision rate for Chinese).The students told us they used the lists as an important indicator for the content of the conferencedocuments and relevant background documents.
The lists helped them prioritise their preparation on themost relevant terms and concepts.
Most of them expressed the opinion that if they are given very limitedtime, they would prefer to use the automatically-generated lists for their preparation.
On the other hand,students reported that the termlists in Chinese offered much less relevant terms and contained quite anumber of ill-formed constructions compared with the lists in English; therefore they felt the lists inChinese were less useful and less reliable.Extraction of proper names Proper names (including names of organisations, names of places, namesand titles of people) are equally if not more important than terms for interpreters, yet many of them arenot included in the automatically-generated lists by the three term extractors (TTC, Syllabs and Teaboat).Therefore, named entity extraction tools in addition to term extraction are needed to generate morecomplete lists for interpreters?
use.
This would be further explored in our future research.File formats, plain text, encodings All the tools we tested can only process plain text (including UTF-8).
Nevertheless, all the meeting documents are normally in one of the word processing formats (.pdf,.doc, .xls or .ppt) other than .txt.
Interpreters need to take some time to convert all the files they obtainfrom their customers into plain text before they can possibly use any tool mentioned above.ReferencesAhmad, K., Davies, A., Fulford, H., and Rogers, M. (1994).
What is a term?
The semi-automatic extraction ofterms from text.
In Hornby, M. S., P?chhacker, F., and Kaindl, K., editors, Translation studies: an interdisci-pline, pages 267?278.
Amsterdam: John Benjamins Publishing Company.Baroni, M. and Bernardini, S. (2004).
Bootcat: Bootstrapping corpora and terms from the web.
In Proc.
ofLREC2004, Lisbon.Baroni, M., Chantree, F., Kilgarriff, A., and Sharoff, S. (2008).
Cleaneval: a competition for cleaning web pages.In Proc.
of the Sixth Language Resources and Evaluation Conference, LREC 2008, Marrakech.Blancafort, H., Bouvier, F., Daille, B., Heid, U., Ramm, A., et al.
(2013).
TTC Web platform: from corpus com-pilation to bilingual terminologies for MT and CAT tools.
In Proceedings, Conference?Futures in technologiesfor translation (TRALOGY II)?.Christ, O.
(1994).
A modular and flexible architecture for an integrated corpus query system.
In COMPLEX?94,Budapest.92Clark, A.
(2003).
Combining distributional and morphological information for part of speech induction.
In Pro-ceedings of EACL, pages 59?66.Daille, B.
(2005).
Variations and application-oriented terminology engineering.
Terminology, 11(1):181?197.Daille, B.
(2012).
Building bilingual terminologies from comparable corpora: The TTC TermSuite.
In 5th Work-shop on Building and Using Comparable Corpora at LREC 2012.Fantinuoli, C. (2006).
Specialized corpora from the web and term extraction for simultaneous interpreters.
InBaroni, M. and Bernardini, S., editors, WaCky!
Working papers on the Web as Corpus, pages 173?190.
Gedit,Bologna.
http://wackybook.sslmit.unibo.it.Gorjanc, V. (2009).
Terminology resources and terminological data management for medical interpreters.
InAndres, D. and P?llabauer, S., editors, Sp?rst Du, wie der Bauch rauf-runter?
Fachdolmetschen im Gesund-heitsbereich.
http://www.uni-graz.at/06gorjanc.pdf.Krippendorff, K. (2004).
Reliability in content analysis: Some common misconceptions and recommendations.Human Communication Research, 30(3).Lafferty, J., McCallum, A., and Pereira, F. C. (2001).
Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings of ICML-01.Liang, M., Li, W., and Xu, J.
(2010).
Using corpora: a practical coursebook.
Foreign Langue Teaching andResearch Press, Beijing.Matsuo, Y. and Ishizuka, M. (2004).
Keyword extraction from a single document using word co-occurrencestatistical information.
International Journal on Artificial Intelligence Tools, 13(01):157?169.Moser-Mercer, B.
(1992).
Banking on terminology conference interpreters in the electronic age.
Meta: Transla-tors?
Journal, 37(3):507?522.Rayson, P. and Garside, R. (2000).
Comparing corpora using frequency profiling.
In Proc.
of the ComparingCorpora Workshop at ACL 2000, pages 1?6, Hong Kong.R?tten, A.
(2003).
Computer-based information management for conference interpreters-or how will i make mycomputer act like an infallible information butler?
In Proc.
Translating and the computer, pages 14?14.Schmid, H. (1994).
Probabilistic part-of-speech tagging using decision trees.
In International Conference on NewMethods in Language Processing, Manchester.Sharoff, S. (2006).
Open-source corpora: using the net to fish for linguistic data.
International Journal of CorpusLinguistics, 11(4):435?462.Sharoff, S. (2012).
Beyond Translation Memories: Finding similar documents in comparable corpora.
In Proc.Translating and the Computer Conference, London.93
