Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 85?88,Beijing, August 2010An Efficient, Generic Approach to Extracting Multi-Word Expressionsfrom Dependency TreesScott Martens and Vincent VandeghinsteCentrum voor Computerlingu?
?stiekKatholieke Universiteit Leuvenscott@ccl.kuleuven.be & vincent@ccl.kuleuven.beAbstractThe Varro toolkit offers an intuitive mech-anism for extracting syntactically mo-tivated multi-word expressions (MWEs)from dependency treebanks by looking forrecurring connected subtrees instead ofsubsequences in strings.
This approachcan find MWEs that are in varying ordersand have words inserted into their compo-nents.
This paper also proposes descrip-tion length gain as a statistical correlationmeasure well-suited to tree structures.1 IntroductionAutomatic MWE extraction techniques operateby using either statistical correlation tests on thedistributions of words in corpora, syntactic pat-tern matching techniques, or by using hypothe-ses about the semantic non-compositionality ofMWEs.
This paper proposes a purely statisticaltechnique for MWE extraction that incorporatessyntactic considerations by operating entirely ondependency treebanks.
On the whole, dependencytrees have one node for each word in the sentence,although most dependency schemes vary from thisto some extent in practice.
See Figure 1 for anexample dependency tree produced automaticallyby the Stanford parser from the English languagedata in the Europarl corpus.
(Marneffe, 2008;Koehn, 2005)Identifying MWEs with subtrees in dependencytrees is not a new idea.
It is close to the formal def-inition offered in Mel?c?uk (1998), and is appliedcomputationally in Debusmann (2004) However,using dependency treebanks to automatically ex-tract MWEs is fairly new and few MWE extrac-Figure 1.
A dependency tree of the sentence?The Minutes of yesterday?s sitting have been dis-tributed.
?tion projects to date take advantage of dependencyinformation directly.
There are a number of rea-sons why this is the case:?
String-based algorithms are not readily ap-plicable to trees.?
Tree structures yield a potentially combina-torial number of candidate MWEs, a prob-lem shared with methods that look for stringswith gaps.?
Statistical techniques used in MWE extrac-tion, like pointwise mutual information, aretwo-variable tests that are not easy to applyto larger sets of words.The tool and statistical procedures used in thisresearch are not language dependent and can op-erate on MWE of any size, producing depen-85(a) ?The Minutes (...)have been distributed?
(b) ?(...)
Minutes of(...) distributed.
?Figure 2.
Two induced subtrees of the dependencytree in Figure 1.
Note that both correspond to dis-continuous phrases in the original sentence.dency pairs, short phrases of any syntactic cate-gory, lengthy formulas and idioms.
There are nounderlying linguistic assumptions in this method-ology except that a MWE must consist of wordsthat have a fixed set of dependency links in atreebank.
Even word order and distance betweenwords is not directly assumed to be significant.The input, however, requires substantial linguis-tic pre-processing ?
particularly, the identificationof at least some of the dependency relations inthe corpora used.
Retrieving MWEs that containabstract categories, like information about the ar-guments of verbs or part-of-speech informationfor unincluded elements, requires using treebanksthat contain that information, rather than purelylexical dependency trees.2 Varro Toolkit for Frequent SubtreeDiscoveryThe Varro toolkit is an open-source application forefficiently extracting frequent closed unorderedinduced subtrees from treebanks with labelednodes and edges.
It is publicly available under anopen source license.1 For a fuller description ofVarro, including the algorithm and data structuresused and a formal definition of frequent closed un-ordered induced subtrees, see Martens (2010).Given some tree like the one in Figure 1, an in-duced subtree is a connected subset of its nodesand the edges that connect them, as shown inFigure 2.
Subtrees do not necessarily represent1http://varro.sourceforge.net/fixed sequences of words in the original text,they include syntactically motivated discontinu-ous phrases.
This dramatically reduces the num-ber of candidate discontinuous MWEs when com-pared to string methods.
An unordered inducedsubtree is a subtree where the words may appearwith different word orders, but the subtree is stillidentified as the same if the dependency structureis the same.
A frequent closed subtree is a sub-tree of a treebank that appears more than somefixed number of times and where there is no sub-tree that contains it and appears the same numberof times.
Finding only closed subtrees reduces thecombinatorial explosion of possible subtrees, andensures that each candidate MWE includes all thewords the that co-occur with it every time it ap-pears.3 Preprocessing and Extracting SubtreesThe English language portion of the EuroparlCorpus, version 3 was parsed using the Stanfordparser, which produces both a constituentcy parseand a dependency tree as its output.2 The depen-dency information for each sentence was trans-formed into the XML input format used by Varro.The result is a treebank of 1.4 million individualparse trees, each representing a sentence, and a to-tal of 36 million nodes.In order to test the suitability of Varro for largetreebanks and intensive extractions, all recurringclosed subtrees that appear at least twice were ex-tracted.
This took a total of 129,312.27 seconds(just over 34 hours), producing 9,976,355 frequentsubtrees, of which 9,909,269 contain more thanone word and are therefore candidate MWEs.A fragment of the Varro output can be seen inFigure 3.
The nodes of the subtrees returned arenot in a grammatical surface order.
However, theoriginal source order can be recovered by usingthe locations where each subtree appears to findthe order in the treebank.
Doing so for the treein Figure 3 shows what kinds of MWEs this ap-proach can extract from treebanks.
The under-lined words in the following sentences are theones included in the subtree in Figure 3:2This portion of the work was done by our colleaguesJo?rg Tiedemann and Gideon Kotze?
at RU Groningen.86Figure 3.
An example of a found subtree and can-didate MWE.
This subtree appears in 2581 uniquelocations in the treebank, and only the locationsof the first few places in the treebank where it ap-pears are reproduced here, but all 2581 are in theVarro output data.The vote will take place tomorrow at 9 a.m.The vote will take place today at noon.The vote will take place tomorrow, Wednesdayat 11:30 a.m.4 Statistical Methods for EvaluatingSubtrees as MWEsTo evaluate the quality of subtrees as MWEs,we propose to use a simplified form of de-scription length gain (DLG), a metric derivedfrom algorithmic information theory and Mini-mum Description Length methods (MDL).
(Ris-sanen, 1978; Gru?nwald, 2005) Given a quantity ofdata of any kind that can be stored as a digital in-formation in a computer, and some process whichtransforms the data in a way that can be reversed,DLG is the measure of how the space required tostore that data changes when it is transformed.To calculate DLG, one must first decide how toencode the trees in the treebank.
It is not neces-sary to actually encode the treebank in any par-ticular format.
All that is necessary is to be ableto calculate how many bits the treebank would re-quire to encode it.Space prevents the full description of the en-coding mechanism used or the way DLG is cal-culated.
The encoding mechanism is largely thesame as the one described in Luccio et al (2001)Converting the trees to strings makes it possible tocalculate the encoding size by calculating the en-tropy of the treebank in that encoding using clas-sical information theoric methods.In effect, the procedure for calculating DLG isto calculate the entropy of the whole treebank,given the encoding method chosen, and then torecalculate its entropy given some subtree whichis removed from the treebank and replaced with asymbol that acts as an abbreviation.
That subtreeis then be added back to the treebank once as partof a look-up table.
These methods are largely thesame as those used by common data compressionsoftware.DLG is the difference between these two en-tropy measures.3Because of the sensitivity of DLG to low fre-quencies, it can be viewed as a kind of non-parametric significance test.
Any frequent struc-ture that cannot be used to compress the treebankhas a negative DLG and is not frequent enough orlarge enough to be considered significant.Varro reports several statistics related to DLGfor each extracted subtree, as shown in Figure 3:?
Unique appearances (reported by the root-Count attribute) is the number of times theextracted subtree appears with a differentroot node.?
Entropy is the entropy of the extracted sub-tree, given the encoding scheme that Varrouses to calculate DLG.?
Algorithmic mutual information (AMI) (re-ported with the mi attribute) is the DLG ofthe extracted subtree divided by its numberof unique appearances in the treebank.?
Compression is the AMI divided by the en-tropy.AMI is comparable to pointwise mutual infor-mation (PMI) in that both are measures of redun-dant bits, while compression is comparable to nor-malized mutual information metrics.3This is a very simplified picture of MDL and DLG met-rics.875 Results and ConclusionsWe used the metrics described above to sort thenearly 10 million frequent subtrees of the parsedEnglish Europarl corpus.
We found that:?
Compression and AMI metrics strongly fa-vor very large subtrees that represent highlyformulaic language.?
DLG alone finds smaller, high frequency ex-pressions more like MWEs favoured by ter-minologists and collocation analysis.For example, the highest DLG subtree matchesthe phrase ?the European Union?.
This is notunexpected given the source of the data and con-stitutes a very positive result.
Among the nearly10 million candidate MWEs extracted, it alsoplaces near the top discontinuous phrases like?...
am speaking ... in my ... capacity as ...?.Using both compression ratio and AMI, thesame subtree appears first.
It is present 26 timesin the treebank, with a compression score of 0.894and an AMI of 386.92 bits.
It corresponds to theunderlined words in the sentence below:The next item is the recommendation forsecond reading (A4-0245/99), on behalf ofthe Committee on Transport and Tourism, onthe common position adopted by the Council(13651/3/98 - C4-0037/99-96/0182 (COD) witha view to adopting a Council Directive on thecharging of heavy goods vehicles for the use ofcertain infrastructures.This is precisely the kind of formulaic speech,with various gaps to fill in, which is of great inter-est for sub-sentential translation memory systems.
(Gotti et al, 2005; Vandeghinste and Martens,2010)We believe this kind of strategy can substan-tially enhance MWE extraction techniques.
It in-tegrates syntax into MWE extraction in an intu-itive way.
Furthermore, description length gainoffers a unified statistical account of an MWE asa linguistically motivated structure that can com-press relevant corpus data.
It is similar to the typesof statistical tests already used, but is also non-parametric and suitable for the study of arbitraryMWEs, not just two-word MWEs or phrases thatoccur without gaps.6 AcknowledgementsThis research is supported by the AMASS++Project,4 directly funded by the Institute for thePromotion of Innovation by Science and Technol-ogy in Flanders (IWT) (SBO IWT 060051) and bythe PaCo-MT project (STE-07007).ReferencesDebusmann, Ralph.
2004.
Multiword expressions asdependency subgraphs.
Proceedings of the 2004ACL Workshop on Multiword Expressions, pp.
56-63.Gotti, Fabrizio, Philippe Langlais, Eliott Macklovitch,Didier Bourigault, Benoit Robichaud and ClaudeCoulombe.
2005.
3GTM: A third-generation trans-lation memory.
Proceedings of the 3rd Computa-tional Linguistics in the North-East Workshop, pp.8?15.Gru?nwald, Peter.
2005.
A tutorial introduction tothe minimum description length principle.
In: Ad-vances in Minimum Description Length: Theoryand Applications, (Peter Gru?nwald, In Jae Myung,Mark Pitt, eds.
), MIT Press, pp.
23?81.Koehn, Philipp.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of the10th MT Summit, pp.
79?86.Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumontand Linda Pagli.
2001.
Exact Rooted SubtreeMatching in Sublinear Time.
Universita` di PisaTechnical Report TR-01-14.de Marneffe, Marie-Catherine and Christopher D.Manning.
2008.
The Stanford typed dependenciesrepresentation.
Proceedings of the 2008 CoLingWorkshop on Cross-framework and Cross-domainParser Evaluation, pp.
1?8.Martens, Scott.
2010.
Varro: An Algorithm andToolkit for Regular Structure Discovery in Tree-banks.
Proceedings of the 2010 Int?l Conf.
on Com-putational Linguistics (CoLing), in press.Mel?c?uk, Igor.
1998.
Collocations and Lexical Func-tions.
In: Phraseology.
Theory, Analysis, and Ap-plications, (Anthony Cowie ed.
), pp.
23?53.Rissanen, Jorma.
1978.
Modeling by shortest datadescription.
Automatica, vol.
14, pp.
465?471.Vandeghinste, Vincent and Scott Martens.
2010.Bottom-up transfer in Example-based MachineTranslation.
Proceedings of the 2010 Conf.
of theEuropean Association for Machine Translation, inpress.4http://www.cs.kuleuven.be/?liir/projects/amass/88
