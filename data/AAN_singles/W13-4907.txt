Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 63?70,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsEffective Morphological Feature Selectionwith MaltOptimizer at the SPMRL 2013 Shared TaskMiguel BallesterosNatural Language Processing GroupPompeu Fabra University.Barcelona, Spainmiguel.ballesteros@upf.eduAbstractThe inclusion of morphological features pro-vides very useful information that helps to en-hance the results when parsing morphologi-cally rich languages.
MaltOptimizer is a tool,that given a data set, searches for the opti-mal parameters, parsing algorithm and opti-mal feature set achieving the best results thatit can find for parsers trained with MaltParser.In this paper, we present an extension of Mal-tOptimizer that explores, one by one and incombination, the features that are geared to-wards morphology.
From our experimentsin the context of the Shared Task on ParsingMorphologically Rich Languages, we extractan in-depth study that shows which featuresare actually useful for transition-based pars-ing and we provide competitive results, in afast and simple way.1 IntroductionSince the CoNLL Shared Tasks on Syntactic Depen-dency parsing (Buchholz and Marsi, 2006; Nivre etal., 2007), the number of treebanks and new pars-ing methods have considerably increased.
Thanks tothat, it has been observed that parsing morphologi-cally rich languages (henceforth, MRLs) is a chal-lenge because these languages include multiple lev-els of information that are difficult to classify and,therefore, to parse.
This is why there has been recentresearch in this direction, with for instance a SpecialIssue in Computational Linguistics (Tsarfaty et al2012b).MaltOptimizer (Ballesteros and Nivre, 2012b;Ballesteros and Nivre, 2012a) is a system that is ca-pable of providing optimal settings for training mod-els with MaltParser (Nivre et al 2006a), a freelyavailable transition-based parser generator.
MaltOp-timizer, among other things, performs an in-depthfeature selection, selecting the attributes that helpto achieve better parsing results.
In this paper ?and in this participation in the Shared Task on Pars-ing Morphologically Rich Languages (Seddah et al2013) ?
we present an extension of MaltOptimizerthat performs a deeper search over the morpholog-ical features that are somewhat one of the keys toparsing MRLs.
Instead of lumping all morphosyn-tactic features together, we define a different field foreach individual feature (case, number, gender, etc.
).Hence, we are able to extract a study that showswhich features are actually useful for parsing MRLswith MaltParser.The new SPMRL-MaltOptimizer imple-mentation is available for download athttp://nil.fdi.ucm.es/maltoptimizer/spmrl.html.It is worth noting that it can be applied to anytreebank in CoNLL data format.1The rest of the paper is organized as follows.
Sec-tion 2 describes MaltOptimizer.
Section 3 showshow we modified MaltOptimizer to make it able toperform a more complete morphological feature se-lection.
Section 4 describes the experiments that wecarried out with the data sets of the Shared Task onParsing Morphologically Rich Languages.
Section5 reports the results of the experiments and the con-clusions that we can extract.
Section 6 discusses re-lated work on MaltOptimizer and parsing morpho-logically rich languages.
And finally, Section 7 con-1http://ilk.uvt.nl/conll/#dataformat63cludes.2 MaltOptimizerMaltOptimizer is a system written in Java that im-plements a full optimization procedure for Malt-Parser based on the experience acquired from pre-vious experiments (Hall et al 2007; Nivre andHall, 2010).
MaltOptimizer attempts to find the bestmodel that it can find, but it does not guarantee thatthe outcome is the best model possible because ofthe difficulty of exploring all the possibilities that areprovided by the parameters, parsing algorithms anddifferent feature windows.
The optimization proce-dure is divided in 3 different phases, as follows:1.
Data analysis and initial optimization.2.
Parsing algorithm selection.3.
Feature selection and LIBLINEAR optimiza-tion.MaltOptimizer divides the treebank into a train-ing set and a held-out test set for evaluation.
In thefirst phase, MaltOptimizer makes an analysis of thetreebank in order to set up the rest of the optimiza-tion, and it attempts the optimization with some gen-eral parameters, such as the way of handling coveredroots.2 After that, it tests the parsing algorithms thatare available in MaltParser by selecting the one thatprovides best results in default settings.
In the thirdphase, it explores a wide range of features that arebased on previous parsing steps and/or the informa-tion annotated in the treebanks.
Finally, it also ex-plores the single hyper-parameter (c) of the LIBLIN-EAR classifier.In the next Section, we present how we updatedMaltOptimizer for our participation in the SharedTask of parsing MRLs.3 Morphological Feature ExplorationThe CoNLL data format contains several columnsof information that help to perform the dependencyparsing of a sentence.
One of the columns is theFEATS column that normally contains a set of mor-phological features, which is normally of the formata=x|b=y|c=z.
At the time of writing, the available2A covered root is a root node covered by a dependency arc.version of MaltOptimizer explores the features in-cluded in this column as a single feature, by lumpingall morphosyntactic features in the MaltParser clas-sifier, and by splitting the information but includingall of them at the same time without making any dis-tinctions.
This is what MaltParser allows by usingthe standard CoNLL format, which contains the fol-lowing information per column.1.
ID: Identifier.2.
FORM: Word form.3.
LEMMA: Lemma or stemmed version of theword.4.
CPOSTAG: Coarse-grained part-of-speechtag.5.
POSTAG: Fine-grained part-of-speech tag.6.
FEATS: Morphosyntactic features (e.g., case,number, tense, etc.).
It is normally of the for-mat a=x|b=y|c=z.7.
HEAD: Head node.8.
DEPREL: Dependency relation to head.9.
PHEAD: Projective head node.10.
PDEPREL: Projective dependency relation tohead.However, MaltParser also provides the option ofparsing new data formats that are derived from theoriginal CoNLL format.
Therefore, there is the pos-sibility to add new columns that may contain use-ful information for parsing.
The new MaltOptimizerimplementation automatically generates a new dataformat and a new data set.
It creates new columnsthat only contain the information of a single featurewhich is included in the FEATS column.Figure 1 shows two versions of a sentence anno-tated in the French treebank from the Shared Task.The one shown above is in the standard CoNLL for-mat, and the one shown below is the extended formatgenerated by MaltOptimizer in which the FEATScolumn has been divided in 10 different columns.641 En en P P mwehead=ADV+|pred=y 4 mod2 tout tout D DET g=m|n=s|s=ind|pred=y 1 dep_cpd3 cas cas N NC g=m|s=c|pred=y 1 dep_cpd4 est ?tre V V m=ind|n=s|p=3|t=pst 0 root5 -il il CL CLS g=m|n=s|p=3|s=suj 4 suj6 plus plus ADV ADV _ 7 mod7 nuanc nuanc A ADJ g=m|n=s|s=qual 4 ats8 .
.
PONCT PONCT s=s 4 ponct1 En en P P _ _ _ _ _ _ ADV+ y _ _ 4 mod2 tout tout D DET ind m s _ _ _ _ y _ _ 1 dep_cpd3 cas cas N NC c m _ _ _ _ _ y _ _ 1 dep_cpd4 est ?tre V V _ _ s 3 ind pst _ _ _ _ 0 root5 -il il CL CLS suj m s 3 _ _ _ _ _ _ 4 suj6 plus plus ADV ADV _ _ _ _ _ _ _ _ _ _ 7 mod7 nuanc nuanc A ADJ qual m s _ _ _ _ _ _ _ 4 ats8 .
.
PONCT PONCT s _ _ _ _ _ _ _ _ _ 4 ponctFigure 1: A sentence from the French treebank in the standard (above) and complex (below) formats.
The projectivecolumns have been removed for simplicity.4 ExperimentsWith the intention of both assessing the usefulnessof the new MaltOptimizer implementation and test-ing which features are useful for each targeted lan-guage, we carried out a series of experiments overthe data sets from the Shared Task on Parsing MRLs(Seddah et al 2013).
We run the new MaltOpti-mizer implementation for all the data sets providedby the Shared Task organizers and we run Malt-Parser with the model suggested.
Therefore, we had36 different runs, 4 for each language (gold and pre-dicted scenarios with 5k treebanks, and gold andpredicted scenarios with full treebanks).In order to have a comparable set of results, weperformed all the optimization processes with thesmaller versions of the treebanks (5k) and both op-timization and training steps with both the smalland larger version for all languages.
Each MaltOp-timizer run took approximately 3-4 hours for opti-mization (the running time also depends on the sizeof the set of morphological features, or other param-eters, such as the number of dependency relations)and it takes around 20 extra minutes to get the finalmodel with MaltParser.
These estimates are givenwith an Intel Xeon server with 8 cores, 2.8GHz anda heap space of, at least, 8GB.5 Results and DiscussionTable 1 shows the results for gold-standard inputwhile Table 2 shows the results for the provided pre-dicted inputs for the best model that the new Mal-tOptimizer implementation can find (Dev-5k, Dev,Test-5k and Test) and a baseline, which is Malt-Parser in default settings (Malt-5k and Malt) on thetest sets.
The first conclusion to draw is that the dif-ference between gold and predicted inputs is nor-mally of 2 points, however for some languages suchas French the drop reaches 6 points.
It is also ev-idenced that, as shown by Ballesteros and Nivre(2012a), some languages benefit more from the fea-ture selection phase, while others achieve higher im-provements by selecting a different parsing algo-rithm.In general terms, almost all languages bene-fit from having an accurate stemmed version ofthe word in the LEMMA column, providing verysubstantial improvements when accurately selectingthis feature.
Another key feature, for almost all lan-guages, is the grammatical CASE that definitely en-hances the performance; we can therefore concludethat it is essential for MRLs.
Both aspects evidencethe lexical challenge of parsing MRLs without usingthis information.There is a positive average difference comparingwith the MaltParser baseline of 4.0 points trainingover the full treebanks and predicted scenario and5.6 points training over the full treebanks and goldscenario.
It is therefore evident how useful MaltOp-timizer is when it can perform an in-depth morpho-logical feature exploration.
In the following subsec-tions we explain the results for each targeted lan-guage, giving special emphasis to the ones that turnout to be more meaningful.5.1 ArabicFor Arabic, we used the shared task Arabic dataset, originally provided by the LDC (Maamouri et65Language Default Phase 1 Phase 2 Phase 3 Diff Dev-5k Dev Malt-5k Malt Test-5k TestArabic 83.48 83.49 83.49 87.95 4.47 85.98 87.60 80.36 82.28 85.30 87.03Basque 67.05 67.33 67.45 79.89 13.30 80.35 81.65 67.13 69.19 81.40 82.07French 77.96 77.96 78.27 85.24 7.28 85.19 86.30 78.16 79.86 84.93 85.71German 79.90 81.09 84.85 87.70 7.80 87.32 90.40 76.64 79.98 83.59 86.96Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49Swedish 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67Table 1: Labeled attachment score per phase compared to default settings for all training sets from the Shared Taskon PMRLs in the gold scenario on the held-out test set for optimization.
The first columns shows results per phase(the procedure of each phase is briefly described in Section 2) on the held-out sets for evaluation.
The Dev-5k andDev columns report labeled attachment score on the development sets.
The columns Malt and Malt-5k report resultsof MaltParser in default settings on the test sets.
And the columns, Test-5k and Test report results for the best modelfound by SPMRL-MaltOptimizer on the test sets.Language Default Phase 1 Phase 2 Phase 3 Diff Dev-5k Dev Malt-5k Malt Test-5k TestArabic 83.20 83.21 83.21 85.68 2.48 80.35 82.28 78.30 80.36 79.64 81.90Basque 68.80 69.33 69.89 77.24 8.44 78.12 79.46 68.12 70.11 77.59 78.58French 77.43 77.43 77.63 79.42 1.99 77.65 79.33 76.54 77.98 77.56 79.00German 78.69 79.87 82.58 83.97 5.28 83.39 86.63 74.81 77.81 79.22 82.75Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49Swedish 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82Table 2: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task onPMRLs in the predicted scenario on the held-out test set for optimization.
The columns of this table report the resultsin the same way as Table 1 but using predicted inputs.al., 2004), specifically its SPMRL 2013 dependencyinstance, derived from the Columbia Catib Tree-bank (Habash and Roth, 2009; Habash et al 2009),extended according to the SPMRL 2013 extensionscheme (Seddah et al 2013).For the gold input, the most useful feature is, byfar, DASHTAG3 with an improvement of 2 points.CASE is also very useful, as it is for most of thelanguages, with 0.67 points.
Moreover, SUBCAT(0.159) and CAT (0.129) provide improvements aswell.In the pred scenario, there is no DASHTAG, andthis allows other features to rise, for instance, CASE(0.66), CPOSTAG (0.12), GENDER (0.08), SUB-CAT (0.07) and CAT (0.06) provide improvements.Finally it is worth noting that the TED accuracy3DASHTAG comes from the original constituent data, whena DASHTAG was present in a head node label, this feature waskept in the Catib corpus.
(Tsarfaty et al 2011) for the lattices is 0.8674 withthe full treebanks and 0.8563 with 5k treebanks,which overcomes the baseline in more than 0.06points, this shows that MaltOptimizer is also usefulunder TED evaluation constraints.5.2 BasqueThe improvement provided by the feature selectionfor Basque (Aduriz et al 2003) is really high.
Itachieves almost 13 points improvement with thegold input and around 8 points with the predictedinput.
The results in the gold scenario are actuallya record if we also consider the experiments per-formed over the treebanks of the CoNLL SharedTasks (Ballesteros and Nivre, 2012a).
One of thereasons is the treatment of covered roots that is opti-mized during the first phase of optimization.
Thiscorpus has multiple root labels, ROOT being themost common one and the one selected by MaltOp-66timizer as default.For the gold input, the CPOSTAG and LEMMAcolumns turn out to be very useful, providing animprovement of 2.5 points and slightly less than 1point respectively, MaltOptimizer selects them allover the more central tokens over the stack and thebuffer.
The Basque treebank contains a very bigset of possible features in the FEATS column, how-ever only some of them provide significant improve-ments, which evidences the usefulness of selectingthem one by one.
The most useful feature with ahuge difference is KASE (or CASE) that provides5.9 points by itself.
MaltOptimizer fills out all theavailable positions of the stack and the buffer withthis feature.
Another useful feature is ERL [type ofsubordinated sentence], providing almost 0.8 points.Moreover, NUMBER (0.3), NORK2 (0.15), ASP[aspect] (0.09), NOR1 (0.08), and NMG (0.06) pro-vide slighter, but significant, improvements as well.4Surprisingly, the predicted input provides bet-ter results in the first 2 phases, which means thatfor some reason MaltParser is able to parse betterby using just the predicted POS column, however,the improvement achieved by MaltOptimizer dur-ing Phase 3 are (just) a bit more than 7 points.
Inthis case, the CPOSTAG column is less useful, pro-viding only 0.13 points, while the LEMMA (1.2) isstill very useful.
CASE provides 4.5 points, whileNUM (0.17), ASP (0.13) and ADM (0.11) provideimprovements as well.5.3 FrenchFor French (Abeille?
et al 2003) there is a huge dif-ference between the results with gold input and theresults with predicted input.
With gold input, thefeature selection provides a bit less than 8 pointswhile there is just an improvement of around 2points with predicted input.
In this case, the lackof quality in the predicted features is evident.
It isalso interesting that the lexical column, FORM, pro-vides a quite substantial improvement when Mal-tOptimizer attempts to modify it, which is some-thing that does not happen with the rest of lan-guages.For the gold input, apart from LEMMA that pro-vides around 0.7 points, the most useful feature is4NORK2, NOR1 and NMG are auxiliaries case markers.MWEHEAD [head of a multi word expression, ifexists] that does not exist in the predicted scenario.MWEHEAD provides more than 4 points; this factinvites us to think that a predicted version of thisfeature would be very useful for French, if possi-ble.
PRED [automatically predicted] (0.8), G [gen-der] (0.6), N [number] (0.2) and S [subcat] (0.14)are also useful.In the predicted scenario, the CPOSTAG columnprovides some improvements (around 0.1) while theLEMMA is less useful than the one in the gold sce-nario (0.2).
The morphological features that are use-ful are S [subcat] (0.3) and G [gender] (0.3).5.4 GermanFor German (Brants et al 2002) the results are moreor less in the average.
For the gold input, LEMMAis the best feature providing around 0.8 points; fromthe morphological features the most useful one is, asexpected, CASE with 0.58 points.
GENDER (0.16)and NUMBER (0.16) are also useful.In the predicted scenario, CASE is again very use-ful (0.67).
Other features, such as, NUMBER (0.10)and PERSON (0.10) provide improvements, but aswe can observe a little bit less than the improve-ments provided in the gold scenario.5.5 HebrewFor the Hebrew (Sima?an et al 2001; Tsarfaty,2013) treebank, unfortunately we did not see a lotof improvements by adding the morphological fea-tures.
For the gold input, only CPOSTAG (0.08)shows some improvements, while the predicted sce-nario shows improvements for NUM (0.08) and PER(0.08).
It is worth noting that the TED accuracy(Tsarfaty et al 2011) for the lattices is 0.8305 whichis ranked second.This outcome is different from the one obtainedby Goldberg and Elhadad (2010), but it is also truethat perhaps by selecting a different parsing algo-rithm it may turn out different, because two parsersmay need different features, as shown by Zhang andNivre (2012).
This is why, it would be very interest-ing to perform new experiments with MaltOptimizerby testing different parsing algorithms included inMaltParser with the Hebrew treebank.675.6 HungarianThe Hungarian (Vincze et al 2010) results arealso very consistent.
During the feature selectionphase, MaltOptimizer achieves an improvement of10 points by the inclusion of morphological features.This also happens in the initial experiments per-formed with MaltOptimizer (Ballesteros and Nivre,2012a), by using the Hungarian treebank of theCoNLL 2007 Shared Task.
The current Hungariantreebank presents covered roots and multiple root la-bels and this is why we also get substantial improve-ments during Phase 1.For the gold input, as expected the LEMMA col-umn is very useful, providing more than 1.4 points,while MaltOptimizer selects it all over the availablefeature windows.
The best morphological featureis again CASE providing an improvement of 5.7points just by itself, in a similar way as in the ex-periments with Basque.
In this case, the SUBPOS[grammatical subcategory] feature that is includedin the FEATS column is also very useful, provid-ing around 1.2 points.
Other features that are usefulare NUMP [number of the head] (0.2), NUM [num-ber of the current token] (0.16), DEF [definiteness](0.11) and DEG [degree] (0.09).In the predicted scenario, we can observe a sim-ilar behavior for all features.
MOOD provides 0.4points while it does not provide improvements in thegold scenario.
The results of the SUBPOS featureare a bit lower in this case (0.5 points), which evi-dences the quality lost by using predicted inputs.5.7 KoreanAs Korean (Choi, 2013) is the language in whichour submission provided the best results comparingto other submissions, it is interesting to dedicate asection by showing its results.
For the 5k input, ourmodel provides the best results of the Shared Task,while the results of the model trained over the fulltreebank qualified the second.For the gold input, the most useful feature isCPOSTAG providing around 0.6 points.
Lookinginto the morphological features, CASE, as usual, isthe best feature with 0.24 points, AUX-Type (0.11),FNOUN-Type (0.08) are also useful.In the predicted scenario, MaltOptimizer per-forms similarly, having CPOSTAG (0.35) and CASE(0.32) as most useful features.
ADJ-Type (0.11) andPUNCT-Type (0.06) are also useful.
The results ofthe features are a bit lower with the predicted input,with the exception of CASE which is better.5.8 PolishPolish (S?widzin?ski and Wolin?ski, 2010) is one of thetwo languages (with Swedish) in which our modelperforms with the worst results.In the gold scenario only the LEMMA (0.76)shows some substantial improvements during theoptimization process; unfortunately, the morpholog-ical features that are extracted when MaltOptimizergenerates the new complex data format did not fire.For the predicted input, LEMMA (0.66) is againthe most useful feature, but as happened in the goldscenario, the rest of the features did not fire duringthe feature selection.5.9 SwedishAs happened with Polish, the results for Swedish(Nivre et al 2006b) are not as good as we could ex-pect; however we believe that the information shownin this paper is useful because MaltOptimizer detectswhich features are able to outperform the best modelfound so far and the model trained with MaltParserin default settings by a bit less than 2 points in thepredicted scenario and more than 2 points in the goldscenario.For the gold scenario only two features are ac-tually useful according to MaltOptimizer, MaltOp-timizer shows improvements by adding GENDER(0.22) and PERFECTFORM (0.05).For the predicted input, MaltOptimizer shows im-provements by adding DEGREE (0.09), GENDER(0.08) and ABBRV (0.06).
However, as we can seethe improvements for Swedish are actually lowercompared to the rest of languages.6 Related WorkThere has been some recent research making use ofMaltOptimizer.
For instance, Seraji et al(2012)used MaltOptimizer to get optimal models for pars-ing Persian.
Tsarfaty et al(2012a) worked withMaltOptimizer and Hebrew by including the opti-mization for presenting new ways of evaluating sta-tistical parsers.
Mambrini and Passarotti (2012),68Agirre et al(2012), Padro?
et al(2013) and Balles-teros et al(2013) applied MaltOptimizer to test dif-ferent features of Ancient Greek, Basque and Span-ish (the last 2) respectively; however at that timeMaltOptimizer did not allow the FEATS column tobe divided.
Finally, Ballesteros et al(2012) appliedMaltOptimizer for different parsing algorithms thatare not included in the downloadable version show-ing that it is also possible to optimize different pars-ing algorithms.7 ConclusionsThis new MaltOptimizer implementation helps thedevelopers to adapt MaltParser models to new lan-guages in which there is a rich set of features.
Itshows which features are able to make a change inthe parsing results and which ones are not, in thisway, it is possible to focus annotation effort for thepurpose of parsing.
We clearly observe that MaltOp-timizer outperforms very substantially the resultsshown in the baseline, which is MaltParser in defaultsettings, and it is also nice to see that the improve-ments provided by MaltOptimizer for the morpho-logical features are actually very high, if we com-pare to the ones obtained by MaltOptimizer for thecorpora of the CoNLL shared tasks (Ballesteros andNivre, 2012a).It is worth noting that the experiments with Mal-tOptimizer do not take so long.
The time needed toperform the optimization is actually very short if wecompare to the efforts needed to achieve results inthe same range of accuracy by careful manual op-timization.
The MaltOptimizer process was spedup following heuristics derived from deep provenexperience (Nivre and Hall, 2010), which meansthat there are several combinations that are untested;however, it is worth noting that these heuristics re-sulted in similar performance to more exhaustivesearch for a big set of languages (Ballesteros, 2013).From the feature study shown in Section 5, we ex-pect that it could be useful for people doing parsingresearch and interested in parsing MRLs.
Finally,comparing our submission with the results of otherteams, we believe that we provide a fast and effec-tive parser optimization for parsing MRLs, havingcompetitive results for most of the languages.AcknowledgmentsI would like to thank Koldo Gojenola who initiallygave me the idea presented in this paper.
I am alsovery thankful to Joakim Nivre for his constant helpand support.
Finally, special thanks to the organizersDjame?
Seddah, Reut Tsarfaty and Sandra Ku?bler.ReferencesAnne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.2003.
Building a treebank for french.
In AnneAbeille?, editor, Treebanks.
Kluwer, Dordrecht.I.
Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,A.
D?
?az de Ilarraza, A. Garmendia, and M. Oronoz.2003.
Construction of a Basque dependency treebank.In Proceedings of the 2nd Workshop on Treebanks andLinguistic Theories (TLT), pages 201?204.Eneko Agirre, Aitziber Atutxa, and Kepa Sarasola.
2012.Contribution of complex lexical information to solvesyntactic ambiguity in Basque.
In Proceedings of the24th International Conference on Computational Lin-guistics (COLING 2012), Mumbai, India, 12/2012.Miguel Ballesteros and Joakim Nivre.
2012a.
MaltOp-timizer: A System for MaltParser Optimization.
InProceedings of the Eighth International Conference onLanguage Resources and Evaluation (LREC 2012).Miguel Ballesteros and Joakim Nivre.
2012b.
Mal-tOptimizer: An Optimization Tool for MaltParser.
InProceedings of the System Demonstration Session ofthe Thirteenth Conference of the European Chapter ofthe Association for Computational Linguistics (EACL2012).Miguel Ballesteros, Carlos Go?mez-Rodr?
?guez, andJoakim Nivre.
2012.
Optimizing Planar and 2-Planar Parsers with MaltOptimizer.
Procesamiento delLenguaje Natural, 49, 09/2012.Miguel Ballesteros, Simon Mille, and Alicia Burga.2013.
Exploring Morphosyntactic Annotation Over aSpanish Corpus for Dependency Parsing .
In Proceed-ings of the Second International Conference on De-pendency Linguistics (DEPLING 2013).Miguel Ballesteros.
2013.
Exploring Automatic FeatureSelection for Transition-Based Dependency Parsing.Procesamiento del Lenguaje Natural, 51.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Erhard Hinrichs and Kiril Simov, edi-tors, Proceedings of the First Workshop on Treebanksand Linguistic Theories (TLT 2002), pages 24?41, So-zopol, Bulgaria.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
In69Proceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL), pages 149?164.Jinho D. Choi.
2013.
Preparing Korean Data for theShared Task on Parsing Morphologically Rich Lan-guages.
ArXiv e-prints, September.Yoav Goldberg and Michael Elhadad.
2010.
Easy firstdependency parsing of modern hebrew.
In Proceed-ings of the NAACL HLT 2010 First Workshop on Sta-tistical Parsing of Morphologically-Rich Languages,SPMRL ?10, pages 103?107, Stroudsburg, PA, USA.Association for Computational Linguistics.Nizar Habash and Ryan Roth.
2009.
Catib: Thecolumbia arabic treebank.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August.
Association for Com-putational Linguistics.Nizar Habash, Reem Faraj, and Ryan Roth.
2009.
Syn-tactic Annotation in the Columbia Arabic Treebank.
InProceedings of MEDAR International Conference onArabic Language Resources and Tools, Cairo, Egypt.Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen Eryig?it,Bea?ta Megyesi, Mattias Nilsson, and Markus Saers.2007.
Single malt or blended?
A study in multilingualparser optimization.
In Proceedings of the CoNLLShared Task of EMNLP-CoNLL 2007, pages 933?939.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools.Francesco Mambrini and Marco Carlo Passarotti.
2012.Will a Parser Overtake Achilles?
First experiments onparsing the Ancient Greek Dependency Treebank.
InProceedings of the Eleventh International Workshopon Treebanks and Linguistic Theories (TLT11).Joakim Nivre and Johan Hall.
2010.
A quick guideto MaltParser optimization.
Technical report, malt-parser.org.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.Maltparser: A data-driven parser-generator for depen-dency parsing.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC), pages 2216?2219.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006b.
Tal-banken05: A Swedish treebank with phrase structureand dependency annotation.
In Proceedings of LREC,pages 1392?1395, Genoa, Italy.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared Task ofEMNLP-CoNLL 2007, pages 915?932.Muntsa Padro?, Miguel Ballesteros, Hector Mart?
?nez, andBernd Bohnet.
2013.
Finding dependency pars-ing limits over a large spanish corpus.
In IJCNLP,Nagoya, Japan.
Association for Computational Lin-guistics.Djame?
Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Can-dito, Jinho Choi, Richa?rd Farkas, Jennifer Foster, IakesGoenaga, Koldo Gojenola, Yoav Goldberg, SpenceGreen, Nizar Habash, Marco Kuhlmann, WolfgangMaier, Joakim Nivre, Adam Przepiorkowski, RyanRoth, Wolfgang Seeker, Yannick Versley, VeronikaVincze, Marcin Wolin?ski, and Alina Wro?blewska.2013.
Overview of the spmrl 2013 shared task: Across-framework evaluation of parsing morphologi-cally rich languages.
In Proceedings of the 4th Work-shop on Statistical Parsing of Morphologically RichLanguages: Shared Task, Seattle, WA.Mojgan Seraji, Bea?ta Megyesi, and Joakim Nivre.
2012.Dependency parsers for persian.
In Proceedings of10th Workshop on Asian Language Resources, at 24thInternational Conference on Computational Linguis-tics (COLING 2012).
ACL Anthology.Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,and Noa Nativ.
2001.
Building a Tree-Bank forModern Hebrew Text.
In Traitement Automatique desLangues.Marek S?widzin?ski and Marcin Wolin?ski.
2010.
Towardsa bank of constituent parse trees for Polish.
In Text,Speech and Dialogue: 13th International Conference(TSD), Lecture Notes in Artificial Intelligence, pages197?204, Brno, Czech Republic.
Springer.Reut Tsarfaty, Joakim Nivre, and Evelina Anders-son.
2011.
Evaluating dependency parsing: Robustand heuristics-free cross-annotation evaluation.
InEMNLP, pages 385?396, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012a.
Cross-framework evaluation for statisticalparsing.
In EACL, pages 44?54.Reut Tsarfaty, Djame?
Seddah, Sandra Kuebler, andJoakim Nivre.
2012b.
Parsing Morphologically RichLanguages: Introduction to the Special Issue.
Compu-tational Linguistics, November.Reut Tsarfaty.
2013.
A Unified Morpho-SyntacticScheme of Stanford Dependencies.
Proceedings ofACL.Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgyMo?ra, Zolta?n Alexin, and Ja?nos Csirik.
2010.
Hun-garian dependency treebank.
In LREC.Yue Zhang and Joakim Nivre.
2012.
Analyzing the effectof global learning and beam-search on transition-baseddependency parsing.
In COLING, pages 1391?1400.70
