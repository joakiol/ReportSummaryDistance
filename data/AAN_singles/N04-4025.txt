Automated Team Discourse Annotation and Performance PredictionUsing LSAMelanie J. MartinDepartment of Computer ScienceNew Mexico State UniversityP.O.
Box 30001, MSC CSLas Cruces, New Mexico 88003-8001mmartin@cs.nmsu.eduPeter W. FoltzDepartment of PsychologyNew Mexico State UniversityP.O.
Box 30001, MSC 3452Las Cruces, New Mexico 88003-8001pfoltz@crl.nmsu.eduAbstractWe describe two approaches to analyzing andtagging team discourse using Latent SemanticAnalysis (LSA) to predict team performance.The first approach automatically categorizesthe contents of each statement made by eachof the three team members using an estab-lished set of tags.
Performance predicting thetags automatically was 15% below humanagreement.
These tagged statements are thenused to predict team performance.
The secondapproach measures the semantic content of thedialogue of the team as a whole and accu-rately predicts the team?s performance on asimulated military mission.1 IntroductionThe growing complexity of tasks frequently surpassesthe cognitive capabilities of individuals and thus, oftennecessitates a team approach.
Teams play an increas-ingly critical role in complex military operations inwhich technological and information demands require amulti-operator environment.
The ability to automaticallypredict team performance would be of great value forteam training systems.Verbal communication data from teams provides arich indication of cognitive processing at both the indi-vidual and the team level and can be tied back to boththe team?s and each individual team member?s abilitiesand knowledge.
The current manual analysis of teamcommunication shows promising results, see for exam-ple, Bowers et al (1998).
Nevertheless, the analysis isquite costly.
Hand coding for content is time consum-ing and can be highly subjective.
Thus, what is requiredare techniques for automatically analyzing team com-munications in order to categorize and predict perform-ance.In the research described in this paper we apply La-tent Semantic Analysis (LSA), to measure free-formverbal interactions among team members.
Because itcan measure and compare the semantic information inthese verbal interactions, it can be used to characterizethe quality and quantity of information expressed.
Thiscan be used to determine the semantic content of anyutterance made by a team member as well as to measurethe semantic similarity of an entire team?s communica-tion to another team.
In this paper we describe researchon developing automated techniques for analyzing thecommunication and predicting team performance usinga corpus of communication of teams performing simu-lated military missions.
We focus on two applications ofthis approach.
The first application is to automaticallypredict the categories of discourse for each utterancemade by team members during a mission.
These taggedstatements can then be used to predict overall team per-formance.
The second application is to automaticallypredict the effectiveness of a team based on an analysisof the entire discourse of the team during a mission.
Wethen conclude with a discussion of how these techniquescan be applied for automatic communications analysisand integrated into training.2 DataOur corpus (UAV-Corpus) consists of 67 transcriptscollected from 11 teams, who each completed 7 mis-sions that simulate flight of an Uninhabited Air Vehicle(UAV) in the CERTT (Cognitive Engineering Researchon Team Tasks) Lab's synthetic team task environment(CERTT UAV-STE).
CERTT's UAV-STE is a three-team member task in which each team member is pro-vided with distinct, though overlapping, training; hasunique, yet interdependent roles; and is presented withdifferent and overlapping information during the mis-sion.
The overall goal is to fly the UAV to designatedtarget areas and to take acceptable photos at these areas.The 67 team-at-mission transcripts in the UAV-Corpus contain approximately 2700 minutes of spokendialogue, in 20545 separate utterances or turns.
Thereare approximately 232,000 words or 660 KB of text.
Allcommunication was manually transcribed.We were provided with the results of manual anno-tation of the corpus by three annotators using the Bow-ers Tag Set (Bowers et al 1998), which includes tagsfor: acknowledgement, action, factual, planning, re-sponse, uncertainty, and non-task related utterances.The three annotators had each tagged 26 or 27 team-at-missions so that 12 team-at-missions were tagged bytwo annotators.
Inter-coder reliability had been com-puted using the C-value measure (Schvaneveldt, 1990).The overall C-value for transcripts with two taggers was0.70.
We computed Cohen?s Kappa to be 0.62 (see Sec-tion 4 and Table 1).In addition to the moderate level inter-coder agree-ment, tagging was done at the turn level, where a turncould range from a single word to several utterances bya single speaker, and the number of tags that taggersassigned to a given turn might not agree.
We hope toaddress these limitations in the data set with a morethorough annotation study in the near future.3 Latent Semantic AnalysisLSA is a fully automatic corpus-based statistical methodfor extracting and inferring relations of expected con-textual usage of words in discourse (Landauer et al,1998).LSA has been used for a wide range of applicationsand for simulating knowledge representation, discourseand psycholinguistic phenomena.
These approacheshave included: information retrieval (Deerwester et al,1990), and automated text analysis (Foltz, 1996).
Inaddition, LSA has been applied to a number of NLPtasks, such as text segmentation (Choi et al, 2001).More recently Serafin et al (2003) used LSA for dia-logue act classification, finding that LSA can effectivelybe used for such classification and that adding featuresto LSA showed promise.To train LSA we added 2257 documents to the cor-pus UAV transcripts.
These documents consisted oftraining documents and pre- and post-training inter-views related to UAVs, resulting in a total of 22802documents in the final corpus.
For the UAV-Corpus weused a 300 dimensional semantic space.4 Automatic Discourse TaggingOur goal was to use semantic content of team dialoguesto better understand and predict team performance.
Theapproach we focus on here is to study the dialogue onthe turn level.
Working within the limitations of themanual annotations, we developed an algorithm to tagtranscripts automatically, resulting in some decrease inperformance, but a significant savings in time and re-sources.We established a lower bounds tagging performanceof 0.27 by computing the tag frequency in the 12 tran-scripts tagged by two taggers.
If all utterances weretagged with the most frequent tag, the percentage ofturns tagged correctly would be 27%.Automatic Annotation with LSA.
In order to test ouralgorithm to automatically annotate the data, we com-puted a "corrected tag" for all 2916 turns in the 12 team-at-mission transcripts tagged by two taggers.
This wasnecessary due to the only moderate agreement betweenthe taggers.
We used the union of the sets of tags as-signed by the taggers as the "corrected tag".The union, rather than the intersection, was usedsince taggers sometimes missed relevant tags within aturn.
The union of tags assigned by multiple taggersbetter captures all likely tag types within the turn.
Adisadvantage to using ?corrected tags?
is the loss ofsequential tag information within individual turns.However the focus of this study was on identifying theexistence of relevant discourse, not on its order withinthe turn.Then, for each of the 12 team-at-mission transcripts,we automatically assigned "most probable" tags to eachturn, based on the corrected tags of the "most similar"turns in the other 11 team-at-missions.
For a given turn,T, the algorithm proceeds as follows:Find the turns in the other 11 team-at-mission tran-scripts, whose vectors in the semantic space have thelargest cosines, when compared with T's vector in thesemantic space.
We choose either the ones with the topn (usually top 10) cosines, or the ones whose cosines areabove a certain threshold (usually 0.6).
The correctedtags for these "most similar" turns are retrieved.
Thesum of the cosines for each tag that appears is computedand normalized to give a probability that the tag is thecorrected tag.
Finally, we determine the predicted tag byapplying a cutoff (0.3 and 0.4 seem to produce the bestresults): all of the tags above the cutoff are chosen asthe predicted tag.
If no tag has a probability above thecutoff, them the single tag with the maximum probabil-ity is chosen as the predicted tag.We also computed the average cosine similarity of Tto its 10 closest tags as a measure of certainty of catego-rization.
For example, if T is not similar to any previ-ously categorized turns, then it would have a lowcertainty.
This permits the flagging of turns that thealgorithm is not likely to tag as reliability.In order to improve our results, we considered waysto incorporate simple discourse elements into our pre-dictions.
We added two discourse features to our algo-rithm: for any turn with a question mark, "?
", weincreased to probability that uncertainty, "U", would beone of the tags in its predicted tag; and for any turn fol-lowing a turn with a question mark, "?
", we increased toprobability that response, "R", would be one of the tagsin its predicted tag.We refer to our original algorithm as ?LSA?
and ouralgorithm with the two discourse features added as?LSA+?.
Using LSA+ with our two methods now per-forms only 11% and 15% below human-human agree-ment (see Table 1).We realize that training our system on tags wherehumans had only moderate agreement is not ideal.
Ourfailure analyses indicated that the distinctions our algo-rithm has difficulty making are the same distinctionsthat the humans found difficult to make, so we believethat improved agreement among human annotatorswould result in similar improvements for our algorithm.The results suggest that we can automatically anno-tate team transcripts with tags.
While the approach isnot quite as accurate as human taggers, LSA is able totag an hour of transcripts in under a minute.
As a com-parison, it can take half an hour or longer for a trainedtagger to do the same task.Measuring Agreement.
The C-value measures the pro-portion of inter-coder agreement, but does not take intoaccount agreement by chance.
In order to adjust forchance agreement we computed Cohen?s Kappa (Cohen1960), as shown in Table 1.Table 1.
Kappa and C-Values.5 Predicting Overall Team PerformanceThroughout the CERTT Lab UAV-STE missions a per-formance measure was calculated to determine eachteam?s effectiveness at completing the mission.
Theperformance score was a composite of objective meas-ures including: amount of fuel/film used, number/typeof photographic errors, time spent in warning and alarmstates, and un-visited waypoints.
This composite scoreranged from 0 to 1000.
The score is highly predictive ofhow well a team succeeded in accomplishing their mis-sion.
We used two approaches to predict these overallteam performance scores: correlating the tag frequencieswith the scores and by correlating entire mission tran-scripts with one another.Team Performance Based on Tags.
We computedcorrelations between the team performance score andtag frequencies in each team-at-mission transcript.The tags for all 20545 utterances were first gener-ated using the LSA+ method.
The tag frequencies foreach team-at-mission transcript were then computed bycounting the number of times each individual tag ap-peared in the transcript and dividing by the total numberof individual tags occurring in the transcript.Our preliminary results indicate that frequency ofcertain types of utterances correlate with team perform-ance.
The correlations for tags predicted by computerare shown in Table 2.Table 2.
Tag to Performance Correlations.Table 2 shows that the automated tagging providesuseful results that can be interpreted in terms of teamprocesses.
Teams that tend to state more facts and ac-knowledge other team members more tend to performbetter.
Those that express more uncertainty and need tomake more responses to each other tend to performworse.
These results are consistent with those found inBowers et al (1998), but were generated automaticallyrather than by the hand-coding done by Bowers.Team Performance Based on Whole Transcripts.Another approach to measuring content in team dis-course is to analyze the transcript as a whole.
Using amethod similar to that used to score essays with LSA(Landauer et al 1998), we used the transcripts to predictthe team performance score.
We generate the predictedteam performance scores was as follows:  Given a sub-set of transcripts, S, with known performance scores,and a transcript, t, with unknown performance score, wecan estimate the performance score for t by computingits similarity to each transcript in S. The similarity be-tween any two transcripts is measured by the cosinebetween the transcript vectors in the UAV-Corpus se-mantic space.
To compute the estimated score for t, wetake the average of the performance scores of the 10closest transcripts in S, weighted by cosines.
A holdoutprocedure was used in which the score for a team?s tran-script was predicted based on the transcripts and scoresof all other teams (i.e.
a team?s score was only predictedby the similarity to other teams).
Our results indicatedthat the LSA estimated performance scores correlatedstrongly with the actual team performance scores (r =0.76, p < 0.01), as shown in Figure 1.
Thus, the resultsindicate that we can accurately predict the overall per-formance of the team (i.e.
how well they fly and com-plete their mission) just based on an analysis of theirtranscript from the mission.CODERS-AGREEMENT   C-VALUE   KAPPAHuman-Human 0.70 0.62LSA-Human 0.59 0.48LSA+-Human 0.63 0.53TAG     PEARSONCORRELATIONSIG.2-TAILEDAcknowledgement 0.335 0.006Fact 0.320 0.008Response -0.321 0.008Uncertainty -0.460 0.000Figure 1.
Correlation: Predicted and Actual TeamPerformance.6 Conclusions and Future WorkOverall, the results of the study show that LSA can beused for tagging content as well as predicting team per-formance based on team dialogues.
Given the limita-tions of the manual annotations, the results from thetagging portion of the study are still comparable to otherefforts of automatic discourse tagging using differentmethods and different corpora (Stolcke et al, 2000),which found performance within 15% of the perform-ance of human taggers.
We plan to conduct a more rig-orous manual annotation study.
We expect thatimproved human inter-coder reliability would eliminatethe need for ?corrected tags?
and allow for sequentialanalysis of tags within turns.
It is also anticipated thatincorporating additional methods that account for syntaxand discourse turns should further improve the overallperformance, see also Serafin et al (2003).Even with the limitations of the discourse tagging,our LSA-based approach demonstrates it can be appliedas a method for doing automated measurement of teamperformance.
Using automatic methods we were able toduplicate some of the results of Bowers, and colleagues,(1998) who analyzed the sequence of content categoriesoccurring in communication in a flight simulator task.They found that high team effectiveness was associatedwith consistent responding to uncertainty, planning, andfact statements with acknowledgments and responses.The LSA-predicted team performance scores corre-lated strongly with the actual team performance meas-ures.
This demonstrates that analyses of discourse canautomatically measure how well a team is performingon a mission.
This has implications both for automati-cally determining what discourse characterizes good andpoor teams as well as developing systems for monitor-ing team performance in near real-time.
We are cur-rently exploring two promising avenues to predictperformance in real time: integration of speech recogni-tion technology, and inter-turn tag sequences.Research into team discourse is a new but growingarea.
However, up to recently, the large amounts oftranscript data have limited researchers from performinganalyses of team discourse.
The results of this studyshow that applying NLP techniques to team discoursecan provide accurate predictions of performance.
Theseautomated tools can help inform theories of team per-formance and also aid in the development of more ef-fective automated team training systems.AcknowledgementsThis research was completed in collaboration with theCERTT laboratory including, Nancy Cooke, StevenShope, Preston Kiekel, Jamie Gorman and Susan Smith.This work was supported by Office of Naval Researchand Army Research Laboratory grants.ReferencesC.
A. Bowers, F. Jentsch, E. Salas, and C.C.
Braun.1998.
Analyzing communication sequences for teamtraining needs assessment.
Human Factors, 40, 672-679.F.
Y. Y. Choi, P. Wiemer-Hastings, and J. D. Moore.2001.
Latent Semantic Analysis for Text Segmenta-tion.
In Proceedings of the 2001 Conference on Em-pirical Methods in Natural Language Processing,Lillian Lee and Donna Harman (Eds.
), 109?117.J.
Cohen.
1960.
A coefficient of agreement for nomi-nal scales.
Educational and Psychological Measure-ment, 20, 34-46.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing By LatentSemantic Analysis.
Journal of the American Societyfor Information Science, 41, 391-407.P.
W. Foltz.
1996.
Latent Semantic Analysis for text-based research.
Behavior Research Methods, Instru-ments and Computers.
28(2), 197-202.T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.
In-troduction to Latent Semantic Analysis.
DiscourseProcesses, 25, 259-284.R.
W. Schvaneveldt.
1990.
Pathfinder associative net-works:  Studies in knowledge organization.
Nor-wood, NJ:  Ablex.R.
Serafin, B.
Di Eugenio, and M. Glass.
2003.
LatentSemantic Analysis for dialogue act classification.HLT-NAACL03, 2003 Human Language TechnologyConference, Edmonton, Canada, May (Short Paper)A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer.
2000.
Dialogue Act Mod-eling for Automatic Tagging and Recognition ofConversational Speech, Computational Linguistics26(3), 339-373.
