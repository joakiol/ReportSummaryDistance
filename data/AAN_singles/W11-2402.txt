Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10?19,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsTowards a Probabilistic Model for Lexical EntailmentEyal ShnarchComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israelshey@cs.biu.ac.ilJacob GoldbergerSchool of EngineeringBar-Ilan UniversityRamat-Gan, Israelgoldbej@eng.biu.ac.ilIdo DaganComputer Science DepartmentBar-Ilan UniversityRamat-Gan, Israeldagan@cs.biu.ac.ilAbstractWhile modeling entailment at the lexical-levelis a prominent task, addressed by most textualentailment systems, it has been approachedmostly by heuristic methods, neglecting someof its important aspects.
We present a prob-abilistic approach for this task which cov-ers aspects such as differentiating various re-sources by their reliability levels, consideringthe length of the entailed sentence, the num-ber of its covered terms and the existence ofmultiple evidence for the entailment of a term.The impact of our model components is vali-dated by evaluations, which also show that itsperformance is in line with the best publishedentailment systems.1 IntroductionTextual Entailment was proposed as a genericparadigm for applied semantic inference (Dagan etal., 2006).
Given two textual fragments, termed hy-pothesis (H) and text (T ), the text is said to textuallyentail the hypothesis (T?H) if a person reading thetext can infer the meaning of the hypothesis.
Since itwas first introduced, the six rounds of the Recogniz-ing Textual Entailment (RTE) challenges1 have be-come a standard benchmark for entailment systems.Entailment systems apply various techniques totackle this task, including logical inference (Tatuand Moldovan, 2007; MacCartney and Manning,2007), semantic analysis (Burchardt et al, 2007)and syntactic parsing (Bar-Haim et al, 2008; Wang1http://www.nist.gov/tac/et al, 2009).
Inference at these levels usually re-quires substantial processing and resources, aim-ing at high performance.
Nevertheless, simple lex-ical level entailment systems pose strong baselineswhich most complex entailment systems did not out-perform (Mirkin et al, 2009a; Majumdar and Bhat-tacharyya, 2010).
Additionally, within a complexsystem, lexical entailment modeling is one of themost effective component.
Finally, the simpler lex-ical approach can be used in cases where complexsystems cannot be used, e.g.
when there is no parserfor a targeted language.For these reasons lexical entailment systems arewidely used.
They derive sentence-level entailmentdecision base on lexical-level entailment evidence.Typically, this is done by quantifying the degree oflexical coverage of the hypothesis terms by the textterms (where a term may be multi-word).
A hy-pothesis term is covered by a text term if either theyare identical (possibly at the stem or lemma level)or there is a lexical entailment rule suggesting theentailment of the former by the latter.
Such rulesare derived from lexical semantic resources, suchas WordNet (Fellbaum, 1998), which capture lexi-cal entailment relations.Common heuristics for quantifying the degree ofcoverage are setting a threshold on the percentageof coverage of H?s terms (Majumdar and Bhat-tacharyya, 2010), counting the absolute number ofuncovered terms (Clark and Harrison, 2010), or ap-plying an Information Retrieval-style vector spacesimilarity score (MacKinlay and Baldwin, 2009).Other works (Corley and Mihalcea, 2005; Zanzottoand Moschitti, 2006) have applied heuristic formu-10las to estimate the similarity between text fragmentsbased on a similarity function between their terms.The above mentioned methods do not capture sev-eral important aspects of entailment.
Such aspectsinclude the varying reliability levels of entailmentresources and the impact of rule chaining and multi-ple evidence on entailment likelihood.
An additionalobservation from these and other systems is thattheir performance improves only moderately whenutilizing lexical-semantic resources2.We believe that the textual entailment field wouldbenefit from more principled models for various en-tailment phenomena.
In this work we formulate aconcrete generative probabilistic modeling frame-work that captures the basic aspects of lexical entail-ment.
A first step in this direction was proposed inShnarch et al (2011) (a short paper), where we pre-sented a base model with a somewhat complicatedand difficult to estimate extension to handle cover-age.
This paper extends that work to a more maturemodel with new extensions.We first consider the ?logical?
structure of lexicalentailment reasoning and then interpret it in proba-bilistic terms.
Over this base model we suggest sev-eral extensions whose significance is then assessedby our evaluations.
Learning the parameters of alexical model poses a challenge since there are nolexical-level entailment annotations.
We do, how-ever, have sentence-level annotations available forthe RTE data sets.
To bridge this gap, we formu-late an instance of the EM algorithm (Dempster etal., 1977) to estimate hidden lexical-level entailmentparameters from sentence-level annotations.Overall, we suggest that the main contribution ofthis paper is in presenting a probabilistic model forlexical entailment.
Such a model can better integrateentailment indicators and has the advantage of beingable to utilize well-founded probabilistic methodssuch as the EM algorithm.
Our model?s performanceis in line with the best entailment systems, whileopening up directions for future improvements.2 BackgroundWe next review several entailment systems, mostlythose that work at the lexical level and in particular2See ablation tests reports in http://aclweb.org/aclwiki/ in-dex.php?title=RTE Knowledge Resources#Ablation Teststhose with which we compare our results on the RTEdata sets.The 5th Recognizing Textual Entailment chal-lenge (RTE-5) introduced a new pilot task (Ben-tivogli et al, 2009) which became the main task inRTE-6 (Bentivogli et al, 2010).
In this task the goalis to find all sentences that entail each hypothesis ina given document cluster.
This task?s data sets re-flect a natural distribution of entailments in a corpusand demonstrate a more realistic scenario than theearlier RTE challenges.As reviewed in the following paragraphs there areseveral characteristic in common to most entailmentsystems: (1) lexical resources have a minimal im-pact on their performance, (2) they heuristically uti-lize lexical resources, and (3) there is no principledmethod for making the final entailment decision.The best performing system of RTE-5 was pre-sented by Mirkin et.
al (2009a).
It applies super-vised classifiers over a parse tree representations toidentify entailment.
They reported that utilizing lex-ical resources only slightly improved their perfor-mance.MacKinlay and Baldwin (2009) presented thebest lexical-level system at RTE-5.
They use a vec-tor space method to measure the lexical overlap be-tween the text and the hypothesis.
Since usuallytexts of RTE are longer than their corresponding hy-potheses, the standard cosine similarity score cameout lower than expected.
To overcome this prob-lem they suggested a simple ad-hoc variant of thecosine similarity score which removed from the textall terms which did not appear in the correspond-ing hypothesis.
While this heuristic improved per-formance considerably, they reported a decrease inperformance when utilizing synonym and derivationrelations from WordNet.On the RTE-6 data set, the syntactic-based sys-tem of Jia et.
al (2010) achieved the best results,only slightly higher than the lexical-level systemof (Majumdar and Bhattacharyya, 2010).
The lat-ter utilized several resources for matching hypoth-esis terms with text terms: WordNet, VerbOcean(Chklovski and Pantel, 2004), utilizing two of itsrelations, as well as an acronym database, num-ber matching module, co-reference resolution andnamed entity recognition tools.
Their final entail-ment decision was based on a threshold over the11number of matched hypothesis terms.
They foundout that hypotheses of different length require dif-ferent thresholds.While the above systems measure the number ofhypothesis terms matched by the text, Clark andHarrison (2010) based their entailment decision onthe number of mismatched hypothesis terms.
Theyutilized both WordNet and the DIRT paraphrasedatabase (Lin and Pantel, 2001).
With WordNet,they used one set of relations to identify the conceptof a term while another set of relations was used toidentify entailment between concepts.
Their resultswere inconclusive about the overall effect of DIRTwhile WordNet produced a net benefit in most con-figurations.
They have noticed that setting a globalthreshold for the entailment decision, decreased per-formance for some topics of the RTE-6 data set.Therefore, they tuned a varying threshold for eachtopic based on an idiosyncracy of the data, by whichthe total number of entailments per topic is approxi-mately a constant.Glickman et al (2005) presented a simple modelthat recasted the lexical entailment task as a variantof text classification and estimated entailment prob-abilities solely from co-occurrence statistics.
Theirmodel did not utilize any lexical resources.In contrary to these systems, our model showsimprovement when utilizing high quality resourcessuch as WordNet and the CatVar (Categorial Varia-tion) database (Habash and Dorr, 2003).
As Majum-dar and Bhattacharyya (2010), our model considersthe impact of hypothesis length, however it does notrequire the tuning of a unique threshold for eachlength.
Finally, most of the above systems do notdifferentiate between the various lexical resourcesthey use, even though it is known that resources re-liability vary considerably (Mirkin et al, 2009b).Our probabilistic model, on the other hand, learnsa unique reliability parameter for each resource itutilizes.
As mentioned above, this work extends thebase model in (Shnarch et al, 2011), which is de-scribed in the next section.3 A Probabilistic ModelWe aim at obtaining a probabilistic score for the like-lihood that the hypothesis terms are entailed by theterms of the text.
There are several prominent as-crowdsurroundJaguarText Hypothesish jh jh nh nt 1t 1t it it mt mt'Resource 1chainyyOROR50 people surround carORORsocial groupResource 1 ORResource 1Resource 3Resource 2 h 1h 1Resource 1MATCHMATCHResource 3Resource 2UNCOVEREDFigure 1: Left: the base model of entailing a hypothesis froma text; Right: a concrete example for it (stop-words removed).Edges in the upper part of the diagram represent entailmentrules.
Rules compose chains through AND gates (omitted forvisual clarity).
Chains are gathered by OR gates to entail terms,and the final entailment decision y is the result of their ANDgate.pects of entailment, mostly neglected by previouslexical methods, which our model aims to capture:(1) the reliability variability of different lexical re-sources; (2) the effect of the length of transitive ruleapplication chain on the likelihood of its validity;and (3) addressing cases of multiple entailment evi-dence when entailing a term.3.1 The Base ModelOur base model follows the one presented in(Shnarch et al, 2011), which is described here indetail to make the current paper self contained.3.1.1 Entailment generation processWe first specify the process by which a decisionof lexical entailment between T andH using knowl-edge resources should be determined, as illustratedin Figure 1 (a general description on the left anda concrete example on the right).
There are twoways by which a term h ?
H is entailed by a termt ?
T .
A direct MATCH is the case in which t andh are identical terms (possibly at the stem or lemmalevel).
Alternatively, lexical entailment can be es-tablished based on knowledge of entailing lexical-12semantic relations, such as synonyms, hypernymsand morphological derivations, available in lexicalresources.
These relations provide lexical entail-ment rules, e.g.
Jaguar ?
car.
We denote the re-source which provided the rule r by R(r).It should be noticed at this point that such rulesspecify a lexical entailment relation that might holdfor some (T,H) pairs but not necessarily for allpairs, e.g.
the rule Jaguar ?
car does not holdin the wildlife context.
Thus, the application of anavailable rule to infer lexical entailment in a given(T,H) pair might be either valid or invalid.
We notehere the difference between covering a term and en-tailing it.
A term is covered when the available re-sources suggest its entailment.
However, since a ruleapplication may be invalid for the particular (T,H)context, a term is entailed only if there is a valid ruleapplication from T to it.Entailment is a transitive relation, therefore rulesmay compose transitive chains that connect t to hvia intermediate term(s) t?
(e.g.
crowd ?
socialgroup ?
people).
For a chain to be valid for thecurrent (T,H) pair, all its composing rule applica-tions should be valid for this pair.
This correspondsto a logical AND gate (omitted in Figure 1 for visualclarity) which takes as input the validity values (1/0)of the individual rule applications.Next, multiple chains may connect t to h (as forti and hj in Figure 1) or connect several terms inT to h (as t1 and ti are indicating the entailment ofhj in Figure 1), thus providing multiple evidence forh?s entailment.
For a term h to be entailed by T itis enough that at least one of the chains from T toh would be valid.
This condition is realized in themodel by an OR gate.
Finally, for T to lexically en-tail H it is usually assumed that every h?H shouldbe entailed by T (Glickman et al, 2006).
Therefore,the final decision follows an AND gate combiningthe entailment decisions for all hypothesis terms.Thus, the 1-bit outcome of this gate y correspondsto the sentence-level entailment status.3.1.2 Probabilistic SettingWhen assessing entailment for (T,H) pair, we donot know for sure which rule applications are valid.Taking a probabilistic perspective, we assume a pa-rameter ?R for each resourceR, denoting its reliabil-ity, i.e.
the prior probability that applying a rule fromR for an arbitrary (T,H) pair corresponds to validentailment3.
Under this perspective, direct MATCHsare considered as rules coming from a special ?re-source?, for which ?MATCH is expected to be close to1.
Additionally, there could be a term h which is notcovered by any of the resources at hand, whose cov-erage is inevitably incomplete.
We assume that eachsuch h is covered by a single rule coming from adummy resource called UNCOVERED, while expect-ing ?UNCOVERED to be relatively small.
Based on the?R values we can now estimate, for each entailmentinference step in Figure 1, the probability that thisstep is valid (the corresponding bit is 1).Equations (1) - (3) correspond to the three steps incalculating the probability for entailing a hypothesis.p(tc??
h) =?r?cp(Lr??
R) =?r?c?R(r) (1)p(T?h) =1?p(T9h)=1??c?C(h)[1?p(tc??
h)] (2)p(T?H) =?h?Hp(T?h) (3)First, Eq.
(1) specifies the probability of a partic-ular chain c, connecting a text term t to a hypothesisterm h, to correspond to a valid entailment betweent and h. This event is denoted by tc?
?h and its prob-ability is the joint probability that the applicationsof all rules r ?
c are valid.
Note that every rule rin a chain c connects two terms, its left-hand-side Land its right-hand-side R. The left-hand-side of thefirst rule in c is t?
T and the right-hand-side of thelast rule in it is h ?
H .
Let us denote the event ofa valid rule application by Lr??R.
Since a-priori arule r is valid with probability ?R(r), and assumingindependence of all r?c, we obtain Eq.
(1).Next, Eq.
(2) utilizes Eq.
(1) to specify the prob-ability that T entails h (at least by one chain).
LetC(h) denote the set of chains which suggest the en-tailment of h. The requested probability is equal to1 minus the probability of the complement event,that is, T does not entail h by any chain.
The lat-ter probability is the product of probabilities that all3Modeling a conditional probability for the validity of r,which considers contextual aspects of r?s validity in the current(T,H) context, is beyond the scope of this paper (see discus-sion in Section 6)13chains c?C(h) are not valid (again assuming inde-pendence of chains).Finally, Eq.
(3) gives the probability that T entailsall of H (T ?
H), assuming independence of H?sterms.
This is the probability that every h ?
H isentailed by T , as specified by Eq.
(2).Altogether, these formulas fall out of the standardprobabilistic estimate for the output of AND and ORgates when assuming independence amongst theirinput bits.As can be seen, the base model distinguishesvarying resource reliabilities, as captured by ?R, de-creases entailment probability as rule chain grows,having more elements in the product of Eq.
(1), andincreases it when entailment of a term is supportedby multiple chains with more inputs to the OR gate.Next we describe two extensions for this base modelwhich address additional important phenomena oflexical entailment.3.2 Relaxing the AND GateBased on term-level decisions for the entailment ofeach h ?
H , the model has to produce a sentence-level decision of T ?
H .
In the model described sofar, for T to entailH it must entail all its terms.
Thisdemand is realized by the AND gate at the bottom ofFigure 1.
In practice, this demand is too strict, andwe would like to leave some option for entailing Heven if not every h?H is entailed.
Thus, it is desiredto relax this strict demand enforced by the AND gatein the model.ORANDb1ORxnbnx1Noisy-ANDyFigure 2: A noisy-AND gateThe Noisy-AND model (Pearl, 1988), depicted inFigure 2, is a soft probabilistic version of the ANDgate, which is often used to describe the interactionbetween causes and their common effect.
In thisvariation, each one of the binary inputs b1, ..., bn ofthe AND gate is first joined with a ?noise?
bit xi byan OR gate.
Each ?noise?
bit is 1 with probability p,which is the parameter of the gate.
The output bit yis defined as:y = (b1 ?
x1) ?
(b2 ?
x2) ?
?
?
?
?
(bn ?
xn)and the conditional probability for it to be 1 is:p(y = 1|b1, ..., bn, n) =n?i=1p(1?bi) = p(n?
?i bi)If all the binary input values are 1, the output is de-terministically 1.
Otherwise, the probability that theoutput is 1 is proportional to the number of ones inthe input, where the distribution depends on the pa-rameter p. In case p = 0 the model reduces to theregular AND.In our model we replace the final strict AND witha noisy-AND, thus increasing the probability of T toentail H , to account for the fact that sometimes Hmight be entailed from T even though some h ?His not directly entailed.The input size n for the noisy-AND is the lengthof the hypotheses and therefore it varies from H toH .
Had we used the same model parameter p for alllengths, the probability to output 1 would have de-pended solely on the number of 0 bits in the inputwithout considering the number of ones.
For exam-ple, the probability to entail a hypothesis with 10terms given that 8 of them are entailed by T (and 2are not) is p2.
The same probability is obtained for ahypothesis of length 3 with a single entailed term.We, however, expect the former to have a higherprobability since a larger portion of its terms is en-tailed by T .There are many ways to incorporate the length ofa hypothesis into the noisy-AND model in order tonormalize its parameter.
The approach we take isdefining a separate parameter pn for each hypothesislength n such that pn = ?1nNA, where ?NA becomesthe underlying parameter value of the noisy-AND,i.e.p(y = 1|b1, ..., bn, n) = p(n?
?bi)n = ?n?
?binNAThis way, if non of the hypothesis terms is entailed,the probability for its entailment is ?NA, indepen-dent of its length:p(y = 1|0, 0, ..., 0, n) = pnn = ?NA14As can be seen from Figure 1, replacing the finalAND gate by a noisy-AND gate is equivalent toadding an additional chain to the OR gate of eachhypothesis term.
Therefore we update Eq.
(2) to:p(T ?
h) =1?
p(T 9 h)=1?
[(1?
?1nNA) ??c?C(h)[1?
p(tc??
h)]](2?
)In the length-normalized noisy-AND model thevalue of the parameter p becomes higher for longerhypotheses.
This increases the probability to entailsuch hypotheses, compensating for the lower proba-bility to strictly entail all of their terms.3.3 Considering Coverage LevelThe second extension of the base model follows ourobservation that the prior validity likelihood for arule application, increases as more of H?s terms arecovered by the available resources.
In other words,if we have a hypothesis H1 with k covered termsand a hypothesis H2 in which only j < k terms arecovered, then an arbitrary rule application for H1 ismore likely to be valid than an arbitrary rule appli-cation for H2.We chose to model this phenomenon by normal-izing the reliability ?R of each resource accordingto the number of covered terms in H .
The normal-ization is done in a similar manner to the length-normalized noisy-AND described above, obtaininga modified version of Eq.
(1):p(tc??
h) =?r?c?1#coveredR(r) (1?
)As a results, the larger the number of covered termsis, the larger ?R values our model uses and, in total,the entailment probability increases.To sum up, we have presented the base model,providing a probabilistic estimate for the entailmentstatus in our generation process specified in 3.1.Two extensions were then suggested: one that re-laxes the strict AND gate and normalizes this re-laxation by the length of the hypothesis; the secondextension adjusts the validity of rule applications asa function of the number of the hypothesis coveredterms.
Overall, our full model combines both exten-sions over the base probabilistic model.4 Parameter EstimationThe difficulty in estimating the ?R values from train-ing data arises because these are term-level param-eters while the RTE-training entailment annotationis given for the sentence-level, each (T,H) pair inthe training is annotated as either entailing or not.Therefore, we use an instance of the EM algorithm(Dempster et al, 1977) to estimate these hidden pa-rameters.4.1 E-StepIn the E-step, for each application of a rule r in achain c for h?H in a training pair (T,H), we com-pute whcr(T,H), the posterior probability that therule application was valid given the training annota-tion:whcr(T,H) =???p(Lr?
?R|T?H) if T?Hp(Lr?
?R|T9H) if T9H(4)where the two cases refer to whether the training pairis annotated as entailing or non-entailing.
For sim-plicity, we write whcr when the (T,H) context isclear.The E-step can be efficiently computed usingdynamic programming as follows; For each train-ing pair (T,H) we first compute the probabilityp(T ?
H) and keep all the intermediate computa-tions (Eq.
(1)- (3)).
Then, the two cases of Eq.
(4),elaborated next, can be computed from these expres-sions.
For computing Eq.
(4) in the case that T?Hwe have:p(Lr??
R|T?H) = p(Lr??
R|T ?
h) =p(T?h|Lr??
R)p(Lr?
?R)p(T?h)The first equality holds since when T entails H ev-ery h ?
H is entailed by it.
Then we apply Bayes?rule.
We have already computed the denominator(Eq.
(2)), p(Lr??
R) ?
?R(r) and it can be shown4that:p(T?h|Lr?
?R) = 1?p(T9h)1?
p(tc??h)?
(1?p(tc?
?h)?R(r))(5)4The first and second denominators reduce elements fromthe products in Eq.
2 and Eq.
1 correspondingly15where c is the chain which contains the rule r.For computing Eq.
(4), in the second case, thatT9H , we have:p(Lr?
?R|T9H) =p(T9H|L r?
?R)p(L r?
?R)p(T9H)In analogy to Eq.
(5) it can be shown thatp(T9H|L r?
?R) = 1?p(T?H)p(T?h)?p(T?h|Lr?
?R)(6)while the expression for p(T?h|Lr?
?R) appears inEq.
(5).This efficient computation scheme is an instanceof the belief-propagation algorithm (Pearl, 1988) ap-plied to the entailment process, which is a loop-freedirected graph (Bayesian network).4.2 M-StepIn the M-step we need to maximize the EM auxiliaryfunction Q(?)
where ?
is the set of all resources re-liability values.
Applying the derivation of the aux-iliary function to our model (first without the exten-sions) we obtain:Q(?)
=?T,H?h?H?c?C(h)?r?c(whcr log ?R(r) +(1?
whcr) log(1?
?R(r)))We next denote by nR the total number of applica-tions of rules from resource R in the training data.We can maximize Q(?)
for each R separately to ob-tain the M-step parameter-updating formula:?R =1nR?T,H?h?H?c?C(h)?r?c|R(r)=Rwhcr (7)The updated parameter value averages the posteriorprobability that rules from resource R have beenvalidly applied, across all its utilizations in the train-ing data.4.3 EM for the Extended ModelIn case we normalize the noisy-AND parameter bythe hypothesis length, for each length we use a dif-ferent parameter value for the noisy-AND and wecannot simply merge the information from all thetraining pairs (T,H).
To find the optimal param-eter value for ?NA, we need to maximize the fol-lowing expression (the derivation of the auxiliaryfunction to the hypothesis-length-normalized noisy-AND ?resource?
):Q(?NA) =?T,H?h?H(whNA log(?1nNA) +(1?
whNA) log(1?
?1nNA)) (8)where n is the length of H , ?NA is the parametervalue of the noisy-AND model andwhNA is the pos-terior probability that the noisy-AND was used tovalidly entail the term h5, i.e.whNA(T,H) =?????p(TNA??
?h|T?H) if T?Hp(TNA??
?h|T9H) if T9HThe two cases of the above equation are similar toEq.
(4) and can be efficiently computed in analogyto Eq.
(5) and Eq.
(6).There is no close-form expression for the param-eter value ?NA that maximizes expression (8).
Since?NA?
[0, 1] is a scalar parameter, we can find ?NAvalue that maximizes Q(?NA) using an exhaustivegrid search on the interval [0, 1], in each iteration ofthe M-step.
Alternatively, for an iterative procedureto maximize expression (8), see Appendix A.In the same manner we address the normalizationof the reliability ?R of each resourcesR by the num-ber of H?s covered terms.
Expression (8) becomes:Q(?R) =?T,H?h?H?c?C(h)?r?c|R(r)=R(whcr log(?covR ) + (1?
whcr) log(1?
?covR ))were 1cov is the number of H terms which are cov-ered.
We can find the ?R that maximizes this equa-tion in one of the methods described above.5 Evaluation and ResultsFor our evaluation we use the RTE-5 pilot task andthe RTE-6 main task data sets described in Sec-tion 2.
In our system, sentences are tokenized andstripped of stop words and terms are tagged for part-of-speech and lemmatized.
We utilized two lexicalresources, WordNet (Fellbaum, 1998) and CatVar5In contrary to Eq.
4, here there is no specific t ?
T thatentails h, therefore we write TNA??
?h16(Habash and Dorr, 2003).
From WordNet we took asentailment rules synonyms, derivations, hyponymsand meronyms of the first senses of T and H terms.CatVar is a database of clusters of uninflected words(lexemes) and their categorial (i.e.
part-of-speech)variants (e.g.
announce (verb), announcer and an-nouncement(noun) and announced (adjective)).
Wededuce an entailment relation between any two lex-emes in the same cluster.
Model?s parameters wereestimated from the development set, taken as train-ing.
Based on these parameters, the entailment prob-ability was estimated for each pair (T,H) in the testset, and the classification threshold was tuned byclassification over the development set.We next present our evaluation results.
First weinvestigate the impact of utilizing lexical resourcesand of chaining rules.
In section 5.2 we evaluate thecontribution of each extension of the base model andin Section 5.3 we compare our performance to thatof state-of-the-art entailment systems.5.1 Resources and Rule-Chaining ImpactAs mentioned in Section 2, in the RTE data sets itis hard to show more than a moderate improvementwhen utilizing lexical resources.
Our analysis as-cribes this fact to the relatively small amount of ruleapplications in both data sets.
For instance, in RTE-6 there are 10 times more direct matches of identi-cal terms than WordNet and CatVar rule applicationscombined, while in RTE-5 this ratio is 6.
As a resultthe impact of rule applications can be easily shad-owed by the large amount of direct matches.Table 1 presents the performance of our (full)model when utilizing no resources at all, WordNet,CatVar and both, with chains of a single step.
Wealso considered rule chains of length up to 4 andpresent here the results of 2 chaining steps withWordNet-2 and (WordNet+CatVar)-2.Overall, despite the low level of rule applications,we see that incorporating lexical resources in ourmodel significantly6 and quite consistently improvesperformance over using no resources at all.
Natu-rally, the optimal combination of resources may varysomewhat across the data sets.In RTE-6 WordNet-2 significantly improved per-6All significant results in this section are according to Mc-Nemar?s test with p < 0.01 unless stated otherwiseformance over the single-stepped WordNet.
How-ever, mostly chaining did not help, suggesting theneed for future work to improve chain modeling inour framework.ModelF1%RTE-5 RTE-6no resources 41.6 44.9WordNet 45.8 44.6WordNet-2 45.7 45.5CatVar 46.9 45.6WordNet + CatVar 48.3 45.6(WordNet + CatVar)-2 47.1 44.0Table 1: Evaluation of the impact of resources and chaining.5.2 Model Components impactWe next assess the impact of each of our proposedextensions to the base probabilistic model.
To thatend, we incorporate WordNet+CatVar (our best con-figuration above) as resources for the base model(Section 3.1) and compare it with the noisy-ANDextension (Eq.
(2?
)), the covered-norm extensionwhich normalizes the resource reliability parame-ter by the number of covered terms (Eq.
(1?))
andthe full model which combines both extensions.
Ta-ble 2 presents the results: both noisy-AND andcovered-norm extensions significantly increase F1over the base model (by 4.5-8.4 points).
This scaleof improvement was observed with all resources andchain-length combinations.
In both data sets, thecombination of noisy-AND and covered-norm ex-tensions in the full model significantly outperformseach of them separately7, showing their complemen-tary nature.
We also observed that applying noisy-AND without the hypothesis length normalizationhardly improved performance over the base model,emphasising the importance of considering hypothe-sis length.
Overall, we can see that both base modelextensions improve performance.Table 3 illustrates a set of maximum likelihoodparameters that yielded our best results (full model).The parameter value indicates the learnt reliabilityof the corresponding resource.7With the following exception: in RTE-5 the full model isbetter than the noisy-AND extension with significance of onlyp = 0.0617ModelF1%RTE-5 RTE-6base model 36.2 38.5noisy-AND 44.6 43.1covered-norm 42.8 44.7full model 48.3 45.6Table 2: Impact of model components.
?MATCH ?WORDNET ?CATVAR ?UNCOVERED ?NA0.80 0.70 0.65 0.17 0.05Table 3: A parameter set of the full model which maximizesthe likelihood of the training set.5.3 Comparison to Prior ArtFinally, in Table 4, we put these results in the con-text of the best published results on the RTE task.We compare our model to the average of the bestruns of all systems, the best and second best per-forming lexical systems and the best full system ofeach challenge.
For both data sets our model is situ-ated high above the average system.
For the RTE-6data set, our model?s performance is third best withMajumdar and Bhattacharyya (2010) being the onlylexical-level system which outperforms it.
However,their system utilized additional processing that wedid not, such as named entity recognition and co-reference resolution8.
On the RTE-5 data set ourmodel outperforms any other published result.ModelF1%RTE-5 RTE-6full model 48.3 45.6avg.
of all systems 30.5 33.82nd best lexical system 40.3a 44.0bbest lexical system 44.4c 47.6dbest full system 45.6c 48.0eTable 4: Comparison to RTE-5 and RTE-6 best entailmentsystems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark andHarrison, 2010), (c)(Mirkin et al, 2009a)(2 submitted runs),(d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al,2010).8We note that the submitted run which outperformed our re-sult utilized a threshold which was a manual modification of thethreshold obtained systematically in another run.
The latter runachieved F1 of 42.4% which is below our result.We conclude that our probabilistic model demon-strates quality results which are also consistent,without applying heuristic methods of the kinds re-viewed in Section 26 Conclusions and Future WorkWe presented, a probabilistic model for lexical en-tailment whose innovations are in (1) consideringeach lexical resource separately by associating anindividual reliability value for it, (2) considering theexistence of multiple evidence for term entailmentand its impact on entailment assessment, (3) settingforth a probabilistic method to relax the strict de-mand that all hypothesis terms must be entailed, and(4) taking account of the number of covered terms inmodeling entailment reliability.We addressed the impact of the various compo-nents of our model and showed that its performanceis in line with the best state-of-the-art inference sys-tems.
Future work is still needed to reflect the im-pact of transitivity.
We consider replacing the ANDgate on the rules of a chain by a noisy-AND, to relaxits strict demand that all its input rules must be valid.Additionally, we would like to integrate ContextualPreferences (Szpektor et al, 2008) and other workson Selectional Preference (Erk and Pado, 2010) toverify the validity of the application of a rule in aspecific (T,H) context.
We also intend to explorethe contribution of our model within a complex sys-tem that integrates multiple levels of inference aswell as its contribution for other applications, suchas Passage Retrieval.ReferencesRoy Bar-Haim, Jonathan Berant, Ido Dagan, Iddo Green-tal, Shachar Mirkin, Eyal Shnarch, and Idan Szpektor.2008.
Efficient semantic deduction and approximatematching over compact parse forests.
In Proc.
of TAC.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
The fifthPASCAL recognizing textual entailment challenge.
InProc.
of TAC.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa TrangDang, and Danilo Giampiccolo.
2010.
The sixthPASCAL recognizing textual entailment challenge.
InProc.
of TAC.Aljoscha Burchardt, Nils Reiter, Stefan Thater, andAnette Frank.
2007.
A semantic approach to textual18entailment: System evaluation and task analysis.
InProc.
of the ACL-PASCAL Workshop on Textual En-tailment and Paraphrasing.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proc.
of EMNLP.Peter Clark and Phil Harrison.
2010.
BLUE-Lite: aknowledge-based lexical entailment system for RTE6.In Proc.
of TAC.Courtney Corley and Rada Mihalcea.
2005.
Measuringthe semantic similarity of texts.
In Proc.
of the ACLWorkshop on Empirical Modeling of Semantic Equiv-alence and Entailment.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL recognising textual entailmentchallenge.
In Lecture Notes in Computer Science, vol-ume 3944, pages 177?190.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the royal statistical society, se-ries [B], 39(1):1?38.Katrin Erk and Sebastian Pado.
2010.
Exemplar-basedmodels for word meaning in context.
In Proc.
of theACL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, and Com-munication).
The MIT Press.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.
Aprobabilistic classification approach for lexical textualentailment.
In Proc.
of AAAI.Oren Glickman, Eyal Shnarch, and Ido Dagan.
2006.Lexical reference: a semantic matching subtask.
InProceedings of the EMNLP.Nizar Habash and Bonnie Dorr.
2003.
A categorial vari-ation database for english.
In Proc.
of NAACL.Houping Jia, Xiaojiang Huang, Tengfei Ma, XiaojunWan, and Jianguo Xiao.
2010.
PKUTM participationat TAC 2010 RTE and summarization track.
In Proc.of TAC.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, 7:343?360.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proc.
of theACL-PASCAL Workshop on Textual Entailment andParaphrasing.Andrew MacKinlay and Timothy Baldwin.
2009.
Abaseline approach to the RTE5 search pilot.
In Proc.of TAC.Debarghya Majumdar and Pushpak Bhattacharyya.2010.
Lexical based text entailment system for maintask of RTE6.
In Proc.
of TAC.Shachar Mirkin, Roy Bar-Haim, Jonathan Berant, IdoDagan, Eyal Shnarch, Asher Stern, and Idan Szpektor.2009a.
Addressing discourse and document structurein the RTE search task.
In Proc.
of TAC.Shachar Mirkin, Ido Dagan, and Eyal Shnarch.
2009b.Evaluating the inferential utility of lexical-semantic re-sources.
In Proc.
of EACL.Judea Pearl.
1988.
Probabilistic reasoning in intelli-gent systems: networks of plausible inference.
MorganKaufmann.Eyal Shnarch, Jacob Goldberger, and Ido Dagan.
2011.A probabilistic modeling framework for lexical entail-ment.
In Proc.
of ACL, pages 558?563.Idan Szpektor, Ido Dagan, Roy Bar-Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Proc.of ACL-08: HLT.Marta Tatu and Dan Moldovan.
2007.
COGEX at RTE3.
In Proc.
of the ACL-PASCAL Workshop on TextualEntailment and Paraphrasing.Rui Wang, Yi Zhang, and Guenter Neumann.
2009.
Ajoint syntactic-semantic representation for recognizingtextual relatedness.
In Proc.
of TAC.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailments withcross-pair similarities.
In Proc.
of ACL.A Appendix: An Iterative Procedure toMaximize Q(?NA)There is no close-form expression for the parametervalue ?NA that maximizes expression (8) from Sec-tion 4.3.
Instead we can apply the following iterativeprocedure.
The derivative of Q(?NA) is:dQ(?NA)d?NA=?(l?whNA?NA?(1?whNA)l??(l?1)NA1?
?lNA)where 1l is the hypothesis length and the summationis over all terms h in the training set.
Setting thisderivative to zero yields an equation which the opti-mal value satisfies:?NA =?l?whNA?
(1?whNA)l??(l?1)NA1??lNA(9)Eq.
(9) can be utilized as a heuristic iterative proce-dure to find the optimal value of ?NA:?NA ??l?whNA?
(1?whNA)l??(l?1)NA1?
?lNA19
