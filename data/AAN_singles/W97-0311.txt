Automat ic  D iscovery  of  Non-Compos i t iona l  Compoundsin Para l le l  Data  *I. Dan MelamedDept.
of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA, 19104, U.S.A.melamed~unagi, c s.  upenn, eduhttp ://www.
cis.
upenn, edu/"melamedAbstractAutomatic segmentation of text into min-imal content-bearing units is an unsolvedproblem even for languages like English.Spaces between words offer an easy first ap-proximation, but this approximation is notgood enough for machine translation (MT),where many word sequences are not trans-lated word-for-word.
This paper presentsan efficient automatic method for discover-ing sequences of words that are translatedas a unit.
The method proceeds by com-paring pairs of statistical translation mod-els induced from parallel texts in two lan-guages.
It can discover hundreds of non-compositional compounds on each itera-tion, and constructs longer compounds outof shorter ones.
Objective evaluation on asimple machine translation task has shownthe method's potential to improve the qual-ity of MT output.
The method makes fewassumptions about the data, so it can beapplied to parallel data other than paralleltexts, such as word spellings and pronunci-ations.1 IntroductionThe optimal way to  analyze linguistic data intoits primitive elements is rarely obvious but oftencrucial.
Identifying phones and words in speechhas been a major focus of research.
Automati-cally finding words in text, the problem addressedhere, is largely unsolved for languages uch as Chi-nese and Thai, which are written without spaces* Many thanks to Mike Collins, Jason Eisner, MitchMarcus and two anonymous reviewers for their feedbackon earlier drafts of this paper.
This research was sup-ported by an equipment grant from Sun MicroSystemsand by ARPA Contract #N66001-94C-6043.
(but see Fung & Wu, 1994; Sproat et al, 1996).Spaces in texts of languages like English offer aneasy first approximation to minimal content-bearingunits.
However, this approximation mis-analyzesnon-compos i t iona l  compounds  (NCCs)  such as"kick the bucket" and "hot dog."
NCCs  are com-pound words whose meanings are a matter of con-vention and cannot be synthesized from the mean-ings of their space-delimited components.
TreatingNCCs as multiple words degrades the performanceof machine translation (MT), information retrieval,natural language generation, and most other NLPapplications.NCCs are usually not translated literally to otherlanguages.
Therefore, one way to discover NCCs isto induce and analyze a translation model betweentwo languages.
This paper is about an information-theoretic approach to this kind of ontological dis-covery.
The method is based on the insight thattreatment of NCCs as multiple words reduces thepredictive power of translation models.
Whethera given sequence of words is an NCC can be de-termined by comparing the predictive power of twotranslation models that differ on whether they treatthe word sequence as an NCC.
Searching a space ofdata models in this manner has been proposed be-fore, e.g.
by Brown et al (1992) and Wang et al(1996), but their particular methods have been lim-ited by the computational expense of inducing datamodels and the typically vast number of potentialNCCs that need to be tested.
The method presentedhere overcomes this limitation by making indepen-dence assumptions that allow hundreds of NCCs tobe discovered from each pair of induced translationmodels.
It is further accelerated by heuristics forgauging the a priori likelihood of validation for eachcandidate NCC.The predictive power of a translation model de-pends on what the model is meant to predict.
Thispaper considers two different applications of trans-97lation models, and their corresponding objectivefunctions.
The different objective functions leadto different mathematical formulations of predictivepower, different heuristics for estimating predictivepower, and different classifications ofword sequenceswith respect o compositionality.
Monolingual prop-erties of NCCs are not considered by either ob-jective function.
So, the method will not detectphrases that are translated word-for-word espitenon-compositional semantics, such as the Englishmetaphors "ivory tower" and "banana republic,"which translate literally into French.
On the otherhand, the method will detect word sequences thatare often paraphrased in translation, but have per-fectly compositional meanings in the monolingualsense.
For example, "tax system" is most oftentranslated into French as "r6gime fiscale."
Each newbatch of validated NCCs raises the value of the ob-jective function for the given application, as demon-strated in Section 8.
You can skip ahead to Table 4for a random sample of the NCCs that the methodvalidated for use in a machine translation task.The NCC detection method makes some assump-tions about the properties of statistical translationmodels, but no assumptions about the data fromwhich the models are constructed.
Therefore, themethod is applicable to parallel data other thanparallel texts.
For example, Section 8 applies themethod to orthographic and phonetic representa-tions of English words to discover the NCCs ofEnglish orthography.2 T rans la t ion  Mode lsA translation model can be constructed auto-matically from texts that exist in two languages(bitexts)  (Brown et al, 1993; Melamed, 1997).The more accurate algorithms used for construct-ing translation models, including the EM algorithm,alternate between two phases.
In the first phase,the algorithm finds and counts the most likely linksbetween word tokens in the two halves of the bi-text.
L inks connect words that are hypothesizedto be mutual translations.
In the second phase, thealgorithm estimates translation probabilities by di-viding the link counts by the total number of links.Let S and 7- represent the distributions of linkedwords in the source and target 1 texts.
A simplet rans la t ion  model  is just a joint probability dis-tribution Pr(s,t) ,  which indicates the probabilitythat a randomly selected link in the bitext links1In the context of symmetric translation models, thewords "source" and "target" are merely labels.s E S with t E 7-.
2 A d i rec ted  t rans la t ionmode l  can be derived in the standard way:Pr(tls ) = Pr(s, t)/Pr(s).3 Ob ject ive  Funct ionsThe decision whether a given sequence of wordsshould count as an NCC can be made automatically,if it can be expressed in terms of an explicit objectivefunction for the given application.
The first appli-cation I will consider is statistical machine trans-lation involving a directed translation model anda target language model, of the sort advocated byBrown et al (1993).
If only the translation modelmay be varied, then the objective function for thisapplication should be based on how well the transla-tion model predicts the distribution of words in thetarget language.
In information theory, one such ob-jective function is called mutual information.
Mu-tual  in fo rmat ion  measures how well one randomvariable predicts another3:Pr(s,t)I(S; T) = ~ ~ Pr(s, t) log Pr(s) Pr(t) (1)sES tETWhen Pr(s, t) is a text translation model, mutualinformation indicates how well the model can predictthe distribution of words in the target text giventhe distribution of words in the source text, andvice versa.
This objective function may also be usedfor optimizing cross-language information retrieval,where translational distributions must be estimatedeither for queries or for documents before queriesand documents can be compared (Oard & Dorr,1996).Figure 1 shows a simple example of howrecognition of NCCs increases the mutual infor-mation of translation models.
The English word"balance" is most often translated into French as"6quilibre" and "sheet" usually becomes "feuille.
"However, a "balance sheet" is a "bilan."
A trans-lation model that doesn't recognize "balance sheet"as an NCC would distribute the translation prob-abilities of "bilan" over multiple English words, asshown in the Incorrect Model.
The Incorrect Modelis uncertain about how "bilan" should be trans-lated.
On the other hand, the Correct Model,which recognizes "balance sheet" as an NCC is com-pletely certain about its translation.
As a result,the mutual information of the Incorrect Model is2 .
71 log ~_._ + 2 ?
gx log ~_._ =~2 log 2, whereas the2 3 2 3mutual information of the Correct Model is log 3.2s E S means that Prs(s) > 0.3See Cover & Thomas (1991) for a good introductionto information theory.98Segment #123English halfbalancesheetbalance sheetFrench half~quilibrefeuillebilanIncorrect Model1/3 - balance ~-~ equUibre1/6~ bilansheet ~ feuilleCorrect Modelbalance " 1/3, ~quilibrebalance sheet - 1/3 bilansheet, 1/3.
feuilleFigure 1: Two translation models that my be inducedfrom the trivial bitext at the top of the figure.
Trans-lation models that know about NCCs have higher mu-tual information than those that do not.4 Pred ic t ive  Va lue  Funct ionsAn explicit objective function immediately leads toa simple test of whether a given sequence of wordsshould be treated as an NCC: Induce two transla-tion models, a tr ia l  t rans la t ion  mode l  that in-volves the candidate NCC and a base t rans la t ionmode l  that does not.
If the value of the objectivefunction is higher in the trial model than in the basemodel, then the NCC is valid; otherwise it is not.
Intheory, this test can be repeated for each sequenceof words in the text.
In practice, texts contain anenormous number of word sequences (Brown et al,1992), only a tiny fraction of which are NCCs, andit takes considerable computational effort to induceeach translation model.
Therefore, it is necessary totest many NCCs on each pair of translation models.Suppose we induce a trial translation model fromtexts E and F involving a number of NCCs in thelanguage ,5 of E, and compare it to a base transla-tion model without any of those NCCs.
We wouldlike to keep the NCCs that caused a net increasein the objective function I and discard those thatcaused a net decrease.
We need some method ofassigning credit for the difference in the value of Ibetween the two models.
More precisely, we need afunction iT(s) over the words s E ,5 such thatI(S; 7-) = ~ iT(s).
(2)sE8Fortunately, the objective function in Equations 1is already a summation over source words.
So, itsvalue can be distributed as follows:iT(S) Z PrCs, "'" Pr(s, t)= ~) log Pr(s) Pr(t) (3)tETThe pred ict ive  value funct ion  iT(s) representsthe contribution of s to the objective function of thewhole translation model.
I will write simply i(s)when T is clear from the context.Comparison of predictive value functions acrosstranslation models can only be done underAssumpt ion  1 Treating the bigram < x, y > as anNCC will not affect the predictive value function ofany s E ,5 other than x, y, and the NCC xy.Let i and i' be the predictive value functions forsource words in the base translation model and inthe trial translation model, respectively.
Under As-sumption 1, the net change in the objective functioneffected by each candidate NCC xy iszx=, = i ' (x) + i ' (y) + i ' (xy)  - - i (u ) .
(4)If A=u > 0, then xy is a valid NCC for the givenapplication.Assumption 1 would likely be false if either x ory was a part of any candidate NCC other than xy.Therefore, NCCs that are tested at the same timemust satisfy the mutua l  exc lus ion condit ion:  Noword s E ,5 may participate in more than one candi-date NCC at the same time.
Assumption 1 may notbe completely safe even with this restriction, due tothe imprecise nature of translation model construc-tion algorithms.5 I terat ionThe mutual exclusion condition implies that mul-tiple tests must be performed to find the majorityof NCCs in a given text.
Furthermore, Equation 4allows testing of only two-word NCCs.
Certainly,longer NCCs exist.
Given parallel texts E and F,the following algorithm runs multiple NCC tests andallows for recognition of progressively onger NCCs:1.
Initialize the stop-list and the NCC list to beempty..
In E, find all occurrences of all NCCs on theNCC list, and replace them with single "fused"tokens, which the translation model construc-tion algorithm will treat as single words.3.
Induce a base translation model between Eand F.994.
For all adjacent bigrams < x ,y  > in E thatare not on the stop-list and whose frequency isat least ?4, compute ~xu, the estimate of A~y,using the equations in Section 6.5.
Make a list of candidate NCCs, containing allthe bigrams for which A~u > 0, sorted by A~u"6.
Remove from the list all candidates xy whereeither x or y is part of another bigram higherin the list.
This step implements the mutualexclusion condition described in Section 4.7.
CopyEto  E'.
For eachbigram < x,y  > re-maining on the candidate NCC list, fuse eachinstance of < x, y > in E' into a single tokenxy.8.
Induce a trial translation model between E'and F.9.
Compute the actual Axu values for all candidateNCCs, using Equation 4.10.
For each candidate NCC xy, if A~y > 0, thenadd xy  to the NCC list; otherwise add xy  to thestop-list.11.
Repeat from Step 2.The algorithm can also be run in "two-sided" mode,so that it looks for NCCs in E and in F on alternateiterations.
This mode enables the translation modelto link NCCs in one language to NCCs in the other.In its simplest form, the algorithm only considersadjacent words as candidate NCCs.
However, func-tion words are translated very inconsistently, and itis difficult to model their translational distributionsaccurately.
To make discovery of NCCs involvingfunction words more likely, I consider content wordsthat are separated by one or two functions words tobe adjacent.
Thus, NCCs like "blow ... whistle" and"icing ... cake" may contain gaps.Fusing NCCs with gaps may fuse some words in-correctly, when the NCC is a frozen expression.
Forexample, we would want to recognize that "icing.
.
.
cake" is an NCC when we see it in new text,but not if it occurs in a sentence like "Mary atethe icing off the cake."
It is necessary to deter-mine whether the gap in a given NCC is fixed ornot.
Thus, the price for this flexibility provided byNCC gaps is that, before Step 7, the algorithm fillsgaps in proposed NCCs by looking through the text.4The threshold ?
reduces errors due to noise in thedata and in the translation model.
It should be opti-mized empirically for each kind of parallel data.
Forparallel texts, I use ?
= 2.Sometimes, NCCs have multiple possible gap fillers,for example "make up {my, your,his,their} mind.
"When the gap filling procedure finds two or threepossible fillers, the most frequent filler is used, andthe rest are ignored in the hope that they will bediscovered on the next iteration.
When there aremore than three possible fillers, the NCC retains thegap.
The token fuser (in Steps 2 and 7) knows toshift all words in the NCC to the location of theleftmost word.
E.g.
an instance of the previous ex-ample in the text might be fused as "make_up_<GAP >_mind his.
"In principl~ the NCC discovery algorithm coulditerate until Axy < 0 for all bigrams.
This wouldbe a classic case of over-fitting the model to thetraining data.
NCC discovery is more useful if it isstopped at the point where the NCCs discovered sofar would maximize the application's objective func-tion on new data.
A domain-independent method tofind this point is to use held-out data or, more gen-erally, to cross-validate between different subsets ofthe training data.
Alternatively, when the applica-tions involves human inspection, e.g.
for bilinguallexicography, a suitable stopping point can be foundby manually inspecting validated NCCs.6 Cred i t  Es t imat ionSections 3 and 4 describe how to carry out NCCvalidity tests, but not how to choose which NCCs totest.
Making this choice at random would make thediscovery process too slow, because the vast majorityof word sequences are not valid NCCs.
The discoveryprocess can be greatly accelerated by testing onlycandidate NCCs for which Equation 4 is likely tobe positive.
This section presents a way to guesswhether Axy > 0 for a candidate NCC xy  be\]oreinducing a translation model that involves this NCC.To do so, it is necessary to estimate i ' (x),  i ' (y),  andi ' (xy),  using only the base translation model.First, a bit of notation.
Let LC and Rc denoteword contexts to the left and to the right.
Let(x : RC = y) be the set of tokens of x whose rightcontext is y, and vice versa for (y : LC = x).
Now,i'(x) and i ' (y), can be estimated underAssumpt ion  2 When x occurs without y in itscontext, it will be linked to the same target words bythe trial translation model as by the base translationmodel, and likewise \]or y without x.Assumption 2 says thati'(x) = i (x:  Rc # y)i '(y) = i(y : LC ~ x)(6)(7)100i'(xy)(by Eq.
8)(by Eq.
9)(by Eq.
10)Pr(xy, t)= E Pr(xy, t) log Pr(xy) Pr(t)fEW= E\ [P r (x :  RC = y,t) + Pr(y:  LC = x,t)l log \[Pr(x: RC = y,t) + Pr(y:  LC = x,t)\]teT Pr(y : LC = x) Pr(t)Pr(x : Rc = y, t)= EPr (x :ac=Y ' t ) l ?gPr~ ac~-~r( t )tETPr(y : LC = x, t)+ EPr (Y :  LC = x,t)!og Pr(y:  LC = x) Pr(t)tET(5)Figure 2: Estimation of i'(xy).
Note that, by definition, Pr(x : RC = y) = Pr(y : LC = X) ---- Pr(xy).Estimating i '(xy) is more difficult because it re-quires knowledge of the entire translational distribu-tions of both x and y, conditioned on all the contextsof x and y.
Since we wish to consider hundreds ofcandidate NCCs simultaneously, and contexts frommany megabytes of text, all this information wouldnot fit on disk, let alne in memory.
The best wecan do is approximate with lower-order distributionsthat are easier to compute.The approximation begins withAssumpt ion  3 I f  xy is a valid NCC, then at mostone of x and y will be linked to a target word when-ever x and y co-occur.Assumption 3 implies that for all t E TPr(xy, t) = Pr(x:  ac = y,t) + Pr(y:  LC = x,t) (8)The approximation continues withAssumpt ion  4 I f  xy is a valid NCC, then for allt E T,  either Pr(x, t) = 0 or Pr(y, t) = 0.Assumption 4 also implies that for all t E T, eitherPr(x : Re = y,t) = 0 (9)orPr(y:  LC = x,t) = 0.
(10)Under Assumptions 3 and 4, we can estimate i '(xy)as shown in Figure 2.The final form of Equation 5 (in Figure 2) allowsus to partition all the terms in Equation 4 into twosets, one for each of the components ofthe candidateNCC:Amy = ?m~y + ?me-y (11)where++- i (x )  (12)Pr(x : RC ~ y, t)E Pr(x : aC ~ y, t) log Pr(x, ac ~ y) Pr(t)tETPr(x : ac = y, t)EPr (x  : ac = Y't) l?g pr~ ; RC ~_ y i~r( t )tET?xe-y = -iCy) (13)Pr(y : LC ~ x, t)+ E Pr(y:  LC ?
X, t) log F r~,  LC" ~ x) Pr(t)tETPr(y : LC = x, t)+ E Pr(y:  LC = x, t) log P r~ .
: I\]C ~-- "x)?~(t)t~TAll the terms in Equation 12 depend only on theprobability distributions Pr(x, t), Pr(x : ac = y, t)and Pr(x : RC ?
y, t).
All the terms in Equation 13depend only on Pr(y,t), Pr(y : LC = x,t)and Pr(y : LC ?
x, t).
These distributions canbe computed efficiently by memory-external sortingand streamed accumulation.7 Bag-o f -Words  T rans la t ionIn bag-of-words translation, each word in the sourcetext is simply replaced with its most likely transla-tion.
No target language model is involved.
For thisapplication, it is sufficient o predict only the maxi-mum likelihood translation of each source word.
Therest of the translational distribution can be ignored.Let mT(s)  be the most likely translation of eachsource word s, according to the translation model:roT(s) = arg ma2?
Pr(s, t) (14)tE rAgain, I will write simply re(s) when T is clear fromthe context.
The objective function V for this ap-101plication follows by analogy with the mutual infor-mation function I in Equation 1:Pr(s,t)V(S; T) = ~ E ~(t, re(s)) Pr(s, t)log Pr(s) Pr(t)8E~ tETPr(s,m(s)) (15)= ~ Pr(s, re(s)) log Pr(s) Pr(m(s))sE8The Kronecker ~ function is equal to one when itsarguments are identical and zero otherwise.The form of the objective function again permitseasy distribution of its value over the s E S:Pr(s,m(s)) (16)vT"(s) = Pr(s, re(s))log Pr(s) Pr(m(s))"The formula for estimating the net change in theobjective function due to each candidate NCC re-mains the same:= ?
(=)  + ?
(y )  + v ' (xy)  - v(x)  - v(y) .
(17)It is easier to estimate the values of v' using only thebase translation model, than to estimate the valuesof i', since only the most likely translations need tobe considered, instead of entire translational distri-butions, v'(x) and v'(y) are again estimated underAssumption 2:v'(x) = v(x : Rc # y) (18)v'(y) = v(y: LC # x) (19)v~(xy) can be estimated without making the strongassumptions 3 and 4.
Instead, I use the weakerAssumption 5 Let t= and ty be the most frequenttranslations of x and y in each other's presence, inthe base translation model.
The most likely transla-tion of xy in the trial translation model will be themore frequent of t= and ty.Assumption 5 implies that?
(zy)  = max\[v(  : Rc = y),vCy : Lc = x)\].
(20)This quantity can be computed exactly at a reason-able computational expense.8 Exper imentsTo demonstrate he method's applicability to dataother than parallel texts, and to illustrate some ofits interesting properties, I describe my last exper-iment first.
I applied the mutual information ob-jective function and its associated predictive valuefunction to a data set consisting of spellings and pro-nunciations of 17381 English words.
Table 1 showsIteration Validated NCCs Example1 er father, herng hangch chat, schoolou court, couldes filesau augustgh laughth this, thinough though, through(none)sh shareio tensionph graph7 tio nationow know, howck stackea nearoo book, tooless dress9 ia partial, facial10 (none)Table 1: The NCCs of English orthography discov-ered by the algorithm.the NCCs of English spelling that the algorithm dis-covered on the first 10 iterations.
The table revealssome interesting behavior of the algorithm.
TheNCCs "er," "ng" and "ow" were validated becausethis data set represents he sounds usually producedby these letter combinations with one phoneme.
TheNCC "es" most often appears in word-final posi-tion, where the "e" is silent.
However, when "es" isnot word-final, the "e" is usually not silent, and themost frequent following letter is "s", which is whythe NCC "ess" was validated.
NCCs like "tio" and"ough" are built up over multiple iterations, some-times out of pairs of previously discovered NCCs.The other two experiments were carried outon transcripts of Canadian parliamentary debates,known as the Hansards.
French and English ver-sions of these texts were aligned by sentence usingthe method of Gale & Church (1991).
Morpholog-ical variants in both languages were stemmed to acanonical form.
Thirteen million words (in both lan-guages combined) were used for training and anothertwo and a half million were used for testing.
Alltranslation models were induced using the method ofMelamed (1997).
Six iterations of the NCC discov-ery algorithm were run in "two-sided" mode, usingthe objective function I, and five iterations were runusing the objective function V. Each iteration took102Iteration Bitext Vocabulary II Number of Number of ValidationNumber Size I\] Proposed NCCs Accepted NCCs Rate SideEnglish 29617French 31664English 29691French 31768English 29739French 318096476182532451612051051214941383316%20%19%17%24%16%Table 2: NCCs proposed and accepted, using the mutual information objective function I.IterationNumber12345BitextSideEnglishFrenchEnglishFrenchEnglishVocabularySize2961731664303333238430711Number ofProposed NCCs776758399355300Number ofAccepted NCCs758748388340286ValidationRate98%99%97%96%95%Table 3: NCCs proposed and accepted, using the simpler objective function V.approximately 78 hours on a 167MHz UltraSPARCprocessor, unning unoptimized Perl code.Tables 2 and 3 chart the NCC discovery process.The NCCs proposed for the V objective functionwere much more likely to be validated than thoseproposed for I, because the predictive value func-tion v ~ is much easier to estimate a priori than thepredictive value function iq In 3 iterations on theEnglish side of the bitext, 192 NCCs were validatedfor I and 1432 were validated for V. Of the 1432NCCs validated for V, 84 NCCs consisted of 3 words,3 consisted of 4 words and 2 consisted of 5 words.The French NCCs were longer on average, due tothe frequent "N de N" construction for noun com-pounds.The first experiment on the Hansards involved themutual information objective function I and its asso-ciated predictive value function in Equation 3.
Thefirst step in the experiment was the construction of5 new versions of the test data, in addition to theoriginal version.
Version k of the test data was con-structed by fusing all NCCs validated up to iterationk on the training data.
The second step was to in-duce a translation model from each version of thetest data.
There was no opportunity to measure theimpact of NCC recognition under the objective func-tion I on any real application, but Figure 3 showsthat the mutual information of successive t st trans-lation models rose as desired.The second experiment was based on the simplerobjective function V and its associated predictivevalue function in Equation 16.
The impact of NCC5.685.665.645.625.605.585.565.545.525.505.481iteration numberFigure 3: Mutual information of successive trans-lation models induced on held-out test data.
Natsare a measure of information like bits, but based onthe natural ogarithm.
Translation models that knowabout NCCs have higher mutual information thanthose that do not.recognition on the bag-of-words translation task wasmeasured directly, using Bitext-Based Lexicon Eval-uation (BIBLE: Melamed, 1995).
BIBLE is a fam-ily of evaluation algorithms for comparing differenttranslation methods objectively and automatically.The algorithms are based on the observation thatif translation method A is better than translationmethod B, and each method produces a translationfrom one half of a held-out test bitext, then the otherhalf of that bitext will be more similar to the trans-lation produced by A than to the translation pro-duced by B.
In the present experiment, the trans-1030.536 .................................................................. ~ ............. : ...............0.534 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.0.552 0.554 0.556 0.558 0.560 0.562 0.564Prec is ionFigure 4: English ~ French BIBLE scores for 6translation models.
Labels 0 to 5 indicate iterationnumber.0.5600.5560.558 .......................................................... : .................................. .
.
.
.
.
.
.
.
.
i ~ ~  ....................... 2 ......... .i................... i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
!50.554 ...... ~ ~ ,  .....0.552 .................................................................................................05500.
24 0.526 0.528 0.530 0.532 0.534Prec is ionFigure 5: French ~ English BIBLE scores for 6translation models.
Labels 0 to 5 indicate iterationnumber.lation method was always bag-of-words translation,but using different translation models.
The simi-larity of two texts was measured in terms of wordprecision and word recall in aligned sentence pairs,ignoring word order.I compared the 6 base translation models inducedin 6 iterations of the algorithm in Section 5.
5 Thefirst model is numbered 0, to indicate that it didnot recognize any NCCs.
The 6 translation modelswere evaluated on the test bitext (E, F) using thefollowing BIBLE algorithm:1.
Fuse all word sequences in E that correspondto NCCs recognized by the translation model.2.
Initialize the counters a and c to zero.3.
Let b be the number of words in F.5The entire algorithm was only run six times, butSteps 2 and 3 were run a seventh time.0.550Englisl~ -> French i -.-0.548 French ::-> English :: .
.
.
.
.
.
.
.
_ .
_~f~w~V77~~ 0.544 ....* ......................0 .542  ................... : ................ .,~ ....................
...~-.,..':::.
........... : ...................0.540 : i  ...........: iI 0.538 1 2 3 4 5IterationFigure 6: F-measures for BIBLE tests on successivetranslation models.4.
For each pair of aligned sentences(e, f)  E (E, F),(a) For each word s in e, add the most likelytranslation of s to the trial target sentence^f .
If themost likely translation is an NCC,then break it up into its components.
If s isnot in the translation model (an unknownword), then add s itself to f.(b) a = a + I\]1(c) For each word in f ,  check whether it occursin f. If so, increment he counter c andremove the word from f.65.
Precision := c/a.
Recall := c/b.The BIBLE algorithm compared the 6 models inboth directions of translation.
The results are de-tailed in Figures 4 and 5.
Figure 6 shows F-measuresthat are standard in the information retrieval iter-ature:2 * precision * recallF = (21)precision + recallThe absolute recall and precision values in these fig-ures are quite low, but this is not a reflection of thequality of the translation models.
Rather, it is an ex-pected outcome of BIBLE evaluation, which is quiteharsh.
Many translations are not word for word inreal bitexts and BIBLE does not even give credit forsynonyms.
The best possible performance on this6Removing words from f in Step 3(c) is necessary toensure that no target word gives credit to more thanone source word translation, and thereby to foil a simplemethod of cheating: If matched words in f are not re-moved, then a trivial translation model where all sourcewords translate to the most frequent target word wouldscore surprisingly high!
E.g.
a French to English trans-lation method that outputs "the the the the .
.
. "
wouldrecall more than 6% of English words.104kind of BIBLE evaluation has been estimated at 62%precision and 60% recall (Melamed, 1995).The purpose of BIBLE is internally valid compari-son, rather than externally valid benchmarking.
Ona sufficiently large test bitext, BIBLE can expose theslightest differences in translation quality.
The num-ber of NCCs validated on each iteration was nevermore than 2.5% of the vocabulary size.
Thus, thecurves in Figures 4 and 5 have a very small range,but the trends are clear.A qualitative assessment of the NCC discoverymethod can be made by looking at Table 4.
It con-tains a random sample of 50 of the English NCCsaccumulated in the first five iterations of the al-gorithm in Section 5, using the simpler objectivefunction V. All of the NCCs in the table are non-compositional with respect o the objective functionV.
Many of the NCCs, like "red tape" and "blazethe trail," are true idioms.
Some NCCs are incom-plete.
E.g.
"flow-" has not yet been recognized as anon-compositional part of "flow-through share," andlikewise for "head" in "rear its ugly head."
TheseNCCs would likely be completed if the algorithmwere allowed to run for more iterations.
Some of theother entries deserve more explanation.First, "Della Noce" is the last name of a Cana-dian Member of Parliament.
Every occurrence ofthis name in the French training text was tok-enized as "Della noce" with a lowercase "n," because"noce" is a common noun in French meaning "mar-riage," and the tokenization algorithm lowercasesall capitalized words that are found in the lexicon.When this word occurs in the French text without"Della," its English translation is "marriage," butwhen it occurs as part of the name, its translation is"Noce."
So, the French bigram "Della Noce" is non-compositional with respect o the objective functionV.
It was validated as an NCC.
On a subsequentiteration, the algorithm found that the English bi-gram "Della Noce" was always linked to one Frenchword, the NCC "Dellamoce," so it decided that theEnglish "Della Noce" must also be an NCC.
This isone of the few non-compositional personal names inthe Hansards.Another interesting entry in the table is the lastone.
The capitalized English words "Generic" and"Association" are translated with perfect consis-tency to "Generic" and "association," respectively,in the training text.
The translation of the middletwo words, however, is non-compositional.
When"Pharmaceutical" and "Industry" occur together,they are rendered in the French text without trans-lation as "Pharmaceutical Industry."
When theyoccur separately, they are translated into "pharma-ceutique" and "industrie."
Thus, the English bi-gram "Pharmaceutical Industry" is an NCC, but thewords that always occur around it are not part of theNCC.Similar reasoning applies to "ship unprocessedura-nium."
The bigram < ship, unprocessed > is anNCC because its components are translated non-compositionally whenever they co-occur.
However,"uranium" is always translated as "uranium," so itis not a part of the NCC.
This NCC demonstratesthat valid NCCs may cross the boundaries of gram-matical constituents.9 Re la ted  WorkIn their seminal work on statistical machine trans-lation, Brown et al (1993) implicitly accounted forNCCs in the target language by estimating "fertil-ity" distributions for words in the source language.A source word s with fertility n could generate asequence of n target words, if each word in the se-quence was also in the translational distribution ofs and the target language model assigned a suffi-ciently high probability to the sequence.
However,Brown et al's models do not account for NCCs inthe source language.
Recognition of source-languageNCCs would certainly improve the performance oftheir models, but Brown e~ al.
warn that.. .
one must be discriminating in choos-ing multi-word cepts.
The caution that wehave displayed thus far in limiting ourselvesto cepts with fewer than two words was mo-tivated primarily by our respect for the fea-tureless desert hat multi-word cepts offera priori.
(Brown et aL, 1993)The heuristics in Section 6 are designed specificallyto find the interesting features in that featurelessdesert.
Furthermore, translational equivalence re-lations involving explicit representations of target-language NCCs are more useful than fertility distri-butions for applications that do translation by tablelookup.Many authors (e.g.
Daille et al, 1994;Smadja et al, 1996) define "collocations" interms of monolingual frequency and part-of-speechpatterns.
Markedly high frequency is a necessaryproperty of NCCs, because otherwise they wouldfall out of  use.
However, at least for translation-related applications, it is not a sufficient property.Non-compositional translation cannot be detectedreliably without looking at translational distri-butions.
The deficiency of criteria that ignoretranslational distributions is illustrated by theirpropensity to validate most personal names as105"collocations."
At least among West Europeanlanguages, translations of the vast majority ofpersonal names are perfectly compositional.Several authors have used mutual information andsimilar statistics as an objective function for wordclustering (Dagan et al, 1993; Brown et al, 1992;Pereira et al, 1993; Wang et al, 1996), for au-tomatic determination f phonemic baseforms (Lu-cassen & Mercer, 1984), and for language modelingfor speech recognition (Ries ct al., 1996).
Althoughthe applications considered in this paper are differ-ent, the strategy is similar: search a space of datamodels for the one with maximum predictive power.Wang et al (1996) also employ parallel texts andindependence assumptions that are similar to thosedescribed in Section 6.
Like Brown et al (1992),they report a modest improvement in model per-plexity and encouraging qualitative results.
Unfor-tunately, their estimation method cannot proposemore than ten or so word-pair clusters before thetranslation model must be re-estimated.
Also, theparticular clustering method that they hoped to im-prove using parallel data is not very robust for lowfrequencies.
So, like Smadja et al, they were forcedto ignore all words that occur less than five times.
Ifappropriate objective functions and predictive valuefunctions can be found for these other tasks, thenthe method in this paper might be applied to them.There has been some research into matchingcompositional phrases across bitexts.
For example,Kupiec (1993) presented a method for finding trans-lations of whole noun phrases.
Wu (1995) showedhow to use an existing translation lexicon to popu-late a database of "phrasal correspondences" for usein example-based MT.
These compositional transla-tion patterns enable more sophisticated approachesto MT.
However, they are only useful if they can bediscovered reliably and efficiently.
Their time maycome when we have a better understanding of howto model the human translation process.10 Conclus ionIt is well known that two languages are moreinformative than one (Dagan et al, 1991).
Ihave argued that texts in two languages are notonly preferable but necessary for discovery of non-compositional compounds for translation-related ap-plications.
Given a method for constructing statis-tical translation models, NCCs can be discovered bymaximizing the models' information-theoretic pre-dictive value over parallel data sets.
This paperpresented an efficient algorithm for such ontologi-cal discovery.
Proper recognition of NCCs resultedin improved performance on a simple MT task.Lists of NCCs derived from parallel data may beuseful for NLP applications that do not involve par-allel data.
Translation-oriented NCC lists can beused directly in applications that have a human inthe loop, such as computer-assisted l xicography,computer-assisted language l arning, and corpus lin-guistics.
To the extent that translation-orienteddefinitions of compositionality overlap with otherdefinitions, NCC lists derived from parallel datamay benefit other applications where NCCs play arole, such as information retrieval (Evans & Zhai,1996) and language modeling for speech recognition(Ries et al, 1996).
To the extent hat different appli-cations have different objective functions, optimizingthese functions can benefit from an understandingof how they differ.
The present work was a steptowards such understanding, because "an explica-tion of a monolingual idiom might best be given af-ter bilingual idioms have been properly understood"(Bar-Hillel, 1964, p. 48).The NCC discovery method makes few assump-tions about the data sets from which the statisticaltranslation models are induced.
As demonstratedin Section 8, the method can find NCCs in Englishletter strings that are aligned with their phoneticrepresentations.
We hope to use this method to dis-cover NCCs in other kinds of parallel data.
A natu-ral next target is bitexts involving Asian languages.Perhaps the method presented here, combined withan appropriate translation model, can make someprogress on the word identification problem for lan-guages like Chinese and Japanese.106Count7861837963363424231717161411101010NCC (in italics) in typical context non-compositional translation in French textcould haveflow-through sharesI repeatthe case I just mentionedtax basesingle parent familyperform < GAP > dutyred tapemiddle of the nightDella Noceheating oilproceeds of crimerat packurban dwellersnuclear generating stationAir India disaster9 Ottawa River8 I dare hope8 Ottawa Valley7 plea bargaining7 manifestly unfounded claims7 machine gun7 a group called Rural Dignity6 a slight bit6 cry for help5 video tape5 sow the seed5 arrange a meeting4 shot-gun wedding4 we lag behind4 Great West Life Company4 Canadian Forces Base and cease negotiations3 severe sentence3 rear its ugly head3 inability to deal effectively with3 en masse3 create a disturbance3 blaze the trail2 wrongful conviction2 weak sister2 of both the users and providers of transportation2 understand the motivation2 swimming pool2 ship unprocessed uranium2 by reason of insanity2 l'agence de Presse libre du QuEbec2 do cold weather research2 the bread basket of the nation2 turn back the boatload of European Jews2 Generic Pharmaceutical Industry Associationpourraitactions accrgditivesje tiens ~ direle casque je viens de mentionnerassiette fiscalefamille monoparentaleassumer .. .
fonctionla paperasserieen pleine nuitDella noce (see text for explanation)mazoutles produits tirds du crimemeutecitadinscentrale nucl~airedcrasement de l'avion indienOutaouaisj'ose croirevall~e de l'Outaouaismarchandageavoir revendiqud ~i tort le statutmitrailleuseune groupe appel~ Rural Dignityla moindreappel au secourvideosemerorganiser un entretienmariage forcenous trainions de la patteGreat West Life Companymettre fin et interrompre l n~gociations~v~re sanctionmanifestsne sait pas traiter de mani~re fficace avecen blocsuscite de perturbationouvre la voieerreur judiciaireparent pauvredes utilisateurs et des transporteurssaisir le motifpiscineexp~dier de l'uranium non raffin~pour cause d'ali~nation mentalel'agence de Presse libre du Qudbec~tudier l'effet du froidle grenier du Canadarenvoyer tout ces juifs europ~ensGeneric Pharmaceutical Industry AssociationTable 4: Random sample of 50 of the English NCCs validated in the first five iterations of the NCC discoveryalgorithm, using the objective function V. "Count" is the number of times the NCC occurs in the trainingtext.
All the NCCs are non-compositional with respect o the objective function V.107ReferencesY.
Bar-Hillel.
(1964) Language and Information.Addison-Wesley: Reading, MA.P.
Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai, R. L. Mercer.
(1992) "Class-Based n-gram Models of Natural Language," Computa-tional Linguistics 8(4).P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra& R. L. Mercer.
(1993) "The Mathematics ofSta-tistical Machine Translation: Parameter Estima-tion," Computational Linguistics 19(2).K.
W. Church & P. Hanks.
(1989) "Word-Association Norms, Mutual Information and Lex-icography," Proceedings of the 27th Annual Meet-ing of the Association for Computational Linguis-tics.
Vancouver, BC.T.
M. Cover & J.
A. Thomas.
(1991) Elements of In-formation Theory.
John Wiley & Sons: New York,NY.I.
Dagan, A. Itai & U. Schwall.
(1991) "Two Lan-guages are More Informative than One," Proceed-ings of the 29th Annual Meeting of the Associationfor Computational Linguistics.
Berkeley, CA.I.
Dagan, S. Marcus & S. Markovitch.
(1993) "Con-textual Word Similarity and Estimation fromSparse Data," Proceedings of the 31st AnnualMeeting of the Association for ComputationalLinguistics.
Columbus, OH.B.
Daille, l~.
Gaussier & J.-M. Lang@.
(1994) "To-wards Automatic Extraction of Monolingual andBilingual Terminology," Proceedings of the 15thInternational Conference on Computational Lin-guistics.
Kyoto, Japan.D.
A. Evans & C. Zhai.
(1996) "Noun-Phrase Anal-ysis in Unrestricted Text for Information Re-trieval," Proceedings of the 34th Annual Meetingof the Association for Computational Linguistics.Santa Cruz, CA.P.
Fung & D. Wu.
(1994) "Statistical Augmenta-tion of a Chinese Machine-Readable Dictionary,"Proceedings of the 2nd Workshop on Very LargeCorpora.
Columbus, OH.W.
Gale, & K. W. Church.
(1991) "A Program forAligning Sentences in Bilingual Corpora" Proceed-ings of the 29th Annual Meeting of the Associationfor Computational Linguistics.
Berkeley, CA.J.
Kupiec.
(1993) "An Algorithm for Finding NounPhrase Correspondences in Bilingual Corpora,"Proceedings of the 31st Annual Meeting of the As-sociation for Computational Linguistics.
Colum-bus, OH.J.
M. Lucassen & R. L. Mercer.
(1984) "AnInformation-Theoretic Approach to the Auto-matic Determination of Phonemic Baseforms,"Proceedings of the IEEE International Confer-ence on Acoustics, Speech and Signal Processing.San Diego, CA.I.
D. Melamed (1995) "Automatic Evaluation andUniform Filter Cascades for Inducing N-bestTranslation Lexicons," Proceedings of the ThirdWorkshop on Very Large Corpora.
Boston, MA.I.
D. Melamed.
(1997) "A Word-to-Word Modelof Translational Equivalence," Proceedings of the35th Conference of the Association for Computa-tional Linguistics.
Madrid, Spain.F.
Pereira, N. Tishby & L. Lee.
(1993) "Distribu-tional Clustering of English Words," Proceedingsof the 31st Annual Meeting of the Association forComputational Linguistics.
Columbus, OH.D.
W. Oard & B. J. Dorr.
(1996) "A Survey of Multi-lingual Text Retrieval," UMIACS TR-96-19.
Uni-versity of Maryland: College Park, MD.K.
Ries, F. D. Buo & A. Waibel.
(1996) "ClassPhrase Models for Language Modeling," Proceed-ings of the Fourth International Conference onSpoken Language Processing.
Philadelphia, PA.F.
Smadja, K. R. McKeown & V.
Hatzivassiloglou.
(1996) "Translating Collocations for BilingualLexicons: A Statistical Approach," Computa-tional Linguistics 22(1).R.
Sproat, C. Shih, W. Gale & N. Chang.
(1996) "AStochastic Finite-State Word-Segmentation Algo-rithm for Chinese," Computational Linguistics22(3):377-404.Y.
Wang, J. Lafferty & A. Waibel.
(1996)"Word Clustering with Parallel Spoken LanguageCorpora," Proceedings of the Fourth Interna-tional Conference on Spoken Language Processing.Philadelphia, PA.D.
Wu.
(1995) "Grammarless Extraction of PhrasalTranslation Examples from Parallel Texts," Pro-ceedings of the Sixth International Conference onTheoretical and Methodological Issues in MachineTranslation.
Leuven, Belgium.108
