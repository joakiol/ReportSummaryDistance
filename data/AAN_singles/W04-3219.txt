Monolingual Machine Translation for Paraphrase GenerationChris QUIRK,  Chris BROCKETT  and  William DOLANNatural Language Processing GroupMicrosoft ResearchOne Microsoft WayRedmond, WA  90852  USA{chrisq,chrisbkt,billdol}@microsoft.comAbstractWe apply statistical machine translation(SMT) tools to generate novel paraphrasesof input sentences in the same language.The system is trained on large volumes ofsentence pairs automatically extracted fromclustered news articles available on theWorld Wide Web.
Alignment Error Rate(AER) is measured to gauge the quality ofthe resulting corpus.
A monotone phrasaldecoder generates contextual replacements.Human evaluation shows that this systemoutperforms baseline paraphrase generationtechniques and, in a departure from previ-ous work, offers better coverage and scal-ability than the current best-of-breedparaphrasing approaches.1 IntroductionThe ability to categorize distinct word sequencesas ?meaning the same thing?
is vital to applicationsas diverse as search, summarization, dialog, andquestion answering.
Recent research has treatedparaphrase acquisition and generation as a machinelearning problem (Barzilay & McKeown, 2001;Lin & Pantel, 2002; Shinyama et al 2002, Barzilay& Lee, 2003, Pang et al, 2003).
We approach thisproblem as one of statistical machine translation(SMT), within the noisy channel model of Brownet al (1993).
That is, we seek to identify the opti-mal paraphrase T* of a sentence S by finding:( ){ }{ })P()|P(maxarg|Pmaxarg*TTSSTTTT==T and S being sentences in the same language.We describe and evaluate an SMT-based para-phrase generation system that utilizes a monotonephrasal decoder to generate meaning-preservingparaphrases across multiple domains.
By adoptingat the outset a paradigm geared toward generatingsentences, this approach overcomes many prob-lems encountered by task-specific approaches.
Inparticular, we show that SMT techniques can beextended to paraphrase given sufficient monolin-gual parallel data.1 We show that a huge corpus ofcomparable and alignable sentence pairs can beculled from ready-made topical/temporal clustersof news articles gathered on a daily basis fromthousands of sources on the World Wide Web,thereby permitting the system to operate outsidethe narrow domains typical of existing systems.2 Related workUntil recently, efforts in paraphrase were notstrongly focused on generation and relied primarilyon narrow data sources.
One data source has beenmultiple translations of classic literary works (Bar-zilay & McKeown 2001; Ibrahim 2002; Ibrahim etal.
2003).
Pang et al (2003) obtain parallel mono-lingual texts from a set of 100 multiply-translatednews articles.
While translation-based approachesto obtaining data do address the problem of how toidentify two strings as meaning the same thing,they are limited in scalability owing to the diffi-culty (and expense) of obtaining large quantities ofmultiply-translated source documents.Other researchers have sought to identify pat-terns in large unannotated monolingual corpora.Lin & Pantel (2002) derive inference rules by pars-ing text fragments and extracting semanticallysimilar paths.
Shinyama et al (2002) identify de-pendency paths in two collections of newspaperarticles.
In each case, however, the informationextracted is limited to a small set of patterns.Barzilay & Lee (2003) exploit the meta-information implicit in dual collections of news-1Barzilay & McKeown (2001), for example, reject theidea owing to the noisy, comparable nature of their data.wire articles, but focus on learning sentence-levelpatterns that provide a basis for generation.
Multi-sequence alignment (MSA) is used to identify sen-tences that share formal (and presumably semantic)properties.
This yields a set of clusters, each char-acterized by a word lattice that captures n-gram-based structural similarities between sentences.Lattices are in turn mapped to templates that canbe used to produce novel transforms of input sen-tences.
Their methodology provides striking resultswithin a limited domain characterized by a highfrequency of stereotypical sentence types.
How-ever, as we show below, the approach may be oflimited generality, even within the training domain.3 Data collectionOur training corpus, like those of Shinyama etal.
and Barzilay & Lee, consists of different newsstories reporting the same event.
While previouswork with comparable news corpora has been lim-ited to just two news sources, we set out to harnessthe ongoing explosion in internet news coverage.Thousands of news sources worldwide are compet-ing to cover the same stories, in real time.
Despitedifferent authorship, these stories cover the sameevents and therefore have significant content over-lap, especially in reports of the basic facts.
In othercases, news agencies introduce minor edits into asingle original AP or Reuters story.
We believethat our work constitutes the first to attempt to ex-ploit these massively multiple data sources forparaphrase learning and generation.3.1 Gathering aligned sentence pairsWe began by identifying sets of pre-clusteredURLs that point to news articles on the Web, gath-ered from publicly available sites such ashttp://news.yahoo.com/, http://news.google.comand http://uk.newsbot.msn.com.
Their clusteringalgorithms appear to consider the full text of eachnews article, in addition to temporal cues, to pro-duce sets of topically/temporally related articles.Story content is captured by downloading theHTML and isolating the textual content.
A super-vised HMM was trained to distinguish story con-tent from surrounding advertisements, etc.2Over the course of about 8 months, we collected11,162 clusters, comprising 177,095 articles andaveraging 15.8 articles per cluster.
The quality of2We hand-tagged 1,150 articles to indicate which por-tions of the text were story content and which were ad-vertisements, image captions, or other unwantedmaterial.
We evaluated several classifiers on a 70/30test train split and found that an HMM trained on ahandful of features was most effective in identifyingcontent lines (95% F-measure).these clusters is generally good.
Impressionisti-cally, discrete events like sudden disasters, busi-ness announcements, and deaths tend to yieldtightly focused clusters, while ongoing stories likethe SARS crisis tend to produce very large andunfocused clusters.To extract likely paraphrase sentence pairs fromthese clusters, we used edit distance (Levenshtein1966) over words, comparing all sentences pair-wise within a cluster to find the minimal number ofword insertions and deletions transforming the firstsentence into the second.
Each sentence was nor-malized to lower case, and the pairs were filteredto reject:?
Sentence pairs where the sentences wereidentical or differed only in punctuation;?
Duplicate sentence pairs;?
Sentence pairs with significantly differentlengths (the shorter is less than two-thirdsthe length of the longer);?
Sentence pairs where the Levenshtein dis-tance was greater than 12.0.3A total of 139K non-identical sentence pairs wereobtained.
Mean Levenshtein distance was 5.17;mean sentence length was 18.6 words.3.2 Word alignmentTo this corpus we applied the word alignmentalgorithms available in Giza++ (Och & Ney,2000), a freely available implementation of IBMModels 1-5 (Brown, 1993) and the HMM align-ment (Vogel et al 1996), along with various im-provements and modifications motivated byexperimentation by Och & Ney (2000).
In order tocapture the many-to-many alignments that identifycorrespondences between idioms and other phrasalchunks, we align in the forward direction and againin the backward direction, heuristically recombin-ing each unidirectional word alignment into a sin-gle bidirectional alignment (Och & Ney 2000).Figure 1 shows an example of a monolingualalignment produced by Giza++.
Each line repre-sents a uni-directional link; directionality is indi-cated by a tick mark on the target side of the link.We held out a set of news clusters from ourtraining data and extracted a set of 250 sentencepairs for blind evaluation.
Randomly extracted onthe basis of an edit distance of 5 ?
n ?
20 (to allowa range of reasonably divergent candidate pairswhile eliminating the most trivial substitutions),the gold-standard sentence pairs were checked byan independent human evaluator to ensure that3Chosen on the basis of ablation experiments and opti-mal AER (discussed in 3.2).they contained paraphrases before they were handword-aligned.To evaluate the alignments, we adhered to thestandards established in Melamed (2001) and Och& Ney (2000, 2003).
Following Och & Ney?smethodology, two annotators each created an ini-tial annotation for each dataset, subcategorizingalignments as either SURE (necessary) or POSSIBLE(allowed, but not required).
Differences were high-lighted and the annotators were asked to reviewtheir choices on these differences.
Finally we com-bined the two annotations into a single gold stan-dard: if both annotators agreed that an alignmentshould be SURE, then the alignment was marked asSURE in the gold-standard; otherwise the alignmentwas marked as POSSIBLE.To compute Precision, Recall, and AlignmentError Rate (AER) for the twin datasets, we usedexactly the formulae listed in Och & Ney (2003).Let A be the set of alignments in the comparison, Sbe the set of SURE alignments in the gold standard,and P be the union of the SURE and POSSIBLEalignments in the gold standard.
Then we have:||||precisionAPA ?=  ||||recallSSA ?=||||AERSASAPA+?+?=Measured in terms of AER4, final interrater agree-ment between the two annotators on the 250 sen-tences was 93.1%.4The formula for AER given here and in Och  & Ney(2003) is intended to compare an automatic alignmentagainst a gold standard alignment.
However, when com-paring one human against another, both comparison andreference distinguish between SURE and POSSIBLE links.Because the AER is asymmetric (though each directionTable 1 shows the results of evaluating align-ment after trainng the Giza++ model.
Although theoverall AER of 11.58% is higher than the best bi-lingual MT systems (Och & Ney, 2003), the train-ing data is inherently noisy, having more incommon with analogous corpora than conventionalMT parallel corpora in that the paraphrases are notconstrained by the source text structure.
The iden-tical word AER of 10.57% is unsurprising giventhat the domain is unrestricted and the alignmentalgorithm does not employ direct string matchingto leverage word identity.5 The non-identical wordAER of 20.88% may appear problematic in a sys-tem that aims to generate paraphrases; as we shallsee, however, this turns out not to be the case.
Ab-lation experiments, not described here, indicatethat additional data will improve AER.3.3 Identifying phrasal replacementsRecent work in SMT has shown that simplephrase-based MT systems can outperform moresophisticated word-based systems (e.g.
Koehn etal.
2003).
Therefore, we adopt a phrasal decoderpatterned closely after that of Vogel et al (2003).We view the source and target sentences S and Tas word sequences s1..sm and t1..tn.
A word align-ment A of S and T can be expressed as a functionfrom each of the source and target tokens to aunique cept (Brown et al 1993); isomorphically, acept represents an aligned subset of the source andtarget tokens.
Then, for a given sentence pair andword alignment, we define a phrase pair as a sub-set of the cepts in which both the source and targettokens are contiguous.
6  We gathered all phrasediffers by less than 5%), we have presented the averageof the directional AERs.5However, following SMT practice of augmenting datawith a bilingual lexicon, we did append an identity lexi-con to the training data.6While this does preclude the usage of ?gapped?
phrasepairs such as or ?
either ?
or, we found such map-Training Data Type: L12Precision   87.46%Recall      89.52%AER         11.58%Identical word precision   89.36%Identical word recall      89.50%Identical word AER         10.57%Non-identical word preci-sion   76.99%Non-identical word recall      90.22%Non-identical word AER     20.88%Table 1.
AER on the Lev12 corpusFigure 1.
An example Giza++ alignmentpairs (limited to those containing no more than fivecepts, for reasons of computational efficiency) oc-curring in at least one aligned sentence somewherein our training corpus into a single replacementdatabase.
This database of lexicalized phrase pairs,termed phrasal replacements, serves as the back-bone of our channel model.As in (Vogel et al 2003), we assigned probabili-ties to these phrasal replacements via IBM Model1.
In more detail, we first gathered lexical transla-tion probabilities of the form P(s | t) by runningfive iterations of Model 1 on the training corpus.This allows for computing the probability of a se-quence of source words S given a sequence of tar-get words T as the sum over all possiblealignments of the Model 1 probabilities:( ) ( )( )????
?==Tt SsAtsPTASPTSP||,|(Brown et al (1993) provides a more detailed deri-vation of this identity.)
Although simple, this ap-proach has proven effective in SMT for severalreasons.
First and foremost, phrasal scoring byModel 1 avoids the sparsity problems associatedwith estimating each phrasal replacement probabil-ity with MLE (Vogel et al 2003).
Secondly, it ap-pears to boost translation quality in moresophisticated translation systems by inducing lexi-cal triggering (Och et al 2004).
Collocations andother non-compositional phrases receive a higherprobability as a whole than they would as inde-pendent single word replacements.One further simplification was made.
Given thatour domain is restricted to the generation of mono-lingual paraphrase, interesting output can be pro-duced without tackling the difficult problem ofinter-phrase reordering.7 Therefore, along the linesof Tillmann et al (1997), we rely on only mono-tone phrasal alignments, although we do allow in-tra-phrasal reordering.
While this means certaincommon structural alternations (e.g., ac-tive/passive) cannot be generated, we are still ableto express a broad range of phenomena:pings to be both unwieldy in practice and very oftenindicative of poor a word alignment.7Even in the realm of MT, such an assumption can pro-duce competitive results (Vogel et al 2003).
In addi-tion, we were hesitant to incur the exponential increasein running time associated with those movement modelsin the tradition of Brown el al (1993), especially sincethese offset models fail to capture important linguisticgeneralizations (e.g., phrasal coherence, headedness).?
Synonymy: injured ?
wounded?
Phrasal replacements: Bush administration?
White House?
Intra-phrasal reorderings: margin of error?
error marginOur channel model, then, is determined solelyby the phrasal replacements involved.
We first as-sume a monotone decomposition of the sentencepair into phrase pairs (considering all phrasal de-compositions equally likely), and the probabilityP(S | T) is then defined as the product of the eachphrasal replacement probability.The target language model was a trigram modelusing interpolated Kneser-Ney smoothing (Kneser& Ney 1995), trained over all 1.4 million sentences(24 million words) in our news corpus.3.4 Generating paraphrasesTo generate paraphrases of a given input, a stan-dard SMT decoding approach was used; this is de-scribed in more detail below.
Prior to decoding,however, the input sentence underwent preprocess-ing: text was lowercased, tokenized, and a fewclasses of named-entities were identified usingregular expressions.To begin the decoding process, we first con-structed a lattice of all possible paraphrases of thesource sentence based on our phrasal translationdatabase.
Figure 2 presents an example.
The latticewas realized as a set of |S| + 1 vertices v0..v|S| and aset of edges between those vertices; each edge waslabeled with a sequence of words and a real num-ber.
Thus a edge connecting vertex vi to vj labeledwith the sequence of words w1..wk and the realnumber p indicates that the source words si+1 to sjcan be replaced by words w1..wk with probability p.Our replacement database was stored as a trie withwords as edges, hence populating the lattice takesworst case O(n2) time.
Finally, since source andtarget languages are identical, we added an identitymapping for each source word si: an edge from vi-1to vi with label si and a uniform probability u. Thisallows for handling unseen words.
A high u valuepermits more conservative paraphrases.We found the optimal path through the lattice asscored by the product of the replacement modeland the trigram language model.
This algorithmreduces easily to the Viterbi algorithm; such a dy-namic programming approach guarantees an effi-cient optimal search (worst case O(kn), where n isthe maximal target length and k is the maximalnumber of replacements for any word).
In addition,fast algorithms exist for computing the n-best listsover a lattice (Soong & Huang 1991).Finally the resultant paraphrases were cleanedup in a post-processing phase to ensure output wasnot trivially distinguishable from other systemsduring human evaluation.
All generic named entitytokens were re-instantiated with their source val-ues, and case was restored using a model like thatused in Vita et al (2003).3.5 Alternate approachesBarzilay &  Lee (2003) have released a commondataset that provides a basis for comparing differ-ent paraphrase generation systems.
It consists of 59sentences regarding acts of violence in the MiddleEast.
These are accompanied by paraphrases gen-erated by their Multi-Sequence Alignment (MSA)system and a baseline employing WordNet (Fell-baum 1998), along with human judgments for eachoutput by 2-3 raters.The MSA WordNet baseline was created by se-lecting a subset of the words in each test sen-tence?proportional to the number of wordsreplaced by MSA in the same sentence?and re-placing each with an arbitrary word from its mostfrequent WordNet synset.Since our SMT approach depends quite heavilyon a target language model, we presented an alter-nate WordNet baseline using a target languagemodel.8 In combination with the language modeldescribed in section 3.4, we used a very simplereplacement model: each appropriately inflectedmember of the most frequent synset was proposedas a possible replacement with uniform probability.This was intended to isolate the contribution of thelanguage model from the replacement model.Given that our alignments, while aggregated intophrases, are fundamentally word-aligned, onequestion that arises is whether the information welearn is different in character than that learned8In contrast, Barzilay and Lee (2003) avoided using alanguage model for essentially the same reason: theirMSA approach did not take advantage of such a re-source.from much simpler techniques.
To explore thishypothesis, we introduced an additional baselinethat used statistical clustering to produce an auto-mated, unsupervised synonym list, again with atrigram language model.
We used standard bigramclustering techniques (Goodman 2002) to produce4,096 clusters of our 65,225 vocabulary items.4 EvaluationWe have experimented with several methods forextracting a parallel sentence-aligned corpus fromnews clusters using word alignment error rate, orAER, (Och & Ney 2003) as an evaluation metric.A brief summary of these experiments is providedin Table 1.
To evaluate the quality of generation,we followed the lead of Barzilay & Lee (2003).We started with the 59 sentences and correspond-ing paraphrases from MSA and WordNet (desig-nated as WN below).
Since the size of this data setmade it difficult to obtain statistically significantresults, we also included 141 randomly selectedsentences from held-out clusters.
We then pro-duced paraphrases with each of the following sys-tems and compared them with MSA and WN:?
WN+LM: WordNet with a trigram LM?
CL: Statistical clusters with a trigram LM?
PR: The top 5 sentence rewrites produced byPhrasal Replacement.For the sake of consistency, we did not use thejudgments provided by Barzilay and Lee; insteadwe had two raters judge whether the output fromeach system was a paraphrase of the input sen-tence.
The raters were presented with an input sen-tence and an output paraphrase from each systemin random order to prevent bias toward any par-ticular judgment.
Since, on our first pass, we foundinter-rater agreement to be somewhat low (84%),we asked the raters to make a second pass of judg-ments on those where they disagreed; this signifi-cantly improved agreement (96.9%).
The results ofthis final evaluation are summarized in Table 2.Figure 2.
A simplified generation lattice: 44 top ranked edges from a total 4,1405 AnalysisTable 2 shows that PR can produce rewordingsthat are evaluated as plausible paraphrases morefrequently than those generated by either baselinetechniques or MSA.
The WordNet baseline per-forms quite poorly, even in combination with atrigram language model: the language model doesnot contribute significantly to resolving lexicalselection.
The performance of CL is likewiseabysmal?again a language model does nothing tohelp.
The poor performance of these synonym-based techniques indicates that they have littlevalue except as a baseline.The PR model generates plausible paraphrasesfor the overwhelming majority of test sentences,indicating that even the relatively high AER fornon-identical words is not an obstacle to successfulgeneration.
Moreover, PR was able to generate aparaphrase for all 200 sentences (including the 59MSA examples).
The correlation between accept-ability and PR sentence rank validates both theranking algorithm and the evaluation methodology.In Table 2, the PR model scores significantlybetter than MSA in terms of the percentage ofparaphrase candidates accepted by raters.
More-over, PR generates at least five (and often hun-dreds more) distinct paraphrases for each testsentence.
Such perfect coverage on this dataset isperhaps fortuitous, but is nonetheless indicative ofscalability.
By contrast Barzilay & Lee (2003) re-port being able to generate paraphrases for only 59out of 484 sentences in their training (test?)
set, atotal of 12%.One potential concern is that PR paraphrasesusually involve simple substitutions of words andshort phrases (a mean edit distance of 2.9 on thetop ranked sentences), whereas MSA outputs morecomplex paraphrases (reflected in a mean edit dis-tance of 25.8).
This is reflected in Table 3, whichprovides a breakdown of four dimensions of inter-est, as provided by one of our independent evalua-tors.
Some 47% of MSA paraphrases involvesignificant reordering, such as an active-passivealternation, whereas the monotone PR decoderprecludes anything other than minor transpositionswithin phrasal replacements.Should these facts be interpreted to mean thatMSA, with its more dramatic rewrites, is ulti-mately more ambitious than PR?
We believe thatthe opposite is true.
A close look at MSA suggeststhat it is similar in spirit to example-based machinetranslation techniques that rely on pairing entiresentences in source and target languages, with thetranslation step limited to local adjustments of thetarget sentence (e.g.
Sumita 2001).
When an inputsentence closely matches a template, results can bestunning.
However, MSA achieves its richness ofsubstitution at the cost of generality.
Inspectionreveals that 15 of the 59 MSA paraphrases, or25.4%, are based on a single high-frequency, do-main-specific template (essentially a running tallyof deaths in the Israeli-Palestinian conflict).
Unlessone is prepared to assume that similar templatescan be found for most sentence types, scalabilityand domain extensibility appear beyond the reachof MSA.In addition, since MSA templates pair entiresentences, the technique can produce semanticallydifferent output when there is a mismatch in in-formation content among template training sen-tences.
Consider the third and fourth rows of Table3, which indicate the extent of embellishment andlossiness found in MSA paraphrases and the top-ranked PR paraphrases.
Particularly noteworthy isthe lossiness of MSA seen in row 4.
Figure 3 illus-trates a case where the MSA paraphrase yields asignificant reduction in information, while PR ismore conservative in its replacements.While the substitutions obtained by the PRmodel remain for the present relatively modest,they are not trivial.
Changing a single content wordis a legitimate form of paraphrase, and the abilityto paraphrase across an arbitrarily large sentenceset and arbitrary domains is a desideratum of para-phrase research.
We have demonstrated that theSMT-motivated PR method is capable of generat-ing acceptable paraphrases for the overwhelmingmajority of sentences in a broad domain.Method B&L59 B&L59 + 141PR #1 54 / 59 = 91.5% 177 / 200 = 89.5%PR #2 53 / 59 = 89.8% 168 / 200 = 84.0%PR #3 46 / 59 = 78.0% 164 / 200 = 82.0%PR #4 49 / 59 = 83.1% 163 / 200 = 81.5%MSA 46 / 59 = 78.0% 46 /   59 = 78.0%PR #5 44 / 59 = 74.6% 155 / 200 = 77.5%WN 23 / 59 = 39.0% 25 /   59 = 37.9%WN+LM 30 / 59 = 50.9% 53 / 200 = 27.5%CL  14 / 59 = 23.7% 26 / 200 = 13.0%Table 2.
Human acceptability judgmentsMSA PR#1Rearrangement 28 / 59 = 47% 0 / 100 =   0%Phrasal alternation 11 / 59 = 19% 3 / 100 =   3%Info added 19 / 59 = 32% 6 / 100 =   6%Info lost 43 / 59 = 73% 31 / 100 = 31%Table 3.
Qualitative analysis of paraphrases6 Future workMuch work obviously remains to be done.
Ourresults remain constrained by data sparsity, despitethe large initial training sets.
One major agendaitem therefore will be acquisition of larger (andmore diverse) data sets.
In addition to obtaininggreater absolute quantities of data in the form ofclustered articles, we also seek to extract alignedsentence pairs that instantiate a richer set of phe-nomena.
Relying on edit distance to identify likelyparaphrases has the unfortunate result of excludinginteresting sentence pairs that are similar in mean-ing though different in form.
For example:The Cassini spacecraft, which is en route to Saturn,is about to make a close pass of the ringedplanet's mysterious moon PhoebeOn its way to an extended mission at Saturn, theCassini probe on Friday makes its closest ren-dezvous with Saturn's dark moon Phoebe.We are currently experimenting with data extractedfrom the first two sentences in each article, whichby journalistic convention tend to summarize con-tent (Dolan et al 2004).
While noisier than the editdistance data, initial results suggest that these canbe a rich source of information about larger phrasalsubstitutions and syntactic reordering.Although we have not attempted to address theissue of paraphrase identification here, we are cur-rently exploring machine learning techniques,based in part on features of document structure andother linguistic features that should allow us tobootstrap initial alignments to develop more data.This will we hope, eventually allow us to addresssuch issues as paraphrase identification for IR.To exploit richer data sets, we will also seek toaddress the monotone limitation of our decoderthat further limits the complexity of our paraphraseoutput.
We will be experimenting with more so-phisticated decoder models designed to handle re-ordering and mappings to discontinuous elements.We also plan to pursue better (automated) metricsfor paraphrase evaluation.7 ConclusionsWe presented a novel approach to the problemof generating sentence-level paraphrases in a broadsemantic domain.
We accomplished this by usingmethods from the field of SMT, which is orientedtoward learning and generating exactly the sorts ofalternations encountered in monolingual para-phrase.
We showed that this approach can be usedto generate paraphrases that are preferred by hu-mans to sentence-level paraphrases produced byother techniques.
While the alternations our systemproduces are currently limited in character, thefield of SMT offers a host of possible enhance-ments?including reordering models?affording anatural path for future improvements.A second important contribution of this work isa method for building and tracking the quality oflarge, alignable monolingual corpora from struc-tured news data on the Web.
In the past, the lack ofsuch a data source has hampered paraphrase re-search; our approach removes this obstacle.AcknowledgementsWe are grateful to Mo Corston-Oliver, Jeff Ste-venson, Amy Muia, and Orin Hargraves of theButler Hill Group for their work in annotating thedata used in the experiments.
This paper has alsobenefited from discussions with Ken Church, MarkJohnson, and Steve Richardson.
We greatly appre-ciate the careful comments of three anonymousreviewers.
We remain, however, solely responsiblefor this content.ReferencesR.
Barzilay and K. R. McKeown.
2001.
Extracting Para-phrases from a parallel corpus.
In Proceedings of theACL/EACL.R.
Barzilay and L. Lee.
2003.
Learning to Paraphrase;an unsupervised approach using multiple-sequencealignment.
In Proceedings of HLT/NAACL.P.
Brown, S. A. Della Pietra, V. J. Della Pietra and R. L.Mercer.
1993.
The Mathematics of Statistical Ma-chine Translation.
Computational Linguistics 19(2):263-311.W.
Dolan, C. Quirk and C. Brockett.
2004.
Unsuper-vised Construction of Large Paraphrase Corpora: Ex-ploiting Massively Parallel News Sources.
To appearin Proceedings of COLING-2004.C.
Fellbaum, ed.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA.J.
Goodman.
2002.
JCLUSTER.
Software available athttp://research.microsoft.com/~joshuago/A.
Ibrahim.
2002.
Extracting Paraphrases from AlignedCorpora.
Master of Engineering Thesis, MIT.A.
Ibrahim, B. Katz, and J. Lin.
2003.
Extracting Struc-tural Paraphrases from Aligned Monolingual Cor-pora.
In Proceedings of the Second InternationalWorkshop on Paraphrasing (IWP 2003).
Sapporo,Japan.R.
Kneser and H. Ney.
1995.
Improved backing-off forN-gram language modeling.
In Proc.
Int.
Conf.
onAcoustics, Speech and Signal Processing: 181-184.Detroit, MI.P.
Koehn, F. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
In Proceedings ofHLT/NAACL.V.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
SovietPhysice-Doklady 10: 707-710.D.
Lin and P. Pantel.
2001.
DIRT - Discovery of Infer-ence Rules from Text.
In Proceedings of ACMSIGKDD Conference on Knowledge Discovery andData Mining: 323-328.I.
D. Melamed.
2001.
Empirical Methods for ExploitingParallel Texts.
The MIT Press.R.
Mihalcea and T. Pedersen.
2003.
An Evaluation Ex-ercise for Word Alignment.
In HLT/NAACL Work-shop: Building and Using Parallel Texts: 1-10.F.
Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proceedings of the ACL: 440-447.Hong Kong, China.F.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
Computa-tional Linguistics 29(1): 19-52.B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedAlignment of Multiple Translations: Extracting Para-phrases and Generating New Sentences.
Proceedingsof HLT/NAACL.Y.
Shinyama, S. Sekine and K. Sudo.
2002.
AutomaticParaphrase Acquisition from News Articles.
In Pro-ceedings of NAACL-HLT.F.
K. Soong and E. F. Huang.
1991.
A tree-trellis basedfast search for finding the n-best sentence hypothesesin continuous speech recognition.
In Proceedings ofthe IEEE International Conference on Acoustics,Speech and Signal Processing 1: 705-708.
Toronto,Canada.E.
Sumita.
2001.
Example-based machine translationusing DP-matching between work sequences.
In Pro-ceedings of the ACL 2001 Workshop on Data-DrivenMethods in Machine Translation: 1?8.C.
Tillmann, S. Vogel, H. Ney, and A. Zubaiga.
1997.
ADP Based Search Using Monotone Alignments inStatistical Translation.
In Proceedings of the ACL.L.
Vita, A. Ittycheriah, S. Roukos, and N. Kambhatla.2003.
tRuEcasing.
In Proceedings of the ACL: 152-159.
Sapporo, Japan.S.
Vogel, H. Ney and C. Tillmann.
1996.
HMM-BasedWord Alignment in Statistical Translation.
In Pro-ceedings of the ACL: 836-841.
Copenhagen Den-mark.S.
Vogel, Y. Zhang, F. Huang, A. Venugopal, B. Zhao,A.
Tribble, M. Eck, and A. Waibel.
2003.
The CMUStatistical Machine Translation System.
In Proceed-ings of MT Summit IX, New Orleans, Louisiana,USA.
