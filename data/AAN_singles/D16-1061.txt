Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 638?647,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEmotion Distribution Learning from TextsDeyu Zhou, Xuan Zhang, Yin Zhou, Quan Zhao, Xin Geng?MOE Key Laboratory of Computer Network and Information IntegrationSchool of Computer Science and EngineeringSoutheast University, Nanjing, China{d.zhou, zhouying1, xuanzhang, zhaoquan, xgeng}@seu.edu.cnAbstractThe advent of social media and its prosperityenable users to share their opinions and views.Understanding users?
emotional states mightprovide the potential to create new businessopportunities.
Automatically identifying user-s?
emotional states from their texts and clas-sifying emotions into finite categories suchas joy, anger, disgust, etc., can be consid-ered as a text classification problem.
How-ever, it introduces a challenging learning sce-nario where multiple emotions with differen-t intensities are often found in a single sen-tence.
Moreover, some emotions co-occurmore often while other emotions rarely co-exist.
In this paper, we propose a novel ap-proach based on emotion distribution learningin order to address the aforementioned issues.The key idea is to learn a mapping functionfrom sentences to their emotion distributionsdescribing multiple emotions and their respec-tive intensities.
Moreover, the relations of e-motions are captured based on the Plutchik?swheel of emotions and are subsequently in-corporated into the learning algorithm in orderto improve the accuracy of emotion detection.Experimental results show that the proposedapproach can effectively deal with the emo-tion distribution detection problem and perfor-m remarkably better than both the state-of-the-art emotion detection method and multi-labellearning methods.1 IntroductionThe advent of social media and its prosperity enablethe creation of massive online user-generated con-?Corresponding authorSentence Trains crash near Thai resort townEmotions anger disgust fear joy sadness surprise2 0 62 0 90 10Table 1: An example of a sentence containing emotions select-ed from SemEval 2007 Task#14, Affective Text, where each ofthe six emotions are indicated using a score of [0, 100].tent including opinions and product reviews.
Ana-lyzing such user-generated content allows the detec-tion of users?
emotional states, which might be po-tentially useful for downstream applications such asbrand watching, product recommendation, and de-tection of health-related issues, etc.
Based on theway emotions are represented, computational mod-els for emotion analysis can be categorized into di-mensional models and categorical models (Calvoand D?Mello, 2010).
Dimensional approaches (Rus-sell, 2003) emphasize the fundamental dimension-s of valence and arousal in understanding emotion-al experience, which have long been studied by e-motion theorists.
Categorical models (Gupta et al,2013) involve the use of a categorical representa-tion, in which emotions are represented by a num-ber of labels.
For example, Ekman?s basic emotionset (Ekman, 1992) consists of anger, disgust, fear,happiness, sadness and surprise.
An example of asentence and the annotated emotions can be foundin Table 1.Considering each basic emotion as class label forthe sentence, emotion detection can be treated as aclassification problem.
There is a large body of pri-or work on emotion classification (Mishne and deRijke, 2006; Lin and He, 2009; Quan et al, 2015;Wang and Pal, 2015).
By choosing the strongest e-motion as the emotion label for the sentence, most of638classification approaches are based on single-labellearning.
However, as shown in Table 1, a sentencemight contain multiple emotions with varying inten-sities.
Although, some lexicon-based approach suchas (Wang and Pal, 2015) can output multiple emo-tions with intensities using non-negative matrix fac-torization.
It can only guarantee convergence to alocal minimum, which is prohibitive on the large,realistically-sized emotion detection problem.Machine learning methods such as multi-labellearning (MLL) can be employed to identify mul-tiple emotions for each sentence (Zhang and Zhou,2014).
MLL usually selects a threshold, then label-s emotions with scores higher than the threshold asrelevant and the others as irrelevant.
However, thesemethods are not able to learn the intensity of eachemotion.
To address this problem, a new machinelearning paradigm called Label Distribution Learn-ing (LDL) (Geng, 2016) was proposed in recentlyyears.
Similarly, in this paper, we propose an e-motion distribution learning (EDL) algorithm.
Dif-ferent from the previous approaches, EDL assumesthat each sentence contains a mixture of basic e-motions with different intensities.
Using categori-cal model, we can label each sentence with an emo-tion vector where each element corresponds to onebasic emotion and the value of each element indi-cates the intensity of the emotion.
We require thateach vector element has a value between 0 and 1and they sum up to 1.
By doing so, the emotionvectors can be considered as emotion distributionsand the proposed EDL algorithm aims to learn themapping from sentences to their corresponding e-motion distributions by minimizing the differencesbetween the true distributions and the predicted dis-tributions.
Both the single-label learning and ML-L can be considered as special cases of EDL in e-motion detection.
Moreover, as some emotions co-occur more often while others rarely co-exist, therelations between basic emotions are captured ac-cording to the Plutchik?s wheel of emotions theo-ry (Plutchik, 1980) and are incorporated in the learn-ing framework as constraints in order to improve theaccuracy of emotion detection.Our work makes the following contributions:?
We propose a novel approach based on emo-tion distribution learning to identify multipleemotions with their intensities from texts.
Tothe best of our knowledge, it is the first attemptto identify both emotions and intensities in thedistribution learning framework.?
The relations between basic emotions are in-corporated into the learning framework as con-straints to improve the emotion detection accu-racy.
To avoid the incorporation of noisy in-formation from the training data, the relationconstraint is set based on the Plutchik?s wheelof emotions theory.?
Experimental results show that the proposedapproach can effectively deal with the emotiondistribution detection problem and perform re-markably better than the state-of-the-art multi-label learning methods and emotion detectionmethod.2 Related WorkIn general, emotion classification can be approachedby two types of methods, lexicon-based or corpus-based.
Lexicon-based approaches rely on emotionlexicons consisting of words and their correspond-ing emotion labels for detecting emotions from tex-t. For example, WordNetAffect (Strapparava andValitutti, 2004) was constructed by extending Word-net, a lexical database of English terms, with infor-mation on affective terms.
EmoSenticNet assignssix WordNetAffect emotion labels to SenticNet con-cepts (Poria et al, 2013), which can be thought ofas an expansion of WordNetAffect emotion label-s to a larger vocabulary.
Many approaches wereproposed based on emotion lexicons.
For example,(Aman and Szpakowicz, 2007) classified emotionaland non-emotional sentences using the constructedemotion lexicon.
(Choudhury et al, 2012) employeda classifier to detect human affective states in socialmedia.
(Wang and Pal, 2015) proposed a model withseveral constraints based on an emotion lexicon foremotion classification.Corpus-based methods aim to train supervisedclassifiers from annotated training data where eachsentence or document is labelled with an emotionclass.
(Mishne and de Rijke, 2006) constructedmodels to predict the levels of various moods ac-cording to the language used by bloggers at a giv-639en time.
(Aman and Szpakowicz, 2007) describedan emotion annotation task of identifying emotioncategory, emotion intensity and the words/phrasesthat indicate emotions in text.
Emotion classifica-tion was conducted using trained support vector ma-chines.
(Agrawal and An, 2012) proposed an un-supervised context-based approach to detect emo-tions from text at the sentence level.
They comput-ed an emotion vector for each potential affect bear-ing word based on the semantic relatedness betweenwords and various emotion concepts.
The scores arethen tuned using the syntactic dependencies with-in the sentence structure.
(Bao et al, 2009) pro-posed an emotion topic model by augmenting laten-t Dirichlet alocation with an intermediate emotionlayer.
(Quan et al, 2015) proposed a logistic regres-sion model for social emotion detection.
Intermedi-ate hidden variables were also introduced to modelthe latent structure of input text corpora.Our work is partly inspired by (Quan et al, 2015).However, our proposed approach differs from (Quanet al, 2015) in two aspects: 1) by introducing theemotion distribution learning framework, many dif-ferent criteria can be used to measure the distancebetween the true distribution and the predicted dis-tribution, such as squared X 2, Euclidean, Jeffery?sdivergence apart from Kullback-Leibler divergenceemployed in logistic regression model.
2) the re-lations between basic emotions are captured basedon the Plutchik?s wheel of emotions theory to avoidthe incorporation of any noisy information from thetraining data.3 Emotion Distribution Learning3.1 Problem SettingAs have discussed in section 1, one sentence mightcontain one or more emotions, and each emotion hasits own intensity.
We use dyx to indicate the intensi-ty of emotion y for sentence x, where x ?
X andy ?
Y .
The emotion intensity is normalized to makedyx ?
[0, 1] and?y dyx = 1 to constitute the emotiondistribution.Note that dyx denotes the proportion that y accountsfor in a full emotion distribution of x.
It is differ-ent from the probability of y being a correct emo-tion label for x. Probability distribution implies thatonly one emotion label is correct for each sentence,while emotion distribution allows multiple emotionsin one sentence.
The goal of EDL is to learn a map-ping from sentences X = Rm to the distributionsover a finite set of labels Y = {y1, y2, ...yc}.
Eachlabel represents one of the basic emotions.3.2 LearningGiven a training set P = {(x1, E1), (x2, E2), ...,(xn, En)}, where xi ?
X is a sentence and Ei ={dy1xi , dy2xi , ..., dycxi } is the emotion distribution asso-ciated with xi.
The goal of EDL is to learn a con-ditional probability mass function p(y|x) from P ,where x ?
X and y ?
Y .
Assuming that p(y|x)is a parametric model p(y|x; ?
), where ?
are mod-el parameters, many different criteria can be usedto measure the distance between two distribution-s, such as Squared X 2, Euclidean, Jeffery?s diver-gence, Kullback-Leibler (K-L) divergence and soon.
Here we use Divergence defined byDJ(Qa||Qb) = 2?j(Qja ?Qjb)2(Qja + Qjb)2, where Qja and Qjb are the j-th element of the t-wo distributions Qa and Qb, respectively.
Diver-gence is balanced, which makes DJ(Qa||Qb) equalto DJ(Qb||Qa).
The formula above calculates thesum of all the distances between emotion intensitiesin the same position.Then the optimal model parameters ??
is deter-mined by??
= argmin????
?iDJ(Ei||E?i) +?1n?k,r|?k,r|1+?2n?u?j,k?jk?
?u,j ?
?u,k?22??
?= argmin???
?2?i,j(dyjxi ?
p(yj |xi, ?
))2(dyjxi + p(yj |xi, ?
))2+ ?1n?k,r|?k,r|1+ ?2n?u?j,k?jk?
?u,j ?
?u,k?22???
(1)640, where Ei is the ground truth emotion distributionof the i-th sentence and the E?i is the predicted oneby p(y|xi; ?).
The second term is a regularizer tomake the predicted emotion distribution sparse, andthe third term considers the relationship between d-ifferent emotions.
As mentioned in section 1, someemotions often co-occur such as joy and love, andsome rarely co-exist such as joy and anger.
There-fore, the third term is employed to incorporate suchprior knowledge.
The weight ?jk models the rela-tionship between the j-th emotion and the k-th emo-tion in the distribution.
In this paper, we capturethe relationships between different emotions basedon Plutchik?s wheel of emotions (Plutchik, 1980)which is produced in psychology view.
Plutchik?swheel of emotions includes several typical emotionsand its eight sectors indicate eight primary emotiondimensions arranged as four pairs of opposites.
Were-produce a wheel of eight emotions?
relationship-s according to Plutchik?s theory, which is shown inFigure 1.????????????????????????????????????????????????????
?Figure 1: Plutchik?s wheel of emotions.In the emotion wheel, emotions sat at oppositeend have an opposite relationship, while emotion-s next to each other are more closely related.
Wequantify the relations between each pair of emotionsbased on the angle between them in wheel of emo-tions (Plutchik, 2001).
For example, emotion pairswith 180 degrees are opposite to each other, whichare described by ?1, while emotion pairs with 90degrees are described by 0, meaning no relation-ship between them.
Emotion pairs with 45 degreeshave the relationship value of 0.5, while emotionpairs with 135 degrees have the relationship valueof ?0.5.
Figure 2 shows the gray-scale image of thepair-wise relationships of emotions presented in Fig-ure 1.
In each cell, the darker the color is, the moresimilar the two emotions are.As for p(y|x; ?
), similar to (Geng, 2016), we as-sume it takes a maximum entropy model, i.e.,p(yk|xi; ?)
=1Ziexp(?r?krxri ) (2), where Zi = ?k exp(?r ?krxri ) is the normaliza-tion factor, xri is the r-th feature of xi, and ?kr isan element in ?.
Substituting Equation 2 into Equa-tion 1 yields the target function,T (?)
= 2?i,j(1?
4Zidyjxi exp (?r ?jrxri )(Zidyjxi + exp (?r ?jrxri ))2)+ ?1n?k,r|?k,r|1+ ?2n?u?j,k?jk?
?u,j ?
?u,k?22.
(3)The minimization of the function T (?)
can be effec-tively solved by the limited-memory quasi-Newtonmethod (L-BFGS).
The basic idea of L-BFGS is toavoid explicit calculation of the inverse Hessian ma-trix used in the Newton method.
L-BFGS approxi-mates the inverse Hessian matrix with an iterativelyupdated matrix instead of actually storing the ful-l matrix.
Here we follow the idea of an effectivequasi-Newton method BFGS.
Consider the second-order Taylor series of T ?(?)
= ?T (?)
at the currentestimate of the parameter vector ?
(l):T ?(?
(l+1)) ?
T ?(?
(l)) +?T ?(?(l+1))T?+12?TH(?
(l))?, (4)where?
= ?(l+1)??
(l) is the update step,?T ?(?
(l))and h(?
(l)) are the gradient and Hessian matrix ofT ?(?
(l)) at ?
(l), respectively.
The minimizer of E-quation 4 is?l = ?H?1(?
(l))?T ?(?(l)).
(5)The line search Newton method uses ?
(l) as thesearch direction p(l) = ?
(l) and updates model pa-rameters by?
(l+1) = ?
(l) + ?
(l)p(l), (6)641where the step length ?
(l) is obtained from a linesearch procedure to satisfy the strong Wolfe condi-tions (Nocedal and Wright, 2006):T ?(?(l)+?
(l)p(l)) ?
T ?(?(l))+c1?
(l)?T ?(?
(l))T p(l)|?T ?(?
(l) + ?
(l)p(l)| ?
c2|?T ?(?
(l))T p(l)|,where 0 < c1 < c2 < 1.
The idea of BFGS is toavoid explicit calculation of H?1(?
(l)) by approxi-mating it with an iteratively updated matrix B, i.e.B(L+1) = (I ?
?
(l)s(l)(u(l))T )?B(l)?
(I ?
?
(l)u(l)(s(l))T )+?
(l)s(l)(s(l))Twheres(l) = ?
(l+1) ?
?
(l),u(l) = ?T ?(?(l+1))?
?T ?(?(l)),?
(l) = 1s(l)u(l) .Figure 2: Gray-scale image of the pair-wise relationships ofemotions shown in Figure 1.As for the optimization of the target functionT (?
), the computation of BFGS is mainly relatedto the first-order gradient of T ?(?
), which can beachieved by?T (?)?
?jr= 4dyjxi pij(1?
pij)(dyjxi ?
pij)(dyjxi + pij)3+?1?k,rsgn(?k,r)+1n?2?k?jk(?j ?
2?k), (7)where pij = 1Zi exp(?r ?jrxri ).
Thus it performsmore efficiently than the standard line search New-ton method.In order to compare with the MLL methods, la-bels in the predicted distribution need to be divid-ed into two sets, i.e, the relevant and irrelevant set-s. For this purpose, an extra virtual label y0 isadded into the label set, i.e., the extended label setY ?
= Y ?
{y0}={y0, y1, y2...yc}.
Using the new ex-tended label set in the training process, the optimalparameter vector ??
is learned.
As y0 is the labelthat distinguishes the relevant and irrelevant emo-tions directly, it is initialized as the threshold used inMLL.
Given a sentence x?, its emotion distributionis predicted by p(y|x?
; ??).
The intensity value of y0splits the predicted distribution into two sets.
Theemotions with the intensity value higher than y0?sare regarded as the relevant emotions, and the restemotions are regarded as irrelevant ones.
Therefore,EDL in fact implements the function of MLL with-out the need of setting the threshold manually.4 Experiments4.1 SetupWe evaluate the proposed approach on the Ren-CECps corpus (Quan and Ren, 2010).
It contains35, 096 sentences selected from blogs in Chinese.Each sentence is annotated with 8 basic emotion-s, such as anger, anxiety, expect, hate, joy, love,sorrow and surprise, together with their emotion s-cores.
Higher score represents higher emotion inten-sity.
We use ASi(j) to represent the score of emo-tion j in sentence i.
Given a sentence xi, the inten-sity of emotion j is calculated by dyjxi = ASi(j)?k ASi(k) .By doing so, each intensity value fulfills dyjxi ?
[0, 1]and?y dyjxi = 1.For each sentence, features are extracted using re-cursive auto-encoders (RAEs) (Socher et al, 2011).RAEs are neural networks that represent meaningsof fixed-size inputs in the reduced dimensional s-pace.
For example, each word in a sentence is repre-sented using a vector w ?
Rd, and the RAE methodreduces the entire sentence to a single vector of sizeRd.
Sentences are sequences of words that can berepresented by a binary tree structure.
The words arethe leaves of the tree and their combined grouping isused to get a notion of the meaning of the sentence.642The internal nodes of the tree correspond to the com-bined meaning of the nodes underneath them.
Eachinternal node is also represented in the same manneras individual words in the form of a vector w?
?
Rd.These internal nodes are the hidden representationsof the neural network.
In the RAE model, the vocab-ulary is stored in an embedding matrix V ?
Rd ?Dwhere D is the cardinality of the vocabulary.
Typi-cally, each word w ?
V is initialized independentlyfollowing a Gaussian distributionwi ?
N(0, ?2).
Inour experiment, we set the dimension of each sen-tence representation to 100.We build a gray-scale image shown in Figure 3by computing the correlation coefficient of the emo-tions from the Ren-CECps corpus.
It can be ob-served that Figure 3 is quite similar to Figure 2,which shows that our proposed way in capturingthe relations between emotions is inline with whathave been revealed by the emotion annotations in theRen-CECps corpus.Figure 3: Gray-scale image of the pair-wise relations of theemotions in the Ren-CECps corpus.4.2 Experimental ResultsAs the output of EDL is a distribution, a naturalchoice of criteria is the averaged similarity or dis-tance between the actual emotion distribution andthe predicted distribution.
There are many metric-s that can be applied to measure the distance be-tween two distributions.
In this paper six of them areused to evaluate the results of EDL, i.e, Euclidean,S?rensen, Squared X 2, KL divergence, Intersectionand Fidelity, as suggested in (Geng and Ji, 2013).Name FormulaDistance Euclidean Euclidean(P,Q)=?
?cj=1(Pj ?Qj)2S?rensen S?rensen(P,Q) = ?cj=1 |Pj?Qj |?cj=1(Pj+Qj)Squared X 2 SquaredX 2(P,Q) =?cj=1 (Pj?Qj)2Pj+QjKullback-Leibler (KL) K-L(P,Q) =?cj=1 Pj ln PjQjSimilarity Intersection Intersection(P,Q)=?cj=1 min(Pj , Qj)Fidelity Fidelity(P,Q) =?cj=1?PjQjTable 2: Evaluation criteria for the Label Distribution Learning(LDL) methods.Name FormulaHamming Loss hloss(h) = 1P ?Pi=1 |h(xi)?Yi|One error one-error(f) = 1P ?Pi=1[argmaxy?Y f(xi, y)] /?
YiCoverage Coverage(f) = 1P ?Pi=1 maxy?Yi rankf (xi, y)?
1Ranking Loss rloss(f) = 1P ?Pi=1 1|Yi||Y?i| ?
|R|,WhereR = (y?, y??
)|f(xi, y?)
?
f(xi, y??
), (y?, y??)
?
Yi ?
Y?iAverage Precision Average(f) = 1P ?Pi=1 1|Yi|?y?Yi |Pi|rankf (xi,y) , wherePi = y?|rankf (xi, y?)
?
rankf (xi, y), (y)?
?
YiTable 3: Evaluation criteria for the MLL methods.The formulae of the six criteria are summarized inTable 4.2.
Note that the virtual label y0 is removedbefore evaluation.As EDL can output both the relevant emotionsand their respective emotion intensities, MLL canbe seen as a special case of EDL that it only outputsemotion labels but not their intensities.
Several e-valuation criteria typically used in MLL can also beused to measure EDL?s ability of distinguishing rel-evant emotions from irrelevant ones, including ham-ming loss, one error, coverage, ranking loss, and av-erage precision as suggested by (Zhang and Zhou,2014), which are summarized in Table 4.2.
Ham-ming loss evaluates how many times an emotion la-bel is misclassified.
One-error evaluates the fractionof sentences whose top-ranked emotion is not in therelevant emotion set.
Coverage evaluates how manysteps are needed to move down the ranked emotionlist so as to cover all the relevant emotions of theexample.
Ranking loss evaluates the fraction of re-versely ordered emotion pairs.
Average precisionevaluates the average fraction of the relevant emo-tions ranked higher than a particular emotion y ?
Y .For each algorithm, ten-fold cross validation isconducted.
EDL is first compared with four existingLabel Distribution Learning (LDL) methods (Geng,643Algorithm Evaluation CriterionEuclidean(?)
S?rensen(?)
Squared X 2(?)
K-L(?)
Intersection(?)
Fidelity(?
)EDL 0.2361?0.0057 0.2346?0.0061 0.1780?0.0037 0.2067?0.0046 0.7654?0.0046 0.9523?0.0019AA-KNN (Geng, 2016) 0.2948?0.0101?
0.2941?0.0123?
0.2688?0.0102?
0.3163?0.0087?
0.7059?0.0078?
0.9258?0.0090?PT-Bayes (Geng, 2016) 0.3295?0.0125?
0.3288?0.0158?
0.2826?0.0115?
0.3263?0.0238?
0.6711?0.0241?
0.9238?0.0060?PT-SVM (Geng, 2016) 0.3614?0.0869?
0.3625?0.0145?
0.3415?0.0089?
0.4073?0.0209?
0.6375?0.0099?
0.9069?0.0073?AA-BP (Geng, 2016) 0.3299?0.0159?
0.3430?0.0264?
0.2885?0.0251?
0.3406?0.0092?
0.6569?0.0166?
0.9229?0.0056?emoDetect (Wang and Pal, 2015) 0.3333?0.0678?
0.3468?0.0719?
0.2928?0.0674?
0.3463?0.0790?
0.6532?0.0719?
0.9212?0.0180?Table 4: Experimental results in comparison with the LDL methods and the emotion detection approach.Algorithm Evaluation CriterionAverage Precision(?)
Coverage(?)
Hamming Loss(?)
One Error(?)
Ranking Loss(?
)EDL 0.6419?0.0235 2.1412?0.0235 0.1772?0.0568 0.5239?0.0945 0.2513?0.0560ML-KNN (Zhang and Zhou, 2014) 0.5917?0.0742?
2.448?0.0981?
0.2459?0.0781?
0.5339?0.0954?
0.2908?0.0431?LIFT (Zhang, 2011) 0.5979?0.0891?
2.4267?0.0492?
0.1779?0.0597?
0.5131?0.0666?
0.2854?0.0427?Rank-SVM (Zhang and Zhou, 2014) 0.5738?0.0892?
2.5861?0.0777?
0.2485?0.0458?
0.5603?0.0921?
0.3055?0.0579?MLLOC (Huang and Zhou, 2012) 0.4135?0.0568?
3.6994?0.0764?
0.1850?0.0659?
0.6971?0.0924?
0.4742?0.0734?BP-MLL (Zhang and Zhou, 2006) 0.4791?0.0999?
3.3773?0.0681?
0.2108?0.0986?
0.6316?0.0988?
0.4293?0.0956?ECC (Read et al, 2011) 0.5121?0.0892?
2.7767?0.0876?
0.1812?0.0945?
0.6969?0.0598?
0.3281?0.0659?Table 5: Experimental results in comparison with the MLL methods.2016), i.e., PT-Bayes, PT-SVM, AA-KNN, AA-BP.k in AA-KNN is set to 8.
Linear kernel is used inPT-SVM.
The number of hidden-layer neurons forAA-BP is set to 60.
The evaluation results of ourproposed approach in comparison to the LDL base-lines are presented in Table 4.2.
For all the mea-sures, ???
indicates ?the smaller the better?, while???
indicates ?the larger the better?.
The best perfor-mance on each measure is highlighted by boldface.The two-tailed t-tests with 5% significance level areperformed to see whether the differences betweenEDL and the baselines are statistically significan-t. We use ?
to indicate significance difference.
Asthe state-of-the-art emotion detection method pro-posed in (Wang and Pal, 2015) can output the e-motion distributions based on a dimensional reduc-tion method, we present its experimental results onthe Ren-CECps corpus in the last row of Table 4.2.It can be observed that EDL performs significant-ly better than all the baseline LDL methods and thestate-of-the-art emotion detection approach on al-l criteria considered here.Since EDL can be seen as an extension of MLL,EDL is compared with 7 widely used MLL methodsusing the virtual label y0, namely ML-KNN (Zhangand Zhou, 2014), ECC (Read et al, 2011), MLLOC(Huang and Zhou, 2012), LIFT (Zhang, 2011), ML-RBF (Zhang, 2009), Rank-SVM (Zhang and Zhou,2014), BP-MLL (Zhang and Zhou, 2006).
Amongthe compared algorithms, ML-kNN is derived fromthe traditional k-nearest neighbor (kNN) algorithm.Maximum a posteriori (MAP) principle is used todetermine which emotion set is related to the giv-en sentence.
CC (classifier chains method) over-comes the limitations of BR and performs better butrequires more computations.
ECC (ensemble clas-sifier chains) applies classifier chains in an ensem-ble framework and obtains high predictive perfor-mances.
MLLOC (Multi-label LOcal Correlation)tries to exploit emotion correlations in the expres-sion data locally.
The global discrimination fittingand local correlation sensitivity are incorporated in-to a unified framework, and solution for the opti-mization are developed.
Rank-SVM provides a wayof controlling the complexity of the overall learningsystem while having a small empirical error.
Thearchitectures of Rank-SVM is based on linear mod-els of Support Vector Machines (SVM) (Boser et al,1992).
LIFT constructs features specific to each e-motion by conducting clustering analysis on its pos-itive or negative instances, and then performs train-ing and testing by querying the clustering result-s (Zhang, 2011).
BP-MLL is derived from the fa-mous backpropagation algorithm through employ-ing a novel error function capturing the character-istics of multi-label learning, i.e., the emotions be-longing to a sentence should be ranked higher thanthose not belonging to that sentence (Zhang and644Zhou, 2006).The virtual label y0 used in EDL and the thresh-old value used in MLL are all set to 2.5.
Besides, the?, ?1 and ?2 are set as 0.25, 0.0001, 0.1 respectively.For the MLL methods, the value of k is set to 8 inML-KNN, ratio is 0.02 and ?
is 2 in ML-RBF.
Lin-ear kernel is used in LIFT.
Rank-SVM uses the RBFkernel with the width ?
equals to 1.
The evaluationresults of the proposed approach in comparison toall MLL baselines are presented in Table 4.2.
EDLperforms best on all evaluation measures.
It verifiesthe advantage of EDL owing to the consideration ofvarying intensity of the basic emotions.4.3 Further AnalysisTo fully understand the emotion detection results,we use word cloud (Harris, 2011) to output the top30 frequent words in the testing data for the emotionlove and anxiety based on the annotation as shownin the left part of Figure 4.
We also output the top30 frequent words for the two emotions based on theprediction generated by EDL as shown in Figure 4?s right part.
It can be observed that most wordsbased on prediction indeed express their associatedemotions.
For example, word ?like?
delivers the e-motion of love (right part of Figure 4(a)) and word?problem?
tells anxiety (right part of Figure 4(b)).Moreover, the annotation and the prediction share 20out of the top 30 most frequent words for the emo-tion love such as ?friend?, ?joy?, ?happiness?, etc asshown in the middle of Figure 4(a) and 19 out of 30for the emotion of anxiety (the middle of Figure 4(b).It demonstrates that EDL can learn emotions fromtext precisely.To investigate the emotion distributions generatedby EDL, a sentence from the Ren-CECps corpus to-gether with the emotion distribution output by EDLis illustrated in Figure 5.
The ground truth emotiondistribution is obtained by normalizing the scoresand the virtual label y0.
As can be seen, the curveof the predicted emotion distribution is very similaras the ground truth distribution, which demonstratesthat EDL can learn the varying intensities of all thebasic emotions well.????????????????????????
???????
?????
????
???
????
??????
???????
?Ground truthyxd????????????????????????
???????
?????
????
???
????
??????
????????Predicted?
?Relevant emotionsIrrelevantemotionsyxd?
?Relevant emotionsIrrelevantemotions????????????????????????????
?Dreams die one by one, but life should go on and we have to eat.Figure 5: A sentence with the emotion distribution predictedby EDL.5 Conclusions and Future WorkIn this paper, we have proposed a novel approachbased on EDL to identify multiple emotions withtheir intensities from texts.
Moreover, the relationsbetween basic emotions is incorporated in the learn-ing framework as constraints to improve the learningaccuracy.
Experimental results show that the pro-posed approach can effectively deal with the emo-tion distribution detection problem and perform re-markably better than the state-of-the-art multi-labellearning methods and the emotion detection method.In future work, we will investigate the efficiency ofthe proposed approach in other datasets and exploreother methods in capturing the inter-relations of e-motions.AcknowledgmentsThis work was funded by the National Nat-ural Science Foundation of China (61273300,61232007, 61528302, 61622203), the Jiangsu Natu-ral Science Funds for Distinguished Young Scholar(BK20140022), the Natural Science Foundation ofJiangsu Province of China (BK20161430), and theCollaborative Innovation Center of Wireless Com-munications Technology.645???????
???????
?????????
???
??
??????
????
????
???????
??????????????
??????
Like?
Friend?
Beauty?
Affection?
Joy?
Year?
Heart?
Ever?
World?
Happiness?
Eat?
Life?
That?
Always?
Real?
Pretty?Warm?
Sentence?
Piece?
All the TimeBased on Annotation Based on Prediction?
(a) Love???
???
????
????????????
?????
?????
?
????
?Based on PredictionBased on Annotation?
Matter?
Thing?
Problem?
Again?
These?
Can?t?
I won?t?
Who?
Should?
Hard?
Work?
Eh?
What about?
How?
Really?
Maybe?
Some?
Eat?
Year???
???????
??
(b) AnxietyFigure 4: Top 30 frequent words for the emotion love and anxiety based on the annotation or the prediction.ReferencesAmeeta Agrawal and Aijun An.
2012.
Unsuper-vised emotion detection from text using semanticand syntactic relations.
In Proceedings of the 2012IEEE/WIC/ACM International Joint Conferences onWeb Intelligence and Intelligent Agent Technology,pages 346?353.Saima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
Lecture Notes in Com-puter Science, 4629:196?205.S.
Bao, Shengliang Xu, Li Zhang, Rong Yan, Zhong Su,Dingyi Han, and Yong Yu.
2009.
Joint emotion-topicmodeling for social affective text mining.
In Proceed-ings of the Ninth IEEE International Conference onData Mining, pages 699?704.Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N.Vapnik.
1992.
A training algorithm for optimal mar-gin classifiers.
In Proceedings of the Fifth AnnualWorkshop on Computational Learning Theory, pages144?152.R.A.
Calvo and S. D?Mello.
2010.
Affect detection: Aninterdisciplinary review of models, methods, and theirapplications.
Affective Computing, IEEE Transactionson, 1(1):18?37.Munmun De Choudhury, Michael Gamon, and Scot-t Counts.
2012.
Happy, nervous or surprised?
clas-sification of human affective states in social media.
InProceedings of the Sixth International AAAI Confer-ence on Weblogs and Social Media, pages 435?438.Paul Ekman.
1992.
An argument for basic emotions.Cogition and emotion, 6(3-4):169?200.Xin Geng and Rongzi Ji.
2013.
Label distribution learn-ing.
In Proceedings of the 13th IEEE InternationalConference on Data Mining Workshops, pages 377?383.Xin Geng.
2016.
Label distribution learning.
IEEETransactions on Knowledge and Data Engineering,28(7):1734?1748.Narendra Gupta, Mazin Gilbert, and Giuseppe Di Fab-brizio.
2013.
Emotion detection in email customercare.
Computational Intelligence, 29(3):489?505.Jacob Harris.
2011.
Word clouds considered harmful.Nieman Journalism Lab.Sheng-Jun Huang and Zhi-Hua Zhou.
2012.
Multi-labellearning by exploiting label correlations locally.
InProceedings of the 26th AAAI Conference on ArtificialIntelligence, pages 949?955, Toronto, Canada.Chenghua Lin and Yulan He.
2009.
Joint sentiment/topicmodel for sentiment analysis.
In Proceedings of the18th ACM Conference on Information and KnowledgeManagement, CIKM ?09, pages 375?384, New York,NY, USA.
ACM.Gilad Mishne and Maarten de Rijke.
2006.
Capturingglobal mood levels using blog posts.
In AAAI Sympo-646sium on Computational Approaches to Analysing We-blogs (AAAI-CAAW), pages 145?152, June.Jorge Nocedal and Stephen Wright.
2006.
Numericaloptimization.
Springer Science & Business Media.Robert Plutchik.
1980.
A general psychoevolutionarytheory of emotion.
Theories of emotion, 1.Robert Plutchik.
2001.
An argument for basic emotions.American Scientist, 89(4):344?350.S.
Poria, A. Gelbukh, A. Hussain, N. Howard, D. Das,and S. Bandyopadhyay.
2013.
Enhanced senticnetwith affective labels for concept-based opinion min-ing.
Intelligent Systems, IEEE, 28(2):31?38.Changqin Quan and Fuji Ren.
2010.
Sentence emotionanalysis and recognition based on emotion words us-ing ren-cecps.
International Journal of Advanced In-telligence, 2(1):105?117.Xiaojun Quan, Qifan Wang, Ying Zhang, Luo Si, and Li-u Wenyin.
2015.
Latent discriminative models forsocial emotion detection with emotional dependency.ACM Trans.
Inf.
Syst., 34(1):2:1?2:19.Jesse Read, Bernhard Pfahringer, Geoff Holmes, andEibe Frank.
2011.
Classifier chains for multi-labelclassification.
Machine Learning, 85(3):333?359.James A. Russell.
2003.
Core affect and the psycholog-ical construction of emotion.
Psychological Review,110(1):145?172.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sen-timent distributions.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 151?161.Carlo Strapparava and Alessandro Valitutti.
2004.Wordnet-affect: an affective extension of wordnet.In Proceedings of the 4th International Conferenceon Language Resources and Evaluation, pages 1083?1086.Yichen Wang and Aditya Pal.
2015.
Detecting emo-tions in social media: A constrained optimization ap-proach.
In Proceedings of the Twenty-Fourth Inter-national Joint Conference on Artificial Intelligence,pages 996?1002.Min-Ling Zhang and Zhi-Hua Zhou.
2006.
Multilabelneural networks with applications to functional ge-nomics and text categorization.
IEEE Transactions onKnowledge and Data Engineering, 18(10):1338?1351.Min-Ling Zhang and Zhi-Hua Zhou.
2014.
A review onmulti-label learning algorithms.
IEEE Transactions onKnowledge and Data Engineering, 26(8):1819?1837.Min-Ling Zhang.
2009.
Ml-rbf: Rbf neural networksfor multi-label learning.
Neural Processing Letters,29(2):61?74.Min-Ling Zhang.
2011.
Lift: Multi-label learning withlabel-specific features.
In Proceedings of the 22nd In-ternational Joint Conference on Artificial Intelligence,pages 1609?1614, Barcelona, Spain.647
