Mechanisms for Mixed-Initiative Human-Computer  CollaborativeDiscourseCurry I. GuinnDepartment ofComputer ScienceDuke UniversityBox 90129Durham, NC 27708cig~cs, duke.
eduAbstractIn this paper, we examine mechanisms forautomatic dialogue initiative setting.
Weshow how to incorporate initiative chang-ing in a task-oriented human-computer di-alogue system, and we evaluate the ef-fects of initiative both analytically and viacomputer-computer dialogue simulation.1 Motivation: Initiative inTask-Oriented DialoguesEfficient human-computer dialogue requires imme-diate utterance-by-utterance a commodation to theneeds of the interaction.
The participants must con-tinuously focus all concerns on the goals of the taskand avoid extraneous paths.
This means that the di-alogue in i t ia t ive  should always pass immediatelyto the participant who is best able to handle thecurrent task.
An agent is said to have dialogue ini-tiative over a mutual goal when that agent controlshow that goal will be solved by the collaborators.
Inthe implemented voice dialogue system "The CircuitFix-it Shop" (Smith et al, 1992; Smith and Hipp,1995), the dialogue fragment given in Figure 1 occursbetween a human user and a computer collaborator.Utterances 2 and 4 indicate that the computer is di-recting the search for the missing wire in the faultycircuit.
However, in Utterance 5 the user takes theinitiative and suggests a different wire than has beenproposed by the computer.
In this paper we willpresent a theory explaining how initiative changesbetween participants and how computational gentscan evaluate who should be in control of solving agoal.2 Automating Dialogue InitiativeCenter for Digital Systems EngineeringResearch Triangle InstituteBox 12194Research Triangle Park, NC 27709cig~rt?, org1 U The LED is displaying an alter-nately flashing one and thetop corner of a seven.2 C There is supposed to be a wirebetween connector nine nineand connector one zero zero.3 U It is there.4 C There is supposed to be a wirebetween connector nine eight andconnector one zero two.5 U There is no wire on connectorone zero four.
\[emphasis mine\]6 C There is supposed to be a wirebetween connector one zero fourandconnector one zero two.Table 1: A Dialogue Fragment in the Circuit Fix-itShoppe Domainis in control and the other agent is passive (themaster-slave assumption).
For instance, in the LAD-DER system (Hendrix et al, 1978) the user has theinitiative while the computer is passive.
In contrast,the VODIS system (Young and Proctor, 1989) hasthe computer taking the initiative while the user ispassive.
Some dialogue systems (like GUS (Bobrowet al, 1977)) allow for a more mixed-initiative inter-action; however, the places where the user can takethe initiative are limited and defined a priori.
Thedialogue model of Smith (1995) allows for either thecomputer or the user to assume degrees of initiative;however, Smith presents no algorithm for the com-puter to change initiative during a dialogue.
Ourmodel of mixed-initiative dialogue allows either par-ticipant to be in control of the dialogue at any pointin time.Like Smith (1995) we believe that the level of ini-Many implemented ialogue systems are question- tiative in the dialogue should mirror the level of ini-answer systems with fixed initiative where one agent tiative in the task (which is a corollary to Grosz's278(1978) the structure of a dialog mirrors the structureo\] the underlying task).
Unlike previous research indialogue initiative, however, we attach an initiativelevel to each goal  in the task tree.
Thus an agentmay have initiative over one goal but not another.As goals get pushed and popped from the problem-solving stack, initiative changes accordingly.
Thusmany initiative changes are done implicitly basedon which goal is being solved.2.1 The  Set t ing  o f  In i t ia t ive  LevelsIn our model of dialogue, initiative levels for eachgoal are defined during the interaction based on 1)explicit and implicit initiative-changing utterancesand 2) competency evaluation.Expl ic i t  and Imp l i c i t  In i t ia t ive -Chang ing  Ut -te rances  Several researchers (Whittaker and Sten-ton, 1988; Walker and Whittaker, 1990) have notedthat dialogue control can be exchanged throughovert cues in the discourse.
Our model concentrateson two specific dialogue cues: questions and answers.When an agent AI asks another agent A2 to satisfya goal G, agent A2 gains initiative over goal G andall subgoals of G until agent A2 passes control ofone of those subgoals back to agent A1.
A similarinitiative-setting mechanism is fired if agent A1 an-nounces that it cannot satisfy goal G. When a goalhas been answered (satisfied) the problem-solvingstack is popped.
The initiative will now belong towhomever the initiative is for the goal on top of thestackJ.
In the following human-computer dialoguegathered by Smith and Hipp(Smith and Hipp, 1995),U: How do I fix this circuit?C: What is the LED displaying?U: Nothing.C: What is the switch at?U: Where is the switch?C: In the lower left corner.U: The switch is down.C: Put the switch up.all initiative changes can be accounted for by ex-plicit initiative-changing utterances or by poppingof the problem-solving stack due to goal resolutionas illustrated in Figure 1.Competency  Eva luat ion  for In i t ia t ive  Set-t ing How does an agent decide whether to ask itscollaborator for help?
An obvious approach is to askfor help when the agent is unable to satisfy a goal onits own.
This approach is the basic mechanism forseveral dialogue systems (Young et al, 1989; SmithiSince each participant is carrying out initiative eval-uation independently, there may be conflicts on whoshould be in control.
Numerous researchers have stud-ied how negotiation may be used to resolve these con-flicts (Guinn, 1994; Guinn, 1993a; Lambert and Car-berry, 1992; McRoy, 1993; Sidner, 1993)and Hipp, 1995; Guinn, 1994).
An additional ap-proach is to ask the collaborator for help if it is be-lieved that the collaborator has a better chance ofsolving the goal (or solving it more efficiently).
Suchan evaluation requires knowledge of the collaborat-ing agent's capabilities as well as an understandingof the agent's own capabilities.Our methodology for evaluating competency in-volves a probabilistic examination of the searchspace of the problem domain.
In the process of solv-ing a goal, there may be many branches that can betaken in an attempt o prove a goal.
Rather thanselecting a branch at random, intelligent behaviorinvolves evaluating (by some criteria) each possiblebranch that may lead toward the solution of a goalto determine which branch is more likely to lead to asolution.
In this evaluation, certain important fac-tors  are examined to weight various branches.
Forexample, during a medical exam, a patient may com-plain of dizziness, nausea, fever, headache, and itchyfeet.
The doctor may know of thousands of possiblediseases, conditions, allergies, etc.
To narrow thesearch, the doctor will try to find a pathology thataccounts for these symptoms.
There may be somediseases that account for all 5 symptoms, others thatmight account for 4 out of the 5 symptoms, and soon.
In this manner, the practitioner sorts and pruneshis list of possible pathologies.
Competency evalu-ation will be based on how likely an agent's branchwill be successful (based on a weighted factor analy-sis) and how likely the collaborator's branch will besuccessful (based on a weighted factor analysis and aprobabilistic model of the collaborator's knowledge).In Section 3 we will sketch out how this calcula-tion is made, present several mode selection schemesbased on this factor analysis, and show the results ofanalytical evaluation of these schemes.
In Section 4we will present he methodology and results of usingthese schemes in a simulated ialogue environment.3 Mathematical Analysis ofEfficiencyOur model of best-first search assumes that for eachgoal there exists a set of n factors, f l , .
- .
,  f~, whichare used to guide the search through the problem-solving space.
Associated with each factor are twoweights, wi, which is the percentage of times a suc-cessful branch will have that factor and xi which isthe percentage of all branches that satisfy fi.
If anagent, a, knows q~',..., qn a percentage of the knowl-edge concerning factors f l , .
.
.
,  f~, respectively, andassuming independence of factors, using Bayes' rulean agent can calculate the success likelihood of each279U: How do I fix Ithis circuit?
~/goal(fix_circuit).Initiative: ComputerProblem-Solving SlackITHINKING\]>observe(switch).hdtiutive: Computerdebug(led.oft).bfftiative: Computergoal(fix_circuit).lnititaive: ComputerProblem-Solving StackC: What is the switch at?\[THINKING\]<:observe(switch).Inith~tive: Userdebug(led,off).Initiutive: Computergoal(fixcircuit).Initiutive: ComputerProblem-Solvlng Stackraise(switch).Initiative: Userdebug(led.off).Inithttive: Computergoal(fix_circuit).bdtiative: ComputerProblem-Solving StackU: Where is the switch?
::>C: Put the switch up.observe(led).Initiative: Computergoal(fix_circuit).Initiative: ComputerProblem-Solving Stackdebug(led,off).Initiative: Computergoal(fix_circuiO.Initiative: ComputerProblem-Solving Stacklocate(switch).Initiative: Computerobserve(switch).bdtiutive: Userdebug(led,offLInitiative: Computergoal(fix_circuit).hdtiative: ComputerProblem-Solving Stackraise(switch).Initiative: Computerdebug(led,off).Initiative: Computergoal(fix_circuit).hlitiative: ComputerProblem-Solving StackC: What is theLED displaying?\[THINKING\]C: In the lower left comer.\[POPI\[THINKING\]observe(led).Initiative: Usergoal(fix_circuit).Initiative: ComputerProblem-Soiling StackU: Nothing.
/IPOP\] iI goal(fix circuit).
IInitiutive: ComputerProblem-Solving Stackobserve(switch).lnitiutive: U.~erdebug(led,oil).Initiative: Computergoal(fix_circuit).blitiative: ComputerProblem-Solving StackIU: The switch is down II24--I debug(ll~d,off).I goal(fixcircuit).Initiative."
ComputerProblem-Solving StackFigure h Tracking Initiative via Explicit Initiative-Changing Utterances and Problem-Solving Stack Manip-ulation280possible branch of a goal G that it knows:p(b) = 1 - f I  1 - F(i)wi (1/k) (1)i=-I Xiwhere b is a branch out of a list of k branches andF(i) = 1 if the agent knows branch b satisfies factorf /and  F(i) = x i (1 -qa)  otherwise.
\[Note: x i (1 -qa)is the probability that the branch satisfies factor fibut the agent does not know this fact.\] We definethe sorted list of branches for a goal G that an agentknows, \[b~,... , b~\], where for each be~, p(b~) is thelikelihood that branch b~ will result in success wherep(b~) >= p(b~), Vi < j.3.1 Eff ic iency Ana lys is  of  D ia logueIn i t iat iveFor efficient initiative-setting, it is also necessaryto establish the likelihood of success for one's col-laborator's lSt-ranked branch, 2nd-ranked branch,and so on.
This calculation is difficult because theagent does not have direct access to its collabora-tor's knowledge.
Again, we will rely on a proba-bilistic analysis.
Assume that the agent does notknow exactly what is in the collaborator's knowledgebut does know the degree to which the collaboratorknows about the factors related to a goal.
Thus, inthe medical domain, the agent may know that thecollaborator knows more about diseases that accountfor dizziness and nausea, less about diseases thatcause fever and headache, and nothing about dis-eases that cause itchy feet.
For computational pur-poses these degrees of knowledge for each factor canbe quantified: the agent, a, may know percentage q~of the knowledge about diseases that cause dizzi-ness while the collaborator, c, knows percentage qCof the knowledge about these diseases.
Suppose theagent has 1) a user model that states that the col-laborator knows percentages q{, q~,..., q~, about fac-tors f l , f2 , .
.
.
, fm respectively and 2) a model ofthe domain which states the approximate numberof branches, N'.
Assuming independence, the ex-pected number of branches which satisfy all n factorsis ExpAUN = N" l-Ii=l Xi" Given that a branch sat-isfies all n factors, the likelihood that the collabora-tor will know that branch is rZin_l qC.
Therefore, theexpected number of branches for which the collabo-rator knows all n factors is ExpAl lN I~i~=1 qg.
Theprobability that one of these branches is a success-producing branch is 1 - \ [L~I  1 -w i  ~ (from Equa-tion 1).
By computing similar probabilities for eachcombination of factors, the agent can compute thelikelihood that the collaborator's first branch will bea successful branch, and so on.
A more detailed he-count of this evaluation is given by Guinn (1993b;1994).We have investigated four initiative-settingschemes using this analysis.
These schemesdo not necessarily correspond to any observablehuman-human or human-computer  dialogue behav-ior.
Rather, they provide a means  for exploring pro-posed dialogue initiative schemes.Random In Random mode,  one agent is given ini-tiative at random in the event of a conflict.
Thisscheme provides a baseline for initiative settingalgorithms.
Hopefully, a proposed algorithmwill do better than Random.SingleSelection In SingleSelection mode,  the moreknowledgeable agent  (defined by which agenthas the greater total percentage of knowledge)is given initiative.
The  initiative is set through-out the dialogue.
Once  a leader is chosen, theparticipants act in a master-slave fashion.Cont inuous  In Cont inuous mode,  the more  knowl-edgeable agent (defined by which agent's first-ranked branch is more  likely to succeed) is ini-tially given initiative.
If that branch fails, thisagent's second-ranked branch is compared tothe other agent's first-ranked branch with thewinner gaining initiative.
In general if Agent 1is working on its ith-ranked branch and Agent 2is working on its jth-ranked branch, we compareA1 A1 p (h i )  toOrac le  In Oracle mode, an all-knowing mediatorselects the agent that has the correct branchranked highest in its list of branches.
Thisscheme is an upper bound on the effectiveness ofinitiative setting schemes.
No initiative settingalgorithm can do better.As knowledge is varied between participants wesee some significant differences between the variousstrategies.
Figure 2 summarizes this analysis.
The xand y axis represent the amount of knowledge thateach agent is given 2, and the z axis represents thepercentage of branches explored from a single goal.SingleSelection and Continuous modes perform sig-nificantly better than Random mode.
On aver-age Continuous mode results in 40% less branchessearched per goal than Random.
Continuous mode2This distribution is normalized to insure that all theknowledge is distributed between each agent.
Agent 1will have ql + (1 ql ) (1-  2 - q ) ql+q2 percent of the knowl-edge while Agent 2 will have q2 + (1 - ql)(1 - q2) q~ql  "~-q2percent of the knowledge.
If ql + q2 = O, then setql -= q2 -= 0.5.281E q..,1.
Rando~ o .
~::,  $ingleSdcctioa xm Co~tiw~o~xxxxxxxx:X:.
*MXX::XX:XMXXXXXX:  Xx::x:::,:::x::: I~ - ,  C1 ~ ~xxxx x:  ~u I~0X-axis: q iZ-axis: E~ect.e4pezceat~g?
of q~ o.7~bzaaches explozed ~.Figure 2: An Analytical Comparison of Dialogue Initiative-Setting Schemesperforms between 15-20% better than SingleSelec-tion.
The large gap between Oracle and Continuousis due to the fact that Continuous initiative selectionis only using limited probabilistic information aboutthe knowledge of each agent.4 Computer SimulationsThe dialogue model outlined in this paper hasbeen implemented, and computer-computer dia-logues have been carried out to evaluate the modeland judge the effectiveness of various dialogue initia-tive schemes.
In a methodology similar to that usedby Power (1979), Carletta (1992) and Walker (1993),knowledge is distributed by a random process be-tween agents, and the resulting interaction betweenthese collaborating agents is observed.
This method-ology allows investigators to test different aspects ofa dialogue theory.
Details of this experimental strat-egy are given by Guinn (1995).4.1 The  Usage of  Computer-ComputerDialoguesThe use of computer-computer simulations to studyand build human-computer dialogue systems iscontroversial.
Since we are building computa-tional models of dialogue, it is perfectly reason-able to explore these computational models throughcomputer-computer simulations.
The difficultylies in what these simulations ay about human-computer or computer-computer dialogues.
Thisauthor argues that computer-computer simulationsare one layer in the multi-layer process of build-ing human-computer dialogue systems.
Computer-computer simulations allow us to evaluate our com-putational models and explore issues that can notbe resolved analytically.
These simulations can helpus prune out some mechanisms and suggest mech-anisms that may work well in a human-computersystem.
For instance, if the dialogues that resultin a computer-computer conversation are incoherentto a human observer, this suggests that the dialoguemechanisms employed may be inappropriate for ahuman-computer system.
On the other hand, if theresulting dialogues are coherent and exhibit featuresthat are desired in a human-computer system, thissuggests that these mechanisms may work well in ahuman-computer system.
The final test, of course,must be in the implementation f a human-computerdialogue system.
The scarcity of such systems ug-gests that it is an extremely expensive process tobuild a functional human-computer dialogue system,and computer-computer simulations can assist in re-ducing these costs.4.2 Simulation ResultsA murder mystery domain was created with 22suspects.
An annotated example dialogue from acomputer-computer collaboration in this domain ispresented in Figure 3.
Agents were given partialinformation through a random process.
By usingeither Random mode or Continuous mode, we canevaluate the effect of those mechanisms in this ex-perimental environment.
Table 2 presents resultscomputed from 5008 collaborations where the agents282Speaker UtteranceWatsonHo lmesWatsonHo lmesWatsonHo lmesWatsonHohnesHolmesWatsonHolmesWatsonHolmesWatsonHolmesWatsonHolmesWatsonWatsonHolmesWatsonHolmesWatsonWho is the murderer of Lord Dunsmore?
~Is it the case that Suspect16 is the murderer of Lord Dunsmore?Is it the case that Suspectl6 had a motive to murder Lord Dunsmore?Suspectl6 had a motive to murder Lord Dunsmore.Is it the case that Suspectl6 had access to the poison?I am unable to determine whether Suspectl6 had access to the poison.I am unable to determine whether Suspectl6 is the murderer of LordDunsmore.I am unable to determine whether Suspectl6 is the murderer of LordDunsmore.Who is the murderer of Lord Dunsmore?
bIs it the case that Suspectl0 is the murderer of Lord Dunsmore?Is it the case that Suspectl0 had a motive to murder Lord Dunsmore?Suspectl0 had a motive to murder Lord Dunsmore.Is it the case that Suspectl0 had access to the poison?Suspectl0 had access to the poison.Is it the case that Suspectl0 had an opportunity to administer thepoison?I am unable to determine whether Suspectl0 had an opportunity toadminister the poison.I am unable to determine whether Suspectl0 is the murderer of LordDunsmore.I am unable to determine whether Suspectl0 is the murderer of LordDunsmore.Is it the case that Suspect9 is the murderer of Lord Dunsmore?Is it the case that Suspect7 is the murderer of Lord Dunsmore?
cI have proven that Suspect9 has a motive to murder Lord Dunsmore andSuspect9 had access to the poison, dI have proven that Suspect7 had access to the poison, Suspect7 hadan opportunity to administer the poison, and Suspect7 has a criminaldisposition.
~Suspect7 is the murderer of Lord Dunsmore.
fawatson gives control of the investigation over to Holmes.
Each part ic ipant uses the Cont inuous Mode a lgor i thm to determine whoshould be in control.bHolmes is giving up control of direct ing the invest igat ion here.CHolmes is challenging Watson's investigative choice.dwatson negot iates for his choice.eHolmes negotiates for his choice.fWatson now has enough information to prove that  Suspect7 is the murderer.Figure 3: A Sample Dialogue283had to communicate o solve the task.Random ContinuousTimes (secs) 82.398 44.528of Utterances 39.921 26.650~uspects Examined 6.188 3.412Table 2: Data on 5008 Non-trivial Dialogues fromthe Murder Mystery Domain5 Extension to Human-ComputerDialoguesCurrently, two spoken-dialogue human-computersystems are being developed using the underlyingalgorithms described in this paper.
The Duke Pro-gramming Tutor instructs introductory computerscience students how to write simple Pascal pro-grams by providing multiple modes of input andoutput (voice/text/graphics) (Bierman et al, 1996).The Advanced Maintenance Assistant and Trainer(AMAT) currently being developed by Research Tri-angle Institute for the U.S. Army allows a mainte-nance trainee to converse with a computer assistantin the diagnosis and repair of a virtual MIA1 tank.While still in prototype development, preliminaryresults suggest hat the algorithms that were suc-cessful for efficient computer-computer collabora-tion are capable of participating in coherent human-machine interaction.
Extensive testing remains to bedone to determine the actual gains in efficiency dueto various mechanisms.One tenet of our theory is that proper initiativesetting requires an effective user model.
There areseveral mechanisms we are exploring in acquiring thekind of user model information necessary for the pre-viously described ialogue mode algorithms.
Stereo-types (Rich, 1979; Chin, 1989) are a valuable toolin domains where user classification is possible andrelevant.
For instance, in the domain of militaryequipment maintenance, users can be easily classi-fied by rank, years of experience, quipment famil-iarity and so on.
An additional source of user modelinformation can be dynamically obtained in envi-ronments where the user interacts for an extendedperiod of time.
A tutoring/training system has theadvantage of knowing exactly what lessons a stu-dent has taken and how well the student did on in-dividual essons and questions.
Dynamically mod-ifying the user model based on on-going problemsolving is difficult.
One mechanism that may proveparticularly effective is negotiating problem-solvingstrategies (Guinn, 1994).
The quality of a collabora-tor's negotiation reflects the quality of its underlyingknowledge.
There is a tradeoff in that negotiationis expensive, both in terms of time and computa-tional complexity.
Thus, a synthesis of user model-ing techniques will probably be required for effectiveand efficient collaboration.6 AcknowledgementsWork on this project has been supported by grantsfrom the National Science Foundation (NSF-IRI-92-21842 ), the Office of Naval Research (N00014-94-1-0938), and ACT II funding from STRICOM for theCombat Service Support Battlelab.ReferencesA.
Bierman, C. Guinn, M. Fulkerson, G. Keim,Z.
Liang, D Melamed, and K Rajagopalan.
1996.Goal-Oriented multimedia dialogue with variableinitiative.
In submitted for publication.D.G.
Bobrow, R.M.
Kaplan, M. Kay, D.A.
Norman,H.
Thompson, and T. Winograd.
1977.
GUS, aframe driven dialog system.
Artificial Intelligence,8:155-173.J.
Carletta.
1992.
Planning to fail, not failing toplan: Risk-taking and recovery in task-orienteddialogue.
In Proceedings of the l~th Interna-tional Conference on Computational Linguistics(COLING-92), pages 896-900, Nantes, France.D.N.
Chin.
1989.
KNOME: Modeling what the userknows in UC.
In A. Kobsa and W. Wahlster, ed-itors, User Models in Dialog Systems, pages 74-107.
Springer-Verlag, New York.B.
J. Grosz.
1978.
Discourse analysis.
In D. Walker,editor, Understanding Spoken Language, chap-ter IX, pages 235-268.
Elsevier, North-Holland,New York, NY.C.I.
Guinn.
1993a.
Conflict resolution in collabora-tive discourse.
In Computational Models of Con-flict Management in Cooperative Problem Solving,Workshop Proceedings from the 13th InternationalJoint Conference on Artificial Intelligence, Cham-bery, France, August.Curry I. Guinn.
1993b.
A computationalmodel of dialogue initiative in collaborative dis-course.
Human-Computer Collaboration: Recon-ciling Theory, Synthesizing Practice, Papers fromthe 1993 Fall Symposium Series, Technical ReportFS-93-05.Curry I. Guinn.
1994.
Meta-Dialogue Behaviors:Improving the EJficiency of Human-Machine Di-alogue -- A Computational Model of Variable Ini-tiative and Negotiation in Collaborative Problem-Solving.
Ph.D. thesis, Duke University.284Curry I. Guinn.
1995.
The role of computer-computer dialogues in human-computer dialoguesystem development.
AAAI Spring Symposiumon Empirical Methods in Discourse Interpretationand Generation, Technical Report SS-95-06.G.G.
Hendrix, E.D.
Sacerdoti, D. Sagalowicz, andJ.
Slocum.
1978.
Developing a natural anguageinterface to complex data.
ACM Transactions onDatabase Systems, pages 105-147, June.L.
Lambert and S. Carberry.
1992.
Modeling ne-gotiation subdialogues.
Proceedings o\] the 30thAnnual Meeting o\] the Association for Computa-tional Linguistics, pages 193-200.S.
McRoy.
1993.
Misunderstanding and the ne-gotiation of meaning.
Human-Computer Collab-oration: Reconciling Theory, Synthesizing Prac-tice, Papers from the 1993 Fall Symposium Series,AAAI Technical Report FS-93-05, September.R.
Power.
1979.
The organization of purposefuldialogues.
Linguistics, 17.E.
Rich.
1979.
User modeling via stereotypes.
Cog-nitive Science, 3:329-354.C.
L. Sidner.
1993.
The role of negotiation incollaborative activity.
Human-Computer Collab-oration: Reconciling Theory, Synthesizing Prac-tice, Papers from the 1993 Fall Symposium Series,AAAI Technical Report FS-93-05, September.R.W.
Smith and D.R.
Hipp.
1995.
Spoken NaturalLanguage Dialog Systems: A Practical Approach.Oxford University Press, New York.R.W.
Smith, D.R.
Hipp, and A.W Biermann.
1992.A dialog control algorithm and its performance.In Proceedings o\] the 3rd Conference on AppliedNatural Language Processing.M.
Walker and S Whittaker.
1990.
Mixed ini-tiative in dialogue: An investigation into dis-course segmentation.
In Proceedings of the 28thAnnual Meeting of the Association for Computa-tional Linguistics, pages 70-78.M.
A. Walker.
1993.
Informational Redundancy andResource Bounds in Dialogue.
Ph.D. thesis, Uni-versity of Pennsylvania.S.
Whittaker and P. Stenton.
1988.
Cues and con-trol in expert-client dialogues.
In Proceedings ofthe 26th Annual Meeting of the Association/orComputational Linguistics, pages 123-130.S.J.
Young and C.E.
Proctor.
1989.
The design andimplementation of dialogue control in voice oper-ated database inquiry systems.
Computer Speechand Language, 3:329-353.S.R.
Young, A.G. Hauptmann, W.H.
Ward, E.T.Smith, and P. Werner.
1989.
High level knowl-edge sources in usable speech recognition systems.Communications o\] the ACM, pages 183-194, Au-gust.285
