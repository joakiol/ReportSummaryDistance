Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 113?117,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Temporal Simulator for Developing Turn-Taking Methods forSpoken Dialogue SystemsEthan O. Selfridge and Peter A. HeemanCenter for Spoken Language UnderstandingOregon Health & Science University20000 NW Walker Rd., Beaverton, OR, 97006selfridg@ohsu.edu, heemanp@ohsu.eduAbstractDeveloping sophisticated turn-taking behavioris necessary for next-generation dialogue sys-tems.
However, incorporating real users intothe development cycle is expensive and cur-rent simulation techniques are inadequate.
Asa foundation for advancing turn-taking behav-ior, we present a temporal simulator that mod-els the interaction between the user and thesystem, including speech, voice activity de-tection, and incremental speech recognition.We describe the details of the simulator anddemonstrate it on a sample domain.1 Introduction and BackgroundEffective turn-taking is critical for successfulhuman-computer interaction.
Recently, approacheshave been proposed to improve system turn-takingbehavior that use reinforcement learning (Jonsdot-tir et al, 2008; Selfridge and Heeman, 2010), de-cision theory (e.g., Raux and Eskenazi, 2009), andhard-coded policies (e.g., Skantze and Schlangen ,2009).
Some of these methods model turn-takingas content-free decisions (Jonsdottir et al, 2008;Skantze and Schlangen, 2009), while others primar-ily rely on dialogue context (Selfridge and Heeman,2010) and lexical cues (e.g., Raux and Eskenazi,2009).
Turn-taking continues to be an area of ac-tive research and its development is vital for next-generation dialogue systems, especially as they al-low for more mixed initiative interaction.Researchers have turned to simulation since de-veloping a dialogue system with real users is ex-pensive, time consuming, and sometimes impossi-ble.
Some turn-taking simulations have been highlystylized and only model utterance content, failing togive a realistic model of timing (Selfridge and Hee-man, 2010).
Others have modeled a content-freeform of turn-taking and only attend to timing andprosodic information (Jonsdottir et al, 2008; Bau-mann, 2008; Padilha and Carletta, 2002).
The for-mer is insufficient for the training of deployable real-time systems, and the latter neglect an important as-pect of turn-taking: semantic information (Gravanoand Hirschberg, 2011).The overall goal is to develop a simulation en-vironment to train behavior policies that can betransferred with minimal modifications to produc-tion systems.
This paper presents some first stepstowards this goal.
We describe a temporal simula-tor that models the timing and content of both userand system speech, as well as that of incrementalspeech recognition (ISR) and voice activity detec-tion (VAD).
We detail the overall temporal simulatorarchitecture, the design of the individual agents thatsimulate dialogue, and an instantiation of a simpledomain.
To demonstrate the utility of the simulator,we implement multiple turn-taking polices and use itto compare these policies under conditions of vary-ing reaction time and speech recognition accuracy.2 Temporal Simulation FrameworkWe now describe the details of the temporal sim-ulator.
Inspired by the Open Agent Architecture(Martin et al, 1999), it is composed of a numberof agents, each running as a separate computer pro-cess.
We first describe the time keeping procedureand then the overall agent communication structure.113Time Keeping: To provide a useful training envi-ronment, the simulator must realistically model, andrun much faster than, ?real-time?.
To do this, thesimulator keeps an internal clock that advances tothe next time slice when all agents have been runfor the current time slice.
This structure allows thesimulator to run far faster than ?real-time?
while sup-porting realistic communication.
This framework issimilar to the clock cycle described by Padilha et al(2002).Agent Communication: Agents use messages tocommunicate.
Messages have three components:the addressee, the content and a time stamp.
Timestamps dictate when the content is to be processedand must always be for a future, not the current, timeslice, as the alternative would imply instantaneouscommunication and overly complicate the softwarearchitecture.
A central hub receives all messagesand passes them to the intended recipient agent atthe appropriate time.
At every slice, each agent runstwo procedures: one that retrieves messages and onethat can send messages.
If there are multiple mes-sages intended for the same time slice, the agentcompletely processes one before moving to the next.3 Dialogue SimulatorWe use the above temporal simulator to simulate di-alogue.
At present, we focus on dyadic interactionand have three agents that are run in a strict order atevery time slice: User, ISR, and System.
Time slicesare modeled as 10 millisecond (ms) increments, asthis is the time scale that speech recognizers run at.In general, the User agent sends messages to theISR agent that sends messages to the System agent.The System agent generally sends messages to boththe User agent and the ISR agent.
The behavior ofall three agents rely on parameters (Table 1) thatmay either be set by hand or estimated from data.The User and System agents have near identical con-struction, the primary difference being that the Sys-tem can misunderstand User speech.User and System Design: Agent speech is gov-erned by a number of timing parameters.
The Take-Turn parameter specifies when the agent will beginspeaking the selected utterance.
The agent gets thefirst word of the utterance, sets the Word Length pa-rameter, and ?begins?
to speak by sending a speechevent message.
The agent outputs the word afterthe specified Word Length, and sets the Inter-WordPause parameter that governs when the next wordwill begin.
When the agent completes the utter-ance, it waits until a future time slice to start an-other (as governed by the Inter-Utterance Pause pa-rameter).
However, if the listening agent interruptsmid-utterance, the speaking agent stops speakingand will not complete the utterance.
Any dialogueagent architecture can be used, providing the inputand output fit with the above specifications.ISR Design: The ISR agent works as both an In-cremental Speech Recognizer and a VAD.
We cur-rently model uncertainty in recognition but not inthe VAD, though this is certainly a plausible andworthwhile addition.
When the ISR agent receivesthe speech event from the User, it sets the VADSpeech Start parameter that models lag in speechdetection, and the Speech End (no word) parameterthat models situations where the user starts speakingbut stops mid-word and produces an unrecognizablesound.
When the word is received from the User,the Speech End (word) parameter is set and a par-tial phrase result is generated based on the probabil-ity that the word will be correctly recognized.
Thisprobability is then used as the basis for a confidencescore that is packaged with the partial phrase result.A Recognition Lag parameter governs the time be-tween User speech and the output of partial phraseresults to the System.
The form of ISR we modelrecognizes words cumulatively (see Figure 1 for anexample) though the confidence score, at present, isonly for the newly recognized word.
The recognizerwill continue to output partials from User words un-til the User stops speaking or the System sends amessage to stop recognizing.
One critical aspect ofISR which we are not modeling is partial instability,where partials are revised as recognition progresses.Partial instability is an area of active research (e.g.Baumann et al 2009) and, while revisions may cer-tainly be modeled in the future, we chose not to forsimplicity?s sake.
We feel that, at present, the Recog-nition Lag parameter is sufficient to model the timefor a partial to become stable.114Table 1: Parameters and demonstration values (ms)Conversant AgentsInter-Word pause (Usr) ?
= 200, ?
= 100Inter-Word pause (Sys) 100Inter-Utt.
pause ?
= 1000, ?
= 500Word Length 400Take-Turn (Usr) 500/200Take-Turn (Sys) 750/100ISR AgentRecog.
Acc.
variableRecog.
Lag 300VADSpeech Start 100Speech End (word) 200Speech End (no word) 6004 Simulation demonstrationWe now demonstrate the utility of the temporal sim-ulator by showing that it can be used to evaluatedifferent turn-taking strategies under conditions ofvarying ASR accuracy.
This is the first step beforeusing it to train policies for use in a live dialoguesystem.For this demonstration the conversant agents, theSystem and User, are built according to the Infor-mation State Update approach (Larsson and Traum,2000), and perform an update for every message as-sociated with the current time slice.
The conver-sant agents are identical except for individual rulesets.
Four types of rule sets are common acrossconversant agents: UNDERSTANDING rules, that up-date the IS using raw message content; DELIBERA-TION rules, that update the IS by comparing new in-formation to old; UTTERANCE rules, that select thenext utterance based on dialogue context; and TURNFigure 1: Sample dialogue with timing informationrules, that select the time to begin the new utteranceby modifying the Take Turn parameter.
Rule sets areexecuted in this order with one exception.
After theUNDERSTANDING rules, the System agent has AC-CEPTANCE rules that use confidence scores to decidewhether to understand the recognition or not.Temporal Simulation Example: We constructeda simple credit card domain, similar to Skantze andSchlangen (2009), where the User says four utter-ances of four digits each.
The System must implic-itly confirm every number and if it is correct, theUser continues.1 It can theoretically do this at anytime, immediately after the word is recognized, af-ter an utterance, or after multiple utterances.
If thesystem says a wrong number the User interrupts theSystem with a ?no?
and begins the utterance again.The System has a Non-Understanding (NU) confi-dence score threshold set at 0.5.
After an NU, theSystem will not understand any more words and willeither confirm any digits recognized before the NUor, if there are no words to confirm, will say an NUutterance (?pardon??).
The User says ?yes?
to thefinal, correct confirmation.
To maintain simplicity,?yes?
and ?no?
are always accurate.
If this were notthe case, there would be a number of dialogues thatwere not successful.
The User takes the turn in twoways.
It either waits 500 ms after a System utteranceto speak or interrupts 200 ms after the System con-firms an misrecognized word, which is in line withhuman reaction time (Fry, 1975).We implemented three different turn-takingstrategies: two Fixed and one Context-based.
Us-ing the Fixed strategy the System either uses a Slowpolicy, waiting 750 ms after no user speech is de-tected, or a Fast policy, waiting only 100 ms. TheFast reaction time results in the System interrupt-ing the User during an utterance when the inter-wordpause becomes longer than 200 ms.
This is becausethe VAD Speech End parameter is 100 ms and theSystem is waiting for 100 ms of silence after SpeechEnd.
The Slow reaction time results in far less in-terruptions.
The Context-based turn-taking strategyuses the recognition score to choose its turn-takingbehavior.
The motivation is that one would want1Unlike an explicit confirmation (?I heard five.
Is thatright??
), an implicit confirm (?Ok, five?)
does not necessitatea strict ?yes?
or ?no?
response.115Figure 2: Mean Time and Interruption for different turn-taking polices and ASR accuracy conditionsto confirm low-confidence recognitions sooner thanthose with high confidence.
If any unconfirmed re-sult has scores less than 0.8 then the System uses theFast reaction time to try to confirm or reject as soonas possible.
Alternatively, if the results all have highconfidences, it can wait until a longer user pause(generally between utterances) by using the Slow re-action time.
All parameter values are shown in Table1.Figure 1 shows a dialogue fragment of a Systemusing the Context-based turn-taking policy.
Num-bers are used for the sake of brevity.
The start ofa box surrounding a word corresponds to when theSpeech message was sent (from the User agent to theISR agent) and the end of the box to when the wordhas been completed and recognition lag timer be-gins.
The point of the ISR box refers to the time slicewhen the partial phrase result and score were sent tothe System.
Note how after the third User word theSystem interrupts to confirm the utterance, since theconfidence score of a previous word dropped below0.8.
Also note how the User interrupts the Systemafter it confirms a wrong number.Comparing turn-taking policies: We evaluatedthe three (two Fixed and one Context-based) turn-taking policies in two conditions of ASR accuracy:Low Error, where the probability of correctness was95%; and High Error, where the probability of cor-rectness was 75%.
We compared the mean dialoguetime (left Figure 2) and the mean number of in-terruptions per dialogue (right Figure 2).
For dia-logue time, we find that all turn-taking policies per-form similarly in the Low Error condition.
How-ever, in the High Error condition the Slow reac-tion time performs much worse since it cannot ad-dress poor recognitions with the speed of the othertwo.
For interruption, the Fast and Context-drivenpolicies have far more than the Slow for the HighError condition.
However, in the Low Error con-dition the Fast policy interrupts far more than theContext-driven.
Given that natural behavior is onegoal of turn-taking, interruption, while effective athandling High Error rates, should be minimized.The Context-based policy provides support for in-terruption when it is needed (High Error Condition)and reduces it when it is not (Low Error Condition).The other policies are either unable to interrupt at all(Slow), increasing the dialogue time, or due to a lackthe flexibility (Fast), interrupt constantly.5 ConclusionWe take the first steps towards a simulation approachthat characterizes both the content of conversantspeech as well as its timing.
The temporal simula-tor models conversant utterances, ISR, and the VAD.The simulator runs quickly (100 times faster thanreal-time), and is simple and highly flexible.
Us-ing an example, we demonstrated that the simula-tor can help understand the ramifications of differ-ent turn-taking policies.
We also highlighted boththe temporal nature of turn-taking ?
interruptions,reaction time, recognition lag...etc.
?
and the con-tent of utterances ?
speech recognition errors, con-fidence scores, and wrong confirmations.
Plans forfuture work include adding realistic prosodic mod-eling and estimating model parameters from data.AcknowledgmentsWe thank to the reviewers for their thoughtful sug-gestions and critique.
We acknowledge fundingfrom the NSF under grant IIS-0713698.116ReferencesT.
Baumann, M. Atterer, and D. Schlangen.
2009.Assessing and improving the performance of speechrecognition for incremental systems.
In Proc.
NAACL:HLT, pages 380?388.T.
Baumann.
2008.
Simulating spoken dialogue witha focus on realistic turn-taking.
In Proc.
of ESSLLIStudent Session.D.
B. Fry.
1975.
Simple reaction-times to speech andnon-speech stimuli.
Cortex, 11(4):355?360.A.
Gravano and J. Hirschberg.
2011.
Turn-taking cuesin task-oriented dialogue.
Computer Speech & Lan-guage, 25(3):601?634.G.R.
Jonsdottir, K.R.
Thorisson, and Eric Nivel.
2008.Learning smooth, human-like turntaking in realtimedialogue.
In Proc.
of IVA, pages 162?175.S.
Larsson and D. Traum.
2000.
Information state and di-alogue managment in the trindi dialogue move enginetoolkit.
Natural Language Engineering, 6:323?340.D.L.
Martin, Adam J. Cheyer, and Douglas B. Moran.1999.
The open agent architecture: A framework forbuilding distributed software systems.
Applied Ar-tificial Intelligence: An International Journal, 13(1-2):91?128.E.
Padilha and J. Carletta.
2002.
A simulation of smallgroup discussion.
In Proc.
of EDILOG, pages 117?124.A.
Raux and M. Eskenazi.
2009.
A finite-state turn-taking model for spoken dialog systems.
In Proc.
ofHLT/NAACL, pages 629?637.G.
Skantze and D. Schlangen .
2009.
Incremental di-alogue processing in a micro-domain.
In Proc.
ofEACL, pages 745?753.E.O.
Selfridge and P.A.
Heeman.
2010.
Importance-Driven Turn-Bidding for spoken dialogue systems.
InProc.
of ACL, pages 177?185.117
