Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 142?145,Sydney, July 2006. c?2006 Association for Computational LinguisticsOn Using Ensemble Methods for Chinese Named Entity RecognitionChia-Wei Wu Shyh-Yi Jan Richard Tzong-HanTsaiWen-Lian HsuInstitute of Information Science, Academia Sinica, Nankang, Taipei,115, Taiwan{cwwu,shihyi,thtsai,Hsu}@iis.sinica.edu.twAbstractIn sequence labeling tasks, applying dif-ferent machine learning models and fea-ture sets usually leads to different results.In this paper, we exploit two ensemblemethods in order to integrate multipleresults generated under different condi-tions.
One method is based on majorityvote, while the other is a memory-basedapproach that integrates maximum en-tropy and conditional random field clas-sifiers.
Our results indicate that thememory-based method can outperformthe individual classifiers, but the major-ity vote method cannot.1 IntroductionSequence labeling and segmentation tasks havebeen studied extensively in the fields of computa-tional linguistics and information extraction.
Sev-eral tasks, including, word segmentation, andsemantic role labeling, provide rich informationfor various applications, such as segmentation inChinese information retrieval and named entityrecognition in biomedical literature mining.Probabilistic state automata models, such as theHidden Markov model  (HMM) [6] and condi-tional random fields (CRF) [5] are some of best,and therefore most popular, approaches for se-quence labeling tasks.
Both HMM and CRF con-sider that the state transition and the stateprediction are conditional on the observation ofdata.
The advantage of the CRF model is thatricher feature sets can be considered, because,unlike HMM, it does not make a dependence as-sumption.
However, the obvious drawback of theCRF model is that it needs more computing re-sources, so we can not apply all the features ofthe model.
One possible way to resolve this prob-lem is to effectively combine the results of vari-ous individual classifiers trained with differentfeature sets.
In this paper, we use two ensemblemethods to combine the results of the classifiers.We also combine the results generated by twomachine learning models: maximum entropy(ME) [1] and CRF.
One ensemble method isbased on the majority vote [3], and the other isthe memory based learner [7].
Although the en-semble methods have been applied in some se-quence labeling tasks [2],[3], similar work inChinese named entity recognition is scarce.Our Chinese named entity tagger uses a charac-ter-based model.
For English named entity tasks,a character-based NER model proposed by DanKlein [4] proves the usefulness of substringswithin words.
In Chinese NER, the character-based model is more straightforward, since thereare no spaces between Chinese words and eachChinese character is actually meaningful.
An-other reason for using a character-based model isthat it can avoid the errors sometimes made by aChinese word segmentor.The remainder of this paper is organized as fol-lows.
In the Section 2, we introduce the machinelearning models, the features we apply in the ma-chine learning models, and the ensemble methods.In Section 3, we briefly describe the experimentaldata and the experiment results.
Then, in Section4, we present our conclusions..2 Method2.1 Machine Learning ModelsIn this section, we introduce ME and CRF.Maximum EntropyME[1] is a statistical modeling technique usedfor estimating the conditional probability of atarget label based on given information.
Thetechnique computes the probability p(y|x), wherey denotes all possible outcomes of the space, andx denotes all possible features of the space.
Thecomputation of p(y|x) depends on a set of fea-142tures in x; the features are helpful for makingpredictions about the outcomes, y.Given a set of features and a training set, the MEestimation process produces a model, in whichevery feature fi has a weight ?i.
The ME modelcan be represented by the following formula:( ) ( ) ( )???????
?= ?iii yxfxzxyp ,exp| ?1,( ) ( )?
?
???????
?=y iii yxfxz ,exp ?.The probability is derived by multiplying theweights of the active features (i.e., those fi (y,x) =1).Conditional Random FieldA conditional random field (CRF)[5] can be seenas an undirected graph model in which the nodescorresponding to the label sequence y are condi-tional on the observed sequence x.
The goal ofCRF is to find the label sequence y that has themaximized probability, given an observation se-quence x.
The formula for the CRF model can bewritten as:( ) ( ) ( )( )xyxxy ,exp1| jj j FZP ?= ?
,where ?j is the parameter of a corresponding fea-ture Fj , Z(x) is an normalizing factor, and Fj canbe written as:( ) (?
= ?= ni iiij iyyfF 0 1 ,,,, xxy ),where i means the relative position in the se-quence, and yi-1 and yi denote the label at positioni-1 and i respectively.
In this paper, we only con-sider linear chain and first-order Markov assump-tion CRFs.
In NER applications, a featurefunction fj (yi-1, yi, x, i) can be set to checkwhether x is a specific character, and whether yi-1is a label (such as Location) and yi is a label (suchas Others).2.2 Chinese Named Entity RecognitionIn this section, we present the features applied inour CRF and ME models, namely, characters,words, and chuck information.Character FeaturesThe character features we apply in the CRFmodel and the ME model are presented in Tables1 and 2 respectively.
The numbers listed in thefeature type column indicate the relative positionof a character in the sliding window.
For example,-1 means the previous character of the targetcharacter.
Therefore, the characters in those posi-tions are applied in the model.
The numbers inparentheses mean that the feature includes acombination of the characters in those positions.The unigrams in Tables 1 and 2 indicate that thelisted features only consider to their own labels,whereas the bigram model considers the combi-nation of the current label and the previous label.Since ME does not consider multiple states in asingle feature, there are only unigrams in Table 2.In addition, as ME can handle more features thanCRF, we apply extra features in the ME modelTable 1 Character features for CRFFeature Typesunigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1),(1,2), (-1,0,1)bigram -2 -1 0 +1 +2, (0,1)Table 2 Character features for MEFeature Typesunigram -2, -1, 0, 1, 2, (-2,-1), (-1,0), (0,1),(1,2), (-1,0,1) (-1,1)Word InformationBecause of the limitations of the closed task, weuse the NER corpus to train the segmentors basedon the CRF model.
To simulate noisy word in-formation in the test corpus, we use a ten-foldmethod for training segmentors to tag the trainingcorpus.
The word features we apply in our NERsystems are presented in Tables 3 and 4.In addition to the word itself, chuck information,i.e., the relative position of a character in a word,is also valuable information.
Hence, we also addchuck information to our models.
As the diversityof Chinese words is greater than that of Chinesecharacters, the number of features that can beused in CRF is much lower than the number thatcan be used in ME.Table 3 Word features for CRFFeature Typesunigram 0bigram 0Table 4 Word features for MEFeature Typesunigram -1, 0, 1, (-2,-1), (-1,0), (0,1), (1,2)2.3 Ensemble MethodsMajority voteWe can not put all the features into the CRFmodel because of its limited resources.
Therefore,we train several CRF classifiers with differentfeature sets so that we can use as many features143as possible.
Then, we use the following simple,equally weighted linear equation, called majorityvote, to combine the results of the CRF classifi-ers.
( ) ( )?
== Ti i xyCxyS 0 ,, ,where S(y,x) is the score of a label y and a char-acter x respectively; T denotes the total numberof CRF models; and the value of Ci(y,x) is 1 ifthe decision of the result of the ith CRF model isy, otherwise it is zero.
The highest score of y ischosen as the label of x.
The results are incorpo-rated into the Viterbi algorithm to search for thepath with the maximum scores.In this paper, the first step in the majority voteexperiment is to train three CRF classifiers withdifferent feature sets.
Then, in the second step,we use the results obtained in the first step togenerate the voting scores for the Viterbi algo-rithm.Memory Based learnerThe memory-based learning method memorizesall examples in a training corpus.
If a word isunknown, the memory-based classifier uses thek-nearest neighbors to find the most similar ex-ample as the answer.
Instead of using the com-plete algorithm of the memory-based learner, wedo not handle unseen data.
In our memory- basedcombination method, the learner remembers allnamed entities from the results of the variousclassifiers and then tags the characters that wereoriginally tagged as ?Other?.
For example, if acharacter x is tagged by one classifier as ?0?(?Others?
tag) and if the memory-based classifierlearns from another classifier that this characteris tagged as PER, then x will be tagged as ?B-PER?
by the memory-based classifier.The obvious drawback of this method is that theprecision rate might decrease as the recall rateincreases.
Therefore, we set the following threerules to filter out samples that are likely to have ahigh error rate.1.
Named entities can not be tagged as differ-ent named entity tags by different classifiers.2.
We set an absolute frequency threshold tofilter out examples that occur less than thethreshold.3.
We set a relative frequency threshold tofilter out examples that occur less than thethreshold.
For example, if a word x appears10 times in the corpus, then half of the in-stances of x have to be tagged as named en-tities; otherwise, x will be filtered out of thememory classifier.In our experiment, we used the memory-basedlearner to memorize the named entities from thetagging results of an ME classifier and a CRFclassifier, and then tagged the tagging results ofthe CRF classifier.3 Experiments3.1 DataWe selected the corpora of City University ofHong Kong (CityU) and Microsoft Research(MSRA) corpora to evaluate our methods.
CityUis a Traditional Chinese corpus, and MSRA isSimplified Chinese corpus.3.2 ResultsTable 5 shows the results of several methods ap-plied to the MSRA corpus.
The memory-basedensemble method, which combines the results ofa maximum entropy model and those of a CRFclassifier, achieves the best performance.
Themajority vote combined with the results of threeCRF models based on different feature sets hasthe worst performance.Table 5 msraPrecision Recall FB1Memory based 86.21 78.14 81.98Majority Vote 85.83 76.06 80.65Only-Character 86.70 75.54 80.74CRF 86.23 77.40 81.58The results obtained on Cityu, presented in Table6, show that the single CRF classifier achievedthe best performance.
None of the ensemblemethods can outperform the non-ensemble meth-ods.Table 6 cityuPrecision Recall FB1Memory based  90.79 86.26 88.47Majority Vote 90.52 84.15 87.22Only-Character 91.32 84.55 87.80CRF 92.01 85.45 88.61Tables 7 and 8 show the results of the memory-based ensemble methods under different rules.We set the frequency threshold as 2 and the rela-tive frequency threshold as 0.5.
The results showthat the relative frequencies rule effectively re-duces the loss of precision caused by more enti-ties being tagged by the memory-based classifier.The memory-based ensemble method works wellon the MSRA corpus, but not on the CityU cor-pus.
In the MSRA corpus, the memory-based144ensemble method outperforms the individualCRF model by approximately 0.4 % in FB1.
Wefound that the memory-based classifier can notachieve a better performance than the CRF modelbecause it misclassifies many organizations?names.
Therefore, we chose another strategy thatrestricts the memory-based classifier to taggingperson names only.
Under this restriction, theperformance of the memory-based classifier im-proves FB1 by approximately 0.2%.Table 7 msra- The performances of memorybased ensemble methods under different rules.Precision Recall FB1Frequency Threshold 86.18 78.16 81.97Relative FrequencyThreshold86.21 78.14 81.98Only Person 86.27 77.58 81.69Table 8 cityu- The performances of memorybased ensemble methods under different rules.Precision Recall FB1Frequency Threshold 90.69 86.55 88.57Relative FrequencyThreshold90.87 86.29 88.52Only Person 92.00 85.66 88.724 ConclusionIn this paper, we use ME and CRF models totrain a Chinese named entity tagger.
Like previ-ous researchers, we found that CRF models out-perform ME models.
We also apply twoensemble methods, namely, majority vote andmemory-based approaches, to the closed NERshared task.
Our results show that integratingindividual classifiers as the majority vote ap-proach does not outperform the individual classi-fiers.
Furthermore, a memory-based combinationonly seems to work when we restrict the mem-ory-based classifier to handling person names.AcknowledgementWe are grateful for the support of National Sci-ence Council under Grant NSC 95-2752-E-001-001-PAE.References1.
Berger, A., Pietra, S.A.D.
and Pietra, V.J.D.
AMaximum Entropy Approach to Natural LanguageProcessing.
Computer Linguistic, 22.
1996 39-71.2.
Florian, R., Ittycheriah, A., Jing, H. and Zhang, T.,Named Entity Recognition through ClassifierCombination.
in Proceedings of Conference onComputational Natural Language Learning, 2003,168-171.3.
Halteren, H.v., Zavrel, J. and Daelemans, W. Im-proving accuracy in word class tagging throughcombination of machine learning systems.
Compu-tational Linguistics, 27 (2).
2001 199-230.4.
Klein, D., Smarr, J., Nguyen, H. and Manning,C.D., Named Entity Recognition with Character-Level Models.
in Conference on ComputationalNatural Language Learning, 2003, 180-183.5.
Lafferty, J., McCallum, A. and Pereira, F. Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data.
InternationalConference on Machine Learning.
2001 282-289.6.
Rabiner, L. A tutorial on hidden Markov modelsand selected applications in speech recognition.Proceedings of the IEEE, 77 (2).
1989 257-286.7.
Sutton, C., Rohanimanesh, K. and McCallum, A.,Dynamic Conditional Random Fields: FactorizedProbabilistic Models for Labeling and SegmentingSequence Data.
in Proceedings of the Twenty-FirstInternational Conference on Machine Learning,2004, 99-107.8.
Zavrel, J. and Daelemans, W. Memory-based learn-ing: using similarity for smoothing.
Proceedings ofthe eighth conference on European chapter of theAssociation for Computational Linguistics.
1997436 - 443.145
