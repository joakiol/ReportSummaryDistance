Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 643?654, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsConcurrent Acquisition of Word Meaning and Lexical CategoriesAfra Alishahia.alishahi@uvt.nlCommunication and Information SciencesTilburg University, The NetherlandsGrzegorz Chrupa?agchrupala@lsv.uni-saarland.deSpoken Language SystemsSaarland University, GermanyAbstractLearning the meaning of words from ambigu-ous and noisy context is a challenging task forlanguage learners.
It has been suggested thatchildren draw on syntactic cues such as lexicalcategories of words to constrain potential ref-erents of words in a complex scene.
Althoughthe acquisition of lexical categories should beinterleaved with learning word meanings, ithas not previously been modeled in that fash-ion.
In this paper, we investigate the inter-play of word learning and category inductionby integrating an LDA-based word class learn-ing module with a probabilistic word learningmodel.
Our results show that the incremen-tally induced word classes significantly im-prove word learning, and their contribution iscomparable to that of manually assigned partof speech categories.1 Learning the Meaning of WordsFor young learners of a natural language, mappingeach word to its correct meaning is a challengingtask.
Words are often used as part of an utterancerather than in isolation.
The meaning of an utter-ance must be inferred from among numerous pos-sible interpretations that the (usually complex) sur-rounding scene offers.
In addition, the linguistic andvisual context in which words are heard and usedis often noisy and highly ambiguous.
Particularly,many words in a language are polysemous and havedifferent meanings.Various learning mechanisms have been proposedfor word learning.
One well-studied mechanismis cross-situational learning, a bottom-up strategybased on statistical co-occurrence of words and ref-erents across situations (Quine 1960, Pinker 1989).Several experimental studies have shown that adultsand children are sensitive to cross-situational evi-dence and use this information for mapping words toobjects, actions and properties (Smith and Yu 2007,Monaghan and Mattock 2009).
A number of com-putational models have been developed based on thisprinciple, demonstrating that cross-situational learn-ing is a powerful and efficient mechanism for learn-ing the correct mappings between words and mean-ings from noisy input (e.g.
Siskind 1996, Yu 2005,Fazly et al010).Another potential source of information that canhelp the learner to constrain the relevant aspects of ascene is the sentential context of a word.
It has beensuggested that children draw on syntactic cues pro-vided by the linguistic context in order to guide wordlearning, a hypothesis known as syntactic bootstrap-ping (Gleitman 1990).
There is substantial evidencethat children are sensitive to the structural regular-ities of language from a very young age, and thatthey use these structural cues to find the referent ofa novel word (e.g.
Naigles and Hoff-Ginsberg 1995,Gertner et al006).
In particular, young childrenhave robust knowledge of some of the abstract lexi-cal categories such as nouns and verbs (e.g.
Gelmanand Taylor 1984, Kemp et al005).Recent studies have examined the interplay ofcross-situational learning and sentence-level learn-ing mechanisms, showing that adult learners of anartificial language can successfully and simultane-ously apply cues and constraints from both sourcesof information when mapping words to their refer-ents (Gillette et al999, Lidz et al010, Koehneand Crocker 2010; 2011).
Several computationalmodels have also investigated this interaction byadding manually annotated part-of-speech tags as643input to word learning algorithms, and suggestingthat integration of lexical categories can boost theperformance of a cross-situational model (Yu 2006,Alishahi and Fazly 2010).However, none of the existing experimental orcomputational studies have examined the acquisitionof word meanings and lexical categories in paral-lel.
They all make the simplifying assumption thatprior to the onset of word learning, the categoriza-tion module has already formed a relatively robustset of lexical categories.
This assumption can be jus-tified in the case of adult learners of a second or ar-tificial language.
But children?s acquisition of cate-gories is most probably interleaved with the acquisi-tion of word meaning, and these two processes mustultimately be studied simultaneously.In this paper, we investigate concurrent acquisi-tion of word meanings and lexical categories.
Weuse an online version of the LDA algorithm toinduce a set of word classes from child-directedspeech, and integrate them into an existing prob-abilistic model of word learning which combinescross-situational evidence with cues from lexicalcategories.
Through a number of simulations of aword learning scenario, we show that our automat-ically and incrementally induced categories signifi-cantly improve the performance of the word learningmodel, and are closely comparable to a set of gold-standard, manually-annotated part of speech tags.2 A Word Learning ModelWe want to investigate whether lexical categories(i.e.
word classes) that are incrementally inducedfrom child-directed speech can improve the perfor-mance of a cross-situational word learning model.For this purpose, we use the model of Alishahi andFazly (2010).
This model uses a probabilistic learn-ing algorithm for combining evidence from word?referent co-occurrence statistics and the meaningsassociated with a set of pre-defined categories.
Theyuse child-directed utterances, manually annotatedwith a small set of part of speech tags, from theManchester corpus (Theakston et al001) in theCHILDES database (MacWhinney 1995).
Their ex-perimental results show that integrating these gold-standard categories into the algorithm boosts its per-formance over a pure cross-situational version.The model of Alishahi and Fazly (2010) has thesuitable architecture for our goal: it provides an in-tegrated learning mechanism which combines evi-dence from word-referent co-occurrence with cuesfrom the meaning representation associated withword categories.
However, the model has two ma-jor shortcomings.
First, it assumes that lexical cate-gories are formed and finalized prior to the onset ofword learning and that a correct and unique categoryfor a target word can be identified at each point intime, assumptions that are highly unlikely.
Second,it does not handle any ambiguity in the meaning ofa word.
Instead, each word is assumed to have onlyone correct meaning.
Considering the high level oflexical ambiguity in most natural languages, this as-sumption unreasonably simplifies the word learningproblem.To investigate the plausibility of integrating wordand category learning, we use an online algorithmfor automatically and incrementally inducing a setof lexical categories.
Moreover, we use each word inits original form instead of lemmatizing them, whichimplies that categories contain different morpholog-ical forms of the same word.
By applying thesechanges, we are able to study the contribution of lex-ical categories to word learning in a more realisticscenario.Representation of input.
The input to the modelconsists of a sequence of utterances, each pairedwith a representation of an observed scene.
We rep-resent an utterance as a set of words, U = {w}(e.g.
{she, went, home, ...}), and the correspondingscene as a set of semantic features, S = {f} (e.g.
{ANIMATE, HUMAN, FEMALE, ...}).Word and category meaning.
We represent themeaning of a word as a time-dependent probabilitydistribution p(t)(?|w) over all the semantic features,where p(t)(f |w) is the probability of feature f be-ing associated with word w at time t. In the absenceof any prior knowledge, the model assumes a uni-form distribution over all features as the meaning ofa novel word.
Also, a function cat(t)(w) gives usthe category to which a word w in utterance U (t) be-longs.At each point in time, a category c contains a setof word tokens.
We assign a meaning to each cat-644egory as a weighted sum of the meaning learnedso far for each of its members, or p(t)(f |c) =(1/|c|)?w?c p(t)(f |w), where |c| is the number ofword tokens in c at the current moment.Learning algorithm.
Given an utterance-scene pair(U (t), S(t)) received at time t, the model first calcu-lates an alignment score a for each word w ?
U (t)and each semantic feature f ?
S(t).
A semantic fea-ture can be aligned to a word according to the mean-ing acquired for that word from previous observa-tions (word-based alignment, or aw).
Alternatively,distributional clues of the word can be used to de-termine its category, and the semantic features canbe aligned to the word according to the meaning as-sociated to its category (category-based alignment,or ac).
We combine these two sources of evidencewhen estimating an alignment score:a(w|f, U (t), S(t)) = ?(w)?
aw(w|f, U(t), S(t)) (1)+(1?
?(w))?
ac(w|f, U(t), S(t))where the word-based and category-based alignmentscores are estimated based on the acquired meaningsof the word and its category, respectively:aw(w|f, U(t), S(t)) =p(t?1)(f |w)?wk?U(t)p(t?1)(f |wk)ac(w|f, U(t), S(t)) =p(t?1)(f |cat(w))?wk?U(t)p(t?1)(f |cat(wk))The relative contribution of the word-based versusthe category-based alignment is determined by theweight function ?(w).
Cross-situational evidenceis a reliable cue for frequent words; on the otherhand, the category-based score is most informativewhen the model encounters a low-frequency word(See Alishahi and Fazly (2010) for a full analysis ofthe frequency effect).
Therefore, we define ?
(w) asa function of the frequency of the word n(w):?
(w) = n(w)/(n(w) + 1)Once an alignment score is calculated for eachword w ?
U (t) and each feature f ?
S(t), the modelrevises the meanings of all the words in U (t) andtheir corresponding categories as follows:assoc(t)(w, f) = assoc(t?1)(w, f) + a(w|f,U(t),S(t))where assoc(t?1)(w, f) is zero if w and f have notco-occurred before.
These association scores arethen used to update the meaning of the words in thecurrent input:p(t)(f |w) =assoc(t)(f, w)?fj?Fassoc(t)(fj , w)(2)where F is the set of all features seen so far.
We usea smoothed version of this formula to accommodatenoisy or rare input.
This process is repeated for allthe input pairs, one at a time.Uniform categories.
Adding the category-basedalignment as a new factor to Eqn.
(1) might im-ply that the role of categories in this model is noth-ing more than smoothing the cross-situational-basedalignment of words and referents.
In order to in-vestigate this issue, we use the following alignmentformula as an informed baseline in our experiments,where we replace ac(?|f, U (t), S(t)) with a uniformdistribution:1a(w|f, U (t), S(t)) = ?(w)?
aw(w|f, U(t), S(t)) (3)+(1?
?
(w))?1|U (t)|where aw(w|f, U (t), S(t)) and ?
(w) are estimated asbefore.
In our experiments in Section 4, we refer tothis baseline as the ?uniform?
condition.3 Online induction of word classes withLDAEmpirical findings suggest that young children formtheir knowledge of abstract categories, such asverbs, nouns, and adjectives, gradually (e.g.
Gel-man and Taylor 1984, Kemp et al005).
In ad-dition, several unsupervised computational mod-els have been proposed for inducing categories ofwords which resemble part-of-speech categories, by1We thank an anonymous reviewers for suggesting this con-dition as an informed baseline.645drawing on distributional properties of their con-text (see for example Redington et al998, Clark2000, Mintz 2003, Parisien et al008, Chrupa?aand Alishahi 2010).
However, explicit accounts ofhow such categories can be integrated in a cross-situational model of word learning have been rare.Here we adopt an online version of the model pro-posed in Chrupa?a (2011), a method of soft wordclass learning using Latent Dirichlet Allocation.
Theapproach is much more efficient than the commonlyused alternative (Brown clustering, (Brown et al1992)) while at the same time matching or outper-forming it when the word classes are used as au-tomatically learned features for supervised learningof various language understanding tasks.
Here weadopt this model as our approach to learning lexicalcategories.In Section 3.1 we describe the LDA model forword classes; in Section 3.2 we discuss the onlineGibbs sampler we use for inference.3.1 Word class learning with LDALatent Dirichlet Allocation (LDA) was introducedby Blei et al2003) and is most commonly usedfor modeling the topic structure in document collec-tions.
It is a generative, probabilistic hierarchicalBayesian model that induces a set of latent variables,which correspond to the topics.
The topics them-selves are multinomial distributions over words.The generative structure of the LDA model is thefollowing:?k ?
Dirichlet(?
), k ?
[1,K]?d ?
Dirichlet(?
), d ?
[1, D]znd ?
Categorical(?d), nd ?
[1, Nd]wnd ?
Categorical(?znd ), nd ?
[1, Nd](4)Chrupa?a (2011) reinterprets the LDA model interms of word classes as follows: K is the numberof classes, D is the number of unique word types,Nd is the number of context features (such as right orleft neighbor) associated with word type d, znd is theclass of word type d in the nthd context, andwnd is thenthd context feature of word type d. Hyperparameters?
and ?
control the sparseness of the vectors ?d and?k.Wordtype FeaturesHow doRdo HowL youR youLyou doL doRTable 1: Matrix of context features1.8M words (CHILDES) 100M words (BNC)train car can willgive bring June Marchshoes clothes man womanbook hole black whitemonkey rabbit business languageTable 2: Most similar word pairsAs an example consider the small corpus consist-ing of the single sentence How do you do.
The rowsin Table 1 show the features w1 .
.
.
wNd for eachword type d if we use each word?s left and rightneighbors as features, and subscript words with Land R to indicate left and right.After inference, the ?d parameters correspond toword class probability distributions given a wordtype while the ?k correspond to feature distributionsgiven a word class: the model provides a probabilis-tic representation for word types independently oftheir context, and also for contexts independently ofthe word type.Probabilistic, soft word classes are more expres-sive than hard categories.
First, they make iteasy and efficient to express shared ambiguities:Chrupa?a (2011) gives an example of words usedas either first names or surnames, and this sharedambiguity is reflected in the similarity of their wordclass distributions.
Second, with soft word classes itbecomes easy to express graded similarity betweenwords: as an example, Table 2 shows a random se-lection out of the 100 most similar word pairs ac-cording to the Jensen-Shannon divergence betweentheir word class distributions, according to a wordclass model with 25 classes induced from (i) 1.8 mil-lion words of the CHILDES corpus or (ii) 100 mil-lion word of the BNC corpus.
The similarities weremeasured between each of the 1000 most frequentCHILDES or BNC words.6463.2 Online Gibbs sampling for LDAThere have been a number of attempts to developonline inference algorithms for topic modeling withLDA.
A simple modification of the standard Gibbssampler (o-LDA) was proposed by Song et al(2005) and Banerjee and Basu (2007).Canini et al2009) experiment with three sam-pling algorithms for online topic inference: (i) o-LDA, (ii) incremental Gibbs sampler, and (iii) a par-ticle filter.
Only o-LDA is truly online in the sensethat it does not revisit previously seen documents.The other two, the incremental Gibbs sampler andthe particle filter, keep seen documents and periodi-cally resample them.
In Canini et al experimentsall of the online algorithms perform worse than thestandard batch Gibbs sampler on a document clus-tering task.Hoffman et al2010) develop an online versionof the variational Bayes (VB) optimization methodfor inference for topic modeling with LDA.
Theirmethod achieves good empirical results comparedto batch VB as measured by perplexity on held-out data, especially when used with large minibatchsizes.Online VB for LDA is appropriate when stream-ing documents: with online VB documents are rep-resented as word count tables.
In our scenario wherewe apply LDA to modeling word classes we need toprocess context features from sentences arriving ina stream: i.e.
we need to sample entries from a ta-ble like Table 1 in order of arrival rather than rowby row.
This means that online VB is not directlyapplicable to online word-class induction.However it also means that one issue with o-LDAidentified by Canini et al2009) is ameliorated.When sampling in a topic modeling setting, docu-ments are unique and are never seen again.
Thus,the topics associated with old documents get staleand need to be periodically rejuvenated (i.e.
resam-pled).
This is the reason why the incremental Gibbssampler and the particle filter algorithms in Caniniet al2009) need to keep old documents around andcannot run in a true online fashion.
Since for wordclass modeling we stream context features as theyarrive, we will continue to see features associatedwith the seen word types, and will automatically re-sample their class assignments.
In exploratory ex-periments we have seen that this narrows the per-formance gap between the o-LDA sampler and thebatch collapsed Gibbs sampler.We present our version of the o-LDA sampler inAlgorithm 1.
For each incoming sentence twe run Jpasses of sampling, updating the counts tables aftereach sampling step.
We sample the class assignmentzti for feature wti according to:P (zt|zt?1,wt,dt) ?
(nzt,dtt?1 + ?)?
(nzt,wtt?1 + ?
)?Vt?1j=1 nzt,wjt?1 + ?,(5)where nz,dt stands for the number of times class zco-occurred with word type d up to step t, and sim-ilarly nz,wt is the number of times feature w was as-signed to class z. Vt is the number of unique featuresseen up to step t, while ?
and ?
are the LDA hyper-parameters.
There are two differences between theoriginal o-LDA and our version: we do not initializethe algorithm with a batch run over a prefix of thedata, and we allow more than one sampling pass persentence.2 Exploratory experiments have shown thatbatch initialization is unnecessary, and that multiplepasses typically improve the quality of the inducedword classes.Algorithm 1 Online Gibbs sampler for word classinduction with LDAfor t = 1??
dofor j = 1?
J dofor i = 1?
It dosample zti ?
P (zti |zti?1,wti ,dti)increment nzti ,wtit and nzti ,dtitFigure 1 shows the top 10 words for each of the10 word classes induced with our online Gibbs sam-pler from 1.8 million words of CHILDES.
Similarly,Figure 2 shows the top 10 words for 5 randomly cho-sen topics out of 50, learned online from 100 millionwords of the BNC.The topics are relatively coherent and at these lev-els of granularity express mostly part of speech andsubcategorization frame information.Note that for each word class we show the wordsmost frequently assigned to it while Gibbs sampling.2Note that we do not allow multiple passes over the streamof sentences.
Rather, while processing the current sentence, weallow the words in this sentence to be sampled more than once.647do are have can not go put did get playis that it what not there he was where putyou not I the we what it they your ato you we and I will not can it onit a that the not he this right got sheare do is have on in can want did goingone I not shall there then you are we itis in are on oh with and of have dothe a your of that it this some not verygoing want bit go have look got will at littleFigure 1: Top 10 words for 10 classes learned fromCHILDESI you he it they we she , You Hea the more some all no The other I twoas if when that where how because If before whatwas is ?s had , has are would did saidthe his her their this an that its your myFigure 2: Top 10 words of 5 randomly chosen classeslearned from BNCSince we are dealing with soft classes, most word-types have non-zero assignment probabilities formany classes.
Thus frequently occurring words suchas not will typically be listed for several classes.4 Evaluation4.1 Experimental setupAs training data, we extract utterances from theManchester corpus (Theakston et al001) in theCHILDES database (MacWhinney 1995), a corpusthat contains transcripts of conversations with chil-dren between the ages of 1 year, 8 months and 3years.
We use the mother?s speech from transcriptsof 12 children (henceforth referred to by children?snames).We run word class induction while simultane-ously outputting the highest scoring word-class la-bel for each word: for a new sentence, we sam-ple class assignments for each feature (doing Jpasses), update the counts, and then for each worddti output the highest scoring class label accordingto argmaxz nz,dtit (where nz,dtit stands for the num-ber of times class z co-occurred with word type dtiup to step t).During development we ran the online word classinduction module on data for Aran, Becky, Carl andAnne and then started the word learning module forthe Anne portion while continuing inducing cate-gories.
We then evaluated word learning on Anne.We chose the parameters of the word class induc-tion module based on those development results:?K1=1 ?
= 10, ?
= 0.1, K = 10 and J = 20.We used cross-validation for the final evaluation.For each of six data files (Anne, Aran, Becky, Carl,Dominic and Gail), we ran word-class induction onthe whole corpus with the chosen file last, and thenstarted applying the word-learning algorithm on thislast chosen file (while continuing with category in-duction).
We evaluated how well word meaningswere learned in those six cases.We follow Alishahi and Fazly (2010) in the con-struction of the input.
We need a semantic represen-tation paired with each utterance.
Such a represen-tation is not available from the corpus and has to beconstructed.
We automatically construct a gold lexi-con for all nouns and verbs in this corpus as follows.For each word, we extract all hypernyms for its firstsense in the appropriate (verb or noun) hierarchy inWordNet (Fellbaum 1998), and add the first word inthe synset of each hypernym to the set of semanticfeatures for the target word.
For verbs, we also ex-tract features from VerbNet (Kipper et al006).
Asmall subset of words (pronouns and frequent quan-tifiers) are also manually added.
This lexicon repre-sents the true meaning of each word, and is used ingenerating the scene representations in the input andin evaluation.For each utterance in the input corpus, we formthe union of the feature representations of all itswords.
Words not found in the lexicon (i.e.
for whichwe could not extract a semantic representation fromWordNet and VerbNet) are removed from the utter-ance (only for the word learning module).In order to simulate the high level of noise thatchildren receive from their environment, we followAlishahi and Fazly (2010) and pair each utterancewith a combination of its own scene representationand the scene representation for the following utter-ance.
This decision was based on the intuition thatconsequent utterances are more likely to be about re-648Utterance: { mommy, ate, broccoli }Scene: { ANIMATE, HUMAN, ...,CONSUMPTION, ACTION, ...BROCCOLI, VEGETABLE, ...PLATE, OBJECT, ... }Figure 3: A sample input item to the word learning modellated topics and scenes.
This results in a (roughly)200% ambiguity.
In addition, we remove the mean-ing of one random word from the scene representa-tion of every second utterance in an attempt to sim-ulate cases where the referent of an uttered word isnot within the perception field (such as ?daddy is nothome yet?).
A sample utterance and its correspond-ing scene are shown in Figure 3.As mentioned before, many words in our inputcorpus are polysemous.
For such words, we extractdifferent sets of features depending on their manu-ally tagged part of speech and keep them in the lex-icon (e.g.
the lexicon contains two different entriesfor set:N and set:V).
When constructing a scene rep-resentation for an utterance which contains an am-biguous word, we choose the correct sense from ourlexicon according to the word?s part of speech tag inManchester corpus.In the experiments reported in the next section,we assess the performance of our model on learningwords at each point in time: for each target word,we compare its set of features in the lexicon withits probability distribution over the semantic fea-tures that the model has learned.
We use mean aver-age precision (MAP) to measure how well p(t)(?|w)ranks the features of w.4.2 Learning curvesTo understand whether our categories contribute tolearning of word?meaning mappings, we comparethe pattern of word learning over time in four con-ditions.
The first condition represents our baseline,in which we do not use category-based alignmentin the word learning model by setting ?
(w) = 1in Eqn.
(1).
In the second condition we use a setof uniformly distributed categories for alignment,as estimated by Eqn.
(3) on page 3 (this conditionis introduced to examine whether categories act asmore than a simple smoothing factor in the align-Category Avg.
MAP Std.
Dev.None 0.626 0.032Uniform 0.633 0.032LDA 0.659 0.029POS 0.672 0.030Table 3: Final Mean Average Precision scoresment process.)
In the third condition we use the cat-egories induced by online LDA in the word learningmodel.
The fourth condition represents the perfor-mance ceiling, in which we use the pre-defined andmanually annotated part of speech categories fromthe Manchester corpus.Table 3 shows the average and the standard devia-tion of the final MAP scores across the six datasets,for the four conditions (no categories, uniform cat-egories, LDA categories and gold part-of-speechtags).
The differences between LDA and None, andbetween LDA and Uniform are statistically signif-icant according to the paired t test (p < 0.01),while the difference between LDA and POS is not(p = 0.16).Figure 4 shows the learning curves in each con-dition, averaged over the six splits explained in theprevious section.
The top panel shows the averagelearning curve over the minimum number of sen-tences across the six sub-corpora (8800 sentences).The curves show that our LDA categories signifi-cantly improve the performance of the model overboth baselines.
That means that using these cate-gories can improve word learning compared to notusing them and relying on cross-situational evidencealone.
Moreover, LDA-induced categories are notmerely acting as a smoothing function the way the?uniform?
categories are.
Our results show that theyare bringing relevant information to the task at hand,that is, improving word learning by using the sen-tential context.
In fact, this improvement is compa-rable to the improvement achieved by integrating the?gold-standard?
POS categories.The middle and bottom panels of Figure 4 zoomin on shorter time spans (5000 and 1000 sentences,respectively).
These diagrams suggest that the pat-tern of improvement over baseline is relatively con-stant, even at very early stages of learning.
In fact,once the model receives enough input data, cross-situational evidence becomes stronger (since fewer649words in the input are encountered for the first time)and the contribution of the categories becomes lesssignificant.4.3 Class granularityIn Figure 5 we show the influence of the number ofword classes used on the performance in word learn-ing.
It is evident that in the range between 5 to 20classes the performance of the word learning moduleis quite stable and insensitive to the exact class gran-ularity.
Even with only 5 classes the model can stillroughly distinguish noun-like words from verb-likewords from pronoun-like words, and this will helplearn the meaning elements derived from the higherlevels of WordNet hierarchy.
Notwithstanding that,ideally we would like to avoid having to pre-specifythe number of classes for the word class inductionmodule: we thus plan to investigate non-parametricmodels such as Hierarchical Dirichlet Process forthis purpose.5 Related WorkThis paper investigates the interplay between twolanguage learning tasks which have so far been stud-ied in isolation: the acquisition of lexical categoriesfrom distributional clues, and learning the mappingbetween words and meanings.
Previous modelshave shown that lexical categories can be learnedfrom unannotated text, mainly drawing on distri-butional properties of words (e.g.
Redington et al1998, Clark 2000, Mintz 2003, Parisien et al008,Chrupa?a and Alishahi 2010).Independently, several computational modelshave exploited cross-situational evidence in learningthe correct mappings between words and meanings,using rule-based inference (Siskind 1996), neuralnetworks (Li et al004, Regier 2005), hierarchicalBayesian models (Frank et al007) and probabilis-tic alignment inspired by machine translation mod-els (Yu 2005, Fazly et al010).There are only a few existing computational mod-els that explore the role of syntax in word learning.Maurits et al2009) investigates the joint acquisi-tion of word meaning and word order using a batchmodel.
This model is tested on an artificial languagewith a simple first order predicate representation ofmeaning, and limited built-in possibilities for word0 2000 4000 6000 80000.350.450.550.65Input ItemsMeanAverage PrecisionPOSLDAUniformNone(a) all sentences0 1000 2000 3000 4000 50000.350.450.550.65Input ItemsMeanAverage PrecisionPOSLDAUniformNone(b) first 5000 sentences0 200 400 600 800 10000.350.450.550.65Input ItemsMeanAverage PrecisionPOSLDAUniformNone(c) first 1000 sentencesFigure 4: Mean average precision for all observed wordsat each point in time for four conditions: with goldPOS categories, with LDA categories, with uniform cate-gories, and without using categories.
Each panel displaysa different time span.6500 2000 4000 6000 80000.400.450.500.550.600.650.70Input ItemsMeanAverage Precision20 LDA classes10 LDA classes5 LDA classesNo categoriesFigure 5: Mean average precision for all observed wordsat each point in time in four conditions: using online LDAcategories of varying numbers of 20, 10 and 5, and with-out using categories.order.
The model of Niyogi (2002) simulates themutual bootstrapping effects of syntactic and seman-tic knowledge in verb learning, that is the use of syn-tax to aid in inducing the semantics of a verb, and theuse of semantics to narrow down possible syntacticframes in which a verb can participate.
However,this model relies on manually assigned priors for as-sociations between syntactic and semantic features,and is tested on a toy language with very limited vo-cabulary and a constrained syntax.Yu (2006) integrates automatically induced syn-tactic word categories into his model of cross-situational word learning, showing that they can im-prove the model?s performance.
Yu?s model alsoprocesses input utterances in a batch mode, and itsevaluation is limited to situations in which only acoarse distinction between referring words (wordsthat could potentially refer to objects in a scene, e.g.concrete nouns) and non-referring words (words thatcannot possibly refer to objects, e.g.
function words)is sufficient.
It is thus not clear whether informationabout finer-grained categories (e.g.
verbs and nouns)can indeed help word learning in a more naturalisticincremental setting.On the other hand, the model of Alishahi andFazly (2010) integrates manually annotated part-of-speech tags into an incremental word learning al-gorithm, and shows that these tags boost the over-all word learning performance, especially for infre-quent words.In a different line of research, a number of mod-els have been proposed which study the acquisitionof the link between syntax and semantics within theCombinatory Categorial Grammar (CCG) frame-work (Briscoe 1997, Villavicencio 2002, Buttery2006, Kwiatkowski et al012).
These approachesset the parameters of a semantic parser on a cor-pus of utterances paired with a logical form as theirmeaning.These models bring in extensive and detailed priorassumptions about the nature of the syntactic repre-sentation (i.e.
atomic categories such as S and NP,and built-in rules which govern their combination),as well as about the representation of meaning viathe formalism of lambda calculus.This is fundamentally different than the approachtaken in this paper, which in comparison only as-sumes very simple syntactic and semantic represen-tations of syntax.
We view word and category learn-ing as stand-alone cognitive tasks with independentrepresentations (word meanings as probabilistic col-lections of properties or features as opposed to sin-gle symbols; categories as sets of word tokens withsimilar context distribution) and we do not bring inany prior knowledge of specific atomic categories.6 ConclusionIn this paper, we show the plausibility of usingautomatically and incrementally induced categorieswhile learning word meanings.
Our results suggestthat the sentential context that a word appears inacross its different uses can be used as a complemen-tary source of guidance for mapping it to its featuralmeaning representation.In Section 4 we show that the improvementachieved by our categories is comparable to thatgained by integrating gold POS categories.
This re-sult is very encouraging, since manually assignedPOS tags are typically believed to set the upperbound on the usefulness of category information.We believe that it automatically induced cate-gories have the potential to do even better: Chrupa?aand Alishahi (2010) have shown that categories in-duced from usage data in an unsupervised fashioncan be used more effectively than POS categories in651a number of tasks.
In our experiments here on thedevelopment data we observed some improvementsover POS categories.
This advantage can result fromthe fact that our categories are more fine-grained (ifalso more noisy) than POS categories, which some-times yields more accurate predictions.One important characteristic of the category in-duction algorithm we have used in this paper is thatit provides a soft categorization scheme, where eachword is associated with a probability distributionover all categories.
In future, we plan to exploit thisfeature: when estimating the category-based align-ment, we can interpolate predictions of multiple cat-egories to which a word belongs, weighted by itsprobabilities associated with membership in eachcategory.AcknowledgementsGrzegorz Chrupa?a was funded by the German Fed-eral Ministry of Education and Research (BMBF)under grant number 01IC10S01O as part ofthe Software-Cluster project EMERGENT (www.software-cluster.org).ReferencesAlishahi, A. and Fazly, A.
(2010).
IntegratingSyntactic Knowledge into a Model of Cross-situational Word Learning.
In Proceedings of the32nd Annual Conference of the Cognitive ScienceSociety.Banerjee, A. and Basu, S. (2007).
Topic models overtext streams: A study of batch and online unsuper-vised learning.
In SIAM Data Mining.Blei, D., Ng, A., and Jordan, M. (2003).
La-tent dirichlet alation.
The Journal of MachineLearning Research, 3:993?1022.Briscoe, T. (1997).
Co-evolution of language andof the language acquisition device.
In Proceed-ings of the eighth conference on European chap-ter of the Association for Computational Linguis-tics, pages 418?427.
Association for Computa-tional Linguistics.Brown, P. F., Mercer, R. L., Della Pietra, V. J., andLai, J. C. (1992).
Class-based n-gram modelsof natural language.
Computational Linguistics,18(4):467?479.Buttery, P. (2006).
Computational models for firstlanguage acquisition.
Computer Laboratory, Uni-versity of Cambridge, Tech.
Rep. UCAM-CLTR-675.Canini, K., Shi, L., and Griffiths, T. (2009).
Onlineinference of topics with latent dirichlet alation.In Proceedings of the International Conference onArtificial Intelligence and Statistics.Chrupa?a, G. (2011).
Efficient induction of proba-bilistic word classes with LDA.
In InternationalJoint Conference on Natural Language Process-ing.Chrupa?a, G. and Alishahi, A.
(2010).
OnlineEntropy-based Model of Lexical Category Acqui-sition.
In CoNLL 2010.Clark, A.
(2000).
Inducing syntactic categories bycontext distribution clustering.
In Proceedings ofthe 2nd workshop on Learning Language in Logicand the 4th conference on Computational NaturalLanguage Learning, pages 91?94.
Association forComputational Linguistics Morristown, NJ, USA.Fazly, A., Alishahi, A., and Stevenson, S. (2010).A Probabilistic Computational Model of Cross-Situational Word Learning.
Cognitive Science,34(6):1017?1063.Fellbaum, C., editor (1998).
WordNet, An ElectronicLexical Database.
MIT Press.Frank, M. C., Goodman, N. D., and Tenenbaum,J.
B.
(2007).
A Bayesian framework for cross-situational word-learning.
In Advances in NeuralInformation Processing Systems, volume 20.Gelman, S. and Taylor, M. (1984).
How two-year-old children interpret proper and common namesfor unfamiliar objects.
Child Development, pages1535?1540.Gertner, Y., Fisher, C., and Eisengart, J.
(2006).Learning words and rules: Abstract knowledge ofword order in early sentence comprehension.
Psy-chological Science, 17(8):684?691.Gillette, J., Gleitman, H., Gleitman, L., and Led-erer, A.
(1999).
Human simulations of vocabularylearning.
Cognition, 73(2):135?76.Gleitman, L. (1990).
The structural sources of verbmeanings.
Language Acquisition, 1:135?176.652Hoffman, M., Blei, D., and Bach, F. (2010).
On-line learning for latent dirichlet alation.
InAdvances in Neural Information Processing Sys-tems.Kemp, N., Lieven, E., and Tomasello, M. (2005).Young Children?s Knowledge of the?
Deter-miner?
and?
Adjective?
Categories.
Journalof Speech, Language and Hearing Research,48(3):592?609.Kipper, K., Korhonen, A., Ryant, N., and Palmer,M.
(2006).
Extensive classifications of englishverbs.
In Proceedings of the 12th EURALEX In-ternational Congress.Koehne, J. and Crocker, M. W. (2010).
Sen-tence processing mechanisms influence cross-situational word learning.
In Proceedings of theAnnual Conference of the Cognitive Science Soci-ety.Koehne, J. and Crocker, M. W. (2011).
The inter-play of multiple mechanisms in word learning.
InProceedings of the Annual Conference of the Cog-nitive Science Society.Kwiatkowski, T., Goldwater, S., Zettelmoyer, L.,and Steedman, M. (2012).
A probabilistic modelof syntactic and semantic acquisition from child-directed utterances and their meanings.
In Pro-ceedings of the 13th Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics.Li, P., Farkas, I., and MacWhinney, B.
(2004).
Earlylexical development in a self-organizing neuralnetwork.
Neural Networks, 17:1345?1362.Lidz, J., Bunger, A., Leddon, E., Baier, R., and Wax-man, S. R. (2010).
When one cue is better thantwo: lexical vs .
syntactic cues to verb learning.Unpublished manuscript.MacWhinney, B.
(1995).
The CHILDES Project:Tools for Analyzing Talk.
Hillsdale, NJ: LawrenceErlbaum Associates, second edition.Maurits, L., Perfors, A. F., and Navarro, D. J.
(2009).Joint acquisition of word order and word refer-ence.
In Proceedings of the 31st Annual Confer-ence of the Cognitive Science Society.Mintz, T. (2003).
Frequent frames as a cue for gram-matical categories in child directed speech.
Cog-nition, 90(1):91?117.Monaghan, P. and Mattock, K. (2009).
Cross-situational language learning: The effects ofgrammatical categories as constraints on referen-tial labeling.
In Proceedings of the 31st AnnualConference of the Cognitive Science Society.Naigles, L. and Hoff-Ginsberg, E. (1995).
Input toVerb Learning: Evidence for the Plausibility ofSyntactic Bootstrapping.
Developmental Psychol-ogy, 31(5):827?37.Niyogi, S. (2002).
Bayesian learning at the syntax-semantics interface.
In Proceedings of the 24thannual conference of the Cognitive Science Soci-ety, pages 697?702.Parisien, C., Fazly, A., and Stevenson, S. (2008).
Anincremental bayesian model for learning syntacticcategories.
In Proceedings of the Twelfth Confer-ence on Computational Natural Language Learn-ing.Pinker, S. (1989).
Learnability and Cognition: TheAcquisition of Argument Structure.
Cambridge,MA: MIT Press.Quine, W. (1960).
Word and Object.
CambridgeUniversity Press, Cambridge, MA.Redington, M., Crater, N., and Finch, S. (1998).
Dis-tributional information: A powerful cue for ac-quiring syntactic categories.
Cognitive Science:A Multidisciplinary Journal, 22(4):425?469.Regier, T. (2005).
The emergence of words: Atten-tional learning in form and meaning.
CognitiveScience, 29:819?865.Siskind, J. M. (1996).
A computational study ofcross-situational techniques for learning word-to-meaning mappings.
Cognition, 61:39?91.Smith, L. and Yu, C. (2007).
Infants rapidly learnwords from noisy data via cross-situational statis-tics.
In Proceedings of the 29th Annual Confer-ence of the Cognitive Science Society.Song, X., Lin, C., Tseng, B., and Sun, M. (2005).Modeling and predicting personal informationdissemination behavior.
In Proceedings of theeleventh ACM SIGKDD international conferenceon Knowledge discovery in data mining, pages479?488.
ACM.653Theakston, A. L., Lieven, E. V., Pine, J. M., andRowland, C. F. (2001).
The role of performancelimitations in the acquisition of verb-argumentstructure: An alternative account.
Journal ofChild Language, 28:127?152.Villavicencio, A.
(2002).
The acquisition of aunification-based generalised categorial grammar.In Proceedings of the Third CLUK Colloquium,pages 59?66.Yu, C. (2005).
The emergence of links betweenlexical acquisition and object categorization: Acomputational study.
Connection Science, 17(3?4):381?397.Yu, C. (2006).
Learning syntax?semantics mappingsto bootstrap word learning.
In Proceedings of the28th Annual Conference of the Cognitive ScienceSociety.654
