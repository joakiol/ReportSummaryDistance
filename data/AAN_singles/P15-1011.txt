Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 106?115,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsRevisiting Word Embedding for Contrasting MeaningZhigang Chen?, Wei Lin?, Qian Chen?,Xiaoping Chen?, Si Wei?, Hui Jiang?and Xiaodan Zhu?
?School of Computer Science and Technology,?NELSLIP,University of Science and Technology of China, Hefei, China?iFLYTEK Research, Hefei, China?Department of EECS, York University, Toronto, Canadaemails: zgchen9517017@gmail.com, weilin2@iflytek.com, cq1231@mail.ustc.edu.cn,xpchen@ustc.edu.cn, siwei@iflytek.com, hj@cse.yorku.ca, zhu2048@gmail.comAbstractContrasting meaning is a basic aspect ofsemantics.
Recent word-embedding mod-els based on distributional semantics hy-pothesis are known to be weak for mod-eling lexical contrast.
We present in thispaper the embedding models that achievean F-score of 92% on the widely-used,publicly available dataset, the GRE ?mostcontrasting word?
questions (Mohammadet al, 2008).
This is the highest perfor-mance seen so far on this dataset.
Sur-prisingly at the first glance, unlike whatwas suggested in most previous work,where relatedness statistics learned fromcorpora is claimed to yield extra gainsover lexicon-based models, we obtainedour best result relying solely on lexical re-sources (Roget?s and WordNet)?corporastatistics did not lead to further improve-ment.
However, this should not be sim-ply taken as that distributional statistics isnot useful.
We examine several basic con-cerns in modeling contrasting meaning toprovide detailed analysis, with the aim toshed some light on the future directions forthis basic semantics modeling problem.1 IntroductionLearning good representations of meaning for dif-ferent granularities of texts is core to human lan-guage understanding, where a basic problem isrepresenting the meanings of words.
Distributedrepresentations learned with neural networks haverecently showed to result in significant improve-ment of performance on a number of languageunderstanding problems (e.g., speech recognitionand automatic machine translation) and on manynon-language problems (e.g., image recognition).Distributed representations have been leveragedto represent words as in (Collobert et al, 2011;Mikolov et al, 2013).Contrasting meaning is a basic aspect of seman-tics, but it is widely known that word embeddingmodels based on distributional semantics hypoth-esis are weak in modeling this?contrasting mean-ing is often lost in the low-dimensional spacesbased on such a hypothesis, and better modelswould be desirable.Lexical contrast has been modeled in (Lin andZhao, 2003; Mohammad et al, 2008; Moham-mad et al, 2013).
The recent literature has alsoincluded research efforts of modeling contrastingmeaning in embedding spaces, leading to state-of-the-art performances.
For example, Yih et al(2012) proposed to use polarity-primed latent se-mantic analysis (LSA), called PILSA, to capturecontrast, which was further used to initialize a neu-ral network and achieved an F-score of 81% onthe same GRE ?most contrasting word?
questions(Mohammad et al, 2008).
More recently, Zhanget al (2014) proposed a tensor factorization ap-proach to solving the problem, resulting in a 82%F-score.In this paper, we present embedding models thatachieve an F-score of 92% on the GRE dataset,which outperforms the previous best result (82%)by a large margin.
Unlike what was suggested inprevious work, where relatedness statistics learnedfrom corpora is often claimed to yield extra gainsover lexicon-based models, we obtained this newstate-of-the-art result relying solely on lexical re-sources (Roget?s and WordNet), and corpus statis-tics does not seem to bring further improvement.To provide a comprehensive understanding, weconstructed our study in a framework that exam-ines a number of basic concerns in modeling con-trasting meaning.
We hope our efforts would helpshed some light on future directions for this basicsemantic modeling problem.1062 Related WorkThe terms contrasting, opposite, and antonymhave different definitions in the literature, whilesometimes they are used interchangeably.
Follow-ing (Mohammad et al, 2013), in this paper we re-fer to opposites as word pairs that ?have a strongbinary incompatibility relation with each other orthat are saliently different across a dimension ofmeaning?, e.g., day and night.
Antonyms are a sub-set of opposites that are also gradable adjectives,with same definition as in (Cruse, 1986) as well.Contrasting word pairs have the broadest mean-ing among them, referring to word pairs having?some non-zero degree of binary incompatibilityand/or have some non-zero difference across a di-mension of meaning.?
Therefore by definition, op-posites are a subset of contrasting word pairs (referto (Mohammad et al, 2013) for detailed discus-sions).Word Embedding Word embedding models learncontinuous representations for words in a low di-mensional space (Turney and Pantel, 2010; Hin-ton and Roweis, 2002; Collobert et al, 2011;Mikolov et al, 2013; Liu et al, 2015), which is notnew.
Linear dimension reduction such as LatentSemantic Analysis (LSA) has been extensivelyused in lexical semantics (see (Turney and Pantel,2010) for good discussions in vector space mod-els.)
Non-linear models such as those describedin (Roweis and Saul, 2000) and (Tenenbaum etal., 2000), among many others, can also be ap-plied to learn word embeddings.
A particularly in-teresting model is stochastic neighbor embedding(SNE) (Hinton and Roweis, 2002), which explic-itly enforces that in the embedding space, the dis-tribution of neighbors of a given word to be similarto that in the original, uncompressed space.
SNEcan learn multiple senses of a word with a mix-ture component.
Recently, neural-network basedmodel such as those proposed by (Collobert et al,2011) and (Mikolov et al, 2013) have attracted ex-tensive attention; particularly the latter, which canscale up to handle large corpora efficiently.Although word embeddings have recentlyshowed to be superior in some NLP tasks, theyare very weak in distinguishing contrasting mean-ing, as the models are often based on thewell-known distributional semantics hypothesis?words in similar context have similar meanings.Contrasting words have similar context too, socontrasting meaning is not distinguished well insuch representations.
Better models for contrast-ing meaning is fundamentally interesting.Modeling Contrasting Meaning Automaticallydetecting contrasting meaning has been studied inearlier work such as (Lin and Zhao, 2003; Mo-hammad et al, 2008; Mohammad et al, 2013).Specifically, as far as the embedding-based meth-ods are concerned, PILSA (Yih et al, 2012) madea progress in achieving one of the best results, bypriming LSA to encode contrasting meaning.
Inaddition, PILSA was also used to initialize a neu-ral network to get a further improvement on theGRE benchmark, where an F-score of 81% wasobtained.
Another recent method was proposedby (Zhang et al, 2014), called Bayesian proba-bilistic tensor factorization.
It considered multi-dimensional semantic information, relations, un-supervised data structure information in tensorfactorization, and achieved an F-score of 82% onthe GRE questions.
These methods employed bothlexical resources and corpora statistics to achievetheir best results.
In this paper, we show that us-ing only lexical resources to construct embeddingsystems can achieve significantly better results (anF-score of 92%).
To provide a more comprehen-sive understanding, we constructed our study in aframework that examines a number of basic con-cerns in modeling contrasting meaning within em-bedding.Note that sentiment contrast may be viewed asa specific case of more general semantic contrastor semantic differentials (Osgood et al, 1957).Tang et al (2014) learned sentiment-specific em-bedding and applied it to sentiment analysis oftweets, which was often solved with more conven-tional methods (Zhu et al, 2014b; Kiritchenko etal., 2014a; Kiritchenko et al, 2014b).3 The ModelsWe described in this section the framework inwhich we study word embedding for contrastingmeaning.
The general aim of the models is to en-force that in the embedding space, the word pairswith higher degrees of contrast will be put fartherfrom each other than those of less contrast.
Howto learn this is critical.
Figure 1 describes a veryhigh-level view of the framework.107Figure 1: A high-level view of the contrasting em-bedding framework.3.1 Top Hidden Layer(s)It is widely recognized that contrasting words,e.g., good and bad, also intend to appear in sim-ilar context or co-occur with each other.
For ex-ample, opposite pairs, special cases of contrastingwords, tend to co-occur more often than chance(Charles and Miller, 1989; Fellbaum, 1995; Mur-phy and Andrew, 1993).
Mohammad et al (2013),in addition, proposed a degree of contrast hypoth-esis, stating that ?if a pair of words, A and B, arecontrasting, then their degree of contrast is pro-portional to their tendency to co-occur in a largecorpus.
?These suggest some non-linear interaction be-tween distributional relatedness and the degree ofcontrast: the increase of relatedness correspondto the increase of both semantic contrast and se-mantic closeness; for example, they can form aU-shaped curve if one plots the word pairs on atwo dimensional plane with y-axis denoting relat-edness scores, while the most contrasting and (se-mantically) close pairs lie on the two side of thex-axis, respectively.
In this paper, when combin-ing word-pair distances learned by different com-ponents of the contrasting inference layer, we usesome top hidden layer(s) to provide a non-linearcombination.
Specifically, we use two hidden lay-ers, which is able to express complicated func-tions (Bishop, 2006).
We use ten hidden units ineach hidden layer.3.2 Stochastic Contrast Embedding (SCE)Hinton and Roweis (2002) proposed a stochas-tic neighbor embedding (SNE) framework.
Infor-mally, the objective is to explicitly enforce that inthe learned embedding space, the distribution ofneighbors of a given word w to be similar to thedistribution of its neighbors in the original, un-compressed space.In our study, we instead use the concept of?neighbors?
to encode the contrasting pairs, andwe call the model stochastic contrasting embed-ding (SCE), depicted by the left component of thecontrast inference layer in Figure 1.
The modelis different from SNE in three respects.
First,as mentioned above, ?neighbors?
here are actu-ally contrasting pairs?we enforce that in the em-bedding space, the distribution of the contrasting?neighbors?
to be close to the distribution of the?neighbors?
in the original, higher-dimensionalspace.
The probability of word wkbeing contrast-ing neighbor of the given word wican be com-puted as:p1(wk|wi) =exp(?d2i,k)?vm6=iexp(?d2i,m)(1)where d is some distance metric between wiandwk, and v is the size of a vocabulary.Second, we train SCE using only lexical re-sources but not corpus statistics, so as to explorethe behavior of lexical resources separately (wewill use the relatedness modeling component be-low to model distributional semantics).
Specifi-cally, we use antonym pairs in lexical resources tolearn contrasting neighbors.
Hence in the originalhigh-dimensional space, all antonyms of a givenword wihave the same probability to be its con-trasting neighbors.
That is, d in Equation (1) takesa binary score, with value 1 indicating an antonympair and 0 not.
In the embedding space, the cor-responding probability of wkto be the contrast-ing neighbor of wi, denoted as q1(wk|wi), can becomputed similarly with Equation (1).
But sincethe embedding is in a continuous space, d is notbinary but can be computed with regular distancemetric such as euclidean and cosine.
The objectiveis minimizing the KL divergence between p(.)
andq(.
).Third, semantic closeness or contrast are not in-dependent.
For example, if a pair of words, A andB, are synonyms, and if the pair of words, A andC, are contrasting, then A and C is likely to be108contrasting than a random chance.
SCE considersboth semantic contrast and closeness.
That is, fora given word wi, we jointly force that in the em-bedding space, its contrasting neighbors and se-mantically close neighbors to be similar to thosein the original uncompressed space.
These twoobjective functions are linearly combined with aparameter ?
and are jointly optimized to learn oneembedding.
The value of ?
is determined on thedevelopment questions of the GRE data.
Later inSection 4, we will discuss how the training pairs ofsemantic contrast and closeness are obtained fromlexical resources.3.3 Marginal Contrast Embedding (MCE)1In this paper, we use also another training criteria,motivated by the pairwise ranking approach (Co-hen et al, 1998).
The motivation is to explicitlyenforce the distances between contrasting pairs tobe larger than distances between unrelated wordpairs by a margin, and enforce the distances be-tween semantically close pairs to be smaller thanunrelated word pairs by another margin.
Morespecifically, we minimize the following objectivefunctions:Objs(mce)=?
(wi,wj)?Smax{0, ?
?di,r+di,j} (2)Obja(mce)=?
(wi,wk)?Amax{0, ?
?
di,k+ di,r}(3)where A and S are the set of contrasting pairs andsemantically close pairs in lexicons respectively;d denotes distance function between two words inthe embedding space.
The subscript r indicates arandomly sampled unrelated word.
We call thismodel Marginal Contrasting Embedding (MCE).Intuitively, if two words wiand wjare seman-tically close, the model maximizes Equation (2),which attempts to force the di,j(distance betweenwiand wj) in the embedding space to be differ-ent from that of two unrelated words di,rby amargin ?.
For each given word pair, we sample100 random words during training.
Similarly, iftwo words wiand wkare contrasting, the model1We made the code of MCE available athttps://github.com/lukecq1231/mce, as MCE achievedthe best performance according to the experimental resultsdescribed later in this paper.maximizes Equation (3), which attempts to forcethe distance between wiand wkto be differentfrom that of two unrelated words di,rby a mar-gin ?.
Same as in SCE, these two objective func-tions are linearly combined with a parameter ?
andare jointly optimized to learn one embedding foreach word.
This joint objective function attemptsto force the values of di,r(distances of unrelatedpairs) to be in between di,k(distances of contrast-ing pairs) and di,j(distances of semantically closepairs) by two margins.3.4 Corpus Relatedness Modeling (CRM)As discussed in previous work and above as well,relatedness obtained with corpora based on dis-tributional hypothesis interplays with semanticcloseness and contrast.
Mohammad et al (2013)proposed a degree of contrast hypothesis, statingthat ?if a pair of words, A and B, are contrast-ing, then their degree of contrast is proportionalto their tendency to co-occur in a large corpus.?
Inembedding, such dependency can be used to helpmeasure the degree of contrast.
Specifically, weuse the skip-gram model (Mikolov et al, 2013) tolearn the relatedness embedding.As discussed above, through the top hidden lay-ers, the word embedding and distances learned inSCE/MCE and CRM, together with that learnedwith SDR below, can be used to predict the GRE?most contrasting word??
questions.
With enoughGRE data, the prediction error may be backpropa-gated to directly adjust or learn embedding in thelook-up tables.
However, given the limited size ofthe GRE data, we only employed the top hiddenlayers to non-linearly merge the distances betweena word pair that are obtained within each of themodules in the Contrast Inference Layer.
We didnot backpropagate the errors to fine-tune alreadylearned word embeddings.Note that embeddings in the look-up tables werelearned independently in different modules in thecontrast inference layer, e.g., in SCE, MCE andCRM, respectively.
And in each module, given thecorresponding objective functions, unconstrainedoptimization (e.g., in the paper SGD) was usedto find embeddings that optimize the correspond-ing objectives.
The embeddings were then usedout-of-box and not further fine-tuned.
Depend-ing on experiment settings, embeddings learned ineach module are either used separately or jointly(through the top hidden lay) to predict test cases.109More details will be discussed in the experimentsection below.3.5 Semantic Differential Reconstruction(SDR)Using factor analysis, Osgood et al (1957) identi-fied three dimensions of semantics that account formost of the variation in the connotative meaningof adjectives.
These three dimensions are evalu-ative (good-bad), potency (strong-weak), and ac-tivity(active-passive).
We hypothesize that suchinformation should help reconstruct contrastingmeaning.The General Inquirer lexicon (Stone1966) rep-resents these three factors but has a limited cov-erage.
We used the algorithm of (Turney andLittman, 2003) to extend the labels to more wordswith Google one billion words corpus (refer toSection 4 for details).
For example, to obtain theevaluative score for a candidate wordw, the point-wise mutual information (PMI) between w and aset of seed words eval+and eval?are computedrespectively, and the evaluative value for w is cal-culated with:eval(w) = PMI(w, eval+)?
PMI(w, eval?
)(4)where eval+contains predefined positive evalua-tive words, e.g., good, positive, fortunate, and su-perior, while eval?includes negative evaluativewords like passive, slow, treble, and old.
The seedwords were selected as described in (Turney andLittman, 2003) to have a good coverage and toavoid redundancy at the same time.
Similarly, thepotency and activity scores of a word can be ob-tained.
The distances of a word pair on these threedimensions can therefore be obtained.4 Experiment Set-UpData Our experiment uses the ?most contrast-ing word?
questions collected by Mohammadet al (2008) from Graduate Record Examination(GRE), which was originally created by Educa-tional Testing Service (ETS).
Each GRE questionhas a target word and five candidate choices; thetask is to identify among the choices the most con-trasting word with regard to the given target word.The dataset consists of a development set and atest set, with 162 and 950 questions, respectively.As an example from (Mohammad et al, 2013),one of the questions has the target word adulter-ate and the five candidate choices: (A) renounce,(B) forbid, (C) purify, (D) criticize, and (E) cor-rect.
While in this example the choice correct hasa meaning that is contrasting with that of adulter-ate, the word purify is the gold answer as it has thegreatest degree of contrast with adulterate.Lexical Resources In our work, we use twopublicly available lexical resources, WordNet(Miller, 1995) (version 3.0) and the Roget?s The-saurus (Kipfer, 2009).
We utilized the labeledantonym relations to obtain more contrasting pairsunder the contrast hypothesis (Mohammad et al,2013), by assuming a contrasting pair is relatedto a pair of opposites (antonyms here).
Specif-ically in WordNet, we consider the word pairswith relations other than antonym as semanticallyclose.
In this way, we obtained a thesaurus con-taining 83,118 words, 494,579 contrasting pairs,and 368,209 close pairs.
Note that we did not onlyuse synonyms to expand the contrasting pairs.
Wewill discuss how this affects the performance inthe experiment section.In the Roget?s Thesaurus, every word or entryhas its synonyms and/or antonyms.
We obtained35,717 antonym pairs and 346,619 synonym pairs,which consist of 43,409 word types.
The antonymand synonym pairs in Roget?s were combined withcontrasting pairs and semantically close pairs inWordNet, respectively.
And in total, we have92,339 word types, 520,734 antonym pairs, and646,433 close pairs.Google Billion-Word Corpus The corpus used inour experiment for modeling lexical relatedness inthe CRM component was Google one billion wordcorpus (Chelba et al, 2013).
Normalization andtokenization were performed using the scripts dis-tributed from https://code.google.com/p/1-billion-word-language-modeling-benchmark/, and sen-tences were shuffled randomly.
We computed em-bedding for a word if its count in the corpus isequal to or larger than five, with the method de-scribed in Section 3.4.
Words with counts lowerthan five were discarded.Evaluation Metric Same as in previous work, theevaluation metric is F-score, where precision isthe percentage of the questions answered correctlyover the questions the models attempt to answer,110and recall is the percentage of the questions thatare answered correctly among all questions.5 Experiment ResultsIn training, we used stochastic gradient descent(SGD) to optimize the objective function, and thedimension of embedding was set to be 200.
InMCE (Equation 2 and 3) the margins ?
and ?
areboth set to be 0.4.
During testing, when using SCEor MCE embedding to answer the GRE questions,we directly calculated distances for a pair betweena question word and a candidate choice in thesetwo corresponding embedding spaces to reporttheir performances.
We also combined SCE/MCEwith other components in the contrast inferencelayer, for which we used ten-fold cross validationto tune the weights of the top hidden layers on ninefold and test on the rest and repeated this for tentimes to report the results.
As discussed above, er-rors were not backpropagated to modify word em-bedding.5.1 General Performance of the ModelsThe performance of the models are showed in Ta-ble 1.
For comparison, we list the results reportedin (Yih et al, 2012) and (Zhang et al, 2014).
Thetable shows that on the GRE dataset, both SCE (a90% F-score) and MCE (92%) significantly out-perform the previous best results reported in (Yihet al, 2012) (81%) and (Zhang et al, 2014) (82%).The F-score of MCE outperforms that of SCE by2%, which suggests the ranking criterion fits thedataset better.
In our experiment, we found thatthe MCE model achieved robust performances ondifferent distance metrics, e.g., the cosine simi-larity and Euclidean distance.
In the paper, wepresent the results with cosine similarity.
SCE isslightly more sensitive to distance metrics, and thebest performing metric on the development set isinner product, so we chose that for testing.Unlike what was suggested in the previouswork, where semantics learned from corpus isclaimed to yield extra gains in performance, weobtained this result by using solely lexical re-sources (Roget?s and WordNet) with SCE andMCE.
Using corpus statistics that model distri-butional hypothesis (MCE+CRM) and utilize se-mantic differential categories (MCE+CRM+SDR)does not bring further improvement here (they areuseful in the experiments discussed below in Sec-tion 5.3).5.2 Roles of Lexical ResourcesTo provide a more detailed comparison, we alsopresent lexicon lookup results, together with thosereported in (Zhang et al, 2014) and (Yih et al,2012).
For our lookup results and those copiedhere from (Zhang et al, 2014), the methods do notrandomly guess an answer if the target word is inthe vocabulary but none of the choices are, whilethe results of (Yih et al, 2012) randomly guessan answer in this situation.
The Encarta thesaurusused in (Yih et al, 2012) is not publicly available,so we did not use it in our experiments.
We duethe differences among the lookup results on Word-Net (WordNet lookup) to the differences in prepro-cessing as well as the way we expanded indirectcontrasting word pairs.
As described in Section 4,we utilized all relations other than antonym pairsto expand our indirect antonym pairs.
These alsohave impact on the W&R lookup results (WordNetand Roget?s pairs are combined).
For both set-tings, our expansion resulted in much better per-formances.Whether the differences between the F-scoresof MCE/SCE and that reported in (Zhang et al,2014) and (Yih et al, 2012) are also due to thedifferences in expanding indirect pairs?
To answerthis, we downloaded the word pairs that Zhang etal.
(2014) used to train their models,2but we usedthem to train our MCE.
The result are presented inTable 1 and the F-score on test set is 91%, whichis only slightly lower than MCE using our lexicon.So the extension is very helpful for lookup meth-ods, but the MCE appears to be able to cover suchinformation by itself.SCE and MCE learn contrasting meaning thatis not explicitly encoded in lexical resources.
Theexperiment results show that such implicit contrastcan be recovered by jointly learning the embed-ding by using contrasting words and other seman-tically close words.To help better understand why corpus statis-tics does not further help SCE and MCE, wefurther demonstrate that most of the target-gold-answer pairs in the GRE test set are connectedby short paths (with length between 1 to 3).More specifically, based on breadth-first search,we found the nearest paths that connect target-gold-answer pairs, in the graph formed by Word-Net and Roget?s?each word is a vertex, and con-trasting words and semantically close words are2https://github.com/iceboal/word-representations-bptf111Development Set Test SetPrec.
Rec.
F1Prec.
Rec.
F1WordNet PILSA (Yih et al, 2012) 0.63 0.62 0.62 0.60 0.60 0.60WordNet MRLSA (Yih et al, 2012) 0.66 0.65 0.65 0.61 0.59 0.60Encarta lookup (Yih et al, 2012) 0.65 0.61 0.63 0.61 0.56 0.59Encarta PILSA (Yih et al, 2012) 0.86 0.81 0.84 0.81 0.74 0.77Encarta MRLSA (Yih et al, 2012) 0.87 0.82 0.84 0.82 0.74 0.78WordNet lookup (Yih et al, 2012) 0.40 0.40 0.40 0.42 0.41 0.42WordNet lookup (Zhang et al, 2014) 0.93 0.32 0.48 0.95 0.33 0.49WordNet lookup 0.97 0.37 0.54 0.97 0.41 0.58Roget lookup (Zhang et al, 2014) 1.00 0.35 0.52 0.99 0.31 0.47Roget lookup 1.00 0.32 0.49 0.97 0.29 0.44W&R lookup (Zhang et al, 2014) 1.00 0.48 0.64 0.98 0.45 0.62W&R lookup 0.98 0.52 0.68 0.97 0.52 0.68(Mohammad et al, 2008) Best 0.76 0.66 0.70 0.76 0.64 0.70(Yih et al, 2012) Best 0.88 0.87 0.87 0.81 0.80 0.81(Zhang et al, 2014) Best 0.88 0.88 0.88 0.82 0.82 0.82SCE 0.94 0.93 0.93 0.90 0.90 0.90MCE (using zhang et al lex.)
0.94 0.93 0.94 0.92 0.91 0.91MCE 0.96 0.94 0.95 0.92 0.92 0.92MCE+CRM 0.94 0.93 0.93 0.90 0.90 0.90MCE+CRM+SDR 0.04 0.94 0.94 0.90 0.90 0.90Table 1: Results on the GRE ?most contrasting words?
questions.connected with these two types of edges respec-tively.
Then we require the shortest path must haveone and only one contrasting edge.
Word pairs thatcannot be connected by such paths are regarded tohave an infinite length of distance.Figure 2: Percentages of target-gold-answer wordpairs, categorized by the shortest lengths of pathsconnecting them.The pie graph in Figure 2 shows the percentagesof target-gold-answer word pairs, categorized bythe lengths of shortest paths defined above.
Wecan see that in the GRE data, the percentage ofpaths with a length larger than three is very small(1%).
It seems that SCE and MCE can learn thisvery well.
Again, they force semantically closepairs to be close in the embedding spaces which?share?
similar contrasting pairs.Figure 3 draws the envelope of histogram ofcosine distance between all target-choice wordpairs in the GRE test set, calculated in the em-bedding space learned with MCE.
The figure in-tuitively shows how the target-gold-answer pairs(most contrasting pairs) are discriminated from theother target-choice pairs.
We also plot the MCEresults without using the random sampling de-picted in Equation (2) and Equation (3), showingthat discriminative power dramatically dropped.Without the sampling, the F-score achieved on thetest data is 83%.5.3 Roles of Corpus-based EmbeddingHowever, the findings presented above should notbe simply taken as that distributional hypothesisis not useful for learning lexical contrast.
Our re-sults and detailed analysis has showed it is due tothe good coverage of the manually created lexi-cal resources and the capability of the SCE and112Figure 4: The effect of removing lexicon items.
?1 ?0.5 0 0.5 100.050.10.150.20.25Cosine similarityFrequencyMCE on most contrasting pairsMCE on othersMCE w/o negative sampling on most contrasting pairsMCE w/o negative sampling on othersFigure 3: The envelope of histogram of cosine dis-tance between word pair embeddings in GRE testset.MCE models in capturing indirect semantic rela-tions.
There may exist circumstances where thecoverage is be lower, e.g., for resource-poor lan-guages or social media text where (indirect) out-of-vocabulary pairs may be frequent.To simulate the situations, we randomly re-moved different percentages of words from thecombined thesaurus used above in our experi-ments, and removed all the corresponding wordpairs.
The performances of different models areshowed in Figure 4.
It is observed that as theout of vocabulary (OOV) becomes more serious,the MCE suffered the most.
Using the seman-tic differential (MCE+SDR) showed to be help-ful as 50% to 70% lexicon entries are kept.
Con-sidering relatedness learned from corpus togetherwith MCE (MCE+CRM), i.e., combining MCEdistances with CRM distances for target-choicepairs, yielded robust performance?the F-score ofMCE+CRM drops significantly slower than thatof MCE, as we removed lexical entries.
We alsocombined MCE distances and CRM distances lin-early (MCE+CRM (linear)), with a coefficient de-termined with the development set.
It showed aperformance worse than that of MCE+CRM when50%?80% entries kept, while as discussed above,MCE+CRM combines the two parts with the non-linear top layers.
In general, using corpora statis-tics make the models more robust as OOV be-comes more serious.
It deserves to note that theuse of corpora here is rather straightforward; morepatterns may be learned from corpora to capturecontrasting expressions as discussed in (Moham-mad et al, 2013).
Also, context such as nega-tion may change contrasting meaning, e.g., sen-timent contrast (Kiritchenko et al, 2014b; Zhu etal., 2014a), in a dramatic and complicated manner,which has been considered in learning sentimentcontrast (Kiritchenko et al, 2014b).6 ConclusionsContrasting meaning is a basic aspect of seman-tics.
In this paper, we present a new state-of-the-art result, a 92% F-score, on the GRE dataset cre-ated by (Mohammad et al, 2008), which is widelyused as the benchmark for modeling lexical con-trast.
The result reported here outperforms thebest reported in previous work (82%) by a largemargin.
Unlike what was suggested in most pre-vious work, we show that this performance can beachieved without relying on corpora statistics.
Toprovide a more comprehensive understanding, weconstructed our study in a framework that exam-113ines a number of concerns in modeling contrast-ing meaning.
We hope our work could help shedsome light on future directions on this basic se-mantic problem.From our own viewpoints, creating more eval-uation data for measuring further progress incontrasting-meaning modeling, e.g., handling realOOV issues, is interesting to us.
Also, the de-gree of contrast may be better formulated as a re-gression problem rather than a classification prob-lem, in which finer or even real-valued annotationwould be desirable.ReferencesChristopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning.
Springer-Verlag New York,Inc., Secaucus, NJ, USA.Walter G. Charles and George A. Miller.
1989.
Con-texts of antonymous adjectives.
Applied Psychol-ogy, 10:357?375.Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One billion word benchmark for mea-suring progress in statistical language modeling.arXiv:1312.3005.William W. Cohen, Robert E. Schapire, and YoramSinger.
1998.
Learning to order things.
Journal ofArticial Intelligence Research (JAIR), 10:243?270.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.David A. Cruse.
1986.
Lexical semantics.
CambridgeUniversity Press.Christiane Fellbaum.
1995.
Co-occurrence andantonymy.
International Journal of Lexicography,8:281?303.Geoffrey Hinton and Sam Roweis.
2002.
Stochasticneighbor embedding.
In Advances in Neural Infor-mation Processing Systems 15, pages 833?840.
MITPress.Barbara Ann Kipfer.
2009.
Rogets 21st Century The-saurus.
Philip Lief Group, third edition edition edi-tion.Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-mad.
2014a.
Nrc-canada-2014: Detecting aspectsand sentiment in customer reviews.
In Proceedingsof International Workshop on Semantic Evaluation,Dublin, Ireland.Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-mad.
2014b.
Sentiment analysis of short informaltexts.
Journal of Artificial Intelligence Research,50:723?762.Dekang Lin and Shaojun Zhao.
2003.
Identifying syn-onyms among distributionally similar words.
In InProceedings of IJCAI-03, pages 1492?1493.Quan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, andYu Hu.
2015.
Learning semantic word embed-dings based on ordinal knowledge constraints.
InProceedings of ACL, Beijing, China.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.George A Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 982?991.
Associ-ation for Computational Linguistics.Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, andPeter D. Turney.
2013.
Computing lexical contrast.Computational Linguistics, 39(3):555?590.Gregory L. Murphy and Jane M. Andrew.
1993.The conceptual basis of antonymy and synonymyin adjectives.
Journal of Memory and Language,32(3):1?19.Charles E Osgood, George J Suci, and Percy Tannen-baum.
1957.
The measurement of meaning.
Univer-sity of Illinois Press.Sam T. Roweis and Lawrence K. Saul.
2000.
Nonlin-ear dimensionality reduction by locally linear em-bedding.
Science, 290:2323?2326.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In Proceedings of ACL, Baltimore, Mary-land, USA, June.Joshua B. Tenenbaum, Vin de Silva, and John C.Langford.
2000.
A global geometric frameworkfor nonlinear dimensionality reduction.
Science,290(5500):2319.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems (TOIS), 21(4):315?346.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of seman-tics.
J. Artif.
Int.
Res., 37(1):141?188, January.114Wen-tau Yih, Geoffrey Zweig, and John C Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1212?1222.
Association for Computational Linguis-tics.Jingwei Zhang, Jeremy Salwen, Michael Glass, andAlfio Gliozzo.
2014.
Word semantic representa-tions using bayesian probabilistic tensor factoriza-tion.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 1522?1531, Doha, Qatar, October.Association for Computational Linguistics.Xiaodan Zhu, Hongyu Guo, Saif Mohammad, andSvetlana Kiritchenko.
2014a.
An empirical study onthe effect of negation words on sentiment.
In Pro-ceedings of ACL, Baltimore, Maryland, USA, June.Xiaodan Zhu, Svetlana Kiritchenko, and Saif Moham-mad.
2014b.
Nrc-canada-2014: Recent improve-ments in the sentiment analysis of tweets.
In Pro-ceedings of International Workshop on SemanticEvaluation, Dublin, Ireland.115
