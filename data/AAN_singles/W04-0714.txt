Topic Identification in Chinese Based on Centering ModelChing-Long Yeh and Yi-Chun ChenDepartment of Computer Science and EngineeringTatung University40 Chungshan N. Rd.
3rd.
SectionTaipei 104 TaiwanR.O.C.chingyeh@cse.ttu.edu.tw, d8806005@mail.ttu.edu.twAbstractIn this paper we are concerned withidentifying the topics of sentences in Chinesetexts.
The key elements of the centering modelof local discourse coherence are employed toidentify the topic which is the most salientelement in a Chinese sentence.
Due to thephenomenon of zero anaphora occurring inChinese texts frequently, in addition to thecentering model, we further employ theconstraint rules to identify the antecedents ofzero anaphors.
Unlike most traditionalapproaches to parsing sentences based on theintegration of complex linguistic informationand domain knowledge, we work on theoutput of a part-of-speech tagger and useshallow parsing instead of complex parsing toidentify the topics from sentences.1 IntroductionOne of the most striking characteristics in atopic-prominent language like Chinese is theimportant element, ?topic,?
in a sentence whichcan represent what the sentence is about (Li andThompson, 1981).
That is, if we can identify topicsfrom Chinese sentences, we can obtain the mostinformation embedded in the text.
In this paper, wetend to identify the topic of each utterance within adiscourse based on the centering model.
However,in many natural languages, elements that can beeasily deduced by the reader are frequently omittedfrom expressions in texts.
The elimination ofanaphoric expressions is termed zero anaphor (ZA)which often occurs in topic position in a Chinesesentence, due to their prominence in discourse.Accordingly, to accomplish the task of topicidentification, we have to solve the problem ofzero anaphora resolution.There are several methods of anaphoraresolution.
One method is to integrate differentknowledge sources or factors (e.g.
gender andnumber agreement, c-command constraints,semantic information) that discount unlikelycandidates until a minimal set of plausiblecandidates is obtained (Grosz et al, 1995; Lappinand Leass, 1994; Okumura and Tamura, 1996;Walker et al, 1998; Yeh and Chen, 2001).Anaphoric relations between anaphors and theirantecedents are identified based on the integrationof linguistic and domain knowledge.
However, it isvery labor-intensive and time-consuming toconstruct a domain knowledge base.
Anothermethod employs statistical models or AItechniques, such as machine learning, to computethe most likely candidate (Aone and Bennett, 1995;Connoly et al, 1994; Ge et al, 1998; Seki et al,2002).
This method can sort out the aboveproblems.
However, it heavily relies upon theavailability of sufficiently large text corpora thatare tagged, in particular, with referentialinformation (Stuckardt, 2002).Our method is an inexpensive, fast and reliableprocedure for anaphora resolution, which relies oncheaper and more reliable NLP tools such as part-of-speech (POS) tagger and shallow parsers(Baldwin, 1997; Ferr?ndez et al, 1998; Kennedyand Boguraev, 1996; Mitkov, 1998; Yeh and Chen,2003).
The resolution process works from theoutput of a POS tagger enriched with annotationsof grammatical function of lexical items in theinput text stream.
The shallow parsing technique isused to detect zero anaphors and identifies thenoun phrases preceding the anaphors asantecedents.In the following sections we first describe thecentering model which including the key elementsof the centering model of local discoursecoherence.
In Section 3 we describe the details ofshallow parsing we employed.
In Section 4 weexplain our ZA resolution method based on thecentering model and the constraint rules.
Themethod of topic identification in Chinese sentencesis illustrated in Section 5.
In the last section theconclusions are made.2 Centering ModelIn the centering theory (Grosz and Sidner, 1986;Grosz et al 1995; Walker et al, 1994; Strube andHahn, 1996), the 'attentional state' was identified asa basic component of discourse structure thatconsisted of two levels of focusing: global andlocal.
For Grosz and Sidner, the centering theoryprovided a model for monitoring local focus andyielded the centering model which was designed toaccount for the difference in the perceivedcoherence of discourses.
In the centering model,each utterance U in a discourse segment has twostructures associated with it, called forward-looking centers, Cf(U), and backward-lookingcenter, Cb(U).
The forward-looking centers of Un,Cf(Un), depend only on the expressions thatconstitute that utterance.
They are not constrainedby features of any previous utterance in thediscourse segment (DS), and the elements of Cf(Un)are partially ordered to reflect relative prominencein Un.
Grosz et al, in their paper (Grosz et al1995), assume that grammatical roles are the majordeterminant for ranking the forward-lookingcenters, with the order ?Subject > Object(s) >Others?.
The superlative element of Cf(Un) maybecome the Cb of the following utterance, Cb(Un+1).In addition to the structures for centers, Cb, andCf, the centering model specifies a set ofconstraints and rules (Grosz et al 1995; Walker etal.
1994).ConstraintsFor each utterance Ui in a discourse segmentconsisting of utterances U1, ?, Um:1.
Ui has exactly one Cb.2.
Every element of Cf(Ui) must be realized in Ui.3.
Ranking of elements in Cf(Ui) guidesdetermination of Cb(Ui+1).4.
The choice of Cb(Ui) is from Cf(Ui-1), and cannot be from Cf(Ui-2) or other prior sets of Cf.Backward-looking centers, Cbs, are oftenomitted or pronominalized and discourses thatcontinue centering the same entity are morecoherent than those that shift from one center toanother.
This means that some transitions arepreferred over others.
These observations areencapsulated in two rules:RulesFor each utterance Ui in a discourse segmentconsisting of utterances U1, ?, Um:I.
If any element of Cf(Ui) is realized by apronoun in Ui+1 then the Cb(Ui+1) must berealized by a pronoun also.II.
Sequences of continuation are preferred oversequence of retaining; and sequences ofretaining are to be preferred over sequences ofshifting.Rule I represents one function of pronominalreference: the use of a pronoun to realize the Cbsignals the hearer that the speaker is continuing totalk about the same thing.
Psychological researchand cross-linguistic research have validated thatthe Cb is preferentially realized by a pronoun inEnglish and by equivalent forms (i.e.
zeroanaphora) in other languages (Grosz et al 1995).Rule II reflect the intuition that continuation ofthe center and the use of retentions when possibleto produce smooth transitions to a new centerprovide a basis for local coherence.For example in (1), the subjects of the utterance(1b) and (1d)  are eliminated, and their antecedentsare identified as the subjects of the precedingutterances (1a) and (1c) respectively1 according tothe centering model.
(1) a.
 i  	Electronics stocki receive USA high-techstock heavy-fall affectElectronics stocksi were affected by high-tech stocks fallen heavily in America.b.
i  (Electronics stocks)i continue fall(Electronics stocks)i continued falling down.c.
 j    Securities stocksj also have relativerespondenceSecurities stocksj also had respondence.d.
j    (Securities stocks) j continue fall by close.
(Securities stocks) j fell by close one afteranother.3 Shallow ParsingShallow (or partial) parsing which is aninexpensive, fast and reliable method does notdeliver full syntactic analysis but is limited toparsing smaller constituents such as noun phrasesor verb phrases (Abney, 1996; Li and Roth, 2001;Mitkov, 1999).
For example, the sentence (2) canbe divided as follows:(2) !
"# $% & '( )*Hualien became the popular tourist attraction.?
[NP ! ]
[VP "# ] [NP $% & '( )*]1We use a ba?
to denote a zero anaphor, where thesubscript a is the index of the zero anaphor itself and thesuperscript b is the index of the referent.
A single without any script represents an intrasentential zeroanaphor.
Also note that a superscript attached to an NPis used to represent the index of the referent.
[NP Hualien] [VP became] [NP the populartourist attraction]Given a Chinese sentence, our method ofshallow parsing is divided into the following steps:First the sentence is divided into a sequence ofPOS-tagged words by employing a segmentationprogram, AUTOTAG, which is a POS taggerdeveloped by CKIP, Academia Sinica (CKIP,1999).
Second the sequence of words is parsed intosmaller constituents such as noun phrases and verbphrases with phrase-level parsing.
Each phrase isrepresented as a word list.
Then the sequence ofword lists is transformed into triples, [S,P,O].
Forexample in (3), (3b) is the output of sentence (3a)produced by AUTOTAG, and (3c) is the triplerepresentation.
(3) a.
[ !
(Nc) "#(VG) $%(VH) &(DE) '((VA) )*(Na)]b.
[NP,[ !
]], [VP,["#]], [NP,[$%,&,'(,)*]]c. [[ !
], ["#], [$%,&,'(,)*]]The definition of triple representation isillustrated in Definition 1.The triple here is asimple representation which consists of threeelements: S, P and O which correspond to theSubject (noun phrase), Predicate (verb phrase) andObject (noun phrase) respectively in a clause.Definition 1:A Triple T is characterized by a 3-tuple:T = [S, P, O] wherez S is a list of nouns whose grammaticalrole is the subject of a clause.z P is a list of verbs or a preposition whosegrammatical role is the predicate of aclause.z O is a list of nouns whose grammaticalrole is the object of a clause.In the step of triple transformation, the sequenceof word lists as shown in (3b) is transformed intotriples by employing the Triple Rules.
The TripleRules is built by referring to the Chinese syntax.There are four kinds of Triples in the Triple Rules,which corresponds to five basic clauses: subject +transitive verb + object, subject + intransitive verb,subject + preposition + object, and a noun phraseonly.
The rules listed below are employed in order:Triple Rules:Triple1(S,P,O) ?
np(S), vtp(P), np(O).Triple2(S,P,none) ?
np(S), vip(P).Triple3(S,P,O) ?
np(S), prep(P), np(O).Triple4(S,none,none) ?
np(S).The vtp(P) denotes the predicate is a transitiveverb phrase, which contains a transitive verb in therightmost position in the phrase; likewise the vip(P)denotes the predicate is an intransitive verb phrase,which contains an intransitive verb in the rightmostposition in the phrase.
In the rule Triple3, theprep(P) denotes the predicate is a preposition.
TheTriple4 is employed if only a sentence containsonly one noun phrase and no other constituent.
Ifall the rules in the Triple Rules failed, the ZATriple Rules are employed to detect zero anaphor(ZA) candidates.ZA Triple Rules:Triple1z1(zero,P,O)?
vtp(P), np(O).Triple1z2(S,P,zero)?
np(S), vtp(P).Triple1z3(zero,P,zero)?
vtp(P).Triple2z1 (zero,P,none)?
vip(P).Triple3z1(zero,P,O) ?
prep(P), np(O).Triple4z1(zero,P,O) ?
co-conj(P), np(O).The zero anaphora in Chinese generally occursin the topic, subject or object position.
The rulesTriple1z1, Triple2z1, and Triple3z1 detect the zeroanaphora occurring in the topic or subject position.The rule Triple1z2 detects the zero anaphora in theobject position and Triple1z3 detect the zeroanaphora occurring in both subject and objectpositions.
In the Triple4, the co-conj(P) denotes acoordinating conjunction appearing in the initialposition of a clause.
For example in (4), there aretwo triples generated.
In the second triple, zerodenotes a zero anaphor according to Triple1z1.
(4) +, -.
/0 12 34Zhangsan entered a competition and won thechampion.?
[[[+,], [-.
], [/0]], [[zero], [12], [34]]][[[Zhangsan], [enter], [competition]], [[zero],[win], [champion]]]4 Zero Anaphora Resolution4.1 ZA Resolution MethodThe process of analyzing Chinese zero anaphorais different from general pronoun resolution inEnglish because zero anaphors are not expressed indiscourse.
Therefore, the ZA resolution method wedevelop is divided into three phases.
First eachsentence of an input document is translated intotriples as described in Section 3.
Second is ZAidentification that verifies each ZA candidatesannotated in triples by employing ZAidentification constraints.
Third is antecedentidentification that identifies the antecedent of eachdetected ZA using rules based on the centeringmodel.In the ZA detection phase, we use the ZA TripleRules described in the Section 3 to detect omittedcases as ZA candidates denoted by zero in triples.Table 1 shows some examples corresponding tothe ZA Triple Rules.ZA Triple Rule ExampleTriple1z1(zero,P,O)56 7 8 9 (1b)zhuangdao yi ge ren(he) bump-to a person(He) bumped into a person.Triple1z2(S,P,zero)+, :; <Zhangsan xihuan maZhangsan like (somebodyor something) Q2Does Zhangsan like(somebody or something)?Triple1z3(zero,P,zero):;xihuan(he) like (somebody orsomething)(He) likes (somebody orsomething).Triple2z1(zero,P,none)= >?
@qu gouwu le(he) go shopping ASPECT(He) has gone shopping.Triple3z1(zero,P,O)A BCzai nabian(he) in there(He) is there.Triple4z1(zero,P,O)D EFG Hgen xiaopengyou wan(he) with child play(He) is playing with littlechildren.Table 1: Examples of zero anaphoraAfter ZA candidates are detected by employingthe ZA Triple Rules, the ZA identificationconstraints are utilized to filter out non-anaphoriccases.
In the ZA identification constraints, theconstraint 1 is employed to exclude the exophora3or cataphora4 which is different from anaphora in2We use a Q to denote a question (ma); aASPECT to denote aspect marker.3Exophora is reference of an expression directly toan extralinguistic referent and the referent does notrequire another expression for its interpretation.4Cataphora arises when a reference is made to anentity mentioned subsequently.texts.
The constraint 2 includes some cases mightbe incorrectly detected as zero anaphors, such aspassive sentences or inverted sentences (Hu, 1995).ZA identification constraintsFor each ZA candidate c in a discourse:1. c can not be in the first utterance in adiscourse segment2.
ZA does not occur in the following case:NP + bei + NP + VP + cNP (topic) + NP (subject) + VP + cIn the antecedent identification phase, weemploy the concept, ?backward-looking center?
ofcentering model to identify the antecedent of eachZA.
First we use noun phrase rules to obtain nounphrases in each utterance, and then the antecedentis identified as the most prominent noun phrase ofthe preceding utterance (Yeh and Chen, 2001):Antecedent identification rule:For each zero anaphor z in a discourse segmentconsisting of utterances U1, ?
, Um:If z occurs in Ui, and no zero anaphor occurs inUi-1then choose the noun phrase with thecorresponding grammatical role in Ui-1 asthe antecedentElse if only one zero anaphor occurs in Ui-1then choose the antecedent of the zeroanaphor in Ui-1 as the antecedent of zElse if more than one zero anaphor occurs inUi-1then choose the antecedent of the zeroanaphor in Ui-1 as the antecedent of zaccording to grammatical role criteria: Topic> Subject > Object > OthersEnd ifDue to topic-prominence in Chinese (Li andThompson, 1981), topic is the most salientgrammatical role.
In general, if the topic is omitted,the subject will be in the initial position of anutterance.
If the topic and subject are omittedconcurrently, the ZA occurs.
Since the antecedentidentification rule is corresponding to the conceptof centering model.4.2 ZA Resolution ExperimentIn the experiment of ZA resolution, we use a testcorpus which is a collection of 150 news articlescontained 998 paragraphs, 4631 utterances, and40884 Chinese words.
By employing the ZA TripleRules and ZA identification constraints mentionedpreviously, zero anaphors occur in topic or subject,and object positions can be detected.
Because theZA Triple Rules cover each possible topic orsubject, and object omission cases, the resultshows that the zero anaphors are over detected andthe precision rate (PR) is 84% calculated usingequation 1.candidates ZA of No.detectedcorrectly  ZA of No.detection ZA of PR = ....(1)The main errors of ZA detection occur in theexperiment when parsing inverted sentences andnon-anaphoric cases (e.g.
exophora or cataphora)(Hu, 1995; Mitkov, 2002).
Cataphora is similar toanaphora, the difference being the direction of thereference.
In this paper, we do not deal with thecase that the referent of a zero anaphor is in thefollowing utterances, but we can detect about 60%cataphora in the test corpus by employing ZAidentification constraint 1.In the phase of antecedent identification, wetake the output of employing the ZA Triple Rulesand ZA identification constraints, and further toidentify the antecedents of zero anaphors by usingantecedent identification rule based on thecentering model.
For example, in the discoursesegment (5), the zero anaphors are detected in theutterances (5b) and (5c).
According to theantecedent identification rule, the noun phrase,     ?Kee-lung General Hospital,?
whosegrammatical role is corresponding to the zeroanaphor i1?
in (5b) is identified as the antecedent.Subsequently, the antecedent of the zero anaphori2?
in (5c) is identified as the antecedent of i1?
in(5b), .
(5) a. IJKL i # MN OP QRJilong yiyuan wei kuoda fuwu fanweiKee-lung hospital for expand servicecoverageKee-lungi General Hospital aims to increaseservice coverage.b.
i1?
ST UV KW OP XY Z [\]jiji tisheng yiliao fuwu pinzhi ji biaozhunhua(Kee-lung General Hospital)i active improvemedical-treatment service quality andstandardization(Kee-lung General Hospital)i activelyimproves the service quality of medicaltreatment and standardization.c.i2?
 huo weishengshu renke wei banli wailaotijian yiyuan(Kee-lung General Hospital)i obtainDepartment-of-Health certify to-be handleforeign-laborer physical-examinationhospital(Kee-lung General Hospital)i is certified byDepartment of Health as a hospital whichcan handle physical examinations of foreignlaborers.The recall rates (RR) and precision rates (PR) ofZA resolution is 70% and 60.3% respectivelycalculated using equation 2 and equation 3.
Errorsoccur in the phase when a zero anaphor refers to anentity other than the corresponding grammaticalrole or the antecedent of the zero anaphor in thepreceding utterance.candidates ZA of No.identifiedcorrectly  ant.
of No.resolution ZA of RR = .. (2)in text occurred ZA of No.identifiedcorrectly  ant.
of No.resolution ZA of PR = ... (3)5 Topic IdentificationTopic identification is similar to themeidentification in (Rambow, 1993).
The themeclearly corresponds to the Cb: the theme, under ageneral definition, is what the current utterance isabout; what utterances are about provides a link toprevious discourse, since otherwise the text wouldbe incoherent.
The role of the Cb is precisely toprovide such a link.In our approach, in addition to the centeringmodel, we further employ the antecedentidentification rule to identify the topic.
When a ZAoccurs in the utterance Ui, the antecedent of the ZAis identified as the topic of Ui.
Otherwise, if thetransition relation, center shifting, occurs, topicwill not be identified as any of the element in thepreceding utterance but the element in the currentutterance according to grammatical role criteria.The topic identification rule is described below:Topic identification rule:For identifying each topic t in a discoursesegment consisting of utterances U1, ?
, Um:If at least one ZA occurs in Uithen refer to grammatical role criteria tochoose the antecedent of the ZA as the tElse if no ZA occurs in Uithen refer to grammatical role criteria tochoose one element of Ui as the tEnd ifWe now take the discourse segment (1) as anexample to identify each topic of the utterances (1a)to (1d) by employing the topic identification rule.As illustrated in Table2, the topic of (1a) is ?Electronics stocks,?
and the topic of (1b) isidentified as the antecedent of  i,   ?Electronics stocks.?
Similarly, the topic of (1d) is ?Securities stocks,?
which is the same asthe topic of (1c).Utterance Topic(1a)  i  	Electronics stocksi wereaffected by high-techstocks fallen heavily inAmericaElectronicsstocks(1b) i  (Electronics stocks)icontinued falling down.Electronicsstocks(1c)  j   Securities stocksj also hadrespondenceSecuritiesstocksj   (Securities stocks)j fell byclose one after anotherSecuritiesstocksTable 2: Examples of zero anaphora6 ConclusionIn this paper, we propose a method of topicidentification in Chinese based on the centeringmodel.
Based on observations on real texts, wefound that to identify the topics in Chinese contextis much related to the issue of zero anaphoraresolution.
We use a zero anaphora resolutionmethod to resolve the problem of ellipsis inChinese text.
The zero anaphora resolution methodworks on the output of a part-of-speech tagger andemploys a shallow parsing instead of a complexparsing to resolve zero anaphors in Chinese text.Due to time limit, we have not applied the result oftopic identification to applications for evaluation.We will further continue improving the accuracyof zero anaphora resolution and develop theapplications based on topic identification, such asinformation extraction/retrieval and textcategorization.7 AcknowledgementsWe give our special thanks to CKIP, AcademiaSinica for making great efforts in computationallinguistics and sharing the Autotag program toacademic research.ReferencesAbney, Steven.
1996.
Tagging and Partial Parsing.In: Ken Church, Steve Young, and GerritBloothooft (eds.
), Corpus-Based Methods inLanguage and Speech.
An ELSNET volume.Kluwer Academic Publishers, Dordrecht.Aone, Chinatsu and Bennett, Scott William.
1995.Evaluating automated and manual acquisition ofanaphora resolution strategies.
In Proceedings ofthe 33rd Annual Meeting of the ACL, Santa Cruz,CA, pages 122?129.Baldwin B.
1997.
CogNIAC: high precisioncoreference with limited knowledge andlinguistic resources.
ACL/EACL workshop onOperational factors in practical, robust anaphorresolution.CKIP.
1999.
^_`abcde  Version 1.0(Autotag), http://godel.iis.sinica.edu.tw/CKIP/,Academia Sinica.Connoly, Dennis, John D. Burger & David S. Day.1994.
A Machine learning approach toanaphoric reference.
Proceedings of theInternational Conference on New Methods inLanguage Processing, 255-261, Manchester,United Kingdom.Eleni Miltsakaki.
1999.
Locating Topics in TextProcessing.
In Proceedings of CLIN '99.Ferr?ndez, A., Palomar, Manuel and Moreno, Lidia.1998.
Anaphor Resolution in Unrestricted Textswith Partial Parsing.
Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING'98)/ACL'98 Conference,pages 385-391.
Montreal, Canada.Ge, Niyu, Hale, John and Charniak, Eugene.
1998.A statistical approach to anaphora resolution.
InProceedings of the Sixth Workshop on VeryLarge Corpora, pages 161 ?170.Grosz, B. J. and Sidner, C. L.. 1986.
Attention,intentions, and the structure of discourse.Computational Linguistics, No 3 Vol 12, pp.175-204.Grosz, B. J., Joshi, A. K. and Weinstein, S. 1995.Centering: A Framework for Modeling theLocal Coherence of Discourse.
ComputationalLinguistics, 21(2), pp.
203-225.Hu, Wenze.
1995.
Functional Perspectives andChinese Word Order.
Ph.
D. dissertation, TheOhio State University.Kennedy, Christopher and Branimir Boguraev.1996.
Anaphora for everyone: pronominalanaphora resolution without a parser.Proceedings of the 16th InternationalConference on Computational Linguistics(COLING'96), 113-118.
Copenhagen, Denmark.Lappin, S. and Leass, H. 1994.
An algorithm forpronominal anaphor resolution.
ComputationalLinguistics, 20(4).Li, Charles N. and Thompson, Sandra A.
1981.Mandarin Chinese ?
A Functional ReferenceGrammar, University of California Press.Li, X.; Roth D. 2001.
Exploring Evidence forShallow Parsing.
Proceedings of Workshop onComputational Natural Language Learning,Toulouse, France.Okumura, Manabu and Kouji Tamura.
1996.
Zeropronoun resolution in Japanese discourse basedon centering theory.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics (COLING-96), 871-876.Mitkov, Ruslan.
1998.
Robust pronoun resolutionwith limited knowledge.
Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING'98)/ACL'98 Conference.Montreal, Canada.Mitkov, Ruslan.
1999.
Anaphora resolution: thestate of the art.
Working paper (Based on theCOLING'98/ACL'98 tutorial on anaphoraresolution).
University of Wolverhampton,Wolverhampton.Mitkov, Ruslan.
2002.
Anaphora Resolution,Longman.Rambow, O.
(1993).
Pragmatic aspects ofscrambling and topicalization in German: ACentering Approach.
In IRCS Workshop onCentering in Discourse.
Univ.
of Pennsylvania,1993.Seki, Kazuhiro, Fujii, Atsushi, and Ishikawa,Tetsuya.
2002.
A Probabilistic Method forAnalyzing Japanese Anaphora Integrating ZeroPronoun Detection and Resolution.
Proceedingsof the 19th International Conference onComputational Linguistics (COLING 2002),pp.911-917.Strube, M. and U. Hahn.
1996.
FunctionalCentering.
Proc.
Of ACL ?96, Santa Cruz, Ca.,pp.270-277.Stuckardt, Roland.
2002.
Machine-Learning-Basedvs.
Manually Designed Approaches to AnaphorResolution: the Best of Two Worlds.
InProceedings of the 4th Discourse Anaphora andAnaphor Resolution Colloquium (DAARC2002),University of Lisbon, Portugal, pages 211-216.Giv?n, T. 1983.
Topic continuity in discourse: Anintroduction.
Topic continuity in discourse.Amsterdam/Philadelphia [Pennsylvania]: JohnBenjamins.Walker, M. A., M. Iida and S. Cote.
1994.
JapanDiscourse and the Process of Centering.Computational Linguistics, 20(2): 193-233.Walker, M. A.
1998.
Centering, anaphoraresolution, and discourse structure.
In Marilyn A.Walker, Aravind K. Joshi, and Ellen F. Prince,editors, Centering in Discourse.
OxfordUniversity Press.Yeh, Ching-Long and Chen, Yi-Chun.
2001.
Anempirical study of zero anaphora resolution inChinese based on centering theory.
InProceedings of ROCLING XIV, Tainan, Taiwan.Yeh, Ching-Long and Chen, Yi-Chun.
2003.
Zeroanaphora resolution in Chinese with partialparsing based on centering theory.
InProceedings of IEEE NLP-KE03, Beijing, China.
