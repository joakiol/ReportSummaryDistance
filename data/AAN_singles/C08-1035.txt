Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 273?280Manchester, August 2008Measuring Topic Homogeneity and itsApplication to Dictionary-based Word Sense DisambiguationAnn Gledson and John KeaneSchool of Computer Science, University of ManchesterOxford Road, Manchester, UK M13 9PL{ann.gledson,john.keane}@manchester.ac.ukAbstractThe use of topical features is abundant inNatural Language Processing (NLP), amajor example being in dictionary-basedWord Sense Disambiguation (WSD).
Yetprevious research does not attempt tomeasure the level of topic cohesion indocuments, despite assertions of its ef-fects.
This paper introduces a quantitativemeasure of Topic Homogeneity using arange of NLP resources and not requiringprior knowledge of correct senses.
Eval-uation is performed firstly by using theWordNet::Domains package to createword-sets with varying levels of homo-geneity and comparing our results withthose expected.
Additionally, to evaluateeach measure?s potential value, the ho-mogeneity results are correlated againstthose of 3 co-occurrence/dictionary-based WSD techniques, tested on 1040Semcor and SENSEVAL sub-documents.Many low-moderate correlations arefound to exist with several in the mod-erate range (above .40).
These correla-tions surpass polysemy and sense-entropy, the 2 most cited factors affectingWSD.
Finally, a combined homogeneitymeasure achieves correlations of up to.52.1 IntroductionTopical features in NLP consist of unorderedbags of words, often the context of a target wordor phrase.
In WSD for example, the word bank inthe sentence: ?If you?re OK being tied to one?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.bank, you can get al your financial productsthere.?
might be assigned its monetary sense,based on the occurrence of the term financial.Often referred to as ?topical features?, these arean important part of many NLP methods, such asWSD (Yarowsky 1995) and Topic Area Detec-tion (TAD) (Hearst 1997).
Furthermore, in theSENSEVAL WSD competitions they are in-cluded in the highest performing systems.We assert that the effectiveness of topical fea-tures in NLP depends upon the level of topichomogeneity in the text.
To illustrate two ex-tremes: the disambiguation of the word bankmight be more difficult if occurring in i) a workof fiction describing a series of activities whichincludes the phrases: ?stroll along the river?
and?pick up her cheque book?
; than in ii) a news re-port on a bank in financial difficulty (a topicallyhomogeneous text).This paper contributes a set of unsupervisedTopic Homogeneity measures requiring noknowledge of correct senses.
A variety of NLPresources are utilized and a set of evaluation me-thods devised, providing useful results.
The pa-per is structured as follows: Section 2 outlinesrelated work; Section 3 describes the experi-ments focusing on the resources used and theirassociated homogeneity measures; in Section 4three evaluation methods are described, includ-ing a WSD task-based evaluation; Conclusionsand future work are presented in Section 5.2 Related WorkTAD research (Hearst 1997) has revealed thatword patterns within texts can be used to locatetopic areas.
Salton and Allan (1994) distinguishbetween homogenous texts, where the topic ofthe text might be ascertained from a small num-ber of paragraphs, and heterogeneous texts,where topic areas change considerably.
Unfortu-nately, this research only detects topic homo-geneity at inter-paragraph level.
We assert that273the strength of relationships between the wordsof a text can vary from one text to another andthat this is likely to affect the usefulness of topicfeatures in NLP tasks.
Caracciolo et-al (2004)also indicate that documents can vary in topicalstructure, mentioning text homogeneity as animportant feature but they do not evaluate thesefindings explicitly.Lexical cohesion (Halliday and Hasan 1976) isthe analysis of the way the phrases and sentencesof a text adhere to form a unified, comprehensi-ble whole.
Morris and Hirst (1991) describe a setof relationships between word-pairs, which in-clude their likelihood to co-occur.
These are usedto create Lexical chains, which are sequences ofrelated words in a document.
The 2 main reasonsfor creating these chains are that they are ?an aidin the resolution of ambiguity?
and they deter-mine ?coherence and discourse structure?.
Wepropose the use of lexical chains, alongside othermethods, to measure document homogeneity.Measuring the semantic relatedness betweenwords is an important area in NLP, and has beenused in areas such as WSD, Lexical Chainingand Malapropism Detection.
Budanitsky & Hirst(2006) evaluate 5 such measures, based on lexi-cally assigned similarities, and conclude that anarea of future research should be the capture of?non-classical?
relationships not found in dictio-naries, such as distributional similarity.
Weedsand Weir (2006) evaluate distributional similari-ties using document retrieval from the BNC.Chen et-al (2006) and Bollegala et-al (2007) useweb search-engine results.
All of the reportedsimilarity measures are between two words (ortexts) only.
We extend these to calculate the top-ic homogeneity of groups containing up to 10words.A small number of unsupervised WSD me-thods exist which take topic areas and/or docu-ment types into account (eg Gliozzo et-al 2004,McCarthy et-al 2007), but these work at corpuslevel and do not measure the level of topical ho-mogeneity in each text.
An exception is the WSDwork by Navigli and Velardi (2005) who reportdiffering results for 3 types of text: unfocussed,mid-technical (eg finance articles) and overlytechnical (eg computer networks).3 ExperimentsWe propose the creation of a quantifiable meas-ure of text homogeneity in order to predict theeffectiveness of using co-occurrence features inWSD.
In this initial set of experiments, docu-ments are divided into simple ~50 content-wordblocks, regardless of topic boundaries, to maxim-ize the range of homogeneity levels in the textsand to nullify the effects of text length.
The long-term objective is for documents to be dividedinto topic areas at the pre-processing stage, usinga TAD algorithm.
We also propose to take a sin-gle homogeneity measure of each entire sub-text,as opposed to using a sliding window approach,as the latter would be over-complex and compa-rable with a similarity-based WSD process.3.1 Input TextsThe documents of Semcor2 and SENSEVAL (2& 3) are used in the experiments.
Each text isdivided into sub-documents of approximately 50content-words 3 .
All documents are convertedinto Semcor 2.1 format.From the resulting sub-documents, 1040 ran-dom Semcor texts and the entire set of 73 SEN-SEVAL 2 and 3 texts are used in the experiments.3.2 Text Pre-ProcessingNon-topic words are found to have a negativeeffect on NLP methods using topic features, suchas WSD (eg Yarowsky 1993, Leacock et-al1998), and are therefore excluded from our topichomogeneity experiments.
Unfortunately, noprecise definition of non-topic words exists.
Un-der ideal conditions, where correct senses areknown, we define non-topic words as word-senses appearing in over 25% of all Semcor textsand/or being marked as factotum in the WordNetDomains 3.2 package (Magnini and Cavaglia2000).As the experiments described in this work as-sume no such prior knowledge, an approximationof the criteria used above is made.
A J48(pruned) decision tree is used to decide whethereach word is non-topical.
The input attributeswere the PoS, sense-count, Semcor distribution(SenseEntropy) of its possible senses, whether allof the word?s senses are factotum, and the per-cent of Semcor documents containing that word.The training and test data was the entire set ofSemcor and SENSEVAL 2 and 3 English all-word task data and the minimum node size wasset to 4000 instances to minimize the tree sizeand prevent over-training.
Using a 10-fold cross-validation test mode the tree obtains 83% accura-2 Using the Brown-1 and Brown-2 documents only3 Splits are made as near as possible to the 50 content-word length whilst keeping sentences intact.274cy.
The learned filter for selecting non-topicalnouns and verbs is:(All-Senses = Factotum) || (Corpus-Hit-Percent >25.0%) ||((Sense_Count > 1) &&  (PoS = v)) ||((Sense_Count > 3) &&  (SenseEntropy > 0.5668))Upon entry into the system, each sub-document has all such words labeled as non-topical.
The remaining words are labeled as top-ic-content.
The confusion matrix output is shownin Table 1.Classified As ?
OTHER NON-TOPICOTHER 15017 5768NON-TOPIC 4278 34292Table 1: J48 Confusion MatrixIn addition, only nouns and verbs are consi-dered in the experiments as these word types areconsidered most likely to contain topical infor-mation.3.3 Homogeneity MeasuresFive homogeneity measures have been createdthat cover a broad range of techniques for embo-dying topic-area information in natural languagetexts.
This is to facilitate comparisons betweendifferent techniques and if such a variety of as-pects is captured, it improves the likelihood of asuccessful combination of the methods to pro-duce an optimised measure.
Each takes a full pre-processed sub-document as input and outputs asingle score.Word EntropyIt is possible to capture topic homogeneity byusing simple measures that require minimal re-liance on external resources.
Word entropy isconsidered as having the potential to reflect topi-cal cohesion.To measure WordEntropy, the frequency ofeach topic-content lemma of an input document dis obtained, and Entropy(d) is measured using thisset of frequencies, as follows:- ?i=1..n  p(xi) log2 p(xi)      [1]Where n is the number of different topic contentlemmas in d, and p(xi) is calculated asfrequency(lemmai)/?j=1..nfrequency(lemmaj)   [2]As Entropy(d) is affected by n and n varies fromone document to another, Entropy(d) is normal-ised by dividing it by the maximum possible En-tropy calculation for d, that is if all lemmas hadequal frequencies.WordNet SimilaritiesWordNet::Similarities 1.04 (Pedersen et-al 2004)is publicly available software which uses aspectsof WordNet to ?measure the semantic similarityand relatedness between a pair of concepts?.
Thepackage can measure similarities between lemmapairs, where no knowledge of the correct sense orPoS is required.
These similarities can be easilyadapted to assist with the measurement of docu-ment homogeneity, by comparing similaritiesbetween sets of word-PoS pairs in the document.Three WordNet Similarities homogeneitymeasures (AvgSimMeasure) are calculated for eachdocument as follows: Step 1: Order the topic-content lemmas of the input text firstly in des-cending order of frequency, and then by first ap-pearance in the text.
Step 2: Take the first nlemmas from this list (where n is all lemmas upto a maximum of 10) and add to FreqLemmas.Step 3: Calculate the mean of all of the similaritymeasures between each pair of lemmas in Freq-Lemmas.
AvgSim can be defined as:Mean(?i=1..n ?
j=i+1..n   SimMeasure(lemmai, lemmaj))   [3]Where SimMeasure(lemmai, lemmaj) is the Word-Net::Similarity calculation between lemmai andlemmaj, where all allowable PoS combinationsfor the two lemmas when using the selected simi-larity measure are included.The WordNet Similarity measures selected foruse are Lesk, JCN4, and Lch (see Pedersen et-al(2004) and Patwardhan et-al (2003) for details),as each measure represents one of the 3 mainalgorithm types available: WordNet Gloss over-laps, information content of the least commonsubsumer and path lengths respectively.Yahoo Internet SearchesThe web as a corpus has been successfully usedfor many areas in NLP such as WSD (Mihalceaand Moldovan 1999), obtaining frequencies forbigrams (Keller and Lapata 2003) and measuringword similarity (Bollegala et-al 2007).
Such re-liance on Web search-engine results does comewith caveats, the most important in this contextbeing that reported hit counts are not always reli-able, mostly due to the counting of duplicatedocuments.
(Kilgarriff 2007).Using web-searches as part of the homogenei-ty measure is considered important to our expe-riments, as it provides up-to-date information onword co-occurrence frequencies in the largestavailable collection of English language docu-4 TheBNC information-content file is loaded.275ments.
In addition, it is a measure that does notrely on WordNet.
It is therefore necessary toproduce a web-based homogeneity measure thatlimits the effects of inaccurate hit counts.The SearchYahoo homogeneity measure iscalculated for each document d as follows:Steps 1 and 2: Perform steps 1 and 2 describedabove (WordNet Similarities).
Step 3: Using aninternet search-engine, obtain the hit counts ofeach member of Freqlemmas.
Step 4: Order theresulting Frequlemmas list of n lemma/hit-countscombinations in descending order of hit-countsand save this list to IndivHitsDesc.
Step 5: Foreach lemma of IndivHitsDesc, save to Combi-HitsDesc preserving the ordering.
Step 6: Foreach member of CombiHitsDesc: CombiHits-Desci, obtain the hit counts of the associatedlemma, along with the concatenated lemmas ofall preceding list members of CombiHitsDesc(CombiHitsDesc0 to CombiHitsDesc[i-1]).
Thislist of lemmas are concatenated together using ?AND ?
as the delimiter.
Step 7: Calculate thegradients of the best-fit lines for the hit-counts ofIndivHitsDesc and CombiHitsDesc: creatinggradIndiv and gradCombi respectively.
Step 8:SearchYahoo is calculated for d as gradIndivminus gradCombi.As SearchYahoo is taken as the difference be-tween the two descending gradients, the measureis more likely to reveal the effects of the proba-bility of the set of lemmas co-occurring in thesame documents, rather than by influences suchas duplicate documents.
If the decline in hit-counts from IndivHitsDesc[i-1] to IndivHitsDesc[i]is high, then the decline in the number of hitsfrom CombiHitsDesc[i-1] to CombiHitsDesc[i] isalso expected to be higher, and the converse forlower drops is also expected.
Deviations fromthese expectations are reflected in the final ho-mogeneity measure and are assumed to be causedby the likelihood of lemmas co-occurring togeth-er in internet texts.A web-service enabled search-engine was re-quired to create a fully automated process.
TheGoogle search-engine hit-counts were less suita-ble, as they did not always decline as the numberof query terms increased.
This is perhaps becauseof the way in which Google combines the resultsof several search-engine hubs.
The Yahoo web-services were therefore selected, as these pro-duced the necessary declines for the measure towork.Further evaluation of the Yahoo Internetsearching homogeneity measure is presented inGledson and Keane (2008), along with compari-sons with similar methods using the Google andWindows LiveSearch web-services.WordNet DomainsMagnini et-al (2002) describe the WordNet Do-mains5 (Magnini and Cavaglia 2000) package as:?an extension of WordNet in which synsetshave been annotated with one or more do-main labels, selected from a hierarchicallyorganized set of about two hundred labels?.
(Magnini et-al 2002 p.361)They describe a domain (eg ?Politics?)
as ?a setof words between which there are strong seman-tic relations?.
This resource is useful for measur-ing topic homogeneity, as it stores topic area in-formation for word-senses directly, and comple-ments the other measures, thus contributing to adiverse set of measures.Two WordNet Domains homogeneity meas-ures are calculated: DomEntropy and Dom-Top3Percent.
These are calculated for each inputdocument d as follows:Step 1: Add each topic-content lemma of d tothe list TopicContents.
Step 2: for each WordNetsense of each topic-content lemma in TopicCon-tents, find all associated domains using the Do-mains package, and add these to a Domain-Counts list.
This list contains each distinct do-main dom present in the document, each with itsassociated count of the number of times it occursin TopicContents: freq(domi).
Step 3: CalculateDomEntropy using the equation [1] above, wheren is the number of items in DomainCounts, andp(xi) = freq(domi) / ?j=1..n freq(domj)  [4]Step 4: Calculate DomTop3Percent as follows:100 (?i=1..3 freq(domi) / ?j=1..n freq(domj)) [5]Lexical Chaining?Lexical chains are defined as clusters of seman-tically related words?
(Doran et-al 2004).
Thesewords are usually related by electronic dictiona-ries such as WordNet or Roget?s Thesaurus, andchains are created using a natural language textas input.The lexical chaining method used in our expe-riments is a greedy version of the algorithm de-scribed in Ecran and Cicekli (2007).
Their me-thod uses the WordNet dictionary and calculatesall possible chains in the text.
We adopt a greedyapproach to chaining, as it is only necessary toget an overall estimate of the levels of topic ho-mogeneity within the text, rather than producing5 We use version 3.2 released Feb 2007276lists of keywords or document summaries.
TheLexChain homogeneity measure is calculated forthe input document d as follows:Step 1: Add each topic-content noun occur-rence in d to the list UnusedNouns.
Step 2: Foreach item in UnusedNouns, find all other items inUnusedNouns that it is related to and add them toits corresponding RelatedNouns list.
Each item ofRelatedNouns is mapped to a score (relScore)using the following system (Ercan and Cicekli2007): 10 points are awarded if the word-senseshave identical lemmas or belong to the sameWordNet 2.1 synset.
7 points are awarded if it isa hyponymy relationship and 4 points areawarded if it is a holonymy relationship.
Step 3:Create chains: Iterate through UnusedNouns re-cursively, adding all related senses to the firstchain, until no further linked nouns can be found.As each new node (UnusedNouns item) is addedto the chain, remove it from UnusedNouns.
Con-tinue creating further chains, until no more re-lated nouns can be found.
Step 4: Calculate thechainScore of each chain by adding together allof the relScores contained for each sense, at eachnode.
Step 5: Set LexChain as the ChainScore ofthe highest scoring chain.3.4 Adjusting for Polysemy and SkewEach of the homogeneity measures (except Wor-dEntropy) has the potential to be affected by theaverage polysemy and sense skews of the docu-ment.
The effects are measured statistically usinglinear regression and the resulting line of best fitequation is used to reverse them.To calculate the adjustments for each measure,the effects of polysemy and skew must be ap-proximated.
This is achieved by applying linearregression6 over the entire result set.
The homo-geneity measure is entered as the dependant vari-able and the appropriate7 average polysemy andskew measures (per doc) are input as indepen-dent variables.
If the homogeneity measure isaffected by either (or both) of the independentvariables and the effect is statistically significant,a line of best fit equation is output representingthe gradient of the effect caused by those varia-ble(s).
The appropriate homogeneity measure foreach input document is adjusted by subtractingthe co-efficient of the gradient multiplied by theappropriate variable(s): polysemy and/or skew.6 Using SPSS 13.0 statistical software package.7 Avg.
document polysemy/skews are only calculatedfor lemmas incl.
in homogeneity measures.4 Evaluation and DiscussionIt is anticipated that the main users of a set oftopic homogeneity measures are other NLP tech-niques.
They are, therefore, best measured interms of the actual results of the processes theyare intended to improve.
Human judgments canbe subjective (Doran et-al 2004) and are there-fore deemed inappropriate for the evaluation ofthis task.Three methods are used to evaluate the homo-geneity measures.
Firstly, each measure is com-pared with its equivalent where only correctsenses are used.
Secondly, the Word-Net::Domains (version 3.2) hierarchy (Magniniand Cavaglia 2000) is used to generate sets ofwords with varying levels of topic homogeneity.Each set is then tested using the proposed meas-ures, and the results compared with those ex-pected.
Finally, the usefulness of each measure istested by evaluating their ability to indicate thelikely outcome of several co-occurrence/dictionary-based WSD measures.
Inthe WSD literature, the main non-topic relatedvariables reported as affecting WSD results arepolysemy and skew, so these two measures willbe used as the baselines.4.1 All Senses vs.
Correct SensesTable 2 show the set of measures correlatedagainst their correct-sense complements, whereonly correct senses are utilized.
Most of the re-sults are in the moderate range (.40-.60) withAvgSimLESK and LexChains achieving correla-tions in the high and very-high ranges.
Only theSENSEVAL DomEntropy result falls below themoderate level, indicating that homogeneity in-formation is, in general, not negated by incorrect-sense noise.Measure Correlation(Pearson?s R)Semcor SENSEVALAvgSimLESK 0.814** 0.834**AvgSimJCN 0.491** 0.610**AvgSimLCH 0.569** 0.474**DomEntropy 0.575** 0.371**DomTop3% 0.535** 0.444**LexChain 0.659** 0.967****Significant at the .01 level (2-tailed)Table 2: Correlation with correct-sense equiva-lents.4.2 WordNet::DomainsThe WordNet Domains package (Magnini andCavaglia 2000) assigns domains to each sense of277the WordNet electronic dictionary.
Therefore, foreach domain a relevant list of words can be ex-tracted.
The domains are arranged hierarchically,allowing sets of words with a varied degree oftopic homogeneity to be selected.
For example,for a highly heterogeneous set, 10 words can beselected from any domain, including factotum(level-0: the non-topic related category).
For aslightly less heterogeneous set, words might beselected randomly from a level-1 category (eg?Applied_Science?
), and any of the categories itsubsumes (eg Agriculture, Architecture, Build-ings etc).
The levels range from level-0 (facto-tum) to level-4; we merge levels 3 and 4 as level-4 domains are relatively few and are viewed assimilar to level-3.
This combined set is hence-forth known as level-3.For our experiments, we have collected 2 ran-dom samples of 10 words for every WordNetdomain (167 domains) and then increased thenumber of sets from level-0 to level-2 domains,to make the number of sets from each level moresimilar.
The final level counts are: levels 0 to 2have 100 word-sets each and level 3 has 192word-sets.
The sets contain 10 words each.
Wethen assign an expected score to each set, equalto its domain level.
**Significant at the .01 level (2-tailed)*Significant at the .05 level (2-tailed)Table 3: Correlation with expected scores forWordNet::Domains selected setsThe first column of results on Table 3 repre-sents the correlations with expected results for all492 word-sets.
The high WordNet::Domains re-sults (DomEntropy and DomTop3%) probablyreflect the fact that they are produced using thesame resource as the creation of the test sets.
Onthe other hand, knowledge of correct senses isnot required for the homogeneity measures andthese scores indicate that they are capable none-theless of capturing topic homogeneity.
TheSearchYahoo, AvgSimsJCN and LexChain me-thods all produce promising results with correla-tions in the moderate range (0.40 to 0.59) andagain indicate that they can capture topic homo-geneity.To indicate whether the measures are morecapable of distinguishing between extreme levelsof homogeneity, we repeated the above tests, butincluded only those sets of level-0 and level-3.The results displayed in the final column of Ta-ble 3 and provide evidence that this might be thecase for the WordNet::Domains measures andSearchYahoo, as the correlations are significantlyhigher for these more extreme test sets.4.3 Dictionary-based WSDThe three WSD algorithms selected for evalua-tion are the technique described in Mihalcea andMoldovan (2000) and two WordNet Similaritiesmeasures: Lesk and JCN, adapted for WSD asdescribed in Patwardhan et-al (2004), in whichthey are found to achieve the best performances.Each WSD technique uses the entire document asthe context for each target word.
The method ofMihalcea and Moldovan (2000) is included as itincorporates several techniques, all complemen-tary to our overall set of evaluation methods.
Forour experiments it is split into three: Mih-ALL:covering all 8 procedures, including one that re-lies on co-location information; Mih-4: utilising?procedure-4?, which involves the use of nounco-occurrence and WordNet hyponymy data; andMih-5-8: using procedures 5 to 8, which involvessynonymy and hyponymy.
The results are ad-justed to remove the effects of polysemy andSenseEntropy (section 3.4).In Table 4, the fine-grained WSD accuracy re-sults (for topic-content words) are compared tothose of the homogeneity measures, (includingthe correct-sense measures which set the high-standard benchmark).
As a baseline, non-adjusted WSD accuracies are compared with theaverage polysemy and average Sense Entropy ofeach document.All of the ?all-senses?
results, except Domai-nEntropy, are statistically significant and achieveat least low-moderate correlations with one ormore of the WSD measures.
All of the measuresoutperform the baseline correlations for most ofthe WSD algorithms displayed.A COMBINED measure is calculated for theall-sense and the correct-sense sets of measuresrespectively.
The measures included (based ontheir individual performances and maintainingmaximum diversity) are AvgSimsJCN, WordEn-tropy, DomTop3Percent, LexChain and Sear-chYahoo.
Each of these result sets are ordered byhomogeneity score (most homogeneousMeasure Correlation(Pearson?s R)All ExtremeSearchYahoo 0.46** 0.80**AvgSimLESK 0.23** 0.15*AvgSimJCN 0.47** 0.45**AvgSimLCH 0.35** 0.25**DomEntropy 0.70** 0.75**DomTop3% 0.75** 0.83**LexChain 0.42** 0.40**278first) and banded into 5 groups making 4 cut-points at equal percentiles and numbering themfrom 5 down to 1 respectively.
The combinedmeasure for each document is the sum of all suchscores.
These measures often outperform all ofthe individual methods and achieve correlationsof up to .52 in Semcor, the largest of the datasets.5 Conclusions and Further WorkThis paper presents a first attempt to measureTopic Homogeneity using a variety of NLP re-sources.
A set of 5 unsupervised homogeneitymeasures are presented that require no priorknowledge of correct senses and which exhibitmoderate to high degrees of correlation with theircorrect-sense-only equivalents.
When used tomeasure word-sets created using the Word-Net::Domains package and which have varyinglevels of homogeneity, they are found to corre-late well with expected results, further supportingour conjecture that they represent topical homo-geneity information.
Finally, when comparedwith WSD topic-content word accuracies, theeffect of topic homogeneity is shown to surpassthat of polysemy and sense-entropy, which havebeen recognized previously as having an influ-ence on such results.
By combining these meas-ures, correlations are improved further, oftenoutperforming the individual methods andachieving up to .52 for over 1000 random Sem-cor sub-documents, again indicating their poten-tial importance.
Correlations in SENSEVAL areoften higher, but due to the lower number ofdocuments, it is more difficult to obtain statisti-cally significant results.Our results provide evidence that improve-ments could be made to WSD and other NLPmethods which utilise topic features, by adaptingthe algorithms used depending on the level oftopic cohesion of the input text.
For example,window-sizes for obtaining contextual data mightbe expanded or reduced, based on the homogene-ity level of the target text.
Furthermore, non-topical features such as collocation and grammat-ical cues might be given more emphasis whendisambiguating heterogeneous documents.Further work includes testing the measures onother NLP tasks.
A machine learning approachmight also be used to further optimize the com-bination of homogeneity measures.
Finally, it isintended that our approach should eventually becombined with a TAD method to improve WSDresults.ReferencesBollegala, Danushka, Yutaka Matuo and MitsuruIshizuka, 2007.
Measuring Semantic Similarity be-tween Words Using Web Search Engines.
In ProcsWorld Wide Web Conference, Banff, Alberta.Caracciolo, Caterina, Willem van Hage and Maartende Rijke, 2004.
Towards Topic Driven Access toFull Text Documents, in Research and AdvancedSENSEVAL 2 & 3 SEMCORMihALLMih4Mih5-8LESK JCN MihALLMih4Mih5-8LESK JCNALL SensesWord Entropy    -.325 -.301 -.419 -.442 -.206  -.286AvgSimLESK    .500  .138 .132 .211 .121 .097AvgSimJCN .286 .276 .357 .233  .255 .231 .284  .081AvgSimLCH .271 .211 .224 .202  .254 .248 .285  .104DomEntropy -.193 -.205    -.163 -.157   -.091DomTop3% .308 .281    .224 .215 .145  .108LexChain    .344  .328 .345 .296  .095SearchYahoo -.232 -.290 -.317 -.295  -.105 -.116  -.085 -.089COMBINED .382 .421 .270 .243 .171 .521 .517 .256  .350CORRECTSensesAvgSimLESK .461 .467  .630 .189 .186 .174 .237 .171 .115AvgSimJCN .257  .272 .357 .325 .218 .162 .234  .210AvgSimLCH .310  .483 .187  .198 .181 .247  .122DomEntropy -.340 -.295    -.334 -.335 -.106  -.270DomTop3% .376 .353   .214 .398 .393 .144  .316LexChain    .372 .297 .426 .440 .164 .112 .364COMBINED .398 .363  .398 .379 .519 .494 .294  .434Baseline AvgPolysemy -.100 -.030 -.234 (.252) (.113) (.146) (.169) -.004 .032 -.144 AvgSenseEntropy .031 -.044 .143 -.307 -.019 -.073 -.064 -.083 .041 -.141Results in bold: significant at the 0.01 level (2-tailed); Results in non-bold: significant at the 0.05 level (2-tailed)Italicised results: not significant at the 0.05 level but considered to be of interestBracketed results: inverse to expectationsAll WSD results are adjusted for polysemy / SenseEntropy, with the exception of where compared with baselines.Table 4: Topic-content word WSD accuracy vs. Homogeneity: Correlations279Technology for Digital Libraries, LNCS, 3232, pp495-500Chen, Hsin-Hsi, Ming-Shun Lin and Yu-Chuan Wei,2006.
Novel Association Measures Using WebSearch with Double Checking.
In Proc.s 21st Intl.Conference on Computational Linguistics and 44thAnnual Meeting of the ACL, pp.
1009-1016Doran, William, Nicola Stokes, Joe Carthy and JohnDunnion, 2004.
Assessing the Impact of LexicalChain Scoring Methods and Sentence ExtractionSchemes on Summarization, LNCS 2945, pp 627-635, CICLing 2004Ercan, Gonenc and Ilyas Cicekli, 2007.
Using LexicalChains for Keyword extraction, InformationProcessing and Management, 43(2007), pp 1705-1714.Gledson, Ann and John Keane, 2008.
Using web-search results to measure word-group similarity.
InProcs 22nd Intl Conference on Computational Lin-guistics (COLING), Manchester.
(To Appear)Gliozzo, Alfio, Carlo Strapparava and Ido Dagan,2004.
Unsupervised and Supervised Exploitation ofSemantic Domains in Lexical Disambiguation,Computer Speech and LanguageHalliday, Michael and Ruqaiya Hasan, 1976.
Cohe-sion in English, Longman GroupHearst , Marti, 1997.
Text Tiling: segmenting text intomulti-paragraph subtopic passages, ComputationalLinguistics, 23(1), pp 33-64Keller, Frank and Mirella Lapata, 2003.
Using theWeb to Obtain Frequencies for Unseen Bigrams,Computational Linguistics, 29(3)Kilgarriff, Adam, 2007.
Googleology is bad science,Computational Linguistics, 33(1), pp 147-151Leacock, Claudia, Martin Choderow and George A.Miller, 1998.
Using Corpus Statistics and WordnetRelations for Sense Identification, ComputationalLinguistics, 24(1), pp 147-165McCarthy, Diana, Rob Koeling, Julie Weeds and JohnCarroll, 2007.
Unsupervised Acquisition of Pre-dominant Word Senses, Computational Linguistics,33(4), pp.
553-590.Magnini, Bernardo and Gabriela Cavagli?, 2000.Integrating Subject Field Codes into WordNet.In Procs LREC-2000, Athens, Greece, 2000, pp1413-1418.Magnini, Bernardo, Carlo Strapparava, Giovanni Pez-zulo and Alfio Gliozzo, 2002.
The Role of DomainInformation in Word Sense Disambiguation.
Natu-ral Language Engineering, 8(4), pp 359-373Mihalcea, Rada and Dan Moldovan, 1999.
A methodfor word sense disambiguation of unrestricted txt.In Procs.
37th Meeting of ACL, pp 152-158Mihalcea, Rada and Dan Moldovan, 2000.
An Itera-tive Approach to Word Sense Disambiguation, InProc.
FLAIRS 2000, pp.
219-223, OrlandoMorris, Jane and Graeme Hirst, 1991.
Lexical Cohe-sion Computed by Thesaural Relations as an Indi-cator of the Structure of Text, Computational Lin-guistics, 17(1), pp.
21-48.Navigli, Roberto, Paola Velardi, 2005.
Structural Se-mantic Interconnections: A Knowledge-Based Ap-proach to Word Sense Disambiguation, IEEETransactions on Pattern Analysis and Machine In-telligence, 27(7),  pp.
1075-1086,  July.Patwardhan, Siddharth, Satanjeev Banerjee and TedPedersen, 2003.
Using Measures of Semantic Rela-tedness for Word Sense Disambiguation, LNCS2588, pp 241-257, CICLing 2003Pedersen, Ted, Siddharth Patwardhan and Jason Mi-chelizzi, 2004.
WordNet::Similarity ?
Measuringthe Relatedness of Concepts, In Procs 19th Nation-al Conference on Artificial Intelligence.Salton, Gerard and James Allan, 1994.
Automatic textdecomposition and structuring, In Procs.
RIAO In-ternational Conference, New York.Weeds, Julie and, David Weir, 2006, Co-occurrenceRetreival: A Flexible Framework for Lexical Dis-tributional Similarity.
Computational Linguistics,31(4), pp 433-475.Yarowsky, David, (1995), Unsupervised word sensedisambiguation rivaling supervised methods.
InProcs 33rd Annual Meeting of the Association forComputational Linguistics (ACL) 1995, pp 189-196280
