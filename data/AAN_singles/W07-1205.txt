Proceedings of the 5th Workshop on Important Unresolved Matters, pages 33?40,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDeep Grammars in a Tree Labeling Approach toSyntax-based Statistical Machine TranslationMark HopkinsDepartment of LinguisticsUniversity of Potsdam, Germanyhopkins@ling.uni-potsdam.deJonas KuhnDepartment of LinguisticsUniversity of Potsdam, Germanykuhn@ling.uni-potsdam.deAbstractIn this paper, we propose a new syntax-based machine translation (MT) approachbased on reducing the MT task to a tree-labeling task, which is further decom-posed into a sequence of simple decisionsfor which discriminative classifiers can betrained.
The approach is very flexible andwe believe that it is particularly well-suitedfor exploiting the linguistic knowledge en-coded in deep grammars whenever possi-ble, while at the same time taking advantageof data-based techniques that have proven apowerful basis for MT, as recent advances instatistical MT show.A full system using the Lexical-FunctionalGrammar (LFG) parsing system XLE andthe grammars from the Parallel Grammardevelopment project (ParGram; (Butt etal., 2002)) has been implemented, and wepresent preliminary results on English-to-German translation with a tree-labeling sys-tem trained on a small subsection of the Eu-roparl corpus.1 MotivationMachine translation (MT) is probably the oldest ap-plication of what we call deep linguistic processingtechniques today.
But from its inception, there havebeen alternative considerations of approaching thetask with data-based statistical techniques (cf.
War-ren Weaver?s well-known memo from 1949).
Onlywith fairly recent advances in computer technologyhave researchers been able to build effective statis-tical MT prototypes, but in the last few years, thestatistical approach has received enormous researchinterest and made significant progress.The most successful statistical MT paradigm hasbeen, for a while now, the so-call phrase-based MTapproach (Och and Ney, 2003).
In this paradigm,sentences are translated from a source language toa target language through the repeated substitutionof contiguous word sequences (?phrases?)
from thesource language for word sequences in the targetlanguage.
Training of the phrase translation modelbuilds on top of a standard statistical word alignmentover the training corpus of parallel text (Brown et al,1993) for identifying corresponding word blocks,assuming no further linguistic analysis of the sourceor target language.
In decoding, i.e.
the applicationof the acquired translation model to unseen sourcesentences, these systems then typically rely on n-gram language models and simple statistical reorder-ing models to shuffle the phrases into an order thatis coherent in the target language.An obvious advantage of statistical MT ap-proaches is that they can adopt (often very id-iomatic) translations of mid- to high-frequency con-structions without requiring any language-pair spe-cific engineering work.
At the same time it is clearthat a linguistics-free approach is limited in whatit can ultimately achieve: only linguistically in-formed systems can detect certain generalizationsfrom lower-frequency constructions in the data andsuccessfully apply them in a similar but different lin-guistic context.
Hence, the idea of ?hybrid?
MT, ex-ploiting both linguistic and statistical information isfairly old.
Here we will not consider classical, rule-based systems with some added data-based resourceacquisition (although they may be among the bestcandidates for high-quality special-purpose transla-tion ?
but adaption to new language pairs and sub-languages is very costly for these systems).
Theother form of hybridization ?
a statistical MT modelthat is based on a deeper analysis of the syntactic33structure of a sentence ?
has also long been iden-tified as a desirable objective in principle (consider(Wu, 1997; Yamada and Knight, 2001)).
However,attempts to retrofit syntactic information into thephrase-based paradigm have not met with enormoussuccess (Koehn et al, 2003; Och et al, 2003)1, andpurely phrase-based MT systems continue to outper-form these syntax/phrase-based hybrids.In this work, we try to make a fresh start withsyntax-based statistical MT, discarding the phrase-based paradigm and designing a MT system fromthe ground up, using syntax as our central guid-ing star ?
besides the word alignment over a par-allel corpus.
Our approach is compatible with andcan benefit substantially from rich linguistic rep-resentations obtained from deep grammars like theParGram LFGs.
Nevertheless, contrary to classi-cal interlingual or deep transfer-based systems, thegenerative stochastic model that drives our systemis grounded only in the cross-language word align-ment and a surface-based phrase structure tree forthe source language and will thus degrade grace-fully on input with parsing issues ?
which we sus-pect is an important feature for making the overallsystem competitive with the highly general phrase-based MT approach.Preliminary evaluation of our nascent system in-dicates that this new approach might well have thepotential to finally realize some of the promises ofsyntax in statistical MT.2 General TaskWe want to build a system that can learn to translatesentences from a source language to a destinationlanguage.
The general set-up is simple.Firstly, we have a training corpus of paired sen-tences f and e, where target sentence e is a goldstandard translation of source sentence f .
Thesesentence pairs are annotated with auxiliary informa-tion, which can include word alignments and syntac-tic information.
We refer to these annotated sentencepairs as complete translation objects.Secondly, we have an evaluation corpus of sourcesentences.
These sentences are annotated with a sub-set of the auxiliary information used to annotate the1(Chiang, 2005) also reports that with his hierarchical gen-eralization of the phrase-based approach, the addition of parserinformation doesn?t lead to any improvements.Figure 1: Example translation object.training corpus.
We refer to these partially annotatedsource sentences as partial translation objects.The task at hand: use the training corpus to learna procedure, through which we can successfully in-duce a complete translation object from a partialtranslation object.
This is what we will define astranslation.3 Specific Task Addressed by this PaperBefore going on to actually describe a translationprocedure (and how to induce it), we need to spec-ify our prior assumptions about how the translationobjects will be annotated.
For this paper, we want toexploit the syntax information that we can gain froman LFG-parser, hence we will assume the followingannotations:(1) In the training and evaluation corpora, thesource sentences will be parsed with the XLE-parser.
The attribute-value information from LFG?sf-structure is restructured so it is indexed by (c-structure) tree nodes; thus a tree node can bear mul-tiple labels for various pieces of morphological, syn-tactic and semantic information.
(2) In the training corpus, the source and targetsentence of every translation object will be alignedusing GIZA++ (http://www.fjoch.com/).In other words, our complete translation objectswill be aligned tree-string pairs (for instance, Fig-ure 1), while our partial translation objects will betrees (the tree portion of Figure 1).
No other annota-tions will be assumed for this paper.34Figure 2: GHKM tree equivalent of example translation object.
The light gray nodes are rule nodes of theGHKM tree.4 Syntax MT as Tree LabelingIt is not immediately clear how one would learn aprocess to map a parsed source sentence into analigned tree-string pair.
To facilitate matters, wewill map complete translation objects to an alternaterepresentation.
In (Galley et al, 2003), the authorsgive a semantics to aligned tree-string pairs by asso-ciating each with an annotated parse tree (hereaftercalled a GHKM tree) representing a specific theoryabout how the source sentence was translated intothe destination sentence.In Figure 1, we show an example translation ob-ject and in Figure 2, we show its associated GHKMtree.
The GHKM tree is simply the parse tree f ofthe translation object, annotated with rules (hereafterreferred to as GHKM rules).
We will not describe indepth the mapping process from translation object toGHKM tree.
Suffice it to say that the alignment in-duces a set of intuitive translation rules.
Essentially,a rule like: ?not 1 ?
ne 1 pas?
(see Figure 2) means:if we see the word ?not?
in English, followed by aphrase already translated into French, then translatethe entire thing as the word ?ne?
+ the translatedphrase + the word ?pas.?
A parse tree node gets la-beled with one of these rules if, roughly speaking,its span is still contiguous when projected (via thealignment) into the target language.The advantage of using the GHKM interpretationof a complete translation object is that our transla-tion task becomes simpler.
Now, to induce a com-plete translation object from a partial translation ob-ject (parse tree), all we need to do is label the nodesof the tree with appropriate rules.
We have reducedthe vaguely defined task of translation to the con-crete task of tree labeling.5 The Generative ProcessAt the most basic level, we could design a naive gen-erative process that takes a parse tree and then makesa series of decisions, one for each node, about whatrule (if any) that node should be assigned.
How-ever it is a bit unreasonable to expect to learn sucha decision without breaking it down somewhat, asthere are an enormous number of rules that could po-tentially be used to label any given parse tree node.So let?s break this task down into simpler decisions.Ideally, we would like to devise a generative processconsisting of decisions between a small number ofpossibilities (ideally binary decisions).We will begin by deciding, for each node, whetheror not it will be annotated with a rule.
This is clearlya binary decision.
Once a generative process hasmade this decision for each node, we get a conve-nient byproduct.
As seen in Figure 3, the LHS ofeach rule is already determined.
Hence after this se-quence of binary decisions, half of our task is al-ready completed.The question remains: how do we determine theRHS of these rules?
Again, we could create a gen-erative process that makes these decisions directly,but choosing the RHS of a rule is still a rather wide-open decision, so we will break it down further.
Foreach rule, we will begin by choosing the template ofits RHS, which is a RHS in which all sequences ofvariables are replaced with an empty slot into whichvariables can later be placed.
For instance, the tem-35Figure 3: Partial GHKM tree, after rule nodes have been identified (light gray).
Notice that once we identifythe rule node, the rule left-hand sides are already determined.plate of ?
?ne?, x1, ?pas??
is ?
?ne?,X, ?pas??
and thetemplate of ?x3, ?,?, x1, x2?
is ?X, ?,?,X?, where Xrepresents the empty slots.Once the template is chosen, it simply needs to befilled with the variables from the LHS.
To do so, weprocess the LHS variables, one by one.
By default,they are placed to the right of the previously placedvariable (the first variable is placed in the first slot).We repeatedly offer the option to push the variableto the right until the option is declined or it is nolonger possible to push it further right.
If the vari-able was not pushed right at all, we repeatedly offerthe option to push the variable to the left until theoption is declined or it is no longer possible to pushit further left.
Figure 4 shows this generative storyin action for the rule RHS ?x3, ?,?, x1, x2?.These are all of the decisions we need to makein order to label a parse tree with GHKM rules.
Atrace of this generative process for the GHKM treeof Figure 2 is shown in Figure 5.
Notice that, asidefrom the template decisions, all of the decisions arebinary (i.e.
feasible to learn discriminatively).
Eventhe template decisions are not terribly large-domain,if we maintain a separate feature-conditional dis-tribution for each LHS template.
For instance, ifthe LHS template is ?
?not?,X?, then RHS template?
?ne?,X, ?pas??
and a few other select candidatesshould bear most of the probability mass.5.1 TrainingHaving established this generative story, training isstraightforward.
As a first step, we can convert eachcomplete translation object of our training corpusto the trace of its generative story (as in Figure 5).Decision to make Decision RHS so farRHS template?
X , X X , Xdefault placement of var 1 1 , Xpush var 1 right?
yes X , 1default placement of var 2 X , 1 2push var 2 left?
no X , 1 2default placement of var 3 X , 1 2 3push var 3 left?
yes X , 1 3 2push var 3 left?
yes X , 3 1 2push var 3 left?
yes 3 , 1 2Figure 4: Trace of the generative story for the right-hand side of a GHKM rule.These decisions can be annotated with whatever fea-ture information we might deem helpful.
Then wesimply divide up these feature vectors by decisiontype (for instance, rule node decisions, template de-cisions, etc.)
and train a separate discriminative clas-sifier for each decision type from the feature vectors.This method is quite flexible, in that it allows us touse any generic off-the-shelf classification softwareto train our system.
We prefer learners that producedistributions (rather than hard classifiers) as output,but this is not required.5.2 Exploiting deep linguistic informationThe use of discriminative classifiers makes our ap-proach very flexible in terms of the information thatcan be exploited in the labeling (or translation) pro-cess.
Any information that can be encoded as fea-tures relative to GHKM tree nodes can be used.
Forthe experiments reported in this paper, we parsedthe source language side of a parallel corpus (asmall subsection of the English-German Europarlcorpus; (Koehn, 2002)) with the XLE system, using36the ParGram LFG grammar and applying probabilis-tic disambiguation (Riezler et al, 2002) to obtaina single analysis (i.e., a c-structure [phrase struc-ture tree] and an f-structure [an associated attribute-value matrix with morphosyntactic feature informa-tion and a shallow semantic interpretation]) for eachsentence.
A fall-back mechanism integrated in theparser/grammar ensures that even for sentences thatdo not receive a full parse, substrings are deeplyparsed and can often be treated successfully.We convert the c-structure/f-structure represen-tation that is based on XLE?s sophisticated word-internal analysis into a plain phrase structure treerepresentation based on the original tokens in thesource language string.
The morphosyntactic fea-ture information from f-structure is copied as addi-tional labeling information to the relevant GHKMtree nodes, and the f-structural dependency relationamong linguistic units is translated into a relationamong corresponding GHKM tree nodes.
The rela-tional information is then used to systematically ex-tend the learning feature set for the tree-node basedclassifiers.In future experiments, we also plan to exploit lin-guistic knowledge about the target language by fac-torizing the generation of target language words intoseparate generation of lemmas and the various mor-phosyntactic features.
In decoding, a morphologicalgenerator will be used to generate a string of surfacewords.5.3 DecodingBecause we have purposely refused to make anyMarkov assumptions in our model, decoding cannotbe accomplished in polynomial time.
Our hypothe-sis is that it is better to find a suboptimal solution ofa high-quality model than the optimal solution of apoorer model.
We decode through a simple searchthrough the space of assignments to our generativeprocess.This is, potentially, a very large and intractiblesearch space.
However, if most assignment deci-sions can be made with relative confidence (i.e.
theclassifiers we have trained make fairly certain deci-sions), then the great majority of search nodes havevalues which are inferior to those of the best so-lutions.
The standard search technique of depth-first branch-and-bound search takes advantage ofsearch spaces with this particular characteristic byfirst finding greedy good-quality solutions and usingtheir values to optimally prune a significant portionof the search space.
Depth-first branch-and-boundsearch has the following advantage: it finds a good(suboptimal) solution in linear time and continuallyimproves on this solution until it finds the optimal.Thus it can be run either as an optimal decoder or asa heuristic decoder, since we can interrupt its execu-tion at any time to get the best solution found so far.Additionally, it takes only linear space to run.6 Preliminary resultsIn this section, we present some preliminary resultsfor an English-to-German translation system basedon the ideas outlined in this paper.Our data was a subset of the Europarl corpusconsisting of sentences of lengths ranging from 8to 17 words.
Our training corpus contained 50000sentences and our test corpus contained 300 sen-tences.
We also had a small number of reservedsentences for development.
The English sentenceswere parsed with XLE, using the English ParGramLFG grammar, and the sentences were word-alignedwith GIZA++.
We used the WEKA machine learn-ing package (Witten and Frank, 2005) to train thedistributions (specifically, we used model trees).For comparison, we also trained and evaluatedthe phrase-based MT system Pharaoh (Koehn, 2005)on this limited corpus, using Pharaoh?s default pa-rameters.
In a different set of MT-as-Tree-Labelingexperiments, we used a standard treebank parsertrained on the PennTreebank Wall Street Journalsection.
Even with this parser, which produces lessdetailed information than XLE, the results are com-petitive when assessed with quantitative measures:Pharaoh achieved a BLEU score of 11.17 on the testset, whereas our system achieved a BLEU score of11.52.
What is notable here is not the scores them-selves (low due to the size of the training corpus).However our system managed to perform compara-bly with Pharaoh in a very early stage of its devel-opment, with rudimentary features and without thebenefit of an n-gram language model.For the XLE-based system we cannot includequantitative results for the same experimental setupat the present time.
As a preliminary qualitative37Decision to make Decision Active featuresrule node (i)?
YES NT=?S?
; HEAD = ?am?rule node (ii)?
YES NT=?NP?
; HEAD = ?I?rule node (iv)?
NO NT=?VP?
; HEAD = ?am?rule node (v)?
YES NT=?VP?
; HEAD = ?am?rule node (vi)?
NO NT=?MD?
; HEAD = ?am?rule node (viii)?
YES NT=?VP?
; HEAD = ?going?rule node (ix)?
NO NT=?RB?
; HEAD = ?not?rule node (xi)?
YES NT=?VB?
; HEAD = ?going?rule node (xiii)?
YES NT=?ADJP?
; HEAD = ?today?RHS template?
(i) X , X NT=?S?push var 1 right?
(i) YES VARNT=?NP?
; PUSHPAST= ?,?push var 2 left?
(i) NO VARNT=?VP?
; PUSHPAST= ?NP?push var 3 left?
(i) YES VARNT=?ADJP?
; PUSHPAST= ?VP?push var 3 left?
(i) YES VARNT=?ADJP?
; PUSHPAST= ?NP?push var 3 left?
(i) YES VARNT=?ADJP?
; PUSHPAST= ?,?RHS template?
(ii) je NT=?NP?
; WD=?I?RHS template?
(v) X NT=?VP?RHS template?
(viii) ne X pas NT=?VP?
; WD=?not?RHS template?
(xi) vais NT=?VB?
; WD=?going?RHS template?
(xiii) aujourd?hui NT=?ADJP?
; WD=?today?Figure 5: Trace of a top-down generative story for the GHKM tree in Figure 2.evaluation, let?s take a closer look at the sentencesproduced by our system, to gain some insight as toits current strengths and weaknesses.Starting with the English sentence (1) (note thatall data is lowercase), our system produces (2).
(1) i agree with the spirit of those amendments .
(2) ichIstimmevotediethe.FEMgeistspirit.MASCdieserthesea?nderungsantra?gechange-proposalszuto..The GHKM tree is depicted in Figure 6.
The keyfeature of this translation is how the English phrase?agree with?
is translated as the German ?stimme... zu?
construction.
Such a feat is difficult to pro-duce consistently with a purely phrase-based sys-tem, as phrases of arbitrary length can be placed be-tween the words ?stimme?
and ?zu?, as we can seehappening in this particular example.
By contrast,Pharaoh opts for the following (somewhat less de-sirable) translation:(3) ichIstimmevotemitwithdemthe.MASCgeistspirit.MASCdieserthesea?nderungsantra?gechange-proposals..A weakness in our system is also evident here.The German noun ?Geist?
is masculine, thus oursystem uses the wrong article (a problem thatPharaoh, with its embedded n-gram language model,does not encounter).In general, it seems that our system is superior toPharaoh at figuring out the proper way to arrange thewords of the output sentence, and inferior to Pharaohat finding what the actual translation of those wordsshould be.Consider the English sentence (4).
Here we havean example of a modal verb with an embedded in-finitival VP.
In German, infinite verbs should go atthe end of the sentence, and this is achieved by oursystem (translating ?shall?
as ?werden?, and ?sub-mit?
as ?vorlegen?
), as is seen in (5).
(4) ) we shall submit a proposal along these lines before theend of this year .
(5) wirwewerdenwilleinea.FEMvorschlagproposal.MASCinindieserthesehaushaltslinienbudget-linesvorbeforediethe.FEMendeend.NEUTdieserthis.FEMjahresyear.NEUTvorlegensubmit..Pharaoh does not manage this (translating ?sub-mit?
as ?unterbreiten?
and placing it mid-sentence).
(6) werdenwillwirweunterbreitensubmiteineavorschlagproposalinindieserthesehaushaltslinienbudget-linesvorbeforeendeenddieserthis.FEMjahryear.NEUT..It is worth noting that while our system gets theword order of the output system right, it makes sev-38Figure 6: GHKM tree output for a test sentence.eral agreement mistakes and (like Pharaoh) doesn?tget the translation of ?along these lines?
right.In Figure 7, we show sample translations by thethree systems under discussion for the first five sen-tences in our evaluation set.
For the LFG-based ap-proach, we can at this point present only results fora version trained on 10% of the sentence pairs.
Thisexplains why more source words are left untrans-lated.
But note that despite the small training set,the word ordering results are clearly superior for thissystem: the syntax-driven rules place the untrans-lated English words in the correct position in termsof German syntax.The translations with Pharaoh contain relativelyfew agreement mistakes (note that it exploits a lan-guage model of German trained on a much largercorpus).
The phrase-based approach does howeverskip words and make positioning mistakes some ofwhich are so serious (like in the last sentence) thatthey make the result hard to understand.7 DiscussionIn describing this pilot project, we have attemptedto give a ?big picture?
view of the essential ideasbehind our system.
To avoid obscuring the presen-tation, we have avoided many of the implementationdetails, in particular our choice of features.
Thereare exactly four types of decisions that we need totrain: (1) whether a parse tree node should be a rulenode, (2) the RHS template of a rule, (3) whether arule variable should be pushed left, and (4) whethera rule variable should be pushed right.
For eachof these decisions, there are a number of possiblefeatures that suggest themselves.
For instance, re-call that in German, embedded infinitival verbs getplaced at the end of the sentence or clause.
Sowhen the system is considering whether to push arule?s noun phrase to the left, past an existing verb,it would be useful for it to consider (as a feature)whether that verb is the first or second verb of itsclause and what the morphological form of the verbis.Even in these early stages of development, theMT-as-Tree-Labeling system shows promise in us-ing syntactic information flexibly and effectively forMT.
Our preliminary comparison indicates that us-ing deep syntactic analysis leads to improved trans-lation behavior.
We hope to develop the systeminto a competitive alternative to phrase-based ap-proaches.ReferencesP.F.
Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mer-cer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?311.Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-suichi, and Christian Rohrer.
2002.
The parallel gram-39source we believe that this is a fundamental element .professional translation wir denken , dass dies ein grundlegender aspekt ist .PHARAOH (50k) wir halten dies fu?r:::eine::::::::::::grundlegende element .TL-WSJ (50k) wir glauben , dass:::::diesen ist ein grundlegendes element .TL-LFG (5k) wir meinen , dass dies:::eine::::::::::::grundlegende element ist .source it is true that lisbon is a programme for ten years .professional translation nun ist lissabon ein programm fu?r zehn jahre .PHARAOH (50k) es ist richtig , dass lissabon ist:::eine programm fu?r zehn:::::jahren .TL-WSJ (50k) es ist richtig , dass lissabon ist:::eine programm fu?r zehn:::::jahren .TL-LFG (5k) es ist true , dass lisbon:::eine programm fu?r zehn:::::jahren ist .source i completely agree with each of these points .professional translation ich bin mit jeder einzelnen dieser aussagen voll und ganz einverstanden .PHARAOH (50k) ich ..... vo?llig einverstanden mit jedem dieser punkte .TL-WSJ (50k) ich bin vo?llig mit::::jedes:::::diese fragen einer meinung .TL-LFG (5k) ich agree completely mit::::jeder dieser punkte .source however , i would like to add one point .professional translation aber ich mo?chte gern einen punkt hinzufu?gen .PHARAOH (50k) allerdings mo?chte ich noch eines sagen .TL-WSJ (50k) ich mo?chte jedoch an noch einen punkt hinzufu?gen .TL-LFG (5k) allerdings mo?chte ich einen punkt add .source this is undoubtedly a point which warrants attention .professional translation ohne jeden zweifel ist dies ein punkt , der aufmerksamkeit verdient .PHARAOH (50k) das ist sicherlich:::eine punkt .... rechtfertigt das aufmerksamkeit .TL-WSJ (50k) das ist ohne zweifel::::eine punkt ,::die warrants beachtung .TL-LFG (5k) das ist undoubtedly .... sache , die attention warrants .Figure 7: Sample translations by (1) the PHARAOH system, (2) our system with a treebank parser (TL-WSJ),(3) our system with the XLE parser (TL-LFG).
(1) and (2) were trained on 50,000 sentence pairs, (3) juston (3) sentence pairs.
Error coding:::::::wrong:::::::::::::::morphological:::::form, incorrectly positioned word, untranslatedsource word, missed word: ...., extra word.mar project.
In Proceedings of COLING-2002 Workshop onGrammar Engineering and Evaluation, pages 1?7.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of ACL, pages263?270.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2003.
What?s in a translation rule?
In Proc.
NAACL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proceedings of the Hu-man Language Technology Conference 2003 (HLT-NAACL2003), Edmonton, Canada.Philipp Koehn.
2002.
Europarl: A multilingual corpus for eval-uation of machine translation.
Ms., University of SouthernCalifornia.Philipp Koehn.
2005.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
In Pro-ceedings of the Sixth Conference of the Association for Ma-chine Translation in the Americas, pages 115?124.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.
J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,A.
Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, VirenJain, Z.Jin, and D. Radev.
2003.
Syntax for statistical ma-chine translation.
Technical report, Center for Language andSpeech Processing, Johns Hopkins University, Baltimore.Summer Workshop Final Report.Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, JohnMaxwell, and Mark Johnson.
2002.
Parsing the Wall StreetJournal using a Lexical-Functional Grammar and discrim-inative estimation techniques.
In Proceedings of the 40thAnnual Meeting of the Association for Computational Lin-guistics (ACL?02), Pennsylvania, Philadelphia.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
Morgan Kaufmann.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statis-tical translation model.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguistics,pages 523?530.40
