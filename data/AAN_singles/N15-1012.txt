Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 113?122,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTransition-Based Syntactic LinearizationYijia Liu ?
?, Yue Zhang ?, Wanxiang Che ?, Bing Qin ?
?Singapore University of Technology and Design?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China{yjliu,car,bqin}@ir.hit.edu.cn yue zhang@sutd.edu.sgAbstractSyntactic linearization algorithms take a bagof input words and a set of optional con-straints, and construct an output sentence andits syntactic derivation simultaneously.
Thesearch problem is NP-hard, and the currentbest results are achieved by bottom-up best-first search.
One drawback of the methodis low efficiency; and there is no theoreticalguarantee that a full sentence can be foundwithin bounded time.
We propose an alter-native algorithm that constructs output struc-tures from left to right using beam-search.
Thealgorithm is based on incremental parsing al-gorithms.
We extend the transition system sothat word ordering is performed in addition tosyntactic parsing, resulting in a linearizationsystem that runs in guaranteed quadratic time.In standard evaluations, our system runs an or-der of magnitude faster than a state-of-the-artbaseline using best-first search, with improvedaccuracies.1 IntroductionLinearization is the task of ordering a bag of wordsinto a grammatical and fluent sentence.
Syntax-based linearization algorithms generate a sentencealong with its syntactic structure.
Depending on howmuch syntactic information is available as inputs, re-cent work on syntactic linearization can be classifiedinto free word ordering (Wan et al, 2009; Zhang etal., 2012; de Gispert et al, 2014), which orders abag of words without syntactic constraints, full treelinearization (He et al, 2009; Bohnet et al, 2010;Song et al, 2014), which orders a bag of wordsInitial State ([ ], [1...n], ?
)Final State ([ ], [ ], A)Induction Rules:SHIFT(?, [i|?
], A)([?| i], ?, A)LEFTARC([?|j i], ?, A)([?|i], ?, A ?
{j ?
i})RIGHTARC([?|j i], ?, A)([?|j], ?, A ?
{j ?
i})Figure 1: The arc-standard parsing algorithm.given a full-spanning syntactic tree, and partial treelinearization (Zhang, 2013), which orders a bag ofwords given some syntactic relations between themas partial constraints.The search space for syntactic linearization ishuge.
Even with a full syntax tree being available asconstraints, permutation of nodes on each level is anNP-hard problem.
As a result, heuristic search hasbeen adopted by most previous work, and the bestresults have been achieved by a time-constrainedbest-first search framework (White, 2004a; Whiteand Rajkumar, 2009; Zhang and Clark, 2011b; Songet al, 2014).
Though empirically highly accurate,one drawback of this approach is that there is noasymptotic upper bound on the time complexity offinding the first full sentence.
As a result, it can take5?10 seconds to process a sentence, and sometimesfail to yield a full sentence at timeout.
This issue ismore severe for larger bags of words, and makes thealgorithms practically less useful.We study the effect of an alternative learningand search framework for the linearization prob-113..NP .VBD .NP .IN .NP ...Dr. Talcott1.led2.a team3.of4.Harvard University5..6Figure 2: Example dependency tree.lem, which has a theoretical upper bound on thetime complexity, and always yields a full sentence inquadratic time.
Our method is inspired by the con-nection between syntactic linearization and syntacticparsing: both build a syntactic tree over a sentence,with the former performing word ordering in addi-tion to derivation construction.
As a result, syntac-tic linearization can be treated as a generalized formof parsing, for which there is no input word order,and therefore extensions to parsing algorithms canbe used to perform linearization.For syntactic parsing, the algorithm of Zhang andNivre (2011) gives competitive accuracies under lin-ear complexity.
Compared with parsers that use dy-namic programming (McDonald and Pereira, 2006;Koo and Collins, 2010), the efficient beam-searchsystem is more suitable for the NP-hard lineariza-tion task.
We extend the parser of Zhang and Nivre(2011), so that word ordering is performed in addi-tion to syntactic tree construction.
Experimental re-sults show that the transition-based linearization sys-tem runs an order of magnitude faster than a state-of-the-art best-first baseline, with improved accuraciesin standard evaluation.
Our linearizer is publiclyavailable under GPL at http://sourceforge.net/projects/zgen/.2 Transition-Based ParsingThe task of dependency parsing is to find a depen-dency tree given an input sentence.
Figure 2 showsan example dependency tree, which consists of de-pendency arcs that represent syntactic relations be-tween pairs of words.
A transition-based depen-dency parsing algorithm (Nivre, 2008) can be for-malized as a transition system, S = (C, T, cs, Ct),where C is the set of states, T is a set of transitionactions, csis the initial state and Ctis a set of ter-minal states.
The parsing process is modeled as anapplication of a sequence of actions, transducing theinitial state into a final state, while constructing de-Transition ?
?
A0 [] [1...6] ?1 SHIFT [1] [2...6]2 SHIFT [1 2] [3...6]3 SHIFT [1 2 3] [4...6]4 SHIFT [1 2 3 4] [5,6]5 SHIFT [1 2 3 4 5] [6]6 RIGHTARC [1 2 3 4] [6] A ?
{4?
5}7 RIGHTARC [1 2 3] [6] A ?
{3?
4}8 RIGHTARC [1 2] [6] A ?
{2?
3}9 SHIFT [1 2 6] []10 RIGHTARC [1 2] [] A ?
{2?
6}11 LEFTARC [2] [] A ?
{1?
2}Table 1: arc-standard transition action sequence forparsing the sentence in Figure 2.pendency arcs.
Each state in the transition systemcan be formalized as a tuple (?, ?,A), where ?
is astack that maintains a partial derivation, ?
is a bufferof incoming input words and A is the set of depen-dency relations that have been built.Our work is based on the arc-standard algorithm(Nivre, 2008).
The deduction system of the arc-standard algorithm is shown in Figure 1.
In thissystem, three transition actions are used: LEFT-ARC, RIGHTARC and SHIFT.
Given a state s =([?| j i], [k|?
], A),?
LEFTARC builds an arc {j ?
i} and pops j offthe stack.?
RIGHTARC builds an arc {j ?
i} and pops ioff the stack.?
SHIFT removes the front word k from the buffer?, and shifts it onto the stack.In the notations above, i, j and k are word indices ofan input sentence.
The arc-standard system assumesthat each input word has been assigned a part-of-speech (POS) tag.The sentence in Figure 2 can be parsed by thetransition sequence shown in Table 1.
Given an inputsentence of n words, the algorithm takes 2n tran-sitions to construct an output, because each wordneeds to be shifted onto the stack once and poppedoff once before parsing finishes, and all the transi-tion actions are either shifting or popping actions.114Initial State ([ ], set(1..n), ?
)Final State ([ ], ?, A)Induction Rules:SHIFT-i-POS(?, ?,A)([?|i], ??
{i}, A)LEFTARC([?|j i], ?, A)([?|i], ?, A ?
{j ?
i})RIGHTARC([?|j i], ?, A)([?|j], ?, A ?
{j ?
i})Figure 3: Deduction system for transition-based lin-earization.
Indices i, j do not reflect word order.3 Transition-Based LinearizationThe main difference between linearization and de-pendency parsing is that the input words are un-ordered for linearization, which results in an un-ordered buffer ?.
At a certain state s = (?, ?,A),any word in the buffer ?
can be shifted onto thestack.
In addition, unlike a parser, the vanilla lin-earization task does not assume that input words areassigned POS.
To extend the arc-standard algorithmfor linearization, we incorporate word and POS intothe SHIFT operation, transforming the arc-standardSHIFT operation to SHIFT-Word-POS, which selectsthe word Word from the buffer ?, tags it with POSand shifts it onto the stack.
Since the order of wordsin an output sentence equals to the order in whichthey are shifted onto the stack, word ordering is per-formed along with the parsing process.Under such extension, the sentence in Figure2 can be generated by the transition sequence(SHIFT-Dr. Talcott-NP, SHIFT-led-VBD, SHIFT-of-NP, SHIFT-a team-NP, SHIFT-of-IN, SHIFT-Harvard University-NP, RIGHTARC, RIGHTARC,RIGHTARC, SHIFT-.-., RIGHTARC, LEFTARC),given the unordered bag of words (Dr. Talcott, led,a team, of, Harvard University, .
).The deduction system for the linearization algo-rithm is shown in Figure 3.
Given an input bag ofn words, this algorithm also takes 2n transition ac-tions to construct an output, by the same reason asthe arc-standard parser.3.1 Search and LearningWe apply the learning and search framework ofZhang and Clark (2011a), which gives state-of-the-Algorithm 1: transition-based linearizationInput: C, a set of input syntactic constraintsOutput: The highest-scored final state1 candidates?
([ ], set(1..n), ?
)2 agenda?
?3 for i?
1..2n do4 for s in candidates do5 for action in GETPOSSIBLEACTIONS(s,C) do6 agenda?
APPLY(s, action)7 candidates?
TOP-K(agenda)8 agenda?
?9 best?
BEST(candidates)10 return bestart transition-based parsing accuracies and runs inlinear time (Zhang and Nivre, 2011).
Pseudocode ofthe search algorithm is shown in Algorithm 1.
It per-forms beam-search by using an agenda to keep thek-best states at each incremental step.
When decod-ing starts, the agenda contains only the initial state.At each step, each state in the agenda is advanced byapplying all possible transition actions (GETPOSSI-BLEACTIONS), leading to a set of new states.
Thek best are selected for the new states, and used toreplace the current states in the agenda, before thenext decoding step starts.
Given an input bag of nwords, the process repeats for 2n steps, after whichall the states in the agenda are terminal states, andthe highest-scored state in the agenda is taken forthe final output.
The complexity of this algorithmis n2, because it takes a fixed 2n steps to constructan output, and in each step the number of possibleSHIFT action is proportional to the size of ?.The search algorithm ranks search hypotheses,which are sequences of state transitions, by theirscores.
A global linear model is used to score searchhypotheses.
Given a hypothesis h, its score is calcu-lated by:Score(h) = ?
(h) ???,where??
is the parameter vector of the model and?
(h) is the global feature vector of h, extracted byinstantiating the feature templates in Table 2 accord-ing to each state in the transition sequence.In the table, S0represents the first word on thetop of the stack, S1represents the second word onthe top of the stack, w represents a word and p rep-115UnigramsS0w; S0p; S0,lw; S0,lp; S0,rw; S0,rp;S0,l2w; S0,l2p; S0,r2w; S0,r2p;S1w; S1p; S1,lw; S1,lp; S1,rw; S1,rp;S1,l2w; S1,l2p; S1,r2w; S1,r2p;BigramS0wS0,lw; S0wS0,lp; S0pS0,lw; S0pS0,lp;S0wS0,rw; S0wS0,rp; S0pS0,rw; S0pS0,rp;S1wS1,lw; S1wS1,lp; S1pS1,lw; S1pS1,lp;S1wS1,rw; S1wS1,rp; S1pS1,rw; S1pS1,rp;S0wS1w; S0wS1p; S0pS1w; S0pS1pTrigramS0wS0pS0,lw; S0wS0,lwS0,lp; S0wS0pS0,lp;S0pS0,lwS0,lp; S0wS0pS0,rw; S0wS0,lwS0,rp;S0wS0pS0,rp; S0pS0,rwS0,rp;S1wS1pS1,lw; S1wS1,lwS1,lp; S1wS1pS1,lp;S1pS1,lwS1,lp; S1wS1pS1,rw; S1wS1,lwS1,rp;S1wS1pS1,rp; S1pS1,rwS1,rp;Linearizionw0; p0; w?1w0; p?1p0; w?2w?1w0; p?2p?1p0;S0,lwS0,l2w; S0,lpS0,l2p; S0,r2wS0,rw; S0,r2pS0,rp;S1,lwS1,l2w; S1,lpS1,l2p; S1,r2wS1,rw; S1,r2pS1,rp;Table 2: Feature templates.resent a POS-tag.
The feature templates can be clas-sified into four types: unigram, bigram, trigram andlinearization.
The first three types are taken fromthe dependency parser of Zhang and Nivre (2011),which capture context information for S0, S1andtheir modifiers.
The original feature templates ofZhang and Nivre (2011) also contain information ofthe front words on the buffer.
However, since thebuffer is unordered for linearization, we do not in-clude these features.The linearization feature templates are specificfor linearization, and captures surface ngram infor-mation.
Each search state represents a partially lin-earized sentence.
We represents the last word in thepartially linearized sentence as w0and the secondlast as w?1.Given a set of labeled training examples, the av-eraged perceptron (Collins, 2002) with early update(Collins and Roark, 2004; Zhang and Nivre, 2011)is used to train the parameters??
of the model.3.2 Input Syntactic ConstraintsThe use of syntactic constraints to achieve better lin-earization performance has been studied in previouswork.
Wan et al (2009) employ POS constraints..NP .VBD .NP .IN .NP ...Dr. Talcott1.led2.a team3.of4.Harvard University5..6Figure 4: Example partial tree.
Words in the samesub dependency trees are grouped by rounded boxes.Word indices do not specify their orders.
Basephrases (e.g.
Dr. Talcott) are treated as single words.in learning a dependency language model.
Zhangand Clark (2011b) take supertags as constraints to aCCG linearizer.
Zhang (2013) demonstrates the pos-sibility of partial-tree linearization, which allows awhole spectrum of input syntactic constraints.
Inpractice, input syntactic constraints, including POSand dependency relations, can be obtained from ear-lier stage of a generation pipeline, such as lexicaltransfer results in machine translation.It is relatively straightforward to apply input con-straints to a best-first system (Zhang, 2013), but lessso for beam-search.
In this section, we utilize theinput syntactic constraints by letting the informationdecide the possible actions for each state, namelythe return value of GETPOSSIBLEACTIONS in Al-gorithm 1, thus, when input POS-tags and depen-dencies are given, the generation system can achievemore specified outputs.3.2.1 POS ConstraintsPOS is the simplest form of constraints to thetransition-based linearization system.
When thePOS of an input word is given, the POS-tag com-ponent in SHIFT-Word-POS operation is fixed, andthe number of SHIFT actions for the word is reducedfrom the number of all POS to 1.3.2.2 Partial Tree ConstraintsIn partial tree linearization, a set of dependencyarcs that form a partial dependency tree is given tothe linearization system as input constraints.
Fig-ure 4 illustrate an example.
The search space canbe reduced by ignoring the transition sequences thatdo not result in a dependency tree that is consis-tent with the input constraints.
Take the partialtree in Figure 4 for example.
At the state s =([Harvard University5], set(1..n)-{5}, ?
), it is illegalto shift the base phrase a team3onto the stack, be-116Algorithm 2: GETPOSSIBLEACTIONS for par-tial tree linearization, where C is a partial treeInput: A state s = ([?|j i], ?, A) and partial tree COutput: A set of possible transition actions T1 if s.?
is empty then2 for k ?
s.?
do3 T ?
T ?
(SHIFT, POS, k)4 else5 if REDUCABLE(s, i, j, C) then6 T ?
T ?
(LEFTARC)7 if REDUCABLE(s, j, i, C) then8 T ?
T ?
(RIGHTARC)9 for k ?
s.?
do10 if SHIFTLEGAL(s, k, C) then11 T ?
T ?
(SHIFT, POS, k)12 return T.stack ?.
.
.4 345(a).. .
.3 1345(b)Figure 5: Two conditions for a valid LEFTARC ac-tion in partial-tree linearization.
The indices corre-spond to those in Figure 4.
A shaded triangle repre-sents the readily built arcs under a root word.cause this action will result in a sub-sequence (Har-vard University5, a team3, of4), which cannot havethe dependency arcs {3 ?
4}, {4 ?
5} by usingarc-standard actions.Algorithm 3 shows pseudocode of GETPOSSI-BLEACTIONS when C is a partial tree.
Given a states = ([?|j i], ?, A) the LEFTARC action builds anarc {j ?
i} and pops the word j off the stack.Since the popped word j cannot be linked to anywords in future transitions, all the descendants of jshould have been processed and removed from thestack.
In addition, constrained by the given partialtree, the arc {j ?
i} should be an arc in C (Fig-ure 5a), or j should be the root of a sub dependencytree in C (Figure 5b).
We denote the conditions asREDUCABLE(s, i, j, C) (lines 5-6).
The case forRIGHTARC is similar to LEFTARC (lines 7-8).For the SHIFT action, the conditions are morecomplex.
Due to space limitation, we briefly sketch.. .
.l k1 3 2stack ?buffer ?345(a).l k. .
.5 4 3l k. .
.35 4(b).l k. .
.1 3 2345(c).l k1 3 6 2. .
.345(d).l k. .
.3 2 5 41 26. .
.3 2 4 2 5(e)Figure 6: 5 relations between k and l. The indicescorrespond to those in Figure 4.
The words in greenboxes must have arcs with k in future transitions.the SHIFTLEGAL function below.
Detailed algo-rithm pseudocode for SHIFTLEGAL is given in thesupplementing material.
For a word k in ?
to beshifted onto the stack, all the words on the stackmust satisfy certain constraints.
There are 5 possi-ble relations between k and a word l on the stack.
(1) If l is a child of k in C (Figure 6a), all the wordson the stack from l to the top of the stack should bereducable to k, because only LEFTARC can be ap-plied between k and these words in future actions.
(2) If l is a grand child of k (Figure 6b), no legalsentence can be constructed if k is shifted onto thestack.
(3) If l is the parent of k (Figure 6c), legalSHIFTs require all the words on the stack from l tothe top to be reducable to k. (4) If l is a grand parentof k, all the words on the stack from l to the top willbecome descendants of l in the output (Figure 6e).Thus these words must be descendants of l in C, orthe root of different subdependency trees.
(5) If l isa siblings of k, we denote a as the least common an-cestor of k and l. a will become in the buffer and lshould be a direct child of a.
All the words from lto the top of the stack should be the descendants ofa in the output (Figure 6d), and thus a should havethe same conditions as in (4).
Finally, if no word onthe stack is in the same subdependency tree as k inC, then k can be safely shifted.117Algorithm 3: GETPOSSIBLEACTIONS for fulltree linearization, where C is a full treeInput: A state s = ([?|j i], ?, A) and gold tree COutput: A set of possible transition actions T1 T ?
?2 if s.?
is empty then3 for k ?
s.?
do4 T ?
T ?
(SHIFT, POS, k)5 else6 if ?j, j ?
(DESCENDANTS(i) ?
s.?)
then7 for j ?
(DESCENDANTS(i) ?
s.?)
do8 T ?
T ?
(SHIFT, POS, j)9 else10 if {j ?
i} ?
C then11 T ?
T ?
(RIGHTARC)12 else if {j ?
i} ?
C then13 T ?
T ?
(LEFTARC)14 else15 fork ?
(SIBLINGS(i)?HEAD(i))?
s.?
do16 T ?
T ?
(SHIFT, POS, k)17 return T3.2.3 Full Tree ConstraintsAlgorithm 2 can also be used with full-tree con-straints, which are a special case of partial-tree con-straints.
However, there is a conceptually simpleralgorithm that leverages full-tree constraints.
Be-cause tree linearization is frequently studied in theliterature, we describe this algorithm in Algorithm3.
When the stack is empty, we can freely moveany word in the buffer ?
onto the stack (line 2-4).
Ifnot all the descendants of the stack top i have beenprocessed, the next transition actions should movethem onto the stack, so that arcs can be constructedbetween i and these words (line 6-8).
If all the de-scendants of i have been processed, the next actionshould eagerly build arcs between top two words iand j on the stack (line 10-13).
If no arc exists be-tween i and j, the next action should shift the parentword of i or a word in i?s sibling tree (line 14-16).4 ExperimentsWe follow previous work and conduct experimentson the Penn Treebank (PTB), using Wall Street Jour-0.350.400.450 10 20 30 40 50iterationBLEUbeamsize X1 X4 X16 X64 X128Figure 7: Dev.
results with different beam sizes.nal sections 2?21 for training, 22 for developmenttesting and 23 for final testing.
Gold-standard de-pendency trees are derived from bracketed sentencesin the treebank using Penn2Malt1, and base nounphrases are treated as a single word (Wan et al,2009; Zhang, 2013).
The BLEU score (Papineni etal., 2002) is used to evaluate the performance of lin-earization, which has been adopted in former liter-als (Wan et al, 2009; White and Rajkumar, 2009;Zhang and Clark, 2011b) and recent shared-tasks(Belz et al, 2011).
We use our implementation ofthe best-first system of Zhang (2013), which givesthe state-of-the-art results, as the baseline.4.1 Influence of Beam sizeWe first study the influence of beam size by per-forming free word ordering on the development testdata.
BLEU score curves with different beam sizesare shown in Figure 7.
From this figure, we can seethat the systems with beam 64 and 128 achieve thebest results.
However, the 128-beam system doesnot improve the performance significantly (48.2 vs47.5), but runs twice slower.
As a result, we set thebeam size to 64 in the remaining experiments.4.2 Input Syntactic ConstraintsTo test the effectiveness of GETPOSSIBLEACTIONSunder different input constraints, we follow Zhang(2013) and feed different amounts of POS-tags anddependencies to our transition-based linearizationsystem.
Input syntactic constraints are obtained byrandomly sampling POS and dependencies from thegold dependency tree.
Nine development experi-ments under different inputs are performed, and the1http://stp.lingfil.uu.se/?nivre/research/Penn2Malt.html118no pos 50% pos all pos no pos 50% pos all pos no pos 50% pos all posno dep no dep no dep 50% dep 50% dep 50% dep all dep all dep all depBL SP BL SP BL SP BL SP BL SP BL SP BL SP BL SP BL SPZ13 42.9 4872 43.4 4856 44.7 4826 50.5 4790 51.4 4737 52.2 4720 73.3 4600 74.7 4431 76.3 4218Ours 47.5 155 47.9 119 48.8 74 54.8 132 55.2 91 56.2 41 77.8 40 79.1 28 81.1 22Table 3: Partial-tree linearizion results on the development test set.
BL ?
the BLEU score, SP ?
number ofmilliseconds to order one sentence.
Z13 refers to the best-first system of Zhang (2013).0.9000.9250.9500.9751.0001?89?1112?1415?1718?2021?2425?3233?164systembestfirstoursFigure 8: Comparison between transition-based andbest-first systems on surface string brevity.Precision Recall Flen Z13 ours Z13 ours Z13 ours< 5 24.63 20.45 14.56 21.82 18.3 21.11< 10 15.20 16.33 10.59 15.88 12.48 16.1< 15 10.82 14.73 9.38 14.08 10.05 14.4< 30 8.18 12.54 8.26 12.43 8.22 12.49Table 4: Precision, recall and F-score comparison ondifferent spans lengths.BLEU scores along with the average time to orderone sentence are shown in Table 3.With more syntactic information in the input, ourlinearization system achieves better performance,showing that GETPOSSIBLEACTIONS can take ad-vantage of the input constraints and yield more spec-ified output.
In addition, because input constraintsreduce the search space, the systems with more syn-tactic information achieve faster decoding speeds.
Incomparison with Zhang (2013), the transition-basedsystem achieves improved accuracies under the set-tings, and the decoding speed can be over two ordersof magnitude faster (22ms vs. 4218ms).
We givemore detailed analysis next.4.3 Comparison with Best-FirstThe beam-search linearizer takes a very differ-ent search strategy compared with best-first search,which affects the error distribution.
As mentionedearlier, one problem of best-first is the lack of the-oretical guarantee on time complexity.
As a result,a time constraint is used and default output can beconstructed when no full output is found (White,2004b; Zhang and Clark, 2011b).
This may resultin incomplete output sentences and intuitively, thisproblem is more severe for larger bag of words.
Incontrast, the transition-based linearization algorithmtakes |2n| steps to generate a sentence and thus guar-antees to order all the input words.
Figure 8 showsthe results by comparing the brevity scores (i.e.
thenumber of words in the output divided by the num-ber of words in reference sentence) on different sizesof inputs.
Best-search can fail to order all the in-put words even on bags of 9 ?
11 words, and thecase is more severe for larger bag of words.
On theother hand, the transition-based method uses all theinput words to generate output and the brevity scoreis constant 1.
Since the BLEU score consists twoparts: the n-gram precision and brevity, this com-parison partly explains why the transition-based lin-earization algorithm achieves higher BLEU scores.To further compare the difference between thetwo systems, we evaluate the qualities of projectivespans, which are dependency treelets.
Both systemsbuild outputs bottom-up by constructing projectivespans, and a break-down of span accuracies againstspan sizes shows the effects of the different searchalgorithms.
The results are shown in Table 4.
Ac-cording to this table, the best-first system tends toconstruct smaller spans more precisely, but the re-call is relatively lower.
Overall, higher F-scores areachieved by the transition-based system.During the decoding process, the best-first sys-tem compares spans of different sizes and expands1190.00.10.22 4 6 8 10 12 14 16 18 20 22 24 26 28 30systembestfirstoursgoldFigure 9: Distributions of spans outputted by thebest-first, transition-based systems and the goldtrees.no pos all pos all posno dep no dep all depWan et al (2009) - 33.7 -Zhang and Clark (2011b) - 40.1 -Zhang et al (2012) - 43.8 -Zhang (2013) 44.7 46.8 76.2This paper 49.4 50.8 82.3Table 5: Final results.those that have higher scores.
As a result, the num-ber of expanded spans do not have a fixed correlationwith the size, and there can be fewer but better smallspans expanded.
In contrast, the transition-basedsystem models transition sequences rather than indi-vidual spans, and therefore the distribution of spansof different sizes in each hypothesis resembles thatof the training data.
Figure 9 verifies the analysis bycounting the distributions of spans with respect tothe length, in the search algorithms of the two sys-tems and the gold dependency trees.
The distribu-tion of the transition-based system is closer to thatof gold dependency trees, while the best-first sys-tem outputs less smaller spans and more longer ones.This explains the higher precision for the best-firstsystem on smaller spans.4.4 Final ResultsThe final results on the test set of Penn Treebank areshown in Table 5.
Compared with previous studies,our transition-based linearization system achievesthe best results on all the tests.
Table 6 shows someexample output sentences, when there are no inputconstraints.
For longer sentences, the transition-based method gives noticeably better results.output BLref.
There is no asbestos in our products now .Z13 There is no asbestos now in our products .
43.5ours There is now our products in no asbestos .
17.8ref.
Previously , watch imports were deniedsuch duty-free treatment .Z13 such duty-free treatment Previously ,watch imports were denied .67.6ours Previously , watch imports were deniedsuch duty-free treatment .100ref.
Despite recent declines in yields , investorscontinue to pour cash into money funds .Z13 continue yields investors pour to recent de-clines in cash , into money funds20.1ours Despite recent declines in yields intomoney funds , investors continue to pourcash .67.0Table 6: Example outputs.5 Related WorkThe input to practical natural language generation(NLG) system (Reiter and Dale, 1997) can rangefrom a bag of words and phrases to a bag of lem-mas without punctuation (Belz et al, 2011).
Thelinearization module of this paper can serve as thefinal stage in a pipeline when the bag of words andtheir optional syntactic information are given.
Therehas also been work to jointly perform linearizationand morphological generation (Song et al, 2014).There has been work on linearization with unla-beled and labeled dependency trees (He et al, 2009;Zhang, 2013).
These methods mostly use greedy orbest-first algorithms to order each tree node.
Ourwork is different by performing word ordering usinga transition process.Besides dependency grammar, linearization withother syntactic grammars, such as CFG and CCG(White and Rajkumar, 2009; Zhang and Clark,2011b), has also been studied.
In this paper, weadopt the dependency grammar for transition-basedlinearization.
However, since transition-based pars-ing algorithms has been successfully applied to dif-ferent grammars, including CFG (Sagae et al, 2005)and CCG (Xu et al, 2014), our linearization methodcan be applied to these grammars.1206 ConclusionWe studied transition-based syntactic linearizationas an extension to transition-based parsing.
Com-pared with best-first systems, the advantage of ourtransition-based algorithm includes bounded timecomplexity, and the guarantee to yield full sen-tences when given a bag of words.
Experimen-tal results show that our algorithm achieves im-proved accuracies, with significantly faster decod-ing speed compared with a state-of-the-art best-firstbaseline.
We publicly release our code at http://sourceforge.net/projects/zgen/.For future work, we will study the incorporationof large-scale language models, and the integrationof morphology generation and linearization.AcknowledgmentsWe thank the anonymous reviewers for their con-structive comments.
This work was supportedby the National Key Basic Research Program ofChina via grant 2014CB340503 and the SingaporeMinistry of Education (MOE) AcRF Tier 2 grantT2MOE201301 and SRG ISTD 2012 038 from Sin-gapore University of Technology and Design.ReferencesAnja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proceedings of the Genera-tion Challenges Session at the 13th European Work-shop on Natural Language Generation, pages 217?226, Nancy, France, September.
Association for Com-putational Linguistics.Bernd Bohnet, Leo Wanner, Simon Mill, and AliciaBurga.
2010.
Broad coverage multilingual deep sen-tence generation with a stochastic multi-level realizer.In Proceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages98?106, Beijing, China, August.
Coling 2010 Orga-nizing Committee.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof the 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), Main Volume, pages 111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofthe 2002 Conference on Empirical Methods in Natu-ral Language Processing, pages 1?8.
Association forComputational Linguistics, July.Adri`a de Gispert, Marcus Tomalin, and Bill Byrne.
2014.Word ordering with phrase-based grammars.
In Pro-ceedings of the 14th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 259?268, Gothenburg, Sweden, April.
As-sociation for Computational Linguistics.Wei He, HaifengWang, Yuqing Guo, and Ting Liu.
2009.Dependency based chinese sentence realization.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 809?816, Suntec, Singapore, Au-gust.
Association for Computational Linguistics.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1?11, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Ryan T McDonald and Fernando CN Pereira.
2006.
On-line learning of approximate dependency parsing algo-rithms.
In EACL.Joakim Nivre.
2008.
Algorithms for deterministic incre-mental dependency parsing.
Computational Linguis-tics, 34(4):513?553.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Ehud Reiter and Robert Dale.
1997.
Building appliednatural language generation systems.
Nat.
Lang.
Eng.,3(1):57?87, March.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005.Automatic measurement of syntactic development inchild language.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 197?204, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.2014.
Joint morphological generation and syntacticlinearization.
In AAAI, pages 1522?1528.Stephen Wan, Mark Dras, Robert Dale, and C?ecile Paris.2009.
Improving grammaticality in statistical sen-tence generation: Introducing a dependency spanningtree algorithm with an argument satisfaction model.In Proceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 852?860, Athens, Greece, March.
Association for Compu-tational Linguistics.121Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore,August.
Association for Computational Linguistics.Michael White.
2004a.
Reining in CCG chart realiza-tion.
In In Proc.
INLG-04, pages 182?191.Michael White.
2004b.
Reining in ccg chart realiza-tion.
In Natural Language Generation, pages 182?191.
Springer.Wenduan Xu, Stephen Clark, and Yue Zhang.
2014.Shift-reduce ccg parsing with a dependency model.
InProceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 218?227, Baltimore, Maryland, June.Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2011a.
Syntactic process-ing using the generalized perceptron and beam search.Computational Linguistics, 37(1):105?151.Yue Zhang and Stephen Clark.
2011b.
Syntax-basedgrammaticality improvement using CCG and guidedsearch.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1147?1157, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Yue Zhang, Graeme Blackwood, and Stephen Clark.2012.
Syntax-based word ordering incorporating alarge-scale language model.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 736?746, Avignon, France, April.
Association for Compu-tational Linguistics.Yue Zhang.
2013.
Partial-tree linearization: Generalizedword ordering for text synthesis.
In IJCAI.122
