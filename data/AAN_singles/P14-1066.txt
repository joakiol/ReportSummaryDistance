Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699?709,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsLearning Continuous Phrase Representations forTranslation ModelingJianfeng Gao    Xiaodong He    Wen-tau Yih    Li DengMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{jfgao,xiaohe,scottyih,deng}@microsoft.comAbstractThis paper tackles the sparsity problem inestimating phrase translation probabilitiesby learning continuous phrase representa-tions, whose distributed nature enables thesharing of related phrases in their represen-tations.
A pair of source and target phrasesare projected into continuous-valued vec-tor representations in a low-dimensionallatent space, where their translation scoreis computed by the distance between thepair in this new space.
The projection isperformed by a neural network whoseweights are learned on parallel trainingdata.
Experimental evaluation has beenperformed on two WMT translation tasks.Our best result improves the performanceof a state-of-the-art phrase-based statisticalmachine translation system trained onWMT 2012 French-English data by up to1.3 BLEU points.1 IntroductionThe phrase translation model, also known as thephrase table, is one of the core components ofphrase-based statistical machine translation (SMT)systems.
The most common method of construct-ing the phrase table takes a two-phase approach(Koehn et al 2003).
First, the bilingual phrasepairs are extracted heuristically from an automat-ically word-aligned training data.
The secondphase, which is the focus of this paper, is parame-ter estimation where each phrase pair is assignedwith some scores that are estimated based oncounting these phrases or their words using thesame word-aligned training data.Phrase-based SMT systems have achievedstate-of-the-art performance largely due to the factthat long phrases, rather than single words, areused as translation units so that useful context in-formation can be captured in selecting translations.However, longer phrases occur less often in train-ing data, leading to a severe data sparseness prob-lem in parameter estimation.
There has been aplethora of research reported in the literature onimproving parameter estimation for the phrasetranslation model (e.g., DeNero et al 2006;Wuebker et al 2010; He and Deng 2012; Gao andHe 2013).This paper revisits the problem of scoring aphrase translation pair by developing a Continu-ous-space Phrase Translation Model (CPTM).The translation score of a phrase pair in this modelis computed as follows.
First, we represent eachphrase as a bag-of-words vector, called word vec-tor henceforth.
We then project the word vector,in either the source language or the target lan-guage, into a respective continuous feature vectorin a common low-dimensional space that is lan-guage independent.
The projection is performedby a multi-layer neural network.
The projectedfeature vector forms the continuous representa-tion of a phrase.
Finally, the translation score of asource-target phrase pair is computed by the dis-tance between their feature vectors.The main motivation behind the CPTM is toalleviate the data sparseness problem associatedwith the traditional counting-based methods bygrouping phrases with a similar meaning acrossdifferent languages.
This style of grouping ismade possible because of the distributed nature ofthe continuous-space representations for phrases.No such sharing was possible in the original sym-bolic space for representing words or phrases.
Inthis model, semantically or grammatically relatedphrases, in both the source and the target lan-guages, would tend to have similar (close) featurevectors in the continuous space, guided by thetraining objective.
Since the translation score is asmooth function of these feature vectors, a small699change in the features should only lead to a smallchange in the translation score.The primary research task in developing theCPTM is learning the continuous representationof a phrase that is effective for SMT.
Motivatedby recent studies on continuous-space languagemodels (e.g., Bengio et al 2003; Mikolov et al2011; Schwenk et al, 2012), we use a neural net-work to project a word vector to a feature vector.Ideally, the projection would discover those latentfeatures that are useful to differentiate good trans-lations from bad ones, for a given source phrase.However, there is no training data with explicitannotation on the quality of phrase translations.The phrase translation pairs are hidden in the par-allel source-target sentence pairs, which are usedto train the traditional translation models.
Thequality of a phrase translation can only be judgedimplicitly through the translation quality of thesentences, as measured by BLEU, which containthe phrase pair.
In order to overcome this chal-lenge and let the BLEU metric guide the projec-tion learning, we propose a new method to learnthe parameters of a neural network.
This newmethod, via the choice of an appropriate objectivefunction in training, automatically forces the fea-ture vector of a source phrase to be closer to thefeature vectors of its candidate translations.
As aresult, the BLEU score is improved when thesetranslations are selected by an SMT decoder toproduce final, sentence-level translations.
Thenew learning method makes use of the L-BFGSalgorithm and the expected BLEU as the objectivefunction defined on N-best lists.To the best of our knowledge, the CPTM pro-posed in this paper is the first continuous-spacephrase translation model that makes use of jointrepresentations of a phrase in the source languageand its translation in the target language (to be de-tailed in Section 4) and that is shown to lead tosignificant improvement over a standard phrase-based SMT system (to be detailed in Section 6).Like the traditional phrase translation model,the translation score of each bilingual phrase pairis modeled explicitly in our model.
However, in-stead of estimating the phrase translation score onaligned parallel data, our model intends to capturethe grammatical and semantic similarity betweena source phrase and its paired target phrase by pro-jecting them into a common, continuous spacethat is language independent.1 Niehues et al (2011) use different translation units in orderto integrate the n-gram translation model into the phrase-based approach.
However, it is not clear how a continuousThe rest of the paper is organized as follows.Section 2 reviews previous work.
Section 3 re-views the log-linear model for phrase-based SMTand Sections 4 presents the CPTM.
Section 5 de-scribes the way the model parameters are esti-mated, followed by the experimental results inSection 6.
Finally, Section 7 concludes the paper.2 Related WorkRepresentations of words or documents as contin-uous vectors have a long history.
Most of the ear-lier latent semantic models for learning such vec-tors are designed for information retrieval(Deerwester et al 1990; Hofmann 1999; Blei et al2003).
In contrast, recent work on continuousspace language models, which estimate the prob-ability of a word sequence in a continuous space(Bengio et al 2003; Mikolov et al 2010), have ad-vanced the state of the art in language modeling,outperforming the traditional n-gram model onspeech recognition (Mikolov et al 2012; Sunder-meyer et al 2013) and machine translation(Mikolov 2012; Auli et al 2013).Because these models are developed for mono-lingual settings, word embedding from these mod-els is not directly applicable to translation.
As aresult, variants of such models for cross-lingualscenarios have been proposed so that words in dif-ferent languages are projected into the shared la-tent vector space (Dumais et al 1997; Platt et al2010; Vinokourov et al 2002; Yih et al 2011;Gao et al 2011; Huang et al 2013; Zou et al2013).
In principle, a phrase table can be derivedusing any of these cross-lingual models, althoughdecoupling the derivation from the SMT trainingoften results in suboptimal performance (e.g.,measured in BLEU), as we will show in Section 6.Recently, there is growing interest in applyingcontinuous-space models for translation.
Themost related to this study is the work of continu-ous space n-gram translation models (Schwenk etal.
2007; Schwenk 2012; Son et al 2012), wherethe feed-forward neural network language modelis extended to represent translation probabilities.However, these earlier studies focused on the n-gram translation models, where the translationprobability of a phrase or a sentence is decom-posed as a product of n-gram probabilities as in astandard n-gram language model.
Therefore, it isnot clear how their approaches can be applied tothe phrase translation model1, which is much moreversion of such a model can be trained efficiently because thefactor models used by Son et al cannot be applied directly.700widely used in modern SMT systems.
In contrast,our model learns jointly the representations of aphrase in the source language as well as its trans-lation in the target language.
The recurrent contin-uous translation models proposed by Kalchbren-ner and Blunsom (2013) also adopt the recurrentlanguage model (Mikolov et al 2010).
But unlikethe n-gram translation models above, they makeno Markov assumptions about the dependency ofthe words in the target sentence.
Continuous spacemodels have also been used for generating trans-lations for new words (Mikolov et al 2013a) andITG reordering (Li et al 2013).There has been a lot of research on improvingthe phrase table in phrase-based SMT (Marcu andWong 2002; Lamber and Banchs 2005; Denero etal.
2006; Wuebker et al 2010; Zhang et al, 2011;He and Deng 2012; Gao and He 2013).
Amongthem, (Gao and He 2013) is most relevant to thework described in this paper.
They estimatephrase translation probabilities using a discrimi-native training method under the N-best rerankingframework of SMT.
In this study we use the sameobjective function to learn the continuous repre-sentations of phrases, integrating the strengths as-sociated with these earlier studies.3 The Log-Linear Model for SMTPhrase-based SMT is based on a log-linear modelwhich requires learning a mapping between input?
?
?
to output ?
?
?.
We are given?
Training samples (??
, ??)
for ?
= 1?
?,where each source sentence ??
is paired witha reference translation in target language ??;?
A procedure GEN to generate a list of N-bestcandidates GEN(??)
for an input ??
, whereGEN  in this study is the baseline phrase-based SMT system, i.e., an in-houseimplementation of the Moses system (Koehnet al 2007) that does not use the CPTM, andeach ?
?
GEN(??)
is labeled by thesentence-level BLEU score (He and Deng2012), denoted by sBleu(?
?, ?)
, whichmeasures the quality of ?
with respect to itsreference translation ??;?
A vector of features ?
?
??
that maps each(?
?, ?)
to a vector of feature values2; and?
A parameter vector ?
?
?
?, which assigns areal-valued weight to each feature.2 Our baseline system uses a set of standard features sug-gested in Koehn et al (2007), which is also detailed in Sec-tion 6.The components GEN(.
), ?
and ?
define a log-linear model that maps ??
to an output sentence asfollows:??
= argmax(?,?)?GEN(??)?T?(?
?, ?, ?)
(1)which states that given ?
and ?, argmax returnsthe highest scoring translation ?
?,  maximizingover  correspondences ?.
In phrase-based SMT, ?consists of a segmentation of the source and targetsentences into phrases and an alignment betweensource and target phrases.
Since computing theargmax  exactly is intractable, it is commonlyperformed approximatedly by beam search (Ochand Ney 2004).
Following Liang et al (2006), weassume that every translation candidate is alwayscoupled with a corresponding ?, called the Viterbiderivation, generated by (1).4 A Continuous-Space Phrase Transla-tion Model (CPTM)The architecture of the CPTM is shown in Figures1 and 2, where for each pair of source and targetphrases (?
?, ??)
in a source-target sentence pair,we first project them into feature vectors ???
and???
in a latent, continuous space via a neural net-work with one hidden layer (as shown in Figure2), and then compute the translation score,score(?
?, ??
), by the distance of their feature vec-tors in that space.We start with a bag-of-words representation ofa phrase ?
?
?
?, where ?
is a word vector and ?is the size of the vocabulary consisting of wordsin both source and target languages, which is setto 200K in our experiments.
We then learn to pro-ject ?
to a low-dimensional continuous space ??:?(?
): ??
?
?
?The projection is performed using a fully con-nected neural network with one hidden layer andtanh activation functions.
Let ?1 be the projec-tion matrix from the input layer to the hidden layerand ?2  the projection matrix from the hiddenlayer to the output layer, we have?
?
?(?)
= tanh (?2T(tanh(?1T?)))
(2)701Figure 2.
A neural network model for phrasesgiving rise to their continuous representations.The model with the same form is used for bothsource and target languages.The translation score of a source phrase f and atarget phrase e can be measured as the similarity(or distance) between their feature vectors.
Wechoose the dot product as the similarity function3:score(?, ?)
?
sim?(??
, ??)
= ??T??
(3)According to (2), we see that the value of the scor-ing function is determined by the projection ma-trices ?
= {?1,?2}.The CPTM of (2) and (3) can be incorporatedinto the log-linear model for SMT (1) by3 In our experiments, we compare dot product and the cosinesimilarity functions and find that the former works better fornonlinear multi-layer neural networks, and the latter worksbetter for linear neural networks.
For the sake of clarity, wechoose dot product when we describe the CPTM and its train-ing in Sections 4 and 5, respectively.4 The baseline SMT needs to be reasonably good in thesense that the oracle BLEU score on the generated n-bestintroducing a new feature ?
?+1  and a new featureweight ??+1.
The new feature is defined as??+1(?
?, ?, ?)
= ?
sim?(??
, ??)(?,?
)??
(4)Thus, the phrase-based SMT system, into whichthe CPTM is incorporated, is parameterized by(?, ?
), where ?
is a vector of a handful of param-eters used in the log-linear model of (1), with oneweight for each feature; and ?
is the projectionmatrices used in the CPTM defined by (2) and (3).In our experiments we take three steps to learn(?, ?):1.
We use a baseline phrase-based SMT sys-tem to generate for each source sentence intraining data an N-best list of translation hy-potheses4.2.
We set ?
to that of the baseline system andlet ?
?+1 = 1, and optimize ?
w.r.t.
a lossfunction on training data5.3.
We fix ?
, and optimize ?
using MERT(Och 2003) to maximize BLEU on dev data.In the next section, we will describe Step 2 in de-tail as it is directly related to the CPTM training.lists needs to be significantly higher than that of the top-1translations so that the CPTM can be effectively trained.5 The initial value of ?
?+1 can also be tuned using the devset.
However, we find in a pilot study that it is good enoughto set it to 1 when the values of all the baseline featureweights, used in the log-linear model of (1), are properly nor-malized, such as by setting ??
= ??/?
for ?
= 1??
,where ?
is the unnormalized weight value of the target lan-guage model.Figure 1.
The architecture of the CPTM, where the mapping from a phrase to its continuous repre-sentation is shown in Figure 2.200K (d)100100 (?)(?1???
)Word vectorNeural networkFeature vector?1?2?
?Raw phrase  ?
or ??
(the process of)         (machine translation)(consists of).
.
.?
(le processus de)(traduction automatique)  (consiste en).
.
.??
?1 ??
??+1??
?1 ??
??+1????????(?)score(?
?, ??)
= ??
?T ??
?Target phrasesContinuous representations oftarget phrasesSource phrasesContinuous representations ofsource phrasesTranslation score as dot product offeature vectors in the continuous space7025 Training CPTMThis section describes the loss function we em-ploy with the CPTM and the algorithm to train theneural network weights.We define the loss function ?(?)
as the nega-tive of the N-best list based expected BLEU, de-noted by xBleu(?).
In the reranking framework ofSMT outlined in Section 3, xBleu(?)
over onetraining sample (?
?, ??)
is defined asxBleu(?)
= ?
?(?|??)sBleu(?
?, ?)??GEN(??)
(5)where sBleu(?
?, ?)
is the sentence-level BLEUscore, and  ?(?|??)
is the translation probabilityfrom ??
to ?
computed using softmax as?(?|??)
=exp(??T?(??,?,?))?
exp(??T?(??,??,?))???GEN(??
)(6)where ?T?
is the log-linear model of (1), whichalso includes the feature derived from the CPTMas defined by (4), and ?
is a tuned smoothing fac-tor.Let ?(?)
be a loss function which is differen-tiable w.r.t.
the parameters of the CPTM, ?.
Wecan compute the gradient of the loss and learn ?using gradient-based numerical optimization al-gorithms, such as L-BFGS or stochastic gradientdescent (SGD).5.1 Computing the GradientSince the loss does not explicitly depend on ?, weuse the chain rule for differentiation:??(?)?
?= ???(?)?sim?(??
, ??)?sim?(??
, ??)??(?,?
)= ?
??(?,?)?sim?(??
, ??)??(?,?
)(7)which takes the form of summation over all phrasepairs occurring either in a training sample (sto-chastic mode) or in the entire training data (batchmode).
?(?,?)
in (7) is known as the error term ofthe phrase pair (?, ?
), and is defined as?(?,?)
= ???(?)?sim?(??,??
)(8)It describes how the overall loss changes with thetranslation score of the phrase pair (?, ?).
We willleave the derivation of ?(?,?)
to Section 5.1.2, andwill first describe how the gradient ofsim?(??
, ??)
w.r.t.
?
is computed.5.1.1 Computing ?????(?
?, ??)/?
?Without loss of generality, we use the followingnotations to describe a neural network:?
??
is the projection matrix for the l-th layerof the neural network;?
?
is the input word vector of a phrase;?
??
is the sum vector of the l-th layer; and?
??
= ?(??)
is the output vector of the l-thlayer, where ?
is an activation function;Thus, the CPTM defined by (2) and (3) can be rep-resented as?1 = ?1T?
?1 = ?
(?1)?2 = ?2T?1?2 = ?(?2)sim?(??
, ??)
= (??2)T?
?2The gradient of the matrix ?2 which projects thehidden vector to the output vector is computed as:?sim?(??
, ??)??2=?(??2)T??2?
?2 + (?
?2)T ???2?
?2= ?
?1 (?
?2 ?
??(?
?2))T+ ?
?1 (?
?2 ?
??(?
?2))T(9)where ?
is the element-wise multiplication (Hada-mard product).
Applying the back propagationprinciple, the gradient of the projection matrixmapping the input vector to the hidden vector ?1is computed as?sim?(??
, ??)?
?1= ??
(?2 (?
?2 ?
??(?
?2)) ?
??(??1))T+??
(?2 (?
?2 ?
??(?
?2)) ?
??(?
?1))T(10)The derivation can be easily extended to a neuralnetwork with multiple hidden layers.5.1.2 Computing ?(?,?
)To simplify the notation, we rewrite our loss func-tion of (5) and (6) over one training sample as703?(?)
= ?xBleu(?)
= ?G(?)Z(?)(11)whereG(?)
= ?
sBleu(?, ??)
exp(?T?(?
?, ?, ?))?Z(?)
= ?
exp(?T?(?
?, ?, ?
))?Combining (8) and (11), we have?(?,?)
=?xBleu(?)?sim?(??
, ??)(12)=1Z(?)(?G(?)?sim?(??
, ??)??Z(?)?sim?(??
, ??)xBleu(?
))Because ?
is only relevant to ?
?+1 which is de-fined in (4), we have??T?(?
?, ?, ?)?sim?(??
, ??
)= ??+1???+1(?
?, ?, ?)?sim?(??
, ??
)= ??+1?
(?, ?
; ?)
(13)where ?
(?, ?
; ?)
is the number of times thephrase pair (?, ?)
occurs in ?
.
Combining (12)and (13), we end up with the following equation?(?,?
)= ?
U(?,?)?(?|??)??+1?
(?, ?
; ?)(?,?)????(??
)where  (14)U(?, ?)
= sBleu(?
?, ?)
?
xBleu(?
).5.2 The Training AlgorithmIn our experiments we train the parameters of theCPTM, ?, using the L-BFGS optimizer describedin Andrew and Gao (2007), together with the lossfunction described in (5).
The gradient is com-puted as described in Sections 5.1.
Although SGDhas been advocated for neural network trainingdue to its simplicity and its robustness to localminima (Bengio 2009), we find that in our taskthat the L-BFGS minimizes the loss in a desirablefashion empirically when iterating over the com-plete training data (batch mode).
For example, theconvergence of the algorithm was found to besmooth, despite the non-convexity in our loss.
An-other merit of batch training is that the gradientover all training data can be computed efficiently.As shown in Section 5.1, computing?sim?(x?
, x?)/??
requires large-scale matrixmultiplications, and is expensive for multi-layerneural networks.
Eq.
(7) suggests that?sim?(x?
, x?)/??
and ?(?,?)
can be computedseparately, thus making the computation cost ofthe former term only depends on the number ofphrase pairs in the phrase table, but not the size oftraining data.
Therefore, the training method de-scribed here can be used on larger amounts oftraining data with little difficulty.As described in Section 4, we take three stepsto learn the parameters for both the log-linearmodel of SMT and the CPTM.
While steps 1 and3 can be easily parallelized on a computer cluster,the CPTM training is performed on a single ma-chine.
For example, given a phrase table contain-ing 16M pairs and a 1M-sentence training set, ittakes a couple of hours to generate the N-best listson a cluster, and about 10 hours to train the CPTMon a Xeon E5-2670 2.60GHz machine.For a non-convex problem, model initializationis important.
In our experiments we always initial-ize ?1 using a bilingual topic model trained onparallel data (see detail in Section 6.2), and ?2 asan identity matrix.
In principle, the loss functionof (5) can be further regularized (e.g.
by adding aterm of ?2 norm) to deal with overfitting.
How-ever, we did not find clear empirical advantageover the simpler early stop approach in a pilotstudy, which is adopted in the experiments in thispaper.6 ExperimentsThis section evaluates the CPTM presented ontwo translation tasks using WMT data sets.
Wefirst describe the data sets and baseline setup.Then we present experiments where we comparedifferent versions of the CPTM and previousmodels.6.1 Experimental SetupBaseline.
We experiment with an in-housephrase-based system similar to Moses (Koehn etal.
2007), where the translation candidates arescored by a set of common features includingmaximum likelihood estimates of source giventarget phrase mappings ????(?|?)
and vice versa????(?|?
), as well as lexical weighting estimates???(?|?)
and ???(?|?
), word and phrase penal-ties, a linear distortion feature, and a lexicalizedreordering feature.
The baseline includes a stand-ard 5-gram modified Kneser-Ney language modeltrained on the target side of the parallel corporadescribed below.
Log-linear weights are estimatedwith the MERT algorithm (Och 2003).704Evaluation.
We test our models on two differentdata sets.
First, we train an English to French sys-tem based on the data of WMT 2006 shared task(Koehn and Monz 2006).
The parallel corpus in-cludes 688K sentence pairs of parliamentary pro-ceedings for training.
The development set con-tains 2000 sentences, and the test set containsother 2000 sentences, all from the official WMT2006 shared task.Second, we experiment with a French to Eng-lish system developed using 2.1M sentence pairsof training data, which amounts to 102M words,from the WMT 2012 campaign.
The majority ofthe training data set is parliamentary proceedingsexcept for 5M words which are newswire.
We usethe 2009 newswire data set, comprising 2525 sen-tences, as the development set.
We evaluate onfour newswire domain test sets from 2008, 2010and 2011 as well as the 2010 system combinationtest set, containing 2034 to 3003 sentences.In this study we perform a detailed empiricalcomparison using the WMT 2006 data set, andverify our best models and results using the largerWMT 2012 data set.The metric used for evaluation is case insensi-tive BLEU score (Papineni et al 2002).
We alsoperform a significance test using the Wilcoxonsigned rank test.
Differences are considered statis-tically significant when the p-value is less than0.05.6.2 Results of the CPTMTable 1 shows the results measured in BLEU eval-uated on the WMT 2006 data set, where Row 1 isthe baseline system.
Rows 2 to 4 are the systemsenhanced by integrating different versions of theCPTM.
Rows 5 to 7 present the results of previousmodels.
Row 8 is our best system.
Table 2 showsthe main results on the WMT 2012 data set.CPTM is the model described in Sections 4.As illustrated in Figure 2, the number of the nodesin the input layer is the vocabulary size ?.
Boththe hidden layer and the output layer have 100nodes6.
That is, ?1 is a ?
?
100 matrix and ?2a 100 ?
100  matrix.
The result shows thatCPTM leads to a substantial improvement overthe baseline system with a statistically significantmargin of 1.0 BLEU points as in Table 1.We have developed a set of variants of CPTMto investigate two design choices we made in de-veloping the CPTM: (1) whether to use a linear6 We can achieve slightly better results using more nodes inthe hidden and output layers, say 500 nodes.
But the modelprojection or a multi-layer nonlinear projection;and (2) whether to compute the phrase similarityusing word-word similarities as suggested by e.g.,the lexical weighting model (Koehn et al 2003).We compare these variants on the WMT 2006data set, as shown in Table 1.CPTML (Row 3 in Table 1) uses a linear neuralnetwork to project a word vector of a phrase ?
toa feature vector ?
: ?
?
?(?)
= ?T?, where ?
isa ?
?
100  projection matrix.
The translationscore of a source phrase f and a target phrase e ismeasured as the similarity of their feature vectors.We choose cosine similarity because it works bet-ter than dot product for linear projection.CPTMW (Row 4 in Table 1) computes the phrasesimilarity using word-word similarity scores.
Thisfollows the common smoothing strategy of ad-dressing the data sparseness problem in modelingphrase translations, such as the lexical weightingmodel (Koehn et al 2003) and the word factoredn-gram translation model (Son et al 2012).
Let ?denote a word, and ?
and ?
the source and targetphrases, respectively.
We definesim(?, ?)
=1|?|?
sim?
(?, ?)
+???1|?|?
sim?
(?, ?)??
?where sim?
(?, ?)
(or sim?
(?, ?)
) is the word-phrase similarity, and is defined as a smooth ap-proximation of the maximum functionsim?
(?, ?)=?
sim(?,??)
exp(?sim(?,??))?????
exp(?sim(?,??))???
?training is too slow to perform a detailed study within a rea-sonable time.
Therefore, all the models reported in this paperuse 100 nodes.# Systems WMT test20061 Baseline 33.062 CPTM 34.10?3 CPTML 33.60?
?4 CPTMW 33.25?5 BLTMPR 33.15?6 DPM 33.29?7 MRFP 33.91?8 Comb (2 + 7) 34.39?
?Table 1: BLEU results for the English to Frenchtask using translation models and systems builton the WMT 2006 data set.
The superscripts ?and ?
indicate statistically significant difference(p < 0.05) from Baseline and CPTM, respec-tively.705where sim?
(?, ?)
(or sim?
(?, ?)
) is the word-phrase similarity, and is defined as a smooth ap-proximation of the maximum functionwhere ?
is the tuned smoothing parameter.Similar to CPTM, CPTMW also uses a nonlin-ear projection to map each word (not a phrase vec-tor as in CPTM) to a feature vector.Two observations can be made by comparingCPTM in Row 2 to its variants in Table 1.
First ofall, it is more effective to model the phrase trans-lation directly than decomposing it into word-word translations in the CPTMs.
Second, we seethat the nonlinear projection is able to generatemore effective features, leading to better resultsthan the linear projection.We  also compare the best version of the CPTMi.e., CPTM, with three related models proposedpreviously.
We start the discussion with the re-sults on the WMT 2006 data set in Table 1.Rows 5 and 6 in Table 1 are two state-of-the-art latent semantic models that are originallytrained on clicked query-document pairs (i.e.,clickthrough data extracted from search logs) forquery-document matching (Gao et al 2011).
Toadopt these models for SMT, we view source-tar-get sentence pairs as clicked query-documentpairs, and trained both models using the samemethods as in Gao et al (2011) on the parallel bi-lingual training data described earlier.
Specifi-cally, BTLMPR is an extension to PLSA, and isthe best performer among different versions of theBi-Lingual Topic Model (BLTM) described inGao et al (2011).
BLTM with Posterior Regular-ization (BLTMPR) is trained on parallel trainingdata using the EM algorithm with a constraint en-forcing a source sentence and its paralleled targetsentence to not only share the same prior topic dis-tribution, but to also have similar fractions ofwords assigned to each topic.
We incorporated themodel into the log-linear model for SMT (1) as7 Gao and He (2013) reported results of MRF models withdifferent feature sets.
We picked the MRF using phrase fea-tures only (MRFP) for comparison since we are mainly inter-ested in phrase representation.follows.
First of all, the topic distribution of asource sentence ??
, denoted by ?(?|??)
, is in-duced from the learned topic-word distributionsusing EM.
Then, each translation candidate ?
inthe N-best list GEN(??)
is scored as?(?|??)
= ?
?
?(?|?)?(?|??)?????(??|?)
can be similarly computed.
Finally, thelogarithms of the two probabilities are incorpo-rated into the log-linear model of (1) as two addi-tional features.
DPM is the Discriminative Projec-tion Model described in Gao et al (2011), whichis an extension of LSA.
DPM uses a matrix to pro-ject a word vector of a sentence to a feature vector.The projection matrix is learned on parallel train-ing data using the S2Net alorithm (Yih et al2011).
DPM can be incorporated into the log-lin-ear model for SMT (1) by introducing a new fea-ture ?
?+1 for each phrase pair, which is definedas the cosine similarity of the phrases in the pro-ject space.As we see from Table 1, both latent semanticmodels, although leading to some slight improve-ment over Baseline, are much less effective thanCPTM.Finally, we compare the CPTM with the Mar-kov Random Field model using phrase features(MRFP in Tables 1 and 2), proposed by Gao andHe (2013)7, on both the WMT 2006 and WMT2012 datasets.
MRFp is a state-of-the-art largescale discriminative training model that uses thesame expected BLEU training criterion, whichhas proven to give superior performance across arange of MT tasks recently (He and Deng 2012,Setiawan and Zhou 2013, Gao and He 2013).Unlike CPTM, MRFp is a linear model thatsimply treats each phrase pair as a single feature.Therefore, although both are trained using the# Systems dev news2011 news2010 news2008 newssyscomb20101 Baseline 23.58 25.24 24.35 20.36 24.142 MRFP 24.07?
26.00?
24.90 20.84?
25.05?3 CPTM 24.12?
26.25?
25.05?
21.15??
24.91?4 Comb (2 + 3) 24.46??
26.56??
25.52??
21.64??
25.22?Table 2:   BLEU results for the French to English task using translation models and systems built onthe WMT 2012 data set.
The superscripts ?
and ?
indicate statistically significant difference (p <0.05) from Baseline and MRFp, respectively.706same expected BLEU based objective function,CPTM and MRFp model the translation relation-ship between two phrases from different angles.MRFp estimates one translation score for eachphrase pair explicitly without parameter sharing,while in CPTM, all phrases share the same neuralnetwork that projects raw phrases to the continu-ous space, providing a more smoothed estimationof the translation score for each phrase pair.The results in Tables 1 and 2 show that CPTMoutperforms MRFP on most of the test sets acrossthe two WMT data sets, but the difference be-tween them is often not significant.
Our interpre-tation is that although CPTM provides a bettersmoothed estimation for low-frequent phrasepairs, which otherwise suffer the data sparsity is-sue, MRFp provides a more precise estimation forthose high-frequent phrase pairs.
That is, CPTMand MRFp capture complementary informationfor translation.
We thus combine CPTM andMRFP (Comb in Tables 1 and 2) by incorporatingtwo features, each for one model, into the log-lin-ear model of SMT (1).
We observe that for bothtranslation tasks, accuracy improves by up to 0.8BLEU over MRFP alone (e.g., on the news2008test set in Table 2).
The results confirm thatCPTM captures complementary translation infor-mation to MRFp.
Overall, we improve accuracyby up to 1.3 BLEU over the baseline on bothWMT data sets.7 ConclusionsThe work presented in this paper makes two majorcontributions.
First, we develop a novel phrasetranslation model for SMT, where joint represen-tations are exploited of a phrase in the source lan-guage and of its translation in the target language,and where the translation score of the pair ofsource-target phrases are represented as the dis-tance between their feature vectors in a low-di-mensional, continuous space.
The space is derivedfrom the representations generated using a multi-layer neural network.
Second, we present a newlearning method to train the weights in the multi-layer neural network for the end-to-end BLEUmetric directly.
The training method is based onL-BFGS.
We describe in detail how the gradientin closed form, as required for efficient optimiza-tion, is derived.
The objective function, whichtakes the form of the expected BLEU computedfrom N-best lists, is very different from the usualobjective functions used in most existing architec-tures of neural networks, e.g., cross entropy (Hin-ton et al 2012) or mean square error (Deng et al2012).
We hence have provided details in the der-ivation of the gradient, which can serve as an ex-ample to guide the derivation of neural networklearning with other non-standard objective func-tions in the future.Our evaluation on two WMT data sets showthat incorporating the continuous-space phrasetranslation model into the log-linear frameworksignificantly improves the accuracy of a state-of-the-art phrase-based SMT system, leading to again up to 1.3 BLEU.
Careful implementation ofthe L-BFGS optimization based on the BLEU-centric objective function, together with the asso-ciated closed-form gradient, is a key to the suc-cess.A natural extension of this work is to expandthe model and learning algorithm from shallow todeep neural networks.
The deep models are ex-pected to produce more powerful and flexible se-mantic representations (e.g., Tur et al, 2012), andthus greater performance gain than what is pre-sented in this paper.8 AcknowledgementsWe thank Michael Auli for providing a datasetand for helpful discussions.
We also thank the fouranonymous reviewers for their comments.ReferencesAndrew, G. and Gao, J.
2007.
Scalable trainingof L1-regularized log-linear models.
InICML.Auli, M., Galley, M., Quirk, C. and Zweig, G.2013 Joint language and translation modelingwith recurrent neural networks.
In EMNLP.Bengio, Y.
2009.
Learning deep architectures forAI.
Fundamental Trends Machine Learning,vol.
2, no.
1, pp.
1?127.Bengio, Y., Duharme, R., Vincent, P., and Janvin,C.
2003.
A neural probabilistic languagemodel.
JMLR, 3:1137-1155.Blei, D. M., Ng, A. Y., and Jordan, M. J.
2003.Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3: 993-1022.Collobert, R., Weston, J., Bottou, L., Karlen, M.,Kavukcuoglu, K., and Kuksa, P. 2011.
Naturallanguage processing (almost) from scratch.Journal of Machine Learning Research, vol.12.707Deerwester, S., Dumais, S. T., Furnas, G. W.,Landauer, T., and Harshman, R. 1990.
Index-ing by latent semantic analysis.
Journal of theAmerican Society for Information Science,41(6): 391-407DeNero, J., Gillick, D., Zhang, J., and Klein, D.2006.
Why generative phrase models underper-form surface heuristics.
In Workshop on Statis-tical Machine Translation, pp.
31-38.Deng, L., Yu, D., and Platt, J.
2012.
Scalablestacking and learning for building deep archi-tectures.
In ICASSP.Diamantaras, K. I., and Kung, S. Y.
1996.
Princi-ple Component Neural Networks: Theory andApplications.
Wiley-Interscience.Dumais S., Letsche T., Littman M. and LandauerT.
1997.
Automatic cross-language retrieval us-ing latent semantic indexing.
In AAAI-97Spring Symposium Series: Cross-LanguageText and Speech Retrieval.Ganchev, K., Graca, J., Gillenwater, J., andTaskar, B.
2010.
Posterior regularization forstructured latent variable models.
Journal ofMachine Learning Research, 11 (2010): 2001-2049.Gao, J., and He, X.
2013.
Training MRF-basedtranslation models using gradient ascent.
InNAACL-HLT, pp.
450-459.Gao, J., Toutanova, K., Yih., W-T. 2011.
Click-through-based latent semantic models for websearch.
In SIGIR, pp.
675-684.He, X., and Deng, L. 2012.
Maximum expectedbleu training of phrase and lexicon translationmodels.
In ACL, pp.
292-301.Hinton, G., and Salakhutdinov, R., 2010.
Discov-ering Binary Codes for Documents by Learn-ing Deep Generative Models.
Topics in Cogni-tive Science, pp.
1-18.Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed,A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-yen, P., Sainath, T., and Kingsbury, B., 2012.Deep neural networks for acoustic modeling inspeech recognition.
IEEE Signal ProcessingMagazine, vol.
29, no.
6, pp.
82-97.Hofmann, T. 1999.
Probabilistic latent semanticindexing.
In SIGIR, pp.
50-57.Huang, P-S., He, X., Gao, J., Deng, L., Acero, A.and Heck, L. 2013.
Learning deep structured se-mantic models for web search using click-through data.
In CIKM.Kalchbrenner, N. and Blunsom, P. 2013.
Recur-rent continuous translation models.
In EMNLP.Koehn, P., Hoang, H., Birch, A., Callison-Burch,C., Federico, M., Bertoldi, N., Cowan, B., Shen,W., Moran, C., Zens, R., Dyer, C., Bojar, O.,Constantin, A., and Herbst, E. 2007.
Moses:open source toolkit for statistical machine trans-lation.
In ACL 2007, demonstration session.Koehn, P. and Monz, C. 2006.
Manual and auto-matic evaluation of machine translation be-tween European languages.
In Workshop onStatistical Machine Translation, pp.
102-121.Koehn, P., Och, F., and Marcu, D. 2003.
Statisti-cal phrase-based translation.
In HLT-NAACL,pp.
127-133.Lambert, P. and Banchs, R. E. 2005.
Data inferredmulti-word expressions for statistical machinetranslation.
In MT Summit X, Phuket, Thailand.Li, P., Liu, Y., and Sun, M. 2013.
Recursive auto-encoders for ITG-based translation.
In EMNLP.Liang,P., Bouchard-Cote,A., Klein, D. andTaskar, B.
2006.
An end-to-end discriminativeapproach to machine translation.
In COLING-ACL.Marcu, D., and Wong, W. 2002.
A phrase-based,joint probability model for statistical machinetranslation.
In EMNLP.Mikolov, T., Karafiat, M., Burget, L., Cernocky,J., and Khudanpur, S. 2010.
Recurrent neuralnetwork based language model.
In INTER-SPEECH, pp.
1045-1048.Mikolov, T., Kombrink, S., Burget, L., Cernocky,J., and Khudanpur, S. 2011.
Extensions of re-current neural network language model.
InICASSP, pp.
5528-5531.Mikolov, T. 2012.
Statistical Language Modelbased on Neural Networks.
Ph.D. thesis, BrnoUniversity of Technology.Mikolov, T., Le, Q. V., and Sutskever, H. 2013a.Exploiting similarities among languages formachine translation.
CoRR.
2013;abs/1309.4148.Mikolov, T., Yih, W. and Zweig, G. 2013b.
Lin-guistic Regularities in Continuous Space WordRepresentations.
In NAACL-HLT.Mimno, D., Wallach, H., Naradowsky, J., Smith,D.
and McCallum, A.
2009.
Polylingual topicmodels.
In EMNLP.708Niehues J., Herrmann, T., Vogel, S., and Waibel,A.
2011.
Wider context by using bilingual lan-guage models in machine translation.Och, F. 2003.
Minimum error rate training in sta-tistical machine translation.
In ACL, pp.
160-167.Och, F., and Ney, H. 2004.
The alignment tem-plate approach to statistical machine translation.Computational Linguistics, 29(1): 19-51.Papineni, K., Roukos, S., Ward, T., and Zhu W-J.2002.
BLEU: a method for automatic evaluationof machine translation.
In ACL.Platt, J., Toutanova, K., and Yih, W. 2010.Translingual Document Representations fromDiscriminative Projections.
In EMNLP.Rosti, A-V., Hang, B., Matsoukas, S., andSchwartz, R. S. 2011.
Expected BLEU trainingfor graphs: bbn system description for WMTsystem combination task.
In Workshop on Sta-tistical Machine Translation.Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J.A.
R. 2007.
Smooth bilingual n-gram transla-tion.
In EMNLP-CoNLL, pp.
430-438.Schwenk, H. 2012.
Continuous space translationmodels for phrase-based statistical machinetranslation.
In COLING.Schwenk, H., Rousseau, A., and Mohammed A.2012.
Large, pruned or continuous space lan-guage models on a GPU for statistical machinetranslation.
In NAACL-HLT Workshop on thefuture of language modeling for HLT, pp.
11-19.Setiawan, H. and Zhou, B., 2013.
Discriminativetraining of 150 million translation parametersand its application to pruning.
In NAACL.Socher, R., Huval, B., Manning, C., Ng, A., 2012.Semantic Compositionality through RecursiveMatrix-Vector Spaces.
In EMNLP.Socher, R., Lin, C., Ng, A. Y., and Manning, C. D.2011.
Parsing natural scenes and natural lan-guage with recursive neural networks.
In ICML.Son, L. H., Allauzen, A., and Yvon, F. 2012.
Con-tinuous space translation models with neuralnetworks.
In NAACL-HLT, pp.
29-48.Sundermeyer, M., Oparin, I., Gauvain, J-L.Freiberg, B., Schluter, R. and Ney, H. 2013.Comparison of feed forward and recurrent neu-ral network language models.
In ICASSP, pp.8430?8434.Tur, G, Deng, L., Hakkani-Tur, D., and He, X.,2012.
Towards deeper understanding: deep con-vex networks for semantic utterance classifica-tion.
In ICASSP.Vinokourov,A., Shawe-Taylor,J.
and Cristia-nini,N.
2002.
Inferring a semantic representa-tion of text via cross-language correlation anal-ysis.
In NIPS.Weston, J., Bengio, S., and Usunier, N. 2011.Large scale image annotation: learning to rankwith joint word-image embeddings.
In IJCAI.Wuebker, J., Mauser, A., and Ney, H. 2010.
Train-ing phrase translation models with leaving-one-out.
In ACL, pp.
475-484.Yih, W., Toutanova, K., Platt, J., and Meek, C.2011.
Learning discriminative projections fortext similarity measures.
In CoNLL.Zhang, Y., Deng, L., He, X., and Acero, A.
2011.A novel decision function and the associated de-cision-feedback learning for speech translation.In ICASSP.Zhila, A., Yih, W., Meek, C., Zweig, G. andMikolov, T. 2013.
Combining heterogeneousmodels for measuring relational similarity.
InNAACL-HLT.Zou, W. Y., Socher, R., Cer, D., and Manning, C.D.
2013.
Bilingual word embeddings forphrase-based machine translation.
In EMNLP.709
