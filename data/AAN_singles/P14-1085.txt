Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 902?912,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsHierarchical Summarization:Scaling Up Multi-Document SummarizationJanara Christensen Stephen SoderlandComputer Science & EngineeringUniversity of WashingtonSeattle, USAjanara@cs.washington.edusoderlan@cs.washington.eduGagan Bansal MausamComputer Science & EngineeringIndian Institute of TechnologyDelhi, Indiagaganbansal1993@gmail.commausam@cse.iitd.ac.inAbstractMulti-document summarization (MDS)systems have been designed for short, un-structured summaries of 10-15 documents,and are inadequate for larger documentcollections.
We propose a new approachto scaling up summarization called hierar-chical summarization, and present the firstimplemented system, SUMMA.SUMMA produces a hierarchy of relativelyshort summaries, in which the top levelprovides a general overview and users cannavigate the hierarchy to drill down formore details on topics of interest.
SUMMAoptimizes for coherence as well as cover-age of salient information.
In an AmazonMechanical Turk evaluation, users pref-ered SUMMA ten times as often as flatMDS and three times as often as timelines.1 IntroductionThe explosion in the number of documentson the Web necessitates automated approachesthat organize and summarize large document col-lections on a complex topic.
Existing methodsfor multi-document summarization (MDS) are de-signed to produce short summaries of 10-15 doc-uments.1MDS systems do not scale to data setsten times larger and proportionately longer sum-maries: they either cannot run on large input orproduce a disorganized summary that is difficultto understand.We present a novel MDS paradigm, hierarchi-cal summarization, which operates on large doc-ument collections, creating summaries that orga-nize the information coherently.
It mimics howsomeone with a general interest in a complex topicwould learn about it from an expert ?
first, the ex-pert would provide an overview, and then more1In the DUC evaluations, summaries have a budget of 665bytes and cover 10 documents.Hierarchical Summarization: Scaling Up Multi-DocumentSummarizationbstractFor topics that cover large amounts of in-formation, simple, short summaries are in-sufficient ?
complex topics require moreinformation and more structure to under-stand.
We propose a new approach to scal-ing up summarization called hierarchicalsummarization, and present the first imple-mented system, SUMMA.SUMMA produces a hierarchy of relativelyshort summaries, where the top level pro-vides a general overview and users cannavigate the hierarchy to drill down formore details on topics of interest.
Com-pared to flat multi-document summaries,users prefer SUMMA ten times as oftenand learn just as much, and compared totimelines, users prefer SUMMA three t mesas often and learn more in twice as manycases.1 IntroductionThe explosion in the number of documents overthe Web necessitates automated approaches thatorganize and summarize large document collec-tions on a complex topic.
Existing methods formulti-document summarization (MDS) can handle10-15 documents and create a short flat summary,but are insufficient for large-scale summarization.For large-scale summarization, we need summa-rizers that organize the information coherently andenable personalized interaction with the summaryso that users can explore the various aspects of in-formation in different levels of detail based on in-dividual interest.To this end, we present a novel MDS paradigm,hierarchical summarization.
Hierarchical summa-rization is designed to operate on large documentcollections.
It mimics how someone with a gen-eral interest in a complex topic would learn aboutx1,1x1,2x1,3x4,1x4,2x4,3x9,1x9,2x8,1x8,2x8,3x7,1x7,2x3,1x3,2x3,3x2,1x2,2 x6,1x6,2x5,1x5,2x5,3On Aug 7 1998, car bombsexploded outside US em-bassies in Kenya and Tanza-nia.
Several days later, theUS began investigations intobombings.
The US retali-ated with missile strikes onsuspected terrorist campsin Afghanistan and Sudanon Aug 20.Some questioned the timingof Clinton?s decision to launchstrikes.
On Aug 22, with binLaden having survived thestrikes, the US outlined otherefforts to damage his net-work.
Russia, Sudan, Pakistan,and Afghanistan condemned thestrikes.Clinton proposed meth-ods to inflict financialdamage on bin Laden.Another possibility isfor the United States tonegotiate with the Tal-iban to surrender binLaden.
But diplomatswho have dealt withthe Taliban doubt thatanything could come ofsuch negotiations.Figure 1: An example of a hierarchical summary for the 1998embassy bombings, with one branch of the hierarchy high-lighted.
Each rectangle represents a summary and each xi,jrepresents a sentence within a summary.
The root summaryprovides an overview of the events of August 1998.
When thelast sentence is selected, a more detailed summary of the mis-sile strikes is produced, and when the middle sentence of thatsummary is selected, a more detailed summary bin Laden?sescape is produced.it from an expert ?
first, the expert would givean overview, and then more specific informationabout various aspects.
It has the following novelcharacteristics:i r A hierarchical summary of the 1998 embassybombings.
Each rectangle represents a summary and eachxi,jis a sentenc within a summary.
The root summary pro-vides an overview of the events of August 1998.
When thethird sentence is selected, a more detailed summary of themissile strikes is displayed.
Selecting the second sentence ofthat summary produces a more detailed summary of the US?options.specific information about various aspects.
Hi-erarchical summarization has the following novelcharacteristics:?
The summary is hierarchically organizedalong one or more organizational principlessuch as time, location, entities, or events.?
Each non-leaf summary is associated with aset of child summaries where each gives de-tails of an element (e.g.
sentence) in the par-ent summary.?
A user can navigate within the hierarchicalsummary by clicking on an element of a par-ent summary to view the associated childsummary.For example, given the topic, ?1998 embassybombings,?
the first summary (Figure 1) might902mention that the US retaliated by strikingAfghanistan and Sudan.
The user can click on thisinformation to learn more about these attacks.
Inthis way, the system can present large amounts ofinformation without overwhelming the user, andthe user can tailor the output to their interests.In this paper, we describe SUMMA, the firsthierarchical summarization system for multi-document summarization.2It operates on a corpusof related news articles.
SUMMA hierarchicallyclusters the sentences by time, and then summa-rizes the clusters using an objective function thatoptimizes salience and coherence.We conducted an Amazon Mechanical Turk(AMT) evaluation where AMT workers comparedthe output of SUMMA to that of timelines and flatsummaries.
SUMMA output was judged superiormore than three times as often as timelines, andusers learned more in twice as many cases.
Usersoverwhelmingly preferred hierarchical summariesto flat summaries (92%) and learned just as much.Our main contributions are as follows:?
We introduce and formalize the novel task ofhierarchical summarization.?
We present SUMMA, the first hierarchicalsummarization system, which operates onnews corpora and summarizes over an or-der of magnitude more documents than tra-ditional MDS systems, producing summariesan order of magnitude larger.?
We present a user study which demonstratesthe value of hierarchical summarization overtimelines and flat multi-document summariesin learning about a complex topic.In the next section, we formalize hierarchicalsummarization.
We then describe our methodol-ogy to implement the SUMMA hierarchical sum-marization system: hierarchical clustering in Sec-tion 3 and creating summaries based on that clus-tering in Section 4.
We discuss our experiments inSection 5, related work in Section 6, and conclu-sions in Section 7.2 Hierarchical SummarizationWe propose a new task for large-scale summariza-tion called hierarchical summarization.
Input to ahierarchical summarization system is a set of re-lated documents D and a budget b for each sum-mary within the hierarchy (in bytes, words, or sen-tences).
The output is the hierarchical summaryH , which we define formally as follows.2http://knowitall.cs.washington.edu/summa/Definition A hierarchical summary H of a docu-ment collection D is a set of summaries X orga-nized into a hierarchy.
The top of the hierarchyis a summary X1representing all of D, and eachsummary Xiconsists of summary units xi,j(e.g.the jth sentence of summary i) that point to a childsummary, except at the leaf nodes of the hierarchy.A child summary adds more detail to the infor-mation in its parent summary unit.
The child sum-mary may include sub-events or background andreactions to the event or topic in the parent.We define several metrics in Section 4 fora well-constructed hierarchical summary.
Eachsummary should maximize coverage of salient in-formation; it should minimize redundancy; andit should have intra-cluster coherence as well asparent-to-child coherence.Hierarchical summarization has two importantstrengths in the context of large-scale summariza-tion.
First, the information presented at the startis small and grows only as the user directs it, soas not to overwhelm the user.
Second, each userdirects his or her own experience, so a user inter-ested in one aspect need only explore that sectionof the data without having to view or understandthe entire summary.
The parent-to-child links pro-vide a means for a user to navigate, drilling downfor more details on topics of interest.There are several possible organizing principlesfor the hierarchy ?
by date, by entities, by loca-tions, or by events.
Some organizing principleswill fit the data in a document collection betterthan others.
A system may select different orga-nization for different portions of the hierarchy, forexample, organizing first by location or prominententity and then by date for the next level.3 Hierarchical ClusteringHaving defined the task, we now describethe methodology behind our implementation,SUMMA.
In future work we intend to design asystem that dynamically selects the best organiz-ing principle for each level of the hierarchy.
Inthis first implementation, we have opted for tem-poral organization, since this is generally the mostappropriate for news events.The problem of hierarchical summarization asdescribed in Section 2 has all of the requirementsof MDS, and additional complexities of inducing ahierarchical structure, processing an order of mag-nitude bigger input, generating a much larger out-put, and enforcing coherence between parent and903Hierarchical Clustering Hs1.
.
.sNsj+1.
.
.sNsl+1.
.
.sNsj+1.
.
.slsi+1.
.
.sjs1.
.
.sisk+1.
.
.sis1.
.
.skHierarchical Summary Xx1,1x1,2x1,3x4,1x4,2x8,1x8,2x7,1x7,2x7,3x3,1x3,2x3,3x2,1x2,2x6,1x6,2x5,1x5,2x5,3Figure 2: Examples of a hierarchical clustering and a hier-archical summary, where the input sentences are s 2 S, thenumber of input sentences is N , and the summary sentencesare x 2 X .
The hierarchical clustering determines the struc-ture of the hierarchical summary.hierarchical structure, processing an order of mag-nitude bigger input, generating a much larger out-put, and enforcing coherence between parent andchild summaries.We simplify the problem by decomposing it intotwo steps: hierarchical clustering and summariz-ing over the clustering (see Figure 2 for an exam-ple).
A hierarchical clustering is a tree in which ifa cluster gpis the parent of cluster gc, then eachsentence in gcis also in gp.
This organizes theinformation into manageable, semantically-relatedsections and induces a hierarchical structure overthe input.The hierarchical clustering serves as input to thesecond step ?
summarizing given the hierarchy.The hierarchical summary follows the hierarchi-cal structure of the clustering.
Each node in thehierarchy has an associated flat summary, whichsummarizes the sentences in that cluster.
More-over, the number of sentences in a flat summary isexactly equal to the number of child clusters of thenode, since the user will click a sentence to get tothe child summary.
See Figure 2 for an illustrationof this correspondence.Because we are interested in temporal hierar-chical summarization, we hierarchically cluster allthe sentences in the input documents by time.Unfortunately, neither agglomerative nor divisiveclustering is suitable, since both assume a binarysplit at each node (Berkhin, 2006).
The number ofclusters at each split should be what is most naturalfor the input data.
We design a recursive clusteringalgorithm that automatically chooses the appropri-ate number of clusters at each split.Before clustering, we timestamp all sentences.We use SUTime (Chang and Manning, 2012) tonormalize temporal references, and we parse thesentences with the Stanford parser (Klein andManning, 2003) and use a set of simple heuristicsto determine if the timestamps in the sentence re-fer to the root verb.
If no timestamp is given, weuse the article date.3.1 Temporal ClusteringAfter acquiring the timestamps, we must hierar-chically cluster the sentences into sets that makesense to summarize together.
Since we wish topartition along the temporal dimension, our prob-lem reduces to identifying the best dates at whichto split a cluster into subclusters.
We identify thesedates by looking for bursts of activity.News tends to be bursty ?
many articles on atopic appear at once and then taper out (Kleinberg,2002).
For example, Figure 3 shows the number ofarticles per day related to 1998 embassy bombingspublished in the New York Times (identified usinga key word search).
There were two main events?
on the 7th, the embassies were bombed andon the 20th, US retaliated through missile strikes.The figure shows a correspondence between theseevents and news spikes.Ideal splits for this example would occur justbefore each spike in coverage.
However, whenthere is little differentiation in news coverage, weprefer clusters evenly spaced across time.
We thuschoose clusters C = {c1, .
.
.
, ck} as follows:maximizeCB(C) + ?E(C)(1)where C is a clustering, B(C) is the burstinessof the set of clusters, E(C) is the evenness of theclusters, and ?
is the tradeoff parameter.B(C) =Xc2Cburst(c) (2)burst(c) is the difference in the number of sen-tences published the day before the first date in cand the average number of sentences published onthe first and second date of c:burst(c) =pub(di) + pub(di+1)2  pub(di 1) (3)where d is a date indexed over time, such that djis a day before dj+1, and diis the first date in c.Figure 2: Examples of input and output o hierarchical sum-marization.
The input sentences ar s ?
S, the number ofinput sentences is N , and the summary sentences are x ?
X .child summaries.We simplify the problem by decomposing it intotwo steps: hierarchical clustering and summariz-ing over the clustering (see Figure 2 for an exam-ple).
A hierarchical clustering is a tree in which ifa cluster gpis the parent of cluster gc, then eachsentence in gcis also in gp.
This organizes theinformation into manageable, semantically-relatedsections and induces a hierarchical structure overthe input.The hierarchical clustering serves as input to thesecond step ?
summarizing given the hierarchy.The hierarchical summary follows the hierarchi-cal structure of the clustering.
Each node in thehierarchy has an associated flat summary, whichsummarizes the senten in that cluster.
More-over, the number of sent es in a flat summary isexactly equal to the number of child clusters of thenode, since the user will click a sentence to get tothe child summary.
See Figure 2 for an illustrationof this correspondence.Because we are inter st d in temporal hierar-chical summarization, we hierarchically cluster allthe sentences in the input documents by time.Unfortunately, neither agglomerative nor divisiveclustering is suitable, since both assume a binarysplit at each no e (Berkhin, 2006).
The number ofclusters at each split should be what is most naturalfor the input data.
We design a recursive clusteringalgorithm that automatically chooses the appropri-ate number of clusters at each split.Before clustering, we timestamp all sentences.We use SUTime (Chang and Manning, 2012) tonormalize temporal references, and we parse thesentences with the Stanford parser (Klein andManning, 2003) and use a set of simple heuristicsto determine if the timestamps in the sentence re-fer to the root verb.
If no timestamp is given, weuse the article date.3.1 Temporal ClusteringAfter acquiring the timestamps, we must hierar-chically cluster the sentences into sets that makesense to summarize together.
Since we wish topartition along the temporal dimension, our prob-lem reduces to identifying the best dates at whichto split a cluster into subclusters.
We identify thesedates by looking for bursts of activity.News tends to be bursty ?
many articles on atopic appear at once and then taper out (Kleinberg,2002).
For exampl , Figure 3 show the number ofarticles per day related to the 1998 embassy bomb-ings published in the New York Times (identifiedusing a key word search).
There were two mainevents ?
on the 7th, the embassies were bombedan on the 20th, the US retaliated through mis-sile strikes Th figure shows a correspondencebetween these events and news spikes.Ideal splits for this example would occur justbefore each spike in coverage.
However, whenthere is little differentiation in news coverage, weprefer clusters e enly spaced across time.
We thuschoose clusters C = {c1, .
.
.
, ck} as follows:maximizeCB(C) + ?E(C)(1)where is a clustering, B(C) is the burstinessof the set of clusters, E(C) is the evenness of theclusters, and ?
is the trad off parameter.B(C) =?c?Cburst(c) (2)burst(c) is the difference in the number of sen-tences published the day before the first date in cand the average number of sentences published onthe first a d sec nd date of c:burst(c) =b(di) + pub(di+1)2?
pub(di?1) (3)where d is a date indexed over time, such that djis a day before dj+1, and diis the first date in c.pub(di) is the number of sentences published ondi.
The evenness of the split is measured by:E(C) = minc?Csize(c) (4)where size(c) is the number of dates in cluster c.We perform hierarchical clustering top-down, ateach point solving for Equation 1. ?
was set usinga grid-search over a development set.9046 8 10 12 14 16 18 20 22 2402040Day of MonthNumberofArticles1Figure 3: News coverage by date for the embassy bombingsin Tanzania and Kenya.
There are spikes in the number ofarticles published at the two major events.3.2 Choosing the number of clustersWe cannot know a priori the number of clustersfor a given topic.
However, when the number ofclusters is too large for the given summary budget,the sentences will have to be too short, and whenthe number of clusters is too small, we will not useenough of the budget.
We set the maximum num-ber of clusters kmaxand minimum number of clus-ters kminto be a function of the budget b and theaverage sentence length in the cluster savg, suchthat kmax?
savg?
b and kmin?
savg?
b/2.Given a maximum and minimum number ofclusters, we must determine the appropriate num-ber of clusters.
At each level, we cluster the sen-tences by the method described above and choosethe number of clusters k according to the gapstatistic (Tibshirani et al, 2000).
Specifically, foreach level, the algorithm will cluster repeatedlywith k varying from the minimum to the maxi-mum.
The algorithm will return the k that max-imizes the gap statistic:Gapn(k) = E?n{log(Wk)} ?
log(Wk) (5)where Wkis the score for the clusters computedwith Equation 1, and E?nis the expectation undera sample of size n from a reference distribution.Ideally, the maximum depth of the clusteringwould be a function of the number of sentencesin each cluster, but in our implementation, we setthe maximum depth to three, which works well forthe size of the datasets we use (300 articles).4 Summarizing within the HierarchyAfter the sentences are clustered, we have a struc-ture for the hierarchical summary that dictates thenumber of summaries and the number of sentencesin each summary.
We also have the set of sen-tences from which each summary is drawn.Intuitively, each cluster summary in the hierar-chical summary should convey the most salientinformation in that cluster.
Furthermore, the hier-archical summary should not include redundantsentences.
A hierarchical summary that is onlysalient and nonredundant may still not be suitableif the sentences within a cluster summary are dis-connected or if the parent sentence for a summarydoes not relate to the child summary.
Thus, a hi-erarchical summary must also have intra-clustercoherence and parent-to-child coherence.4.1 SalienceSalience is the value of each sentence to the topicfrom which the documents are drawn.
We measuresalience of a summary (Sal(X)) as the sum of thesaliences of individual sentences (?iSal(xi)).Following previous research in MDS, we com-puted individual saliences using a linear regres-sion classifier trained on ROUGE scores over theDUC?03 dataset (Lin, 2004; Christensen et al,2013).
This method finds those sentences moresalient that mention nouns or verbs that occur fre-quently in the cluster.In preliminary experiments, we noticed thatmany sentences that were reaction sentences weregiven a higher salience than action sentences.
Forexample, the reaction sentence, ?President Clintonvowed to track down the perpetrators behind thebombs that exploded outside the embassies in Tan-zania and Kenya on Friday,?
would have a higherscore than the action sentence, ?Bombs explodedoutside the embassies in Tanzania and Kenya onFriday.?
This problem occurs because the first sen-tence has a higher ROUGE score (it covers moreimportant words than the second sentence).
To ad-just for this problem, we use only words identifiedin the main clause (heuristically identified via theparse tree) to compute our salience scores.4.2 RedundancyWe identify redundant sentences using a linearregression classifier trained on a manually la-beled subset of the DUC?03 sentences.
The fea-tures include shared noun counts, sentence length,TF*IDF cosine similarity, timestamp difference,and features drawn from information extractionsuch as number of shared tuples in Open IE(Mausam et al, 2012).9054.3 Summary CoherenceWe require two types of coherence: coherence be-tween the parent and child summaries and coher-ence within each summary Xi.We rely on the approximate discourse graph(ADG) that was proposed in (Christensen et al,2013) as the basis for measuring coherence.
Eachnode in the ADG is a sentence from the dataset.An edge from sentence sito sjwith positiveweight indicates that sjmay follow siin a coher-ent summary, e.g.
continued mention of an eventor entity, or coreference link between siand sj.A negative edge indicates an unfulfilled discoursecue or co-reference mention.Parent-to-Child Coherence: Users navigate thehierarchical summary from parent sentence tochild summary, so if the parent sentence bears norelation to the child summary, the user will be un-derstandably confused.
The parent sentence musthave positive evidence of coherence with the sen-tences in its child summary.We estimate parent to child coherence as the co-herence between a parent sentence and each sen-tence in its child summary as:PCoh(X) =?c?C?i=1..|Xc|wG+(xpc, xc,i)) (6)where xpcis the parent sentence for cluster c andwG+(xpc, xc,i) is the sum of the positive edgeweights from xpcto xc,iin the ADG G.Intra-cluster Coherence: In traditional MDS, thedocuments are usually quite focused, allowing forhighly focused summaries.
In hierarchical sum-marization, however, a cluster summary may spanhundreds of documents and a wide range of infor-mation.
For this reason, we may consider a sum-mary acceptable even if it has limited positive evi-dence of coherence in the ADG, as long as thereis no negative evidence in the form of negativeedges.
For example, the following is a reasonablesummary for events spanning two weeks:s1Bombs exploded at two US embassies.s2US missiles struck in Afghanistan and Sudan.Our measure of intra-cluster coherence mini-mizes the number of missing references.
Theseare coreference mentions or discourse cues wherenone of the sentences read before (either in an an-cestor summary or in the current summary) con-tain an antecedent:CCoh(X) = ?
?c?C?i=1..|Xc|#missingRef(xc,i) (7)4.4 Objective FunctionHaving estimated salience, redundancy, and twoforms of coherence, we can now put this informa-tion together into a single objective function thatmeasures the quality of a candidate hierarchicalsummary.Intuitively, the objective function should bal-ance salience and coherence.
Furthermore, thesummary should not contain redundant informa-tion and each cluster summary should honor thegiven budget, i.e., maximum summary length b.We treat redundancy and budget as hard con-straints and coherence and salience as soft con-straints.
Lastly, we require that sentences aredrawn from the cluster that they represent and thatthe number of sentences in the summary corre-sponding to each non-leaf cluster c is equivalentto the number of child clusters of c. We optimize:maximize: F (x) , Sal(X) + ?PCoh(X) + ?CCoh(X)s.t.
?c ?
C :?i=1..|Xc|len(xc,i) < b?xi, xj?
X : redundant(xi, xj) = 0?c ?
C, ?xc?
Xc: xc?
c?c ?
C : |Xc| = #children(c)The tradeoff parameters ?
and ?
were set basedon a development set.4.5 AlgorithmOptimizing this objective function is NP-hard, sowe approximate a solution by using beam searchover the space of partial hierarchical summaries.Notice the contribution from a sentence dependson individual salience, coherence (CCoh) basedon sentences visible on the user path down the hi-erarchy to this sentence, and coherence (PCoh)based on its parent sentence and its child sum-mary.
Since most of the sentence contributions de-pend on the path from the root to the sentence, webuild our partial summary by incrementally addinga sentence top-down in the hierarchy and from firstsentence to last within a cluster summary.To account for PCoh, we estimate the contribu-tion of the sentence by jointly identifying its bestchild summary.
However, we do not fix the childsummary at this time ?
we simply use it to estimatePCohwhen using that sentence.
Since computingthe best child summary is also intractable we ap-proximate a solution by a local search algorithmover the child cluster.Overall, our algorithm is a two level nestedsearch algorithm ?
beam search in the outer loop to906search through the space of partial summaries andlocal search (hill climbing with random restarts) inthe inner loop to pick the best sentence to add tothe existing partial summary.
We use a beam ofsize ten in our implementation.5 ExperimentsOur experiments are designed to evaluate how ef-fective hierarchical summarization is in summa-rizing a large, complex topic and how well thishelps users learn about the topic.
Our evaluationaddresses the following questions:?
Do users prefer hierarchical summaries fortopic exploration?
(Section 5.1)?
Are hierarchical summaries more effectivethan other methods for learning about com-plex events?
(Section 5.2)?
How informative are the hierarchical sum-maries compared to the other methods?
(Sec-tion 5.3)?
How coherent is the hierarchical structure inthe summaries?
(Section 5.4)We compared SUMMA against two baseline sys-tems which represent the main NLP methods forlarge-scale summarization: an algorithm for cre-ating timelines over sentences (Chieu and Lee,2004),3and a state-of-the-art flat MDS system(Lin and Bilmes, 2011).4Each system was giventhe same budget (over 10 times the traditionalMDS budget, which is 665 bytes).We evaluated the questions on ten news topics,representing a range of tasks: (1) Pope John PaulII?s death and the 2005 Papal Conclave, (2) Bush v.Gore, (3) the Tulip Revolution, (4) Daniel Pearl?skidnapping, (5) the Lockerbie bombing handoverof suspects, (6) the Kargil War, (7) NATO?s bomb-ing of Yugoslavia in 1999, (8) Pinochet?s arrest inLondon, (9) the 2005 London bombings, and (10)the crash and investigation of SwissAir Flight 111.We chose topics containing a set of related eventsthat unfolded over several months and were promi-nent enough to be reported in at least 300 articles.We drew our articles from the Gigaword corpus,which contains articles from the New York Timesand other major newspapers.
For each topic, weused the 300 documents that best matched a key3Unfortunately, we were unable to obtain more recenttimeline systems from authors of the systems.4(Christensen et al, 2013) is a state-of-the-art coherentMDS system, but does not scale to 300 documents.word search.
We selected topics which were be-tween five and fifteen years old so that evaluatorswould have relatively less pre-existing knowledgeabout the topic.5.1 User PreferenceIn our first experiment, we simply wished to eval-uate which system users most prefer.
We hiredAmazon Mechanical Turk (AMT) workers and as-signed two topics to each worker.
We paired upworkers such that one worker would see outputfrom SUMMA for the first topic and a competingsystem for the second and the other worker wouldsee the reverse.
For quality control, we askedworkers to complete a qualification task first, inwhich they were required to write a short summaryof a news article.
We also manually removed spamfrom our results.
Previous work has used AMTworkers for summary evaluations and has shownhigh correlations with expert ratings (Christensenet al, 2013).
Five workers were hired to view eachtopic-system pair.We asked the workers to choose which formatthey preferred and to explain why.
The results areas follows:SUMMA 76% TIMELINE 24%SUMMA 92% FLAT-MDS 8%Users preferred the hierarchical summariesthree times more often than timelines and overten times more often than flat summaries.
Whenwe examined the reasons given by the users, wefound that the people who preferred the hierar-chical summaries liked that they gave a big pic-ture overview and were then allowed to drill downdeeper.
Some also explained that it was eas-ier to remember information when presented withthe overview first.
Typical responses included,?Could gather and absorb the information at myown pace,?
and, ?Easier to follow and understand.
?When users preferred the timelines, they usuallyremarked that it was more familiar, i.e.
?I likedthe familiarity of the format.
I am used to thesetimelines and they feel comfortable.?
Users com-plained that the flat summaries were disjointed,confusing, and very frustrating to read.5.2 Knowledge AcquisitionEvaluating how much a user learned is inherentlydifficult, more so when the goal is to allow the userthe freedom to explore information based on indi-vidual interest.
For this reason, instead of asking aset of predefined questions, we assess the knowl-907edge gain by following the methodology of (Sha-haf et al, 2012) ?
asking users to write a paragraphsummarizing the information learned.Using the same setup as in the previous exper-iment, for each topic, five AMT workers spentthree minutes reading through a timeline or sum-mary and were then asked to write a descriptionof what they had learned.
Workers were not al-lowed to see the timeline or summary while writ-ing.
We collected five descriptions for each topic-system combination.
We then asked other AMTworkers to read and compare the descriptions writ-ten by the first set of workers.
Each evaluator waspresented with a corresponding Wikipedia articleand descriptions from a pair of users (timeline vs.SUMMA or flat MDS vs. SUMMA).
The descrip-tions were randomly ordered to remove bias.
Theworkers were asked which user appeared to havelearned more and why.
For each pair of descrip-tions, four workers evaluated the pair.
Standardchecks such as approval rating, location filtering,etc.
were used for removing spam.
The results ofthis experiment are as follows:Prefer Indiff.
PreferSUMMA 58% 17% TIMELINE 25%SUMMA 40% 22% FLAT-MDS 38%Descriptions written by workers using SUMMAwere preferred over twice as often as those fromtimelines.
We looked more closely at those caseswhere the participants either preferred the time-lines or were indifferent and found that this pref-erence was most common when the topic was notdominated by a few major events, but was insteada series of similarly important events.
For exam-ple, in the kidnapping and beheading of DanielPearl there were two or three obviously majorevents, whereas in the Kargil War there were manysmaller important events.
In latter cases, the hier-archical summaries provided little advantage overthe timelines because it was more difficult to ar-range the sentences hierarchically.Since SUMMA was judged to be so much supe-rior to flat MDS systems in Section 5.1, it is sur-prising that users descriptions from flat MDS werepreferred nearly as often as those from SUMMA.While the flat summaries were disjointed, theywere good at including salient information, withthe most salient tending to be near the start of thesummary.
Thus, descriptions from both SUMMAand flat MDS generally covered the most salientinformation.5.3 InformativenessIn this experiment, we assess the salience of theinformation captured by the different systems, andthe ability of SUMMA to organize the informationso that more important information is placed athigher levels.ROUGE Evaluation: We first automaticallyassessed informativeness by calculating theROUGE-1 scores of the output of each of the sys-tems.
For the gold standard comparison summary,we use the Wikipedia articles for the topics.5Note that there is no good translation of ROUGEfor hierarchical summarization.
Thus, we simplyuse the traditional ROUGE metric, which willnot capture any of the hierarchical format.
Thisscore will essentially serve as a rough measure ofcoverage of the entire summary to the Wikipediaarticle.
The scores for each of the systems are asfollows:P R F1SUMMA 0.25 0.67 0.31TIMELINE 0.28 0.65 0.33FLAT-MDS 0.30 0.64 0.34None of the differences are significant.
Fromthis evaluation, one can gather that the systemshave similar coverage of the Wikipedia articles.Manual Evaluation: While ROUGE serves as arough measure of coverage, we were interested ingathering more fine-grained information on the in-formativeness of each system.
We performed anadditional manual evaluation that assesses the re-call of important events for each system.We first identified which events were most im-portant in a news story.
Because reading 300 arti-cles per topic is impractical, we asked AMT work-ers to read a Wikipedia article on the same topicand then identify the three most important eventsand the five most important secondary events.
Weaggregated responses from ten workers per topicand chose the three most common primary and fivemost common secondary events.One of the authors then manually identified thepresence of these events in the hierarchical sum-maries, the timelines and the flat MDS summaries.Below we show event recall (the percentage of theevents that were mentioned).5We excluded one topic (the handover of the Lockerbiebombing suspects) because the corresponding Wikipedia ar-ticle had insufficient information.908Events SUMMA TIMELINE FLAT-MDSPrim.
96% 74% 93%Sec.
76% 53% 64%The difference in recall between SUMMA andTIMELINE was significant in both cases, and thedifference between SUMMA and FLAT-MDS wasnot.
In general, the flat summaries were quite re-dundant, which contributed to the slightly lowerevent recall.
The timelines, on the other hand,were both incoherent and at the same time re-ported less important facts.We also evaluated at what level in the hierar-chy the events were identified for the hierarchicalsummaries.
The event recall shows the percentageof events mentioned at that level or above in thehierarchical summary:Events Level 1 Level 2 Level 3Prim.
63% 81% 96%Sec.
27% 51% 76%81% of the primary events are present in the firstor second level, and 76% of the secondary eventsare mentioned by the third level.
While recog-nizing primary events is relatively simple becausethey are repeated frequently, identification of im-portant secondary events often requires externalknowledge.5.4 Parent-to-Child CoherenceWe next tested the hierarchical coherence.
One ofthe authors graded how much each non-leaf sen-tence in a summary was coherent with its childsummary on a scale of one to five, with one be-ing incoherent and five being perfectly coherent.We used the coherence scale from DUC?04.6Level 1 Level 2Coherence 3.8 3.4We found that for the top level of the summary,the parent sentence generally represented the mostimportant event in the cluster and the child sum-mary usually expressed details or reactions of theevent.
The lower coherence scores were often theresult of too few lexical connections or lack of atheme or story.
While the facts of the sentencesmade sense together, the summaries sometimesdid not read as if they were written by a human,but as a series of disparate sentences.For the second level, the problems were morebasic.
The parent sentence occasionally expresseda less important fact that the child summary did6http://duc.nist.gov/duc2004/quality.questions.txtnot then expand on or, more commonly, the childsummary was not focused enough.
This resultstems from two problems in our algorithm.
First,summarizing sentences are rare, making goodchoices for parent sentences difficult to find.
Thesecond problem relates to the difficulty in identify-ing whether two sentences are on the same topic.For example, suppose the parent sentence is, ?ASwissair plane Wednesday night crashed off NovaScotia, Canada.?
A very good child sentence is,?The airline confirmed that all passengers died.
?However, based on their surface features, the sen-tence, ?A plane made an unscheduled landing aftera Swissair plane crashed off the coast of Canada,?appears to be a better choice.Even though there is scope for improvement, wefind these coherence scores encouraging for a firstalgorithm for the task.6 Related WorkTraditional approaches to large-scale summariza-tion have included flat summaries and timelines.There are two primary shortcomings to these ap-proaches: first, they require the user to sortthrough large amounts of potentially overwhelm-ing information, and second, the output is static?
users with different interests will see the sameinformation.
Below we describe related work ontraditional MDS, structured summaries, timelines,discovering threads of documents and the uses ofhierarchies in generating summaries.6.1 Traditional MDSTraditionally, MDS systems have focused on threeto six sentence summaries covering 10-15 docu-ments.
Most extractive summarization researchaims to maximize coverage while reducing redun-dancy (e.g.
(Carbonell and Goldstein, 1998; Sag-gion and Gaizauskas, 2004; Radev et al, 2004)).Lin and Bilmes (2011) proposed a state-of-the-artsystem that uses submodularity in sentence selec-tion to accomplish these goals.
Christensen et al(2013) presented an algorithm for coherent MDS,but it does not scale to larger output.Structured Summaries: Some research has ex-plored generating structured summaries.
Theseapproaches attempt to identify major aspects ofa topic, but do not compile content to describethose aspects.
Rather, they rely on pre-existing, la-beled paragraphs (for example, a paragraph titled,?Symptoms of Meningitis?).
Aspects are identi-fied either by a training corpus of articles in the909same domain (Sauper and Barzilay, 2009), by anentity-aspect LDA model (Li et al, 2010), or byWikipedia templates of related topics (Yao et al,2011).
These methods assume a common struc-ture for all topics in a category, and do not allowfor more than two levels in the structure.Timeline Generation: Recent papers in timelinegeneration have emphasized the relationship withsummarization.
Yan et al (2011b) balanced co-herence and diversity to create timelines, Yan etal.
(2011a) used inter-date and intra-date sentencedependencies, and Chieu and Lee (2004) used sen-tence similarity.
Others have emphasized identify-ing important dates, primarily by bursts of news(Swan and Allen, 2000; Akcora et al, 2010; Huet al, 2011; Kessler et al, 2012).
While time-lines can be useful for understanding events, theydo not generalize to other domains.
Additionally,long timelines can be overwhelming, short time-lines have low information content, and there isno method for personalized exploration.Document Threads: A related track of researchinvestigates discovering threads of documents.While we aim to summarize collections of infor-mation, this track seeks to identify relationshipsbetween documents.
This research operates on thedocument level, while ours operates on the sen-tence level.
Shahaf and Guestrin (2010) formal-ized the characteristics of a good chain of articlesand proposed an algorithm to connect two speci-fied articles.
Gillenwater et al (2012) proposeda probabilistic technique for extracting a diverseset of threads from a given collection.
Shahaf etal.
(2012) extended work on coherent threads tofinding coherent maps of documents, where a mapis set of intersecting threads representing how thethreads interact and relate.Summarization and Hierarchies: A few papershave examined the relationship between summa-rization and hierarchies.
Some focused on cre-ating a hierarchical summary of a single docu-ment (Buyukkokten et al, 2001; Otterbacher etal., 2006), relying on the structure inherent in sin-gle documents.
Others investigated creating hier-archies of words or phrases to organize documents(Lawrie et al, 2001; Lawrie, 2003; Takahashi etal., 2007; Haghighi and Vanderwende, 2009).Other research identifies the hierarchical struc-ture of the documents and generates a summarythat prioritizes more general information accord-ing to the structure (Ouyang et al, 2009; Celikyil-maz and Hakkani-Tur, 2010), or gains coverage bydrawing sentences from different parts of the hier-archy (Yang and Wang, 2003; Wang et al, 2006).7 ConclusionsWe have introduced a new paradigm for large-scale summarization called hierarchical summa-rization, which allows a user to navigate a hier-archy of relatively short summaries.
We presentSUMMA, an implemented hierarchical news sum-marization system,7and demonstrate its effective-ness in a user study that compares SUMMA witha timeline system and a flat MDS system.
Whencompared to timelines, users learned more withSUMMA in twice as many cases, and SUMMA waspreferred more than three times as often.
Whencompared to flat summaries, users overwhelmingpreferred SUMMA and learned just as much.This first implementation performs temporalclustering ?
in future work, we will investigate dy-namically selecting an organizing principle that isbest suited to the data at each level of the hierar-chy: by entity, by location, by event, or by date.We also intend to scale the system to even largerdocument collections, and explore joint clusteringand summarization.
Lastly, we plan to researchhierarchical summarization in other domains.AcknowledgmentsWe thank Amitabha Bagchi, Niranjan Balasubra-manian, Danish Contractor, Oren Etzioni, TonyFader, Carlos Guestrin, Prachi Jain, Lucy Van-derwende, Luke Zettlemoyer, and the anonymousreviewers for their helpful suggestions and feed-back.
We thank Hui Lin and Jeff Bilmes forproviding us with their code.
This research wassupported in part by ARO contract W911NF-13-1-0246, DARPA Air Force Research Labora-tory (AFRL) contract FA8750-13-2-0019, UW-IITD subcontract RP02815, and the Yahoo!
Fac-ulty Research and Engagement Award.
This pa-per is also supported in part by the IntelligenceAdvanced Research Projects Activity (IARPA)via AFRL contract number FA8650-10-C-7058.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.The views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the official policiesor endorsements, either expressed or implied, ofIARPA, AFRL, or the U.S. Government.7http://knowitall.cs.washington.edu/summa/910ReferencesC.
G. Akcora, M. A. Bayir, M. Demirbas, and H. Fer-hatosmanoglu.
2010.
Identifying breakpoints inpublic opinion.
In 1st KDD Workshop on Social Me-dia Analytics.Berkhin Berkhin.
2006.
A survey of clustering datamining techniques.
Grouping MultidimensionalData, pages 25?71.Orkut Buyukkokten, Hector Garcia-Molina, and An-dreas Paepcke.
2001.
Seeing the whole in parts:Text summarization for web browsing on handhelddevices.
In Proceedings of WWW 2001, pages 652?662.Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedingsof SIGIR 1998, pages 335?336.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.
A hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of ACL 2010, pages 815?824.Angel Chang and Christopher Manning.
2012.
SU-Time: A library for recognizing and normalizingtime expressions.
In Proceedings of LREC 2012.Hai Leong Chieu and Yoong Keok Lee.
2004.
Querybased event extraction along a timeline.
In Proceed-ings of SIGIR 2004, pages 425?432.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2013.
Towards coherent multi-document summarization.
In Proceedings ofNAACL 2013.Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.2012.
Discovering diverse and salient threads indocument collections.
In Proceedings of EMNLP-CoNLL 2012, pages 710?720.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-document summariza-tion.
Proceedings of NAACL 2009, pages 362?370.Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K.Usadi, and Xiaoyan Zhu.
2011.
Generatingbreakpoint-based timeline overview for news topicretrospection.
In Proceedings of ICDM 2011.Remy Kessler, Xavier Tannier, Caroline Hag`ege,V?eronique Moriceau, and Andr?e Bittar.
2012.
Find-ing salient dates for building thematic timelines.
InProceedings of ACL 2012, pages 730?739.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Meeting of the Association for ComputationalLinguistics, pages 423?430.Jon Kleinberg.
2002.
Bursty and hierarchical struc-ture in streams.
In Proceedings of the Eighth ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?02, pages 91?101.Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.2001.
Finding topic words for hierarchical summa-rization.
In Proceedings of SIGIR ?01, pages 349?357.Dawn J. Lawrie.
2003.
Language models for hierar-chical summarization.
Ph.D. thesis, University ofMassachusetts Amherst.Peng Li, Jing Jiang, and Yinglin Wang.
2010.
Gener-ating templates of entity summaries with an entity-aspect model and pattern mining.
In Proceedings ofACL 2010, pages 640?649.Hui Lin and Jeff Bilmes.
2011.
A class of submodularfunctions for document summarization.
In Proceed-ings of ACL 2011, pages 510?520.Chin-Yew Lin.
2004.
ROUGE: A package for au-tomatic evaluation of summaries.
In Text Summa-rization Branches Out: Proceedings of the ACL-04Workshop, pages 74?81.Mausam, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open languagelearning for information extraction.
In Proceedingsof EMNLP 2012, pages 523?534.Jahna Otterbacher, Dragomir Radev, and Omer Ka-reem.
2006.
News to go: Hierarchical text sum-marization for mobile devices.
In Proceedings ofSIGIR 2006, pages 589?596.You Ouyang, Wenji Li, and Qin Lu.
2009.
Anintegrated multi-document summarization approachbased on word hierarchical representation.
In Pro-ceedings of the ACLShort 2009, pages 113?116.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Information Processingand Management, 40(6):919?938.Horacio Saggion and Robert Gaizauskas.
2004.
Multi-document summarization by cluster/profile rele-vance and redundancy removal.
In Proceedings ofDUC 2004.Christina Sauper and Regina Barzilay.
2009.
Automat-ically generating Wikipedia articles: A structure-aware approach.
In Proceedings of ACL 2009, pages208?216.Dafna Shahaf and Carlos Guestrin.
2010.
Connectingthe dots between news articles.
In Proceedings ofKDD 2010, pages 623?632.Dafna Shahaf, Carlos Guestrin, and Eric Horvitz.2012.
Trains of thought: Generating informationmaps.
In Proceedings of WWW 2012.911Russell Swan and James Allen.
2000.
Automatic gen-eration of overview timelines.
In Proceedings of SI-GIR 2000, pages 49?56.Kou Takahashi, Takao Miura, and Isamu Shioya.
2007.Hierarchical summarizing and evaluating for webpages.
In Proceedings of the 1st workshop onemerging research opportunities for Web Data Man-agement (EROW 2007).Robert Tibshirani, Guenther Walther, and TrevorHastie.
2000.
Estimating the number of clusters ina dataset via the gap statistic.
Journal of the RoyalStatistical Society, Series B, 32(2):411?423.Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi.2006.
Multi-document summarization for terrorisminformation extraction.
In Proceedings of ISI?06.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li, and Yan Zhang.
2011a.
Timeline gen-eration through evolutionary trans-temporal summa-rization.
In Proceedings of EMNLP 2011, pages433?443.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011b.
Evolutionarytimeline summarization: A balanced optimizationframework via iterative substitution.
In Proceedingof SIGIR 2011, pages 745?754.Christopher C. Yang and Fu Lee Wang.
2003.
Fractalsummarization: summarization based on fractal the-ory.
In Proceedings of SIGIR 2003, pages 391?392.Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, FengZhou, and Hongyan Liu.
2011.
Autopedia: Auto-matic domain-independent wikipedia article genera-tion.
In Proceedings of WWW 2011, pages 161?162.912
