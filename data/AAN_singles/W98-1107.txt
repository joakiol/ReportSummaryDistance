Semantic Lexicon Acquisition for LearningNatural Language InterfacesCynth ia  A .
Thompson and  Raymond J .
MooneyDepar tment  of Computer  SciencesUniversity of TexasAust in,  TX 78712cthomp@cs.utexas.edu,  mooney@cs.utexas.eduAbst rac tThis paper describes a system, WOLFIE (WOrd LearningFrom Interpreted Examples), that acquires a semanticlexicon from a corpus of sentences paired with represen-tations of their meaning.
The lexicon learned consists ofwords paired with meaning representations.
WOLFIE ispart of an integrated system that learns to parse novelsentences into semantic representations, such as logicaldatabase queries.
Experimental results are presenteddemonstrating WOLFIE'S ability to learn useful exiconsfor a database interface in four different natural lan-guages.
The lexicons learned by WOLFIE are comparedto those acquired by a comparable system developed bySiskind (1996).1 In t roduct ion  &: Overv iewThe application of learning methods to natural-languageprocessing (NLP) is a growing area.
Using machinelearning to help automate the construction of NLP sys-tems can eliminate much of the difficulty of building such .systems by hand.
The semantic lexicon, or the mappingfrom words to meanings, is one component that is typ-ically challenging and time consuming to construct andupdate by hand, as noted by Copestake t al.
(1995) andWalker and Amsler (1986).
In addition, new lexicons areneeded when transferring a system to new applicationsor domains.
Johnston et al (1995) also discuss the needfor systems that can learn the meanings of novel words.This paper describes a system, WOLFIZ (WOrd Learn-ing From Interpreted Examples), that learns a seman-tic lexicon of word/meaning pairs from input consistingof sentences paired with semantic representations.
Thegoal of this research is to automate l xicon constructionfor an integrated NLP system that acquires emanticparsers and lexicons.
A subgoal is to learn a lexicon thatis as good or better than a manually-built one based onperformance on a chosen task.Although a few others (Siskind, 1996; Hastings andLytinen, 1994; Brent, 1991) have presented systems forsemantic lexicon acquisition, this work is unique in com-bining several features.
First, interaction with a system,CHILL (Zelle, 1995), that learns to parse sentences intotheir semantic representations, i  demonstrated.
Sec-ond, it uses a fairly simple batch, greedy algorithm thatis quite fast and accurate.
Third, it is easily extendibleto new representation formalisms.
Finally.
it is able robootstrap from an existing lexicon.We tested WOLFIE on its ability to acquire a semanticlexicon for the task of answering eographical databasequeries, using a corpus of queries collected from humansubjects and annotated by an expert with their exe-cutable logical form.
To perform this test.
WOLFIE wasintegrated with CHILL, which learns parsers but requiresa semantic lexicon (previously built manually).
The re-sults demonstrate hat the final acquired system per-forms nearly as accurately at answering novel questionswhen using a learned lexicon as compared to a hand-builtlexicon.
The system is also compared to an alternativelexicon acquisition system developed by Siskind (1996),demonstrating superior performance on this task.
Fi-nally, we translated the corpus from English into Span-ish, Japanese, and Turkish and ran experiments on learn-ing database interfaces with these languages as well.Overall, the results demonstrate a robust ability toacquire accurate lexicons to be directly used for se-mantic parsing.
Because we have developed this inte-grated system, the task of building a semantic pars-ing system for a new domain is simplified.
One nowonly needs to build one representative corpus of sen-tence/representation pairs for the new domain.
This onecorpus allows the acquisition of both a semantic lexiconand a semantic parser that can together process thatcorpus.2 BackgroundThe output produced by WOLFm can be used to assista larger language acquisition system; in particular, it iscurrently used as part of the input to a parser acquisitionsystem called CHILL (Constructive Heuristics Inductionfor Language Learning).
CHILL uses inductive logic pro-gramming (Muggleton, 1992; Lavra~ and D~eroski, 1994)to learn a deterministic shift-reduce parser written in57<Sentence, Representation>; Training~.
Examples JIIICHILL I < Ii IIPrologfWOLFIEi<Phrase, ,Meaning>Figure 1: The Integrated SystemProlog.
The input to CHILL is a corpus of sentencespaired with semantic representations, the same input re-quired by WOLFm.
The parser learned is capable ofmapping the sentences into their correct representations,as well as generalizing well to novel sentences.CHILL requires a lexicon as background knowledge inorder to learn to parse into deeper semantic representa-tions.
By using WOLFIE, the lexicon can be providedautomatically, easing the task of parser acquisition.
Fig-ure 1 illustrates the inputs and outputs of the completesystem.
The output of WOLFIE is a lexicon of (phrase,meaning} pairs; these aspects will be discussed morethoroughly in the following sections.One of the components of CHILL is 311 initial ow~rly-general parser, used to analyze the training data.
Thisinitial parser is specialized by the learner to generateonly correct parses for the training examples.
Given acorrect lexicon, the overly-general parser should be ableto parse all of the training examples.In this paper, we limit our discussion of CHmL to itsability to learn parsers that map natural-language ques-tions directly into Prolog queries that can be executed toproduce an answer (Zelle and Mooney, 1996).
Followingare two sample queries for a database on U.S. Geographypaired with their corresponding Prolog query:What is the capital of the state with the biggestpopulation?answer(C, (capital(S,C), l a rgestCP ,(state(S), population(S,P))))).What state is Texarkana located in?answer(S, (state (S),eq(C, cityid (texarkana, 3 ),l oc  (C, S ) ) ) .Given a sufficient corpus of such sentence/representationpairs, CHILL iS able to learn a parser that correctly parsesmany novel sentences into logical queries.CHILL treats parser induction as a problem of learn-ing rules to control the actions of the shift-reduce parsermentioned above.
During parsing, the current context iscontained in the contents of a stack and a buffer contain-ing the remaining input.
When parsing is complete, thestack contains the representation f the input sentence.There are three types of operators used by the parserto construct logical queries.
One is the introductiononto the stack of a predicate needed in the sentencerepresentation, due to the appearance a phrase at thefront of the input buffer.
The semantic lexicon asso-ciates phrases and their representations for use by thistype of operator.
A second type of operator unifies vari-ables appearing in stack items.
For example, in the firstrepresentation f a sample query given above, the firstargument of answer is unified with the second argumentof cap i ta l .
Finally, a stack item may be embeddedinto the argument of another stack item, as is requiredfor the first sentence/representation pair given above, toembed s ta te(_ )  and popu lat ion(_ , _ )  into the secondargument of la rgest .In sum, we concentrate on using machine learningmethods to build a system for processing sentences ina narrow domain, but with the goal of obtaining deepsemantic representations.
This is in contrast o work inthis general area that attempts to process broader cor-pora, but only obtains shallow representations as a resultof processing.3 The  Semant ic  Lex icon  Acqu is i t ionProb lemWe now define the learning problem at hand.
Given a setof sentences, each consisting of an ordered list of wordsand annotated with a single semantic representation, weassume that each representation can be f rac tured  intoall of its components (Siskind, 1992).
The fracturingmethod depends upon the given representation a d mustbe explicitly provided or implicit in the algorithm thatforms hypotheses for word meanings.
Given a valid setof components, they can be constructed into a valid sen-tence meaning using a relation we will call compose.The goal is to find a semantic lexicon that will as-sist parsing.
Such a lexicon consists of (phrase, mean-ing) pairs, where the phrases and their meanings areextracted from the input sentences and their represen-tations, respectively, such that each sentence's represen-tation can be composed from a set of components eachchosen from the possible meanings of a (unique) phraseappearing in the sentence.
If such a lexicon is found, wesay that the lexicon covers the corpus.
We will also talkabout the coverage of components of a representation (or58sentence/representation pair)by a lexicon entry.
Ideally,we would like to minimize the ambiguity and size of thelearned lexicon, since this should ease the parser acqui-sition task.
Note that this notion of semantic lexiconacquisition is distinct from work on learning selectionalrestrictions (Manning, 1993; Brent, 1991) and learningclusters of semantically similar words (Riloff and Shep-erd, 1997).Note that we allow phrases to have multiple mean-ings (homonymy) and for multiple phrases to have thesame meaning (synonymy).
Also, some phrases in thesentences may have a null meaning.
We make only afew fairly straightforward assumptions about the input.First is compositionality, i.e.
the meaning of a sentence iscomposed  from the meanings of phrases in that sentence.Since we allow multi-word phrases in the lexicon (e.g.
(\[kick the bucket\] ,  die(_))), this assumption seemsfairly unproblematic.
Second, we assume each compo-nent of the representation is due to the meaning of aword or phrase in the sentence, not to an external sourcesuch as noise.
Third, we assume the meaning for eachword in a sentence appears only once in the sentence'srepresentation.
The second and third assumptions arepreliminary, and we are exploring methods for relaxingthem.
If any of these assumptions are violated, we donot guarantee coverage of the training corpus; however,the system can still be run and learn a potentially usefullexicon.4 The WOLFIE Algor i thm and anExampleIn order to limit search, a greedy algorithm is usedto learn phrase meanings.
At each step, the bestphrase/meaning pair is chosen, according to a heuris-tic described below, and added to the lexicon.
The ini-tial list of candidate meanings for a phrase is formedby finding the common substructure between sampledpairs of representations of sentences in which the phraseappears.
1 In the current implementation, phrases arelimited to at most two words.
This is for efficiencyreasons only, and in the future we hope to incorporatean efficient method for including potentially meaningfulphrases of more than two words.The WOLFIE algorithm, outlined in Figure 2, has beenimplemented to handle two kinds of semantic represen-tations.
One is a case-role meaning representation basedon conceptual dependency (Schank, 1975}.
For example,the sentence "The man ate the cheese" is represented by:\[ingest, agent : \[person, sex:male, age : adult\],1We restrict ourselves to a sampled pairs instead of all pairsbecause this provides enough information to get good initial can-didate meanings.
Using all pairs is possible but not generallynecessary.For each phrase (of at most two words):1) Sample the examples in which the phrase appears2) Find largest common subexpressions of pairs ofrepresentations from these examplesUntil the input representations are covered, or there areno remaining candidate pairs do:1) Add the best phrase/meaning pair to the lexicon.2) Constrain meanings of phrases occurring in thesame sentences as the phrase just learnedReturn the lexicon of learned phrase/meaning pairs.Figure 2: WOLFIE Algorithm Overviewpat ient  : \ [ food ,  type:  cheese \ ]  \].
Experiments in thisdomain were presented in Thompson (1995).The second representation handled is the logical queryrepresentation illustrated earlier, and is the focus of thecurrent paper.
To find the common substructure be-tween pairs of query representations, we use a methodsimilar to finding the Least General Generalization offirst-order clauses (Plotkin, 1970).
However.
instead ofusing subsumption to guide generalization, we find theset of largest common substructures that two representa-tions share.
For example, given the two queries from Sec-tion 2, the (unique} common substructure is s ta te  (_).2One of the key ideas of the algorithm is that eachphrase/meaning choice can constrain the candidatemeanings of phrases yet to be learned.
This is the sec-ond step of the loop in Figure 2.
Such constraints existbecause of the assumption that each portion of the rep-resentation is due to at most one phrase in the sentence.?
Therefore, once part of a sentence's representation is cov-ered by the meaning of one of its phrases, no other phrasein the sentence has to be paired with that meaning (forthat sentence}.For example, assume we have the sentence/rep-resentation pairs in Section 2, plus the additional pair:What is the highest point of the state with thebiggest area?answer(P, (high-point (S,P) ,largest(A, (state(S),area(S,A))))).As a simplification, assume sentences are stripped ofphrases that we know a priori have a null meaning (al-though in general this is not required}.
In the exam-pie sentences, these phrases are \[what\], \[is\], \[with\],and \[the\].
From these three examples, the mean-ing of \[state\], the only phrase common to all sen-tences, is determined to be s ta te (_ ) ,  which is theonly predicate the three representations have in com-mon.
Before determining this, the candidate meaning for~Since CHILL initializes the pv.r~e stack with the ansv6r predi-cate, it is first stripped from the input given to WOLFIE.59\[biggest\] is \[largest(_, state(_))\] (the largest sub-structure shared by the representations of the two sen-tences containing "biggest").
However, since s ta te (_ )is now covered by (\[state\], s ta te (_ ) ) ,  it can be elim-inated from consideration as part of the meaninl~ of\[biggest\], and the candidate meaning for \[biggest\] be-comes \ [ la rgest  (_,_)\].We now describe the algorithm in more detail.
Thefirst step is to select a random sample of the sentencesthat each one and two word phrase appears in, and de-rive an initial set of candidate meanings for each phrase.This is done by deriving common substructure betweenpairs of representations of sentences that contain thesephrases.
For example, let us suppose we have the follow-ing pairs as input:What is the capital of the state with the biggestpopulation?answer (C, (capital (S,C),largest (P, (state (S), population(S, P) ) ) ) ).What is the highest point of the state with thebiggest area?ansver (P, (high-point (S, P),largest(A, (state(S), area(S,A))))).What state is Texarkana located in?ansver ($, (statue (S),eq(C, cityid (tezarkana,_)), loc (C, S) ) ).What is the area of the United States?answer (A, (area (C, A), eq (C, countryid (usa)) ) ).What is the population of the states borderingMinnesota?answer(P, (population(S,P), s tate(S) ,next .
to  (S, H), eq(M, state id (minnesota)  ) ).The sets of initial candidate meanings for some of thephrases in this corpus are:\[biggest\]: \ [ la rgest  (_, s ta te (_ ) ) \ ] ,\[state\]: \ [ s ta te  (_), l a rgest  (_, s ta te  (J)\],\[area\]: \[area(_)\],\ [populat ion\] :  [ (popu lat ion(_ ,_ ) ,  s ta te (_ ) ) \ ] ,\[capital\]: \ [ ( cap i ta l  (S,_),l a rgest (P ,  (state(S), population(S,P))))\].Note that \[state\] has two candidate meanings, each gen-erated from a different pair of representations of sen-tences in which it appears.
A detail is that for phrasesthat only appear in one sentence, we use the entire rep-resentation of the sentence in which they appear as aninitial candidate meaning.
An example in this corpus is\[capital\].
As we will see, this type of pair typically hasa low score, so the meaning will usually get pared downto just the correct portion of the representation, if any.Finally, if a phrase is ambiguous, the pairwise matchingsto generate candidate items, together with the constrain-ing of representations: would enable multiple meaningsto be learned for it.After deriving these initial meanings, the greedysearch begins.
The heuristic used to evaluate candidateshas five weighted components:1.
Ratio of the number of times the phrase appearswith the meaning to the number of times the phraseappears, or P(meaninglphrase ).2.
Ratio of the number of times the phrase appearswith the meaning to the number of times the mean-ing appears, or P(phraselmeaning ).3.
Frequency of the phrase, or P(phrase~.4.
Percent of orthographic overlap between the phraseand its meaning.5.
The generality of the meaning.The first measure helps reduce ambiguity (homonymy)by preferring phrases that indicate a particular meaningwith high probability.
The second measure helps reduce%monymy by favoring pairs in which the meaning ap-pears with few other phrases.
The third measure is usedbecause frequent phrases are more likely to be pairedwith a correct meaning since we have more informationabout the representations of sentences in which they ap-pear.The fourth measure is useful in some domains sincesometimes phrases have many characters in commonwith their meanings, as in a rea  and area(_) .
It mea-sures the maximum number of consecutive characters incommon between the phrase and the terms and predi-cates in the meanings, as an average of the percent ofboth the number of characters in the phrase and in theterm and predicate names.
However, as we will demon-strate in our experiments, the use of this portion of theheuristic is not required to learn useful lexicons.The final measure, generality, measures the number ofterms and predicates in the meaning.
Preferring a mean-ing with fewer terms helps evenly distribute the predi-cates in a sentence's representation among the meaningsof the phrases in that sentence and thus leads to a lexi-con that is more likely to be correct.
To see this, we notethat some words are likely to co-occur with one another,and so their joint representation (meaning) is likely tobe in the list of candidate meanings for both words.
Bypreferring a more general meaning, we more easily ig-nore these incorrect joint meanings.
In the candidateset above for example, if all else were equal, the general-ity portion of the heuristic would prefer s ta te (_ )  overla rgest  (_, s ta te  (_)) as the meaning of s ta te .For purposes of this example, we will use a weight of50 for each of the first four parameters, and a weight of 860for the last.
The first four components have smaller val-ues than the last, so they have higher weights.
Resultsare not overly-sensitive to the heuristic weights.
Au-tomatically setting the weights using cross-validation onthe training set (Kohavi and John, 1995) had little effecton overall performance.
In all of the experiments, thesesame weights were used.
To break ties, we choose less"ambiguous" phrases first and learn short phrases beforelonger ones.
A phrase is considered more ambiguous ifit currently has more meanings in the partially learnedlexicon.The heuristic measure for the above six pairs is:\[\[biggest\], la..rgest (_,state(_))/: 50(2/2) + 50(2/2) +50(2/21) + 50((4/12 + 4/7)/2)- 8.2  = 111\[\[area\], area(_)\]: 50(2/2) + 50(2/2) + 50(2/21) +50((4/4+ 4 /4) /2 ) -  8 .1  = 147\[\[state\], state(_) \ ] :  50(3/3) + 50(3/4) + 50(3/21) +50( (5 /5  + 5 /5 ) /2 )  - s ?
1 = 13~\[\[state\].
l a rgest ( _ , s ta te (_ ) ) \ ] :  110\[\[population\],  (popu la t ion( : , _ ) ,  s ta te (_ ) ) / :  130\[\[capital\], ( cap i ta l (S , _ ) ,  l a rgest  (P,( s ta te (S) ,  popu la t ion(S ,P ) ) ) ) \ ] :  101The best pair by our measure is (\[area\], area(_)) ,  so itis added to the lexicon.The next step in the algorithm is to constrain the re-maining candidate meanings for the learned phrase, ifany, so as to only consider sentences for which no mean-ing has yet been learned for the phrase.
In our exam-ple, the learned pair covers all occurrences of \[area\], sothere are no remaining meanings that need to be con-strained.
Next, for the remaining unlearned phrases,their candidate meanings are constrained to take intoaccount the meaning just learned, as was discussed atthe beginning of this section.
In our example, learning\[area\] would not affect any of the meanings listed above,but the  next best pair, \[\[state\], s tate(_ ) / ,  would con-strain the (only) candidate meaning for \ [populat ion  lto become populat ion(_ ,_) ,  the candidate meaningfor \[capital\] to become (cap i ta l (S , _ ) ,  l a rgest (P ,popu la t ion(S ,p ) ) ) ,  and the candidate meaning for\[biggest I o become la rgest  (_,_).
The greedy searchcontinues until the lexicon covers the training corpus.A detail of the search not yet mentioned is to check ifcovered sentence/representation pairs can be parsed byCHILL'S overly-general parser.
If this is not the case, weknow that some phrase in the sentence has a meaningthat is not useful to CHILL.
Therefore, whenever a sen-tence is covered, we check whether it can be parsed.
Ifnot, we retract he most recently learned pair, and adjustthat phrase's candidate meanings to omit that meaning.We call this the parsability heuristic.5 Experimental ResultsThis section describes our experimental results on adatabase query application.
The corpus contains 250questions about U.S. geography paired with logical rep-resentations.
This domain was chosen due to the avail-ability of an existing hand-built natural language inter-face, Geobase, to a simple geography database containingabout 800 facts.
This interface was supplied with TurboProlog 2.0 (Borland International, 1988), and was de-signed specifically for this domain.
The questions werecollected from uninformed undergraduates and mappedinto their logical form by an exp, r .
Examples from thecorpus were given in the previous ections.
To broadenthe test, we had the same 250 sentences translated intoSpanish, Turkish~ and Japanese.
The Japanese transla-tions are in word-segmented Roman orthography.
Trans-lated questions were paired with the appropriate logicalqueries from the English corpus.To evaluate the learned lexicons, we measured theirutility as background knowledge for CHILL.
This is per-formed by choosing a random set of 25 test examplesand then creating lexicons and parsers using increasinglylarger subsets of the remaining 225 examples.
The testexamples are parsed using the learned parser, the result-ing queries ubmitted to the database, the answers com-pared to those generated by the correct representation,and the percentage ofcorrect answers recorded.
By mak-ing a comparison to the "gold standard" of retrieving acorrect answer to the original query, we avoid measuresof partial accuracy which do not give a picture of the realusefulness of the parser.
To improve the statistical sig-nificance of the results, we repeated the above steps forten different random splits of the data into training andtest sets.
For all significance tests we used a two-tailed,paired t-test and a significance level of p < 0.05.We compared our system to that developed by Siskind(1996).
Siskind's system is an on-line (incremental)learner, while ours is batch.
To make a closer com-parison between the two, we ran his in a "simulated"batch mode, by repeatedly presenting the corpus 500times, analogous to running 500 epochs to train a neu-ral network.
We also made comparisons to the parserslearned by CHILL when using a hand-coded lexicon asbackground knowledge.
This lexicon was available forthis domain because when CHILL was originally devel-oped, WOLFIE had not yet been developed.In this application, there are many terms, such as stateand city names, whose meanings are easily extractedfrom the database.
Therefore, all tests below were runwith such names given to the learner as an initial lexicon,although this is not required for learning in general.i7OSO4030201000i io .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
o ~  / .
.
.
.
~.~,L- .
.
a -  .
.
.
.
.
.
.
m~ /  i i i iso loo 1~ 2OOYr J~r~ Ex~p~,q,sZ~706O5O2O100,i" i /  G ecII:wi~.li =i i i l5O 100 150 200Tr Wrlln 0 E.xa~plesFigure 3: Accuracy on English Geography Corpus Figure 4: Accuracy Given Closed Class Words5.1 Comparisons using EnglishThe first experiment was a comparison of the two sys-tems on the original English corpus.
However, sinceSiskind has no measure of orthographic overlap, and itcould arguably give our system an unfair advantage onthis data, we ran WOLFIE with a weight of zero for thiscomponent.
We also did not use the parsability heuris-tic for this test.
By making these adjustments, we at-tempted to generate the fairest head-to-head comparisonbetween the two systems.Figure 3 shows learning curves for CHILL when usingthe lexicons learned by WOLFIE (CHILL+WOLFIE) andby Siskind's system (CHILL+Siskind).
The uppermostcurve (CHILL+corrlex) is CHILL'S performance whengiven the hand-built lexicon.
Finally, the horizontal lineshows the performance of the Geobase benchmark.
Theresults show that a lexicon learned by WOLFIE led toparsers that were almost as accurate as those generatedusing a hand-buih lexicon.
The best accuracy isachievedby the hand-built lexicon, followed by WOLFIE followedby Siskind's ystem.
All the systems do as well or betterthan Geobase by 225 training examples.
The differencesbetween WOLFIE and Siskind's system are statisticallysignificant at 25 and 175 examples.
These results showthat WOLFZZ can learn lexicons that lead to successfullearning of parsers, and that are somewhat better fromthis perspective than those learned by a competing sys-tem.As noted above, these tests were run with only themeaning of database constants provided as backgroundknowledge.
Next, we examined the effect of also provid-ing closed-class words as background knowledge.
Fig-ure 4 shows the resulting learning curves.
For thesetests, we also show the advantage ofadding both the or-thographic overlap and parsability heuristics to WOLFIZ(CHILL-fullWOLFIE).
Both the additional backgrou-knowledge and the improved heuristic increase the oreall performance a couple of percentage points.
The dferences between Siskind's ystem and WOLFIE withoparsing or overlap are statistically significant at 75, 17and 225 examples.
Finally, we noted that Siskind's 3tern run in batch mode on this test averaged 54.8% at 2"examples, versus non-batch mode which attained 49.6accuracy, giving evidence that batch mode does improhis system.One of the implicit hypotheses ofour approach is thcoverage of the training pairs implies a good lexicon.can compare the coverage of WOLFIE'S lexicons to thoof Siskind's and verify that WOLFm's have better coerage.
For the first experiment above, WOLFIE cover100% of the 225 training examples, while Siskind coered 94.4%.
For the second experiment, he coveragwere 100% and 94.5%, respectively.
This may accoufor some of the performance difference between the t,~systems.Further differences may be explained by the percerage of training examples usable by CHmh, which is t.percentage parsable by its overly-general parser.
For t.first experiment, CHILl., could parse 93.7% of the 225 eamples when given the lexicons learned by WOLFIE bonly 78% of the examples when given lexicons learnby Siskind's ystem.
When the lexicon learners are givclosed class words, these percentages rise to 98.1% a..84.6%, respectively.
In addition, the lexicons learnby Siskind's system were more ambiguous than tholearned by WOLFIE.
WOLFm'S lexicons had 1.1 meaings per word for the second experiment (after 225 traiing examples) versus 1.7 meanings per word in Siskinclexicons.
These differences most likely contribute to t.differences seen in the generalization accuracy of CHIT_627O6O5O403O20100_- ....
.
:S- 71: : : :7 : -o//" tat'"~ .
/" .:"?
/, d"': ,I I I I50 100 ,SO 200Tr e~;'~ E,i~'Jple~2~Figure 5: Accuracy on SpanishThe ability to learn multiple-word phrases is not a signif-icant source of the advantage of WOLFIE over Siskind'ssystem, since only 2% of the lexicon entries learned byWOLFIE On average contained two-word phrases.5.2 Comparisons using SpanishNext.
we examined the performance of the two systemson the Spanish version of the corpus.
We again omittedorthographic overlap and the parsability heuristic.
Fig-ure .5 shows the results.
In these tests, we also gave closedclass words to the lexicon learners as background knowl-edge, since these results were slightly better for English.Though the performance ompared to a hand-built lex-icon is not quite as close as in English, the accuracy ofthe parser using the learned lexicon is very similar.5.3 Accuracy on Other LanguagesWe also had the geography query sentences translatedinto Japanese and Turkish, and ran similar tests to deter-mine how well WOLFIE could learn lexicons for these lan-guages, and how well CHILL could learn to parse them.Figure 6 shows the results.
For all four of these tests,we used the parsability heuristic, but did not give thelearner access to the closed class words of any of thelanguages.
We also set the weight of the orthographicoverlap heuristic to zero for all four languages, ince thisgives little advantage in the foreign languages.
The per-formance differences among the four languages are quitesmall, demonstrating that our methods are not languagedependent.6 Re la ted  WorkPedersen and Chen (1995) describe a method for acquir-ing syntactic and semantic features of an unknown word.They assume access to an initial concept hierarchy, and7060403020'0, Sp.tr l~ ~- .
/l ;aaa//7I I I IFigure 6: Accuracy on All Four Languages2:~,Odo not present any experimental results.
Many systems(Fukumoto and Tsujii, 1995; Haruno, 1995; Johnstonet al, 1995; Webster and Marcus, 1995) focus only onacquisition of verbs or nouns, rather than all types ofwords.
Also, these either do not experimentally evalu-ate their systems, or do not show the usefulness of thelearned lexicons.
Manning (1993) and Brent (1991) ac-quire subcategorization information for verbs.
Finally,several systems (Knight, 1996; Hastings and Lytinen.1994; Russell, 1993) learn new words from context, as-suming that a large initial lexicon and parsing systemare available.Tishby and Gorin (Tishby and Gorin, 1994) learn asso-ciations between words and actions (as meanings of thosewords).
Their system was tested on a corpus of sentencespaired with representations but they do not demonstratethe integration of learning a semantic parser using thelearned lexicon.The aforementioned work by Siskind is the closest.His approach is somewhat more general in that it han-dles noise and referential uncertainty (multiple possiblemeanings for a sentence), while ours is specialized forapplications where a single meaning is available.
Theexperimental results in the previous ection demonstratethe advantage ofour method for such an application.
Hissystem does not currently handle multiple-word phrases.Also, his system operates in an incremental or on-linefashion, discarding each sentence as it processes it, whileours is batch.
While he argues for psychological p ausi-bility, we do not.
In addition, his search for word mean-ings is most analogous to a version space search, whileours is a greedy search.
Finally, and perhaps most signif-icantly, his system does not compute statistical correla-tions between words and their possible meanings, whileours does.63His system proceeds in two stages, first learning whatsymbols are part of a word's meaning, and then learningthe structure of those symbols.
For example, it mightfirst learn that cap i ta l  is part of the meaning of capi-tal, then in the second stage learn that cap i ta l  can haveeither one or two arguments.
By using common sub-structures, we can combine these two stages in WOLFIE.This work also has ties to the work on automaticconstruction of translation lexicons (Wu and Xia, 1995;Melamed, 1995; Kumano and Hirakawa, 1994; Catizoneet al, 1993: Gale and Church, 1991).
While most ofthese methods also compute association scores betweenpairs (in their case, word/word pairs) and use a greedyalgorithm to choose the best translation(s) for each word,they do not take advantage of the constraints betweenpairs.
One exception is Melamed (1996); however, hisapproach does not allow for phrases in the lexicon or forsynonymy within one text segment, while ours does.7" Future  WorkAlthough the current greedy search method has per-formed quite well, a better search heuristic or alterna-tive search strategy could result in improvements.
Amore important issue is lessening the burden of build-ing a large annotated training corpus.
We are exploringtwo options in this regard.
One is to use active learning(COhn et al, 1994) in which the system chooses whichexamples are most usefully annotated from a larger cor-pus of unannotated data.
This approach can dramati-cally reduce the amount of annotated ata required toachieve a desired accuracy (Engelson and Dagan, 1996).Second, we are currently developing a corpus of sen-tences paired with SQL database queries.
Extendingour system to handle this representation should be afairly simple matter.
Such corpora should be easily con-structed by recording queries ubmitted to existing SQLapplications along with their original English forms, ortranslating existing lists of SQL queries into English(presumably an easier direction to translate).
The factthat the same training data can be used to learn both asemantic lexicon and a parser also helps limit the overallburden of constructing a complete NL interface.On a separate note, the learning algorithm may beapplicable to other domains, such as learning for trans-lation or diagnosis.
We hope to investigate these possi-bilities in the future as well.8 Conc lus ionsAcquiring a semantic lexicon from a corpus of sen-tences labeled with representations of their meaning isan important problem that has not been widely studied.WOLFIE demonstrates that a fairly simple greedy sym-bolic learning algorithm performs fairly well on this taskand obtains performance superior to a previous lexiconacquisition system on a corpus of geography queries.
Ourresults also demonstrate hat our methods extend to avariety of natural anguages besides English.Most experiments in corpus-based natural languagehave presented results on some subtask of natural an-guage, and there are few results on whether the learnedsubsystems can be successfully integrated to build a com-plete NLP system.
The experiments presented in thispaper demonstrated how two learning systems.
WOLFIEand CHILL were successfully integrated to learn a com-plete NLP system for parsing database queries into exe-cutable logical form given only a single corpus of anno-tated queries.9 AcknowledgementsWe would like to thank Jeff Siskind for providing us withhis software, and for all his help in adapting it for usewith our corpus.
This research was supported by theNational Science Foundation under grants IRI-9310819and IRI-9704943.
Thanks also to Agapito Sustaita.
EsraErdem, and Marshall Mayberry for their translation ef-forts.Re ferencesBorland International.
1988.
Turbo Prolog 2.0 ReferenceGuide.
Borland International, Scotts Valley, CA.M.
Brent.
1991.
Automatic acquisition of subcategoriza-tion frames from untagged text.
In Proceedings of the29th Annual Meeting of the Association for Computa-tional Linguistics, pages 209-214.R.
Catizone, G. Russell, and S. Warwick.
1993.
Derivingtranslation data from bilingual texts.
Ill Proceedingsof the First International Lexical Acquisition Work-shop.D.
Cohn, L. Atlas, and R. Ladner.
1994.
Improvinggeneralization with active learning.
Machine Learn-ing, 15(2):201-221.A.
Copestake, T. Briscoe, P. Vossen, A. Ageno,I.
Castellon, F. Ribas, G. Rigan, H. Rodrlguez, andA.
Samiotou.
1995.
Acquisition of lexical translationrelations from MRDS.
Machine Translation, 9.S.
Engelson and I. Dagan.
1996.
Minimizing manual an-notation cost in supervised training from corpora.
InProceedings of the 34th Annual Meeting of the Associ-ation for Computational Linguistics, Santa Cruz, CA.Fumiyo Fukumoto and Jun'ichi Tsujii.
1995.
Represen-tation and acquisition of verbal polysemy.
In Papersfrom the 1995 AAAI Symposium on the Representa-tion and Acquisition of Lezical Knowledge: Polysemy,Ambiguity, and Generativity, pages 39-44, Stanford,CA, March.W.
Gale and K. Church.
1991.
Identifying word cor-respondences in parallel texts.
In Proceedings of theFourth DARPA Speech and Natural Language Work-shop.Masahiko Haruno.
1995.
A case frame learning methodfor Japanese polysemous verbs.
In Papers from the641995 AAAI Symposium on the Representation a d Ac-quisition of Lexical Knowled9e: Polyserny, Ambiguity,and Generativity, pages 45-50, Stanford, CA, March.P.
Hastings and S. Lytinen.
1994.
The ups and downsof lexical acquisition.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence, pages7.54-759.M.
Johnston, B. Boguraev, and J. Pustejovsky.
1995.The acquisition and interpretation of complex nomi-nals.
In Papers from the I995 AAAI Symposium onthe Representation a d Acquisition of Lexical Knowl-edge: Polyserny, Ambiguity, and Generativity, pages69-74, Stanford.
CA.Kevin Nnight.
1996.
Learning word meanings by in-struction.
In Proceedings of the Thirteenth NationalConference on Artificial Intelligence, pages 447-454,Portland, Or, August.R.
Kohavi andG.
John.
1995.
Automatic parameter se-lection by minimizing estimated error.
In Proceedingso/the Twelfth International Conference on MachineLearning, pages 304-312, Tahoe City, CA.A.
Kumano and H. Hirakawa.
1994.
Building an MTdictionary from parallel texts based on linguistic andstatistical information.
In Proceedings of the FifteenthInternational Conference on Computational Linguis-tics.N.
Law'at and S. D~eroski.
1994.
Inductive Logic Pro-9ramming: Techniques and Applications.
Ellis Hor-wood.Christopher D. Manning.
1993.
Automatic acquisitionof a large subcategorization dictionary from corpora.In Proceedings of the 31st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 235-242,Columbus, Ohio.I.D.
Melamed.
1995.
Automatic evaluation and uniformfilter cascades for inducing n-best ranslation lexicons.In Proceedings of the Third Wor~hop on Very LargeCorpora.I.D.
Melamed.
1996.
Automatic construction of cleanbroad-coverage translation lexicons.
In Second Con-ference of the Association for Machine Translation inthe Americas.S.
H. Muggleton, editor.
1992.
Inductive Logic Prograrn-ming.
Academic Press, New York, NY.Ted Pedersen and Weidong Chen.
1995.
Lexical acqui-sition via constraint solving.
In Papers from the 1995AAAI Symposium on the Representation a d Acquisi-tion off Lezical Knowledge: Polysemy, Ambiguity, andGenerativity, pages 118---122, Stanford, CA.G.
D. Plotkin.
1970.
A note on inductive generalization.In B. Meltzer and D. Michie, editors, Machine Intelli-gence (Vol.
5).
Elsevier North-Holland, New York.E.
Riloff and K. Sheperd.
1997.
A corpus-based ap-proach for building semantic lexicons.
In Proceedingsof the Second Conference on Empirical Methods inNatural Language Processing, pages 117-124, Provi-dence, Rhode Island.D.
Russell.
1993.
Language Acquisition in a Unification-Based Grammar Processing System Using a RealWorld Knowledge Base.
Ph.D. thesis, University ofIllinois, Urbana, IL.R.
C. Schank.
1975.
Conceptual Information Processing.North-Holland, Oxford.Jeffrey M. Siskind.
1992.
Naive Physics, Event Per-ception, Lezical Semantics and Language Acquisition.Ph.D.
thesis, Department of Electrical Engineeringand Computer Science, Massachusetts Institute ofTechnology, Cambridge, IVlA, December.Jeffrey M. Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1):39-91, October.Cynthia A. Thompson.
199.5.
Acquisition of a lexiconfrom semantic representations of sentences.
In Pro-ceedings of the 33rd Annual Meeting of the Associ-ation for Computational Linguistics, pages 335-337.Cambridge, MA.N.
Tishby and A. Gorin.
1994.
Algebraic learning ofstatistical associations for language acquisition.
Com-puter Speech and Langua9 e, 8:51-78.D.
Walker and R. Amsler.
1986.
The use of machine-readable dictionaries in sublanguage analysis.
InR.
Grishman and R. Kittredge, editors, A r,'zlyz-ing Language in Restricted Domains, pages 69-83.Lawrence Erlbaum Associates, Hillsdale, NJ.Mort Webster and Mitch Marcus.
1995.
Automatic ac-quisition of the lexical semantics of verbs from sen-tence frames.
In Proceedings of the 27th Annual Meet-ing of the Association for Computational Linguistics.Dekai Wu and Xuanyin Xia.
1995.
Large-scale auto-matic extraction of an English-Chinese translation lex-icon.
Machine Translation, 9(3-4):285-313.J.
M. Zelle and R. J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic programming.In Proceedings of the Thirteenth National Conferenceon Artificial Intelligence, Portland, OR, August.J.
M. Zelle.
1995.
Using Inductive Logic Programrnin 9to Automate the Construction of Natural LanguageParsers.
Ph.D. thesis, Department of Computer Sci-ences, University of Texas, Austin, TX, August.
Alsoappears as Artificial Intelligence Laboratory TechnicalReport AI 96-249.65
