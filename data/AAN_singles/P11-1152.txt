Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1516?1525,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIntegrating history-length interpolation and classes in language modelingHinrich Schu?tzeInstitute for NLPUniversity of StuttgartGermanyAbstractBuilding on earlier work that integrates dif-ferent factors in language modeling, we view(i) backing off to a shorter history and (ii)class-based generalization as two complemen-tary mechanisms of using a larger equivalenceclass for prediction when the default equiv-alence class is too small for reliable estima-tion.
This view entails that the classes in alanguage model should be learned from rareevents only and should be preferably appliedto rare events.
We construct such a modeland show that both training on rare events andpreferable application to rare events improveperplexity when compared to a simple directinterpolation of class-based with standard lan-guage models.1 IntroductionLanguage models, probability distributions overstrings of words, are fundamental to many ap-plications in natural language processing.
Themain challenge in language modeling is to estimatestring probabilities accurately given that even verylarge training corpora cannot overcome the inherentsparseness of word sequence data.
One way to im-prove the accuracy of estimation is class-based gen-eralization.
The idea is that even though a particularword sequence s may not have occurred in the train-ing set (or too infrequently for accurate estimation),the occurrence of sequences similar to s can help usbetter estimate p(s).Plausible though this line of reasoning is, the lan-guage models most commonly used today do notincorporate class-based generalization.
This is par-tially due to the additional cost of creating classesand using classes as part of the model.
But anequally important reason is that most models thatintegrate class-based information do so by way of asimple interpolation and achieve only a modest im-provement in performance.In this paper, we propose a new type of class-based language model.
The key novelty is that werecognize that certain probability estimates are hardto improve based on classes.
In particular, the bestprobability estimate for frequent events is often themaximum likelihood estimator and this estimator ishard to improve by using other information sourceslike classes or word similarity.
We therefore design amodel that attempts to focus the effect of class-basedgeneralization on rare events.Specifically, we propose to employ the samestrategy for this that history-length interpo-lated (HI) models use.
We define HI modelsas models that interpolate the predictions ofdifferent-length histories, e.g., p(w3|w1w2) =?1(w1w2)p?
(w3|w1w2) + ?2(w1w2)p?
(w3|w2) +(1 ?
?1(w1w2) ?
?2(w1w2))p?
(w3) where p?
is asimple estimate; in this section, we use p?
= pML,the maximum likelihood estimate, as an example.Jelinek-Mercer (Jelinek and Mercer, 1980) andmodified Kneser-Ney (Kneser and Ney, 1995)models are examples of HI models.HI models address the challenge that frequentevents are best estimated by a method close to max-imum likelihood by selecting appropriate values forthe interpolation weights.
For example, if w1w2w3is frequent, then ?1 will be close to 1, thus ensur-ing that p(w3|w1w2) ?
pML(w3|w1w2) and that thecomponents pML(w3|w2) and pML(w3), which areunhelpful in this case, will only slightly change thereliable estimate pML(w3|w1w2).1516The main contribution of this paper is to proposethe same mechanism for class language models.
Infact, we will use the interpolation weights of a KNmodel to determine how much weight to give to eachcomponent of the interpolation.
The difference to aKN model is merely that the lower-order distributionis not the lower-order KN distribution (as in KN),but instead an interpolation of the lower-order KNdistribution and a class-based distribution.
We willshow that this method of integrating history interpo-lation and classes significantly increases the perfor-mance of a language model.Focusing the effect of classes on rare events hasanother important consequence: if this is the rightway of using classes, then they should not be formedbased on all events in the training set, but only basedon rare events.
We show that doing this increasesperformance.Finally, we introduce a second discountingmethod into the model that differs from KN.
Thiscan be motivated by the fact that with two sourcesof generalization (history-length and classes) moreprobability mass should be allocated to these twosources than to the single source used in KN.
Wepropose a polynomial discount and show a signifi-cant improvement compared to using KN discount-ing only.This paper is structured as follows.
Section 2discusses related work.
Section 3 reviews the KNmodel and introduces two models, the Dupont-Rosenfeld model (a ?recursive?
model) and a top-level interpolated model, that integrate the KNmodel (a history interpolation model) with a classmodel.
Section 4 details our experimental setup.Results are presented in Section 5.
Based on ananalysis of strengths and weaknesses of Dupont-Rosenfeld and top-level interpolated models, wepresent a new polynomial discounting mechanismthat does better than either in Section 6.
Section 7presents our conclusions.2 Related workA large number of different class-based models havebeen proposed in the literature.
The well-knownmodel by Brown et al (1992) is a class sequencemodel, in which p(u|w) is computed as the prod-uct of a class transition probability and an emissionprobability, p(g(u)|g(w))p(u|g(u)), where g(u) isthe class of u.
Other approaches condition the prob-ability of a class on n-grams of lexical items (as op-posed to classes) (Whittaker and Woodland, 2001;Emami and Jelinek, 2005; Uszkoreit and Brants,2008).
In this work, we use the Brown type ofmodel: it is simpler and has fewer parameters.
Mod-els that condition classes on lexical n-grams could beextended in a way similar to what we propose here.Classes have been used with good results in anumber of applications, e.g., in speech recognition(Yokoyama et al, 2003), sentiment analysis (Wie-gand and Klakow, 2008), and question answering(Momtazi and Klakow, 2009).
Classes have alsobeen shown to improve the performance of exponen-tial models (Chen, 2009).Our use of classes of lexical n-grams for n > 1has several precedents in the literature (Suhm andWaibel, 1994; Kuo and Reichl, 1999; Deligne andSagisaka, 2000; Justo and Torres, 2009).
The nov-elty of our approach is that we integrate phrase-levelclasses into a KN model.Hierarchical clustering (McMahon and Smith,1996; Zitouni and Zhou, 2007; Zitouni and Zhou,2008) has the advantage that the size of the class tobe used in a specific context is not fixed, but can bechosen at an optimal level of the hierarchy.
There isno reason why our non-hierarchical flat model couldnot be replaced with a hierarchical model and wewould expect this to improve results.The key novelty of our clustering method is thatclusters are formed based on rare events in the train-ing corpus.
This type of clustering has been appliedto other problems before, in particular to unsuper-vised part-of-speech tagging (Schu?tze, 1995; Clark,2003; Reichart et al, 2010).
However, the impor-tance of rare events for clustering in language mod-eling has not been investigated before.Our work is most similar to the lattice-based lan-guage models proposed by Dupont and Rosenfeld(1997).
Bilmes and Kirchhoff (2003) generalizelattice-based language models further by allowingarbitrary factors in addition to words and classes.We use a special case of lattice-based language mod-els in this paper.
Our contributions are that we intro-duce the novel idea of rare-event clustering into lan-guage modeling and that we show that the modifiedmodel performs better than a strong word-trigram1517symbol denotation?
[[w]] ?w (sum over all unigrams w)c(wij) count of wijn1+(?wij) # of distinct w occurring before wijTable 1: Notation used for Kneser-Ney.baseline.3 ModelsIn this section, we introduce the three models thatwe compare in our experiments: Kneser-Ney model,Dupont-Rosenfeld model, and top-level interpola-tion model.3.1 Kneser-Ney modelOur baseline model is the modified Kneser-Ney(KN) trigram model as proposed by Chen and Good-man (1999).
We give a comprehensive descriptionof our implementation of KN because the detailsare important for the integration of the class modelgiven below.
We use the notation in Table 1.We estimate pKN on the training set as follows.pKN(w3|w21) =c(w31) ?
d???(c(w31))?
[[w]] c(w21w)+?3(w21)pKN(w3|w2)?3(w21) =?
[[w]] d???(c(w21w))?
[[w]] c(w21w)pKN(w3|w2) =n1+(?w32) ?
d??(n1+(?w32))?
[[w]] n1+(?w2w)+?2(w2)pKN(w3)?2(w2) =?
[[w]] d??(n1+(?w2w))?
[[w]] n1+(?w2w)pKN(w3) ={ n1+(?w3)?d?(n1+(?w3))?
[[w]] n1+(?w) if c(w3) > 0?1 if c(w3) = 0?1 =?
[[w]] d?(n1+(?w))?
[[w]] n1+(?w)The parameters d?, d?
?, and d???
are the discountsfor unigrams, bigrams and trigrams, respectively, asdefined by Chen and Goodman (1996, p. 20, (26)).Note that our notation deviates from C&G in thatthey use the single symbol D1 for the three differentvalues d?
(1), d??
(1), and d???
(1) etc.3.2 Dupont-Rosenfeld modelHistory-interpolated models attempt to find a goodtradeoff between using a maximally informative his-tory for accurate prediction of frequent events andgeneralization for rare events by using lower-orderdistributions; they employ this mechanism recur-sively by progressively shortening the history.The key idea of the improved model we will adoptis that class generalization ought to play the samerole in history-interpolated models as the lower-order distributions: they should improve estimatesfor unseen and rare events.
Following Dupont andRosenfeld (1997), we implement this idea by lin-early interpolating the class-based distribution withthe lower order distribution, recursively at eachlevel.
For a trigram model, this means that we in-terpolate pKN(w3|w2) and pB(w3|w1w2) on the firstbackoff level and pKN(w3) and pB(w3|w2) on thesecond backoff level, where pB is the (Brown) classmodel (see Section 4 for details on pB).
We call thismodel pDR for Dupont-Rosenfeld model and defineit as follows:pDR(w3|w21) =c(w31) ?
d???(c(w31))?
[[w]] c(w21w)+ ?3(w21)[?1(w21)pB(w3|w21)+(1 ?
?1(w21))pDR(w3|w2)]pDR(w3|w2) =n1+(?w32) ?
d??(n1+(?w32))?
[[w]] n1+(?w2w)+ ?2(w2)[?2(w2)pB(w3|w2)+(1 ?
?2(w2))pDR(w3)]where ?i(v) is equal to a parameter ?i if the history(w21 or w2) is part of a cluster and 0 otherwise:?i(v) ={?i if v ?
B2?
(i?1)0 otherwiseB1 (resp.
B2) is the set of unigram (resp.
bigram) his-tories that is covered by the clusters.
We cluster bi-gram histories and unigram histories separately andwrite pB(w3|w1w2) for the bigram cluster model andpB(w3|w2) for the unigram cluster model.
Cluster-ing and the estimation of these two distributions aredescribed in Section 4.1518The unigram distribution of the Dupont-Rosenfeld model is set to the unigram distributionof the KN model: pDR(w) = pKN(w).The model (or family of models) defined byDupont and Rosenfeld (1997) is more general thanour version pDR.
Most importantly, it allows a trulyparallel backoff whereas in our model the recursivebackoff distribution pDR is interpolated with a classdistribution pB that is not backed off.
We prefer thisversion because it makes it easier to understand thecontribution that unique-event vs. all-event classesmake to improved language modeling; the parame-ters ?
are a good indicator of this effect.An alternative way of setting up the Dupont-Rosenfeld model would be to interpolatepKN(w3|w1w2) and pB(w3|w1w2) etc ?
but this isundesirable.
The strength of history interpolation isthat estimates for frequent events are close to ML,e.g., pKN(share|cents a) ?
pML(share|cents a) forour corpus.
An ML estimate is accurate for largecounts and we should not interpolate it directlywith pB(w3|w1w2).
For pDR, the discount d???
thatis subtracted from c(w1w2w3) is small relative toc(w1w2w3) and therefore pDR ?
pML in this case(exactly as in pKN).3.3 Top-level interpolationClass-based models are often combined with othermodels by interpolation, starting with the work byBrown et al (1992).
Since we cluster both unigramsand bigrams, we interpolate three models:pTOP(w3|w1w2)= ?1(w1w2)pB(w3|w1w2) + ?2(w2)pB(w3|w2)+ (1 ?
?1(w1w2) ?
?2(w2))pKN(w3|w1w2)where ?1(w1w2) = ?1 if w1w2 ?
B2 and 0 other-wise, ?2(w2) = ?2 if w2 ?
B1 and 0 otherwise and?1 and ?2 are parameters.
We call this the top-levelmodel pTOP because it interpolates the three modelsat the top level.
Most previous work on class-basedmodel has employed some form of top-level inter-polation.4 Experimental SetupWe run experiments on a Wall Street Journal (WSJ)corpus of 50M words, split 8:1:1 into training, val-idation and test sets.
The training set contains256,873 unique unigrams and 4,494,222 unique bi-grams.
Unknown words in validation and test setsare mapped to a special unknown word u.We use the SRILM toolkit (Stolcke, 2002) forclustering.
An important parameter of the class-based model is size |Bi| of the base set, i.e., the totalnumber of n-grams (or rather i-grams) to be clus-tered.
As part of the experiments we vary |Bi| sys-tematically to investigate the effect of base set size.We cluster unigrams (i = 1) and bigrams (i = 2).For all experiments, |B1| = |B2| (except in caseswhere |B2| exceeds the number of unigrams, see be-low).
SRILM does not directly support bigram clus-tering.
We therefore represent a bigram as a hyphen-ated word in bigram clustering; e.g., Pan Am is rep-resented as Pan-Am.The input to the clustering is the vocabulary Biand the cluster training corpus.
For a particular baseset size b, the unigram input vocabulary B1 is set tothe b most frequent unigrams in the training set andthe bigram input vocabulary B2 is set to the b mostfrequent bigrams in the training set.In this section, we call the WSJ training corpusthe raw corpus and the cluster training corpus thecluster corpus to be able to distinguish them.
Werun four different clusterings for each base set size(except for the large sets, see below).
The clustercorpora are constructed as follows.?
All-event unigram clustering.
The clustercorpus is simply the raw corpus.?
All-event bigram clustering.
The cluster cor-pus is constructed as follows.
A sentence of theraw corpus that contains s words is includedtwice, once as a sequence of the ?s/2?
bigrams?w1?w2 w3?w4 w5?w6 .
.
.
?
and once as asequence of the ?
(s ?
1)/2?
bigrams ?w2?w3w4?w5 w6?w7 .
.
.
?.?
Unique-event unigram clustering.
The clus-ter corpus is the set of all sequences of two un-igrams ?
B1 that occur in the raw corpus, onesequence per line.
Each sequence occurs onlyonce in this cluster corpus.?
Unique-event bigram clustering.
The clustercorpus is the set of all sequences of two bi-grams ?
B2 that occur in the training corpus,1519one sequence per line.
Each sequence occursonly once in this cluster corpus.As mentioned above, we need both unigram andbigram clusters because we want to incorporateclass-based generalization for histories of lengths 1and 2.
As we will show below this significantly in-creases performance.
Since the focus of this paper isnot on clustering algorithms, reformatting the train-ing corpus as described above (as a sequence of hy-phenated bigrams) is a simple way of using SRILMfor bigram clustering.The unique-event clusterings are motivated by thefact that in the Dupont-Rosenfeld model, frequentevents are handled by discounted ML estimates.Classes are only needed in cases where an event wasnot seen or was not frequent enough in the train-ing set.
Consequently, we should form clusters notbased on all events in the training corpus, but onlyon events that are rare ?
because this is the type ofevent that classes will then be applied to in predic-tion.The two unique-event corpora can be thoughtof as reweighted collections in which each uniqueevent receives the same weight.
In practice thismeans that clustering is mostly influenced by rareevents since, on the level of types, most events arerare.
As we will see below, rare-event clusteringsperform better than all-event clusterings.
This isnot surprising as the class-based component of themodel can only benefit rare events and it is there-fore reasonable to estimate this component based ona corpus dominated by rare events.We started experimenting with reweighted cor-pora because class sizes become very lopsided inregular SRILM clustering as the size of the base setincreases.
The reason is that the objective functionmaximizes mutual information.
Highly differenti-ated classes for frequent words contribute substan-tially to this objective function whereas putting allrare words in a few large clusters does not hurt theobjective much.
However, our focus is on usingclustering for improving prediction for rare events;this means that the objective function is counter-productive when contexts are frequency-weighted asthey occur in the corpus.
After overweighting rarecontexts, the objective function is more in sync withwhat we use clusters for in our model.pML maximum likelihoodpB Brown cluster modelpE cluster emission probabilitypT cluster transition probabilitypKN KN modelpDR Dupont-Rosenfeld modelpTOP top-level interpolationpPOLKN KN and polynomial discountingpPOL0 polynomial discounting onlyTable 2: Key to probability distributionsIt is important to note that the same intu-ition underlies unique-event clustering thatalso motivates using the ?unique-event?
dis-tributions n1+(?w32)/(?n1+(?w2w)) andn1+(?w3)/(?n1+(?w)) for the backoff distri-butions in KN.
Viewed this way, the basic KNmodel also uses a unique-event corpus (although adifferent one) for estimating backoff probabilities.In all cases, we set the number of clusters tok = 512.
Our main goal in this paper is to comparedifferent ways of setting up history-length/class in-terpolated models and we do not attempt to optimizek.
We settled on a fixed number of k = 512 becauseBrown et al (1992) used a total of 1000 classes.
512unigram classes and 512 bigram classes roughly cor-respond to this number.
We prefer powers of 2 tofacilitate efficient storage of cluster ids (one suchcluster id must be stored for each unigram and eachbigram) and therefore choose k = 512.
Clusteringwas performed on an Opteron 8214 processor andtook from several minutes for the smallest base setsto more than a week for the largest set of 400,000items.To estimate n-gram emission probabilities pE, wefirst introduce an additional cluster for all unigramsthat are not in the base set; emission probabilitiesare then estimated by maximum likelihood.
Clustertransition probabilities pT are computed using add-one smoothing.
Both pE and pT are estimated onthe raw corpus.
The two class distributions are thendefined as follows:pB(w3|w1w2) = pT(g(w3)|g(w1w2))pE(w3|g(w3))pB(w3|w2) = pT(g(w3)|g(w2))pE(w3|g(w3))where g(v) is the class of the uni- or bigram v.1520pDRall events unique events|Bi| ?1 ?2 perp.
?1 ?2 perp.1a 1?104 .20 .40 87.42 .2 .4 87.412a 2?104 .20 .50 86.97 .2 .5 86.883a 3?104 .10 .40 87.14 .2 .5 86.574a 4?104 .10 .40 87.22 .3 .5 86.315a 5?104 .05 .30 87.54 .3 .6 86.106a 6?104 .01 .30 87.71 .3 .6 85.96pTOPall events unique events|Bi| ?1 ?2 perp.
?1 ?2 perp.1b 1?104 .020 .03 87.65 .02 .02 87.712b 2?104 .030 .04 87.43 .03 .03 87.473b 3?104 .020 .03 87.52 .03 .03 87.344b 4?104 .010 .04 87.58 .03 .04 87.245b 5?104 .003 .03 87.74 .03 .04 87.156b 6?104 .000 .02 87.82 .03 .04 87.09Perplexity of KN model: 88.03Table 3: Optimal parameters for Dupont-Rosenfeld (left) and top-level (right) models on the validation set and per-plexity on the validation set.
The two tables compare performance when using a class model trained on all events vs aclass model trained on unique events.
|B1| = |B2| is the number of unigrams and bigrams in the clusters; e.g., lines 1aand 1b are for models that cluster 10,000 unigrams and 10,000 bigrams.Table 2 is a key to the probability distributions weuse.5 ResultsTable 3 shows the performance of pDR and pTOP for arange of base set sizes |Bi| and for classes trained onall events and on unique events.
Parameters ?i and?i are optimized on the validation set.
Perplexity isreported for the validation set.
All following tablesalso optimize on the validation set and report resultson the validation set.
The last table, Table 7, alsoreports perplexity for the test set.Table 3 confirms previous findings that classesimprove language model performance.
All modelshave a perplexity that is lower than KN (88.03).When comparing all-event and unique-event clus-terings, a clear tendency is apparent.
In all-eventclustering, the best performance is reached for|Bi| = 20000: perplexity is 86.97 with this baseset size for pDR (line 2a) and 87.43 for pTOP (line2b).
In unique-event clustering, performance keepsimproving with larger and larger base sets; the bestperplexities are obtained for |Bi| = 60000: 85.96for pDR and 87.09 for pTOP (lines 6a, 6b).The parameter values also reflect this differencebetween all-event and unique-event clustering.
Forunique-event results of pDR, we have ?1 ?
.2 and?2 ?
.4 (1a?6a).
This indicates that classes and his-tory interpolation are both valuable when the modelis backing off.
But for all-event clustering, the val-ues of ?i decrease: from a peak of .20 and .50 (2a)to .01 and .30 (6a), indicating that with larger basesets, less and less value can be derived from classes.This again is evidence that rare-event clustering isthe correct approach: only clusters derived in rare-event clustering receive high weights ?i in the inter-polation.This effect can also be observed for pTOP: thevalue of ?1 (the weight of bigrams) is higher forunique-event clustering than for all-event clustering(with the exception of lines 1b&2b).
The quality ofbigram clusters seems to be low in all-event cluster-ing when the base set becomes too large.Perplexity is generally lower for unique-eventclustering than for all-event clustering: this is thecase for all values of |Bi| for pDR (1a?6a); and for|Bi| > 20000 for pTOP (3b?6b).Table 4 compares the two models in two differentconditions: (i) b-: using unigram clusters only and(ii) b+: using unigram clusters and bigram clusters.For all events, there is no difference in performance.However, for unique events, the model that includesbigrams (b+) does better than the model without bi-grams (b-).
The effect is larger for pDR than forpTOP because (for unique events) a larger weight forthe unigram model (?2 = .05 instead of ?2 = .04)apparently partially compensates for the missing bi-gram clusters.Table 3 shows that rare-event models do betterthan all-event models.
Given that training large classmodels with SRILM on all events would take sev-eral weeks or even months, we restrict our direct1521pDR pTOPall unique all unique?1 ?2 perp.
?1 ?2 perp.
?1 ?2 perp.
?1 ?2 perp.b- .3 87.71 .5 86.62 .02 87.82 .05 87.26b+ .01 .3 87.71 .3 .6 85.96 0 .02 87.82 .03 .04 87.09Table 4: Using both unigram and bigram clusters is better than using unigrams only.
Results for |Bi| = 60,000.pDR pTOP|Bi| ?1 ?2 perp.
?1 ?2 perp.1 6?104 0.3 0.6 85.96 0.03 0.04 87.092 1?105 0.3 0.6 85.59 0.04 0.04 86.933 2?105 0.3 0.6 85.20 0.05 0.04 86.774 4?105 0.3 0.7 85.14 0.05 0.04 86.74Table 5: Dupont-Rosenfeld and top-level models for|Bi| ?
{60000, 100000, 200000, 400000}.
Clusteringtrained on unique-event corpora.comparison of all-event and rare-event models to|Bi| ?
60, 000 in Tables 3-4 and report only rare-event numbers for |Bi| > 60, 000 in what follows.As we can see in Table 5, the trends observed inTable 3 continue as |Bi| is increased further.
Forboth models, perplexity steadily decreases as |Bi| isincreased from 60,000 to 400,000.
(Note that for|Bi| = 400000, the actual size of B1 is 256,873since there are only that many words in the trainingcorpus.)
The improvements in perplexity becomesmaller for larger base set sizes, but it is reassuringto see that the general trend continues for large baseset sizes.
Our explanation is that the class compo-nent is focused on rare events and the items that arebeing added to the clustering for large base sets areall rare events.The perplexity for pDR is clearly lower than thatof pTOP, indicating the superiority of the Dupont-Rosenfeld model.11Dupont and Rosenfeld (1997) found a relatively large im-provement of the ?global?
linear interpolation model ?
ptop inour terminology ?
compared to the baseline whereas ptop per-forms less well in our experiments.
One possible explanation isthat our KN baseline is stronger than the word trigram baselinethey used.6 Polynomial discountingFurther comparative analysis of pDR and pTOP re-vealed that pDR is not uniformly better than pTOP.We found that pTOP does poorly on frequent events.For example, for the history w1w2 = cents a, thecontinuation w3 = share dominates.
pDR deals wellwith this situation because pDR(w3|w1w2) is the dis-counted ML estimate, with a discount that is smallrelative to the 10,768 occurrences of cents a sharein the training set.
In the pTOP model on the last linein Table 5, the discounted ML estimate is multipliedby 1?
.05?
.04 = .91, which results in a much lessaccurate estimate of pTOP(share|cents a).In contrast, pTOP does well for productive histo-ries, for which it is likely that a continuation unseenin the training set will occur.
An example is the his-tory in the ?
almost any adjective or noun can follow.There are 6251 different words that (i) occur after inthe in the validation set, (ii) did not occur after inthe in the training set, and (iii) occurred at least 10times in the training set.
Because their training setunigram frequency is at least 10, they have a goodchance of being assigned to a class that capturestheir distributional behavior well and pB(w3|w1w2)is then likely to be a good estimate.
For a historywith these properties, it is advantageous to furtherdiscount the discounted ML estimates by multiply-ing them with .91. pTOP then gives the remainingprobability mass of .09 to words w3 whose proba-bility would otherwise be underestimated.What we have just described is already partiallyaddressed by the KN model ?
?
(v) will be rela-tively large for a productive history like v = inthe.
However, it looks like the KN discounts arenot large enough for productive histories, at least notin a combined history-length/class model.
Appar-ently, when incorporating the strengths of a class-based model into KN, the default discounting mech-anism does not reallocate enough probability mass1522from high-frequency to low-frequency events.
Weconclude from this analysis that we need to increasethe discount values d for large counts.We could add a constant to d, but one of the ba-sic premises of the KN model, derived from the as-sumption that n-gram marginals should be equal torelative frequencies, is that the discount is larger formore frequent n-grams although in many implemen-tations of KN only the cases c(w31) = 1, c(w31) = 2,and c(w31) ?
3 are distinguished.This suggests that the ideal discount d(x) in an in-tegrated history-length/class language model shouldgrow monotonically with c(v).
The simplest way ofimplementing this heuristically is a polynomial ofform ?xr where ?
and r are parameters.
r controlsthe rate of growth of the discount as a function of x;?
is a factor that can be scaled for optimal perfor-mance.The incorporation of the additional polynomialdiscount into KN is straightforward.
We use a dis-count function e(x) that is the sum of d(x) and thepolynomial:e(x) = d(x) +{?xr for x ?
40 otherwisewhere (e, d) ?
{(e?, d?
), (e?
?, d??
), (e??
?, d???)}.
Thismodel is identical to pDR except that d is replacedwith e. We call this model pPOLKN.
pPOLKN directlyimplements the insight that, when using class-basedgeneralization, discounts for counts x ?
4 should belarger than they are in KN.We also experiment with a second version of themodel:e(x) = ?xrThis second model, called pPOL0, is simpler and doesnot use KN discounts.
It allows us to determinewhether a polynomial discount by itself (without us-ing KN discounts in addition) is sufficient.Results for the two models are shown in Table 6and compared with the two best models from Ta-ble 5, for |Bi| = 400,000, classes trained on uniqueevents.
pPOLKN and pPOL0 achieve a small improve-ment in perplexity when compared to pDR (line 3&4vs 2).
This shows that using discounts that are largerthan KN discounts for large counts is potentially ad-vantageous.
?1/?1 ?2/?2 ?
r perp.1 pTOP .05 .04 86.742 pDR .30 .70 85.143 pPOLKN .30 .70 .05 .89 85.014 pPOL0 .30 .70 .80 .41 84.98Table 6: Results for polynomial discounting comparedto pDR and pTOP.
|Bi| = 400,000, clusters trained onunique events.perplexitytb:l model |Bi| val test1 3 pKN 88.03 88.282 3:6a pDR 6?104 ae b+ 87.71 87.973 3:6a pDR 6?104 ue b+ 85.96 86.224 3:6b pTOP 6?104 ae b+ 87.82 88.085 3:6b pTOP 6?104 ue b+ 87.09 87.356 4 pDR 6?104 ae b- 87.71 87.977 4 pDR 6?104 ue b- 86.62 86.888 4 pTOP 6?104 ae b- 87.82 88.089 4 pTOP 6?104 ue b- 87.26 87.5110 5:4 pDR 2?105 ue b+ 85.14 85.3911 5:4 pTOP 2?105 ue b+ 86.74 86.9812 6:3 pPOLKN 4?105 ue b+ 85.01 85.2613 6:4 pPOL0 4?105 ue b+ 84.98 85.22Table 7: Performance of key models on validation andtest sets.
tb:l = Table and line the validation result is takenfrom.
ae/ue = all-event/unique-event.
b- = unigrams only.b+ = bigrams and unigrams.The linear interpolation ?p+(1??
)q of two dis-tributions p and q is a form of linear discounting:p is discounted by 1 ?
?
and q by ?.
See (Katz,1987; Jelinek, 1990; Ney et al, 1994).
It can thusbe viewed as polynomial discounting for r = 1.Absolute discounting could be viewed as a form ofpolynomial discounting for r = 0.
We know of noother work that has explored exponents between 0and 1 and shown that for this type of exponent, oneobtains competitive discounts that could be arguedto be simpler than more complex discounts like KNdiscounts.6.1 Test set performanceWe report the test set performance of the key mod-els we have developed in this paper in Table 7.
Theexperiments were run with the optimal parameters1523on the validation set as reported in the table refer-enced in column ?tb:l?
; e.g., on line 2 of Table 7,(?1, ?2) = (.01, .3) as reported on line 6a of Ta-ble 3.There is an almost constant difference betweenvalidation and test set perplexities, ranging from +.2to +.3, indicating that test set results are consistentwith validation set results.
To test significance, weassigned the 2.8M positions in the test set to 48 dif-ferent bins according to the majority part-of-speechtag of the word in the training set.2 We can thencompute perplexity for each bin, compare perplexi-ties for different experiments and use the sign test fordetermining significance.
We indicate results thatwere significant at p < .05 (n = 48, k ?
32 suc-cesses) using a star, e.g., 3<?
2 means that test setperplexity on line 3 is significantly lower than testset perplexity on line 2.The main findings on the validation set alo holdfor the test set: (i) Trained on unique events and witha sufficiently large |Bi|, both pDR and pTOP are bet-ter than KN: 10<?1, 11<?1.
(ii) Training on uniqueevents is better than training on all events: 3<?
2,5<?4, 7<?6, 9<?8.
(iii) For unique events, usingbigram and unigram classes gives better results thanusing unigram classes only: 3<?7.
Not significant:5 < 9.
(iv) The Dupont-Rosenfeld model pDR is bet-ter than the top-level model pTOP: 10<?11.
(v) Themodel POL0 (polynomial discounting) is the bestmodel overall: Not significant: 13 < 12.
(vi) Poly-nomial discounting is significantly better than KNdiscounting for the Dupont-Rosenfeld model pDR al-though the absolute difference in perplexity is small:13<?10.Overall, pDR and pPOL0 achieve considerable re-ductions in test set perplexity from 88.28 to 85.39and 85.22, respectively.
The main result of the ex-periments is that Dupont-Rosenfeld models (whichfocus on rare events) are better than the standardlyused top-level models; and that training classes onunique events is better than training classes on allevents.2Words with a rare majority tag (e.g., FW ?foreign word?
)and unknown words were assigned to a special class OTHER.7 ConclusionOur hypothesis was that classes are a generalizationmechanism for rare events that serves the same func-tion as history-length interpolation and that classesshould therefore be (i) primarily trained on rareevents and (ii) receive high weight only if it is likelythat a rare event will follow and be weighted in away analogous to the weighting of lower-order dis-tributions in history-length interpolation.We found clear statistically significant evidencefor both (i) and (ii).
(i) Classes trained on unique-event corpora perform better than classes trained onall-event corpora.
(ii) The pDR model (which ad-justs the interpolation weight given to classes basedon the prevalence of nonfrequent events following)is better than top-level model pTOP (which uses afixed weight for classes).
Most previous work onclass-based models has employed top-level interpo-lation.
Our results strongly suggest that the Dupont-Rosenfeld model is a superior model.A comparison of Dupont-Rosenfeld and top-levelresults suggested that the KN discount mechanismdoes not discount high-frequency events enough.We empirically determined that better discounts areobtained by letting the discount grow as a func-tion of the count of the discounted event and im-plemented this as polynomial discounting, an ar-guably simpler way of discounting than Kneser-Neydiscounting.
The improvement of polynomial dis-counts vs. KN discounts was small, but statisticallysignificant.In future work, we would like to find a theoreti-cal justification for the surprising fact that polyno-mial discounting does at least as well as Kneser-Neydiscounting.
We also would like to look at otherbackoff mechanisms (in addition to history lengthand classes) and incorporate them into the model,e.g., similarity and topic.
Finally, training classes onunique events is an extreme way of highly weight-ing rare events.
We would like to explore trainingregimes that lie between unique-event clustering andall-event clustering and upweight rare events less.Acknowledgements.
This research was fundedby Deutsche Forschungsgemeinschaft (grant SFB732).
We are grateful to Thomas Mu?ller, HelmutSchmid and the anonymous reviewers for their help-ful comments.1524ReferencesJeff Bilmes and Katrin Kirchhoff.
2003.
Factored lan-guage models and generalized parallel backoff.
InHLT-NAACL.Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
CoRR, cmp-lg/9606011.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech & Language, 13(4):359?393.Stanley F. Chen.
2009.
Shrinking exponential languagemodels.
In HLT/NAACL, pages 468?476.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In EACL, pages 59?66.Sabine Deligne and Yoshinori Sagisaka.
2000.
Statisti-cal language modeling with a class-based n-multigrammodel.
Computer Speech & Language, 14(3):261?279.Pierre Dupont and Ronald Rosenfeld.
1997.
Latticebased language models.
Technical Report CMU-CS-97-173, Carnegie Mellon University.Ahmad Emami and Frederick Jelinek.
2005.
Randomclustering for language modeling.
In ICASSP, vol-ume 1, pages 581?584.Frederick Jelinek and Robert L. Mercer.
1980.
Inter-polated estimation of Markov source parameters fromsparse data.
In Edzard S. Gelsema and Laveen N.Kanal, editors, Pattern Recognition in Practice, pages381?397.
North-Holland.Frederick Jelinek.
1990.
Self-organized language mod-eling for speech recognition.
In Alex Waibel and Kai-Fu Lee, editors, Readings in speech recognition, pages450?506.
Morgan Kaufmann.Raquel Justo and M. Ine?s Torres.
2009.
Phrase classes intwo-level language models for ASR.
Pattern Analysis& Applications, 12(4):427?437.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing, 35(3):400?401.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.
InICASSP, volume 1, pages 181?184.Hong-Kwang J. Kuo and Wolfgang Reichl.
1999.Phrase-based language models for speech recognition.In European Conference on Speech Communicationand Technology, volume 4, pages 1595?1598.John G. McMahon and Francis J. Smith.
1996.
Improv-ing statistical language model performance with auto-matically generated word hierarchies.
ComputationalLinguistics, 22:217?247.Saeedeh Momtazi and Dietrich Klakow.
2009.
A wordclustering approach for language model-based sen-tence retrieval in question answering systems.
In ACMConference on Information and Knowledge Manage-ment, pages 1911?1914.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependencies in stochasticlanguage modelling.
Computer Speech and Language,8:1?38.Roi Reichart, Omri Abend, and Ari Rappoport.
2010.Type level clustering evaluation: new measures and apos induction case study.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 77?87.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In EACL 7, pages 141?148.Andreas Stolcke.
2002.
SRILM - An extensible lan-guage modeling toolkit.
In International Conferenceon Spoken Language Processing, pages 901?904.Bernhard Suhm and Alex Waibel.
1994.
Towards bet-ter language models for spontaneous speech.
In Inter-national Conference on Spoken Language Processing,pages 831?834.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Annual Meet-ing of the Association for Computational Linguistics,pages 755?762.E.W.D.
Whittaker and P.C.
Woodland.
2001.
Efficientclass-based language modelling for very large vocab-ularies.
In ICASSP, volume 1, pages 545?548.Michael Wiegand and Dietrich Klakow.
2008.
Opti-mizing language models for polarity classification.
InECIR, pages 612?616.T.
Yokoyama, T. Shinozaki, K. Iwano, and S. Furui.2003.
Unsupervised class-based language modeladaptation for spontaneous speech recognition.
InICASSP, volume 1, pages 236?239.Imed Zitouni and Qiru Zhou.
2007.
Linearly interpo-lated hierarchical n-gram language models for speechrecognition engines.
In Michael Grimm and Kris-tian Kroschel, editors, Robust Speech Recognition andUnderstanding, pages 301?318.
I-Tech Education andPublishing.Imed Zitouni and Qiru Zhou.
2008.
Hierarchical lineardiscounting class n-gram language models: A multi-level class hierarchy approach.
In International Con-ference on Acoustics, Speech, and Signal Processing,pages 4917?4920.1525
