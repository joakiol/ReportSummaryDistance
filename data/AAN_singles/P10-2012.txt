Proceedings of the ACL 2010 Conference Short Papers, pages 60?67,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCognitively Plausible Models of Human Language ProcessingFrank KellerSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKkeller@inf.ed.ac.ukAbstractWe pose the development of cognitivelyplausible models of human language pro-cessing as a challenge for computationallinguistics.
Existing models can only dealwith isolated phenomena (e.g., gardenpaths) on small, specifically selected datasets.
The challenge is to build models thatintegrate multiple aspects of human lan-guage processing at the syntactic, seman-tic, and discourse level.
Like human lan-guage processing, these models should beincremental, predictive, broad coverage,and robust to noise.
This challenge canonly be met if standardized data sets andevaluation measures are developed.1 IntroductionIn many respects, human language processing isthe ultimate goldstandard for computational lin-guistics.
Humans understand and generate lan-guage with amazing speed and accuracy, they areable to deal with ambiguity and noise effortlesslyand can adapt to new speakers, domains, and reg-isters.
Most surprisingly, they achieve this compe-tency on the basis of limited training data (Hartand Risley, 1995), using learning algorithms thatare largely unsupervised.Given the impressive performance of humansas language processors, it seems natural to turnto psycholinguistics, the discipline that studies hu-man language processing, as a source of informa-tion about the design of efficient language pro-cessing systems.
Indeed, psycholinguists have un-covered an impressive array of relevant facts (re-viewed in Section 2), but computational linguistsare often not aware of this literature, and resultsabout human language processing rarely informthe design, implementation, or evaluation of artifi-cial language processing systems.At the same time, research in psycholinguis-tics is often oblivious of work in computationallinguistics (CL).
To test their theories, psycholin-guists construct computational models of hu-man language processing, but these models of-ten fall short of the engineering standards thatare generally accepted in the CL community(e.g., broad coverage, robustness, efficiency): typ-ical psycholinguistic models only deal with iso-lated phenomena and fail to scale to realistic datasets.
A particular issue is evaluation, which is typ-ically anecdotal, performed on a small set of hand-crafted examples (see Sections 3).In this paper, we propose a challenge that re-quires the combination of research efforts in com-putational linguistics and psycholinguistics: thedevelopment of cognitively plausible models ofhuman language processing.
This task can be de-composed into a modeling challenge (buildingmodels that instantiate known properties of hu-man language processing) and a data and evalu-ation challenge (accounting for experimental find-ings and evaluating against standardized data sets),which we will discuss in turn.2 Modeling Challenge2.1 Key PropertiesThe first part of the challenge is to develop a modelthat instantiates key properties of human languageprocessing, as established by psycholinguistic ex-perimentation (see Table 1 for an overview andrepresentative references).1 A striking property ofthe human language processor is its efficiency androbustness.
For the vast majority of sentences, itwill effortlessly and rapidly deliver the correctanalysis, even in the face of noise and ungrammat-icalities.
There is considerable experimental evi-1Here an in the following, we will focus on sentenceprocessing, which is often regarded as a central aspect ofhuman language processing.
A more comprehensive answerto our modeling challenge should also include phonologicaland morphological processing, semantic inference, discourseprocessing, and other non-syntactic aspects of language pro-cessing.
Furthermore, established results regarding the inter-face between language processing and non-linguistic cogni-tion (e.g., the sensorimotor system) should ultimately be ac-counted for in a fully comprehensive model.60ModelProperty EvidenceRank Surp Pred StackEfficiency and robustness Ferreira et al (2001); Sanford and Sturt (2002) ?
?
?
+Broad coverage Crocker and Brants (2000) + + ?
+Incrementality and connectedness Tanenhaus et al (1995); Sturt and Lombardo (2005) + + + +Prediction Kamide et al (2003); Staub and Clifton (2006) ?
?
+ ?Memory cost Gibson (1998); Vasishth and Lewis (2006) ?
?
+ +Table 1: Key properties of human language processing and their instantiation in various models of sentence processing (seeSection 2 for details)dence that shallow processing strategies are usedto achieve this.
The processor also achieves broadcoverage: it can deal with a wide variety of syntac-tic constructions, and is not restricted by the do-main, register, or modality of the input.Human language processing is also word-by-word incremental.
There is strong evidence thata new word is integrated as soon as it is avail-able into the representation of the sentence thusfar.
Readers and listeners experience differentialprocessing difficulty during this integration pro-cess, depending on the properties of the new wordand its relationship to the preceding context.
Thereis evidence that the processor instantiates a strictform of incrementality by building only fully con-nected trees.
Furthermore, the processor is ableto make predictions about upcoming material onthe basis of sentence prefixes.
For instance, listen-ers can predict an upcoming post-verbal elementbased on the semantics of the preceding verb.
Orthey can make syntactic predictions, e.g., if theyencounter the word either, they predict an upcom-ing or and the type of complement that follows it.Another key property of human language pro-cessing is the fact that it operates with limitedmemory, and that structures in memory are subjectto decay and interference.
In particular, the pro-cessor is known to incur a distance-based memorycost: combining the head of a phrase with its syn-tactic dependents is more difficult the more depen-dents have to be integrated and the further awaythey are.
This integration process is also subjectto interference from similar items that have to beheld in memory at the same time.2.2 Current ModelsThe challenge is to develop a computational modelthat captures the key properties of human languageprocessing outlined in the previous section.
Anumber of relevant models have been developed,mostly based on probabilistic parsing techniques,but none of them instantiates all the key proper-ties discussed above (Table 1 gives an overview ofmodel properties).2The earliest approaches were ranking-basedmodels (Rank), which make psycholinguistic pre-dictions based on the ranking of the syntacticanalyses produced by a probabilistic parser.
Ju-rafsky (1996) assumes that processing difficultyis triggered if the correct analysis falls below acertain probability threshold (i.e., is pruned bythe parser).
Similarly, Crocker and Brants (2000)assume that processing difficulty ensures if thehighest-ranked analysis changes from one word tothe next.
Both approaches have been shown to suc-cessfully model garden path effects.
Being basedon probabilistic parsing techniques, ranking-basedmodels generally achieve a broad coverage, buttheir efficiency and robustness has not been evalu-ated.
Also, they are not designed to capture syntac-tic prediction or memory effects (other than searchwith a narrow beam in Brants and Crocker 2000).The ranking-based approach has been gener-alized by surprisal models (Surp), which pre-dict processing difficulty based on the change inthe probability distribution over possible analy-ses from one word to the next (Hale, 2001; Levy,2008; Demberg and Keller, 2008a; Ferrara Bostonet al, 2008; Roark et al, 2009).
These modelshave been successful in accounting for a range ofexperimental data, and they achieve broad cover-age.
They also instantiate a limited form of predic-tion, viz., they build up expectations about the nextword in the input.
On the other hand, the efficiencyand robustness of these models has largely notbeen evaluated, and memory costs are not mod-eled (again except for restrictions in beam size).The prediction model (Pred) explicitly predictssyntactic structure for upcoming words (Dembergand Keller, 2008b, 2009), thus accounting for ex-perimental results on predictive language process-ing.
It also implements a strict form of incre-2We will not distinguish between model and linking the-ory, i.e., the set of assumptions that links model quantitiesto behavioral data (e.g., more probably structures are easierto process).
It is conceivable, for instance, that a stack-basedmodel is combined with a linking theory based on surprisal.61Factor EvidenceWord senses Roland and Jurafsky (2002)Selectional re-strictionsGarnsey et al (1997); Pickering andTraxler (1998)Thematic roles McRae et al (1998); Pickering et al(2000)Discourse ref-erenceAltmann and Steedman (1988); Grod-ner and Gibson (2005)DiscoursecoherenceStewart et al (2000); Kehler et al(2008)Table 2: Semantic factors in human language processingmentality by building fully connected trees.
Mem-ory costs are modeled directly as a distance-basedpenalty that is incurred when a prediction has to beverified later in the sentence.
However, the currentimplementation of the prediction model is neitherrobust and efficient nor offers broad coverage.Recently, a stack-based model (Stack) has beenproposed that imposes explicit, cognitively mo-tivated memory constraints on the parser, in ef-fect limiting the stack size available to the parser(Schuler et al, 2010).
This delivers robustness, ef-ficiency, and broad coverage, but does not modelsyntactic prediction.
Unlike the other models dis-cussed here, no psycholinguistic evaluation hasbeen conducted on the stack-based model, so itscognitive plausibility is preliminary.2.3 Beyond ParsingThere is strong evidence that human language pro-cessing is driven by an interaction of syntactic, se-mantic, and discourse processes (see Table 2 foran overview and references).
Considerable exper-imental work has focused on the semantic prop-erties of the verb of the sentence, and verb sense,selectional restrictions, and thematic roles have allbeen shown to interact with syntactic ambiguityresolution.
Another large body of research has elu-cidated the interaction of discourse processing andsyntactic processing.
The most-well known effectis probably that of referential context: syntacticambiguities can be resolved if a discourse con-text is provided that makes one of the syntacticalternatives more plausible.
For instance, in a con-text that provides two possible antecedents for anoun phrase, the processor will prefer attaching aPP or a relative clause such that it disambiguatesbetween the two antecedents; garden paths are re-duced or disappear.
Other results point to the im-portance of discourse coherence for sentence pro-cessing, an example being implicit causality.The challenge facing researchers in compu-tational and psycholinguistics therefore includesthe development of language processing modelsthat combine syntactic processing with semanticand discourse processing.
So far, this challenge islargely unmet: there are some examples of modelsthat integrate semantic processes such as thematicrole assignment into a parsing model (Narayananand Jurafsky, 2002; Pado?
et al, 2009).
However,other semantic factors are not accounted for bythese models, and incorporating non-lexical as-pects of semantics into models of sentence pro-cessing is a challenge for ongoing research.
Re-cently, Dubey (2010) has proposed an approachthat combines a probabilistic parser with a modelof co-reference and discourse inference based onprobabilistic logic.
An alternative approach hasbeen taken by Pynte et al (2008) and Mitchellet al (2010), who combine a vector-space modelof semantics (Landauer and Dumais, 1997) with asyntactic parser and show that this results in pre-dictions of processing difficulty that can be vali-dated against an eye-tracking corpus.2.4 Acquisition and CrosslinguisticsAll models of human language processing dis-cussed so far rely on supervised training data.
Thisraises another aspect of the modeling challenge:the human language processor is the product ofan acquisition process that is largely unsupervisedand has access to only limited training data: chil-dren aged 12?36 months are exposed to between10 and 35 million words of input (Hart and Ris-ley, 1995).
The challenge therefore is to developa model of language acquisition that works withsuch small training sets, while also giving rise toa language processor that meets the key criteriain Table 1.
The CL community is in a good posi-tion to rise to this challenge, given the significantprogress in unsupervised parsing in recent years(starting from Klein and Manning 2002).
How-ever, none of the existing unsupervised models hasbeen evaluated against psycholinguistic data sets,and they are not designed to meet even basic psy-cholinguistic criteria such as incrementality.A related modeling challenge is the develop-ment of processing models for languages otherthan English.
There is a growing body of ex-perimental research investigating human languageprocessing in other languages, but virtually all ex-isting psycholinguistic models only work for En-glish (the only exceptions we are aware of areDubey et al?s (2008) and Ferrara Boston et al?s62(2008) parsing models for German).
Again, theCL community has made significant progress incrosslinguistic parsing, especially using depen-dency grammar (Hajic?, 2009), and psycholinguis-tic modeling could benefit from this in order tomeet the challenge of developing crosslinguisti-cally valid models of human language processing.3 Data and Evaluation Challenge3.1 Test SetsThe second key challenge that needs to be ad-dressed in order to develop cognitively plausiblemodels of human language processing concernstest data and model evaluation.
Here, the state ofthe art in psycholinguistic modeling lags signif-icantly behind standards in the CL community.Most of the models discussed in Section 2 have notbeen evaluated rigorously.
The authors typicallydescribe their performance on a small set of hand-picked examples; no attempts are made to test ona range of items from the experimental literatureand determine model fit directly against behavioralmeasures (e.g., reading times).
This makes it veryhard to obtain a realistic estimate of how well themodels achieve their aim of capturing human lan-guage processing behavior.We therefore suggest the development of stan-dard test sets for psycholinguistic modeling, simi-lar to what is commonplace for tasks in computa-tional linguistics: parsers are evaluated against thePenn Treebank, word sense disambiguation sys-tems against the SemEval data sets, co-referencesystems against the Tipster or ACE corpora, etc.Two types of test data are required for psycholin-guistic modeling.
The first type of test data con-sists of a collection of representative experimentalresults.
This collection should contain the actualexperimental materials (sentences or discoursefragments) used in the experiments, together withthe behavioral measurements obtained (readingtimes, eye-movement records, rating judgments,etc.).
The experiments included in this test setwould be chosen to cover a wide range of ex-perimental phenomena, e.g., garden paths, syntac-tic complexity, memory effects, semantic and dis-course factors.
Such a test set will enable the stan-dardized evaluation of psycholinguistic models bycomparing the model predictions (rankings, sur-prisal values, memory costs, etc.)
against behav-ioral measures on a large set of items.
This wayboth the coverage of a model (how many phenom-ena can it account for) and its accuracy (how welldoes it fit the behavioral data) can be assessed.Experimental test sets should be complementedby test sets based on corpus data.
In order to as-sess the efficiency, robustness, and broad cover-age of a model, a corpus of unrestricted, naturallyoccurring text is required.
The use of contextual-ized language data makes it possible to assess notonly syntactic models, but also models that capturediscourse effects.
These corpora need to be anno-tated with behavioral measures, e.g., eye-trackingor reading time data.
Some relevant corpora havealready been constructed, see the overview in Ta-ble 3, and various authors have used them formodel evaluation (Demberg and Keller, 2008a;Pynte et al, 2008; Frank, 2009; Ferrara Bostonet al, 2008; Patil et al, 2009; Roark et al, 2009;Mitchell et al, 2010).However, the usefulness of the psycholinguis-tic corpora in Table 3 is restricted by the absenceof gold-standard linguistic annotation (though theFrench part of the Dundee corpus, which is syn-tactically annotated).
This makes it difficult to testthe accuracy of the linguistic structures computedby a model, and restricts evaluation to behavioralpredictions.
The challenge is therefore to collecta standardized test set of naturally occurring textor speech enriched not only with behavioral vari-ables, but also with syntactic and semantic anno-tation.
Such a data set could for example be con-structed by eye-tracking section 23 of the PennTreebank (which is also part of Propbank, and thushas both syntactic and thematic role annotation).In computational linguistics, the developmentof new data sets is often stimulated by competi-tions in which systems are compared on a stan-dardized task, using a data set specifically de-signed for the competition.
Examples include theCoNLL shared task, SemEval, or TREC in com-putational syntax, semantics, and discourse, re-spectively.
A similar competition could be devel-oped for computational psycholinguistics ?
maybealong the lines of the model comparison chal-lenges that held at the International Conferenceon Cognitive Modeling.
These challenges providestandardized task descriptions and data sets; par-ticipants can enter their cognitive models, whichwere then compared using a pre-defined evalua-tion metric.33The ICCM 2009 challenge was the Dynamic Stock andFlows Task, for more information see http://www.hss.cmu.edu/departments/sds/ddmlab/modeldsf/.63Corpus Language Words Participants Method ReferenceDundee Corpus English, French 50,000 10 Eye-tracking Kennedy and Pynte (2005)Potsdam Corpus German 1,138 222 Eye-tracking Kliegl et al (2006)MIT Corpus English 3,534 23 Self-paced reading Bachrach (2008)Table 3: Test corpora that have been used for psycholinguistic modeling of sentence processing; note that the Potsdam Corpusconsists of isolated sentences, rather than of continuous text3.2 Behavioral and Neural DataAs outlined in the previous section, a number ofauthors have evaluated psycholinguistic modelsagainst eye-tracking or reading time corpora.
Partof the data and evaluation challenge is to extendthis evaluation to neural data as provided by event-related potential (ERP) or brain imaging studies(e.g., using functional magnetic resonance imag-ing, fMRI).
Neural data sets are considerably morecomplex than behavioral ones, and modeling themis an important new task that the community isonly beginning to address.
Some recent work hasevaluated models of word semantics against ERP(Murphy et al, 2009) or fMRI data (Mitchell et al,2008).4 This is a very promising direction, and thechallenge is to extend this approach to the sentenceand discourse level (see Bachrach 2008).
Again,it will again be necessary to develop standardizedtest sets of both experimental data and corpus data.3.3 Evaluation MeasuresWe also anticipate that the availability of new testdata sets will facilitate the development of newevaluation measures that specifically test the va-lidity of psycholinguistic models.
Established CLevaluation measures such as Parseval are of lim-ited use, as they can only test the linguistic, but notthe behavioral or neural predictions of a model.So far, many authors have relied on qualita-tive evaluation: if a model predicts a differencein (for instance) reading time between two typesof sentences where such a difference was alsofound experimentally, then that counts as a suc-cessful test.
In most cases, no quantitative evalu-ation is performed, as this would require model-ing the reading times for individual item and in-dividual participants.
Suitable procedures for per-forming such tests do not currently exist; linearmixed effects models (Baayen et al, 2008) pro-vide a way of dealing with item and participantvariation, but crucially do not enable direct com-parisons between models in terms of goodness offit.4These data sets were released as part of the NAACL-2010 Workshop on Computational Neurolinguistics.Further issues arise from the fact that we of-ten want to compare model fit for multiple experi-ments (ideally without reparametrizing the mod-els), and that various mutually dependent mea-sures are used for evaluation, e.g., processing ef-fort at the sentence, word, and character level.
Animportant open challenge is there to develop eval-uation measures and associated statistical proce-dures that can deal with these problems.4 ConclusionsIn this paper, we discussed the modeling anddata/evaluation challenges involved in developingcognitively plausible models of human languageprocessing.
Developing computational models isof scientific importance in so far as models are im-plemented theories: models of language process-ing allow us to test scientific hypothesis about thecognitive processes that underpin language pro-cessing.
This type of precise, formalized hypoth-esis testing is only possible if standardized datasets and uniform evaluation procedures are avail-able, as outlined in the present paper.
Ultimately,this approach enables qualitative and quantitativecomparisons between theories, and thus enhancesour understanding of a key aspect of human cog-nition, language processing.There is also an applied side to the proposedchallenge.
Once computational models of humanlanguage processing are available, they can beused to predict the difficulty that humans experi-ence when processing text or speech.
This is use-ful for a number applications: for instance, nat-ural language generation would benefit from be-ing able to assess whether machine-generated textor speech is easy to process.
For text simplifica-tion (e.g., for children or impaired readers), such amodel is even more essential.
It could also be usedto assess the readability of text, which is of interestin educational applications (e.g., essay scoring).
Inmachine translation, evaluating the fluency of sys-tem output is crucial, and a model that predictsprocessing difficulty could be used for this, or toguide the choice between alternative translations,and maybe even to inform human post-editing.64ReferencesAltmann, Gerry T. M. and Mark J. Steedman.1988.
Interaction with context during humansentence processing.
Cognition 30(3):191?238.Baayen, R. H., D. J. Davidson, and D. M. Bates.2008.
Mixed-effects modeling with crossed ran-dom effects for subjects and items.
Journal ofMemory and Language to appear.Bachrach, Asaf.
2008.
Imaging Neural Correlatesof Syntactic Complexity in a Naturalistic Con-text.
Ph.D. thesis, Massachusetts Institute ofTechnology, Cambridge, MA.Brants, Thorsten and Matthew W. Crocker.
2000.Probabilistic parsing and psychological plau-sibility.
In Proceedings of the 18th Interna-tional Conference on Computational Linguis-tics.
Saarbru?cken/Luxembourg/Nancy, pages111?117.Crocker, Matthew W. and Thorsten Brants.
2000.Wide-coverage probabilistic sentence process-ing.
Journal of Psycholinguistic Research29(6):647?669.Demberg, Vera and Frank Keller.
2008a.
Datafrom eye-tracking corpora as evidence for theo-ries of syntactic processing complexity.
Cogni-tion 101(2):193?210.Demberg, Vera and Frank Keller.
2008b.
A psy-cholinguistically motivated version of TAG.
InProceedings of the 9th International Workshopon Tree Adjoining Grammars and Related For-malisms.
Tu?bingen, pages 25?32.Demberg, Vera and Frank Keller.
2009.
A com-putational model of prediction in human pars-ing: Unifying locality and surprisal effects.
InNiels Taatgen and Hedderik van Rijn, editors,Proceedings of the 31st Annual Conference ofthe Cognitive Science Society.
Cognitive Sci-ence Society, Amsterdam, pages 1888?1893.Dubey, Amit.
2010.
The influence of discourse onsyntax: A psycholinguistic model of sentenceprocessing.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics.
Uppsala.Dubey, Amit, Frank Keller, and Patrick Sturt.2008.
A probabilistic corpus-based model ofsyntactic parallelism.
Cognition 109(3):326?344.Ferrara Boston, Marisa, John Hale, ReinholdKliegl, Umesh Patil, and Shravan Vasishth.2008.
Parsing costs as predictors of reading dif-ficulty: An evaluation using the Potsdam Sen-tence Corpus.
Journal of Eye Movement Re-search 2(1):1?12.Ferreira, Fernanda, Kiel Christianson, and An-drew Hollingworth.
2001.
Misinterpretations ofgarden-path sentences: Implications for modelsof sentence processing and reanalysis.
Journalof Psycholinguistic Research 30(1):3?20.Frank, Stefan L. 2009.
Surprisal-based compar-ison between a symbolic and a connectionistmodel of sentence processing.
In Niels Taat-gen and Hedderik van Rijn, editors, Proceed-ings of the 31st Annual Conference of the Cog-nitive Science Society.
Cognitive Science Soci-ety, Amsterdam, pages 1139?1144.Garnsey, Susan M., Neal J. Pearlmutter, Elisa-beth M. Myers, and Melanie A. Lotocky.
1997.The contributions of verb bias and plausibilityto the comprehension of temporarily ambiguoussentences.
Journal of Memory and Language37(1):58?93.Gibson, Edward.
1998.
Linguistic complexity:locality of syntactic dependencies.
Cognition68:1?76.Grodner, Dan and Edward Gibson.
2005.
Conse-quences of the serial nature of linguistic input.Cognitive Science 29:261?291.Hajic?, Jan, editor.
2009.
Proceedings of the 13thConference on Computational Natural Lan-guage Learning: Shared Task.
Association forComputational Linguistics, Boulder, CO.Hale, John.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapterof the Association for Computational Linguis-tics.
Association for Computational Linguistics,Pittsburgh, PA, volume 2, pages 159?166.Hart, Betty and Todd R. Risley.
1995.
Meaning-ful Differences in the Everyday Experience ofYoung American Children.
Paul H. Brookes,Baltimore, MD.Jurafsky, Daniel.
1996.
A probabilistic model oflexical and syntactic access and disambigua-tion.
Cognitive Science 20(2):137?194.Kamide, Yuki, Gerry T. M. Altmann, and Sarah L.Haywood.
2003.
The time-course of predictionin incremental sentence processing: Evidence65from anticipatory eye movements.
Journal ofMemory and Language 49:133?156.Kehler, Andrew, Laura Kertz, Hannah Rohde, andJeffrey L. Elman.
2008.
Coherence and coref-erence revisited.
Journal of Semantics 25(1):1?44.Kennedy, Alan and Joel Pynte.
2005.
Parafoveal-on-foveal effects in normal reading.
Vision Re-search 45:153?168.Klein, Dan and Christopher Manning.
2002.
Agenerative constituent-context model for im-proved grammar induction.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics.
Philadelphia, pages128?135.Kliegl, Reinhold, Antje Nuthmann, and Ralf Eng-bert.
2006.
Tracking the mind during reading:The influence of past, present, and future wordson fixation durations.
Journal of ExperimentalPsychology: General 135(1):12?35.Landauer, Thomas K. and Susan T. Dumais.
1997.A solution to Plato?s problem: The latent se-mantic analysis theory of acquisition, inductionand representation of knowledge.
Psychologi-cal Review 104(2):211?240.Levy, Roger.
2008.
Expectation-based syntacticcomprehension.
Cognition 106(3):1126?1177.McRae, Ken, Michael J. Spivey-Knowlton, andMichael K. Tanenhaus.
1998.
Modeling the in-fluence of thematic fit (and other constraints)in on-line sentence comprehension.
Journal ofMemory and Language 38(3):283?312.Mitchell, Jeff, Mirella Lapata, Vera Demberg, andFrank Keller.
2010.
Syntactic and semantic fac-tors in processing difficulty: An integrated mea-sure.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Lin-guistics.
Uppsala.Mitchell, Tom M., Svetlana V. Shinkareva, An-drew Carlson, Kai-Min Chang, Vicente L.Malave, Robert A. Mason, and Marcel AdamJust3.
2008.
Predicting human brain activity as-sociated with the meanings of nouns.
Science320(5880):1191?1195.Murphy, Brian, Marco Baroni, and Massimo Poe-sio.
2009.
EEG responds to conceptual stimuliand corpus semantics.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing.
Singapore, pages 619?627.Narayanan, Srini and Daniel Jurafsky.
2002.
ABayesian model predicts human parse prefer-ence and reading time in sentence processing.
InThomas G. Dietterich, Sue Becker, and ZoubinGhahramani, editors, Advances in Neural In-formation Processing Systems 14.
MIT Press,Cambridge, MA, pages 59?65.Pado?, Ulrike, Matthew W. Crocker, and FrankKeller.
2009.
A probabilistic model of semanticplausibility in sentence processing.
CognitiveScience 33(5):794?838.Patil, Umesh, Shravan Vasishth, and ReinholdKliegl.
2009.
Compound effect of probabilis-tic disambiguation and memory retrievals onsentence processing: Evidence from an eye-tracking corpus.
In A. Howes, D. Peebles,and R. Cooper, editors, Proceedings of 9th In-ternational Conference on Cognitive Modeling.Manchester.Pickering, Martin J. and Martin J. Traxler.
1998.Plausibility and recovery from garden paths: Aneye-tracking study.
Journal of ExperimentalPsychology: Learning Memory and Cognition24(4):940?961.Pickering, Martin J., Matthew J. Traxler, andMatthew W. Crocker.
2000.
Ambiguity reso-lution in sentence processing: Evidence againstfrequency-based accounts.
Journal of Memoryand Language 43(3):447?475.Pynte, Joel, Boris New, and Alan Kennedy.
2008.On-line contextual influences during readingnormal text: A multiple-regression analysis.
Vi-sion Research 48(21):2172?2183.Roark, Brian, Asaf Bachrach, Carlos Cardenas,and Christophe Pallier.
2009.
Deriving lex-ical and syntactic expectation-based measuresfor psycholinguistic modeling via incrementaltop-down parsing.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing.
Singapore, pages 324?333.Roland, Douglas and Daniel Jurafsky.
2002.
Verbsense and verb subcategorization probabilities.In Paola Merlo and Suzanne Stevenson, editors,The Lexical Basis of Sentence Processing: For-mal, Computational, and Experimental Issues,John Bejamins, Amsterdam, pages 325?346.Sanford, Anthony J. and Patrick Sturt.
2002.66Depth of processing in language comprehen-sion: Not noticing the evidence.
Trends in Cog-nitive Sciences 6:382?386.Schuler, William, Samir AbdelRahman, TimMiller, and Lane Schwartz.
2010.
Broad-coverage parsing using human-like mem-ory constraints.
Computational Linguistics26(1):1?30.Staub, Adrian and Charles Clifton.
2006.
Syntac-tic prediction in language comprehension: Evi-dence from either .
.
.
or.
Journal of Experimen-tal Psychology: Learning, Memory, and Cogni-tion 32:425?436.Stewart, Andrew J., Martin J. Pickering, and An-thony J. Sanford.
2000.
The time course of theinfluence of implicit causality information: Fo-cusing versus integration accounts.
Journal ofMemory and Language 42(3):423?443.Sturt, Patrick and Vincenzo Lombardo.
2005.Processing coordinated structures: Incremen-tality and connectedness.
Cognitive Science29(2):291?305.Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehen-sion.
Science 268:1632?1634.Vasishth, Shravan and Richard L. Lewis.
2006.Argument-head distance and processing com-plexity: Explaining both locality and antilocal-ity effects.
Language 82(4):767?794.67
