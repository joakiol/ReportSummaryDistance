Determining Term Subjectivity and Term Orientation for Opinion MiningAndrea Esuli1 and Fabrizio Sebastiani2(1) Istituto di Scienza e Tecnologie dell?Informazione ?
Consiglio Nazionale delle RicercheVia G Moruzzi, 1 ?
56124 Pisa, Italyandrea.esuli@isti.cnr.it(2) Dipartimento di Matematica Pura e Applicata ?
Universita` di PadovaVia GB Belzoni, 7 ?
35131 Padova, Italyfabrizio.sebastiani@unipd.itAbstractOpinion mining is a recent subdisciplineof computational linguistics which is con-cerned not with the topic a document isabout, but with the opinion it expresses.To aid the extraction of opinions fromtext, recent work has tackled the issueof determining the orientation of ?subjec-tive?
terms contained in text, i.e.
decid-ing whether a term that carries opinion-ated content has a positive or a negativeconnotation.
This is believed to be of keyimportance for identifying the orientationof documents, i.e.
determining whether adocument expresses a positive or negativeopinion about its subject matter.We contend that the plain determinationof the orientation of terms is not a realis-tic problem, since it starts from the non-realistic assumption that we already knowwhether a term is subjective or not; thiswould imply that a linguistic resource thatmarks terms as ?subjective?
or ?objective?is available, which is usually not the case.In this paper we confront the task of de-ciding whether a given term has a positiveconnotation, or a negative connotation, orhas no subjective connotation at all; thisproblem thus subsumes the problem of de-termining subjectivity and the problem ofdetermining orientation.
We tackle thisproblem by testing three different variantsof a semi-supervised method previouslyproposed for orientation detection.
Ourresults show that determining subjectivityand orientation is a much harder problemthan determining orientation alone.1 IntroductionOpinion mining is a recent subdiscipline of com-putational linguistics which is concerned not withthe topic a document is about, but with the opinionit expresses.
Opinion-driven content managementhas several important applications, such as deter-mining critics?
opinions about a given product byclassifying online product reviews, or tracking theshifting attitudes of the general public toward a po-litical candidate by mining online forums.Within opinion mining, several subtasks can beidentified, all of them having to do with tagging agiven document according to expressed opinion:1. determining document subjectivity, as in de-ciding whether a given text has a factual na-ture (i.e.
describes a given situation or event,without expressing a positive or a negativeopinion on it) or expresses an opinion on itssubject matter.
This amounts to performingbinary text categorization under categoriesObjective and Subjective (Pang and Lee,2004; Yu and Hatzivassiloglou, 2003);2. determining document orientation (or polar-ity), as in deciding if a given Subjective textexpresses a Positive or a Negative opinionon its subject matter (Pang and Lee, 2004;Turney, 2002);3. determining the strength of document orien-tation, as in deciding e.g.
whether the Posi-tive opinion expressed by a text on its subjectmatter is Weakly Positive, Mildly Positive,or Strongly Positive (Wilson et al, 2004).To aid these tasks, recent work (Esuli and Se-bastiani, 2005; Hatzivassiloglou and McKeown,1997; Kamps et al, 2004; Kim and Hovy, 2004;Takamura et al, 2005; Turney and Littman, 2003)has tackled the issue of identifying the orientationof subjective terms contained in text, i.e.
determin-ing whether a term that carries opinionated contenthas a positive or a negative connotation (e.g.
de-ciding that ?
using Turney and Littman?s (2003)examples ?
honest and intrepid have apositive connotation while disturbing andsuperfluous have a negative connotation).193This is believed to be of key importance for iden-tifying the orientation of documents, since it isby considering the combined contribution of theseterms that one may hope to solve Tasks 1, 2 and 3above.
The conceptually simplest approach to thislatter problem is probably Turney?s (2002), whohas obtained interesting results on Task 2 by con-sidering the algebraic sum of the orientations ofterms as representative of the orientation of thedocument they belong to; but more sophisticatedapproaches are also possible (Hatzivassiloglou andWiebe, 2000; Riloff et al, 2003; Wilson et al,2004).Implicit in most works dealing with term orien-tation is the assumption that, for many languagesfor which one would like to perform opinion min-ing, there is no available lexical resource whereterms are tagged as having either a Positive or aNegative connotation, and that in the absence ofsuch a resource the only available route is to gen-erate such a resource automatically.However, we think this approach lacks real-ism, since it is also true that, for the very samelanguages, there is no available lexical resourcewhere terms are tagged as having either a Subjec-tive or an Objective connotation.
Thus, the avail-ability of an algorithm that tags Subjective termsas being either Positive or Negative is of littlehelp, since determining if a term is Subjective isitself non-trivial.In this paper we confront the task of de-termining whether a given term has a Pos-itive connotation (e.g.
honest, intrepid),or a Negative connotation (e.g.
disturbing,superfluous), or has instead no Subjectiveconnotation at all (e.g.
white, triangular);this problem thus subsumes the problem of decid-ing between Subjective and Objective and theproblem of deciding between Positive and Neg-ative.
We tackle this problem by testing three dif-ferent variants of the semi-supervised method fororientation detection proposed in (Esuli and Se-bastiani, 2005).
Our results show that determiningsubjectivity and orientation is a much harder prob-lem than determining orientation alone.1.1 Outline of the paperThe rest of the paper is structured as follows.
Sec-tion 2 reviews related work dealing with term ori-entation and/or subjectivity detection.
Section 3briefly reviews the semi-supervised method fororientation detection presented in (Esuli and Se-bastiani, 2005).
Section 4 describes in detail threedifferent variants of it we propose for determining,at the same time, subjectivity and orientation, anddescribes the general setup of our experiments.
InSection 5 we discuss the results we have obtained.Section 6 concludes.2 Related work2.1 Determining term orientationMost previous works dealing with the propertiesof terms within an opinion mining perspectivehave focused on determining term orientation.Hatzivassiloglou and McKeown (1997) attemptto predict the orientation of subjective adjectivesby analysing pairs of adjectives (conjoined byand, or, but, either-or, or neither-nor)extracted from a large unlabelled document set.The underlying intuition is that the act of conjoin-ing adjectives is subject to linguistic constraintson the orientation of the adjectives involved; e.g.and usually conjoins adjectives of equal orienta-tion, while but conjoins adjectives of oppositeorientation.
The authors generate a graph whereterms are nodes connected by ?equal-orientation?or ?opposite-orientation?
edges, depending on theconjunctions extracted from the document set.
Aclustering algorithm then partitions the graph intoa Positive cluster and a Negative cluster, basedon a relation of similarity induced by the edges.Turney and Littman (2003) determine term ori-entation by bootstrapping from two small sets ofsubjective ?seed?
terms (with the seed set for Pos-itive containing terms such as good and nice,and the seed set for Negative containing termssuch as bad and nasty).
Their method is basedon computing the pointwise mutual information(PMI) of the target term t with each seed termti as a measure of their semantic association.Given a target term t, its orientation value O(t)(where positive value means positive orientation,and higher absolute value means stronger orien-tation) is given by the sum of the weights of itssemantic association with the seed positive termsminus the sum of the weights of its semantic as-sociation with the seed negative terms.
For com-puting PMI, term frequencies and co-occurrencefrequencies are measured by querying a documentset by means of the AltaVista search engine1 witha ?t?
query, a ?ti?
query, and a ?t NEAR ti?
query,and using the number of matching documents re-turned by the search engine as estimates of theprobabilities needed for the computation of PMI.Kamps et al (2004) consider instead the graphdefined on adjectives by the WordNet2 synonymyrelation, and determine the orientation of a target1http://www.altavista.com/2http://wordnet.princeton.edu/194adjective t contained in the graph by comparingthe lengths of (i) the shortest path between t andthe seed term good, and (ii) the shortest path be-tween t and the seed term bad: if the former isshorter than the latter, than t is deemed to be Pos-itive, otherwise it is deemed to be Negative.Takamura et al (2005) determine term orienta-tion (for Japanese) according to a ?spin model?,i.e.
a physical model of a set of electrons eachendowed with one between two possible spin di-rections, and where electrons propagate their spindirection to neighbouring electrons until the sys-tem reaches a stable configuration.
The authorsequate terms with electrons and term orientationto spin direction.
They build a neighbourhood ma-trix connecting each pair of terms if one appears inthe gloss of the other, and iteratively apply the spinmodel on the matrix until a ?minimum energy?configuration is reached.
The orientation assignedto a term then corresponds to the spin direction as-signed to electrons.The system of Kim and Hovy (2004) tackles ori-entation detection by attributing, to each term, apositivity score and a negativity score; interest-ingly, terms may thus be deemed to have both apositive and a negative correlation, maybe withdifferent degrees, and some terms may be deemedto carry a stronger positive (or negative) orienta-tion than others.
Their system starts from a setof positive and negative seed terms, and expandsthe positive (resp.
negative) seed set by adding toit the synonyms of positive (resp.
negative) seedterms and the antonyms of negative (resp.
positive)seed terms.
The system classifies then a targetterm t into either Positive or Negative by meansof two alternative learning-free methods based onthe probabilities that synonyms of t also appear inthe respective expanded seed sets.
A problem withthis method is that it can classify only terms thatshare some synonyms with the expanded seed sets.Kim and Hovy also report an evaluation of humaninter-coder agreement.
We compare this evalua-tion with our results in Section 5.The approach we have proposed for determin-ing term orientation (Esuli and Sebastiani, 2005)is described in more detail in Section 3, since itwill be extensively used in this paper.All these works evaluate the performance ofthe proposed algorithms by checking them againstprecompiled sets of Positive and Negative terms,i.e.
checking how good the algorithms are at clas-sifying a term known to be subjective into eitherPositive or Negative.
When tested on the samebenchmarks, the methods of (Esuli and Sebastiani,2005; Turney and Littman, 2003) have performedwith comparable accuracies (however, the methodof (Esuli and Sebastiani, 2005) is much more effi-cient than the one of (Turney and Littman, 2003)),and have outperformed the method of (Hatzivas-siloglou and McKeown, 1997) by a wide marginand the one by (Kamps et al, 2004) by a verywide margin.
The methods described in (Hatzi-vassiloglou and McKeown, 1997) is also limitedby the fact that it can only decide the orientationof adjectives, while the method of (Kamps et al,2004) is further limited in that it can only workon adjectives that are present in WordNet.
Themethods of (Kim and Hovy, 2004; Takamura etal., 2005) are instead difficult to compare with theother ones since they were not evaluated on pub-licly available datasets.2.2 Determining term subjectivityRiloff et al (2003) develop a method to determinewhether a term has a Subjective or an Objectiveconnotation, based on bootstrapping algorithms.The method identifies patterns for the extractionof subjective nouns from text, bootstrapping froma seed set of 20 terms that the authors judge to bestrongly subjective and have found to have highfrequency in the text collection from which thesubjective nouns must be extracted.
The resultsof this method are not easy to compare with theones we present in this paper because of the dif-ferent evaluation methodologies.
While we adoptthe evaluation methodology used in all of the pa-pers reviewed so far (i.e.
checking how good oursystem is at replicating an existing, independentlymotivated lexical resource), the authors do not testtheir method on an independently identified set oflabelled terms, but on the set of terms that the algo-rithm itself extracts.
This evaluation methodologyonly allows to test precision, and not accuracy toutcourt, since no quantification can be made of falsenegatives (i.e.
the subjective terms that the algo-rithm should have spotted but has not spotted).
InSection 5 this will prevent us from drawing com-parisons between this method and our own.Baroni and Vegnaduzzo (2004) apply the PMImethod, first used by Turney and Littman (2003)to determine term orientation, to determine termsubjectivity.
Their method uses a small set Ssof 35 adjectives, marked as subjective by humanjudges, to assign a subjectivity score to each adjec-tive to be classified.
Therefore, their method, un-like our own, does not classify terms (i.e.
take firmclassification decisions), but ranks them accordingto a subjectivity score, on which they evaluate pre-cision at various level of recall.1953 Determining term subjectivity andterm orientation by semi-supervisedlearningThe method we use in this paper for determiningterm subjectivity and term orientation is a variantof the method proposed in (Esuli and Sebastiani,2005) for determining term orientation alone.This latter method relies on training, in a semi-supervised way, a binary classifier that labelsterms as either Positive or Negative.
A semi-supervised method is a learning process wherebyonly a small subset L ?
Tr of the training dataTr are human-labelled.
In origin the trainingdata in U = Tr ?
L are instead unlabelled; itis the process itself that labels them, automati-cally, by using L (with the possible addition ofother publicly available resources) as input.
Themethod of (Esuli and Sebastiani, 2005) starts fromtwo small seed (i.e.
training) sets Lp and Ln ofknown Positive and Negative terms, respectively,and expands them into the two final training setsTrp ?
Lp and Trn ?
Ln by adding them new setsof terms Up and Un found by navigating the Word-Net graph along the synonymy and antonymy re-lations3.
This process is based on the hypothesisthat synonymy and antonymy, in addition to defin-ing a relation of meaning, also define a relation oforientation, i.e.
that two synonyms typically havethe same orientation and two antonyms typicallyhave opposite orientation.
The method is iterative,generating two sets Trkp and Trkn at each iterationk, where Trkp ?
Trk?1p ?
.
.
.
?
Tr1p = Lpand Trkn ?
Trk?1n ?
.
.
.
?
Tr1n = Ln.
Atiteration k, Trkp is obtained by adding to Trk?1pall synonyms of terms in Trk?1p and all antonymsof terms in Trk?1n ; similarly, Trkn is obtained byadding to Trk?1n all synonyms of terms in Trk?1nand all antonyms of terms in Trk?1p .
If a total ofKiterations are performed, then Tr = TrKp ?
TrKn .The second main feature of the method pre-sented in (Esuli and Sebastiani, 2005) is that termsare given vectorial representations based on theirWordNet glosses (i.e.
textual definitions).
Foreach term ti in Tr ?
Te (Te being the test set, i.e.the set of terms to be classified), a textual represen-tation of ti is generated by collating all the glossesof ti as found in WordNet4.
Each such represen-3Several other WordNet lexical relations, and severalcombinations of them, are tested in (Esuli and Sebastiani,2005).
In the present paper we only use the best-performingsuch combination, as described in detail in Section 4.2.
Theversion of WordNet used here and in (Esuli and Sebastiani,2005) is 2.0.4In general a term ti may have more than one gloss, sincetation is converted into vectorial form by standardtext indexing techniques (in (Esuli and Sebastiani,2005) and in the present work, stop words areremoved and the remaining words are weightedby cosine-normalized tf idf ; no stemming is per-formed)5.
This representation method is based onthe assumption that terms with a similar orienta-tion tend to have ?similar?
glosses: for instance,that the glosses of honest and intrepid willboth contain appreciative expressions, while theglosses of disturbing and superfluouswill both contain derogative expressions.
Notethat this method allows to classify any term, in-dependently of its POS, provided there is a glossfor it in the lexical resource.Once the vectorial representations for all termsin Tr?Te have been generated, those for the termsin Tr are fed to a supervised learner, which thusgenerates a binary classifier.
This latter, once fedwith the vectorial representations of the terms inTe, classifies each of them as either Positive orNegative.4 ExperimentsIn this paper we extend the method of (Esuli andSebastiani, 2005) to the determination of term sub-jectivity and term orientation altogether.4.1 Test setsThe benchmark (i.e.
test set) we use for our exper-iments is the General Inquirer (GI) lexicon (Stoneet al, 1966).
This is a lexicon of terms labelledaccording to a large set of categories6, each onedenoting the presence of a specific trait in theterm.
The two main categories, and the ones wewill be concerned with, are Positive/Negative,which contain 1,915/2,291 terms having a posi-tive/negative orientation (in what follows we willalso refer to the category Subjective, which wedefine as the union of the two categories Positiveand Negative).
In opinion mining research the GIwas first used by Turney and Littman (2003), whoreduced the list of terms to 1,614/1,982 entries af-it may have more than one sense; dictionaries normally asso-ciate one gloss to each sense.5Several combinations of subparts of a WordNet gloss aretested as textual representations of terms in (Esuli and Sebas-tiani, 2005).
Of all those combinations, in the present paperwe always use the DGS?
combination, since this is the onethat has been shown to perform best in (Esuli and Sebastiani,2005).
DGS?
corresponds to using the entire gloss and per-forming negation propagation on its text, i.e.
replacing all theterms that occur after a negation in a sentence with negatedversions of the term (see (Esuli and Sebastiani, 2005) for de-tails).6The definitions of all such categories are available athttp://www.webuse.umd.edu:9090/196ter removing 17 terms appearing in both categories(e.g.
deal) and reducing all the multiple entriesof the same term in a category, caused by multi-ple senses, to a single entry.
Likewise, we takeall the 7,582 GI terms that are not labelled as ei-ther Positive or Negative, as being (implicitly)labelled as Objective, and reduce them to 5,009terms after combining multiple entries of the sameterm, caused by multiple senses, to a single entry.The effectiveness of our classifiers will thus beevaluated in terms of their ability to assign the to-tal 8,605 GI terms to the correct category amongPositive, Negative, and Objective7.4.2 Seed sets and training setsSimilarly to (Esuli and Sebastiani, 2005), ourtraining set is obtained by expanding initial seedsets by means of WordNet lexical relations.
Themain difference is that our training set is nowthe union of three sets of training terms Tr =TrKp ?TrKn ?TrKo obtained by expanding, throughK iterations, three seed sets Tr1p, T r1n, T r1o , onefor each of the categories Positive, Negative, andObjective, respectively.Concerning categories Positive and Negative,we have used the seed sets, expansion policy, andnumber of iterations, that have performed best inthe experiments of (Esuli and Sebastiani, 2005),i.e.
the seed sets Tr1p = {good} and Tr1n ={bad} expanded by using the union of synonymyand indirect antonymy, restricting the relationsonly to terms with the same POS of the originalterms (i.e.
adjectives), for a total of K = 4 itera-tions.
The final expanded sets contain 6,053 Pos-itive terms and 6,874 Negative terms.Concerning the category Objective, the pro-cess we have followed is similar, but with a fewkey differences.
These are motivated by the factthat the Objective category coincides with thecomplement of the union of Positive and Neg-ative; therefore, Objective terms are more var-ied and diverse in meaning than the terms in theother two categories.
To obtain a representativeexpanded set TrKo , we have chosen the seed setTr1o = {entity} and we have expanded it byusing, along with synonymy and antonymy, theWordNet relation of hyponymy (e.g.
vehicle /car), and without imposing the restriction that thetwo related terms must have the same POS.
Thesechoices are strictly related to each other: the termentity is the root term of the largest generaliza-tion hierarchy in WordNet, with more than 40,0007We make this labelled term set available for download athttp://patty.isti.cnr.it/?esuli/software/SentiGI.tgz.terms (Devitt and Vogel, 2004), thus allowing toreach a very large number of terms by using thehyponymy relation8.
Moreover, it seems reason-able to assume that terms that refer to entities arelikely to have an ?objective?
nature, and that hy-ponyms (and also synonyms and antonyms) of anobjective term are also objective.
Note that, ateach iteration k, a given term t is added to Trkoonly if it does not already belong to either Trp orTrn.
We experiment with two different choicesfor the Tro set, corresponding to the sets gener-ated in K = 3 and K = 4 iterations, respectively;this yields sets Tr3o and Tr4o consisting of 8,353and 33,870 training terms, respectively.4.3 Learning approaches and evaluationmeasuresWe experiment with three ?philosophically?
dif-ferent learning approaches to the problem of dis-tinguishing between Positive, Negative, and Ob-jective terms.Approach I is a two-stage method which con-sists in learning two binary classifiers: the firstclassifier places terms into either Subjective orObjective, while the second classifier placesterms that have been classified as Subjective bythe first classifier into either Positive or Negative.In the training phase, the terms in TrKp ?
TrKn areused as training examples of category Subjective.Approach II is again based on learning two bi-nary classifiers.
Here, one of them must discrim-inate between terms that belong to the Positivecategory and ones that belong to its complement(not Positive), while the other must discriminatebetween terms that belong to the Negative cate-gory and ones that belong to its complement (notNegative).
Terms that have been classified bothinto Positive by the former classifier and into (notNegative) by the latter are deemed to be positive,and terms that have been classified both into (notPositive) by the former classifier and into Nega-tive by the latter are deemed to be negative.
Theterms that have been classified (i) into both (notPositive) and (not Negative), or (ii) into bothPositive and Negative, are taken to be Objec-tive.
In the training phase of Approach II, theterms in TrKn ?
TrKo are used as training exam-ples of category (not Positive), and the terms inTrKp ?
TrKo are used as training examples of cat-egory (not Negative).Approach III consists instead in viewing Posi-tive, Negative, and Objective as three categories8The synonymy relation connects instead only 10,992terms at most (Kamps et al, 2004).197with equal status, and in learning a ternary clas-sifier that classifies each term into exactly oneamong the three categories.There are several differences among these threeapproaches.
A first difference, of a conceptualnature, is that only Approaches I and III viewObjective as a category, or concept, in its ownright, while Approach II views objectivity as anonexistent entity, i.e.
as the ?absence of subjec-tivity?
(in fact, in Approach II the training exam-ples of Objective are only used as training exam-ples of the complements of Positive and Nega-tive).
A second difference is that Approaches I andII are based on standard binary classification tech-nology, while Approach III requires ?multiclass?(i.e.
1-of-m) classification.
As a consequence,while for the former we use well-known learn-ers for binary classification (the naive Bayesianlearner using the multinomial model (McCallumand Nigam, 1998), support vector machines us-ing linear kernels (Joachims, 1998), the Roc-chio learner, and its PrTFIDF probabilistic version(Joachims, 1997)), for Approach III we use theirmulticlass versions9.Before running our learners we make a pass offeature selection, with the intent of retaining onlythose features that are good at discriminating ourcategories, while discarding those which are not.Feature selection is implemented by scoring eachfeature fk (i.e.
each term that occurs in the glossesof at least one training term) by means of the mu-tual information (MI) function, defined asMI(fk) =?c?{c1,...,cm},f?
{fk,fk}Pr(f, c) ?
log Pr(f, c)Pr(f) Pr(c) (1)and discarding the x% features fk that minimizeit.
We will call x% the reduction factor.
Note thatthe set {c1, .
.
.
, cm} from Equation 1 is interpreteddifferently in Approaches I to III, and always con-sistently with who the categories at stake are.Since the task we aim to solve is manifold, wewill evaluate our classifiers according to two eval-uation measures:?
SO-accuracy, i.e.
the accuracy of a classifierin separating Subjective from Objective, i.e.in deciding term subjectivity alone;?
PNO-accuracy, the accuracy of a classifierin discriminating among Positive, Negative,9The naive Bayesian, Rocchio, and PrTFIDF learnerswe have used are from Andrew McCallum?s Bow package(http://www-2.cs.cmu.edu/?mccallum/bow/),while the SVMs learner we have used is Thorsten Joachims?SV M light (http://svmlight.joachims.org/),version 6.01.
Both packages allow the respective learners tobe run in ?multiclass?
fashion.Table 1: Average and best accuracy values overthe four dimensions analysed in the experiments.Dimension SO-accuracy PNO-accuracyAvg (?)
Best Avg (?)
BestApproachI .635 (.020) .668 .595 (.029) .635II .636 (.033) .676 .614 (.037) .660III .635 (.036) .674 .600 (.039) .648LearnerNB .653 (.014) .674 .619 (.022) .647SVMs .627 (.033) .671 .601 (.037) .658Rocchio .624 (.030) .654 .585 (.033) .616PrTFIDF .637 (.031) .676 .606 (.042) .660TSR0% .649 (.025) .676 .619 (.027) .66050% .650 (.022) .670 .622 (.022) .65780% .646 (.023) .674 .621 (.021) .64790% .642 (.024) .667 .616 (.024) .65195% .635 (.027) .671 .606 (.031) .65899% .612 (.036) .661 .570 (.049) .647TrKo setTr3o .645 (.006) .676 .608 (.007) .658Tr4o .633 (.013) .674 .610 (.018) .660and Objective, i.e.
in deciding both term ori-entation and subjectivity.5 ResultsWe present results obtained from running everycombination of (i) the three approaches to classifi-cation described in Section 4.3, (ii) the four learn-ers mentioned in the same section, (iii) five dif-ferent reduction factors for feature selection (0%,50%, 90%, 95%, 99%), and (iv) the two differenttraining sets (Tr3o and Tr4o) for Objective men-tioned in Section 4.2.
We discuss each of thesefour dimensions of the problem individually, foreach one reporting results averaged across all theexperiments we have run (see Table 1).The first and most important observation is that,with respect to a pure term orientation task, ac-curacy drops significantly.
In fact, the best SO-accuracy and the best PNO-accuracy results ob-tained across the 120 different experiments are.676 and .660, respectively (these were obtainedby using Approach II with the PrTFIDF learnerand no feature selection, with Tro = Tr3o for the.676 SO-accuracy result and Tro = Tr4o for the.660 PNO-accuracy result); this contrasts sharplywith the accuracy obtained in (Esuli and Sebas-tiani, 2005) on discriminating Positive from Neg-ative (where the best run obtained .830 accuracy),on the same benchmarks and essentially the samealgorithms.
This suggests that good performanceat orientation detection (as e.g.
in (Esuli and Se-bastiani, 2005; Hatzivassiloglou and McKeown,1997; Turney and Littman, 2003)) may not be a198Table 2: Human inter-coder agreement values re-ported by Kim and Hovy (2004).Agreement Adjectives (462) Verbs (502)measure Hum1 vs Hum2 Hum2 vs Hum3Strict .762 .623Lenient .890 .851guarantee of good performance at subjectivity de-tection, quite evidently a harder (and, as we havesuggested, more realistic) task.This hypothesis is confirmed by an experimentperformed by Kim and Hovy (2004) on testingthe agreement of two human coders at taggingwords with the Positive, Negative, and Objec-tive labels.
The authors define two measures ofsuch agreement: strict agreement, equivalent toour PNO-accuracy, and lenient agreement, whichmeasures the accuracy at telling Negative againstthe rest.
For any experiment, strict agreement val-ues are then going to be, by definition, lower orequal than the corresponding lenient ones.
The au-thors use two sets of 462 adjectives and 502 verbs,respectively, randomly extracted from the basicEnglish word list of the TOEFL test.
The inter-coder agreement results (see Table 2) show a de-terioration in agreement (from lenient to strict) of16.77% for adjectives and 36.42% for verbs.
Fol-lowing this, we evaluated our best experiment ac-cording to these measures, and obtained a ?strict?accuracy value of .660 and a ?lenient?
accuracyvalue of .821, with a relative deterioration of24.39%, in line with Kim and Hovy?s observa-tion10.
This confirms that determining subjectivityand orientation is a much harder task than deter-mining orientation alone.The second important observation is that thereis very little variance in the results: across all 120experiments, average SO-accuracy and PNO-accuracy results were .635 (with standard devia-tion ?
= .030) and .603 (?
= .036), a mere6.06% and 8.64% deterioration from the best re-sults reported above.
This seems to indicate thatthe levels of performance obtained may be hard toimprove upon, especially if working in a similarframework.Let us analyse the individual dimensions of theproblem.
Concerning the three approaches to clas-sification described in Section 4.3, Approach IIoutperforms the other two, but by an extremelynarrow margin.
As for the choice of learners, onaverage the best performer is NB, but again by avery small margin wrt the others.
On average, the10We observed this trend in all of our experiments.best reduction factor for feature selection turns outto be 50%, but the performance drop we witnessin approaching 99% (a dramatic reduction factor)is extremely graceful.
As for the choice of TrKo ,we note that Tr3o and Tr4o elicit comparable levelsof performance, with the former performing bestat SO-accuracy and the latter performing best atPNO-accuracy.An interesting observation on the learners wehave used is that NB, PrTFIDF and SVMs, un-like Rocchio, generate classifiers that depend onP (ci), the prior probabilities of the classes, whichare normally estimated as the proportion of train-ing documents that belong to ci.
In many classi-fication applications this is reasonable, as we mayassume that the training data are sampled from thesame distribution from which the test data are sam-pled, and that these proportions are thus indica-tive of the proportions that we are going to en-counter in the test data.
However, in our appli-cation this is not the case, since we do not have a?natural?
sample of training terms.
What we haveis one human-labelled training term for each cat-egory in {Positive,Negative,Objective}, and asmany machine-labelled terms as we deem reason-able to include, in possibly different numbers forthe different categories; and we have no indica-tion whatsoever as to what the ?natural?
propor-tions among the three might be.
This means thatthe proportions of Positive, Negative, and Ob-jective terms we decide to include in the train-ing set will strongly bias the classification resultsif the learner is one of NB, PrTFIDF and SVMs.We may notice this by looking at Table 3, whichshows the average proportion of test terms classi-fied as Objective by each learner, depending onwhether we have chosen Tro to coincide with Tr3oor Tr4o ; note that the former (resp.
latter) choicemeans having roughly as many (resp.
roughly fivetimes as many) Objective training terms as thereare Positive and Negative ones.
Table 3 showsthat, the more Objective training terms there are,the more test terms NB, PrTFIDF and (in partic-ular) SVMs will classify as Objective; this is nottrue for Rocchio, which is basically unaffected bythe variation in size of Tro.6 ConclusionsWe have presented a method for determining bothterm subjectivity and term orientation for opinionmining applications.
This is a valuable advancewith respect to the state of the art, since past workin this area had mostly confined to determiningterm orientation alone, a task that (as we have ar-199Table 3: Average proportion of test terms classi-fied as Objective, for each learner and for eachchoice of the TrKo set.Learner Tr3o Tr4o VariationNB .564 (?
= .069) .693 (.069) +23.0%SVMs .601 (.108) .814 (.083) +35.4%Rocchio .572 (.043) .544 (.061) -4.8%PrTFIDF .636 (.059) .763 (.085) +20.0%gued) has limited practical significance in itself,given the generalized absence of lexical resourcesthat tag terms as being either Subjective or Ob-jective.
Our algorithms have tagged by orienta-tion and subjectivity the entire General Inquirerlexicon, a complete general-purpose lexicon thatis the de facto standard benchmark for researchersin this field.
Our results thus constitute, for thistask, the first baseline for other researchers to im-prove upon.Unfortunately, our results have shown thatan algorithm that had shown excellent, state-of-the-art performance in deciding term orienta-tion (Esuli and Sebastiani, 2005), once modifiedfor the purposes of deciding term subjectivity, per-forms more poorly.
This has been shown by test-ing several variants of the basic algorithm, someof them involving radically different supervisedlearning policies.
The results suggest that decid-ing term subjectivity is a substantially harder taskthat deciding term orientation alone.ReferencesM.
Baroni and S. Vegnaduzzo.
2004.
Identifying subjec-tive adjectives through Web-based mutual information.
InProceedings of KONVENS-04, 7th Konferenz zur Verar-beitung Natu?rlicher Sprache (German Conference on Nat-ural Language Processing), pages 17?24, Vienna, AU.Ann Devitt and Carl Vogel.
2004.
The topology of WordNet:Some metrics.
In Proceedings of GWC-04, 2nd GlobalWordNet Conference, pages 106?111, Brno, CZ.Andrea Esuli and Fabrizio Sebastiani.
2005.
Determin-ing the semantic orientation of terms through gloss analy-sis.
In Proceedings of CIKM-05, 14th ACM InternationalConference on Information and Knowledge Management,pages 617?624, Bremen, DE.Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997.Predicting the semantic orientation of adjectives.
In Pro-ceedings of ACL-97, 35th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 174?181,Madrid, ES.Vasileios Hatzivassiloglou and Janyce M. Wiebe.
2000.
Ef-fects of adjective orientation and gradability on sentencesubjectivity.
In Proceedings of COLING-00, 18th Inter-national Conference on Computational Linguistics, pages174?181, Saarbru?cken, DE.Thorsten Joachims.
1997.
A probabilistic analysis of theRocchio algorithm with TFIDF for text categorization.
InProceedings of ICML-97, 14th International Conferenceon Machine Learning, pages 143?151, Nashville, US.Thorsten Joachims.
1998.
Text categorization with supportvector machines: learning with many relevant features.
InProceedings of ECML-98, 10th European Conference onMachine Learning, pages 137?142, Chemnitz, DE.Jaap Kamps, Maarten Marx, Robert J. Mokken, and MaartenDe Rijke.
2004.
Using WordNet to measure semantic ori-entation of adjectives.
In Proceedings of LREC-04, 4th In-ternational Conference on Language Resources and Eval-uation, volume IV, pages 1115?1118, Lisbon, PT.Soo-Min Kim and Eduard Hovy.
2004.
Determining the sen-timent of opinions.
In Proceedings of COLING-04, 20thInternational Conference on Computational Linguistics,pages 1367?1373, Geneva, CH.Andrew K. McCallum and Kamal Nigam.
1998.
A compari-son of event models for naive Bayes text classification.
InProceedings of the AAAI Workshop on Learning for TextCategorization, pages 41?48, Madison, US.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of ACL-04, 42ndMeeting of the Association for Computational Linguistics,pages 271?278, Barcelona, ES.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction pattern boot-strapping.
In Proceedings of CONLL-03, 7th Conferenceon Natural Language Learning, pages 25?32, Edmonton,CA.P.
J.
Stone, D. C. Dunphy, M. S. Smith, and D. M. Ogilvie.1966.
The General Inquirer: A Computer Approach toContent Analysis.
MIT Press, Cambridge, US.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting emotional polarity of words using spinmodel.
In Proceedings of ACL-05, 43rd Annual Meetingof the Association for Computational Linguistics, pages133?140, Ann Arbor, US.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orientationfrom association.
ACM Transactions on Information Sys-tems, 21(4):315?346.Peter Turney.
2002.
Thumbs up or thumbs down?
Seman-tic orientation applied to unsupervised classification of re-views.
In Proceedings of ACL-02, 40th Annual Meetingof the Association for Computational Linguistics, pages417?424, Philadelphia, US.Theresa Wilson, Janyce Wiebe, and Rebecca Hwa.
2004.Just how mad are you?
Finding strong and weak opinionclauses.
In Proceedings of AAAI-04, 21st Conference ofthe American Association for Artificial Intelligence, pages761?769, San Jose, US.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts from opin-ions and identifying the polarity of opinion sentences.
InProceedings of EMNLP-03, 8th Conference on Empiri-cal Methods in Natural Language Processing, pages 129?136, Sapporo, JP.200
