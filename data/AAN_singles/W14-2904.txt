Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?25,Baltimore, Maryland, USA, June 22-27, 2014.c?2014 Association for Computational LinguisticsInter-Annotator Agreement for ERE AnnotationSeth Kulick and Ann Bies and Justin MottLinguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104{skulick,bies,jmott}@ldc.upenn.eduAbstractThis paper describes a system for inter-annotator agreement analysis of ERE an-notation, focusing on entity mentions andhow the higher-order annotations such asEVENTS are dependent on those entitymentions.
The goal of this approach is toprovide both (1) quantitative scores for thevarious levels of annotation, and (2) infor-mation about the types of annotation in-consistencies that might exist.
While pri-marily designed for inter-annotator agree-ment, it can also be considered a systemfor evaluation of ERE annotation.1 IntroductionIn this paper we describe a system for analyz-ing dually human-annotated files of Entities, Re-lations, and Events (ERE) annotation for consis-tency between the two files.
This is an importantaspect of training new annotators, to evaluate theconsistency of their annotation with a ?gold?
file,or to evaluate the agreement between two anno-tators.
We refer to both cases here as the task of?inter-annotator agreement?
(IAA).The light ERE annotation task was defined aspart of the DARPA DEFT program (LDC, 2014a;LDC, 2014b; LDC, 2014c) as a simpler versionof tasks like ACE (Doddington et al., 2004) to al-low quick annotation of a simplified ontology ofentities, relations, and events, along with iden-tity coreference.
The ENTITIES consist of co-referenced entity mentions, which refer to a spanof text in the source file.
The entity mentions arealso used as part of the annotation of RELATIONSand EVENTS, as a stand in for the whole ENTITY.The ACE program had a scoring metric de-scribed in (Doddington et al., 2004).
However,our emphasis for IAA evaluation is somewhat dif-ferent than that of scoring annotation files for ac-curacy with regard to a gold standard.
The IAAsystem aims to produce output to help an annota-tion manager understand the sorts of errors occur-ring, and the general range of possible problems.Nevertheless, the approach to IAA evaluation de-scribed here can be used for scoring as well.
Thisapproach is inspired by the IAA work for tree-banks in Kulick et al.
(2013).Because the entity mentions in ERE are the fun-damental units used for the ENTITY, EVENT andRELATION annotations, they are also the funda-mental units upon which the IAA evaluation isbased.
The description of the system therefore be-gins with a focus on the evaluation of the consis-tency of the entity mention annotations.
We derivea mapping between the entity mentions betweenthe two files (henceforth called File A and FileB).
We then move on to ENTITIES, RELATIONS,and EVENTS, pointing out the differences betweenthem for purposes of evaluation, but also their sim-ilarities.1This is a first towards a more accurate use ofthe full ENTITIES in the comparison and scoringof ENTITIES and EVENTS annotations.
Work toexpand in this direction is in progress.
When amore complete system is in place it will be moreappropriate to report corpus-based results.2 Entity MentionsThere are two main aspects to the system?s han-dling of entity mentions.
First we describe themapping of entity mentions between the two an-notators.
As in Doddington et al.
(2004), the pos-sibility of overlapping mentions can make this acomplex problem.
Second, we describe how oursystem?s output categorizes possible errors.1This short paper focuses on the design of the IAA sys-tem, rather than reporting on the results for a specific dataset.The IAA system has been run on dually annotated ERE data,however, which was the source for the examples in this paper.21m-502m-892m-398SOUTH OF IRANm-463A'smentionsB's mentions THE EASTTHE EAST     AND    SOUTH OF     IRANFigure 1: Case of ambiguous Entity Mention map-ping disambiguated by another unambiguous map-ping2.1 MappingAs mentioned in the introduction, our system de-rives a mapping between the entity mentions inFiles A and B, as the basis for all further eval-uation of the ERE annotations.
Entity mentionsin Files A and B which have exactly the same lo-cation (offset and length) are trivially mapped toeach other.
We refer to these as ?exact?
matches.The remaining cases fall into two categories.One is the case of when an entity mention in onefile overlaps with one and only one entity men-tion in the other file.
We refer to these as the ?un-ambiguous?
overlapping matches.
It is also pos-sible for an entity mention in one file to overlapwith more than one entity mention in the other file.We refer to these as the ?ambiguous?
overlappingmatches, and these patterns can get quite complexif multiple ambiguous overlapping matches are in-volved.2.1.1 Disambiguation by separateunambiguous mappingHere an ambiguous overlapping is disambiguatedby the presence of an unambiguous mapping, andthe choice for mapping the ambiguous case is de-cided by the desire to maximize the number ofmapped entity mentions.Figure 1 shows such a case.
File A has two en-tity mentions annotations (m-502 and m-463) andFile B has two entity mention annotations (m-398and m-892).
These all refer to the same span oftext, so m-502 (THE EAST) and m-463 (SOUTHOF IRAN) both overlap with m-398 in File B(THE EAST AND SOUTH OF IRAN).
m-463 inaddition overlaps with m-892 (IRAN).We approach the mapping from the perspectiveof File A.
If we assign the mapping for m-463 tobe m-398, it will leave m-502 without a match,since m-398 will already be used in the mapping.Therefore, we assign m-502 and m-398 to map tom-905m-788TALIBAN     MILITIA  m-892A'smentionsB's mentions THE NOW-OUSTED TALIBAN     MILITIAFigure 2: Case of Entity Mention mapping re-solved by maximum overlapeach other, while m-463 and m-892 are mapped toeach other.
The goal is to match as many mentionsas possible, which this accomplishes.2.1.2 Disambiguation by maximum overlapThe other case is shown in Figure 2.
Here there aretwo mentions in File A, m-892 (TALIBAN MILI-TIA) and m-905 (TALIBAN), both overlappingwith one mention in File B, m-788 (THE NOW-OUSTED TALIBAN MILITIA), so it is not pos-sible to have a matching of all the mentions.
Wechoose the mapping with greatest overlap, in termsof characters, and so m-892 and m-788 are takento match, while m-905 is left without a match.For such cases of disambiguation by maximumoverlap, it may be possible that a different match-ing, the one with less overlap, might be a betterfit for one of the higher levels of annotation.
Thisissue will be resolved in the future by using ENTI-TIES rather than ENTITY MENTIONS as the unitsto compare for the RELATION and EVENT levels.2.2 Categorization of annotationinconsistenciesOur system produces an entity mention report thatlists the number of exact matches, the number ofoverlap matches, and for Files A and B how manyentity mentions each had that did not have a corre-sponding match in the other annotator?s file.Entity mentions can overlap in different ways,some of which are more ?serious?
than other.
Wecategorize each overlapping entity mention basedon the nature of the edge differences in the non-exact match, such as the presence or absence of adeterminer or punctuation, or other material.In addition, both exact and overlap mentionscan match based on location, but be different asfar as the entity mention level (NAMed, NOMi-nal, and PROnominal).
The software also outputsall such mismatches for each match.22SUPPORTERS IN PAKISTANm-333 m-1724m-1620m-3763ENTITYA'sENTITYB'sA's ENTITY and B's ENTITY are a "complete" matchSUPPORTERS SUPPORTERSSUPPORTERS IN PAKISTANFigure 3: Complete match between File A and FileB ENTITIES despite overlapping mentionsA's ENTITY and B's ENTITY are an "incomplete" matchAL-QAEDAm-437 m-840m-2580m-424 AL-QAEDA NETWORK AL-QAEDAm-593A's ENTITYB's ENTITY 0AL-QAEDA AL-QAEDAFigure 4: Incomplete match between File A andFile B ENTITIES, because File B does not have amention corresponding to m-593 in File A3 EntitiesAn ENTITY is a group of coreferenced entity men-tions.
We use the entity mention mapping dis-cussed in Section 2 to categorize matches betweenthe ENTITIES as follows:Complete match: This means that for some EN-TITY x in File A and ENTITY y in File B, thereis a 1-1 correspondence between the mentions ofthese two ENTITIES.
For purposes of this catego-rization, we do not distinguish between exact andoverlap mapping but include both as correspond-ing mention instances, because this distinction wasalready reported as part of the mention mapping.Figure 3 shows an example of a completematch.
File A has two mentions, m-333 (SUP-PORTERS) and m-1724 (another instance of SUP-PORTERS).
These are co-referenced together toform a single ENTITY.
In File B there aretwo mentions, m-3763 (SUPPORTERS IN PAK-ISTAN) an m-1620 (another instance of SUP-PORTERS IN PAKISTAN).
It was determined bythe algorithm for entity mention mapping in Sec-tion 2.1 that m-333 and m-3763 are mapped toeach other, as are m-1724 and m-1620, althougheach pair of mentions is an overlapping match, notan exact match.
At the ENTITY level of corefer-ences mentions, there is a 1-1 mapping betweenthe mentions of A?s ENTITY and B?s ENTITY.Therefore these two ENTITIES are categorized ashaving a complete mapping between them.Incomplete match: This means that for some EN-TITY x in file A and ENTITY y in file B, there maybe some mentions that are part of x in A that haveno match in File B, but all the mentions that arepart of x map to mentions that are part of EN-TITY y in File B, and vice-versa.
Figure 4 showsan example of an incomplete match.
File A hasthree entity mentions, m-437 (AL-QAEDA), m-593 (AL-QAEDA NETWORK), and m-840 (AL-QAEDA again), coreferenced together as a singleENTITY.
File B has two entity mentions, m-424(AL-QAEDA) and m-2580 (AL-QAEDA again),coreferenced together as a single ENTITY.
Whilem-437 maps to m-424 and m-840 maps to m-2580,m-593 does not have a match in File B, causingthis to be categorized as an incomplete match.No match: It is possible that some ENTITIES maynot map to an ENTITY in the other file, if the con-ditions for neither type of match exist.
For exam-ple, if in Figure 4 m-593 mapped to a mention inFile B that was part of a different ENTITY than m-424 and m-2580, then there would not be even anincomplete match between the two ENTITIES.Similar to the mentions, ENTITIES as a wholecan match as complete or incomplete, but still dif-fer on the entity type (ORGanization, PERson,etc.).
We output such type mismatches as separateinformation for the ENTITY matching.4 RelationsA RELATION is defined as having:1) Two RELATION arguments, each of which is anENTITY.2) An optional ?trigger?, a span of text.3) A type and subtype.
(e.g., ?Physical.Located?
)For this preliminary stage of the system, wematch RELATIONS in a similar way as we dothe ENTITIES, by matching the corresponding en-tity mentions, as stand-ins for the ENTITY argu-ments for the RELATION.
We use the previously-established mapping of mentions as basis of theRELATION mapping.2We report four types of RELATION matching:31) exact match - This is the same as the complete2This is a stricter mapping requirement than is ultimatelynecessary, and future work will adjust the basis of RELATIONmapping to be full ENTITIES.3Because of space reasons and because RELATIONS areso similar to EVENTS, we do not show here an illustration ofRELATION mapping.23match for ENTITIES, except in addition checkingfor a trigger match and type/subtype.2) types different - a match for the arguments, al-though the type or subtypes of the RELATIONS donot match.
(The triggers may or may not be differ-ent for this case.
)3) triggers different - a match for the argumentsand type/subtype, although with different triggers.4) no match - the arguments for a RELATION inone file do not map to arguments for any one sin-gle RELATION in the other file.5 EventsThe structure of an EVENT is similar to that of aRELATION.
Its components are:1) One or more EVENT arguments.
Each EVENTargument is an ENTITY or a date.2) An obligatory trigger argument.3) A type and subtype (e.g., ?Life.MARRY?
)In contrast to RELATIONS, the trigger argumentis obligatory.
There must be at least one ENTITYargument (or a date argument) in order for theEVENT to qualify for annotation, although it doesnot need to be exactly two, as with RELATIONS.The mapping between EVENTS works essen-tially as for ENTITIES and RELATIONS, once againbased on the already-established mapping of theentity mentions.4There are two slight twists, how-ever.
It is possible for the only EVENT argumentto be a date, which is not an entity mention, and sowe must also establish a mapping for EVENT datearguments, as we did for the entity mentions.
Be-cause the trigger is obligatory, we treat it with thesame level of importance as the arguments, and es-tablish a mapping between EVENT triggers as well.We report three types of EVENT matching:51) exact match - all arguments match, as does thetrigger, as well as the type/subtype.2) types different - a match for the argumentsand trigger, although the type or subtypes of theEVENTS do not match.3) no match - either the arguments for a EVENT in4As with relations, this is a stricter mapping than neces-sary, and future work will adjust to use ENTITIES as EVENTarguments.5Currently, if an EVENT argument does not map to anymention in the other file, we consider the EVENT to be a ?nomatch?.
In the future we will modify this (and likewise forRELATIONS) to be more forgiving, along the lines of the ?in-complete match?
for ENTITIES.JULY 30, 2008 m-489m-255 m-268POLICEm-515 triggeragentagentpersonJULY 30, 2008MEXICO CITYplacedate m-502m-292 THE POLICEAPPREHENDEDAPPREHENDEDMEXICO CITY A DRUG TRAFFICKERpersontriggerplacedateA'sEVENTB'sEVENT A DRUG TRAFFICKERFigure 5: EVENT matchone file do not map to arguments for any one singleEVENT in the other file, or the triggers do not map.Figure 5 shows an example of an exact matchfor two EVENTS, one each in File A and B. Allof the arguments in one EVENT map to an argu-ment in the other EVENT, as does the trigger.
Notethat the argument m-502 (an entity mention, PO-LICE) in File A maps to argument m-255 (an en-tity mention, THE POLICE) in File B as an over-lap match, although the EVENTS are considered anexact match.6 Future workWe did these comparisons based on the lowest en-tity mention level in order to develop a prelimi-nary system.
However, the arguments for EVENTSand RELATIONS are ENTITIES, not entity men-tions, and the system be adjusted to do the correctcomparison.
Work to adjust the system in this di-rection is in progress.
When the full system is inplace in this way, we will report results as well.
Infuture work we will be developing a quantitativescoring metric based on the work described here.AcknowledgmentsThis material is based on research sponsored byAir Force Research Laboratory and Defense Ad-vance Research Projects Agency under agreementnumber FA8750-13-2-0045.
The U.S. Govern-ment is authorized to reproduce and distributereprints for Governmental purposes notwithstand-ing any copyright notation thereon.
The views andconclusions contained herein are those of the au-thors and should not be interpreted as necessar-ily representing the official policies or endorse-ments, either expressed or implied, of Air ForceResearch Laboratory and Defense Advanced Re-search Projects Agency or the U.S. Government.24ReferencesGeorge Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
Automatic content extraction(ACE) program - task definitions and performancemeasures.
In LREC 2004: 4th International Confer-ence on Language Resources and Evaluation.Seth Kulick, Ann Bies, Justin Mott, MohamedMaamouri, Beatrice Santorini, and Anthony Kroch.2013.
Using derivation trees for informative tree-bank inter-annotator agreement evaluation.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages550?555, Atlanta, Georgia, June.
Association forComputational Linguistics.LDC.
2014a.
DEFT ERE Annotation Guidelines: En-tities v1.6.
Technical report, Linguistic Data Con-sortium.LDC.
2014b.
DEFT ERE Annotation Guidelines:Events v1.3.
Technical report, Linguistic Data Con-sortium.LDC.
2014c.
DEFT ERE Annotation Guidelines: Re-lations v1.3.
Technical report, Linguistic Data Con-sortium.25
