Top-Down CohesionSegmentation inSummarizationDoina TatarAndreea Diana MihisGabriela SerbanUniversity "Babes?-Bolyai" Cluj-Napoca (Romania)email: dtatar@cs.ubbcluj.roAbstractThe paper proposes a new method of linear text segmentation based onlexical cohesion of a text.
Namely, first a single chain of disambiguatedwords in a text is established, then the rips of this single chain are consid-ered as boundaries for the segments of the cohesion text structure (Cohe-sion TextTiling or CTT).
The summaries of arbitrarily length are obtainedby extraction using three different methods applied to the obtained seg-ments.
The informativeness of the obtained summaries is compared withthe informativeness of the pair summaries of the same length obtained us-ing an earlier method of logical segmentation by text entailment (LogicalTextTiling or LTT).
Some experiments about CTT and LTT methods arecarried out for four ?classical" texts in summarization literature showingthat the quality of the summarization using cohesion segmentation (CTT)is better than the quality using logical segmentation (LTT).389390 Tatar, Mihis, and Serban1 IntroductionText summarization has become the subject of an intense research in the last yearsand it is still an emerging field (Orasan, 2006; Radev et al, 2002; Hovy, 2003; Mani,2001).
The research is done in the extracts (which we are treating in this paper) andabstracts areas.
The most important task of summarization is to identify the mostinformative (salient) parts of a text comparatively with the rest.
A good segmentationof a text could help in this identification (Boguraev and Neff, 2000; Barzilay andElhadad, 1999; Reynar, 1998).This paper proposes a new method of linear text segmentation based on lexicalcohesion of a text.
Namely, first a single chain of disambiguated words in a text isestablished, then the rips of this chain are considered.
These rips are boundaries of thesegments in the cohesion structure of the text.
Due to some similarities with TextTilingalgorithm for topic shifts detection of Hearst (1997), the method is called CohesionTextTiling (CTT).The paper is structured as follows: in Section 2 we present the problem of WordSense Disambiguation by a chain algorithm and the derived CTT method.
In Sec-tion 3, some notions about textual entailment and logical segmentation of a text byLTT method are discussed.
Summarization by different methods after segmentationis the topic of Section 4.
The parallel application of CTT and LTT methods to four"classical" texts in summarization literature, two narrative and two newspapers, andsome statistics of the results are presented in Section 5.
We finish the article withconclusions and possible further work directions.2 A top-down cohesion segmentation method2.1 Lexical chainsA lexical chain is a sequence of words such that the meaning of each word from the se-quence can be obtained unambiguously from the meaning of the rest of words (Morrisand Hirst, 1991; Barzilay and Elhadad, 1999; Harabagiu and Moldovan, 1997; Silberand McCoy, 2002; Stokes, 2004).
The map of all lexical chains of a text provides arepresentation of the lexical cohesive structure of the text.
Usually a lexical chain isobtained in a bottom-up fashion, by taking each candidate word of a text, and findingan appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) orWordNet (Barzilay and Elhadad, 1999).
If it is found, the word is inserted with theappropriate sense in the current chain, and the senses of the other words in the chainare updated.
If no relation is found, then a new chain is initiated.Our method approaches the construction of lexical chains in a reverse order: wefirst disambiguate the whole text and then construct the lexical chains which cover asmuch as possible the text.2.2 CHAD algorithmIt is known that in the last years many researchers studied the possibility to globallydisambiguate a text.
In Tatar et al (2007) is presented CHAD algorithm, a Lesk?stype algorithm based on WordNet, that doesn?t require syntactic analysis and syntac-tic parsing.
As usually for a Lesk?s type algorithm, it starts from the idea that a word?sdictionary definition is a good indicator for the senses of this word and uses the defi-Top-down Cohesion Segmentation in Summarization 391nition in the dictionary directly.
The base of the algorithm is the disambiguation of atriplet of words, using Dice?s overlap or Jaccard?s measures.
Shortly, CHAD beginswith the disambiguation of a triplet w1w2w3 and then adds to the right the followingwords to be disambiguated.
Hence it disambiguates at a time a new triplet, where firsttwo words are already associated with the best senses and the disambiguation of thethird word depends on the disambiguation of these first two words.Due to the brevity of definitions in WordNet (WN), the first sense in WN for a wordwi (WN 1st sense) must be associated in some cases in a "forced" way.
The forcedcondition represents the situation that any sense of wi is related with the senses of thewords wi?2 and wi?1.
Thus the forced condition signals that a lexical chain stops,and, perhaps, a new one begins.Comparing the precision obtained with CHAD and the precision obtained by theWN 1st sense algorithm for 10 files of Brown corpus (Tatar et al, 2007) we obtainedthe result: for 7 files the difference was greater or equal to 0.04 (favorable to WN 1st),and for 3 files was lower.
For example, in the worst case (Brown 01 file), the precisionsobtained by CHAD are: 0.625 for Dice?s measure, 0.627 for Overlap measure, 0.638for Jaccard?s measure while the precision obtained by WN 1st sense is 0.688.
Let usremark that CHAD is used to mark the discontinuity in cohesion, while WN 1st sensealgorithm is unable to do this.2.3 CHAD and lexical chainsThe CHAD algorithm shows what words in a sentence are unrelated as senses withthe previously words: these are the words which receive a "forced" first WN sense.Of course, these are regarded differently from the words which receive a "justified"first WN sense.
Scoring each sentence of a text by the number of "forced" to first WNsense words in this sentence, we will provide a representation of the lexical cohesivestructure of the text.
If F is this number, then the valleys (the local minima) in thegraph representing the function 1/F will represent the boundaries between lexicalchains (see Figure 2).Lexical chains could serve further as a basis for an algorithm of segmentation.
Asour method of determination of lexical chains is linear, the corresponding segmenta-tion is also linear.
The obtained segments could be used effectively in summarization.In this respect, our method of summarization falls in the discourse-based category.In contrast with other theories about discourse segmentation, as Rhetorical Struc-ture Theory (RST) of Mann and Thompson (1988), attentional/intentional structureof Grosz and Sidner (1986) or parsed RST tree of Marcu (1997), our CTT method(and also, as presented below, our LTT method) supposes a linear segmentation (ver-sus hierarchical segmentation) which results in an advantage from a computationalviewpoint.3 Segmentation by Logical TextTiling3.1 Text entailmentText entailment is an autonomous field of Natural Language Processing and it rep-resents the subject of some recent Pascal Challenges.
As is established in an earlierpaper (Tatar et al, 2007), a text T entails an hypothesis H, denoted by T ?
H, iff His less informative than T .
A method to prove T ?
H which relies on this definition392 Tatar, Mihis, and Serbanconsists in the verification of the relation: sim(T,H)T ?
sim(T,H)H .
Here sim(T,H)Tand sim(T,H)H are text-to-text similarities introduced in Corley and Mihalcea (2005).The method used by our tool for Text entailment verification calculates the similaritybetween T and H by cosine, thus the above relation becomes cos(T,H)T ?
cos(T,H)H(Tatar et al, 2007).3.2 Logical segmentationTatar et al (2008) present a method named logical segmentation because the score ofa sentence is the number of sentences of the text which are entailed by it.
Representingthe scores of sentences as a graph, we obtain a structure which indicates how the mostimportant sentences alternate with ones less important and which organizes the textaccording to its logical content.
Simply, a valley (a local minimum) in the obtainedlogical structure of the text is a boundary between two logical segments (see Figure 1).The method is called Logical TextTiling (LTT), due to some similarities with theTextTiling algorithm for topic shifts detection (Hearst, 1997).
The drawback of LTT,that the number of the segments is fixed for a given text (as it results from its logicalstructure), is eliminated by a method to dynamically correlate the number of the logi-cal segments obtained by LTT with the required length of the summary.
Let us remarkthat LTT does not require a predicate-argument analysis.
The only semantic structureprocessing required is the Text Entailment verification.4 Summarization by segmentation4.1 Scoring the segmentsAn algorithm of segmentation has usually the following function:INPUT: a list of sentences S1, ...,Sn and a list of scores score(S1), ...,score(Sn);OUTPUT: a list of segments Seg1, ...,SegN .Given a set of N segments (obtained by CTT or LTT) we need a criterion to selectthose sentences from a segment which will be introduced in the summary.
Thus,after the score of a sentence is calculated, we calculate a score of a segment.
The finalscore, Score f inal, of a sentence is weighted by the score of the segment which containsit.
The summary is generated by selecting from each segment a number of sentencesproportional with the score of the segment.
The method has some advantages when adesired level of granularity of summarization is imposed.The summarization algorithm with Arbitrarily Length of the summary (AL) is thefollowing:INPUT: The segments Seg1, ...SegN, the length of summary X (as parameter),Score f inal(Si) for each sentence Si;OUTPUT: A summary SUM of length X, where from each segment Seg j are selectedNSenSeg j sentences.
The method of selecting the sentences is given by defini-tions Sum1,Sum2,Sum3 (Section 4.2).Remark: A number of segments Seg j may have NSenSeg j > 1.
If X < N then anumber of segments Seg j must have NSenSeg j = 0Top-down Cohesion Segmentation in Summarization 393In Section 5 (Experiments) the variant of summarization algorithm as above is de-noted as Var1.
In the variant Var2 a second choice of computing the score of asegment is considered.
Namely, the score is not normalized, and it is equal with thesum of its sentences scores, without been divided to the segment length.
The draw-back of Var1 is that in some cases a very long segment can contain some sentenceswith a high score and many sentences with a very low score, the final score of thissegment will be a small one and those important sentences will not be included in thefinal summary.
The drawback of Var2 is that of increased importance of the lengthof the segment in some cases.
Thus, the score of a short segment with high sentencesscores will be less then one of a long segment with small sentences scores, and againsome important sentences will be lost.4.2 Strategies for summary calculusThe method of extracting sentences from the segments is decisive for the quality ofthe summary.
The deletion of an arbitrary amount of source material between twosentences which are adjacent in the summary has the potential of losing essential in-formation.
We propose and compare some simple strategies for including sentencesin the summary:?
Our first strategy is to include in the summary the first sentence from each seg-ment, as this is of special importance for a segment.
The corresponding sum-mary will be denoted by Sum1.?
The second way is that for each segment the sentence(s) with a maximal scoreare considered the most important for this segment, and hence they are includedin the summary.
The corresponding summary is denoted by Sum2.?
The third way of reasoning is that from each segment the most informativesentence(s) (the least similar) relative to the previously selected sentences arepicked up.
The corresponding summary is denoted by Sum3.5 ExperimentsIn our experiments for CTT method each sentence is scored as following: Score(Si) =1nuwi where nuwi is the number of words "forced" to get the first WN sense in thesentence Si.
If nuwi = 0 then Score(Si) = 2.
The graph of the logical structure for thetext Hirst is presented in Figure 1 while the graph for the cohesion structure for thesame text is presented in Figure 2.We have applied CTT and LTT methods of segmentation and summarization tofour texts denoted in the following by: Hirst (Morris and Hirst, 1991), Koan (Richie,1991), Tucker1 (Tucker, 1999) and Tucker2 (Tucker, 1999).1 The denotations are asfollowing: LSi for LTT with Sumi method, CSi for CTT with Sumi method.
Also anideal summary (IdS) has been constructed by taking the majority occurrences of thesentences in all LSi and CSi summaries.
IdS is the last raw of the table.
For the textTucker1 the summaries with five sentences obtained by us and the summary obtainedby the author with CLASP (Tucker, 1999) are presented in Table 1.1All these texts are shown on-line at http://www.cs.ubbcluj.ro/~dtatar/nlp/ (first entries).394 Tatar, Mihis, and SerbanLogical structure of Hirst text0111 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41Positions of sentencesFigure 1: The logical structure of the text HirstCohesion structure of the Hirst text02.251 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41Positions of sentencesFigure 2: The cohesion structure of the text HirstTable 1: The summaries with the length 5 for the text Tucker1 compared with theauthor?s summaryMethod 5 sent Tucker1LS1 1,16,17,34,35 6,8,16,23,34LS2 1,16,17,34,35LS3 1,32,34,43,44CS1 1,23,31,34,40CS2 8,23,31,35,43CS3 1,9,27,34,39IdS 1,16,17,34,35Top-down Cohesion Segmentation in Summarization 395Table 2: The average informativeness of CTT and LTT summaries for all textsMethod Var1 Var2 Var1 + Var2LS1 0.606538745 0.603946109 0.605242427LS2 0.580429322 0.577647764 0.579038543LS3 0.594914426 0.600104854 0.59750964CS1 0.607369111 0.603053171 0.605211141CS2 0.592993154 0.589675201 0.591334178CS3 0.631625044 0.594702506 0.613163775average 0.602311633 0.594854934 0.598583284LT Taverage 0.593960831 0.593899576 0.593930203CT Taverage 0.6106624 0.5958102 0.60323635.1 Evaluation of the summarizationThere is no an unique formal method to evaluate the quality of a summary.
In thispaper we use as a measure of the quality of a summary, the similarity (calculatedas cosine) between the summarized (initial) text and the summaries obtained withdifferent methods.
We call this similarity "the informativeness".The informativeness of the different types of summaries Sum1,Sum2,Sum3 (seeSection 4.2) and of different lengths (5, 6 and 10) is calculated for each text.
Then, theaverage informativeness for all four texts is calculated.
A view with the these averageresults of informativeness, calculated with different methods, in variants Var1 andVar2, is given in the Table 2.Let us remark that for obtaining summaries with different lengths, after a first seg-mentation with CTT and LTT methods the algorithm AL from Section 4.1 is applied.Table 2 displays the results announced in the abstract: the quality of CTT sum-maries is better than the quality of the LTT summaries from the point of view ofinformativeness.5.2 Implementation detailsThe methods presented in this paper are fully implemented: we used our own systemsof Text Entailment verification, Word Sense Disambiguation, top-down lexical chainsdetermination, LTT and CTT segmentation, summarization with Sumi and AL meth-ods.
The programs are realized in Java and C++.
WordNet (Miller, 1995) is used byour system of Word Sense Disambiguation.6 Conclusion and further workThis paper shows that the text segmentation by lexical chains and by text entailmentrelation between sentences are good bases for obtaining highly accurate summaries.Moreover, our method replaces the usually bottom-up lexical chain construction witha top-down one, where first a single chain of disambiguated words is established andthen it is divided in a sequence of many shorter lexical chains.
The segmentation oftext follows the sequence of lexical chains.
Our methods of summarization control thelength of the summaries by a process of scoring the segments.
Thus, more material isextracted from the strongest segments.396 Tatar, Mihis, and SerbanThe evaluation indicates acceptable performance when informativeness of sum-maries is considered.
However, our methods have the potential to be improved: inCTT method we correspond a segment to a lexical chain.
We intend to improve ourscoring method of a segment by considering some recent method of scoring lexicalchains (Ercan and Cicekli, 2008).
Also, we intend to study how anaphora resolutioncould improve the lexical chains and the segmentation.
We further intend to applythe presented methods to the corpus of texts DUC2002 and to evaluate them with thestandard ROUGE method (for our experiments we didn?t have the necessary humanmade summaries).AcknowledgmentsThis work has been supported by PN2 Grant TD 400/2007.ReferencesBarzilay, R. and M. Elhadad (1999).
Using lexical chains for Text summarization.
InJ.
Mani and M. Maybury (Eds.
), Advances in Automated Text Summarization.
MITPress.Boguraev, B. and M. Neff (2000).
Lexical Cohesion, Discourse Segmentation andDocument Summarization.
In Proceedings of the 33rd Hawaii International Con-ference on System Sciences.Corley, C. and R. Mihalcea (2005).
Measuring the semantic similarity of texts.
InProceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalenceand Entailment, Ann Arbor.Ercan, G. and I. Cicekli (2008).
Lexical cohesion based topic modeling for summa-rization.
In Proceedings of the Cicling 2008, pp.
582?592.Harabagiu, S. and D. Moldovan (1997).
TextNet ?
a textbased intelligent system.Natural Language Engineering 3(2), 171?190.Hearst, M. (1997).
Texttiling: Segmenting text into multi-paragraph subtopic pas-sages.
Computational Linguistics 23(1), 33?64.Hovy, E. (2003).
Text summarization.
In R. Mitkov (Ed.
), The Oxford Handbook ofComputational Linguistics.
Oxford University Press.Mani, I.
(2001).
Automatic summarization.
John Benjamins.Marcu, D. (1997).
From discourse structure to text summaries.
In Proceedings of theACL/EACL ?97 Workshop on Intelligent Scalable Text Summarization, pp.
82?88.Miller, G. (1995).
WordNet: a lexical database for english.
Comm.
of theACM 38(11), 39?41.Morris, J. and G. Hirst (1991).
Lexical cohesion computed by thesaural relations asan indicator of the structure of text.
Computational Linguistics 17(1), 21?48.Top-down Cohesion Segmentation in Summarization 397Orasan, C. (2006).
Comparative evaluation of modular automatic summarization sys-tems using CAST.
Ph.
D. thesis, University of Wolverhampton.Radev, D., E. Hovy, and K. McKeown (2002).
Introduction to the special issues onsummarization.
Computational Linguistics 28, 399?408.Reynar, J.
(1998).
Topic Segmentation: algorithms and applications.
Ph.
D. thesis,Univ.
of Penn.Richie, D. (1991).
The koan.
In Z. Inklings (Ed.
), Some Stories, Fables, Parables andSermons, pp.
25?27.Silber, H. and K. McCoy (2002).
Efficiently computed lexical chains, as an inter-mediate representation for automatic text summarization.
Computational Linguis-tics 28(4), 487?496.Stokes, N. (2004).
Applications of Lexical Cohesion Analysis in the Topic Detectionand Tracking Domain.
Ph.
D. thesis, National University of Ireland, Dublin.Tatar, D., A. M. G. Serban, and R. Mihalcea (2007).
Text entailment as directionalrelation.
In Proceedings of CALP07 Workshop at RANLP2007, Borovets, Bulgaria,pp.
53?58.Tatar, D., A. Mihis, and D. Lupsa (2008).
Text entailment for logical segmentationand summarization.
In 13th International Conference on Applications of NaturalLanguage to Information Systems, pp.
233?244.Tatar, D., G. Serban, A. Mihis, M. Lupea, D. Lupsa, and M. Frentiu (2007).
A chaindictionary method for wsd and applications.
In Proceedings of the InternationalConference on Knowledge Engineering,Principles and Techniques, pp.
41?49.Tucker, R. (1999).
Automatic summarising and the CLASP system.
Ph.
D. thesis,University of Cambridge.
