An Algorithmic Framework for the Decoding Problem inStatistical Machine TranslationRaghavendra Udupa U Tanveer A FaruquieIBM India Research LabBlock-1A, IIT, Hauz KhasNew Delhi - 110 016India{uraghave, ftanveer}@in.ibm.comHemanta K MajiDept.
of Computer Scienceand Engineering, IIT KanpurKanpur - 208 016India,hkmaji@iitk.ac.inAbstractThe decoding problem in Statistical Ma-chine Translation (SMT) is a computation-ally hard combinatorial optimization prob-lem.
In this paper, we propose a new al-gorithmic framework for solving the decod-ing problem and demonstrate its utility.
Inthe new algorithmic framework, the decod-ing problem can be solved both exactly andapproximately.
The key idea behind theframework is the modeling of the decod-ing problem as one that involves alternat-ing maximization of two relatively simplersubproblems.
We show how the subprob-lems can be solved efficiently and how theirsolutions can be combined to arrive at a so-lution for the decoding problem.
A fam-ily of provably fast decoding algorithms canbe derived from the basic techniques under-lying the framework and we present a fewillustrations.
Our first algorithm is a prov-ably linear time search algorithm.
We usethis algorithm as a subroutine in the otheralgorithms.
We believe that decoding algo-rithms derived from our framework can beof practical significance.1 IntroductionDecoding is one of the three fundamental prob-lems in classical SMT (translation model andlanguage model being the other two) as pro-posed by IBM in the early 1990?s (Brown et al,1993).
In the decoding problem we are given thelanguage and translation models and a sourcelanguage sentence and are asked to find themost probable translation for the sentence.
De-coding is a discrete optimization problem whosesearch space is prohibitively large.
The chal-lenge is, therefore, in devising a scheme to ef-ficiently search the solution space for the solu-tion.Decoding is known to belong to a class of com-putational problems popularly known as NP-hard problems (Knight, 1999).
NP-hard prob-lems are known to be computationally hard andhave eluded polynomial time algorithms (Gareyand Johnson, 1979).
The first algorithms forthe decoding problem were based on what isknown among the speech recognition commu-nity as stack-based search (Jelinek, 1969).
Theoriginal IBM solution to the decoding prob-lem employed a restricted stack-based search(Berger et al, 1996).
This idea was further ex-plored by Wang and Waibel (Wang and Waibel,1997) who developed a faster stack-based searchalgorithm.
In perhaps the first work on thecomputational complexity of Decoding, KevinKnight showed that the problem is closely re-lated to the more famous Traveling Salesmanproblem (TSP).
Independently, Christoph Till-man adapted the Held-Karp dynamic program-ming algorithm for TSP (Held and Karp, 1962)to Decoding (Tillman, 2001).
The original Held-Karp algorithm for TSP is an exponential timedynamic programming algorithm and Tillman?sadaptation to Decoding has a prohibitive com-plexity of O(l3m22m) ?
O (m52m) (where mand l are the lengths of the source and tar-get sentences respectively).
Tillman and Neyshowed how to improve the complexity of theHeld-Karp algorithm for restricted word re-ordering and gave a O(l3m4) ?
O (m7) algo-rithm for French-English translation (Tillmanand Ney, 2000).
An optimal decoder based onthe well-known A?
heuristic was implementedand benchmarked in (Och et al, 2001).
Sinceoptimal solution can not be computed for prac-tical problem instances in a reasonable amountof time, much of recent work has focused ongood quality suboptimal solutions.
An O(m6)greedy search algorithm was developed (Ger-mann et al, 2003) whose complexity was re-duced further to O(m2)(Germann, 2003).In this paper, we propose an algorithmicframework for solving the decoding problem andshow that several efficient decoding algorithmscan be derived from the techniques developed inthe framework.
We model the search problemas an alternating search problem.
The search,therefore, alternates between two subproblems,both of which are much easier to solve in prac-tice.
By breaking the decoding problem intotwo simpler search problems, we are able to pro-vide handles for solving the problem efficiently.The solutions of the subproblems can be com-bined easily to arrive at a solution for the orig-inal problem.
The first subproblem fixes analignment and seeks the best translation withthat alignment.
Starting with an initial align-ment between the source sentence and its trans-lation, the second subproblem asks for an im-proved alignment.
We show that both of theseproblems are easy to solve and provide efficientsolutions for them.
In an iterative search for alocal optimal solution, we alternate between thetwo algorithms and refine our solution.The algorithmic framework provides handlesfor solving the decoding problem at several lev-els of complexity.
At one extreme, the frame-work yields an algorithm for solving the decod-ing problem optimally.
At the other extreme, ityields a provably linear time algorithm for find-ing suboptimal solutions to the problem.
Weshow that the algorithmic handles provided byour framework can be employed to develop avery fast decoding algorithm which finds goodquality translations.
Our fast suboptimal searchalgorithms can translate sentences that are 50words long in about 5 seconds on a simple com-puting facility.The rest of the paper is devoted to the devel-opment and discussion of our framework.
Westart with a mathematical formulation of thedecoding problem (Section 2).
We then developthe alternating search paradigm and use it todevelop several decoding algorithms (Section 3).Next, we demonstrate the practical utility of ouralgorithms with the help of results from our ini-tial experiments (Section 5).2 DecodingThe decoding problem in SMT is one of findingthe most probable translation e?
in the targetlanguage of a given source language sentence fin accordance with the Fundamental Equationof SMT (Brown et al, 1993):e?
= argmaxe Pr(f |e)Pr(e).
(1)In the remainder of this paper we will referto the search problem specified by Equation 1as STRICT DECODING.Rewriting the translation model Pr(f |e) as?a Pr(f ,a|e), where a denotes an alignmentbetween the source sentence and the target sen-tence, the problem can be restated as:e?
= argmaxe?aPr(f ,a|e)Pr(e).
(2)Even when the translation model Pr(f |e) isas simple as IBM Model 1 and the languagemodel Pr(e) is a bigram language model, thedecoding problem is NP-hard (Knight, 1999).Unless P = NP, there is no hope of an efficientalgorithm for the decoding problem.
Since theFundamental Equation of SMT does not yieldan easy handle to design a solution (exact oreven an approximate one) for the problem, mostresearchers have instead worked on solving thefollowing relatively simpler problem (Germannet al, 2003):(e?, a?)
= argmax(e,a) Pr(f ,a|e)Pr(e).
(3)We call the search problem specifiedby Equation 3 as RELAXED DECODING.Note that RELAXED DECODING relaxesSTRICT DECODING to a joint optimizationproblem.
The search in RELAXED DECODINGis for a pair (e?, a?).
While RELAXED DECODINGis simpler than STRICT DECODING, it is also,unfortunately, NP hard for even IBM Model1 and Bigram language model.
Therefore, allpractical solutions to RELAXED DECODINGhave focused on finding suboptimal solutions.The challenge is in devising fast search strate-gies to find good suboptimal solutions.
Table 1lists the combinatorial optimization problemsin the domain of decoding.In the remainder of the paper,m and l denotethe length of the source language sentence andits translation respectively.3 Framework for DecodingWe begin with a couple of useful observationsabout the decoding problem.
Although decep-tively simple, these observations are very cru-cial for developing our framework.
They arethe source for algorithmic handles for breakingthe decoding problem into two relatively eas-ier search problems.
The first of these observa-tions concerns with solving the problem whenwe know in advance the mapping between thesource and target sentences.
This leads to thedevelopment of an extremely simple algorithmfor decoding when the alignment is known (orProblem SearchSTRICT DECODING e?
= argmaxePr(f |e)Pr(e)RELAXED DECODING (e?, a?)
= argmax(e,a)Pr(f ,a|e)Pr(e)FIXED ALIGNMENT DECODING e?
= argmaxePr(f , a?|e)Pr(e)VITERBI ALIGNMENT a?
= argmaxaPr(f ,a|e?
)Table 1: Combinatorial Search Problems in Decodingcan be guessed).
Our second observation is onfinding a better alignment between the sourceand target sentences starting with an initial(possibly suboptimal) alignment.
The insightprovided by the two observations are employedin building a powerful algorithmic framework.3.1 Handles for attacking the DecodingProblemOur goal is to arrive at algorithmic handlesfor attacking RELAXED DECODING.
In this sec-tion, we make couple of useful observations anddevelop algorithmic handles from the insightprovided by them.
The first of the two observa-tions is:Observation 1 For a given target length l anda given alignment a?
that maps source words totarget positions, it is easy to compute the opti-mal target sentence e?.e?
= argmaxe Pr(f , a?|e)Pr(e).
(4)Let us call the search problem specified byEquation 4 as FIXED ALIGNMENT DECODING.What Observation 1 is saying is that once thetarget sentence length and the source to tar-get mapping is fixed, the optimal target sen-tence (with the specified target length andalignment) can be computed efficiently.
Aswe will show later, the optimal solution forFIXED ALIGNMENT DECODING can be com-puted in O (m) time for IBM models 1-5 usingdynamic programming.
As we can always guessan alignment (as is the case with many decodingalgorithms in the literature), the above observa-tion provides an algorithmic handle for findingsuboptimal solutions for RELAXED DECODING.Our second observation is on computing theoptimal alignment between the source sentenceand the target sentence.Observation 2 For a given target sentence e?,it is easy to compute the optimal alignment a?that maps the source words to the target words.a?
= argmaxa Pr(f ,a|e?).
(5)It is easy to determine the optimal (Viterbi)alignment between the source sentence and itstranslation.
In fact, for IBM models 1 and 2,the Viterbi alignment can be computed using astraight forward algorithm in O (ml) time.
Forhigher models, an approximate Viterbi align-ment can be computed iteratively by an iter-ative procedure called local search.
In each it-eration of local search, we look in the neighbor-hood of the current best alignment for a betteralignment (Brown et al, 1993).
The first itera-tion can start with any arbitrary alignment (saythe Viterbi alignment of Model 2).
It is possi-ble to implement one iteration of local search inO (ml) time.
Typically, the number of iterationsis bounded in practice by O (m), and therefore,local search takes O(m2l)time.Our framework is not strictly dependent onthe computation of an optimal alignment.
Anyalignment which is better than the currentalignment is good enough for it to work.
It isstraight forward to find one such alignment us-ing restricted swaps and moves in O (m) time.In the remainder of this paper, we use the termViterbi to denote any linear time algorithm forcomputing an improved alignment between thesource sentence and its translation.3.2 Illustrative AlgorithmsIn this section, we show how the handles pro-vided by the above two observations can be em-ployed to solve RELAXED DECODING.
The twohandles are in some sense complementary toeach other.
When the alignment is known, wecan efficiently determine the optimal translationwith that alignment.
On the other hand, whenthe translation is known, we can efficiently de-termine a better alignment.
Therefore, we canuse one to improve the other.
We begin with thefollowing simple linear time decoding algorithmwhich is based on the first observation.Algorithm NaiveDecodeInput: Source language sentence f of lengthm > 0.Optional Inputs: Target sentence length l,alignment a?
between the source words and tar-get positions.Output: Target language sentence e?
of lengthl.1.
If l is not specified, let l = m.2.
If an alignment is not specified, guess somealignment a?.3.
Compute the optimal translation e?
by solv-ing FIXED ALIGNMENT DECODING,i.e., e?
= argmaxe Pr(f , a?|e)Pr(e).4. return e?.When the length of the translation is notspecified, NaiveDecode assumes that the trans-lation is of the same length as the source sen-tence.
If an alignment that maps the sourcewords to target positions is not specified, thealgorithm guesses an alignment a?
(a?
can be thetrivial alignment that maps the source word fjto target position j, that is, a?j = j, or canbe guessed more intelligently).
It then com-putes the optimal translation for the sourcesentence f , with the length of the target sen-tence and the alignment between the source andthe target sentences kept fixed to l and a?
re-spectively, by maximizing Pr(f , a?|e)Pr(e).
AsFIXED ALIGNMENT DECODING can be solvedin O (m) time, NaiveDecode takes only O(m)time.The value of NaiveDecode lies not in itself perse, but in its instrumental role in designing moresuperior algorithms.
The power of NaiveDecodecan be demonstrated with the following optimalalgorithm for RELAXED DECODING.Algorithm NaiveOptimalDecodeInput: Source language sentence f of lengthm > 0.Output: Target language sentence e?
of lengthl, m2 ?
l ?
2m.1.
Let e?
= null and a?
= null.2.
For each l = m2 , .
.
.
, 2m do3.
For each alignment a between the sourcewords and the target positions do(a) Let e = NaiveDecode(f , l,a).
(b) If Pr (f , e,a) > Pr (f , e?, a?)
theni.
e?
= eii.
a?
= a.4.
return (e?, a?
).NaiveOptimalDecode considers various tar-get lengths and all possible alignments be-tween the source words and the target posi-tions.
For each target length l and alignmenta it employs NaiveDecode to find the best so-lution.
There are (l + 1)m candidate align-ments for a target length l and O (m) can-didate target lengths.
Therefore, NaiveOp-timalDecode explores ?
(m(l + 1)m) alignments.For each of these candidate alignments, itmakes a call to NaiveDecode.
The time com-plexity of NaiveOptimalDecode is, therefore,O(m2(l + 1)m).
Although an exponential timealgorithm, it can compute the optimal solutionfor RELAXED DECODING.With NaiveDecode and NaiveOptimalDecodewe have demonstrated the power of the algo-rithmic handle provided by Observation 1.
Itis important to note that these two algorithmsare at the two extremities of the spectrum.NaiveDecode is a linear time decoding algorithmthat computes a suboptimal solution for RE-LAXED DECODING while NaiveOptimalDecodeis an exponential time algorithm that computesthe optimal solution.
What we want are algo-rithms that are close to NaiveDecode in com-plexity and to NaiveOptimalDecode in qual-ity.
It is possible to reduce the complexity ofNaiveOptimalDecode significantly by carefullyreducing the number of alignments that are ex-amined.
Instead of examining all ?
(m(l+1)m)alignments, if we examine only a small num-ber, say g (m), alignments in NaiveOptimalDe-code, we can find a solution in O (mg (m)) time.In the next section, we show how to restrictthe search to only a small number of promis-ing alignments.3.3 Alternating MaximizationWe now show how to use the two algorithmichandles to come up with a fast search paradigm.We alternate between searching the best trans-lation given an alignment and searching thebest alignment given a translation.
Since thetwo subproblems are complementary, they canbe used to improve the solution computed byone another by alternating between the twoproblems.Algorithm AlternatingSearchInput: Source language sentence f of lengthm > 0.Output: Target language sentence e(o) oflength l (m/2 ?
l ?
2m).1.
Let e(o) = null and a(o) = null.2.
For each l = m/2, .
.
.
, 2m do(a) Let e = null and a = null.
(b) While there is improvement in solutiondoi.
Let e = NaiveDecode (f , l,a).ii.
Let a?
= V iterbi (f , e).
(c) If Pr (f , e,a) > Pr (f , e(o),a(o)) theni.
e(o) = eii.
a(o) = a.3.
return e(o).AlternatingSearch searches for a good trans-lation by varying the length of the tar-get sentence.
For a sentence length l,the algorithm finds a translation of lengthl and then iteratively improves the trans-lation.
In each iteration it solves twosubproblems: FIXED ALIGNMENT DECODINGand VITERBI ALIGNMENT.
The input to eachiteration are the source sentence f , the tar-get sentence length l, and an alignment a be-tween the source and target sentences.
So, Al-ternatingSearch finds a better translation e forf by solving FIXED ALIGNMENT DECODING.For this purpose it employs NaiveDecode.
Hav-ing computed e, the algorithm computes a bet-ter alignment (a?)
between e and f by solvingVITERBI ALIGNMENT using Viterbi algorithm.The new alignment thus found is used by the al-gorithm in the subsequent iteration.
At the endof each iteration the algorithm checks whetherit has made progress.
The algorithm returns thebest translation of the source f across a rangeof target sentence lengths.The analysis of AlternatingSearch is compli-cated by the fact that the number of iterations(see step 2.b) depends on the input.
It is rea-sonable to assume that the length of the sourcesentence (m) is an upper bound on the numberof iterations.
In practice, however, the numberof iterations is typically O (1).
There are 3m/2candidate sentence lengths for the translation(l varies from m/2 to 2m) and both NaiveDe-code and Viterbi are O (m).
Therefore, the timecomplexity of AlternatingSearch is O(m2).4 A Linear Time Algorithm forFIXED ALIGNMENT DECODINGA key component of all our algorithms isa linear time algorithm for the problemFIXED ALIGNMENT DECODING.
Recall that inFIXED ALIGNMENT DECODING, we are giventhe target length l and a mapping a?
from sourcewords to target positions.
The goal is then tofind the optimal translation with a?
as the align-ment.
In this section, we give a dynamic pro-gramming based solution to this problem.
Oursolution is based on a new formulation of IBMtranslation models.
We begin our discussionwith a few technical definitions.Alignment a?
maps each of the source wordsfj, j = 1, .
.
.
,m to a target position in the range[0, .
.
.
, l].
Define a mapping ?
from [0, .
.
.
, l] tosubsets of {1, .
.
.
,m} as follows:?
(i) = {j : j ?
{1, .
.
.
,m} ?
a?j = i} ?
i = 0, .
.
.
, l.?
(i) is the set of source positions which aremapped to the target location i by the align-ment a?
and the fertility of the target position iis ?i = |?
(i)|.We can rewrite each of the IBM modelsPr (f , a?|e) as follows:Pr (f , a?|e) = ?l?i=1TiDiNi.Table 2 shows the breaking of Pr (f , a?|e) intothe constituents Ti,Di and Ni.
As a conse-quence, we can write Pr (f , a?|e)Pr (e) as:Pr (f , a?|e)Pr (e) = ?
?l?i=1TiDiNiLiwhere Li = trigram(ei|ei?2, ei?1) and ?
is thetrigram probability of the boundary word.The above reformulation of the optimiza-tion function of the decoding problem allowsus to employ Dynamic Programming for solv-ing FIXED ALIGNMENT DECODING efficiently.Note that each word ei has only a constant num-ber of candidates in the vocabulary.
Therefore,the set of words e1, .
.
.
, el that maximizes theLHS of the above optimization function can befound in O (m) time using the standard Dy-namic Programming algorithm (Cormen et al,2001).5 Experiments and ResultsIn this section we describe our experimentalsetup and present the initial results.
Our goalModel ?
Ti Di Ni1 ?(m|l)(l+1)m?k??
(i) t(fk |ei) 1 12 ?
(m|l) ?k??
(i) t(fk |ei)?k??
(i) a(i|k,m, l) 13 n(?0|m)pm?2?00 p?01?k??
(i) t(fk |ei)?k??
(i) d(k|i,m, l) ?i!
n(?i|ei)Table 2: Pr (f, a?|e) for IBM Modelswas not only to evaluate the performance of ouralgorithms on real data, but also to evaluatehow easy it is to code the algorithm and whethera straightforward implementation of the algo-rithm with no parameter tuning can give satis-factory results.We implemented the algorithms in C++ andconducted the experiments on an IBM RS-6000dual processor machine with 1 GB of RAM.
Webuilt a French-English translation model (IBMModel 3) by training over a corpus of 100 K sen-tence pairs from the Hansard corpus.
The trans-lation direction was from French to English.
Webuilt an English language model by trainingover a corpus consisting of about 800 millionwords.
We divided the test sentences into sev-eral classes based on their length.
Each lengthclass consisted of 300 test French sentences.We implemented four algorithms -1.1 (NaiveDe-code), 1.2 (Alternating Search with l restrictedto m), 2.1 (NaiveDecode with l varying fromm/2 to 2m) and 2.2 (Alternating Search).
Inorder to compare the performance of the al-gorithms proposed in this paper with a previ-ous decoding algorithm, we also implementedthe dynamic programming based algorithm by(Tillman, 2001).
For each of the algorithms, wecomputed the following:1.
Average time taken for translation foreach length class.2.
NIST score of the translations for eachlength class.3.
Average value of the optimizationfunction for the translations for eachlength class.The results of the experiments are summa-rized in Plots 1, 2 and 3.
In all the plots, thelength class is denoted by the x-axis.
11-20 indi-cates the class with sentences of length between11 words to 20 words.
51 indicates the groupof sentences with sentence length 51 or more.Plot 1 shows the average time taken by the al-gorithms for translating the sentences in eachlength class.
Time is shown in seconds on a logscale.
Plot 2 shows the NIST score of the trans-lations for each length class while Plot 3 showsthe average log score of the translations (-ve logof Pr (f ,a|e)Pr (e)) again for each length class.It can be seen from Plot 1 that all of our al-gorithms are indeed very fast in practice.
Theyare, in fact, an order faster than the Held-Karpalgorithm.
Our algorithms are able to trans-late even long sentences (50+ words) in a fewseconds.Plot 3 shows that the log scores of the trans-lations computed by our algorithms are veryclose to those computed by the Held-Karp al-gorithm.
Plot 2 compares the NIST scores ob-tained with each of the algorithm.
Among thefour algorithms based on our framework, Al-gorithm 2.2 gives the best NIST scores as ex-pected.
Although, the log scores of our algo-rithms are comparable to those of the Held-Karp algorithm, our NIST scores are lower.
Itshould be noted that the mathematical quan-tity that our algorithm tries to optimize is thelog score.
Plot 3 shows that our algorithms arequite good at finding solutions with good scores.0.010.11101001000100000-10 11-20 21-30 31-40 41-50 51-TimeinsecondsSentence LengthDecoding Time"algorithm 1.1""algorithm 1.2""algorithm 2.1""algorithm 2.2""algorithm H-K"Figure 1: Average decoding time6 ConclusionsThe algorithmic framework developed in thispaper is powerful as it yields several decodingalgorithms.
At one end of the spectrum is aprovably linear time algorithm for computinga suboptimal solution and at the other end isan exponential time algorithm for computing33.544.555.566.570-10 11-20 21-30 31-40 41-50 51-NISTScoreSentence LengthNIST Scores"algorithm 1.1""algorithm 1.2""algorithm 2.1""algorithm 2.2""algorithm H-K"Figure 2: NIST scores0501001502002503003504000-10 11-20 21-30 31-40 41-50 51-logscoreSentence LengthLogscores"algorithm 1.1""algorithm 1.2""algorithm 2.1""algorithm 2.2""algorithm H-K"Figure 3: Log scorethe optimal solution.
We have also shown thatalternating maximization can be employed tocome up with O(m2)decoding algorithm.
Twoquestions in this connection are:1.
Is it possible to reduce the complexityof AlternatingSearch to O (m)?2.
Instead of exploring each alignmentseparately, is it possible to explore abunch of alignments in one shot?Answers to these questions will result in fasterand more efficient decoding algorithms.7 AcknowledgementsWe are grateful to Raghu Krishnapuram for hisinsightful comments on an earlier draft of thispaper and Pasumarti Kamesam for his help dur-ing the course of this work.ReferencesA.
Berger, P. Brown, S. Della Pietra, V. DellaPietra, A. Kehler, and R. Mercer.
1996.
Lan-guage translation apparatus and method us-ing context-based translation models.
UnitedStates Patent 5,510,981.P.
Brown, S. Della Pietra, V. Della Pietra,and R. Mercer.
1993.
The mathematics ofmachine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.T.
H. Cormen, C. E. Leiserson, R. L. Rivest,and C. Stein.
2001.
The MIT Press, Cam-bridge.M.
R. Garey and D. S. Johnson.
1979.
W. H.Freeman and Company, New York.U.
Germann, M. Jahr, D. Marcu, and K. Ya-mada.
2003.
Fast decoding and optimal de-coding for machine translation.
Artificial In-telligence.Ulrich Germann.
2003.
Greedy decoding forstatistical machine translation in almost lin-ear time.
In Proceedings of HLT-NAACL2003.
Edmonton, Canada.M.
Held and R. Karp.
1962.
A dynamic pro-gramming approach to sequencing problems.J.
SIAM, 10(1):196?210.F.
Jelinek.
1969.
A fast sequential decoding al-gorithm using a stack.
IBM Journal Reseachand Development, 13:675?685.Kevin Knight.
1999.
Decoding complexity inword-replacement translation models.
Com-putational Linguistics, 25(4).F.
Och, N. Ueffing, and H. Ney.
2001.
An ef-ficient a* search algorithm for statistical ma-chine translation.
In Proceedings of the ACL2001 Workshop on Data-Driven Methods inMachine Translation, pages 55?62.
Toulouse,France.C.
Tillman and H. Ney.
2000.
Word reorder-ing and dp-based search in statistical machinetranslation.
In Proceedings of the 18th COL-ING, pages 850?856.
Saarbrucken, Germany.Christoph Tillman.
2001.
Word re-orderingand dynamic programming based searchalgorithm for statistical machine transla-tion.
Ph.D. Thesis, University of TechnologyAachen, pages 42?45.R.
Udupa and T. Faruquie.
2004.
An english-hindi statistical machine translation system.In Proceedings of the 1st IJCNLP, pages 626?632.
Sanya, Hainan Island, China.Y.
Wang and A. Waibel.
1997.
Decoding al-gorithm in statistical machine translation.
InProceedings of the 35th ACL, pages 366?372.Madrid, Spain.
