Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1162?1171,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPDescriptive and Empirical Approaches to Capturing UnderlyingDependencies among Parsing ErrorsTadayoshi Hara1 Yusuke Miyao11Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN2School of Computer Science, University of Manchester3NaCTeM (National Center for Text Mining){harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jpJun?ichi Tsujii1,2,3AbstractIn this paper, we provide descriptive andempirical approaches to effectively ex-tracting underlying dependencies amongparsing errors.
In the descriptive ap-proach, we define some combinations oferror patterns and extract them from givenerrors.
In the empirical approach, on theother hand, we re-parse a sentence witha target error corrected and observe er-rors corrected together.
Experiments onan HPSG parser show that each of theseapproaches can clarify the dependenciesamong individual errors from each pointof view.
Moreover, the comparison be-tween the results of the two approachesshows that combining these approachescan achieve a more detailed error analysis.1 IntroductionFor any kind of technology, analyzing causes oferrors given by a system is a very helpful processfor improving its performance.
In recent sophisti-cated parsing technologies, the process has takenon more and more important roles since criticalideas for parsing performance have already beenintroduced and the researches are now focusing onexploring the rest of the pieces for making addi-tional improvements.In most cases for parsers?
error analysis, re-searchers associate output errors with failures inhandling certain linguistic phenomena and attemptto avoid them by adding or modifying correspond-ing settings of their parsers.
However, such ananalysis cannot been done so smoothly since pars-ing errors sometimes depend on each other and theunderlying dependencies behind superficial phe-nomena cannot be captured easily.In this paper, we propose descriptive and em-pirical approaches to effective extraction of de-pendencies among parsing errors and engage in adeeper error analysis with them.
In our descriptiveapproach, we define various combinations of errorpatterns as organized error phenomena on the ba-sis of linguistic knowledge, and then extract suchcombinations from given errors.
In our empiricalapproach, on the other and, we re-parse a sentenceunder the condition where a target error is cor-rected, and errors which are additionally correctedare regarded as dependent errors.
By capturing de-pendencies among parsing errors through system-atic approaches, we can effectively collect errorswhich are related to the same linguistic properties.In the experiments, we applied both of our ap-proaches to an HPSG parser Enju (Miyao and Tsu-jii, 2005; Ninomiya et al, 2006), and then evalu-ated the obtained error classes.
After examiningthe individual approaches, we explored the com-bination of them.2 Parser and its evaluationA parser is a system which interprets structuresof given sentences from some grammatical or insome cases semantical viewpoints, and interpretedstructures are utilized as essential information forvarious natural language tasks such as informa-tion extraction, machine translation, and so on.In most cases, an output structure of a parser isbased on a certain grammatical framework such asCFG, CCG (Steedman, 2000), LFG (Kaplan andBresnan, 1995) or HPSG (Pollard and Sag, 1994).Since such a framework can usually produce morethan one probable structure for a sentence, a parser1162John aux_arg12ARG1 ARG2verb_arg1ARG1has : come :Figure 1: Predicate argument relationsAbbr.
Full Abbr.
Fullaux auxiliary lgs logical subjectverb verb coord coordinationprep prepositional conj conjunctiondet determiner argN1... take argument(s)adj adjunction (N1th, ...)app apposition mod modify a wordrelative relativeTable 1: Descriptions for predicate typesoften utilizes some kind of disambiguation modelfor choosing the best one.While various parsers take different mannersin capturing linguistic phenomena based on theirframeworks, they are at least required to obtainsome kinds of relations between the words in sen-tences.
On the basis of the requirements, a parseris usually evaluated on how correctly it gives in-tended linguistic relations.
?Predicate argumentrelation?
is one of the most common evaluationmeasurements for a parser since it is a very fun-damental linguistic behavior and is less dependenton parser systems.
This measure divides linguis-tic structural phenomena in a sentence into min-imal predicative events.
In one predicate argu-ment relation, a word which represents an event(predicate) takes some words as participants (argu-ments).
Although no fixed formulation exists forthe relations, there are to a large extent commonconceptions for them based on linguistic knowl-edge among researchers.Figure 1 shows an example of predicate argu-ment relations given by Enju.
In the sentence?John has come.
?, ?has?
is a predicate of type?aux arg12?
and takes ?John?
and ?come?
as thefirst and second arguments.
?come?
is also a pred-icate of the type ?verb arg1?
and takes ?John?
asthe first and the only argument.
In this formalism,each predicate type is represented as a combina-tion of ?the grammatical nature of a word?
and?the arguments which it takes,?
which are repre-sented by the descriptions in Table 1.
?aux arg12?in Figure 1 indicates that it is an auxiliary wordand takes two arguments ?ARG1?
and ?ARG2.
?In order to improve the performance of a parser,analyzing parsing errors is very much worth theI watched the girl on TV Correct answer:ARG1 ARG2ARG1 ARG2I watched the girl on TV Parser output:ARG1 ARG2ARG1 ARG2Obtain inconsistent outputs as errorsError: I watched the girl on TVARG1ARG1 ErrorFigure 2: An example of parsing errorsError: The book on which read the shelf  I yesterdayARG1ARG2ARG2ARG1Figure 3: Co-occurring parsing errorseffort.
Since the errors are output according toa given evaluation measurement such as ?predi-cate argument relation,?
we researchers carefullyexplore them and infer the linguistic phenom-ena which cause the erroneous outputs.
Figure 2shows an example of parsing errors for sentence ?Iwatched the girl on TV.?
Note that the errors arebased on predicate argument relations as shownabove and that the predicate types are abbreviatedin this figure.
When we focus on the error output,we can observe that ?ARG1?
of predicate ?on?was mistaken by the parser.
In this case, ?ARG1?represents a modifiee of the preposition, and wethen conclude that the ill attachment of a prepo-sitional phrase caused this error.
By continuingsuch error analysis, weak points of the parser arerevealed and can be useful clues for further im-provements.However, in most researches on parsing tech-nologies, error analysis has been limited to narrowand shallow explorations since there are variousdependencies behind erroneous outputs.
In Fig-ure 3, for example, two errors were given: wrongoutputs for ?ARG1?
of ?which?
and ?ARG2?
of?read.?
Both of these two errors originated fromthe fact that the relative clause took a wrong an-tecedent ?the shelf.?
In this sentence, the former1163Error:ARG1ARG1They completed the sale of forARG1ARG1it to him $1,000ConflictionThey completed the sale of forARG1ARG1it to him $1,000Analysis 2: (Impossible)They completed the sale of forARG1ARG1it to him $1,000Analysis 1: (Possible)Can each error occur independently?ARG1ARG1ARG1 ARG1Figure 4: Sketch of error propagation?ARG1?
directly corresponds to the antecedentwhile the latter ?ARG2?
indirectly referred to thesame antecedent as the object of the verb ?read.
?The two predicate argument relations thus took thesame word as their common arguments, and there-fore the two errors co-occurred.On the other hand, one-way inductive relationsalso exist among errors.
In Figure 4, ?ARG1?
of?for?
and ?to?
were mistaken by a parser.
We canknow that each of the errors was caused by an illattachment of a prepositional phrase with the sameanalysis as shown in Figure 2.
What is importantin this example is the manner in their occurrences.The former error can appear by itself (Analysis 1)while the latter cannot because of the structuralconflict with the former error (Analysis 2).
Theappearance of the latter error thus induces that ofthe former error.
In error analysis, we have to cor-rectly capture such various relations, which leadsus to a costly and less rewarding analysis.In order to make advancements on this prob-lem, we propose two types of approaches to real-izing a deeper error analysis on parsing.
In the ex-periments, we examine our approaches for actualerrors which are given by the HPSG parser Enju(Miyao and Tsujii, 2005; Ninomiya et al, 2006).Enju was developed for capturing detailed syntac-tic or semantic properties and relations for a sen-tence with an HPSG framework (Pollard and Sag,1994).
In this research, we focus on error analysisbased on predicate argument relations, and in theexperiments with Enju, utilize the relations whichErroneous phenomena Matched patterns[Argument selection]Prepositional attachment ARG1 of prep argAdjunction attachment ARG1 of adj argConjunction attachment ARG1 of conj argHead selection for ARG1 of det argnoun phraseCoordination ARG1/2 of coord arg[Predicate type selection]Preposition/Adjunction prep arg / adj argGerund acts as modifier/not verb mod arg / verb argCoordination/conjunction coord arg / conj arg# of arguments prep argX / prep argYfor preposition (X 6= Y )Adjunction/adjunctive noun adj arg / noun arg[More structural errors]To-infinitive for see Figure 7modifier/argument of verbSubject for passive sentence see Figure 8or not[Others]Comma any error around ?,?Relative clause attachment see Figure 9Table 2: Patterns defined for descriptive approachare represented in parsed tree structures.3 Two approaches for error analysisIn this section, we propose two approaches for er-ror analysis which enable us to capture underlyingdependencies among parsing errors.
Our descrip-tive approach matches the patterns of error com-binations with given parsing errors and collectsmatched erroneous participants.
Our empirical ap-proach, on the other hand, detects co-occurringerrors by re-parsing a sentence under a situationwhere each of the errors is forcibly corrected.3.1 Descriptive approachOur descriptive approach for capturing dependen-cies among parsing errors is to extract certain rep-resentative structures of errors and collect the er-rors which involve them.
Parsing errors have a ten-dency to occur with certain patterns of structuresrepresenting linguistic phenomena.
We first definesuch patterns through observations with a part oferror outputs, and then match them with the rest.Table 2 summarizes the patterns for erroneousphenomena which we defined for matching inthe experiments.
In the table, the patterns for14 phenomena are given and classified into fourtypes according to their matching manners.
Eachof the patterns for ?Argument selection?
examinewhether a focused argument for a certain predi-cate type is erroneous or not.
Figure 5 shows thepattern for ?Prepositional attachment,?
which col-1164prep_argARG1 ErrorParser output: ?They completed the sale of for :ARG1ARG1it to : him $1,000Pattern:prep_arg12 prep_arg12Correct output:ARG1ARG1They completed the sale of for :it to : him $1,000 prep_arg12 prep_arg12Parser output:Example:Figure 5: Pattern for ?Prepositional attachment?gerund:     verb_argParser output: gerund: verb_mod_argCorrect answer:(Patterns of correct answer and parser output can be interchanged)Pattern:Example:The customers walk the doora   package   for   themexpecting: verb_mod_arg123 you to havein ARG1MOD ARG2ARG3Parser output:Correct output:The customers walk the doora   package   for   themexpecting:     verb_arg123 you to haveinNot existARG2ARG3ARG1(MOD)Figure 6: Pattern for ?Gerund acts as modifier ornot?lects wrong ARG1 for predicate type ?prep arg?.From the sentence in the figure, we can obtaintwo errors for ?Prepositional attachment?
aroundprepositions ?to?
and ?for.?
On the other hand,each ?Predicate type selection?
pattern collects er-rors around a word whose predicate type is erro-neous.
Figure 6 shows the pattern for ?Gerundacts as modifier or not,?
which collects errorsaround gerunds whose predicate types are erro-neous.
From the example sentence in the figure,we can obtain an erroneous predicate type for ?ex-pecting?
and collect errors around it for ?Gerundacts as modifier or not.
?We can implement more structural errors thansimple argument or predicate type selections.
Fig-ures 7 and 8 show the patterns for ?To-infinitivefor modifier/argument of verb?
and ?Subject forpassive sentence or not?
respectively.
The pat-tern for the latter phenomenon collects errors onrecognitions of prepositional phrases which be-have as subjects for passive expressions.
The pat-tern collects errors not only around prepositionsbut also around the verbs which take the preposi-Parser output: aux_arg12to :verb1 ?ARG3 verb2Correct output: aux_mod_arg12MODto :ARG2Unknown subject ARG1 ARG1verb1 ?
verb2The  figures  ?
were  adjusted to : remove ...aux_arg12Example:Parser output:Correct answer:ARG3The  figures  ?
were  adjusted to : remove ...aux_mod_arg12MOD ARG2Unknown subject ARG1 ARG1Pattern: (Patterns of correct answer and parser output can be interchanged)Figure 7: Pattern for ?To-infinitive for modi-fier/argument of verb?Example:Pattern:Parser output: prep_arg12Unknown subject verb1 ?ARG1ARG1 ?Correct output: lgs_arg2 ARG2verb1 ?
?ARG1A  50-state  study  released in  September  by : Friends  ?Unknown subject ARG1ARG1 prep_arg12Parser output:Correct answer: A  50-state  study  released in  September  by : Friends  ?ARG1ARG2 lgs_arg12ARG2(Patterns of correct answer and parser output can be interchanged)Figure 8: Pattern for ?Subject for passive sentenceor not?tional phrases as a subject.Since these patterns are based on linguisticknowledge given by a human, the process couldprovide a relatively precise analysis with a lowercost than a totally manual analysis.3.2 Empirical approachOur empirical approach, on the other hand, brieflytraces the parsing process which results in each ofthe target errors.
We collect co-occurring errorsas strongly relevant ones, and then extract depen-dencies among the obtained groups.
Parsing errorscould originate from wrong processing at certainstages in the parsing, and errors with a commonorigin would by necessity appear together.
We re-parse a target sentence under the condition where acertain error is forcibly corrected and then collecterrors which are corrected together as the ?rela-tive?
ones.
An error group where all errors arerelative to each other can be regarded as a ?co-occurring error group.?
Errors in the same co-1165Example:Pattern:relative_arg1ARG1Parser output: ARG1/2ErrorParser output:Correct answer:The book on relative_arg1 read ARG2the shelf  I yesterdayARG1ARG2ARG1which :The book on relative_arg1 read the shelf  I yesterdaywhich :Figure 9: Pattern for ?Relative clause attachment?our work forceError 1Re-parse a sentence under the condition whereeach error is forcibly correctedError 1Error 2Error 3Correct Error 2Error 1Error 1Extract co-occurring error groups and inductive relationsError 4 Error 1Error 4Error 3Error 3CorrectCorrectCorrectcorrected togethercorrected togethercorrected togethercorrected together,,,Error 1 Error 2 Error 3 Error 4todayARG1Correct answer:It    has    no    bearing onour work force todayonParser output: ARG1 ARG1ARG2ARG2 ARG1 ARG1 ARG1It    has    no    bearingError 2 Error 3 Error 4 Error 5Error 5 Error 4Correct corrected togetherError 1 Error 3Error 2, , ,Error 4,Error 2 Error 4,,Error 2 Error 3,,Error 5InduceCo-occurring error group Co-occurring error groupFigure 10: An image of our empirical approachoccurring error group are expected to participatein the same phenomenon.
Dependencies amongerrors are then expected to be summarized with in-ductions among co-occurring error groups.Figure 10 shows an image of this approach.
Inthis example, ?today?
should modify noun phrase?our work force?
while the parser decided that ?to-day?
was also in the noun phrase.
As a result, thereare five errors: three wrong outputs for ?ARG2?of ?on?
(Error 1) and ?ARG1?
of ?our?
(Error 2)and ?work?
(Error 3), excess relation ?ARG1?
of?force?
(Error 4), and missing relation ?ARG1?
for?today?
(Error 5).
By correcting each of the errors1, 2, 3 and 4, all of these errors are corrected to-gether, and therefore classified into the same co-occurring error group.
Although error 5 cannotparticipate in the group, correcting error 5 can cor-rect all of the errors in the group, and therefore an# ofError types Errors Patterns?
Analyzed 2,078 1,671[Argument selection]Prepositional attachment 579 579Adjunction attachment 261 261Conjunction attachment 43 40Head selection for noun phrase 30 30Coordination 202 184[Predicate type selection]Preposition/Adjunction 108 54Gerund acts as modifier or not 84 31Coordination/conjunction 54 27# of arguments for preposition 51 17Adjunction/adjunctive noun 13 13[More structural errors]To-infinitive for 120 22modifier/argument of verbSubject for passive sentence 8 3or not[Others]Comma 444 372Relative clause attachment 102 38?
Unanalyzed 2,631 ?Total 4,709 ?Table 3: Errors extracted with descriptive analysisinductive relation is given from error 5 to the co-occurring error group.
We can then finally obtainthe inductive relations as shown at the bottom ofFigure 10.
This approach can trace the actual be-havior of the parser precisely, and can thereforecapture underlying dependencies which cannot befound only by observing error outputs.4 ExperimentsWe applied our approaches to parsing errors givenby the HPSG parser Enju, which was trained onthe Penn Treebank (Marcus et al, 1994) section2-21.
We first examined each approach, and thenexplored the combination of the approaches.4.1 Evaluation of descriptive approachWe examined our descriptive approach.
We firstparsed sentences in the Penn Treebank section 22with Enju, and then observed the errors.
Based onthe observation, we next described the patterns asshown in Section 3.
After that, we parsed section0 and then applied the patterns to the errors.Table 3 summarizes the extracted errors.
As thetable shows, with the 14 error patterns, we suc-cessfully matched 1,671 locations in error outputsand covered 2,078 of 4,709 errors, which com-prised of more than 40% of the total errors.
Thiswas the first step of the application of our ap-proach, and in the future work we would like to1166Evaluated sentences (erroneous) 1,811 (1,009)Errors (Correctable) 4,709 (3,085)Co-occurring errors 1,978Extracted inductive relations 501F-score (LP/LR) 90.69 (90.78/93.59)Table 4: Summary of our empirical approach      Figure 11: Frequency of each size of co-occurringerror groupadd more patterns for capturing more phenomena.When we focused on individual patterns, wecould observe that the simple error phenomenasuch as the attachments were dominant.
The firstreason for this would be that such phenomenawere among minimal linguistic events.
This wouldmake the phenomena components of other morecomplex ones.
The second reason for the dom-inance would be that the patterns for these errorphenomena were easy to implement only with ar-gument inconsistencies, and only one or a few pat-terns could cover every probable error.
Amongthese dominant error types, the number of prepo-sitional attachments was outstanding.
The er-ror types which required matching with predicatetypes were fewer than the attachment errors sincethe limited patterns on the predicate types wouldnarrow the possible linguistic behavior of the can-didate words.
When we focus on more structuralerrors, the table shows that the rates of the partici-pant errors to matched locations were much largerthan those for simpler pattern errors.
Once our pat-terns matches, they could collect many errors atthe same time.4.2 Evaluation of empirical approachNext, we applied our empirical approach in thesame settings as in the previous section.
We firstparsed sentences in section 0 and then applied ourapproach to the obtained errors.
In the experi-ments, some errors could not be forcibly correctedby our approach.
The parser ?cut off?
less proba-ble parse substructures before giving the predicateSentence: The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  once  it  enters thelungs  ,  with  even  brief  exposures  to  it  causing  symptoms  that  show  up  decades  later,  researchers  said(a)(b)(c) (d)(a) fiber      , : crocidoliteapp_arg12fiber      , : crocidolitecoord_arg12Correct answer:Parser output:is     usually     resilient     ?
the     lungs        ,        with(b)symptoms    that     show : up    decades    later(c)Parser output:Correct answer: verb_arg1symptoms    that     show : up    decades    laterverb_arg12(d)ARG1 ARG2ARG1 ARG2ARG1 ARG1ARG1 ARG2ARG1 ARG1Correct answer:Parser output: is     usually     resilient     ?
the     lungs        ,        withARG1 ARG1Correct answer:Parser output:It    causing    symptoms    that    show    up    decades    laterARG1It    causing    symptoms    that    show    up    decades    laterARG1Figure 12: Obtained co-occurring error groupsargument relation for reducing the cost of parsing.In this research, we ignored the errors which weresubject to such ?cut off?
as ?uncorrectable?
ones,and focused only on the remaining ?correctable?errors.
In our future work, we would like to con-sider the ?uncorrectable?
errors.Table 4 shows the summary of the analysis withour approach.
Enju gave 4,709 errors for section0.
Among these errors, the correctable errors were3,085, and from these errors, we successfully ob-tained 1,978 co-occurring error groups and 501 in-ductive relations.
Figure 11 shows the frequencyfor each size of co-occurring groups.
About a halfof the groups contains only single errors, whichwould indicate that the errors could have only one-way inductive relations with other errors.
The restof this section explores examples of the obtainedco-occurring error groups and inductive relations.Figure 12 shows an example of the extracted co-occurring error groups.
For the sentence shown atthe top of the figure, Enju gave seven errors.
Byintroducing our empirical approach, these errorswere definitely classified into four co-occurring er-ror groups (a) to (d), and there were no inductiverelations detected among them.
Group (a) containstwo errors on the comma?s local behavior as ap-position or coordination.
Group (b) contains theerrors on the words which gave almost the sameattachment behaviors.
Group (c) contains the er-rors on whether the verb ?show?
took ?decades?1167Error types # of correctable errors # of independent errors Correction effect (errors)[Argument selection]Prepositional attachment 531 397 766Adjunction attachment 196 111 352Conjunction attachment 33 12 79Head selection for noun phrase 22 0 84Coordination 146 62 323[Predicate type selection]Preposition/Adjunction 72 30 114Gerund acts as modifier or not 39 18 62Coordination/conjunction 36 16 61# of arguments for preposition 24 23 26Adjunction/adjunctive noun 8 6 10[More structural errors]To-infinitive for 75 27 87modifier/argument of verbSubject for passive sentence or not 8 3 9[Others]Comma 372 147 723Relative clause attachment 84 27 119Total 1,646 979 ?Table 5: Induction relations between errors for each linguistic phenomenon and other errorsSentence: She  says  she  offered  Mrs.  Yeargin a  quiet  resignationand  thought  she  could  help  save  her  teaching  certificate(a) (b)Correcting (a) induced correcting (b)(b) Correct answer:Parser output:?
thought  she  could  help   save : her  teaching  certificateverb_arg123?
thought  she  could  help   save : her  teaching  certificateverb_arg12ARG1 ARG2ARG1ARG1 ARG2 ARG3(a) Correct answer:Parser output:?
thought   she   could     help : save   her   teaching   certificateverb_arg12?
thought   she   could     help : save   her   teaching   certificateaux_arg12ARG1 ARG2ARG2 ARG2ARG1 ARG2ARG2ARG2Figure 13: Inductive relation between obtained co-occurring error groupsas its object or not.
Group (d) contains an error onthe attachment of the adverb ?later?.
Regardlessof the overlap of the regions in the sentence for(c) and (d), our approach successfully classifiedthe errors into the two independent groups.
Withour approach, it would be empirically shown thatthe errors in each group actually co-occurred andthe group was independent.
This would enable usto concentrate on each of the co-occurring errorgroups without paying attention to the influencesfrom the errors in other groups.Figure 13 shows another example of the anal-ysis with our empirical approach.
In this case, 8errors for a sentence were classified into two co-occurring error groups (a) and (b), and our ap-proach showed that correction in group (a) re-sulted in correcting group (b) together.
The errorsin group (a) were on whether ?help?
behaved as anauxiliary or pure verbal role.
The errors in group(b) were on whether ?save?
took only one object?her teaching certificate,?
or two objects ?her?
and?teaching certificate.?
Between group (a) and (b),no ?structural?
conflict could be found when cor-recting only each of the groups.
We could thenguess that the inductive relation between these twogroups was implicitly given by the disambigua-tion model of the parser.
By dividing the errorsinto minimum units and clarifying the effects ofcorrecting a target error, error analysis with ourempirical approach could suggest some policy forparser improvements.4.3 Combination of two approachesOn the basis of the experiments shown in the pre-vious sections, we would like to explore possibili-ties for obtaining a more detailed analysis by com-bining the two approaches.4.3.1 Interactions between a target linguisticphenomenon and other errorsOur descriptive approach could classify the pars-ing errors according to the linguistic phenomenathey participated in.
We then attempt to reveal howsuch classified errors interacted with other errorsfrom the viewpoints of our empirical approach.
Inorder to enable the analysis by our empirical ap-proach, we focused only on the correctable errors.1168Sentence: It  invests  heavily  in  dollar-denominated  securities  overseas  and  iscurrently  waiving  management  fees  ,  which  boosts  its  yield (a)(b)(a)It  invests  heavily  in  dollar-denominated  securities    overseas :adj_arg1?Adjunction attachment?ARG1ARG1Pattern matched:is  currently  waiving  management  fees              ,         which           boosts   its  yield(b)?Comma?
, ?Relative clause attachment?Pattern matched:ARG1ARG1ARG1ARG1ARG1ARG1Error:Error:Figure 14: Combination of results given by de-scriptive and empirical approaches (1)Table 5 reports the degree to which the classi-fied errors were related to other individual errors.The leftmost numbers show the numbers of cor-rectable errors, which were the focused errors inthe experiments.
The central numbers show thenumbers of ?independent?
errors, that is, the errorswhich could be corrected only by correcting them-selves.
The rightmost numbers show ?correctioneffects,?
that is, the number of errors which wouldconsequently be corrected if all of the errors forthe focused phenomena were forcibly corrected.?Independent?
errors are obtained by collectingerror phenomena groups which consist of unionsof co-occurring error groups and each error inwhich is not induced by other errors.
Figure 14shows an example of ?independent?
errors.
Forthe sentence at the top of the figure, the parser hadfour errors on ARG1 of ?overseas,?
the comma,?which?
and ?boosts.?
Our empirical approachthen classified these errors into two co-occurringerror groups (a) and (b), and there was no induc-tive relation between the groups.
Our descrip-tive approach, on the other hand, matched all ofthe errors with the patterns for ?Adjunction at-tachment,?
?Comma?
and ?Relative clause attach-ment.?
Since the error for the ?Adjunction attach-ment?
equals to a co-occurring group (a) and is notinduced by other errors, the error is ?independent.
?Table 5 shows that, for ?Prepositional attach-ment?, ?Adjunction attachments,?
?# of argu-ments for preposition?
and ?Adjunction/adjunctivenoun,?
more than half of the errors for the focusedphenomena are ?independent.?
Containing many?independent?
errors would mean that the parsershould handle these phenomena further more in-tensively as an independent event.Sentence: Clark  J.  Vitulli was  named  senior  vice  president  and  general  managerof  this  U.S.  sales  and  marketing  arm  of  Japanese  auto  Maker  Mazda  Motor  Corp(b) (a)(b)(a)senior  vice  president  and  general  manager  of  this  U.S.  sales   and :coord_arg12?Coordination?
(fragment)ARG1ARG1Pattern matched:Correcting (a) induced correcting (b)manager   of     this : U.S.   sales    and : marketing  arm  of?Coordination?
(fragment),?Head selection of noun phrase?Pattern matched:det_arg1 coord_arg12ARG2ARG1ARG2 ARG1ARG2 ARG1 ARG1 ARG1 ARG2Error:Error:Figure 15: Combination of results given by de-scriptive and empirical approaches (2)The ?correction effect?
for a focused linguisticphenomenon can be obtained by counting errors inthe union of the correctable error set for the phe-nomenon and the error sets which were induced bythe individual errors in the set.
We would show anexample of correction effect in Figure 15.
In thefigure, the parser had six errors for the sentenceat the top: three false outputs for ARG1 of ?and,??this?
and ?U.S.,?
two false outputs for ARG2 of?of?
and ?and,?
and missing output for ARG1 of?sales.?
Our empirical approach classified theseerrors into two co-occurring error groups (a) and(b), and extracted an inductive relation from (a) to(b).
Our descriptive approach, on the other hand,matched two errors on ?and?
with pattern ?Coor-dination?
and one error on ?this?
with ?Head se-lection for noun phrase.?
When we focus on theerror for ?Head selection of noun phrase?
in co-occurring group (a), the correction of the error in-duced the rest of the errors in (a), and further in-duced the error in (b) according to the inductiverelation from (a) to (b).
Therefore, a ?correctioneffect?
for the error results in six errors.Table 5 shows that, for ?Conjunction attach-ment,?
?Head selection for noun phrase?
and ?Co-ordination,?
each ?correction effect?
results inmore than twice the forcibly corrected errors.
Im-proving the parser so that it can resolve such high-correction-effect erroneous phenomena may ad-ditionally improve the parsing performances to agreat extent.
On the other hand, ?Head selectionfor noun phrase?
contains no ?independent?
error,and therefore could not be handled independentlyof other erroneous phenomena at all.
Consider-1169ing the effects from outer events might make thetreatment of ?Head selection for noun phrase?
amore complicated process than other phenomena,regardless of its high ?correction effect.
?Table 5 would thus suggest which phenomenonwe should resolve preferentially from the threepoints of view: the number of errors, the numberof ?independent?
errors and its ?correction effect.
?Considering these points, ?Prepositional attach-ment?
seems most preferable for handling first.4.3.2 Possibilities for further analysisSince the errors for the phenomenon were system-atically collected with our descriptive approach,we can work on further focused error analyseswhich would answer such questions as ?Whichpreposition causes most errors in attachments?
?,?Which pair of a correct answer and an erroneousoutput for predicate argument relations can occurmost frequently?
?, and so on.
Our descriptive ap-proach would enable us to thoroughly obtain suchanalyses with more closely-defined patterns.
Inaddition, our empirical approach would clarify theinfluences of the obtained error properties on theparser?s behaviors.
The results of the focused anal-yses might reasonably lead us to the features thatcan be captured as parameters for model training,or policies for re-ranking the parse candidates.The combination of our approaches would giveus interesting clues for planning effective strate-gies for improving the parser.
Our challenges forcombining the two approaches are now in the pre-liminary stage and there would be many possibili-ties for further detailed analysis.5 Related workAlthough there have been many researches whichanalyzed errors on their own systems in the part ofthe experiments, there have been few researcheswhich focused mainly on error analysis itself.In the field of parsing, McDonald and Nivre(2007) compared parsing errors between graph-based and transition-based parsers.
They observedthe accuracy transitions from various points ofview, and the obtained statistical data suggestedthat error propagation seemed to occur in thegraph structures of parsing outputs.
Our researchproceeded for one step in this point, and attemptedto reveal the way of the propagations.
In exam-ining the combination of the two types of pars-ing, McDonald and Nivre (2007) utilized similarapproaches to our empirical analysis.
They al-lowed a parser to give only structures given bythe parsers.
They implemented the ideas for eval-uating the parser?s potentials whereas we imple-mented the ideas for observing error propagations.Dredze et al (2007) showed the possibilitythat many parsing errors in the domain adaptationtasks came from inconsistencies between annota-tion manners of training resources.
Such findingswould further suggest that, comparing given errorswithout considering the inconsistencies could leadto the misunderstanding of what occurs in domaintransitions.
The summarized error dependenciesgiven by our approaches would be useful clues forextracting such domain-dependent error phenom-ena.Gime?nez and Ma`rquez (2008) proposed an au-tomatic error analysis approach in machine trans-lation (MT) technologies.
They were developinga metric set which could capture features in MToutputs at different linguistic levels with differentlevels of granularity.
As we considered the parsingsystems, they explored the way to resolve costlyand non-rewarding error analysis in the MT field.One of their objectives was to enable researchersto easily access detailed linguistic reports on theirsystems and to concentrate only on analyses forthe system improvements.
From this point of view,our research might provide an introduction intosuch rewarding analysis in parsing.6 ConclusionsWe proposed empirical and descriptive approachesto extracting dependencies among parsing errors.In the experiments, with each of our approaches,we successfully obtained relevant errors.
More-over, the possibility was shown that the combina-tion of our approaches would give a more detailederror analysis which would bring us useful cluesfor parser improvements.In our future work, we will improve the per-formance of our approaches by adding more pat-terns for the descriptive approach and by handlinguncorrectable errors for the empirical approach.With the obtained robust information, we will ex-plore rewarding ways for parser improvements.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan).1170ReferencesMark Dredze, John Blitzer, Partha Pratim Talukdar,Kuzman Ganchev, Joa?o V. Grac?a, and FernandoPereira.
2007.
Frustratingly hard domain adapta-tion for dependency parsing.
In Proceedings of theCoNLL Shared Task Session of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 1051?1055.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
Towardsheterogeneous automatic MT error analysis.
InProceedings of the Sixth International Conferenceon Language Resources and Evaluation (LREC?08),pages 1894?1901.Ronald M. Kaplan and Joan Bresnan.
1995.
Lexical-functional grammar: A formal system for gram-matical representation.
Formal Issues in Lexical-Functional Grammar, pages 29?130.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert Macintyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn Treebank: Annotatingpredicate argument structure.
In Proceedings ofARPA Human Language Technology Workshop.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency pars-ing models.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 122?131.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilis-tic disambiguation models for wide-coverage HPSGparsing.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics(ACL), pages 83?90.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fastHPSG parsing.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 155?163.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Mark Steedman.
2000.
The Syntactic Process.
THEMIT Press.1171
