Charting the Depths of Robust Speech ParsingW.
Kasper t, B. Kiefer t, H.-U.
Kriegert, C. J. Rupp$, and K. L. Worm $tGerman Research Center for Artificial Intelligence (DFKI)$Computat ional  Linguistics Department,  Universit~t des Saarlandes{kasper, kiefer, krieger}@dfki, de and {c j, worm}@coli, uni-sb, deAbst rac tWe describe a novel method for coping with un-grammatical input based on the use of chart-likedata structures, which permit anytime process-ing.
Priority is given to deep syntactic anal-ysis.
Should this fail, the best partial analy-ses are selected, according to a shortest-pathsalgorithm, and assembled in a robust process-ing phase.
The method has been applied ina speech translation project with large HPSGgrammars.1 IntroductionThis paper describes a new method of deal-ing robustly with deficient speech or text in-put, which may be due to recognition errors,spontaneous speech phenomena, orungrammat-ical constructions.
Two key features of this ap-proach are:?
the priority given to a deep and restrictivegrammatical nalysis,?
and the use of chart-like data structures ateach level of processing.The initial input is taken from a Word Hy-pothesis Graph, or WHG, (Oerder and Ney,1993) from which the best ranked paths aresuccessively selected until a result is found ora time limit proportional to the length of theutterance 1 is reached.
Each path is parsed withan incremental chart parser that uses a Head-Driven Phrase Structure grammar (HPSG).The parser is adapted to input from WHGs andoptimized to meet the needs of real-time speechprocessing.
Since the goal of the parsing compo-nent is to process as many WHG paths as pos-sible, in order to find a grammatical utterance1This is currently up to four times real time.and analyze it with highest accuracy, neitherrelaxation of the constraints imposed by thegrammar nor repair rules are used at this stage.If the analysis of the current path is successful,the parsing process is complete.
However, inmost cases there is no spanning and syntacti-cally correct analysis.
So a sequence of partialanalyses is determined by incrementally eval-uating the passive edges in the parser's chart.These analyzed fragments are passed on to arobust semantic processing component for fur-ther treatment, while the next best WHG pathis analyzed by the parser 2.
Robust semanticprocessing similarly builds up a chart-like datastructure including analyzed fragments and theresults of applying robustness rules at the se-mantic level.
After the first path of the WHGhas been (unsuccessfully) analyzed, processingin both the restrictive parser and the robust-ness component proceeds in parallel, with theaid of a parallel virtual machine, until one ofthe following conditions is fulfilled:1. a spanning rammatical nalysis is found,2.
all the WHG paths have been explored, or3.
the time limit is reached.In the case of either of the latter two condi-tions, robust semantic processing is allowed alimited time to complete processing and thenthe best result or sequence of results is selectedfrom its chart.Our approach has been implemented inVERBMOBIL (Wahlster, 1993), a large scale re-search project in the area of spoken language2This means that the maximal sequential delay be-tween parsing and robust semantics processing is theparse time for one path.
Similarly, the limit on pars-ing time, essentially, applies to both components405translation.
Its goal is to develop a system thattranslates negotiation dialogues between Ger-man, English, and Japanese speakers in face-to-face or video conferencing situations.
Thisapplication highlights the basic problem asso-ciated with machine processing of spontaneousspeech, namely that the input to the naturallanguage processing component is perturbed bytwo influences:1.
Speakers make mistakes, correct them-selves during speaking, produce false startsand use ungrammatical constructions.2.
The acoustic signal produced by a humanspeaker is mapped by a speech recognizeronto a written form; this mapping is rarelycompletely correct.This introduces two levels of uncertainty intothe processing of speech, which make the taskof linguistically analyzing a spoken utterance ina speech processing system doubly hard.
In ad-dition, the dialogue context imposes trict timeconstraints, as the overall system must attemptto emulate real time performance.The strategy we adopt responds to time con-straints by universally incorporating an anytimeproperty (Dean and Boddy, 1988) into the se-lection procedures.
As will be seen, this prop-erty derives from the way in which intermedi-ate results are stored and the selections whichcan be made from among these.
However, theoverriding priority of this same strategy is tomaximize the chance that a truly grammaticalpath will be found and analyzed, if one existsin the WHG.
This means that while we haveimplemented extensive mechanisms to achieverobustness, their design, and in particular theseparation of processing into a restrictive parserand a robust postprocessor, are subservient tothe cases where a fully grammatical nalysis ispossible, since these results are in any case bet-ter.
These decisions may be in conflict withmuch of the literature on robust parsing (e.g.,(Hindle, 1983; Hipp, 1993; Heinecke et al,1998)), but the alternative of relaxing the pars-ing constraints would appear to be a dead endin the context of the VERBMOBIL architecture.In the first place, the chances of locating thebest grammatical path in the lattice would bereduced, e.g., by the acceptance of a precedingungrammatical one.
Secondly, a more liberalparser would raise the spectre of an explosionof edges in the parser's chart, so that in factless paths could be processed overall, regardlessof their quality.
Either of these conditions couldprove fatal.This paper focuses on the aspects of theVERBMOBIL analysis component which ensurethat the most accurate results available are pro-vided to the system as a whole.
We first de-scribe the basic inventory we need to explainour approach: the unification-based bottom-upchart parser, the HPSG grammar, and the in-terface terms which are exchanged between theparser and the robust semantic processing.
Af-ter that, we come to the basic algorithm whichdetermines best partial analyses.
We also givean example of how the evaluation function onedges might look.
In section 4, we focus onthe robust semantic processing whose task isto store and combine the partial results, beforechoosing a final result out of a set of possiblecandidates.
We end this paper by presentingempirical results on the usefulness of our ap-proach.2 P re l iminar ies2.1 The  Char t  ParserThe parser used in the system is a bottom-up chart parser.
Since the grammar is a pureunification-based grammar, there is no context-free backbone and the chart edges are labelledwith typed feature structures.
At the moment,there is no local ambiguity packing of chartedges.
Therefore, the worst case complexity ofparsing is potentially exponential, but since theparser employs a best-first strategy, exponentialbehavior is rarely found in practice.The parser provides a flexible priority systemfor guiding the parsing process, using parsingtasks on an agenda.
A parsing task representsthe combination of a passive chart edge and anactive chart edge or a rule.
When such a com-bination succeeds, new tasks are generated andfor each new task, a priority is assigned.This priority system helps to obtain good par-tial results, even in cases where the search spacecannot be fully explored ue to parsing time re-strictions.
A higher time bound would alloweither the processing of more WHG paths or amore elaborate analysis of the given input, both406of which may lead to better results.
The deci-sion when to switch to the next best path of agiven WHG depends on the length of the inputand on the time already used.
After the pars-ing of one path is finished, the passive edges ofthe chart form a directed acyclic graph which isdirectly used as input to compute best partialanalyses.We note here that the parser processes the n-best paths of a WHG fully incrementally.
I e.,when the analysis of a new input path begins,only those input items are added to the chartthat have not been part of a previously treatedpath.
Everything else that has been computedup to that point remains in the chart and canbe used to process the new input without beingrecomputed.2.2 The HPSG GrammarsThe grammars for English, German, andJapanese follow the paradigm of HPSG (Pol-lard and Sag, 1994) which is the most advancedunification-based grammatical theory based ontyped feature structures.
The fundamental con-cept is that of a sign, a structure incorporatinginformation from all levels of linguistic analysis,such as phonology, morphology, syntax, and se-mantics.
This structure makes all informationsimultaneously available and provides declara-tive interfaces between these levels.
The gram-mars use Minimal Recursion Semantics (Copes-take et al, 1996) as the semantic representationformalism, allowing us to deal with ambiguityby underspecification.To give an impression of the size of gram-mars, we present he numbers for the Germangrammar.
It consists of 2,389 types, 76 ruleschemata, 4,284 stems and an average of sixentries per stem.
Morphological information iscomputed online which further increases the lex-ical ambiguity.2.3 Par t ia l  Ana lyses  and  theSyntax-Semant ics  Inter faceOur architecture requires that the linguisticanalysis module is capable of delivering not justanalyses of complete utterances, but also ofphrases and even of lexical items in the specialinterface format of VITs (VERBMOBIL InterfaceTerms) (Bos et al, 1998).
There are three con-siderations which the interface has to take intoaccount:1.
Only maximal projections, i.e., completephrases, are candidates for robust process-ing.
This qualifies, e.g., prepositional ndnoun phrases.
On the other hand, thisapproach leaves gaps in the coverage ofthe input string as not every word needsto be dominated by a maximal projec-tion.
In particular, verbal projections be-low the sentential level usually are incom-plete phrases.
The use of intermediate, in-complete projections is avoided for severalreasons:?
intermediate projections are highlygrammar and language specific and?
there are too many of them.2.
Phrases must be distinguished from ellipti-cal utterances.
A major difference is thatelliptical utterances express a speech act.E.g., a prepositional phrase can be a com-plete utterance xpressing an answer to aquestion (On Monday.)
or a question itself(On Monday?).
If the phrase occurs in asentence, it is not associated with a speechact of its own.
This distinction is dealt within the grammars by specifying special typesfor these complete utterances, phrases, andlexical items.3.
For robust processing, the interface mustexport a certain amount of informationfrom syntax and morphology together withthe semantics of the phrase.
In addition,it is necessary to represent semanticallyempty parts of speech, e.g., separable verbprefixes in German.3 Comput ing  Best  Par t ia l  Ana lysesIn contrast o a traditional parser which nevercomes up with an analysis for input not cov-ered by the grammar, our approach focuses onpartial analyses without giving up the correct-ness of the overall deep grammar.
These par-tial analyses are combined in a later stage (seeSection 4) to form total analyses.
But whatis a partial analysis?
Obviously a derivation(sub)tree licensed by the grammar which coversa continuous part of the input (i.e., a passiveparser edge).
But not every passive edge is agood candidate, since otherwise we would endup with perhaps thousands of them.
Our ap-407proach lies in between these two extremes: com-puting a connected sequence of best partial anal-yses which cover the whole input.
The idea hereis to view the set of passive edges of a parseras a directed graph which needs to be evaluatedaccording to a user-defined (and therefore gram-mar and language specific) metric.
Using thisgraph, we then compute the shortest paths w.r.t.the evaluation function, i.e., paths through thisgraph with minimum cost.Since this graph is acyclic and topologicallysorted (vertices are integers and edges alwaysconnect a vertex to a larger vertex), we havechosen the DAG-shortest-path algorithm (Cot-men et al, 1990) which runs in O(V + E).
Thisfast algorithm is a solution to the single-sourceshortest-paths problem.
We modified and ex-tended this algorithm to cope with the needs weencountered in speech parsing: (i) one can useseveral start and end vertices (e.g., in the caseof n-best chains or WHGs); (ii) all best shortestpaths are returned (i.e., we obtain a shortest-path subgraph); and (iii) evaluation and selec-tion of the best edges is done incrementally asisthe case for parsing the n-best chains (i.e., onlynew passive edges entered into the chart areevaluated and may be selected by our shortest-path algorithm).We now sketch the basic algorithm.
LetG = (V, E) denote the set of passive edges, ?the set of start vertices, E the set of end ver-tices, and let n be the vertex with the high-est number (remember, vertices are integers):n = max(V).
In the algorithm, we make useof two global vectors of length n which storeinformation associated with each vertex: distkeeps track of the distance of a vertex to oneof the start vertices (the so-called shortest-pathestimate), whereas pred records the predeces-sors of a given vertex, weight defines the costof an edge and is assigned its value during theevaluation stage of our algorithm according tothe user-defined function Estimate.
Finally, Adjconsists of all vertices adjacent to a given vertex(we use an adjacency-list representation).Clearly, before computing the shortest path,the distance of a vertex to one of the start ver-tices is infinity, except for the start vertices,and there is of course no shortest path subgraph(pred(v) +-- 0).Initialise-Single-Source( G, S) : ?
:=~global dist, pred;for each v E V(G) dodist(v) +-- co;pred(v) +-- 0od;for each s E S dodist(s) +-- 0od.After initialization, we perform evaluationand relaxation on every passive edge, taken intopologically sorted order.
Relaxing an edge(u, v) means checking whether we can improvethe shortest path(s) to v via u.
There aretwo cases to consider: either we overwrite theshortest-path estimate for v since the new oneis better (and so have a new predecessor for v,viz., u), or the shortest-path estimate is as goodas the old one, hence we have to add v to thepredecessors of v. In case the shortest-path es-timate is worse, there is clearly nothing to do.Relax(u, v) :?==~global dist, pred;if dist(v) > dist(u) + weight(u, v)then dodist(v) +-- dist(u) + weight(u, v);pred(v) ~ {u)ode lse dowhen dist(v) = dist(u) + weight(u, v) dopred(v) +-- pred(v) U {u)ododft.The shortest paths are then determined by es-timating and relaxing edges, beginning with thestart vertices S. The shortest path subgraph isstored in pred and can be extracted by walk-ing from the end vertices ?
'back' to the startvertices.DAG-Shortest-Paths(G, S, C) :?--~global pred;Initialis e-Single-S ource (G ,  ) ;for each u E V(G) \ C taken in topologicallysorted order dofor each v e Adj(u) doweight(u, v) +-- Estimate(u, v);Relax (u, v)odod;re turn  pred.408After we have determined the shortest-pathsubgraph, the feature structures associated withthese edges are selected and transformed to thecorresponding VITs which are then sent to therobust semantic processing component.This approach has an important property:even if certain parts of the input have not un-dergone at least one rule application, there arestill lexical edges which help to form a best paththrough the passive dges.
Hence, this approachshows anytime behavior which is a necessary e-quirement in time-critical (speech) applications:even if the parser is interrupted at a certainpoint, we can always return a shortest path upto that moment hrough our chart.Let us now give an example to see what theevaluation function on edges (i.e., derivationtrees) might look like3:?
n-ary trees (n > 1) with utterance status(e.g., NPs, PPs): value 1?
lexical items: value 2?
otherwise: value ooIf available, other properties, uch as prosodicinformation or probabilistic scores can also beutilized in the evaluation function to determinethe best edges.P RSFigure 1: Computing best partial analyses.Note that the paths PR and QR are chosen,but not ST, although S is the longest edge.
Byusing uniform costs, all three paths would beselected.Depending on the evaluation, our methoddoes not necessarily favor paths with longestedges as the example in Figure 1 shows - -  theabove strategy instead prefers paths contain-ing no lexical edges (where this is possible) andaThis is a slightly simplified form of the evaluationthat is actually used for the German grammar.there might be several such paths having thesame cost.
Longest (sub)paths, however, canbe obtained by employing an exponential func-tions during the evaluation of an edge e E E:Estimate (e) = - (max ($) - rain (8) )length (e).4 Robust  Semant ic  P rocess ingThe second phase of processing, after produc-ing a set of partial analyses, consists of assem-bling and combining the fragments, where pos-sible.
We call this robust semantic processing(Worm and Rupp, 1998), since the structuresbeing dealt with are semantic representations(VITs) and the rules applied refer primarily tothe semantic ontent of fragments, though theyalso consider syntactic and prosodic informa-tion, e.g., about irregular boundaries.This phase falls into three tasks:1. storing the partial analyses from theparser,2.
combining them on the basis of a set ofrules, and3.
selecting a result.For storing of partial results, both deliveredfrom the parser or constructed later, we makeuse of a chart-like data structure we call VIThypothesis graph (VHG), since it bears a resem-blance to the WHG which is input to the parser.It is organized according to WHG vertices.
Wegive an example in Figure 2, which will be ex-plained in 4.1.Combination of partial results takes placeusing a set of rules which describe how frag-mentary analyses can be combined.
There arelanguage-independent rules, e.g., describing thecombination of a semantic functor with a possi-ble argument, and language specific ones, suchas those for dealing with self-corrections in Ger-man.
Each operation carried out delivers a con-fidence value which influences the score assignedto an edge.The overall mechanism behind the robust se-mantic processing resembles that of a chartparser.
It runs in parallel with the HPSGparser; each time the parser delivers partial re-sults, they are handed over and processed, whilethe parser may continue to look for a betterpath in the WHG.
The processing strategy is40981: oa'st + Ihnen + den h~ll~ n T~g ~'109998.3~ f43.11i 3: pa'st (9999.01 I~ V 2: Ihnen (9999.01 ~I42: pa'sl + Ihnel (19998.31 \[3.2143: a'sl + Ihnen (19999.0}\[3~2\] ~1: den halben Taq (89999.0)23: den halben Taq (80999.1) \[1\]41: Ihnen + den halbert Ta.q (90998.9) \]'2,23\]Figure 2: The VHG for the first example.
Only three VITs are delivered by the parser (the shortestpath), although the number of passive edges is 217.agenda-based, giving priority to new parser re-sults.Selection of a result means that the best edgecovering the whole input, or if that has not beenachieved, an optimal sequence of edges has tobe selected.
We use a simple graph search al-gorithm which finds the path with the highestsum of individual scores.Note that the robust semantic processing hasthe anytime property as well: as soon as the firstpartial result has been entered into the chart, aresult can be delivered on demand.4.1 An  ExampleConsider the utterance (1) where the case of theNP den halben Tag ('half the day') is accusativeand thus does not match the subcategorizationrequirements of the verb passen ('suit') whichwould require nominative.
(1) Pa6t Ihnen den halben Tag?
'Does half the day suit you?
'According to the grammar, this string is ill-formed, thus no complete analysis can beachieved.
However, the parser delivers frag-ments for pa~t, Ihnen, and den halben Tag.
(2) verb_arg_r  : :\[ \[type (Vl, verbal), missing_arg (Vl) \],\[type (V2, term), pos sible_arg (V2, Vl) \] \]\[apply_fun (V1, V2, V3),assign_mood(V3,V4)\] & V4.When these results are stored, the rule in(2) will combine the verb with its first argu-ment, Ihnen.
Each rule consists of three parts:mnemonic rule name, tests on a sequence of in-put VITs and the operations performed to con-struct the ouput VIT.
The first separator is : :,the second --->.
A further application of thesame rule accounts for the second argument, denhalben Tag.
However, the confidence value forthe second combination will reflect the viola-tion of the case requirement.
The resulting edgespans the whole input and is selected as output.The corresponding VHG is shown in Figure 2.4.2 Br idg ingNot all cases can be handled as simply.
Of-ten, there are partial results in the input whichcannot be integrated into a spanning result.
Inthese cases, a mechanism called bridging is ap-plied.
Consider the self-correction i (3).
(3) Ich treffe .. .
habe einen Terrain amMontag.
'I (will) meet .. .
have an appointment onMonday.
'Again, the parser will only find partial results.Combinations of ich with tre~e lead nowhere;the combination of the second verb with the NPdoes not lead to a complete analysis either (cf.Figure 3).
Note that if a nominal argument canbind several argument roles, for each such read-ing there is a passive dge in the VHG.
Its scorereflects to what degree the selectional require-ments of the verb, in terms of the required caseand sortal restrictions, have been met.If no spanning result exists when all ruleshave been applied, the bridging mechanism pro-duces new active edges which extend edges al-ready present.
Here, it extends the active edgeaiming to combine ich with a verbal functor toend after tre\]\]e, thus allowing for a combinationwith the VP already built, habe einen Termin410r18: Ich ~9999.0) n~/O: treffe (9999.0) \[?76: ich + treffe (19998.3) \]18,10\[77: ich + treffe (19999.0) \[18,10\]258: ich + habe (19998.3)259: ich + habe (19999.0) 1260: ich + habe (19998.7) 1264: icb + babe + einen Termin am Montaq (179998.71 \[18~49\]263: ich + habe + einen Termin am Montaq (179998.0) \[18,49\]262: ich + habe + einen Termin am Montaq (179998.71 \[18,48\]261: ich + habe + Ainen Termiq a m M~ntao ~179999.0~ \[184812: habe (9999.0) rl 1: einen Termin am Montaq (159999.0} n3,2\]18.2\]8,21 48: babe + einen Termin am Montaq (169999.0) \[2,1\]49: babe + einen Termin am Montaq (169996.7) r2~1\]Figure 3: The VHG for the second example.am Montag.
Extending the active edges fromleft to right corresponds to the linear nature ofself-corrections, in which material to the rightreplaces ome to the left.4.3 Scoring and Result SelectionThe scoring function for edges takes into ac-count their length, the coverage of the edge, thenumber of component edges it consists of, andthe confidence value for the operation which cre-ated it.
It has to satisfy the following property,which is illustrated in Figure 4: If there are twoedges which together span an interval (edges aand b) and another edge which has been builtfrom them (edge c), the latter should get a bet-ter score than the sequence of the original twoedges.
If there is another edge from the parserwhich again spans the complete interval (edged), it should get a better score than the edgebuilt from the two components.ddc: \[a,b\]Figure 4: Requirements for the scoring function.The selection is done in two different ways.If there is more than one spanning result, thescores of the spanning results are weighted ac-cording to a statistical model describing se-quence probabilities based on semantic predi-cates (Ruland et al, 1998) and the best is se-lected.
Otherwise, the best sequence, i.e., theone with the highest score, is chosen in squaretime, using a standard graph search algorithm.5 Empirical ResultsFor an intermediate valuation of the robustsemantic processing phase, we ran our systemconsisting of HPSG parser and robust semanticprocessing on a dialogue from the VERBMOBILcorpus of spontaneous appointment egotiationdialogues, producing WHGs from the originalrecorded audio data.
The dialogue consists of90 turns.
These 90 turns were split into 130 seg-ments according to pauses by the speech recog-nizer.
The segments received 213 segment anal-yses, i.e., there are 1.6 analyses per segment onaverage.
172 (80.8%) of these were generatedby the parser and 41 (19.2%) were assembledfrom parser esults by robust semantic process-ing.
Of these 41 results, 34 (83%) were sensiblyimproved, while 7 (17~0) did not represent a realimprovement.This evaluation is local in the sense that weonly consider the input-output behaviour of ro-bust semantic processing.
We do this in order toexclude the effects of insufficiencies introducedby other modules in the system, since theywould distort the picture.
For this same rea-son, the criterion we apply is whether the resultdelivered is a sensible combination of the frag-411ments received, without reference to the originalutterance or the translation produced.
How-ever, in the long run we plan to compare thecomplete system's behaviour with and withoutthe robust processing strategy.6 Conc lus ionThe approach to the robust analysis of spokenlanguage input, that we have described above,exhibits three crucial properties.1.
The restrictive parser is given the maxi-mum opportunity of finding a correct anal-ysis for a grammatical sequence of word hy-potheses, where this exists.2.
The robustness component assembles par-tial analyses as a fallback, if no grammati-cal sequence can be found.3.
Almost arbitrary time constraints can besupported.
Though, obviously, more pro-cessing time would usually improve the re-sults.The latter property depends directly on thechart-like data structures used at each level ofprocessing.
Whether it be the input WHG,VHG for robust processing or, most signifi-cantly, the parser's chart; each is formally a di-rected acyclic graph and each permits a selec-tion of the best intermediate r sult at, virtually,any stage in processing, for a given evaluationfunction.The relatively efficient processing of WHG in-put achieved by parsing and robustness compo-nents working in parallel depends quite heav-ily on the successive processing of ranked WHGpaths, effectively as alternative input strings.AcknowledgmentsWe would like to thank the anonymous ACLreviewers for their detailed comments.
Thisresearch was supported by the German Fed-eral Ministry for Education and Research undergrants nos.
01 IV 701 R4 and 01 IV 701 V0.Re ferencesJohan Bos, C.J.
Rupp, Bianka Buschbeck-Wolf,and Michael Dorna.
1998.
Managing infor-mation at linguistic interfaces.
In Proc.
ofthe 17 th COLING/36 th ACL, pages 160-166,Montr@al, Canada.Ann Copestake, Dan Flickinger, and Ivan A.Sag.
1996.
Minimal recursion semantics, anintroduction.
Ms, Stanford.Thomas H. Cormen, Charles E. Leiserson, andRonald L. Rivest.
1990.
Introduction to Al-gorithms.
MIT Press, Cambridge, MA.Thomas Dean and Mark Boddy.
1988.
An anal-ysis of time-dependent planning.
In Proceed-ings of the 7th National Conference on Arti-ficial Intelligence, AAAI-88, pages 49-54.Johannes Heinecke, Jfirgen Kunze, WolfgangMenzel, and Ingo SchrSder.
1998.
E l imina-tive parsing with graded constraints.
In Proc.of the 17 th COLING/36 ~h ACL, pages 526-530, Montr@al, Canada.Donald Hindle.
1983.
Deterministic parsing ofsyntactic non-fluencies.
In Proc.
of the 21 thACL, pages 123-128, Cambridge, MA.Dwayne Richard Hipp.
1993.
Design and De-velopment of Spoken Natural-Language Dia-log Parsing Systems.
Ph.D. thesis, Depart-ment of Computer Science, Duke University,Durham, NC.Martin Oerder and Hermann Ney.
1993.Word graphs: An efficient interface betweencontinuous-speech recognition and languageunderstanding.
In Proc.
Int.
Conf.
on Acous-tics, Speech and Signal Processing (ICASSP),Minneapolis, MN.
IEEE Signal ProcessingSociety.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
Univer-sity of Chicago Press, Chicago.Tobias Ruland, C. J. Rupp, JSrg Spilker, HansWeber, and Karsten L. Worm.
1998.
Mak-ing the most of multiplicity: A multi-parsermulti-strategy architecture for the robustprocessing of spoken language.
In Proc.
ofthe 1998 International Conference on Spo-ken Language Processing (ICSLP 98), Syd-ney, Australia.Wolfgang Wahlster.
1993.
VERBMOBIL -translation of face-to-face dialogs.
In Proc.MT Summit IV, pages 127-135, Kobe, Japan,July.Karsten L. Worm and C. J. Rupp.
1998.
To-wards robust understanding of speech bycombination of partial analyses.
In Proc.
ofthe 13 th ECAL pages 190-194, Brighton,UK.412
