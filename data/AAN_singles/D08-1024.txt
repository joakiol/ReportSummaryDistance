Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224?233,Honolulu, October 2008. c?2008 Association for Computational LinguisticsOnline Large-Margin Training ofSyntactic and Structural Translation FeaturesDavid ChiangInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292, USAchiang@isi.eduYuval Marton and Philip ResnikDepartment of Linguistics and the UMIACSLaboratory for Computational Linguisticsand Information ProcessingUniversity of MarylandCollege Park, MD 20742, USA{ymarton,resnik}@umiacs.umd.eduAbstractMinimum-error-rate training (MERT) is a bot-tleneck for current development in statisticalmachine translation because it is limited inthe number of weights it can reliably opti-mize.
Building on the work of Watanabe etal., we explore the use of the MIRA algorithmof Crammer et al as an alternative to MERT.We first show that by parallel processing andexploiting more of the parse forest, we canobtain results using MIRA that match or sur-pass MERT in terms of both translation qual-ity and computational cost.
We then test themethod on two classes of features that addressdeficiencies in the Hiero hierarchical phrase-based model: first, we simultaneously train alarge number of Marton and Resnik?s soft syn-tactic constraints, and, second, we introducea novel structural distortion model.
In bothcases we obtain significant improvements intranslation performance.
Optimizing them incombination, for a total of 56 feature weights,we improve performance by 2.6 B???
on asubset of the NIST 2006 Arabic-English eval-uation data.1 IntroductionSince its introduction by Och (2003), minimum er-ror rate training (MERT) has been widely adoptedfor training statistical machine translation (MT) sys-tems.
However, MERT is limited in the number offeature weights that it can optimize reliably, withfolk estimates of the limit ranging from 15 to 30 fea-tures.One recent example of this limitation is a seriesof experiments by Marton and Resnik (2008), inwhich they added syntactic features to Hiero (Chi-ang, 2005; Chiang, 2007), which ordinarily uses nolinguistically motivated syntactic information.
Eachof their new features rewards or punishes a deriva-tion depending on how similar or dissimilar it isto a syntactic parse of the input sentence.
Theyfound that in order to obtain the greatest improve-ment, these features had to be specialized for par-ticular syntactic categories and weighted indepen-dently.
Not being able to optimize them all at onceusing MERT, they resorted to running MERT manytimes in order to test different combinations of fea-tures.
But it would have been preferable to use atraining method that can optimize the features all atonce.There has been much work on improving MERT?sperformance (Duh and Kirchoff, 2008; Smith andEisner, 2006; Cer et al, 2008), or on replacingMERT wholesale (Turian et al, 2007; Blunsom etal., 2008).
This paper continues a line of research ononline discriminative training (Tillmann and Zhang,2006; Liang et al, 2006; Arun and Koehn, 2007),extending that of Watanabe et al (2007), who usethe Margin Infused Relaxed Algorithm (MIRA) dueto Crammer et al (2003; 2006).
Our guiding princi-ple is practicality: like Watanabe et al, we train ona small tuning set comparable in size to that usedby MERT, but by parallel processing and exploit-ing more of the parse forest, we obtain results us-ing MIRA that match or surpass MERT in terms ofboth translation quality and computational cost on alarge-scale translation task.Taking this further, we test MIRA on two classesof features that make use of syntactic informationand hierarchical structure.
First, we generalize Mar-ton and Resnik?s (2008) soft syntactic constraints by224training all of them simultaneously; and, second, weintroduce a novel structural distortion model.
We ob-tain significant improvements in both cases, and fur-ther large improvements when the two feature setsare combined.The paper proceeds as follows.
We describe ourtraining algorithm in section 2; our generalizationof Marton and Resnik?s soft syntactic constraints insection 3; our novel structural distortion features insection 4; and experimental results in section 5.2 Learning algorithmThe translation model is a standard linear model(Och and Ney, 2002), which we train using MIRA(Crammer and Singer, 2003; Crammer et al, 2006),following Watanabe et al (2007).
We describe thebasic algorithm first and then progressively refine it.2.1 Basic algorithmLet e, by abuse of notation, stand for both outputstrings and their derivations.
We represent the fea-ture vector for derivation e as h(e).
Initialize the fea-ture weights w. Then, repeatedly:?
Select a batch of input sentences f1, .
.
.
, fm.?
Decode each fi to obtain a set of hypothesistranslations ei1, .
.
.
, ein.?
For each i, select one of the ei j to be the oracletranslation e?i , by a criterion described below.Let ?hi j = h(e?i ) ?
h(ei j).?
For each ei j, compute the loss `i j, which issome measure of how bad it would be to guessei j instead of e?i .?
Update w to the value of w?
that minimizes:12?w?
?
w?2 + Cm?i=1max1?
j?n(`i j ?
?hi j ?
w?)
(1)where we set C = 0.01.
The first term meansthat we want w?
to be close to w, and secondterm (the generalized hinge loss) means that wewant w?
to score e?i higher than each ei j by amargin at least as wide as the loss `i j.When training is finished, the weight vectors fromall iterations are averaged together.
(If multiplepasses through the training data are made, we onlyaverage the weight vectors from the last pass.)
Thetechnique of averaging was introduced in the con-text of perceptrons as an approximation to taking avote among all the models traversed during training,and has been shown to work well in practice (Fre-und and Schapire, 1999; Collins, 2002).
We followMcDonald et al (2005) in applying this technique toMIRA.Note that the objective (1) is not the same as thatused by Watanabe et al; rather, it is the same asthat used by Crammer and Singer (2003) and relatedto that of Taskar et al (2005).
We solve this opti-mization problem using a variant of sequential min-imal optimization (Platt, 1998): for each i, initialize?i j = C for a single value of j such that ei j = e?i ,and initialize ?i j = 0 for all other values of j. Then,repeatedly choose a sentence i and a pair of hypothe-ses j, j?, and letw?
?
w?
+ ?
(?hi j ?
?hi j?)
(2)?i j ?
?i j + ?
(3)?i j?
?
?i j?
?
?
(4)where?
= clip[?
?i j,?i j?
](`i j ?
`i j?)
?
(?hi j ?
?hi j?)
?
w??
?hi j ?
?hi j?
?2(5)where the function clip[x,y](z) gives the closest num-ber to z in the interval [x, y].2.2 Loss functionAssuming B???
as the evaluation criterion, the loss`i j of ei j relative to e?i should be related somehowto the difference between their B???
scores.
How-ever, B???
was not designed to be used on individ-ual sentences; in general, the highest-B???
transla-tion of a sentence depends on what the other sen-tences in the test set are.
Sentence-level approxi-mations to B???
exist (Lin and Och, 2004; Lianget al, 2006), but we found it most effective to per-form B???
computations in the context of a set O ofpreviously-translated sentences, following Watan-abe et al (2007).
However, we don?t try to accu-mulate translations for the entire dataset, but simplymaintain an exponentially-weighted moving averageof previous translations.225More precisely: For an input sentence f, let e besome hypothesis translation and let {rk} be the set ofreference translations for f. Let c(e; {rk}), or simplyc(e) for short, be the vector of the following counts:|e|, the effective reference length mink |rk|, and, for1 ?
n ?
4, the number of n-grams in e, and the num-ber of n-gram matches between e and {rk}.
Thesecounts are sufficient to calculate a B???
score, whichwe write as B???(c(e)).
The pseudo-document O isan exponentially-weighted moving average of thesevectors.
That is, for each training sentence, let e?
bethe 1-best translation; after processing the sentence,we update O, and its input length O f :O ?
0.9(O + c(e?))
(6)O f ?
0.9(O f + |f|) (7)We can then calculate the B???
score of hypothe-ses e in the context of O.
But the larger O is, thesmaller the impact the current sentence will have onthe B???
score.
To correct for this, and to bring theloss function roughly into the same range as typicalmargins, we scale the B???
score by the size of theinput:B(e; f, {rk}) = (O f + |f|) ?
B???
(O + c(e; {rk})) (8)which we also simply write as B(e).
Finally, the lossfunction is defined to be:`i j = B(e?i ) ?
B(ei j) (9)2.3 Oracle translationsWe now describe the selection of e?.
We know ofthree approaches in previous work.
The first is toforce the decoder to output the reference sentenceexactly, and select the derivation with the highestmodel score, which Liang et al (2006) call bold up-dating.
The second uses the decoder to search forthe highest-B???
translation (Tillmann and Zhang,2006), which Arun and Koehn (2007) call max-B???updating.
Liang et al and Arun and Koehn experi-ment with these methods and both opt for a thirdmethod, which Liang et al call local updating: gen-erate an n-best list of translations and select thehighest-B???
translation from it.
The intuition is thatdue to noise in the training data or reference transla-tions, a high-B???
translation may actually use pe-culiar rules which it would be undesirable to en-courage the model to use.
Hence, in local updating,Model scoreB??
?score0.40.50.60.70.80.91-90 -85 -80 -75 -70 -65 -60?
= 0?
= 0.5?
= 1?
= ?Figure 1: Scatter plot of 10-best unique translations of asingle sentence obtained by forest rescoring using variousvalues of ?
in equation (11).the search for the highest-B???
translation is limitedto the n translations with the highest model score,where n must be determined experimentally.Here, we introduce a new oracle-translation selec-tion method, formulating the intuition behind localupdating as an optimization problem:e?
= arg maxe(B(e) + h(e) ?
w) (10)Instead of choosing the highest-B???
translationfrom an n-best list, we choose the translation thatmaximizes a combination of (approximate) B??
?and the model.We can also interpret (10) in the following way:we want e?
to be the max-B???
translation, but wealso want to minimize (1).
So we balance these twocriteria against each other:e?
= arg maxe(B(e) ?
?
(B(e) ?
h(e) ?
w)) (11)where (B(e) ?
h(e) ?
w) is that part of (1) that de-pends on e?, and ?
is a parameter that controls howmuch we are willing to allow some translations tohave higher B???
than e?
if we can better minimize(1).
Setting ?
= 0 would reduce to max-B???
up-dating; setting ?
= ?
would never update w at all.Setting ?
= 0.5 reduces to equation (10).Figure 1 shows the 10-best unique translations fora single input sentence according to equation (11)under various settings of ?.
The points at far right arethe translations that are scored highest according to226the model.
The ?
= 0 points in the upper-left cornerare typical of oracle translations that would be se-lected under the max-B???
policy: they indeed havea very high B???
score, but are far removed from thetranslations preferred by the model; thus they wouldcause violent updates to w. Local updating wouldselect the topmost point labeled ?
= 1.
Our schemewould select one of the ?
= 0.5 points, which haveB???
scores almost as high as the max-B???
transla-tions, yet are not very far from the translations pre-ferred by the model.2.4 Selecting hypothesis translationsWhat is the set {ei j} of translation hypotheses?
Ide-ally we would let it be the set of all possible transla-tions, and let the objective function (1) take all ofthem into account.
This is the approach taken byTaskar et al (2004), but their approach assumes thatthe loss function can be decomposed into local lossfunctions.
Since our loss function cannot be so de-composed, we select:?
the 10-best translations according to the model;we then rescore the forest to obtain?
the 10-best translations according to equation(11) with ?
= 0.5, the first of which is the oracletranslation, and?
the 10-best translations with ?
= ?, to serve asnegative examples.The last case is what Crammer et al (2006) callmax-loss updating (where ?loss?
refers to the gener-alized hinge loss) and Taskar et al (2005) call loss-augmented inference.
The rationale here is that sincethe objective (1) tries to minimize max j(`i j ?
?hi j ?w?
), we should include the translations that have thehighest (`i j ?
?hi j ?
w) in order to approximate theeffect of using the whole forest.See Figure 1 again for an illustration of the hy-potheses selected for a single sentence.
The max-B???
points in the upper left are not included (andwould have no effect even if they were included).The ?
= ?
points in the lower-right are the negativeexamples: they are poor translations that are scoredtoo high by the model, and the learning algorithmattempts to shift them to the left.To perform the forest rescoring, we need to useseveral approximations, since an exact search forB??
?-optimal translations is NP-hard (Leusch et al,2008).
For every derivation e in the forest, we calcu-late a vector c(e) of counts as in Section 2.2 exceptusing unclipped counts of n-gram matches (Dreyeret al, 2007), that is, the number of matches for an n-gram can be greater than the number of occurrencesof the n-gram in any reference translation.
This canbe done efficiently by calculating c for every hyper-edge (rule application) in the forest:?
the number of output words generated by therule?
the effective reference length scaled by the frac-tion of the input sentence consumed by the rule?
the number of n-grams formed by the applica-tion of the rule (1 ?
n ?
4)?
the (unclipped) number of n-gram matchesformed by the application of the rule (1 ?
n ?4)We keep track of n-grams using the same schemeused to incorporate an n-gram language model intothe decoder (Wu, 1996; Chiang, 2007).To find the best derivation in the forest, we tra-verse it bottom-up as usual, and for every set of al-ternative subtranslations, we select the one with thehighest score.
But here a rough approximation lurks,because we need to calculate B on the nodes of theforest, but B does not have the optimal substructureproperty, i.e., the optimal score of a parent node can-not necessarily be calculated from the optimal scoresof its children.
Nevertheless, we find that this rescor-ing method is good enough for generating high-B??
?oracle translations and low-B???
negative examples.2.5 ParallelizationOne convenient property of MERT is that it is em-barrassingly parallel: we decode the entire tuning setsending different sentences to different processors,and during optimization of feature weights, differ-ent random restarts can be sent to different proces-sors.
In order to make MIRA comparable in effi-ciency to MERT, we must parallelize it.
But withan online learning algorithm, parallelization requiresa little more coordination.
We run MIRA on each227processor simultaneously, with each maintaining itsown weight vector.
A master process distributes dif-ferent sentences from the tuning set to each of theprocessors; when each processor finishes decodinga sentence, it transmits the resulting hypotheses,with their losses, to all the other processors and re-ceives any hypotheses waiting from other proces-sors.
Those hypotheses were generated from differ-ent weight vectors, but can still provide useful in-formation.
The sets of hypotheses thus collected arethen processed as one batch.
When the whole train-ing process is finished, we simply average all theweight vectors from all the processors.Having described our training algorithm, whichincludes several practical improvements to Watan-abe et al?s usage of MIRA, we proceed in the re-mainder of the paper to demonstrate the utility of theour training algorithm on models with large numbersof structurally sensitive features.3 Soft syntactic constraintsThe first features we explore are based on a lineof research introduced by Chiang (2005) and im-proved on by Marton and Resnik (2008).
A hi-erarchical phrase-based translation model is basedon synchronous context-free grammar, but does notnormally use any syntactic information derived fromlinguistic knowledge or treebank data: it uses trans-lation rules that span any string of words in the inputsentence, without regard for parser-defined syntac-tic constituency boundaries.
Chiang (2005) exper-imented with a constituency feature that rewardedrules whose source language side exactly spans asyntactic constituent according to the output of anexternal source-language parser.
This feature canbe viewed as a soft syntactic constraint: it biasesthe model toward translations that respect syntacticstructure, but does not force it to use them.
However,this more syntactically aware model, when tested inChinese-English translation, did not improve trans-lation performance.Recently, Marton and Resnik (2008) revisitedthe idea of constituency features, and succeeded inshowing that finer-grained soft syntactic constraintsyield substantial improvements in B???
score forboth Chinese-English and Arabic-English transla-tion.
In addition to adding separate features for dif-ferent syntactic nonterminals, they introduced a newtype of constraint that penalizes rules when thesource language side crosses the boundaries of asource syntactic constituent, as opposed to simplyrewarding rules when they are consistent with thesource-language parse tree.Marton and Resnik optimized their features?weights using MERT.
But since MERT does notscale well to large numbers of feature weights, theywere forced to test individual features and manu-ally selected feature combinations each in a sepa-rate model.
Although they showed gains in trans-lation performance for several such models, manylarger, potentially better feature combinations re-mained unexplored.
Moreover, the best-performingfeature subset was different for the two languagepairs, suggesting that this labor-intensive feature se-lection process would have to be repeated for eachnew language pair.Here, we use MIRA to optimize Marton andResnik?s finer-grained single-category features all atonce.
We define below two sets of features, a coarse-grained class that combines several constituency cat-egories, and a fine-grained class that puts differentcategories into different features.
Both kinds of fea-tures were used by Marton and Resnik, but only afew at a time.
Crucially, our training algorithm pro-vides the ability to train all the fine-grained features,a total of 34 feature weights, simultaneously.Coarse-grained features As the basis for coarse-grained syntactic features, we selected the followingnonterminal labels based on their frequency in thetuning data, whether they frequently cover a spanof more than one word, and whether they repre-sent linguistically relevant constituents: NP, PP, S,VP, SBAR, ADJP, ADVP, and QP.
We define twonew features, one which fires when a rule?s sourceside span in the input sentence matches any of theabove-mentioned labels in the input parse, and an-other which fires when a rule?s source side spancrosses a boundary of one of these labels (e.g., itssource side span only partially covers the words ina VP subtree, and it also covers some or all or thewords outside the VP subtree).
These two featuresare equivalent to Marton and Resnik?s XP= and XP+feature combinations, respectively.228Fine-grained features We selected the followingnonterminal labels that appear more than 100 timesin the tuning data: NP, PP, S, VP, SBAR, ADJP,WHNP, PRT, ADVP, PRN, and QP.
The labels thatwere excluded were parts of speech, nonconstituentlabels like FRAG, or labels that occurred only twoor three times.
For each of these labels X, we addeda separate feature that fires when a rule?s source sidespan in the input sentence matches X, and a secondfeature that fires when a span crosses a boundary ofX.
These features are similar to Marton and Resnik?sX= and X+, except that our set includes features forWHNP, PRT, and PRN.4 Structural distortion featuresIn addition to parser-based syntactic constraints,which were introduced in prior work, we introducea completely new set of features aimed at improv-ing the modeling of reordering within Hiero.
Again,the feature definition gives rise to a larger number offeatures than one would expect to train successfullyusing MERT.In a phrase-based model, reordering is per-formed both within phrase pairs and by the phrase-reordering model.
Both mechanisms are able tolearn that longer-distance reorderings are morecostly than shorter-distance reorderings: phrasepairs, because phrases that involve more extreme re-orderings will (presumably) have a lower count inthe data, and phrase reordering, because models areusually explicitly dependent on distance.By contrast, in a hierarchical model, all reorderingis performed by a single mechanism, the rules of thegrammar.
In some cases, the model will be able tolearn a preference for shorter-distance reorderings,as in a phrase-based system, but in the case of a wordbeing reordered across a nonterminal, or two non-terminals being reordered, there is no dependence inthe model on the size of the nonterminal or nonter-minals involved in reordering.So, for example, if we have rulesX?
(il dit X1, he said X1) (12)X?
(il dit X1,X1 he said) (13)we might expect that rule (12) is more common ingeneral, but that rule (13) becomes more and more???????
?Figure 2: Classifying nonterminal occurrences for thestructural distortion model.rare as X1 gets larger.
The default Hiero featureshave no way to learn this.To address this defect, we can classify everynonterminal pair occurring on the right-hand sideof each grammar rule as ?reordered?
or ?not re-ordered?, that is, whether it intersects any other wordalignment link or nonterminal pair (see Figure 2).We then define coarse- and fine-grained versions ofthe structural distortion model.Coarse-grained features Let R be a binary-valued random variable that indicates whether a non-terminal occurrence is reordered, and let S be aninteger-valued random variable that indicates howmany source words are spanned by the nonterminaloccurrence.
We can estimate P(R | S ) via relative-frequency estimation from the rules as they are ex-tracted from the parallel text, and incorporate thisprobability as a new feature of the model.Fine-grained features A difficulty with thecoarse-grained reordering features is that the gram-mar extraction process finds overlapping rules in thetraining data and might not give a sensible proba-bility estimate; moreover, reordering statistics fromthe training data might not carry over perfectly intothe translation task (in particular, the training datamay have some very freely-reordering translationsthat one might want to avoid replicating in transla-tion).
As an alternative, we introduce a fine-grainedversion of our distortion model that can be traineddirectly in the translation task as follows: define229a separate binary feature for each value of (R, S ),where R is as above and S ?
{?, 1, .
.
.
, 9,?10} and ?means any size.
For example, if a nonterminal withspan 11 has its contents reordered, then the features(true,?10) and (true, ?)
would both fire.
Groupingall sizes of 10 or more into a single feature is de-signed to avoid overfitting.Again, using MIRA makes it practical to trainwith the full fine-grained feature set?coincidentallyalso a total of 34 features.5 Experiment and resultsWe now describe our experiments to test MIRA andour features, the soft-syntactic constraints and thestructural distortion features, on an Arabic-Englishtranslation task.
It is worth noting that this exper-imentation is on a larger scale than Watanabe etal.
?s (2007), and considerably larger than Martonand Resnik?s (2008).5.1 Experimental setupThe baseline model was Hiero with the followingbaseline features (Chiang, 2005; Chiang, 2007):?
two language models?
phrase translation probabilities p( f | e) andp(e | f )?
lexical weighting in both directions (Koehn etal., 2003)?
word penalty?
penalties for:?
automatically extracted rules?
identity rules (translating a word into it-self)?
two classes of number/name translationrules?
glue rulesThe probability features are base-100 log-probabilities.The rules were extracted from all the allow-able parallel text from the NIST 2008 evalua-tion (152+175 million words of Arabic+English),aligned by IBM Model 4 using GIZA++ (union ofboth directions).
Hierarchical rules were extractedfrom the most in-domain corpora (4.2+5.4 millionwords) and phrases were extracted from the remain-der.
We trained the coarse-grained distortion modelon 10,000 sentences of the training data.Two language models were trained, one on datasimilar to the English side of the parallel text andone on 2 billion words of English.
Both were 5-gram models with modified Kneser-Ney smoothing,lossily compressed using a perfect-hashing schemesimilar to that of Talbot and Brants (2008) but usingminimal perfect hashing (Botelho et al, 2005).We partitioned the documents of the NIST 2004(newswire) and 2005 Arabic-English evaluation datainto a tuning set (1178 sentences) and a develop-ment set (1298 sentences).
The test data was theNIST 2006 Arabic-English evaluation data (NISTpart, newswire and newsgroups, 1529 sentences).To obtain syntactic parses for this data, we tok-enized it according to the Arabic Treebank standardusing AMIRA (Diab et al, 2004), parsed it withthe Stanford parser (Klein and Manning, 2003), andthen forced the trees back into the MT system?s tok-enization.1We ran both MERT and MIRA on the tuningset using 20 parallel processors.
We stopped MERTwhen the score on the tuning set stopped increas-ing, as is common practice, and for MIRA, we usedthe development set to decide when to stop train-ing.2 In our runs, MERT took an average of 9 passesthrough the tuning set and MIRA took an average of8 passes.
(For comparison, Watanabe et al report de-coding their tuning data of 663 sentences 80 times.
)5.2 ResultsTable 1 shows the results of our experiments withthe training methods and features described above.All significance testing was performed against thefirst line (MERT baseline) using paired bootstrap re-sampling (Koehn, 2004).First of all, we find that MIRA is competitive withMERT when both use the baseline feature set.
In-1The only notable consequence this had for our experimen-tation is that proclitic Arabic prepositions were fused onto thefirst word of their NP object, so that the PP and NP bracketswere coextensive.2We chose this policy for MIRA to avoid overfitting.
How-ever, we could have used the tuning set for this purpose, just aswith MERT: in none of our runs would this change have mademore than a 0.2 B???
difference on the development set.230Dev NIST 06 (NIST part)Train Features # nw nw ng nw+ngMERT baseline 12 52.0 50.5 32.4 44.6syntax (coarse) 14 52.2 50.9 33.0+ 45.0+syntax (fine) 34 52.1 50.4 33.5++ 44.8distortion (coarse) 13 52.3 51.3+ 34.3++ 45.8++distortion (fine) 34 52.0 50.9 34.5++ 45.5++MIRA baseline 12 52.0 49.8?
34.2++ 45.3++syntax (fine) 34 53.1++ 51.3+ 34.5++ 46.4++distortion (fine) 34 53.3++ 51.5++ 34.7++ 46.7++distortion+syntax (fine) 56 53.6++ 52.0++ 35.0++ 47.2++Table 1: Comparison of MERT and MIRA on various feature sets.
Key: # = number of features; nw = newswire, ng =newsgroups; + or ++ = significantly better than MERT baseline (p < 0.05 or p < 0.01, respectively), ?
= significantlyworse than MERT baseline (p < 0.05).deed, the MIRA system scores significantly higheron the test set; but if we break the test set down bygenre, we see that the MIRA system does slightlyworse on newswire and better on newsgroups.
(Thisis largely attributable to the fact that the MIRA trans-lations tend to be longer than the MERT transla-tions, and the newsgroup references are also rela-tively longer than the newswire references.
)When we add more features to the model, the twotraining methods diverge more sharply.
When train-ing with MERT, the coarse-grained pair of syntaxfeatures yields a small improvement, but the fine-grained syntax features do not yield any further im-provement.
By contrast, when the fine-grained fea-tures are trained using MIRA, they yield substan-tial improvements.
We observe similar behavior forthe structural distortion features: MERT is not ableto take advantage of the finer-grained features, butMIRA is.
Finally, using MIRA to combine bothclasses of features, 56 in all, produces the largest im-provement, 2.6 B???
points over the MERT baselineon the full test set.We also tested some of the differences betweenour training method and Watanabe et al?s (2007); theresults are shown in Table 2.
Compared with localupdating (line 2), our method of selecting the ora-cle translation and negative examples does better by0.5 B???
points on the development data.
Using loss-augmented inference to add negative examples to lo-cal updating (line 3) does not appear to help.
Never-theless, the negative examples are important: for ifSetting Devfull 53.6local updating, no LAI 53.1?local updating, LAI 53.0???
= 0.5 oracle, no LAI failedno sharing of updates 53.1?
?Table 2: Effect of removing various improvements inlearning method.
Key: ?
or ??
= significantly worse thanfull system (p < 0.05 or p < 0.01, respectively); LAI =loss-augmented inference for additional negative exam-ples.we use our method for selecting the oracle transla-tion without the additional negative examples (line4), the algorithm fails, generating very long transla-tions and unable to find a weight setting to shortenthem.
It appears, then, that the additional negativeexamples enable the algorithm to reliably learn fromthe enhanced oracle translations.Finally, we compared our parallelization methodagainst a simpler method in which all processorslearn independently and their weight vectors are allaveraged together (line 5).
We see that sharing in-formation among the processors makes a significantdifference.6 ConclusionsIn this paper, we have brought together two existinglines of work: the training method of Watanabe et al(2007), and the models of Chiang (2005) and Marton231and Resnik (2008).
Watanabe et al?s work showedthat large-margin training with MIRA can be madefeasible for state-of-the-art MT systems by using amanageable tuning set; we have demonstrated thatparallel processing and exploiting more of the parseforest improves MIRA?s performance and that, evenusing the same set of features, MIRA?s performancecompares favorably to MERT in terms of both trans-lation quality and computational cost.Marton and Resnik (2008) showed that it is pos-sible to improve translation in a data-driven frame-work by incorporating source-side syntactic analy-sis in the form of soft syntactic constraints.
Thiswork joins a growing body of work demonstratingthe utility of syntactic information in statistical MT.In the area of source-side syntax, recent researchhas continued to improve tree-to-string translationmodels, soften the constraints of the input tree invarious ways (Mi et al, 2008; Zhang et al, 2008),and extend phrase-based translation with source-side soft syntactic constraints (Cherry, 2008).
Allthis work shows strong promise, but Marton andResnik?s soft syntactic constraint approach is par-ticularly appealing because it can be used unobtru-sively with any hierarchically-structured translationmodel.
Here, we have shown that using MIRA toweight all the constraints at once removes the cru-cial drawback of the approach, the problem of fea-ture selection.Finally, we have introduced novel structural dis-tortion features to fill a notable gap in the hierar-chical phrase-based approach.
By capturing how re-ordering depends on constituent length, these fea-tures improve translation quality significantly.
Insum, we have shown that removing the bottleneckof MERT opens the door to many possibilities forbetter translation.AcknowledgmentsThanks to Michael Bloodgood for performing ini-tial simulations of parallelized perceptron training.Thanks also to John DeNero, Kevin Knight, DanielMarcu, and Fei Sha for valuable discussions andsuggestions.
This research was supported in part byDARPA contract HR0011-06-C-0022 under subcon-tract to BBN Technologies and HR0011-06-02-001under subcontract to IBM.ReferencesAbhishek Arun and Philipp Koehn.
2007.
Onlinelearning methods for discriminative training of phrasebased statistical machine translation.
In Proc.
MTSummit XI.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.
Adiscriminative latent variable model for statistical ma-chine translation.
In Proc.
ACL-08: HLT.Fabiano C. Botelho, Yoshiharu Kohayakawa, and NivioZiviani.
2005.
A practical minimal perfect hashingmethod.
In 4th International Workshop on Efficientand Experimental Algorithms (WEA05).Daniel Cer, Daniel Jurafsky, and Christopher D. Man-ning.
2008.
Regularization and search for minimumerror rate training.
In Proc.
Third Workshop on Statis-tical Machine Translation.Colin Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In Proc.
ACL-08: HLT.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ACL 2005.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2).Michael Collins.
2002.
Discriminative training methodsfor Hidden Markov Models: Theory and experimentswith perceptron algorithms.
In Proc.
EMNLP 2002.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004.Automatic tagging of Arabic text: From raw text tobase phrase chunks.
In Proc.
HLT/NAACL 2004.Companion volume.Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.2007.
Comparing reordering constraints for SMT us-ing efficient B???
oracle computation.
In Proc.
2007Workshop on Syntax and Structure in Statistical Trans-lation.Kevin Duh and Katrin Kirchoff.
2008.
Beyond log-linearmodels: Boosted minimum error rate training for n-best re-ranking.
In Proc.
ACL-08: HLT, Short Papers.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Dan Klein and Chris D. Manning.
2003.
Fast exact infer-ence with a factored model for natural language pars-ing.
In Advances in Neural Information ProcessingSystems 15 (NIPS 2002).232Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProc.
HLT-NAACL 2003.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP2004.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2008.
Complexity of finding the BLEU-optimal hy-pothesis in a confusion network.
In Proc.
EMNLP2008.
This volume.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proc.
COLING-ACL2006.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE:a method for evaluating automatic evaluation metricsfor machine translation.
In Proc.
COLING 2004.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proc.
ACL-08: HLT.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proc.
ACL 2005.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
ACL-08: HLT.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
ACL 2002.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL 2003.John C. Platt.
1998.
Fast training of support vectormachines using sequential minimal optimization.
InBernhard Scho?lkopf, Christopher J. C. Burges, andAlexander J. Smola, editors, Advances in Kernel Meth-ods: Support Vector Learning, pages 195?208.
MITPress.David A. Smith and Jason Eisner.
2006.
Minimumrisk annealing for training log-linear models.
InProc.
COLING/ACL 2006, Poster Sessions.David Talbot and Thorsten Brants.
2008.
Random-ized language models via perfect hash functions.
InProc.
ACL-08: HLT.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,and Christopher Manning.
2004.
Max-margin pars-ing.
In Proc.
EMNLP 2004, pages 1?8.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured predic-tion models: A large margin approach.
In Proc.
ICML2005.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical MT.
InProc.
COLING-ACL 2006.Joseph Turian, Benjamin Wellington, and I. DanMelamed.
2007.
Scalable discriminative learning fornatural language parsing and translation.
In Advancesin Neural Information Processing Systems 19 (NIPS2006).Taro Watanabe, Jun Suzuki, Hajime Tsukuda, and HidekiIsozaki.
2007.
Online large-margin training for statis-tical machine translation.
In Proc.
EMNLP 2007.Dekai Wu.
1996.
A polynomial-time algorithm forstatistical machine translation.
In Proc.
34th AnnualMeeting of the Association for Computational Linguis-tics, pages 152?158.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
ACL-08: HLT.233
