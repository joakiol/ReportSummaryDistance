Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 81?89,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsUncertainty reduction as a measure of cognitive processing effortStefan L. FrankUniversity of AmsterdamAmsterdam, The Netherlandss.l.frank@uva.nlAbstractThe amount of cognitive effort required toprocess a word has been argued to dependon the word?s effect on the uncertaintyabout the incoming sentence, as quanti-fied by the entropy over sentence probabil-ities.
The current paper tests this hypoth-esis more thoroughly than has been donebefore by using recurrent neural networksfor entropy-reduction estimation.
A com-parison between these estimates and word-reading times shows that entropy reduc-tion is positively related to processing ef-fort, confirming the entropy-reduction hy-pothesis.
This effect is independent fromthe effect of surprisal.1 IntroductionIn the field of computational psycholinguistics, acurrently popular approach is to account for read-ing times on a sentence?s words by estimates of theamount of information conveyed by these words.Processing a word that conveys more informationis assumed to involve more cognitive effort, whichis reflected in the time required to read the word.In this context, the most common formaliza-tion of a word?s information content is its sur-prisal (Hale, 2001; Levy, 2008).
If word stringwt1 (short for w1, w2, .
.
.
wt) is the sentence sofar and P (wt+1|wt1) the occurrence probability ofthe next word wt+1, then that word?s surprisal isdefined as ?
log P (wt+1|wt1).
It is well estab-lished by now that word-reading times indeed cor-relate positively with surprisal values as estimatedby any sufficiently accurate generative languagemodel (Boston et al, 2008; Demberg and Keller,2008; Frank, 2009; Roark et al, 2009; Smith andLevy, 2008).A lesser known alternative operationalization ofa word?s information content is based on the un-certainty about the rest of the sentence, quantifiedby Hale (2003, 2006) as the entropy of the prob-ability distribution over possible sentence struc-tures.
The reduction in entropy that results fromprocessing a word is taken to be the amount ofinformation conveyed by that word, and was ar-gued by Hale to be predictive of word-readingtime.
However, this entropy-reduction hypothesishas not yet been comprehensively tested, possiblybecause of the difficulty of computing the requiredentropies.
Although Hale (2006) shows how sen-tence entropy can be computed given a PCFG, thiscomputation is not feasible when the grammar isof realistic size.Here, we empirically investigate the entropy-reduction hypothesis more thoroughly than hasbeen done before, by using recurrent neural net-works as language models.
Since these networksdo not derive any structure, they provide estimatesof sentence entropy rather than sentence-structureentropy.
In practice, these two entropies will gen-erally be similar: If the rest of the sentence ishighly uncertain, so is its structure.
Sentence en-tropy can therefore be viewed as a simplificationof structure entropy; one that is less theory depen-dent since it does not rely on any particular gram-mar.
The distinction between entropy over sen-tences and entropy over structures will simply beignored in the remainder of this paper.Results show that, indeed, a significant fractionof variance in reading-time data is accounted forby entropy reduction, over and above surprisal.2 Entropy and sentence processing2.1 Sentence entropyLet W be the set of words in the language and W ithe set of all word strings of length i.
The set ofcomplete sentences, denoted S, contains all wordstrings of any length (i.e., ?
?i=0 W i), except that aspecial end-of-sentence marker </s> is attachedto the end of each string.81A generative language model defines a proba-bility distribution over S. The entropy of this dis-tribution isH = ?
?wj1?SP (wj1) log P (wj1).As words are processed one by one, the sen-tence probabilities change.
When the first t words(i.e., the string wt1 ?
W t) of a sentence have beenprocessed, the entropy of the probability distribu-tion over sentences isH(t) = ?
?wj1?SP (wj1|wt1) log P (wj1|wt1).
(1)In order to simplify later equations, we definethe function h(y|x) = ?P (y|x) log P (y|x), suchthat Eq.
1 becomesH(t) =?wj1?Sh(wj1|wt1).If the first t words of wj1 do not equal wt1 (or wj1has fewer than t + 1 words),1 then P (wj1|wt1) = 0so h(wj1|wt1) = 0.
This means that, for computingH(t), only the words from t + 1 onwards need tobe taken into account:H(t) =?wjt+1?Sh(wjt+1|wt1).The reduction in entropy due to processing thenext word, wt+1, is?H(t + 1) = H(t)?H(t + 1).
(2)Note that positive ?H corresponds to adecrease in entropy.
According to Hale(2006), the nonnegative reduction in entropy (i.e.,max{0, ?H}) reflects the cognitive effort in-volved in processing wt+1 and should therefore bepredictive of reading time on that word.2.2 Suffix entropyComputing H(t) is computationally feasible onlywhen there are very few sentences in S, or whenthe language can be described by a small grammar.To estimate entropy in more realistic situations, an1Since wj1 ends with < /s > and wt1 does not, the twostrings must be different.
Consequently, if wj1 is t words long,then P (wj1|wt1) = 0.obvious solution is to look only at the next fewwords instead of all complete continuations of wt1.Let Sm be the subset of S containing all (andonly) sentences of length m or less, counting alsothe </s> at the end of each sentence.
Note thatthis set includes the ?empty sentence?
consistingof only </s>.
The set of length-m word stringsthat do not end in </s> is Wm.
Together, thesesets form Wm = Wm ?
Sm, which contains allthe relevant strings for defining the entropy overstrings up to length m.2 After processing wt1, theentropy over strings up to length t + n is:Hn(t) =?wj1?Wt+nh(wj1|wt1) =?wjt+1?Wnh(wjt+1|wt1).It now seems straightforward to define suffix-entropy reduction by analogy with sentence-entropy reduction as expressed in Eq.
2: Simplyreplace H by Hn to obtain?Hsufn (t + 1) = Hn(t)?Hn(t + 1).
(3)As indicated by its superscript label, ?Hsufnquantifies the reduction in uncertainty about theupcoming n-word suffix.
However, this is concep-tually different from the original ?H of Eq.
2,which is the reduction in uncertainty about theidentity of the current sentence.
The differencebecomes clear when we view the sentence proces-sor?s task as that of selecting the correct elementfrom S. If this set of complete sentences is ap-proximated by Wt+n, and the task is to select oneelement from that set, an alternative definition ofsuffix-entropy reduction arises:?Hsentn (t + 1)=?wj1?Wt+nh(wj1|wt1) ?
?wj1?Wt+nh(wj1|wt+11 )=?wjt+1?Wnh(wjt+1|wt1) ?
?wjt+2?Wn?1h(wjt+2|wt+11 )= Hn(t)?Hn?1(t + 1).
(4)The label ?sent?
indicates that ?Hsentn quantifiesthe reduction in uncertainty about which sentenceforms the current input.
This uncertainty is ap-proximated by marginalizing over all word stringslonger than t + n.It is easy to see thatlimn??
?Hsufn = limn??
?Hsentn = ?H,2The probability of a string wm1 ?
W m is the summedprobability of all sentences with prefix wm1 .82so both approximations of entropy reduction ap-propriately converge to ?H in the limit.
Nev-ertheless, they formalize different quantities andmay well correspond to different cognitive factors.If it is true that cognitive effort is predicted bythe reduction in uncertainty about the identity ofthe incoming sentence, we should find that word-reading times are predicted more accurately by?Hsentn than by ?Hsufn .2.3 Relation to next-word entropyIn the extreme case of n = 1, Eq.
4 reduces to?Hsent1 (t + 1) = H1(t)?H0(t + 1) = H1(t),so the reduction of entropy over the single nextword wt+1 equals the next-word entropy just be-fore processing that word.
Note that ?Hsent1 (t+1)is independent of the word at t + 1, making it aseverely impoverished measure of the uncertaintyreduction caused by that word.
We would there-fore expect reading times to be predicted more ac-curately by ?Hsentn with n > 1, and possibly evenby ?Hsuf1 .Roark et al (2009) investigated the relation be-tween H1(t + 1) and reading time on wt+1, andfound a significant positive effect: Larger next-word entropy directly after processing wt+1 cor-responded to longer reading time on that word.This is of particular interest because H1(t + 1)necessarily correlates negatively with entropy re-duction ?Hsentn (t + 1): If entropy is large afterwt+1, chances are that it did not reduce muchthrough processing of wt+1.
Indeed, in our dataset, H1(t + 1) and ?Hsentn (t + 1) correlate be-tween r = ?.29 and r = ?.26 (for n = 2 ton = 4) which is highly significantly (p ?
0) dif-ferent from 0.
Roark et al?s finding of a positiverelation between H1(t + 1) and reading time onwt+1 therefore seems to disconfirm the entropy-reduction hypothesis.3 MethodA set of language models was trained on a corpusof POS tags of sentences.
The advantage of usingPOS tags rather than words is that their probabil-ities can be estimated much more accurately and,consequently, more accurate prediction of word-reading time is possible (Demberg and Keller,2008; Roark et al, 2009).
Subsequent to training,the models were made to generate estimates of sur-prisal and entropy reductions ?Hsufn and ?Hsentnover a test corpus.
These estimates were then com-pared to reading times measured over the wordsof the same test corpus.
This section presents thedata sets that were used, language-model details,and the evaluation metric.3.1 DataThe models were trained on the POS tag se-quences of the full WSJ corpus (Marcus et al,1993).
They were evaluated on the POS-taggedDundee corpus (Kennedy and Pynte, 2005), whichhas been used in several studies that investigate therelation between word surprisal and reading time(Demberg and Keller, 2008; Frank, 2009; Smithand Levy, 2008).
This 2 368-sentence (51 501words) collection of British newspaper editorialscomes with eye-tracking data of 10 participants.POS tags for the Dundee corpus were taken fromFrank (2009).For each word and each participant, readingtime was defined as the total fixation time on thatword before any fixation on a later word of thesame sentence.
Following Demberg and Keller(2008), data points (i.e., word/participant pairs)were removed if the word was not fixated, waspresented as the first or last on a line, containedmore than one capital letter or a non-letter (e.g.,the apostrophe in a clitic), or was attached to punc-tuation.
Mainly due to the large number (over46%) of nonfixations, 62.8% of data points wereremoved, leaving 191 380 data points (between16 469 and 21 770 per participant).3.2 Language modelEntropy is more time consuming to compute thansurprisal, even for n = 1, because it requires es-timates of the occurrence probabilities at t + 1 ofall word types, rather than just of the actual nextword.
Moreover, the number of suffixes rises ex-ponentially as suffix length n grows, and, conse-quently, so does computation time.Roark et al (2009) used an incremental PCFGparser to obtain H1 but this method rapidly be-comes infeasible as n grows.
Low-order Markovmodels (e.g., a bigram model) are more efficientand can be used for larger n but they do not formparticularly accurate language models.
Moreover,Markov models lack cognitive plausibility.Here, Simple Recurrent Networks (SRNs) (El-man, 1990) are used as language models.
Whentrained to predict the upcoming input in a word se-quence, these networks can generate estimates of83P (wt+1|wt1) efficiently and relatively accurately.They thereby allow to approximate sentence en-tropy more closely than the incremental parsersused in previous studies.
Unlike Markov models,SRNs have been claimed to form cognitively re-alistic sentence-processing models (Christiansenand MacDonald, 2009).
Moreover, it has beenshown that SRN-based surprisal estimates can cor-relate more strongly to reading times than surprisalvalues estimated by a phrase-structure grammar(Frank, 2009).3.2.1 Network architecture and processingThe SRNs comprised three layers of units: the in-put layer, the recurrent (hidden) layer, and the out-put layer.
Each input unit corresponds to one POStag, making 45 input units since there are 45 dif-ferent POS tags in the WSJ corpus.
The network?soutput units represent predictions of subsequentinputs.
The output layer also has one unit for eachPOS tag, plus an extra unit that represents </s>,that is, the absence of any further input.
Hence,there were 46 output units.
The number of recur-rent units was fairly arbitrarily set to 100.As is common in these networks, the input layerwas fully connected to the recurrent layer, whichin turn was fully connected to the output layer.Also, there were time-delayed connections fromthe recurrent layer to itself.
In addition, each re-current and output unit received a bias input.The vectors of recurrent- and output-layer ac-tivations after processing wt1 are denoted arec(t)and aout(t), respectively.
At the beginning of eachsentence, arec(0) = 0.5.The input vector aiin, representing POS tag i,consists of zeros except for a single element (cor-responding to i) that equals one.
When input i isprocessed, the recurrent layer?s state is updated ac-cording to:arec(t) = frec(Wrecarec(t?
1) + Winaiin + brec),where matrices Win and Wrec contain the net-work?s input and recurrent connection weights, re-spectively; brec is the vector of recurrent-layer bi-ases; and activation function frec(x) is the logisticfunction f(x) = (1+e?x)?1 applied elementwiseto x.
The new output vector is now given byaout(t) = fout(Woutarec(t) + bout),where Wout is the matrix of output connectionweights; bout the vector of output-layer biases; andfout(x) the softmax functionfi,out(x1, .
.
.
, x46) =exi?j exj .This function makes sure that aout sums to oneand can therefore be viewed as a probability dis-tribution: The i-th element of aout(t) is the SRN?sestimate of the probability that the i-th POS tagwill be the input at t + 1, or, in case i correspondsto < /s >, the probability that the sentence endsafter t POS tags.3.2.2 Network trainingTen SRNs, differing only in their random initialconnection weights and biases, were trained us-ing the standard backpropagation algorithm.
Eachstring of WSJ POS tags was presented once, withthe sentences in random order.
After each POS in-put, connection weights were updated to minimizethe cross-entropy between the network outputs anda 46-element vector that encoded the next input (ormarked the end of the sentence) by the correspond-ing element having a value of one and all othersbeing zero.3.3 Evaluation3.3.1 Obtaining surprisal and entropySince aout(t) is basically the probability distribu-tion P (wt+1|wt1), surprisal and H1 can be read offdirectly.
To obtain H2, H3, and H4, we use thefact thatP (wt+nt+1 |wt1) =n?i=1P (wt+i|wt+i?11 ).
(5)Surprisal and entropy estimates were averagedover the ten SRNs.
So, for each POS tag of theDundee corpus, there was one estimate of surprisaland four of entropy (for n = 1 to n = 4).Since Hn(t) approximates H(t) more closelyas n grows, it would be natural to expect a betterfit to reading times for larger n. On the other hand,it goes without saying that Hn is only a very roughmeasure of a reader?s actual uncertainty about theupcoming n inputs, no matter how accurate thelanguage model that was used to compute theseentropies.
Crucially, the correspondence betweenHn and the uncertainty experienced by a readerwill grow even weaker with larger n. This is ap-parent from the fact that, as proven in the Ap-pendix, Hn can be expressed in terms of H1 andHn?1:Hn(t) = H1(t) + E(Hn?1(t + 1)),841 2 3 400.250.5suffix length ncorrelationwithsurprisal?Hnsuf?HnsentFigure 1: Coefficient of correlation between es-timates of surprisal and entropy reduction, as afunction of suffix length n.where E(x) is the expected value of x. Obviously,the expected value of Hn?1 is less appropriate asan uncertainty measure than is Hn?1 itself.
Hence,Hn can be less accurate than Hn?1 as a quantifi-cation of the actual cognitive uncertainty.
For thisreason, we may expect larger n to result in worsefit to reading-time data.33.3.2 Negative entropy reductionHale (2006) argued for nonnegative entropy re-duction max{0, ?H}, rather than ?H itself, asa measure of processing effort.
For ?Hsent, thedifference between the two is negligible becauseonly about 0.03% of entropy reductions are neg-ative.
As for ?Hsuf, approximately 42% of val-ues are negative so whether these are left outmakes quite a difference.
Since preliminary ex-periments showed that word-reading times are pre-dicted much more accurately by ?Hsuf than bymax{0, ?Hsuf}, only ?Hsuf and ?Hsent wereused here, that is, negative values were included.3.3.3 Relation between information measuresBoth surprisal and entropy reduction can be takenas measures for the amount of information con-veyed by a word, so it is to be expected that theyare positively correlated.
However, as shown inFigure 1, this correlation is in fact quite weak,ranging from .14 for ?Hsuf4 to .38 for ?Hsent1 .In contrast, ?Hsufn and ?Hsentn correlate verystrongly to each other: The coefficients of correla-tion range from .73 when n = 1 to .97 for n = 4.3Not to mention the realistic possibility that the cognitivesentence-processing system does not abide by the normativechain rule expressed in Eq.
5.0 4 8 1210?410?310?210?1100Effect sizeSignificance(p?value)p = .053.84Figure 2: Cumulative ?2 distribution with 1 de-gree of freedom, plotting statistical significance(p-value) as a function of effect size.3.3.4 Fit to reading timesA generalized linear regression model for gamma-distributed data was fitted to the reading times.4This model contained several well-known predic-tors of word-reading time: the number of lettersin the word, the word?s position in the sentence,whether the next word was fixated, whether theprevious word was fixated, log of the word?s rel-ative frequency, log of the word?s forward andbackward transitional probabilities,5 and surprisalof the part-of-speech.
Next, one set of entropy-reduction estimates was added to the regression.The effect size is the resulting decrease in the re-gression model?s deviance, which is indicative ofthe amount of variance in reading time accountedfor by those estimates of entropy reduction.
Fig-ure 2 shows how effect size is related to statis-tical significance: A factor forms a significant(p < .05) predictor of reading time if its effectsize is greater than 3.84.4 Results and Discussion4.1 Effect of entropy reductionFigure 3 shows the effect sizes for both measuresof entropy reduction, and their relation to suffixlength n. All effects are in the correct direction,that is, larger entropy reduction corresponds tolonger reading time.
These results clearly supportthe entropy-reduction hypothesis: A significant4The reading times, which are approximately gamma dis-tributed, were first normalized to make the scale parametersof the gamma distributions the same across participants.5These are, respectively, the relative frequency of theword given the previous word, and its relative frequencygiven the next word.851 2 3 40510suffix length n?Hneffectsize?Hnsuf?HnsentFigure 3: Size of the effect of ?Hsufn and ?Hsentnas a function of suffix length n.fraction of variance in reading time is accountedfor by the entropy-reduction estimates ?Hsentn ,over and above what is explained by the other fac-tors in the regression analysis, including surprisal.Moreover, the effect of ?Hsentn is larger than thatof ?Hsufn , indicating that it is indeed uncertaintyabout the identity of the current sentence, ratherthan uncertainty about the upcoming input(s), thatmatters for cognitive processing effort.
Only atn = 1 was the effect size of ?Hsentn smaller thanthat of ?Hsufn , but it should be kept in mind that?Hsent1 is independent of the incoming word andis therefore quite impoverished as a measure of theeffort involved in processing the word.
Moreover,the difference between ?Hsent1 and ?Hsuf1 is notsignificant (p > .4), as determined by the boot-strap method (Efron and Tibshirani, 1986).
In con-trast, the differences are significant when n > 1(all p < .01), in spite of the high correlation be-tween ?Hsentn and ?Hsufn .Another indication that cognitive processing ef-fort is modeled more accurately by ?Hsentn than by?Hsufn is that the effect size of ?Hsentn seems lessaffected by n. Even though ?H , the reduction inentropy over complete sentences, is approximatedmore closely as suffix length grows, increasing nis strongly detrimental to the effect of ?Hsufn : Itis no longer significant for n > 2.
Presumably,this can be (partly) attributed to the impoverishedrelation between formal entropy and psychologi-cal uncertainty, as explained in Section 3.3.1.
Inany case, the effect of ?Hsentn is more stable.
Al-though ?Hsufn and ?Hsentn necessarily converge asn ?
?, the two effect sizes seem to diverge up to1 2 3 45101520effectsizesuffix length nH1surprisal?HnsentFigure 4: Effect size of entropy reduction(?Hsentn ), next-word entropy (H1), or surprisal,over and above the other two predictors.n = 3: The difference between the effect sizesof ?Hsentn and ?Hsufn is marginally significantly(p < .07) larger for n = 3 than for n = 2.4.2 Effects of other factorsIt is also of interest that surprisal has a significanteffect over and above entropy reduction, in the cor-rect (i.e., positive) direction.
When surprisal esti-mates are added to a regression model that alreadycontains ?Hsentn , the effect size ranges from 8.7for n = 1 to 13.9 for n = 4.
This show that thereexist independent effects of surprisal and entropyreduction on processing effort.Be reminded from Section 2.3 that Roark et al(2009) found a positive relation between readingtime on wt+1 and H1(t + 1), the next-word en-tropy after processing wt+1.
When that value isadded as a predictor in the regression model thatalready contains surprisal and entropy reduction?Hsentn , model fit greatly improves.
In fact, as canbe seen from comparing Figures 3 and 4, the ef-fect of ?Hsentn is strengthened by including next-word entropy in the regression model.
Moreover,each of the factors surprisal, entropy reduction,and next-word entropy has a significant effect overand above the other two.
In all cases, these ef-fects were in the positive direction.
This confirmsRoark et al?s finding and shows that it is in factcompatible with the entropy-reduction hypothesis,in contrast to what was suggested in Section 2.3.865 Discussion and conclusionThe current results contribute to a growing body ofevidence that the amount of information conveyedby a word in sentence context is indicative of theamount of cognitive effort required for processing,as can be observed from reading time on the word.Several previous studies have shown that surprisalcan serve as a cognitively relevant measure for aword?s information content.
In contrast, the rele-vance of entropy reduction as a cognitive measurehas not been investigated this thoroughly before.Hale (2003; 2006) presents entropy-reduction ac-counts of particular psycholinguistic phenomena,but does not show that entropy reduction gener-ally correlates with word-reading times.
Roark etal.
(2009) presented data that could be taken as ev-idence against the entropy-reduction hypothesis,but the current paper showed that the next-wordentropy effect, found by Roark et al, is indepen-dent of the entropy-reduction effect.It is tempting to take the independent effectsof surprisal and entropy reduction as evidencefor two distinct cognitive representations or pro-cesses, one related to surprisal, the other to en-tropy reduction.
However, it is very well possiblethat these two information measures are merelycomplementary formalizations of a single, cogni-tively relevant notion of word information.
Sincethe quantitative results presented here provide noevidence for either view, a more detailed qualita-tive analysis is needed.In addition, the relation between reading timeand the two measures of word information maybe further clarified by the development of mech-anistic sentence-processing models.
Both the sur-prisal and entropy-reduction theories provide onlyfunctional-level descriptions (Marr, 1982) of therelation between information content and process-ing effort, so the question remains which under-lying mechanism is responsible for longer read-ing times on words that convey more information.That is, we are still without a model that pro-poses, at Marr?s computational level, some spe-cific sentence-processing mechanism that takeslonger to process a word that has higher surprisalor leads to greater reduction in sentence entropy.For surprisal, Levy (2008) makes a first step inthat direction by presenting a mechanistic accountof why surprisal would predict word-reading time:If the state of the sentence-processing system isviewed as a probability distribution over all possi-ble interpretations of complete sentences, and pro-cessing a word comes down to updating this distri-bution to incorporate the new information, then theword?s surprisal equals the Kullback-Leibler di-vergence from the old distribution to the new.
Thisdivergence is presumed to quantify the amount ofwork (and, therefore, time) needed to update thedistribution.
Likewise, Smith and Levy (2008) ex-plain the surprisal effect in terms of a reader?s opti-mal preparation to incoming input.
When it comesto entropy reduction, however, no reading-timepredicting mechanism has been proposed.
Ideally,of course, there should be a single computational-level model that predicts the effects of both sur-prisal and entropy reduction.One recent model (Frank, 2010) shows that thereading-time effects of both surprisal and entropyreduction can indeed result from a single pro-cessing mechanism.
The model simulates sen-tence comprehension as the incremental and dy-namical update of a non-linguistic representationof the state-of-affairs described by the sentence.In this framework, surprisal and entropy reduc-tion are defined with respect to a probabilisticmodel of the world, rather than a model of thelanguage: The amount of information conveyedby a word depends on what is asserted by thesentence-so-far, and not on how the sentence?sform matches the statistical patterns of the lan-guage.
As it turns out, word-processing times inthe sentence-comprehension model correlate pos-itively with both surprisal and entropy reduction.The model thereby forms a computational-levelaccount of the relation between reading time andboth measures of word information.
Accordingto this account, the two information measures donot correspond to two distinct cognitive processes.Rather, there is one comprehension mechanismthat is responsible for the incremental revision ofa mental representation.
Surprisal and entropy re-duction form two complementary quantificationsof the extent of this revision.AcknowledgmentsThe research presented here was supported bygrant 277-70-006 of the Netherlands Organizationfor Scientific Research (NWO).
I would like tothank Rens Bod, Reut Tsarfaty, and two anony-mous reviewers for their helpful comments.87ReferencesM.
F. Boston, J. Hale, U. Patil, R. Kliegl, and S. Va-sishth.
2008.
Parsing costs as predictors of read-ing difficulty: An evaluation using the Potsdam Sen-tence Corpus.
Journal of Eye Movement Research,2:1?12.M.
H. Christiansen and M. C. MacDonald.
2009.
Ausage-based approach to recursion in sentence pro-cessing.
Language Learning, 59:129?164.V.
Demberg and F. Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109:193?210.B.
Efron and R. Tibshirani.
1986.
Bootstrap methodsfor standard errors, confidence intervals, and othermeasures of statistical accuracy.
Statistical Science,1:54?75.J.
L. Elman.
1990.
Finding structure in time.
Cogni-tive Science, 14:179?211.S.
L. Frank.
2009.
Surprisal-based comparison be-tween a symbolic and a connectionist model of sen-tence processing.
In N. A. Taatgen and H. van Rijn,editors, Proceedings of the 31st Annual Conferenceof the Cognitive Science Society, pages 1139?1144.Austin, TX: Cognitive Science Society.S.
L. Frank.
2010.
The role of world knowledge insentence comprehension: an information-theoreticanalysis and a connectionist simulation.
Manuscriptin preparation.J.
Hale.
2001.
A probabilistic Early parser as a psy-cholinguistic model.
In Proceedings of the sec-ond conference of the North American chapter ofthe Association for Computational Linguistics, vol-ume 2, pages 159?166.
Pittsburgh, PA: Associationfor Computational Linguistics.J.
Hale.
2003.
The information conveyed by words.Journal of Psycholinguistic Research, 32:101?123.J.
Hale.
2006.
Uncertainty about the rest of the sen-tence.
Cognitive Science, 30:643?672.A.
Kennedy and J. Pynte.
2005.
Parafoveal-on-fovealeffects in normal reading.
Vision Research, 45:153?168.R.
Levy.
2008.
Expectation-based syntactic compre-hension.
Cognition, 106:1126?1177.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: the Penn Treebank.
Computational Linguis-tics, 19:313?330.D.
Marr.
1982.
Vision.
San Francisco: W.H.
Freemanand Company.B.
Roark, A. Bachrach, C. Cardenas, and C. Pallier.2009.
Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling viaincremental top-down parsing.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 324?333.
Associ-ation for Computational Linguistics.N.
J. Smith and R. Levy.
2008.
Optimal processingtimes in reading: a formal model and empirical in-vestigation.
In B. C. Love, K. McRae, and V. M.Sloutsky, editors, Proceedings of the 30th AnnualConference of the Cognitive Science Society, pages595?600.
Austin, TX: Cognitive Science Society.88AppendixIt is of some interest that Hn can be expressed interms of H1 and the expected value of Hn?1.
First,note thath(wjt+1|wt1) = ?P (wjt+1|wt1) log P (wjt+1|wt1)= ?P (wt+1|wt1)P (wjt+2|wt+11 ) log(P (wt+1|wt1)P (wjt+2|wt+11 ))= P (wjt+2|wt+11 )h(wt+1|wt1) + P (wt+1|wt1)h(wjt+2|wt+11 ).For entropy Hn(t), this makesHn(t) =?wjt+1?Wnh(wjt+1|wt1)=?wjt+1?WnP (wjt+2|wt+11 )h(wt+1|wt1) +?wjt+1?WnP (wt+1|wt1)h(wjt+2|wt+11 )=?wt+1?W1??
?h(wt+1|wt1)?wjt+2?Wn?1P (wjt+2|wt+11 )???+?wt+1?W1??
?P (wt+1|wt1)?wjt+2?Wn?1h(wjt+2|wt+11 )??
?=?wt+1?W1h(wt+1|wt1) +?wt+1?W1P (wt+1|wt1)Hn?1(t + 1)= H1(t) + E(Hn?1(t + 1)).89
