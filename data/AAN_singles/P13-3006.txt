Proceedings of the ACL Student Research Workshop, pages 38?45,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsWhat causes a causal relation?Detecting Causal Triggers in Biomedical Scientific DiscourseClaudiu Miha?ila?
and Sophia AnaniadouThe National Centre for Text Mining,School of Computer Science,The University of Manchester,131 Princess Street, Manchester M1 7DN, UKclaudiu.mihaila@manchester.ac.uksophia.ananiadou@manchester.ac.ukAbstractCurrent domain-specific information extrac-tion systems represent an important resourcefor biomedical researchers, who need to pro-cess vaster amounts of knowledge in shorttimes.
Automatic discourse causality recog-nition can further improve their workload bysuggesting possible causal connections andaiding in the curation of pathway models.
Wehere describe an approach to the automaticidentification of discourse causality triggers inthe biomedical domain using machine learn-ing.
We create several baselines and experi-ment with various parameter settings for threealgorithms, i.e., Conditional Random Fields(CRF), Support Vector Machines (SVM) andRandom Forests (RF).
Also, we evaluate theimpact of lexical, syntactic and semantic fea-tures on each of the algorithms and look at er-rors.
The best performance of 79.35% F-scoreis achieved by CRFs when using all three fea-ture types.1 IntroductionThe need to provide automated, efficient and accu-rate means of retrieving and extracting user-orientedbiomedical knowledge has significantly increasedaccording to the ever-increasing amount of knowl-edge pusblished daily in the form of research ar-ticles (Ananiadou and McNaught, 2006; Cohenand Hunter, 2008).
Biomedical text mining hasseen significant recent advancements in recent years(Zweigenbaum et al 2007), including named en-tity recognition (Fukuda et al 1998), coreferenceresolution (Batista-Navarro and Ananiadou, 2011;Savova et al 2011) and relation (Miwa et al 2009;Pyysalo et al 2009) and event extraction (Miwaet al 2012b; Miwa et al 2012a).
Using biomed-ical text mining technology, text can now be en-riched via the addition of semantic metadata andthus can support tasks such as analysing molecu-lar pathways (Rzhetsky et al 2004) and semanticsearching (Miyao et al 2006).However, more complex tasks, such as questionanswering and automatic summarisation, require theextraction of information that spans across severalsentences, together with the recognition of relationsthat exist across sentence boundaries, in order toachieve high levels of performance.The notion of discourse can be defined as a co-herent sequence of clauses and sentences.
Theseare connected in a logical manner by discourse re-lations, such as causal, temporal and conditional,which characterise how facts in text are related.
Inturn, these help readers infer deeper, more com-plex knowledge about the facts mentioned in thediscourse.
These relations can be either explicitor implicit, depending whether or not they are ex-pressed in text using overt discourse connectives(also known as triggers).
Take, for instance, the casein example (1), where the trigger Therefore signalsa justification between the two sentences: because?a normal response to mild acid pH from PmrB re-quires both a periplasmic histidine and several glu-tamic acid residues?, the authors believe that the?regulation of PmrB activity could involve protona-tion of some amino acids?.
(1) In the case of PmrB, a normal response to mildacid pH requires not only a periplasmic histidine38but also several glutamic acid residues.Therefore, regulation of PmrB activity may in-volve protonation of one or more of these aminoacids.Thus, by identifying this causal relation, searchengines become able to discover relations betweenbiomedical entities and events or between experi-mental evidence and associated conclusions.
How-ever, phrases acting as causal triggers in certain con-texts may not denote causality in all cases.
There-fore, a dictionary-based approach is likely to pro-duce a very high number of false positives.
Inthis paper, we explore several supervised machine-learning approaches to the automatic identificationof triggers that actually denote causality.2 Related WorkA large amount of work related to discourse pars-ing and discourse relation identification exists in thegeneral domain, where researchers have not onlyidentified discourse connectives, but also developedend-to-end discourse parsers (Pitler and Nenkova,2009; Lin et al 2012).
Most work is based onthe Penn Discourse Treebank (PDTB) (Prasad et al2008), a corpus of lexically-grounded annotations ofdiscourse relations.Until now, comparatively little work has been car-ried out on causal discourse relations in the biomed-ical domain, although causal associations betweenbiological entities, events and processes are centralto most claims of interest (Kleinberg and Hripcsak,2011).
The equivalent of the PDTB for the biomed-ical domain is the BioDRB corpus (Prasad et al2011), containing 16 types of discourse relations,e.g., temporal, causal and conditional.
The numberof purely causal relations annotated in this corpus is542.
There are another 23 relations which are a mix-ture between causality and one of either background,temporal, conjunction or reinforcement relations.
Aslightly larger corpus is the BioCause (Miha?ila?
etal., 2013), containing over 850 manually annotatedcausal discourse relations in 19 full-text open-accessjournal articles from the infectious diseases domain.Using the BioDRB corpus as data, some re-searchers explored the identification of discourseconnectives (Ramesh et al 2012).
However, theydo not distinguish between the types of discourserelations.
They obtain the best F-score of 75.7% us-ing CRF, with SVM reaching only 65.7%.
Theseresults were obtained by using only syntactic fea-tures, as sematic features were shown to lower theperformance.
Also, they prove that there exist dif-ferences in discourse triggers between the biomedi-cal and general domains by training a model on theBioDRB and evaluating it against PDTB and vice-versa.3 MethodologyIn this section, we describe our data and the featuresof causal triggers.
We also explain our evaluationmethodology.3.1 DataThe data for the experiments comes from the Bio-Cause corpus.
BioCause is a collection of 19 open-access full-text journal articles pertaining to thebiomedical subdomain of infectious diseases, manu-ally annotated with causal relationships.
Two typesof spans of text are marked in the text, namely causaltriggers and causal arguments.
Each causal relationis composed of three text-bound annotations: a trig-ger, a cause or evidence argument and an effect argu-ment.
Some causal relations have implicit triggers,so these are excluded from the current research.Figure 1 shows an example of discourse causalityfrom BioCause, marking the causal trigger and thetwo arguments with their respective relation.
Namedentities are also marked in this example.BioCause contains 381 unique explicit triggers inthe corpus, each being used, on average, only 2.10times.
The number decreases to 347 unique triggerswhen they are lemmatised, corresponding to an av-erage usage of 2.30 times per trigger.
Both countsettings show the diversity of causality-triggeringphrases that are used in the biomedical domain.3.2 FeaturesThree types of features have been employed in thedevelopment of this causality trigger model, i.e., lex-ical, syntactic and semantic.
These features are cat-egorised and described below.3.2.1 Lexical featuresThe lexical features are built from the actual to-kens present in text.
Tokenisation is performed by39Figure 1: Causal relation in the BioCause.the GENIA tagger (Tsuruoka et al 2005) using thebiomedical model.
The first two features representthe token?s surface expression and its base form.Neighbouring tokens have also been considered.We included the token immediately to the left andthe one immediately to the right of the current to-ken.
This decision is based on two observations.Firstly, in the case of tokens to the left, most trig-gers are found either at the beginning of the sentence(311 instances) or are preceded by a comma (238 in-stances).
These two left contexts represent 69% ofall triggers.
Secondly, for the tokens to the right, al-most 45% of triggers are followed by a determiner,such as the, a or an, (281 instances) or a comma (71instances).3.2.2 Syntactic featuresThe syntax, dependency and predicate argumentstructure are produced by the Enju parser (Miyaoand Tsujii, 2008).
Figure 2 depicts a partial lexicalparse tree of a sentence which starts with a causaltrigger, namely Our results suggest that.
From thelexical parse trees, several types of features havebeen generated.The first two features represent the part-of-speechand syntactic category of a token.
For instance,the figure shows that the token that has the part-of-speech IN.
These features are included due to thefact that either many triggers are lexicalised as anadverb or conjunction, or are part of a verb phrase.For the same reason, the syntactical category pathfrom the root of the lexical parse tree to the token isalso included.
The path also encodes, for each par-ent constituent, the position of the token in its sub-tree, i.e., beginning (B), inside (I) or end (E); if thetoken is the only leaf node of the constituent, this ismarked differently, using a C. Thus, the path of that,highlighted in the figure, is I-S/I-VP/B-CP/C-CX.Secondly, for each token, we extracted the pred-Figure 2: Partial lexical parse tree of a sentence startingwith a causal trigger.icate argument structure and checked whether a re-lation exista between the token and the previous andfollowing tokens.
The values for this feature repre-sent the argument number as allocated by Enju.Thirdly, the ancestors of each token to the thirddegree are instantiated as three different features.
Inthe case that such ancestors do not exist (i.e., theroot of the lexical parse tree is less than three nodesaway), a ?none?
value is given.
For instance, thetoken that in Figure 2 has as its first three ancestorsthe constituents marked with CX, CP and VP.Finally, the lowest common ancestor in the lexi-cal parse tree between the current token and its leftneighbour has been included.
In the example, thelowest common ancestor for that and suggest is VP.These last two feature types have been producedon the observation that the lowest common ancestorfor all tokens in a causal trigger is S or VP in over70% of instances.
Furthermore, the percentage ofcases of triggers with V or ADV as lowest commonancestor is almost 9% in each case.
Also, the aver-40age distance to the lowest common ancestor is 3.3.2.3 Semantic featuresWe have exploited several semantic knowledgesources to identify causal triggers more accurately,as a mapping to concepts and named entities acts asa back-off smoothing, thus increasing performance.One semantic knowledge source is the BioCausecorpus itself.
All documents annotated for causal-ity in BioCause had been previously manually an-notated with biomedical named entity and event in-formation.
This was performed in the context of var-ious shared tasks, such as the BioNLP 2011 SharedTask on Infectious Diseases (Pyysalo et al 2011).We therefore leverage this existing information toadd another semantic layer to the model.
More-over, another advantage of having a gold standardannotation is the fact that it is now possible to sepa-rate the task of automatic causal trigger recognitionfrom automatic named entity recognition and eventextraction.
The named entity and event annotationin the BioCause corpus is used to extract informa-tion about whether a token is part of a named entityor event trigger.
Furthermore, the type of the namedentity or event is included as a separate feature.The second semantic knowledge source is Word-Net (Fellbaum, 1998).
Using this resource, the hy-pernym of every token in the text has been includedas a feature.
Only the first sense of every token hasbeen considered, as no sense disambiguation tech-nique has been employed.Finally, tokens have been linked to the UnifiedMedical Language System (UMLS) (Bodenreider,2004) semantic types.
Thus, we included a featureto say whether a token is part of a UMLS type andanother for its semantic type if the previous is true.3.3 Experimental setupWe explored with various machine learning algo-rithms and various settings for the task of identifyingcausal triggers.On the one hand, we experimented with CRF(Lafferty et al 2001), a probabilistic modellingframework commonly used for sequence labellingtasks.
In this work, we employed the CRFSuite im-plementation1.1http://www.chokkan.org/software/crfsuiteOn the other hand, we modelled trigger detectionas a classification task, using Support Vector Ma-chines and Random Forests.
More specifically, weemployed the implementation in Weka (Hall et al2009; Witten and Frank, 2005) for RFs, and Lib-SVM (Chang and Lin, 2011) for SVMs.4 Results and discussionSeveral models have been developed and 10-foldcross-evaluated to examine the complexity of thetask, the impact of various feature types (lexical,syntactic, semantic).
Table 1 shows the performanceevaluation of baseline systems and other classifiers.These are described in the following subsections.
Itshould be noted that the dataset is highly skewed,with a ratio of positive examples to negative exam-ples of approximately 1:52.Classifier P R F1Baseline Dict 8.36 100 15.43Dep 7.51 76.66 13.68Dict+Dep 14.30 75.33 24.032-way CRF 89.29 73.53 79.35SVM 81.62 61.05 69.85RandFor 78.16 66.96 72.133-way CRF 89.13 64.04 72.87SVM 74.21 56.82 64.36RandFor 73.80 60.95 66.76Table 1: Performance of various classifiers in identifyingcausal connectives4.1 BaselineSeveral baselines have been devised.
The first base-line is a dictionary-based heuristic, named Dict.
Alexicon is populated with all annotated causal trig-gers and then this is used to tag all instances of itsentries in the text as connectives.
The precision ofthis heuristic is very low, 8.36%, which leads to anF-score of 15.43%, considering the recall is 100%.This is mainly due to triggers which are rarely usedas causal triggers, such as and, by and that.Building on the previously mentioned observationabout the lowest common ancestor for all tokens in acausal trigger, we built a baseline system that checksall constituent nodes in the lexical parse tree for theS, V, VP and ADV tags and marks them as causal41triggers.
The name of this system is Dep.
Not onlydoes Dep obtain a lower precision than Dict, but italso performs worse in terms of recall.
The F-scoreis 13.68%, largely due to the high number of inter-mediate nodes in the lexical parse tree that have VPas their category.The third baseline is a combination of Dict andDep: we consider only constituents that have thenecessary category (S, V, VP or ADV) and includea trigger from the dictionary.
Although the recalldecreases slightly, the precision increases to almosttwice that of both Dict and Dep.
This produces amuch better F-score of 24.03%.4.2 Sequence labelling taskAs a sequence labelling task, we have modelledcausal trigger detection as two separate tasks.Firstly, each trigger is represented in the B-I-O for-mat (further mentioned as the 3-way model).
Thus,the first word of every trigger is tagged as B (be-gin), whilst the following words in the trigger aretagged as I (inside).
Non-trigger words are taggedas O (outside).The second model is a simpler version of the pre-vious one: it does not distinguish between the firstand the following words in the trigger.
In otherwords, each word is tagged either as being part ofor outside the trigger, further known as the 2-waymodel.
Hence, a sequence of contiguous tokensmarked as part of a trigger form one trigger.CRF performs reasonably well in detecting causaltriggers.
In the 3-way model, it obtains an F-score ofalmost 73%, much better than the other algorithms.It also obtains the highest precision (89%) and recall(64%).
However, in the 2-way model, CRF?s perfor-mance is slightly lower than that of Random Forests,achieving only 79.35%.
Its precision, on the otherhand, is the highest in this model.
The results fromboth models were obtained by combining featuresfrom all three feature categories.Table 2 show the effect of feature types on bothmodels of CRFs.
As can be observed, the best per-formances, in terms of F-score, including the previ-ously mentioned ones, are obtained when combin-ing all three types of features, i.e., lexical, syntacticand semantic.
The best precision and recall, how-ever, are not necessarily achieved by using all threefeature types.
In the two-way model, the best preci-Features P R F12-wayLex 88.99 67.09 73.59Syn 92.20 68.68 75.72Sem 87.20 63.30 69.36Lex-Syn 87.76 73.29 78.73Lex+Sem 89.54 69.10 75.61Syn+Sem 87.48 72.62 78.13Lex-Syn-Sem 89.29 73.53 79.353-wayLex 85.87 56.34 65.18Syn 87.62 61.44 70.22Sem 80.78 51.43 59.39Lex+Syn 87.80 63.04 72.59Lex+Sem 85.50 58.11 66.80Syn+Sem 84.83 64.94 72.41Lex-Syn-Sem 89.13 64.04 72.87Table 2: Effect of feature types on the sequence labellingtask, given in percentages.sion is obtained by using the syntactic features only,reaching over 92%, almost 3% higher than when allthree feature types are used.
In the three-way model,syntactic and semantic features produce the best re-call (almost 65%), which is just under 1% higherthan the recall when all features are used.4.3 Classification taskAs a classification task, an algorithm has to decidewhether a token is part of a trigger or not, similarlyto the previous two-way subtask in the case of CRF.Firstly, we have used RF for the classificationtask.
Various parameter settings regarding the num-ber of constructed trees and the number of randomfeatures have been explored.The effect of feature types on the performance ofRF is shown in Table 3.
As can be observed, thebest performance is obtained when combining lexi-cal and semantic features.
Due to the fact that causaltriggers do not have a semantic mapping to conceptsin the named entity and UMLS annotations, the treesin the random forest classifier can easily producerules that distinguish triggers from non-triggers.
Assuch, the use of semantic features alone produce avery good precision of 84.34%.
Also, in all caseswhere semantic features are combined with otherfeature types, the precision increases by 0.5% in thecase of lexical features and 3.5% in the case of syn-tactic features.
However, the recall of semantic fea-42tures alone is the lowest.
The best recall is obtainedwhen using only lexical features.Features P R F1Lex 78.47 68.30 73.03Syn 68.19 62.36 65.15Sem 84.34 56.83 67.91Lex+Syn 77.11 65.92 71.09Lex+Sem 79.10 67.91 73.08Syn+Sem 71.83 64.45 67.94Lex+Syn+Sem 77.98 67.31 72.25Table 3: Effect of feature types on Random Forests.Secondly, we explored the performance of SVMsin detecting causal triggers.
We have experimentedwith two kernels, namely polynomial (second de-gree) and radial basis function (RBF) kernels.
Foreach of these two kernels, we have evaluated vari-ous combinations of parameter values for cost andweight.
Both these kernels achieved similar results,indicating that the feature space is not linearly sepa-rable and that the problem is highly complex.The effect of feature types on the performance ofSVMs is shown in Table 4.
As can be observed,the best performance is obtained when combiningthe lexical and semantic feature types (69.85% F-score).
The combination of all features produces thebest precision, whilst the best recall is obtained bycombining lexical and semantic features.Features P R F1Lex 80.80 60.94 69.47Syn 82.94 55.60 66.57Sem 85.07 56.51 67.91Lex+Syn 86.49 53.63 66.81Lex+Sem 81.62 61.05 69.85Syn+Sem 84.49 55.31 66.85Lex+Syn+Sem 87.70 53.96 66.81Table 4: Effect of feature types on SVM.4.4 Error analysisAs we expected, the majority of errors arise from se-quences of tokens which are only used infrequentlyas non-causal triggers.
This applies to 107 triggertypes, whose number of false positives (FP) is higherthan the number of true positives (TP).
In fact, 64trigger types occur only once as a causal instance,whilst the average number of FPs for these types is14.25.
One such example is and, for which the num-ber of non-causal instances (2305) is much greaterthan that of causal instances (1).
Other examplesof trigger types more commonly used as causal trig-gers, are suggesting (9 TP, 54 FP), indicating (8 TP,41 FP) and resulting in (6 TP, 14 FP).
For instance,example (2) contains two mentions of indicating, butneither of them implies causality.
(2) Buffer treated control cells showed intensegreen staining with syto9 (indicating viabil-ity) and a lack of PI staining (indicating nodead/dying cells or DNA release).5 Conclusions and Future WorkWe have presented an approach to the automaticidentification of triggers of causal discourse rela-tions in biomedical scientific text.
The task hasproven to be a highly complex one, posing manychallenges.
Shallow approaches, such as dictionarymatching and lexical parse tree matching, performvery poorly, due to the high ambiguity of causaltriggers (with F-scores of approximately 15% eachand 24% when combined).
We have explored vari-ous machine learning algorithms that automaticallyclassify tokens into triggers or non-triggers and wehave evaluated the impact of multiple lexical, syn-tactic and semantic features.
The performance ofSVMs prove that the task of identifying causal trig-gers is indeed complex.
The best performing classi-fier is CRF-based and combines lexical, syntacticaland semantical features in order to obtain an F-scoreof 79.35%.As future work, integrating the causal relations inthe BioDRB corpus is necessary to check whether adata insufficiency problem exists and, if so, estimatethe optimal amount of necessary data.
Furthermore,evaluations against the general domain need to beperformed, in order to establish any differences inexpressing causality in the biomedical domain.
Onepossible source for this is the PDTB corpus.
A moredifficult task that needs attention is that of identify-ing implicit triggers.
Finally, our system needs to beextended in order to identify the two arguments of43causal relations, the cause and effect, thus allowingthe creation of a complete discourse causality parser.AcknowledgementsThis work was partially funded by the Engineer-ing and Physical Sciences Research Council [grantnumber EP/P505631/1].ReferencesSophia Ananiadou and John McNaught, editors.
2006.Text Mining for Biology And Biomedicine.
ArtechHouse, Inc.Riza Theresa B. Batista-Navarro and Sophia Anani-adou.
2011.
Building a coreference-annotated corpusfrom the domain of biochemistry.
In Proceedings ofBioNLP 2011, pages 83?91.Olivier Bodenreider.
2004.
The unified medical lan-guage system (UMLS): integrating biomedical termi-nology.
Nucleic Acids Research, 32(suppl 1):D267?D270.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Kevin Bretonnel Cohen and Lawrence Hunter.
2008.Getting started in text mining.
PLoS ComputationalBiology, 4(1):e20, 01.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Ken-ichiro Fukuda, Tatsuhiko Tsunoda, Ayuchi Tamura,and Toshihisa Takagi.
1998.
Toward information ex-traction: Identifying protein names from biological pa-pers.
In Proceedings of the Pacific Symposium on Bio-computing, volume 707, pages 707?718.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11:10?18, November.Samantha Kleinberg and George Hripcsak.
2011.
A re-view of causal inference for biomedical informatics.Journal of Biomedical Informatics, 44(6):1102 ?
1112.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012.
Apdtb-styled end-to-end discourse parser.
Natural Lan-guage Engineering, FirstView:1?34, 10.Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, andSophia Ananiadou.
2013.
BioCause: Annotating andanalysing causality in the biomedical domain.
BMCBioinformatics, 14(1):2, January.Makoto Miwa, Rune S?tre, Yusuke Miyao, and Jun?ichiTsujii.
2009.
Protein-protein interaction extraction byleveraging multiple kernels and parsers.
InternationalJournal of Medical Informatics, 78(12):e39?e46, June.Makoto Miwa, Paul Thompson, and Sophia Ananiadou.2012a.
Boosting automatic event extraction from theliterature using domain adaptation and coreferenceresolution.
Bioinformatics, 28(13):1759?1765.Makoto Miwa, Paul Thompson, John McNaught, Dou-glas B. Kell, and Sophia Ananiadou.
2012b.
Extract-ing semantically enriched events from biomedical lit-erature.
BMC Bioinformatics, 13:108.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Computa-tional Linguistics, 34(1):3580, March.Yusuke Miyao, Tomoko Ohta, Katsuya Masuda, Yoshi-masa Tsuruoka, Kazuhiro Yoshida, Takashi Ninomiya,and Jun?ichi Tsujii.
2006.
Semantic retrieval for theaccurate identification of relational concepts in mas-sive textbases.
In ACL.Emily Pitler and Ani Nenkova.
2009.
Using syntax todisambiguate explicit discourse connectives in text.
InACL/AFNLP (Short Papers), pages 13?16.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse TreeBank 2.0.In Nicoletta Calzolari, Khalid Choukri, Bente Mae-gaard, Joseph Mariani, Jan Odjik, Stelios Piperidis,and Daniel Tapias, editors, In Proceedings of the 6thInternational Conference on language Resources andEvaluation (LREC), pages 2961?2968.Rashmi Prasad, Susan McRoy, Nadya Frid, AravindJoshi, and Hong Yu.
2011.
The biomedical discourserelation bank.
BMC Bioinformatics, 12(1):188.Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, andJun?ichi Tsujii.
2009.
Static relations: a piece in thebiomedical information extraction puzzle.
In Proceed-ings of the Workshop on Current Trends in BiomedicalNatural Language Processing, BioNLP ?09, pages 1?9, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-livan, Chunhong Mao, Chunxia Wang, Bruno So-bral, Jun?ichi Tsujii, and Sophia Ananiadou.
2011.Overview of the infectious diseases (ID) task ofBioNLP shared task 2011.
In Proceedings of theBioNLP Shared Task 2011 Workshop, pages 26?35,Portland, Oregon, USA, June.
Association for Com-putational Linguistics.44Polepalli Balaji Ramesh, Rashmi Prasad, Tim Miller,Brian Harrington, and Hong Yu.
2012.
Automatic dis-course connective detection in biomedical text.
Jour-nal of the American Medical Informatics Association.Andrey Rzhetsky, Ivan Iossifov, Tomohiro Koike,Michael Krauthammer, Pauline Kra, Mitzi Morris,Hong Yu, Ariel Pablo Duboue?, Wubin Weng, W.JohnWilbur, Vasileios Hatzivassiloglou, and Carol Fried-man.
2004.
Geneways: a system for extracting, ana-lyzing, visualizing, and integrating molecular pathwaydata.
Journal of Biomedical Informatics, 37(1):43 ?53.Guergana K Savova, Wendy W Chapman, Jiaping Zheng,and Rebecca S Crowley.
2011.
Anaphoric rela-tions in the clinical narrative: corpus creation.
Jour-nal of the American Medical Informatics Association,18(4):459?465.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advancesin Informatics - 10th Panhellenic Conference on In-formatics, volume 3746 of LNCS, pages 382?392.Springer-Verlag, Volos, Greece, November.Ian Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques (SecondEdition).
Morgan Kaufmann.Pierre Zweigenbaum, Dina Demner-Fushman, Hong Yu,and Kevin B. Cohen.
2007.
Frontiers of biomedicaltext mining: current progress.
Briefings in Bioinfor-matics, 8(5):358?375.45
