Proceedings of the 12th Conference of the European Chapter of the ACL, pages 469?477,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLattice Parsing to Integrate Speech Recognition and Rule-Based MachineTranslationSel?uk K?pr?AppTek, Inc.METU TechnopolisAnkara, Turkeyskopru@apptek.comAdnan Yaz?c?Department of Computer EngineeringMiddle East Technical UniversityAnkara, Turkeyyazici@metu.edu.trAbstractIn this paper, we present a novel approachto integrate speech recognition and rule-based machine translation by lattice pars-ing.
The presented approach is hybridin two senses.
First, it combines struc-tural and statistical methods for languagemodeling task.
Second, it employs achart parser which utilizes manually cre-ated syntax rules in addition to scores ob-tained after statistical processing duringspeech recognition.
The employed chartparser is a unification-based active chartparser.
It can parse word graphs by using amixed strategy instead of being bottom-upor top-down only.
The results are reportedbased on word error rate on the NISTHUB-1 word-lattices.
The presented ap-proach is implemented and compared withother syntactic language modeling tech-niques.1 IntroductionThe integration of speech and language technolo-gies plays an important role in speech to texttranslation.
This paper describes a unification-based active chart parser and how it is utilizedfor language modeling in speech recognition orspeech translation.
The fundamental idea behindthe proposed solution is to combine the strengthsof unification-based chart parsing and statisticallanguage modeling.
In the solution, all sentencehypotheses, which are represented in word-latticeformat at the end of automatic speech recognition(ASR), are parsed simultaneously.
The chart isinitialized with the lattice and it is processed un-til the first sentence hypothesis is selected by theparser.
The parser also utilizes the scores assignedto words during the speech recognition process.This leads to a hybrid solution.An important benefit of this approach is that itallows one to make use of the available grammarsand parsers for language modeling task.
So as tobe used for this task, syntactic analyzer compo-nents developed for a rule-based machine trans-lation (RBMT) system are modified.
In speechtranslation (ST), this approach leads to a perfectintegration of the ASR and RBMT components.Language modeling effort in ASR and syntac-tic analysis effort in RBMT are overlapped andmerged into a single task.
Its advantages aretwofold.
First, this allows us to avoid unnecessaryduplication of similar jobs.
Secondly, by using theavailable components, we avoid the difficulty ofbuilding a syntactic language model all from thebeginning.There are two basic methods that are beingused to integrate ASR and rule-based MT systems:First-best method and the N-best list method.
Bothtechniques are motivated from a software engi-neering perspective.
In the First-best approach(Figure 1.a), the ASR module sends a single rec-ognized text to the MT component to translate.Any ambiguity existing in the recognition processis resolved inside the ASR.
In contrast to the First-best approach, in the N-best List approach (Figure1.b); the ASR outputs N possible recognition hy-potheses to be evaluated by the MT component.The MT picks the first hypothesis and translates itif it is grammatically correct.
Otherwise, it movesto the second hypothesis and so on.
If none of theavailable hypotheses are syntactically correct, thenit translates the first one.We propose a new method to couple ASR andrule-based MT system as an alternative to the ap-469proaches mentioned above.
Figure 1 representsthe two currently in-use coupling methods fol-lowed by the new approach we introduce (Fig-ure 1.c).
In the newly proposed technique, whichwe call the N-best word graph approach, the ASRmodule outputs a word graph containing all N-besthypotheses.
The MT component parses the wordgraph, thus, all possible hypotheses at one time.c)SpeechSpeechRecognizerRecognizerSpeechRecognizerRule?basedMTRule?basedRule?basedMTMTTarget TextTarget TextTarget TextRecognized Text1.
Recognized TextN.
Recognized Text...a)b)Figure 1: ASR and rule-based MT coupling: a)First-best b) N-best list c) N-best word graph.While integrating the SR system with the rule-based MT system, this study uses word graphsand chart parsing with new extensions.
Parsing ofword lattices has been a topic of research over thepast decade.
The idea of chart parsing the wordgraph in SR systems has been previously usedin different studies in order to resolve ambigu-ity.
Tomita (1986) introduced the concept of word-lattice parsing for the purpose of speech recogni-tion and used an LR parser.
Next, Paeseler (1988)used a chart parser to process word-lattices.
How-ever, to the best of our knowledge, the specificmethod for chart parsing a word graph introducedin this paper has not been previously used for cou-pling purposes.Recent studies point out the importance of uti-lizing word graphs in speech tasks (Dyer et al,2008).
Previous work on language modeling canbe classified according to whether a system usespurely statistical methods or whether it uses themin combination with syntactic methods.
In this pa-per, the focus is on systems that contain syntacticapproaches.
In general, these language modelingapproaches try to parse the ASR output in word-lattice format in order to choose the most prob-able hypothesis.
Chow and Roukos (1989) useda unification-based CYK parser for the purpose ofspeech understanding.
Chien et al (1990) and We-ber (1994) utilized probabilistic context free gram-mars in conjunction with unification grammars tochart-parse a word-lattice.
There are various dif-ferences between the work of Chien et al (1990)and Weber (1994) and the work presented in thispaper.
First, in the previously mentioned studies,the chart is populated with the same word graphthat comes from the speech recognizer without anypruning, whereas in our approach the word graphis reduced to an acceptable size.
Otherwise, theefficiency becomes a big challenge because thesearch space introduced by a chart with over thou-sands of initial edges can easily be beyond currentpractical limits.
Another important difference inour approach is the modification of the chart pars-ing algorithm to eliminate spurious parses.Ney (1991) deals with the use of probabilis-tic CYK parser for continous speech recognitiontask.
Stolcke (1995) summarizes extensively theirapproach to utilize probabilistic Earley parsing.Chappelier et al (1999) gives an overview of dif-ferent approaches to integrate linguistic modelsinto speech recognition systems.
They also re-search various techniques of producing sets of hy-potheses that contain more ?semantic?
variabil-ity than the commonly used ones.
Some of therecent studies about structural language model-ing extract a list of N-best hypotheses using anN-gram and then apply structural methods to de-cide on the best hypothesis (Chelba, 2000; Roark,2001).
This contrasts with the approach presentedin this study where, instead of a single sentence,the word-lattice is parsed.
Parsing all sentence hy-potheses simultaneously enables a reduction in thenumber of edges produced during the parsing pro-cess.
This is because the shared word hypothe-ses are processed only once compared to the N-best list approach, where the shared words are pro-cessed each time they occur in a hypothesis.
Sim-ilar to the current work, other studies parse thewhole word-lattice without extracting a list (Hall,2005).
A significant distinction between the workof Hall (2005) and our study is the parsing algo-rithm.
In contrast to our chart parsing approachaugmented by unification based feature structures,Charniak parser is used in Hall (2005)?s along withPCFG.The rest of the paper is organized as follows:In the following section, an overview of the pro-posed language model is presented.
Next, in Sec-tion 3, the parsing process of the word-lattice isdescribed in detail.
Section 4 describes the exper-470iments and reports the obtained results.
Finally,Section 5 concludes the paper.2 Hybrid language modelingThe general architecture of the system is depictedin Figure 2.
The HTK toolkit (Woodland, 2000)word-lattice file format is used as the default fileformat in the proposed solution.
The word-latticeoutput from ASR is converted into a finite statemachine (FSM).
This conversion enables us tobenefit from standard theory and algorithms onFSMs.
In the converted FSM, non-determinism isremoved and it is minimized by eliminating redun-dant nodes and arcs.
Next, the chart is initializedwith the deterministic and minimal FSM.
Finally,this chart is used in the structural analysis.Selected HypothesisASRMorphological AnalysisFSM ConversionMinimizationInitializationChart ParsingWord GraphFSMMinimized FSMInitial ChartChart w/ feature structuresLexiconMorphology RulesSyntax RulesSpeechFigure 2: The hybrid language model architecture.Structural analysis of the word-lattice is accom-plished in two consecutive tasks.
First, morpho-logical analysis is performed on the word level andany information carried by the word is extractedto be used in the following stages.
Second, syn-tactic analysis is performed on the sentence level.The syntactic analyzer consists of a chart parser inwhich the rules modeling the language grammarare augmented with functional expressions.3 Word Graph ProcessingThe word graphs produced by an ASR are beyondthe limits of a unification-based chart parser.
Asmall-sized lattice from the NIST HUB-1 data set(Pallett et al, 1994) can easily contain a couple ofhundred states and more than one thousand arcs.The largest lattice in the same data set has 25 000states and almost 1 million arcs.
No unification-based chart parser is capable of coping with an in-put of this size.
It is unpractical and unreasonableto parse the lattice in the same form as it is outputfrom the ASR.
Instead, the word graph is prunedto a reasonable size so that it can be parsed accord-ing to acceptable time and memory limitations.3.1 Word graph to FSM conversionThe pruning process starts by converting the time-state lattice to a finite state machine.
This way,algorithms and data structures for FSMs are uti-lized in the following processing steps.
Each wordin the time-state lattice corresponds to a state nodein the new FSM.
The time slot information is alsodropped in the recently built automata.
The linksbetween the words in the lattice are mapped as theFSM arcs.In the original representation, the word labelsin the time-state lattices are on the nodes, and theacoustic scores and the statistical language modelscores are on the arcs.
Similarly, the words arealso on the nodes.
This representation does not fitinto the chart definition where the words are onthe arcs.
Therefore, the FSM is converted to anarc labeled FSM.
The conversion is accomplishedby moving back the word label on a state to theincoming arcs.
The weights on the arcs representthe negative logarithms of probabilities.
In orderto find the weight of a path in the FSM, all weightson the arcs existing on that path are added up.The resulting FSM contains redundant arcs thatare inherited from the word graph.
Many arcs cor-respond to the same word with a different score.The FSM is nondeterministic because, at a givenstate, there are different alternative arcs with thesame word label.
Before parsing the convertedFSM, it is essential to find an equivalent finite au-tomata that is deterministic and that has as fewnodes as possible.
This way, the work necessaryduring parsing is reduced and efficient processingis ensured.The minimization process serves to shrink downthe FSM to an equivalent automata with a suitablesize for parsing.
However, it is usually the casethat the size is not small enough to meet the timeand memory limitations in parsing.
N-best list se-lection can be regarded as the last step in constrict-ing the size.
A subset of possible hypotheses is se-lected among many that are contained in the mini-471mized FSM.
The selection mechanism favors onlythe best hypotheses according to the scores presentin the FSM arcs.3.2 Chart parsingThe parsing engine implemented for this work isan active chart parser similar to the one describedin Kay (1986).
The language grammar that is pro-cessed by the parser can be designed top-down,bottom-up or in a combined manner.
It employsan agenda to store the edges prior to inserting tothe chart.
Edges are defined to be either completeor incomplete.
Incomplete edges describe the rulestate where one or more syntactic categories areexpected to be matched.
An incomplete edge be-comes complete if all syntactic categories on theright-hand-side of the rule are matched.Parsing starts from the rules that are associ-ated to the lexical entries.
This corresponds tothe bottom-up parsing strategy.
Moreover, pars-ing also starts from the rules that build the finalsymbol in the grammar.
This corresponds to thetop-down parsing strategy.
Bottom-up rules andtop-down rules differ in that the former containsa non-terminal that is marked as the trigger orcentral element on the left-hand-side of the rule.This central element is the starting point for theexecution of the bottom-up rule.
After the cen-tral element is matched, the extension continuesin a bidirectional manner to complete the missingconstituents.
Bottom-up incomplete edges are de-scribed with double-dotted rules to keep track ofthe beginning and end of the matched fragment.The anticipated edges are first inserted into theagenda.
Edges popped out from the agenda areprocessed with the fundamental rule of chart pars-ing.
The agenda allows the reorganization of theedge processing order.
After the application of thefundamental rule, new edges are predicted accord-ing to either bottom-up or top-down parsing strat-egy.
This strategy is determined by how the cur-rent edge has been created.3.3 Chart initializationThe chart initialization procedure creates from aninput FSM, which is derived from the ASR wordlattice, a valid chart that can be parsed in an activechart parser.
The initialization starts with fillingin the distance value for each node.
The distanceof a node in the FSM is defined as the number ofnodes on the longest path from the start state tothe current state.
After the distance value is setfor all nodes in the FSM, an edge is created foreach arc.
The edge structure contains the start andend values in addition to the weight and label datafields.
These position values represent the edgelocation relative to the beginning of the chart.
Thestarting and ending node information for the arc isalso copied to the edge.
This node information islater utilized in chart parsing to eliminate spuriousparses.
The number of edges in the chart equals tothe number of edges in the input FSM at the endof initialization.Consider the simple FSM F1 depicted in Fig-ure 3, the corresponding two-dimensional chartand the related hypotheses.
The chart is populatedwith the converted word graph before parsing be-gins.
Words in the same column can be regardedas a single lexical entry with different senses (e.g.,?boy?
and ?boycott?
in column 2).
Words span-ning more than one column can be regarded as id-iomatic entries (e.g.
?escalated?
from column 3to 5).
Merged cells in the chart (e.g., ?the?
and?yesterday?
at columns 1 and 6, respectively) areshared in both sentence hypotheses.F1:0 1the2boycott3escalated4yesterday5boy 6goes 7to schoolChart:0 1 2 3 4 5 60 the 11 boy 5 5 goes 6 6 to 7 7 school 33 yesterday 41 boycott 2 2 escalated 3Hypotheses:?
The boy goes to school yesterday?
The boycott escalated yesterdayFigure 3: Sample FSM F4, the correspondingchart and the hypotheses.3.4 Extended Chart ParsingIn a standard active chart parser, the chart depictedin Figure 3 could produce some spurious parses.For example, both of the complete edges in the ini-tial chart at location [1-2] (i.e.
?boy?
and ?boycott)can be combined with the word ?goes?, although?boycott goes?
is not allowed in the original wordgraph.
We have eliminated these kinds of spuri-472ous parses by making use of the arcstart and ar-cfinish values.
These labels indicate the startingand ending node identifiers of the path spanned bythe edge in subject.
The application of this ideais illustrated in Figure 4.
Different from the orig-inal implementation of the fundamental rule, theprocedure has the additional parameters to definestarting and ending node identifiers.
Before creat-ing a new incomplete edge, it is checked whetherthe node identifiers match or not.When we consider the chart given in Figure 3,?1 boycott 2?
and ?5 goes 6?
cannot be combined ac-cording to the new fundamental rule in a parsetree because the ending node id, i.e.
2, of the for-mer does not match the starting node id, i.e.
5,of the latter.
In another example, ?0 the 1?
can becombined with both ?1 boy 5?
and ?1 boycott 2?
be-cause their respective node identifiers match.
Af-ter the two edges, ?boycott?
and ?escalated?, arecombined and a new edge is generated, the start-ing node identifiers for the entire edge will be asin ?1 boycott escalated 3?.The utilization of the node identifiers enablesthe two-dimensional modeling of a word graph ina chart.
This extension to chart parsing makesthe current approach word-graph based rather thanconfusion-network based.
Parse trees that con-flict with the input word graph are blocked and allthe processing resources are dedicated to properedges.
The chart parsing algorithm is listed in Fig-ure 4.3.5 Unification-based chart parsingThe grammar rules are implemented using LexicalFunctional Grammar (LFG) paradigm.
The pri-mary data structure to represent the features andvalues is a directed acyclic graph (dag).
The sys-tem also includes an expressive Boolean formal-ism, used to represent functional equations to ac-cess, inspect or modify features or feature sets inthe dag.
Complex feature structures (e.g.
lists,sets, strings, and conglomerate lists) can be asso-ciated with lexical entries and grammatical cate-gories using inheritance operations.
Unification isused as the fundamental mechanism to integrateinformation from lexical entries into larger gram-matical constituents.The constituent structure (c-structure) repre-sents the composition of syntactic constituents fora phrase.
It is the term used for parse tree inLFG.
The functional structure (f-structure) is theinput : grammar , word?graphoutput : c h a r ta lgor i thm CHART?PARSE ( grammar , word?graph )I N I T I A L I Z E ( c h a r t , agenda , word?graph )whi le agenda i s not emptyedge ?
POP ( agenda )PROCESS?EDGE ( edge )end whi l eend algor i thmprocedure PROCESS?EDGE (A ?
B ?
?
?
C, [j, k], [ns, ne] )PUSH ( c h a r t , A ?
B ?
?
?
C, [j, k], [ns, ne] )FUNDAMENTAL?RULE (A ?
B ?
?
?
C, [j, k], [ns, ne] )PR ED I CT (A ?
B ?
?
?
C, [j, k], [ns, ne] )end procedureprocedure FUNDAMENTAL?RULE (A ?
B ?
?
?
C, [j, k], [ns, ne] )i f B = ?D / / edge i s i n c o m p l e t ef o r each (D ?
??
?, [i, j], [nr, ns] ) in c h a r tPUSH ( agenda , (A ?
?
?
D?
?
C, [i, k], [nr, ne] ) )end f o rend i fi f C = D?
/ / edge i s i n c o m p l e t ef o r each (D ?
??
?, [k, l], [ne, nf ] ) in c h a r tPUSH ( agenda , (A ?
B ?
?D?
?, [j, l], [ns, nf ] ) )end f o rend i fi f B i s n u l l and C i s n u l l / / edge i s c o m p l e t ef o r each (D ?
?A ?
?
?
?, [k, l], [ne, nf ] ) in c h a r tPUSH ( agenda , (D ?
?
?
A?
?
?, [j, l], [ns, nf ] ) )end f o rf o r each (D ?
?
?
?
?
A?, [i, j], [nr, ns] ) in c h a r tPUSH ( agenda , (D ?
?
?
?A ?
?, [i, k], [nr, ne] ) )end f o rend i fend procedureprocedure PR ED I CT (A ?
B ?
?
?
C, [j, k], [ns, ne] )i f B i s n u l l and C i s n u l l / / edge i s c o m p l e t ef o r each D ?
?A?
in grammar where A i s t r i g g e rPUSH ( agenda , (D ?
?
?
A ?
?, [j, k], [ns, ne] ) )end f o re l s ei f B = ?D / / edge i s i n c o m p l e t ef o r each D ?
?
in grammarPUSH ( agenda , (D ?
?
?, [j, j], [ns, ns] ) )end f o rend i fi f C = D?
/ / edge i s i n c o m p l e t ef o r each D ?
?
in grammarPUSH ( agenda , (D ?
?
?, [k, k], [ne, ne] ) )end f o rend i fend i fend procedureFigure 4: Extended chart parsing algorithm usedto parse word graphs.
Fundamental rule and pre-dict procedures are updated to handle word graphsin a bidirectional manner.representation of grammatical functions in LFG.Attribute-value-matrices are used to describe f-structures.
A sample c-structure and the corre-sponding f-structures in English are shown in Fig-ure 5.
For simplicity, many details and feature val-ues are not given.
The dag containing the infor-mation originated from the lexicon and the infor-mation extracted from morphological analysis isshown on the leaf levels of the parse tree in Figure5.
The final dag corresponding to the root node isbuilt during the parsing process in cascaded unifi-cation operations specified in the grammar rules.473???????????????????
?cat sform ?look?tense pastsubj?
?form ?he?proper plus??obleak???
?form ?kids?def pluspform ?after????????????????????????
?snp vppro v ppp npdet nhe looked after the kids?????????
?cat proproper pluscase nomnum sgperson 3???????????
?cat vtense past??
[cat prep] ?
?cat detdef plus????????
?cat nproper minusnum plperson 3??????
?Figure 5: The c-structure and the associated f-structures.3.6 Parse evaluation and recoveryAfter all rules are executed and no more edges areleft in the agenda, the chart parsing process endsand parse evaluation begins.
The chart is searchedfor complete edges with the final symbol of thegrammar (e.g.
SBAR) as their category.
Any suchedge spanning the entire input represents the fullparse.
If there is no such edge then the parse re-covery process takes control.If the input sentence is ambiguous, then, at theend of parsing, there will multiple parse trees inthe chart that span the entire input.
Similarly,a grammar built with insufficient constraints canlead to multiple parse trees.
In this case, all possi-ble edges are evaluated for completeness and co-herence (Bresnan, 1982) starting from the edgewith the highest weight.
A parse tree is completeif all the functional roles (SUBJ, OBJ, SCOMP etc.
)governed by the verb are actually present in the c-structure; it is coherent if all the functional rolespresent are actually governed by the verb.
Theparse tree that is evaluated as complete and co-herent and has the highest weight is selected forfurther processing.In general, a parsing process is said to be suc-cessful if a parse tree can be built according to theinput sentence.
The building of the parse tree failswhen the sentence is ungrammatical.
For the goalof MT, however, a parse tree is required for thetransfer stage and the generation stage even if theinput is not grammatical.
Therefore, for any inputsentence, a corresponding parse tree is built at theend of parsing.If parsing fails, i.e.
if all rules are exhausted andno successful parse tree has been produced, thenthe system tries to recover from the failure by cre-ating a tree like structure.
Appropriate completeedges in the chart are used for this purpose.
Theidea is to piece together all partial parses for theinput sentence, so that the number of constituentedges is minimum and the score of the final tree ismaximum.
While selecting the constituents, over-lapping edges are not chosen.The recovery process functions as follows:?
The whole chart is traversed and a completeedge is inserted into a candidate list if it hasthe highest score for that start-end position.If two edges have the same score, then thefarthest one to the leaf level is preferred.?
The candidate list is traversed and a com-bination with the minimum number of con-stituents is selected.
The edges with thewidest span get into the winning combina-tion.?
The c-structures and f-structures of the edgesin the winning combination are joined into awhole c-structure and f-structure which rep-resent the final parse tree for the input.4 ExperimentsThe experiments carried out in this paper are runon word graphs based on 1993 benchmark tests forthe ARPA spoken language program (Pallett et al,1994).
In the large-vocabulary continuous speechrecognition (CSR) tests reported by Pallett et al(1994), Wall Street Journal-based CSR corpus ma-terial was made use of.
Those tests intended tomeasure basic speaker-independent performanceon a 64K-word read-speech test set which con-sists of 213 utterances.
Each of the 10 differentspeakers provided 20 to 23 utterances.
An acous-tic model and a trigram language model is trainedusing Wall Street Journal data by Chelba (2000)who also generated the 213 word graphs used inthe current experiments.
The word graphs, re-ferred as HUB-1 data set, contain both the acous-tic scores and the trigram language model scores.Previously, the same data set was used in other474studies (Chelba, 2000; Roark, 2001; Hall, 2005)for language modeling task in ASR.4.1 N-best list pruningThe 213 word graphs in the HUB-1 data set arepruned as described in Section 3 in order to pre-pare them for chart parsing.
AT&T toolkit (Mohriet al, 1998) is used for determinization and min-imization of the word graphs and for n-best pathextraction.
Prior to feeding in the word graphs tothe FSM tools, the acoustic model and the trigramlanguage model in the original lattices are com-bined into a single score using Equation 1, whereS represents the combined score of an arc, A isthe acoustic model (AM) score, L is the languagemodel (LM) score, ?
is the AM scale factor and ?is the LM scale factor.S = ?A+ ?
L (1)Figure 6 depicts the word error rates for thefirst-best hypotheses obtained heuristically by us-ing ?
= 1 and ?
values from 1 to 25.
The low-est WER (13.32) is achieved when ?
is set to 1and ?
to 15.
This result is close with the findingsfrom Hall (2005) who reported to use 16 as the LMscale factor for the same data set.
WER score forLM-only was 26.8 where in comparison the AM-only score was 29.64.
The results imply that thelanguage model has more predicting power overthe acoustic model in the HUB-1 lattices.
For therest of the experiments, we used 1 and 15 as theacoustic model and language model scale factors,respectively.4.2 Word graph accuracyUsing the scale factors found in the previous sec-tion we built N-best word graphs for different Nvalues.
In order to measure the word graph ac-curacy we constructed the FSM for reference hy-potheses, FRef , and we took the intersection of allthe word graphs with the reference FSM.
Table 1lists the word graph accuracy rate for different Nvalues.
For example, an accuracy rate of 30.98 de-notes that 66 word graphs out of 213 contain thecorrect sentences.
The accuracy rate for the origi-nal word graphs in the data set (last row in Table 1)is 66.67 which indicates that only 142 out of 213contain the reference sentence.
That is, in 71 of theinstances, the reference sentence is not included in0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25WER?10.0013.3229.64 bbbbbbbbbbbbb bbbbbbbbbbbbbFigure 6: WER for HUB-1 first-best hypothesesobtained using different language-model scalingfactors and ?
= 1.
The unsteadiness of the WERfor ?
= 10 needs further investigation.Table 1: Word graph accuracy for different N val-ues in the data set with 213 word graphs.N Accuracy1 30.9810 51.1720 56.3430 58.2240 59.1550 59.15N Accuracy60 59.1570 59.1580 59.1590 60.10100 60.10full 66.67the untouched word graph.
The accurate rates ex-press the maximum sentence error rate (SER) thatcan be achieved for the data set.4.3 Linguistic ResourcesThe English grammar used in the chart parser con-tained 20 morphology analysis rules and 225 syn-tax analysis rules.
All the rules and the unificationconstraints are implemented in LFG formalism.The number of rules to model the language gram-mar is quite few compared to probabilistic CFGswhich contain more than 10 000 rules.
The mono-lingual analysis lexicon consists of 40 000 lexicalentries.4.4 Chart parsing experimentWe conducted experiments to compare the per-formance for N-best list parsing and N-best wordgraph parsing.
Compared to the N-best list ap-proach, in N-best word graph parsing approach,the shared edges are processed only once for allhypotheses.
This saves a lot on the number of475Table 2: Number of complete and incompleteedges generated for the NIST HUB-1 data set us-ing different approaches.Approach Hypotheses CompleteedgesIncompleteedgesN-best list 4869 798 K 12.125 M1 164 2490N-best 4869 150.8 K 1.662 Mword graph 1 31 341complete and incomplete edges generated duringparsing.
Hence, the overall processing time re-quired to analyze the hypotheses are reduced.
Inan N-best list approach, where each hypothesis isprocessed separately in the analyzer, there are dif-ferent charts and different parsing instances foreach sentence hypothesis.
Shared words in dif-ferent sentences are parsed repeatedly and sameedges will be created at each instance.Table 2 represents the number of complete andincomplete edges generated for the NIST HUB-1data set.
For each hypothesis, 164 complete edgesand 2490 incomplete edges are generated on theaverage in the N-best list approach.
In the N-bestword graph approach, the average number of com-plete edges and incomplete edges reduced to 31and 341, respectively.
The decrease is 81.1% incomplete edges and 86.3% in incomplete edges forthe NIST HUB-1 data set.
The profit introducedin the number of edges by using the N-best wordgraph approach is immense.4.5 Language modeling experimentIn order to compare this approach to previouslanguage modeling approaches we used the samedata set.
Table 3 lists the WER for the NISTHUB-1 data set for different approaches includ-ing ours.
The N-best word graph approach pre-sented in this paper scored 12.6 WER and stillneeds some improvements.
The English analy-sis grammar that was used in the experiments wasdesigned to parse typed text containing punctua-tion information.
The speech data, however, doesnot contain any punctuation.
Therefore, the gram-mar has to be adjusted accordingly to improve theWER.
Another common source of error in parsingis because of unnormalized text.Table 3: WER taken from Hall and Johnson(2003) for various language models on HUB-1 lat-tices in addition to our approach presented in thefifth row.Model WERCharniak Parser (Charniak, 2001) 11.8Attention Shifting 11.9(Hall and Johnson, 2004)PCFG (Hall, 2005) 12.0A* decoding (Xu et al, 2002) 12.3N-best word graph (this study) 12.6PCFG (Roark, 2001) 12.7PCFG (Hall and Johnson, 2004) 13.040m-word trigram 13.7(Hall and Johnson, 2003)PCFG (Hall and Johnson, 2003) 15.55 ConclusionsThe primary aim of this research was to proposea new and efficient method for integrating an SRsystem with an MT system employing a chartparser.
The main idea is to populate the initialchart parser with the word graph that comes outof the SR component.This paper presents an attempt to blend statisti-cal SR systems with rule-based MT systems.
Thegoal of the final assembly of these two compo-nents was to achieve an enhanced Speech Transla-tion (ST) system.
Specifically, we propose to parsethe word graph generated by the SR module insidethe rule-based parser.
This approach can be gener-alized to any MT system employing chart parsingin its analysis stage.
In addition to utilizing rule-based MT in ST, this study used word graphs andchart parsing with new extensions.For further improvement of the overall system,our future studies include the following: 1.
Ad-justing the English syntax analysis rules to handlespoken text which does not include any punctua-tion.
2.
Normalization of the word arcs in the in-put lattice to match words in the analysis lexicon.AcknowledgmentsThanks to Jude Miller and Mirna Miller for pro-viding the Arabic reference translations.
We alsothank Brian Roark and Keith Hall for providingthe test data, and Nagendra Goel, Cem Bozs?ahin,Ays?enur Birt?rk and Tolga ?ilog?lu for their valu-able comments.476ReferencesJ.
Bresnan.
1982.
Control and complementation.
InJ.
Bresnan, editor, The Mental Representation ofGrammatical Relations, pages 282?390.
MIT Press,Cambridge, MA.J.-C. Chappelier, M. Rajman, R. Aragues, andA.
Rozenknop.
1999.
Lattice parsing for speechrecognition.
In TALN?99, pages 95?104.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting on Association for Computational Linguis-tics.
Association for Computational Linguistics.Ciprian Chelba.
2000.
Exploiting Syntactic Structurefor Natural Language Modeling.
Ph.D. thesis, JohnsHopkins University.Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee.
1990.An augmented chart data structure with efficientword lattice parsing scheme in speech recognitionapplications.
In Proceedings of the 13th conferenceon Computational linguistics, pages 60?65, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Lee-Feng Chien, K. J. Chen, and Lin-Shan Lee.1993.
A best-first language processing model in-tegrating the unification grammar and markov lan-guage model for speech recognition applications.IEEE Transactions on Speech and Audio Process-ing, 1(2):221?240.Yen-Lu Chow and Salim Roukos.
1989.
Speechunderstanding using a unification grammar.
InICAASP?89: Proc.
of the International Conferenceon Acoustics, Speech, and Signal Processing, pages727?730.
IEEE.Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing word lattice transla-tion.
In Proceedings of ACL-08: HLT, pages 1012?1020, Columbus, Ohio, June.
Association for Com-putational Linguistics.Keith Hall and Mark Johnson.
2003.
Language mod-elling using efficient best-first bottom-up parsing.In ASR?03: IEEE Workshop on Automatic SpeechRecognition and Understanding, pages 507?512.IEEE.Keith Hall and Mark Johnson.
2004.
Attention shiftingfor parsing speech.
In ACL ?04: Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, page 40, Morristown, NJ, USA.Association for Computational Linguistics.Keith Hall.
2005.
Best-First Word Lattice Parsing:Techniques for Integrated Syntax Language Model-ing.
Ph.D. thesis, Brown University.Martin Kay.
1986.
Algorithm schemata and data struc-tures in syntactic processing.
Readings in naturallanguage processing, pages 35?70.C.
D. Manning and H. Sch?tze.
2000.
Foundations ofStatistical Natural Language Processing.
The MITPress.Mehryar Mohri, Fernando C. N. Pereira, and MichaelRiley.
1998.
A rational design for a weighted finite-state transducer library.
In WIA ?97: Revised Pa-pers from the Second International Workshop on Im-plementing Automata, pages 144?158, London, UK.Springer-Verlag.Hermann Ney.
1991.
Dynamic programming pars-ing for context-free grammars in continuous speechrecognition.
IEEE Transactions on Signal Process-ing, 39(2):336?340.A.
Paeseler.
1988.
Modification of Earley?s algo-rithm for speech recognition.
In Proceedings ofthe NATO Advanced Study Institute on Recent ad-vances in speech understanding and dialog systems,pages 465?472, New York, NY, USA.
Springer-Verlag New York, Inc.David S. Pallett, Jonathan G. Fiscus, William M.Fisher, John S. Garofolo, Bruce A. Lund, andMark A. Przybocki.
1994.
In HLT ?94: Proceedingsof the workshop on Human Language Technology,pages 49?74, Morristown, NJ, USA.
Association forComputational Linguistics.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Andreas Stolcke.
1995.
An efficient probabilis-tic context-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201.Masaru Tomita.
1986.
An efficient word lattice pars-ing algorithm for continuous speech recognition.Acoustics, Speech, and Signal Processing, IEEE In-ternational Conference on ICASSP ?86., 11:1569?1572.Hans Weber.
1994.
Time synchronous chart parsing ofspeech integrating unification grammars with statis-tics.
In Proceedings of the Eighth Twente Workshopon Language Technology, pages 107?119.Phil Woodland.
2000.
HTK Speech RecognitionToolkit.
Cambridge University Engineering Depart-ment, http://htk.eng.cam.ac.uk.Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002.A study on richer syntactic dependencies for struc-tured language modeling.
In ACL ?02: Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 191?198.
Associationfor Computational Linguistics.477
