Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 59?63,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 12: Turkish Lexical Sample TaskZeynep OrhanDepartment ofComputer Engineering,Fatih University 34500,B?y?k?ekmece,Istanbul, Turkeyzorhan@fatih.edu.trEmine ?elikDepartment ofComputer Engineering,Fatih University 34500,B?y?k?ekmece,Istanbul, Turkeyeminemm@gmail.comNeslihan Demirg?
?Department ofComputer Engineering,Fatih University 34500,B?y?k?ekmece,Istanbul, Turkeynesli_han@hotmail.comAbstractThis paper presents the task definition, re-sources, and the single participant systemfor Task 12: Turkish Lexical Sample Task(TLST), which was organized in the Se-mEval-2007 evaluation exercise.
Themethodology followed for developing thespecific linguistic resources necessary forthe task has been described in this context.A language-specific feature set was definedfor Turkish.
TLST consists of three piecesof data: The dictionary, the training data,and the evaluation data.
Finally, a singlesystem that utilizes a simple statisticalmethod was submitted for the task andevaluated.1 IntroductionEffective parameters for word sense disambigua-tion (WSD) may vary for different languages andword types.
Although, some parameters are com-mon in many languages, some others may be lan-guage specific.
Turkish is an interesting languagethat deserves being examined semantically.
Turk-ish is based upon suffixation, which differentiatesit sharply from the majority of European languages,and many others.
Like all Turkic languages, Turk-ish is agglutinative, that is, grammatical functionsare indicated by adding various suffixes to stems.Turkish has a SOV (Subject-Object-Verb) sentencestructure but other orders are possible under certaindiscourse situations.
As a SOV language whereobjects precede the verb, Turkish has postpositionsrather than prepositions, and relative clauses thatprecede the verb.
Turkish, as a widely-spoken lan-guage, is appropriate for semantic researches.TLST utilizes some resources that are explainedin Section 2-5.
In Section 6 evaluation of the sys-tem is provided.
In section 7 some concluding re-marks and future work are discussed.2 CorpusLesser studied languages, such as Turkish sufferfrom the lack of wide coverage electronic re-sources or other language processing tools like on-tologies, dictionaries, morphological analyzers,parsers etc.
There are some projects for providingdata for NLP applications in Turkish like METUCorpus Project (Oflazer et al, 2003).
It has twoparts, the main corpus and the treebank that con-sists of parsed, morphologically analyzed and dis-ambiguated sentences selected from the main cor-pus, respectively.
The sentences are given in XMLformat and provide many syntactic features thatcan be helpful for WSD.
This corpus and treebankcan be used for academic purposes by contract.The texts in main corpus have been taken fromdifferent types of Turkish written texts publishedin 1990 and afterwards.
It has about two millionwords.
It includes 999 written texts taken from 201books, 87 papers and news from 3 different Turk-ish daily newspapers.
XML and Text EncodingInitiative (TEI) style annotation have been used.The distribution of the texts in the Treebank issimilar to the main corpus.
There are 6930 sen-tences in this Treebank.
These sentences have beenparsed, morphologically analyzed and disambigu-ated.
In Turkish, a word can have more than oneanalysis, so having disambiguated texts is veryimportant.59Figure 1: XML file structure of the TreebankWordsMain Englishtranslation#Senses MFSTrainsizeTestsizeTotal #ofinstancesNounsara distance, break, interval, look for 7 53 192 63 255ba?
head, leader, beginning, top, main, principal 5 34 68 22 90el hand, stranger, country 3 75 113 38 151g?z eye, glance, division, drawer 3 48 92 27 119k?z girl, virgin, daughter, get hot, get angry 2 72 96 21 117?n front, foreground, face, breast, prior, preliminary anterior 5 21 72 23 95s?ra queue, order, sequence, turn, regularity, occasion desk 7 30 85 28 113?st upper side, outside, clothing 7 20 69 23 92yan side, direction, auxiliary, askew, burn, be on fire be alight 5 21 65 31 96yol way, road, path, method, manner, means 6 17 68 29 97Average  5 39 92 31 123Verbsal take, get,  red 24 180 963 125 1088bak look, fac, examine 4 136 207 85 292?al??
work, study, start 4 33 103 61 164?
?k climb, leave, increase 6 45 138 87 225ge?
pass,happen, late 11 51 164 90 254gel come, arrive, fit, seem 20 154 346 215 561gir enter, fit, begin, penetrate 6 88 163 84 247git go, leave, last, be over, pass 13 130 214 120 334g?r see, understand, consider 5 155 206 68 274konu?
talk, speak 6 42 129 63 192Average   9.9 101.4 263.3 99.8 363.1Othersb?y?k big, extensive, important, chief, great, elder 6 34 97 26 123do?ru straight, true, accurate, proper, fair, line towards, around 6 29 81 38 119k??
?k little, small, young, insignificant, kid 4 14 45 14 59?yle such, so, that 4 20 51 23 74son last, recent, final 2 76 86 18 104tek single, unique, alone 2 38 40 10 50Average   4 35.2 66.7 21.5 88.2Table 1: Target words in the SEMEVAL-1 Turkish Lexical Sample task<?xml version="1.0" encoding="windows-1254" ?>- <Set sentences="1">- <S No="1"><W IX="1" LEM="" MORPH="" IG="[(1,"so?uk+Adj")(2,"Adv+Ly")]"REL="[2,1,(MODIFIER)]">So?uk?a</W><W IX="2" LEM="" MORPH="" IG="[(1,"yan?tla+Verb+Pos+Past+A1sg")]"REL="[3,1,(SENTENCE)]">yan?tlad?m</W><W IX="3" LEM="" MORPH="" IG="[(1,".+Punc")]" REL="[,( )]">.</W></S></Set>60Frequencies of the words have been found as itis necessary to select appropriate ambiguous wordsfor WSD.
There are 5356 different root words and627 of these words have 15 or more occurrences,and the rest have less.The XML files contains tagging information inthe word (morphological analysis) and sentencelevel as a parse tree as shown in Figure 1.
In theword level, inflectional forms are provided.
And inthe sentence level relations among words aregiven.
The S tag is for sentence and W tag is forthe word.
IX is used for index of the word in thesentence, LEM is left as blank and lemma is givenin the MORPH tag as a part of it with the morpho-logical analysis of the word.
REL is for parsinginformation.
It consists of three parts, two numbersand a relation.
For example REL="[2, 1, (MODI-FIER)]" means this word is modifying the first in-flectional group of the second word in the sen-tence.
The structure of the treebank data was de-signed by METU.
Initially lemmas were decided tobe provided as a tag by itself, however, lemmas areleft as blank.
This does not mean that lemmas arenot available in the treebank; the lemmas are givenas a part of ?IG?
tag.
Programs are available forextracting this information for the time being.
Allparticipants can get these programs and thereby thelemmas easily and instantly.The sense tags were not included in the treebankand had to be added manually.
Sense tagging hasbeen checked in order to obtain gold standard data.Initial tagging process has been finished by a sin-gle tagger and controlled.
Two other native speakerin the team tagged and controlled the examples.That is, this step was completed by three taggers.Problematic cases were handled by a commissionand the decision was finalized when about 90%agreement has been reached.3 DictionaryThe dictionary is the one that is published byTDK 1  (Turkish Language Foundation) and it isopen to public via internet.
This dictionary lists thesenses along with their definitions and examplesentences that are provided for some senses.
Thedictionary is used only for sense tagging andenumeration of the senses for standardization.
Nospecific information other than the sense numbers1http://tdk.org.tr/tdksozluk/sozara.htmis taken from the dictionary; therefore there is noneed for linguistic processing of the dictionary.4 Training and Evaluation DataIn Table 1 statistical information about the finaltraining and testing sets of TLST is summarized.The data have been provided for 3 words in thetrial set and 26 words in the final training and test-ing sets (10 nouns, 10 verbs and 6 other POS forthe rest of POS including adjectives and adverbs).It has been tagged about 100 examples per word,but the number of samples is incremented or dec-remented depending on the number of senses thatspecific word has.
For a few words, however,fewer examples exist due to the sparse distributionof the data.
Some ambiguous words had fewer ex-amples in the corpus, therefore they were eithereliminated or some other examples drawn fromexternal resources were added in the same format.On the average, the selected words have 6.7senses, verbs, however, have more.
Approximately70% of the examples for each word were deliveredas training data, whereas approximately 30% wasreserved as evaluation data.
The distribution of thesenses in training and evaluation data has beenkept proportional.
The sets are given as plain textfiles for each word under each POS.
The samplesfor the words that can belong to more than onePOS are listed under the majority class.
POS isprovided for each sample.We have extracted example sentences of the tar-get word(s) and some features from the XML files.Then tab delimited text files including structuraland sense tag information are obtained.
In thesefiles each line has contextual information that arethought to be effective (Orhan and Altan, 2006;Orhan and Altan, 2005) in Turkish WSD about thetarget words.
In the upper level for each of themXML file id, sentence number and the order of theambiguous word are kept as a unique key for thatspecific target.
In the sentence level, three catego-ries of information, namely the features related tothe previous words, target word itself and the sub-sequent words in the context are provided.61Feature ExampleFile id 00002213148.xmlSentence number 9Order 0Previous related word root/lemma tapPrevious related word POS(corrected) verbPrevious related word onthology level1  abstractionPrevious related word onthology level2 attributePrevious related word onthology level3  emotionPrevious related word POS verbPrevious related word POS(derivation) advPrevious related word case marker ?Previous related word possessor flPrevious related word-target word relation mod?f?erTarget word root/lemma sevTarget  word POS verbTarget  word POS(derivation) nounTarget  word case marker ablTarget  word possessor trTarget  word-subsequent word relation objectSubsequent related word root/lemma s?k?lSubsequent related word POS(corrected) verbSubsequent related word onthology level1  abstractionSubsequent related word onthology level2 attributeSubsequent related word onthology level3  emotionSubsequent related word POS verbSubsequent related word POS(derivation) verbSubsequent related word case marker ?Subsequent related word possessor flSubsequent related word-target word relation sentenceFine-grained sense number 2Coarse-grained sense number 2Sentence#ne   tuhaf   ?ey   ;   de?il   mi   ?iyi   olmamdan   ;   onu   taparcas?nasevmemden   s?k?ld?
.#Table 2: Features and exampleIn the treebank relational structure, there can bemore than one word in the previous context relatedto the target, however there is only a single word inthe subsequent one.
Therefore the data for allwords in the  previous context is provided sepa-rately.
The features that are employed for previousand the subsequent words are the same and theyare the root word, POS(corrected), tags for ontol-ogy level 1, level 2 and level 3, POS, inflectedPOS, case marker, possessor and relation.
How-ever for the target word only the root word, POS,inflected POS, case marker, possessor and relationare taken into consideration.
Fine and coarse-grained (FG and CG respectively) sense numbersand the sentence that has the ambiguous word havebeen added as the last three feature.
FG senses arethe ones that are decided to be the exact senses.CG senses are given as a set that are thought to bepossible alternatives in addition to the FG sense.Table 2 demonstrates the whole list of featuresprovided in a single line of data files along with anexample.
The ???
in the features shows the missingvalues.
This is actually corresponding to the fea-tures that do not exist or can not be obtained fromthe treebank due to some problematic cases.
The62line that corresponds to this entry will be the fol-lowing line (as tab delimited):00002213148.xml 9 0 tap verb abstractionattribute emotion verb adv ?
fl mod?f?er sev verbnoun abl tr object s?k?l verb abstraction attributeemotion verb verb ?
fl sentence 2 2 #ne tuhaf ?ey ;de?il mi ?iyi olmamdan ; onu taparcas?nasevmemden s?k?ld?
.#5 OntologyA small scale ontology for the target words andtheir context was constructed.
The Turkish Word-Net developed at Sabanc?
University2 is somehowinsufficient.
Only the verbs have some levels ofrelations similar to English WordNet.
The nouns,adjectives, adverbs and other words that are fre-quently used in Turkish and in the context of theambiguous words were not included.
This is not asuitable resource for fulfilling the requirements ofTLST and an ontology specific to this task wasrequired.
The ontology covers the examples thatare selected and has three levels of relations thatare supposed to be effective in the disambiguationprocess.
We tried to be consistent with the Word-Net tags; additionally we constructed the ontologynot only for nouns and verbs but for all the wordsthat are in the context of the ambiguous words se-lected.
Additionally we tried to strengthen the rela-tion among the context words by using the sametags for all POS in the ontology.
This is somehowdeviating from WordNet methodology, since eachword category has its own set of classification in it.6 EvaluationWSD is a new area of research in Turkish.
Thesense tagged data provided in TLST are the firstresources for this specific domain in Turkish.
Dueto the limited and brand new resources availableand the time restrictions the participation was less.We submitted a very simple system that utilizesstatistical information.
It is similar to the Na?veBayes approach.
The features in the training datawas used individually and the probababilities ofthe senses are calculated.
Then in the test phase theprobabilities of each sense is calculated with thegiven features and the three highest-scored sensesare selected as the answer.
The average precisionand recall values for each word category are given2http://www.hlst.sabanciuniv.edu/TL/in Table 3.
The values are not so high, as it can beexpected.
The size of the training data is limited,but the size is the highest possible under these cir-cumstances, but it should be incremented in thenear future.
The number of senses is high and pro-viding enough instances is difficult.
The data andthe methodology for WSD will be improved by theexperience obtained in SemEval evaluation exer-cise.The evaluation is done only for FG and CGsenses.
For FG senses no partial points are as-signed and 1 point is assigned for a correct match.On the other hand, the CG senses are evaluatedpartially.
If the answer tags are matching with anyof the answer tags they are given points.FG CGWordsP R P RNouns 0,15 0,50 0,65 0,43Verbs 0,10 0,38 0,56 0,50Others 0,13 0,50 0,57 0,44Average 0,13 0,46 0,59 0,46Table 3: Average Precision and Recall values7 ConclusionIn TLST we have prepared the first resources forWSD researches in Turkish.
Therefore it has sig-nificance in Turkish WSD studies.
Although theresources and methodology have some deficien-cies, a valuable effort was invested during the de-velopment of them.
The resources and the method-ology for Turkish WSD will be improved by theexperience obtained in SemEval and will be opento public in the very near future fromhttp://www.fatih.edu.tr/~zorhan/senseval/senseval.htm.ReferencesOrhan, Z. and Altan, Z.
2006.
Impact of Feature Selec-tion for Corpus-Based WSD in Turkish, LNAI,Springer-Verlag, Vol.
4293: 868-878Orhan Z. and Altan Z.
2005.
Effective Features for Dis-ambiguation of Turkish Verbs, IEC'05, Prague, CzechRepublic: 182-186Oflazer, K., Say, B., Tur, D. Z. H. and Tur, G. 2003.Building A Turkish Treebank, Invited Chapter InBuilding And Exploiting Syntactically-AnnotatedCorpora, Anne Abeille Editor, Kluwer AcademicPublishers.63
