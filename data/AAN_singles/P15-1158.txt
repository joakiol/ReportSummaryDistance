Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1639?1649,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsWhy discourse affects speakers?
choice of referring expressionsNaho OritaGraduate School of Information SciencesTohoku Universitynaho@ecei.tohoku.ac.jpEliana VornovComputer Science and LinguisticsUniversity of Marylandevornov@umd.eduNaomi H. FeldmanLinguistics and UMIACSUniversity of Marylandnhf@umd.eduHal Daum?e IIIComputer Science and UMIACSUniversity of Marylandhal@umiacs.umd.eduAbstractWe propose a language production modelthat uses dynamic discourse informationto account for speakers?
choices of refer-ring expressions.
Our model extends pre-vious rational speech act models (Frankand Goodman, 2012) to more naturally dis-tributed linguistic data, instead of assuminga controlled experimental setting.
Simula-tions show a close match between speakers?utterances and model predictions, indicat-ing that speakers?
behavior can be modeledin a principled way by considering the prob-abilities of referents in the discourse andthe information conveyed by each word.1 IntroductionDiscourse information plays an important role invarious aspects of linguistic processing, such aspredictions about upcoming words (Nieuwland andVan Berkum, 2006) and scalar implicature process-ing (Breheny et al, 2006).
The relationship be-tween discourse information and speakers?
choicesof referring expression is one of the most studiedproblems.
Speakers?
choices of referring expres-sions have long been thought to depend on thesalience of entities in the discourse (Giv?on, 1983).For example, speakers normally do not choose apronoun to refer to a new entity in the discourse,but are more likely to use pronouns for referentsthat have been referred to earlier in the discourse.A number of grammatical, semantic, and distribu-tional factors related to salience have been found toinfluence choices of referring expressions (Arnold,2008).
While the relationship between discoursesalience and speakers?
choices of referring expres-sions is well known, there is not yet a formal ac-count of why this relationship exists.In recent years, a number of formal models havebeen proposed to capture inferences between speak-ers and listeners in the context of Gricean prag-matics (Grice, 1975; Frank and Goodman, 2012).These models take a game theoretic approach inwhich speakers optimize productions to convey in-formation for listeners, and listeners infer meaningbased on speakers?
likely productions.
These mod-els have been argued to account for human commu-nication (Jager, 2007; Frank and Goodman, 2012;Bergen et al, 2012a; Smith et al, 2013), and stud-ies report that they robustly predict various linguis-tic phenomena in experimental settings (Goodmanand Stuhlm?uller, 2013; Degen et al, 2013; Kao etal., 2014; Nordmeyer and Frank, 2014).
However,these models have not yet been applied to languageproduced outside of the laboratory, nor have theyincorporated measures of discourse salience thatcan be computed over corpora.In this paper, we propose a probabilistic modelto explain speakers?
choices of referring expres-sions based on discourse salience.
Our model ex-tends the rational speech act model from Frankand Goodman (2012) to incorporate updates to lis-teners?
beliefs as discourse proceeds.
The modelpredicts that a speaker?s choice of referring expres-sions should depend directly on the amount of in-formation that each word carries in the discourse.Simulations probe the contribution of each modelcomponent and show that the model can predict1639speakers?
pronominalization in a corpus.
Theseresults suggest that this model formalizes underly-ing principles that account for speakers?
choices ofreferring expressions.The paper is organized as follows.
Section 2reviews relevant studies on choices of referring ex-pressions.
Section 3 describes the details of ourmodel.
Section 4 describes the data, preprocessingand annotation procedure.
Section 5 presents simu-lation results.
Section 6 summarizes this study anddiscusses implications and future directions.2 Relevant Work2.1 Discourse salienceSpeakers?
choices of referring expressions havelong been an object of study.
Pronominalizationhas been examined particularly often in both theo-retical and experimental studies.
Discourse theoriespredict that speakers use pronouns when they thinkthat a referent is salient in the discourse (Giv?on,1983; Ariel, 1990; Gundel et al, 1993; Grosz etal., 1995), where salience of the referent is influ-enced by various factors such as grammatical posi-tion (Brennan, 1995), recency (Chafe, 1994), top-icality (Arnold, 1998), competitors (Fukumura etal., 2011), visual salience (Vogels et al, 2013b),and so on.Discourse theories have characterized the linkbetween referring expressions and discoursesalience by stipulating constructs such as a scaleof topicality (Giv?on, 1983), accessibility hierarchy(Ariel, 1990), or implicational hierarchy (Gundel etal., 1993).
All of these assume fixed form-saliencecorrespondences in that a certain referring expres-sion encodes a certain degree of salience.
However,it is not clear how this form-salience mapping holdsnor why it should be.There is also a rich body of research that pointsto the importance of production cost (Rohde et al,2012; Bergen et al, 2012b; Degen et al, 2013)and listener models (Bard et al, 2004; Van derWege, 2009; Galati and Brennan, 2010; Fukumuraand van Gompel, 2012) in language production.These studies suggest that only considering dis-course salience of the referent may not preciselycapture speakers?
choices of referring expressions,and it is necessary to examine discourse salience inrelation to these other factors.2.2 Formal modelsComputational models relevant to speakers?choices of referring expressions have been pro-posed, but there is a gap between questions thatprevious models have addressed and the questionsthat we have raised above.Gr?uning and Kibrik (2005) and Khudyakova etal.
(2011) examine the significance of various fac-tors that might influence choices of referring ex-pressions by using machine learning models suchas neural networks, logistic regression and decisiontrees.
Although these models qualitatively showsome significant factors, they are data-driven ratherthan being explanatory, and have not focused onwhy and how these factors result in the observedreferring choices.Formal models that go beyond identifying super-ficial factors focus on only pronouns rather thanaccounting for speakers?
word choices per se.
Forexample, Kehler et al (2008) formalize a relation-ship between pronoun comprehension and produc-tion using Bayes?
rule to account for comprehen-der?s semantic bias in experimental data.
Rij et al(2013) use ACT-R (Anderson, 2007) to examinethe effects of working memory load in pronouninterpretation.
These models show how certain fac-tors influence pronoun production/interpretation,but it is not clear how these models would predictspeakers?
choices of referring expressions.Relevant formal models in computational lin-guistics include Centering theory (Grosz et al,1995; Poesio et al, 2004) and Referring Expres-sion Generation (Krahmer and Van Deemter, 2012).These models propose deterministic constraintsgoverning when pronouns are preferred in local dis-course, but it is not clear how these would accountfor speakers?
choices of referring expressions, norit is clear why there should be such deterministicconstraints.2.3 Uniform Information DensityOne potential formal explanation for the relationbetween discourse salience and speakers?
choicesof referring expressions is the Uniform Informa-tion Density hypothesis (UID) (Levy and Jaeger,2007; Tily and Piantadosi, 2009; Jaeger, 2010).UID states that speakers prefer to smooth the in-formation density distribution of their utterancesover time to achieve optimal communication.
Thistheory predicts that speakers should use pronounsinstead of longer forms (e.g., the president) when a1640referent is predictable in the context, whereas theyshould use longer forms for unpredictable referentsthat carry more information (Jaeger, 2010).Tily and Piantadosi (2009) empirically exam-ined the relationship between predictability of areferent and choice of referring expressions.
Theyfound that predictability is a significant predictorof writers?
choices of referring expressions, in thatpronouns are used when a referent is predictable.While these results appear to support UID, thereare several inconsistencies with previous UID ac-counts.
Information content of words has beenestimated using an n-gram language model (Levyand Jaeger, 2007), a verb?s subcategorization fre-quency (Jaeger, 2010), and so on, whereas herethe information content is that of referents withrespect to discourse salience.
In addition, selectingbetween a pronoun and a more specified referringexpression involves deciding how much informa-tion to convey, whereas previous applications ofUID (Levy and Jaeger, 2007) have been concernedwith deciding between different ways of expressingthe same information content.
We show in the nextsection that we can derive predictions about refer-ring expressions directly from a model of languageproduction.2.4 SummaryPrevious linguistic studies have focused on identi-fying factors that might influence choices of refer-ring expressions.
However, it is not clear from thisprevious work how and why these factors resultin the observed patterns of referring expressions.Where formal models relevant to this topic do exist,they have not been built to explain why there is arelation between discourse salience and speakers?choices of referring expressions.
Even UID, whichrelates predictability to word length, is not set upto account for the choice between words that varyin their information content.In the next section, we propose a speaker modelthat formalizes the relation between discoursesalience and speakers?
choices of referring expres-sions, considering production cost and speakers?inference about listeners in a principled and ex-planatory way.3 Speaker model3.1 Rational speaker-listener modelWe adopt the rational speaker-listener model fromFrank and Goodman (2012) and extend this modelto predict speakers?
choices of referring expres-sions using discourse information.The main idea of Frank and Goodman?s modelis that a rational pragmatic listener uses Bayesianinference to infer the speaker?s intended referentrsgiven the word w, their vocabulary (e.g., ?blue?,?circle?
), and shared context that consists of a setof objects O (e.g., visual access to object referents)as in (1), assuming that a speaker has chosen theword informatively.P (rs|w,O) =PS(w|rs, O)P (rs)?r?
?OP (w|r?, O)P (r?
)(1)While our work does not make use of this pragmaticlistener, it does build on the speaker model assumedby the pragmatic listener.
This speaker model (thelikelihood term in the listener model) is definedusing an exponentiated utility function as in (2).PS(w|rs, O) ?
e?U(w;rs,O)(2)The utility U(w; rs, O) is defined as I(w; rs, O)?D(w), where I(w; rs, O) represents informative-ness of word w (quantified as surprisal) and D(w)represents its speech cost.
If a listener interpretsword w literally and cost D(w) is constant, the ex-ponentiated utility function can be reduced to (3)where |w| denotes the number of referents that theword w can be used to refer to.PS(w|rs, O) ?1|w|(3)Thus, the speaker model chooses a word based onits specificity.
We show in the next section thatthis corresponds to a speaker who is optimizinginformativeness for a listener with uniform beliefsabout what will be referred to in the discourse.
Theassumption of uniform discourse salience workswell in a simple language game where there area limited number of referents that have roughlyequal salience, but we show that a model that lacksa sophisticated notion of discourse falls short inmore realistic settings.3.2 Incorporating discourse salienceTo extend Frank and Goodman?s model to a natu-ral linguistic situation, we assume that the speakerestimates the listener?s interpretation of a word (orreferring expression)w based on discourse informa-tion.
We extend the speaker model from (3) by as-suming that a speaker S choosesw to optimize a lis-tener?s belief in speaker?s intended referent r rela-tive to the speaker?s own speech cost Cw.
This cost1641is another factor in the speaker model, roughly cor-responding to utterance complexity such as wordlength.1PS(w|r) ?
PL(r|w) ?1Cw(4)The term PL(r|w) in (4) represents informative-ness of word w: the speaker chooses w that mosthelps a listener L to infer referent r. The term Cwin (4) is a cost function: the speaker chooses w thatis least costly to speak.The speaker?s listener model, PL(r|w), infersreferent r that is referred to by word w accordingto Bayes?
rule as in (5).PL(r|w) =P (w|r)P (r)?r?P (w|r?
)P (r?
)(5)The first term in the numerator, P (w|r), is a wordprobability: the listener in the speaker?s mindguesses how likely the speaker would be to usew torefer to r. The second term in the numerator, P (r),is the discourse salience (or predictability) of refer-ent r. The denominator ?r?P (w|r?
)P (r?)
is a sumof potential referents r?that could be referred to byword w. The terms in this sum are non-zero onlyfor referents that are compatible with the meaningof the word.
If there are many potential referentsthat could be referred to by word w, that wordwould be more ambiguous thus less informative.The whole of the right side in Equation (5) repre-sents the speaker?s assumption about the listener:given word w the listener would infer referent rthat is salient in a discourse and less ambiguouslyreferred to by word w.If P (r) is uniform over referents and P (w|r) isconstant across words and referents, this listenermodel reduces to1|w|.
Thus, Frank and Goodman(2012)?s speaker model in (3) is a special case ofour speaker model in (4) that assumes uniformdiscourse salience and constant cost.Our model predicts that the speaker?s probabilityof choosing a word for a given referent shoulddepend on its cost relative to its information content.To see this, we combine (4) and (5), yieldingPS(w|r) ?P (w|r)P (r)?r?P (w|r?
)P (r?
)?1Cw(6)Because the speaker is deciding what word to usefor an intended referent, and the term P (r) denotes1Our speaker model corresponds to Frank and Goodman?sexponentiated utility function (2), with ?
equal to one andwith their cost D(w) being the log of our cost Cw.the probability of this referent, P (r) is constant inthe speaker model and does not affect the relativeprobability of a speaker producing different words.We further assume for simplicity that P (w|r) isconstant across words and referents.
This meansthat all referents have about the same number ofwords that can be used to refer to them, and thatall words for a given referent are equally probablefor a naive listener.
In this scenario, the speaker?sprobability of choosing a word isPS(w|r) ?1?r?P (r?
)?1Cw(7)where the sum denotes the total discourse probabil-ity of the referents referred to by that word.The information content of an event is definedas the negative log probability of that event.
In thisscenario, the information conveyed by a word is thelogarithm of the first term in (7), ?
log?r?P (r?
).This means that in deciding which word to use,the highest cost a speaker should be willing to payfor a word should depend directly on that word?sinformation content.This relationship between cost and informationcontent allows us to derive the prediction tested byTily and Piantadosi (2009) that the use of referringexpressions should depend on the predictabilityof a referent.
For referents that are highly pre-dictable from the discourse, different referring ex-pressions (e.g., pronouns and proper names) willhave roughly equal information content, and speak-ers should choose the referring expression that hasthe lowest cost.
In contrast, for less predictable ref-erents, proper names will carry substantially moreinformation than pronouns, leading speakers to paya higher cost for the proper names.
These are thesame predictions that have been discussed in thecontext of UID, but here the predictions are derivedfrom a principled model of speakers who are try-ing to provide information to listeners.
The extentto which our model can also capture other casesthat have been put forward as evidence for the UIDhypothesis remains a question for future research.3.3 Predicting behavior from corporaThe model described in Section 3.2 is fully general,applying to arbitrary word choices, discourse prob-abilities, and cost fuctions.
As an initial step, oursimulations focus on the choice between pronounsand proper names.
Our work tests the speakermodel from (4) directly, asking whether it can pre-dict the referring expressions from corpora of writ-1642ten and spoken language.
Implementing the modelrequires computing word probabilities P (w|r), dis-course salience P (r), and word costs Cw.We simplify the word probability P (w|r) in thespeaker?s listener model as in (8):P (w|r) =1V(8)where the count V is the number of words that canrefer to referent r. We assume that V is constantacross all referents.
Our reasoning is as follows.There could be many ways to refer to a single entity.For example, to refer to entity Barack Obama, wecould say ?he?, ?The U.S.
president?, ?Barack?, andso on.
We assume that there are the same numberof referring expressions for each entity and thateach referring expression is equally probable underthe listener?s likelihood model.In our simulations, we assume that a speaker ischoosing between a proper name and a pronoun.For example, we assume that an entity BarackObama has one and only one proper name ?BarackObama?, and this entity is unambiguously associ-ated with male and singular.
Although we use anexample with two possible referring expressions,as long as P (w|r) is constant across all referentsand words, it does not make a difference to thecomputation in (5) how many competing words weassume for each referent.To estimate the salience of a referent, P (r), ourframework employs factors such as referent fre-quency or recency.
Although there are other impor-tant factors such as topicality of the referent (Oritaet al, 2014) that are not incorporated in our sim-ulations, this model sets up a framework to testthe role and interaction of various potential factorssuggested in the discourse literature.Salience of the referent is computed differentlydepending on its information status: old or new.The following illustrates the speaker?s assumptionsabout the listener?s discourse model:For each referent r ?
[1, Rd]:1.
If r = old, choose r in proportion to Nr(thenumber of times referent r has been referredto in the preceding discourse).2.
Otherwise, r = new with probability propor-tional to ?
(a hyperparameter that controlshow likely the speaker is to refer to a newreferent).3.
If r = new, sample that new referent r fromthe base distribution over entities with proba-bility1U?
(count U?denotes a total number ofunseen entities that is estimated from a namedentity list (Bergsma and Lin, 2006)).The above discourse model is frequency-based.We can replace the termNrfor the old referent withf(di,j) = e?di,j/athat captures recency, where therecency function f(di,j) decays exponentially withthe distance between the current referent riand thesame referent rjthat has previously been referredto.
This framework for frequency and recency ofnew and old referents exactly corresponds to pri-ors in the Chinese Restaurant Process (Teh et al,2006) and the distance-dependent Chinese Restau-rant Process (Blei and Frazier, 2011).The denominator in (5) represents the sum ofpotential referents that could be referred to by wordw.
We assume that a pronoun can refer to a largenumber of unseen referents if gender and numbermatch, but a proper name cannot.
For example, ?he?could refer to all singular and male referents, but?Barack Obama?
can only refer to Barack Obama.This assumption is reflected as a probability ofunseen referents for the pronoun as illustrated in(10) below.In our simulations, the speaker?s cost functionCwis estimated based on word length as in (9).
Weassume that longer words are costly to produce.Cw= length(w) (9)Suppose that the speaker is considering using?he?
to refer to Barack Obama, which has beenreferred to NOtimes in the preceding discourse,and there is another singular and male entity, JoeBiden, in the preceding discourse that has beenreferred to NBtimes.
In this situation, the modelcomputes the probability that the speaker uses ?he?to refer to Barack Obama as follows:PS(?he?|Obama)?
PL(Obama|?he?)
?1C?he?=P (?he?|Obama)P (Obama)?r?P (?he?|r?
)P (r?)?1C?he?=1V?NO(1V?NO)+(1V?NB)+(1V???Using&mascU?)?1C?he?
(10)where count Using&mascin the denominator of thelast line denotes the number of unseen singular &male entities that could be referred to by ?he?.
Weestimate this number for each type of pronoun we1643evaluate (singular-female, singular-male, singular-neuter, and plural) based on the named entity listin Bergsma and Lin (2006).
The term (1V?
?
?Using&mascU?)
is the sum of probabilities of unseenreferents that could be referred to by the pronoun?he?.
The unseen referents can be interpreted as apenalty for the inexplicitness of pronouns.
In thecase of proper names, the denominator is alwaysthe same as the numerator, under the assumptionthat each entity has one unique proper name.4 Data4.1 CorporaOur model was run on both adult-directed speechand child-directed speech.
We chose to use theSemEval-2010 Task 1 subset of OntoNotes (Re-casens et al, 2011), a corpus of news text, as ourcorpus of adult-directed speech.
The Gleason et al(1984) subset of CHILDES (MacWhinney, 2000)was chosen as our corpus of child-directed speech.The model requires coreference chains, agree-ment information, grammatical position, and partof speech.
These were extracted from each corpus,either manually or automatically.
The coreferencechains let us easily count how many times/howrecently each referent is mentioned in the dis-course, which is necessary for computing discoursesalience.
The agreement information (gender andnumber of each referent) is required so that themodel can identify all possible competing refer-ents for pronouns.
For instance, Barack Obamawill be ruled out as a possible competitor for thepronoun she.
The grammatical position that eachproper name occupies2determines the form of thealternative pronoun that could be used there.
Forexample, the difference between he and him is thegrammatical position that each can appear in.
Thepart of speech is used to identify the form of thereferring expression (pronouns and proper names),which is what our model aims to predict.3OntoNotes includes information about corefer-ence chains, part of speech, and grammatical de-pendencies.
Gleason CHILDES has parsed part ofspeech and grammatical dependencies (Sagae etal., 2010), but it does not have coreference chains.2Dependency tags used were ?SUBJ?, ?OBJ?, and ?PMOD?in OntoNotes and ?SBJ?
and ?OBJ?
in CHILDES.3The part of speech used to extract the target NPs were?PRP?
(pronoun), ?NNP?
(proper name), and ?NNPS?
(plu-ral proper name) from OntoNotes and ?pro?
(pronoun) and?n:prop?
(proper name) from CHILDES.Neither corpus has agreement information.
The fol-lowing section describes manual annotations thatwe have done for this study.
Due to time constraints,we annotated only a part of the CHILDES Gleasoncorpus, 9 out of 70 scripts.4.2 Annotation4.2.1 Mention annotationWe considered only maximally spanning nounphrases as mentions, ignoring nested NPs andnested coreference chains.
For the sentence ?BothAl Gore and George W. Bush have different ideason how to spend that extra money?
from OntoNotes,the extracted NPs are Both Al Gore and George W.Bush and different ideas about how to spend thatextra money.These maximally spanning NPs were automati-cally extracted from the OntoNotes data, but weremanually annotated for the CHILDES data usingbrat (Stenetorp et al, 2012) by two annotators.44.2.2 Agreement annotationMany mentions (46,246 out of 56,575 mentions inOntoNotes and 10,141 out of 10,530 mentions inCHILDES Gleason) were automatically annotatedusing agreement information from the named entitylist in Bergsma and Lin (2006), leaving 10,329to be manually annotated from OntoNotes (about18%) and 389 from CHILDES (about 4%).5The guidelines we followed for this manualagreement annotation were largely based on pro-noun replacement tests.
NPs that referred to a sin-gle man and could be replaced with he or him werelabeled ?male singular?, NPs that could be replacedby it, such as the comment, were labeled ?neutersingular?, and so on.
NPs that could not be replacedwith a pronoun, such as about 30 years earningsfor the average peasant, who makes $145 a year,were excluded from the analysis.4.2.3 Coreference annotationWe used the provided coreference chains for theOntoNotes data, but for the CHILDES data, it wasnecessary to do this manually using brat.
The guide-lines we followed for determining whether men-tions coreferred came from the OntoNotes corefer-4Interannotator agreement for the CHILDES mention an-notation was: precision 0.97, recall 0.98, F-score 0.97 (fortwo scripts).5Interannotator agreement for the manual annotation ofagreement information was 97% (for 500 mentions).1644ence guidelines (BBN Technologies, 2007).65 ExperimentsOur experiments are designed to quantify the contri-butions of the various components of the completemodel described in Section 3.2 that incorporatesdiscourse salience, cost, and unseen referents.
Wecontrast the complete model with three impover-ished models that lack precisely one of these com-ponents.
The comparison model without discourseuses a uniform discourse salience distribution.
Themodel without cost uses constant speech cost.
Themodel without good estimates of unseen referentsalways assigns probability1V?
?
?1C?to unseenreferents in the denominator of (5), regardless ofwhether the word is a proper name or pronoun.
Inother words, this model does not have good esti-mates of unseen referents like the complete modeldoes.We use the adult- and child-directed corpora toexamine to what extent each model captures speak-ers?
referring expressions.
We selected pronounsand proper names in each corpus according to sev-eral criteria.
First, the referring expression hadto be in a coreference chain that had at least oneproper name, in order to facilitate computing thecost of the proper name alternative.
Second, pro-nouns were only included if they were third personpronouns in subject or object position, and index-icals and reflexives were excluded.
Finally, forthe CHILDES corpus, children?s utterances wereexcluded.After filtering pronouns and proper names ac-cording to these criteria, 553 pronouns and 1,332proper names (total 1,885 items) in the OntoNotescorpus, and 165 pronouns and 149 proper names(total 314 items) in the CHILDES Gleason corpusremained for use in the analysis.Each model chooses referring expressions giveninformation extracted from each corpus as de-scribed in Section 4.1.
For evaluation, we com-puted accuracies (total, pronoun, and proper name)and model log likelihood (summing logPS(w|r)for the words in the corpus) for each model.5.1 ResultsTable 1 summarizes the results of each model withthe OntoNotes and CHILDES datasets.
The new6Interannotator agreement for CHILDES coreference an-notation was computed using B3(Bagga and Baldwin, 1998):precision: 0.99, recall: 1.00 (for one script).referent hyperparameter ?
and the decay parameterfor discourse recency salience were fixed at 0.1 and3.0, respectively.75.1.1 NewsOverall, the recency salience measure provides abetter fit than the frequency salience measure withrespect to accuracies, suggesting that recency bet-ter captures speakers?
representations of discoursesalience that influence choices of referring expres-sions.
On the other hand, the models with fre-quency discourse salience have higher model loglikelihood than the models with recency do.
Thisis because of the peakiness of the recency models.Model log likelihood computed over pronouns andproper names (complete model) were -1022.33 and-222.76, respectively, with recency, and -491.81 and-467.06 with frequency.
The recency model tendsto return a higher probability for a proper namethan the frequency model does.
Some pronounsreceive a very low probability for this reason, andthis lowers the model log likelihood.The model without discourse and the model with-out cost consistently failed to predict pronouns(these models predicted all proper names).
Thishappens because in the model without discourse,the information content of pronouns is extremelylow due to the large number of consistent unseenreferents.
In the model without cost, pronouns aredisfavored because they always convey less infor-mation than proper names.
The log likelihoods ofthese models were also below that of the completemodel.
These results show that pronominalizationdepends on subtle interaction between discoursesalience and speech cost.
Neither of them is suf-ficient to explain the distribution of pronouns andnouns on its own.The total accuracy of the model without goodestimates of unseen referents was the worst amongthe four models, but this model did predict pro-nouns to some extent.
Because the number ofproper names is larger than the number of pronounsin this dataset, the difference in total accuracies be-tween the model without good estimates of unseenreferents and the models without discourse or costreflects this asymmetry.
Comparison between thecomplete model and the model without good esti-mates of unseen referents also suggests that havingknowledge of unseen referents helps correctly pre-7We chose the best parameter values based on multipleruns, but results were qualitatively consistent across a rangeof parameter values.1645Corpus Model Discourse Total accuracy Pronoun accuracy Proper name accuracy Log-lhoodOntoNotescomplete recency 80.27% 59.49% 88.89% -1245.09frequency 73.10% 62.74% 77.40% -958.87-discourse NA 70.66% 0.00% 100.00% -6904.77-cost recency 70.66% 0.00% 100.00% -1537.71frequency 70.66% 0.00% 100.00% -1017.38-unseen recency 64.14% 68.17% 62.46% -1567.51frequency 56.98% 76.67% 48.80% -1351.58CHILDEScomplete recency 49.68% 11.52% 91.95% -968.64frequency 46.18% 10.30% 85.91% -360.28-discourse NA 47.45% 0.00% 100.00% -2159.22-cost recency 47.45% 0.00% 100.00% -1055.54frequency 47.45% 0.00% 100.00% -392.72-unseen recency 50.31% 13.94% 90.60% -961.54frequency 48.41% 21.21% 78.52% -332.73Table 1: Accuracies and model log-likelihooddict the use of proper names in the first mention ofa referent.5.1.2 Child-directed speechUnlike the adult-directed news text, neither recencynor frequency discourse salience provides a goodfit to the data.
The low accuracies of pronouns andthe high accuracies of proper names in all modelsindicate that the models are more likely to predictproper names than pronouns.
There are severalpossible reasons for this.
First, the CHILDES tran-scripts involve long conversations in a natural set-tings.
Compared to the news, interlocutors are notfocusing on a specific topic, but rather they oftenswitch topics (e.g., a child interrupts her parents?conversation about her father?s coworker to talkabout her eggs).
This topic switching makes it dif-ficult for the model to estimate discourse salienceusing simple frequency or recency measures.
Sec-ond, interlocutors are a family and they share agood deal of common knowledge/background (e.g.,a mother said she as the first mention of her child?sfriend?s mother).
The current model is not ableto incorporate this kind of background knowledge.Third, many referents are visually available.
Thecurrent model is not able to use visual salience.
Ingeneral, these problems arise due to our impover-ished estimates of salience, and we would expect amore sophisticated discourse model that accuratelymeasured salience to show better performance.5.2 SummaryExperiments with the adult-directed news corpusshow a close match between speakers?
utterancesand model predictions.
On the other hand, exper-iments with child-directed speech show that themodels were more likely to predict proper nameswhere pronouns were used, suggesting that the esti-mates of discourse salience using simple measureswere not sufficient to capture a conversation.6 DiscussionThis paper proposes a language production modelthat extends the rational speech act model fromFrank and Goodman (2012) to incorporate updatesto listeners?
beliefs as discourse proceeds.
We showthat the predictions suggested from UID in this do-main can be derived from our speaker model, pro-viding an explanation from first principles for therelation between discourse salience and speakers?choices of referring expressions.
Experiments withan adult-directed news corpus show a close matchbetween speakers?
utterances and model predic-tions, and experiments with child-directed speechshow a qualitatively similar pattern.
This suggeststhat speakers?
behavior can be modeled in a princi-pled way by considering the probabilities of refer-ents in the discourse and the information conveyedby each word.A controversial issue in language production isto what extent speakers consider a listener?s dis-course model (Fukumura and van Gompel, 2012).By incorporating an explicit model of listeners, ourmodel provides a way to explore this question.
Forexample, the speaker?s listener model PL(r|w) in(4) might differ between contexts and could also beextended to sum over possible listener identity q inmixed contexts, as in (11).PL(r|w) = ?qP (r|w, q)P (q) (11)This provides a way to probe speakers?
sensitiv-ity to differences in listener characteristics acrosssituations.1646Although the simulations in this paper employedsimple measures for discourse salience (referentfrequency and recency), the discourse models usedby speakers are likely to be more complex.
Stud-ies show that semantic information that cannot becaptured with these simple measures, such as topi-cality (Orita et al, 2014) and animacy (Vogels etal., 2013a), affects speakers?
choices of referringexpressions.
Future work will test to what extentthis latent discourse information could affect themodel predictions.Our model predicts a tight coupling between theprobability of a referent being mentioned, p(r),and the choice of referring expression.
However,these two quantities appear to be dissociated insome cases.
For example, Fukumura and Van Gom-pel (2010) show that semantic bias (as a measureof predictability) affects what to refer to (i.e., thereferent), but not how to refer (i.e., the referringexpression), while grammatical position does af-fect how you refer.
One way of dissociating theprobability of mention from the choice of referringexpression in our model would be through the likeli-hood term, the word probability p(w|r).
While wehave assumed this word probability to be constantacross words and referents, Kehler et al (2008) usegrammatical position to define this probability andshow that their model captures experimental data.Syntactic constraints (such as Binding principles)also influence form choices, and this kind of knowl-edge may also be reflected in the word probability.Examining the role of the word probability p(w|r)more closely would allow us to further explorethese issues.Despite these limitations, our model providesa principled explanation for speakers?
choices ofreferring expressions.
In future work we hope tolook at a broader range of referring expressions,such as null pronouns and definite descriptions,and to explore the extent to which our model canbe applied to other linguistic phenomena that relyon discourse information.AcknowledgmentsWe thank the UMD probabilistic modeling readinggroup for helpful comments and discussion.ReferencesJohn R Anderson.
2007.
How can the human mindoccur in the physical universe?
Oxford UniversityPress.Mira Ariel.
1990.
Accessing noun-phrase antecedents.Routledge.Jennifer Arnold.
1998.
Reference form and discoursepatterns.
Ph.D. thesis, Stanford University Stanford,CA.Jennifer Arnold.
2008.
Reference produc-tion: Production-internal and addressee-orientedprocesses.
Language and cognitive processes,23(4):495?527.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In The first interna-tional conference on language resources and evalua-tion workshop on linguistics coreference, volume 1,pages 563?566.Ellen Gurman Bard, Matthew P Aylett, J Trueswell,and M Tanenhaus.
2004.
Referential form, word du-ration, and modeling the listener in spoken dialogue.Approaches to studying world-situated language use:Bridging the language-as-product and language-as-action traditions, pages 173?191.BBN Technologies.
2007.
OntoNotes English co-reference guidelines version 7.0.Leon Bergen, Noah Goodman, and Roger Levy.
2012a.That?s what she (could have) said: How alternativeutterances affect language use.
In Proceedings ofthe 34th Annual Conference of the Cognitive ScienceSociety.Leon Bergen, Noah D Goodman, and Roger Levy.2012b.
That?s what she (could have) said: Howalternative utterances affect language use.
In Pro-ceedings of the thirty-fourth annual conference ofthe cognitive science society.Shane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics, pages 33?40,Sydney, Australia, July.
Association for Computa-tional Linguistics.David M Blei and Peter I Frazier.
2011.
Distance de-pendent Chinese restaurant processes.
The Journalof Machine Learning Research, 12:2461?2488.Richard Breheny, Napoleon Katsos, and John Williams.2006.
Are generalised scalar implicatures generatedby default?
an on-line investigation into the role ofcontext in generating pragmatic inferences.
Cogni-tion, 100(3):434?463.Susan E Brennan.
1995.
Centering attention indiscourse.
Language and Cognitive Processes,10(2):137?167.Wallace Chafe.
1994.
Discourse, consciousness, andtime.
Discourse, 2(1).1647Judith Degen, Michael Franke, and Gerhard J?ager.2013.
Cost-based pragmatic inference about referen-tial expressions.
In Proceedings of the 35th AnnualConference of the Cognitive Science Society, pages376?381.Michael Frank and Noah Goodman.
2012.
Predictingpragmatic reasoning in language games.
Science,336(6084):998?998.Kumiko Fukumura and Roger PG Van Gompel.
2010.Choosing anaphoric expressions: Do people takeinto account likelihood of reference?
Journal ofMemory and Language, 62(1):52?66.Kumiko Fukumura and Roger PG van Gompel.
2012.Producing pronouns and definite noun phrases: Dospeakers use the addressee?s discourse model?
Cog-nitive Science, 36(7):1289?1311.Kumiko Fukumura, Roger PG Van Gompel, TrevorHarley, and Martin J Pickering.
2011.
How doessimilarity-based interference affect the choice of re-ferring expression?
Journal of Memory and Lan-guage, 65(3):331?344.Alexia Galati and Susan E Brennan.
2010.
Attenuat-ing information in spoken communication: For thespeaker, or for the addressee?
Journal of Memoryand Language, 62(1):35?51.Talmy Giv?on.
1983.
Topic continuity in discourse: Aquantitative cross-language study, volume 3.
JohnBenjamins Publishing.Jean Berko Gleason, Rivka Y Perlmann, and Es-ther Blank Greif.
1984.
What?s the magic word:Learning language through politeness routines.
Dis-course Processes, 7(4):493?502.Noah D Goodman and Andreas Stuhlm?uller.
2013.Knowledge and implicature: Modeling language un-derstanding as social cognition.
Topics in CognitiveScience.H Paul Grice.
1975.
Logic and conversation.
Syntaxand semantics, 3:41?58.Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Andr?e Gr?uning and Andrej A Kibrik.
2005.
Modelingreferential choice in discourse: A cognitive calcu-lative approach and a neural network approach.
InRuslan Mitkov, editor, Anaphora Processing: Lin-guistic, Cognitive and Computational Modelling,pages 163?198.
John Benjamins.Jeanette K Gundel, Nancy Hedberg, and Ron Zacharski.1993.
Cognitive status and the form of referring ex-pressions in discourse.
Language, pages 274?307.Florian T Jaeger.
2010.
Redundancy and reduc-tion: Speakers manage syntactic information density.Cognitive psychology, 61(1):23?62.Gerhard Jager.
2007.
Game dynamics connects seman-tics and pragmatics.
In Ahti-Veikko Pietarinen, edi-tor, Game theory and linguistic meaning, pages 89?102.
Elsevier.Justine T Kao, Jean Wu, Leon Bergen, and Noah DGoodman.
2014.
Nonliteral understanding of num-ber words.
Proceedings of the National Academy ofSciences, 111(33):12002?12007.Andrew Kehler, Laura Kertz, Hannah Rohde, and Jef-frey L Elman.
2008.
Coherence and coreferencerevisited.
Journal of Semantics, 25(1):1?44.Mariya V Khudyakova, Grigory B Dobrov, Andrej AKibrik, and Natalia V Loukachevitch.
2011.
Com-putational modeling of referential choice: Major andminor referential options.
In Proceedings of theCogSci 2011 Workshop on the Production of Refer-ring Expressions.
Boston (July 2011).Emiel Krahmer and Kees Van Deemter.
2012.
Compu-tational generation of referring expressions: A sur-vey.
Computational Linguistics, 38(1):173?218.Roger Levy and T. Florian Jaeger.
2007.
Speakers op-timize information density through syntactic reduc-tion.
In Proceedings of the 20th Conference on Neu-ral Information Processing Systems (NIPS).Brian MacWhinney.
2000.
The CHILDES project:Tools for analyzing talk.Mante S Nieuwland and Jos JA Van Berkum.
2006.When peanuts fall in love: N400 evidence for thepower of discourse.
Journal of Cognitive Neuro-science, 18(7):1098?1111.Ann E Nordmeyer and Michael Frank.
2014.
Apragmatic account of the processing of negative sen-tences.
In Proceedings of the 36th Annual Confer-ence of the Cognitive Science Society.Naho Orita, Eliana Vornov, Naomi H Feldman, and Jor-dan Boyd-Graber.
2014.
Quantifying the role ofdiscourse topicality in speakers?
choices of referringexpressions.
In Association for Computational Lin-guistics, Workshop on Cognitive Modeling and Com-putational Linguistics.Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-genio, and Janet Hitzeman.
2004.
Centering: Aparametric theory and its instantiations.
Computa-tional Linguistics, 30(3):309?363.Marta Recasens, Lluis Marquez, Emili Sapena,M.
Ant`onia Mart?
?, and Mariona Taul?e.
2011.SemEval-2010 task 1 OntoNotes English: Corefer-ence resolution in multiple languages.Jacolien Rij, Hedderik Rijn, and Petra Hendriks.
2013.How WM load influences linguistic processing inadults: A computational model of pronoun inter-pretation in discourse.
Topics in Cognitive Science,5(3):564?580.1648Hannah Rohde, Scott Seyfarth, Brady Clark, GerhardJ?ager, and Stefan Kaufmann.
2012.
Communicat-ing with cost-based implicature: A game-theoreticapproach to ambiguity.
In The 16th Workshop onthe Semantics and Pragmatics of Dialogue, Paris,September.Kenji Sagae, Eric Davis, Alon Lavie, Brian MacWhin-ney, and Shuly Wintner.
2010.
Morphosyntactic an-notation of CHILDES transcripts.
Journal of ChildLanguage, 37(03):705?729.Nathaniel J Smith, Noah Goodman, and Michael Frank.2013.
Learning and using language via recursivepragmatic reasoning about other agents.
In Ad-vances in neural information processing systems,pages 3039?3047.Pontus Stenetorp, Sampo Pyysalo, Goran Topic,Tomoko Ohta, Sophia Ananiadou, and Junichi Tsujii.2012.
brat: a web-based tool for NLP-assisted textannotation.
In Proceedings of the DemonstrationsSession at EACL 2012.Yee Whye Teh, Michael I Jordan, Matthew J Beal, andDavid M Blei.
2006.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associ-ation, 101.Harry Tily and Steven Piantadosi.
2009.
Refer effi-ciently: Use less informative expressions for morepredictable meanings.
In Proceedings of the work-shop on the production of referring expressions:Bridging the gap between computational and empir-ical approaches to reference.Mija Van der Wege.
2009.
Lexical entrainment andlexical differentiation in reference phrase choice.Journal of Memory and Language, 60(4):448?463.Jorrig Vogels, Emiel Krahmer, and Alfons Maes.2013a.
When a stone tries to climb up a slope: theinterplay between lexical and perceptual animacy inreferential choices.
Frontiers in psychology, 4.Jorrig Vogels, Emiel Krahmer, and Alfons Maes.2013b.
Who is where referred to how, and why?
theinfluence of visual saliency on referent accessibilityin spoken language production.
Language and Cog-nitive Processes, 28(9):1323?1349.1649
