Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?82,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsJoint Event Extraction via Structured Prediction with Global FeaturesQi Li Heng Ji Liang HuangDepartments of Computer Science and LinguisticsThe Graduate Center and Queens CollegeCity University of New YorkNew York, NY 10016, USA{liqiearth, hengjicuny, liang.huang.sh}@gmail.comAbstractTraditional approaches to the task of ACEevent extraction usually rely on sequentialpipelines with multiple stages, which suf-fer from error propagation since event trig-gers and arguments are predicted in isola-tion by independent local classifiers.
Bycontrast, we propose a joint frameworkbased on structured prediction which ex-tracts triggers and arguments together sothat the local predictions can be mutu-ally improved.
In addition, we proposeto incorporate global features which ex-plicitly capture the dependencies of multi-ple triggers and arguments.
Experimentalresults show that our joint approach withlocal features outperforms the pipelinedbaseline, and adding global features fur-ther improves the performance signifi-cantly.
Our approach advances state-of-the-art sentence-level event extraction, andeven outperforms previous argument la-beling methods which use external knowl-edge from other sentences and documents.1 IntroductionEvent extraction is an important and challeng-ing task in Information Extraction (IE), whichaims to discover event triggers with specific typesand their arguments.
Most state-of-the-art ap-proaches (Ji and Grishman, 2008; Liao and Gr-ishman, 2010; Hong et al, 2011) use sequentialpipelines as building blocks, which break downthe whole task into separate subtasks, such astrigger identification/classification and argumentidentification/classification.
As a common draw-back of the staged architecture, errors in upstreamcomponent are often compounded and propagatedto the downstream classifiers.
The downstreamcomponents, however, cannot impact earlier deci-sions.
For example, consider the following sen-tences with an ambiguous word ?fired?
:(1) In Baghdad, a cameraman died when anAmerican tank fired on the Palestine Hotel.
(2) He has fired his air defense chief .In sentence (1), ?fired?
is a trigger of type Attack.Because of the ambiguity, a local classifier maymiss it or mislabel it as a trigger of End-Position.However, knowing that ?tank?
is very likely to bean Instrument argument of Attack events, the cor-rect event subtype assignment of ?fired?
is obvi-ously Attack.
Likewise, in sentence (2), ?air de-fense chief?
is a job title, hence the argument clas-sifier is likely to label it as an Entity argument forEnd-Position trigger.In addition, the local classifiers are incapableof capturing inter-dependencies among multipleevent triggers and arguments.
Consider sentence(1) again.
Figure 1 depicts the correspondingevent triggers and arguments.
The dependency be-tween ?fired?
and ?died?
cannot be captured by thelocal classifiers, which may fail to attach ?camera-man?
to ?fired?
as a Target argument.
By usingglobal features, we can propagate the Victim ar-gument of the Die event to the Target argumentof the Attack event.
As another example, know-ing that an Attack event usually only has one At-tacker argument, we could penalize assignmentsin which one trigger has more than one Attacker.Such global features cannot be easily exploited bya local classifier.Therefore, we take a fresh look at this prob-lem and formulate it, for the first time, as a struc-tured learning problem.
We propose a novel jointevent extraction algorithm to predict the triggersand arguments simultaneously, and use the struc-tured perceptron (Collins, 2002) to train the jointmodel.
This way we can capture the dependenciesbetween triggers and argument as well as explore73In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.AttackDieInstrumentPlaceVictimTargetInstrumentTargetPlaceFigure 1: Event mentions of example (1).
There are two event mentions that share three arguments,namely the Die event mention triggered by ?died?, and the Attack event mention triggered by ?fired?.arbitrary global features over multiple local pre-dictions.
However, different from easier tasks suchas part-of-speech tagging or noun phrase chunkingwhere efficient dynamic programming decoding isfeasible, here exact joint inference is intractable.Therefore we employ beam search in decoding,and train the model using the early-update percep-tron variant tailored for beam search (Collins andRoark, 2004; Huang et al, 2012).We make the following contributions:1.
Different from traditional pipeline approach,we present a novel framework for sentence-level event extraction, which predicts triggersand their arguments jointly (Section 3).2.
We develop a rich set of features for eventextraction which yield promising perfor-mance even with the traditional pipeline(Section 3.4.1).
In this paper we refer to themas local features.3.
We introduce various global features to ex-ploit dependencies among multiple triggersand arguments (Section 3.4.2).
Experi-ments show that our approach outperformsthe pipelined approach with the same set oflocal features, and significantly advances thestate-of-the-art with the addition of globalfeatures which brings a notable further im-provement (Section 4).2 Event Extraction TaskIn this paper we focus on the event extraction taskdefined in Automatic Content Extraction (ACE)evaluation.1 The task defines 8 event types and33 subtypes such as Attack, End-Position etc.
Weintroduce the terminology of the ACE event ex-traction that we used in this paper:1http://projects.ldc.upenn.edu/ace/?
Event mention: an occurrence of an eventwith a particular type and subtype.?
Event trigger: the word most clearly ex-presses the event mention.?
Event argument: an entity mention, tempo-ral expression or value (e.g.
Job-Title) thatserves as a participant or attribute with a spe-cific role in an event mention.?
Event mention: an instance that includes oneevent trigger and some arguments that appearwithin the same sentence.Given an English text document, an event ex-traction system should predict event triggers withspecific subtypes and their arguments from eachsentence.
Figure 1 depicts the event triggers andtheir arguments of sentence (1) in Section 1.
Theoutcome of the entire sentence can be considered agraph in which each argument role is representedas a typed edge from a trigger to its argument.In this work, we assume that argument candi-dates such as entities are part of the input to theevent extraction, and can be from either gold stan-dard or IE system output.3 Joint Framework for Event ExtractionBased on the hypothesis that facts are inter-dependent, we propose to use structured percep-tron with inexact search to jointly extract triggersand arguments that co-occur in the same sentence.In this section, we will describe the training anddecoding algorithms for this model.3.1 Structured perceptron with beam searchStructured perceptron is an extension to the stan-dard linear perceptron for structured prediction,which was proposed in (Collins, 2002).
Given asentence instance x ?
X , which in our case is asentence with argument candidates, the structuredperceptron involves the following decoding prob-74lem which finds the best configuration z ?
Y ac-cording to the current model w:z = argmaxy?
?Y(x)w ?
f(x, y?)
(1)where f(x, y?)
represents the feature vector for in-stance x along with configuration y?.The perceptron learns the model w in an on-line fashion.
Let D = {(x(j), y(j))}nj=1 be the setof training instances (with j indexing the currenttraining instance).
In each iteration, the algorithmfinds the best configuration z for x under the cur-rent model (Eq.
1).
If z is incorrect, the weightsare updated as follows:w = w + f(x, y)?
f(x, z) (2)The key step of the training and test is the de-coding procedure, which aims to search for thebest configuration under the current parameters.
Insimpler tasks such as part-of-speech tagging andnoun phrase chunking, efficient dynamic program-ming algorithms can be employed to perform ex-act inference.
Unfortunately, it is intractable toperform the exact search in our framework be-cause: (1) by jointly modeling the trigger labelingand argument labeling, the search space becomesmuch more complex.
(2) we propose to make useof arbitrary global features, which makes it infea-sible to perform exact inference efficiently.To address this problem, we apply beam-searchalong with early-update strategy to perform inex-act decoding.
Collins and Roark (2004) proposedthe early-update idea, and Huang et al (2012) laterproved its convergence and formalized a generalframework which includes it as a special case.
Fig-ure 2 describes the skeleton of perceptron train-ing algorithm with beam search.
In each step ofthe beam search, if the prefix of oracle assign-ment y falls out from the beam, then the top re-sult in the beam is returned for early update.
Onecould also use the standard-update for inference,however, with highly inexact search the standard-update generally does not work very well becauseof ?invalid updates?, i.e., updates that do not fix aviolation (Huang et al, 2012).
In Section 4.5 wewill show that the standard perceptron introducesmany invalid updates especially with smaller beamsizes, also observed by Huang et al (2012).To reduce overfitting, we used averaged param-eters after training to decode test instances in ourexperiments.
The resulting model is called aver-aged perceptron (Collins, 2002).Input: Training set D = {(x(j), y(j))}ni=1,maximum iteration number TOutput: Model parameters w1 Initialization: Set w = 0;2 for t?
1...T do3 foreach (x, y) ?
D do4 z ?
beamSearch (x, y,w)5 if z 6= y then6 w?
w + f(x, y[1:|z|])?
f(x, z)Figure 2: Perceptron training with beam-search (Huang et al, 2012).
Here y[1:i] de-notes the prefix of y that has length i, e.g.,y[1:3] = (y1, y2, y3).3.2 Label setsHere we introduce the label sets for trigger and ar-gument in the model.
We use L ?
{?}
to denotethe trigger label alphabet, where L represents the33 event subtypes, and ?
indicates that the tokenis not a trigger.
Similarly, R ?
{?}
denotes theargument label sets, whereR is the set of possibleargument roles, and ?
means that the argumentcandidate is not an argument for the current trig-ger.
It is worth to note that the set R of each par-ticular event subtype is subject to the entity typeconstraints defined in the official ACE annotationguideline2.
For example, the Attacker argumentfor an Attack event can only be one of PER, ORGand GPE (Geo-political Entity).3.3 DecodingLet x = ?
(x1, x2, ..., xs), E?
denote the sentenceinstance, where xi represents the i-th token in thesentence and E = {ek}mk=1 is the set of argumentcandidates.
We usey = (t1, a1,1, .
.
.
, a1,m, .
.
.
, ts, as,1, .
.
.
, as,m)to denote the corresponding gold standard struc-ture, where ti represents the trigger assignment forthe token xi, and ai,k represents the argument rolelabel for the edge between xi and argument candi-date ek.2http://projects.ldc.upenn.edu/ace/docs/English-Events-Guidelines v5.4.3.pdf75y = (t1, a1,1, a1,2, t2, a2,1, a2,2,| {z }arguments for x2t3, a3,1, a3,2)g(1) g(2) h(2, 1) h(3, 2)Figure 3: Example notation with s = 3,m = 2.For simplicity, throughout this paper we useyg(i) and yh(i,k) to represent ti and ai,k, respec-tively.
Figure 3 demonstrates the notation withs = 3 and m = 2.
The variables for the toy sen-tence ?Jobs founded Apple?
are as follows:x = ?(Jobs,x2?
??
?founded, Apple),E?
??
?
{JobsPER,AppleORG}?y = (?,?,?, Start Org?
??
?t2, Agent, Org?
??
?args for founded,?,?,?
)Figure 4 describes the beam-search procedurewith early-update for event extraction.
Duringeach step with token i, there are two sub-steps:?
Trigger labeling We enumerate all possibletrigger labels for the current token.
The linearmodel defined in Eq.
(1) is used to score eachpartial configuration.
Then the K-best par-tial configurations are selected to the beam,assuming the beam size is K.?
Argument labeling After the trigger label-ing step, we traverse all configurations in thebeam.
Once a trigger label for xi is found inthe beam, the decoder searches through theargument candidates E to label the edges be-tween each argument candidate and the trig-ger.
After labeling each argument candidate,we again score each partial assignment andselect the K-best results to the beam.After the second step, the rank of different triggerassignments can be changed because of the argu-ment edges.
Likewise, the decision on later argu-ment candidates may be affected by earlier argu-ment assignments.The overall time complexity for decoding isO(K ?
s ?m).3.4 FeaturesIn this framework, we define two types of fea-tures, namely local features and global features.We first introduce the definition of local and globalfeatures in this paper, and then describe the im-plementation details later.
Recall that in the lin-ear model defined in Eq.
(1), f(x, y) denotes thefeatures extracted from the input instance x alongInput: Instance x = ?
(x1, x2, ..., xs), E?
andthe oracle output y if for training.K: Beam size.L ?
{?
}: trigger label alphabet.R?
{?
}: argument label alphabet.Output: 1-best prediction z for x1 Set beam B ?
[] /*empty configuration*/2 for i?
1...s do3 buf ?
{z?
?
l | z?
?
B, l ?
L ?
{?
}}B ?K-best(buf )4 if y[1:g(i)] 6?
B then5 return B[0] /*for early-update*/6 for ek ?
E do /*search for arguments*/7 buf ?
?8 for z?
?
B do9 buf ?
buf ?
{z?
?
?
}10 if z?g(i) 6= ?
then /*xi is a trigger*/11 buf ?
buf ?
{z?
?
r | r ?
R}12 B ?K-best(buf )13 if y[1:h(i,k)] 6?
B then14 return B[0] /*for early-update*/15 return B[0]Figure 4: Decoding algorithm for event extrac-tion.
z?l means appending label l to the end ofz.
During test, lines 4-5 & 13-14 are omitted.with configuration y.
In general, each feature in-stance f in f is a function f : X ?
Y ?
R, whichmaps x and y to a feature value.
Local features areonly related to predictions on individual trigger orargument.
In the case of unigram tagging for trig-ger labeling, each local feature takes the form off(x, i, yg(i)), where i denotes the index of the cur-rent token, and yg(i) is its trigger label.
In practice,it is convenient to define the local feature functionas an indicator function, for example:f1(x, i, yg(i)) ={1 if yg(i) = Attack and xi = ?fire?0 otherwiseThe global features, by contrast, involve longerrange of the output structure.
Formally,each global feature function takes the form off(x, i, k, y), where i and k denote the indicesof the current token and argument candidate indecoding, respectively.
The following indicatorfunction is a simple example of global features:f101(x, i, k, y) =????
?1 if yg(i) = Attack andy has only one ?Attacker?0 otherwise76Category Type Feature DescriptionTriggerLexical1.
unigrams/bigrams of the current and context words within the window of size 22. unigrams/bigrams of part-of-speech tags of the current and context words within thewindow of size 23. lemma and synonyms of the current token4.
base form of the current token extracted from Nomlex (Macleod et al, 1998)5.
Brown clusters that are learned from ACE English corpus (Brown et al, 1992; Miller etal., 2004; Sun et al, 2011).
We used the clusters with prefixes of length 13, 16 and 20 foreach token.Syntactic6.
dependent and governor words of the current token7.
dependency types associated the current token8.
whether the current token is a modifier of job title9.
whether the current token is a non-referential pronounEntityInformation10.
unigrams/bigrams normalized by entity types11.
dependency features normalized by entity types12.
nearest entity type and string in the sentence/clauseArgumentBasic1.
context words of the entity mention2.
trigger word and subtype3.
entity type, subtype and entity role if it is a geo-political entity mention4.
entity mention head, and head of any other name mention from co-reference chain5.
lexical distance between the argument candidate and the trigger6.
the relative position between the argument candidate and the trigger: {before, after,overlap, or separated by punctuation}7. whether it is the nearest argument candidate with the same type8.
whether it is the only mention of the same entity type in the sentenceSyntactic9.
dependency path between the argument candidate and the trigger10.
path from the argument candidate and the trigger in constituent parse tree11.
length of the path between the argument candidate and the trigger in dependency graph12.
common root node and its depth of the argument candidate and parse tree13.
whether the argument candidate and the trigger appear in the same clauseTable 1: Local features.3.4.1 Local featuresIn general there are two kinds of local features:Trigger features The local feature func-tion for trigger labeling can be factorized asf(x, i, yg(i)) = p(x, i) ?
q(yg(i)), where p(x, i) isa predicate about the input, which we call text fea-ture, and q(yg(i)) is a predicate on the trigger label.In practice, we define two versions of q(yg(i)):q0(yg(i)) = yg(i) (event subtype)q1(yg(i)) = event type of yg(i)q1(yg(i)) is a backoff version of the standard un-igram feature.
Some text features for the sameevent type may share a certain distributional sim-ilarity regardless of the subtypes.
For example,if the nearest entity mention is ?Company?, thecurrent token is likely to be Personnel no matterwhether it is End-Postion or Start-Position.Argument features Similarly, the local fea-ture function for argument labeling can be rep-resented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ?q(yg(i), yh(i,k)), where yh(i,k) denotes the argu-ment assignment for the edge between triggerword i and argument candidate ek.
We define twoversions of q(yg(i), yh(i,k)):q0(yg(i), yh(i,k)) =????
?yh(i,k) if yh(i,k) is Place,Time or Noneyg(i) ?
yh(i,k) otherwiseq1(yg(i), yh(i,k)) ={1 if yh(i,k) 6=None0 otherwiseIt is notable that Place and Time arguments areapplicable and behave similarly to all event sub-types.
Therefore features for these arguments arenot conjuncted with trigger labels.
q1(yh(i,k)) canbe considered as a backoff version of q0(yh(i,k)),which does not discriminate different argumentroles but only focuses on argument identification.Table 1 summarizes the text features about the in-put for trigger and argument labeling.
In our ex-periments, we used the Stanford parser (De Marn-effe et al, 2006) to create dependency parses.3.4.2 Global featuresTable 2 summarizes the 8 types of global featureswe developed in this work.
They can be roughlydivided into the following two categories:77Category Feature DescriptionTrigger1.
bigram of trigger types occur in the same sentence or the same clause2.
binary feature indicating whether synonyms in the same sentence have the same trigger label3.
context and dependency paths between two triggers conjuncted with their typesArgument4.
context and dependency features about two argument candidates which share the same role within thesame event mention5.
features about one argument candidate which plays as arguments in two event mentions in the samesentence6.
features about two arguments of an event mention which are overlapping7.
the number of arguments with each role type of an event mention conjuncted with the event subtype8.
the pairs of time arguments within an event mention conjuncted with the event subtypeTable 2: Global features.Transport(transport)Entity(women)Entity(children)Artifact Artifactconj and(a)Entity(cameramen)Die(died)Attack(fired)VictimTargetadvcl(b)End-Position(resigned)Entity Entity[co-chief executive of [Vivendi Universal Entertainment]]Position EntityOverlapping(c)Figure 5: Illustration of global features (4-6) in Table 2.Event ProbabilityAttack 0.34Die 0.14Transport 0.08Injure 0.04Meet 0.02Table 3: Top 5 event subtypes that co-occur withAttack event in the same sentence.Trigger global feature This type of featurecaptures the dependencies between two triggerswithin the same sentence.
For instance: feature (1)captures the co-occurrence of trigger types.
Thiskind of feature is motivated by the fact that twoevent mentions in the same sentence tend to be se-mantically coherent.
As an example, from Table 3we can see that Attack event often co-occur withDie event in the same sentence, but rarely co-occurwith Start-Position event.
Feature (2) encouragessynonyms or identical tokens to have the same la-bel.
Feature (3) exploits the lexical and syntacticrelation between two triggers.
A simple exampleis whether an Attack trigger and a Die trigger arelinked by the dependency relation conj and.Argument global feature This type of featureis defined over multiple arguments for the sameor different triggers.
Consider the following sen-tence:(3) Trains running to southern Sudan were usedto transport abducted women and children.The Transport event mention ?transport?
hastwo Artifact arguments, ?women?
and ?chil-dren?.
The dependency edge conj and be-tween ?women?
and ?children?
indicates thatthey should play the same role in the event men-tion.
The triangle structure in Figure 5(a) is an ex-ample of feature (4) for the above example.
Thisfeature encourages entities that are linked by de-pendency relation conj and to play the same roleArtifact in any Transport event.Similarly, Figure 5(b) depicts an example offeature (5) for sentence (1) in Section 1.
In this ex-ample, an entity mention is Victim argument to Dieevent and Target argument to Attack event, and thetwo event triggers are connected by the typed de-pendency advcl.
Here advcl means that the word?fired?
is an adverbial clause modier of ?died?.Figure 5(c) shows an example of feature (6) forthe following sentence:(4) Barry Diller resigned as co-chief executive ofVivendi Universal Entertainment.The job title ?co-chief executive of Vivendi Uni-versal Entertainment?
overlaps with the Orga-nization mention ?Vivendi Universal Entertain-ment?.
The feature in the triangle shape can beconsidered as a soft constraint such that if a Job-Title mention is a Position argument to an End-Position trigger, then the Organization mention78which appears at the end of it should be labeledas Entity argument for the same trigger.Feature (7-8) are based on the statistics aboutdifferent arguments for the same trigger.
For in-stance, in many cases, a trigger can only have onePlace argument.
If a partial configuration mis-takenly classifies more than one entity mention asPlace arguments for the same trigger, then it willbe penalized.4 Experiments4.1 Data set and evaluation metricWe utilized the ACE 2005 corpus as our testbed.For comparison, we used the same test set with 40newswire articles (672 sentences) as in (Ji and Gr-ishman, 2008; Liao and Grishman, 2010) for theexperiments, and randomly selected 30 other doc-uments (863 sentences) from different genres asthe development set.
The rest 529 documents (14,840 sentences) are used for training.Following previous work (Ji and Grishman,2008; Liao and Grishman, 2010; Hong et al,2011), we use the following criteria to determinethe correctness of an predicted event mention:?
A trigger is correct if its event subtype andoffsets match those of a reference trigger.?
An argument is correctly identified if its eventsubtype and offsets match those of any of thereference argument mentions.?
An argument is correctly identified and clas-sified if its event subtype, offsets and argu-ment role match those of any of the referenceargument mentions.Finally we use Precision (P), Recall (R) and F-measure (F1) to evaluate the overall performance.4.2 Baseline systemChen and Ng (2012) have proven that perform-ing identification and classification in one step isbetter than two steps.
To compare our proposedmethod with the previous pipelined approaches,we implemented two Maximum Entropy (Max-Ent) classifiers for trigger labeling and argumentlabeling respectively.
To make a fair comparison,the feature sets in the baseline are identical to thelocal text features we developed in our framework(see Figure 1).4.3 Training curvesWe use the harmonic mean of the trigger?s F1measure and argument?s F1 measure to measurethe performance on the development set.1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21# of training iteration0.440.460.480.500.520.540.560.580.60Harmonicmeanlocal+globallocalFigure 6: Training curves on dev set.Figure 6 shows the training curves of the aver-aged perceptron with respect to the performanceon the development set when the beam size is 4.As we can see both curves converge around itera-tion 20 and the global features improve the over-all performance, compared to its counterpart withonly local features.
Therefore we set the numberof iterations as 20 in the remaining experiments.4.4 Impact of beam sizeThe beam size is an important hyper parameter inboth training and test.
Larger beam size will in-crease the computational cost while smaller beamsize may reduce the performance.
Table 4 showsthe performance on the development set with sev-eral different beam sizes.
When beam size = 4, thealgorithm achieved the highest performance on thedevelopment set with trigger F1 = 67.9, argumentF1 = 51.5, and harmonic mean = 58.6.
Whenthe size is increased to 32, the accuracy was notimproved.
Based on this observation, we chosebeam size as 4 for the remaining experiments.4.5 Early-update vs. standard-updateHuang et al (2012) define ?invalid update?
to bean update that does not fix a violation (and insteadreinforces the error), and show that it strongly(anti-)correlates with search quality and learningquality.
Figure 7 depicts the percentage of in-valid updates in standard-update with and with-out global features, respectively.
With global fea-tures, there are numerous invalid updates when the79Beam size 1 2 4 8 16 32Training time (sec) 993 2,034 3,982 8,036 15,878 33,026Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8Table 4: Comparison of training time and accuracy on the dev set.1 2 4 8 16 32beam size0.000.050.100.150.200.250.300.350.400.45% ofinvalid updateslocal+globallocalFigure 7: Percentage of the so-called ?invalid up-dates?
(Huang et al, 2012) in standard perceptron.Strategy F1 on Dev F1 on TestTrigger Arg Trigger ArgStandard (b = 1) 68.3 47.4 64.4 49.8Early (b = 1) 68.9 49.5 65.2 52.1Standard (b = 4) 68.4 50.5 67.1 51.4Early (b = 4) 67.9 51.5 67.5 52.7Table 5: Comparison between the performance(%) of standard-update and early-update withglobal features.
Here b stands for beam size.beam size is small.
The ratio decreases mono-tonically as beam size increases.
The model withonly local features made much smaller numbersof invalid updates, which suggests that the use ofglobal features makes the search problem muchharder.
This observation justify the application ofearly-update in this work.
To further investigatethe difference between early-update and standard-update, we tested the performance of both strate-gies, which is summarized in Table 5.
As we cansee the performance of standard-update is gener-ally worse than early-update.
When the beam sizeis increased (b = 4), the gap becomes smaller asthe ratio of invalid updates is reduced.4.6 Overall performanceTable 6 shows the overall performance on the blindtest set.
In addition to our baseline, we compareagainst the sentence-level system reported in Honget al (2011), which, to the best of our knowledge,is the best-reported system in the literature basedon gold standard argument candidates.
The pro-posed joint framework with local features achievescomparable performance for triggers and outper-forms the staged baseline especially on arguments.By adding global features, the overall performanceis further improved significantly.
Compared tothe staged baseline, it gains 1.6% improvementon trigger?s F-measure and 8.8% improvement onargument?s F-measure.
Remarkably, compared tothe cross-entity approach reported in (Hong et al,2011), which attained 68.3% F1 for triggers and48.3% for arguments, our approach with globalfeatures achieves even better performance on ar-gument labeling although we only used sentence-level information.We also tested the performance with argumentcandidates automatically extracted by a high-performing name tagger (Li et al, 2012b) and anIE system (Grishman et al, 2005).
The resultsare summarized in Table 7.
The joint approachwith global features significantly outperforms thebaseline and the model with only local features.We also show that it outperforms the sentence-level baseline reported in (Ji and Grishman, 2008;Liao and Grishman, 2010), both of which at-tained 59.7% F1 for triggers and 36.6% for argu-ments.
Our approach aims to tackle the problem ofsentence-level event extraction, thereby only usedintra-sentential evidence.
Nevertheless, the perfor-mance of our approach is still comparable with thebest-reported methods based on cross-documentand cross-event inference (Ji and Grishman, 2008;Liao and Grishman, 2010).5 Related WorkMost recent studies about ACE event extractionrely on staged pipeline which consists of separatelocal classifiers for trigger labeling and argumentlabeling (Grishman et al, 2005; Ahn, 2006; Ji andGrishman, 2008; Chen and Ji, 2009; Liao and Gr-ishman, 2010; Hong et al, 2011; Li et al, 2012a;Chen and Ng, 2012).
To the best of our knowl-edge, our work is the first attempt to jointly modelthese two ACE event subtasks.80MethodsTriggerIdentification (%)Trigger Identification+ classification (%)ArgumentIdentification (%) Argument Role (%)P R F1 P R F1 P R F1 P R F1Sentence-level in Hong et al (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7Cross-entity in Hong et al (2011)?
N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3Table 6: Overall performance with gold-standard entities, timex, and values.
?beyond sentence level.Methods Trigger F1 Arg F1Ji and Grishman (2008)cross-doc Inference67.3 42.6Ji and Grishman (2008)sentence-level59.7 36.6MaxEnt classifiers 64.7 (?1.2) 33.7 (?10.2)Joint w/ local 63.7 (?2.0) 35.8 (?10.7)Joint w/ local + global 65.6 (?1.9) 41.8 (?10.9)Table 7: Overall performance (%) with predictedentities, timex, and values.
?
indicates the perfor-mance drop from experiments with gold-standardargument candidates (see Table 6).For the Message Understanding Conference(MUC) and FAS Program for Monitoring Emerg-ing Diseases (ProMED) event extraction tasks,Patwardhan and Riloff (2009) proposed a proba-bilistic framework to extract event role fillers con-ditioned on the sentential event occurrence.
Be-sides having different task definitions, the keydifference from our approach is that their rolefiller recognizer and sentential event recognizerare trained independently but combined in the teststage.
Our experiments, however, have demon-strated that it is more advantageous to do bothtraining and testing with joint inference.There has been some previous work on jointmodeling for biomedical events (Riedel and Mc-Callum, 2011a; Riedel et al, 2009; McClosky etal., 2011; Riedel and McCallum, 2011b).
(Mc-Closky et al, 2011) is most closely related to ourapproach.
They casted the problem of biomedi-cal event extraction as a dependency parsing prob-lem.
The key assumption that event structure canbe considered as trees is incompatible with ACEevent extraction.
In addition, they used a separateclassifier to predict the event triggers before ap-plying the parser, while we extract the triggers andargument jointly.
Finally, the features in the parserare edge-factorized.
To exploit global features,they applied a MaxEnt-based global re-ranker.
Incomparison, our approach is a unified frameworkbased on beam search, which allows us to exploitarbitrary global features efficiently.6 Conclusions and Future WorkWe presented a joint framework for ACE event ex-traction based on structured perceptron with inex-act search.
As opposed to traditional pipelinedapproaches, we re-defined the task as a struc-tured prediction problem.
The experiments provedthat the perceptron with local features outperformsthe staged baseline and the global features furtherimprove the performance significantly, surpassingthe current state-of-the-art by a large margin.As shown in Table 7, the overall performancedrops substantially when using predicted argu-ment candidates.
To improve the accuracy of end-to-end IE system, we plan to develop a completejoint framework to recognize entities together withevent mentions for future work.
Also we are inter-ested in applying this framework to other IE taskssuch as relation extraction.AcknowledgmentsThis work was supported by the U.S. Army Re-search Laboratory under Cooperative AgreementNo.
W911NF-09-2-0053 (NS-CTA), U.S. NSFCAREER Award under Grant IIS-0953149,U.S.
NSF EAGER Award under Grant No.
IIS-1144111, U.S. DARPA Award No.
FA8750-13-2-0041 in the ?Deep Exploration and Filtering ofText?
(DEFT) Program, a CUNY Junior FacultyAward, and Queens College equipment funds.
Theviews and conclusions contained in this documentare those of the authors and should not be inter-preted as representing the official policies, eitherexpressed or implied, of the U.S. Government.The U.S. Government is authorized to reproduceand distribute reprints for Government purposesnotwithstanding any copyright notation here on.81ReferencesDavid Ahn.
2006.
The stages of event extraction.In Proceedings of the Workshop on Annotating andReasoning about Time and Events, pages 1?8.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Zheng Chen and Heng Ji.
2009.
Language specificissue and feature exploration in chinese event ex-traction.
In Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Companion Volume: Short Pa-pers, pages 209?212.Chen Chen and Vincent Ng.
2012.
Joint modeling forchinese event extraction with rich linguistic features.In COLING, pages 529?544.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 111.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC, volume 6, pages 449?454.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
Nyu?s english ace 2005 system description.In Proceedings of ACE 2005 Evaluation Workshop.Washington.Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,Guodong Zhou, and Qiaoming Zhu.
2011.
Usingcross-entity inference to improve event extraction.In Proceedings of ACL, pages 1127?1136.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through cross-document inference.
In Pro-ceedings of ACL, pages 254?262.Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-bin Hou.
2012a.
Employing compositional seman-tics and discourse consistency in chinese event ex-traction.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 1006?1016.Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, andFei Huang.
2012b.
Joint bilingual name tagging forparallel corpora.
In Proceedings of the 21st ACMinternational conference on Information and knowl-edge management, pages 1727?1731.Shasha Liao and Ralph Grishman.
2010.
Using doc-ument level cross-event inference to improve eventextraction.
In Proceedings of ACL, pages 789?797.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
Nomlex: Alexicon of nominalizations.
In Proceedings of EU-RALEX, volume 98, pages 187?193.David McClosky, Mihai Surdeanu, and Christopher D.Manning.
2011.
Event extraction as dependencyparsing.
In Proceedings of ACL, pages 1626?1635.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In Proceedings of HLT-NAACL,volume 4, pages 337?342.Siddharth Patwardhan and Ellen Riloff.
2009.
A uni-fied model of phrasal and sentential evidence for in-formation extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing: Volume 1-Volume 1, pages 151?160.Sebastian Riedel and Andrew McCallum.
2011a.
Fastand robust joint models for biomedical event extrac-tion.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 1?12.Sebastian Riedel and Andrew McCallum.
2011b.
Ro-bust biomedical event extraction with dual decom-position and minimal domain adaptation.
In Pro-ceedings of the BioNLP Shared Task 2011 Work-shop, pages 46?50.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In Proceedingsof the Workshop on Current Trends in BiomedicalNatural Language Processing: Shared Task, pages41?49.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised relation extraction with large-scaleword clustering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages521?529.82
