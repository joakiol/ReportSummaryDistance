Towards Understanding Text with a Very Large VocabularyDamaris Ayuso, R. Bobrow, Dawn MacLaughlin, Marie Meteer,Lance Ramshaw, Rich Schwartz, Ralph WeischedelBBN Systems and Technologies Corporation10 Moulton St.Cambridge, MA 021381.
IntroductionIn order to meet he information processing demands ofthe next decade, natural language systems must have thecapability of processing very large amounts of text,commonly called "messages", from highly diverse sourceswritten in any of a few dozen languages.
One of the keyissues in building systems with this scale of competence ishandling large numbers of different words and word senses.Natural anguage understanding systems today are typicallylimited to vocabularies of less than 10,000 words;tomorrow's ystems will need vocabularies at least 5 timesthat to effectively handle the volume and diversity ofmessages needing to be processed.One method of handling large vocabularies i simplyincreasing the size of the lexicon.
Research efforts at IBM\[Chodorow, et al 1988; Neff, et al 1989\], Bell Labs\[Church, et al 1989\], New Mexico State University \[Wilks1987\], and elsewhere have used mechanical processing ofon-line dictionaries to infer at least minimal syntactic andsemantic information from dictionary definitions.
However,even assuming a very large lexicon already exists, it cannever be complete.
Systems aiming for coverage ofunrestricted language in broad domains must continually dealwith new words and novel word senses.Systems with very large lexicons have the additionalproblems of an exploding search space, of disambiguatingmultiple syntactic and semantic possibilities when fullinterpretations are possible, and of combining partialinterpretations into something meaningful when a fullinterpretation is not found.
For instance, in The Wall StreetJournal, the average sentence l ngth is 21 words, more thantwice the average sentence l ngth of the corpus for the AirTravel Information System used in spoken language systemsresearch.
If the worst case complexity of a parser is n 3, thenthe search space can be eight times worse than in spokenlanguage interfaces.A key element of our approach to these problems is theuse of probabilistic models to control the greatly increasedsearch space inherent in large vocabularies.
We haveobserved that the state of the art in natural anguageprocessing (NLP) today is analogous to that in speechprocessing roughly prior to 1980, when purely knowledge-based approaches required much detailed, hand-craftedknowledge from several sources (e.g., acoustic, phonetic,etc.).
Speech systems then, like NLP systems today, werebrittle, required much hand-crafting, were limited inaccuracy, and were not scalable.
A revolution in speechtechnology has occurred since 1980, when probabilisticmodels were incorporated into the control structure forcombining multiple sources of knowledge (providingimproved accuracy and increased scalability) and asalgorithms for training the system on large bodies("corpora") of data were applied (providing reduced cost inmoving the technology toa new application domain).We are exploring the use of probabilistic models andtraining in NLP in a new pilot study, whose overall goal isto increase the robustness, precision, and scalability ofnatural language understanding systems.
In the initial phaseof the study, we are addressing issues raised by the hugevocabularies in ope n texts.
We are experimenting with avariety of techniques for disambiguating word uses, selectingsyntactic interpretations, and acquiring information aboutnew words--techniques that can be applied both when a wordis initially encountered and in handling the word moreeffectively the next ime it is encountered:This paper eports the results of the first three months ofthis new effort.
We have applied techniques from speechprocessing, such as "tri-tag" models and probability modelson context-free grammars.
We report on our initialexperiments in using tri-tag models for hypothesizing partsof speech, as well as new results on the size of the corpusneeded for training these models, and their use in processingunknown words.
We discuss our use of a context-freeprobabilistic language model to help in selecting the correctparse from among multiple parses.
Finally, we present apreliminary approach to the problem of learning the lexicalsyntax of new words in context and using our probabilisticlanguage model to aid in selecting the interpretation tolearnfrom.3542.
Probabilistic Part of SpeechModelsOne straightforward way to use probabilities is inassigning parts of speech to words.
Models predicting partof speech can serve to cut down the search space a parsermust consider in processing known words and can be used asone input to more complex strategies for inferring lexicaland semantic information about unknown words.
We haveexplored the use of such models in both contexts.Simple but powerful models to predict part of speech canbe derived using a corpus that has been tagged (or labelled)as to part of speech [Church 1988; de Marken 19901.
Usinga tagged corpus to train the model is called "supervisedtraining", since a human has prepared the correct trainingdata.
This is in contrast to "unsupervised training" wherethe process is fully automated.
For example, inunsupervised part of speech tagging, one can use a corpuswithout annotation for training, a dictionary that lists partsof speech for the most frequently occurring words, and aninitial probability assignment, e.g., a uniform probabilitydistribution or probability estimates from a previous, relateddomain.
An iterative procedure then revises the probabilityestimates so as to maximize the probability over the wholecorpus.Our supervised training experiments used a tri-tag modelbased on a corpus from the University of Pennsylvaniaconsisting of Wall Street Journal articles in which each wordor punctuation mark has been tagged with one of 47 parts ofspeech, as shown in the following example:A tri-tag model predicts the relative likelihood of aparticular tag given the two preceding tags, e.g.
how likelyis the tag RB on the third word in the above example, giventhat the two previous words were tagged NNS and VBD.Using the UPenn corpus, we counted for each possible pairof tags, the number of times that the pair was followed byeach possible third tag, and then derived from those counts aprobabilistic tri-tag model.
We also estimated from thetraining data the conditional probability of each particularword given a known tag (e.g., how likely is the work to be"terms" if the tag is NNS); this is called the "word emit"probability.
Both of these probability estimates usedpaddingto an arbitrary estimate to avoid setting the probability forunseen tri-tags or unseen word senses to zero.Given these probabilities, one can then predict themaximum-likelihood tag sequence for a given wordsequence.
Using the tri-tag probabilities, we computed theprobabilities of all possible paths in the tag space throughthe sentence, selected the path whose overall probability washighest, and then took the tag predictions from that path.We replicated the result [Church 19881 that this process isable to predict the parts of speech with only a 3-5% errorrate when the possible parts of speech of the words areknown.
We believe that this error rate could be reduced stillfurther and extend the success to unknown words.Using the UPenn set of parts of speech, unknown wordscan be in any of the 22 open-class parts of speech.
The tri-tag model can be used to estimate the most probable one.While random choice among the 22 open classes would beexpected to show an error rate for new words of 91.5%, ourinitial results using the model showed an error rate of only51.6%.
The best previously reported error rate was 75%[Kuhn & de Mori 19901.
Note that the error rate should bereduced even further by using more knowledge, such ascapitalization knowledge and morphology.While supervised training is shown here to be veryeffective, it requires a correctly tagged corpus.
We have donesome experiments to quantify how much tagged data isreally necessary, and to suggest ways to handle new wordswhen using such models.In these experiments, we demonstrated that the training setcan, in fact, be much smaller than might have beenexpected.
One rule of thumb suggests that the training setneeds to be large enough to contain 10 instances of eachtype of tag sequence in order for their probabilities to beestimated with reasonable accuracy.
This would imply thata tri-tag model using 47 possible parts of speech would needa bit more than a million words of training.
However, wefound that much less training data was necessary, asillustrated in Figure 1.64K 250K 500K 750K 1MSIZE OF TRAINING SETFigure 1: Size of Tri-tag Training SetsIn our experiments, the error rate for a supervised tri-tagmodel increased only from 3.30% to 3.87% when the sizeof the training set was reduced from 1 million words to64,000 words.
This is probably because most of thepossible tri-tag sequences never actually appear.
All that isreally necessary, recalling the rule of thumb, is enoughtraining to allow for 10 of each of the tag sequences that dooccur.
There were 16,170 unique mples in our training set,so the rule of thumb would suggest that 160,000 wordswould be sufficient training.
This would explain why thedegradation in performance was slight when the size of thecorpus was reduced.
The benefits of probabilistic modelingtherefore seem applicable to new tag sets, subdomains, orlanguages without needing prohibitively large corpora.3.
Probabilistic Language ModelProbabilities can also quantify the likelihoods ofalternative complete interpretations of a sentence.
In theseexperiments, we used the grammar of the Delphicomponent from BBN's HARC system \[Stallard 1989\],which combines syntax and semantics in a unificationformalism.
We developed a context-free model, whichestimates the probability of each rule in the grammarindependently (incontrast to a context-sensitive model, suchas the tri-tag model described above, which bases theprobability of a tag on what other tags are in the adjacentcontext).In our context-free model, we associate a probability witheach rule of the grammar.
For each distinct major category(left-hand side) of the grammar, there is a set of context-freerulesLHS <- RHS 1LHS <- RHS2LHS <- RHSn.For each rule, we estimate the probability of the right-handside given the left-hand side.The probability of a syntactic structure S, given the inputstring W, is then modelled by the product of theprobabilities of the rules used in S. (\[Chitrao & Grishman1990\] used a similar context-free model.)
Using this model,we explored the following issues:What method of training the rule probabilities shouldbe employed?How much (little) training data is required for reliableestimates??
How is system performance impacted??
Do the results uggest refinements in the probabilitymodel?Our intention is to use the Treebank corpus beingdeveloped at the University of Pennsylvania as a source ofcorrect structures for training.
However, until that materialbecomes available, we have run initial experiments usingsmall training sets taken from an existing question-answering corpus of sentences about a personnel database.To our surprise, we found that as little as 100 sentences ofsupervised training (in which a person, using graphicaltools, identifies the correct parse) is sufficient o improvethe ranking of the interpretations found.
In our tests, theNLP system produces all interpretations satisfying allsyntactic and semantic onstraints.
From that set, theintended interpretation must be chosen.
The context-freeprobability model reduced the error rate on an independenttest set by a factor of two to four, compared to randomselection from the interpretations satisfying all knowledge-based constraints.We tested the predictive power of rule probabilities usingthis model both in unsupervised and in supervised mode.
Inthe former case, the input is all parse trees (whether corrector not) for the sentences in the training set.
In the lattercase, the training data included a specification of the correctparse as hand picked by the grammar's author from amongthe parse trees produced by the system.The detailed results from using a training set of 81sentences appear in the histogram in Figure 2.3012?
"-~10-0 UNSUPERVISED.
- .
- .
.
.
.SUPERVISEDTEST 1 TEST 2 TEST 3\ [ ' "7  Best Possible\ [~\ ]  ChanceTestFigure 2: Predictions of Probabilistic Language ModelThe "best possible" error rates for each test indicates thepercentage of cases for which none of the interpretationsproduced by the system was judged correct, so that noselection scheme could achieve a lower error rate than that.The "chance" score gives the error rate that would beexpected with random selection from all interpretationsproduced.
The "test" column shows the error rate with thesupervised or unsupervised probability model in question.The first supervised test had an 81.4% improvement, and thesecond a 50.8% improvement, and the third a 56%improvement.
These results state how much better thanchance the given model did as a percentage of the maximumpossible improvement.We expect to improve the model's performance byrecording probabilities for other features in addition to justthe set of rules involved in producing them.
For example,in the grammar used for this test, two different attachmentsfor a prepositional phrase produced trees with the same setof rules, but differing in shape.
Thus the simple, context-free model based on the product of rule probabilities couldnot capture preferences concerning such attachment.
Byadding to the model probabilities for such additional features,we expect that the power of the probabilisfic model to356automatically select the correct parse can be substantiallyincreased.4.
Learning Lexical SyntaxOne purpose for probabilistic models is to contribute tohandling new words or partially understood sentences.
Wehave done preliminary experiments that show that there ispromise in learning lexical syntactic and semantic featuresfrom context when probabilistic tools are used to helpcontrol the ambiguity.In our experiments, we used a corpus of sentences eachwith one word that the system did not know.
To create thecorpus, we began with a corpus of sentences known to parsefrom the personnel question-answering domain (our goal,again, is to use the Treebank data from the University ofPennsylvania for such training when it becomes available).We then replaced one word in each sentence with anundefmed word.For example, in the following sentence, the word"contact" is undefined in the system: Who in Division Fouris the contact for MIT?
That word has both a noun and averb part of speech; however, the pattern of parts of speechof the words surrounding "contact" causes the tri-tag modelto return a high probability that the word is a noun.
Usingunification variables for all possible features of a noun, theparser produces multiple parses.
Applying the context-freerule probabilities to select the most probable of the resultingparses allows the system to conclude both syntactic andsemantic facts about "contact".
Syntactically, the systemdiscovers that it is a count noun, with third person singularagreement.
Semantically, the system learns (from the use ofwho) that contact is in the semantic class PERSONS.Furthermore, the partially-specified semantic representa-tion for the sentence as a whole also shows the semanticrelation to SCHOOLS, which is expressed here by the forphrase.
Thus, even a single use of an unknown word incontext can supply useful data about its syntactic andsemantic features.Probalistic modelling plays a key role in this process.While context sensitive techniques for inferring lexicalfeatures can contribute a great deal, they can still leavesubstantial ambiguity.
As a simple example, suppose theword "list" is undefined in the sentence "List theemployees."
The tri-tag model predicts both a noun and averb part of speech in that position.
Using an underspecifiednoun sense combined with the usual definitions for the restof the words yields no parses.
However, an underspecifiedverb sense yields three parses, differing in thesubcategorization frame of the verb "list".
For morecomplex sentences, even with this very limited protocol, thenumber of parses for the appropriate word sense can reachinto the hundreds.Using the rule probabilities acquired through supervisedtraining (described in the previous section), the likelihood ofthe ambiguous interpretations resulting from a sentence withan unknown word was computed.
Then we tested whetherthe tree ranked most highly matched the tree previouslyselected by a person as the correct one.
This tree equivalencetest was based on the trees' smcture and on the rule appliedat each node; while an underspecified tree might have someless-specified feature values than the chosen fully-specifiedtree, it would still be equivalent in the sense above.Of 160 inputs with an unknown word, in 130 cases themost likely tree matched the correct one, for an error rate of18.75%, while picking at random would have resulted in anerror rate of 63.14%, for an improvement by better than afactor of 3.
This suggests that probabilistic modeling canbe a powerful tool for controlling the high degree ofambiguity in efforts to automatically acquire lexical data.We have also begun to explore heuristics for combininglexical data for a single word acquired from a number ofpartial parses.
There are some cases in which the bestapproach is to unify the two learned sets of lexical features,so that the derived sense becomes the sum of theinformation learned from the two examples.
For instance,the verb subcategorization information learned from oneexample could be thus combined with agreementinformation learned from another.
On the other hand, thereare many cases, including alternative subcategorizationframes, where each of the encountered options needs to beincluded as separate alternatives.5.
ConclusionsIn trying to address the problems inherent in understandingtext using very large vocabularies, we found that the use ofprobabilistic models was crucial in obtaining useful results.The three main problems addressed by this paper were (1)reducing ambiguity resulting from multiple parts of speech,(2) reducing parse ambiguity, and (3) learning lexicalinformation of new words encountered in the text.Using supervised training for tri-tag probabilities, weachieved a 3-5% error rate on a test set in picking the correctpart of speech.
Our experiments showed that a smallertraining set than previously expected (64,000 words ratherthan 1 million) was needed in order to achieve a good levelof performance.For reducing interpretation ambiguity, our context-freeprobability model, trained in supervised mode on only 81sentences, was able to reduce the error rate for selecting thecorrect parse on independent test sets by a factor of 2-4.For the problem of processing new words in the text, thetri-tag model reduced the error rate for picking the correctpart of speech for such words from 91.5% to 51.6%.
Andonce the possible parts of speech for a word are known (orhypothesized using the tri-tag model), the probabilisticlanguage model proved useful in indicating which parses(obtained using the unknown word) should be looked at forlearning more complex lexical information about he word.6.
Future WorkWe plan to explore ways in which to reduce the error ratesresulting from our current models.
For example, thepotential of using a weighted combination of n-tag modelsfor a range of n, as opposed to a single tri-tag model, can bestudied.
We also plan to use a more complex probabilisticmodel of grammar, one that more realistically represents hebiases in language, for example, by using conditionalprobabilities relying on more than just one level of context-free rules.In a different direction, we plan to explore automaticmethods for learning semantic information.
We will explorethe use of the Common Facts Database (CFDB) \[Crowther1989\], which was derived from an on-line dictionary with abase vocabulary of 65K words.
The CFDB would be usefulin assigning semantic lasses to noun phrases, for example,as well as in providing information on the classes of verbarguments.AcknowledgementsThe work reported here was supported by the AdvancedResearch Projects Agency and was monitored by the RomeAir Development Center under Contract No.
F30602-87-D-0093.
The views and conclusions contained in thisdocument are those of the authors and should not beinterpreted as necessarily representing the official policies,either expressed or implied, of the Defense AdvancedResearch Projects Agency or the United States Government.Church, K., Gale, W.A., Hanks, P., and Hindle, D. Parsing,Word Associations and Typical Predicate-ArgumentRelations.
Proceedings of the Speech and Natural LanguageWorkshop Oct. 1989, 75-81.Crowther, W. A Common Facts Database.
Proceedings ofthe Speech and Natural Language Workshop Feb. 1989, 89-93.de Marcken, C.G.
Parsing the LOB Corpus.
Proceedings ofthe 28th Annual Meeting of the Association forComputational Linguistics 1990, 243-251.Kuhn, R., and De Mori, R. A Cache-Based NaturalLanguage Model for Speech Recognition.
IEEETransactions on Pattern Analysis and Machine Intelligence12 (1990), 570-583.Neff, M.S., and Boguraev, B.K.
Dictionaries, DictionaryGrammars, and Dictionary Parsing.
Proceedings of the 27thAnnual Meeting of the Association for ComputationalLinguistics 1989, 91-101.Stallard, D. Unification-Based Semantic Interpretation i theBBN Spoken Language System.
Proceedings of the Speechand Natural Language Workshop Oct. 1989, 39-46.Wilks, Y.
A Tractable Machine Dictionary as a Resource forComputational Semantics.
NAIC 1987 Natural LanguagePlanning Workshop 1987, 97-123.ReferencesChitrao, M.V.
and Grishman, R. Statistical Parsing ofMessages.
Proceedings of the Speech and Natural LanguageWorkshop June 1990.Chodorow, M.S., Ravin, Y., and Sachar, H.E.
A Tool forInvestigating the Synonymy Relation in a SenseDisambiguated Thesaurus.
ACL Proceedings of the SecondConference on Applied Natural Language Processing 1988,144-152.Church, K. A Stochastic Parts Program and Noun PhraseParser for Unrestricted Text.
Proceedings of the SecondConference on Applied Natural Language Processing, ACL,1988, 136-143.358
