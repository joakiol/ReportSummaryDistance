Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 73?80,Prague, June 2007. c?2007 Association for Computational LinguisticsSemantic Labeling of Compound Nominalization in ChineseJinglei Zhao, Hui Liu & Ruzhan LuDepartment of Computer ScienceShanghai Jiao Tong University800 Dongchuan Road Shanghai, China{zjl,lh charles,rzlu}@sjtu.edu.cnAbstractThis paper discusses the semantic interpre-tation of compound nominalizations in Chi-nese.
We propose four coarse-grained se-mantic roles of the noun modifier and use aMaximum Entropy Model to label such re-lations in a compound nominalization.
Thefeature functions used for the model areweb-based statistics acquired via role relatedparaphrase patterns, which are formed by aset of word instances of prepositions, sup-port verbs, feature nouns and aspect mark-ers.
By applying a sub-linear transformationand discretization of the raw statistics, a rateof approximately 77% is obtained for classi-fication of the four semantic relations.1 IntroductionA nominal compound (NC) is the concatenation ofany two or more nominal concepts which functionsas a third nominal concept (Finin, 1980).
(Leonard,1984) observed that the amount of NCs had been in-creasing explosively in English in recent years.
NCssuch as satellite navigation system are abundant innews and technical texts.
In other languages such asChinese, NCs have been more productive since ear-lier days as evidenced by the fact that many simplewords in Chinese are actually a result of compound-ing of morphemes.Many aspects in Natural Language Processing(NLP), such as machine translation, information re-trieval, question answering, etc.
call for the auto-matic interpretation of NCs, that is, making explicitthe underlying semantic relationships between theconstituent concepts.
For example, the semantic re-lations involved in satellite communication systemcan be expressed by the conceptual graph (Sowa,1984) in Figure 1, in which, for instance, the se-mantic relation between satellite and communica-tion is MANNER.
Due to the productivity of NCsand the lack of syntactic clues to guide the interpre-tation process, the automatic interpretation of NCshas been proven to be a very difficult problem inNLP.In this paper, we deal with the semantic interpre-tation of NCs in Chinese.
Especially, we will fo-cus on a subset of NCs in which the head word is averb nominalization.
Nominalization is a commonphenomenon across languages in which a predica-tive expression is transformed to refer to an eventor a property.
For example, the English verb com-municate has the related nominalized form commu-nication.
Different from English, Chinese has littlemorphology.
Verb nominalization in Chinese has thesame form as the verb predicate.Nominalizations retain the argument structure ofthe corresponding predicates.
The semantic relationbetween a noun modifier and a verb nominalizationhead can be characterized by the semantic role themodifier can take respecting to the correspondingverb predicate.
Our method uses a Maximum En-tropy model to label coarse-grained semantic rolesin Chinese compound nominalizations.
Unlike mostapproaches in compound interpretation and seman-tic role labeling, we don?t exploit features fromany parsed texts or lexical knowledge sources.
In-stead, features are acquired using web-based statis-73[satellite]m(MANNER)m[communication]m(TELIC)m[system]Figure 1: The conceptual graph for satellite communication systemtics (PMI-IR) produced from paraphrase patterns ofthe compound Nominalization.The remainder of the paper is organized as fol-lows: Section 2 describes related works.
Section3 describes the semantic relations for our labelingtask.
Section 4 introduces the paraphrase patternsused.
Section 5 gives a detailed description of ouralgorithm.
Section 6 presents the experimental re-sult.
Finally, in Section 7, we give the conclusionsand discuss future work.2 Related Works2.1 Nominal Compound InterpretationThe methods used in the semantic interpretation ofNCs fall into two main categories: rule-based onesand statistic-based ones.
The rule-based approachessuch as (Finin, 1980; Mcdonald, 1982; Leonard,1984; Vanderwende, 1995) think that the interpreta-tion of NCs depends heavily on the constituent con-cepts and model the semantic interpretation as a slot-filling process.
Various rules are employed by suchapproaches to determine, for example, whether themodifier can fill in one slot of the head.The statistic-based approaches view the seman-tic interpretation as a multi-class classification prob-lem.
(Rosario and Hearst, 2001; Moldovan et al,2004; Kim and Baldwin, 2005) use supervised meth-ods and explore classification features from a simplestructured type hierarchy.
(Kim and Baldwin, 2006)use a set of seed verbs to characterize the semanticrelation between the constituent nouns and exploresa parsed corpus to classify NCs.
(Turney, 2005) useslatent relational analysis to classify NCs.
The simi-larity between two NCs is characterized by the sim-ilarity between their related pattern set.
(Lauer, 1995) is the first to use paraphrase basedunsupervised statistical models to classify semanticrelations of NCs.
(Lapata, 2000; Grover et al, 2005;Nicholson, 2005) use paraphrase statistics computedfrom parsed texts to interpret compound nominaliza-tion, but the relations used are purely syntactic.
La-pata(2000) only classifies syntactic relations of sub-ject and object.
Grover(2005) and Nicholson (2005)classify relations of subject, object and prepositionalobject.2.2 Semantic Role Labeling of NominalizationMost previous work on semantic role labeling ofnominalizations are conducted in the situation wherea verb nominalization is the head of a general nounphrase.
(Dahl et al, 1987; Hull and Gomez, 1996)use hand-coded slot-filling rules to determine the se-mantic roles of the arguments of a nominalization.In such approaches, first, parsers are used to identifysyntactic clues such as prepositional types.
Then,rules are applied to label semantic roles accordingto clues and constraints of different roles.Supervised machine learning methods becomeprevalent in recent years in semantic role labelingof verb nominalizations as part of the resurgenceof research in shallow semantic analysis.
(Pradhanet al, 2004) use a SVM classifier for the semanticrole labeling of nominalizations in English and Chi-nese based on the FrameNet database and the Chi-nese PropBank respectively.
(Xue, 2006) uses theChinese Nombank to label nominalizations in Chi-nese.
Compared to English, the main difficulty ofusing supervised method for Chinese, as noted byXue (2006), is that the precision of current parsersof Chinese is very low due to the lack of morphol-ogy, difficulty in segmentation and lack of sufficienttraining materials in Chinese.2.3 Web as a large CorpusData sparseness is the most notorious hinder for ap-plying statistical methods in natural language pro-cessing.
However, the World Wide Web can be seenas a large corpus.
(Grefenstette and Nioche, 2000;Jones and Ghani, 2000) use the web to generate cor-pora for languages for which electronic resourcesare scarce.
(Zhu and Rosenfeld, 2001) use Web-based n-gram counts for language modeling.
(Kellerand Lapata, 2003) show that Web page counts andn-gram frequency counts are highly correlated in alog scale.743 Semantic RelationsAlthough verb nominalization is commonly con-sidered to have arguments as the verb predicate,Xue(2006) finds that there tend to be fewer argu-ments and fewer types of adjuncts in verb nomi-nalizations compared to verb predicates in Chinese.We argue that this phenomenon is more obvious incompound nominalization.
By analyzing a set ofcompound nominalizations of length two from a bal-anced corpus(Jin et al, 2003), we find the semanticrelations between a noun modifier and a verb nomi-nalization head can be characterized by four coarse-grained semantic roles: Proto-Agent (PA), Proto-Patient (PP), Range (RA) and Manner (MA).
Thisis illustrated by Table1.Relations ExamplesPA ????
(Blood Circulation)ja[?
(Bird Migration)PP ?
?+n (Enterprise Management)??
?a (Animal Categorization)MA -1; (Laser Storage)?
(?& (Satellite Communication)RA ???
(Global Positioning)?u (Long-time Development)Table 1: Semantic Relations between Noun Modifierand Verb Nominalization Head.Due to the linking between semantic roles andsyntactic roles (Dowty, 1991), the relations aboveoverlap with syntactic roles, for example, Proto-Agent with Subject and Proto-Patient with Object,but they are not the same, as illustrated by theexample ??
?a(Animal Categorization).
Al-though the predicate ?a(categorize) in Chinese isan intransitive verb, the semantic relation between??
(animal) and ?a(categorization) is Proto-Patient.4 Paraphrase Patterns4.1 MotivationsSyntactic patterns provide clues for semantic rela-tions (Hearst, 1992).
For example, Hearst(1992)uses the pattern ?NP such as List?
to indicate thatnouns in List are hyponyms of NP.
To classify thefour semantic relations listed in section 3, we pro-pose some domain independent surface paraphrasepatterns to characterize each semantic relation.
Thepatterns we adopted mainly exploit a set of word in-stances of prepositions, support verbs, feature nounsand aspect markers.Prepositions are strong indicators of semanticroles in Chinese.
For example, in sentence 1), thepreposition r(ba) indicates that the noun ?
(door)and ?n(Zhangsan) is the Proto-Patient and Proto-Agent of verb?
(lock) respectively.1) a.
?nr???b.
Zhangsan ba door locked.c.
Zhangsan locked the door.The prepositions we use to characterize each rela-tion are listed in table 2.Relations Prepositional IndicatorsPP (bei), 4(rang), (jiao), d(you)PA r(ba), ?
(jiang), ?
(suo), ?
(dui)MA ?L(tongguo), ^(yong), ?
(yi)RA 3(zai), u(yu), l(cong)Table 2: Prepositional indicators of different rela-tions in Chinese.Support verbs such as ?1(conduct), \?
(put-to) can take verb nominalizations as objects.
Whencombined with prepositions, they could be goodindicators of semantic roles.
For example in 2),the verb ?1(conduct) together with the preposi-tion ?
(dui) indicate that the relation between ?a(categorization) and??
(animal) is PA.2) a.
????1?ab.
dui animal conduct categorization.c.
conduct categorization regarding animal.Nouns such as ?
{(method), ??
(manner), ??
(range) and /:(place) can be used as featureswhen co-occurring with the compound nominaliza-tions under consideration.
For example, if ???
(global range) co-occurs frequently with ??
(positioning), it will indicate a possible RA rela-tion between ?
(global) and??
(positioning).Another set of word instances we use is as-pect, tense and modal markers.
As we have men-tioned, verb nominalizations have the same form as75the corresponding verb predicates in Chinese.
As-pect?tense and modal markers make a good indica-tor for recognizing a verb predicate.
For example ifa verb is directly followed by an aspect marker suchas(le), which indicates a finished state, it couldbe safely viewed as a predicate.
Such markers arevery useful in paraphrase patterns.
This can be illus-trated by 3), in which, the tense marker m?
(start)indicates a strong agentive meaning of the noun ja(bird) and provides good clues of the relation PPbetweenja(bird) and[?
(migration) in the com-pound ja[?
(bird migration).3) a.
jam?[?b.
Bird start migrate.c.
Birds start to migrate.4.2 Paraphrase Pattern TemplatesWe use the set of word instances above to formpattern templates which could be instantiated bythe compound nominalization under considerationto form paraphrase patterns.
The templates are ex-pressed using the employed search engine?s querylanguage.
Currently, we employ totally 30 featuretemplates for the four semantic relations.
A sampleof the pattern templates is listed in Tabel 3, in which,x, y is the variable which need to be instantiated bythe noun modifier and verb nominalization respec-tively.Relations Paraphrase Pattern TemplatesPP ??x?1y?
(?dui x conduct y?)?rx?
?y?
(?ba x?
?y?)?yXx?
(?y zhe x?)?x?
?y?
(?x bei?
?y?
)PA ?x?
?y?
(?bei x?
?y?)?xm?y?
(?x start y?)?x?
???y?
(?x?
?can y?)?x?y?
(?x suo y?
)MA ??Lx?
?y?
-??Lxy?
(?tongguo x?
?y?
-?tongguo xy?)?x?{?
?y?
(?x method?
?y?
)RA ?3x?
?y?
-?3y?
(?zai x?
?y?
-?zai y?)?lx?
?y?
(?cong x?
?y?)?x???
?y?
(?x range?
?y?
)Table 3: A Sample Set of the Paraphrase PatternTemplates.5 System Description5.1 Data SourceCorpusNominalizationRecognizerCompoundNominalizationsPMI StatisticMEClassifierCompoundExtractorPatternTemplatesSearch EngineData PreprocessingSemanticRelationsFigure 2: System ArchitectureFigure 2 illustrates the system architecture of ourapproach.
We view the semantic labeling of com-pound nominalization as a data-driven classificationproblem.
The data used for the experiment is auto-extracted from the Chinese National Corpus (Jin etal., 2003), which is a balanced segmented and POStagged corpus with 8M characters.
Because the cor-pus doesn?t distinguish verb predicates with verbnominalizations, a verb nominalization recognizer isfirst used to recognize all the verb nominalizationsin the corpus, and then, a compound extractor identi-fies all the compound nominalizations having a nounmodifier and a verb nominalization head in the cor-pus.
We manually examined a sample of the resultset and finally randomly select 300 correct noun-nominalization pairs as our training and testing setfor semantic interpretation.One PHD student majored in computer scienceand one in linguistics were employed to label allthe 300 data samples simultaneously according tothe relation set given in section 3.
The annotator?sagreement was measured using the Kappa statistic(Siegel and Castellan, 1988) illustrated in (1), ofwhich Pr(A) is the probability of the actual out-come and Pr(E) is the probability of the expectedoutcome as predicted by chance.
The Kappa score76of the annotation is 87.3%.K = Pr(A) ?
Pr(E)1 ?
Pr(E) (1)After discussion, the two annotators reachedagreement on a final version of the data sample la-beling.
In which, the proportion of relations PP, PA,MA, RA is 45.6%, 27.7%, 16.7% and 10% respec-tively, giving a baseline of 45.6% of the classifica-tion problem by viewing all the relations to be PP.Finally, the 300 data instances were partitioned intoa training set and a testing set containing 225 and 75instances respectively.5.2 Maximum Entropy ModelWe use the Maximum Entropy (ME) Model (Bergeret al, 1996) for our classification task.
Given a setof training examples of a random process, ME isa method of estimating the conditional probabilityp(y|x) that, given a context x, the process will out-put y.
In our task, the output corresponds to the fourrelation labels PP, PA, MA and RA.The modeling of ME is based on the MaximumEntropy Principle, that is, modeling all that is knownand assuming nothing about what is unknown.
Thecomputation of p(y|x) is illustrated as the formula(2).
fi(x, y) are binary valued feature functions withthe parameter ?i used to express the statistics of thedata sample.
Z?
(x) is a normalization factor.p?
(y|x) =1Z?
(x)exp(?i?ifi(x, y))(2)5.3 PMI-IR Score as FeaturesThe feature functions we adopted for ME differen-tiate from most other works on the semantic label-ing task, which mainly exploited features from well-parsed text.
Instead, we use a web-based statis-tic called PMI-IR which mainly measures the co-occurrence between the data to classify and the set ofparaphrase pattern templates we stated in section 4.The PMI-IR measure was first adopted by (Turney,2001) for mining synonyms from the Web.
(Etzioniet al, 2004) uses the PMI-IR measure to evaluate theinformation extracted from the Web.Given a compound nominalization pair p(x, y)and a set of paraphrase pattern templates t1, t2, ,,tn, the PMI-IR score between p and ti can be com-puted by formula (3).PMI(p, ti) =Hits(p, ti)Hits(p) (3)In which, PMI(p, ti) is the co-occurrence webpage counts of p(x, y) and ti.
For example, ifthe template t is ??
(dui) x ?1(conduct) y?and the compound nominalization is the pair p(??
(animal),?a(categorization)), then Hits(p, t)is the web counts returned from the search engine forthe pattern ??(dui)??
(animal)?1(conduct)?a(categorization)?.5.4 Scaling of PMI FeaturesWeb counts are inflated which need to be scaled toattain a good estimation of the underlying probabil-ity density function in ME.
In our approach, first, alog sub-linear transformation is used to preprocessthe raw PMI-IR feature function for the ME model.Then, a discretization algorithm called CAIM (Kur-gan and Cios, 2004) is used to transform the contin-uous feature functions into discrete ones.CAIM is a supervised discretization algorithmwhich can discretize an attribute into the smallestnumber of intervals and maximize the class-attributeinterdependency.
Suppose that the data set consistsof M examples and each example belongs to onlyone of the S classes.
F indicates the continuous fea-ture functions produced from paraphrase patterns inour task.
D is a discretization scheme on F , whichdiscretizes F into n non-overlapping discrete inter-vals.
The class variable and the discretization vari-able of attribute F are treated as two random variblesdefining a two-dimensional frequency matrix(calledquanta matrix) that is shown in Table 4, in which,qir is the total number of continuous values belong-ing to the ith class that are within interval (dr?1, dr],while Mi+ is the total number of values belong-ing to the ith class, and M+r is the total numberof values of attribute F that are within the interval(dr?1, dr], for i = 1, 2, ..., S and r = 1, 2, ..., n.The CAIM algorithm uses a greedy search to findthe specific discretization sechme D according tothe Class-Attribute Interdependency Maximization(CAIM) criterion defined as(4), where maxr is themaximum value among all qir values.77Class [d0,d1] ... [dr?1,dr] ... [dn?1,dn] Class TotalC1 q11 ... q1r ... q1n M1+: : ... : ... : :Ci qi1 ... qir ... qin Mi+: : ... : ... : :Cs qS1 ... qSr ... qSn MS+Interval Total M+1 ... M+r ... M+n MTable 4: The Quanta Matrix for Attribute F and Discretization Scheme DCAIM(C,D|F ) = 1nn?r=1max2rM+r(4)6 Results and DiscussionIn this section, we present our experimental resultson the semantic relation labeling of our CompoundNominalization Dataset.
We compared the perfor-mance between two different engines, also, betweenthe raw PMI and the scaled one.Two search engines, Google (www.google.com)and Baidu (www.baidu.com) are used and comparedto obtain the PMI scores between a verb nominaliza-tion pair and the set of paraphrase patterns.
The re-sult of using Google and Baidu are comparable.
Forexample, when using raw PMI score as the featuresof ME classification model, Google based algorithmobtains a correct classification rate of 65.3%, whileBaidu based algorithm obtains a correct classifica-tion rate of 62.7%.
The main difference between thetwo search engines is their indexing and rating algo-rithm of the web pages.
Compared to Google, Baiduuses a stop wordlist, including empty markers suchas(le), to filter the queries.
While this is benefi-cial for common users, it hurts our algorithm whichdepends heavily on such information.Compared with using raw PMI as the classifi-cation features, feature scaling improves much onthe classification result.
Using Log transformation,Both Google based and Baidu based algorithm in-crease about 4 percent on the correct classificationrate and when CAIM algorithm is employed to pre-process the data, both algorithm?s correct classifica-tion rates increase more than 8 percent.
We thinkthat the usefulness of log sub-linear transformationis mainly due to the fact that the Web is extremelybiased and inflated.
The compression of the inflatedfeature space can enable the ME model to give agood estimation of the underlying probability den-sity function of the data.
As to the usefulness ofthe discretization of the data, we think that it ismainly because that the web-based statistics containmuch noise and the features produced from para-phrase patterns are highly correlated with specificclasses.
CAIM discretization algorithm can maxi-mize the class-attribute interdependence in the dataand can be seen as a noise pruning process in somesense.Among the four semantic relations labeled, PPgets the best precision and recall overall and rela-tions such as RA gets a lower F-score.
We thinkthat this is mainly due to the difficulty in selectingparaphrase patterns for RA compared to PP.
Somepatterns are not as indicative as others for the rela-tions considered.
For example, the paraphrase pat-terns ?3x?
?y?
-?3y?
(?in x?
?y?
-?in y?)
for RAis not as indicative as the pattern ??x?1y?
(duix conduct y) for PP.
Discovering and selecting themost indicative patterns for each relation is the keyelement for our algorithm.We can make a rough comparison to the relatedworks in the literature.
In syntactic relation label-ing of compound nominalization in English, Lap-ata (2000) and Grover et al (2005) both applyparsed text and obtains 87.3%, 77% accuracy forthe subject-object and subject-object-prepositionalobjects classification tasks respectively.
Nicholson(2005) uses both the parsed text and the web for theclassification of subject-object-prepositional objectsand the result is comparatively poor.
Compared tosuch works, the relations we exploited in the label-ing task is purely semantic which makes the clas-sification task more difficult and we don?t use anyparsed text as input.
Considering the difficulty of78Google BaiduPrecision Recall F-Score precision Recall F-ScoreRaw PMIPP 72.5 82.9 77.3 65.3 88.9 75.2PA 47.6 50.0 48.8 50.0 42.1 45.7MA 75.0 50.0 60.0 50.0 27.3 35.3RA 66.7 50.0 57.1 80.0 44.4 57.1Rate 65.3 62.7LogPP 66.7 85.7 75.0 68.2 83.3 75.0PA 64.7 55.0 59.5 60.0 47.4 52.9MA 80.0 66.7 72.7 66.7 54.5 60.0RA 100 37.5 54.5 71.4 55.5 62.5Rate 69.3 66.7Log+DiscretizationPP 82.5 94.3 88.0 80.9 94.4 87.2PA 81.3 65.0 72.2 64.7 57.9 61.1MA 75.0 50.0 60.0 87.5 63.6 73.7RA 54.5 75.0 63.2 64.5 55.6 58.8Rate 77.3 76.0Table 5: Results comparing different search engines, raw PMI as features vs. scaled features.
Rate is thecorrect classification rate for the four semantic relations overall.the problem and the unsupervised nature of our al-gorithm, the results (accuracy 77.3%) are very en-couraging.7 Conclusions and Future WorkIn this paper, we view the semantic relation label-ing of compound nominalization as a classificationproblem.
We propose four coarse-grained semanticroles of the noun modifier for the verb nominaliza-tion head.
A Maximum Entropy model is appliedfor the classification task.
The features used for themodel are web-based statistics acquired via class re-lated paraphrase patterns, which mainly use a set ofword instances of prepositions, support verbs, fea-ture nouns and aspect markers.
The experimentalresult illustrates that our method is very effective.We believe that the method we proposed is notonly limited in the semantic interpretation of com-pound nominalizations, but can also be used as away to compensate the low accuracy of the moregeneral task of semantic role labeling of nominal-ization phrases caused by the inefficiency of Chineseparsers.The major limitation of our approach is that theparaphrase pattern templates we use now are hand-coded according to the linguistic theory.
To achievemore generality of our method, in the future, weshould study automatic template induction and fea-ture selection algorithms for the classifier to selectthe set of most indicative pattern templates for eachsemantic relation.8 AcknowledgementsThis work is supported by NSFC Major ResearchProgram 60496326: Basic Theory and Core Tech-niques of Non Canonical Knowledge.ReferencesA.L.
Berger, V.J.
Della Pietra, and S.A. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39?71.D.A.
Dahl, M.S.
Palmer, and R.J. Passonneau.
1987.Nominalizations in PUNDIT.
Proceedings of the 25thAnnual Meeting of the Association for ComputationalLinguistics, Stanford University, Stanford, CA, July.79DR Dowty.
1991.
Thematic ProtoRoles and Argu-ment Selection.
Second Conference on Maritime Ter-monology, Turku, 33:31?38.O.
Etzioni, M. Cafarella, D. Downey, S. Kok, A.M.Popescu, T. Shaked, S. Soderland, D.S.
Weld, andA.
Yates.
2004.
Web-scale information extractionin knowitall:(preliminary results).
Proceedings of the13th international conference on World Wide Web,pages 100?110.T.W.
Finin.
1980.
The semantic interpretation of com-pound nominals.
Dissertation Abstracts InternationalPart B: Science and Engineering[DISS.
ABST.
INT.PT.
B- SCI.
& ENG.
],, 41(6):1980.G.
Grefenstette and J. Nioche.
2000.
Estimation of En-glish and non-English Language Use on the WWW.Arxiv preprint cs.CL/0006032.C.
Grover, A. Lascarides, and M. Lapata.
2005.
A com-parison of parsing technologies for the biomedical do-main.
Natural Language Engineering, 11(01):27?65.M.A.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
Proceedings of the 14th con-ference on Computational linguistics-Volume 2, pages539?545.R.D.
Hull and F. Gomez.
1996.
Semantic interpretationof nominalizations.
AAAI Conference, pages 1062?1068.Guangjin Jin, Shulun Guo, Hang Xiao, and YunfanZhang.
2003.
Standardization for Corpus Processing.Applied Linguistics, pages 16?24.R.
Jones and R. Ghani.
2000.
Automatically buildinga corpus for a minority language from the web.
Pro-ceedings of the Student Research Workshop at the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 29?36.F.
Keller and M. Lapata.
2003.
Using the web to ob-tain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459?484.S.N.
Kim and T. Baldwin.
2005.
Automatic interpre-tation of noun compounds using WordNet similarity.Proc.
of IJCNLP-05, pages 945?956.S.N.
Kim and T. Baldwin.
2006.
Interpreting Seman-tic Relations in Noun Compounds via Verb Semantics.Proceedings of the COLING/ACL 2006 Main Confer-ence Poster Sessions, pages 491?498.LA Kurgan and KJ Cios.
2004.
CAIM discretizationalgorithm.
Knowledge and Data Engineering, IEEETransactions on, 16(2):145?153.M.
Lapata.
2000.
The automatic interpretation of nomi-nalizations.
Proceedings of AAAI.M.
Lauer.
1995.
Designing Statistical Language Learn-ers: Experiments on Compound Nouns.
Ph.D. thesis,Ph.
D. thesis, Macquarie University, Sydney.R.
Leonard.
1984.
The interpretation of English nounsequences on the computer.
North-Holland.D.B.
Mcdonald.
1982.
Understanding noun compounds.Carnegie-Mellon University.D.
Moldovan, A. Badulescu, M. Tatu, D. Antohe, andR.
Girju.
2004.
Models for the semantic classificationof noun phrases.
Proceedings of HLT/NAACL-2004Workshop on Computational Lexical Semantics.J.
Nicholson.
2005.
Statistical Interpretation of Com-pound Nouns.
Ph.D. thesis, University of Melbourne.S.
Pradhan, H. Sun, W. Ward, J.H.
Martin, and D. Juraf-sky.
2004.
Parsing Arguments of Nominalizations inEnglish and Chinese.
Proc.
of HLT-NAACL.B.
Rosario and M. Hearst.
2001.
Classifying the seman-tic relations in noun compounds via a domain-specificlexical hierarchy.
Proceedings of the 2001 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-01), pages 82?90.S.
Siegel and NJ Castellan.
1988.
Nonparametric statis-tics for the behavioral sciences.
McGraw-HiU BookCompany, New York.JF Sowa.
1984.
Conceptual structures: information pro-cessing in mind and machine.
Addison-Wesley Long-man Publishing Co., Inc. Boston, MA, USA.P.D.
Turney.
2001.
Mining the Web for synonyms:PMI-IR versus LSA on TOEFL.
Proceedings of theTwelfth European Conference on Machine Learning,pages 491?502.P.D.
Turney.
2005.
Measuring semantic similarity bylatent relational analysis.
Proceedings of the Nine-teenth International Joint Conference on Artificial In-telligence (IJCAI-05), pages 1136?1141.L.H.
Vanderwende.
1995.
The analysis of noun se-quences using semantic information extracted from on-line dictionaries.
Ph.D. thesis, Georgetown Univer-sity.N.
Xue.
2006.
Semantic Role Labeling of NominalizedPredicates in Chinese.
Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the ACL.X.
Zhu and R. Rosenfeld.
2001.
Improving trigram lan-guage modeling with the World Wide Web.
Acous-tics, Speech, and Signal Processing, 2001.
Proceed-ings.(ICASSP?01).
2001 IEEE International Confer-ence on, 1.80
