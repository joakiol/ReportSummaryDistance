On the Mathematical Propertiesof Linguistic Theories IC.
Raymond PerraultArtificial Intelligence CenterSRI InternationalMenlo Park, CA 94025andCenter for the Study of Language and InformationStanford UniversityMetatheoretical findings regarding the decidability, generative capacity, and recognition complexityof several syntactic theories are surveyed.
These include context-free, transformational, lexical-func-tional, generalized phrase structure, tree adjunct, and stratificational grammars.
The paper concludeswith a discussion of the implications of these results with respect o linguistic theory.1.
Int roduct ionThe development of new formalisms for expressinglinguistic theories has been accompanied, at least sinceChomsky and Miller's early work on context-freelanguages, by the study of their metatheory.
In particular,numerous results on the decidability, generative capacity,and, more recently, the recognition complexity of theseformalisms have been published (and rumored!).
Thispaper surveys some of these results and discusses theirsignificance for linguistic theory.
However, we will avoidentirely the issue of whether one theory is more descrip-tively adequate than another.
We will consider context-?
free, transformational, exical-functional, generalizedphrase structure, tree adjunct, and stratificationalgrammars)Although this paper focuses on metatheoretic resultsas arbiters among theories as models of human linguisticcapacities, they may have other uses as well.
Complexityresults could be utilized for making decisions about theimplementation of parsers as components of computer-based language-understanding systems.
However, asStanley Peters has pointed out, no one should underesti-mate"the pleasure to be derived from ferreting out theseresults!
32.
Preliminary DefinitionsWe assume that the reader is familiar with the basic defi-nitions of regular, context-free (CF), context-sensitive(CS), recursive, and recursively enumerable (r.e.
)languages, as well as with their acceptors (see Hopcroftand Ullman 1979).
We will be much concerned with theproblem of recognizing whether a string is contained in agiven language (the recognition problem) and with that ofl This research was sponsored in part by the National Science andEngineering Research Council of Canada under Grant A9285.
It wasmade possible in part by a gift from the Systems Development Founda-tion.
An earlier version of this paper appeared in the Proceedings of the21st Annual Meeting of the Association for Computational Linguistics,Cambridge, MA, June 1983.I would like to thank Bob Berwick, Alex Borgida, Jim Hoover,Aravind Joshi, Lauri Karttunen, Fernando Pereira, Stanley Peters, PeterSells, Hans Uszkoreit, and the referees for their suggestions.ZAlthough we will not examine them here, formal studies of othersyntactic theories have been undertaken: e.g.
Warren (1979) forMontague's PTQ (1973).
Pereira and Shieber (1984) use techniquesfrom the denotational semantics of programming languages to investi-gate the feature systems of several unification-based theories.3It may be worth pointing out that the introduction of formal argumen-tation in linguistics has not always been beneficial.
Some pseudoformalarguments against rival theories were unquestionably accepted by anaudience that did not always have the mathematical sophistication to becritical.
For example, Postal's claim (1964b) that two-level stratifica-tional grammars generated only context-free languages was based on animprecise definition by its proponents, as well as by the failure to seethat among the more precise definitions were many very powerful ones.Copyright 1985 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted provided thatthe copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
To copyotherwise, or to republish, requires a fee and/or  specific permission.0362-613X/84/030165-12503.00Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 165C.
Raymond Perrault On the Mathematical Properties of Linguistic Theoriesgenerating one (or all) derivations of the string (the pars-ing problem).Some elementary definitions from complexity theorymay be useful.
Further details may be found in Aho etal.
(1974).
Complexity theory is the study of theresources required by algorithms, usually space and time.Let f(x) be a function, say, the recognition function for alanguage L. The most interesting results we could obtainregarding f would be a lower bound on the resourcesneeded to compute f on a machine of a given architec-ture, say, avon  Neumann computer or a parallel array ofneurons.
These results over whole classes of machinesare very difficult to obtain, and none of any significanceexist for parsing problems.Restricting ourselves to a specific machine model andan algorithm M for f, we can ask about the cost (e.g., intime or space) c(x) of executing M on a specific input x.Typically, c is too fine-grained to be useful: what onestudies instead is a function c w whose argument is aninteger n denoting the size of the input to M, and whichgives some measure of the cost of processing inputs oflength n. Complexity theorists have been most interestedin the asymptotic behaviour of Cw, i.e., the behaviour ofc w as n gets large.If one is interested in upper bounds on the behaviourof M, one usually defines Cw(n) as the maximum of c(x)over all inputs x of size n. This is called the worst-casecomplexity function for M. Other definitions are possi-ble: for example, one can define the expected complexityfunction Ce(n) for M as the average of c(x) over all inputsof length n. c e might be more useful than c w if one had anidea as to the distribution of possible inputs to M. Notonly are realistic distributions rarely available, but theintroduction of probabilistic considerations makes thestudy of expected complexity technically more difficultthan that of worst-case complexity.
For a given problem,expected and worst-case measures may be quitedifferent.
4It is quite difficult to get detailed descriptions of Cw;for many purposes, however, a cruder estimate is suffi-cient.
The next abstraction involves ".lumping".
classesof c w functions into simpler ones that demonstrate theirasymptotic behaviour more clearly and are easier tomanipulate.
This is the purpose of O-notation (read "big-oh notation").
Let f(n) and g(n) be two functions.
Func-tion f is said to be O(g) if a constant multiple of g is anupper bound for f, for all but a finite number of values ofn.
More precisely, f is O(g) if there is are constants Kand n o such that for all n > no, fin) < K * g(n).Given an algorithm M, we will say that M is TIME(g)or, equivalently, that its worst-case time complexity isO(g) if the worst-case time cost function Cw(n) for M isO(g).
5 This merely says that almost all inputs to M of sizen can be processed in time at most a constant imes g(n).It does not say that all inputs require g(n) time, or Ruzzomachine that implements f. Also, if two algorithms A iand A 2 are  available for a function f, and if their worst-case complexity can be given respectively as O(g) andO(g), and gl <- g2, it may still be true that for a largenumber of cases (maybe even all those likely to beencountered in practice), A 2 will be the preferable algo-rithm simply because the constant K 1 for gl may be muchlarger than is K 2 for g2" A parsing-related example isgiven in Section 3.In examining known results pertaining to the recogni-tion complexity of various theories, it is useful to consid-er how robust they are in the face of changes in themachine model from which they were derived.
Thesemodels can be divided into two classes: sequential andparallel.
Sequential models (Aho et al 1974) include thefamiliar single- and multitape Turing machines (TM) aswell as random-access machines (RAM) and random-ac-cess stored-program achines (RASP).
A RAM is like aTM except that its working memory is random-accessrather than sequential.
A RASP is like a RAM but storesits program in its memory.
Of all these models, the RASPis most like a yon Neumann computer.All these sequential models can simulate one anotherin ways that do not require great changes in timecomplexity.
For example, a k-tape Turing Machine thatruns in time O(t) can be simulated by a RAM in timeO(t log0, conversely, a RAM running in O(t) can be simu-lated by a k-tape TM in time O(t2)..
In fact, all the famil-iar sequential models are polynomially related: they cansimulate one another with at most a polynomial loss inefficiency.
6 Thus, if a syntactic model is known to have adifficult recognition problem when implemented on onesequential model, execution of an equivalent algorithmon another sequential machine will not be much easier.Transforming a sequential algorithm to one on a paral-lel machine with a fixed number K of processors providesat most a factor K improvement in speed.
More interest-ing results are obtained when the number of processors iallowed to grow with the size of the problem, e.g., withthe length of the string to be parsed.
These processorscan be viewed as connected together in a circuit, withinputs entering at one end and outputs being produced atthe other.
The depth of the circuit, or the maximumnumber of processors that data must be passed throughfrom input to output, corresponds to the parallel timerequired to complete the computation.
A problem thathas a solution on a sequential machine in polynomial timeand in space s will have a solution on a parallel machinewith a polynomial number of processors and circuit depth(and hence parallel time) O(s2).
This means that algo-rithms with sequential solutions requiring small space(such as deterministic CSLs) have fast parallel solutions.4 Hoare's Quicksort algorithm, for example, has expected time complex-ity of O(n logn) and worst-case complexity of O(n2), using notationdefined in the next paragraph.Similarly, let M be SPACE(g) if the worst-case space complexity of Mis O(g).6 RAMs and RASPs are allowed to store arbitrarily large numbers intheir registers.
These results assume that the cost of performingelementary operations on those numbers is proportional to their length,i.e.
to their logarithm.166 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984.C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesFor a comprehensive survey of parallel computation, seeCook (1981).3.
Context-Free LanguagesRecognition techniques for context-free languages arewell known (Aho and Ullman 1972).
The so-called CKYor "dynamic programming" method is attributed by Hays(1962) to J. Cocke; it was discovered independently byKasami (1965) and Younger (1967), who showed it tobe O(n3).
It requires the grammar to be in ChomskyNormal Form, and putting an arbitrary grammar in CNFmay square its size.
Berwick and Weinberg (1982) pointout that, since the complexity of parsing algorithms isgenerally at least linearly dependent on the size of thegrammar, this requirement may make CKY less than opti-mal for parsing short sentences.Earley's algorithm recognizes trings in arbitrary CFGsin time O(n 3) and space O(n2), and in time O(n e) forunambiguous CFGs.
Graham, Harrison, and Ruzzo(1980) offer an algorithm that unifies CKY and Earley'salgorithm (1970), and discuss implementation details.Valiant (1975) showed how to interpret he CKY algo-rithm as the finding of the transitive closure of a matrixand thus reduced CF recognition to matrix multiplication,for which subcubic algorithms exist.
Because of theenormous constants of proportionality associated withthis method, it is not likely to be of much practical use,either an implementation method or as a "psychologicallyrealistic" model.Ruzzo (1979) has shown how CFLs can be recognizedby Boolean circuits of depth O(log(n)2), and thereforethat parallel recognition can be accomplished in timeO(log(n)2).
The required circuit size is polynomial in n.So as not to be mystified by the upper bounds on CFrecognition, it is useful to remember that no known CFLrequires more than linear time, nor is there even anonconstructive proof of the existence of such alanguage.This is also a good place to recall the differencebetween recognition and parsing: if parsing requires thatdistinct structures be produced for all parses, it will beTIME(2n), since in some grammars entences of length nmay have 2 n parses (Church and Patil 1982).
For anempirical comparison of various parsing methods, seeSlocum (1981).4.
Transformational GrammarFrom its earliest days, discussions of transformationalgrammar (TG) have included consideration of matterscomputational.Peters and Ritchie (1973a) provided some the firstnontrivial results regarding the generative power of TGs.Their model reflects the Aspects version quite faithfully,including transformations that move and add constitu-ents, and delete them subject o recoverability.
All trans-formations are obligatory, and applied cyclically from thebottom up.
They show that every r.e.
set can be gener-ated by applying a set of transformations to a context-sensitive base.
The proof is quite simple: the right-handsides of the type-0 rules that generate the r.e.
set arepadded with a new "blank" symbol to make them at leastas long as their left-hand sides.
Rules are added to allowthe blank symbols to commute with all others.
Thesecontext-sensitive rules are then used as the base of a TGwhose only transformation deletes the blank symbols.Thus, if the transformational formalism itself issupposed to characterize the grammatical strings of possi-ble natural languages, then the only languages beingexcluded by the formalism are those that are not enumer-able under any model of computation.
The characteriza-tion assumption is further discussed in Section 9.At the expense of a considerably more intricate argu-ment, the previous result can be strengthened (Peters andRitchie 1971) to show that every r.e.
set can be gener-ated by a context-free based TG, as long as a filter - anintersection with a regular set - can be applied to thephrase-markers produced by the transformations.
Infact, the base grammar can be independent of thelanguage being generated.
The proof involves the simu-lation of a TM by a TG.
The transformations first gener-ate an "input tape" for the TM being simulated, thenapply the TM productions, one per cycle of the grammar.The filter ensures that the base grammar will generatejust as many S nodes as necessary to generate the inputstring and do the simulation.
In this case too, if thetransformational formalism is supposed to characterizethe possible natural anguages, the universal base hypoth-esis (Peters and Ritchie 1969), according to which allnatural languages can be generated from the same basegrammar, is empirically vacuous: any recursively enumer-able language can.Following Peters and Ritchie's work, several attemptswere made to find a restricted form of the transforma-tional model that is descriptively adequate, yet whosegenerated languages are recursiVe (see, for example,LaPointe 1977).
Since a key part of the proof in Petersand Ritchie (1971) involves the user of a filter on thefinal derivation trees, Peters and Ritchie (1973c) exam-ined the consequences of forbidding final filtering.
Theyshow that, if S is the recursive symbol in the CF base, thegenerated language L is predictably enumerable and expo-nentially bounded.
A language L is predictably enumerableif there is an "easily" computable function t(n) that givesan upper bound on the number of tape squares neededby its enumerating TM to enumerate the first n elementsof L. L is exponentially bounded if there is a constant Ksuch that, for every string x in L, there is another stringt .
x m L whose length is at most K times the length of x.The class of nonfiltering languages is quite unusual,including all the CFLs (obviously), but also properlyintersecting the CSLs, the recursive languages, and ther.e.
languages.The source of nonrecursivity in transformationallygenerated languages is that transformations can deletelarge parts of the tree, thus producing surface trees thatare arbitrarily smaller than the deep structure trees theyComputational Linguistics, Volume 10, Numbers 3-4, July-December 1984 167C.
Raymond Perrault On the Mathematical Properties of Linguistic Theorieswere derived from.
This is what Chomsky's"recoverability of deletions" condition was meant toavoid.
In his thesis, Petrick (1965) defines the followingcondition on transformational derivations: a derivationsatisfies the terminal-length-increasing condition if thelength of the yield of any subtree u, resulting from theapplication of the transformational cycle to a subtree t, isgreater than the length of the yield of any subtree u rresulting from the application of the cycle to a subtree trof t.Petrick shows that, if all recursion in the base gram-mar "passes through S" and all derivations atisfy theterminal-length-increasing condition, then the generatedlanguage is recursive.
Using a slightly more restrictedmodel of transformations Rounds (1973) strengthens thisresult by showing that the resulting languages are in factcontext-sensitive.In an unpublished paper, Myhill shows that, ifPetrick's condition is weakened to terminal-length-non-decreasing, the resulting languages can be recognized inspace that is at most exponential in the length of theinput.
This implies that recognition can be done in atmost double-exponential time, but Rounds (1975) provesthat not only can recognition be done in exponential time,but that every language recognizable in exponential timecan be generated by a TG satisfying the terminal-length-nondecreasing condition and recoverability of deletions.This is a very strong result, because of the closureproperties of the class of exponential-time languages> Tosee why this is so requires a few more definitions.Let P be the class of all languages that can be recog-nized in polynomial time on a deterministic TM, and NPthe class of all languages that can be recognized in polyn-omial time on a nondeterministic TM.
P is obviouslycontained in NP, but the converse is not known, althoughthere is much evidence that it is false.There is a class of problems, the so-called NP-eompleteproblems, which are in NP and "as difficult" as any otherproblems in NP in the following sense: if any of themcould be shown to be in P, all the problems in NP wouldalso be in P. One way to show that a language L isNP-complete is to show that L is in NP and that everyother language L o in NP can be polynomially transformedinto L, - i.e., that there is a deterministic TM, operatingin polynomial time, that will transform an input w to Linto an input w o to L o such that w is in L if and only if w ois in L o.
In practice, to show that a language isNP-complete, one shows that it is in NP and that somealready known NP-complete language can be polynomial-ly transformed into it.All the known NP-complete languages can be recog-nized in exponential time on a deterministic machine,and none have been shown to be recognizable in lessthan exponential time.
Thus, since the restricted trans-formational languages of Rounds characterize the expo-nential languages, if all of them were to be in P, P wouldbe equal to NP.
Putting it another way, if P is not equalto NP, some transformational languages (even thosesatisfying the terminal-length-nonincreasing condition)have no "tractable" (i.e., polynomial-time) recognitionprocedures on any deterministic TM.
It should be notedthat this result also holds for all the other known sequen-tial models of computation, as they are all polynomiallyrelated, and even for parallel machines with as many as apolynomial number of processors.All the results outlined so far in this section areinspired by the model of transformational grammarpresented in Aspects.
More recent versions of the theoryare substantially different, primarily in that most of theconstructions handled in terms of deletions from the basetrees are now handled using traces (i.e., constituents withno lexical material) indexed to other constituents.
In hiscontribution to this issue (p. 189), Berwick presents aformalization of the theory of Government and Binding(GB) and some of its consequences.
The formalization isunusual in that it reduces grammaticality o well-formed-ness conditions on what he calls annotated surface struc-tures.
From these conditions, two results follow.
One isthat for every GB grammar G there is a constant K suchthat for every string w in L(G) and for every annotatedsurface structure s whose yield is w, the number of nodesin s is bounded by K*length(w).
This, of course, ensuresthat the L(G) is recursive.
The second result is that GBlanguages all have the linear growth or arithmetic growthproperty: for every sufficiently long string w in a GBlanguage L there is another string w p in L which is atmost K symbols horter than w.A few comments about Berwick's formalization andresults are in order.
To begin with, the formalization isclearly a quite radical simplification of current practiceamong GB practitioners, as it does not reflectD-structure, LF, or PF, nor case theory, the theta-eriter-ion, and control theory.
Thus, in its current form, theformalization does not include the machinery necessaryto account for passives and raising.
It also assumes thatX-bar theory limits the base to trees generated by CFGswith no useless nonterminals and no cycles, exceptpresumably through the S and NP nodes.
This excludesaccounts of stacked adjectives, as in the white speckledshaggy Pekingese, and of stacked relative clauses.We suspect hat most of these features could be addedto the formalization without affecting either result, andthat it is extremely useful to have even a first approxi-mation of one to work with.
Although Berwick is muteon the subject, we conjecture that recognition in themodel he gives can be done in polynomial time.
What isless clear is what will happen to recognition complexityunder models that include the other constraints.Berwick's result about the linear growth property hasno immediate functional consequence for complexity oreven for weak generative capacity.
It is presented as aproperty that natural languages eem to have and thusthat should be predicted by the linguistic model.168 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984C.
Raymond Perrault On the Mathematical Properties of Linguistic Theories5.
LexicaI-Functional GrammarIn part, transformational grammar seeks to account for arange of constraints or dependencies within sentences.Of particular interest are subcategorization, predicate-ar-gument dependencies, and long-distance dependencies,such as wh-movement.
Several recent theories suggestdifferent ways of accounting for these dependencies, butwithout making use of transformations.
We examinethree of these in the next several sections: lexical-func-tional grammar, generalized phrase structure grammar,and tree adjunct grammar.In the lexical-functional grammar (LFG) of Kaplanand Bresnan (1982), two levels of syntactic structure arepostulated: constituent and functional.
All the workdone previously by transformations i  instead encodedboth in the lexicon and in links established betweennodes in the constituent and functional structures.The languages generated by LFGs, or LFLs, are CSLsand properly include the CFLs (Kaplan and Bresnan1982).
Berwick (1982) shows that a set of strings whoserecognition problem is known to be NP-complete, name-ly, the set of satisfiable Boolean formulas, is an LFL.Therefore, as was the case for Rounds's restricted classof TGs, if P is not equal to NP, then some languagesgenerated by LFGs do not have polynomial-time r cogni-tion algorithms.
Indeed only the "basic" parts of theLFG mechanism are necessary to the reduction.
Thisincludes mechanisms necessary for feature agreement,for forcing verbs to take certain cases, and for allowinglexical ambiguity.
Thus, no simple change in the formal-ism is likely to avoid the combinatorial consequences ofthe full mechanism.
It should be noted that the c-struc-tures and f-structures necessary to make satisfiableBoolean formulas into an LFL are not much larger thanthe strings themselves; the complexity comes in findingthe assignment of truth-values to the variables.
In hispaper in this issue (p. 189), Berwick argues that thecomplexity of LFLs stems from their ability to unify treesof arbitrary size, and that such a mechanism does notexist in GB.
However, the recognition complexity of GBlanguages, as formalized in Berwick (1984) or in more"faithful" models, remains open, and may arise fromother constraints.Both Berwick and Roach have examined the relationbetween LFG and the class of languages generated byindexed grammars (Aho 1968), a class known to be aproper subset of the CSLs, but including someNP-complete languages (Rounds 1973).
They claim(personal communication) that the indexed languages area proper subset of the LFLs.6.
Generalized Phrase Structure GrammarIn a series of papers, Gerald Gazdar and his colleagues(1982) have argued for a joint account of syntax andsemantics that is like LFG in eschewing the use of trans-formations, but unlike it in positing only one level ofsyntactic description.
The syntactic apparatus is basedon a nonstandard interpretation of phrase-structure rulesand on the use of metarules.
The formal consequencesof both these devices have been investigated.6.1.
Node admissibilityThere are two ways of interpreting the function of CFrules.
The first, and most common, is to treat them asrules for rewriting strings.
Derivation trees can then beseen as canonical representatives of classes of derivationsproducing the same string, differing only in the order inwhich the same productions are applied.The second interpretation of CF rules is as constraintson derivation trees: a legal derivation tree is one in whicheach node is "admitted' by a rule, i.e., each node domi-nates a sequence of nodes in a manner sanctioned by arule.
For CF rules, the two interpretations obviouslygenerate the same strings and the same set of trees.Following a suggestion of McCawley's, Peters andRitchie (1973b) showed that, if one considered context-sensitive rules from the node-admissibility point of view,the languages defined were still CF.
Thus, for example,the use of CS rules in the base to impose subcategori-zation restrictions does not increase the weak generativecapacity of the base component.
(For some differentrestrictions of context-sensitive rules that guarantee thatonly CFLs will be generated, see Baker (1972).
)Rounds (1970b) gives a simpler proof of Peters andRitchie's node admissibility result, using the techniquesfrom tree-automata heory, a generalization to trees offinite state automata theory for strings.
Just as a finite-state automaton (FSA) accepts a string by reading it onecharacter at a time, changing its state at each transition, afinite-state tree automaton (FSTA) traverses trees, propa-gating states.
The top-down FSTA "attaches" a startingstate (from a finite set) to the root of the tree.
Transi-tions are allowed by productions of the form(q, a, n) => (q, ..... q,,)such that if state q is being applied to a node labeled aand dominating n descendants, then state qi should beapplied to its ith descendant.
Acceptance occurs if allleaves of the tree end up labeled with states in theaccepting subset.
The bottom-up FSTA is similar: start-ing states are attached to the leaves of the tree and theproductions are of the form(a, n, (q, ..... q )  => q)indicating that, if a node labeied a dominates n descend-ants, each labeled with states ql to q,e then node a getslabeled with state q.
Acceptance occurs when the root islabeled by a state from the subset of accepting states.As is the case with FSAs, FSTAs of both varieties canbe either deterministic or nondeterministic.
A set oftrees is said to be recognizable if it is accepted by anondeterministic bottom-up FSTA.
Once again, as withFSAs, any set of trees accepted by a nondeterministicbottom-up FSTA is accepted by a deterministic bottom-up FSTA, but the result does not hold for top-downComputational Linguistics, Volume 10, Numbers 3-4, July-December 1984 169C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesFSTA, even though the recognizable sets are exactly thelanguages recognized by nondeterministic top-downFSTAs.A set of trees is local if it is the set of derivation treesof a CF grammar.
Clearly, every local set is recognizableby a one-state bottom-up FSTA that checks at each nodeto verify that it satisfies a CF production.
Furthermore,the yield of a recognizable set of trees (the set of stringsit generates) is CF.
Not all recognizable sets are local: anexample is the set of trees that satisfies the constraints ofX-bar theory and the 0-criterion.
However, they can allbe mapped into local sets by a simple homomorphicmapping.
7 Rounds's proof (1970a) that CS rules undernode admissibility generate only CFLs involves showingthat the set of trees accepted by the rules is recognizable- i.e., that there is a nondeterministic bottom-up FSTAthat can check at each node that some node admissibilitycondition holds there.
This requires confirming that the"strictly context-free" part of the rule holds and that aproper analysis of the tree passing through the nodesatisfies the "context-sensitive" part of the rule.Joshi and Levy (1977) strengthened Peters andRitchie's result by showing that the node admissibilityconditions could also include arbitrary Boolean combina-tions of dominance conditions: a node could specify abounded set of labels that must occur either immediatelyabove it along a path to the root, or immediately below iton a path to the frontier.In general, the CF grammars constructed in the proofof weak equivalence to the CS grammars under nodeadmissibility are much larger than the original, and notuseful for practical recognition.
Joshi, Levy, and Yueh(1981), however, show how Earley's algorithm can beextended to a parser that uses the local constraints direct-ly.6.2.
Metaru lesThe second important mechanism used by Gazdar(1982) is metarules, or rules that apply to rules toproduce other rules.
Using standard notation for CFrules, one example of a metarule that could replace theApects transformation k own as "particle movement" isV-~ VNPtX  => V-~ VPtN\[-PRO\]XThe symbol X here behaves like variables in structuralanalyses of Aspects transformations.
If such variables arerestricted to being used as abbreviations, that is, if theyare allowed to range only from a finite subset of stringsover the vocabulary, then closing the grammar under themetarules produces only a finite set of derived rules; andthus the generative power of the formalism is notincreased.
If, on the other hand, X is allowed to rangeover strings of unbounded length, as are the essential vari-ables of transformational theory, then the consequencesare less clear.
It is well known, for example, that, if the7 This mapping is a bottom-up finite-state ree transducer that simplylabels each node with the state the recognizing bottom-up FSTA wouldhave been in at that node.right-hand sides of phrase structure rules are allowed tobe arbitrary regular expressions, the generated languagesare still context-free.
Might something like this not behappening with essential variables in metarules?
It turnsout that such is not the case.The formal consequences of the presence of essentialvariables in metarules depend on the presence of anotherdevice, the so-called phantom categories.
It may beconvenient in formulating metarules to allow, in the left-hand sides of rules, occurrences of syntactic ategoriesthat are never introdu~ced by the grammar, i.e., that neverappear in the right-hand sides of rules.
In standard CFLs,these are called useless categories," rules containing themcan simply be dropped, with no change in weak genera-tive capacity.
Not so with metarules: it is possible formetarules to be used to rewrite rules containing phantomcategories into rules without them.
Such a device wasproposed at one time as a way to implement passives inthe GPSG framework.Uszkoreit and Peters (1983) have shown that essentialvariables in metarules are powerful devices indeed: CFgrammars with metarules that use at most one essentialvariable and allow phantom categories can generate allrecursively enumerable sets.
Even if phantom categoriesare banned, some nonrecursive sets can be generated aslong as the use of at least one essential variable isallowed.Two constraints on metarules have been proposed torestrict the generative capacity of metarule systems.Gazdar (1982) has suggested replacing essential vari-ables by abbreviative ones, i.e.
variables that can onlyrange over a finite set of (predetermined) alternatives.Shieber et al (1983) argue that a generalization is lost inso doing, in the sense that the class of instantiations ofthe variable must be defined bye extension rather than byintension.
Given the alternative, this seems a small priceto pay.The other constraint, suggested by Gazdar and Pullum(1982), is finite closure of the metarule derivation proc-ess: no metarule is allowed to apply more than once inthe derivation of a rule.
Shieber et al (1983) presentseveral examples, namely the treatment of discontinuousnoun phrases in Walpiri, adverb distribution in German,and causatives in Japanese, that cannot be handled underthe finite closure constraint.It should be noted that other ways of using one gram-mar to generate the rules of another have been proposed.VanWijngaarden (1969), for example, presented ascheme in which one grammar's sentences are the rules ofanother.
Greibach (1974) gives some of its properties.7.
T ree  Ad junct  GrammarThe tree adjunct grammars (TAG) of Joshi and hiscolleagues (1982, 1984) provide a different way ofaccounting for syntactic dependencies.
A TAG consistsof two finite sets of finite trees, the centre trees and theadjunct trees.170 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesThe centre trees correspond to the surface structuresof the "kernel" sentences of the languages.
The root ofthe adjunct trees is labelled with a nonterminal symbolthat also appears exactly once on the frontier of the tree.All other frontier nodes are labelled with terminalsymbols.
Derivations in TAGs are defined by repeatedapplication of the adjunction operation.
If c is a centretree containing an occurrence of a nonterminal A, and a?
is an adjunct tree whose root (and one node n on thefrontier) is labelled A, then the adjunction of a to c isperformed by "detaching" from c the subtree t rooted atA, attaching a in its place, and reattaching t at node n.Adjunction may then be seen as a tree analogue of acontext-free derivation for strings (Rounds 1970a).
Thestring languages obtained by taking the yields of the treelanguages generated by TAGs are called tree adjunctlanguages (TAL).In TAGs, all long-distance dependencies are the resultof adjunctions eparating nodes that at one point in thederivation were "close".
Both crossing and noncrossingdependencies can be represented (Joshi 1983)).
Theformal properties of TALs are fully discussed by Joshi,Levy, and Takahashi (1975); Joshi and Levy (1982);and Yokomori and Joshi (to appe~ar).
Of particular inter-est are the following.TALs properly contain the CFLs and are properlycontained in the indexed languages, which in turn areproperly contained in the CSLs.
Although the indexedlanguages contain NP-complete languages, TALs aremuch better behaved: Joshi and Yokomori report(personal communication) an O(n 4) recognition algorithmand conjecture that an O(n 3) bound may be possible.8.
Stratificational GrammarThe constituent and functional structures of LFG, themetarules of GPSG, the constraints on deep and surfacestructures in TG, and the two-level grammars of vanWijngaarden are all different ways in which syntacticconstraints can be distributed across more than onestructure.
The Stratificational Grammar (SG) of Lamband Gleason (Lamb 1966, Gleason 1964) is yet another.SG postulates the existence of several coupled compo-nents, known as strata; phonology, morphology, syntax,and semology are examples of linguistic strata.
Eachstratum specifies a set of correct structures, and an utter-ance has a representative structure at each stratum.
Thestrata are linearly ordered and constrained b.9 a realiza-tion relation.Following Gleason's model, Borgida (1983) definesthe realization relation so that it couples the applicationof specific pairs of productions (or sequences ofproductions) in the different grammars.
Note that this isa generalization of the pairing of syntactic and semanticrules suggested by Montague, for example.With any derivation in a rewrite grammar, one canassociate a string of the productions used in the deriva-tion.
If a canonical order is imposed on the derivations -for example, that the leftmost nonterminal must be thenext one to be expanded - a unique string ofproductions can be associated with each derivation tree.A two-level stratifieational grammar consists of tworewrite grammars G 1 and G 2, called tactics, with sets ofproductions Pi and P2, respectively, and a realizationrelation R, which is a finite set of pairs, each consistingof a string of productions of P1 and a string ofproductions of P2.
A derivation D~ in G~ is realized by aderivation D 2 in G 2 if the strings of productions z and s 2associated with D~ and D 2 can be decomposed intosubstrings ~=ur.u" and s2=vr..ve respectively, such thatR(u,,v,), for all i from 1 to n. The language generated bya two-level SG is the set of string generated erivations inG 2 that realize derivations in Grextended to more thantwo strata.Because the realization relation binds derivations, it isthe strong generative capacity of the tactics that deter-mines the languages generated.
Borgida (1983) studiedthe languages of two-level SGs as the strong generativecapacity of the tactics is systematically varied.
Some ofhis results are unexpected.
All r.e.
languages can begenerated by two-level SGs with CF tactics.
On the otherhand, if the upper tactics are restricted to being right-re-cursive, only CFLs can be generated, even with type 0lower tactics.
If the grammars are restricted to have nolength-decreasing rules, the languages describable by SGslie in the class of quasi-real time languages, defined asrecognizable by nondeterministic TMs in linear time.The principal feature of SGs that accounts for highgenerative power is the presence of left recursion in thetactics: to escape from the regular languages, one needsleft recursion on at least one stratum; to escape context-free languages, two non-right-recursive strata are needed.These results apply to SGs with arbitrary number of stra-ta.9.
Seeking SignificanceHow, then, can metatheoretical results be useful inselecting among syntactic theories?
The obvious route,of course, is to claim that the computationally mostrestrictive theory is preferable.
However, this compar-ison is useful only if the theories to be compared rest ona number of shared assumptions and observationsconcerning the scope of the syntax, the computationalproperties of the human processor and the relationbetween the processor and the syntactic theory.In this section, we first briefly consider the assumptionof common syntactic coverage and the computationalconsequences of theory decomposition.
We then askhow metatheoretical results can be used first as lowerbounds and then as upper bounds on acceptable theories.9.1.
CoverageCompeting linguistic theories must obviously agree onthe burden of their respective syntactic omponents.
Weconsider here one example of a constraint for which twoanalyses have been presented, one purportedlycompletely syntactic, and the other partly semantic.
TheComputational Linguistics, Volume 10, Numbers 3-4, July-December 1984 171C.
Raymond Perrault On the Mathematical Properties of Linguistic Theoriesproblem at hand is the distribution of the so-called polari-ty-sensitive items, such as any and the metaphorical senseof lift a finger.
Simply put, these terms need to appearwithin the scope of a polarity reverser, such as not, orrarely.
The question is: how are scope and polarity rever-ser defined?
In Linebarger's yntactic analysis (1980),the scope relation is defined on the logical forms of thegovernment and binding theory (GB):An item is in the immediate scope of NOT if (1) itoccurs only in the proposition which is the entire scope ofNOT and (2) within the proposition there are no logicalelements intervening between it and NOT.In this analysis, scope and intervening must be definedconfigurationally, and one assumes that logical element isdefined in the lexicon.
Note that not is the only lexicalelement hat can be a license.
Linebarger assumes thatall other cases are, strictly speaking, ill formed andsalvaged only by the availability of an implicature whichcan be formalized to contain the polarity items in theappropriate r lation to NOT.
(Ladusaw 1983)Ladusaw's analysis (1979), within the framework ofMontague grammar, is in three parts:1.
A negative polarity item will be acceptable only if itis in the scope of a polarity-reversing expression.2.
For any two expressions a and /3, constituents of asentence S, a is in the scope of/3 with respect o acomposition structure of S, S t, iff the interpretationof a is used in the formulation of the argument of/3'sinterpretation i S t .3.
An expression D is a polarity reverser with respect oan interpretation function ,~ if and only if, for allexpressions X and y,sq~(X) _c (~(y) = > ck(d(y)) c ck(d(S))In (1), "acceptable" is predicated of negative polarityitems; these are clearly parts of surface structures, andthus syntactic objects.
The condition on acceptability isin terms of scope and polarity-reversing expression.
In (3),polarity reverser is applied to syntactic objects anddefined in terms of their denotations.
In (2) a is in thescope of/3.
is defined again of syntactic objects a and/3,but in terms of the function that interprets the structurethey occur in, not of their denotations.
So the conditionapplies to syntactic structures, but is defined in terms ofthe denotations of parts of that structure and in terms ofthe interpretation function itself.
Although it would besatisfying to do so, there appears to be no natural way torecast Ladusaw's constraint as one that is fully semantic,namely, by making the interpretation function partial(i.e., in a way that allows John knows anything to begrammatical but uninterpretable) because the definitionof scope is in terms of the interpretation function, not thedenotations themselves.
We seem condemned to straddlethe fence on this one.Thus we have here one theory that deals, completelywithin the syntactic domain, only with the license not,and another that accounts for a much broader range oflicenses by imposing on syntactic structures conditionsdefined in terms of their interpretations and of the inter-pretation function itself.
They are computationallyincomparable.We close this section with an aside on the separationof constraints.
Constraint separation can occur in twoways.
In the case of polarity-sensitive items, it takesplace across the syntax-semantics boundary.
In severalsyntactic theories, such as GB and LFG, it can also occurwithin the syntactic theory itself: grammaticality in LFG,for example, is defined in terms of the existence of pairsof appropriately related constituent and functional struc-tures.In general, the class resulting from the intersection ofthe separated classes will be at least as large as either ofthem: e.g., the intersection of two CFLs is not always aCFL.
More interesting is the fact that separation some-times has beneficial computational effects.
Consider, forexample, the constraint in many programming languagesthat variables can only occur in the scope of a declara-tion for them.
This constraint cannot be imposed by aCFG but can be by an indexed grammar, at the cost of adramatic increase in recognition complexity.
In practice,however, the requirement is simply not checked by theparser, which only recognizes CFLs.
The declarationconditions are checked separately by a process that trav-erses the parse tree.
In this case, the overall recognitioncomplexity remains some low-order polynomial.
It is notclear to me whether one wants to consider the declara-tion requirement syntactic or not.
The point is that, inthis case, the "unified account" is more general, andcomputationally more onerous, than the modular one.Some arguments of this kind can be found in Berwickand Weinberg (1982).9.2.
Metatheoretical results as lower boundsThe first use of formal results is to argue that a theoryshould be rejected if it is insufficiently powerful toaccount for observed constraints.
Chomsky used thisstrategy initially against finite-state grammars 9 and thenagainst CFGs.
It obviously first requires extracting fromempirical observation (and decisions about idealization)what the minimal generative capacity and recognitioncomplexity of actual languages are.
Several argumentshave been made against he weak generative adequacy ofCFGs.
The best known of these are Bar-Hillel's claim(1961) based on the occurrence of respectively andPostal's (1964a) on nominalization i  Mohawk.
Higgin-botham (1984) claims non-context-freeness for English8 Following Faueonnier, Ladusaw's denotation functions take as theirvalues sets, ordered as usual.
Sentences, for example, get as deno-tations the set of all worlds in which they are true.9 There has always been interest in finite-state grammars to account forsome perceptual constraints on sentence recognition, such as the diffi-culty of center-embedded sentences - e.g., "The rat that the cat thatthe dog chase ate died" (Langendoen 1975, Church 1981, Langendoenand Langsam 1984).
They have also provided useful models inmorphology (Kay 1983, Koskenniemi 1983) and phonology (Church1983),172 Computational Linguistics, Volume 10, Numbers 3-4, July-D~cember 1984C.
Raymond Perrault On the Mathematical Properties of Linguistic Theorieson the basis of sentences containing such that.
Postaland Langendoen (this issue, p. 177) do so with cases ofsluicing?
Pullum and Gazdar (1982) (convincingly, Ibelieve) refute the first two cases by claiming that theconstraints on which they are based do not in fact hold.Similarly, Pullum (this issue, p. 182) argues against Postaland Langendoen, and against Higginbotham, again onthe basis of the linguistic facts?
Pullum and Gazdar alsoconsider the case of verb and noun-phrase ordering inDutch; although they show that no evidence has beengiven suggesting that the weak generative capacity ofDutch is greater than context-free, the phrase structuretrees generated by their fragment are not obviouslyadequate for a compositional semantic analysis.
Thispoint is also made by Bresnan et al (1982).The most convincing evidence so far against he weakcontext-freeness of natural anguages comes from Swiss-German?
Shieber (1984) shows that, like Dutch, Swiss-German allows cross-serial order in subordinate clausesbut also requires that objects be marked for case, as inGerman.
Given that the verb hdlfed 'hell ~' takes a dativeobject while aastriiche 'paint' and lo'nd 'let' take accusa-tive objects, we get the following subordinate clauses,which can be made into complete sentences by prefixingthem with Jan sdit das ' Jan says that'??
.
.
mer em Hans es huus h~ilfed aastriiche?
.
.
we Hans-DAT the house-ACC helped paint?
.
.
we helped Hans paint the house?
.
.
*me em Hans es huus 16nd aastriiche?
.
.
we Hans-DAT the house-ACC let paint?
.
.
we let Hans paint the house?
.
.
mer d'chind em Hans es huus ltind h~ilfedaastriiche?
.
.
we the children-ACC Hans-DAT the house-ACClet help paint?
.
.
we let the children help Hans paint the house?
.
.
*mer d'chind de Hans es huus ltmd h~ilfedaastriiche?
.
.
we the children-ACC Hans-ACC the house-ACClet help paint?
.
.
we let the children help Hans paint the houseThe proof that Swiss-German (SG) is not context-freeis classic: intersect SG with the following regularlanguage:Jan s~iit das mer (d'chind)*(em Hans)*es huus htind wele (laa)*(hfilfe)* aastriche.With some care, Shieber argues from the data that SG13 L is the languageJan s~iit das mer (d'chind) m (em Hans) mes huus h~ind wele (laa) m (h~ilfe) m aastriche.which is not context-free.
Since context-free languagesare closed under intersection with regular languages,Swiss-German is not context-free either.Hintikka (1977) claims that English is not recursive,let alne context-free, based on the distribution of thewords any and every.
His account of why John knowseverything is grammatical while John knows anything isnot, is that any can appear only in contexts where replac-ing it with every changes the meaning?
If equivalence ofmeaning is taken to be logical equivalence, this meansthat gramm~/ticality s dependent on the determination ofequivalence of logical formulas, an undecidable problem?Several responses could be made to Hintikka's claim?One is to argue, as did Ladusaw (1979), that theconstraint is semantic, not syntactic?
Another route,followed by Chomsky (1980), is to claim that a simplersolution is available, namely, one that replaces logicalequivalence with syntactic identity of some kind oflogical form.
This is the basis for Linebarger's analysis.9.3.
Metatheoret ica l  results as upper boundsIn the preceding section, we discussed ways in whichformal results about syntactic theories can be usedagainst them on the grounds that they show them to beinsufficiently powerful to account for the observed ata.Now, given a theory that is powerful enough, can itsformal properties be used against it on the basis that itfails to exclude impossible languages?The classic case of an argument of this form is Petersand Ritchie's argument against the TG model, discussedin Section 4.More generally, the premises are the following:1.
The possible languages are decidable.2.
The correct syntactic theory must generate exactlythe possible languages?3.
The correct syntactic theory is T.4.
The class Of languages C generated by T is a prioritoo large to be the class of possible languages?One conclusion from this argument is that theory T isincorrect, i.e., that assumption (3) fails?
Chomsky rejectsassumption (1) instead, insisting that the possiblelanguages are those that can be learned, t?Although Chomsky also claims that the class of possi-ble languages is finite, tt the crucial concern here is that,finite or not, the class of possible languages could containlanguages that are not recursive, or even not recursivelyenumerabie.
For example, let L be a non-recursivelanguage and L r its complement (also non-recursive).Let s be some string of L and s p some string of L r. Theprocedure by which the subject chobses L if s is encount-ered before s r and L r otherwise will learn one of L or L ~.10 Learning algorithms can be compared along several dimensions.
Fora mathematical framework for learnability theory, see Osherson et al(1983).11 Actually, finiteness is claimed for the class of core grammars, fromwhich the possible languages are assumed to be derived.
Corelanguages and possible languages would be the same only "under ideal-ized conditions that are never realized in fact in the real world of heter-ogeneous peech communities .
.
.
.
Each actual 'language' willincorporate a periphery of borrowings, historical residues, inventions,and so on, which we can hardly expect o - and indeed would not wantto - incorporate within a principled theory of UG."
(Chomsky 1981: 8)Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 173C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesChomsky (1980) argues convincingly that there is nocase for natural languages being necessarily recursive.Nevertheless, languages might just happen to be recur-sive.
Putnam (1961) gives three reasons he claims "pointin this direction":1.
"Speakers can presumably classify sentences asacceptable or unacceptable, deviant or nondeviant,et cetera, without reliance on extra-linguisticcontexts.
There are of course exceptions to thisrule...",2.
Grammaticality judgments can be made for nonsensesentences ,3.
Grammars can be learned.The first reason is most puzzlir~g.
The reference to"extra-linguistic context" is irrelevant; without it, reason(1) seems to be assert ing th~/t acceptability can bedecided except where it cannot be.
With respect o thesecond reason, the fact that grammaticality judgmentscould be made for some nonsense sentences in no wayaffects the question of whether they can be made for allgrammatical sentences.
Finally, languages could belearnable without being recursive, as it is possible that allthe rules that need to be acquired could be on the basisof sentences for which the recognition proceduresucceeds.Peters and Ritchie (1973a) contains a suggestive buthardly conclusive case for contingent recursivity:1.
Every TG has an exponentially bounded cyclingfunction, and thus generates only recursivelanguages,2.
Every natural language has a descriptively adequateTG,  and3.
The complexity of languages investigated so far istypical of the class.If learnability rather than recognizability is the defin-ing characteristic of possible languages, no claim refutinga theory on the grounds that it allows difficult languageswill bear any weight, unless it can also be shown thatpossible languages are in fact easier to recognize than therecognizability theory predicts them to be.
However, oureveryday experience with language understanding leadsus to think that syntactic recognition is a computationallyefficient process - an observation, of course, that is thebasis for Marcus's claim (1980) that a large part of it canbe done in linear time, if not in real time.
How are we toreconcile this with the O(g)-results we have for mosttheories, where g is at least quadratic?
t2These intuitive conclusions are based on observations(1) of "everyday" sentences, (2) where some nonsyntac-tic processing is done in parallel, (3) by the humanprocessor.
Each of these points is important.12 It has already been pointed out that O(g) results are upper bounds,and showing that a recognition problem, for example, is O(g) does notmean that, for any Language, it is necessary to reach the upper-bound.Better upper-bounds can be achieved by tighter proofs, not just bybetter algorithms.Although recognition may appear to be done in realtime for most sentences encountered ay to day, theO-results are asymptotic worst-case measures.
It istherefore essential to obtain measures of recognitiontimes for a variety of strings of words, whether sentencesor not, and especially see if there are short, difficult ones.There are at least two cases of interest here.
The first isthat of garden-path sentences uch as The horse racedpast the barn fell and Have the students who failed theexam take the supplementary, which are globally unambig-uous but locally ambiguous.
These appear to be psycho-logically difficult.
Another case is that of sentences that,in most grammars, are ambiguous because of attachmentchoices, such as those discussed by Church and Patil(1982).
Finding one parse of these sentences i easy, butfinding them all may be exponentially difficult.
Psycho-logical measures how these sentences not to be difficult,suggesting that not all parses are constructed or  that theycan all be examined in parallel.O-results depend on some underlying machine model,and most of the results known for language recognitionhave been obtained on RAMs.
Can implementationchanges improve things on relevant range?
Asmentioned above, the sequential models are all polynomi-ally related, and no problem not having a polynomialtime solution on a sequential machine is likely to haveone on a parallel machine limited to at most a polynomialnumber of processors, at least if P is not equal to NP.Both these results restrict the improvement one canobtain by changing implementation, but are of little usein comparing algorithms of low complexity.
Berwick andWeinberg (1982) give examples of how algorithms of lowcomplexity may have different implementations differingby large constant factors.
In particular, changes in theform of the grammar and in its representation may havethis effect.It is well-known that implementation of machines withinfinite storage on finite devices leads to a change inspecification.
A context-free parser implemented on amachine with finite memory will have a bounded stackand therefore recognize only finite-state languages.
Thelanguage recognized by the implemented machine couldtherefore be recognized by another machine in lineartime.
Although one would rarely use this strategy as adesign principle, a variant of it is more plausible: use arestriction of the general method for a subset of theinputs and revert to the general method when the specialcase fails.
Marcus's parser (1980) with its bounded look-ahead is a good example.
Sentences parsable within theallowed look-ahead have "quick" parses, but some gram-matical sentences, such as "garden path" sentencescannot be recognized without an extension to the mech-anism that would distort the complexity measures.
Aconsequence of the possibility of implementation of thischaracter is that observations of their operation ought toshow "discontinuities" in the processing time, dependingon whether an input is in or out of the restricted subset.174 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesThere is obviously much more of this story to be told.Al low me to speculate as to how it might go.
We mayend up with a space of linguistic theories, differing in theidealization of the data they assume, in the way theydecompose constraints, and in the procedural specifica-tions they postulate.
I take it that two theories maydiffer in that the second simply provides more detail thanthe first as to how constraints specified by the first are tobe used.
Our observations, in particular our measure-ments of necessary resources, are drawn from the"ultimate implementation", but this does not mean thatthe "ultimately low-level theory" is necessarily the mostinformative, or that less procedural theories are notuseful stepping stones to more pr6cedural ones.It is also not clear that theories of different computa-tional power may not be useful as descriptions of differ-ent parts of the syntactic apparatus.
For example, it maybe easier to learn statements of constraints within theframework of a general machine.
The constraints oncelearned might then be subjected to transformation toproduce more efficient special-purpose processors alsoimposing resource limitations.Whatever we decide to make of existing formalresults, it is clear that continuing contact with thecomplexity community is important.
The driving prob-lems there are the P = NP question, the determination oflower bounds, the study of time-space tradeoffs, and thecomplexity of parallel computations.
We still have somemethodological house-cleaning to do, but I don't see howwe can avoid being affected by the outcome of theirinvestigations.ReferencesAho, A.V.
1968 Indexed Grammars: An Extension of the Context-Free Grammars.
JACM 15:647-671.Aho, A.V.
; Hopcroft, J.E.
; and Ullman, J.D.
1974 The Design andAnalysis of Computer Algorithms.
Addison-Wesley, Reading, Massa-chusetts.Aho, A.V.
and Ullman, J.D.
1972 The Theory of Parsing, Translation,and Compiling.
Prentice Hall, Englewood Cliffs, New Jersey.Baker, B.S.
1972 Arbitrary Grammars Generating Context-FreeLanguages.
Center for Research in Computing Technology,Harvard University.Bar-Hillel, Y.; Perlis, M.; and Shamir, E. 1961 On Formal Propertiesof Simple Phrase Structure Grammars.
Z. Phonetik, Sprach.
Komm.14: 143-172.Berwick, R.C.
and Weinberg, A.
1982 Parsing Efficiency, Computa-tional Complexity, and the Evaluation of Grammatical Theories.Linguistic Inquiry 13: 165-191.Berwick, R.C.
1982 Computational Complexity and Lexical Func-tional Grammar.
American Journal of Computational Linguistics8(3-4): 97-109.Berwick, R.C.
1984 Strong Generative Capacity, Weak GenerativeCapacity and Modern Linguistic Theories.
CL 10(3-4):, 189-203.Borgida, A.T. 1983 Some Formal Results about Stratificational Gram-mars and their Relevance to Linguistics.
Math.
Sys.
Th.
16: 29-56.Bresnan, J.; Kaplan, R.M.
; Peters, P.S.
; and Zaenen, A.
1982 Cross-serial Dependencies in Dutch.
Ling.
lnq.
13.Chomsky, N. 1980 Rules and Representations.
Columbia UniversityPress, New York, New York.Chomsky, N. 1981 Lectures on Government and Binding: the PisaLectures.
Forts Publications Holland, Dordrecht.Church, K. 1981 On Memory Limitations in Natural Language Proc-essing.
Master Th., M.I.T.Church, K. 1983 A Finite-State Parser for Use in Speech Recognition.Proceedings of 21st Annual Meeting of the ACL.
Cambridge, Massa-chusetts: 91-97.Church, K. and Patti, R. 1982 Coping with Syntactic Ambiguity orHow to Put the Block on the Table.
American Journal of Computa-tional Linguistics 8(3-4): 139-149.Cook, S.A. 1981 Towards a Complexity Theory of SynchronousParallel Computation.
L'Enseignement Mathdmatique 27: 99-124.Earley, J.
1970 An Efficient Context-Free Parsing Algorithm.Communications of A CM 13: 94-102.Gazdar, G. 1982 Phrase Structure Grammar.
In Jacobson, P. andPullum, G., Eds., The Nature of Syntactic Representation.
Reidel,Dordrecht.Gazdar, G. and Pullum, G. 1982 Generalized Phrase Structure Gram-mar: A Theoretical Synopsis.
Indiana Univ.
Linguistic Club.Gleason, H.A.
Jr, 1964 The Organization of Language: a Stratifica-tional View.
In Monograph Series on Language and Linguistics, no.21.
Georgetown University Press, Washington.Graham, S.L.
; Harrison, M.A.
; and Ruzzo, W.L.
1980 An ImprovedContext-Free Recognizer.
ACM Trans.
on Prog.
Lang.
and Systems2: 415-462.Greibach, S.A. 1974 Some Restrictions on W-Grammars.
Int.
J. ofComp.
and Info.
Sc.
3: 415-462.Hays, D.G.
1962 Automatic Language Data Processing.
Prentice Hall,Englewood Cliffs, New Jersey.Higginbotham, J.
1984 English Is Not a Context-Free Language.Ling.
Inq.
15: 225-234.Hintikka, J.K.K.
1977 Quantifiers in Natural Language: Some LogicalProblems.
II.
Linguistics and Philosophy 2:153-172.Hopcroft, J.E.
and Ullman, J.
1979 Introduction to Automata Theory.Languages and Computation.
Addison Wesley, Reading, Massachu-setts.Joshi, A.K.
1983 Factoring Recursion and Dependencies: an Aspectof Tree Adjoining Grammars and a Comparison of Some FormalProperties of TAGs.
Proceedings of 21st Annual Meeting of theACL.
Cambridge, Massachusetts: 7-15.Joshi, A.K.
1984 How Much Context-Sensitivity Is Required toProvide Reasonable Structual Descriptions: Tree Adjoining Gram-mars.
In Natural Language Processing: Psycholinguistic.
Computa-tional and Theoretical Properties.
Cambridge University Press, NewYork, New York.Joshi, A.K.
and Levy, L.S.
1977 Constraints on StructuralDescriptions: Local Transformation.
SIAM J. on Computing 6:272-284.Joshi, A.K.
and Levy, L.S.
1982 Phrase Structure Trees Bear MoreFruit than You Would Have Thought.
American Journal of Compu-tational Linguistics 8(1): 1-11.Joshi, A.K.
; Levy, L.S.
; and Takahashi, M. 1975 Tree Adjunct Gram-mars.
J. Comp.
and Sys.
Sc.
10: 136-163.Joshi, A.K.
; Levy, L.S.
; and Yueh, K. 1980 Local Constraints onProgramming Languages, Part 1: Syntax.
Th.
Comp.
Sc.
12:265-290.Kaplan R. and Bresnan, J.
1982 LexicakFunctional Grammar: aFormal System for Grammatical Representation.
In Bresnan, J.,Ed., The Mental Representation f Grammatical Relations.
MIT Press,Cambridge, Massachusetts: 173-281.Kasami, T. 1965 An Efficient Recognition and Syntax Algorithm forContext-Free Languages.
AF-CRL-65-758, Air Force CambridgeResearch Laboratory, Bedford, Massachusetts.Kay, M. 1983 When Meta-Rules Are Not Meta-Rules.
in Sparck-Jones, K. and Wilks, Y., Eds., Automatic Natural Language Parsing.John Wiley, New York.Koskenniemi, K. 1983 Two-Level Model for Morphological Analysis.Ph.D.
Th., Univ.
of Helsinki.Ladusaw, W. 1979 Polarity Sensitivity as Inherent Scope Relations.Ph.D.
Th., University of Texas at Austin.Ladusaw, W. 1983 Logical Forms and Conditions on Grammaticality.Ling.
and PhiL 6: 373-392.Lamb, S. 1966 Outline of Stratificational Grammar.
GeorgetownUniversity Press, Washington, DC.Langendoen, D.T.
1975 Finite-State Parsing of Phrase-StructureLanguages and the Status of Readjustment Rules in Grammar.Ling.
lnq.
6(4): 533-554.Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 175C.
Raymond Perrault On the Mathematical Properties of Linguistic TheoriesLangendoen, D.T.
and Langsam, Y.
1984 The Representation ofConstituent Structures for Finite-State Parsing.
Proceedings ofCOLING-84.
Stanford, California: 24-27.LaPointe, S. 1977 Recursiveness and Deletion.
Ling.
Anal 3:227-265.Linebarger, M. 1980 The Grammar of Negative Polarity.
Ph.D. Th.,MIT.Marcus, M.P.
1980 A Theory of Syntactic Recognition for NaturalLanguage.
MIT Press, Cambridge, Massachusetts.Montague, R. 1973 The Proper Treatment of Quantification i Ordi-nary English.
In Hintikka, J.K.K.
; Moravcsik, J.; and Suppes, P.,Eds., Approaches to Natural Language: Proceedings of the 1970 Stan-ford Workshop on Grammar and Semantics.
Reidel, Dordrecht: 221-242.Osher~on, D.N.
; Stob, M.; and Weinstein, S. 1983 Formal Theories ofLanguage Acquisition: Practical and Theoretical Perspectives.Proceedings of 1JCAI-83: 566-572.Pereira, F.C.N.
and Shieber, S. 1984 The Semantics of GrammarFormalisms Seen as Computer Languages.
Proceedings ofCOLING-84:123-129.Peters, P.S.
and Ritchie, R.W.
1969 A Note on the Universal BaseHypothesis.
Ling.
and Phil 5: 150-152.Peters, P.S.
and Ritchie, R.W.
1971 On Restricting the Base Compo-nent of Transformational Grammars.
Inf.
and Control 18: 483-501.Peters, P.S.
and Ritchie, R.W.
1973a On the Generative Power ofTransformational Grammars.
Inf.
Sc.
6: 49-83.Peters, P.S.
and Ritchie, R.W.
1973b Context-Sensitive ImmediateConstituent Analysis - Context-Free Languages.
Math.
Sys.
Theory6: 324-333.Peters, P.S.
and Ritchie, R.W.
1973c Non-Filtering and Local Filter-ing Grammars.
In Hintikka, J.K.K.
; Moravcsik, J.; and Suppes, P.,Eds., Approaches to Natural Language.
Reidel, Dordrecht: 180-194.Petrick, S.R.
1965 Recognition Procedure for Transformational Gram-mars.
Ph.D.
Th., MIT.Postal, P.M. 1964a Limitations of phrase-structure grammars.
In TheStructure of IL~nguage: Readings in the Philosophy of Language.
Pren-tice Hall, Englewood Cliffs, New Jersey: 137-151.Postal, P.M. 1964b Constituent Structure: A Study of ContemporaryModels of Syntactic Structure.
Int.
J. of Amer.
Ling.
3.Postal, P.M. and Langendoen, D.T.
1984 English and the Class ofContext-Free Languages.
CL 10(3-4): 177-181.Pullum, G.K. 1984 On Two Recent Attempts to Show that English IsNot a CFL.
CL 10(3-4): 182-186.Pullum, G.K. and Gazdar, G. 1982 Natural and Context-FreeLanguages.
Ling.
and Phil 4: 471-504.Putnam, H. 1961 Some Issues in the Theory of Grammar.Proceedings, American Math.
Soc.Rounds, W.C. 1970a Mappings and Grammars on Trees.
Math.
Sys.Th.
4(3): 257-287.Rounds, W.C. 1970b Tree-Oriented Proofs of Some Theorems onContext-Free and Indexed Languages.
Second Symp.
on Th.
Comp.Sc., ACM: 109-116.Rounds, W.C. 1973 Complexity of Recognition in Intermediate-LevelLanguages.
Symp.
on Sw. &Aut.
Th., IEEE: 145-158.Rounds, W.C. 1975 A Grammatical Characterization f Exponential-Time Language.
Syrup.
on Found.
of Comp.
Sc., IEEE: 135-143.Ruzzo, W.L.
1979 Uniform Circuit Complexity.
Proceedings of 20thAnnual ACM Syrup.
on Found.
of Comp.
Sc.
: 312-318.Shieber, S.M.
1984 Evidence Against he Context-Freeness of NaturalLanguage.
TN-330, SRI International, Menlo Park, California.
Toappear in Linguistics and Philosophy.Shieber, S.M.
; Stucky, S.U.
; Uszkoreit, H.; and Robinson, J.J. 1983Formal Constraints on Metarules.
Proceedings of 21st Annual Meet-ing of the ACL.
Cambridge, Massachusetts: 22-27.Slocum, J.
1981 A Practical Comparison of Parsing Strategies.Proceedings of the 19th Annual Meeting of the ACL.
Stanford, Cali-fornia: 1-6.Uszkoreit, H. and Peters, P.S.
1983 Essential Variables in Metarules.Technical Note 305, SRI International, Menlo Park, California.Valiant, L. 1975 General Context-Free Recognition in Less ThanCubic Time.
J. Comp.
and Sys.
Sc.
10: 308-315.Van Wijngaarden, A.
1969 Report on the Algorithmic LanguageALGOL 68.
Numerische Mathematik 14: 79-218.Warren, D.S.
1979 Syntax and Semantics of Parsing: An Applicationto Montague Grammar.
Ph.D.
Th., University of Michigan.Yokomori, T. and Joshi, A.K.
to appear Semi-linearity, Parikh-bound-edness and Tree Adjunct Languages.
Inf.
Pr.
Letters.Younger, D.H. 1967 Recognition and Parsing of Context-FreeLanguages in Time n ~.
Inf.
and Control 14: 189-208.176 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
