Coling 2010: Poster Volume, pages 285?293,Beijing, August 2010An Efficient Shift-Reduce Decoding Algorithm for Phrased-BasedMachine TranslationYang Feng, Haitao Mi, Yang Liu and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{fengyang,htmi,yliu,liuqun}@ict.ac.cnAbstractIn statistical machine translation, decod-ing without any reordering constraint isan NP-hard problem.
Inversion Transduc-tion Grammars (ITGs) exploit linguisticstructure and can well balance the neededflexibility against complexity constraints.Currently, translation models with ITGconstraints usually employs the cube-timeCYK algorithm.
In this paper, we presenta shift-reduce decoding algorithm that cangenerate ITG-legal translation from left toright in linear time.
This algorithm runsin a reduce-eager style and is suited tophrase-based models.
Using the state-of-the-art decoder Moses as the baseline, ex-periment results show that the shift-reducealgorithm can significantly improve boththe accuracy and the speed on differenttest sets.1 IntroductionIn statistical machine translation, for the diver-sity of natural languages, the word order ofsource and target language may differ and search-ing through all possible translations is NP-hard(Knight, 1999).
So some measures have to betaken to reduce search space: either using a searchalgorithm with pruning technique or restrictingpossible reorderings.Currently, beam search is widely used (Till-mann and Ney, 2003; Koehn, 2004) to reducesearch space.
However, the pruning techniqueadopted by this algorithm is not risk-free.
As aresult, the best partial translation may be ruled outduring pruning.
The more aggressive the prun-ing is, the more likely the best translation escapes.There should be a tradeoff between the speed andthe accuracy.
If some heuristic knowledge is em-ployed to guide the search, the search algorithmcan discard some implausible hypotheses in ad-vance and focus on more possible ones.Inversion Transduction Grammars (ITGs) per-mit a minimal extra degree of ordering flexibilityand are particularly well suited to modeling or-dering shifts between languages (Wu, 1996; Wu,1997).
They can well balance the needed flex-ibility against complexity constraints.
Recently,ITG has been successfully applied to statisticalmachine translation (Zens and Ney, 2003; Zenset al, 2004; Xiong et al, 2006).
However, ITGgenerally employs the expensive CYK parsing al-gorithm which runs in cube time.
In addition, theCYK algorithm can not calculate language modelexactly in the process of decoding, as it can notcatch the full history context of the left words in ahypothesis.In this paper, we introduce a shift-reduce de-coding algorithm with ITG constraints which runsin a left-to-right manner.
This algorithm parsessource words in the order of their correspondingtranslations on the target side.
In the meantime,it gives all candidate ITG-legal reorderings.
Theshift-reduce algorithm is different from the CYKalgorithm, in particular:?
It produces translation in a left-to-right man-ner.
As a result, language model probabilitycan be calculated more precisely in the lightof full history context.?
It decodes much faster.
Applied with distor-285target side target side target side(a) straight (b) inverted (c) discontinuousFigure 1: Orientation of two blocks.tion limit, shift-reduce decoding algorithmcan run in linear time, while the CYK runsin cube time.?
It holds ITG structures generated during de-coding.
That is to say, it can directly giveITG-legal spans, which leads to faster de-coding.
Furthermore, it can be extended tosyntax-based models.We evaluated the performance of the shift-reduce decoding algorithm by adding ITG con-straints to the state-of-the-art decoder Moses.
Wedid experiments on three data sets: NIST MT08data set, NIST MT05 data set and China Work-shop on Machine Translation 2007 data set.
Com-pared to Moses, the improvements of the accuracyare 1.59, 0.62, 0.8 BLEU score, respectively, andthe speed improvements are 15%, 24%, 30%, re-spectively.2 Decoding with ITG constraintsIn this paper, we employ the shift-reduce algo-rithm to add ITG constraints to phrase-based ma-chine translation model.
It is different from thetraditional shift-reduce algorithm used in naturallanguage parsing.
On one hand, as natural lan-guage parsing has to cope with a high degree ofambiguity, it need take ambiguity into considera-tion.
As a result, the traditional one often suffersshift-reduce divergence.
Nonetheless, the shift-reduce algorithm in this paper does not pay atten-tion to ambiguity and acts in a reduce-eager man-ner.
On the other hand, the traditional algorithmcan not ensure that all reorderings observe ITGconstraints, so we have to modify the traditionalalgorithm to import ITG constraints.We will introduce the shift-reduce decoding al-gorithm in the following two steps: First, we1\1zairu1?
?2shijian2N3diaocha3]4ziliaode4>M5diannao5;?6zaoqie6The laptop with inquiry data on the event was stolen(a)A1The laptopdiannao5withA2zairu1inquiryA3diaocha3dataA4ziliaode4A5on the eventshijian2A6was stolenzaoqie6A7A8A9A10A11(b)Figure 2: A Chinese-to-English sentence pair andits corresponding ITG tree.will deduce how to integrate the shift-reduce al-gorithm and ITG constraints and show its correct-ness (Section 2.1).
Second, we will describe theshift-reduce decoding algorithm in details (Sec-tion 2.2).2.1 Adding ITG constraintsIn the process of decoding, a source phrase is re-garded as a block and a source sentence is seenas a sequence of blocks.
The orientation of twoblocks whose translations are adjacent on the tar-get side can be straight, inverted or discontinu-ous, as shown in Figure 1.
According to ITG,two blocks which are straight or inverted can bemerged into a single block.
For parsing, differ-ent mergence order of a sequence of continuousblocks may yield different derivations.
In con-trast, the phrase-based machine translation doesnot compute reordering probabilities hierarchi-cally, so the mergence order will not impact thecomputation of reordering probabilities.
As aresult, the shift-reduce decoding algorithm neednot take into consideration the shift-reduce diver-gence.
It merges two continuous blocks as soonas possible, acting in a reduce-eager style.Every ITG-legal sentence pair has a corre-286S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptopS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop withS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry(a) (b) (c)S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry dataS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry dataS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry data on the event(d) (e) (f)S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry data on the eventS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry data on the eventS zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6The laptop with inquiry data on the event(g) (h) (i)Figure 3: The partial translation procedure of the sentence in Figure 2.sponding ITG tree, and source words coveredby every node (eg.
A1, ..., A11 in Figure 2(b))in the ITG tree can be seen as a block.
Bywatching the tree in Figure 2, we can find thata block must be adjacent to the block either onits left or on its right, then they can be mergedinto a larger block.
For example, A2 matchesthe block [zairu1] and A8 matches the block[shijian2 diaocha3 ziliaode4].1 The two blocksare adjacent and they are merged into a largerblock [zairu1 shijian2 diaocha3 ziliaode4],covered by A9.
The procedure of translatingzairu1 shijian2 diaocha3 ziliaode4 diannao5is illustrated in Figure 3.For a hypothesis during decoding, we assign itthree factors: the current block, the left neigh-boring uncovered span and the right neighbor-ing uncovered span.
For example, in Figure3(c), the current block is [diaocha3] and the leftneighboring uncovered span is [shijian2] and theright neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-ing block, for it is not adjacent to [diaocha3].
Thenext covered block is [ziliaode4] (as shown inFigure 3(d)).
For [diaocha3] and [ziliaode4] areadjacent, they are merged.
In Figure 3(e), the cur-rent block is [diaocha3 ziliaode4].A sentence is translated with ITG constraints iff1The words within a block are sorted by their order in thesource sentence.its source side can be covered by an ITG tree.
Thatis to say, for every hypothesis during decoding, thenext block to cover must be selected from the leftor right neighboring uncovered span.First, we show that if the next block to cover isselected in this way, the translation must observeITG constraints.
For every hypothesis during de-coding, the immediate left and right words of thecurrent block face the following three conditions:(1) The immediately left word is not coveredand the immediately right word is covered, thenthe next block to cover must be selected from theleft neighboring uncovered span, eg.
for the cur-rent block [diaocha3 ziliaode4] in Figure 3(e).
Inthis condition, the ITG tree can be constructed inthe following two ways: either all words in the leftneighboring uncovered span are translated first,then this span is merged with the current span(taking three nodes as an example, this case isshown in Figure 4(a)), or the right part of the leftneighboring uncovered span is merged with thecurrent block first, then the new block is mergedwith the rest part of the left neighboring uncov-ered span (shown in Figure 4(b)).
In a word, onlyafter all words in the left neighboring uncoveredspan are covered, other words can be covered.
(2) The immediately right word is not coveredand the immediately left word is covered.
Simi-larly, only after all words in the right neighboringuncovered span are covered, other words can be287(a) (b)Figure 4: The two ways that the current block ismerged with its left neighboring uncovered span.The third node in the first row denotes the currentblock, the first and second nodes in the first rowdenote left and right parts of the left neighboringuncovered span, respectively.covered.
(3) The immediately left and right words areneither covered.
The next block can be selectedfrom either the left or the right neighboring uncov-ered span until the immediate left or right word iscovered.The above operations can be performed recur-sively until the whole source sentence is mergedinto a single block, so the reordering observes ITGconstraints.Now, we show that translation which is not gen-erated in the above way must violate ITG con-straints.If the next block is selected out of the neighbor-ing uncovered spans, the current block can be nei-ther adjacent to the last covered block nor adjacentto the selected next block, so the current block cannot be merged with any block and the whole sen-tence can not be covered by an ITG tree.
As inFigure 3(b), if the next block to cover is [zaoqie6],then [zairu1] is neither adjacent to [diannao5]nor adjacent to [zaoqie6].We can conclude that if we select the next blockfrom the left or right neighboring uncovered spanof the current block, then the translation must ob-serve ITG constraints.2.2 Shift-Reduce Decoding AlgorithmIn order to generate the translation with ITG con-straints, the shift-reduce algorithm have to keeptrace of covered blocks, left and right neighboringuncovered spans.
Formally, the shift-reduce de-coding algorithm uses the following three stacks:?
St: the stack for covered blocks.
The blocksare pushed in the order that they are covered,not the order that they are in the source sen-tence.?
Sl : the stack for the left uncovered spans ofthe current block.
When a block is pushedinto St, its corresponding left neighboringuncovered span is pushed into Sl.?
Sr :the stack for the right uncovered spans ofthe current block.
When a block is pushedinto St, its corresponding right neighboringuncovered span is pushed into Sr.A translation configuration is a triple c =?St, Sl, Sr?.
Given a source sentence f =f1, f2, ..., fm, we import a virtual start word andthe whole translation procedure can be seen asa sequence of transitions from cs to ct, wherecs = ?
[0], ?, [1,m]?
is the initial configura-tion, ct = ?
[0,m], ?, ??
is the terminal con-figuration.
The configuration for Figure 3 (e) is?
[0][5][1][3, 4], [2], [6]?.We define three types of transitions froma configuration to another .
Assume the cur-rent configuration c = ?
[ft11, ft12]...[ftk1, ftk2],[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] ?,then :?
Transitions LShift pop the top element[flu1, flu2] from Sl and select a block [i, j]from [flu1, flu2] to translate.
In addition,they push [i, j] into St, and if i 6= flu1, theypush [flu1, i ?
1] into Sl, and if j 6= flu2,they push [j+1, flu2] into Sr.
The precondi-tion to operate the transition is that Sl is notnull and the top span of Sl is adjacent to thetop block of St.
Formally, the preconditionis flu2 + 1 = ftk1.?
Transitions RShift pop the top element[frv1, frv2] of Sr and select a block [i, j]from [frv1, frv2] to translate.
In addition,they push [i, j] into St, and if i 6= frv1, theypush [frv1, i?1] into Sl, and if j 6= frv2, theypush [j + 1, frv2] into Sr.
The preconditionis that Sr is not null and the top span of Sr is288adjacent to the top block of St.
Formally, theprecondition is ftk2 + 1 = frv1.?
Transitions Reduce pop the top two blocks[ftk?11, ftk?12], [ftk1, ftk2] from St and pushthe merged span [ftk?11, ftk2] into St. Theprecondition is that the top two blocks are ad-jacent.
Formally, the precondition is ftk?12+1 = ftk1The transition sequence of the example in Fig-ure 2 is listed in Figure 5.
For the purpose ofefficiency, transitions Reduce are integrated withtransitions LShift and RShift in practical imple-mentation.
Before transitions LShift and RShiftpush [i, j] into St, they check whether [i, j] is ad-jacent to the top block of St.
If so, they changethe top block into the merged block directly.In practical implementation, in order to furtherrestrict search space, distortion limit is applied be-sides ITG constraints: a source phrase can be cov-ered next only when it is ITG-legal and its distor-tion does not exceed distortion limit.
The distor-tion d is calculated by d = |starti ?
endi?1 ?
1|,where starti is the start position of the currentphrase and endi?1 is the last position of the lasttranslated phrase.3 Related WorkGalley and Manning (2008) present a hierarchi-cal phrase reordering model aimed at improvingnon-local reorderings.
Via the hierarchical mer-gence of two blocks, the orientation of long dis-tance words can be computed.
Their shift-reducealgorithm does not import ITG constraints and ad-mits the translation violating ITG constraints.Zens et al (2004) introduce a left-to-right decoding algorithm with ITG constraintson the alignment template system (Och et al,1999).
Their algorithm processes candidatesource phrases one by one through the wholesearch space and checks if the candidate phrasecomplies with ITG constraints.
Besides, their al-gorithm checks validity via cover vector and doesnot formalize ITG structure.
The shift-reduce de-coding algorithm holds ITG structure via threestacks.
As a result, it can offer ITG-legal spansdirectly and decode faster.
Furthermore, withTransition St Sl Sr[0] ?
[1, 6]RShift [0][5] [1, 4] [6]LShift [0][5][1] ?
[2, 4][6]RShift [0][5][1][3] [2] [4][6]RShift [0][5][1][3][4] [2] [6]Reduce [0][5][1][3, 4] [2] [6]LShift [0][5][1][3, 4][2] ?
[6]Reduce [0][5][1][2, 4] ?
[6]Reduce [0][5][1, 4] ?
[6]Reduce [0][1, 5] ?
[6]Reduce [0, 5] ?
[6]RShift [0, 5][6] ?
?Reduce [0, 6] ?
?Figure 5: Transition sequence for the example inFigure 2.
The top nine transitions correspond toFigure 3 (a), ... , Figure 3 (i), respectively.the help of ITG structure, it can be extended tosyntax-based models easily.Xiong et al (2006) propose a BTG-basedmodel, which uses the context to determine theorientation of two adjacent spans.
It employs thecube-time CYK algorithm.4 ExperimentsWe compare the shift-reduce decoder with thestate-of-the-art decoder Moses (Koehn et al,2007).
The shift-reduce decoder was imple-mented by modifying the normal search algo-rithm of Moses to our shift-reduce algorithm,without cube pruning (Huang and Chiang, 2005).We retained the features of Moses: four trans-lation features, three lexical reordering features(straight, inverted and discontinuous), linear dis-tortion, phrase penalty, word penalty and languagemodel, without importing any new feature.
Thedecoding configurations used by all the decoders,including beam size, phrase table limit and so on,were the same, so the performance was comparedfairly.First, we will show the performance of shift-reduce algorithm on three data sets with largetraining data sets (Section 4.1).
Then, we willanalyze the performance elaborately in terms ofaccuracy, speed and search ability with a smaller289training data set (Section 4.2).
All experimentswere done on Chinese-to-English translation tasksand all results are reported with case insensitiveBLEU score.
Statistical significance were com-puted using the sign-test described in Collins etal.
(Collins et al, 2005).4.1 Performance EvaluationWe did three experiments to compare the perfor-mance of the shift-reduce decoder, Moses and thedecoder with ITG constraints using cover vector(denoted as CV).
2 The shift-reduce decoder de-coded with two sets of parameters: one was tunedby itself (denoted as SR) and the other was tunedby Moses (denoted as SR-same), using MERT(Och, 2003).
Two searching algorithms of Mosesare considered: one is the normal search algorithmwithout cubing pruning (denoted as Moses), theother is the search algorithm with cube pruning(denoted as Moses-cb).
For all the decoders, thedistortion limit was set to 6, the nbest size was setto 100 and the phrase table limit was 50.In the first experiment, the development set ispart of NIST MT06 data set including 862 sen-tences, the test set is NIST MT08 data set andthe training data set contains 5 million sentencepairs.
We used a 5-gram language model whichwere trained on the Xinhua and AFP portion ofthe Gigaword corpus.
The results are shown inTable 1(a).In the second experiment, the development dataset is NIST MT02 data set and the test set is NISTMT05 data set.
Language model and the trainingdata set are the same to that of the first experiment.The result is shown in Table 1(b).In the third experiment, the development setis China Workshop on Machine Translation 2008data set (denoted as CWMT08) and the test setis China Workshop on Machine Translation 2007data set (denoted as CWMT07).
The training setcontains 2 Million sentence pairs and the languagemodel are a 6-gram language model trained onthe Reuter corpus and English corpus.
Table 1(c)gives the results.In the above three experiments, SR decoder2The decoder CV is implemented by adding the ITG con-straints to Moses using the algorithm described in (Zens etal., 2004).NIST06 NIST08 speedMoses 30.24 25.08 4.827Moses-cb 30.27 23.80 1.501CV 30.35 26.23** 4.335SR-same ??
25.09 3.856SR 30.47 26.67** 4.126(a)NIST02 NIST05 speedMoses 35.68 35.80 7.142Moses-cb 35.42 35.03 1.811CV 35.45 36.56** 6.276SR-same ??
35.84 5.008SR 35.99* 36.42** 5.432(b)CWMT08 CWMT07 speedMoses 27.75 25.91 3.061Moses-cb 27.82 25.16 0.548CV 27.71 26.58** 2.331SR-same ??
25.97 1.988SR 28.14* 26.71** 2.106(c)Table 1: Performance comparison.
Moses: Moseswithout cube pruning, Moses-cb: Moses withcube pruning, CV: the decoder using cover vector,SR-same: the shift-reduce decoder decoding withparameters tunes by Moses, SR: the shift-reducedecoder with parameters tuned by itself.
The sec-ond column stands for develop set, the third col-umn stands for test set and speed column showsthe average time (seconds) of translating one sen-tence in the test set.
**: significance at the .01level.improves the accuracy by 1.59, 0.62, 0.8 BLEUscore (p < .01), respectively, and improves thespeed by 15%, 24%, 30%, respectively.
we cansee that SR can improve both the accuracy andthe speed while SR-same can increase the speedsignificantly with a slight improvement on the ac-curacy.
As both SR and CV decode with ITGconstraints, they match each other on the accu-29027.0027.5028.0028.5029.0029.5030.001  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17BLEUaverage decoding speed (s)d=-1d=-1d=-1SRSR-sameMosesFigure 6: Performance comparison on NIST05.For a curve, the dots correspond to distortion limit4, 6, 8, 10, 14 and no distortion from left to right.d = ?1 stands for no distortion limit.racy.
However, the speed of SR is faster than CV.Cube pruning can improve decoding speed dra-matically, but it is not risk-free pruning technol-ogy, so the BLEU score declines obviously.4.2 Performance AnalysisWe make performance analysis with the same ex-periment configuration as the second experimentin Section 4.1, except that the training set inthe analysis experiment is FBIS corpus, includ-ing 289k sentence pairs.
In the following exper-iments, Moses employs the normal search algo-rithm without cube pruning.For the decoders employ the linear distortionfeature, the distortion limit will influence thetranslation accuracy.
Besides, with different dis-tortion limit, the proportion of ITG-legal transla-tion generated by Moses will differ.
The smallerthe distortion limit is, the greater the proportion is.So we first compare the performance with differ-ent distortion limit.We compare the shift-reduce decoder withMoses using different distortion limit.
The re-sults are shown in Figure 6.
When distortion limitis set to 6, every decoder gets a peak value andSR has an improvement of 0.66 BLEU score overMoses.
From the curves, we can see that theBLEU score of SR-same with distortion limit 828.0028.5029.0029.5030.0030.5031.0031.5032.0032.5033.0033.5034.0034.5035.0035.5036.0036.5037.004  6  8  10  12  14  16BLEUdistortion limitSRSR-sameMoses(a) ITG set25.0025.5026.0026.5027.0027.5028.004  6  8  10  12  14  16BLEUdistortion limitSRSR-sameMoses(b) rest setFigure 7: Accuracy comparison on the ITG setand rest set of NIST05.
The ITG set includes thesentences the translations of which generated byMoses are ITG-legal, and the rest set contains therest sentences.
distortion limit = 16 denotes nodistortion limit.is lower than that of Mose with distortion limit6.
This is because the decoding speed of SR-same with distortion limit 8 is not faster than thatof Moses with distortion limit 6.
On the whole,compared to Moses, SR-same can improve the ac-curacy slightly with much faster decoding speed,and SR can obtain improvements on both the ac-curacy and the speed.We split the test set into two sets: one containsthe sentences, the translations of which generatedby Moses are ITG-legal (denoted as ITG set) andthe other contains the rest (denoted as rest set).From Figure 7, we can see that no matter on theITG set or on the rest set, SR decoder can gain ob-vious accuracy improvements with all distortion291ITG restdMoses SR-same total < = > Moses SR-same total < = >4 28.67 28.68 1050 8 1042 0 25.61 25.82 32 0 0 326 31.34 31.42 758 51 705 2 25.78 25.72 324 32 2 2908 32.59 32.93* 594 72 516 6 25.68 25.65 488 82 3 40310 34.36 34.99** 456 80 365 11 26.04 26.50* 626 147 3 47612 33.16 33.61** 454 63 380 11 27.01 27.13 628 165 1 46214 35.98 36.25* 383 60 316 7 26.35 26.67* 699 203 1 495-1 34.13 34.96** 351 39 308 4 26.17 26.78** 731 154 0 577Table 2: Search ability comparison.
The ITG set and the rest set of NIST05 were tested, respectively.On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEUscore of SR-same, the number of sentences in the ITG set, the number of sentences the translationprobabilities of which computed by Moses, compared to that computed by SR, is lower, equal andgreater.
The rest set goes similarly.
*: significance at the .05 level, **: significance at the .01 level.limit.
While SR-same decoder only gets better re-sults on the ITG set with all distortion limit.
Thismay result from the use of the linear distortionfeature.
Moses may generate hypotheses the dis-tortion of which is forbidden in the shift-reducedecoder.
This especially sharpens on the rest set.So SR-same may suffer from an improper lineardistortion parameter.The search ability of Moses and the shift-reduce decoder are evaluated, too.
The translationmust be produced with the same set of parameters.In our experiments, we employed the parameterstuned by Moses.
The test was done on the ITG andthe rest set, respectively.
The results are shown inTable 2.
As the distortion limit becomes greater,the number of the ITG-legal translation generatedby Moses becomes smaller.
On the ITG set, trans-lation probabilities from the shift-reduce decoderis either greater or equal to that from Moses onmost sentences, and BLEU scores of shift-reducedecoder is greater than that of Moses with alldistortion limit.
Although the search space ofshift-reduce decoder is smaller than that of Moses,shift-reduce decoder can give the translation thatMoses can not reach.
On the rest set, for most sen-tences, the translation probabilities from Moses isgreater than that from shift-reduce decoder.
Butonly when distortion limit is 6 and 8, the BLEUscore of Moses is greater than that of the shift-reduce decoder.
We may conclude that greaterscore does not certainly lead to greater BLEUscore.5 Conclusions and Future WorkIn this paper, we present a shift-reduce decod-ing algorithm for phrase-based translation modelthat can generate the ITG-legal translation in lin-ear time.
The algorithm need not consider shift-reduce divergence and performs reduce operationas soon as possible.
We compare the performanceof the shift-reduce decoder with the state-of-the-art decoder Moses.
Experiment results show thatthe shift-reduce algorithm can improve both theaccuracy and the speed significantly on differenttest sets.
We further analyze the performance andfind that on the ITG set, the shift-reduce decoderis superior over Moses in terms of accuracy, speedand search ability, while on the rest set, it doesnot display advantage, suffering from improperparameters.Next, we will extend the shift-reduce algorithmto syntax-based translation models, to see whetherit works.6 AcknowledgementThe authors were supported by National NaturalScience Foundation of China Contract 60736014,National Natural Science Foundation of ChinaContract 60873167 and High Technology R&DProgram Project No.
2006AA010108.
We aregrateful to the anonymous reviewers for theirvaluable comments.292ReferencesCollins, Michael, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL, pages 531?540.Galley, Michel and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proc.
of EMNLP, pages 848?856.Huang, Liang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies (IWPT), pages53?64.Knight, Kevin.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25:607?615.Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of the 45th ACL, Demonstra-tion Session.Koehn, Philipp.
2004.
Pharaoh: A beam search de-coder for phrased-based statistical machine transla-tion.
In Proc.
of AMTA, pages 115?124.Och, Frans J., Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statisticalmachine translation.
In Proc.
of EMNLP, pages 20?28.Och, Frans J.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL,pages 160?167.Tillmann, Chirstoph and Hermann Ney.
2003.Word reordering and a dynamic programming beamsearch algorithm for statistical machine translation.Computational Linguistics, 29:97?133.Wu, Dekai.
1996.
A polynomial-time algorithm forstatistical machine translation.
In Proc.
of ACL,pages 152?158.Wu, Dekai.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23:377?403.Xiong, Deyi, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proc.
of ACL, pages521?528.Zens, Richard and Hermann Ney.
2003.
A compara-tive study on reordering constraints in statistical ma-chine translation.
In Proc.
of ACL, pages 144?151.Zens, Richard, Hermann Ney, Taro Watanable, andEiichiro Sumita.
2004.
Reordering constraintsfor phrase-based statistical machine translation.
InProc.
of COLING, pages 205?211.293
