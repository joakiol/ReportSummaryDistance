Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 673?677,Dublin, Ireland, August 23-24, 2014.TUGAS: Exploiting Unlabelled Data for Twitter Sentiment AnalysisSilvio Amir+, Miguel Almeida?
?, Bruno Martins+, Jo?ao Filgueiras+, and M?ario J. Silva++INESC-ID, Instituto Superior T?ecnico, Universidade de Lisboa, Portugal?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?
?oes, Instituto Superior T?ecnico, Universidade de Lisboa, Portugalsamir@inesc-id.pt, miguel.almeida@priberam.pt, bruno.g.martins@tecnico.ulisboa.ptjfilgueiras@inesc-id.pt, mjs@inesc-id.ptAbstractThis paper describes our participation inthe message polarity classification task ofSemEval 2014.
We focused on exploitingunlabeled data to improve accuracy, com-bining features leveraging word represen-tations with other, more common features,based on word tokens or lexicons.
Weanalyse the contribution of the differentfeatures, concluding that unlabeled datayields significant improvements.1 IntroductionResearch in exploiting social media for mea-suring public opinion, evaluating popularity ofproducts and brands, anticipating stock-markettrends, or predicting elections showed promisingresults (O?Connor et al., 2010; Mitchell et al.,2013).
However, this type of content poses a par-ticularly challenging problem for text analysis sys-tems.
Typical messages show heavy use of Inter-net slang, emoticons and other abbreviations anddiscourse conventions.
The lexical variation intro-duced by this creative use of language, togetherwith the unconventional spelling and occasionaltypos, leads to very large vocabularies.
On theother hand, messages are very short, and there-fore word feature representations tend to becomevery sparse, degrading the performance of ma-chine learned classifiers.The growing interest in this problem motivatedthe creation of a shared task for Twitter SentimentAnalysis in the 2013 edition of SemEval.
TheMessage Polarity Classification task was formal-ized as follows: Given a message, decide whetherthe message is of positive, negative, or neutral sen-timent.
For messages conveying both a positiveThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/Positive Neutral NegativeTrain 2014 3230 4109 1265Tweets 2013 1572 1640 601Tweets 2014 982 669 202SMS 2013 492 1207 394Tweets Sarcasm 2014 33 13 40LiveJournal 2014 427 411 304Table 1: Number of examples per class in eachSemEval dataset.
The first row represents all train-ing data; the other rows are sets used for testing.and negative sentiment, whichever is the strongersentiment should be chosen (Nakov et al., 2013).We describe our participation on the 2014 edi-tion of this task, for which a set of manually la-belled messages was created.
Complying with theTwitter policies for data access, the corpus wasdistributed as a list of message IDs and each par-ticipant was responsible for downloading the ac-tual tweets.
Using the provided script, we col-lected a training set with 8604 tweets.
After sub-mission, the 2014 test sets were also made avail-able.
Along with the Tweets 2014 test set, evalu-ation was also performed on a set of tweets withsarcasm, on a set of LiveJournal blog entries, andon sets of tweets and SMS messages from the 2013edition of the task.
Table 1 shows the class distri-bution for each of these datasets.In the 2013 edition (task 2B), the NRC-Canadasystem (Mohammad et al., 2013) earned first placeby scoring 69.02% on the Official SemEval metric(see Section 4) with a significant margin with re-spect to the other systems: the second (G?untherand Furrer, 2013) and third (Reckman et al., 2013)best systems scored 65.27% and 64.86%, respec-tively.
The main novelty in the NRC-Canada sys-tem was the use of sentiment lexicons, specificfor the Twitter domain, generated from unlabeledtweets using emoticons and hashtags as indicatorsof sentiment.
They found that these lexicons had astrong impact on the results ?
more than word and673character n-grams.The automatically induced lexicons are a wayto use information from unlabeled data to aid inthe classification task.
In our approach, we takethis reasoning further, and focus on the impact ofvarious ways to incorporate knowledge from un-labeled data.
This allows us to mimic many real-world scenarios where labelled data is scarce butunlabeled data is plentiful.2 Word RepresentationsIn text classification it is common to represent doc-uments as bags-of-words, i.e., as unordered col-lections of words.
However, in the case of veryshort social media texts, these representations be-come less effective, as they lead to increased datasparseness.
We focused our experiments in com-paring and complementing these approaches withdenser representations, which we now describe.2.1 Bag-Of-Words and ?BM25In a representation based on bags-of-words,each message is represented as a vector m ={w1, w2, ..., wn} ?
RV, where V is the size ofthe vocabulary.
In order to have weights that re-flect how relevant a word is to each of the classes,we weighted the individual terms according to the?BM25 heuristic (Paltoglou and Thelwall, 2010):?BM25(wi) = tfi?
log((Np?dfi,p+s)?dfi,n+s(Nn?dfi,n+s)?dfi,p+s), (1)where tfirepresents the frequency of term i in themessage, Nais the size of corpus a, dfi,ais thedocument frequency of term i in the corpus a (i.e.,in one of two subsets for the training data, corre-sponding to either positive or negative messages),and s is a smoothing constant, which we set to0.5.
This term weighting function was previouslyshown to be effective for sentiment analysis.2.2 Brown ClustersBrown et al.
(1992) proposed a greedy agglomer-ative hierarchical clustering procedure that groupswords to maximize the mutual information of bi-grams.
Clusters are initialized as consisting of asingle word each, and are then greedily merged ac-cording to a mutual information criterion, to forma lower-dimensional representation of a vocabu-lary.
The hierarchical nature of the clustering al-lows words to be represented at different levels inthe hierarchy.
This approach provides a denserrepresentation of the messages, mitigating the fea-ture sparseness problem.
We used a publicly avail-able1set of 1000 Brown clusters induced from acorpus of 56 million Twitter messages.We leveraged the word clusters by mappingeach word to the corresponding cluster, and wethen represented each message as a bag-of-clustersvector in RK, where K = 1000 is the numberof clusters.
These word cluster features were alsoweighted with the ?BM25 scheme.2.3 Concise Semantic AnalysisConcise Semantic Analysis is a form of termand document representation that assigns, to eachterm, its weight on each of the classes (Li et al.,2011).
These weights, computed from the fre-quencies of the term on the training data, reflecthow associated the term is to each class.
Theweight of term j in class c is given by (Lopez-Monroy et al., 2013):wcj=?k?Pclog2(1 +tfkjlen(k)), (2)where Pcis the set of documents with label cand tfkjis the term frequency of term j in doc-ument k. To prevent labels with a higher numberof examples, or terms with higher frequencies, tohave stronger weights, an additional normalizationstep is performed to obtain nwcj, the normalizedweight of term j in class c:nwcj=wcj?l?Lwlj??t?Twct.
(3)In the formula, L is the set of class labels and T isthe set of terms, making wljthe weight of termj for a class l, and wctthe weight of a term tin class c. After defining every term as a vectortj= {nw1j, .
.
.
, nwCj} ?
RC, where C is thenumber of classes, each message m is representedby summing each of its terms?
weight vectors:mcsa=?j?mtfjlen(m)?
tj.
(4)In the formula, tfjis the frequency of term j in m.2.4 Dense Word VectorsEfficient approaches have recently been intro-duced to train neural networks capable of produc-ing continuous representations of words (Mikolov1http://www.ark.cs.cmu.edu/TweetNLP/674Lexicon #1-grams #2-grams #pairsBing Liu 6789 - -MPQA 8222 - -SentiStrength 2546 - -NRC EmoLex 14177 - -Sentiment140 62468 677698 480010NRC HashSent 54129 316531 308808Table 2: Number of unigrams, bigrams, and collo-cation pairs, in the lexicons used in our system.et al., 2013).
These approaches allow fast train-ing of projections from a representation based onbags-of-words, where vectors have very high di-mension (of the order of 104), but are also verysparse and integer-valued, to vectors of muchlower dimensions (of the order of 102), with fulldensity and continuous values.To induce word embeddings, a corpus of 17 mil-lion Twitter messages was collected with the Twit-ter crawler of Boanjak et al.
(2012).
Then, us-ing word2vec2, we induced representations for theword tokens occurring in the messages.
All the to-kens were represented as vectors wj?
Rn, withn = 100.
A message was modeled as the sum ofthe vector representations of the individual words:mvec=?j?mwj.
(5)We also created a polarity class vector pcfor eachclass c, defined as:pc=1Nc?m?cmvec, (6)where m is a message of class c and Ncis the totalnumber of instances in class c. These vectors canbe interpreted as prototypes of their classes, andare used in the classVec features described below.3 The TUGAS SystemWe now describe the TUGAS approach, detailingthe considered features and our modeling choices.3.1 Word FeaturesTo reduce the feature space of the model,messages were lower-cased, Twitter user men-tions (@username) were replaced with the to-ken <USER> and URLs were replaced withthe <URL> token.
We also normalized wordsto include at most 3 repeated characters (e.g.,2https://code.google.com/p/word2vec/?helloooooo!?
to ?hellooo!?).
Following Pang etal.
(2002), negation was directly integrated intothe word representations.
All the tokens occurringbetween a negation word and the next punctuationmark, were suffixed with the NEG annotation.We used the following groups of features:?
bow-uni: vector of word unigrams?
bow-bc: vector of Brown word clusters?
csa: Concise Semantic Analysis vector mcsa?
wordVec: word2vec message vector mvec?
classVec: Euclidean distance between mes-sage vector mvecand each class vector pc3.2 Lexicon FeaturesThe document model was enriched with featuresthat take into account the presence of words with aknown prior polarity, such as happy or sad.
We in-cluded words from manually annotated sentimentlexicons: Bing Liu Opinion Lexicon (Hu and Liu,2004), MPQA (Wilson et al., 2005) and the NRCEmotion Lexicon (Mohammad and Turney, 2013).We also used the two automatically generated lex-icons from Mohammad et al.
(2013): the NRCHashtag Sentiment Lexicon and the Sentiment140Lexicon.
Table 2 summarizes the number of termsof each lexicon.As Mohammad et al.
(2013), we added the fol-lowing set of lexicon features, for each lexicon,and for each combination of negated/non-negatedwords and positive/negative polarity.?
The sum of the sentiment scores of all(negated/non-negated) terms with (posi-tive/negative) sentiment?
The largest of those scores?
The sentiment score of the last word in themessage that is also present in the lexicon?
The number of terms within the lexiconNotice that terms can be unigrams, bigrams, andcollocations pairs.
A group of these features wascomputed for each of the sentiment lexicons.3.3 Syntactic FeaturesWe extracted syntactic features aimed at the Twit-ter domain, such as the use of heavy punctuation,emoticons and character repetition.
Concretely,the following features were computed from theoriginal Twitter messages:?
Number of words originally with more than 3repeated characters?
Number of sequences of exclamation marksand/or question marks675Tweets Test 2013 Tweets Test 2014 SMS 2013 Live Journal 2014 Tweets Sarcasm 2014Features Acc F1 Official Acc F1 Official Acc F1 Official Acc F1 Official Acc F1 Officialbow-uni 65.62 59.30 54.60 69.94 66.30 65.60 68.80 62.40 54.90 60.42 58.30 56.60 47.67 43.90 41.50submitted 69.55 67.50 65.60 71.45 69.00 69.00 70.57 67.60 62.70 68.21 68.20 69.80 53.49 50.10 52.90- lexicons 66.90 64.30 61.70 70.37 67.00 66.40 66.46 63.50 58.30 64.27 64.20 65.50 48.84 45.10 47.00- classVec 69.37 67.30 65.40 71.83 69.30 69.60 69.14 66.60 62.10 67.51 67.50 69.30 53.49 50.10 52.90- wordVec 69.63 67.70 66.00 70.32 67.70 68.00 66.79 64.90 60.90 68.04 68.00 69.70 53.49 50.50 53.50- bow-bc 68.06 66.40 65.10 67.40 64.30 65.30 67.89 65.20 60.40 68.30 68.30 70.00 52.33 49.90 49.90+ syntactic 69.58 67.60 65.70 71.24 68.30 68.50 70.38 67.40 62.40 67.95 68.00 69.70 52.33 48.80 50.00+ csa 67.45 63.70 60.50 70.10 67.30 67.50 71.48 67.60 62.10 66.11 66.00 68.30 53.49 51.30 50.30+ bow-uni 67.69 62.50 58.50 70.64 67.30 66.70 72.77 67.10 60.40 67.60 67.20 67.10 51.16 48.00 43.90Table 3: Impact of removing or adding groups of features.
The row marked as submitted, in bold, is theone that we submitted to the shared task.
The bold column is the official score used to rank participants.?
Number of positive/negative emoticons, de-tected with a pre-existing regular expression3?
Number of capitalized words3.4 Model TrainingWe used the L2-regularized logistic regression im-plementation from scikit-learn4.
Given a set of minstance-label pairs (xi, yi), with i = 1, .
.
.
,m,xi?
Rn, and yi?
{?1,+1}, learning the clas-sifier involves solving the following optimizationproblem, where C > 0 is a penalty parameter.minw12w?w + Cm?i=1log(1 + e?yiw?xi).
(7)In scikit-learn, the problem is solved througha trust region Newton method, using a wrapperover the implementation available in the liblin-ear5package.
For multi-class problems, scikit-learn uses the one-vs-the-rest strategy.
This par-ticular implementation also suports the introduc-tion of class weights, which we set to be inverselyproportional to the class frequency in the trainingdata, thus making each class equally important.The selection of groups of features to be in-cluded in the submitted run, as well as the tun-ing of the regularization constant, were obtainedby cross-validation on the training dataset.4 ResultsWe report results using the following metrics:?
Accuracy, defined as the percentage oftweets correctly classified.?
Overall F1, computed by averaging the F1score of all three classes.?
The Official SemEval score, computed byaveraging the F1 scores of the positive andnegative classes (Nakov et al., 2013).3http://sentiment.christopherpotts.net/4http://scikit-learn.org/5http://www.csie.ntu.edu.tw/?cjlin/liblinear/Feature group Acc F1 Officialbow-bc 66.33 63.30 60.30wordVec 62.34 60.00 57.90bow-uni 65.62 59.30 54.60csa 61.58 56.70 52.90Table 4: Performance comparison using differentword representations in isolation.We tried including or excluding various groupsof features, and obtained the best results on thetraining set using Brown clusters (bow-bc), lexi-con features (lexicon), word2vec word represen-tations (wordVec), and the Euclidean distance be-tween the word2vec representation and each classvector (classVec).
These features were the onesused in our submission.
Inclusion of syntacticfeatures (syntactic), Concise Semantic Analysis(csa), and word unigrams (bow-uni) was found todecrease performance during cross-validation, andthus these features were not included.Table 4 shows the results on the Twitter 2014test set using only a single group of word represen-tation features to train the model, from each of thetechniques introduced in Section 2.
This table sug-gests that exploiting unlabeled data is beneficial,as representing words through their Brown clus-ters (bow-bc) or through word2vec (wordVec)yields better results than unigrams or CSA.Table 3 shows results on five different test sets,including two from the 2013 challenge (Nakov etal., 2013), when features are added or removedfrom the official submission, one group at a time.Adding representations like bow-uni or csa actu-ally hurts the performance, suggesting that, giventhe relatively small set of training instances, usingcoarse-level features in isolation, such as Brownclusters, can yield better results.More importantly, we verify that lexicon-basedand Brown cluster features have the largest impact676(2.6% and 3.7%, respectively, in the official met-ric).
These results indicate that leveraging unla-beled data yields significant improvements.5 ConclusionsThis paper describes the participation of theTUGAS team in the message polarity classifica-tion task of SemEval 2014.
We showed that thereare significant gains in leveraging unlabeled datafor the task of classifying the sentiment of Twit-ter texts.
Our score of 69% ranks at fifth place in42 submissions, roughly 2% points below the topscore of 70.96%.
We believe that the direction ofleveraging unlabeled data is still vastly unexploredand, for future work, we intend to: (a) experi-ment with semi-supervised learning approaches,further exploiting unlabeled tweets; and (b) makeuse of domain adaptation strategies to leverage onlabelled non-Twitter data.AcknowledgementsThis work was partially supported by theEU/FEDER programme, QREN/POR Lis-boa (Portugal), under the Intelligo project(contract 2012/24803).
The researchers fromINESC-ID were supported by Fundac?
?ao paraa Ci?encia e Tecnologia (FCT), through con-tracts Pest-OE/EEI/LA0021/2013, EXCL/EEI-ESS/0257/2012 (DataStorm), project grantsPTDC/CPJ-CPO/116888/2010 (POPSTAR), andEXPL/EEI-ESS/0427/2013 (KD-LBSN), andPh.D.
scholarship SFRH/BD/89020/2012.ReferencesMatko Boanjak, Eduardo Oliveira, Jos?e Martins, Ed-uarda Mendes Rodrigues, and Lu?
?s Sarmento.
2012.Twitterecho: a distributed focused crawler to sup-port open research with twitter data.
In 21st Interna-tional Conference Companion on World Wide Web,pages 1233?1240.Peter F. Brown, Peter V. Desouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467?479.Tobias G?unther and Lenz Furrer.
2013.
GU-MLT-LT: Sentiment analysis of short messages using lin-guistic features and stochastic gradient descent.
In7th International Workshop on Semantic Evaluation,pages 328?332.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In 10th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 168?177.Zhixing Li, Zhongyang Xiong, Yufang Zhang, Chuny-ong Liu, and Kuan Li.
2011.
Fast text categorizationusing concise semantic analysis.
Pattern Recogni-tion Letters, 32(3):441?448.
?Adrian Pastor Lopez-Monroy, Manuel Montes-yGomez, Hugo Jair Escalante, Luis Villasenor-Pineda, and Esa?u Villatoro-Tello.
2013.
INAOE?sparticipation at PAN?13: Author profiling task.
InCLEF 2013 Evaluation Labs and Workshop.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeffrey Dean.
2013.
Distributed represen-tations of words and phrases and their composition-ality.
In 27th Annual Conference on Neural Infor-mation Processing Systems, pages 3111?3119.Lewis Mitchell, Kameron Decker Harris, Morgan RFrank, Peter Sheridan Dodds, and Christopher MDanforth.
2013.
The geography of happiness:connecting twitter sentiment and expression, de-mographics, and objective characteristics of place.PLoS ONE, 8(5):e64417.Saif M Mohammad and Peter D Turney.
2013.
Crowd-sourcing a word?emotion association lexicon.
Com-putational Intelligence, 29(3):436?465.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: building the state-of-the-art in sentiment analysis of tweets.
In 7th Inter-national Workshop on Semantic Evaluation, pages321?327.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: Sentiment analysis inTwitter.
In 7th International Workshop on SemanticEvaluation, pages 312?320.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R Routledge, and Noah A Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In 4th InternationalAAAI Conference on Weblogs and Social Media,pages 122?129.Georgios Paltoglou and Mike Thelwall.
2010.
A studyof information retrieval weighting schemes for sen-timent analysis.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1386?1395.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In ACL-02 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 79?86.Hilke Reckman, Baird Cheyanne, Jean Crawford,Richard Crowell, Linnea Micciulla, Saratendu Sethi,and Fruzsina Veress.
2013. teragram: Rule-baseddetection of sentiment phrases using SAS sentimentanalysis.
In 7th International Workshop on Seman-tic Evaluation, pages 513?519.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 347?354.677
