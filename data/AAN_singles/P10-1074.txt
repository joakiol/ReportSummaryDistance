Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 720?728,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsHierarchical Joint Learning:Improving Joint Parsing and Named Entity Recognitionwith Non-Jointly Labeled DataJenny Rose Finkel and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel|manning}@cs.stanford.eduAbstractOne of the main obstacles to produc-ing high quality joint models is the lackof jointly annotated data.
Joint model-ing of multiple natural language process-ing tasks outperforms single-task modelslearned from the same data, but still under-performs compared to single-task modelslearned on the more abundant quantitiesof available single-task annotated data.
Inthis paper we present a novel model whichmakes use of additional single-task anno-tated data to improve the performance ofa joint model.
Our model utilizes a hier-archical prior to link the feature weightsfor shared features in several single-taskmodels and the joint model.
Experimentson joint parsing and named entity recog-nition, using the OntoNotes corpus, showthat our hierarchical joint model can pro-duce substantial gains over a joint modeltrained on only the jointly annotated data.1 IntroductionJoint learning of multiple types of linguistic struc-ture results in models which produce more consis-tent outputs, and for which performance improvesacross all aspects of the joint structure.
Jointmodels can be particularly useful for producinganalyses of sentences which are used as input forhigher-level, more semantically-oriented systems,such as question answering and machine trans-lation.
These high-level systems typically com-bine the outputs from many low-level systems,such as parsing, named entity recognition (NER)and coreference resolution.
When trained sepa-rately, these single-task models can produce out-puts which are inconsistent with one another, suchas named entities which do not correspond to anynodes in the parse tree (see Figure 1 for an ex-ample).
Moreover, one expects that the differenttypes of annotations should provide useful infor-mation to one another, and that modeling themjointly should improve performance.
Because anamed entity should correspond to a node in theparse tree, strong evidence about either aspect ofthe model should positively impact the other as-pect.However, designing joint models which actu-ally improve performance has proven challeng-ing.
The CoNLL 2008 shared task (Surdeanuet al, 2008) was on joint parsing and semanticrole labeling, but the best systems (Johansson andNugues, 2008) were the ones which completelydecoupled the tasks.
While negative results arerarely published, this was not the first failed at-tempt at joint parsing and semantic role label-ing (Sutton and McCallum, 2005).
There havebeen some recent successes with joint modeling.Zhang and Clark (2008) built a perceptron-basedjoint segmenter and part-of-speech (POS) taggerfor Chinese, and Toutanova and Cherry (2009)learned a joint model of lemmatization and POStagging which outperformed a pipelined model.Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morpho-logical segmentation and tagging of Hebrew, andGoldberg and Tsarfaty (2008) developed a jointmodel of segmentation, tagging and parsing of He-brew, based on lattice parsing.
No discussion ofjoint modeling would be complete without men-tion of (Miller et al, 2000), who trained a Collins-style generative parser (Collins, 1997) over a syn-tactic structure augmented with the template entityand template relations annotations for the MUC-7shared task.One significant limitation for many joint mod-els is the lack of jointly annotated data.
We builta joint model of parsing and named entity recog-nition (Finkel and Manning, 2009b), which hadsmall gains on parse performance and moderategains on named entity performance, when com-pared with single-task models trained on the samedata.
However, the performance of our model,trained using the OntoNotes corpus (Hovy et al,2006), fell short of separate parsing and named720FRAGINTJUHLikeNPNPDTaNNgrossPPINofNPQPDTaCD[billionNNSdollars]MONEYNPJJlastNNyearFigure 1: Example from the data where separate parse and named entity models give conflicting output.entity models trained on larger corpora, annotatedwith only one type of information.This paper addresses the problem of how tolearn high-quality joint models with smaller quan-tities of jointly-annotated data that has been aug-mented with larger amounts of single-task an-notated data.
To our knowledge this work isthe first attempt at such a task.
We use a hi-erarchical prior to link a joint model trained onjointly-annotated data with other single-task mod-els trained on single-task annotated data.
The keyto making this work is for the joint model to sharesome features with each of the single-task models.Then, the singly-annotated data can be used to in-fluence the feature weights for the shared featuresin the joint model.
This is an important contribu-tion, because it provides all the benefits of jointmodeling, but without the high cost of jointly an-notating large corpora.
We applied our hierarchi-cal joint model to parsing and named entity recog-nition, and it reduced errors by over 20% on bothtasks when compared to a joint model trained ononly the jointly annotated data.2 Related WorkOur task can be viewed as an instance of multi-tasklearning, a machine learning paradigm in whichthe objective is to simultaneously solve multiple,related tasks for which you have separate labeledtraining data.
Many schemes for multitask learn-ing, including the one we use here, are instancesof hierarchical models.
There has not been muchwork on multi-task learning in the NLP com-munity; Daume?
III (2007) and Finkel and Man-ning (2009a) both build models for multi-domainlearning, a variant on domain adaptation wherethere exists labeled training data for all domainsand the goal is to improve performance on all ofthem.
Ando and Zhang (2005) utilized a multi-task learner within their semi-supervised algo-rithm to learn feature representations which wereuseful across a large number of related tasks.
Out-side of the NLP community, Elidan et al (2008)used an undirected Bayesian transfer hierarchyto jointly model the shapes of multiple mammalspecies.
Evgeniou et al (2005) applied a hier-archical prior to modeling exam scores of stu-dents.
Other instances of multi-task learning in-clude (Baxter, 1997; Caruana, 1997; Yu et al,2005; Xue et al, 2007).
For a more general discus-sion of hierarchical models, we direct the reader toChapter 5 of (Gelman et al, 2003) and Chapter 12of (Gelman and Hill, 2006).3 Hierarchical Joint LearningIn this section we will discuss the main con-tribution of this paper, our hierarchical jointmodel which improves joint modeling perfor-mance through the use of single-task modelswhich can be trained on singly-annotated data.Our experiments are on a joint parsing and namedentity task, but the technique is more general andonly requires that the base models (the joint modeland single-task models) share some features.
Thissection covers the general technique, and we willcover the details of the parsing, named entity, andjoint models that we use in Section 4.3.1 Intuitive OverviewAs discussed, we have a joint model which re-quires jointly-annotated data, and several single-task models which only require singly-annotateddata.
The key to our hierarchical model is that thejoint model must have features in common witheach of the single models, though it can also havefeatures which are only present in the joint model.721PARSE JOINT NER???
??
?p ?pDp?j ?jDj?n ?nDnFigure 2: A graphical representation of our hierar-chical joint model.
There are separate base modelsfor just parsing, just NER, and joint parsing andNER.
The parameters for these models are linkedvia a hierarchical prior.Each model has its own set of parameters (featureweights).
However, parameters for the featureswhich are shared between the single-task modelsand the joint model are able to influence one an-other via a hierarchical prior.
This prior encour-ages the learned weights for the different modelsto be similar to one another.
After training hasbeen completed, we retain only the joint model?sparameters.
Our resulting joint model is of higherquality than a comparable joint model trained ononly the jointly-annotated data, due to all of the ev-idence provided by the additional single-task data.3.2 Formal ModelWe have a set M of three base models: aparse-only model, an NER-only model and ajoint model.
These have corresponding log-likelihood functions Lp(Dp; ?p), Ln(Dn; ?n), andLj(Dj ; ?j), where the Ds are the training data foreach model, and the ?s are the model-specific pa-rameter (feature weight) vectors.
These likelihoodfunctions do not include priors over the ?s.
Forrepresentational simplicity, we assume that eachof these vectors is the same size and correspondsto the same ordering of features.
Features whichdon?t apply to a particular model type (e.g., parsefeatures in the named entity model) will alwaysbe zero, so their weights have no impact on thatmodel?s likelihood function.
Conversely, allowingthe presence of those features in models for whichthey do not apply will not influence their weightsin the other models because there will be no evi-dence about them in the data.
These three modelsare linked by a hierarchical prior, and their fea-ture weight vectors are all drawn from this prior.The parameters ??
for this prior have the same di-mensionality as the model-specific parameters ?mand are drawn from another, top-level prior.
In ourcase, this top-level prior is a zero-mean Gaussian.1The graphical representation of our hierarchicalmodel is shown in Figure 2.
The log-likelihood ofthis model isLhier-joint(D; ?)
= (1)?m?M(Lm(Dm; ?m)?
?i(?m,i ?
??,i)22?2m)??i(?
?,i ?
?i)22?2?The first summation in this equation computes thelog-likelihood of each model, using the data andparameters which correspond to that model, andthe prior likelihood of that model?s parameters,based on a Gaussian prior centered around thetop-level, non-model-specific parameters ?
?, andwith model-specific variance ?m.
The final sum-mation in the equation computes the prior likeli-hood of the top-level parameters ??
according to aGaussian prior with variance ??
and mean ?
(typ-ically zero).
This formulation encourages eachbase model to have feature weights similar to thetop-level parameters (and hence one another).The effects of the variances ?m and ??
warrantsome discussion.
??
has the familiar interpretationof dictating how much the model ?cares?
aboutfeature weights diverging from zero (or ?).
Themodel-specific variances, ?m, have an entirely dif-ferent interpretation.
They dictate how how strongthe penalty is for the domain-specific parametersto diverge from one another (via their similarity to??).
When ?m are very low, then they are encour-aged to be very similar, and taken to the extremethis is equivalent to completely tying the parame-ters between the tasks.
When ?m are very high,then there is less encouragement for the parame-ters to be similar, and taken to the extreme this isequivalent to completely decoupling the tasks.We need to compute partial derivatives in or-der to optimize the model parameters.
The partialderivatives for the parameters for each base modelm are given by:?Lhier(D; ?)?
?m,i= ?Lm(Dm, ?m)??m,i?
?m,i ?
?
?,i?2d (2)where the first term is the partial derivative ac-cording to the base model, and the second term is1Though we use a zero-mean Gaussian prior, this top-level prior could take many forms, including an L1 prior, oranother hierarchical prior.722the prior centered around the top-level parameters.The partial derivatives for the top level parameters??
are:?Lhier(D; ?)???,i=(?m?M?
?,i ?
?m,i?2m)?
?
?,i ?
?i?2?
(3)where the first term relates to how far each model-specific weight vector is from the top-level param-eter values, and the second term relates how fareach top-level parameter is from zero.When a model has strong evidence for a feature,effectively what happens is that it pulls the valueof the top-level parameter for that feature closer tothe model-specific value for it.
When it has littleor no evidence for a feature then it will be pulledin the direction of the top-level parameter for thatfeature, whose value was influenced by the modelswhich have evidence for that feature.3.3 Optimization with Stochastic GradientDescentInference in joint models tends to be slow, and of-ten requires the use of stochastic optimization inorder for the optimization to be tractable.
L-BFGSand gradient descent, two frequently used numer-ical optimization algorithms, require computingthe value and partial derivatives of the objectivefunction using the entire training set.
Instead,we use stochastic gradient descent.
It requires astochastic objective function, which is meant to bea low computational cost estimate of the real ob-jective function.
In most NLP models, such as lo-gistic regression with a Gaussian prior, computingthe stochastic objective function is fairly straight-forward: you compute the model likelihood andpartial derivatives for a randomly sampled subsetof the training data.
When computing the termfor the prior, it must be rescaled by multiplyingits value and derivatives by the proportion of thetraining data used.
The stochastic objective func-tion, where D?
?
D is a randomly drawn subset ofthe full training set, is given byLstoch(D; ?)
= Lorig(D?
; ?)?|D?||D|?i(??,i)22?2?
(4)This is a stochastic function, and multiple calls toit with the same D and ?
will produce differentvalues because D?
is resampled each time.
Whendesigning a stochastic objective function, the crit-ical fact to keep in mind is that the summed valuesand partial derivatives for any split of the data needto be equal to that of the full dataset.
In practice,stochastic gradient descent only makes use of thepartial derivatives and not the function value, sowe will focus the remainder of the discussion onhow to rescale the partial derivatives.We now describe the more complicated caseof stochastic optimization with a hierarchical ob-jective function.
For the sake of simplicity, letus assume that we are using a batch size of one,meaning |D?| = 1 in the above equation.
Notethat in the hierarchical model, each datum (sen-tence) in each base model should be weightedequally, so whichever dataset is the largest shouldbe proportionally more likely to have one of itsdata sampled.
For the sampled datum d, we thencompute the function value and partial derivativeswith respect to the correct base model for that da-tum.
When we rescale the model-specific prior, werescale based on the number of data in that model?straining set, not the total number of data in all themodels combined.
Having uniformly randomlydrawn datum d ?
?m?MDm, let m(d) ?
Mtell us to which model?s training data the datumbelongs.
The stochastic partial derivatives willequal zero for all model parameters ?m such thatm 6= m(d), and for ?m(d) it becomes:?Lhier-stoch(D; ?)?
?m(d),i= (5)?Lm(d)({d}; ?m(d))??m(d),i?
1|Dm(d)|(?m(d),i ?
?
?,i?2d)Now we will discuss the stochastic partial deriva-tives with respect to the top-level parameters ?
?,which requires modifying Equation 3.
The firstterm in that equation is a summation over allthe models.
In the stochastic derivative we onlyperform this computation for the datum?s modelm(d), and then we rescale that value based on thenumber of data in that datum?s model |Dm(d)|.
Thesecond term in that equation is rescaled by the to-tal number of data in all models combined.
Thestochastic partial derivatives with respect to ??
be-come:?Lhier-stoch(D; ?)??
?,i= (6)1|Dm(d)|(?
?,i ?
?m(d),i?2m)?
1?m?M|Dm|(??,i?2?
)where for conciseness we omit ?
under the as-sumption that it equals zero.An equally correct formulation for the partialderivative of ??
is to simply rescale Equation 3by the total number of data in all models.
Earlyexperiments found that both versions gave simi-lar performance, but the latter was significantly723B-PERHilaryI-PERClintonOvisitedB-GPEHaitiO.
(a)PERHilary ClintonOvisitedGPEHaitiO.(b)ROOTPERPER-iHilaryPER-iClintonOvisitedGPEGPE-iHaitiO.
(c)Figure 3: A linear-chain CRF (a) labels each word,whereas a semi-CRF (b) labels entire entities.
Asemi-CRF can be represented as a tree (c), where iindicates an internal node for an entity.slower to compute because it required summingover the parameter vectors for all base models in-stead of just the vector for the datum?s model.When using a batch size larger than one, youcompute the given functions for each datum in thebatch and then add them together.4 Base ModelsOur hierarchical joint model is composed of threeseparate models, one for just named entity recog-nition, one for just parsing, and one for joint pars-ing and named entity recognition.
In this sectionwe will review each of these models individually.4.1 Semi-CRF for Named Entity RecognitionFor our named entity recognition model we use asemi-CRF (Sarawagi and Cohen, 2004; Andrew,2006).
Semi-CRFs are very similar to the morepopular linear-chain CRFs, but with several keyadvantages.
Semi-CRFs segment and label thetext simultaneously, whereas a linear-chain CRFwill only label each word, and segmentation is im-plied by the labels assigned to the words.
Whendoing named entity recognition, a semi-CRF willhave one node for each entity, unlike a regularCRF which will have one node for each word.2See Figure 3a-b for an example of a semi-CRFand a linear-chain CRF over the same sentence.Note that the entity Hilary Clinton has one nodein the semi-CRF representation, but two nodes inthe linear-chain CRF.
Because different segmen-tations have different model structures in a semi-CRF, one has to consider all possible structures(segmentations) as well as all possible labelings.It is common practice to limit segment length inorder to speed up inference, as this allows for theuse of a modified version of the forward-backwardalgorithm.
When segment length is not restricted,the inference procedure is the same as that usedin parsing (Finkel and Manning, 2009c).3 In thiswork we do not enforce a length restriction, anddirectly utilize the fact that the model can be trans-formed into a parsing model.
Figure 3c shows aparse tree representation of a semi-CRF.While a linear-chain CRF allows features overadjacent words, a semi-CRF allows them over ad-jacent segments.
This means that a semi-CRF canutilize all features used by a linear-chain CRF, andcan also utilize features over entire segments, suchas First National Bank of New York City, instead ofjust adjacent words like First National and Bankof.
Let y be a vector representing the labeling foran entire sentence.
yi encodes the label of the ithsegment, along with the span of words the seg-ment encompasses.
Let ?
be the feature weights,and f(s, yi, yi?1) the feature function over adja-cent segments yi and yi?1 in sentence s.4 The loglikelihood of a semi-CRF for a single sentence s isgiven by:L(y|s; ?)
= 1Zs|y|?i=1exp{?
?
f(s, yi, yi?1)} (7)The partition function Zs serves as a normalizer.It requires summing over the set ys of all possiblesegmentations and labelings for the sentence s:Zs =?y?ys|y|?i=1exp{?
?
f(s, yi, yi?1)} (8)2Both models will have one node per word for non-entitywords.3While converting a semi-CRF into a parser results inmuch slower inference than a linear-chain CRF, it is still sig-nificantly faster than a treebank parser due to the reducednumber of labels.4There can also be features over single entities, but thesecan be encoded in the feature function over adjacent entities,so for notational simplicity we do not include an additionalterm for them.724FRAGINTJUHLikeNPNPDTaNNgrossPPINofNP-MONEYQP-MONEY-iDT-MONEY-iaCD-MONEY-ibillionNNS-MONEY-idollarsNPJJlastNNyearFigure 4: An example of a sentence jointly annotated with parse and named entity information.
Namedentities correspond to nodes in the tree, and the parse label is augmented with the named entity informa-tion.Because we use a tree representation, it iseasy to ensure that the features used in the NERmodel are identical to those in the joint parsingand named entity model, because the joint model(which we will discuss in Section 4.3) is alsobased on a tree representation where each entitycorresponds to a single node in the tree.4.2 CRF-CFG for ParsingOur parsing model is the discriminatively trained,conditional random field-based context-free gram-mar parser (CRF-CFG) of (Finkel et al, 2008).The relationship between a CRF-CFG and a PCFGis analogous to the relationship between a linear-chain CRF and a hidden Markov model (HMM)for modeling sequence data.
Let t be a com-plete parse tree for sentence s, and each lo-cal subtree r ?
t encodes both the rule fromthe grammar, and the span and split informa-tion (e.g NP(7,9) ?
JJ(7,8)NN(8,9) which coversthe last two words in Figure 1).
The feature func-tion f(r, s) computes the features, which are de-fined over a local subtree r and the words of thesentence.
Let ?
be the vector of feature weights.The log-likelihood of tree t over sentence s is:L(t|s; ?)
= 1Zs?r?texp{?
?
f(r, s)} (9)To compute the partition function Zs, whichserves to normalize the function, we must sumover ?
(s), the set of all possible parse trees forsentence s. The partition function is given by:Zs =?t???(s)?r?t?exp{?
?
f(r, s)}We also need to compute the partial derivativeswhich are used during optimization.
Let fi(r, s)be the value of feature i for subtree r over sen-tence s, and let E?
[fi|s] be the expected value offeature i in sentence s, based on the current modelparameters ?.
The partial derivatives of ?
are thengiven by?L??i=?
(t,s)?D((?r?tfi(r, s))?
E?
[fi|s])(10)Just like with a linear-chain CRF, this equationwill be zero when the feature expectations in themodel equal the feature values in the training data.A variant of the inside-outside algorithm is usedto efficiently compute the likelihood and partialderivatives.
See (Finkel et al, 2008) for details.4.3 Joint Model of Parsing and Named EntityRecognitionOur base joint model for parsing and named entityrecognition is the same as (Finkel and Manning,2009b), which is also based on the discriminativeparser discussed in the previous section.
The parsetree structure is augmented with named entity in-formation; see Figure 4 for an example.
The fea-tures in the joint model are designed in a man-ner that fits well with the hierarchical joint model:some are over just the parse structure, some areover just the named entities, and some are over thejoint structure.
The joint model shares the NERand parse features with the respective single-taskmodels.
Features over the joint structure only ap-pear in the joint model, and their weights are onlyindirectly influenced by the singly-annotated data.In the parsing model, the grammar consists ofonly the rules observed in the training data.
In thejoint model, the grammar is augmented with ad-725Training TestingRange # Sent.
Range # Sent.ABC 0?55 1195 56?69 199MNB 0?17 509 18?25 245NBC 0?29 589 30?39 149PRI 0?89 1704 90?112 394VOA 0?198 1508 199?264 385Table 1: Training and test set sizes for the fivedatasets in sentences.
The file ranges refer tothe numbers within the names of the originalOntoNotes files.ditional joint rules which are composed by addingnamed entity information to existing parse rules.Because the grammars are based on the observeddata, and the two models have different data, theywill have somewhat different grammars.
In our hi-erarchical joint model, we added all observed rulesfrom the joint data (stripped of named entity infor-mation) to the parse-only grammar, and we addedall observed rules from the parse-only data to thegrammar for the joint model, and augmented themwith named entity information in the same manneras the rules observed in the joint data.Earlier we said that the NER-only model usesidentical named entity features as the joint model(and similarly for the parse-only model), but thisis not quite true.
They use identical feature tem-plates, such as word, but different realizationsof those features will occur with the differentdatasets.
For instance, the NER-only model mayhave word=Nigel as a feature, but because Nigelnever occurs in the joint data, that feature is nevermanifested and no weight is learned for it.
We dealwith this similarly to how we dealt with the gram-mar: if a named entity feature occurs in either thejoint data or the NER-only data, then both mod-els will learn a weight for that feature.
We do thesame thing for the parse features.
This modelingdecision gives the joint model access to potentiallyuseful features to which it would not have had ac-cess if it were not part of the hierarchical model.55 Experiments and DiscussionWe compared our hierarchical joint model to a reg-ular (non-hierarchical) joint model, and to parse-only and NER-only models.
Our baseline ex-periments were modeled after those in (Finkeland Manning, 2009b), and while our results werenot identical (we updated to a newer release ofthe data), we had similar results and found thesame general trends with respect to how the joint5In the non-hierarchical setting, you could include thosefeatures in the optimization, but, because there would be noevidence about them, their weights would be zero due to reg-ularization.model improved on the single models.
We usedOntoNotes 3.0 (Hovy et al, 2006), and made thesame data modifications as (Finkel and Manning,2009b) to ensure consistency between the parsingand named entity annotations.
Table 2 has ourcomplete set of results, and Table 1 gives the num-ber of training and test sentences.
For each sec-tion of the data (ABC, MNB, NBC, PRI, VOA)we ran experiments training a linear-chain CRFon only the named entity information, a CRF-CFGparser on only the parse information, a joint parserand named entity recognizer, and our hierarchi-cal model.
For the hierarchical model, we usedthe CNN portion of the data (5093 sentences) forthe extra named entity data (and ignored the parsetrees) and the remaining portions combined for theextra parse data (and ignored the named entity an-notations).
We used ??
= 1.0 and ?m = 0.1,which were chosen based on early experiments ondevelopment data.
Small changes to ?m do notappear to have much influence, but larger changesdo.
We similarly decided how many iterations torun stochastic gradient descent for (20) based onearly development data experiments.
We did notrun this experiment on the CNN portion of thedata, because the CNN data was already beingused as the extra NER data.As Table 2 shows, the hierarchical model didsubstantially better than the joint model overall,which is not surprising given the extra data towhich it had access.
Looking at the smaller cor-pora (NBC and MNB) we see the largest gains,with both parse and NER performance improvingby about 8% F1.
ABC saw about a 6% gain onboth tasks, and VOA saw a 1% gain on both.
Ourone negative result is in the PRI portion: parsingimproves slightly, but NER performance decreasesby almost 2%.
The same experiment on develop-ment data resulted in a performance increase, sowe are not sure why we saw a decrease here.
Onegeneral trend, which is not surprising, is that thehierarchical model helps the smaller datasets morethan the large ones.
The source of this is two-fold: lower baselines are generally easier to im-prove upon, and the larger corpora had less singly-annotated data to provide improvements, becauseit was composed of the remaining, smaller, sec-tions of OntoNotes.
We found it interesting thatthe gains tended to be similar on both tasks for alldatasets, and believe this fact is due to our use ofroughly the same amount of singly-annotated datafor both parsing and NER.One possible conflating factor in these experi-ments is that of domain drift.
While we tried to726Parse Labeled Bracketing Named EntitiesPrecision Recall F1 Precision Recall F1ABC Just Parse 69.8% 69.9% 69.8% ?Just NER ?
77.0% 75.1% 76.0%Baseline Joint 70.2% 70.5% 70.3% 79.2% 76.5% 77.8%Hierarchical Joint 75.5% 74.4% 74.9% 85.1% 82.7% 83.9%MNB Just Parse 61.7% 65.5% 63.6% ?Just NER ?
69.6% 49.0% 57.5%Baseline Joint 61.7% 66.2% 63.9% 70.9% 63.5% 67.0%Hierarchical Joint 72.6% 70.2% 71.4% 74.4% 75.5% 74.9%NBC Just Parse 59.9% 63.9% 61.8% ?Just NER ?
63.9% 60.9% 62.4%Baseline Joint 59.3% 64.2% 61.6% 68.9% 62.8% 65.7%Hierarchical Joint 70.4% 69.9% 70.2% 72.9% 74.0% 73.4%PRI Just Parse 78.6% 77.0% 76.9% ?Just NER ?
81.3% 77.8% 79.5%Baseline Joint 78.0% 78.6% 78.3% 86.3% 86.0% 86.2%Hierarchical Joint 79.2% 78.5% 78.8% 84.2% 85.5% 84.8%VOA Just Parse 77.5% 76.5% 77.0% ?Just NER ?
85.2% 80.3% 82.7%Baseline Joint 77.2% 77.8% 77.5% 87.5% 86.7% 87.1%Hierarchical Joint 79.8% 77.8% 78.8% 87.7% 88.9% 88.3%Table 2: Full parse and NER results for the six datasets.
Parse trees were evaluated using evalB, andnamed entities were scored using micro-averaged F-measure (conlleval).get the most similar annotated data available ?
datawhich was annotated by the same annotators, andall of which is broadcast news ?
these are still dif-ferent domains.
While this is likely to have a nega-tive effect on results, we also believe this scenarioto be a more realistic than if it were to also be datadrawn from the exact same distribution.6 ConclusionIn this paper we presented a novel method forimproving joint modeling using additional datawhich has not been labeled with the entire jointstructure.
While conventional wisdom says thatadding more training data should always improveperformance, this work is the first to our knowl-edge to incorporate singly-annotated data into ajoint model, thereby providing a method for thisadditional data, which cannot be directly used bythe non-hierarchical joint model, to help improvejoint modeling performance.
We built single-taskmodels for the non-jointly labeled data, designingthose single-task models so that they have featuresin common with the joint model, and then linkedall of the different single-task and joint modelsvia a hierarchical prior.
We performed experi-ments on joint parsing and named entity recogni-tion, and found that our hierarchical joint modelsubstantially outperformed a joint model whichwas trained on only the jointly annotated data.Future directions for this work include automat-ically learning the variances, ?m and ??
in the hi-erarchical model, so that the degree of informationsharing between the models is optimized based onthe training data available.
We are also interestedin ways to modify the objective function to placemore emphasis on learning a good joint model, in-stead of equally weighting the learning of the jointand single-task models.AcknowledgmentsMany thanks to Daphne Koller for discussionswhich led to this work, and to Richard Socherfor his assistance and input.
Thanks also to ouranonymous reviewers and Yoav Goldberg for use-ful feedback on an earlier draft of this paper.This material is based upon work supported bythe Air Force Research Laboratory (AFRL) un-der prime contract no.
FA8750-09-C-0181.
Anyopinions, findings, and conclusion or recommen-dations expressed in this material are those of theauthor(s) and do not necessarily reflect the view ofthe Air Force Research Laboratory (AFRL).
Thefirst author is additionally supported by a StanfordGraduate Fellowship.727ReferencesMeni Adler and Michael Elhadad.
2006.
An unsupervisedmorpheme-based hmm for hebrew morphological disam-biguation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44th annualmeeting of the Association for Computational Linguistics,pages 665?672, Morristown, NJ, USA.
Association forComputational Linguistics.Rie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method for textchunking.
In ACL ?05: Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics,pages 1?9, Morristown, NJ, USA.
Association for Com-putational Linguistics.Galen Andrew.
2006.
A hybrid markov/semi-markov con-ditional random field for sequence segmentation.
In Pro-ceedings of the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP 2006).J.
Baxter.
1997.
A bayesian/information theoretic model oflearning to learn via multiple task sampling.
In MachineLearning, volume 28.R.
Caruana.
1997.
Multitask learning.
In Machine Learning,volume 28.Michael Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In ACL 1997.Hal Daume?
III.
2007.
Frustratingly easy domain adaptation.In Conference of the Association for Computational Lin-guistics (ACL), Prague, Czech Republic.Gal Elidan, Benjamin Packer, Geremy Heitz, and DaphneKoller.
2008.
Convex point estimation using undirectedbayesian transfer hierarchies.
In UAI 2008.T.
Evgeniou, C. Micchelli, and M. Pontil.
2005.
Learningmultiple tasks with kernel methods.
In Journal of MachineLearning Research.Jenny Rose Finkel and Christopher D. Manning.
2009a.
Hi-erarchical bayesian domain adaptation.
In Proceedingsof the North American Association of Computational Lin-guistics (NAACL 2009).Jenny Rose Finkel and Christopher D. Manning.
2009b.
Jointparsing and named entity recognition.
In Proceedings ofthe North American Association of Computational Lin-guistics (NAACL 2009).Jenny Rose Finkel and Christopher D. Manning.
2009c.Nested named entity recognition.
In Proceedings ofEMNLP 2009.Jenny Rose Finkel, Alex Kleeman, and Christopher D. Man-ning.
2008.
Efficient, feature-based conditional randomfield parsing.
In ACL/HLT-2008.Andrew Gelman and Jennifer Hill.
2006.
Data Analysis Us-ing Regression and Multilevel/Hierarchical Models.
Cam-bridge University Press.A.
Gelman, J.
B. Carlin, H. S. Stern, and Donald D. B. Rubin.2003.
Bayesian Data Analysis.
Chapman & Hall.Yoav Goldberg and Reut Tsarfaty.
2008.
A single genera-tive model for joint morphological segmentation and syn-tactic parsing.
In Proceedings of ACL-08: HLT, pages371?379, Columbus, Ohio, June.
Association for Compu-tational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes: The90% solution.
In HLT-NAACL 2006.Richard Johansson and Pierre Nugues.
2008.
Dependency-based syntactic-semantic analysis with propbank andnombank.
In CoNLL ?08: Proceedings of the TwelfthConference on Computational Natural Language Learn-ing, pages 183?187, Morristown, NJ, USA.
Associationfor Computational Linguistics.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical parsing toextract information from text.
In In 6th Applied NaturalLanguage Processing Conference, pages 226?233.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markovconditional random fields for information extraction.
In InAdvances in Neural Information Processing Systems 17,pages 1185?1192.Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu?
?sMa`rquez, and Joakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing of syntactic and semanticdependencies.
In Proceedings of the 12th Conferenceon Computational Natural Language Learning (CoNLL),Manchester, UK.Charles Sutton and Andrew McCallum.
2005.
Joint pars-ing and semantic role labeling.
In Conference on NaturalLanguage Learning (CoNLL).Kristina Toutanova and Colin Cherry.
2009.
A global modelfor joint lemmatization and part-of-speech prediction.
InProceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International Joint Con-ference on Natural Language Processing of the AFNLP,pages 486?494, Suntec, Singapore, August.
Associationfor Computational Linguistics.Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishna-puram.
2007.
Multi-task learning for classification withdirichlet process priors.
J. Mach.
Learn.
Res., 8.Kai Yu, Volker Tresp, and Anton Schwaighofer.
2005.
Learn-ing gaussian processes from multiple tasks.
In ICML ?05:Proceedings of the 22nd international conference on Ma-chine learning.Yue Zhang and Stephen Clark.
2008.
Joint word segmenta-tion and POS tagging using a single perceptron.
In ACL2008.728
