Flexible Spoken Dialogue System based on User Modelsand Dynamic Generation of VoiceXML ScriptsKazunori Komatani Fumihiro Adachi Shinichi UenoTatsuya Kawahara Hiroshi G. OkunoGraduate School of InformaticsKyoto UniversityYoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan{komatani,adachi,ueno,kawahara,okuno}@kuis.kyoto-u.ac.jpAbstractWe realize a telephone-based collab-orative natural language dialogue sys-tem.
Since natural language involvesvery various expressions, a large num-ber of VoiceXML scripts need to be pre-pared to handle all possible input patterns.We realize flexible dialogue managementfor various user utterances by generatingVoiceXML scripts dynamically.
More-over, we address appropriate user mod-eling in order to generate cooperative re-sponses to each user.
Specifically, we setup three dimensions of user models: skilllevel to the system, knowledge level onthe target domain and the degree of hasti-ness.
The models are automatically de-rived by decision tree learning using realdialogue data collected by the system.
Ex-perimental evaluation shows that the co-operative responses adapted to individualusers serve as good guidance for noviceusers without increasing the dialogue du-ration for skilled users.Keywords: spoken dialogue system, user model,VoiceXML, cooperative responses, dialoguestrategy1 IntroductionA Spoken dialogue system is one of the promisingapplications of the speech recognition and naturallanguage understanding technologies.
A typical taskof spoken dialogue systems is database retrieval.Some IVR (interactive voice response) systems us-ing the speech recognition technology are being putinto practical use as its simplest form.
According tothe spread of cellular phones, spoken dialogue sys-tems via telephone enable us to obtain informationfrom various places without any other special appa-ratuses.In order to realize user-friendly interaction, spo-ken dialogue systems should be able to (1) acceptvarious user utterances to enable mixed-initiative di-alogue and (2) generate cooperative responses.
Cur-rently, a lot of IVR systems via telephone operateby using VoiceXML, which is a script language toprescribe procedures of spoken dialogues.
How-ever, only the next behaviors corresponding to ev-ery input are prescribed in the VoiceXML scripts,so the dialogue procedure is basically designed assystem-initiated one, in which the system asked re-quired items one by one.
In order to realize mixed-initiative dialogue, the system should be able to ac-cept various user-initiated utterances.
By allowingto accept various user utterances, the combinationof words included in the utterances accordingly getsenormous, and then it is practically impossible toprepare VoiceXML scripts that correspond to theenormous combinations of input words in advance.It is also difficult to generate cooperative responsesadaptively in the above framework.We propose a framework to generate VoiceXMLscripts dynamically in order to realize the mixed-initiative dialogue, in which the system is needed toaccept various user utterances.
This framework real-izes flexible dialogue management without requiringpreparation for a large number of VoiceXML scriptsin advance.
Furthermore, it enables various behav-iors adaptive to the dialogue situations such as ob-tained query results.Another problem to realize user-friendly interac-tion is how to generate cooperative responses.
Whenwe consider the responses generated from the sys-tem side, the dialogue strategies, which determinewhen to make guidance and what the system shouldtell to the user, are the essential factors in spoken di-alogue systems.
There are many studies in respect ofthe dialogue strategy such as confirmation manage-ment using confidence measures of speech recogni-tion results (Komatani and Kawahara, 2000; Hazenet al, 2000), dynamic change of dialogue initiative(Litman and Pan, 2000; Chu-Carroll, 2000; Lamelet al, 1999), and addition of cooperative contentsto system responses (Sadek, 1999).
Nevertheless,whether a particular response is cooperative or notdepends on individual user?s characteristic.In order to adapt the system?s behavior to individ-ual users, it is necessary to model the user?s patterns(Kass and Finin, 1988).
Most of conventional stud-ies on user models have focused on the knowledgeof users.
Others tried to infer and utilize user?s goalsto generate responses adapted to the user (van Beek,1987; Paris, 1988).
Elzer et al (2000) proposeda method to generate adaptive suggestions accord-ing to users?
preferences.
However, these studiesdepend on knowledge of the target domain greatly,and therefore the user models need to be deliberatedmanually to be applied to new domains.
Moreover,they assumed that the input is text only, which doesnot contain errors.We propose more comprehensive user models togenerate user-adapted responses in spoken dialoguesystems taking account of information specific tospoken dialogue.
Spoken utterances include vari-ous information such as the interval between the ut-terances, the presence of barge-in and so on, whichcan be utilized to judge the user?s character.
Thesefeatures also possess generality in spoken dialoguesystems because they are not dependent on domain-specific knowledge.
As user models in spoken di-alogue systems, Eckert et al (1997) defined stereo-types of users such as patient, submissive and ex-perienced, in order to evaluate spoken dialogue sys-tems by simulation.
We introduce user models notfor defining users?
behaviors beforehand, but for de-tecting users?
patterns in real-time interaction.We define three dimensions in the user models:?skill level to the system?, ?knowledge level on thetarget domain?
and ?degree of hastiness?.
The usermodels are trained by decision tree learning algo-rithm, using real data collected from the Kyoto citybus information system.
Then, we implement theuser models on the system and evaluate them usingdata collected with 20 novice users.2 Flexible Spoken Dialogue System basedon Dynamic Generation of VoiceXMLScriptsVoiceXML1 is a script language to prescribe pro-cedures in spoken dialogues mainly on telephone,and is becoming to a standard language of IVR sys-tems.
The VoiceXML scripts consist of three parts:(1) specifications of system?s prompts, (2) specifica-tions of grammars to accept a user?s utterance, and(3) description of the next behaviors.However, most of existing services using theVoiceXML imposes rigid interaction, in which userutterances are restricted by system-initiated promptsand a user is accordingly allowed to specify only re-quested items one by one.
It is more user-friendlythat users can freely convey their requests by naturallanguage expressions.We present a framework to realize flexible inter-action by generating VoiceXML scripts dynamically(Pargellis et al, 1999; Nyberg et al, 2002).
Theframework enables users to express their requestsby natural language even in VoiceXML-based sys-tems.
Furthermore, cooperative responses in the Ky-oto city bus information system that has been devel-oped in our laboratory are also presented in this sec-tion.2.1 Dynamic Generation of VoiceXML ScriptsIn VoiceXML scripts, acceptable keywords and cor-responding next states must be explicitly specified.However, since there exists enormous combinationsof keywords in natural language expressions, it ispractically impossible to describe all VoiceXMLscripts that correspond to the combinations.
Then,we introduce the framework in which VoiceXML1VoiceXML Forum.
http://www.voicexml.org/speech synthesizedspeechVoiceXMLdialoguemanagerTTS enginespeechrecognizergrammarruleskeywordsresponsesentencesFront endCGI scriptuserVWS (Voice Web Server)VoiceXMLgeneratordatabaseFigure 1: Overview of spoken dialogue systembased on dynamic generation of VoiceXML scriptsscripts are generated dynamically to enable the sys-tem to accept natural language expressions.Figure 1 shows the overview of the framework.The front end that operates based on VoiceXMLscripts is separated from the dialogue managementportion, which accepts speech recognition resultsand generates corresponding VoiceXML scripts.The user utterance is recognized based on a gram-mar rule specified in VoiceXML scripts, and key-words extracted from a speech recognition result arepassed to a CGI script.
The CGI script retrieves cor-responding information from the database on Web,and generates a VoiceXML script for the succeed-ing interaction.
If sufficient information is not ob-tained from a user utterance, a script that prompts tofill remaining contents is generated, and if the userutterance contains ambiguity, a script that makes adisambiguating question is generated.Consequently, the generation of VoiceXMLscripts enables to accept natural language expres-sions without preparing a large number of the scriptscorresponding to various inputs beforehand.
Theframework also enables to generate cooperative re-sponses adapted to the situation such as retrieval re-sults without spoiling portability.Sys: Please tell me your current bus stop, your destinationor the specific bus route.User: Shijo-Kawaramachi.Sys: Do you take a bus from Shijo-Kawaramachi?User: Yes.Sys: Where will you get off the bus?User: Arashiyama.Sys: Do you go from Shijo-Kawaramachi to Arashiyama?User: Yes.Sys: Bus number 11 bound for Arashiyama has departedSanjo-Keihanmae, two bus stops away.Figure 2: Example dialogue of the bus system2.2 Kyoto City Bus Information SystemWe have developed the Kyoto city bus informationsystem, which locates the bus a user wants to take,and tell him/her how long it will take before arriving.The system can be accessed via telephone includingcellular phones2.
From any places, users can easilyget the information on the bus that changes everyminute.
Users are requested to input the bus stop toget on, the destination, or the bus route number byspeech, and consequently get the bus information.There are 652 bus stops and 85 bus routes in Kyotocity.
The bus stops can also be specified by the nameof famous places or public facilities nearby.
Figure2 shows a simple example of the dialogue.2.3 Generation of Cooperative Responses inKyoto City Bus Information SystemIn spoken dialogue systems, it is important to gener-ate cooperative responses by guessing the user?s in-tention.
The cooperative responses consist of addinginformation more than the user has requested, tellingthe reason when there is no query result, present-ing the alternatives, and so on (Sadek, 1999).
Inorder to generate cooperative responses, it is neces-sary to change responses according to the situations.Since we separate language-understanding modulesfrom individual VoiceXML scripts that are gener-ated dynamically, our system can generate appropri-ate responses cooperatively according to query re-sults.
The examples of the cooperative responsesimplemented in our Kyoto city bus information sys-tem are listed below.2+81-75-326-3116(1) Suggestive answersThe information related to a query result is offeredwhen the query result is negative.
(Example)User: From Hyakumanben to Kyoto Station by busnumber 17.Sys: Bus number 17 does not come for a while, butbus number 206 has departed Takano, four busstops away.
(2) Corrective answersThe reason is presented when there is no query resultor when a user utterance contains an error.
(Example)User: From Kyoto Station to Arashiyama.Sys: There is no bus that directly goes from Kyotostation to Arashiyama.
(3) Intensional answersIf several query results are obtained, generating in-tensional answer is more cooperative than listing allthe results.
(Example)User: From Shijo-Kawaramachi to Shijo-Karasuma.Sys: All buses to Shijo-Karasuma do not come for awhile.
(4) Completion answersIt is cooperative to generate responses including in-formation more than the user?s request.
However,whether the information should be presented or notdiffers in individual users.
(Example)User: From Kyoto station by bus number 5.Sys: Bus number 5 will depart at 17:34 from plat-form ?A1?.
(There are a lot of platforms in Kyoto station.
)3 Response Generation Using User ModelsWe propose comprehensive user models to generateuser-adaptive responses taking account of informa-tion specific to spoken dialogue.
Whether a partic-ular response is regarded as cooperative depends onindividual user?s characteristics.
So, we address ap-propriate user modeling in order to generate cooper-ative responses to the users.3.1 Classification of User ModelsWe define three dimensions as user models listed be-low.?
Skill level to the system?
Knowledge level on the target domain?
Degree of hastinessSkill Level to the SystemSince spoken dialogue systems are notwidespread yet, there arises a difference in theskill level of users in operating the systems.
Itis desirable that the system changes its behaviorincluding response generation and initiative man-agement in accordance with the skill level of theuser.
In conventional systems, a system-initiatedguidance has been invoked on the spur of themoment either when the user says nothing orwhen speech recognition is not successful.
In ourframework, we address a radical solution for theunskilled users by modeling the skill level as theuser?s property before such a problem arises.Knowledge Level on the Target DomainThere also exists a difference in the knowledgelevel on the target domain among users.
Thus, it isnecessary for the system to change information topresent to users.
For example, it is not cooperativeto tell too detailed information to strangers.
On theother hand, for inhabitants, it is useful to omit tooobvious information and to output more detailed in-formation.
Therefore, we introduce a dimension thatrepresents the knowledge level on the target domain.Degree of HastinessIn speech communications, it is more importantto present information promptly and concisely com-pared with the other communication modes such asbrowsing.
Especially in the bus system, the concise-ness is preferred because the bus information is ur-gent to most users.
Therefore, we also take accountof degree of hastiness of the user, and accordinglychange the system?s responses.3.2 Response Generation Strategy Using UserModelsNext, we describe the response generation strategiesadapted to individual users based on the proposeduser models: skill level, knowledge level and hasti-ness.
Basic design of dialogue management is basedon mixed-initiative dialogue, in which the systemmakes follow-up questions and guidance if neces-sary while allowing a user to utter freely.
It is in-vestigated to add various contents to the system re-sponses as cooperative responses in (Sadek, 1999).Such additional information is usually cooperative,but some people may feel such a response redun-dant.Thus, we introduce the user models and con-trol the generation of additional information.
Byintroducing the proposed user models, the systemchanges generated responses by the following twoaspects: dialogue procedure and contents of re-sponses.Dialogue ProcedureThe dialogue procedure is changed based on theskill level and the hastiness.
If a user is identified ashaving the high skill level, the dialogue managementis carried out in a user-initiated manner; namely, thesystem generates only open-ended prompts.
On theother hand, when user?s skill level is detected as low,the system takes an initiative and prompts necessaryitems in order.When the degree of hastiness is low, the systemmakes confirmation on the input contents.
Con-versely, when the hastiness is detected as high, sucha confirmation procedure is omitted; namely, thesystem immediately makes a query and outputs theresult without making such a confirmation.Contents of ResponsesInformation that should be included in the sys-tem response can be classified into the following twoitems.1.
Dialogue management information2.
Domain-specific informationThe dialogue management information specifieshow to carry out the dialogue including the instruc-tion on user?s expression for yes/no questions like?Please reply with either yes or no.?
and the expla-nation about the following dialogue procedure like?Now I will ask in order.?
This dialogue manage-ment information is determined by the user?s skill58.8>=the maximum number of filled slotsdialogue stateinitial state otherwisepresense of barge-inrate of no input0.07>30 1 2average ofrecognition score58.8<skill levelhighskill levelhighskill levellowskill levellowFigure 3: Decision tree for the skill levellevel to the system, and is added to system responseswhen the skill level is considered as low.The domain-specific information is generated ac-cording to the user?s knowledge level on the targetdomain.
Namely, for users unacquainted with thelocal information, the system adds the explanationabout the nearest bus stop, and omits complicatedcontents such as a proposal of another route.The contents described above are also controlledby the hastiness.
For users who are not in hurry, thesystem generates the additional contents that corre-spond to their skill level and knowledge level as co-operative responses.
On the other hand, for hastyusers, the contents are omitted to prevent the dia-logue from being redundant.3.3 Classification of User based on DecisionTreeIn order to implement the proposed user models asclassifiers, we adopt a decision tree.
It is constructedby decision tree learning algorithm C5.0 (Quinlan,1993) with data collected by our dialogue system.Figure 3 shows an example of the derived decisiontree for the skill level.We use the features listed in Figure 4.
They in-clude not only semantic information contained in theutterances but also information specific to spoken di-alogue systems such as the silence duration prior tothe utterance, the presence of barge-in and so on.Except for the last category of Figure 4 including?attribute of specified bus stops?, most of the fea-tures are domain-independent.The classification of each dimension is done forevery user utterance except for knowledge level.
Themodel of a user can change during a dialogue.
Fea-tures extracted from utterances are accumulated ashistory information during the session.?
features obtained from a single utterance?
dialogue state (defined by already filled slots)?
presence of barge-in?
lapsed time of the current utterance?
recognition result (something recognized / un-certain / no input)?
score of speech recognizer?
the number of filled slots by the current utter-ance?
features obtained from the session?
the number of utterances?
dialogue state of the previous utterance?
lapsed time from the beginning of the session?
the number of the repetition of a same question?
the average number of the repetition of a samequestion?
ratio of the time of user utterances in wholeelapsed time?
ratio of the occurrence of barge-in out of thewhole number of utterances?
recognition result of the previous utterance?
ratio of something recognized?
ratio of getting uncertain results?
ratio of no input?
the number of barge-in?
the number of something recognized?
the number of getting uncertain results?
the number of no input?
average of recognition scores?
the maximum number of slots filled by a singleutterance?
features obtained from the session (used only in clas-sification of knowledge level)?
attribute of specified bus stops?
the way of specifying bus stops (whether a busstop is specified by its correct name or not)?
ratio of each attribute of specified bus stops?
ratio of each way of specifying bus stopsFigure 4: Features to classify the user modelsFigure 5 shows an example of the system behav-ior with the proposed user models.
The skill level isclassified as being low by the decision tree, becausethe first user?s utterance includes only one contentword.
Then, dialogue procedure is changed to thesystem-initiated one.
Similarly, the hastiness is clas-sified as being low by the decision tree, and the sys-tem includes the explanation on the dialogue pro-cedure and the instruction on the expression in theresponses.
They are omitted if the hastiness is iden-tified as high.3.4 Decision Tree Learning for User ModelsWe train and evaluate the decision tree for the usermodels using dialogue data collected by our sys-case: [skill level: low][knowledge level: low][hastiness: low]Sys: Please tell me your current bus stop, your destinationor the specific bus route number.User: Maruyama Park.Sys: Let me confirm one by one.(?
The explanation is added by [skill level: low].
)Will you get on at Maruyama Park?(?
The content is confirmed by [hastiness: low].
)Please reply with yes or no.(?
The instruction is added by [skill level: low].
)User: Yes.Sys: Where will you get off the bus?
Please specify by abus stop name or a well-known landmark.(?
The explanation is added by [skill level: low].
)User: Tofukuji temple.Sys: Will you get off the bus at Tofukuji temple?Please reply with yes or no.User: Yes.Sys: The following buses are located.(?
The explanation is added by [skill level: low].
)The nearest bus stop to Maruyama Park is Gion.(?
The domain information is added by [knowl-edge level: low].
)Bus number 202 bound for Nishioji-Kujo has de-parted Higashiyama-Sanjo, which is two stops away....Figure 5: An example dialogue with the proposeduser modelslow indeterminable high totalskill level 743 253 496 1492knowledge level 275 808 409 1492hastiness 421 932 139 1492Table 1: Number of manually annotated labels fordecision tree learningtem.
The data was collected from December 10th2001 to May 10th 2002.
The number of the ses-sions (telephone calls) is 215, and the total numberof utterances included in the sessions is 1492.
Weannotated the subjective labels of the user modelsby hand.
The annotator judges the user models forevery utterance based on the recorded speech dataand logs.
The labels were given to the three dimen-sions described in section 3.1 among ?high?, ?inde-terminable?
or ?low?.
It is possible that the annotatedmodel of a user changes during a dialogue, espe-cially from ?indeterminable?
to ?low?
or ?high?.
Thenumber of the labeled utterances is shown in Table1.condition #1 #2 #3skill level 80.8% 75.3% 85.6%knowledge level 73.9% 63.7% 78.2%hastiness 74.9% 73.7% 78.6%Table 2: Classification accuracy of the proposed usermodelsUsing the labeled data, we trained the decisiontree and evaluated the classification accuracy of theproposed user models.
All the experiments were car-ried out by the method of 10-fold cross validation.The process, in which one tenth of all data is used asthe test data, and the remainder is used as the train-ing data, is repeated ten times, and the average ofthe classification accuracy is computed.
The resultis shown in Table 2.
The conditions #1, #2 and #3 inTable 2 are described as follows.#1: The 10-fold cross validation is carried out perutterance.#2: The 10-fold cross validation is carried out persession (call).#3: We calculate the accuracy under more realis-tic condition.
The accuracy is calculated notin three classes (high / indeterminable / low)but in two classes that actually affect the dia-logue strategies.
For example, the accuracy forthe skill level is calculated for the two classes:low and the others.
As to the classification ofknowledge level, the accuracy is calculated fordialogue sessions, because the features such asthe attribute of a specified bus stop are not ob-tained in every utterance.
Moreover, in orderto smooth unbalanced distribution of the train-ing data, a cost corresponding to the reciprocalratio of the number of samples in each class isintroduced.
By the cost, the chance rate of twoclasses becomes 50%.The difference between condition #1 and #2 isthat the training was carried out in a speaker-closedor speaker-open manner.
The former shows betterperformance.The result in condition #3 shows useful accuracyin the skill level.
The following features play im-portant part in the decision tree for the skill level:userprofilesdatabaseon WebCGIthe system except forproposed user modelsuserVWS(Voice Web Server)VoiceXMLgeneratordialoguemanageruser modelidentifierVoiceXMLrecognition results(keywords)recognition results(including features otherthan language info.
)Figure 6: Overview of the Kyoto city bus informa-tion system with user modelsthe number of filled slots by the current utterance,presence of barge-in and ratio of no input.
For theknowledge level, recognition result (something rec-ognized / uncertain / no input), ratio of no input andthe way to specify bus stops (whether a bus stop isspecified by its exact name or not) are effective.
Thehastiness is classified mainly by the three features:presence of barge-in, ratio of no input and lapsedtime of the current utterance.3.5 System OverviewFigure 6 shows an overview of the Kyoto city bus in-formation system with the user models.
The systemoperates by generating VoiceXML scripts dynami-cally as described in section 2.1.
The real-time businformation database is provided on the Web, whichcan be accessed via Internet.
Then, we explain themodules in the following.VWS (Voice Web Server)The Voice Web Server drives the speech recog-nition engine and the TTS (Text-To-Speech)module accordingly to the specifications by thegenerated VoiceXML script.Speech RecognizerThe speech recognizer decodes user utterancesbased on specified grammar rules and vocabu-lary, which are defined by VoiceXML at eachdialogue state.Dialogue ManagerThe dialogue manager generates response sen-tences based on recognition results (bus stopnames or a route number) received from theVWS.
If sufficient information to locate a busis obtained, it retrieves the corresponding businformation on the Web.VoiceXML GeneratorThis module dynamically generates VoiceXMLscripts that contain response sentences andspecifications of speech recognition grammars,which are given by the dialogue manager.User model identifierThis module classifies user?s characters basedon the user models using features specific tospoken dialogue as well as semantic attributes.The obtained user profiles are sent to the dia-logue manager, and are utilized in the dialoguemanagement and response generation.4 Experimental Evaluation of the Systemwith User ModelsWe evaluated the system with the proposed usermodels using 20 novice subjects who had not usedthe system.
The experiment was performed in thelaboratory under adequate control.
For the speechinput, the headset microphone was used.4.1 Experiment ProcedureFirst, we explained the outline of the system to sub-jects and gave a document in which experiment con-ditions and scenarios were described.
We preparedtwo sets of eight scenarios.
Subjects were requestedto acquire the bus information according to the sce-narios using the system with/without the user mod-els.
In the scenarios, neither the concrete names ofbus stops nor the bus number were given.
For exam-ple, one of the scenarios was as follows: ?You are inKyoto for sightseeing.
After visiting the Ginkakujitemple, you go to Maruyama Park.
Supposing sucha situation, please get information on the bus.?
Wealso set the constraint in order to vary the subjects?hastiness such as ?Please hurry as much as possiblein order to save the charge of your cellular phone.
?The subjects were also told to look over question-naire items before the experiment, and filled in themduration (sec.)
# turngroup 1 with UM 51.9 4.03(with UM?
w/o UM) w/o UM 47.1 4.18group 2 w/o UM 85.4 8.23(w/o UM?
with UM) with UM 46.7 4.08UM: User ModelTable 3: Duration and the number of turns in dia-logueafter using each system.
This aims to reduce the sub-ject?s cognitive load and possible confusion due toswitching the systems (Over, 1999).
The question-naire consisted of eight items, for example, ?Whenthe dialogue did not go well, did the system guide in-telligibly??
We set seven steps for evaluation abouteach item, and the subject selected one of them.Furthermore, subjects were asked to write downthe obtained information: the name of the bus stopto get on, the bus number and how much time ittakes before the bus arrives.
With this procedure,we planned to make the experiment condition closeto the realistic one.The subjects were divided into two groups; a half(group 1) used the system in the order of ?withuser models ?
without user models?, the other half(group 2) used in the reverse order.The dialogue management in the system withoutuser models is also based on the mixed-initiativedialogue.
The system generates follow-up ques-tions and guidance if necessary, but behaves in afixed manner.
Namely, additional cooperative con-tents corresponding to skill level described in section3.2 are not generated and the dialogue procedure ischanged only after recognition errors occur.
Thesystem without user models behaves equivalently tothe initial state of the user models: the hastiness islow, the knowledge level is low and the skill level ishigh.4.2 ResultsAll of the subjects successfully completed the giventask, although they had been allowed to give up if thesystem did not work well.
Namely, the task successrate is 100%.Average dialogue duration and the number ofturns in respective cases are shown in Table 3.Though the users had not experienced the system atgroup 1 with UM 0.72(with UM ?
w/o UM) w/o UM 0.70group 2 w/o UM 0.41(w/o UM ?
with UM) with UM 0.63Table 4: Ratio of utterances for which the skill levelwas judged as highall, they got accustomed to the system very rapidly.Therefore, as shown in Table 3, the duration andthe number of turns were decreased obviously in thelatter half of the experiment in both groups.
How-ever, in the initial half of the experiment, the group1 completed with significantly shorter dialogue thangroup 2.
This means that the incorporation of theuser models is effective for novice users.
Table 4shows a ratio of utterances for which the skill levelwas identified as high.
The ratio is calculated by di-viding the number of utterances that were judged ashigh skill level by the number of all utterances.
Theratio is much larger for group 1 who initially usedthe system with user models.
This fact means thatthe novice users got accustomed to the system morerapidly with the user models, because they were in-structed on the usage by cooperative responses gen-erated when the skill level is low.
The results demon-strate that cooperative responses generated accord-ing to the proposed user models can serve as goodguidance for novice users.In the latter half of the experiment, the dialogueduration and the number of turns were almost samebetween the two groups.
This result shows that theproposed models prevent the dialogue from becom-ing redundant for skilled users, although generatingcooperative responses for all users made the dia-logue verbose in general.
It suggests that the pro-posed user models appropriately control the genera-tion of cooperative responses by detecting charactersof individual users.5 ConclusionsWe have presented a framework to realize flexibleinteraction by dynamically generating VoiceXMLscripts.
This framework realizes mixed-initiative di-alogues and the generation of cooperative responsesin VoiceXML-based systems.We have also proposed and evaluated user mod-els for generating cooperative responses adaptivelyto individual users.
The proposed user models con-sist of the three dimensions: skill level to the sys-tem, knowledge level on the target domain and thedegree of hastiness.
The user models are identifiedby decision tree using features specific to spoken di-alogue systems as well as semantic attributes.
Theyare automatically derived by decision tree learning,and all features used for skill level and hastiness areindependent of domain-specific knowledge.
So, it isexpected that the derived user models can be gener-ally used in other domains.The experimental evaluation with 20 novice usersshows that the skill level of novice users was im-proved more rapidly by incorporating user mod-els, and accordingly the dialogue duration becomesshorter more immediately.
The result is achievedby the generated cooperative responses based on theproposed user models.
The proposed user modelsalso suppress the redundancy by changing the dia-logue procedure and selecting contents of responses.Thus, the framework generating VoiceXMLscripts dynamically and the proposed user modelsrealize a user-adaptive dialogue strategies, in whichthe generated cooperative responses serve as goodguidance for novice users without increasing the di-alogue duration for skilled users.ReferencesJennifer Chu-Carroll.
2000.
MIMIC: An adaptivemixed initiative spoken dialogue system for informa-tion queries.
In Proc.
of the 6th Conf.
on applied Nat-ural Language Processing, pages 97?104.Wieland Eckert, Esther Levin, and Roberto Pieraccini.1997.
User modeling for spoken dialogue system eval-uation.
In Proc.
IEEE Workshop on Automatic SpeechRecognition and Understanding, pages 80?87.Stephanie Elzer, Jennifer Chu-Carroll, and Sandra Car-berry.
2000.
Recognizing and utilizing user prefer-ences in collaborative consultation dialogues.
In Proc.of the 4th Int?l Conf.
on User Modeling, pages 19?24.Timothy J. Hazen, Theresa Burianek, Joseph Polifroni,and Stephanie Seneff.
2000.
Integrating recognitionconfidence scoring with language understanding anddialogue modeling.
In Proc.
Int?l Conf.
Spoken Lan-guage Processing (ICSLP).Robert Kass and Tim Finin.
1988.
Modeling the user innatural language systems.
Computational Linguistics,14(3):5?22.Kazunori Komatani and Tatsuya Kawahara.
2000.Flexible mixed-initiative dialogue management usingconcept-level confidence measures of speech recog-nizer output.
In Proc.
Int?l Conf.
Computational Lin-guistics (COLING), pages 467?473.Lori Lamel, Sophie Rosset, Jean-Luc Gauvain, and SamirBennacef.
1999.
The LIMSI ARISE system fortrain travel information.
In IEEE Int?l Conf.
Acoust.,Speech & Signal Processing (ICASSP).Diane J. Litman and Shimei Pan.
2000.
Predicting andadapting to poor speech recognition in a spoken dia-logue system.
In Proc.
of the 17th National Confer-ence on Artificial Intelligence (AAAI2000).Eric Nyberg, Teruko Mitamura, Paul Placeway, MichaelDuggan, and Nobuo Hataoka.
2002.
Dialogxml:Extending voicexml for dynamic dialog manage-ment.
In Proc.
of Human Language Technology 2002(HLT2002), pages 286?291.Paul Over.
1999.
Trec-7 interactive track report.
In Proc.of the 7th Text REtrieval Conference (TREC7).Andrew Pargellis, Jeff Kuo, and Chin-Hui Lee.
1999.Automatic dialogue generator creates user defined ap-plications.
In Proc.
European Conf.
Speech Commun.& Tech.
(EUROSPEECH).Cecile L. Paris.
1988.
Tailoring object descriptions toa user?s level of expertise.
Computational Linguistics,14(3):64?78.J.
Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann, San Mateo, CA.http://www.rulequest.com/see5-info.html.David Sadek.
1999.
Design considerations on dia-logue systems: From theory to technology -the caseof artimis-.
In Proc.
ESCA workshop on InteractiveDialogue in Multi-Modal Systems.Peter van Beek.
1987.
A model for generating betterexplanations.
In Proc.
of the 25th Annual Meeting ofthe Association for Computational Linguistics (ACL-87), pages 215?220.
