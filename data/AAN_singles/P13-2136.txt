Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777?783,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Information Theoretic Approach to Bilingual Word ClusteringManaal Faruqui and Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mfaruqui, cdyer}@cs.cmu.eduAbstractWe present an information theoretic objec-tive for bilingual word clustering that in-corporates both monolingual distributionalevidence as well as cross-lingual evidencefrom parallel corpora to learn high qual-ity word clusters jointly in any number oflanguages.
The monolingual componentof our objective is the average mutual in-formation of clusters of adjacent words ineach language, while the bilingual com-ponent is the average mutual informationof the aligned clusters.
To evaluate ourmethod, we use the word clusters in anNER system and demonstrate a statisti-cally significant improvement in F1 scorewhen using bilingual word clusters insteadof monolingual clusters.1 IntroductionA word cluster is a group of words which ideallycaptures syntactic, semantic, and distributionalregularities among the words belonging to thegroup.
Word clustering is widely used to reducethe number of parameters in statistical modelswhich leads to improved generalization (Brown etal., 1992; Kneser and Ney, 1993; Clark, 2003; Kooet al 2008; Turian et al 2010), and multilingualclustering has been proposed as a means to im-prove modeling of translational correspondencesand to facilitate projection of linguistic resourceacross languages (Och, 1999; Ta?ckstro?m et al2012).
In this paper, we argue that generally moreinformative clusters can be learned when evidencefrom multiple languages is considered while cre-ating the clusters.We propose a novel bilingual word clusteringobjective (?2).
The first term deals with eachlanguage independently and ensures that the datais well-explained by the clustering in a sequencemodel (?2.1).
The second term ensures that thecluster alignments induced by a word alignmenthave high mutual information across languages(?2.2).
Since the objective consists of terms rep-resenting the entropy monolingual data (for eachlanguage) and parallel bilingual data, it is partic-ularly attractive for the usual situation in whichthere is much more monolingual data availablethan parallel data.
Because of its similarity to thevariation of information metric (Meila?, 2003), wecall this bilingual term in the objective the alignedvariation of information.2 Word ClusteringA word clustering C is a partition of a vocabulary?
= {x1, x2, .
.
.
, x|?|} into K disjoint subsets,C1, C2, .
.
.
, CK .
That is, C = {C1, C2, .
.
.
, CK};Ci ?
Cj = ?
for all i 6= j and ?Kk=1Ck = ?.2.1 Monolingual objectiveWe use the average surprisal in a probabilistic se-quence model to define the monolingual clusteringobjective.
Let ci denote the word class of wordwi.
Our objective assumes that the probability ofa word sequence w = ?w1, w2, .
.
.
, wM ?
isp(w) =M?i=1p(ci | ci?1)?
p(wi | ci), (2.1)where c0 is a special start symbol.
The term p(ci |ci?1) is the probability of class ci following classci?1, and p(wi | ci) is the probability of class ciemitting word wi.
Using the MLE esitmates aftertaking the negative logarithm, this term reduces to777the following as shown in (Brown et al 1992):H(C;w) = 2K?k=1#(Ck)M log#(Ck)M?
?i?j 6=i#(Ci, Cj)M log#(Ci, Cj)Mwhere #(Ck) is the count of Ck in the corpus wunder the clustering C, #(Ci, Cj) is the count ofthe number of times that cluster Ci precedes Cjand M is the size of the corpus.
Using the mono-lingual objective to cluster, we solve the followingsearch problem:C?
= arg minCH(C;w).
(2.2)2.2 Bilingual objectiveNow let us suppose we have a second lan-guage with vocabulary ?
= {y1, y2, .
.
.
, y|?|},which is clustered into K disjoint subsets D ={D1, D2, .
.
.
, DK}, and a corpus of text in thesecond language, v = ?v1, v2, .
.
.
, vN ?.
Obvi-ously we can cluster both languages using themonolingual objective above:C?, D?
= arg minC,DH(C;w) +H(D; v).This joint minimization for the clusterings for bothlanguages clearly has no benefit since the twoterms of the objective are independent.
We mustalter the object by further assuming that we havea priori beliefs that some of the words in w and vhave the same meaning.To encode this belief, we introduce the notionof a weighted vocabulary alignment A, which isa function on pairs of words in vocabularies ?
and?
to a value greater than or equal to 0, i.e., A :??
?
7?
R?0.
For concreteness, A(x, y) will bethe number of times that x is aligned to y in a wordaligned parallel corpus.
By abuse of notation, wewrite marginal weights A(x) = ?y?
?A(x, y)and A(y) = ?x?
?A(x, y).
We also define theset marginals A(C,D) = ?x?C?y?DA(x, y).Using this weighted vocabulary alignment, westate an objective that encourages clusterings tohave high average mutual information when align-ment links are followed; that is, on average howmuch information does knowing the cluster of aword x ?
?
impart about the clustering of y ?
?,and vice-versa?C DCFigure 1: Factor graphs of the monolingual (left)& proposed bilingual clustering problem (right).We call this quantity the aligned variation ofinformation (AVI).AVI(C,D;A) =EA(x,y) [?
log p(cx | dy)?
log p(dy | cx)]Writing out the expectation and gathering terms,we obtainAVI(C,D;A) = ??x???y?
?A(x, y)A(?, ?)
?
[2 log A(C,D)A(?, ?)
?
log p(C)?
log p(D)],where it is assumed that 0 log x = 0.Our bilingual clustering objective can thereforebe stated as the following search problem over alinear combination of the monolingual and bilin-gual objectives:arg minC,Dmonolingual?
??
?H(C;w) +H(D; v) +??bilingual?
??
?
?AVI(C,D) .
(2.3)Understanding AVI.
Intuitively, we can imag-ine sampling a random alignment from the distri-bution obtained by normalizing A(?, ?).
AVI givesus a measure of how much information do we ob-tain, on average, from knowing the cluster in onelanguage about the clustering of a linked elementchosen at random proportional to A(x, ?)
(or con-ditioned the other way around).
In the followingsections, we denote AVI(C,D;A) by AVI(C,D).To further understand AVI, we remark that AVI re-duces to the VI metric when the alignment mapswords to themselves in the same language.
As aproper metric, VI has a number of attractive prop-erties, and these can be generalized to AVI (with-out restriction on the alignment map), namely:?
Non-negativity: AVI(C,D) ?
0;?
Symmetry: AVI(C,D) = AVI(D,C);?
Triangle inequality:AVI(C,D) + AVI(D,E) ?
AVI(C,E);778?
Identity of indiscernables:AVI(C,D) = 0 iff C ?
D.12.3 ExampleFigure 2 provides an example illustrating the dif-ference between the bilingual vs. monolingualclustering objectives.
We compare two differentclusterings of a two-sentence Arabic-English par-allel corpus (the English half of the corpus con-tains the same sentence, twice, while the Ara-bic half has two variants with the same mean-ing).
While English has a relatively rigid SVOword order, Arabic can alternate between the tradi-tional VSO order and an more modern SVO order.Since our monolingual clustering objective reliesexclusively on the distribution of clusters beforeand after each token, flexible word order alterna-tions like this can cause unintuitive results.
Tofurther complicate matters, verbs can inflect dif-ferently depending on whether their subject pre-cedes or follows them (Haywood and Nahmad,1999), so a monolingual model, which knowsnothing about morphology and may only relyon distributional clues, has little chance of per-forming well without help.
This is indeed whatwe observe in the monolingual objective opti-mal solution (center), in which AwlAd (boys) andyElbwn (play+PRES + 3PL) are grouped into asingle class, while yElb (play+PRES + 3SG) is inits own class.
However, the AVI term (which is ofcourse not included) has a value of 1.0, reflectingthe relatively disordered clustering relative to thegiven alignment.
On the right, we see the optimalsolution that includes the AVI term in the cluster-ing objective.
This has an AVI of 0, indicating thatknowing the clustering of any word is completelyinformative about the words it is aligned to.
By in-cluding this term, a slightly worse monolingual so-lution is chosen, but the clustering corresponds tothe reasonable intuition that words with the samemeaning (i.e., the two variants of to play) shouldbe clustered together.2.4 InferenceFigure 1 shows the factor graph representationof our clustering models.
Finding the optimalclustering under both the monolingual and bilin-gual objectives is a computationally hard combi-natorial optimization problem (Och, 1995).
Weuse a greedy hill-climbing word exchange algo-rithm (Martin et al 1995) to find a minimum1C ?
D iff ?i|{D(y)|?
(x, y) ?
A, C(x) = i}| = 1value for our objective.
We terminate the opti-mization procedure when the number of wordsexchanged at the end of one complete iterationthrough both the languages is less than 0.1% ofthe sum of vocabulary of the two languages andat least five complete iterations have been com-pleted.2 For every language the word clusters areinitialised in a round robin order according to thetoken frequency.3 ExperimentsEvaluation of clustering is not a trivial problem.One branch of work seeks to recast the problemas the of part-of-speech (POS) induction and at-tempts to match linguistic intuitions.
However,hard clusters are particularly useful for down-stream tasks (Turian et al 2010).
We thereforechose to focus our evaluation on the latter prob-lem.
For our evaluation, we use our word clustersas an input to a named entity recognizer whichuses these clusters as a source of features.
Ourevaluation task is the German corpus with NERannotation that was created for the shared taskat CoNLL-2003 3.
The training set contains ap-proximately 220,000 tokens and the developmentset and test set contains 55,000 tokens each.
Weuse Stanford?s Named Entity Recognition system4which uses a linear-chain conditional random fieldto predict the most likely sequence of NE la-bels (Finkel and Manning, 2009).Corpora for Clustering: We used parallel cor-pora for {Arabic, English, French, Korean &Turkish}-German pairs from WIT-3 corpus (Cet-tolo et al 2012) 5, which is a collection of trans-lated transcriptions of TED talks.
Each languagepair contained around 1.5 million German words.The corpus was word aligned in two directionsusing an unsupervised word aligner (Dyer et al2013), then the intersected alignment points weretaken.Monolingual Clustering: For every languagepair, we train German word clusters on the mono-lingual German data from the parallel data.
Notethat the parallel corpora are of different sizes andhence the monolingual German data from everyparallel corpus is different.
We treat the F1 score2In practice, the number of exchanged words drops of exponentially,so this threshold is typically reached in not many iterations.3http://www.cnts.ua.ac.be/conll2003/ner/4http://nlp.stanford.edu/ner/index.shtml5https://wit3.fbk.eu/mt.php?release=2012-03779The boys are playingAl- AwlAd ylEbwnAl- AwlAdylEb) ?
*()'& ?$?"?
() &'(, ?$?"?
(ylEbylEbwnAl-AwlAdareplayingboysTheAl-ylEbareplayingboysTheAwlAdylEbwnH(D;v) = 4H(C;w) = 4.56H(C;w) = 4H(D;v) = 3.88H(C;w) + H(D;v)= 8.56H(C;w) + H(D;v)= 7.88AVI(C,D)= 0AVI(C,D)= 1.0Figure 2: A two-sentence English-Arabic parallel corpus (left); a 3-class clustering that maximizes themonolingual objective (?
= 0; center); and a 3-class clustering that maximizes the joint monolingualand bilingual objective (any ?
> 0.68; right).obtained using monolingual word clusters (?
= 0)as the baseline.
Table 1 shows the F1 score ofNER6 when trained on these monolingual Germanword clusters.Bilingual Clustering: While we have formu-lated a joint objective that enables using bothmonolingual and bilingual evidence, it is possibleto create word clusters using the bilingual signalonly by removing the first term in Eq.
2.3.
Ta-ble 1 shows the performance of NER when theword clusters are obtained using only the bilingualinformation for different language pairs.
As canbe seen, these clusters are helpful for all the lan-guage pairs.
For Turkish the F1 score improvesby 1.0 point over when there are no distributionalclusters which clearly shows that the word align-ment information improves the clustering quality.We now need to supplement the bilingual infor-mation with monolingual information to see if theimprovement sustains.We varied the weight of the bilingual objec-tive (?)
from 0.05 to 0.9 and observed the ef-fect in NER performance on English-German lan-guage pair.
The F1 score is maximum for ?
=0.1 and decreases monotonically when ?
is ei-ther increased or decreased.
This indicates thatbilingual information is helpful, but less valuablethan monolingual information.
Preliminary exper-iments showed that the value of ?
= 0.1 is fairlyrobust across other language pairs and hence wefix it to that for all the experiments.We run our bilingual clustering model (?
=6Faruqui and Pado?
(2010) show that for the size of our generalizationdata in German-NER, K = 100 should give us the optimum value.0.1) across all language pairs and note the F1scores.
Table 1 (unrefined) shows that except forArabic-German & French-German, all other lan-guage pairs deliver a better F1 score than only us-ing monolingual German data.
In case of Arabic-German there is a drop in score by 0.25 points.Although, we have observed improvement in F1score over the monolingual case, the gains donot reach significance according to McNemar?stest (Dietterich, 1998).Thus we propose to further refine the quality ofword alignment links as follows: Let x be a wordin language ?
and y be a word in language ?
andlet there exists an alignment link between x andy.
Recall that A(x, y) is the count of the align-ment links between x and y observed in the par-allel data, and A(x) and A(y) are the respectivemarginal counts.
Then we define an edge associ-ation weight e(x, y) = 2?A(x,y)A(x)+A(y) This quantityis an association of the strength of the relationshipbetween x and y, and we use it to remove all align-ment links whose e(x, y) is below a given thresh-old before running the bilingual clustering model.We vary e from 0.1 to 0.7 and observe the new F1scores on the development data.
Table 1 (refined)shows the results obtained by our refined model.The values shown in bold are the highest improve-ments over the monolingual model.For English and Turkish we observe a statisti-cally significant improvement over the monolin-gual model (cf.
Table 1) with p < 0.007 andp < 0.001 according to McNemar?s test.
Ara-bic improves least with just an improvement of0.02 F1 points over the monolingual baseline.
We780Dev TestLanguage Pair ?
?
= 0 ?
= 0.1 ?
= 0.1 ?
= 0 ?
= 0.1(only bi) (only mono) (unrefined) (refined) (only mono) (refined)No clusters 68.27 72.32En-De 68.95 70.04 70.33 70.64?
72.30 72.98?Fr-De 69.16 69.74 69.69 69.89 72.66 72.83Ar-De 69.01 69.65 69.40 69.67 72.90 72.37Tr-De 69.29 69.46 69.64 70.05?
72.41 72.54Ko-De 68.95 69.70 69.78 69.95 72.71 72.54Average 69.07 69.71 69.76 70.04?
72.59 72.65Table 1: NER performance using different word clustering models.
Bold indicates an improvement overthe monolingual (?
= 0) baseline; ?
indicates a significant improvement (McNemar?s test, p < 0.01).see that the optimal value of e changes from onelanguage pair to another.
For French and Englishe = 0.1 gives the best results whereas for Turk-ish and Arabic e = 0.5 and for Korean e = 0.7.Are these thresholds correlated with anything?
Wesuggest that higher values of e correspond to moreintrinsically noisy alignments.
Since alignmentmodels are parameterized based on the vocabu-laries of the languages they are aligning, largervocabularies are more prone to degenerate solu-tions resulting from overfitting.
So we are notsurprised to see that sparser alignments (resultingfrom higher values of e) are required by languageslike Korean, while languages like French and En-glish make due with denser alignments.Evaluation on Test Set: We now verify our re-sults on the test set.
We take the best bilin-gual word clustering model obtained for every lan-guage pair (e = 0.1 for En, Fr.
e = 0.5 for Ar,Tr.
e = 0.7 for Ko) and train NER classifiersusing these.
Table 1 shows the performance ofGerman NER classifiers on the test set.
All thevalues shown in bold are better than the mono-lingual baselines.
English again has a statisticallysignificant improvement over the baseline.
Frenchand Turkish show the next best improvements.The English-German cluster model performs bet-ter than the mkcls7 tool (72.83%).4 Related WorkOur monolingual clustering model is purely distri-butional in nature.
Other extensions to word clus-tering have incorporated morphological and or-thographic information (Clark, 2003).
The workof Snyder and Barzilay (2010), which focused onPOS induction is very closely related.
The ear-liest work on bilingual word clustering was pro-posed by (Och, 1999) which, like us, uses a lan-7http://www.statmt.org/moses/giza/mkcls.htmlguage modeling approach (Brown et al 1992;Kneser and Ney, 1993) for monolingual optimiza-tion and a similarity function for bilingual simi-larity.
Ta?ckstro?m et al(2012) use cross-lingualword clusters to show transfer of linguistic struc-ture.
While their clustering method is superficiallysimilar, the objective function is more heuristic innature than our information-theoretic conceptionof the problem.
Multilingual learning has beenapplied to a number of unsupervised and super-vised learning problems, including word sense dis-ambiguation (Diab, 2003; Guo and Diab, 2010),topic modeling (Mimno et al 2009; Boyd-Graberand Blei, 2009), and morphological segmenta-tion (Snyder and Barzilay, 2008).Also closely related is the technique of cross-lingual annotation projection.
This has beenapplied to bootstrapping syntactic parsers (Hwaet al 2005; Smith and Smith, 2007; Co-hen et al 2011), morphology (Fraser, 2009),tense (Schiehlen, 1998) and T/V pronoun us-age (Faruqui and Pado?, 2012).5 ConclusionsWe presented a novel information theoretic modelfor bilingual word clustering which seeks a clus-tering with high average mutual information be-tween clusters of adjacent words, and also highmutual information across observed word align-ment links.
We have shown that improvement inclustering can be obtained across a range of lan-guage pairs, evaluated in terms of their value asfeatures in an extrinsic NER task.
Our model canbe extended for clustering any number of givenlanguages together in a joint framework, and in-corporate both monolingual and parallel data.Acknowledgement: We woud like to thank W.Ammar, V. Chahuneau and W. Ling for valuablediscussions.781ReferencesJ.
Boyd-Graber and D. M. Blei.
2009.
Multilingualtopic models for unaligned text.
In Proceedings ofthe Twenty-Fifth Conference on Uncertainty in Arti-ficial Intelligence, UAI ?09, pages 75?82, Arlington,Virginia, United States.
AUAI Press.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,and J. C. Lai.
1992.
Class-based n-gram modelsof natural language.
Comput.
Linguist., 18(4):467?479, December.M.
Cettolo, C. Girardi, and M. Federico.
2012.
Wit3:Web inventory of transcribed and translated talks.
InProceedings of the 16th Conference of the EuropeanAssociation for Machine Translation (EAMT), pages261?268, Trento, Italy, May.A.
Clark.
2003.
Combining distributional and mor-phological information for part of speech induction.In Proceedings of the tenth conference on Euro-pean chapter of the Association for ComputationalLinguistics - Volume 1, EACL ?03, pages 59?66,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.S.
B. Cohen, D. Das, and N. A. Smith.
2011.
Unsuper-vised structure prediction with non-parallel multilin-gual guidance.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 50?61, Stroudsburg, PA,USA.
Association for Computational Linguistics.M.
T. Diab.
2003.
Word sense disambiguation within amultilingual framework.
Ph.D. thesis, University ofMaryland at College Park, College Park, MD, USA.AAI3115805.T.
G. Dietterich.
1998.
Approximate statistical testsfor comparing supervised classification learning al-gorithms.
Neural Computation, 10:1895?1923.C.
Dyer, V. Chahuneau, and N. A. Smith.
2013.A simple, fast, and effective reparameterization ofIBM Model 2.
In Proc.
NAACL.M.
Faruqui and S. Pado?.
2010.
Training and Evalu-ating a German Named Entity Recognizer with Se-mantic Generalization.
In Proceedings of KON-VENS 2010, Saarbru?cken, Germany.M.
Faruqui and S. Pado?.
2012.
Towards a model of for-mal and informal address in english.
In Proceedingsof the 13th Conference of the European Chapter ofthe Association for Computational Linguistics.
As-sociation for Computational Linguistics.J.
R. Finkel and C. D. Manning.
2009.
Nested namedentity recognition.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1 - Volume 1, EMNLP ?09,pages 141?150, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.A.
Fraser.
2009.
Experiments in morphosyntactic pro-cessing for translating to and from German.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 115?119, Athens, Greece,March.
Association for Computational Linguistics.W.
Guo and M. Diab.
2010.
Combining orthogonalmonolingual and multilingual sources of evidencefor all words wsd.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, ACL ?10, pages 1542?1551, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.J.
A. Haywood and H. M. Nahmad.
1999.
A newArabic grammar of the written language.
LundHumphries Publishers.R.
Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, andO.
Kolak.
2005.
Bootstrapping parsers via syntacticprojection across parallel texts.
Natural LanguageEngineering, pages 311?325.R.
Kneser and H. Ney.
1993.
Forming word classesby statistical clustering for statistical language mod-elling.
In R. Khler and B. Rieger, editors, Contri-butions to Quantitative Linguistics, pages 221?226.Springer Netherlands.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proc.
ofACL.S.
Martin, J. Liermann, and H. Ney.
1995.
Algorithmsfor bigram and trigram word clustering.
In SpeechCommunication, pages 1253?1256.M.
Meila?.
2003.
Comparing Clusterings by the Varia-tion of Information.
In Learning Theory and KernelMachines, pages 173?187.D.
Mimno, H. M. Wallach, J. Naradowsky, D. A.Smith, and A. McCallum.
2009.
Polylingual topicmodels.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 2 - Volume 2, EMNLP ?09, pages 880?889, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.F.
J. Och.
1995.
Maximum-Likelihood-Scha?tzung vonWortkategorien mit Verfahren der kombinatorischenOptimierung.
Studienarbeit, University of Erlangen.F.
J. Och.
1999.
An efficient method for determin-ing bilingual word classes.
In Proceedings of theninth conference on European chapter of the Asso-ciation for Computational Linguistics, EACL ?99,pages 71?76, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.M.
Schiehlen.
1998.
Learning tense translation frombilingual corpora.D.
A. Smith and N. A. Smith.
2007.
Probabilis-tic Models of Nonprojective Dependency Trees.In Proceedings of the 2007 Joint Conference on782Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 132?140, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.B.
Snyder and R. Barzilay.
2008.
Unsupervised mul-tilingual learning for morphological segmentation.In In The Annual Conference of the Association forComputational Linguistics.B.
Snyder and R. Barzilay.
2010.
Climbing the towerof babel: Unsupervised multilingual learning.
InJ.
Frnkranz and T. Joachims, editors, Proceedingsof the 27th International Conference on MachineLearning (ICML-10), June 21-24, 2010, Haifa, Is-rael, pages 29?36.
Omnipress.O.
Ta?ckstro?m, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer oflinguistic structure.
In The 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, volume 1, page 11.
Association for Com-putational Linguistics.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 384?394, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.783
