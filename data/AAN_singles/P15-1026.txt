Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 260?269,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsQuestion Answering over Freebase withMulti-Column Convolutional Neural NetworksLi Dong?
?Furu Wei?Ming Zhou?Ke Xu?
?SKLSDE Lab, Beihang University, Beijing, China?Microsoft Research, Beijing, Chinadonglixp@gmail.com {fuwei,mingzhou}@microsoft.comkexu@nlsde.buaa.edu.cnAbstractAnswering natural language questionsover a knowledge base is an important andchallenging task.
Most of existing sys-tems typically rely on hand-crafted fea-tures and rules to conduct question under-standing and/or answer ranking.
In this pa-per, we introduce multi-column convolu-tional neural networks (MCCNNs) to un-derstand questions from three different as-pects (namely, answer path, answer con-text, and answer type) and learn their dis-tributed representations.
Meanwhile, wejointly learn low-dimensional embeddingsof entities and relations in the knowledgebase.
Question-answer pairs are used totrain the model to rank candidate answers.We also leverage question paraphrases totrain the column networks in a multi-tasklearning manner.
We use FREEBASE asthe knowledge base and conduct exten-sive experiments on the WEBQUESTIONSdataset.
Experimental results show thatour method achieves better or comparableperformance compared with baseline sys-tems.
In addition, we develop a methodto compute the salience scores of questionwords in different column networks.
Theresults help us intuitively understand whatMCCNNs learn.1 IntroductionAutomatic question answering systems return thedirect and exact answers to natural language ques-tions.
In recent years, the development of large-scale knowledge bases, such as FREEBASE (Bol-lacker et al, 2008), provides a rich resource toanswer open-domain questions.
However, how?Contribution during internship at Microsoft Research.to understand questions and bridge the gap be-tween natural languages and structured semanticsof knowledge bases is still very challenging.Up to now, there are two mainstream methodsfor this task.
The first one is based on seman-tic parsing (Berant et al, 2013; Berant and Liang,2014) and the other relies on information extrac-tion over the structured knowledge base (Yao andVan Durme, 2014; Bordes et al, 2014a; Bordeset al, 2014b).
The semantic parsers learn to un-derstand natural language questions by convertingthem into logical forms.
Then, the parse resultsare used to generate structured queries to searchknowledge bases and obtain the answers.
Re-cent works mainly focus on using question-answerpairs, instead of annotated logical forms of ques-tions, as weak training signals (Liang et al, 2011;Krishnamurthy and Mitchell, 2012) to reduce an-notation costs.
However, some of them still as-sume a fixed and pre-defined set of lexical trig-gers which limit their domains and scalability ca-pability.
In addition, they need to manually de-sign features for semantic parsers.
The secondapproach uses information extraction techniquesfor open question answering.
These methods re-trieve a set of candidate answers from the knowl-edge base, and the extract features for the questionand these candidates to rank them.
However, themethod proposed by Yao and Van Durme (2014)relies on rules and dependency parse results to ex-tract hand-crafted features for questions.
More-over, some methods (Bordes et al, 2014a; Bordeset al, 2014b) use the summation of question wordembeddings to represent questions, which ignoresword order information and cannot process com-plicated questions.In this paper, we introduce the multi-columnconvolutional neural networks (MCCNNs) to au-tomatically analyze questions from multiple as-pects.
Specifically, the model shares the sameword embeddings to represent question words.260MCCNNs use different column networks to ex-tract answer types, relations, and context informa-tion from the input questions.
The entities andrelations in the knowledge base (namely FREE-BASE in our experiments) are also represented aslow-dimensional vectors.
Then, a score layer isemployed to rank candidate answers according tothe representations of questions and candidate an-swers.
The proposed information extraction basedmethod utilizes question-answer pairs to automat-ically learn the model without relying on manuallyannotated logical forms and hand-crafted features.We also do not use any pre-defined lexical triggersand rules.
In addition, the question paraphrasesare also used to train networks and generalize forthe unseen words in a multi-task learning manner.We have conducted extensive experiments on WE-BQUESTIONS.
Experimental results illustrate thatour method outperforms several baseline systems.The contributions of this paper are three-fold:?
We introduce multi-column convolutionalneural networks for question understandingwithout relying on hand-crafted features andrules, and use question paraphrases to trainthe column networks and word vectors in amulti-task learning manner;?
We jointly learn low-dimensional embed-dings for the entities and relations in FREE-BASE with question-answer pairs as supervi-sion signals;?
We conduct extensive experiments on theWEBQUESTIONS dataset, and provide someintuitive interpretations for MCCNNs by de-veloping a method to detect salient questionwords in the different column networks.2 Related WorkThe state-of-the-art methods for question answer-ing over a knowledge base can be classified intotwo classes, i.e., semantic parsing based and in-formation retrieval based.Semantic parsing based approaches aim atlearning semantic parsers which parse natural lan-guage questions into logical forms and then queryknowledge base to lookup answers.
The most im-portant step is mapping questions into predefinedlogical forms, such as combinatory categorialgrammar (Cai and Yates, 2013) and dependency-based compositional semantics (Liang et al,2011).
Some semantic parsing based systemsrequired manually annotated logical forms totrain the parsers (Zettlemoyer and Collins, 2005;Kwiatkowski et al, 2010).
These annotations arerelatively expensive.
So recent works (Liang etal., 2011; Kwiatkowski et al, 2013; Berant et al,2013; Berant and Liang, 2014; Bao et al, 2014;Reddy et al, 2014) mainly aimed at using weaksupervision (question-answer pairs) to effectivelytrain semantic parsers.
These methods achievedcomparable results without using logical forms an-notated by experts.
However, some methods reliedon lexical triggers or manually defined features.On the other hand, information retrieval basedsystems retrieve a set of candidate answers andthen conduct further analysis to obtain answers.Their main difference is how to select correct an-swers from the candidate set.
Yao and Van Durme(2014) used rules to extract question features fromdependency parse of questions, and used rela-tions and properties in the retrieved topic graphas knowledge base features.
Then, the productionof these two kinds of features was fed into a lo-gistic regression model to classify the question?scandidate answers into correct/wrong.
In contrast,we do not use rules, dependency parse results, orhand-crafted features for question understanding.Some other works (Bordes et al, 2014a; Bordeset al, 2014b) learned low-dimensional vectors forquestion words and knowledge base constitutes,and used the sum of vectors to represent questionsand candidate answers.
However, simple vectoraddition ignores word order information and high-order n-grams.
For example, the question repre-sentations of who killed A and who A killed aresame in the vector addition model.
We insteaduse multi-column convolutional neural networkswhich are more powerful to process complicatedquestion patterns.
Moreover, our multi-columnnetwork architecture distinguishes between infor-mation of answer type, answer path and answercontext by learning multiple column networks,while the addition model mixes them together.Another line of related work is applying deeplearning techniques for the question answeringtask.
Grefenstette et al (2014) proposed a deeparchitecture to learn a semantic parser from anno-tated logic forms of questions.
Iyyer et al (2014)introduced dependency-tree recursive neural net-works for the quiz bowl game which asked play-ers to answer an entity for a given paragraph.
Yu et261al.
(2014) proposed a bigram model based on con-volutional neural networks to select answer sen-tences from text data.
The model learned a simi-larity function between questions and answer sen-tences.
Yih et al (2014) used convolutional neu-ral networks to answer single-relation questionson REVERB (Fader et al, 2011).
However, thesystem worked on relation-entity triples instead ofmore structured knowledge bases.
For instance,the question shown in Figure 1 is answered by us-ing several triples in FREEBASE.
Also, we canutilize richer information (such as entity types) instructured knowledge bases.3 SetupGiven a natural language question q = w1.
.
.
wn,we retrieve related entities and properties fromFREEBASE and use them as the candidate answersCq.
Our goal is to score these candidates and pre-dict answers.
For instance, the correct output ofthe question when did Avatar release in UK is2009-12-17.
It should be noted that there maybe several correct answers for a question.
In or-der to train the model, we use question-answerpairs without annotated logic forms.
We furtherdescribe the datasets used in our work as follows:WebQuestions This dataset (Berant et al, 2013)contains 3,778 training instances and 2,032 testinstances.
We further split the training instancesinto the training set and the development set by80%/20%.
The questions were collected by query-ing the Google Suggest API.
A breadth-first searchbeginning with wh- was conducted.
Then, answerswere annotated in Amazon Mechanical Turk.
Allthe answers can be found in FREEBASE.Freebase It is a large-scale knowledge base thatconsists of general facts (Bollacker et al, 2008).These facts are organized as subject-property-object triples.
For example, the fact Avatar isdirected by James Cameron is represented by(/m/0bth54, film.film.directed by, /m/03 gd) inRDF format.
The preprocess method presentedin (Bordes et al, 2014a) was used to make FREE-BASE fit in memory.
Specifically, we kept thetriples where one of the entities appeared in thetraining/development set of WEBQUESTIONS orCLUEWEB extractions provided in (Lin et al,2012), and removed the entities appearing lessthan five times.
Then, we obtained 18M triplesthat contained 2.9M entities and 7k relation types.As described in (Bordes et al, 2014a), this prepro-cess method does not ease the task because WE-BQUESTIONS only contains about 2k entities.WikiAnswers Fader et al (2013) extracted thesimilar questions on WIKIANSWERS and usedthem as question paraphrases.
There are 350,000paraphrase clusters which contain about two mil-lion questions.
They are used to generalize for un-seen words and question patterns.4 MethodsThe overview of our framework is shown inFigure 1.
For instance, for the question whendid Avatar release in UK, the related nodes ofthe entity Avatar are queried from FREEBASE.These related nodes are regarded as candidate an-swers (Cq).
Then, for every candidate answer a,the model predicts a score S (q, a) to determinewhether it is a correct answer or not.We use multi-column convolutional neural net-works (MCCNNs) to learn representations ofquestions.
The models share the same word em-beddings, and have multiple columns of convolu-tional neural networks.
The number of columnsis set to three in our QA task.
These columnsare used to analyze different aspects of a ques-tion, i.e., answer path, answer context, and answertype.
The vector representations learned by thesecolumns are denoted as f1(q) , f2(q) , f3(q).
Wealso learn embeddings for the candidate answersappeared in FREEBASE.
For every candidate an-swer a, we compute its vector representations anddenote them as g1(a) ,g2(a) ,g3(a).
These threevectors correspond to the three aspects used inquestion understanding.
Using these vector rep-resentations defined for questions and answers, wecan compute the score for the question-answer pair(q, a).
Specifically, the scoring function S (q, a) isdefined as:S (q, a) =f1(q)Tg1(a)?
??
?answer path+ f2(q)Tg2(a)?
??
?answer context+ f3(q)Tg3(a)?
??
?answer type(1)where fi(q) and gi(a) have the same dimension.As shown in Figure 1, the score layer computesscores and adds them together.4.1 Candidate GenerationThe first step is to retrieve candidate answers fromFREEBASE for a question.
Questions should con-tain an identified entity that can be linked to the262when did Avatar release in UK<L> <R>Convolutional LayerMax-Pooling LayerShared WordRepresentationsAvatarm.0bth54James Cameronm.03_gdfilm.film.directed_bytype.object.typepeople.personfilm.producertype.object.typem.09w09jkfilm.film.release_date_stype.object.type film.film_regional_release_dateUnited Statesof Americam.09c7w0film.film_regional_release_date.film_release_regionfilm.film_regional_release_date.release_date2009-12-18datetimevalue_typem.0gdp17zfilm.film.release_date_s type.object.typefilm.film_regional_release_dateUnited Kingdomm.07sscfilm.film_regional_release_date.film_release_regionfilm.film_regional_release_date.release_date2009-12-17datetimevalue_typeScore LayerScore+ +Dot ProductAnswerPathAnswerContextAnswerTypeFigure 1: Overview for the question-answer pair (when did Avatar release in UK, 2009-12-17).
Left:network architecture for question understanding.
Right: embedding candidate answers.knowledge base.
We use the Freebase SearchAPI (Bollacker et al, 2008) to query named en-tities in a question.
If there is not any named en-tity, noun phrases are queried.
We use the top oneentity in the ranked list returned by the API.
Thisentity resolution method was also used in (Yao andVan Durme, 2014).
Better methods can be devel-oped, while it is not the focus of this paper.
Then,all the 2-hops nodes of the linked entity are re-garded as the candidate answers.
We denote thecandidate set for the question q as Cq.4.2 MCCNNs for Question UnderstandingMCCNNs use multiple convolutional neural net-works to learn different aspects of questions fromshared input word embeddings.
For every singlecolumn, the network structure presented in (Col-lobert et al, 2011) is used to tackle the variable-length questions.We present the model in the left part of Figure 1.Specifically, for the question q = w1.
.
.
wn, thelookup layer transforms every word into a vectorwj= Wvu(wj), where Wv?
Rdv?|V |is theword embedding matrix, u(wj) ?
{0, 1}|V |is theone-hot representation of wj, and |V | is the vocab-ulary size.
The word embeddings are parameters,and are updated in the training process.Then, the convolutional layer computes repre-sentations of the words in sliding windows.
Forthe i-th column of MCCNNs, the convolutionallayer computes n vectors for question q.
The j-th vector is:x(i)j= h(W(i)[wTj?s.
.
.wTj.
.
.wTj+s]T+ b(i))(2)where (2s + 1) is the window size, W(i)?Rdq?
(2s+1)dvis the weight matrix of convolutionallayer, b(i)?
Rdq?1is the bias vector, and h (?)
isthe nonlinearity function (such as softsign, tanh,and sigmoid).
Paddings are used for left and rightabsent words.Finally, a max-pooling layer is followed to ob-tain the fixed-size vector representations of ques-tions.
The max-pooling layer in the i-th columnof MCCNNs computes the representation of thequestion q via:fi(q) = maxj=1,...,n{x(i)j}(3)where max{?}
is an element-wise operator overvectors.4.3 Embedding Candidate AnswersVector representations g1(a) ,g2(a) ,g3(a) arelearned for the candidate answer a.
The vectorsare employed to represent different aspects of a.The embedding methods are described as follows:Answer Path The answer path is the set ofrelations between the answer node and the entityasked in question.
As shown in Figure 1, the2-hops path between the entity Avatar and thecorrect answer is (film.film.release date s,263film.film regional release date.release date).The vector representation g1(a) is computedvia g1(a) =1?up(a)?1Wpup(a), where ??
?1is 1-norm, up(a) ?
R|R|?1is a binary vectorwhich represents the presence or absence of everyrelation in the answer path, Wp?
Rdq?|R|isthe parameter matrix, and |R| is the numberof relations.
In other words, the embeddingsof relations that appear on the answer path areaveraged.Answer Context The 1-hop entities and relationsconnected to the answer path are regarded as theanswer context.
It is used to deal with constraintsin questions.
For instance, as shown in Figure 1,the release date of Avatar in UK is asked, soit is not enough that only the triples on answerpath are considered.
With the help of context in-formation, the release date in UK has a higherscore than in USA.
The context representation isg2(a) =1?uc(a)?1Wcuc(a), where Wc?
Rdq?|C|is the parameter matrix, uc(a) ?
R|C|?1is a bi-nary vector expressing the presence or absence ofcontext nodes, and |C| is the number of entitiesand relations which appear in answer context.Answer Type Type information in FREEBASE isan important clue to score candidate answers.
Asillustrated in Figure 1, the type of 2009-12-17is datetime, and the type of James Cameron ispeople.person and film.producer.
For the ex-ample question when did Avatar release in UK,the candidate answers whose types are datetimeshould be assigned with higher scores than others.The vector representation is defined as g3(a) =1?ut(a)?1Wtut(a), where Wt?
Rdq?|T |is thematrix of type embeddings, ut(a) ?
R|T |?1is abinary vector which indicates the presence or ab-sence of answer types, and |T | is the number oftypes.
In our implementation, we use the relationcommon.topic.notable types to query types.
Ifa candidate answer is a property value, we insteaduse its value type (e.g., float, string, datetime).4.4 Model TrainingFor every correct answer a ?
Aqof the questionq, we randomly sample k wrong answers a?fromthe set of candidate answers Cq, and use them asnegative instances to estimate parameters.
To bemore specific, the hinge loss is considered for pairs(q, a) and (q, a?
):l(q, a, a?)=(m?
S(q, a) + S(q, a?
))+(4)where S(?, ?)
is the scoring function defined inEquation (1), m is the margin parameter employedto regularize the gap between two scores, and(z)+= max{0, z}.
The objective function is:min?q1|Aq|?a?Aq?a?
?Rql(q, a, a?
)(5)where |Aq| is the number of correct answers, andRq?
Cq\Aqis the set of k wrong answers.The back-propagation algorithm (Rumelhart etal., 1986) is used to train the model.
It back-propagates errors from top to the other layers.Derivatives are calculated and gathered to updateparameters.
The AdaGrad algorithm (Duchi et al,2011) is then employed to solve this non-convexoptimization problem.
Moreover, the max-normregularization (Srebro and Shraibman, 2005; Sri-vastava et al, 2014) is used for the column vectorsof parameter matrices.4.5 InferenceDuring the test, we retrieve all the candidate an-swers Cqfor the question q.
For every candidatea?, we compute its score S(q, a?).
Then, the candi-date answers with the highest scores are regardedas predicted results.Because there may be more than one correctanswers for some questions, we need a criterionto determine the score threshold.
Specifically, thefollowing equation is used to determine outputs:?Aq= {a?
| a?
?
Cqandmaxa?
?Cq{S(q, a?)}
?
S(q, a?)
< m}(6)where m is the margin defined in Equation (4).The candidates whose scores are not far from thebest answer are regarded as predicted results.Some questions may have a large set of can-didate answers.
So we use a heuristic method toprune their candidate sets.
To be more specific, ifthe number of candidates on the same answer pathis greater than 200, we randomly keep 200 candi-dates for this path.
Then, we score and rank allthese generated candidate answers together.
If oneof the candidates on the pruned path is regarded asa predicted answer, we further score the other can-didates that are pruned on this path and determinethe final results.2644.6 Question Paraphrases for Multi-TaskLearningWe use the question paraphrases dataset WIKIAN-SWERS to generalize for words and question pat-terns which are unseen in the training set ofquestion-answer pairs.
The question understand-ing results of paraphrases should be same.
Con-sequently, the representations of two paraphrasescomputed by the same column of MCCNNsshould be similar.
We use dot similarity to definethe hinge loss lp(q1, q2, q3) as:lp(q1, q2, q3) =3?i=1(mp?
fi(q1)Tfi(q2) + fi(q1)Tfi(q3))+(7)where q1, q2are questions in the same paraphrasecluster P , q3is randomly sampled from anothercluster, and mpis the margin.
The objective func-tion is defined as:min?P?q1,q2?P?q3?RPlp(q1, q2, q3)(8)where RPcontains kpquestions which are ran-domly sampled from other clusters.
The same op-timization algorithm described in Section 4.4 isused to update parameters.5 ExperimentsIn order to evaluate the model, we use the datasetWEBQUESTIONS (Section 3) to conduct experi-ments.Settings The development set is used to selecthyper-parameters in the experiments.
The nonlin-earity function f = tanh is employed.
The di-mension of word vectors is set to 25.
They are ini-tialized by the pre-trained word embeddings pro-vided in (Turian et al, 2010).
The window sizeof MCCNNs is 5.
The dimension of the poolinglayers and the dimension of answer embeddingsare set to 64.
The parameters are initialized bythe techniques described in (Bengio, 2012).
Themax value used for max-norm regularization is 3.The initial learning rate used in AdaGrad is set to0.01.
A mini-batch consists of 10 question-answerpairs, and every question-answer pair has k nega-tive samples that are randomly sampled from itscandidate set.
The margin values in Equation (4)and Equation (7) is set to m = 0.5 and mp= 0.1.Method F1 P@1(Berant et al, 2013) 31.4 -(Berant and Liang, 2014) 39.9 -(Bao et al, 2014) 37.5 -(Yao and Van Durme, 2014) 33.0 -(Bordes et al, 2014a) 39.2 40.4(Bordes et al, 2014b) 29.7 31.3MCCNN (our) 40.8 45.1Table 1: Evaluation results on the test split of WE-BQUESTIONS.5.1 Experimental ResultsThe evaluation metrics macro F1 score (Berant etal., 2013) and precision @ 1 (Bordes et al, 2014a)are reported.
We use the official evaluation scriptprovided by Berant et al (2013) to compute the F1score.
Notably, the F1 score defined in (Yao andVan Durme, 2014) is slightly different from others(how to compute scores for the questions withoutpredicted results).
We instead use the original def-inition in experiments.As shown in Table 1, our method achieves bet-ter or comparable results than baseline methods onWEBQUESTIONS.
To be more specific, the firstthree rows are semantic parsing based methods,and the other baselines are information extractionbased methods.
These approaches except (Bordeset al, 2014a; Bordes et al, 2014b) rely on hand-crafted features and predefined rules.
The resultsshow that automatically question understandingcan be as good as the models using manually de-signed features.
Besides, our multi-column convo-lutional neural networks based model outperformsthe methods that use the sum of word embeddingsas question representations (Bordes et al, 2014a;Bordes et al, 2014b).5.2 Model AnalysisWe also conduct ablation experiments to comparethe results using different experiment settings.
Asshown in Table 2, the abbreviation w/o means re-moving a particular part from the model.
Wefind that answer path information is most impor-tant among these three columns, and answer typeinformation is more important than answer con-text information.
The reason is that answer pathand answer type are more direct clues for ques-tions, but answer context is used to handle addi-tional constraints in questions which are less com-mon in the dataset.
Moreover, we compare to the265Setting F1 P@1all 40.8 45.1w/o path 32.5 37.1w/o type 37.7 40.9w/o context 39.1 41.0w/o multi-column 38.4 41.8w/o paraphrase 40.0 43.91-hop 29.3 32.2Table 2: Evaluation results of different set-tings on the test split of WEBQUESTIONS.
w/opath/type/context: without using the specific col-umn.
w/o multi-column: tying parameters of mul-tiple columns.
w/o paraphrase: without usingquestion paraphrases for training.
1-hop: using 1-hop paths to generate candidate answers.model using single-column networks (w/o multi-column), i.e., tying the parameters of differentcolumns.
The results indicate that using multiplecolumns to understand questions from differentaspects improves the performance.
Besides, wefind that using question paraphrases in a multi-tasklearning manner contributes to the performance.In addition, we evaluate the results only using 1-hop paths to generate candidate answers.
Com-pared to using 2-hops paths, we find that the per-formance drops significantly.
This indicates onlyusing the nodes directly connected to the queriedentity in FREEBASE cannot handle many ques-tions.5.3 Salient Words DetectionIn order to analyze the model, we detect salientwords in questions.
The salience score of a ques-tion word depends on how much the word affectsthe computation of question representation.
Inother words, if a word plays more important rolein the model, its salience score should be larger.We compute several salience scores for a sameword to illustrate its importance in differentcolumns of networks.
For the i-th column, thesalience score of word wjin the question q = wn1is defined as:ei(wj) =???fi(wn1)?
fi(wj?11w?jwnj+1)??
?2(9)where the word wjis replaced with w?j, and ??
?2denotes Euclidean norm.
In practice, we replacewjwith several stop words (such as is, to, and a),and then compute their average score.what type of  car does weston drivewhat countries speak german as   a  first languagewho is  the current leader of cuba todaywhere is  the microsoft located Answer PathAnswer TypeAnswer ContextFigure 2: Salient words detection results for ques-tions.
From left to right, the three bars of everyword correspond to salience scores in answer pathcolumn, answer type column, and answer contextcolumn, respectively.
The salience scores are nor-malized by the max values of different columns.As shown in Figure 2, we compute saliencescores for several questions, and normalize themby the max values in different columns.
We clearlysee that these words play different roles in a ques-tion.
The overall conclusion is that the wh- words(such as what, who and where) tend to be impor-tant for question understanding.
Moreover, nounsdependent of the wh- words and verbs are impor-tant clues to obtain question representations.
Forinstance, the figure demonstrates that the nounstype/country/leader and the verbs speak/locatedare salient in the columns of networks.
Theseobservations agree with previous works (Li andRoth, 2002).
Some manually defined rules (Yaoand Van Durme, 2014) used in the question an-swering task are also based on them.5.4 ExamplesQuestion representations computed by differentcolumns of MCCNNs are used to query their mostsimilar neighbors.
We use cosine similarity in ex-periments.
This experiment demonstrates whetherthe model learns different aspects of questions.For example, if a column of networks is employedto analyze answer types, the answer types of near-est questions should be same as the query.As shown in Table 3, these three columns of ta-ble correspond to different columns of networks.To be more specific, the first column is used toprocess answer path.
We find that the modellearns different question patterns for the same266Column 1 (Answer Path) Column 2 (Answer Type) Column 3 (Answer Context)what to do in hollywood can this weekendwhat to do in midland tx this weekendwhat to do in cancun with familywhat to do at fairfield canwhat to see in downtown asheville ncwhat to see in toronto top 10where be george washington originally fromwhere be george washington carver fromwhere be george bush fromwhere be the thame river sourcewhere be the main headquarters of googlein what town do ned kelly and he family grow upwhere do charle draw go to collegewhere do kevin love go to collegewhere do pauley perrette go to collegewhere do kevin jame go to collegewhere do charle draw go to high schoolwhere do draw bree go to college wikianswerwho found collegehumorwho found the roanoke settlementwho own skywestwho start mary kaywho be the owner of kfcwho own wikimedium foundationwho be the leader of north korea todaywho be the leader of syrium nowwho be the leader of cuba 2012who be the leader of france 2012who be the current leader of cuba todaywho be the minority leader of the house of representative nowwho be judy garland fatherwho be clint eastwood datewho be emma stone fatherwho be robin robert fatherwho miley cyrus engage towho be chri cooley marry towhat type of money do japanese usewhat kind of money do japanese usewhat type of money do jamaica usewhat type of currency do brazil usewhat type of money do you use in cubawhat money do japanese usewhat be the two official language of paraguaywhat be the local language of israelwhat be the four official language of nigeriumwhat be the official language of jamaicawhat be the dominant language of jamaicawhat be the official language of brazil nowwhat be the timezone in vancouverwhat be my timezone in californiumwhat be los angeles california time zonewhat be my timezone in oklahomawhat be my timezone in louisianawhat be the time zone in franceTable 3: Using question representations obtained by different column networks to query the nearestneighbors.
From left to right, the three columns are used to analyze information about answer path,answer type, and answer context, respectively.
Lemmatization is used to better show question patterns.path.
For instance, the vector representations of?who found/own/start *?
and ?who be the ownerof *?
obtained by the first column are similar.
Thesecond column is employed to extract answer typeinformation from questions.
The answer typesof example questions in Table 3 are same, whilethey may ask different relations.
The third col-umn learns to embed question information into an-swer context.
We find that the similar questionsare clustered together by this column.5.5 Error AnalysisWe investigate the predicted results on the devel-opment set, and show several error causes as fol-lows.Candidate Generation Some entity mentions inquestions are linked incorrectly, hence we can-not obtain the desired candidate answers.
Asdescribed in (Yao and Van Durme, 2014), theFreebase Search API returned correct entities for86.4% of questions in top one results.
Becausesome questions use the abbreviation or a part ofits mention to express an entity.
For example, itis not trivial to link jfk to John F. Kennedy in thequestion ?where did jfk and his wife live?.
A bet-ter entity retrieval step should be developed for theopen question answering scenario.Time-Aware Questions We need to compare datevalues for some time-aware questions.
For in-stance, to answer the question ?who is johnnycash?s first wife?, we have to know the order ofseveral marriages by comparing the marriage date.Its correct response should contain only one en-tity (vivian liberto).
However, our system addi-tionally outputs june carter cash who is his sec-ond wife, because both the candidate answers areconnected to johnny cash by the relation peo-ple.person.spouse s. In order to solve this is-sue, we need to define some ad-hoc operators usedfor comparisons or develop more advanced se-mantic representations.Ambiguous Questions Some questions are am-biguous to obtain their correct representations.
Forexample, the question what has anna kendrickbeen in is used to ask what movies she has playedin.
This question does not have explicit clue wordsto indicate the meanings, so it is difficult to rankthe candidates.
Moreover, the question who isaidan quinn is employed to ask what his occupa-tion is.
It also lacks sufficient clues for questionunderstanding, and using who is to ask occupationis rare in the training data.6 Conclusion and Future WorkThis paper presents a method for question answer-ing over FREEBASE using multi-column convo-lutional neural networks (MCCNNs).
MCCNNsshare the same word embeddings, and use multi-ple columns of convolutional neural networks tolearn the representations of different aspects ofquestions.
Accordingly, we use low-dimensionalembeddings to represent multiple aspects of can-didate answers, i.e., answer path, answer type,and answer context.
We estimate the parame-ters from question-answer pairs, and use questionparaphrases to train the columns of MCCNNs ina multi-task learning manner.
Experimental re-sults on WEBQUESTIONS show that our approach267achieves better or comparable performance com-paring with baselines.
There are several interest-ing directions that are worth exploring in the fu-ture.
For instance, we are integrating more exter-nal knowledge source, such as CLUEWEB (Lin etal., 2012), to train MCCNNs in a multi-task learn-ing manner.
Furthermore, as our model is capableof detecting the most important words in a ques-tion, it would be interesting to use the results tomine effective question patterns.AcknowledgmentsThis research was supported by NSFC (Grant No.61421003) and the fund of the State Key Lab ofSoftware Development Environment (Grant No.SKLSDE-2015ZX-05).ReferencesJunwei Bao, Nan Duan, Ming Zhou, and Tiejun Zhao.2014.
Knowledge-based question answering as ma-chine translation.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 967?976.
Association for Computational Linguistics.Yoshua Bengio.
2012.
Practical recommendationsfor gradient-based training of deep architectures.
InNeural Networks: Tricks of the Trade, pages 437?478.Jonathan Berant and Percy Liang.
2014.
Seman-tic parsing via paraphrasing.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1415?1425.
Association for Computational Linguis-tics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on freebase fromquestion-answer pairs.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 1533?1544.
Associationfor Computational Linguistics.Kurt D. Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In International Conference onManagement of Data, pages 1247?1250.Antoine Bordes, Sumit Chopra, and Jason Weston.2014a.
Question answering with subgraph embed-dings.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 615?620.
Association for Compu-tational Linguistics.Antoine Bordes, Jason Weston, and Nicolas Usunier.2014b.
Open question answering with weakly su-pervised embedding models.
In Machine Learningand Knowledge Discovery in Databases - EuropeanConference, ECML PKDD 2014, Nancy, France,September 15-19, 2014.
Proceedings, Part I, pages165?180.Qingqing Cai and Alexander Yates.
2013.
Large-scalesemantic parsing via schema matching and lexiconextension.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 423?433.
Associa-tion for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
J. Mach.
Learn.
Res., 12:2493?2537,November.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
JMLR, 12:2121?2159,July.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1535?1545, Stroudsburg, PA,USA.
Association for Computational Linguistics.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2013.
Paraphrase-driven learning for open questionanswering.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 1608?1618.
Asso-ciation for Computational Linguistics.Edward Grefenstette, Phil Blunsom, Nando de Freitas,and Moritz Karl Hermann, 2014.
Proceedings of theACL 2014Workshop on Semantic Parsing, chapter ADeep Architecture for Semantic Parsing, pages 22?27.
Association for Computational Linguistics.Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,Richard Socher, and Hal Daum?e III.
2014.
A neuralnetwork for factoid question answering over para-graphs.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 633?644.
Association for Compu-tational Linguistics.Jayant Krishnamurthy and Tom M Mitchell.
2012.Weakly supervised training of semantic parsers.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages754?765.
Association for Computational Linguis-tics.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing proba-bilistic ccg grammars from logical form with higher-order unification.
In Proceedings of the 2010 con-ference on empirical methods in natural languageprocessing, pages 1223?1233.
Association for Com-putational Linguistics.268Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and LukeZettlemoyer.
2013.
Scaling semantic parsers withon-the-fly ontology matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1545?1556.
Associa-tion for Computational Linguistics.Xin Li and Dan Roth.
2002.
Learning question classi-fiers.
In COLING, pages 1?7.P.
Liang, M. I. Jordan, and D. Klein.
2011.
Learn-ing dependency-based compositional semantics.
InAssociation for Computational Linguistics (ACL),pages 590?599.Thomas Lin, Mausam, and Oren Etzioni.
2012.
En-tity linking at web scale.
In Proceedings of the JointWorkshop on Automatic Knowledge Base Construc-tion and Web-scale Knowledge Extraction, AKBC-WEKEX ?12, pages 84?88, Stroudsburg, PA, USA.Association for Computational Linguistics.Siva Reddy, Mirella Lapata, and Mark Steedman.2014.
Large-scale semantic parsing withoutquestion-answer pairs.
Transactions of the Associa-tion of Computational Linguistics ?
Volume 2, Issue1, pages 377?392.D.E.
Rumelhart, G.E.
Hinton, and R.J. Williams.
1986.Learning representations by back-propagating er-rors.
Nature, 323(6088):533?536.Nathan Srebro and Adi Shraibman.
2005.
Rank, trace-norm and max-norm.
In Proceedings of the 18thannual conference on Learning Theory, pages 545?560.
Springer-Verlag.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, 15:1929?1958.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In ACL.Xuchen Yao and Benjamin Van Durme.
2014.
Infor-mation extraction over structured data: Question an-swering with freebase.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages956?966.
Association for Computational Linguis-tics.Xuchen Yao, Jonathan Berant, and BenjaminVan Durme, 2014.
Proceedings of the ACL 2014Workshop on Semantic Parsing, chapter FreebaseQA: Information Extraction or Semantic Parsing?,pages 82?86.
Association for ComputationalLinguistics.Wen-tau Yih, Xiaodong He, and Christopher Meek.2014.
Semantic parsing for single-relation ques-tion answering.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 643?648.Association for Computational Linguistics.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep Learning for AnswerSentence Selection.
In NIPS Deep Learning Work-shop, December.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In In Proceedings of the 21st Conferenceon Uncertainty in AI, pages 658?666.269
