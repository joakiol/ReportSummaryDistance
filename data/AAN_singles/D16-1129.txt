Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1214?1223,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsWhat makes a convincing argument?
Empirical analysis and detectingattributes of convincingness in Web argumentationIvan Habernal?
and Iryna Gurevych??
?Ubiquitous Knowledge Processing Lab (UKP)Department of Computer Science, Technische Universita?t Darmstadt?Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Researchwww.ukp.tu-darmstadt.deAbstractThis article tackles a new challenging task incomputational argumentation.
Given a pair oftwo arguments to a certain controversial topic,we aim to directly assess qualitative propertiesof the arguments in order to explain why oneargument is more convincing than the otherone.
We approach this task in a fully empiricalmanner by annotating 26k explanations writ-ten in natural language.
These explanationsdescribe convincingness of arguments in thegiven argument pair, such as their strengths orflaws.
We create a new crowd-sourced cor-pus containing 9,111 argument pairs, multi-labeled with 17 classes, which was cleanedand curated by employing several strict qualitymeasures.
We propose two tasks on this dataset, namely (1) predicting the full label dis-tribution and (2) classifying types of flaws inless convincing arguments.
Our experimentswith feature-rich SVM learners and Bidirec-tional LSTM neural networks with convolu-tion and attention mechanism reveal that sucha novel fine-grained analysis of Web argumentconvincingness is a very challenging task.
Werelease the new corpus UKPConvArg2 and theaccompanying software under permissive li-censes to the research community.1 IntroductionPeople engage in argumentation in various contexts,both online and in the real life.
Existing definitionsof argumentation do not solely focus on giving rea-sons and laying out a logical framework of premisesand conclusions, but also highlight its social pur-pose which is to convince or to persuade (O?Keefe,2011; van Eemeren et al, 2014; Blair, 2011).
As-sessing the quality and strength of perceived argu-ments therefore plays an inherent role in argumen-tative discourse.
Despite strong theoretical foun-dations and plethora of normative theories, such asWalton?s schemes and their critical questions (Wal-ton, 1989), an ideal model of critical discussion inthe pragma-dialectic view (Van Eemeren and Groo-tendorst, 1987), or research into fallacies (Boudry etal., 2015), assessing qualitative criteria of everydayargumentation represents a challenge for argumenta-tion scholars and practitioners (Weltzer-Ward et al,2009; Swanson et al, 2015; Rosenfeld and Kraus,2015).Addressing qualitative aspects of arguments hasrecently started gaining attention in the field of com-putational argumentation.
Scoring strength of per-suasive essays (Farra et al, 2015; Persing and Ng,2015), exploring interaction in persuasive dialogueson Reddit (Tan et al, 2016), or detecting convinc-ing arguments (Habernal and Gurevych, 2016) areamong recent attempts to tackle the quality of argu-mentation.
However, these approaches are holisticand do not necessarily explain why a given argumentis strong or convincing.We asked the following research questions.
First,can we assess what makes an argument convincingin a purely empirical fashion as opposite to theo-retical normative approaches?
Second, to what ex-tent can the problem be tackled by computationalmodels?
To address these questions, we exploit ourrecently introduced UKPConvArg1 corpus (Haber-nal and Gurevych, 2016).
This data set consists of11,650 argument pairs ?
two arguments with the1214Prompt: Should physical education be mandatory inschools?
Stance: Yes!Argument 1 Argument 2PE should be compulsory be-cause it keeps us constantly fitand healthy.
If you really dis-like sports, then you can quitit when you?re an adult.
Butwhen you?re a kid, the bestthing for you to do is study,play and exercise.
If you preferto be lazy and lie on the couchall day then you are most likelyto get sick and unfit.
Besides,PE helps kids be better at team-work.physical educationshould be manda-tory cuhz 112,000people have diedin the year 2011so far and it?sbecause of thelack of physicalactivity and peo-ple are becomingobese!!!
!A1 is more convincing than A2, because:?
?A1 is more intelligently written and makessome good points (teamwork, for example).
A2used ?cuhz?
and I was done reading becausethat sounds stupid.??
?A1 gives more reasons and goes into detail, A2only has one fact??
?A1 makes several compelling points.
A2 usespoor spelling and grammar.
?Figure 1: An annotated argument pair from the UKPConvArgcorpus with three reasons explaining the decision about con-vincingness (ID arg54258 arg202285).same standpoint to the given topic, annotated witha binary relation describing which argument fromthe pair is more convincing.
Each pair also containsseveral reasons written in natural language explain-ing which properties of the arguments influence theirconvincingness.
An example of such an argumentpair is shown in Figure 1.We use these natural language reasons as a proxyto assess qualitative properties of the arguments ineach argument pair.
Our main contributions are:(1) We propose empirically inspired labels of qual-ity properties of Web arguments and design a hier-archical annotation scheme.
(2) We create a newlarge crowd-sourced benchmark data set containing9,111 argument pairs multi-labeled with 17 cate-gories which is improved by local and global fil-tering techniques.
(3) We experiment with sev-eral computational models, both traditional and neu-ral network-based, and evaluate their performancequantitatively and qualitatively.The newly created data set UKPConvArg2 isavailable under CC-BY-SA license along with theexperimental software for full reproducibility atGitHub.12 Related WorkThe growing field of computational argumentationhas been traditionally devoted to structural tasks,such as argument component detection and classifi-cation (Habernal and Gurevych, 2017; Habernal andGurevych, 2015), argument structure parsing (Peld-szus and Stede, 2015; Stab and Gurevych, 2014),or argument schema classification (Lawrence andReed, 2015), leaving the issues of argument evalu-ation or quality assessment as an open future work.There are only few attempts to tackle the quali-tative aspects of arguments, especially in the Webdiscourse.
Park and Cardie (2014) classified propo-sitions in Web arguments into four classes with re-spect to their level of verifiability.
Focusing onconvincingness of Web arguments, Habernal andGurevych (2016) annotated 16k pairs of argumentswith a binary relation ?is more convincing?
and alsoelicited explanation for the annotators?
decisions.Recently, research in persuasive essay scoringhas started combining holistic approaches based onrubrics for several dimensions typical to this genrewith explicit argument detection.
Persing and Ng(2015) manually labeled 1,000 student persuasiveessays with a single score on the 1?4 scale andtrained a regression predictor with a rich feature setusing LIBSVM.
Among traditional features (suchas POS or semantic frames), an argument structureparser by Stab and Gurevych (2014) was employed.Farra et al (2015) also deal with essay scoring butrather then tackling the argument structure, they fo-cus on methods for detecting opinion expressions.Persuasive essays however represent a genre with arather strict qualitative and formal requirements (astaught in curricula) and substantially differ from on-line argumentation.Argument evaluation belongs to the central re-search topics among argumentation scholars (Toul-1https://github.com/UKPLab/emnlp2016-empirical-convincingness1215min, 2003; Walton et al, 2008; Van Eemeren andGrootendorst, 1987).
Yet treatment of assessing ar-gumentation quality, persuasiveness, or convincing-ness is traditionally based on evaluating relevance,sufficiency or acceptability of premises (Govier,2010; Johnson and Blair, 2006) or categorizing fal-lacies (Hamblin, 1970; Tindale, 2007).
However,the nature of these normative approaches causes agap between the ?ideal?
models and empirically en-countered real-world arguments, such as those onthe Web (van Eemeren et al, 2014; Walton, 2012).Regarding the methodology utilized later in thispaper, deep (recursive) neural networks have gainedextreme popularity in NLP in recent years.
LongShort-Term Memory networks (LSTM) with At-tention mechanism have been applied on textualentailment (Rockta?schel et al, 2016), Question-Answering (Golub and He, 2016), or source-codesummarization (Allamanis et al, 2016).3 DataAs our source data set, we took the publiclyavailable UKPConvArg1 corpus.2 It is basedon arguments originated from 16 debates fromWeb debate platforms createdebate.com andconvinceme.net, each debate has two sides(usually pro and con).
Arguments from each of the32 debate sides are connected into a set of argumentpairs, and each argument pair is annotated with abinary relation (argument A is more/less convincingthan argument B), resulting in total into 11,650 argu-ment pairs.
Annotations performed by Habernal andGurevych (2016) also contain several reasons writ-ten by crowd-workers that explain why a particularargument is more or less convincing; see an examplein Figure 1.As these reasons were written in an uncontrolledsetting, they naturally reflect the main properties ofargument quality in a downstream task, which is todecide which argument from a pair is more con-vincing.
It differs from scoring arguments in iso-lation, which is inherently harder not only due tosubjectivity in argument ?strength?
decision but alsobecause of possible annotator?s prior bias (Haber-nal and Gurevych, 2016).
Assessing an argument2https://github.com/UKPLab/acl2016-convincing-argumentsin context helps to emphasize its main flaws orstrengths.
This approach is also known as knowl-edge elicitation ?
acquiring appropriate informationfrom experts by asking ?why??
(Reed and Rowe,2004).We therefore used the reasons as a proxy for de-veloping a scheme for labeling argument quality at-tributes.
This was done in a purely bottom-up em-pirical manner, as opposed to using ?standard?
eval-uation criteria known from argumentation literature(Johnson and Blair, 2006; Schiappa and Nordin,2013).
In particular, we split all reasons into severalreason units by simple preprocessing (splitting us-ing Stanford CoreNLP (Manning et al, 2014), seg-mentation into Elementary Discourse Units by RSTtools (Surdeanu et al, 2015)) and identified the ref-erenced arguments (A1 or A2) by pattern matchingand dependency parsing.
For example, each reasonfrom Figure 1 would be transformed into two reasonunits.3 Overall, we obtained about 70k reason unitsfrom the entire UKPConvArg1 corpus.3.1 Annotation schemeIn order to develop a code book for assigning a la-bel to each reason unit, we ran several pilot ex-pert annotation studies (each with 200-300 reasonunits).
Having a set of ?
25 distinct labels, weran two larger studies on Amazon Mechanical Turk(AMT), each with 500 reason units and 10 workers.The workers were split into two groups; we thenestimated gold labels for each group using MACE(Hovy et al, 2013) and compared both groups?
re-sults in order to find systematic discrepancies.
Fi-nally, we ended up with a set of 19 distinct labels(classes).
As the number of classes is too big fornon-expert crowd workers, we developed a hierar-chical annotation process guided by questions thatnarrow down the final class decision.
The scheme isdepicted in Figure 2.4 Workers were shown only thereason units without seeing the original arguments.3We picked this example for its simplicity, in reality the textsare much more fuzzy.4It might seem that some labels are missing, such as C8-2and C8-3; these belong to those removed during the pilot stud-ies.1216Figure 2: Decision tree-based annotation schema for labeling reason units using Mechanical Turk.
CX-Y represent the final labels.3.2 AnnotationWe sampled 26,000 unique reason units ordered bythe original author competence provided as part ofthe UKPConvArg corpus.
We expected that work-ers with higher competence tend to write better rea-sons for their explanations.
Using the previously in-troduced scheme, 776 AMT workers annotated thebatch during two weeks; we required assignmentsfrom 5 workers for a single item.
We employedMACE (Hovy et al, 2013) for gold label and workercompetence estimation with 95% threshold to ignorethe less confident labels.
Several workers were re-jected based on their low computed competence andother criteria, such as too short submission times.3.3 Data cleaningWe performed several cleaning procedures to in-crease quality and consistency of the annotated data(apart from initial MACE filtering already explainedabove).Local cleaning First, we removed 3,859 reasonunits annotated either with C1-2 (?not an explana-tion?)
and C8-6 (?too topic-specific?, which usuallyparaphrases some details from the related argumentand is not general enough).
In the next step, weremoved reason units with wrong polarity.
In par-ticular, all reason units labeled with C8-* or C9-*should refer to the more convincing argument inthe argument pair (as they describe positive prop-erties), whereas all reasons with labels C5-*, C6-*,and C7-* should refer to the less convincing argu-ment.
The target arguments for reason units wereknown from the heuristic preprocessing (see above);in this step 2,455 units were removed.Global cleaning Since the argument pairs fromone debate can be projected into an argument graph(Habernal and Gurevych, 2016), we utilized this?global?
context for further consistency cleaning.Suppose we have two argument pairs, P1(A ?B) and P2(B ?
C) (where?
means ?is more con-vincing than?).
Let P1(RB) be reason unit targeting1217B in argument pair P1 and similarly P2(RB) rea-son unit targeting B in argument pair P2.
In otherwords, two reason units target the same argument intwo different argument pairs (in one of them the ar-gument is more convincing while in the other pairit is less convincing).
There might then exist con-tradicting combination of classes for P1(RB) andP2(RB).
For example classes C9-2 and C7-3 arecontradicting, as the same argument cannot be both?on the topic?
and ?off-topic?
at the same time.When such a conflict between two reason unitsoccurred, we selected the reason with a higher scoreusing the following formula:wW ?
???
?A=GwA ?
?
?A 6=GwA??
(1)where wW is the competence of the original au-thor of the reason unit (originated from the UKP-ConvArg corpus), A = G are crowdsourced as-signments for a single reason unit that match thefinal predicted gold label, A 6= G are assignmentsthat differ from the final predicted gold label, wAis the competence of worker for assignment A, ?
isa penalty for non-gold labels, and ?
is the sigmoidfunction to squeeze the score between 0 and 1.We found 25 types of global contradictions be-tween labels for reason units and used them forcleaning the data; in total 3,790 reason units wereremoved in this step.
After all cleaning procedures,annotations from reason units were mapped back toargument pairs, resulting into a multi-label annota-tion of one or both arguments from the given pair.
Intotal 9,111 pairs from the UKPConvArg corpus wereannotated.For example, the final annotations of argumentpair shown in Figure 1 contain four labels ?
C8-1 (asthe more convincing argument ?has more details, in-formation, facts, or examples / more reasons / betterreasoning / goes deeper / is more specific?
), C9-3(as the more convincing argument ?has provokingquestion / makes you think?
), C5-2 (as the less con-vincing argument ?has language issues / bad gram-mar /...?
), and C6-1 (as the less convincing argument?provides not enough support / ...?
).
Only four ofsix reason units for this argument pair were anno-tated because of the competence score of their au-thors.# of labels/pair # of pairs1 4,5842 2,9593 1,1624 3305 686 8Total 9,111Table 1: Number of annotated labels per argument pairs.Figure 3: Distribution of labels in the annotated argument pairs.Consult Figure 2 for label descriptions.Table 1 shows number of labels per argumentpairs; about a half of the argument pairs have onlyone label.
Figure 3 shows distribution of label inthe entire data set which is heavily skewed towardsC8-1 label.
This is not surprising, as this label wasused for reason units pointing out that the more con-vincing argument provided more reasons, details, in-formation or better reasoning ?
a feature inherent toargumentation seen as giving reasons (Freeley andSteinberg, 2008).3.4 Data validationSince the qualitative attributes of arguments wereannotated indirectly by labeling their correspondingreason units without seeing the original arguments,we wanted to validate correctness of this approach.We designed a validation study, in which workerswere shown the original argument pair and two setsof labels.
The first set contained the true labels as an-notated previously, while we randomly replaced fewlabels in the second set.
The goal was then to decidewhich set of labels better explains that argument A is1218more convincing than argument B.
For example, forthe argument pair from Figure 1, one set of shownlabels would be {C8-1, C9-3, C5-2, C6-1} (the cor-rect set) while the other ?distracting?
set would be{C8-1, C9-3, C5-1, C7-3} .We randomly sampled 500 argument pairs andcollected 9 assignments per pair on AMT; we againused MACE with 95% threshold.
Accuracy of work-ers on 235 argument pairs achieved 82%.
We canthus conclude that workers tend to prefer explana-tions based on labels from the reason units and us-ing the annotation process presented in this sectionis reliable.
Total costs of the annotations includingpilot studies, bonuses, and data validation were USD3,300.4 ExperimentsWe propose two experiments, both performed in 16-fold cross-domain validation.
In each fold, argumentpairs from 15 debates are used and the remainingone is used for testing.
In both experiments, it is as-sumed that the more convincing argument in a pair isknown and we concatenate (using a particular delim-iter) both arguments such that the more convincingargument comes first.4.1 Predicting full multi-label distributionThis experiment is a multi-label classification.Given an argument pair annotated with several la-bels, the goal is to predict all these labels.We use two deep learning models.
Our firstmodel, Bidirectional Long Short-Term Memory(BLSTM) network contains two LSTM blocks (for-ward and backward), each with 64 hidden units onthe output.
The output is concatenated into a sin-gle vector and pushed through sigmoid layer with17 output units (corresponding to 17 labels).
We usecross entropy loss function in order to minimize dis-tance of label distributions in training and test data(Nam et al, 2014).
In the input layer, we rely onpre-trained word embeddings from Glove (Penning-ton et al, 2014) whose weights are updated duringtraining the network.The second models is BLSTM extended withan attention mechanism (Rockta?schel et al, 2016;Golub and He, 2016) combined with convolutionlayers over the input.
In particular, the input em-BLSTM BLSTM/CNN/ATTDebate H-loss one-E H-loss one-EBan plastic water bot-tles?0.092 0.283 0.090 0.305Christianity or Atheism 0.105 0.212 0.105 0.218Evolution vs.
Creation 0.093 0.196 0.094 0.234Firefox vs. Internet Ex-plorer0.080 0.312 0.078 0.345Gay marriage: right orwrong?0.095 0.243 0.094 0.270Should parents usespanking?0.082 0.312 0.083 0.344If your spouse commit-ted murder...0.094 0.297 0.094 0.272India has the potential tolead the world0.088 0.294 0.086 0.322Is it better to have a lousyfather or to be fatherless?0.086 0.367 0.085 0.381Is porn wrong?
0.098 0.278 0.100 0.270Is the school uniform agood or bad idea?0.081 0.279 0.077 0.406Pro-choice vs. Pro-life 0.095 0.218 0.098 0.218Should Physical Educa-tion be mandatory?0.095 0.273 0.095 0.277TV is better than books 0.091 0.265 0.087 0.300Personal pursuit or com-mon good?0.095 0.328 0.094 0.343W.
Farquhar ought to behonored...0.054 0.528 0.052 0.570Average 0.089 0.293 0.088 0.317Table 2: Results of multi-label classification from Experiment1.
Hamming-loss and One-Error are shown for two systems ?Bidirectional LSTM and Bidirectional LSTM with Convolutionand Attention.bedding layer is convoluted using 4 different convo-lution sizes (2, 3, 5, 7), each with 1,000 randomlyinitialized weight vectors.
Then we perform max-over-time pooling and concatenate the output into asingle vector.
This vector is used as the attentionmodule in BLSTM.We evaluate the system using two widely usedmetrics in multi-label classification.
First, Hammingloss is the average per-item per-class total error; thesmaller the better (Zhang and Zhou, 2007).
Second,we report One-error (Sokolova and Lapalme, 2009)which corresponds to the error of the predicted la-bel with highest probability; the smaller the better.We do not report other metrics (such as Area UnderPRC-curves, MAP, or cover) as they require tuninga threshold parameter, see a survey by Zhang andZhou (2014).Results from Table 2 do not show significant dif-ferences between the two models.
Putting the one-error numbers into human performance context canbe done only indirectly, as the data validation pre-1219sented in Section 3.4 had a different set-up.
Herewe can see that the error rate of the most confi-dent predicted label is about 30%, while human per-formed similarly by choosing from a two differentlabel sets in a binary settings, so their task was in-herently harder.Error analysis and discussion We examinedoutputs from the label distribution prediction forBLSTM/ATT/CNN.
It turns out that the output layerleans toward predicting the dominant label C8-1,while prediction of other labels is seldom.
We sus-pect two causes, first, the highly skewed distribu-tion of labels (see Figure 3) and, second, insufficienttraining data sizes where 13 classes have less than 1ktraining examples (while Goodfellow et al (2016)recommend at least 5k instances per class).Although multi-label classification may beviewed as a set of binary classification tasks thatdecides for each label independently (and thus al-lows for employing other ?standard?
classifiers suchas SVM), this so-called binary relevance approachignores dependencies between the labels.
That iswhy we focused directly on deep-learning methods,as they are capable of learning and predicting a fulllabel distribution (Nam et al, 2014).4.2 Predicting flaws in less convincingargumentsIn the second experiment, we focus on predict-ing flaws in arguments using coarse-grained labels.While this task makes several simplifications in thelabeling, it still provides meaningful insights into ar-gument quality assessment.
For this purpose, we useonly argument pairs where the less convincing argu-ment is labeled with a single label (no multi-labelclassification).
Second, we merged all labels fromcategories C5-* C6-* C7-* into three classes cor-responding to their parent nodes in the annotationdecision schema from Figure 2.
Table 3 shows dis-tribution of the gold data for this task with explana-tion of the labels.
It is worth noting that predictingflaws in the less convincing argument is still context-dependent and requires the entire argument pair be-cause some of the quality labels are relative to themore convincing argument (such as ?less reasoning?or ?not enough support?
).For this experiment, we modified the output layerLabel Instances DescriptionC5 856 Language and presentation issuesC6 1,203 Reasoning and factuality issuesC7 1,651 Off-topic, non-argument, nonsenseTotal 3,710Table 3: Gold data distribution for the second experiment.
Ar-gument pairs with a single label for the less convincing argu-ment.of the neural models from the previous experiment.The non-linear output function is softmax and wetrain the networks using categorical cross-entropyloss.
We also add another baseline model that em-ploys SVM with RBF kernel5 and a rich set of lin-guistically motivated features, similarly to (Haber-nal and Gurevych, 2016).
The feature set includesuni- and bi-gram presence, ratio of adjective andadverb endings that may signalize neuroticism (Cor-ney et al, 2002), contextuality measure (Heylighenand Dewaele, 2002), dependency tree depth, ratioof exclamation or quotation marks, ratio of modalverbs, counts of several named entity types, ratioof past vs. future tense verbs, POS n-grams, pres-ence of dependency tree production rules, seven dif-ferent readability measures (e.g., Ari (Senter andSmith, 1967), Coleman-Liau (Coleman and Liau,1975), Flesch (Flesch, 1948), and others), five sen-timent scores (from very negative to very positive)(Socher et al, 2013), spell-checking using standardUnix words, ratio of superlatives, and some sur-face features such as sentence lengths, longer wordscount, etc.6 It results into a sparse 60k-dimensionalfeature vector space.Results in Table 4 suggest that the SVM-RBFbaseline system performs poorly and its results areon par with a majority class baseline (not reported indetail).
Both deep learning models significantly out-perform the baseline, yielding Macro-F1 score about0.35.
The attention-based model performs betterthan simple BLSTM in two classes (C5 and C6), butthe overall Macro-F1 score is not significantly bet-ter.5We used LISBVM (Chang and Lin, 2011) with the defaulthyper-parameters.
As Ferna?ndez-Delgado et al (2014) show,SVM with gaussian kernels is a reasonable best choice on aver-age.6Detailed explanation of the features can be found directlyin the attached source codes.1220Class C5 Class C6 Class C7Model P R F1 P R F1 P R F1 M-F1 C.I.SVM-RBF 0.351 0.023 0.044 0.394 0.083 0.137 0.446 0.918 0.600 0.260 0.014BLSTM 0.265 0.600 0.368 0.376 0.229 0.285 0.479 0.301 0.370 0.341 0.015BLSTM/ATT/CNN 0.270 0.625 0.378 0.421 0.247 0.311 0.484 0.301 0.371 0.353 0.015Table 4: Results for experiment 2.
P = precision, R = recall, M-F1 = macro F1, C.I.
= confidence interval at 0.95.
Both BLSTMand BLSTM/ATT/CNN are significantly better than SVM-RBF (p < 0.05, exact Liddell?s test).Error analysis We manually examined severaldozens of predictions where the BLSTM modelfailed but the BLSTM/ATT/CNN model was correctin order to reveal some phenomena that the system iscapable to cope with.
First, the BLSTM/ATT/CNNmodel started catching some purely abusive, sar-castic, and attacking arguments.
Also, the lan-guage/grammar issues were revealed in many cases,as well as using slang in arguments.Examining predictions in which both systemsfailed reveal some fundamental limitations of thecurrent purely data-driven computational approach.While the problem of not catching off-topic argu-ments can be probably modeled by incorporatingthe debate description or some sort of debate topicmodel into the attention vector, the more commonissue of non-sense arguments or fallacious argu-ments (which seem like actual arguments on thefirst view) needs much deeper understanding of real-world knowledge, logic, and reasoning.5 ConclusionThis paper presented a novel task in the field of com-putational argumentation, namely empirical assess-ment of reasons for argument convincingness.
Wecreated a new large benchmark data set by utilizinga new annotation scheme and several filtering strate-gies for crowdsourced data.
Then we tackled twochallenging tasks, namely multi-label classificationof argument pairs in order to reveal qualitative prop-erties of the arguments, and predicting flaws in theless convincing argument from the given argumentpair.
We performed all evaluations in a cross-domainscenario and experimented with feature-rich SVMand two state-of-the-art neural network models.
Theresults are promising but show that the task is inher-ently complex as it requires deep reasoning about thepresented arguments that goes beyond capabilities ofthe current computational models.
By releasing theUKPConvArg2 data and code to the community, webelieve more progress can be made in this directionin the near future.AcknowledgmentsThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg-ProfessorshipProgram under grant No I/82806, by the German In-stitute for Educational Research (DIPF), by the Ger-man Research Foundation (DFG) via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), by the GRK 1994/1 AIPHES (DFG), by the Ar-guAna Project GU 798/20-1 (DFG), and by Ama-zon Web Services in Education Grant award.
Lastly,we would like to thank the anonymous reviewers fortheir valuable feedback.ReferencesMiltiadis Allamanis, Hao Peng, and Charles Sutton.2016.
A convolutional attention network for extremesummarization of source code.
In Maria Florina Bal-can and Kilian Q. Weinberger, editors, Proceedings ofThe 33rd International Conference on Machine Learn-ing, pages 2091?2100, New York City, NY, June.J.
Anthony Blair.
2011.
Argumentation as rational per-suasion.
Argumentation, 26(1):71?81.Maarten Boudry, Fabio Paglieri, and Massimo Pigliucci.2015.
The Fake, the Flimsy, and the Fallacious: De-marcating Arguments in Real Life.
Argumentation,29(4):431?456.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A Library for Support Vector Machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.Meri Coleman and T. L. Liau.
1975.
A computer read-ability formula designed for machine scoring.
Journalof Applied Psychology, 60:283?284.Malcolm Corney, Olivier de Vel, Alison Anderson, andGeorge Mohay.
2002.
Gender-preferential text min-ing of e-mail discourse.
In Proceedings of the 18th An-1221nual Computer Security Applications Conference (AC-SAC02), pages 282?289.Noura Farra, Swapna Somasundaran, and Jill Burstein.2015.
Scoring persuasive essays using opinions andtheir targets.
In Proceedings of the Tenth Workshop onInnovative Use of NLP for Building Educational Ap-plications, pages 64?74, Denver, Colorado, June.
As-sociation for Computational Linguistics.Manuel Ferna?ndez-Delgado, Eva Cernadas, Sene?n Barro,and Dinani Amorim.
2014.
Do we Need Hun-dreds of Classifiers to Solve Real World ClassificationProblems?
Journal of Machine Learning Research,15:3133?3181.Rudolf Flesch.
1948.
A new readability yardstick.
Jour-nal of Applied Psychology, 32:221?233.Austin J. Freeley and David L. Steinberg.
2008.
Argu-mentation and Debate.
Cengage Learning, Stamford,CT, USA, 12th edition.David Golub and Xiaodong He.
2016.
Character-Level Question Answering with Attention.
In arXivpreprint.
http://arxiv.org/abs/1604.00727.Ian Goodfellow, Yoshua Bengio, and Aaron Courville.2016.
Deep learning.
Book in preparation for MITPress.Trudy Govier.
2010.
A Practical Study of Argument.Wadsworth, Cengage Learning, 7th edition.Ivan Habernal and Iryna Gurevych.
2015.
Exploiting de-bate portals for semi-supervised argumentation miningin user-generated web discourse.
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 2127?2137, Lisbon, Por-tugal, September.
Association for Computational Lin-guistics.Ivan Habernal and Iryna Gurevych.
2016.
Which argu-ment is more convincing?
Analyzing and predictingconvincingness of Web arguments using bidirectionalLSTM.
In Proceedings of the 54th Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1589?1599, Berlin, Ger-many.
Association for Computational Linguistics.Ivan Habernal and Iryna Gurevych.
2017.
Argu-mentation Mining in User-Generated Web Discourse.Computational Linguistics, 43(1).
In press.
Preprint:http://arxiv.org/abs/1601.02403.Charles L. Hamblin.
1970.
Fallacies.
Methuen, London,UK.Francis Heylighen and Jean-Marc Dewaele.
2002.
Vari-ation in the contextuality of language: An empiricalmeasure.
Foundations of Science, 7(3):293?340.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard Hovy.
2013.
Learning Whom to Trustwith MACE.
In Proceedings of NAACL-HLT 2013,pages 1120?1130, Atlanta, Georgia.
Association forComputational Linguistics.Ralph H. Johnson and Anthony J. Blair.
2006.
LogicalSelf-Defense.
International Debate Education Associ-ation.John Lawrence and Chris Reed.
2015.
Combining ar-gument mining techniques.
In Proceedings of the 2ndWorkshop on Argumentation Mining, pages 127?136,Denver, CO, June.
Association for Computational Lin-guistics.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David McClosky.2014.
The Stanford CoreNLP natural language pro-cessing toolkit.
In Association for Computational Lin-guistics (ACL) System Demonstrations, pages 55?60.Jinseok Nam, Jungi Kim, Eneldo Loza Menc?
?a, IrynaGurevych, and Johannes Fu?rnkranz.
2014.
Large-Scale Multi-label Text Classification ?
Revisiting Neu-ral Networks.
In Toon Calders, Floriana Esposito,Eyke Hu?llermeier, and Rosa Meo, editors, Proceed-ings of the European Conference on Machine Learningand Principles and Practice of Knowledge Discoveryin Databases (ECML PKDD), volume 8725 LNCS,pages 437?452, Nancy, France.
Springer Berlin / Hei-delberg.Daniel J. O?Keefe.
2011.
Conviction, persuasion, andargumentation: Untangling the ends and means of in-fluence.
Argumentation, 26(1):19?32.Joonsuk Park and Claire Cardie.
2014.
Identifying ap-propriate support for propositions in online user com-ments.
In Proceedings of the First Workshop on Argu-mentation Mining, pages 29?38, Baltimore, Maryland,June.
Association for Computational Linguistics.Andreas Peldszus and Manfred Stede.
2015.
Joint pre-diction in mst-style discourse parsing for argumenta-tion mining.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing, pages 938?948, Lisbon, Portugal, September.
As-sociation for Computational Linguistics.Jeffrey Pennington, Richard Socher, and ChristopherManning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP), pages 1532?1543, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.Isaac Persing and Vincent Ng.
2015.
Modeling argumentstrength in student essays.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: Long Pa-pers), pages 543?552, Beijing, China.
Association forComputational Linguistics.Chris Reed and Glenn Rowe.
2004.
Araucaria: softwarefor argument analysis, diagramming and representa-tion.
International Journal on Artificial IntelligenceTools, 13(04):961?979, dec.1222Tim Rockta?schel, Edward Grefenstette, Karl MoritzHermann, Toma?s Kocisky?, and Phil Blunsom.2016.
Reasoning about entailment with neural at-tention.
In Proceedings of the 2016 InternationalConference on Learning Representations (ICLR).http://arxiv.org/abs/1509.06664.Ariel Rosenfeld and Sarit Kraus.
2015.
Providing ar-guments in discussions based on the prediction of hu-man argumentative behavior.
In Proceedings of theTwenty-Ninth AAAI Conference on Artificial Intelli-gence, pages 1320?1327.Edward Schiappa and John P. Nordin.
2013.
Argumen-tation: Keeping Faith with Reason.
Pearson UK, 1stedition.J.
R. Senter and E. A. Smith.
1967.
Automated read-ability index.
Technical report AMRL-TR-66-220,Aerospace Medical Research Laboratories, Ohio.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semanticcompositionality over a sentiment treebank.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1631?1642, Seattle, Washington, USA, October.
Associationfor Computational Linguistics.Marina Sokolova and Guy Lapalme.
2009.
A system-atic analysis of performance measures for classifica-tion tasks.
Information Processing & Management,45(4):427?437.Christian Stab and Iryna Gurevych.
2014.
Identifying ar-gumentative discourse structures in persuasive essays.In Proceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 46?56, Doha, Qatar, October.
Association forComputational Linguistics.Mihai Surdeanu, Tom Hicks, and Marco AntonioValenzuela-Escarcega.
2015.
Two practical rhetoricalstructure theory parsers.
In Proceedings of the 2015Conference of the North American Chapter of the As-sociation for Computational Linguistics: Demonstra-tions, pages 1?5, Denver, Colorado, June.
Associationfor Computational Linguistics.Reid Swanson, Brian Ecker, and Marilyn Walker.
2015.Argument Mining: Extracting Arguments from OnlineDialogue.
In Proceedings of the 16th Annual Meetingof the Special Interest Group on Discourse and Dia-logue, pages 217?226, Prague, Czech Republic.
Asso-ciation for Computational Linguistics.Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2016.
WinningArguments: Interaction Dynamics and PersuasionStrategies in Good-faith Online Discussions.
InProceedings of the 25th International Conferenceon World Wide Web, pages 613?624, Montreal, CA,Februar.
International World Wide Web ConferencesSteering Committee.Christopher W. Tindale.
2007.
Fallacies and ArgumentAppraisal.
Cambridge University Press, New York,NY, USA, critical reasoning and argumentation edi-tion.Stephen E. Toulmin.
2003.
The Uses of Argument,Updated Edition.
Cambridge University Press, NewYork.Frans H. Van Eemeren and Rob Grootendorst.
1987.
Fal-lacies in pragma-dialectical perspective.
Argumenta-tion, 1(3):283?301.Frans H. van Eemeren, Bart Garssen, Erik C. W. Krabbe,A.
Francisca Snoeck Henkemans, Bart Verheij, andJean H. M. Wagemans.
2014.
Handbook of Argumen-tation Theory.
Springer, Berlin/Heidelberg.Douglas Walton, Christopher Reed, and FabrizioMacagno.
2008.
Argumentation Schemes.
CambridgeUniversity Press.Douglas N. Walton.
1989.
Informal Logic: A Handbookfor Critical Argument.
Cambridge University Press.Douglas Walton.
2012.
Using argumentation schemesfor argument extraction: A bottom-up method.
Inter-national Journal of Cognitive Informatics and NaturalIntelligence, 6(3):33?61.Lisa Weltzer-Ward, Beate Baltes, and Laura Knight Lynn.2009.
Assessing quality of critical thought in on-line discussion.
Campus-Wide Information Systems,26(3):168?177.Min Ling Zhang and Zhi Hua Zhou.
2007.
ML-KNN: Alazy learning approach to multi-label learning.
PatternRecognition, 40(7):2038?2048.Min-Ling Zhang and Zhi-Hua Zhou.
2014.
A Review onMulti-Label Learning Algorithms.
IEEE Transactionson Knowledge and Data Engineering, 26(8):1819?1837.1223
