Exploiting Lexical Expansions and Boolean Compositions forWeb QueryingBernardo Magnini and Roberto PreveteITC-irst, Istituto per la Ricerca Scientifica e TecnologicaVia Sommarive38050 Povo (TN), Italy,{magnini I prevete} @irst.itc.ithttp://ecate.itc.it: 1024/projects/question-answering.htmlAbstractThis paper describes an experiment aiming atevaluating the role of NLP based optimizations(i.e.
morphological derivation and synonymyexpansion) in web search strategies.
Keywordsand their expansions are composed in twodifferent Boolean expressions (i.e.
expansioninsertion and Cartesian combination) and thencompared with a keyword conjunctivecomposition, considered as the baseline.Results confirm the hypothesis that linguisticoptirnizations significantly improve the searchengine performances.IntroductionThe purpose of this work was to verify if, and inwhich measure, some linguistic optimizationson the input query can improve the performanceof an existing search engine on the web 1.First of all we tried to determine a properbaseline to compare the optimized searchstrategies.
Such a baseline should reflect asmuch as possible the average use of the searchengine by typical users when querying the web.A query is usually composed of a limitednumber of keywords (i.e.
two or three), in alemmatized form, that the search enginecomposes by default in a conjunctive1 The results reported in this paper are part of a moreextended project under development at ITC-irst,which involves a collaboration with Kataweb, anItalian web portal.
We thank both Kataweb andInktomi Corporation for kindly having placed thesearch engine for the experiments atour disposal.expression.
Starting from this level (we call it"basic level") we have designed two moresophisticated search strategies that introduce anumber of linguistic optirnizations over thekeywords and adopt wo composition modalitiesallowed by the "advanced search" capabilities ofthe search engine.
One modality (i.e.
Keywordexpansion Insertion Search - KIS) first expandseach keyword of the base level withmorphological derivations and synonyms, thenit builds a Boolean expression where eachexpansion is added to the base keyword list.
Thesecond modality (i.e.
Keyword Cartesianexpansion Search KCS) adopts the sameexpansions of the previous one, but composes aBoolean expression where all the possible tuplesamong the base keywords and expansions areconsidered.The working hypothesis i  that the introductionof lexical expansions should bring animprovement in the retrieval of relevantdocuments.
To verify the hypothesis, acomparative valuation has been carried outusing the three search modalities describedabove over a set of factual questions.
The resultsof the queries have been manually scored alonga five value scale, with the aim of taking intoaccount not only the presence in the documentof the answer to the question, but also thedegree of contextual information provided bythe document i self with respect o the question.Both the presence of the answer and thecontextual information have been estimated bytwo relevance functions, one that considers thedocument position, the other that does not.The experiment results confirm that theintroduction of a limited number of lexicalexpansions (i.e.
2-3) improves the engineperformance.
In addition, the Cartesian13composition of the expansions behavessignificantly better than the; search modalitybased on keyword insertion.Some of the problems that we faced with in thiswork have been already discussed in previousworks in the literature.
The use of queryexpansions for text retrieval is a debated topic.Voorhees (1998) argues that WordNet derivedquery expansions are effective for very shortqueries, while they do not bring anyimprovements for long queries.
From a numberof experiments (Mandala et al, 1998) concludethat WordNet query expansions can increaserecall but degrade precision performances.
Threereasons are suggested to explain this behavior:(i) the lack of relations among terms of differentparts of speech in WordNet; (ii) many semanticrelations are not present in WordNet; (iii) propernames are not included in WordNet.
(Gonzalo etal., 1998) pointed out some more weaknesses ofWordNet for Information Retrieval purposes, inparticular the lack of domain information andthe fact that sense distinctions are excessivelyfine-grained for the task.
A related topic ofquery expansion is query I~anslation, which isperformed in Cross-Language InformationRetrieval (Verdejo et al 2000).This work brings additional elements in favor ofthe thesis that using linguistic expansions canimprove IR in a web search scenario.
In additionwe argue that, to be effective, query expansionhas to be combined with proper searchmodalities.
The evaluation experiment wecarried out, even within the limitations due totime and budget constraints, was designed totake into account the indications that came outat the recent TREC workshop on QuestionAnswering (Voorhees, 2000).The paper is structured as follows.
Section 1 and2 respectively present he modalities for thelinguistic expansion and for the querycomposition.
Section 3 reports the experimentalsetting for the comparative valuation of thethree search modalities.
Section 4 describes anddiscusses the results obtained, while in theconclusions we propose some directions forfuture work.1 Lexical expansionTwo kinds of lexical expansion have been usedin the experiment: morphological derivationsand synonym expansions.
Both of them try toexpand a "basic-keyword", that is a keyworddirecdy derived from a natural languagequestion.
The language used in the experimentsis Italian.1.1 Basic keywordsThe idea is that this level of keywords houldreflect as much as possible the words used by anaverage user to query a web search engine.Given a question expressed with a naturallanguage sentence, its basic keywords arederived selecting the lernmas for each contentword of the question.
Verbs are transformed intheir corresponding ominalization.
Furthermorewe decided to consider collocations andmultiwords as single keywords, as most of thecurrently available search engines allow the userto specify "phrases" in a very simple way.
In theexperiments presented in the paper multiwordexpressions are manually recognized and thenadded to the basic keyword list.Figure 1 shows a couple of questions with theirrespective basic keywords.NL-QUEST ION:  Chi ha inventato la lucee lettr ica?
(Who invented the electric light?
)BASIC-KEYWORDS : inventore (inventor)luce_e let t r ica  (electric_light)NL-QUESTION: Quale ~ il fiume pi~lungo del mondo?
(Which is the longest worldriver?
)BASIC-KEYWORDS: fiume (river) pi~_lungo(longest) mondo (world)Figure 1: Basic keywords extraction from questions.1.2 Morphological derivationMorphological derivations are consideredbecause they introduce new lemmas that wemight find in possible correct answers to thequestion, improving in this way the enginerecall.
For instance, for a question like "Chi hainventato la luce elettrica?"
("Who invented theelectric light?")
we can imagine differentcontexts for the correct answer, such as "la luceelettrica fu inventata da Edison" ("Electric light14was invented by Edison"), "L'inventore dellaluce elettrica fu Edison" ("The inventor ofelectric light was Edison"), "L'invenzione dellaluce elettrica % dovuta a Edison" ("The inventionof electric light is due to Edison"), wheredifferent morphological derivations of the samebasic keyword "inventore" ("inventor") appear.Derivations have been automatically extractedfrom an Italian monolingual dictionary (Disc,1997), and collected without considering thederivation order (i.e.
"inventare" belongs to thederivation set of "inventore" even if in the actualderivation it is the noun that derives from theverb).1.3 SynonymsKeyword expansion based on synonyms canpotentially improve the system recall, as theanswer to the question might contain synonymsof the basic keyword.
For instance, the answerto the question "Chi ha inventato la luceelettrica?"
("Who invented the electric light?
")might be one among "Lo scopntore della luteelettrica fu Edison" ("The discoverer of electriclight was Edison"), "'L'inventore dellailluminazione elettrica fu Edison" ("Theinventor of electric illumination was Edison"),"La scopritore della illuminazione lettrica fuEdison" ("The discoverer of electricillumination was Edison"), where differentsynonyms of "inventore" ( inventor") and "luceelettrica'" ("electric light") appear.
In theexperiment reported in section 3 Italiansynonyms have been manually extracted fromthe ItalianWordnet database (Roventini et al,2000), a further extension of the Italian Wordnetproduced by the EuroWordNet project (Vossen,1998).
Once the correct synset for a basickeyword is selected, its synonyms are added tothe expansion list.
In the near future we plan toautomate the process of synset selection usingword domain disambiguation, a variant of wordsense disambiguation based on subject fieldcode information added to WordNet (Magniniand Cavaglih, 2000).1.4 Expans ion chainsThe expansions described in the previoussections could be recursively applied to everylemma derived by a morphological or asynonym expansion.
For example, at the firstexpansion level we can pass from "inventore""inventor" to its synonym "scopritore""discoverer", from which in turn we canmorphologically derive the noun "discovery",and so on (cfr.
Figure 2).
This would allow theretrieval of answers uch as "La scoperta dellalampada d incandescenza ~ dovuta a Edison"("The discovery of the incandescent lamp is dueto Edison").Although in the experiment reported in thispaper we do not use recursive xpansions (i.e.we stop at the first level of the expansion chain),a long term goal of this work is to verify theireffects on the document relevance.inventore)derivationdaivaaca )(inventor)scopritore (discoverer),ideatore (artificer)invenzione (invention)I sy~ ) scoperta  (discoverer)inventare  (invenOI synyn~ ) scopr i re  (discover)Figure 2: Lexical chain for "inventore" ( inventor")2 Query compositionsWe wanted to take advantage of the "advanced"capabilities of the search engine.
In particularwe experimented the "Boolean phraase"modality, which allows the user to submitqueries with keywords composed by means oflogical operators.
However we quickly realisedthat realistic choices were restricted to disjointcompositions of short AND clauses (i.e.
with alimited number of elements, typically not morethan four).
This constrained us to twohypothesis, described in sections 2.2 and 2.3,which have been compared with a baselinecomposition strategy, described in 2.1.2.1 Keyword  "aa~lY' composi t ion search(KAS)This search strategy corresponds to the defaultmethod that most search engines implement.Given a list of basic keywords, no expansion isperformed and keywords are composed in anAND clause.
An example is reported in Figure3.15NL-QUEST ION:  Chi ha inventato la lucee lettr ica?
(Who invented the electric lightDBASIC-KEYWORDS : inventore  (inventor)l uee_e le t t r i ca  (electric_light)EXPANS IONS :COMPOSIT ION:  ( inventore ANDluce_elettr ica)Figure 3: Example of AND composition search2.2 Keyword expansion insertion search(Icls)In this composition modality a disjunctiveexpression is constructed where each disjointelement is an AND clause formed by the basekeywords plus the insertion of a singleexpansion.
In addition, to guarantee that at leastthe same documents of the KAS modality areretrieved, both an AND clause with the basickeywords and all the single basic keywords areadded as disjoint elements.
Figure 4 reports anexample.
If the AND combination of the basickeywords produces a non empty set ofdocuments, then the KIS modality should returnthe same set of documents remTanged by thepresence of the keyword expansions.
What weexpect is an improvement in the position of asignificant document, which is relevant whenhuge amounts of documents are retrieved.NL-QUEST ION:  Chi ha inventato la lucee lettr ica?
(Who inven~d the e~ctric l~ht~BASIC-KEYWORDS : inventore (mvenW~luce_e let t r ica  (e~ctric lighOEXPANSIONS:inventore~mmflm > scopritore, ideatorednivai~ > invenziones~myn~ ) scopertadniv~ > inventareI sy~rm > scoprireluce_e let t r icaI ~ ) lampada_a_ incandescenzaCOMPOSIT ION:(OR (inventoreAND luce_elettricaANDscopritore)OR (inventoreAND luce_elettricaANDideatore)OR (inventoreAND luce_elettricaANDinvenzione)OR (inventoreAND luce_elettricaANDscoperta)OR (inventore AND luce_elettricaANDinventare)OR (inventoreAND luce_elettricaANDscoprire)OR (inventore AND luce_elettricaANDlampada_a_incandescenza)OR (inventoreAND luce_elettrica)OR inventore OR luce_elettrica).
Figure 4: Example of expansion insertioncomposition2.3 Keyword Cartesian compositionsearch (KCS)In this composition modality a disjunctiveexpression is constructed where each disjointelement is an AND clause formed by one of thepossible tuple derived by the expansion set ofeach base keyword.
In addition, to guaranteethat at least the same documents of the KASmodality are retrieved, the single basickeywords are added as disjoint elements.
Figure5 reports an example.As in the previous case we expect hat at leastthe same results of the KAS search are returned,because the AND composition of the basickeywords is guaranteed.
We also expect apossible improvement of the recall, because newAND clauses are inserted.NL-QUEST ION:  Chi ha inventato  la luceelettr ica?BAS IC-KEYWORDS:  inventoreluce_e le t t r i caEXPANSIONS:inventoresynonyms > scopritore,  ideatoredn~v~m > invenz ioneI ~ ) scoperta) inventareI s~ny.~ ) scopr i reluce_e let t r ica\[ s~mnyn~ ) lampadaa_ incandescenzaCOMPOSIT ION:(OR (inventore AND luce_elettrica)OR (inventore AND lampada_a_incandescenza)OR (scopritore AND luce_elettrica)OR (scopritore AND lampada_a_incandescenza)OR (ideatore AND luce_elettrica)OR (ideatore AND lampada_a_incandescenza)OR (invenzione AND luce_elettrica)OR (invenzione AND lampada_a_incandescenza)OR (scoperta AND luce_elettrica)OR (scoperta AND lampada_a_incandescenza)16OR ( inventareAND luce_elettrica)OR (inventare AND lampada_a_incandescenza)OR (scoprire AND luce_elettrica)OR (scopr i reAND lampadaa_incandescenza)OR inventore OR luce_elettrica))Figure 5: Example of Cartesian composition search3 Comparison experimentThis section reports about the problems wefaced with comparing the three search strategiespresented in section 2.
The question set, thedocument assessment and the scoring used inthe experiment are described.3.1 Creating the Question SetInitially, a question set of 40 fact-based, short-answer questions uch as "Chi ~ l'autore dellaDivina Commedia?"
("Who is the author of TheDivine Comedy?")
was created.
Language wasItalian and each question was guaranteed to haveat least one web document hat answered thequestion.
Ambiguous questions (about 15%)were not eliminated (see Voorhees, 2000 for adiscussion).
A total of 20 questions from theinitial question set have been randomlyselected, this way preventing possible bias infavour of queries that would perform better withlexical expansions.
Figure 6 reports the finalquestion set of the experiment.Chi ha inventato la luce elettrica?
(Who invented the electric light?
)Come si chiama l'autore del libro "IMalavoglia"?
(Who is the author of the book '7Malavoglia"?
)Chi ha scoperto la legge di gravit~t?
(who discovered the gravitational law)Chi ha inventato la stampa?
(Who is the inventor of printing)Chi ha vinto il campionato di calcio nel1985 ?
(Who won the soccer championship in1985?
)Chi ~ il regista di "I Mostri"(Who is the director of "I Mostri')Quale attore ha recitato con Benigni nel film"I1 piccolo Diavolo"?
(Who played with Benigni in the film "'llpiccolo Diavolo "?
)Chi ha ucciso John Kennedy?
(Who assassinated John Kennedy?
)Chi detiene il recod italiano dei 200 metri?
(Who holds the Italian record for the 200-meters dash ?
)10 Chi ~ stato il primo uomo sulla Luna?
(Who was the first man on the moon?
)11 Chi ha inventato il Lisp?
(Who is the inventor of the Lisp) ..12 Premio nobel per la letteratura nel 1998(1998 Nobel Prize in literature)13 Quale ~ il flume pih lungo del mondo?
(Which is the longest river of the worM?
)14 In quale squadra di calcio Italiana ha gioeatoVan Basten?
(Which Italian soccer team did Van Bastenplay in ?
)15 Chi ha vinto i mondiali di Calcio nel 1986?
(Who won the Worm Cup Soccer in 1986?
)16 Chi ha progettato laReggia di Caserta?
(Who was the architect of the Caserta royalpalace?
)17 Dove ~ nato Alessandro Manzoni?
(Where was Alessandro Manzoni born?
)18 Quale ~ il lago pifi grande d'Italia?
(Which is the largest Italian lake?
)19 Chi ha fondato la Microsoft?
(who is the founder of Microsoft?
)20 Chi ~ il padre della relativitY?
(who is the father of the relativity theory?
)Figure 6: Question set used in the experiments.Each question was then associated with acorresponding human-generated set of basickeywords, resulting in an ordered list of \[nl-question, basic-keywords \] pairs.
We supposed amaximum of 3 basic keywords for eachquestion, obtaining an average of 2.25.
This isin line with (Jansen et al, 1998) where it isreported that, over a sample of 51.473 queriessubmitted to a major search service (Excite), theaverage query length was 2.35.
Basic keywordsare then expanded with their morphologicalderivations and synonyms (see Section 2), withan average of two expansions for question(rnin=0, max=6).3.2 Document assessmentAn automatic query generator has been realisedthat, given a question with its basic keywordsand lexical expansions, builds up three queries,corresponding to KAS, KIS and KCS, andsubmits them to the search engine.
Results arecollected considering up to ten documents forsearch; then the union set is used for theevaluation experiment.
There was no way forthe assessor to relate a document o the searchmodality the document was retrieved by.
Query17generation, web querying and result displayingwere all been made mntime, during theevaluation session.Fifteen researchers at ITC-irst were selected asassessors in the experiment.
They were asked tojudge the web documents returned by the querygenerator with respect to a given question,choosing avalue among the fo\]tlowing five:1) answer in context: The answercorresponding to the question is recovered andthe document context is appropriate.
Forexample, if the question is "Who is the inventorof the electric light?"
then "Edison" is reportedin the document, in some way, as the inventor ofthe electric light and the whole document dealswith inventors and/or Edison's life.2) answer_nocontext: The answer to thequestion is recovered but the document contextis not appropriate.
(e.g.
the document does notdeal neither with inventors or Edison's life).3) noanswerin_context: The answercorresponding to the question is not recoveredbut the document context is appropriate.4) noanswerno_context: The answercorresponding to the question is not recoveredand the document context is not appropriate.5) no_document: he requested ocument is notretrieved.The following instructions were provided toassessors :?
The judgement has to be based on thedocument text only, that is no further linksexploration is allowed.?
If a question is considered ambiguous thengive it just one interpretation and use thatinterpretation to judge aH question-relateddocuments consistently.
For example, if thequestion "Chi ~ il vincitore del Tour deFrance? "
("Who is the winner of the Tourde France?")
is considered ambiguousbecause the answer may change over time,then the assessor could decide that thecorrect interpretation is "Who is the winnerof the 1999 Tour de France?"
and judge allthe documents consistently.?
A document contains the answer only if it isexplicitly reported in the text.
That is, if thequestion is "Who is the author of Options?
"it is not sufficient hat the string "RobertSheckley" or "Sheckley" is in the text, butthe document has to say that RobertSheckley is the author of Options.Each question was judged independently bythree assessors.
The number of texts to bejudged for a question ranged from 10 to 18, withan average of 12.
For each question k weobtained three sets VKm.k, VKXS,k and VKCS,k of(pos, assessment) pairs corresponding to thethree search methods, where pos is the position?
of the document in the ordered list returned bythe search method, and assessment is theassessment of one participant.3.3 Assessment scor ingWe eliminated all the (pos, assessment) pairswhose assessment was equal to no_document.Said i a (pos, assessment) pair belonging toVKAS, k, Vras, k or VKcs.
k we define:0 i f  assessment is no_answer_no_context~1 if assessment is no_ answer_ in_ contextr( i) = 12 if assessment is answer no_ context\[3 if assessment is answer_ in_ contextGiven a question k and a set V~ of (pos,assessment) pairs corresponding to an orderedlist Lk of documents, toevaluate the relevance ofL~ with respect to k we have defined tworelevance functions, defined in \[1\]: f?
thatconsiders the document position, andf  that doesnot.X v(i) X v(i) / p(i)f - (k) = i~v~ f .
(k) = i~v,ram ~l / jj=lwhere- p(i) is the position of the web document in theordered list.- v(O=~(r(i)).r(O+13(r(O)a(x), 13(x) : 10,1,2,3} ~ (0,1) aretuning functions that allow to weight heassessments.- m is the maximum length of an ordered list ofweb documents.For each search method we obtained a set of 20~,  f?)
pairs by the assessing process, i.e., weobtained 20 (f, f?
)~s, kpairs, 20 (f, f?
)ms, kpairsand 20 (f, f?
)KCS, kpairs.184 Results and discussionDuring the assessing process, some requestedURLs were not retrieved.
We have a total of 546URLs and 516 retrieved web documents,meaning that about 6% of URLs were notretrieved (see Table 1).KAS KIS KCS TotalTotal URLs 146 200 200 546Retrieved 137 191 188 516URLs% Retrieved 94% \[ 95% 94% 94%URLs LTable 1: URLs returned by KAS, KIS and KCSmethods and URLs retrieved uring the assessingprocess.Table 2 shows the assessments on the KASsearch method, which we consider the baselineof the experiment, being search by keywords astandard search method on the Web.Results are presented for three partitions of thequestion set.
QS1 is the subset of questionswhose number of morphological derivations andsynonyms is higher than three; QS2 is the subsetwhose number of lexical expansions i  equal totwo or three; QS3 is the subset whose number oflexical expansions is lower than two.
The tablereports the average values of f. (i.e.
documentorder not considered) and f?
(i.e.
orderconsidered) with respect o each partition.
Theobtained values, f 0.23 and f?
0.25, indicatethat, on average, about 2 web documents havean answer  in context  assessment and 7 webdocuments have noanswer  no contextassessment out of 10 documents returned by thismethod.QS1Qs2qs3allKASMeanY- AC- pos.)
C+pos.
)0.14 0.20y.
(- pos.
)0.200.37 0.31 0.430.22 0.23 0.200.21 0.23 0.25Sdevf?
(+ pos.
)0.230.340.210.23Table 2: Mean and standard deviation of therelevance values f. (without position) and f?
(withposition) of retrieved web documents returned byKAS method.Table 3 reports the relevance values for thedocuments retrieved respectively by KIS andKCS.
For KIS we have a growth of the 19%and 13% compared with the KAS method.
ForKCS the average growth is 33 % and 22%compared with KAS.
On QS2 there is aremarkable improvement in the KCSperformances compared with KAS (+59% and+77%).
In this case the average value off+ isgreater than f .
meaning that KCS recovers goodweb documents in a better position than KAS.On QS3 there is also a good performance ofboth KIS and KCS compared with K.AS (+18%and +17% for KIS, +23% and +17% for KCS).On the contrary, on the subset QS1 both KISand KCS performances are comparable to KAS.QS1QS2QS3allKIS KCS% KAS % KASY f?
(- pos.)
(+ pos.
)+7 % -15%-3 % +19 %+18 % +17 %+19 % +13 %f.
f+(- pos.)
(+ pos.
)+7% - 15 %+59 % +77 %+23 % +17 %+33 % +22 %Table 3: KIS and KCS increasing of the averagerelevance with respect to K/kS.From the data presented here it does not emergea clear correlation between the performance of asearch method and the number of lexicalexpansions.
It can be noted that both KIS andKCS perform quite well, compared with KAS,on the set of questions having no expansions.This can be explained because KIS and KCScreate queries less restrictive than KAS and areable to recover the same documents of KAS aswell as other documents that can be meaningful.In case lexical expansions are present, the bestperformance compared with KAS is carried outby KCS method on question 1 (Figure 6), whichhave a total of four derivations and foursynonyms.
In this case K.AS recovered twodocuments and KCS more than ten documents,improving also the answer  in contextassessments hanks to both the morphologicalderivation "invenzione" (" invention") and thesynonym "lampadina elettrica"("electr ic  lamp").19It is not clear if synonyms affect searchperformance more than morphologicalderivation or vice versa.
It seems that synonymsand morphological derivations are significantexpansions in the same way.
If we consider theset of the questions characterised by animprovement i  the KCS and KIS performancecompared with K.AS performance, then there arefour questions having the number of synonymsgreater than the number of morphologicalderivations, three questions having the numberof synonyms lower than the number ofmorphological derivations and three questionshaving the number of synonyms equal to thenumber of morphological derivations (zeroincluded).If we consider the set of questions having thenumber of synonyms higher than the number ofmorphological derivations, then there are fourcases out of eight where KIS and KCS enhancethe performance of KAS.
If instead we considerthe set of questions having the number ofsynonyms lower than the number ofmorphological derivations there are three casesout of six where KIS and KCS enhance theperformance of KAS.Finally, Table 4 synthetically shows how KISand KCS perform with respect o document"context retrieval", that is the degree ofcontextual information provided by thedocument with respect o the question, nomatter if the answer to the question was presentor not in the document itself.
To focus oncontext we set the tuning functions tx(x) and~(x) to tx(O )=0, or(l)= 1, tx(2)=O, ot(3)=1/3 and~(x)=O.
The reason for considering a contextretrieval score is that, in case the answer is notpresent, context increases the probability thatother relevant documents can be foundfollowing hypertextual links, possibly includingthe correct answer to the question.Results obtained with KIS and KCS confirmthat they provide a significant increase (from31% to 41%) of context retrieval score.KISKCS% context retrieval increasing withrespect o KASf.
(- l, os.)
f?
(+ pos.
)37% +31%41% + 38 %Table 4: KIS and KCS context retrieval increasingwith respect to KAS.ConclusionA comparative experiment among three searchstrategies has been carried out with the aim ofestimating the benefits of lexical expansions and?
of composition strategies over the basickeywords of a query.
Results lead us believethat search strategies that combine a number oflinguistic optirnizations with a proper Booleancomposition can improve the performance of anexisting search engine on the web.
In particulargiven KAS (no expansions, with ANDcomposition search) as baseline, KIS (expansioninsertion search) performs better but one case(i.e.
with expansions greater than 3) and KCS(Cartesian composition search) performs betterthan KIS.
Furthermore, KCS has a maximumperformance, with expansions equal to 2 or 3,significantly higher than KIS, probably becauseKCS retrieves web documents that are notretrieved by K/S, which basically reearranges theorder of KAS documents.At present we still have no clear data todetermine which number and which kind (i.e.morphological derivations and synonyms) oflexical expansions performs better for a singlequestion, even if all the three search strategiesdefinitely perform better with questions with alimited number of expansions (i.e.
two or three).An evaluation that will take into considerationssuch variations i  planned for the near future.
Acrucial related problem for the future is that ofthe automatic evaluation of the search strategies(see Breck et al, 2000), which will enormouslyspeed up the design and evaluation cycle.The experiments reported in this paper are partof a feasibility study for the realisation of aNatural Language Based search engine on theWeb.
At the present state of development, somesteps in the query expansion (i.e.
multiwordrecognition and synset selection) have beendone manually, while both the keywordcomposition and the actual search are automaticand very efficient.
In order to completelyautomate the process, the main source ofinefficiency is likely to be keywordsdisambiguation in WordNet.
The idea is to use a20two stage disarnbiguation algorithm (Voorhees,1998), based on topic information, whichperforms linearly with respect to the number ofwords to be disambiguated.ReferencesBreck, E.J., Burger J.D., Ferro L., HirschmanL., House D., Light M., Mani I (2000) Howto Evaluate Your Question Answering SystemEvery Day ...and Still Get Real Work Done.Proceedings of LREC-2000, SecondInternational Conference on LanguageResources and Evaluation, pp.
1495-1500.Disc (1997) Dizionario Italiano SabatiniColetti, Firenze, Giunti.Gonzalo J., Verdejo F., Peters C. and CalzolariN.
(1998) Applying EuroWordnet to Cross-language Text Retrieval.
Computers and theHumanities, 32, 2-3, pp.
185-207.Gonzalo J., Verdejo F., Chugur I., Cigar J.
(1998) Indexing with WordNet synsets canimprove text retrieval.
Proceedings of theWorkshop "Usage of Wordnet in NLPsystems" Coling-ACL.Jansen B. J., Spink A., Bateman J., SaracevicT.
(1998) Real life information retrieval: Astudy of user queries on the Web.
SIGIRForum, 32(1), 5-17.Magnini B. and Cavaglih G. (2000) IntegratingSubject Field Codes into Wordnet.Proceedings of LREC-2000, SecondInternational Conference on LanguageResources and Evaluation, pp.
1413-1418.Mandala R., Takenobu T. and Hozumi T. (1998)The Use of WordNet in InformationRetrieval.
Proceedings ofColing-ACL.Roventini A., Alonge A., Bertagna F., CalzolariN., Magnini B., Martinelli R. (2000)ItalWordNet, a large semantic database forItalian.
Proceedings of LREC-2000, SecondInternational Conference on LanguageResources and Evaluation, pp.
783-790.Verdejo F., Gonzalo J., Penas A., Lopez F.,Fernandez D. (2000) Evaluating wordnets inCross-language Information Retrieval: theITEM search engine.
Proceedings of LREC-2000, Second International Conference onLanguage Resources and Evaluation, pp.1769-1774.Voorhees, Ellen M., Using WordNet for TextRetrieval, in Fellbaum C. (1998) WordNet,an Electronic Lexical Database.
MIT Press.Voorhees, Ellen M. and Tice Dawn M. (2000)Implementing a Question AnsweringEvaluation.
Proceedings o f  the Workshop"Using Evaluation within HLT Programs:Results and Trends", Athens, Greece, May30, 2000.Vossen P. (1998) EuroWordnet: a multilingualdatabase with lexical semantic networks.Kluver Academic Publishers.21
