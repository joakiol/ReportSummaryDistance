Hacking Wikipedia for Hyponymy Relation AcquisitionAsuka Sumida Kentaro TorisawaJapan Advanced Institute of Science and Technology1-1 Asahidai, Nomi-shi, Ishikawa-ken, 923-1211 JAPAN{a-sumida,torisawa}@jaist.ac.jpAbstractThis paper describes a method for extract-ing a large set of hyponymy relations fromWikipedia.
The Wikipedia is much more con-sistently structured than generic HTML doc-uments, and we can extract a large number ofhyponymy relations with simple methods.
Inthis work, we managed to extract more than1.4 ?
106 hyponymy relations with 75.3%precision from the Japanese version of theWikipedia.
To the best of our knowledge, thisis the largest machine-readable thesaurus forJapanese.
The main contribution of this paperis a method for hyponymy acquisition fromhierarchical layouts in Wikipedia.
By us-ing a machine learning technique and patternmatching, we were able to extract more than6.3 ?
105 relations from hierarchical layoutsin the Japanese Wikipedia, and their precisionwas 76.4%.
The remaining hyponymy rela-tions were acquired by existing methods forextracting relations from definition sentencesand category pages.
This means that extrac-tion from the hierarchical layouts almost dou-bled the number of relations extracted.1 IntroductionThe goal of this study has been to automatically ex-tract a large set of hyponymy relations, which play acritical role in many NLP applications, such as Q&Asystems (Fleischman et al, 2003).
In this paper, hy-ponymy relation is defined as a relation between a hy-pernym and a hyponym when ?the hyponym is a (kindof) hypernym.
?1.1This is a slightly modified definition of the one in (Milleret al, 1990).
Linguistic literature, e.g.
(A.Cruse, 1998), dis-tinguishes hyponymy relations, such as ?national university?
and?university?, and concept-instance relations, such as ?Tokyo Uni-versity?
and ?university?.
However, we regard concept-instanceCurrently, most useful sources of hyponymy re-lations are hand-crafted thesauri, such as WordNet(Fellbaum, 1998).
Such thesauri are highly reliable,but their coverage is not large and the costs of ex-tension and maintenance is prohibitively high.
To re-duce these costs, many methods have been proposedfor automatically building thesauri (Hearst, 1992; Et-zioni et al, 2005; Shinzato and Torisawa, 2004; Pan-tel and Pennacchiotti, 2006).
But often these meth-ods need a huge amount of documents and compu-tational resources to obtain a reasonable number ofhyponymy relations, and we still do not have a the-saurus with sufficient coverage.In this paper, we attempt to extract a large num-ber of hyponymy relations without a large documentcollection or great computational power.
The keyidea is to focus on Wikipedia2, which is much moreconsistently organized than normal documents.
Ac-tually, some studies have already attempted to ex-tract hyponymy relations or semantic classificationsfrom Wikipedia.
Hyponymy relations were extractedfrom definition sentences (Herbelot and Copestake,2006; Kazama and Torisawa, 2007).
Disambiguationof named entities was also attempted (Bunescu andPasca, 2006).
Category pages were used to extractsemantic relations (Suchanek et al, 2007).
Lexicalpatterns for semantic relations were learned (Ruiz-Casado et al, 2005).The difference between our work and these at-tempts is that we focus on the hierarchical layout ofnormal articles in Wikipedia.
For instance, the ar-ticle titled ?Penguin?
is shown in Fig.
1(b).
Thisarticle has a quite consistently organized hierarchi-cal structure.
The whole article is divided into thesections ?Anatomy?, ?Mating habits?, ?Systematicsand evolution?, ?Penguins in popular culture?
and soon.
The section ?Systematics and evolution?
has therelations as a part of hyponymy relations in this paper because wethink the distinction is not crucial for many NLP applications.2http://ja.wikipedia.org/wiki883'''Penguins''' are a group of[[Aquatic animal|aquatic]],[[flightless bird]]s.== Anatomy ==== Mating habits ====Systematics and evolution=====Systematics===* Aptenodytes**[[Emperor Penguin]]** [[King Penguin]]* Eudyptes== Penguins in popular culture ==== Book ==* Penguins* Penguins of the World== Notes ==* Penguinone* the [[Penguin missile]][[Category:Penguins]][[Category:Birds]]1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:(a) The source code of the ar-ticle ?Penguin?
in ?WikipediaPenguin(b) The example of the arti-cle ?Penguin?
in WikipediaPenguinsAnatomyMating_habits Systematics_and_evolutionSystematicsAptenodytesEmperor_PenguinKing_PenguinEudyptesPenguins_in_popular cultureBookPenguins Penguins_of_the_WorldNotesPenguinone the_Penguin_missile(c) The displayed page of the article ?Penguin?
in WikipediaFigure 1: The example of a Wikipedia articlesubsection ?Systematics?, which is further divided to?Aptenodytes?, ?Eudyptes?
and so on.
Some of suchsection-subsection relations can be regarded as validhyponymy relations.
In the article about ?Penguin?,relations such as the one between ?Aptenodytes?
and?Emperor Penguin?
and the one between ?Book?
and?Penguins of the World?
are valid hyponymy rela-tions.
The main objective of this work is to develop amethod to extract only such hyponymy relations.The rest of the paper is organized as follows.
Wefirst explain the structure of Wikipedia in Section 2.Next, we introduce our method in Section 3.
Somealternative methods are presented in Section 4.
Wethen show the experimental results in Section 5.2 The Structure of WikipediaThe Wikipedia is built on the MediaWiki softwarepackage3.
MediaWiki interprets the source codewritten in the MediaWiki syntax to produce human-readable web pages.
For example, Fig.
1(b) is a resultof interpreting the source code in Fig.
1(a).
An impor-tant point is that the MediaWiki syntax is stricter thanthe HTML syntax and usage of the syntax in mostWikipedia articles are constrained by editorial policy.This makes it easier to extract information from theWikipedia than from generic HTML documents.3http://www.mediawiki.org/wiki/MediaWikiUsually, a Wikipedia article starts with a definitionsentence, such as ?Penguins are a group of aquatic,flightless birds?
in Fig.
1(a).
Then, the hierarchicalstructure marked in the following manner follows.Headings Headings describe the subject of a para-graph.
See line 2-5, 10-11, 14 of Fig.
1(a).Headings are marked up as ?=+title=+?
in theMediaWiki syntax, where title is a subject ofthe paragraph.
Note that ?+?
here means a fi-nite number of repetition of symbols.
?=+sec-tion=+?
means that ?=section=?, ?==section==?and ?===section===?
are legitimate mark up inthe Wikipedia syntax.
We use this ?+?
notationin the following explanation as well.Bulleted lists Bulleted lists are lists of unordereditems.
See line 6-9, 12-13, 15-16 of Fig.
1.
Bul-leted lists are marked as ?*+title?
in the Medi-aWiki syntax, where title is a subject of a listeditem.Ordered lists Ordered lists are lists of numbereditems.
Ordered lists are marked up as ?#+title?in MediaWiki syntax, where title is a subject ofa numbered item.Definition lists Definition lists contain terms and itsdefinitions.
Our method focuses only on theterms.
Definition lists are marked as ?
;title?where title is a term.The basic hierarchical structure of aWikipedia arti-cle is organized by a pre-determined ordering amongthe above items.
For instance, a bulleted list itemis assumed to occupy a lower position in the hierar-chy than a heading item.
In general, items occupya higher position in the order of headings, definitionlists, bulleted lists, and ordered lists.
In addition, re-call that headings, bullet list and ordered list allowedthe repetitions of symbols ?=?, ?*?
and ?#?.
Thenumber of repetition indicates the position in the hi-erarchy and the more repetition the item contains, thelower the position occupied by the item becomes.
Forinstance, ?==Systematics and evolution==?
occupiesa higher position than ?===Systematics===?
as illus-trated in Fig.
1(a) (b).Then, it is easy to extract a hierarchical structurebased on the order among the mark-up items by pars-ing the source code of an article.
Fig.
1(c) illustratesthe hierarchical structure extracted from the sourcecode in Fig.
1(a).8843 Proposed MethodThis section describes our method for extractinghyponymy relations from hierarchical structures inWikipedia articles.
The method consists of threesteps:Step 1 Extract hyponymy relation candidates fromhierarchical structures in the Wikipedia.Step 2 Select proper hyponymy relations by apply-ing simple patterns to the extracted candidates.Step 3 Select proper hyponymy relations from thecandidates by using a machine learning tech-nique.Each step is described below.3.1 Step 1: Extracting Relation CandidatesThe Step 1 procedure extracts the title of a marked-upitem and a title of its (direct) subordinate marked-upitem as a hyponymy relation for each marked-up item.For example, given the hierarchy in Fig.
1(c), theStep1 procedure extracted hyponymy relation can-didates such as ?Aptenodytes/Emperor Penguin?and?Book/Penguins of the World?.
(Note that we de-note hyponymy relations or their candidates as ?hy-pernym/hyponym?
throughout this paper.)
However,these relation candidates include many wrong hy-ponymy relations such as ?Penguins in popular cul-ture/Book?.
Steps 2 and 3 select proper relations fromthe output of the Step 1 procedure.3.2 Step 2: Selecting Hyponymy Relations bySimple PatternsStep 2 selects plausible hyponymy relations by ap-plying simple patterns to hyponymy relation can-didates obtained in Step 1.
This is based on ourobservation that if a hypernym candidate matchesa particular pattern, it is likely to constitute a cor-rect relation.
For example, in Japanese, if a hy-pernym candidate is ?
omona X (Popular or typ-ical X)?, X is likely to be a correct hypernym ofthe hyponym candidates that followed it in the arti-cle.
Fig.2 shows a Japanese Wikipedia article abouta zoo that includes ?omona doubutsu (Popularanimals)?, ?
Mazeran Pengin (Magellanic Pen-guin)?, ?Raion (Lion)?
and so on.
From this ar-ticle, the Step 1 procedure extracts a hyponymy re-lation candidate ?Popular Animals/Magellanic Pen-guin?, and the Step 2 procedure extracts ?Ani-mals/Magellanic Penguin?
after matching ?Popular?Magellanic PenguinLionHokkaido Brown BearPopular animalsFigure 2: Example for Step2Xno ichiran(list of X), Xichiran(list ofX), Xsyousai(details of X), Xrisuto(X list),daihyoutekinaX(typical X), daihyouX(typical X),syuyounaX(popular or typical X), omonaX(popularor typical X), syuyouX(popular or typical X),kihontekinaX(basic X), kihon(basic X),chomeinaX(notable X), ookinaX(large X),omonaX(popular or typical X), ta noX(other X),ichibuX(partial list of X)Figure 3: Patterns for Step 2to the hypernym candidate and removing the string?Popular?
from the candidate.
Fig.
3 lists all the pat-terns we used.
Note that the non-variable part of thepatterns is removed from the matched hypernym can-didates.3.3 Step 3: Selecting Proper HyponymyRelations by Machine LearningThe Step 3 procedure selects proper hyponymy rela-tions from the relation candidates that do not matchthe patterns in Step 2.
We use Support Vector Ma-chines (SVM) (Vapnik, 1998) for this task.
For eachhyponymy relation candidate, we firstly apply mor-phological analysis and obtain the following types offeatures for each hypernym candidate and hyponymcandidate, and append them into a single feature vec-tor, which is given to the classifier.POS We found that POS tags are useful clues forjudging the validity of relations.
For instance, if ahypernym includes proper nouns (and particularly to-ponyms), it is unlikely to constitute a proper relation.We assigned each POS tag a unique dimension in thefeature space and if a hypernym/hyponym consists ofa morpheme with a particular POS tag, then the cor-responding element of the feature vector was set toone.
When hypernyms/hyponyms are multiple mor-pheme expressions, the feature vectors for every mor-pheme were simply summed.
(The obtained featurevector works as disjunction of each feature vector.
)An important point is that, since the last morpheme ofhypernyms/hyponyms works as strong evidence forthe validity of relations, the POS tag of the last mor-pheme was mapped to the dimension that is differentfrom the POS tags of the other morphemes.885MORPH Morphemes themselves are also mappedto a dimension of the feature vectors.
The lastmorphemes are also mapped to dimensions thatare different from those of the other morphemes.This feature is used for recognizing particular mor-phemes that strongly suggest the validity of hy-ponymy relations.
For instance, if the morpheme?zoku (genus)?
comes in the end of the hyper-nym, the relation is likely to be valid, as exem-plified by the relation ?koutei pengin zoku(Aptenodytes genus)/koutei pengin (EmperorPenguin)?.EXP Expressions of hypernym/hyponym candi-dates themselves also give a good clue for judgingthe validity of the relation.
For instance, there aretypical strings that can be the title of a marked-upitem but cannot be a proper hypernym or a properhyponym.
Examples of these strings include ?Back-ground?
and ?Note?.
By mapping each expression toan element in a feature vector and setting the elementto one, we can prevent the candidates containing suchexpressions from being selected by the classifier.ATTR We used this type of features according toour observation that if a relation candidate includes anattribute, it is a wrong relation.
The attributes of anobject can be defined as ?what we want to know aboutthe object?.
For instance, we regard ?Anatomy?
as at-tributes of creatures in general, and the relation suchas ?Penguin/Anatomy?
cannot be regarded as properhyponymy relations.
To set up this type of features,we automatically created a set of attributes and thefeature was set to one if the hypernym/hyponym isincluded in the set.
The attribute set was created inthe following manner.
We collected all the titles ofthe marked-up items from all the articles, and countedthe occurrences of each title.
If a title appears morethan one time, then it was added to the attribute set.Note that this method relies on the hypothesis thatthe same attribute is used in articles about more thanone object (e.g., ?Penguin?
and ?Sparrow? )
belong-ing to the same class (e.g., ?animal?).
(Actually, inthis counting of titles, we excluded the titles of itemsin the bulleted lists and the ordered lists in the bottomlayer of the hierarchical structures.
This is becausethese items are likely to constitute valid hyponymyrelations.
We also excluded that match the patternsin Fig.
3.)
As a result, we obtained the set of 40,733attributes and the precision of a set was 73% accord-ing to the characterization of attributes in (Tokunagaet al, 2005).LAYER We found that if a hyponymy relation isextracted from the bottom of the hierarchy, it tendsto be a correct relation.
For example, in Fig.
1(c),the hyponymy relation ?Penguin/Anatomy?
which isextracted from the top of hierarchy is wrong, but thehyponymy relation ?Aptenodytes/Emperor Penguin ?which is extracted from the bottom of the layer is cor-rect.
To capture this tendency, we added the mark thatmarks up a hypernym and a hyponym to the features.Each mark is mapped to a dimension in the featurevector, and the corresponding element was set to oneif a hypernym/hyponym candidate appears with themark.As the final output of our method, we merged theresults of Steps 2 and 3.4 Alternative MethodsThis section describes existing methods for acquiringhyponymy relations from theWikipedia.
We comparethe results of these methods with the output of ourmethod in the next section.4.1 Extraction from Definition SentencesDefinition sentences in the Wikipedia article wereused for acquiring hyponymy relations by (Kazamaand Torisawa, 2007) for named entity recognition.Their method is developed for the English version ofthe Wikipedia and required some modifications to theJapanese version.
These modification was inspired byTsurumaru?s method (Tsurumaru et al, 1986).Basically, definition sentences have forms similarto ?hyponym word wa hypernym word no isshu dearu(hyponym is a kind of hypernym)?
in dictionariesin general, and contain hyponymy relations in them.In the Wikipedia, such sentences usually come justafter the titles of articles, so it is quite easy to recog-nize them.
To extract hyponymy relations from def-inition sentences, we manually prepared 1,334 pat-terns, which are exemplified in Table 4, and appliedthem to the first sentence.4.2 Extraction from Category PagesSuchanek et al (Suchanek et al, 2007) extractedhyponymy relations from the category pages in theWikipedia using WordNet information.
Although wecannot use WordNet because there is no Japaneseversion of WordNet, we can apply their idea to theWikipedia only.The basic idea is to regard the pairs of the categoryname provided in the top of a category page and the886hyponym wa.
*hypernym no hitotsu.
(hyponym is one of hypernym)hyponym wa .
*hypernym no daihyoutekina mono dearu.
(hyponym is a typical hypernym)hyponym wa.
*hypernym no uchi no hitotsu.
(hyponym is one of hypernym)Note that hyponym and hypernym match only withNPs.Figure 4: Examples of patterns for definition sen-tencesitems listed in the page as hyponymy relation.Thus, the method is quite simple.
But the relationsextracted by this are not limited to hyponymy rela-tions, unfortunately.
For instance, the category page?football?
includes ?football team?.
Such loosely as-sociated relations are harmful for obtaining preciserelations.
Suchanek used WordNet to prevent such re-lations from being included in the output.
However,we could not develop such a method because of thelack of a Japanese WordNet.5 ExperimentsFor evaluating our method, we used the Japaneseversion of Wikipedia from March 2007, which in-cludes 820,074 pages4.
Then, we removed ?userpages?,?special pages?, ?template pages?, ?redirec-tion pages?, and ?category pages?
from it.In Step 3, we used TinySVM5 with polynomial ker-nel of degree 2 as a classifier.
From the relation can-didates given to the Step 3 procedure, we randomlypicked up 2,000 relations as a training set, and 1,000relations as a development set.
We also used the mor-phological analyzer MeCab 6 in Step 3.Table 1 summarizes the performance of ourmethod.
Each row of the table shows A) the pre-cision of the hyponymy relations, B) the number ofthe relations, and C) the expected number of correctrelations estimated from the precision and the num-ber of the extracted relations, after each step of theprocedure.
Note that Step 2?
indicates the hyponymyrelation candidates that did not match the pattern inFig.3 and that were given to the Step 3 procedure.The difference between Step 2?
and Step 3 indicatesthe effect of our classifier.
Step 2&3 is the final resultobtained by merging the results of Step 2 and Step 3.As the final output, we obtained more than 6.3 ?
1054This pages include ?incomplete pages?
that are not countedin the number of pages presented in the top page of theWikipedia.5http://chasen.org/ taku/software/TinySVM/index.html6http://mecab.sourceforge.netTable 1: Performance of each stepPrecision # of rels.
estimated # ofcorrect rels.Step 1 44% 2,768,856 1,218,296Step 2 71.5% 221,605 158,447Step 2?
40.0% 2,557,872 1,023,148Step 3 78.1% 416,858 325,670Step 2 & 3 76.4% 633,122 484,117aatisuto / erubisu puresuriiArtist / Elvis Presleysakura / someiyoshinoCherry Blossom / Yoshino Cherryheiya / nakagawa heiyaPlain / Nakagawa Plainikou oyobi kenzoubutsu / tsuki no piramiddoRuins and buildings / the Pyramid of the Moonsuponsaa / genzai?Sponsors / Present?shutsuen sakuhin / taidan go?Art work / After leaving a group??*?
indicates an incorrectly recognized relation.Figure 5: Examples of acquired hyponymy relationsrelations and their precision was 76.4%.
Note thatthe precision was measured by checking 200 randomsamples for each step except for Step 3 and Step 2&3,for which the precision was obtained in a way de-scribed later.
Note that all the numbers were obtainedafter removing duplicates in the relations.
Exampleof the relations recognized by Step 2 or Step 3 areshown in Fig.
5.Table 2 shows the effect of each type of features inStep 3.
Each row indicates the precision, recall andF-measure against 400 samples that are randomly se-lected from the relation candidates given to Step 3,when we removed a type of features from feature vec-tor and when we used all the types.
(The 400 sam-ples included 142 valid relations.)
We can see that alltypes except for LAYER contributed to an improve-ment of the F-measure.
When the LAYER featureswere removed, the F-measure was improved to 1.1but the precision was on an unacceptable level (55%)and cannot be used in actual acquisition.Table 3 summarizes the statistics of all the methodsfor acquisition from Wikipedia.
It shows A) the pre-Table 2: Effect of each features in Step3Feature Type a Precision Recall F-measure-POS 60.0% 57.0% 58.4-MORPH 85.0% 47.8% 61.2-EXP 82.2% 35.9% 50.0-ATTR 79.7% 47.1% 59.2-LAYER 55.0% 76.7% 64.1ALL 78.1% 52.8% 63.0887Table 3: The result for extracting hyponymy relationsfrom definition sentences, category structures,and hi-erarchy structures# of # of correctPrecision rels.
rels.Hierarchy (Proposed) 76.4 % 633,122 484,117Definition snts 77.5% 220,892 171,191Category 70.5% 596,463 420,506Total 75.3% 1,426,861 1,075,814cision of the relations (200 random samples), B) thenumber of relations, and C) the expected number ofcorrect relations estimated from the precision and thenumber of extracted relations.
We obtained 1.4?
106hyponymy relations without duplication in total with75.3% precision from definition sentences, categorystructures, and hierarchical structures.
They covered6.6 ?
105 distinct hyponyms and 1.0 ?
105 distincthypernyms.
Note that the number of duplicated rela-tions in these results was just 23,616.
This suggeststhat we could extract different types of hyponymy re-lations from each of these methods.6 ConclusionThis paper described a method for extracting a largeset of hyponymy relations from the hierarchical struc-tures of articles in Wikipedia.
We could extract633,122 relations from hierarchical layouts in theJapanese Wikipedia and their precision was 76.4%.Combining with existing methods that extract rela-tions from definition sentences and category struc-tures, we were able to extract 1,426,861 relations with75.3% precision in total without duplication.
To thebest of our knowledge, this is the largest machine-readable thesaurus for Japanese available.ReferencesD.
A.Cruse.
1998.
Lexical Semantics.
Cambridge Text-books in Linguistics.Razvan C. Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EACL,pages 9?16.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: an experimental study.
Artif.
Intell., 165(1):91?134.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online questionanswering: Answering questions before they are asked.In ACL2003, pages 1?7.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th International Conference on Computational Lin-guistics, pages 539?545.Aurelie Herbelot and Ann Copestake.
2006.
Acquiringontological relationships from wikipedia using rmrs.
InProceedings of the ISWC 2006 Workshop on Web Con-tent Mining with Human Language Technologies.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing wikipedia as external knowledge for named entityrecognition.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 698?707.George A. Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine J. Miller.
1990.
Introduc-tion to wordnet: An on-line lexical database.
In Journalof Lexicography, pages 235?244.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:leveraging generic patterns for automatically harvestingsemantic relations.
In ACL ?06 : Proceedings of the 21stInternational Conference on Computational Linguisticsand the 44th annual meeting of the ACL, pages 113?120.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic extraction of semantic rela-tionships for wordnet by means of pattern learning fromwikipedia.
In NLDB, pages 67?79.Keiji Shinzato and Kentaro Torisawa.
2004.
Acquiringhyponymy relations from web documents.
In HLT-NAACL ?04 : Proceedings of Human Language Tech-nology Conference/North American chapter of the As-sociation for Computational Linguistics annual meet-ing, pages 73?80.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowl-edge unifying wordnet and wikipedia.
In WWW ?07 :Proceedings of the 16th International World Wide WebConference.Kosuke Tokunaga, Jun?ichi Kazama, and Kentaro Tori-sawa.
2005.
Automatic discovery of attribute wordsfromweb documents.
In IJCNLP 2005, pages 106?118.Hiroaki Tsurumaru, Toru Hitaka, and Sho Yoshida.
1986.An attempt to automatic thesaurus construction froman ordinary japanese language dictionary.
In Proceed-ings of the 11th conference on Computational linguis-tics, pages 445?447.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.Wiley-Interscience.888
