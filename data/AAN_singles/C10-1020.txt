Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 170?178,Beijing, August 2010Bipolar Person Name Identification of Topic Documents UsingPrincipal Component AnalysisChein Chin ChenDepartment of InformationManagementNational Taiwan Universitypaton@im.ntu.edu.twChen-Yuan WuDepartment of InformationManagementNational Taiwan Universityr97725035@ntu.edu.twAbstractIn this paper, we propose an unsuper-vised approach for identifying bipolarperson names in a set of topic documents.We employ principal component analysis(PCA) to discover bipolar word usagepatterns of person names in the docu-ments and show that the signs of the en-tries in the principal eigenvector of PCApartition the person names into bipolargroups spontaneously.
Empirical evalua-tions demonstrate the efficacy of theproposed approach in identifying bipolarperson names of topics.1 IntroductionWith the advent of Web2.0, many online colla-borative tools, e.g., weblogs and discussion fo-rums are being developed to allow Internet usersto express their perspectives on a wide variety oftopics via Web documents.
One benefit is thatthe Web has become an invaluable knowledgebase for Internet users to learn about a topiccomprehensively.
Since the essence of Web2.0is knowledge sharing, collaborative tools aregenerally designed with few constraints so thatusers will be motivated to contribute their know-ledge.
As a result, the number of topic docu-ments on the Internet is growing exponentially.Research subjects, such as topic threading andtimeline mining (Nallapati et al, 2004; Feng andAllan, 2007; Chen and Chen, 2008), are thusbeing studied to help Internet users comprehendnumerous topic documents efficiently.A topic consists of a sequence of relatedevents associated with a specific time, place, andperson(s) (Nallapati et al, 2004).
Topics thatinvolve bipolar (or competitive) viewpoints areoften attention-getting and attract a large numberof topic documents.
For such topics, identifyingthe polarity of the named entities, especially per-son names, in the topic documents would helpreaders learn the topic efficiently.
For instance,for the 2008 American presidential election, In-ternet users can find numerous Web documentsabout the Democrat and Republican parties.Identifying important people in the competingparties would help readers form a balanced viewof the campaign.Existing works on topic content mining focuson extracting important themes in topics.
In thispaper, we propose an unsupervised approach thatidentifies bipolar person names in a set of topicdocuments automatically.
We employ principalcomponent analysis (PCA) (Smith, 2002) to dis-cover bipolar word usage patterns of importantperson names in a set of topic documents, andshow that the signs of the entries in the principaleigenvector of PCA partition the person namesin bipolar groups spontaneously.
In addition, wepresent two techniques, called off-topic blockelimination and weighted correlation coefficient,to reduce the effect of data sparseness on personname bipolarization.
The results of experimentsbased on two topic document sets written inEnglish and Chinese respectively demonstratethat the proposed PCA-based approach is effec-tive in identifying bipolar person names.
Fur-thermore, the approach is language independent.1702 Related WorkOur research is closely related to opinion mining,which involves identifying the polarity (or sen-timent) of a word in order to extract positive ornegative sentences from review documents (Ga-napathibhotla and Liu, 2008).
Hatzivassiloglouand McKeown (1997) validated that languageconjunctions, such as and, or, and but, are effec-tive indicators for judging the polarity of con-joined adjectives.
The authors observed thatmost conjoined adjectives (77.84%) have thesame orientation, while conjunctions that use butgenerally connect adjectives of different orienta-tions.
They proposed a log-linear regressionmodel that learns the distributions of conjunctionindicators from a training corpus to predict thepolarity of conjoined adjectives.
Turney andLittman (2003) manually selected seven positiveand seven negative words as a polarity lexiconand proposed using pointwise mutual informa-tion (PMI) to calculate the polarity of a word.
Aword has a positive orientation if it tends to co-occur with positive words; otherwise, it has anegative orientation.
More recently, Esuli andSebastiani (2006) developed a lexical resource,called SentiWordNet, which calculates the de-grees of objective, positive, and negative senti-ments of a synset in WordNet.
The authors em-ployed a bootstrap strategy to collect trainingdatasets for the sentiments and trained eight sen-timent classifiers to assign sentiment scores to asynset.
Kanayama and Nasukawa (2006) positedthat polar clauses with the same polarity tend toappear successively in contexts.
The authors de-rived the coherent precision and coherent densityof a word in a training corpus to predict theword?s polarity.
Ganapathibhotla and Liu (2008)investigated comparative sentences in productreviews.
To identify the polarity of a compara-tive word (e.g., longer) with a product feature(e.g., battery life), the authors collected phrasesthat describe the Pros and Cons of products fromEpinions.com and proposed one-side association(OSA), which is a variant of PMI.
OSA assigns apositive (negative) orientation to the compara-tive-feature combination if the synonyms of thecomparative word and feature tend to co-occurin the Pros (resp.
Cons) phrases.Our research differs from existing approachesin three respects.
First, most works identify thepolarity of adjectives and adverbs because thesyntactic constructs generally express sentimen-tal semantics.
In contrast, our method identifiesthe polarity of person names.
Second, to the bestof our knowledge, all existing polarity identifica-tion methods require external informationsources (e.g., WordNet, manually selected polar-ity words, or training corpora).
However, ourmethod identifies bipolar person names by simp-ly analyzing person name usage patterns in topicdocuments without using external information.Finally, our method does not require any lan-guage constructs, such as conjunctions; hence, itcan be applied to different languages.3 Method3.1 Data PreprocessingGiven a set of topic documents, we firstdecompose the documents into a set of non-overlapping blocks B = {b1, b2, ?, bn}.
A blockcan be a paragraph or a document, depending onthe granularity of PCA sampling.
Let U = {u1,u2, ?, um} be a set of textual units in B.
In thisstudy, a unit refers to a person name.
Then, thedocument set can be represented as an mxn unit-block association matrix A.
A column in A,denoted as bi, represents a decomposed block i.It is an m-dimensional vector whose j?th entry,denoted as bi,j, is the frequency of uj in bi.
Inaddition, a row in A, denoted as ui, represents atextual unit i; and it is an n-dimensional vectorwhose j?th entry, denoted as ui,j, is the frequencyof ui in bj.3.2 PCA-based Person Name BipolarizationPrincipal component analysis is a well-knownstatistical method that is used primarily to identi-fy the most important feature pattern in a high-dimensional dataset (Smith, 2002).
In our re-search, it identifies the most important unit pat-tern in the topic blocks by first constructing anmxm unit relation matrix R, in which the (i,j)-entry (denoted as ri,j) denotes the correlationcoefficient of ui and uj.
The correlation is com-puted as follows:,)()()()(),(12~,12~,1~,~,,??????????????
nkjkjnkikinkjkjikijijiuuuuuuuuuucorrrwhere ui~=1/n?nk=1ui,k and uj~=1/n?nk=1uj,k are theaverage frequencies of units i and j respectively.171The range of ri,j is within [-1,1] and the valuerepresents the degree of correlation between uiand uj under the decomposed blocks.
If ri,j = 0,we say that ui and uj are uncorrelated; that is,occurrences of unit ui and unit uj in the blocksare independent of each other.
If ri,j > 0, we saythat units ui and uj are positively correlated.
Thatis, ui and uj tend to co-occur in the blocks; oth-erwise, both tend to be jointly-absent.
If ri,j < 0,we say that ui and uj are negatively correlated;that is, if one unit appears, the other tends not toappear in  the same block simultaneously.
Notethat if ri,j ?
0, |ri,j| scales the strength of a positiveor negative correlation.
Moreover, since the cor-relation coefficient is commutative, ri,j will beidentical to rj,i such that matrix R will be symme-tric.A unit pattern is represented as a vector v ofdimension m in which the i?th entry vi indicatesthe weight of i?th unit in the pattern.
Since ma-trix R depicts the correlation of the units in thetopic blocks, given a constituent of v, vTRv com-putes the variance of the pattern to characterizethe decomposed blocks.
A pattern is important ifit characterizes the variance of the blocks specif-ically.
PCA can then identify the most importantunit pattern by using the following object func-tion:max vTRv,s.t.
vTv = 1.Without specifying any constraint on v, theobjective function becomes arbitrarily large withlarge entry values of v. Constraint vTv = 1 limitsthe search space within the set of length-normalized vectors.
Chen and Chen (2008) showthat the desired v for the above constrained op-timization problem is the eigenvector of R withthe largest eigenvalue.
Furthermore, as R is asymmetric matrix, such an eigenvector alwaysexists (Spence et al, 2000) and the optimizationproblem is solvable.PCA is not the only method that identifies im-portant textual patterns in terms of eigenvectors.For instance, Gong and Liu (2001), Chen andChen (2008) utilize the eigenvectors of symme-tric matrices to extract salient concepts and sa-lient themes from documents respectively1.
The1 The right singular vectors of a matrix A used by Gong andLiu (2001) are equivalent to the eigenvectors of a symme-tric matrix ATA whose entries are the inner products of thecorresponding columns of A.difference between PCA and other eigenvector-based approaches lies in the way the unit relationmatrix is constructed.
PCA calculates ri,j by us-ing the correlation coefficient, whereas the otherapproaches employ the inner product or cosineformula 2  (Manning et al, 2008) to derive therelationship between textual units.
Specifically,the correlation coefficient is identical to the co-sine formula if we normalize each unit with itsmean:),,(cosine)()()()(),(**12*,12*,1*,*,12~,12~,1~,~,jinkkjnkkinkkjkinkjkjnkikinkjkjikijiuuuuuuuuuuuuuuuucorr??????????????????????
?where ui* = ui ?
ui~[1,1,?,1]T; uj* = uj ?
uj~[1,1,?,1]T; and are the mean-normalized vectors ofui and uj, respectively.
Conceptually, the meannormalization process is the only difference be-tween PCA and other eigenvector-based ap-proaches.Since the eigenvectors of a symmetric matrixform an orthonormal basis of Rm, they may con-tain negative entries (Spence et al, 2000).
Eventhough Kleinberg (1999) and Chen and Chen(2008) have shown experimentally that negativeentries in an eigenvector are as important as pos-itive entries for describing a certain unit pattern,the meaning of negative entries in their ap-proaches is unexplainable.
This is because tex-tual units (e.g., terms, sentences, and documents)in information retrieval are usually characterizedby frequency-based metrics, e.g., term frequency,document frequency, or TFIDF (Manning et al,2008), which can never be negative.
In PCA,however, the mean normalization process of thecorrelation coefficient gives bipolar meaning topositive and negative entries and that helps uspartition textual units into bipolar groups in ac-cordance with their signs in v.2 The inner product is equivalent to the cosine formulawhen the calculated vectors are length normalized (Man-ning et al, 2008).172u1u2u2u1abovetheaveragebelowtheaveragenormalizationv = <-0.707, 0.707>Figure 1.
The effect of the mean normalizationprocess.The synthesized example in Figure 1 illu-strates the effect of the normalization process.
Inthis example, we are only interested in textualunits u1 and u2; the corpus consists of ten blocks.Graphically, each block can be represented as apoint in a 2-dimensional vector space.
The meannormalization process moves the origin of the 2-dimensional vector space to the centroid of theblocks that makes negative unit values explaina-ble.
A negative unit of a block in this normalizedvector space indicates that the number of occur-rences of the unit in the block is less than theunit?s average; by contrast, a positive unit meansthat the number of occurrences of the unit in ablock is above the average.
In the figure, themost important unit pattern v <-0.707, 0.707>calculated by PCA is represented by the dashedline.
The signs of v?s entries indicate that theoccurrence of u1 will be lower than the averageif u2 occurs frequently in a block.
In addition, asthe signs of entries in an eigenvector are inverti-ble (Spence et al, 2000), the constituent of valso claims that if u1 occurs frequently in a block,then the probability that we will observe u2 inthe same block will be lower than expected.
Theinstances of bipolar word usage behavior pre-sented in v are consistent with the distribution ofthe ten blocks.
As mentioned in Section 2, Ka-nayama and Nasukawa (2006) validated that po-lar text units with the same polarity tend to ap-pear together to make contexts coherent.
Conse-quently, we believe that the signs in PCA?s prin-cipal eigenvector are effective in partitioningtextual units into bipolar groups.3.3 Sparseness of Textual UnitsA major problem with employing PCA toprocess textual data is the sparseness of textualunits.
To illustrate this problem, we collected411 news documents about the 2009 NBA Finalsfrom Google News and counted the frequencythat each person name occurred in the docu-ments.
We also evaluate the documents in theexperiment section to determine if the proposedapproach is capable of bipolarizing the personnames into the teams that played in the finalscorrectly.
We rank the units according to theirfrequencies and list the frequencies in descend-ing order in Figure 2.
The figure shows that thefrequency distribution follows Zipf?s law (Man-ning et al, 2008); and for most units, the distri-bution in a block will be very sparse.Figure 2.
The rank-frequency distribution of per-son names on logarithmic scales (base 10).We observe that a unit will not to occur in ablock in the following three scenarios.
1) Thepolarity of the block is the opposite of the polari-ty of the unit.
For instance, if the unit representsa player in one team and the block narrates in-formation about the other team, the block?s au-thor would not mention the unit in the block toensure that the block?s content is coherent.
2)Even if the polarity of a block is identical to thatof the unit; the length of the block may not besufficient to contain the unit.
3) The block is off-topic so the unit will not appear in the block.
Inthe last two scenarios, the absence of units willimpact the estimation of the correlation coeffi-cient.
To alleviate the problem, we propose twotechniques, the weighted correlation coefficientand off-block elimination, which we describe inthe following sub-sections.Weighted Correlation CoefficientThe so-called data sparseness problem in scena-rio 2 affects many statistical information retriev-al and language models (Manning et al, 2008).For units with the same polarity, data sparsenesscould lead to underestimation of their correla-tions because the probability that the units willoccur together is reduced.
Conversely, for uncor-related units or units with opposite polarities,173data sparseness may lead to overestimation oftheir correlations because they are frequentlyjointly-absent in the decomposed blocks.
Whilesmoothing approaches, such as Laplace?s law(also known as adding-one smoothing), havebeen developed to alleviate data sparseness inlanguage models (Manning et al, 2008), they arenot appropriate for PCA.
This is because the cor-relation coefficient of PCA measures the diver-gence between units from their means, so addingone to each block unit will not change the diver-gence.
To summarize, data sparseness could in-fluence the correlation coefficient when units donot co-occur.
Thus, for two units ui and uj, weseparate B into co-occurring and non-co-occurring parts and apply the followingweighted correlation coefficient:,)()()1()()()1(/)()()()()1(),(),( ),(2~,2~,),( ),(2~,2~,),(~,~,),(~,~,?
??
????
???
??????????????????????????????????
?jicob jicoBbjbjjbjjicob jicoBbibiibijicoBbjbjibijicobjbjibijiwuuuuuuuuuuuuuuuuuucorr?????
?where corrw(ui,uj) represents the weighted corre-lation coefficient between units i and j; and co(i,j)denotes the set of blocks in which units i and jco-occur.
The range of parameter ?
is within[0,1].
It weights the influence of non-co-occurring blocks when calculating the correla-tion coefficient.
When ?
= 0.5, the equation isequivalent to the standard correlation coefficient;and when ?
= 0, the equation only considers theblocks in which units i and j co-occur.
Converse-ly, when ?
= 1, only non-co-occurring blocks areemployed to calculate the units?
correlation.
Inthe experiment section, we will examine the ef-fect of ?
on bipolar person name identification.Off-topic Block EliminationIncluding off-topic blocks in PCA will lead tooverestimation of the correlation between units.This is because units are usually jointly-absentfrom off-topic blocks that make uncorrelated oreven negatively correlated units positively corre-lated.
To eliminate the effect of off-topic blockson unit bipolarization, we construct a centroid ofall the decomposed blocks by averaging bi?s.Then, blocks whose cosine similarity to the cen-troid is lower than a predefined threshold ?
areexcluded from calculation of the correlationcoefficient.4 Performance EvaluationsIn this section, we evaluate two topics with bipo-lar (or competitive) viewpoints to demonstratethe efficacy of the proposed approach.4.1 The 2009 NBA FinalsFor this experiment, we collected 411 news doc-uments about the 2009 NBA Finals from GoogleNews during the period of the finals (from2009/06/04 to 2009/06/16).
The matchup of thefinals was Lakers versus Orlando Magic.
In thisexperiment, a block is a topic document, as pa-ragraph tags are not provided in the evaluateddocuments.
First, we parsed the blocks by usingStanford Named Entity Recognizer3 to extract allpossible named entities.
We observed that theparser sometimes extracted false entities (such asLakers Kobe) because the words in the headlineswere capitalized and that confused the parser.
Toreduce the effect of false extraction by the parser,we examined the extracted named entities ma-nually.
After eliminating false entities, the data-set comprised 546 unique named entities; 538were person names and others represented or-ganizations, such as basketball teams and bas-ketball courts.
To examine the effect of theweighted correlation coefficient, parameter ?
isset between 0 and 1, and increased in incrementsof 0.1; and the threshold ?
used by off-topicblock elimination is set at 0.3.
The frequencydistribution of the person names, shown in Fig-ure 2, indicates that many of the person namesrarely appeared in the examined blocks, so theirdistribution was too sparse for PCA.
Hence, inthe following subsections, we sum the frequen-cies of the 538 person names in the examinedblocks.
We select the first k frequent personnames, whose accumulated term frequenciesreach 60% of the total frequencies, for evalua-tion.
In other words, the evaluated person namesaccount for 60% of the person name occurrencesin the examined blocks.For each parameter setting, we perform prin-cipal component analysis on the examinedblocks and the selected entities, and partition theentities into two bipolar groups according to3 http://nlp.stanford.edu/software/CRF-NER.shtml174their signs in the principal eigenvector.
To eva-luate the accuracy rate of bipolarization, we needto label the team of each bipolar group.
Then,the accuracy rate is the proportion of the entitiesin the groups that actually belong to the labeledteams.
Team labeling is performed by examiningthe person names in the larger bipolarizationgroup.
If the majority of the entities in the groupbelong to the Lakers (Magic), we label the groupas Lakers (Magic) and the other group as Magic(Lakers).
If the two bipolar groups are the samesize, the group that contains the most Lakers(Magic) entities is labeled as Lakers (Magic),and the other group is labeled as Magic (Lakers).If both groups contain the same number of Lake-rs (Magic) entities, we randomly assign teamlabels because all assignments produce the sameaccuracy score.
To the best of our knowledge,there is no similar work on person name bipola-rization; therefore, for comparison, we use abaseline method that assigns the same polarity toall the person names.Magic LakersDwight Howard 0.0884 Derek Fisher -0.0105Hedo Turkoglu 0.1827 Kobe Bryant -0.2033Jameer Nelson 0.3317 Lamar Odom -0.1372Jeff Van Gundy*+ 0.3749 LeBron James*^ -0.0373Magic Johnson* 0.3815 Mark Jackson*^ -0.2336Rafer Alston 0.3496 Pau Gasol -0.1858Rashard Lewis 0.1861 Paul Gasol*+ -0.1645Stan Van Gundy 0.4035 Phil Jackson -0.2553Table 1.
The bipolarization results for NBA per-son names.
(?
= 0.8 and ?
= 0.3)Table 1 shows the bipolarization results forfrequent person names in the dataset.
The para-meter ?
is set at 0.8 because of its superior per-formance.
The left-hand column of the table liststhe person names labeled as Magic and their en-try values in the principal eigenvector; and theright-hand column lists the person names labeledas Lakers.
It is interesting to note that the eva-luated entities contain person names irrelevant tothe players in the NBA finals.
For instance, thefrequency of Magic Johnson, an ex-Lakers play-er, is high because he constantly spoke in sup-port of the Lakers during the finals.
In addition,many documents misspell Pau Gasol as Paul Ga-sol.
Even though the names refer to the sameplayer, the named entity recognizer parses themas distinct entities.
We propose two evaluationstrategies, called strict evaluation and non-strictevaluation.
The strict evaluation strategy treatsthe person names that do not refer to the players,coaches in the finals as false positives.
Under thenon-strict strategy, the person names that areclosely related to Lakers or Magic players, suchas a player?s relatives or misspellings, aredeemed true positives if they are bipolarized intothe correct teams.
In Table 1, a person name an-notated with the symbol * indicates that the enti-ty is bipolarized incorrectly.
For instance, MagicJohnson is not a member of Magic.
The symbol^ indicates that the person name is neutral (orirrelevant) to the teams in the finals.
In addition,the symbol + indicates that the person namerepresents a relative of a member of the teamhe/she is bipolarized to; or the name is a miss-pelling, but it refers to a member of the bipola-rized team.
This kind of bipolarization is correctunder the non-strict evaluation strategy.
Asshown in Table 1, the proposed method bipola-rizes the important persons in the finals correctlywithout using any external information source.The accuracy rates of strict and non-strict evalu-ation are 68.8% and 81.3% respectively.
Therates are far better than those of the baseline me-thod, which are 37.5% and 43.8% respectively.If we ignore the neutral entities, which are al-ways wrong no matter what bipolarization ap-proach is employed, the strict and non-strict ac-curacies are 78.6% and 92.9% respectively.
Inthe non-strict evaluation, we only mis-bipolarized Magic Johnson as Magic.
The mis-take also reflects a problem with person nameresolution when the person names that appear ina document are ambiguous.
In our dataset, theword ?Magic?
sometimes refers to Magic John-son and sometimes to Orlando Magic.
Here, wedo not consider a sophisticated person name res-olution scheme; instead, we simply assign thefrequency of a person name to all its specificentities (e.g., Magic to Magic Johnson, and Kobeto Kobe Bryant) so that specific person namesare frequent enough for PCA.
As a result, MagicJohnson tends to co-occur with the members ofMagic and is incorrectly bipolarized to the Mag-ic team.
Another interesting phenomenon is thatLeBron James (a player with Cavaliers) is incor-rectly bipolarized to Lakers.
This is becauseKobe Bryant (a player with Lakers) and LeBronJames were rivals for the most valuable player(MVP) award in the 2009 NBA season.
Thedocuments that mentioned Kobe Bryant duringthe finals often compared him with LeBron175James to attract the attention of readers.
As thenames often co-occur in the documents, LeBronJames was wrongly classified as a member ofLakers.Figures 3 and 4 illustrate the effects of theweighted correlation coefficient and off-topicblock elimination on NBA person name bipola-rization.
As shown in the figures, eliminatingoff-topic blocks generally improves the systemperformance.
It is noteworthy that, when off-topic blocks are eliminated, large ?
values pro-duce good bipolarization performances.
As men-tioned in Section 3.3, a large ?
implies that non-co-occurring blocks are important for calculatingthe correlation between a pair of person names.When off-topic blocks are eliminated, the set ofnon-co-occurring blocks specifically reveals op-posing or jointly-absent relationships betweenentities.
Therefore, the bipolarization perfor-mance improves as ?
increases.
Conversely,when off-topic blocks are not eliminated, the setof non-co-occurring blocks will contain off-topicblocks.
As both entities in a pair tend to be ab-sent in off-topic blocks, a large ?
value will leadto overestimation of the correlation between bi-polar entities.
Consequently, the bipolarizationaccuracy decreases as ?
increases.
It is also in-teresting to note that the bipolarization perfor-mance decreases as ?
decreases.
We observedthat some of the topic documents are recaps ofthe finals, which tend to mention Magic andLakers players together.
As a small ?
valuemakes co-occurrence blocks important, recap-style documents will overestimate the correlationbetween bipolar entities.
Consequently, the bipo-larization performance is inferior when ?
issmall.Figure 3.
The effects of the weighted correlationcoefficient and off-topic block elimination onNBA person name bipolarization.
(Strict)Figure 4.
The effects of the weighted correlationcoefficient and off-topic block elimination onNBA person name bipolarization.
(Non-strict)4.2 Taiwan?s 2009 Legislative By-ElectionsFor this experiment, we evaluated Chinese newsdocuments about Taiwan?s 2009 legislative by-elections, in which two major parties, the Demo-cratic Progressive Party (DPP) and the KouMin-Tang (KMT), campaigned for three legislativepositions.
Since the by-elections were regional,not many news documents were published dur-ing the campaign.
In total, we collected 89 newsdocuments that were published in The LibertyTimes 4  during the election period (from2009/12/27 to 2010/01/11).
Then, we used aChinese word processing system, called ChineseKnowledge and Information Processing (CKIP)5,to extract possible Chinese person names in thedocuments.
Once again, the names were ex-amined manually to remove false extractions.The dataset comprised 175 unique person names.As many of the names only appeared once, weselected the first k frequent person names whoseaccumulated frequency was at least 60% of thetotal term frequency count of the person namesfor evaluation.
We calculated the accuracy ofperson name bipolarization by the same methodas the NBA experiment in order to assess howwell the bipolarized groups represented theKMT and the DPP.
As none of the selectednames were misspelled, we do not show the non-strict accuracy of bipolarization.
The threshold ?is set at 0.3, and each block is a topic document.Table 2 shows the bipolarization results forthe frequent person names of the candidates ofthe respective parties, the party chair persons,and important party staff members.
The accuracyrates of the bipolarization and the baseline me-4 http://www.libertytimes.com.tw/index.htm5 http://ckipsvr.iis.sinica.edu.tw/176thods are 70% and 50%, respectively.
It is note-worthy that the chairs of the DPP and the KMT,who are Ing-wen Tsai and Ying-jeou Ma respec-tively, are correctly bipolarized.
We observedthat, during the campaign, the chairs repeatedlyhelped their respective party?s candidates gainsupport from the public.
As the names of thechairs and the candidates often co-occur in thedocuments, they can be bipolarized accurately.We also found that our approach bipolarized twocandidates incorrectly if the competition be-tween them was fierce.
For instance, Kun-chengLai and Li-chen Kuang campaigned intensivelyfor a single legislative position.
As they oftencommented on each other during the campaign,they tend to co-occur in the topic documents.PCA therefore misclassifies them as positivelycorrelated and incorrectly groups Kun-cheng Laiwith the KMT party.KMT (???)
DPP (???
)Kun-cheng Lai (???
)* 0.39 Wen-chin Yu (???
)* -0.56Li-chen Kuang (???)
0.40 Den-yih Wu (???
)* -0.03Li-ling Chen (???)
0.01 Chao-tung Chien (???)
-0.56Ying-jeou Ma (???)
0.05 Ing-wen Tsai (???)
-0.17Tseng-chang Su (???)
-0.01Jung-chung Kuo (???)
-0.01Table 2.
The bipolarization results for the elec-tion dataset.
(?
= 0.7)Figure 5.
The effects of the weighted correlationcoefficient and off-topic block elimination.Figure 5 shows that off-topic block elimina-tion is effective in person name bipolarization.However, the weighted correlation coefficientonly improves the bipolarization performanceslightly.
We have investigated this problem andbelieve that the evaluated person names in thedocuments are frequent enough to prevent thedata sparseness problem.
While the weightedcorrelation coefficient does not improve the bi-polarization performance significantly, the pro-posed PCA-based approach can still identify thebipolar parties of important persons accurately.Unlike the results in the last section, the accura-cy rate in this experiment does not decrease as ?decreases.
This is because the topic documentsgenerally report news about a single party.
Asthe documents rarely recap the activities of par-ties, the co-occurrence blocks accurately reflectthe bipolar relationship between the persons.Hence, a small ?
value can identify bipolar per-son names effectively.The evaluations of the NBA and the electiondatasets demonstrate that the proposed PCA-based approach identifies bipolar person namesin topic documents effectively.
As the writingstyles of topic documents in different domainsvary, the weighted correlation coefficient maynot always improve bipolarization performance.However, because we eliminate off-topic blocks,a large ?
value always produces superior bipola-rization performances.5 ConclusionIn this paper, we have proposed an unsupervisedapproach for identifying bipolar person names intopic documents.
We show that the signs of theentries in the principal eigenvector of PCA canpartition person names into bipolar groups spon-taneously.
In addition, we introduce two tech-niques, namely the weighted correlation coeffi-cient and off-topic block elimination, to addressthe data sparseness problem.
The experimentresults demonstrate that the proposed approachidentifies bipolar person names of topics suc-cessfully without using any external knowledge;moreover, it is language independent.
The re-sults also show that off-topic block eliminationalong with a large ?
value for the weighted cor-relation coefficient generally produce accurateperson name bipolarization.
In the future, wewill integrate text summarization techniqueswith the proposed bipolarization method to pro-vide users with polarity-based topic summaries.We believe that summarizing important informa-tion about different polarities would help usersgain a comprehensive knowledge of a topic.AcknowledgeThe authors would like to thank the anonymous re-viewers for their valuable comments and suggestions.This work was supported in part by NSC 97-2221-E-002-225-MY2.177ReferencesChen, Chien Chin and Meng Chang Chen.
2008.TSCAN: a novel method for topic summarizationand content anatomy.
In Proceedings of the 31stannual international ACM SIGIR Conference onResearch and Development in Information Re-trieval, pages 579-586.Esuli, Andrea and Fabrizio Sebastiani.
2006.
SEN-TIWORDNET: A Publicly Available Lexical Re-source for Opinion Mining.
In Proceedings of the5th Conference on Language Resources and Eval-uation.Feng, Ao and James Allan.
2007.
Finding and Link-ing Incidents in News.
In Proceedings of the six-teenth ACM Conference on information and know-ledge management, pages 821-830.Ganapathibhotla, Murthy and Bing Liu.
2008.
MiningOpinions in Comparative Sentences.
In Proceed-ings of the 22nd International Conference onComputational Linguistics, pages 241-248.Gong, Yihong and Xin Liu.
2001.
Generic text sum-marization using relevance measure and latent se-mantic analysis.
In Proceedings of the 24th annualinternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages19-25.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1997.
Predicting the Semantic Orienta-tion of Adjectives.
In Proceedings of the eighthconference on European chapter of the Associa-tion for Computational Linguistics, pages 174-181.Kanayama, Hiroshi and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 355-363.Kleinberg, Jon M.. 1999.
Authoritative sources in ahyperlinked environment.
Journal of the ACM 46,5, pages 604-632.Manning, Christopher D., Prabhakar Raghavan andHinrich Schutze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Nallapati, Ramesh, Ao Feng, Fuchun Peng and JamesAllan.
2004.
Event Threading within News Topics.In Proceedings of the thirteenth ACM internation-al conference on Information and knowledge man-agement, pages 446-453.Smith, Lindsay I.. 2002.
A Tutorial on PrincipalComponents Analysis.
Cornell University.Spence, Lawrence E., Arnold J. Insel and Stephen H.Friedberg.
2000.
Elementary Linear Algebra, AMatrix Approach.
Prentice Hall.Turney, Peter D., and Michael L. Littman.
2003.Measuring Praise and Criticism: Inference of Se-mantic Orientation from Association.
ACM Trans-actions on Information Systems (TOIS), pages 315-346.178
