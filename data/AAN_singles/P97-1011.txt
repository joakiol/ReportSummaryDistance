Learning Features that Predict Cue UsageBarbara Di Eugenio" J ohanna D.  Moore  t Mass imo Pao lucc i  "+Un ivers i ty  of P i t t sburghP i t t sburgh ,  PA 15260, USA{d ieugen i ,  jmoore  ,pao lucc i}@cs  .pitt.
eduAbst rac tOur goal is to identify the features that pre-dict the occurrence and placement of dis-course cues in tutorial explanations in or-der to aid in the automatic generation ofexplanations.
Previous attempts to deviserules for text generation were based on in-tuition or small numbers of constructed ex-amples.
We apply a machine learning pro-gram, C4.5, to induce decision trees for cueoccurrence and placement from a corpus ofdata coded for a variety of features previ-ously thought o affect cue usage.
Our ex-periments enable us to identify the featureswith most predictive power, and show thatmachine learning can be used to induce de-cision trees useful for text generation.1 In t roduct ionDiscourse cues are words or phrases, such as because,first, and although, that mark structural and seman-tic relationships between discourse entities.
Theyplay a crucial role in many discourse processingtasks, including plan recognition (Litman and Allen,1987), text comprehension (Cohen, 1984; Hobbs,1985; Mann and Thompson, 1986; Reichman-Adar,1984), and anaphora resolution (Grosz and Sidner,1986).
Moreover, research in reading comprehensionindicates that felicitous use of cues improves compre-hension and recall (Goldman, 1988), but that theirindiscriminate use may have detrimental effects onrecall (Millis, Graesser, and Haberlandt, 1993).Our goal is to identify general strategies for cue us-age that can be implemented for automatic text gen-eration.
From the generation perspective, cue usageconsists of three distinct, but interrelated problems:(1) occurrence: whether or not to include a cue in thegenerated text, (2) placement: where the cue shouldbe placed in the text, and (3) selection: what lexicalitem(s) should be used.Prior work in text generation has focused on cueselection (McKeown and Elhadad, 1991; Elhadadand McKeown, 1990), or on the relation between*Learning Research & Development CentertComputer Science Department, and Learning Re-search ~z Development Centertlntelllgent Systems Programcue occurrence and placement and specific rhetori-cal structures (RSsner and Stede, 1992; Scott andde Souza, 1990; Vander Linden and Martin, 1995).Other hypotheses about cue usage derive from workon discourse coherence and structure.
Previousresearch (Hobbs, 1985; Grosz and Sidner, 1986;Schiffrin, 1987; Mann and Thompson, 1988; Elhadadand McKeown, 1990), which has been largely de-scriptive, suggests factors such as structural featuresof the discourse (e.g., level of embedding and segmentcomplexity), intentional and informational relationsin that structure, ordering of relata, and syntacticform of discourse constituents.Moser and Moore (1995; 1997) coded a corpusof naturally occurring tutorial explanations for therange of features identified in prior work.
Becausethey were also interested in the contrast between oc-currence and non-occurrence of cues, they exhaus-tively coded for all of the factors thought to con-tribute to cue usage in all of the text.
From theirstudy, Moscr and Moore identified several interestingcorrelations between particular features and specificaspects of cue usage, and were able to test specifichypotheses from the hterature that were based onconstructed examples.In this paper, we focus on cue occurrence andplacement, and present an empirical study of the hy-potheses provided by previous research, which havenever been systematically evaluated with naturallyoccurring data.
Wc use a machine learning program,C4.5 (Quinlan, 1993), on the tagged corpus of Moserand Moore to induce decision trees.
The number ofcoded features and their interactions makes the man-ual construction of rules that predict cue occurrenceand placement an intractable task.Our results largely confirm the suggestions fromthe hterature, and clarify them by highhghting themost influential features for a particular task.
Dis-course structure, in terms of both segment structureand levels of embedding, affects cue occurrence themost; intentional relations also play an importantrole.
For cue placement, the most important factorsare syntactic structure and segment complexity.The paper is organized as follows.
In Section 2 wediscuss previous research in more detail.
Section 3provides an overview of Moser and Moore's codingscheme.
In Section 4 we present our learning exper-iments, and in Section 5 we discuss our results andconclude.802 Re la ted  WorkMcKeown and Elhadad (1991; 1990) studied severaiconnectives (e.g., but, since, because), and includemany insightful hypotheses about cue selection; theirobservation that the distinction between but and ?l-thoug/~ depends on the point of the move is relatedto the notion of core discussed below.
However, theydo not address the problem of cue occurrence.Other researchers (R6sner and Stede, 1902; Scottand de Souza, 1990) are concerned with generatingtext from "RST trees", hierarchical structures whereleaf nodes contain content and internal nodes indi-cate the rt~etorical relations, as defined in Rhetori-cal Structure Theory (RST) (Mann and Thompson,1988), that exist between subtrees.
They proposedheuristics for including and choosing cues based onthe rhetorical relation between spans of text, the or-der of the relata, and the complexity of the relatedtext spans.
However, (Scott and de Souza, 1990)was based on a small number of constructed exam-pies, and (R6sner and Stede, 1992) focused on a smallnumber of RST relations.
(Litman, 1996) and (Siegel and McKeown, 1994)have applied machine learning to disambiguate be-tween the discourse and sentcntial usages of cues;however, they do not consider the issues of occur-rence and placement, and approach the problem fromthe point of view of interpretation.
We closely followthe approach in (Litman, 1996) in two ways.
First,we use C4.5.
Second, we experiment first with eachfeature individually, and then with "interesting" sub-sets of features.3 Re la t iona l  D iscourse  Ana lys i sThis section briefly describes Relational DiscourseAnal~tsis (RDA) (Moser, Moore, and Glendening,1996), the coding scheme used to tag the data forour machine learning experiments.
1RDA is a scheme devised for analyzing tutorial ex-planations in the domain of electronics troubleshoot-ing.
It synthesizes ideas from (Grosz and Sidner,1986) and from RST (Mann and Thompson, 1988).Coders use RDA to exhaustively analyze each expla-nation in the corpus, i.e., every word in each expla-nation belongs to exactly one element in the anal-ysis.
An explanation may consist of multiple seg-ments.
Each segment originates with an intentionof the speaker.
Segments are internally structuredand consist of a core, i.e., that element hat most di-rectly expresses the segment purpose, and any num-ber of contributors, i.e.
the remaining constituents.For each contributor, one analyzes its relation to thecore from an intentional perspective, i.e., how it isintended to support the core, and from an informa-tional perspective, i.e., how its content relates to that1For more detail about the RDA coding scheme see(Moser and Moore, 1995; Moser and Moore, 1997).of the core.
The set of intentional relations in RDAis a modification of the presentational relations ofRST, while informational relations are similar to thesubject matter elations in RST.
Each segment con-stituent, both core and contributors, may itself be asegment with a core:contributor structure.
In somecases the core is not explicit.
This is often the casewith the whole tutor's explanation, since its purposeis to answer the student's explicit question.As an example of the application of RDA,  considerthe partial tutor explanation in (1) 2 .
The purpose ofthis segment is to inform the student that she madethe strategy error of testing inside part3 too soon.The constituent that makes the purpose obvious, inthis case (l-B), is the core of the segment.
The otherconstituents help to serve the segment purpose bycontributing to it.
(1-C) is an example ofsubsegmentwith its own core:contributor structure; its purposeis to give a reason for testing part2 first.The RDA analysis of (I) is shown schematically inFigure 1.
The core is depicted as the mother of allthe relations it participates in.
Each relation node islabeled with both its intentional and informationalrelation, with the order of relata in the label indicat-ing the linear order in the discourse.
Each relationnode has up to two daughters: the cue, if any, andthe contributor, in the order they appear in the dis-course .Coders analyze each explanation i  the corpus andenter their analyses into a database.
The corpus con-sists of 854 clauses comprising 668 segments, for atotal of 780 relations.
Table 1 summarizes the dis-tribution of different relations, and the number ofcued relations in each category.
Joints are segmentscomprising more than one core, but no contributor;clusters are multiunit structures with no recogniz-able core:contributor relation.
(l-B) is a cluster com-posed of two units (the two clauses), related only atthe informational level by a temporal relation.
Bothclauses describe actions, with the first action descrip-tion embedded in a matriz ("You should").
Cues aremuch more likely to occur in clusters, where only in-formational relations occur, than in core:contributorstructures, where intentional and informational rela-tions co-occur (X 2 = 33.367, p <.001, df = 1).
Inthe following, we will not discuss joints and clustersany further.An important result pointed out by (Moser andMoore, 1995) is that cue placement depends on coreposition.
When the core is first and a cue is asso-ciated with the relation, the cue never occurs withthe core.
In contrast, when the core is second, if acue occurs, it can occur either on the core or on thecontributor.aTo make the example more intelligible, we replacedreferences to parts of the circuit with the labels partl,part2 and part3.81(i)AlthoughThis isbecauseAlso,andA.
you know that part1 is good,B.
you should eliminate part2before troubleshooting inside part3.C.D.E.1.
part2 is moved frequentlyand thus 2. is more susceptible to damage than part3.it is more work to open up part3 for testingthe process of opening drawers and extending cards in part3may induce problems which did not already exist.concedecriterion:actAlthough AB.
you should eliminate part2before troubleshooting inside part3conv ince  Conusnce conugneeact:reason act:reason act:reason(Th 2because }conv incecause:effectC.1 andthusFigure 1: The RDA analysis of (1)4 Learn ing  f rom the  corpus4.1 The  a lgor i thmWe chose the C4.5 learning algorithm (Quinlan,1993) because it is well suited to a domain such asours with discrete valued attributes.
Moreover, C4.5produces decision trees and rule sets, both often usedin text generation to implement mappings from func-tion features to forms?
Finally, C4.5 is both read-ily available, and is a benchmark learning algorithmthat has been extensively used in NLP applications,e.g.
(Litman, 1996; Mooney, 1996; Vander Lindenand Di Eugenio, 1996).As our dataset is small, the results we report arebased on cross-validation, which (Weiss and Ku-likowski, 1091) recommends as the best method toevaluate decision trees on datasets whose cardinalityis in the hundreds.
Data for learning should be di-vided into training and test sets; however, for smalldatasets this has the disadvantage that a sizable por-tion of the data is not available for learning.
Cross-validation obviates this problem by running the algo-rithm N times (N=10 is a typical value): in each run,(N~l)th of the data, randomly chosen, is used as thetraining set, and the remaining ~th  used as the test3We will discuss only decision trees here.set.
The error rate of a tree obtained by using thewhole dataset for training is then assumed to be theaverage rror rate on the test set over the N runs.Further, as C4.5 prunes the initial tree it obtains toavoid overfitting, it computes both actual and esti-mated error rates for the pruned tree; see (Quinlan,1993, Ch.
4) for details.
Thus, below we will reportthe average estimated error rate on the test set, ascomputed by 10-fold cross-validation experiments.4.2 The  featuresEach data point in our dataset corresponds to acore:contributor relation, and is characterized by thefollowing features, summarized in Table 2.Segment  S t ructure .
Three features capture theglobal structure of the segment in which the currentcore:contributor relation appears.?
(Con)Trib(utor)-pos(ition) captures the posi-tion of a particular contributor within the largersegment in which it occurs, and encodes thestructure of the segment in terms of how manycontributors precede and follow the core.
For ex-ample, contributor (l-D) in Figure 1 is labeledas BIA3-2after, as it is the second contributorfollowing the core in a segment with 1 contrib-utor before and 3 after the core.82of relation tl Total I # of cued relations IICore:Contributor 406 181Joints 64 19Clusters 310 276Total 780 476Table 1: Distributions of relations and cue occurrences\[I feature type feature dencriptionSegment ntructure Trib-pos relative position of contrib in segment tnumber of contribs before and after coreInten-structure intentional structure of segmentInfor-structure informational structure of segmentCore:contributor Inten-rel enable, convince, concederelation Info-rel 4 classes of about 30 distinct relationsSyn-rel independent sentences / segments,coordinated clauses, subordinated clausesAdjacency are core and contributor adjacent?Embedding Core-type segment, minimal unitTrib-type segment, minimal unitAbove / Below number of relations hierarchicallyabove / below current relationTable 2: Features?
/nten(tional)-structure indicates which contrib-utors in the segment bear the same intentionalrelations to the core.?
Infor(mationalJ-structure.
Similar to inten-tional structure, but applied to informationalrelations.Core :cont r ibutor  relat ion.
These features morespecifically characterize the current core:contributorrelation.?
lnten(tionalJ-rel(ation).
One of concede, con-vince, enable.?
Infor(maiional)-rel(ation).
About 30 informa-tional relations have been coded for.
However,as preliminary experiments showed that usingthem individually results in overfitting the data,we classify them according to the four classesproposed in (Moser, Moore, and Glendening,1996): causality, similarity, elaboration, tempo-ral.
Temporal relations only appear in clusters,thus not in the data we discuss in this paper.?
Syn(tactic)-rel(atiou).
Captures whether thecore and contributor are independent units (seg-ments or sentences); whether they are coordi-nated clauses; or which of the two is subordinateto the other.?
Adjacency.
Whether core and contributor areadjacent in linear order.Embedd ing .
These features capture segment em-bedding, Core-type and Trib-type qualitatively, andA bore/Below quantitatively.?
Core-type/(ConJTrib(utor)-type.
Whether thecore/the contributor is a segment, or a mini-mal unit (further subdivided into action, state,matriz).?
Above//Belozo encode the number of relations hi-erarchically above and below the current rela-tion.4.3 The exper imentsInitially, we performed learning on all 406 instancesof core:contributor relations.
We quickly determinedthat this approach would not lead to useful decisiontrees.
First, the trees we obtained were extremelycomplex (at least 50 nodes).
Second, some of the sub-trees corresponded to clearly identifiable subclassesof the data, such as relations with an implicit core,which suggested that we should apply learning tothese independently identifiable subclasses.
Thus,we subdivided the data into three subsets:?
Core/: core:contributor relations with the corein first position?
Core~: core:contributor relations with the corein second position?
Impl(icit)-core: core:contributor relations withan implicit coreWhile this has the disadvantage of smaller trainingsets, the trees we obtain are more manageable andmore meaningful.
Table 3 summarizes the cardinal-ity of these sets, and the frequencies of cue occur-rence.8311 O t set II # of Z tio s I # of c ed reZatio s IICorel 127Core2 155Impl-core 12452100(on Trib: 43) (on Core: 57)29II Total II 406 I 181Table 3: Distributions of relations and cue occurrencesWe ran four sets of experiments.
In three of themwe predict cue occurrence and in one cue placement.
44.3.1 Cue  Occur renceTable 4 summarizes our main results concerningcue occurrence, and includes the error rates asso-ciated with different feature sets.
We adopt Lit-man's approach (1906) to determine whether two er-ror rates El and ?2 are significantly different.
Wecompute 05% confidence intervals for the two errorrates using a t-test.
?1 is significantly better than?~ if the upper bound of the 95% confidence inter-val for ?1 is lower than the lower bound of the 95%confidence interval for g2-~For each set of experiments, we report the following:1.
A baseline measure obtained by choosing themajority class.
E.g., for Corel 58.9% of the re-lations are not cued; thus, by deciding to neverinclude a cue, one would be wrong 41.1% of thetimes.2.
The best individual features whose predictivepower is better than the baseline: as Table 4makes apparent, individual features do not havemuch predictive power.
For neither Gorcl norImpl-core does any individual feature performbetter than the baseline, and for Core~ only onefeature is sufficiently predictive.3.
(One of) the best induced tree(s).
For each tree,we list the number of nodes, and up to six of thefeatures that appear highest in the tree, withtheir levels of embedding.
5 Figure 2 shows thetree for Core~ (space constraints prevent us fromincluding figures for each tree).
In the figure,the numbers in parentheses indicate the numberof cases correctly covered by the leaf, and thenumber of expected errors at that leaf.Learning turns out to be most useful for Corel,where the error reduction (as percentage) from base-line to the upper bound of the best result is 32%;~AII our experiments are run with groupin 9turned on,so that C4.5 groups values together ather than creatinga branch per value.
The latter choice always results intrees overfitted to the data in our domain.
Using classesof informational relations, rather than individual infor-mational relations, constitutes a sort of a priori grouping.SThe trees that C4.5 generates are right-branching, sothis description is fairly adequate.error reduction is 19% for Core2 and only 3% forImpl- core.The best tree was obtained partly by informedchoice, partly by trial and error.
Automatically try-ing out all the 211 -- 2048 subsets of features wouldbe possible, but it would require manual examina-tion of about 2,000 sets of results, a daunting task.Thus, for each dataset wc considered only the follow-ing subsets of features.1.
All features.
This always results in C4.5 select-ing a few features (from 3 to 7) for the final tree.2.
Subsets built out of the 2 to 4 attributes appear-ing highest in the tree obtained by running C4.5on all features.3.
In Table 2, three features -- Trib-pos, In~e~-struck, Infor-s~ruct- concern segment struc-ture, eight do not.
We constructed three subsetsby always including the eight features that donot concern segment structure, and adding oneof those that does.
The trees obtained by includ-ing Trib-pos, I~tert-struc~, Infor-struc~ at thesame time are in general more complex, and notsignificantly better than other trees obtained byincluding only one of these three features.
Weattribute this to the fact that these features en-code partly overlapping information.Finally, the best tree was obtained as follows.
Webuild the set of trees that are statistically equivalentto the tree with the best error rate (i.e., with thelowest error rate upper bound).
Among these trees,we choose the one that we deem the most perspicuousin terms of features and of complexity.
Namely, wepick the simplest tree with Trib-Pos as the root ifone exists, otherwise the simplest tree.
Trees thathave Trib-Pos as the root are the most useful fortext generation, because, given a complex segment,Trib-Pos is the only attribute that unambiguouslyidentifies a specific contributor.Our results make apparent hat the structure ofsegments plays a fundamental role in determiningcue occurrence.
One of the three features concerningsegment structure (Trib-Pos, Inten-Structure, Infor-StrucZure) appears as the root or just below the rootin all trees in Table 4; more importantly, this sameconfiguration occurs in all trees equivalent to the besttree (even if the specific feature encoding segmentstructure may change).
The level of embedding in a84Core l Core2 Impl-coreBaseline 41.1 35.4 23.5Best features 0 Info-rel: 33.44-0.94 OBest tree 25.64-1.24 (I5)O. Trlb-pos1.
Tril>-type2.
Syn-rel3.
C0re-type4.
Above5.
Inten-rel27.44-1.28 (18)O. Tr ib-PosI.
Inten-rel2.
Info-rel3.
Above4.
Core-type5.
Below22.1+0.57 (10)O. Core-type1.
Infor-struct2.
Inten-relTable 4: Summary of learning resultsTr ib  POS } { B 1A0- I prc.B l A 1-1 prc.B 1A2-1 pre.B 1A3- I pre.
{B IA , - I  pre.
/ ~ _ 8 1 ) p ~  B2A0-  I pre.B2A0-2pre.B2A2.2pr?i ~ B2A I- 1 pre.B2A 1-2pr*2B3A0-3pre { B21A2.
~ N .
~ .
~  B3A0-1P rc 'B3A0-2prc  }(4/I.2)No-Cue Cue \[ Intcn Rcl J{causal.
elaboration} //\[ ,,,,o~o }Cue \[ Core Type ){ mat .
.
{ ac t ion  )\[ ae~ow ) No-Cu~Cue \[ T r ib  Pos \] {B IA l - lp re .B1A2-1prc .
{B IA0-1 pre /  ~ B I A3-1pr?.
B2A0-  I pre.B2AO-2prc.B2A l - I prc.B2A 1-2pro \ B3A0-1 pre.B3A0-2pre } ( 16/5~/(15/3.3)Cue  No-Cue{cneb'c} / ~ { .... i .......... d}(70/ I  2.7)\[ Int-o Rel  J Cue{ sioailarity }~ /I 2 ,No-Cue{ segment  }(T .b  Pos J{B1A0-1pre , / /  \ \ [B IA l - lp re .B lA2-1pr?
.B2A0-2pre } / B 1A3- I prc .B2A0-  I pro.B2A 1 - I pre.B2A 1-2pre (1915.8, ~Zr  B3A0- I prc.B3A0=2prc }(713 3)No-Cue CueFigure 2: Decision tree for Core2 - -  occurrencesegment, as encoded by Core-type, Trib-type, Aboveand Below also figures prominently.InLen-rel appears in all trees, confirming the in-tuit ion that the speaker's purpose affects cue occur-rence.
More specifically, in Figure 2, Inten-reldistin-guishes two different speaker purposes, convince andenable.
The same split occurs in some of the besttrees induced on Core1, with the same outcome: i.e.,convince directly correlates with the occurrence of acue, whereas for enable other features must be takeninto account.
6 Informational relations do not appearas often as intentional relations; their discriminatorypower seems more relevant for clusters.
Preliminaryewe can't draw any conclusions concerning concede,as there are only 24 occurrences of concede out of 406core:contributor relations.experiments show that cue occurrence in clusters de-pends only on informational nd syntactic relations.Finally, Adjacency does not seem to play any sub-stantial role.4.3.2 Cue  P lacementWhile cue occurrence and placement are interre-lated problems, we performed learning on them sep-arately.
First, the issue of placement arises only inthe case of Core~; for Core1, cues only occur on thecontributor.
Second, we attempted experiments onCore2 that discriminated between occurrence andplacement at the same time, and the derived treeswere complex and not perspicuous.
Thus, we ran anexperiment on the 100 cued relations from Core~ toinvestigate which factors affect placing the cue on thecontributor in first position or on the core in second;85Baseline 43%Best features Syn-reh 24.1:t:0.69Trib-pos: 40+0.88Best tree 20.6+0.97 (5)O. Syn-rcl1.
Trib-posTable 5: Cue placement on Core212d: Ttab depends on Core i?
: Core and Tab are independent clauses21d: Core depends on Tab cc.cp.ct: Core and Tnb are coordinaledphrases"N~d .
: ,:c ,=p ,:, I { izd}  . "
. "
."
.,26,'2.
VCue-on-Tr ib  \[ Tr ib-PoshB/AO71Pre.~'B.
I A 1.~ I Pro' ~ { B2AO-Iofe B2AI-IprcCue-on-Core  Cue~on-Tr ibFigure 3: Decision tree for Core~- -  placementsee Table 5.We ran the same trials discussed above on thisdataset.
In this case, the best tree - -  see Figure 3- -  results from combining the two best individualfeatures, and reduces the error rate by 50%.
Themost discriminant feature turns out to be the syn-tactic relation between the contributor and the core.However, segment structure still plays an importantrole, via Trib-pos.While the importance of S~ln-rel for placementseems clear, its role concerning occurrence requiresfurther exploration.
It is interesting to note that thetree induced on Gorel - -  the only case in which Syn-rel is relevant for occurrence - -  indudes the same dis-tinction as in Figure 3: namely, if the contributor de-pends on the core, the contributor must be marked,otherwise other features have to be taken into ac-count.
Scott and de Souza (1990) point out that"there is a strong correlation between the syntacticspecification of a complex sentence and its perceivedrhetorical structure."
It seems that certain syntacticstructures function as a cue.5 Discuss ion  and  Conc lus ionsWe have presented the results of machine learning ex-periments concerning cue occurrence and placement.As (Litman, 1996) observes, this sort of empiricalwork supports the utility of machine learning tech-niques applied to coded corpora.
As our study shows,individual features have no predictive power for cueoccurrence.
Moreover, it is hard to see how the bestcombination of individual features could be found bymanual inspection.Our results also provide guidance for those build-ing text generation systems.
This study clearly in-dicates that segment structure, most notably theordering of core and contributor, is crucial for de-termining cuc occurrence.
Recall that it was onlyby considering Corel and Core~ relations in distinctdatasets that we were able to obtain perspicuous de-cision trees that signifcantly reduce the error rate.This indicates that the representations producedby discourse planners should distinguish those ele-ments that constitute the core of each discourse seg-ment, in addition to representing the hierarchicalstructure of segments.
Note that the notion of coreis related to the notions of nucleus in RST, intendedeffect in (Young and Moore, 1994), and of point ofa move in (Elhadad and McKeown,  1990), and thattext generators representing these notions exist.Moreover, in order to use the decision trees derivedhere, decisions about whether or not to make the coreexplicit and how to order the core and contributor(s)must be made before deciding cue occurrence, e.g.,by exploiting other factors such as focus (McKeown,1985) and a discourse history.Once decisions about core:contributor orderingand cuc occurrence have been made, a generatormust still determine where to place cues and se-lect appropriate Icxical items.
A major focus ofour future research is to explore the relationship be-tween the selection and placement decisions.
Else-where, we have found that particular lexical itemstend to have a preferred location, defined in terms offunctional (i.e., core or contributor) and linear (i.e.,first or second relatum) criteria (Moser and Moore,1997).
Thus, if a generator uses decision trees suchas the one shown in Figure 3 to determine where acuc should bc placed, it can then select an appro-priate cue from those that can mark the given in-tentional / informational relations, and are usuallyplaced in that functional-linear location.
To evaluatethis strategy, we must do further work to understandwhether there are important distinctions among cues(e.g., so, because) apart from their different preferredlocations.
The work of Elhadad (1990) and Knott(1996) will help in answering this question.Future work comprises further probing into ma-chine learning techniques, in particular investigatingwhether other learning algorithms are more appro-priate for our problem (Mooney, 1996), especially al-gorithms that take into account some a priori knowl-edge about features and their dependencies.AcknowledgementsThis research is supported by the Office of NavalResearch, Cognitive and Neural Sciences Division(Grants N00014-91-J-1694 and N00014-93-I-0812).Thanks to Megan Moser for her prior work on thisproject and for comments on this paper; to ErinGlendening and Liina Pylkkanen for their coding ef-forts; to Haiqin Wang for running many experiments;to Giuseppe Carenini and Stefll Briininghaus for dis-cussions about machine learning.86ReferencesCohen, Robin.
1984.
A computational theory of thefunction of clue words in argument understand-ing.
In Proceedings of COLINGS~, pages 251-258,Stanford, CA.Elhadad, Michael and Kathleen McKeown.
1990.Generating connectives.
In Proceedings of COL-INGgO, pages 97-101, Helsinki, Finland.Goldman, Susan R. 1988.
The role of sequencemarkers in reading and recall: Comparison of na-tive and normative nglish speakers.
Technical re-port, University of California, Santa Barbara.Grosz, Barbara J. and Candace L. Sidner.
1986.
At-tention, intention, and the structure of discourse.Computational Linguistics, 12(3):175-204.Hobbs, Jerry R. 1985.
On the coherence and struc-ture of discourse.
Technical Report CSLI-85-37,Center for the Study of Language and Informa-tion, Stanford University.Knott, Alistair.
1996.
A Data-Driver, methodologyfor motivating a set of coherence relations.
Ph.D.thesis, University of Edinburgh.Litman, Diane J.
1996.
Cue phrase classificationusing machine learning.
Journal of Artificial In-telligence Research, 5:53-94.Litman, Diane J. and James F. Allen.
1987.
Aplan recognition model for subdialogues in conver-sations.
Cognitive Science, 11:163-200.Mann, William C. and Sandra A. Thompson.
1986.Relational propositions in discourse.
DiscourseProcesses, 9:57-90.Mann, William C. and Sandra A. Thompson.1988.
Rhetorical Structure Theory: Towards afunctional theory of text organization.
TEXT,8(3):243-281.McKeown, Kathleen R. 1985.
Tezt Generation: Us-ing Discourse Strategies and Focus Constraints toGenerate Natural Language Tezt.
Cambridge Uni-versity Press, Cambridge, England.McKeown, Kathleen R. and Michael Elhadad.
1991.A contrastive valuation of functional unificationgrammar for surface language generation: A casestudy in the choice of connectives.
In C. L. Paris,W.
R. Swartout, and W. C. Mann, eds., Natu-ral Language Generation in Artificial Intelligenceand Computational Linguistics.
Kluwer AcademicPublishers, Boston, pages 351-396.Millis, Keith, Arthur Graesser, and Karl Haberlandt.1993.
The impact of connectives on the memoryfor expository text.
Applied Cognitive Psychology,7:317-339.Mooney, Raymond J.
1996.
Comparative xperi-ments on disambiguating word senses: An illus-tration of the role of bias in machine learning.
InConference on Empirical Methods in Natural Lan-guage Processing.Moser, Megan and Johanna D. Moore.
1995.
In-vestigating cue selection and placement in tutorialdiscourse.
In Proceedings of ACLgS, pages 130-135, Boston, MA.Moser, Megan and Johanna D. Moore.
1997.
A cor-pus analysis of discourse cues and relational dis-course structure.
Submitted for publication.Moser, Megan, Johanna D. Moore, and Erin Glen-dening.
1996.
Instructions for Coding Explana-tions: Identifying Segments, Relations and Mini-real Units.
Technical Report 96-17, University ofPittsburgh, Department of Computer Science.Quinlan, J. Ross.
1993.
C~.5: Programs for MachineLearning.
Morgan Kaufmann.Reichman-Adar, Rachel.
1984.
Extendedperson-machine interface.
Artificial Intelligence,22(2):157-218.RSsner, Dietmar and Manfred Stede.
1992.
Cus-tomizing RST for the automatic production oftechnical manuals.
In R. Dale, E. Hovy, D. RSsner,and O.
Stock, eds., 6th International Workshopor* Natural Language Generation, Springer-Verlag,Berlin, pages 199-215.Schiffrin, Deborah.
1987.
Discourse Markers.
Cam-bridge University Press, New York.Scott, Donia and Clarisse Sieckenius de Souza.
1990.Getting the message across in RST-based text gen-eration.
In R. Dale, C. Mellish, and M.
Zock,eds., Current Research in Natural Language Gen-eration.
Academic Press, New York, pages 47-73.Siegel, Eric V. and Kathleen R. McKeown.
1994.Emergent linguistic rules from inducing decisiontrees: Disambiguating discourse clue words.
InProceedings of AAAI94, pages 820-826.Vander Linden, Keith and Barbara Di Eugenio.1996.
Learning micro-planning rules for preven-tative expressions.
In 8th International Workshopon Natural Language Generation, Sussex, UK.Vander Linden, Keith and James H. Martin.
1995.Expressing rhetorical relations in instructionaltext: A case study of the purpose relation.
Com-putational Linguistics, 21(1):29-58.Weiss, Sholom M. and Casimir Kulikowski.
1991.Computer Systems that learn: classification andprediction methods from statistics, neural nets,machine learning, and ezpert systems.
MorganKaufmann.Young, R. Michael and Johanna D. Moore.
1994.DPOCL: A Principled Approach to DiscoursePlanning.
In 7th International Workshop on Natu-ral Language Generation, Kennebunkport, Maine.87
