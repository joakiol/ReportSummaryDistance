Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 60?65,Prague, June 2007. c?2007 Association for Computational LinguisticsMachine Learning with Semantic-Based Distances Between Sentences forTextual EntailmentDaniel Ferre?sTALP Research CenterSoftware DepartmentUniversitat Polite`cnica de Catalunyadferres@lsi.upc.eduHoracio Rodr?
?guezTALP Research CenterSoftware DepartmentUniversitat Polite`cnica de Catalunyahoracio@lsi.upc.eduAbstractThis paper describes our experiments onTextual Entailment in the context of theThird Pascal Recognising Textual Entail-ment (RTE-3) Evaluation Challenge.
Oursystem uses a Machine Learning approachwith Support Vector Machines and Ad-aBoost to deal with the RTE challenge.
Weperform a lexical, syntactic, and semanticanalysis of the entailment pairs .
From thisinformation we compute a set of semantic-based distances between sentences.
The re-sults look promising specially for the QA en-tailment task.1 IntroductionThis paper describes our participation in the RTE-3 challenge.
It is our first attempt to RTE and wehave taken profit of an analysis of the approachesfollowed in previous challenges (see (Dagan et al,2005), and (Bar-Haim et al, 2006) for overviewsof RTE-1 and RTE-2).
Our approach, however, isbased on a set of semantic-based distance measuresbetween sentences used by our group in previouscontests in Question Answering (TREC 2004, see(Ferre?s et al, 2005), and CLEF 2004, see (Ferre?set al, 2004)) , and Automatic Summarization (DUC2006, see (Fuentes et al, 2006)).
Although the useof such measures (distance between question andsentences in passages candidates to contain the an-swer, distance between query and sentences candi-dates to be included in the summary, ...) is differentfor RTE task, our claim is that with some modifica-tions the approach can be useful in this new scenario.The organization of this paper is as follows.
Af-ter this introduction we present in section 2 a de-scription of the measures upon which our approachis built.
Section 3 describes in detail our proposal.Results are discussed in section 4.
Conclusions andfurther work is finally included in section 5.2 System DescriptionOur approach for computing distance measures be-tween sentences is based on the degree of overlap-ping between the semantic content of the two sen-tences.
Obtaining the semantic content implies adepth Linguistic Processing.
Upon this semanticrepresentation of the sentences several distance mea-sures are computed.
We next describe such issues.2.1 Linguistic ProcessingLinguistic Processing (LP) consists of a pipe ofgeneral purpose Natural Language (NL) processorsthat performs tokenization, morphologic tagging,lemmatization, Named Entities Recognition andClassification (NERC) with 4 basic classes (PER-SON, LOCATION, ORGANIZATION, and OTH-ERS), syntactic parsing and semantic labelling, withWordNet synsets, Magnini?s domain markers andEuroWordNet Top Concept Ontology labels.
TheSpear1 parser performs full parsing and robust de-tection of verbal predicate arguments.
The syntacticconstituent structure of each sentence (including thespecification of the head of each constituent) and therelations among constituents (subject, direct and in-direct object, modifiers) are obtained.
As a result1Spear.
http://www.lsi.upc.edu/?surdeanu/spear.html60of the performance of these processors each sen-tence is enriched with a lexical and syntactic lan-guage dependent representations.
A semantic lan-guage independent representation of the sentence(called environment) is obtained from these analy-ses (see (Ferre?s et al, 2005) for details).
The en-vironment is a semantic network like representationbuilt using a process to extract the semantic units(nodes) and the semantic relations (edges) that holdbetween the different tokens in the sentence.
Theseunits and relations belong to an ontology of about100 semantic classes (as person, city, action, mag-nitude, etc.)
and 25 relations (mostly binary) be-tween them (e.g.
time of event, actor of action, lo-cation of event, etc.).
Both classes and relations arerelated by taxonomic links (see (Ferre?s et al, 2005)for details) allowing inheritance.
Consider, for in-stance, the sentence ?Romano Prodi 1 is 2 the 3prime 4 minister 5 of 6 Italy 7?.
The following envi-ronment is built:i en proper person(1), entity has quality(2),entity(5), i en country(7), quality(4),which entity(2,1), which quality(2,5), mod(5,7),mod(5,4).2.2 Semantic-Based Distance MeasuresWe transform each environment into a labelled di-rected graph representation with nodes assigned topositions in the sentence, labelled with the corre-sponding token, and edges to predicates (a dummynode, 0, is used for representing unary predicates).Only unary (e.g.
entity(5) in Figure 1) and binary(e.g.
in Figure 2 which quality(2,5)) predicates areused.
Over this representation a rich variety oflexico-semantic proximity measures between sen-tences have been built.
Each measure combines twocomponents:?
A lexical component that considers the set ofcommon tokens occurring in both sentences.The size of this set and the strength of the com-patibility links between its members are usedfor defining the measure.
A flexible way ofmeasuring token-level compatibility has beenset ranging from word-form identity, lemmaidentity, overlapping of WordNet synsets, ap-proximate string matching between Named En-tities etc.
For instance, ?Romano Prodi?
is lex-ically compatible with ?R.
Prodi?
with a scoreof 0.5 and with ?Prodi?
with a score of 0.41.?Italy?
and ?Italian?
are also compatible withscore 0.7.
This component defines a set of (par-tial) weighted mapping between the tokens ofthe two sentences that will be used as anchorsin the next component.?
A semantic component computed over the sub-graphs corresponding to the set of lexicallycompatible nodes (anchors).
Four differentmeasures have been defined:?
Strict overlapping of unary predicates.?
Strict overlapping of binary predicates.?
Loose overlapping of unary predicates.?
Loose overlapping of binary predicates.The loose versions allow a relaxed match-ing of predicates by climbing up in the ontol-ogy of predicates (e.g.
provided that A and Bare lexically compatible, i en city(A) can matchi en proper place(B), i en proper named entity(B),location(B) or entity(B)) 2.
Obviously, loose over-lapping implies a penalty on the score that dependson the length of the path between the two predicatesand their informative content.Romano Prodi1 is2 prime4 minister5 Italy70i_en_proper_person i_en_countryentity_has_qualitywhich_qualitywhich_entity modmodquality entityFigure 1: Example of an environment of a sentence.2The ontology contains relations as i en cityisa i en proper place, i en proper place isai en proper named entity, proper place isa location,i en proper named entity isa entity, location isa entity613 System ArchitectureWe have adapted the set of measures described be-fore for RTE in the following way:1.
We follow a Machine Learning (ML) approachfor building a classifier to perform the RTEtask.
In previous applications the way ofweighting and combining the different mea-sures was based on a crude optimization usinga development corpus.2.
We extract a more complex set of features fordescribing the semantic content of the Text (T)and the Hypothesis (H) as well as the set of se-mantic measures between them.
Table 1 con-tains a brief summary of the features used.3.
We perform minor modifications on the token-level compatibility measures for dealing withthe asymmetry of the entailment relation (basi-cally using the hyponymy and the verbal entail-ment relations of WordNet)4.
We add three new task-specific features (seeTable 1)The overall architecture of the system is depictedin Figure 2.
As usual in ML, the system proceeds intwo phases, learning and classification.
The left sideof the figure shows the learning process and the rightpart the classification process.
The set of examples(tuples H, T) is first processed, in both phases, by LPfor obtaining a semantic representation of the tuple(Hsem and Tsem).
From this representation a Fea-ture Extraction component extracts a set of features.This set is used in the learning phase for getting aclassifier that is applied to the set of features of thetest, during the classification phase, in order to ob-tain the answer.4 ExperimentsBefore the submission we have performed a set ofexperiments in order to choose the Machine Learn-ing algorithms and the training sets to apply in thefinal submission.HTraining setT HTest setTLinguistic ProcessingLinguistic ProcessingHsTraining set (sem)TsFeature Extraction Feature ExtractionHsTest set (sem)TsFeatures FeaturesMachine Learning ClassifierAnswersFigure 2: System Architecture.4.1 Machine Learning ExperimentsWe used the WEKA3 ML platform (Witten andFrank, 2005) to perform the experiments.
We tested9 different ML algorithms: AdaBoostM1, BayesNetworks, Logistic Regression, MultiBoostAB,Naive Bayes, RBF Network, LogitBoost (Simple Lo-gistic in WEKA), Support Vector Machines (SMO inWEKA), and Voted Perceptron.
We used the previ-ous corpora of the RTE Challenge (RTE-1 and RTE-2) and the RTE-3 development test.
A filtering pro-cess has been applied removing pairs with more thantwo sentences in the text or hypothesis, resulting atotal of 3335 Textual Entailment (TE) pairs.
The re-sults over 10-fold-Cross-Validation using a data setcomposed by RTE-1, RTE-2, and RTE-3 develop-ment set are shown in Table 2.The results shown that AdaBoost, LogitBoost, andSVM obtain the best results.
Then we selected Ad-aBoost and SVM to perform the classification of theRTE-3 test set.
The SVM algorithm tries to computethe hyperplane that best separates the set of trainingexamples (the hyperplane with maximum margin)(Vapnik, 1995).
On the other hand, AdaBoost com-3WEKA.
http://www.cs.waikato.ac.nz/ml/weka/62Features #features Descriptionsemantic content of T 12 #locations, #persons, #dates, #actions, ...semantic content of H 12 ...intersection of T and H 12 ...length of intersectionscore of intersectionStrict overlapping of unary predicates 5 ratio of intersection related to shortest envratio of intersection related to longest envratio of intersection related to both (union of)Strict overlapping of binary predicates 5 .
.
.Loose overlapping of unary predicates 5 .
.
.Loose overlapping of binary predicates 5 ...Verbal entailment (WordNet) 1 V1 ?
T, V2 ?
H, such that V1 verbal entails V2Antonymy 1 A1 ?
T, A2 ?
H, such that A1 and A2 are antonyms andno token compatible with A2#occurs in H Negation 1 Difference between # negation tokens in H and TTable 1: Features used for classification with Machine Learning algorithms.Algorithm #correct AccuracyAdaBoostM1 1989 59.6402BayesNet 1895 56.8216Logistic 1951 58.5007MultiBoostAB 1959 58.7406NaiveBayes 1911 57.3013RBFNetwork 1853 55.5622LogitBoost 1972 59.1304SVM 1972 59.1304VotedPerceptron 1969 59.0405Table 2: Results over 10-fold-Cross-Validation us-ing a filtered data set composed by RTE-1, RTE-2,and RTE-3 (a total of 3335 entailment pairs).bines a set of weak classifiers into a strong one us-ing lineal combination (Freund and Schapire, 1996).The idea is combining many moderately accuraterules into a highly accurate prediction rule.
A weaklearning algorithm is used to find the weak rules.4.2 Training Set ExperimentsWe designed two experiments in order to decide thebest training set to apply in the RTE-3 challenge.
Weperformed an experiment using RTE-1 and RTE-2data sets as a training set and the RTE-3 develop-ment set filtered (541 TE pairs) as a test set.
In thisexperiment AdaBoost and SVM obtained accuraciesof 0.6672 and 0.6396 respectively (see results in Ta-ble 3.
We performed the same experiment joiningthe Answer Validation Exercise4 (AVE) 2006 En-glish data set (Pen?as et al, 2006) and the MicrosoftResearch Paraphrase Corpus5 (MSRPC) (Dolan etal., 2004) to the previous corpora (RTE-1 and RTE-2) resulting a total of 8585 entailment pairs filteringpairs with a text or a hypothesis with more than 1sentence.
In our approach we considered that para-phrases were bidirectional entailments.
The para-phrases of the MSRPC have been used as textual en-tailments in only one direction: the first sentence inthe paraphrase has been considered the hypothesisand the second one has been considered the text.Using the second corpus for training and the RTE-3 development set as test set resulted in a notabledegradation of accuracy (see Table 3).AccuracyAlgorithm Corpus A Corpus BAdaBoost 66.72% 53.78%SVM 63.95% 59.88%Table 3: Results over the RTE-3 development setfiltered (541 TE pairs) using as training set corpus A(RTE-1 and RTE-2) and corpus B (RTE-1, RTE-2,MSRPC, and AVE2006 English)Finally, we performed a set of experiments to de-tect the contribution of the different features used forMachine Learning.
These experiments revealed that4AVE.
http://nlp.uned.es/QA/AVE5MSRPC.
http://research.microsoft.com/nlp/msr_paraphrase.htm63the three most relevant features were: Strict overlap-ping of unary predicates, Semantic content of Hy-pothesis, and Loose overlapping of unary predicates.4.3 Official ResultsOur official results at RTE-3 Challenge are shownin Table 4.
We submitted two experiments: the firstone with AdaBoost (run1) and the second one withSVM (run2).
Training data set for final experimentswere corpus: RTE-1 (development and test), RTE-2 (development and test), and RTE-3 development.The test set was the RTE-3 test set without filteringthe entailments (text or hypothesis) with more thanone sentence.
In this case we joined multiple sen-tences in a unique sentence that has been processedby the LP component.We obtained accuracies of 0.6062 and 0.6150.
Inthe QA task we obtained the best per-task resultswith accuracies of 0.7450 and 0.7000 with AdaBoostand SVM respectively.AccuracyTask run1 run2AdaBoost SVMIE 0.4350 0.4950IR 0.6950 0.6800QA 0.7450 0.7000SUM 0.5500 0.5850Overall 0.6062 0.6150Table 4: RTE-3 official results.5 Conclusions and Further WorkThis paper describes our experiments on Textual En-tailment in the context of the Third Pascal Recog-nising Textual Entailment (RTE-3) Evaluation Chal-lenge.
Our approach uses Machine Learning al-gorithms (SVM and AdaBoost) with semantic-baseddistance measures between sentences.
Although fur-ther analysis of the results is in process, we observedthat our official per-task results at RTE-3 show a dif-ferent distribution compared with the global resultsof all system at RTE-2 challenge.
The RTE-2 per-task analysis showed that most of the systems scoredhigher in accuracy in the multidocument summariza-tion (SUM) task while in our system this measure islow.
Our system at RTE-3 challenge scored higherin the QA and IR tasks with accuracies of 0.7450and 0.6950 respectively in the first run.ReferencesRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In Proceedings of the Second PAS-CAL Challenges Workshop on Recognising TextualEntailment.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailment chal-lenge.
In Joaquin Quin?onero Candela, Ido Dagan,Bernardo Magnini, and Florence d?Alche?
Buc, editors,MLCW, volume 3944 of Lecture Notes in ComputerScience, pages 177?190.
Springer.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In COL-ING ?04: Proceedings of the 20th international con-ference on Computational Linguistics, page 350, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Daniel Ferre?s, Samir Kanaan, Alicia Ageno, EdgarGonza?lez, Horacio Rodr?
?guez, Mihai Surdeanu, andJordi Turmo.
2004.
The TALP-QA System for Span-ish at CLEF 2004: Structural and Hierarchical Relax-ing of Semantic Constraints.
In Carol Peters, PaulClough, Julio Gonzalo, Gareth J. F. Jones, MichaelKluck, and Bernardo Magnini, editors, CLEF, volume3491 of Lecture Notes in Computer Science, pages557?568.
Springer.Daniel Ferre?s, Samir Kanaan, Edgar Gonza?lez, AliciaAgeno, Horacio Rodr?
?guez, Mihai Surdeanu, and JordiTurmo.
2005.
TALP-QA System at TREC 2004:Structural and Hierarchical Relaxation Over Seman-tic Constraints.
In Proceedings of the Text RetrievalConference (TREC-2004).Yoav Freund and Robert E. Schapire.
1996.
Experimentswith a new boosting algorithm.
In International Con-ference on Machine Learning, pages 148?156.Maria Fuentes, Horacio Rodr?
?guez, Jordi Turmo, andDaniel Ferre?s.
2006.
Femsum at duc 2006: Semantic-based approach integrated in a flexible eclectic mul-titask summarizer architecture.
In Proceedings ofthe Document Understanding Conference 2006 (DUC2006).
HLT-NAACL 2006 Workshop., New York City,NY, USA, June.Anselmo Pen?as, ?Alvaro Rodrigo, Valent?
?n Sama, and Fe-lisa Verdejo.
2006.
Overview of the answer val-idation exercise 2006.
In Working Notes for the64CLEF 2006 Workshop.
ISBN: 2-912335-23-x, Ali-cante, Spain, September.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc., NewYork, NY, USA.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques, SecondEdition (Morgan Kaufmann Series in Data Manage-ment Systems).
Morgan Kaufmann, June.AcknowledgmentsThis work has been supported by the Spanish Re-search Dept.
(TEXT-MESS, TIN2006-15265-C06-05).
Daniel Ferre?s is supported by a UPC-Recercagrant from Universitat Polite`cnica de Catalunya(UPC).
TALP Research Center is recognized asa Quality Research Group (2001 SGR 00254) byDURSI, the Research Department of the CatalanGovernment.65
