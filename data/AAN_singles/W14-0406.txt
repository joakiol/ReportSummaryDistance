Felix Bildhauer & Roland Sch?fer (eds.
), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 36?43,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsThe PAIS`A Corpus of Italian Web TextsVerena Lyding?verena.lyding@eurac.eduEgon Stemle?egon.stemle@eurac.eduClaudia Borghetti?claudia.borghetti@unibo.itMarco Brunello?marcobrunello84@gmail.comSara Castagnoli?s.castagnoli@unibo.itFelice Dell?Orletta?felice.dellorletta@ilc.cnr.itHenrik Dittmann?henrik.dittmann@bordet.beAlessandro Lenci?alessandro.lenci@ling.unipi.itVito Pirrelli?vito.pirrelli@ilc.cnr.itAbstractPAIS`A is a Creative Commons licensed,large web corpus of contemporary Italian.We describe the design, harvesting, andprocessing steps involved in its creation.1 IntroductionThis paper provides an overview of the PAIS`A cor-pus of Italian web texts and an introductory de-scription of the motivation, procedures and facili-ties for its creation and delivery.Developed within the PAIS`A project, the cor-pus is intended to meet the objective to help over-come the technological barriers that still preventweb users from making use of large quantities ofcontemporary Italian texts for language and cul-tural education, by creating a comprehensive andeasily accessible corpus resource of Italian.The initial motivation of the initiative stemmedfrom the awareness that any static repertoire ofdigital data, however carefully designed and de-veloped, is doomed to fast obsolescence, if con-tents are not freely available for public usage, con-tinuously updated and checked for quality, incre-mentally augmented with new texts and annota-tion metadata for intelligent indexing and brows-ing.
These requirements brought us to design aresource that was (1) freely available and freelyre-publishable, (2) comprehensively covering con-temporary common language and cultural contentand (3) enhanced with a rich set of automatically-annotated linguistic information to enable ad-vanced querying and retrieving of data.
On top?EURAC Research Bolzano/Bozen, IT?University of Bologna, IT?University of Leeds, UK?Institute of Computational Linguistics ?Antonio Zam-polli?
- CNR, IT?Institut Jules Bordet, BE?University of Pisa, ITof that, we set out to develop (4) a dedicated in-terface with a low entry barrier for different targetgroups.
The end result of this original plan repre-sents an unprecedented digital language resourcein the Italian scenario.The main novelty of the PAIS`A web corpus isthat it exclusively draws on Creative Commons li-censed data, provides advanced linguistic annota-tions with respect to corpora of comparable sizeand corpora of web data, and invests in a carefullydesigned query interface, targeted at different usergroups.
In particular, the integration of richly an-notated language content with an easily accessible,user-oriented interface makes PAIS`A a unique andflexible resource for language teaching.2 Related WorkThe world wide web, with its inexhaustibleamount of natural language data, has become anestablished source for efficiently building largecorpora (Kilgarriff and Grefenstette, 2003).
Toolsare available that make it convenient to bootstrapcorpora from the web based on mere seed termlists, such as the BootCaT toolkit (Baroni andBernardini, 2004).
The huge corpora created bythe WaCky project (Baroni et al., 2009) are an ex-ample of such an approach.A large number of papers have recently beenpublished on the harvesting, cleaning and pro-cessing of web corpora.1However, freely avail-able, large, contemporary, linguistically anno-tated, easily accessible web corpora are still miss-ing for many languages; but cf.
e.g.
(G?en?ereuxet al., 2012) and the Common Crawl Foundations(CCF) web crawl2.1cf.
the Special Interest Group of the Association forComputational Linguistics on Web as Corpus (SIGWAC)http://sigwac.org.uk/2CCF produces and maintains a repository of web crawldata that is openly accessible: http://commoncrawl.org/363 Corpus Composition3.1 Corpus designPAIS`A aimed at creating a comprehensive corpusresource of Italian web texts which adheres to thecriteria laid out in section 1.
For these criteria tobe fully met, we had to address a wide variety ofissues covering the entire life-cycle of a digital textresource, ranging from robust algorithms for webnavigation and harvesting, to adaptive annotationtools for advanced text indexing and querying anduser-friendly accessing and rendering online inter-faces customisable for different target groups.Initially, we targeted a size of 100M tokens, andplanned to automatically annotate the data withlemma, part-of-speech, structural dependency, andadvanced linguistic information, using and adapt-ing standard annotation tools (cf.
section 4).
In-tegration into a querying environment and a dedi-cated online interface were planned.3.2 LicensesA crucial point when planning to compile a cor-pus that is free to redistribute without encounter-ing legal copyright issues is to collect texts that arein the public domain or at least, have been madeavailable in a copyleft regime.
This is the casewhen the author of a certain document decided toshare some rights (copy and/or distribute, adaptetc.)
on her work with the public, in a way thatend users do not need to ask permission to the cre-ator/owner of the original work.
This is possibleby employing licenses other than the traditional?all right reserved?
copyright, i.e.
GNU, CreativeCommons etc., which found a wide use especiallyon the web.
Exploratory studies (Brunello, 2009)have shown that Creative Commons licenses arewidely employed throughout the web (at least onthe Italian webspace), enough to consider the pos-sibility to build a large corpus from the web ex-clusively made of documents released under suchlicenses.In particular, Creative Commons provides fivebasic ?baseline rights?
: Attribution (BY), ShareAlike (SA), Non Commercial (NC), No Deriva-tive Works (ND).
The licenses themselves arecomposed of at least Attribution (which can beused even alone) plus the other elements, al-lowing six different combinations:3(1) Attribu-tion (CC BY), (2) Attribution-NonCommercial3For detailed descriptions of each license see http://creativecommons.org/licenses/(CC BY-NC), (3) Attribution-ShareAlike (CC BY-SA), (4) Attribution-NoDerivs (CC BY-ND), (5)Attribution-NonCommercial-ShareAlike (CC BY-NC-SA), and (6) Attribution-NonCommercial-NoDerivs (CC BY-NC-ND).Some combinations are not possible becausecertain elements are not compatible, e.g.
ShareAlike and No Derivative Works.
For our purposeswe decided to discard documents released with thetwo licenses containing the No Derivative Worksoption, because our corpus is in fact a derivativework of collected documents.3.3 The final corpusThe corpus contains approximately 388,000 docu-ments from 1,067 different websites, for a total ofabout 250M tokens.
All documents contained inthe PAIS`A corpus date back to Sept./Oct.
2010.The documents come from several web sourceswhich, at the time of corpus collection, providedtheir content under Creative Commons license(see section 3.2 for details).
About 269,000 textsare from Wikimedia Foundation projects, withapproximately 263,300 pages from Wikipedia,2380 pages from Wikibooks, 1680 pages fromWikinews, 740 pages from Wikiversity, 410 pagesfrom Wikisource, and 390 Wikivoyage pages.The remaining 119,000 documents comefrom guide.supereva.it (ca.
19,000),italy.indymedia.org (ca.
10,000) andseveral blog services from more than another1,000 different sites (e.g.
www.tvblog.it(9,088 pages), www.motoblog.it (3,300),www.ecowebnews.it (3,220), andwww.webmasterpoint.org (3,138).Texts included in PAIS`A have an average lengthof 683 words, with the longest text4counting66,380 running tokens.
A non exhaustive list ofaverage text lengths by source type is provided intable 1 by way of illustration.The corpus has been annotated for lemma, part-of-speech and dependency information (see sec-tion 4.2 for details).
At the document level, thecorpus contains information on the URL of originand a set of descriptive statistics of the text, includ-ing text length, rate of advanced vocabulary, read-ability parameters, etc.
(see section 4.3).
Also,each document is marked with a unique identifier.4The European Constitution from wikisource.org:http://it.wikisource.org/wiki/Trattato_che_adotta_una_Costituzione_per_l?Europa37Document source Avg text lengthPAIS`A total 683 wordsWikipedia 693 wordsWikibooks 1844 wordsguide.supereva.it 378 wordsitaly.indymedia.it 1147 wordstvblog.it 1472 wordsmotoblog.it 421 wordsecowebnews.it 347 wordswebmasterpoint.org 332 wordsTable 1: Average text length by sourceThe annotated corpus adheres to the stan-dard CoNLL column-based format (Buchholz andMarsi, 2006), is encoded in UTF-8.4 Corpus Creation4.1 Collecting and cleaning web dataThe web pages for PAIS`A were selected in twoways: part of the corpus collection was madethrough CC-focused web crawling, and anotherpart through a targeted collection of documentsfrom specific websites.4.1.1 Seed-term based harvestingAt the time of corpus collection (2010), we usedthe BootCaT toolkit mainly because collectingURLs could be based on the public Yahoo!
searchAPI5, including the option to restrict search to CC-licensed pages (including the possibility to specifyeven the particular licenses).
Unfortunately, Ya-hoo!
discontinued the free availability of this API,and BootCaT?s remaining search engines do notprovide this feature.An earlier version of the corpus was collectedusing the tuple list originally employed to builditWaC6.
As we noticed that the use of this list, incombination with the restriction to CC, biased thefinal results (i.e.
specific websites occurred veryoften as top results) , we provided as input 50,000medium frequent seed terms from a basic Italianvocabulary list7, in order to get a wider distribu-tion of search queries, and, ultimately, of texts.As introduced in section 3.2, we restricted theselection not just to Creative Commons-licensed5http://developer.yahoo.com/boss/6http://wacky.sslmit.unibo.it/doku.php?id=seed_words_and_tuples7http://ppbm.paravia.it/dib_lemmario.phptexts, but specifically to those licenses allowingredistribution: namely, CC BY, CC BY-SA, CCBY-NC-SA, and CC BY-NC.Results were downloaded and automaticallycleaned with the KrdWrd system, an environmentfor the unified processing of web content (Stegerand Stemle, 2009).Wrongly CC-tagged pages were eliminated us-ing a black-list that had been manually populatedfollowing inspection of earlier corpus versions.4.1.2 TargetedIn September 2009, the Wikimedia Foundation de-cided to release the content of their wikis underCC BY-SA8, so we decided to download the largeand varied amount of texts made available throughthe Italian versions of these websites.
This wasdone using the Wikipedia Extractor9on officialdumps10of Wikipedia, Wikinews, Wikisource,Wikibooks, Wikiversity and Wikivoyage.4.2 Linguistic annotation and toolsadaptationThe corpus was automatically annotated withlemma, part-of-speech and dependency infor-mation, using state-of-the-art annotation toolsfor Italian.
Part-of-speech tagging was per-formed with the Part-Of-Speech tagger describedin Dell?Orletta (2009) and dependency-parsed bythe DeSR parser (Attardi et al., 2009), using Mul-tilayer Perceptron as the learning algorithm.
Thesystems used the ISST-TANL part-of-speech11and dependency tagsets12.
In particular, the pos-tagger achieves a performance of 96.34% andDeSR, trained on the ISST-TANL treebank con-sisting of articles from newspapers and period-icals, achieves a performance of 83.38% and87.71% in terms of LAS (labelled attachmentscore) and UAS (unlabelled attachment score) re-spectively, when tested on texts of the same type.However, since Gildea (2001), it is widely ac-knowledged that statistical NLP tools have a dropof accuracy when tested against corpora differingfrom the typology of texts on which they weretrained.
This also holds true for PAIS`A: it contains8Previously under GNU Free Documentation License.9http://medialab.di.unipi.it/wiki/Wikipedia_Extractor10http://dumps.wikimedia.org/11http://www.italianlp.it/docs/ISST-TANL-POStagset.pdf12http://www.italianlp.it/docs/ISST-TANL-DEPtagset.pdf38lexical and syntactic structures of non-canonicallanguages such as the language of social media,blogs, forum posts, consumer reviews, etc.
As re-ported in Petrov and McDonald (2012), there aremultiple reasons why parsing the web texts is dif-ficult: punctuation and capitalization are often in-consistent, there is a lexical shift due to increaseduse of slang and technical jargon, some syntacticconstructions are more frequent in web text thanin newswire, etc.In order to overcome this problem, two main ty-pologies of methods and techniques have been de-veloped: Self-training (McClosky et al., 2006) andActive Learning (Thompson et al., 1999).For the specific purpose of the NLP tools adap-tation to the Italian web texts, we adopted two dif-ferent strategies for the pos-tagger and the parser.For what concerns pos-tagging, we used an activelearning approach: given a subset of automaticallypos-tagged sentences of PAIS`A, we selected theones with the lowest likelihood, where the sen-tence likelihood was computed as the product ofthe probabilities of the assignments of the pos-tagger for all the tokens.
These sentences weremanually revised and added to the training corpusin order to build a new pos-tagger model incor-porating some new knowledge from the target do-main.For what concerns parsing, we used a self-training approach to domain adaptation describedin Dell?Orletta et al.
(2013), based on ULISSE(Dell?Orletta et al., 2011).
ULISSE is an unsu-pervised linguistically-driven algorithm to selectreliable parses from a collection of dependencyannotated texts.
It assigns to each dependencytree a score quantifying its reliability based on awide range of linguistic features.
After collect-ing statistics about selected features from a cor-pus of automatically parsed sentences, for eachnewly parsed sentence ULISSE computes a reli-ability score using the previously extracted featurestatistics.
From the top of the parses (ranked ac-cording to their reliability score) different pools ofparses were selected to be used for training.
Thenew training contains the original training set aswell as the new selected parses which include lex-ical and syntactic characteristics specific of the tar-get domain (Italian web texts).
The parser trainedon this new training set improves its performancewhen tested on the target domain.We used this domain adaptation approach forthe following three main reasons: a) it is unsuper-vised (i.e.
no need for manually annotated trainingdata); b) unlike the Active Learning approach usedfor pos-tagging, it does not need manual revisionof the automatically parsed samples to be used fortraining; c) it was previously tested on Italian textswith good results (Dell?Orletta et al., 2013).4.3 Readability analysis of corpus documentsFor each corpus document, we calculated severaltext statistics indicative of the linguistic complex-ity, or ?readability?
of a text.The applied measures include, (1) text length intokens, that is the number of tokens per text, (2)sentences per text, that is a sentence count, and (3)type-token ratio indicated as a percentage value.In addition, we calculated (4) the advanced vo-cabulary per text, that is a word count of the textvocabulary which is not part of the the basic Ital-ian vocabulary (?vocabolario di base?)
for writtentexts, as defined by De Mauro (1991)13, and (5)the Gulpease Index (?Indice Gulpease?)
(Lucisanoand Piemontese, 1988), which is a measure for thereadability of text that is based on frequency rela-tions between the number of sentences, words andletters of a text.All values are encoded as metadata for the cor-pus.
Via the PAIS`A online interface, they canbe employed for filtering documents and buildingsubcorpora.
This facility was implemented withthe principal target group of PAIS`A users in mind,as the selection of language examples accordingto their readability level is particularly relevant forlanguage learning and teaching.4.4 Attempts at text classification for genre,topic, and functionLack of information about the composition of cor-pora collected from the web using unsupervisedmethods is probably one of the major limitationsof current web corpora vis-`a-vis more traditional,carefully constructed corpora, most notably whenapplications to language teaching and learning areenvisaged.
This also holds true for PAIS`A, es-13The advanced vocabulary was calculated on the ba-sis of a word list consisting of De Mauro?s ?vocabolariofondamentale?
(http://it.wikipedia.org/wiki/Vocabolario_fondamentale) and ?vocabolariodi alto uso?
(http://it.wikipedia.org/wiki/Vocabolario_di_alto_uso), together with highfrequent function words not contained in those two lists.39pecially for the harvested14subcorpus that wasdownloaded as described in section 4.1.
We there-fore carried out some experiments with the ulti-mate aim to enrich the corpus with metadata abouttext genre, topic and function, using automatedtechniques.In order to gain some insights into the com-position of PAIS`A, we first conducted some man-ual investigations.
Drawing on existing literatureon web genres (e.g.
(Santini, 2005; Rehm et al.,2008; Santini et al., 2010)) and text classificationaccording to text function and topic (e.g.
(Sharoff,2006)), we developed a tentative three-fold taxon-omy to be used for text classification.
Followingfour cycles of sample manual annotation by threeannotators, categories were adjusted in order tobetter reflect the nature of PAIS`A?s web documents(cf.
(Sharoff, 2010) about differences between do-mains covered in the BNC and in the web-derivedukWaC).
Details about the taxonomy are providedin Borghetti et al.
(2011).
Then, we started tocross-check whether the devised taxonomy wasindeed appropriate to describe PAIS`A?s composi-tion by comparing its categories with data result-ing from the application of unsupervised methodsfor text classification.Interesting insights have emerged so far re-garding the topic category.
Following Sharoff(2010), we used topic modelling based on La-tent Dirichlet Allocation for the detection of top-ics: 20 clusters/topics were identified on the ba-sis of keywords (the number of clusters to re-trieve is a user-defined parameter) and projectedonto the manually defined taxonomy.
This re-vealed that most of the 20 automatically iden-tified topics could be reasonably matched toone of the 8 categories included in the tax-onomy; exceptions were represented by clus-ters characterised by proper nouns and gen-eral language words such bambino/uomo/famiglia(?child?/?man?/?family?)
or credere/sentire/sperare(?to believe?/?feel?/?hope?
), which may in fact beindicative of genres such as diary or personal com-ment (e.g.
personal blog).
Only one of the cate-gories originally included in the taxonomy ?
natu-ral sciences ?
was not represented in the clusters,which may indicate that there are few texts withinPAIS`A belonging to this domain.
One of the ma-14In fact, even the nature of the targeted texts is not pre-cisely defined: for instance, Wikipedia articles can actuallyencompass a variety of text types such as biographies, intro-ductions to academic theories etc.
(Santini et al., 2010, p. 15)jor advantages of topic models is that each corpusdocument can be associated ?
to varying degrees ?to several topics/clusters: if encoded as metadata,this information makes it possible not only to fil-ter texts according to their prevailing domain, butalso to represent the heterogeneous nature of manyweb documents.5 Corpus Access and Usage5.1 Corpus distributionThe PAIS`A corpus is distributed in two ways: it ismade available for download and it can be queriedvia its online interface.
For both cases, no restric-tions on its usage apply other than those definedby the Creative Commons BY-NC-SA license.
Forcorpus download, both the raw text version and theannotated corpus in CoNLL format are provided.The PAIS`A corpus together with all project-related information is accessible via the projectweb site at http://www.corpusitaliano.it5.2 Corpus interfaceThe creation of a dedicated open online interfacefor the PAIS`A corpus has been a declared primaryobjective of the project.The interface is aimed at providing a power-ful, effective and easy-to-employ tool for mak-ing full use of the resource, without having to gothrough downloading, installation or registrationprocedures.
It is targeted at different user groups,particularly language learners, teachers, and lin-guists.
As users of PAIS`A are expected to showvarying levels of proficiency in terms of languagecompetence, linguistic knowledge, and concern-ing the use of online search tools, the interfacehas been designed to provide four separate searchcomponents, implementing different query modes.Initially, the user is directed to a basic keywordsearch that adopts a ?Google-style?
search box.Single search terms, as well as multi-word combi-nations or sequences can be searched by insertingthem in a simple text box.The second component is an advanced graph-ical search form.
It provides elaborated searchoptions for querying linguistic annotation layersand allows for defining distances between searchterms as well as repetitions or optionally occurringterms.
Furthermore, the advanced search supportsregular expressions.The third component emulates a command-linesearch via the powerful CQP query language of40the Open Corpus Workbench (Evert and Hardie,2011).
It allows for complex search queries inCQP syntax that rely on linguistic annotation lay-ers as well as on metadata information.Finally, a filter interface is presented in a fourthcomponent.
It serves the purpose of retriev-ing full-text corpus documents based on keywordsearches as well as text statistics (see section 4.3).Like the CQP interface, the filter interface is alsosupporting the building of temporary subcorporafor subsequent querying.By default, search results are displayed asKWIC (KeyWord In Context) lines, centredaround the search expression.
Each search hit canbe expanded to its full sentence view.
In addition,the originating full text document can be accessedand its source URL is provided.Based on an interactive visualisation for depen-dency graphs (Culy et al., 2011) for each searchresult a graphical representations of dependencyrelations together with the sentence and associatedlemma and part-of-speech information can be gen-erated (see Figure 1).Figure 1: Dependency diagramTargeted at novice language learners of Italian,a filter for automatically restricting search resultsto sentences of limited complexity has been in-tegrated into each search component.
When ac-tivated, search results are automatically filteredbased on a combination of the complexity mea-sures introduced in section 4.3.5.3 Technical detailsThe PAIS`A online interface has been developed inseveral layers: in essence, it provides a front-endto the corpus as indexed in Open Corpus Work-bench (Evert and Hardie, 2011).
This corpusquery engine provides the fundamental search ca-pabilities through the CQP language.
Based onthe CWB/Perl API that is part of the Open CorpusWorkbench package, a web service has been de-veloped at EURAC which exposes a large part ofthe CQP language15through a RESTful API.16The four types of searches provided by the on-line interface are developed on top of this web ser-vice.
The user queries are translated into CQPqueries and passed to the web service.
In manycases, such as the free word order queries in thesimple and advanced search forms, more than oneCQP query is necessary to produce the desiredresult.
Other functionalities implemented in thislayer are the management of subcorpora and thefiltering by complexity.
The results returned bythe web service are then formatted and presentedto the user.The user interface as well as the mechanismsfor translation of queries from the web forms intoCQP have been developed server-side in PHP.The visualizations are implemented client-side inJavaScript and jQuery, the dependency graphsbased on the xLDD framework (Culy et al., 2011).5.4 Extraction of lexico-syntactic informationPAIS`A is currently used in the CombiNet project?Word Combinations in Italian ?
Theoretical anddescriptive analysis, computational models, lexi-cographic layout and creation of a dictionary?.17The project goal is to study the combinatory prop-erties of Italian words by developing advancedcomputational linguistics methods for extractingdistributional information from PAIS`A.In particular, CombiNet uses a pattern-basedapproach to extract a wide range of multiwordexpressions, such as phrasal lexemes, colloca-tions, and usual combinations.
POS n-gramsare automatically extracted from PAIS`A, and thenranked according to different types of associa-tion measures (e.g., pointwise mutual informa-tion, log-likelihood ratios, etc.).
Extending theLexIt methodology (Lenci et al., 2012), CombiNetalso extracts distributional profiles from the parsedlayer of PAIS`A, including the following types ofinformation:1. syntactic slots (subject, complements, modi-15To safeguard the system against malicious attacks, secu-rity measures had to be taken at several of the layers, whichunfortunately also make some of the more advanced CQP fea-tures inaccessible to the user.16Web services based on REST (Representational StateTransfer) principles employ standard concepts such as a URIand standard HTTP methods to provide an interface to func-tionalities on a remote host.173-year PRIN(2010/2011)-project, coordination by Raf-faele Simone ?
University of Rome Tre41fiers, etc.)
and subcategorization frames;2. lexical sets filling syntactic slots (e.g.
proto-typical subjects of a target verb);3. semantic classes describing selectional pref-erences of syntactic slots (e.g.
the direct obj.of mangiare/?to eat?
typically selects nounsreferring to food, while its subject selects an-imate nouns); semantic roles of predicates.The saliency and typicality of combinatory pat-terns are weighted by means of different statisti-cal indexes and the resulting profiles will be usedto define a distributional semantic classification ofItalian verbs, comparable to the one elaborated inthe VerbNet project (Kipper et al., 2008).6 EvaluationWe performed post-crawl evaluations on the data.For licensing, we analysed 200,534 pages thatwere originally collected for the PAIS`A corpus,and only 1,060 were identified as containing noCC license link (99.95% with CC mark-up).
Then,from 10,000 randomly selected non-CC-licensedItalian pages 15 were wrongly identified as CC li-censed containing CC mark-up (0.15% error).
Forlanguage identification we checked the harvestedcorpus part with the CLD2 toolkit18, and > 99%of the data was identified as Italian.The pos-tagger has been adapted to peculiari-ties of the PAIS`A web texts, by manually correct-ing sample annotation output and re-training thetagger accordingly.
Following the active learningapproach as described in section 4.2 we built a newpos-tagger model based on 40.000 manually re-vised tokens.
With the new model, we obtainedan improvement in accuracy of 1% on a test-setof 5000 tokens extracted from PAIS`A.
Final tag-ger accuracy reached 96.03%.7 Conclusion / Future WorkIn this paper we showed how a contemporary andfree language resource of Italian with linguisticannotations can be designed, implemented and de-veloped from the web and made available for dif-ferent types of language users.Future work will focus on enriching the cor-pus with metadata by means of automatic clas-sification techniques, so as to make a better as-sessment of corpus composition.
A multi-faceted18Compact Language Detection 2, http://code.google.com/p/cld2/approach combining linguistic features extractedfrom texts (content/function words ratio, sentencelength, word frequency, etc.)
and informationextracted from document URLs (e.g., tags like?wiki?, ?blog?)
might be particularly suitable forgenre and function annotation.Metadata annotation will enable more advancedapplications of the corpus for language teachingand learning purposes.
In this respect, existingexemplifications of the use of the PAIS`A inter-face for language learning and teaching (Lyding etal., 2013) could be followed by further pedagogi-cal proposals as well as empowered by dedicatedteaching guidelines for the exploitation of the cor-pus and its web interface in the class of Italian asa second language.In a more general perspective, we envisagea tighter integration between acquisition of newtexts, automated text annotation and developmentof lexical and language learning resources allow-ing even non-specialised users to carve out anddevelop their own language data.
This ambitiousgoal points in the direction of a fully-automatisedcontrol of the entire life-cycle of open-access Ital-ian language resources with a view to address anincreasingly wider range of potential demands.AcknowledgementsThe three years PAIS`A project19, concluded inJanuary 2013, received funding from the ItalianMinistry of Education, Universities and Research(MIUR)20, by the FIRB program (Fondo per gliInvestimenti della Ricerca di Base)21.ReferencesG.
Attardi, F. Dell?Orletta, M. Simi, and J. Turian.2009.
Accurate dependency parsing with a stackedmultilayer perceptron.
In Proc.
of Evalita?09, Eval-uation of NLP and Speech Tools for Italian, ReggioEmilia.M.
Baroni and S. Bernardini.
2004.
Bootcat: Boot-strapping corpora and terms from the web.
In Proc.of LREC 2004, pages 1313?1316.
ELDA.M.
Baroni, S. Bernardini, A. Ferraresi, andE.
Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed19An effort of four Italian research units: University ofBologna, CNR Pisa, University of Trento and EuropeanAcademy of Bolzano/Bozen.20http://www.istruzione.it/21http://hubmiur.pubblica.istruzione.it/web/ricerca/firb42web-crawled corpora.
Journal of LRE, 43(3):209?226.C.
Borghetti, S. Castagnoli, and M. Brunello.
2011.
Itesti del web: una proposta di classificazione sullabase del corpus pais`a.
In M. Cerruti, E. Corino,and C. Onesti, editors, Formale e informale.
La vari-azione di registro nella comunicazione elettronica.,pages 147?170.
Carocci, Roma.M.
Brunello.
2009.
The creation of free linguistic cor-pora from the web.
In I. Alegria, I. Leturia, andS.
Sharoff, editors, Proc.
of the Fifth Web as CorpusWorkshop (WAC5), pages 9?16.
Elhuyar Fundazioa.S.
Buchholz and E. Marsi.
2006.
CoNLL-X SharedTask on Multilingual Dependency Parsing.
In Proc.Tenth Conf.
Comput.
Nat.
Lang.
Learn., numberJune in CoNLL-X ?06, pages 149?164.
Associationfor Computational Linguistics.C.
Culy, V. Lyding, and H. Dittmann.
2011. xldd:Extended linguistic dependency diagrams.
In Proc.of the 15th International Conference on InformationVisualisation IV2011, pages 164?169, London, UK.T.
De Mauro.
1991.
Guida all?uso delle parole.
Edi-tori Riuniti, Roma.F.
Dell?Orletta, G. Venturi, and S. Montemagni.
2011.Ulisse: an unsupervised algorithm for detecting re-liable dependency parses.
In Proc.
of CoNLL 2011,Conferences on Natural Language Learning, Port-land, Oregon.F.
Dell?Orletta, G. Venturi, and S. Montemagni.
2013.Unsupervised linguistically-driven reliable depen-dency parses detection and self-training for adapta-tion to the biomedical domain.
In Proc.
of BioNLP2013, Workshop on Biomedical NLP, Sofia.F.
Dell?Orletta.
2009.
Ensemble system for part-of-speech tagging.
In Proceedings of Evalita?09, Eval-uation of NLP and Speech Tools for Italian, ReggioEmilia.S.
Evert and A. Hardie.
2011.
Twenty-first centurycorpus workbench: Updating a query architecturefor the new millennium.
In Proc.
of the Corpus Lin-guistics 2011, Birmingham, UK.M.
G?en?ereux, I. Hendrickx, and A. Mendes.
2012.A large portuguese corpus on-line: Cleaning andpreprocessing.
In PROPOR, volume 7243 of Lec-ture Notes in Computer Science, pages 113?120.Springer.A.
Kilgarriff and G. Grefenstette.
2003.
Introductionto the special issue on the web as corpus.
Computa-tional Linguistics, 29(3):333?347.K.
Kipper, A. Korhonen, N. Ryant, and M. Palmer.2008.
A large-scale classification of english verbs.Journal of LRE, 42:21?40.A.
Lenci, G. Lapesa, and G. Bonansinga.
2012.
Lexit:A computational resource on italian argument struc-ture.
In N. Calzolari, K. Choukri, T. Declerck,M.
U?gur Do?gan, B. Maegaard, J. Mariani, J. Odijk,and S. Piperidis, editors, Proc.
of LREC 2012, pages3712?3718, Istanbul, Turkey, May.
ELRA.P.
Lucisano and M. E. Piemontese.
1988.
Gulpease:una formula per la predizione della difficolt dei testiin lingua italiana.
Scuola e citt`a, 39(3):110?124.V.
Lyding, C. Borghetti, H. Dittmann, L. Nicolas, andE.
Stemle.
2013.
Open corpus interface for italianlanguage learning.
In Proc.
of the ICT for LanguageLearning Conference, 6th Edition, Florence, Italy.D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProc.
of ACL 2006, ACL, Sydney.S.
Petrov and R. McDonald.
2012.
Overview of the2012 shared task on parsing the web.
In Proc.
ofSANCL 2012, First Workshop on Syntactic Analysisof Non-Canonical Language, Montreal.G.
Rehm, M. Santini, A. Mehler, P. Braslavski,R.
Gleim, A. Stubbe, S. Symonenko, M. Tavosanis,and V. Vidulin.
2008.
Towards a reference corpus ofweb genres for the evaluation of genre identificationsystems.
In Proc.
of LREC 2008, pages 351?358,Marrakech, Morocco.M.
Santini, A. Mehler, and S. Sharoff.
2010.
Ridingthe Rough Waves of Genre on the Web.
Conceptsand Research Questions.
In A. Mehler, S. Sharoff,and M. Santini, editors, Genres on the Web: Compu-tational Models and Empirical Studies., pages 3?33.Springer, Dordrecht.M.
Santini.
2005.
Genres in formation?
an ex-ploratory study of web pages using cluster analysis.In Proc.
of the 8th Annual Colloquium for the UKSpecial Interest Group for Computational Linguis-tics (CLUK05), Manchester, UK.S.
Sharoff.
2006.
Creating General-Purpose CorporaUsing Automated Search Engine Queries.
In M. Ba-roni and S. Bernardini, editors, Wacky!
WorkingPapers on the Web as Corpus, pages 63?98.
Gedit,Bologna.S.
Sharoff.
2010.
Analysing similarities and differ-ences between corpora.
In 7th Language Technolo-gies Conference, Ljubljana.J.
M. Steger and E. W. Stemle.
2009.
KrdWrd ?
TheArchitecture for Unified Processing of Web Content.In Proc.
Fifth Web as Corpus Work., Donostia-SanSebastian, Basque Country.C.
A. Thompson, M. E. Califf, and R. J. Mooney.
1999.Active learning for natural language parsing and in-formation extraction.
In Proc.
of ICML99, the Six-teenth International Conference on Machine Learn-ing, San Francisco, CA.43
