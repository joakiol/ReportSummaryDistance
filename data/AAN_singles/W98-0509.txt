Decision Procedures for Dependency Parsing Using GradedConstraintsWol fgang Menze l  and I ngo  SchrSder(menzel I ingo.schroeder@informatik.uni-hamburg.de )Fachbereich Informatik, Universit~t HamburgVogt-K611n-Strage 30, 22527 Hamburg, GermanyI Abst rac tWe present an approach to the parsing of depen-dency structures which brings together the no-tion of parsing as candidate limination, the useof graded constraints, and the parallel disam-biguation of related structural representations.The approach aims at an increased level of ro-bustness by accepting constraint violations in acontrolled way, combining redundant and possi-bly conflicting information on different represen-tational levels, and facilitating partial parsing asa natural mode of behavior.1 In t roduct ionLanguage understanding is based on a vari-ety of contributions from different representa-tional levels.
From this perspective, one of themost attractive features of dependency basedgrammar models seems to be their relationalnature which allows to accommodate variouskinds of relationships in a very similar fashion.Since the basic representational framework is arather general one it can be (re-)interpreted inmany different ways.
Thus, dependency rela-tions lend themselves to model the surface syn-tactic structure of an utterance (with labels likesubject-of, direct-object-of, determiner-of, etc.
),its thematic structure (with labels like agent-of,theme-of, etc.)
and even the referential struc-ture (with labels like referential-identity, part-of, possessor-of, etc.).
This representationalsimilarity obviates the necessity to integrate toomany disparate informational contributions intoa single tree-like representation.
Instead, rep-resentational levels can be separated from eachother in a clean manner with appropriate map-pings being defined to relate the different com-ponents to each other.Another less obvious advantage of depen-dency formalisms i their suitability for the ap-SynSem: : : iDie Knochen sieht die Katze.The bones sees the cat.1 2 3 4 5Figure 1: Collection of dependency trees: Eachtree represents a description level.plication of eliminative parsing techniques.
Incontrast o the traditional view on parsing asa constructive process, which builds new treestructures from elementary building blocks andintermediate r sults, eliminative approaches or-ganize structural analysis as a candidate limi-nation procedure, removing unsuitable interpre-tations from a maximum set of possible ones.Hence, parsing is constructed as a strictly mono-tonic process of ambiguity reduction.In this paper we describe different algorith-mic solutions to eliminative parsing.
The novelcontribution consists in the use of graded con-straints, which allow to model traditional gram-mar regularities as well as preferences and de-faults.
Details of the linguistic modeling arepresented by Heinecke and SchrSder (1998).2 E l lm lnat ive  Pars ingThe idea of eliminative parsing is not a novel oneand virtually every tagger can be considered acandidate limination procedure which removesitems from the maximum set of tags accord-ing to different decision criteria.
Interestingly,dependency-based parsing can be viewed as a78generalized tagging procedure.
One of the firstparsing systems which built on this propertyis the Constraint Grammar approach (Karlssonet al, 1995).
Underspecified dependency struc-tures are represented assyntactic tags 1 and dis-ambiguated by a set of constraints that excludeinappropriate r adings.
Maruyama (1990) firsttried to extend the idea to allow the treatmentof complete dependency structures.
Therefore,he has to generalize the notion of a"tag" to pairsconsisting of a label and the identifier of thedominating node, i. e., the tagset needs to be-come sensitive to the individual tokens of the ut-terance under consideration sacrificing the sta-tus of the tagset being fixed a-priori.
As inthe case of atomic tags, constraints are specifiedwhich delete inappropriate dependency relationsfrom the initial space of possibilities.
The ap-proach is not restricted to linear input stringsbut can also treat lattices of input tokens, whichallows to accommodate lexical ambiguity as wellas recognition uncertainty in speech understand-ing applications (Harper et al, 1994).Obviously, it is again the relational nature ofdependency models which provides for the ap-plicability of candidate limination procedures.Since the initial state of the analysis is given byan - admittedly large - set of possible depen-dency relations per token, the problem space re-mains finite for finite utterances.
An analogousapproach for constituency-based grammar mod-els would encounter considerable difficulties, be-cause the number and the kind of non-terminalnodes which need to be included in the tagsetremains completely unclear prior to the parsingitself.Eliminative approaches toparsing come alongwith a number of interesting properties whichmake them particularly attractive as computa-tional models for language comprehension.1.
As long as constraint checking is restrictedto strictly local configurations of depen-dency relations the decision procedures in-herits this locality property and thus ex-hibits a considerable potential for con-current implementation (Helzerman andlIn this framework tags denote, for instance, the sub-ject of the sentence, a determlner modifying a noun tothe right, a preposition modifying a noun to the lei~ etc.However, only the category of the dominating node isspecified, not its exact identity.79Harper, 1992).2.
Since partial structural descriptions areavailable concurrently they can be com-pared in a competitive manner.
Note how-ever that such a comparison imposes addi-tional synchronization a d communicationrequirements on parallel realizations.3.
As the elimlnative approach considers pars-ing a procedure of disambiguation, thequality of the results to be expected be-comes directly related to the amount ofeffort one is prepared to spend.
This isa clear contrast o constructive methodswhich, upon request usually will attemptto generate alternative interpretations, thusleading to a corresponding decrease of clar-ity about the structural properties of theinput utterance (in terms of Karlsson et al(1995)).4.
The progress of disambiguation can easilybe assessed by constantly monitoring thesize of value sets.
Moreover, under certainconditions the amount of remaining effortfor obtaining a completely disambiguatedsolution can be estimated.
This appearsto be an important characteristic for thedevelopment of anytime procedures, whichare able to adapt their behavior with re-spect o external resource limitations (Men-zel, 1994; Menzel, 1998).3 Graded Const ra in tsBoth the comparison of competitive structuralhypotheses a  well as the adaptation to resourcelimitations require to generalize the approach byallowing constraints of different strength.
Whiletraditional constraints only make binary deci-sions about the well-formedness of a configura-tion the strength of a constraint additionally re-fiects a human judgment of how critical a viola-tion of that particular constraint is considered.Such a grading, expressed as a penalty factor,allows to model a number of observations whichare quite common to linguistic structures:?
Many phenomena can more easily be de-scribed as preferences rather than strictregularities.
Among them are structuralconditions about attachment positions orlinear ordering as well as selectional restric-tions.IIIIIIIIIIIIIIIIIIIPreferences usually reflect different fre-quencies of use and in certain cases can beextracted from large collections of sampledata.Some linguistic cues are inherently uncer-tain (e. g., prosodic markers), and thereforeresist a description by means of crisp rulesets.By introducing raded constraints the pars-ing problem becomes an optimiT.ation problemaiming at a solution which violates constraintsthat are as few and as weak as possible.
This,on the one hand, leads to a higher degree ofstructural disambiguation since different solu-tion candidates now may receive a differentscore due to preference constraints.
Usually, acomplete disambiguation is achieved providedthat enough preferential knowledge is encodedby means of constraints.
Remaining ambigu-ity which cannot be constrained further is oneof the major ditticulties for systems using crispconstraints (Harper et al, 1995).
On the otherhand, weighed constraints allow to handle con-tradictory evidence which is typical for cases ofill-formed input.
Additionally, the gradings areexpected to provide a basis for the realization oftime adaptive behavior.One of the most important advantages whichcan be attributed to the use of graded con-straints is their ability to provide the mappingbetween different levels in a multi-level repre-sentation, where many instances of preferen-tial relationships can be found.
This separationof structural representations facilitates a clearmodularization of the constraint grammar al-though constraints are applied to a single com-putational space.
In particular, the propaga-tion of gradings between representational lev-els supports a mutual compensation of informa-tion deficits (e. g., a syntactic disambiguationcan be achieved by means of semantic support)and even cross-level conflicts can be arbitrated(e. g., a syntactic preference might be inconsis-tent with a selectional restriction).Combining candidate elimination techniques,graded constraints, and multi-level disambigua-tion within a single computational paradigmaims first of all at an increased level of robust-ness of the resulting parsing procedure (Menzeland Schr~der, 1998).
Robustness is enhanced by80three different contributions:1.
The use of graded constraints makes con-straint violations acceptable.
In a cer-tain sense, the resulting behavior can beconsidered a kind of constraint retractionwhich is guided by the individual grad-ings of violated constraints.
Therefore, a"blind" weakening of the constraint systemis avoided and hints for a controlled appli-cation are preserved.2.
The propagation of evidence among multi-ple representational levels exploits the re-dundancy of the grammar model about dif-ferent aspects of language use in order tocompensate he loss of constraining infor-mation due to constraint retraction.
Natu-rally, the use of additional representationallevels also means an expansion of the searchspace, but this undesired effect can be dealtwith because once a single point of relativecertainty has been found on an arbitrarylevel, the system can use it as an anchorpoint from which constraining informationis propagated to the other levels.
For in-stance, if selectional restrictions provideenough evidence for a particular solution,an ambiguous case can be resolved.
Evencontradictory indications can be treated inthat manner.
In such a case conflict resolu-tion is obtained according to the particularstrength of evidence resulting from the ob-served constraint violations.3.
The seamless integration of partial parsingis achieved by allowing arbitrary categories(not just finite verbs) to serve as the topnode of a dependency tree.
Of course, theseconfigurations eed to be penalized appro-priately in order to restrict their selectionto those cases where no alternative inter-pretations remain.
Note that under thisapproach partial parsing is not introducedby me~n.~ of an additional mechanism butfalls out as a general result of the underly-ing parsing procedure.Certainly, all the desired advantages men-tioned above become noticeable only if a con-straint modeling of grammatical relations can beprovided which obeys the rather restrictive lo-cality conditions and efficient implementationsof the disambiguation procedure become avail-able.4 Pars ing  As  Const ro in t  Sat i s fac t ionParsing of natural language sentences can beconsidered a constraint satisfaction problem ifone manages to specify exactly what the con-straint variables hould be, how constraints canbe used to find appropriate value assignments,and how these value assignments represent spe-cific structural solutions to the parsing problem.These are the questions we address in this sec-tion.The original definition of constraint de-pendency grammars by Maruyama (1990)is extended to graded constraint dependencygrammars which are represented by a tuple(~, L, C, ~)..
The lexicon !~ is a set of wordforms each of which has some lexical informa-tion associated with it.
The set of represen-tational evels L = {(/x, Lx) , .
.
.
,  ( l , ,L , )} con-sists of pairs (li, Li) where li is a name of theith representational level and l~ E Li is thej th  appropriate label for level l,.
Think of(5yn, {subj, obj, det}) as a simple example of arepresentational level.The constraints from the set C can be di-vided into disjunct subsets C* with C = Ui Cidepending on the constraints' arity i which de-notes the number of constraint variables relatedby the constraint.
Mainly due to computationalreasons, but also in order to keep the scope ofconstraints trictly local, at most binary con-straints, i. e., constraints with arity not largerthen two, are considered: C = C 1 U C2.
2The assessment function ~ : C ~ \[0, 1\] mapsa constraint c E C to a weight ~b(c) which in-dicates how serious one considers a violation ofthat constraint.
Crisp constraints which maynot be violated at all, i. e., they correspond totraditional constraints, have a penalty factor ofzero (~(c) = 0) while others have higher grades(i. e., 0 < ~b(e) < 1) and thus may be violatedby a solution.
3=The restriction to at most binary constraints doesnot decrease the theoretical expressiveness of the formal-ism but has some practical consequences for the gram-mar writer as he/she occasionally has to adopt ratherartificial constructs for the description ofsome linguisticphenomena (Menzel and SchrSder, 1998).3Constraints c with ~b(c) = 1.0 are totally ineffectiveas will become clear in the next paragraphs.Given a natural language sentence W =(wl, .
.
.
,win) and a graded constraint depen-dency grammar the parsing problem can bestated as follows: For each representationallevel Ii and each word of the sentence wj aconstraint variable ~ is established.
Let theset of all constraint variables be V. The do-main dom(~) = Li x {0,1, .
.
.
,  j -  1, j-I- 1,. .
.
n}of variable ~,  i. e., the set of possible values forthat variable, consists of all pairs (l,/=) where !is an appropriate label for level li (i. e., l E Li)and/?
is the index of the dominating word wk(i. e., word wj is subordinated to word wh) orzero if the word wj is the root of the dependencystructure on level li.A problem candidate p of the parsing prob-lem is a unique value assignment to each of theconstraint variables.
In other words, for eachvariable ~ a single value p(~) = d~ E dom(~)has to be chosen.The solution is the problem candidate thatviolates less and/or less important constraintsthan any other problem candidate.
In order tomake this intuitive notion more formal the func-tion ~ is extended to assess not only constraintsbut also problem candidates p.= R T I I Ia cEC o ~EV =where a, 1 < a < 2, is the arityand ~ is a tuple of variablesA single constraint c can be violated once,more than once or not at all by a problem candi-date since constraints judge local configurations,not complete problem candidates.~(c, ~ = { ~b(C)l.0 :: ifelsedViolates cwhere dis a (unary or binary) tu-pie of valuesNote that satisfying a constraint does notchange the grade of the problem candidate be-cause of the multiplicative nature of the assess-ing function.The final solution Ps is found by maximumselection.p.
= argm=81IIIIiIIIIIIIIIIIIIIThus the system uniquely determines thedominating node for each of the input wordforms.
Additional conditions for well-formedstructural representations like projectivity orthe absence of cyclic dependency relations mustbe taken extra care of.In our current implementation the acyclicoit), property is ensured by a special built-incontrol structure, while projectivity has to beestablished by means of specifically designedconstraints.
This enables the grammar writerto carefully model the conditions under whichnon-projective dependency structures may oc-cur.
Note, however, that there are cases of non-projective structures that cannot be eliminatedby using only local (i. e. at most binary) con-straints.Another problem arises from the fact thatconstraints are universally quantified and exis-tence conditions (like "there must be a subjectfor each finite verb") c~nnot be expressed di-rectly.
This diilqculty, however, is easily over-come by the introduction of "reverse" depen-dencies on additional auxiliary levels, which areused to model the valency requirements of adominating node.
Since each valency to be sat-urated requires an auxiliary level, the overallnumber of levels in a multi-level representationmay easily grow to more than some ten.Moreover, the formal description given so faris only valid for linear strings but not for wordgraphs.
An  extension to the treatment of wordgraphs requires the modification of the notion ofa problem candidate.
While in the case of linearinput an assignment of exactly one value to eachvariable represents a possible structure, this isnot valid for word graphs.
Instead, only thosevariables that correspond to a word hypothesison one particular path through the word graphmust receive a unique value while all other vari-ables must be assigned no value at all.
Thisadditional path condition usually is also not en-coded as normal grammar constraints but mustbe guaranteed by the control mechanism.5 An  ExampleTo illustrate the formalization we now gothrough an example.
To avoid unnecessary de-tails we exclude the treatment of auxiliary levelsfrom our discussion, thus restricting ourselvesto the modeling of valency possibilities and ab-82stracting from valency necessities.
The problemis simplified further by selecting an extremelylimited set of dependency labels.
Consider againthe example from Figure 1:(I) Die Knochenpl siehtsg die Katzesg.The bones sees the cat.
"The cat sees the bones.
"Two representational levels, one for syntacticfunctions and one for semantic ase-fillers, areintroduced:L = { (Syn, {subj, oSj, det}),(Sam, {agent, theme, def}) }Figure 2 contains some of the constraints nec-essary to parse the example sentence.
Basically,a constraint consists of a logical formula whichis parameterized byvariables (in our example Xand Y) which can be bound to an edge in thedependency tree.
It is associated with a name(e. g., SubjNumber) and a class (e. g., Subj) foridentification and modularization purposes re-spectively.
The constraint score is given justbefore the actual formula.
Selector functionsare provided which facilitate access to the labelof an edge (e. g., X.labe\[) and to lexical prop-erties of the dominating node (e. g., X1"num)and the dominated one (e. g., X~num).
Beinguniversally quantified, a typical constraint takesthe form of an implication with the premise de-scribing the conditions for its application.
Ac-cordingly, the constraint SubjNumber of Figure 2reads as follows: For each subject (X.\[abel=subj)it holds that the dominated and the dominatingnodes agree with each other in regard to n-tuber(X,i.num=X'tnum).Figure 1 from the introduction graphicallypresents the desired solution structure which isrepeated as a constraint variable assignment inFigure 3.
4All (shown) constraints are satisfied by thevariable assignment except SubjOrder which isviolated once, viz.
by the assignment _V~y n =(suSj, 3).
Therefore, the structure has a score4The presentation of solutions as dependency treesbecomes less intuitive as soon as more levels, especiallyauxiliary levels, are introduced.ili{X} : SubjNumber : Subj : 0.1 :X.label=subj -~ X~num=Xtnum'Subjects agree with finite verbs regarding number.
'{X} : SubjOrder : Subj : 0.9 :X.label=subj -F X~pos<Xtpos'Subjects are usually placed in front of the verb.
{X} : SemType : SelRestr : 0.8 :X.label (E { agent, theme } -~type_match( Xlid.
X.label, X,I.id )'Verbs restrict the semantic types of theirarguments.
'{X, Y} : SubjAgentObjTheme : Mapping : 0.2 :XTid=YTid A X~id=Y~id -~( X.label=subj ~-~ Y.label=agent ) ^( X.label=obj ~ Y.label=theme )'The subject is the agent and the object the theme.
'{X, Y} : Unique : Unique : 0.0 :X.label E {subj,obj,agent,theme} ^ Xtid=Ytid-4 Y.labelpX.label'Some labels are unique for a given verb.
'Figure 2: Some of the constraints needed forthe disambiguation f the example sentence.equal to the constraint's score, namely 0.9.
Fur-thermore there is no structure which has a bet-ter assessment.
The next example is similar tothe last, except hat the finite verb appears inplural form now.
(2) Die Knochenpl sehenpl die Katzesg.The bones see the cat.
"The bones ee the cat.
"A solution structure analogous to the one dis-cussed above would have a score of 0.09 be-cause not only the constraint SubjOrder but alsothe constraint SubjNumber would have been vi-olated.
But the alternative structure where theV~y n = (det,2) VSem = (de:f, 2)V~yn = (obj,3) V~e m = (theme, 3)V~y n = (root, O) V~e m = (root,0)U~y n = (det, 5) V~e m = (de.f, 5)"~yn = (sub.j, 3) V~e m = (agent, 3)F igure 3: Constraint variable assignments cor-responding to the  dependency trees in Figure 1subj/agent and the obj/theme edges are inter-changed (meaning that the bones do the seeing)has a better score of 0.8 this time because it onlyviolates the constraint SemType.
This result ob-viously resembles performance of human beingswho first of all note the semantic oddness of theexample (2) before trying to repair the syntacticdeviations when reading this sentence in isola-tion.Thus, the approach successfully arbitrates be-tween conflicting information from different lev-els, using the constraint scores to determinewhich of the problem candidates i  chosen asthe final solution.6 Const ra in t  Sat i s fac t ion  P roceduresA lot of research as been carried out in the fieldof algorithm~ for constraint satisfaction prob-lems (Meseguer, 1989; Kumar, 1992) and con-straint optimization problems (Tsang, 1993).Although CSPs are NP--complete problems ingeneral and, therefore, one cannot expect a bet-ter than exponential complexity in the worstcase, a lot of methods have been developed toallow for a reasonable complexity in most practi-cal cases.
Some heuristic methods, for instance,try to arrive at a solution more efficiently at theexpense of giving up the property of correctness,i.
e., they find the globally best solution in mostcases while they are not guaranteed to do so inall cases.This allows to influence the temporal char-acteristics of the parsing procedure, a possibil-ity which seems especially important in interac-tive applications: If the system has to deliver areasonable solution within a specific time inter-val a dynamic scheduling of computational re-sources depending on the remaining ambiguityand available time is necessary.
While differ-ent kinds of search are more suitable with re-gard to the correctness property, local pruningstrategies lend themselves to resource adaptiveprocedures.6.1 Consistency-Based MethodsAs long as only crisp constraints are considered,procedures based on local consistency, particu-larly arc consistency can be used (Maruyama,1990; Harper et al, 1995).
These methods tryto delete values from the domain of constraintvariables by considering only local informationand have a polynomial worst case complexity.83Unfortunately, they possibly stop deleting val-ues before a unique solution has been found.
Insuch a case, even if arc consistency has been es-tablished one cannot be sure whether the prob-lem has zero, one, or more than one solutionbecause alternative value assignments may belocally consistent, but globally mutually incom-patible.
Consequently, in order to find actualsolutions an additional search as to be carriedout for which, however, the search space is con-siderably reduced already.6.2 SearchThe most straightforward method for constraintparsing is a simple search procedure where theconstraint variables are successively bound tovalues and these value assignments are tested forconsistency.
In case of an inconsistency alterna-tive values are tried until a solution is found orthe set of possible values is exhausted.
The ba-sic search algorithm is Branch & Bound whichexploits the fact that the score of every subset ofvariable assignments is already an upper boundof the final score.
Additional constraint viola-tions only make the score worse, bemuse thescores of constraints do not exceed a value ofone.
Therefore, large parts of the search spacecan be abandoned as soon as the score becomestoo low.
To further ~ improve the efficiency anagenda is used to sort the search space nodesso that the most promising candidates are triedfirst.
By not allowing the agenda to grow largerthan a specified size, one can exclude searchstates with low scores from further considera-tion.
Note that correctness cannot be guaran-teed in that case anymore.
Figure 4 presents thealgorithm in pseudo code notation.Unfortunately, the time requirements of thesearch algorithms are almost unpredictable sincean intermediate state of computation does notgive a reliable estimation of the effort that re-mains to be done.6.3  Prun ingAs explained in Section 6.1 consistency-basedprocedures use local information to delete valuesfrom the domain of variables.
While these meth-ods only do so if the local information sufficesto guarantee that the value under considerationcan safely be deleted, pruning oes one step fur-ther.
Values are successively selected for dele-tion based on a heuristic (i. e., possibly incor-procedure  ConstraintSearchset b := 0.0 ; best score so farset r := 0 ; set of solutionsset a := {(0, V, 1.0)) ; agendawhile a ~ 0 do ; process agendaget best item (a, V, s) from agenda  ; best firsti f  V = 0 theni f  a = b thenadd (B, V, s) to relseset r := {(/3, IF, , )}set b :-- aflflselect v E Vset V' := V\{~}foreach d E dora(v) doset B' :=  B ucompute new score s ~ for B ~if s ~ ~ b thenadd (B', V', s') to agendafldonetruncate agenda  (if desired)done; complete assignment?
; best so far?
; equally good; better; try next free variable; try all values; already worse?Figure 4: Search procedure for constraint pars-ing: Best-first branch & bound algorithm withlimited agenda length (beam search)rect) assessment until a single solution remains(cf.
Figure 5).
The selection function considersonly local information (as do the consistency-based methods) for efficiency reasons.
Takinginto account global optimality criteria would nothelp at all since then the selection would be asdifficult as the whole problem, i. e., one wouldhave to expect an exponential worst-case com-plexity.procedure pruning(V)while 3(~fi V): Idom(~)~ >I doselect (~, d) to be deleteddelete d from domaindoneFigure 5: The pruning algorithm repeatedlyselects and deletes values from the domain ofvariables.Obviously, the selection heuristics plays themajor role while pruning.Simple selection functions only consider theminimum support a value gets from anothervariable (Menzel, 1994).
They combine the mu-tual compatibilities of the value under consid-eration and all possible values for another vari-84able.
Then the minimum support for each valueis determined and finally the value with the leastsupport is selected for deletion.
In other words,the value for which at least one variable's valueshave only low or no support is ruled out as apossible solution.Formally the following formulas (using the no-tation of Section 4) determine the value d ofvariable # to deleted next:score(d, d') 2s(v, d) = rni, d'~do,,,(,/)vev\{ } Idom(v')l(~, d) = axg min,(v,  d)(~,d)where score(d, d') is the accumulated assessmentof the pair of subordinations (d, d ~):score(d,d') = I\] ?(c,d).?(c,d').
I'\[ ?
(c,d,d')c6C z c6C 2While this heuristics works quite well for lin-ear strings it fails if one switches to word graphs.Figure 6 gives an example of a very simple wordgraph which cannot be handled correctly by thesimple heuristics.i laughsstart The children !
~ stop0-- -~ , , ---- ' -~ ?
= e:.
X .
.
.~ j  , - - -~  0: laugh0 1 2 3F igure  6: Simple word graphAlternative word hypotheses whose timespans are not disjunct do not support each otherby definition.
Therefore, the subordination ofchildren under laugh in Figure 6 is equally disfa-vored by laughs as is the subordination of chil-dren under laughs by laugh.
Unfortunately, thislack of support is not based on negative videncebut on the simple fact that laugh and laughsare temporal alternatives and may, thus, notbe existent in a solution simultaneously.
Sincethe simple heuristics does not know anythingabout this distinction it may arbitrarily selectthe wrong value for deletion.A naive extension to the above heuristicswould be to base the assessment ot on the min-imal support from all variables but on the corn-85bined support from those variables that share atleast one path through the word graph with thevariable under consideration.
But the path cri-terion is computational\]y expensive to computeand, therefore, needs to be approximated dur-ing pruning.
Instead of considering all possiblepaths through the graph, we compute the max-imum support at each time point t and on eachlevel I and select the minimum of these valuesto be removed from the space of hypotheses:score(d, d') 2dr Edom(v *)a(v,d,Z,t) = max Idom(v')l~etime(,')level(.l)=~8(v,d) = ,(v, d, Z, t)t,l= argmins(v .d)where time(v) denotes the time interval of theword hypothesis (cf.
Figure 6 or 7) that corre-sponds to the variable v and level(v) denotes therepresentational level of variable v.For temporally overlapping nodes the proce-dure selects a single one to act as a representa-tive of all the nodes within that particular timeslice.
Therefore, information about the exactidentity of the node which caused the lack ofsupport is lost.
But since the node which givesa maximum support is used as a time.slice rep-resentative it seems likely that any other choicemight be even worse.Although preliminary experiments producedpromising results (around 3 % errors) it can beexpected that the quality of the results dependson the kind of grammar used and utterances an-alyzed.
Since the problem deserves further in-vestigation, it is too early to give final results.The example in Figure 7 shows a simple casethat demonstrates the shortcomings of the re-fined heuristics.
Although these and children arenot allowed to occur in a solution simultane-ously, exactly these two words erroneously re-main undeleted and finally make up the subjectin the analysis.
First, all values for the article aare deleted because of a missing number agree-ment with the possible dominating nodes andthereafter the values for the word houses are dis-carded since the semantic type does not matchthe selectional restrictions of the verb very well.IIIIIIIIIiIIIiIII!The heuristics is not aware of the distinctionbetween the time points and word graph nodesand, therefore, counts the determiner these assupporting the noun children.?
a ?
children0 ---~,-- O~, ,~ 0~0these0 1 2 3F igure  7: Hypothetic simplified word graphwhich may be analyzed "incorrectly" by the timeslice pruning heuristics.7 E f f i c iency  I ssuesAlthough pruning strategies bear a great po-tential for efficient and time-adaptive parsingschemes, the absolute computational expensesfor a "blind" application of constraints are stillunacceptably high.
Additional techniques haveto be employed to decrease actual computationtimes.
One of the starting points for such im-provements is the extremely large number ofconstraint evaluations during parsing: A fewmillion constraint checks are quite common forrealistic grammars and sentences ofeven modestsize.Two approaches seem to be suitable for the re-duction of the number of constraint evaluations:?
Reduced application of constraints: A de-tailed analysis of how constraints are ap-plied and under what circumstances theyfail shows that most constraint checks areF igure  8: Window of the graphical grammarenvironment xcdg86'~seless" since the tested constraint is sat-isfied for some trivial reason.
For in-stance, because most constraints are veryspecific about what levels are constrainedand whether and how the dependency edgesare connected, this information can be ex-ploited in order to reduce the number ofconstraint checks.
By applying constraintsonly to the relevant levels the number ofconstraint evaluation has been cut downto (at most) 40%.
Taking into accountthe topological structure of the edges un-der consideration improves the efficiency byanother 30% to 50%.Reduction of the number of constraint vari-ables: A typical grammar contains a rela-tively large number of representational lev-els and for most word forms there are sev-eral entries in the lexicon.
Since the lexi-cal ambiguity of the word form usually isrelevant only to one or very few levels, con-straint variables need not be established forall lexical entries and all levels.
For in-stance, the German definite determiner diehas eight different morpho-syntactic featurecombinations if one only considers varia-tions of gender, case, and number.
All theseforms behave quite similarly with respectto non-syntactic levels.
Consequently, itmakes no difference if one merges the con-straint variables for the non-syntactic lev-els except that now less constraint checksmust be carried out.
By considering therelevance of particular types of lexical am-biguity for constraint variables of differentlevels one achieves an efficient reatment ofdisjunctive feature sets in the lexicon (Foth,1998).
This technique reduced the time re-quirements by 75% to 90% depending onthe details of the grammatical modeling.
Inparticular, a clean modularization, both inthe constraint set and the dictionary en-tries, results in considerable gains of effi-ciency.In order to support the grammar writer, agraphical grammar environment has been devel-?
oped (cf.
Figure 8).
It includes an editor fordependency trees (cf.
Figure 9) which allows todetect undesired constraint violations easily.IFigure 9: Window of the editor for dependencytrees8 Conc lus ionA parsing approach aiming at dependency struc-tures for different representational levels hasbeen presented.
The approach improves in ro-bustness by assessing partial structures, inte-grating multiple representational levels, and em-ploying partial parsing techniques.
Knowledgeabout the grammar but also extralinguistic in-formation about the domain under considera-tion is encoded by meaus of graded constraintswhich allows for the arbitration between con-flicting information.
Different decision proce-dures for the defined parsing problem have beenintroduced and some efficiency issues have beendiscussed.The approach has successfully been appliedto a number of modestly sized projects (Menzeland Schr~Sder, 1998; Heinecke t al., 1998).Further investigations will focus on possibili-ties for incremental processing of speech inputand the realization of resource adaptive behav-ior.ReferencesKilian Foth.
1998.
Disjunktive Lexikoninforma-tion im eliminativen Parsing.
Studienarbeit,FB Informatik, Universit~t Hamburg.Mary P. Harper, L. H. Jarnieson, G. D. Mitchell,G.
Ying, S. Potisuk, P. N. Srinivasan,R.
Chen, C: B. Zoltowski, L. L. McPheters,B.
Pellom, and R. A. Helzerman.
1994.
In-tegrating language models with speech recog-nition.
In Proceedings of the AAAI-9~ Work-shop on the Integration of Natural Languageand Speech Processing, pages 139-146.87Mary P. Harper, Randall A. Helzermann, C. B.Zoltowski, B. L. Yeo, Y. Ohan, T. Stew-ard, and B. L. Pellom.
1995.
Implementa-tion issues in the development of the PARSECparser.
Software - Practice and Experience,25(8):831-862.Johannes Heineeke and Ingo Schr'6der.
1998.Robust analysis of (spoken) language.
InProc.
KONVENS '98, Bonn, Germany.Johannes Heinecke, Jiirgen Kunze, WolfgangMenzel, and Ingo SchrSder.
1998.
Elimina-tire parsing with graded constraints.
In Proc.Joint Conference COLING/A CL '98.Randall A. Helzerman and Mary P. Harper.1992.
Log time parsing on the MasPar MP-1.In Proceedings of the 6th International Con-ference on Parallel Processing, pages 209-217.Fred Karlsson, Atro Voutilainen; Julaa Heikkil~i,and Arto Anttila, editors.
1995.
ConstraintGrammar - A Language-Independent Systemfor Parsing Unrestricted Tezt.
Mouton deGruyter, Berlin, New York.Vipin Kumar.
1992.
Algorithms for constraintsatisfaction problems: A survey.
A1 Maga-zine, 13(1):32--44.Hiroshi Maruyama.
1990.
Structural disam-biguation with constraint propagation.
InProceedings of the e8th Annual Meetin 9 of theA CL, pages 31-38, Pittsburgh.Woffgang Menzel and Ingo SchrSder.
1998.Constraint-based diagnosis for intelligent lan-guage tutoring systems.
In Proceedings ofthe ITFJKNOWS Conference at the 1FIP '98Congress, Wien/Budapest.Woffgang Menzel.
1994.
Parsing of spoken lan-guage under time constraints.
In A. Cohn, ed-itor, Proceedings of the 11th European Confer-ence on Artificial Intelligence , pages 560-564,Amsterdam.Wolfgang Menzel.
1998.
Constraint Satisfac-tion for Robust Parsing of Spoken Language.Journal for Experimental nd Theoretical Ar-tificial InteUigence, 10:77-89.Pedro Meseguer.
1989.
Constraint satisfactionproblems: An overview.
A1 Communications,2(1):3-!7.E.
Tsang.
1993.
Foundations of Constraint Sat-isfaction.
Academic Press, Harcort Brace andCompany, London.
