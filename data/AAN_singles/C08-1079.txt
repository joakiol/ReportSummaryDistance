Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 625?632Manchester, August 2008Exploring Domain Differences for the Design of Pronoun ResolutionSystems for Biomedical TextNgan L.T.
Nguyen Jin-Dong KimDepartment of Computer Science, University of Tokyo, Hongo 7-3-1, Tokyo, Japan{nltngan, jdkim}@is.s.u-tokyo.ac.jpAbstractMuch effort in the research community hasbeen spent on solving the anaphora resolu-tion or pronoun resolution problem, and inparticular for news texts.
In order to selec-tively inherit the previous works and solvethe same problem for a new domain, wecarried out a comparative study with threedifferent corpora: MUC, ACE for the newstexts, and GENIA for bio-medical papers.Our corpus analysis and experimental re-sults show the significant differences in theuse of pronouns in the two domains, thusby properly considering the characteristicsof a domain, we can improve the perfor-mance of pronoun resolution for that do-main.1 IntroductionPronoun resolution is the task of determining theantecedent of an anaphoric pronoun, or a pro-noun pointing back to some previously mentioneditem in a text.
For example, in the sentence, ?TheIL-2 gene displays both T cell specific and in-ducible expression: it is only expressed in CD4+ Tcells after antigenic or mitogenic stimulation,?
thepronoun ?it?
should be resolved to refer to ?theIL-2 gene,?
and thus, we have an anaphora link.Pronoun resolution is an important task in thefamily of reference resolution tasks, includinganaphora resolution and co-reference resolution,which are known as significant parts of text un-derstanding systems.
Recently the need to havemore powerful information extraction systems forc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.biomedical technical papers has motivated re-searchers to solve the same task for the biomed-ical domain.
Castano (Castano et al, 2002) re-solved the sortal and pronominal anaphora, by us-ing a salience measure, which is the sum of all fea-ture scores.
Kim and Park (Kim and C.Park, 2004)introduced BioAR, a biomedical anaphora resolu-tion system that relates entity mentions in text withtheir corresponding Swiss-Prot entries.
This sys-tem resolves anaphoric pronouns by using heuris-tic rules and seven patterns for parallelism.
How-ever, the sizes of the data sets used in their exper-iments were small.
In the former system, 46 and54 MEDLINE abstracts were used for the devel-opment set, and the test set respectively, and thetest set in the latter work contained only sixteenanaphoric pronouns.
Contrary to their work, in thiswork we made use of GENIA, a large co-referenceannotated corpus for the bio domain, containing1999 MEDLINE abstracts.While there are quite a few works on this taskfor the bio-medical domain, for other domains, andespecially for the news domain, a myriad of workson pronoun resolution has been carried out bythe NLP researchers (Mitkov, 2002).
Since Soon(Soon et al, 2001) started the trend of using themachine learning approach by using a binary clas-sifier in a pairwise manner for solving co-referenceresolution problem, many machine learning-basedsystems have been built, using both supervisedand, unsupervised learning methods (Haghighi andKlein, 2007).
Such methods were claimed to becomparable with traditional methods.
However,the problems caused by domain differences, whichstrongly affect a deep-semantics related task likepronoun resolution, have not yet been studied wellenough.In order to recognize the important factors in625building an effective machine learning-based pro-noun resolution system, and in particular for thebio-domain, we have built a machine learning-based pronoun resolver and observed the contribu-tions of different features in the pronoun resolutionprocess.
In our experiments for the news domain,we used the MUC-7 and ACE corpora, and for thebiomedical domain, we employed the GENIA co-reference corpus.Section 2 describes the noticeable issues relatedto the corpora, and their preprocessing.
Section 3describes the implementation of our pronoun reso-lution system, including the resolution model andthe features used.
Our experiment settings, eval-uation scheme, and experimental results are pre-sented in Section 4.
Finally, we conclude our paperin Section 5.2 CorporaIn this section, we briefly introduce three corporaused in our experiments: MUC-7, ACE, and GE-NIA, and discuss the differences in their annota-tion schemes.
Afterwards, we analyzed the majordifferences in the distributions of anaphoric pro-nouns in these data sets, which provide importantinformation for the design of features used in thepronoun resolution process.The MUC-7 co-reference corpus is a collectionof news wire articles from the source for NorthAmerican News Text Corpora.
It contains thetraining, dry run test, and formal run test sets.
Thedry run and formal run have different domains; thedry run (and training) consists of aircrash scenar-ios, while the formal run consists of missile launchscenarios.
The ACE (phase 2) corpus for namedentity detection contains three data sets: news wire(NWIRE), broadcast news (BNEWS), and news-paper (NPAPER).
Each data set is divided into2 parts for training (train), and for developmenttesting (devtest).
For the bio-domain, we use theGENIA co-reference corpus, containing 1999 ab-stracts selected from MEDLINE: a huge source ofbio-domain scientific papers.These three corpora are all manually annotatedwith co-reference information; i.e., the informa-tion where mentions refer to the same entities.However, since the annotation schemes used arenot the same, these corpora contain some signif-icant differences, which may affect our referenceresolution systems.Figure 1: The symmetric and asymmetric annota-tion schemes.
The dotted lines represent implicitlinks between the elements.2.1 Variations in co-reference annotationschemesWe started by introducing some important ter-minologies together with some noticeable issuesrelated to the common co-reference annotationscheme.
Later, we mention the differences amongthe annotation schemes of the three corpora usedin our experiments.There are three main elements in the co-reference corpus annotation: the anaphoric expres-sions, which are anaphoric pronouns in the caseof the pronominal anaphora, their antecedents, andthe referred concepts.
Depending on either theasymmetric scheme employed in MUC (Lynette,1997) and GENIA (Hong, 2004) or the symmetricscheme in ACE (NIST, 2003), the annotation taskis defined as either an anaphor-antecedent linking,or mention-concept linking task, correspondingly(See Figure 1).
Moreover, each annotation schemeprovides its own guidelines for recognizing and an-notating these three elements, causing the varia-tions across different co-reference annotated cor-pora.In the annotation schemes, mentions which mayjoin in the co-reference relationship are calledmarkable.
All of the three annotation schemesrecord both a maximal and a minimal boundary ofmarkables, in concerning the evaluation schemes.However, the types of markables to be annotated,and the ways to decide their maximal boundary,are not the same in every annotation scheme.Table 1 shows the concepts annotated for eachcorpora according to the annotation schemes.While the number of concepts in the ACE corpusis limited to only 5 entity types, the GENIA andMUC annotation schemes do not clearly specifythe concept types.
This means that every possibleconcept in the text domains can join the anaphorarelations; i.e., can be annotated as markables.
Thisin turn makes the resolution task become more dif-ficult.626Table 1: Possible concepts according to the anno-tation schemesGENIA ACE MUC(Not specifiedexplicitly)-Bio entities5 types of enti-ties-Person-Organization-Facility-Location-GPE(Geo-political Entity)(Not specifiedexplicitly)-Person-Organization-Location-Date-Time-Money-PercentTable 2: Possible types of anaphor according to theannotation schemes (O: allowed, X: not allowed,U: unspecified)TYPE GENIA ACE MUCPersonal pronoun O O ODemonstrative pronoun O O OPossessive pronoun O O OReflexive pronoun O O UIndefinite pronoun (e.g.,both)O U UPleonastic pronoun it X U UBound anaphor X U OMention with empty head(e.g., five of)X U Uhere, there U O UThe possible types of annotated anaphoric pro-nouns are given in Table 2.
O denotes the type ofpronoun, which may be annotated as markable, incontrast to X, which denotes the type of pronoun,which is not allowed to be annotated as markable.The notation U represents the annotation schemethat does not state how a type should be treatedbecause that type is not popular in the domain, orthe scheme does not allow the annotation of such atype implicitly.Using the similar notations as in Table 2, Ta-ble 3 shows the possible syntactic structures ofantecedents according to the annotation schemes,which are also the structures of markables in realannotations.
In practice, such structural varia-tions may cause troubles for automatically mark-able recognition, so in the experiments with pro-noun resolution, gold markables are often used toeliminate error-prone problems.2.2 Corpus preprocessingOur objective anaphoric pronouns are limited tothe following types: personal pronouns (all cases),possessive pronouns, and demonstrative pronouns,which have a nominal antecedent.
In additionTable 3: Possible types of antecedent accordingto the annotation schemes (O: allowed, X: not al-lowed, U: unspecified)TYPE GENIA ACE MUCPronominal X O ONoun used as a modifier (em-bedded in NP)X O OName, named entity (embeddedin NP)X O OGerund U U XNP with a head noun (definiteand indefinite)O O OConjoint NP (with more thanone head)O X OCoordinated NP O O OPredicate nominal X O ONP with a restrictive appositivephraseX O ONP with a non-restrictive ap-positive phraseX O ONP with a restrictive preposi-tional phraseO O ONP with a non-restrictiveprepositional phraseX O ONP with a restrictive relativeclauseO O ONP with a non-restrictive rela-tive clauseO O OInfinitive clause O U UDate, Currency expressions,and percentagesU U OProper adjective (e.g.,French) U O Uhere, there X O Uto these types of pronouns, the annotated cor-pora contain other types of pronominal anaphora,including ?both,?
?one,?
numeric mentions (GE-NIA), and bound anaphora (ACE).
However, anal-ysis statistics show that such pronouns occupy lessthan 5% of the total pronouns in the GENIA cor-pus, thus we have ignored them.In the preprocessing step, for each corpus, weextract the gold pronominal anaphora links, whichlink the anaphoric pronouns with their antecedents.Although MUC and GENIA used the same asym-metric annotation schemes, picking one gold an-tecedent in a set of co-referenced mentions is notstraightforward, since pronouns in GENIA are notallowed to be linked with a pronomial antecedent,while in the MUC corpus, this kind of link is al-lowed.
In order to achieve the fairest compara-tive experimental results, we uniformly choose thenearest item in the co-reference chain of a pro-noun, and make a gold anaphora link.
This pol-icy is best suited for ACE, thanks to the symmetricscheme used.627Table 4: Sizes of the data sets (number ofanaphoric pronoun)GENIA ACE MUCTraining set 1442 2427 371Test set 357 633 240Figure 2: Analysis of anaphoric pronoun in differ-ent data sets2.3 StatisticsIn the following step, we analyze the extractedanaphora links for the three corpora.
The analy-sis statistics in Figure 2 show the differences of thedistributions of pronoun types and pronoun prop-erties in three data sets: MUC-7, GENIA, andBNEWS from ACE.
Note that only four majortypes out of the nine types of anaphoric pronounsmentioned in the previous section are counted.
Inparticular, the chosen types correspond to thoserows in Table 2 that contain at least two O.We can see that all of the anaphoric pronouns inGENIA are neutral-gender and third-person pro-nouns.
Another difference is that the number ofdemonstrative pronouns in GENIA comes to about20%, which is much more than in other data sets.As each type of pronoun has its own referen-tial characteristics, such differences in the distribu-tions of pronouns can significantly affect the pro-noun resolution.
This will be shown in our experi-ments, and analysis of the experimental results willbe given in the following section.3 Implementation3.1 Pronoun resolution modelWe built a machine learning based pronoun res-olution engine using a Maximum Entropy rankermodel (Berger et al, 1996), similar with Denis andBaldridge?s model (Denis and Baldridge, 2007).For every anaphoric pronoun ?, the ranker selectsthe most likely antecedent candidate ?, from a setof k candidate markables.Pr(?j|?)
=exp (?ni=1?ifi(?, ?j))?kexp (?ni=1?ifi(?, ?k))(1)We constructed the training examples in the fol-lowing way: for each gold anaphora link in thetraining corpus, we created a positive instance, andnegative training instances are created by pairingthe pronoun with all of the other markables ap-pearing in a window of w preceding sentences.
Inall the experiments on ACE and MUC, we set wto 10 sentences, while for GENIA, w is set to 5.This setting is based on our corpus analysis show-ing that many of the gold antecedents in the bio-domain texts are in at most three sentences fromtheir anaphors.
In the resolution phase, the samemethod for collecting instances was also applied.3.2 FeaturesTable 5 shows the primitive features used in oursystem, which are grouped into feature groups ac-cording to the type of information that they carry.Note that the actual features used by the ranker aredistance features (sdist, and tdist), and not only theprimitive features themselves, but also the combi-nations of these primitive features.
The pronounresolution model makes use of the discriminativepower of these combinatory features.
For exam-ple, the combination of P num and C num tests theagreement in number between the anaphoric pro-noun and its candidate.
Such agreements in num-ber and gender are one of the constraints in theanaphora phenomenon, and have been exploited inalmost all machine learning-based pronoun resolu-tion frameworks (Soon et al, 2001).Each primitive feature is from a layer of textanalysis (see Layer), which can be morphological(mor.
), syntactic (syn.
), or semantic (sem.).
Thesecond column represents the feature sets that areused in our experiments.
The explanation columnin the table shows the way we extract feature val-ues from texts, with the exception of the primitive628feature P semw, reflecting the context informationof the anaphoric pronoun.
This feature value is de-termined in the following way.
If the pronoun isa subject, then P semw is its governing head verb,and if it is a possessive adjective, then P semw isthe head noun of the noun phrase containing thatpronoun.
A default value is used if the pronounbelongs to neither of the above cases.The last column of this table shows an exam-ple of the feature characterization for the anaphoralink PMA-its in this discourse: ?By comparison,PMA is a very inefficient inducer of the jun genefamily in Jurkat cells.
Similar to its effect on theinduction of AP1 by okadaic acid, PMA inhibitsthe induction of c-jun mRNA by okadaic acid.
?We divided the feature groups into 3 feature sets:fundamental, baseline and additional.
The funda-mental feature set contains the indispensable fea-tures for solving pronoun resolution.
The base-line feature set mostly includes morphological fea-tures, reflecting the properties of text mentions,and in particular the pronoun properties such asgender, number, etc.
The features in the addi-tional feature set are used to exploit higher levelsof knowledge through more semantic and syntacticfeatures.
We also include in this feature set the fea-tures that have been used in some previous work inorder to clarify their contributions in our system.4 Experiments4.1 Experiment setting and evaluationscoringFor each corpus, we trained our resolver on thetraining set, and then applied it to the develop-ment test set.
For the case of the ACE corpus, weonly used the train part of the BNEWS data set fortraining, and applied on the corresponding devtestdata set.
We randomly splitted the GENIA cor-pus it into 2 parts: the train, and the heldout datasets, which contain 1599 and 400 abstracts, respec-tively.
For the MUC corpus, we used the dryrunpart for training, and the formal part for testing.Similar to previous works, all of the experimen-tal results in this paper are reported in success rate(Mitkov, 2002), calculated using the following for-mula.Success rate = Number of successfully resolved anaphorsNumber of all anaphors(2)The input of our resolver are the gold mentionsannotated in the corpora.
The output anaphoralinks of a pronoun resolution system are evalu-ated following two criteria.
In criterion 1, therecognized antecedent of an anaphoric pronoun isconsidered correct only when it matches the an-tecedent in the gold anaphora link of that pro-noun.
Criterion 2 is a bit looser when the recog-nized antecedent just needs to match one of theantecedents of a pronoun in its co-reference chain.This criterion has been used by most of the pre-vious works, including Denis and Baldridge?s sys-tem (Denis and Baldridge, 2007).4.2 Baseline resolverIn this experiment, we use the baseline feature setpresented in section 3.2.
One of the reasons inchoosing these features for the baseline system, isthat they are basic features that have been used bymost of the previous reference resolution systems.Moreover, we wanted to see how these featurescontribute to the resolution process for differentcorpora, presented in the next section.Our baseline system achieved a 71.41% successrate on the BNEWS data set (Table 6, criterion2), which is comparable to the result of Denis andBaldridge?s system on the same data set (Denis andBaldridge, 2007).
Moreover, we can see that thedifferences caused by the two criteria are not thesame for every data set.
For the news domain datasets, the differences vary from 4.17% (MUC) to6.8% (ACE), which is high in comparison with thepercentages of GENIA, which were less than twopercent.
This can be explained by the fact that pro-nouns in news texts are used more repeatedly thanthose in bio-medical texts.
Because bio-entities areneutral-gender mentions, and are referred by theneutral gender and third person pronouns, the re-peated use of pronouns may increase the ambiguityof the text, and confuse the readers.To prevent the confusion of the readers, wechose just one data set BNEWS to represent theACE corpus and present our further analysis ex-periments on these three data sets: GENIA, ACE(BNEWS), and MUC (MUC-7).4.3 Contributions of the features in thebaseline resolverIn order to observe the effects of the features in thebaseline pronoun resolver, we omitted each fea-ture group from the whole feature set, retrainedour resolution models with the new feature set,and applied them to the three data sets: GENIA,629Table 5: Features used in the pronoun resolverLayer Feature set Group Primitive Feature Explanation Examplemor.fundamental mention type P type pronoun type possessive pronounC type candidate mention type proper namebaselinesdist CP sdis distance in sentence 1tdist CP tdis normalized distance in token 17numb P numb number of p singularC numb number of c unknownpers P pers person of p third personC pers person of c third persongend P gend gender of p neutralC gend gender of c neutralpfam P pfam family of p itC pfam family of c nullstring P word pronoun string itssyn.C head candidate head string PMAadditionalposP lpos POS of the left word of p TOP rpos POS of the right word of p NNC lpos POS of the left word of c COMMAC rpos POS of the right word of c VBZ (is)parg P parg argument role of p nullC parg argument role of c arg1sem.
netype C netype entity type of c nullmor.
last3c C last3c the last 3 characters of c pmasyn.comb P semw see Section 3.2 effectother C 1stnp first NP in a sentence or not falseTable 6: Baseline system evalutation (C1: criterion1, C2: criterion 2, D: difference between criterion1 and 2)GENIA ACE MUCC1 70.31 64.61 57.08C2 71.43 71.41 61.25Diff 1.12 6.80 4.17BNEWS, and MUC.
Pronoun type and mentiontype are the most significant features, and thus, arenot omitted in this experiment.Table 7 shows the experimental results: the firstcolumn is the feature group name, and the follow-ing three columns show the resolution accuracy ofthe three corpora.
The figures in the parenthesesshow the degradation, when we exclude the corre-sponding group from the baseline feature set.
Ourdata analysis show some noticeable issues:Number features (numb) :The number-combination features are the mostsignificant features in bio-texts while they are notso effective on ACE, and even perform negativelyon MUC.
One of the reasons behind this, is thatin the bio-texts, all of the anaphoric pronounshave a deterministic number; i.e., either singularor plural (Section 2.3), while the news texts con-tain first- and second-person pronouns whose num-bers are unspecified.
Another reason emerges fromthe non-pronominal types of mentions, which playa role as antecedents.
The number property ofthese mentions is characterized in the markabledetection phase based on the part-of-speech tag,the head noun, and the phrase structure of thosementions.
In particular, the MUC corpus con-tains many coordinated-structured mentions (Sec-tion 2.1), which are difficult for markable charac-terization.Person features and pronoun family (persand pfam) :The absence of the pers features caused thebiggest loss for the resolution success rate on theACE corpus, because the co-reference chains inthis corpus contain a lot of pronouns, and it iseasier for the pronoun resolver to determine apronominal antecedent than to determine a non-pronominal antecedent.
The same phenomena canbe observed with pfam features.
The bio-text onlycontains third-person anaphoric pronouns (Section2.3), so the person features do not have any profits.Distance features (sdist and tdist) :Our baseline resolver again confirmed that thesentence distance is an indispensable feature inpronoun resolution.
However, the token-based dis-tance did not show any improvements on the MUCcorpus.
Analyzing the MUC anaphora links, wefound that these tdist features resulted in 10 cor-rect anaphora links, but also mis-recognized 10 an-tecedents.630Table 7: Feature contributions in the baseline sys-tem (evaluation criterion 1)Excluded GENIA ACE MUCnone 70.31 64.61 57.08-sdist 67.23(?3.08) 63.51(?1.10) 51.67(?5.41)-tdist 70.03(?0.28) 59.56(?5.05) 57.08(+0.00)-numb 65.83(?4.48) 61.77(?2.84) 58.33(+1.25)-pers 70.31(+0.00) 57.19(?7.42) 55.42(?1.66)-gend 69.75(?0.56) 64.45(?0.16) 56.67(?0.41)-pfam 71.15(+0.84) 63.51(?1.10) 57.92(+0.84)-string 68.07(?2.24) 61.93(?2.68) 55.83(?1.25)4.4 Contributions of additional features tothe baseline feature setIn addition to the baseline feature set, we enhancedour resolver with more features.
Among them,there are two noticeable features: the grammati-cal role of pronouns or antecedent candidates, andthe named entity type of the candidates.
The otherfeature groups are used in Denis and Baldridge?ssystem, which we also want to test in our system.Table 8 shows the resolution results and theincrease when adding the corresponding featuregroup.
With the exception of the last3c features,the others significantly improved the resolutionsuccess rate on bio-texts, although they did nothave clear contributions to the news domain datasets.
The following is our further analysis to seethe way that these features can contribute to thepronoun resolution process.Semantic features (netype)The first feature we would like to observe isthe combination of C netype and P semw features,which contributed to the increase by 3.64 points.We further conducted a small test by excluding thiscombination from the netype feature group, but thesuccess rate remained unchanged from the baselineresult.
This signifies that this combination con-tributed the most to the above increase.The combination of C netype and P semw fea-tures exploits the co-ocurrence of the semantictype of the candidate antecedent and the contextword, which appears in some relationship with thepronoun.
This combination feature uses the infor-mation similar to the semantic compatibility fea-tures proposed by Yang (Yang et al, 2005) andBergsma (Bergsma and Lin, 2006).
Dependingon the pronoun type, the feature extractor decideswhich relationship is used.
For example, the re-solver successfully recognizes the antecedent ofthe pronoun its in this discourse: ?HSF3 is con-stitutively expressed in the erythroblast cell lineHD6 , the lymphoblast cell line MSB , and em-bryo fibroblasts , and yet its DNA-binding activ-ity is induced only upon exposure of HD6 cells toheat shock ,?
because HSF3 was detected as a Pro-tein entity, which has a strong association with thegoverning head noun activity of the pronoun.Another example is the correct anaphora linkbetween ?it?
and ?the viral protein?
in the fol-lowing sentence, which the other features failed todetect.
?Tax , the viral protein , is thought to becrucial in the development of the disease , sinceit transforms healthy T cells in vitro and inducestumors in transgenic animals.?
The correct an-tecedent was recognized due to the bias given tothe association of the Protein entity type, and thegoverning verb, ?transform?
of the pronoun.
Theexperimental results show the contribution of thedomain knowledge to the pronoun resolution, andthe potential combination use of such knowledgewith the syntactic features.Parse features (parg)The combinations of the primitive features ofgrammatical roles significantly improved the per-formance of our resolver.
The following examplesshow the correct anaphora links resulting from us-ing the parse features:?
?By comparison, PMA is a very inefficient in-ducer of the jun gene family in Jurkat cells.Similar to its effect on the induction of AP1by okadaic acid, PMA inhibits the inductionof c-jun mRNA by okadaic acid.
?In this example, the possessive pronoun ?its?
inthe second sentence corefers to ?PMA?, the sub-ject of the preceding sentence.Among the combination features in this group,one noticeable feature is the combination ofC parg, Sdist, and P type which contains the as-sociation of the grammatical role of the candidate,the sentence-based distance, and the pronoun type.The idea of adding this combination is based onthe Centering theory (Walker et al, 1998), a the-ory of discourse successfully used in pronoun res-olution.
This simple feature shows the potential ofencoding centering theory in the machine learningfeatures, based on the parse information.Feature integrationFinally, we integrated all of the positive fea-ture groups for each data set in the above experi-ments, and tested this combining feature set.
Table631Table 8: Additional features and their contribu-tions (evaluation criterion 1)Added GENIA ACE MUCnone 70.31 64.61 57.08+pos 75.63(+5.32) 62.88(?1.73) 57.50(+0.42)+parg 73.67(+3.36) 63.82(?0.79) 58.75(+1.67)+netype 73.95(+3.64) 64.30(?0.31) 58.33(+1.25)+last3c 67.51(?2.80) 62.09(?2.52) 56.67(?0.41)+comb 72.83(+2.52) 63.82(?0.79) 56.25(?0.83)Table 9: Feature integrationGENIA ACE MUCC1 79.55 (+9.24) 64.61 (+0.00) 60.42 (+3.34)C2 80.95 (+9.52) 71.41 (+0.00) 66.25 (+5.00)9 shows a significant increase in the performanceof the resolver on GENIA and MUC.5 Conclusion and future workThrough the differences in the corpus annotationschemes, in the corpora themselves, and in contri-butions of resolution factors to the pronoun resolu-tion process, we can see that adapting pronoun res-olution for a different domain is not an easy task.
Agood study on the types of anaphoric pronouns andentity mention structures beforehand can help de-sign a better feature set for our machine learning-based pronoun resolution system and thus, cansave much time and labor.As shown in this paper, for the news do-main, the properties of anaphoric pronouns containrich information about their antecedents, which isvery useful in the resolution process.
While inbiomedical text, it is more important to capturethe information to connect a pronoun and its an-tecedent from their surrounding context, becausethe anaphoric pronouns themselves contain almostno information of their antecedents with the excep-tion of the numbers.As a future work, it would be interesting to seehow the system performs in other domains.
Moreexperiments should be designed to make the influ-ences of annotation schemes on the pronoun reso-lution process clearer.ReferencesBerger, Adam L., Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Bergsma, Shane and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL,pages 33?40.Castano, Jose, Jason Zhang, and James Pusterjovsky.2002.
Anaphora resolution in biomedical literature.In Int?l Symposium Reference Resolution in NLP.Denis, Pascal and J. Baldridge.
2007.
A ranking ap-proach to pronoun resolution.
In Proceedings of the20th International Joint Conference on Artificial In-telligence (IJCAI07).Haghighi, Aria and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 848?855.Hong, Huaqing.
2004.
Coreference annotation schemefor medco corpus.Kim, Jung-Jae and Jong C.Park.
2004.
Bioar:Anaphora resolution for relating protein names toproteome database entries.
In Proceedings of theACL 2004: Workshop on Reference Resolution andits Applications, pages 79?86.Lynette, Hirschman.
1997.
Muc-7 coreference taskdefinition.Mitkov, Ruslan.
2002.
Anaphora resolution.
PearsonEducation, London, Great Britain.NIST.
2003.
Entity detection and tracking - phrase1 edt and metonymy annotation guidelines version2.5.1 20030502.Soon, W., H. Ng, and D. Lim.
2001.
A machinelearning approach to coreference resolution of nounphrases.
Computational Linguistics, 27(4):521?544.Walker, Marilyn A., Aravind K. Joshi, and Ellen F.Prince.
1998.
Centering Theory in Discourse.Clarendon Press, Oxford.Yang, Xiaofeng, Jian Su, and Chew-Lim Tan.
2005.Improving pronoun resolution using statistics-basedsemantic compatibility information.
In Proceedingsof the 43rd Annual meeting of the Association forComputational Linguistics (ACL05), pages 427?434.632
