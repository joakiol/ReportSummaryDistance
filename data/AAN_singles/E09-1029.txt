Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246?254,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsCompany-Oriented Extractive Summarization of Financial News?Katja Filippova?, Mihai Surdeanu?, Massimiliano Ciaramita?, Hugo Zaragoza?
?EML Research gGmbH ?Yahoo!
ResearchSchloss-Wolfsbrunnenweg 33 Avinguda Diagonal 17769118 Heidelberg, Germany 08018 Barcelona, Spainfilippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.comAbstractThe paper presents a multi-document sum-marization system which builds company-specific summaries from a collection of fi-nancial news such that the extracted sen-tences contain novel and relevant infor-mation about the corresponding organiza-tion.
The user?s familiarity with the com-pany?s profile is assumed.
The goal ofsuch summaries is to provide informationuseful for the short-term trading of the cor-responding company, i.e., to facilitate theinference from news to stock price move-ment in the next day.
We introduce anovel query (i.e., company name) expan-sion method and a simple unsupervized al-gorithm for sentence ranking.
The sys-tem shows promising results in compari-son with a competitive baseline.1 IntroductionAutomatic text summarization has been a field ofactive research in recent years.
While most meth-ods are extractive, the implementation details dif-fer considerably depending on the goals of a sum-marization system.
Indeed, the intended use of thesummaries may help significantly to adapt a par-ticular summarization approach to a specific taskwhereas the broadly defined goal of preserving rel-evant, although generic, information may turn outto be of little use.In this paper we present a system whose goal isto extract sentences from a collection of financial?This work was done during the first author?s internshipat Yahoo!
Research.
Mihai Surdeanu is currently affiliatedwith Stanford University (mihais@stanford.edu).Massimiliano Ciaramita is currently at Google(massi@google.com).news to inform about important events concern-ing companies, e.g., to support trading (i.e., buy orsell) the corresponding symbol on the next day, ormanaging a portfolio.
For example, a company?sannouncement of surpassing its earnings?
estimateis likely to have a positive short-term effect on itsstock price, whereas an announcement of job cutsis likely to have the reverse effect.
We demonstratehow existing methods can be extended to achieveprecisely this goal.In a way, the described task can be classifiedas query-oriented multi-document summarizationbecause we are mainly interested in informationrelated to the company and its sector.
However,there are also important differences between thetwo tasks.?
The name of the company is not a query,e.g., as it is specified in the context of theDUC competitions1, and requires an exten-sion.
Initially, a query consists exclusivelyof the ?symbol?, i.e., the abbreviation of thename of a company as it is listed on the stockmarket.
For example, WPO is the abbrevia-tion used on the stock market to refer to TheWashington Post?a large media and educa-tion company.
Such symbols are rarely en-countered in the news and cannot be used tofind all the related information.?
The summary has to provide novel informa-tion related to the company and should avoidgeneral facts about it which the user is sup-posed to know.
This point makes the taskrelated to update summarization where onehas to provide the user with new information1http://duc.nist.gov; since 2008 TAC: http://www.nist.gov/tac.246given some background knowledge2.
In ourcase, general facts about the company are as-sumed to be known by the user.
Given WPO,we want to distinguish between The Wash-ington Post is owned by The Washington PostCompany, a diversified education and mediacompany and The Post recently went throughits third round of job cuts and reported an11% decline in print advertising revenues forits first quarter, the former being an exampleof background information whereas the lat-ter is what we would like to appear in thesummary.
Thus, the similarity to the queryalone is not the decisive parameter in com-puting sentence relevance.?
While the summaries must be specific for agiven organization, important but general fi-nancial events that drive the overall marketmust be included in the summary.
For exam-ple, the recent subprime mortgage crisis af-fected the entire economy regardless of thesector.Our system proceeds in the three steps illus-trated in Figure 1.
First, the company symbol isexpanded with terms relevant for the company, ei-ther directly ?
e.g., iPod is directly related to AppleInc.
?
or indirectly ?
i.e., using information aboutthe industry or sector the company operates in.
Wedetail our symbol expansion algorithm in Section3.
Second, this information is used to rank sen-tences based on their relatedness to the expandedquery and their overall importance (Section 4).
Fi-nally, the most relevant sentences are re-rankedbased on the degree of novelty they carry (Section5).The paper makes the following contributions.First, we present a new query expansion tech-nique which is useful in the context of company-dependent news summarization as it helps identifysentences important to the company.
Second, weintroduce a simple and efficient method for sen-tence ranking which foregrounds novel informa-tion of interest.
Our system performs well in termsof the ROUGE score (Lin & Hovy, 2003) com-pared with a competitive baseline (Section 6).2 DataThe data we work with is a collection of financialnews consolidated and distributed by Yahoo!
Fi-2See the DUC 2007 and 2008 update tracks.nance3 from various sources4.
Each story is la-beled as being relevant for a company ?
i.e., itappears in the company?s RSS feed ?
if the storymentions either the company itself or the sector thecompany belongs to.
Altogether the corpus con-tains 88,974 news articles from a period of about5 months (148 days).
Some articles are labeledas being relevant for several companies.
The totalnumber of (company name, news collection) pairsis 46,444.The corpus is cleaned of HTML tags, embed-ded graphics and unrelated information (e.g., ads,frames) with a set of manually devised rules.
Thefiltering is not perfect but removes most of thenoise.
Each article is passed through a languageprocessing pipeline (described in (Atserias et al,2008)).
Sentence boundaries are identified bymeans of simple heuristics.
The text is tokenizedaccording to Penn TreeBank style and each to-ken lemmatized using Wordnet?s morphologicalfunctions.
Part of speech tags and named entities(LOC, PER, ORG, MISC) are identified by meansof a publicly available named-entity tagger5 (Cia-ramita & Altun, 2006, SuperSense).
Apart fromthat, all sentences which are shorter than 5 tokensand contain neither nouns nor verbs are sorted out.We apply the latter filter as we are interested intextual information only.
Numeric informationcontained, e.g., in tables can be easily and morereliably obtained from the indices tables availableonline.3 Query ExpansionIn company-oriented summarization query expan-sion is crucial because, by default, our query con-tains only the symbol, that is the abbreviation ofthe name of the company.
Unfortunately, exist-ing query expansion techniques which utilize suchknowledge sources as WordNet or Wikipedia arenot useful for symbol expansion.
WordNet doesnot include organizations in any systematic way.Wikipedia covers many companies but it is unclearhow it can be used for expansion.3http://finance.yahoo.com4http://biz.yahoo.com, http://www.seekingalpha.com, http://www.marketwatch.com, http://www.reuters.com, http://www.fool.com, http://www.thestreet.com, http://online.wsj.com, http://www.forbes.com,http://www.cnbc.com, http://us.ft.com,http://www.minyanville.com5http://sourceforge.net/projects/supersensetag247ExpansionQueryExpandedQueryRelatednessto QueryFilteringRelevantSentencesRankingNoveltyCompanyProfileYahoo!
FinanceSymbolSummaryNewsFigure 1: System architectureIntuitively, a good expansion method shouldprovide us with a list of products, or properties,of the company, the field it operates in, the typi-cal customers, etc.
Such information is normallyfound on the profile page of a company at Yahoo!Finance6.
There, so called ?business summaries?provide succinct and financially relevant informa-tion about the company.
Thus, we use businesssummaries as follows.
For every company sym-bol in our collection, we download its businesssummary, split it into tokens, remove all wordsbut nouns and verbs which we then lemmatize.Since words like company are fairly uninforma-tive in the context of our task, we do not want toinclude them in the expanded query.
To filter outsuch words, we compute the company-dependentTF*IDF score for every word on the collection ofall business summaries:score(w) = tfw,c ?
log?Ncfw?
(1)where c is the business summary of a company,tfw,c is the frequency of w in c, N is the totalnumber of business summaries we have, cfw isthe number of summaries that contain w. Thisformula penalizes words occurring in most sum-maries (e.g., company, produce, offer, operate,found, headquarter, management).
At the mo-ment of running the experiments, N was about3,000, slightly less than the total number of sym-6http://finance.yahoo.com/q/pr?s=AAPLwhere the trading symbol of any company can be usedinstead of AAPL.bols because some companies do not have a busi-ness summary on Yahoo!
Finance.
It is impor-tant to point out that companies without a businesssummary are usually small and are seldom men-tioned in news articles: for example, these compa-nies had relevant news articles in only 5% of thedays monitored in this work.Table 1 gives the ten high scoring words forthree companies (Apple Inc. ?
the computer andsoftware manufacture, Delta Air Lines ?
the air-line, and DaVita ?
dyalisis services).
Table 1shows that this approach succeeds in expandingthe symbol with terms directly related to the com-pany, e.g., ipod for Apple, but also with more gen-eral information like the industry or the companyoperates in, e.g., software and computer for Apple.All words whose TF*IDF score is above a certainthreshold ?
are included in the expanded query (?was tuned to a value of 5.0 on the developmentset).4 Relatedness to QueryOnce the expanded query is generated, it can beused for sentence ranking.
We chose the system ofOtterbacher et al (2005) as a a starting point forour approach and also as a competitive baselinebecause it has been successfully tested in a simi-lar setting?it has been applied to multi-documentquery-focused summarization of news documents.Given a graph G = (S,E), where S is the setof all sentences from all input documents, and E isthe set of edges representing normalized sentencesimilarities, Otterbacher et al (2005) rank all sen-248AAPL DAL DVAapple air dialysismusic flight davitamac delta esrdsoftware lines kidneyipod schedule inpatientcomputer destination outpatientperipheral passenger patientmovie cargo hospitalplayer atlanta diseasedesktop fleet serviceTable 1: Top 10 scoring words for three companiestence nodes based on the inter-sentence relationsas well as the relevance to the query q. Sentenceranks are found iteratively over the set of graphnodes with the following formula:r(s, q) = ?rel(s|q)Pt?S rel(t|q)+(1??
)Xt?Ssim(s, t)Pv?S sim(v, t)r(t, q) (2)The first term represents the importance of a sen-tence defined in respect to the query, whereas thesecond term infers the importance of the sentencefrom its relation to other sentences in the collec-tion.
?
?
(0, 1) determines the relative importanceof the two terms and is found empirically.
Anotherparameter whose value is determined experimen-tally is the sentence similarity threshold ?
, whichdetermines the inclusion of a sentence in G. Ot-terbacher et al (2005) report 0.2 and 0.95 to bethe optimal values for ?
and ?
respectively.
Thesevalues turned out to produce the best results alsoon our development set and were used in all ourexperiments.
Similarity between sentences is de-fined as the cosine of their vector representations:sim(s, t) =Pw?s?t weight(w)2qPw?s weight(w)2 ?qPw?t weight(w)2(3)weight(w) = tfw,sidfw,S (4)idfw,S = log( |S| + 10.5 + sfw)(5)where tfw,s is the frequency of w in sentence s,|S| is the total number of sentences in the docu-ments from which sentences are to be extracted,and sfw is the number of sentences which containthe word w (all words in the documents as wellas in the query are stemmed and stopwords are re-moved from them).
Relevance to the query is de-fined in Equation (6) which has been previouslyused for sentence retrieval (Allan et al, 2003):rel(s|q) =Xw?qlog(tfw,s + 1) ?
log(tfw,q + 1) ?
idfw,S (6)where tfw,x stands for the number of times w ap-pears in x, be it a sentence (s) or the query (q).
Ifa sentence shares no words other than stopwordswith the query, the relevance becomes zero.
Notethat without the relevance to the query part Equa-tion 2 takes only inter-sentence similarity into ac-count and computes the weighted PageRank (Brin& Page, 1998).In defining the relevance to the query, in Equa-tion (6), words which do not appear in too manysentences in the document collection weigh more.Indeed, if a word from the query is contained inmany sentences, it should not count much.
But itis also true that not all words from the query areequally important.
As it has been mentioned inSection 3, words like product or offer appear inmany business summaries and are equally relatedto any company.
To penalize such words, whencomputing the relevance to the query, we multiplythe relevance score of a given word w with the in-verted document frequency of w on the corpus ofbusiness summaries Q ?
idfw,Q:idfw,Q = log( |Q|qfw)(7)We also replace tfw,s with the indicator functions(w) since it has been reported to be more ad-equate for sentences, in particular for sentencealignment (Nelken & Shieber, 2006):s(w) ={1 if s contains w0 otherwise(8)Thus, the modified formula we use to computesentence ranks is as follows:rel(s|q) =Xw?qs(w) ?
log(tfw,q + 1) ?
idfw,S ?
idfw,Q (9)We call these two ranking algorithms that usethe formula in (2) OTTERBACHER and QUERYWEIGHTS, the difference being the way the rel-evance to the query is computed: (6) or (9).
Weuse the OTTERBACHER algorithm as a baseline inthe experiments reported in Section 6.2495 Novelty BiasApart from being related to the query, a good sum-mary should provide the user with novel infor-mation.
According to Equation (2), if there are,say, two sentences which are highly similar to thequery and which share some words, they are likelyto get a very high score.
Experimenting with thedevelopment set, we observed that sentences aboutthe company, such as e.g., DaVita, Inc. is a lead-ing provider of kidney care in the United States,providing dialysis services and education for pa-tients with chronic kidney failure and end stage re-nal disease, are ranked high although they do notcontribute new information.
However, a non-zerosimilarity to the query is indeed a good filter of theinformation related to the company and to its sec-tor and can be used as a prerequisite of a sentenceto be included in the summary.
These observationsmotivate our proposal for a ranking method whichaims at providing relevant and novel informationat the same time.Here, we explore two alternative approaches toadd the novelty bias to the system:?
The first approach bypasses the relatednessto query step introduced in Section 4 com-pletely.
Instead, this method merges the dis-covery of query relatedness and novelty intoa single algorithm, which uses a sentencegraph that contains edges only between sen-tences related to the query, (i.e., sentences forwhich rel(s|q) > 0).
All edges connectingsentences which are unrelated to the queryare skipped in this graph.
In this way we limitthe novelty ranking process to a subset of sen-tences related to the query.?
The second approach models the problemin a re-ranking architecture: we take thetop ranked sentences after the relatedness-to-query filtering component (Section 4) and re-rank them using the novelty formula intro-duced below.The main difference between the two approachesis that the former uses relatedness-to-query andnovelty information but ignores the overall impor-tance of a sentence as given by the PageRank al-gorithm in Section 4, while the latter combines allthese aspects ?i.e., importance of sentences, relat-edness to query, and novelty?
using the re-rankingarchitecture.To amend the problem of general informationranked inappropriately high, we modify the word-weighting formula (4) so that it implements a nov-elty bias, thus becoming dependent on the query.A straightforward way to define the novelty weightof a word would be to draw a line between the?known?
words, i.e., words appearing in the busi-ness summary, and the rest.
In this approach allthe words from the business summary are equallyrelated to the company and get the weight of 0:weight(w) ={0 if Q contains wtfw,sidfw,S otherwise(10)We call this weighting scheme SIMPLE.
Asan alternative, we also introduce a more elab-orate weighting procedure which incorporatesthe relatedness-to-query (or rather distance fromquery) in the word weight formula.
Intuitively, themore related to the query a word is (e.g., DaVita,the name of the company), the more familiar to theuser it is and the smaller its novelty contributionis.
If a word does not appear in the query at all, itsweight becomes equal to the usual tfw,sidfw,S :weight(w) =1 ?
tfw,q ?
idfw,QPwi?qtfwi,q ?
idfwi,Q!?
tfw,sidfw,S (11)The overall novelty ranking formula is basedon the query-dependent PageRank introduced inEquation (2).
However, since we already incorpo-rate the relatedness to the query in these two set-tings, we focus only on related sentences and thusmay drop the relatedness to the query part from(2):r?
(s, q) = ?
+ (1 ?
?
)?t?Ssim(s, t, q)?u?S sim(t, u, q)(12)We set ?
to the same value as in OTTERBACHER.We deliberately set the sentence similarity thresh-old ?
to a very low value (0.05) to prevent thegraph from becoming exceedingly bushy.
Notethat this novelty-ranking formula can be equallyapplied in both scenarios introduced at the begin-ning of this section.
In the first scenario, S standsfor the set of nodes in the graph that contains onlysentences related to the query.
In the second sce-nario, S contains the highest ranking sentencesdetected by the relatedness-to-query component(Section 4).2505.1 Redundancy FilterSome sentences are repeated several times in thecollection.
Such repetitions, which should beavoided in the summary, can be filtered out ei-ther before or after the sentence ranking.
We ap-ply a simple repetition check when incrementallyadding ranked sentences to the summary.
If a sen-tence to be added is almost identical to the onealready included in the summary, we skip it.
Iden-tity check is done by counting the percentage ofnon-stop word lemmas in common between twosentences.
95% is taken as the threshold.We do not filter repetitions before the rank-ing has taken place because often such repetitionscarry important and relevant information.
The re-dundancy filter is applied to all the systems de-scribed as they are equally prone to include repe-titions.6 EvaluationWe randomly selected 23 company stock names,and constructed a document collection for eachcontaining all the news provided in the Yahoo!
Fi-nance news feed for that company in a period oftwo days (the time period was chosen randomly).The average length of a news collection is about600 tokens.
When selecting the company names,we took care of not picking those which have onlya few news articles for that period of time.
Thisresulted into 9.4 news articles per collection on av-erage.
From each of these, three human annotatorsindependently selected up to ten sentences.
All an-notators had average to good understanding of thefinancial domain.
The annotators were asked tochoose the sentences which could best help themdecide whether to buy, sell or retain stock for thecompany the following day and present them inthe order of decreasing importance.
The anno-tators compared their summaries of the first fourcollections and clarified the procedure before pro-ceeding with the other ones.
These four collec-tions were then later used as a development set.All summaries ?
manually as well as automat-ically generated ?
were cut to the first 250 wordswhich made the summaries 10 words shorter onaverage.
We evaluated the performance automat-ically in terms of ROUGE-2 (Lin & Hovy, 2003)using the parameters and following the methodol-ogy from the DUC events.
The results are pre-sented in Table 2.
We also report the 95% confi-dence intervals in brackets.
As in DUC, we usedMETHOD ROUGE-2Otterbacher 0.255 (0.226 - 0.285)Query Weights 0.289 (0.254 - 0.324)Novelty Bias (simple) 0.315 (0.287 - 0.342)Novelty Bias 0.302 (0.277 - 0.329)Manual 0.472 (0.415 - 0.531)Table 2: Results of the four extraction methodsand human annotatorsjackknife for each (query, summary) pair and com-puted a macro-average to make human and au-tomatic results comparable (Dang, 2005).
Thescores computed on summaries produced by hu-mans are given in the bottom line (MANUAL) andserve as upper bound and also as an indicator forthe inter-annotator agreement.6.1 DiscussionFrom Table 2 follows that the modifications weapplied to the baseline are sensible and indeedbring an improvement.
QUERY WEIGHTS per-forms better than OTTERBACHER and is in turnoutperformed by the algorithms biased to novel in-formation (the two NOVELTY systems).
The over-lap between the confidence intervals of the base-line and the simple version of the novelty algo-rithm is minimal (0.002).It is remarkable that the achieved improvementis due to a more balanced relatedness to the queryranking (9), as well as to the novelty bias re-ranking.
The fact that the simpler novelty weight-ing formula (10) produced better results than themore elaborated one (11) requires a deeper anal-ysis and a larger test set to explain the difference.Our conjecture so far is that the SIMPLE approachallows for a better combination of both noveltyand relatedness to query.
Since the more complexnovelty ranking formula penalizes terms relatedto the query (Equation (11)), it favors a scenariowhere novelty is boosted in detriment of related-ness to query, which is not always realistic.It is important to note that, compared with thebaseline, we did not do any parameter tuning for?
and the inter-sentence similarity threshold.
Theimprovement between the system of Otterbacheret al (2005) and our best model is statisticallysignificant.2516.2 System CombinationRecall from Section 5 that the motivation for pro-moting novel information came from the fact thatsentences with background information about thecompany obtained very high scores: they were re-lated but not novel.
The sentences ranked by OT-TERBACHER or QUERY WEIGHTS required a re-ranking to include related and novel sentences inthe summary.
We checked whether novelty re-ranking brings an improvement if added on topof a system which does not have a novelty bias(baseline or QUERY WEIGHTS) and compared itwith the setting where we simply limit the noveltyranking to all the sentences related to the query(NOVELTY SIMPLE and NOVELTY).
In the simi-larity graph, we left only edges between the first30 sentences from the ranked list produced byone of the two algorithms described in Section 4(OTTERBACHER or QUERY WEIGHTS).
Then weranked the sentences biased to novel informationthe same way as described in Section 5.
The re-sults are presented in Table 3.
What we evalu-ate here is whether a combination of two methodsperforms better than the simple heuristics of dis-carding edges between sentences unrelated to thequery.METHOD ROUGE-2Otterbacher + Novelty simple 0.280 (0.254 - 0.306)Otterbacher + Novelty 0.273 (0.245 - 0.301)Query Weights + Novelty simple 0.275 (0.247 - 0.302)Query Weights + Novelty 0.265 (0.242 - 0.289)Table 3: Results of the combinations of the fourmethodsFrom the four possible combinations, there isan improvement over the baseline only (0.255 vs.0.280 resp.
0.273).
None of the combinations per-forms better than the simple novelty bias algo-rithm on a subset of edges.
This experiment sug-gests that, at least in the scenario investigated here(short-term monitoring of publicly-traded compa-nies), novelty is more important than relatednessto query.
Hence, the simple novelty bias algo-rithm, which emphasizes novelty and incorporatesrelatedness to query only through a loose con-straint (rel(s|q) > 0) performs better than com-plex models, which are more constrained by therelatedness to query.7 Related WorkSummarization has been extensively investigatedin recent years and to date there exists a multi-tude of very different systems.
Here, we reviewthose that come closest to ours in respect to thetask and that concern extractive multi-documentquery-oriented summarization.
We also mentionsome work on using textual news data for stockindices prediction which we are aware of.Stock market prediction: Wu?thrich et al(1998) were among the first who introduced an au-tomatic stock indices prediction system which re-lies on textual information only.
The system gen-erates weighted rules each of which returns theprobability of the stock going up, down or remain-ing steady.
The only information used in the rulesis the presence or absence of certain keyphrasesprovided by a human expert who ?judged themto be influential factors potentially moving stockmarkets?.
In this approach, training data is re-quired to measure the usefulness of the keyphrasesfor each of the three classes.
More recently, Ler-man et al (2008) introduced a forecasting systemfor prediction markets that combines news anal-ysis with a price trend analysis model.
This ap-proach was shown to be successful for the fore-casting of public opinion about political candi-dates in such prediction markets.
Our approachcan be seen as a complement to both these ap-proaches, necessary especially for financial mar-kets where the news typically cover many events,only some related to the company of interest.Unsupervized summarization systems extractsentences whose relevance can be inferred fromthe inter-sentence relations in the document col-lection.
In (Radev et al, 2000), the centroid ofthe collection, i.e., the words with the highestTF*IDF, is considered and the sentences whichcontain more words from the centroid are ex-tracted.
Mihalcea & Tarau (2004) explore sev-eral methods developed for ranking documentsin information retrieval for the single-documentsummarization task.
Similarly, Erkan & Radev(2004) apply in-degree and PageRank to build asummary from a collection of related documents.They show that their method, called LexRank,achieves good results.
In (Otterbacher et al, 2005;Erkan, 2006) the ranking function of LexRank isextended to become applicable to query-focusedsummarization.
The rank of a sentence is deter-mined not just by its relation to other sentences in252the document collection but also by its relevanceto the query.
Relevance to the query is defined asthe word-based similarity between query and sen-tence.Query expansion has been used for improv-ing information retrieval (IR) or question answer-ing (QA) systems with mixed results.
One of theproblems is that the queries are expanded wordby word, ignoring the context and as a result theextensions often become inadequate7.
However,Riezler et al (2007) take the entire query into ac-count when adding new words by utilizing tech-niques used in statistical machine translation.Query expansion for summarization has not yetbeen explored as extensively as in IR or QA.Nastase (2008) uses Wikipedia and WordNet forquery expansion and proposes that a concept canbe expanded by adding the text of all hyper-links from the first paragraph of the Wikipediaarticle about this concept.
The automatic eval-uation demonstrates that extracting relevant con-cepts from Wikipedia leads to better performancecompared with WordNet: both expansion systemsoutperform the no-expansion version in terms ofthe ROUGE score.
Although this method provedhelpful on the DUC data, it seems less appropriatefor expanding company names.
For small compa-nies there are short articles with only a few links;the first paragraphs of the articles about largercompanies often include interesting rather thanrelevant information.
For example, the text pre-ceding the contents box in the article about AppleInc.
(AAPL) states that ?Fortune magazine namedApple the most admired company in the UnitedStates?8.
The link to the article about the For-tune magazine can be hardly considered relevantfor the expansion of AAPL.
Wikipedia categoryinformation, which has been successfully used insome NLP tasks (Ponzetto & Strube, 2006, interalia), is too general and does not help discriminatebetween two companies from the same sector.Our work suggests that query expansion isneeded for summarization in the financial domain.In addition to previous work, we also show that an-other key factor for success in this task is detectingand modeling the novelty of the target content.7E.g., see the proceedings of TREC 9, TREC 10: http://trec.nist.gov.8Checked on September 17, 2008.8 ConclusionsIn this paper we presented a multi-documentcompany-oriented summarization algorithmwhich extracts sentences that are both relevant forthe given organization and novel to the user.
Thesystem is expected to be useful in the context ofstock market monitoring and forecasting, that is,to help the trader predict the move of the stockprice for the given company.
We presented anovel query expansion method which works par-ticularly well in the context of company-orientedsummarization.
Our sentence ranking method isunsupervized and requires little parameter tuning.An automatic evaluation against a competitivebaseline showed supportive results, indicating thatthe ranking algorithm is able to select relevantsentences and promote novel information at thesame time.In the future, we plan to experiment with po-sitional features which have proven useful forgeneric summarization.
We also plan to test thesystem extrinsically.
For example, it would be ofinterest to see if a classifier may predict the moveof stock prices based on a set of features extractedfrom company-oriented summaries.Acknowledgments: We would like to thank theanonymous reviewers for their helpful feedback.ReferencesAllan, James, Courtney Wade & Alvaro Bolivar(2003).
Retrieval and novelty detection at thesentence level.
In Proceedings of the 26th An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval Toronto, On., Canada, 28 July ?
1 Au-gust 2003, pp.
314?321.Atserias, Jordi, Hugo Zaragoza, MassimilianoCiaramita & Giuseppe Attardi (2008).
Se-mantically annotated snapshot of the EnglishWikipedia.
In Proceedings of the 6th Interna-tional Conference on Language Resources andEvaluation, Marrakech, Morocco, 26 May ?
1June 2008.Brin, Sergey & Lawrence Page (1998).
Theanatomy of a large-scale hypertextual websearch engine.
Computer Networks and ISDNSystems, 30(1?7):107?117.Ciaramita, Massimiliano & Yasemin Altun(2006).
Broad-coverage sense disambiguation253and information extraction with a supersensesequence tagger.
In Proceedings of the 2006Conference on Empirical Methods in NaturalLanguage Processing, Sydney, Australia,22?23 July 2006, pp.
594?602.Dang, Hoa Trang (2005).
Overview of DUC2005.
In Proceedings of the 2005 DocumentUnderstanding Conference held at the HumanLanguage Technology Conference and Confer-ence on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada, 9?10 October 2005.Erkan, Gu?nes?
(2006).
Using biased random walksfor focused summarization.
In Proceedingsof the 2006 Document Understanding Confer-ence held at the Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,,New York, N.Y., 8?9 June 2006.Erkan, Gu?nes?
& Dragomir R. Radev (2004).LexRank: Graph-based lexical centrality assalience in text summarization.
Journal of Arti-ficial Intelligence Research, 22:457?479.Lerman, Kevin, Ari Gilder, Mark Dredze & Fer-nando Pereira (2008).
Reading the markets:Forecasting public opinion of political candi-dates by news analysis.
In Proceedings ofthe 22st International Conference on Computa-tional Linguistics, Manchester, UK, 18?22 Au-gust 2008, pp.
473?480.Lin, Chin-Yew & Eduard H. Hovy (2003).
Au-tomatic evaluation of summaries using N-gramco-occurrence statistics.
In Proceedings of theHuman Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics, Edmonton, Alberta,Canada, 27 May ?1 June 2003, pp.
150?157.Mihalcea, Rada & Paul Tarau (2004).
Textrank:Bringing order into texts.
In Proceedings of the2004 Conference on Empirical Methods in Nat-ural Language Processing, Barcelona, Spain,25?26 July 2004, pp.
404?411.Nastase, Vivi (2008).
Topic-driven multi-document summarization with encyclopedicknowledge and activation spreading.
In Pro-ceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Hon-olulu, Hawaii, 25?27 October 2008.
To appear.Nelken, Rani & Stuart M. Shieber (2006).
To-wards robust context-sensitive sentence align-ment for monolingual corpora.
In Proceedingsof the 11th Conference of the European Chapterof the Association for Computational Linguis-tics, Trento, Italy, 3?7 April 2006, pp.
161?168.Otterbacher, Jahna, Gu?nes?
Erkan & DragomirRadev (2005).
Using random walks forquestion-focused sentence retrieval.
In Pro-ceedings of the Human Language TechnologyConference and the 2005 Conference on Empir-ical Methods in Natural Language Processing,Vancouver, B.C., Canada, 6?8 October 2005,pp.
915?922.Ponzetto, Simone Paolo & Michael Strube (2006).Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Pro-ceedings of the Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,New York, N.Y., 4?9 June 2006, pp.
192?199.Radev, Dragomir R., Hongyan Jing & MalgorzataBudzikowska (2000).
Centroid-based summa-rization of mutliple documents: Sentence ex-traction, utility-based evaluation, and user stud-ies.
In Proceedings of the Workshop on Au-tomatic Summarization at ANLP/NAACL 2000,Seattle, Wash., 30 April 2000, pp.
21?30.Riezler, Stefan, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal & Yi Liu (2007).Statistical machine translation for query expan-sion in answer retrieval.
In Proceedings ofthe 45th Annual Meeting of the Association forComputational Linguistics, Prague, Czech Re-public, 23?30 June 2007, pp.
464?471.Wu?thrich, B, D. Permunetilleke, S. Leung, V. Cho,J.
Zhang & W. Lam (1998).
Daily prediction ofmajor stock indices from textual WWW data.
InIn Proceedings of the 4th International Confer-ence on Knowledge Discovery and Data Mining- KDD-98, pp.
364?368.254
