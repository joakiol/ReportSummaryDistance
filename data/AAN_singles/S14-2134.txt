Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 754?760,Dublin, Ireland, August 23-24, 2014.UniPi: Recognition of Mentions of Disorders in Clinical TextGiuseppe Attardi, Vittoria Cozza, Daniele SartianoDipartimento di InformaticaUniversit?
di PisaLargo B. Pontecorvo, 3I-56127 Pisa, Italy{attardi, cozza, sartiano}@di.unipi.itAbstractThe paper describes our experiments ad-dressing the SemEval 2014 task on theAnalysis of Clinical text.
Our approachconsists in extending the techniques ofNE recognition, based on sequence label-ling, to address the special issues of thistask, i.e.
the presence of overlapping anddiscontiguous mentions and the require-ment to map the mentions to unique iden-tifiers.
We explored using supervisedmethods in combination with word em-beddings generated from unannotated da-ta.1 IntroductionClinical records provide detailed information onexamination and findings of a patient consulta-tion expressed in a narrative style.
Such recordsabound in mentions of clinical conditions, ana-tomical sites, medications, and procedures,whose accurate identification is crucial for anyfurther activity of text mining.
Many differentsurface forms are used to represent the sameconcept and the mentions are interleaved withmodifiers, e.g.
adjectives, verb or adverbs, or areabbreviated involving implicit terms.For example, inAbdomen is soft, nontender,nondistended, negative bruitsthe mention occurrences are ?Abdomennontender?
and ?Abdomen bruits?, whichrefer to the disorders: ?nontender abdomen?and ?abdomininal bruit?, with only the sec-ond having a corresponding UMLS ConceptUnique Identifier (CUI).
In this case the twomentions overlap and both are interleaved withother terms, not part of the mentions.Secondly, mentions can be nested, as in thisexample:left pleural and parenchymalcalcificationswhere the mention calcifications is nestedwithin pleural calcifications.Mentions of this kind are a considerable de-parture from those dealt in typical Named Entityrecognition, which are contiguous and non-overlapping, and therefore they represents a newchallenge for text analysis.The analysis of clinical records poses addi-tional difficulties with respect to other biomedi-cal NER tasks, which use corpora from the med-ical literature.
Clinical records are entered bymedical personnel on the fly and so they containmisspellings and inconsistent use of capitaliza-tion.The task 7 at SemEval 2014, Analysis ofClinical Text, addresses the problem of recogni-tion of mentions of disorders and is divided intwo parts:A. recognition of mentions of bio-medicalconcepts that belong to the UMLS se-mantic group disorders;B. mapping of each disorder mention to aunique UMLS CUI (Concept UniqueIdentifiers).The challenge organizers provided the followingresources:?
A training corpus of clinical notes fromMIMIC II database manually annotatedfor disorder mentions and normalized toan UMLS CUI, consisting of 9432 sen-tences, with 5816 annotations.?
A collection of unannotated notes, consist-ing of 1,611,080 sentences.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/754We also had access to the UMLS ontology (Bo-denreider, 2004).Our approach to portion A of the task was toadapt a sequence labeller, which provides goodaccuracy in Named Entity recognition in thenewswire domain, to handle the peculiarities ofthe clinical domain.We performed mention recognition in twosteps:1. identifying contiguous portions of a men-tion;2. combining separated portions of mentionsinto a full mention.In order to use a traditional sequence tagger forthe first step, we had to convert the input datainto a suitable format, in particular, we dealt withnested mentions by transforming them into non-overlapping sequences, through replication.For recombining discontiguous mentions, weemployed a classifier, trained to recognizewhether pairs of mentions belong to the sameentity.
The classifier was trained using also fea-tures extracted from the dependency tree of asentence, in particular the distance of terms alongthe tree path.
Terms related by a dependencyhave distance 1 and terms having a commonhead have distance 2.
By limiting the pairs1 to beconsidered for combination to those within dis-tance 3, we both ensure that only plausible com-binations are performed and reduce the cost ofthe algorithm.For dealing with portion B of the task, we ap-ply fuzzy matching (Fraiser, 2011) between theextracted mentions and the textual description ofentities present in selected sections of UMLSdisorders.
The CUI from the match with highestscore is chosen.In the following sections, we describe how wecarried out the experiments, starting with the pre-processing of the data, then with the training ofseveral versions of NE recognizer, the training ofthe classifier for mention combination.
We thenreport on the results and discuss some error anal-ysis on the results.2 Preprocessing of the annotated dataThe training data was pre-processed, in order toobtain corpora in a suitable format for:1. training a sequence tagger2.
training the classifier for mention com-bination.1 Not implemented in the submitted runs.Annotations in the training data adopt a pipe-delimited stand-off character-offset format.
Theexample in the introduction has these annota-tions:00098-016139-DISCHARGE_SUMMARY.txt || Dis-ease_Disorder || C0221755 ||1141 || 1148 || 1192 || 119800098-016139-DISCHARGE_SUMMARY.txt || Dis-ease_Disorder || CUI-less ||1141 || 1148 || 1158 || 1167The first annotation marks Disease_Disorderas annotation type, C0221755 as CUI, while theremaining pairs of numbers represent characteroffsets within the original text that correspond tospans of texts containing the mention, i.e.
Abdo-men nondistended.
The second annotation issimilar and refers to Abdomen bruits.In order to prepare the training corpus for aNE tagger, the data had to be transformed andconverted into IOB2 notation.
However a stand-ard IOB notation does not convey informationabout overlapping or discontiguous mentions.In order to deal with overlapping mentions, asis the case for word ?Abdomen?
in our earlierexample, multiple copies of the sentence are pro-duced, each one annotated with disjoint men-tions.
If two mentions overlap, two versions aregenerated, one annotated with just the first men-tion and one with the second.
If several overlap-ping mentions are present in a sentence, copiesare generated for all possible combinations ofnon-overlapping mentions.For dealing with discontiguous mentions, eachannotated entity is assigned an id, uniquely iden-tifying the mention within the sentence.
This idis added as an extra attribute to each token, rep-resented as an extra column in the tab separatedIOB file format for the NE tagger.We processed with the Tanl pipeline (Attardiet al., 2009; Attardi et al., 2010).
We first ex-tracted the text from the training corpus in XMLformat and added the mentions annotations astags enclosing them, with spans and mentions idas attributes.
We then applied sentence splitting,tokenization, PoS tagging and dependency pars-ing using DeSR (Attardi, 2006).The tags were converted to IOB format.Here are two sample tokens in the resultingannotation, with attributes id, form, pos, head,deprel, entity, entity id:2 http://en.wikipedia.org/wiki/Inside_Outside_Beginning7551 Abdomen NNP 2 SBJ B-DISO 1?5 nontender NN 10 NMOD B-DISO 13 Named Entity TaggingThe core of our approach relies on an initialstage of Named Entity recognition.
We per-formed several experiments, using different NEtaggers in different configurations and using bothfeatures from the training corpus and featuresobtained from the unannotated data.3.1 Tanl NERWe performed several experiments using theTanl NE Tagger (Attardi et al., 2009), a generic,customizable statistical sequence labeller, suita-ble for many tasks of sequence labelling, such asPOS tagging or Named Entity Recognition.The tagger implements a Conditional MarkovModel and can be configured to use differentclassification algorithms and to specify featuretemplates for extracting features.
In our experi-ments we used a linear SVM classification algo-rithm.We experimented with several configurations,all including a set of word shape features, as in(Attardi et al., 2009): (1) the previous word iscapitalized; (2) the following word is capitalized;(3) the current word is in upper case; (4) the cur-rent word is in mixed case; (5) the current wordis a single uppercase character; (6) the currentword is a uppercase character and a dot; (7) thecurrent word contains digits; (8) the current wordis two digits; (9) the current word is four digits;(10) the current word is made of digits and ?/?
;(11) the current word contains ?$?
; (12) the cur-rent word contains ?%?
; (13) the current wordcontains an apostrophe; (14) the current word ismade of digits and dots.A number of dictionary features were alsoused, including prefix and suffix dictionaries,bigrams, last words, first word and frequentwords, all extracted from the training corpus.Additionally, a dictionary of disease terms wasused, consisting of about 22,000 terms extractedfrom the preferred terms for CUIs belonging tothe UMLS semantic type ?Disease or Syn-drome?.The first character of the POS tag was alsoused as feature, extracted from a window of to-kens before and after the current token.Finally attribute features are extracted fromattributes (Form, PoS, Lemma, NE, Disease) ofsurrounding tokens, denoted by their relative po-sition to the current token.
The best combinationof Attribute features obtained with runs on thedevelopment set was the following:Feature TokensPOS[0] wi-2 wi?1 wi wi+1DISEASE wi wi+1 wi+2Table 1.
Attribute features used in the runs.3.2 Word EmbeddingsWe explored ways to use the unannotated data inNE recognition by exploiting word embeddings(Collobert et al, 2011).
In a paper published afterour submission, Tang et al.
(2014) show thatword embeddings are beneficial to BiomedicalNER.We used the word embeddings for 100,000terms created through deep learning on the Eng-lish Wikipedia by Al-Rfou et al.
(2013).
We thenbuilt, with the same procedure, embedding forterms from the supplied unlabelled data.
Thecorpus was split, tokenized and normalized and avocabulary was created with the most frequentwords not already present among the Wikipediaword embeddings.
Four versions of the embed-dings were created, varying the size of the vo-cabulary and the size of the context window, asdescribed in Table 1.Run1 Run2 Run3 Run4Vocabulary size 50,000 50,000 30,000 30,000Context     5 2 5 2Hidden Layers 32 32 32 32Learning Rate 0.1 0.1 0.1 0.1Embedding size 64 64 64 64Table 2.
Word Embedding Parameters.We developed and trained a Deep Learning NEtagger (nlpnet, 2014) based on the SENNA archi-tecture (SENNA, 2011) using these word em-beddings.As an alternative to using the embeddings di-rectly as features, we created clusters of wordembeddings using the Dbscan algorithm (Ester etal., 1996) implemented in the sklearn library.
Wecarried out several experiments, varying the pa-rameters of the algorithm.
The configuration thatproduced the largest number of clusters had 572clusters.
The clusters turned out not to be muchsignificant, since a single cluster had about29,000 words, another had 5,000 words, and theothers had few, unusual words.We added the clusters as a dictionary featureto our NE tagger.
Unfortunately, most of the756terms fell within 4 clusters, so the feature turnedout to be little discriminative.3.3 Stanford NERWe performed experiments also with a taggerbased on a different statistical approach: theStanford Named Entity Recognizer.
This taggeris based on the Conditional Random Fields(CRF) statistical model and uses Gibbs samplinginstead of other dynamic programming tech-niques for inference on sequence models (Finkelet al., 2005).
This tagger normally works wellenough using just the form of tokens as featureand we applied it so.3.4 NER accuracyWe report the accuracy of the various NE taggerswe tested on the development set, using the scor-er from the CoNLL Shared Task 2003 (TjongKim Sang and De Meulder, 2003).We include here also the results withCRFsuite, the CRF tagger used in (Tang et al.,2014).NER Precision Recall F- scoreTanl 80.41 65.08 71.94Tanl+clusters     80.43 64.48 71.58nlpnet 80.29 62.51 70.29Stanford 80.30 64.89 71.78CRFsuite 79.69 61.97 69.72Table 3.
Accuracy of various NE taggers on thedevelopment set.Based on these results we chose the Tanl taggerand the Stanford NER for our submitted runs.All these taggers are known to be capable ofachieving state of the art performance or close toit (89.57 F1) in the CoNLL 2003 shared task onthe WSJ Penn Treebank.The accuracy on the current benchmark ismuch lower, despite the fact that there is onlyone category and the terminology for disorders isdrawn from a restricted vocabulary.It has been noted by Dingare et al.
(2005) thatNER over biomedical texts achieves lower accu-racy compared to other domains, quite within therange of the above results.
Indeed, comparedwith the newswire domain or other domains, theentities in the biomedical domain tend to be morecomplex, without the distinctive shape featuresof the newswire categories.4 Discontiguous mentionsDiscontiguous mention detection can be formu-lated as a problem of deciding whether two con-tiguous mentions belong to the same mention.
Assuch, it can be cast into a classification problem.A similar approach was used successfully for thecoreference resolution task at SemEval 2010 (At-tardi, Dei Rossi et al., 2010)4.1 Mentions  detectionWe trained a Maximum Entropy classifier(Ratnaparkhi, 1996) to recognize whether twoterms belong to the same mention.The training instances for the pair-wise learnerconsist of each pair of terms within a sentenceannotated as disorders.
A positive instance iscreated if the terms belong to the same mention,negative otherwise.The classifier was trained using the followingfeatures, extracted for each pair of words for dis-eases.Distance features?
Token distance: quantized distance be-tween the two words;?
Ancestor distance: quantized distance be-tween the words in the parse tree if one isthe ancestor of the otherSyntax features?
Head: whether the two words have thesame head;?
DepPath: concatenation of the dependen-cy relations of the two words to theircommon parentDictionary features?
UMLS: whether the two words are bothpresent in an UMLS definitionThe last feature is motivated by the fact that, ac-cording to the task description, most of the dis-order mentions correspond to diseases in theSNOMED terminology.4.2 Merging of mentionsThe mentions detected in the first phase aremerged using the following process.
Sentenceare parsed and then for each pair of words thatare tagged as disorder, features are extracted andpassed to the classifier.If the classifier assigns a probability greaterthan a given threshold the two words are com-bined into a larger mention.
The process is thenrepeated trying to further extend each mention757with additional terms by combining mentionsthat share a word.5 Mapping entities to CUIsTask B requires mapping each recognized entityto a concept in the SNOMED-CT terminology,assigning to it a unique UMLS CUI, if possible,or else marking it as CUI-less.
The CUIs arelimited to those corresponding to SNOMEDcodes and belonging to the following UMLS se-mantic types: ?Acquired Abnormality" or ?Con-genital Abnormality", ?Injury or Poisoning","Pathologic Function", "Disease or Syndrome","Mental or Behavioral Dysfunction", "Cell orMolecular Dysfunction", "Experimental Modelof Disease" or "Anatomical Abnormality", ?Ne-oplastic Process" or "Sign or Symptom".In order to speed up search, we created twoindices: an inverted index from words in the def-inition of a CUI to the corresponding CUI and aforward index from a CUI to its definition.For assigning a CUI to a mention, we searchin the dictionary of CUI preferred terms, first foran exact match, then for a normalized  mentionand finally for a fuzzy match (Fraiser, 2011).Normalization entails dropping punctuation andstop words.
Fuzzy matching is sometimes tooliberal, for example it matches ?chronic ob-structive pulmonary?
with ?chronic ob-structive lung disease?
; so we also put aceiling on the edit distance between the phrases.The effectiveness of the process is summa-rized in these results on the development set:ExactmatchesNormalizedmatchesFuzzymatchesNomatches1352 868 304 5488Table 4.
CUI identifications on the devel set.6 ExperimentsThe training corpus for the submission consistedof the merge of the train and development sets.We submitted three runs, using different ordifferently configured NE tagger.Two runs were submitted using the Tanl tag-ger using the features listed in Table 5, whereDISEASE and CLUSTER meaning is explainedearlier.Feature UniPI_run0 UniPI_ run1POS[0] wi-2 wi?1 wi wi+1 wi-2 wi?1 wi wi+1CLUSTER wi wi+1 wi wi+1DISEASE wi wi+1 wi+2Table 5.
Attribute features used in the runs.Since the clustering produced few large clusters,the inclusion of this feature did not affect sub-stantially the results.A third run (UniPi_run_2) was performed us-ing the Stanford NER with default settings.7 ResultsThe results obtained in the three submitted runs,are summarized in Table 6, in terms of accuracy,precision, recall and F-score.
For comparison,also the results obtained by the best performingsystems are included.Run Precision Recall F- scoreTask AUnipi_run0 0.539 0.684 0.602Unipi_run1     0.659 0.612 0.635Unipi_run2 0.712 0.601 0.652SemEval best 0.843 0.786 0813Task A relaxedUnipi_run0 0.778 0.885 0.828Unipi_run1 0.902 0.775 0.834Unipi_run2 0.897 0.766 0.826SemEval best 0.936 0.866 0.900Table 6.
UniPI Task A results, compared to thebest submission.Run AccuracyTask BUnipi_run0 0.467Unipi_run1     0.428Unipi_run2 0.417SemEval best 0.741Task B relaxedUnipi_run0 0.683Unipi_run1 0.699Unipi_run2 0.693SemEval best 0.873Table 7.
UniPI Task B results, compared to thebest submission.8 Error analysisSince the core step of our approach is the NErecognition, we tried to analyze possible causesof its errors.Some errors might be due to mistakes by thePOS tagger.
For example, often some words oc-cur in full upper case, leading to classify adjec-tives like ABDOMINAL as NNP instead of JJ.Training our POS tagger on the GENIA corpusor using the GENIA POS tagger might havehelped a little.
Spelling errors like abdominla758instead of abdominal could also have been cor-rected.Another choice that might have affected theNER accuracy was our decision to duplicate thesentences in order to remove mention overlaps.An alternative solution might have been to usetwo categories in the IOB annotation: one cate-gory for full contiguous disorder mentions andanother for partial disorder mentions.
This mighthave reduced the confusion in the tagger, sinceisolated words like abdomen get tagged as dis-order, having been so annotated in the trainingset.
Distinguishing the two cases, abdomenwould become a disorder mention in the step ofmention merging.
Counting the errors in the de-velopment set we found that 939 out of the 1757errors were indeed individual words incorrectlyidentified as disorders.8.1 After submission experimentsAfter the submission, we changed the algorithmfor merging mentions, in order to avoid nestedspans, retaining only the larger one.
Tests on thedevelopment set show that this change leads to asmall improvement in the strict evaluation:Run Precision Recall F- scoreTask Adevel_run1 0.596 0.653 0.624devel run1_after 0.668 0.637 0.652Task A relaxeddevel_run1 0.865 0.850 0.858devel run1_after 0.864 0.831 0.847Table 8.
UniPI Task A post submission results.9 ConclusionsWe reported our participation to SemEval 2014on the Analysis of Clinical Text.
Our approach isbased on using a NER, for identifying contiguousmentions and on a Maximum Entropy classifierfor merging discontiguous ones.The training data was transformed into a for-mat suitable for a standard NE tagger, that doesnot accept discontiguous or nested mentions.
Ourmeasurements on the development set showedthat different NE tagger reach a similar accuracy.We explored using word embeddings as fea-tures, generated from the unsupervised data pro-vided, but they did not improve the accuracy ofthe NE tagger.AcknowledgementsPartial support for this work was provided byproject RIS (POR RIS of the Regione Toscana,CUP n?
6408.30122011.026000160).ReferencesRami Al-Rfou?, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed Word Representationsfor Multilingual NLP.
In Proceedings of Confer-ence on Computational Natural Language Learn-ing, CoNLL ?13,  pages 183-192, Sofia, Bulgaria.Giuseppe Attardi.
2006.
Experiments with a Mul-tilanguage Non-Projective Dependency Parser.
InProceedings of the Tenth Conference on NaturalLanguage Learning, CoNLL ?06, pages 166-170,New York, NY.Giuseppe Attardi et al., 2009.
Tanl  (Text Analyticsand Natural Language Processing).
SemaWiki pro-ject: http://medialab.di.unipi.it/wiki/SemaWiki.Giuseppe Attardi, Stefano Dei Rossi, Felice Dell'Or-letta and Eva Maria Vecchi.
2009.
The TanlNamed Entity Recognizer at Evalita 2009.
In Pro-ceedings of Workshop Evalita?09 - Evaluation ofNLP and Speech Tools for Italian, Reggio Emilia,ISBN 978-88-903581-1-1.Giuseppe Attardi, Felice Dell'Orletta, Maria Simi andJoseph Turian.
2009.
Accurate Dependency Pars-ing with a Stacked Multilayer Perceptron.
In Pro-ceedings of Workshop Evalita?09 - Evaluation ofNLP and Speech Tools for Italian, Reggio Emilia,ISBN 978-88-903581-1-1.Giuseppe Attardi, Stefano Dei Rossi and Maria Simi.2010.
The Tanl Pipeline.
In Proceedings of LRECWorkshop on Web Services and Processing Pipe-lines in HLT, WSPP, La Valletta, Malta, pages 14-21Giuseppe Attardi, Stefano Dei Rossi and Maria Simi.2010.
TANL-1: Coreference Resolution by ParseAnalysis and Similarity Clustering.
In Proceedingsof the 5th International Workshop on SemanticEvaluation, SemEval 2010, Uppsala, Sweden, pag-es 108-111Olivier Bodenreider.
2004.
The Unified Medical Lan-guage System (UMLS): integrating biomedicalterminology.
Nucleic Acids Research, vol.
32, no.supplement 1, pages D267?D270.Ronan Collobert et al.
2011.
Natural Language Pro-cessing (Almost) from Scratch.
Journal of MachineLearning Research, 12, pages 2461?2505.Shipra Dingare, Malvina Nissim, Jenny Finkel, Chris-topher Manning and Claire Grover.
2005.
A Sys-tem for Identifying Named Entities in BiomedicalText: how Results From two Evaluations Reflecton Both the System and the Evaluations.
CompFunct Genomics.
Feb-Mar; 6(1-2): pages 77?85.759Martin Ester, et al.
1996.
A density-based algorithmfor discovering clusters in large spatial databaseswith noise.
In Proceedings of 2nd InternationalConference on Knowledge Discovery and Data Mi-ing, KDD 96, pages 226?231.Jenny Rose Finkel, Trond Grenager and ChristopherManning 2005.
Incorporating Non-local Infor-mation into Information Extraction Systems byGibbs Sampling.
In Proceedings of the 43nd Annu-al Meeting of the Association for ComputationalLinguistics, 2005, pages 363?370.Neil Fraser.
2011.
Diff, Match and Patch libraries forPlain Text.
(Based on Myer's diff algorithm).Adwait Ratnaparkhi.
1996.
A Maximum EntropyPart-Of-Speech Tagger.
In Proceedings of the Em-pirical Methods in Natural Language ProcessingConference, EMNLP ?96, pages 17-18.Buzhou Tang, Hongxin Cao, Xiaolong Wang, QingcaiChen, and Hua Xu.
2014.
Evaluating Word Repre-sentation Features in Biomedical Named EntityRecognition Tasks.
BioMed Research Internation-al, Volume 2014, Article ID 240403.Erik F. Tjong Kim Sang and Fien De Meulder 2003.Introduction to the CoNLL ?03 Shared Task: Lan-guage-Independent Named Entity Recognition.
In:Proceedings of CoNLL ?03, Edmonton, Canada,pages 142-147.SENNA.
2011. http://ml.nec-labs.com/senna/nlpnet.
2014. https://github.com/attardi/nlpnet760
