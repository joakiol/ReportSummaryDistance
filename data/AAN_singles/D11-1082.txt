Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 889?898,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Correction Model for Word AlignmentsJ.
Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, Jian-ming XuIBM T.J. Watson Research Center1101 Kitchawan Road, Rt.
134Yorktown Heights, NY 10598{jsmc,abei,roukos,bxiang,jianxu}@us.ibm.comAbstractModels of word alignment built as sequencesof links have limited expressive power, but areeasy to decode.
Word aligners that model thealignment matrix can express arbitrary align-ments, but are difficult to decode.
We pro-pose an alignment matrix model as a cor-rection algorithm to an underlying sequence-based aligner.
Then a greedy decoding al-gorithm enables the full expressive power ofthe alignment matrix formulation.
Improvedalignment performance is shown for all ninelanguage pairs tested.
The improved align-ments also improved translation quality fromChinese to English and English to Italian.1 IntroductionWord-level alignments of parallel text are crucial forenabling machine learning algorithms to fully uti-lize parallel corpora as training data.
Word align-ments appear as hidden variables in IBM Models 1-5 (Brown et al, 1993) in order to bridge a gap be-tween the sentence-level granularity that is explicitin the training data, and the implicit word-level cor-respondence that is needed to statistically model lex-ical ambiguity and word order rearrangements thatare inherent in the translation process.
Other no-table applications of word alignments include cross-language projection of linguistic analyzers (such asPOS taggers and named entity detectors,) a subjectwhich continues to be of interest.
(Yarowsky et al,2001), (Benajiba and Zitouni, 2010)The structure of the alignment model is tightlylinked to the task of finding the optimal alignment.Many alignment models are factorized in order touse dynamic programming and beam search for ef-ficient marginalization and search.
Such a factoriza-tion encourages - but does not require - a sequential(often left-to-right) decoding order.
If left-to-rightdecoding is adopted (and exact dynamic program-ming is intractable) important right context may ex-ist beyond the search window.
For example, the link-age of an English determiner may be considered be-fore the linkage of a distant head noun.An alignment model that jointly models all of thelinks in the entire sentence does not motivate a par-ticular decoding order.
It simply assigns comparablescores to the alignment of the entire sentence, andmay be used to rescore the top-N hypotheses of an-other aligner, or to decide whether heuristic pertur-bations to the output of an existing aligner constitutean improvement.
Both the training and decoding offull-sentence models have presented difficulties inthe past, and approximations are necessary.In this paper, we will show that by using an ex-isting alignment as a starting point, we can make asignificant improvement to the alignment by propos-ing a series of heuristic perturbations.
In effect, wetrain a model to fix the errors of the existing aligner.From any initial alignment configuration, these per-turbations define a multitude of paths to the refer-ence (gold) alignment.
Our model learns alignmentmoves that modify an initial alignment into the ref-erence alignment.
Furthermore, the resulting modelassigns a score to the alignment and thus could beused in numerous rescoring algorithms, such as top-N rescorers.In particular, we use the maximum entropy frame-889work to choose alignment moves.
The model is sym-metric: source and target languages are interchange-able.
The alignment moves are sufficiently rich toreach arbitrary phrase to phrase alignments.
Sincemost of the features in the model are not language-specific, we are able to test the correction modeleasily on nine language pairs; our corrections im-proved the alignment quality compared to the inputalignments in all nine.
We also tested the impact ontranslation and found a 0.48 BLEU improvement onChinese to English and a 1.26 BLEU improvementon English to Italian translation.2 Alignment sequence modelsSequence models are the traditional workhorse forword alignment, appearing, for instance, in IBMModels 1-5.
This type of alignment model is notsymmetric; interchanging source and target lan-guages results in a different aligner.
This parameter-ization does not allow a target word to be linked tomore than one source word, so some phrasal align-ments are simply not considered.
Often the choice ofdirectionality is motivated by this restriction, and thechoice of tokenization style may be designed (Lee,2004) to reduce this problem.
Nevertheless, alignersthat use this parameterization internally often incor-porate various heuristics in order to augment theiroutput with the disallowed alignments - for example,swapping source and target languages to obtain asecond alignment (Koehn et al, 2007) with differentlimitations.
Training both directions jointly (Lianget al, 2006) and using posterior probabilities dur-ing alignment prediction even allows the model tosee limited right context.
Another alignment combi-nation strategy (Deng and Zhou, 2009) directly op-timizes the size of the phrase table of a target MTsystem.Generative models (such as Models 1-5, and theHMM model (Vogel et al, 1996)) motivate a narra-tive where alignments are selected left-to-right andtarget words are then generated conditioned uponthe alignment and the source words.
Generativemodels are typically trained unsupervised, from par-allel corpora without manually annotated word-levelalignments.Discriminative models of alignment incorporatesource and target words, as well as more linguisti-cally motivated features into the prediction of align-ment.
These models are trained from annotatedword alignments.
Examples include the maximumentropy model of (Ittycheriah and Roukos, 2005) orthe conditional random field jointly normalized overthe entire sequence of alignments of (Blunsom andCohn, 2006).3 Joint ModelsAn alternate parameterization of alignment is thealignment matrix (Niehues and Vogel, 2008).
For asource sentence F consisting of words f1...fm, anda target sentence E = e1...el, the alignment matrixA = {?ij} is an l ?
m matrix of binary variables.If ?ij = 1, then ei is said to be linked to fj .
If eiis unlinked then ?ij = 0 for all j.
There is no con-straint limiting the number of source tokens to whicha target word is linked either; thus the binary ma-trix allows some alignments that cannot be modeledby the sequence parameterization.
All 2lm binarymatrices are potentially allowed in alignment matrixmodels.
For typical l and m, 2lm  (m + 1)l, thenumber of alignments described by a comparable se-quence model.
This parameterization is symmetric -if source and target are interchanged, then the align-ment matrix is transposed.A straightforward approach to the alignment ma-trix is to build a log linear model (Liu et al, 2005)for the probability of the alignment A.
(We continueto refer to ?source?
and ?target?
words only for con-sistency of notation - alignment models such as thisare indifferent to the actual direction of translation.
)The log linear model for the alignment (Liu et al,2005) isp(A|E,F ) = exp (?i ?i?i(A,E, F ))Z(E,F ) (1)where the partition function (normalization) is givenbyZ(E,F ) =?Aexp(?i?i?i(A,E, F )).
(2)Here the ?i(A,E, F ) are feature functions.
Themodel is parameterized by a set of weights ?i, onefor each feature function.
Feature functions are oftenbinary, but are not required to be.
Feature functions890may depend upon any number of components ?ij ofthe alignment matrix A.The sum over all alignments of a sentence pair(2lm terms) in the partition function is computa-tionally impractical except for very short sentences,and is rarely amenable to dynamic programming.Thus the partition function is replaced by an ap-proximation.
For example, the sum over all align-ments may be restricted to a sum over the n-bestlist from other aligners (Liu et al, 2005).
This ap-proximation was found to be inconsistent for smalln unless the merged results of several aligners wereused.
Alternately, loopy belief propagation tech-niques were used in (Niehues and Vogel, 2008).Loopy belief propagation is not guaranteed to con-verge, and feature design is influenced by consider-ation of the loops created by the features.
Outsideof the maximum entropy framework, similar modelshave been trained using maximum weighted bipar-tite graph matching (Taskar et al, 2005), averagedperceptron (Moore, 2005), (Moore et al, 2006), andtransformation-based learning (Ayan et al, 2005).4 Alignment Correction ModelIn this section we describe a novel approach to wordalignment, in which we train a log linear (maximumentropy) model of alignment by viewing it as correc-tion model that fixes the errors of an existing aligner.We assume a priori that the aligner will start froman existing alignment of reasonable quality, and willattempt to apply a series of small changes to thatalignment in order to correct it.
The aligner naturallyconsists of a move generator and a move selector.The move generator perturbs an existing align-ment A in order to create a set of candidate align-mentsMt(A), all of which are nearby to A in thespace of alignments.
We index the set of moves bythe decoding step t to indicate that we generate en-tirely different (even non-overlapping) sets of movesat different steps t of the alignment prediction.
Typ-ically the moves affect linkages local to a particularword, e.g.
the t?th source word.The move selector then chooses one of the align-ments At+1 ?
Mt(At), and proceeds iteratively:At+2 ?
Mt+1(At+1), etc.
until suitable termina-tion criteria are reached.
Pseudocode is depicted inFig.
(1.)
In practice, one move for each source andInput: sentence pair E1 .. El, F1 .. FmInput: alignment AOutput: improved alignment Afinalfor t = 1?
l dogenerate moves:Mt(At)select move:At+1 ?
argmaxA?Mt(At)p(A|At, E, F )Afinal ?
Al+1{repeat for source words}Figure 1: pseudocode for alignment correctiontarget word is sufficient.4.1 Move generationMany different types of alignment perturbations arepossible.
Here we restrict ourselves to a very sim-ple move generator that changes the linkage of ex-actly one source word at a time, or exactly one targetword at a time.
Many of our corrections are simi-lar to those of (Setiawan et al, 2010), although ourmotivation is perhaps closer to (Brown et al, 1993),who used similar perturbations to approximate in-tractable sums that arise when estimating the param-eters of the generative models Models 3-5, and ap-proach refined in (Och and Ney, 2003).
We note thatour corrections are designed to improve even a high-quality starting alignment; in contrast the model of(Fossum et al, 2008) considers deletion of linksfrom an initial alignment (union of aligners) that islikely to overproduce links.From the point of view of the alignment ma-trix, we consider changes to one row or one col-umn (generically, one slice) of the alignment matrix.At each step t, the move setMt(At) is formed bychoosing a slice of the current alignment matrix At,and generating all possible alignments from a fewfamilies of moves.
Then the move generator picksanother slice and repeats.
The m + l slices are cy-cled in a fixed order: the first m slices correspond tosource words (ordered according to a heuristic top-down traversal of the dependency parse tree if avail-able), and the remaining l slices correspond to targetwords, similarly parse-ordered.
For each slice weconsider the following families of moves, illustratedby rows.?
add link to row i - for one j such that ?ij = 0,891make ?ij = 1 (shown here for row i = 1.)?
?
?a ?
?
?b ?
?
?c ?
?
?=??
?
?a ?
?
?b ?
?
?c ?
?
??
remove one or more links from row i - for somej such that ?ij = 1, make ?ij = 0 (shown herefor i = 3.)?
?
?a ?
?
?b ?
?
?c ?
?
?=??
?
?a ?
?
?b ?
?
?c ?
?
??
move a link in row i - for one j and one j?
suchthat ?ij = 1 and ?ij?
= 0, make ?ij = 0 and?ij?
= 1 (shown here for i = 1.)?
?
?a ?
?
?b ?
?
?c ?
?
?=??
?
?a ?
?
?b ?
?
?c ?
?
??
leave row i unchangedSimilar families of moves apply to column slices(source words.)
In practice, perturbations are re-stricted by a window (typically ?5 from existinglinks.)
If the given source word is unlinked, weconsider adding a link to each target word in a win-dow (?5 from nearby links.)
The window size re-strictions mean that some reference alignments arenot reachable from the starting point.
However, thisis unlikely to limit performance - an oracle alignerachieves 97.6%F -measure on the Arabic-Englishtraining set.4.2 Move selectionA log linear model for the selection of the candidatealignment at t+1 from the set of alignmentsMt(At)generated by the move generator at step t takes theform:p(At+1|E,F,Mt(At)) =ePi ?i?i(At+1,E,F )Z(E,F,Mt(At))(3)where the partition function is now given byZ(E,F,M) =?A?MePi ?i?i(A,E,F ) (4)and At+1 ?
Mt(At) is required for correct normal-ization.
This equation is notationally very similarto equation (1), except that the predictions of themodel are restricted to a small set of nearby align-ments.
For the move generator considered in this pa-per, the summation in Eq.
(4) is similarly restricted,and hence training the model is tractable.
The setof candidate alignmentsMt(At) typically does notcontain the reference (gold) alignment; we modelthe best alignment among a finite set of alternatives,rather than the correct alignment from among allpossible alignments.
This is a key difference be-tween our model and (Liu et al, 2005).Note that if we extended our definition of pertur-bation to the limiting case that the alignment set in-cluded all possible alignments then we would clearlyrecover the standard log linear model of alignment.4.3 TrainingSince the model is designed to predict perturbationto an alignment, it is trained from a collection oferrorful alignments and corresponding reference se-quences of aligner moves that reach the reference(gold) alignment.
We construct a training set froma collection of sentence pairs and reference align-ments for training (A?n, En, Fn)Nn=1, as well as col-lections of corresponding ?first pass?
alignments An1produced by another aligner.
For each n, we form anumber of candidate alignment sets Mt(Ant ), onefor each source and target word.
For training pur-poses, the true alignment from the set is taken to bethe one identical withA?n in the slice targeted by themove generator at the current step.
(A small numberof move sets do not have an exact match and are dis-carded.)
Then we form an objective function fromthe log likelihood of reference alignment, smoothedwith a gaussian priorL =?nLn +?i(?i/?
)2 (5)892where the likelihood of each training sample isLn =?
?log p1(A0n|E,Fn;M(f?, A0n, E, Fn))+?
?log p1(A0n|E,Fn;M(e?, A0n, E, Fn)) (6)The likelihood has a term for each sentence pairand for each decoder step.
The model is trainedby gradient ascent using the l-BFGS method (Liuand Nocedal, 1989), which has been successfullyused for training log linear models (Blunsom andCohn, 2006) in many natural language tasks, includ-ing alignment.5 FeaturesA wide variety of features were used in the model.We group the features in three broad categories:link-based, geometrical, and parse-based.Link-based features are those which decomposeinto a (linear) sum of alignment matrix elements ?ij .An example link-based feature is one that fires if asource language noun is linked to a target languagedeterminer.
Note that this feature may fire more thanonce in a given sentence pair: as with most fea-tures in our model, it is an integer-valued featurethat counts the number of times a structure appearsin a sentence pair.
These features do not capture anycorrelation between different ?ij .
Among the link-based features are those based on Model 1 transla-tion matrix parameters ?
(ei|fj) and ?
(fj |ei).
Webin the model 1 parameters, and form integer-valuedfeatures for each bin that count the number of linkswith ?0 < ?
(ei|fj) < ?1.Geometrical features are those which capture cor-relation between different ?ij based on adjacency ornearness.
They capture the idea that nearby wordsin one language link to nearby words in the otherlanguage - the motivation of HMM-based modelsof alignment.
An example is a feature that countsthe number of times that the next word in the sourcelanguage is linked to the next word in the target lan-guage:?
(A,E, F ) =?ij?ij?i+1,j+1 (7)Parse-based features are those which capture cor-relation between different ?ij , but use parsing to de-termine links which are correlated - for example, if adeterminer links to the same word as its head noun.As an example, if ei is the headword of ei?
, and fj isthe headword of fj?
, then?
(A,E, F ) =?ij?ij?i?j?
(8)counts the number of times that a dependency rela-tion in one language is preserved by alignment in theother language.
This feature can also be decorated,either lexically, or with part-of-speech tags (as manyfeatures in all three categories are.
)5.1 Unsupervised AdaptationWe constructed a heuristic phrase dictionary for un-supervised adapatation.
After aligning a large unan-notated parallel corpus with our aligner, we enumer-ate fully lexicalized geometrical features that can beextracted from the resulting alignments - these areentries in a phrase dictionary.
These features aretied, and treated as a single real-valued feature thatfires during training and decoding phases if a set ofhypothesized links matches the geometrical featureextracted from the unannotated data.
The value ofthis real-valued feature is the log of the number ofoccurrences of the identical (lexicalized) geometri-cal feature in the aligned unannotated corpus.6 ResultsWe design our experiments to validate that a cor-rection model using simple features, mostly non-language-specific, can improve the alignment accu-racy of a variety of existing aligners for a variety oflanguage pairs; we do not attempt to exactly matchfeatures between comparison aligners - this is un-likely to lead to a robust correction model.6.1 Arabic-English alignment resultsWe trained the Arabic-English alignment systemon 5125 sentences from Arabic-English treebanks(LDC2008E61, LDC2008E22) that had been an-notated for word alignment.
Reference parseswere used during the training.
Results are mea-sured on a 500 sentence test set, sampled froma wide variety of parallel corpora, including vari-ous genres.
During alignment, only automatically-generated parses (based on the parser of (Rat-naparkhi, 1999)) were available.
Alignments on893initial align correction model R (%) P (%) F (%) ?FGIZA++ 76 76 76corr(GIZA++) 86 94 90 14?corr(ME-seq) 88 92 90 14?HMM 73 73 73corr(HMM) 87 92 89 16?corr(ME-seq) 87 93 90 17?ME-seq 82 84 83corr(HMM) 88 92 90 7?corr(GIZA++) 87 94 91 8?corr(ME-seq) 89 94 91 8?Table 1: Alignment accuracy for Arabic-English systems in percentage recall (R), precision(P), and F -measure.
?denotes statistical significance (see text.
)lang method R (%) P(%) F (%) ?FZH?EN GIZA++ 55 67 61ME-seq 66 72 69corr(ME-seq) 74 76 75 6?Table 2: Alignment accuracy for Chinese(ZH)-English(EN) systems.
?
denotes statistical significancelang aligner R(%) P(%) F (%) ?FIT?
EN ME-seq 74 87 80corr(ME-seq) 84 92 88 8?EN?IT ME-seq 75 86 80corr(ME-seq) 84 92 88 8?PT?EN ME-seq 77 83 80corr(ME-seq) 87 91 89 9?EN?PT ME-seq 79 87 83corr(ME-seq) 88 90 89 6?JA?EN ME-seq 72 78 75corr(ME-seq) 77 83 80 5?RU?EN ME-seq 81 85 83corr(ME-seq) 82 92 87 4?DE?EN ME-seq 77 82 79corr(ME-seq) 78 87 82 3?ES?EN ME-seq 93 86 90corr(ME-seq) 92 88 90 0.6FR?EN ME-seq 89 91 90corr(ME-seq) 88 92 90 0.1Table 3: Alignment accuracy for additional languages.
?
denotes statistical significance; ?
statistical significance notavailable.
IT=Italian, PT=Portuguese, JA=Japanese, RU=Russian, DE=German, ES=Spanish, FR=French894the training and test sets were decoded with threeother aligners, so that the robustness of the cor-rection model to different input alignments couldbe validated.
The three aligners were GIZA++(Och and Ney, 2003) (with the MOSES (Koehnet al, 2007) postprocessing option -alignmentgrow-diag-final-and) the posterior HMMaligner of (Ge, 2004), a maximum entropy sequen-tial model (ME-seq) (Ittycheriah and Roukos, 2005).ME-seq is our primary point of comparison: it isdiscriminatively trained (on the same training data,)uses a rich set of features, and provides the bestalignments of the three.
Three correction modelswere trained: corr(GIZA++) is trained to correctthe alignments produced by GIZA++, corr(HMM)is trained to correct the alignments produced by theHMM aligner, and corr(ME-seq) is trained to correctthe alignments produced by the ME-seq model.In Table (1) we show results for our system cor-recting each of the aligners as measured in the usualrecall, precision, and F -measure.1 The resultingimprovements in F -measure of the alignments pro-duced by our models over their corresponding base-lines is statistically significant (p < 10?4, indicatedby a ?.)
Statistical significance is tested by a MonteCarlo bootstrap (Efron and Tibshirani, 1986) - sam-pling with replacement the difference in F -measureof the two system?s alignments of the same sentencepair.
Both recall and precision are improved, but theimprovement in precision is somewhat larger.
Wealso show cross-condition results in which a correc-tion model trained to correct HMM alignments is ap-plied to correct ME-seq alignments.
These resultsshow that our correction model is robust to differentstarting aligners.6.2 Chinese-English alignment resultsTable (2) presents results for Chinese-English wordalignments.
The training set for the corr(ME-seq) model consisted of approximately 8000 hand-aligned sentences sampled from LDC2006E93 andLDC2008E57.
The model was trained to correctthe output of the ME-seq aligner, and tested onthe same condition.
For this language pair, refer-ence parses were not available in our training set, so1We do not distinguish sure and possible links in our anno-tations - under this circumstance, alignment error rate(Och andNey, 2003) is 1?
F .automatically-generated parses were used for bothtraining and test sets.
Results are measured on a 512sentence test set, sampled from a wide variety of par-allel corpora of various genres.
We compare perfor-mance with GIZA++, and with the ME-seq aligner.Again the resulting improvement over the ME-seqaligner is statistically significant.
However, here theimprovement in recall is somewhat larger than theimprovement in precision.6.3 Additional language pairsTable (3) presents alignment results for seven otherlanguage pairs.
Separate alignment corrector mod-els were trained for both directions of Italian ?English and Portuguese ?
English.
The trainingand test data vary by language, and are sampleduniformly from a diverse set of corpora of variousgenres, including newswire, and technical manuals.Manual alignments for training and test data wereannotated.
We compare performance with the ME-seq aligner trained on the same training data.
Aswith the Chinese results above, customization andfeature development for the language pairs was min-imal.
In general, machine parses were always avail-able for the English half of the pair.
Machine parseswere also available for French and Spanish.
Ma-chine part of speech tags were available for all lan-guage (although character-based heuristic was sub-stituted for Japanese.)
Large amounts (up to 10 mil-lion sentence pairs) of unaligned parallel text wasavailable for model 1 type features.
Our model ob-tained improved alignment F -measure in all lan-guage pairs, although the improvements were smallfor ES?EN and FR?EN, the language pairs forwhich the baseline accuracy was the highest.6.4 AnalysisSome of the improvement can be attributed to ?look-ahead?
during the decoding.
For example, theEnglish word ?the?, which (during Arabic-Englishalignment) should often be aligned to the same Ara-bic words to which its headword is linked.
The num-ber of errors associated with ?the?
dropped from 383(186 false alarms, 197 misses) in the ME-seq modelto 137 (60 false alarms and 77 misses) in the currentmodel.In table 5, we show contributions to performanceresulting from various classes of features.
The895Zh-En Ar-Enmethod correct miss fa correct miss fahmm 147 256 300GIZA++ 139 677 396 132 271 370ME-seq 71 745 133 127 276 191corr(ME-seq) 358 458 231 264 139 114Table 4: Analysis of 2?1 alignments errors (misses and false alarms) for Zh-En and Ar-En alignerslargest contribution is noted by removing featuresbased on the Model 1 translation matrices.
Thesefeatures contain a wealth of lexical informationlearned from approximately 7 ?
106 parallel sen-tences - information that cannot be learned froma relatively small amount of word-aligned train-ing data.
Geometrical features contribute morethan parse-based features, but the contribution fromparse-based features is important, and these aremore difficult to incorporate into sequential mod-els.
We note that all of the comparison aligners hadequivalent lexical information.We show a small improvement from the unsuper-vised adaptation - learning phrases from the parallelcorpus that are not captured by the lexical featuresbased on model 1.
The final row in the table showsthe result of running the correction model on its ownoutput.
The improvement is not statistically signif-icant, but it is important to note the performance isstable - a further indication that the model is robustto a wide variety of input alignments, and that ourdecoding scheme is a reasonable approach to find-ing the best alignment.In table 4, we characterize the errors based on thefertility of the source and target words.
We focuson the case that exactly one target word is linked toexactly two source words.
These are the links thatfeature R(%) P(%) F (%) Nexactbase 89 94 91 136base-M1 82 88 85 89base-geometric 83 90 86 92base-parse 87 93 90 116base+un.adapt 89 94 92 141+iter2 90 94 92 141Table 5: Importance of feature classes - ablation experi-mentscorpus-level p90alignment TER BLEU TER BLEUME-seq 56.06 32.65 64.20 21.31corr(Me-seq) 56.25 33.10 63.47 22.02both 56.07 33.13 63.41 22.14Table 6: Translation results, Zh to En.
BLEU=BLEUr4n4alignment TER BLEUr1n4ME-seq 35.02 69.94corr(Me-seq ) 33.10 71.20Table 7: Translation results, En to Itare poorly suited for the HMM and ME-seq mod-els used in this comparison because of the chosendirectionality: the source (Arabic, Chinese) wordsare the states and the target (English) words are theobservation.
The HMM is able to produce theselinks only by the use of posterior probabilities, ratherthan viterbi decoding.
The ME-seq model only pro-duces these links because of language-specific post-processing.
GIZA++ has an underlying sequentialmodel, but uses both directionalities.
The correc-tion model improved performance across all three ofthese links structures.
The single exception is thatthe number of 2?1 false alarms increased (Zh-Enalignments) but in this case, the first pass ME-seqalignment produced few false alarms because it sim-ply proposed few links of this form.
It is also notablethat 1?2 links are more numerous than 2?1 links,in both language pairs.
This is consequence of thechoice of directionality and tokenization style.6.5 Translation ImpactWe tested the impact of improved alignments onthe performance of a phrase-based translation sys-tem (Ittycheriah and Roukos, 2007) for three lan-896guage pairs.
Our alignment did not improve theperformance of a mature Arabic to English trans-lation system, but two notable successes were ob-tained: Chinese to English, and English to Italian.It is well known that improved alignment perfor-mance does not always improve translation perfor-mance (Fraser and Marcu, 2007).
A mature machinetranslation system may incorporate alignments ob-tained from multiple aligners, or from both direc-tions of an asymmetric aligner.
Furthermore, withlarge amounts of training data (the Gale Phase 4Arabic English corpus consisting of 8 ?
106 sen-tences,) a machine translation system is subject toa saturation effect: correcting an alignment maynot yield a significant improvement because the thephrases learned from the correct alignment have al-ready been acquired in other contexts.For the Chinese to English translation system (ta-ble 6) the training corpus consisted of 11?
106 sen-tence pairs, subsampled to 106.
The test set wasNIST MT08 Newswire, consisting of 691 sentencesand 4 reference translations.
Corpus-level perfor-mance (columns 2 and 3) improved when measuredby BLEU, but not by TER.
Performance on themost difficult sentences (near the 90th percentile,columns 4 and 5) improved on both BLEU and TER(Snover et al, 2006), and the improvement in BLEUwas larger for the more difficult sentences than itwas overall.
Translation performance further im-proved, by a smaller amount, using bothME-seq andcorr(ME-seq) alignments during the training.The improved alignments impacted the transla-tion performance of the English to Italian transla-tion system (table 7) even more strongly.
Here thetraining corpus consisted of 9.4?106 sentence pairs,subsampled to 387000 pairs.
The test set consistedof 7899 sentences.
Overall performance improvedas measured by both TER and BLEU (1.26 points.
)7 ConclusionsA log linear model for the alignment matrix is usedto guide systematic improvements to an existingaligner.
Our system models arbitrary alignment ma-trices and allows features that incorporate such in-formation as correlations based on parse trees inboth languages.
We train models to correct the er-rors of several existing aligners; we find the resultingmodels are robust to using different aligners as start-ing points.
Improvements in alignment F -measure,often significant improvements, show that our modelsuccessfully corrects input alignments from existingmodels in all nine language pairs tested.
The result-ing Chinese-English and English-Italian word align-ments also improved translation performance, espe-cially on the English-Italian test, and notably on theparticularly difficult subset of the Chinese sentences.Future work will assess its impact on translation forthe other language pairs, as well as its impact onother tasks, such as named entity projection.8 AcknowledgementsWe would like to acknowledge the support ofDARPA under Grant HR0011-08-C-0110 for fund-ing part of this work.
The views, opinions, and/orfindings contained in this article/presentation arethose of the author/presenter and should not be in-terpreted as representing the official views or poli-cies, either expressed or implied, of the Defense Ad-vanced Research Projects Agency or the Departmentof Defense.ReferencesNecip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.2005.
Alignment link projection using transformation-based learning.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages 185?192, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Yassine Benajiba and Imed Zitouni.
2010.
Enhanc-ing mention detection using projection via alignedcorpora.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,EMNLP ?10, pages 993?1001.
Association for Com-putational Linguistics.Phil Blunsom and Trevor Cohn.
2006.
Discriminativeword alignment with conditional random fields.
In InProc.
of ACL-2006, pages 65?72.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Yonggang Deng and Bowen Zhou.
2009.
Optimizingword alignment combination for phrase table training.In Proceedings of the ACL-IJCNLP 2009 Conference897Short Papers, ACLShort ?09, pages 229?232, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.B.
Efron and R. Tibshirani.
1986.
Bootstrap meth-ods for standard errors, confidence intervals, and othermeasures of statistical accuracy.
Statistical Science,1(1):pp.
54?75.Victoria Fossum, Kevin Knight, and Steven Abney.
2008.Using syntax to improve word alignment precision forsyntax-based machine translation.
In Proceedings ofthe Third Workshop on Statistical Machine Transla-tion, StatMT ?08, pages 44?52.
Association for Com-putational Linguistics.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
Comput.
Linguist., 33(3):293?303.Niyu Ge.
2004.
Improvement in word alignments.
InDARPA/TIDES MT workshop.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for arabic-english machinetranslation.
In HLT-EMNLP, pages 89?96.Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In Human Language Technolo-gies 2007: The Conference of the NA-ACL, pages 57?64, Rochester, New York, April.
Association for Com-putational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL.Young-Suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In Proceedings of HLT-NAACL 2004: Short Papers on XX, HLT-NAACL ?04,pages 57?60.
Association for Computational Linguis-tics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the main con-ference on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics, pages 104?111.
Associa-tion for Computational Linguistics.Dong C. Liu and Jorge Nocedal.
1989.
On the lim-ited memory bfgs method for large scale optimization.Mathematical Programming, 45:503?528.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linearmodels for word alignment.
In ACL ?05: Proceedingsof the 43rd Annual Meeting on Association for Com-putational Linguistics, pages 459?466.
Association forComputational Linguistics.Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006.Improved discriminative bilingual word alignment.
InACL-44: Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 513?520.
Association for Computa-tional Linguistics.Robert C. Moore.
2005.
A discriminative framework forbilingual word alignment.
In In Proceedings of HLT-EMNLP, pages 81?88.Jan Niehues and Stephan Vogel.
2008.
Discrimina-tive word alignment via alignment matrix modeling.In Proceedings of the Third Workshop on StatisticalMachine Translation, pages 18?25, Columbus, Ohio,June.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Adwait Ratnaparkhi.
1999.
Learning to parse natu-ral language with maximum entropy models.
Mach.Learn., 34:151?175, February.Hendra Setiawan, Chris Dyer, and Philip Resnik.
2010.Discriminative word alignment with a function wordreordering model.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?10, pages 534?544.
Association forComputational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, LineaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
InProceedings of Association for Machine Translation inthe Americas.Ben Taskar, Simon Lacoste-julien, and Dan Klein.
2005.A discriminative matching approach to word align-ment.
In In Proceedings of HLT-EMNLP, pages 73?80.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational linguistics, pages 836?841.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof the first international conference on Human lan-guage technology research, HLT ?01, pages 1?8.
As-sociation for Computational Linguistics.898
