NEW YORK UNIVERSITY PROTEUS SYSTEM :MUC-4 TEST RESULTS AND ANALYSI SRalph Grishman, John Sterling, and Catherine MacleodThe PROTEUS Projec tComputer Science DepartmentNew York University715 Broadway, 7th FloorNew York, NY 10003{ grishman,sterling,macleod)@cs.nyu.eduRESULTSThe "ALL TEMPLATES" results of our "official" runs were as follows :RECALL PRECISIO NTST3 41 47TST4 46 46Evaluating the degree of improvement over the MUC-3 runs is complicated by the changes between MUC-3 an dMUC-4: there were changes in the template structure, the MURDER templates were eliminated, content mappin gconstraints were incorporated into the scoring program, and the rules for manual remapping were much more con -strained.
We resumed system development specifically for MUC (with regular runs and rescorings) in mid-March ,approximately two weeks before the "Dry Run" was due, and the modifications prior to the Dry Run primaril yreflected the changes needed for the new template structure (no significant changes were made to concepts, ver bmodels, inference rules, etc .).
The changes between our final MUC-3 scores and our Dry Run scores thus roughl yreflect the changes due to the change in the task -- for both TST1 and TST2, a loss of about 7 points of recall .
Dur-ing the following 8 weeks, we made a number of system modifications which recovered much of this loss of recal land substantially improved system precision.TST1 RECALL TST1 PRECISION TST2 RECALL TST2 PRECISIONMay 91 56 41 44 36March 92 49 38 37 3 5May 92 57 54 40 45During the period from mid-March, when we adapted the system for the MUC-4 templates and began scoring runs ,until the evaluation at the end of May, approximately 5 to 6 person-months were involved in developmen tspecifically addressed to MUC-4 performance .
This does not count the time we spent since MUC-3 on researc husing the MUC-3 data, on such topics as semantic pattern acquisition, Wordnet, and grammar evaluation ; most o fthis work was not directly used in the MUC-4 system .IMPROVEMENTSWe made a number of small improvements in upgrading our MUC-3 system for the MUC-4 evaluation :(1) We integrated the BBN stochastic part-of-speech tagger into our system .
We had done this for MUC-3, butin a rather crude way, keeping only the most probable part-of-speech assigned by the tagger.
This made thesystem run faster, but with some loss of recall.
For MUC-4, we made full use of the probabilities assigne dby the tagger, combining them with the other contributions to our scoring function (e.g., semantic scores ,syntactic penalties) and selecting the highest-scoring analysis .
This yielded a small improvement in syste mrecall (1% on the TST1 corpus) .12 4(2) We incorporated a more elaborate time analysis component to handle constructs such as "Three weeks late r. .." and "Two weeks after <event I>, <event 2> .
.
.
", in addition to the absolute times (explicit dates) andtimes relative to the dateline ("two weeks ago") which were handled in our MUC-3 system .
The system nowproduces a time graph relating events, and computes absolute times as the information becomes available .This produced a small benefit in recall and precision .
(3) In our MUC-3 system, if no parse could be obtained of the entire sentence, we identified the longest stringstarting at the first word which could be analyzed as a sentence.
We now have the option of taking th eremaining words, identifying the longest clauses and noun phrases, and processing these (in addition to th elongest initial substring).
We refer to this as "syntactic debris" .
Because most sentences obtain a full-sentence parse, this option has only a small effect .
On TST3, selecting "syntactic debris" increased recall b y1% and reduced precision by 1% .
(4) We implemented a simple mechanism for dynamically shifting the parsing strategy .
For each sentence, up toa certain point, all hypotheses are followed, in a best-first order determined by our scoring function .
Once aspecified number of hypotheses have been generated (15000 in the official runs), we shift to a mode wher eonly the highest-ranking hypothesis for each non-terminal and each span of sentence words is retained .
Thismode may yield a sub-optimal analysis (because many constraints are non-local), but will converge to someanalyisis much more quickly (effectively shifting from an exponential to a polynomial-time algorithm) .
(5) We made several improvements to reference resolution .
In particular, we refined the semantic slot/fille rrepresentation we use for people in order to improve anaphor-antecedent matching .
(6) We have been steadily expanding our grammatical coverage .Except as needed for our other system changes, we made relatively few additions to the sets of concepts and lexica lmodels developed for MUC-3 .
1 We did not extend the effort at extensive corpus analysis pursued prior to MUC-3 ;rather we experimented with various strategies which would lead to greater automation of this process in the future(see the sections below on "Wordnet" and "Acquiring Selectional Constraints") .DISCOURSEAt MUC-3, discourse analysis was frequently cited as a serious shortcoming of many of the systems .
In ou rsystem, discourse analysis (beyond reference resolution) is reflected mainly in decisions about merging events t oform templates .
Roughly speaking, our MUC-3 system tried to merge events (barring conflicting time, location ,etc.)?
when they affected the same target?
when they appeared in the same sentence?
when an attack (including bombing, arson, etc.)
was followed by effect (death, damage, injury, etc .
)For MUC-4 we tried 3 variations on our discourse analysis procedure :(1) blocking attack/effect merging across paragraph boundaries(2) in addition, making use of anaphoric references to events in the merging procedure (so that "Five civilian swere killed in the attack."
would cause the templates for the attack and the killings to be merged even if th eantecedent of "attack" were in a prior paragraph) .
(3) identifying and attempting to merge general and specific descriptions of events (this happens quite often i nnewspaper-style articles, where the introductory paragraph is a summary of several distinct events which ar ereported separately later in the article).
This linking of general and specific events was then used by refer-ence resolution to order the search for antecedents .
(This can be viewed as an attempt at a Grosz/Sidne rfocus stack.
)Variation 1 did slightly better than the MUC-3 base system (on TST3, it got 1% better recall at no loss in preci-sion) .
Variations 2 and 3, although more "linguistically principled", did slightly worse (variation 2 lost 2% recall ,1% precision on TST3) .
We therefore used variation 1 for our official run .'
The set of lexico-semantic models grew by about 25% over MUC-3 ; the set of concepts (except for geographical names) by about 15% .A partial failure analysis for TST3 suggested that many of the template errors could be attributed to gaps or errors in the models or concepts, an dhence that further improvements in these two components were crucial to improved performance .125An examination of some of the errors indicated that, while variations 2 and 3 did OK in and of themselves,they were sensitive to errors in prior stages of processing (in particular, shortcomings in semantic interpretation ledto occasional incorrect anaphora resolution, which in turn led to excess event merging).
In contrast, paragraphboundaries, while not as reliable a discourse indicator, are more reliably observed .
Thus, the best component inisolation may not be the best choice for a system, because it may be too sensitive to errors made by prior com-ponents .RELATED RESEARCHMuch of our time since MUC-3 was involved in research using the MUC-3/MUC-4 corpus and task .
Wedescribe here very briefly some of our work related to semantic acquisition, evaluation, and multi-lingual systems .WORDNETOne of our central interests lies in improving the methods used for acquiring semantic knowledge for newdomains.
As we noted earlier, we did not invest much additional effort (beyond that for MUC-3) in manual dataanalysis in order to augment the conceptual hierarchy and lexico-semantic models.
We instead conducted severa lexperiments aimed at more automatic semantic acquisition .One of these experiments involved using Wordnet, a large hierarchy of word senses (produced by Georg eMiller at Princeton), as a source of information to supplement our semantic classification hierarchy .
We added toour hierarchy everything in Wordnet under the concepts person and building .We identified a number of additional events in this way .
Some were correct .
Some were incorrect, involvingunintended senses of words .
For example, the sentenc eEl Salvador broke diplomatic relations .would be interpreted as an attack because "relations" (such as "close relations", i .e ., relatives) are people in Word-net.
Even more obscure is thatHe fought his way back .becomes an attack because "back" (as in "running back", a football player) is a person.
Some of the additionalevents were correct as events, but should not have appeared in templates, either because they were military ("th eenemy") or because they were anaphoric references to prior phrases ("the perpetrator") and so should have beenreplaced by appropriate antecedents .These results suggest that Wordnet may be a good source of concepts, but that it will not be of net benefi tunless manually reviewed with respect to a particular application .ACQUIRING SELECTIONAL CONSTRAINTSAn alternative source of semantic information is the texts themselves .
NYU has conducted a number of stu-dies aimed at gleaning selectional constraints and semantic classes from the co-occurrence patterns in the sampl etexts in a domain.In the past year, we focussed on the task of acquiring the selectional constraints needed for the MUC texts .We have tried to automate this task by parsing 1000 MUC messages (without semantic constraints) and collectin gfrequency information on subject-verb-object and head-modifier patterns.
Where possible, we used theclassification hierarchy (which we had built by hand) to generalize words in these patterns to word classes .
Wethen used these patterns as selectional constraints in parsing new text ; we found that they did slightly better than th econstraints we had created by hand last year [1] .
The gain was small -- not likely to affect template score -- butshould be an advantage in moving to a new domain, particularly if even larger corpora are available .We have not yet completed the complementary task of building the word classes from this distributionalinformation .GRAMMAR EVALUATIONTo understand why some systems did better than others, we need some glass-box evaluation of individua lcomponents.
As we know, it is very hard to define any glass-box evaluation which can be applied across systems .126We have experimented with one aspect of this, grammar (parse) evaluation, which can at least be applied acros sthose systems which generate a full sentence parse.We use as our standard for comparison the Univ .
of Pennsylvania Tree Bank, which includes parse trees fora portion of the MUC terrorist corpus .
We take our parse trees, restructure them (automatically) to conform betterto the Penn parses, strip labels from brackets, and then compare the bracket structure to that of the Tree Bank .
Theresult is a recall/precision score which should be meaningful across systems .We have experimented with a number of parsing strategies, and found that parse recall is well correlate dwith template recall [2] .In principle, we would like to try to extend these comparisons to "deeper" relations, such as functiona lsubject/object relations.
These will be harder to define, but may be applicable over a broader range of systems .MULTI-LINGUAL MUCWe were fortunate to have two researchers from Spain, Antonio Moreno Sandoval and Cristina OlmedaMoreno, who over the past nine months have built a Spanish version of our MUC system (a Spanish grammar, dic-tionary, and lexico-semantic models) [3] .
As this system has developed, we have gradually revised and extende dour system so that we can have a language-independent core with language-specific modules .REFERENCES[1]Ralph Grishman and John Sterling .
Acquisition of Selectional Patterns .
To appear in Proc.
14th Intl Conf onComputational Linguistics (COLING 92), Nantes, France, July 1992.
[2] Ralph Grishman, Catherine Macleod, and John Sterling .
Evaluating Parsing Strategies Using Standardize dParse Files.
Proc .
Third Conference on Applied Natural Language Processing .
Trento, Italy, April, 1992 .
[3] Cristina Olmeda Moreno and Antonio Moreno Sandoval .
El tratamiento semantico en un sistema automatic ode extracci6n de informaci6n To appear in Proceedings of Semantica I, Zaragoza, Spain, May, 1992.127
