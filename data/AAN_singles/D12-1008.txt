Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 82?93, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsOptimising Incremental Dialogue Decisions Using Information Density forInteractive SystemsNina Dethlefs, Helen Hastie, Verena Rieser and Oliver LemonSchool of Mathematical and Computer SciencesHeriot-Watt University, Edinburgh, Scotlandn.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.ukAbstractIncremental processing allows system design-ers to address several discourse phenomenathat have previously been somewhat neglectedin interactive systems, such as backchannelsor barge-ins, but that can enhance the re-sponsiveness and naturalness of systems.
Un-fortunately, prior work has focused largelyon deterministic incremental decision mak-ing, rendering system behaviour less flexibleand adaptive than is desirable.
We present anovel approach to incremental decision mak-ing that is based on Hierarchical Reinforce-ment Learning to achieve an interactive op-timisation of Information Presentation (IP)strategies, allowing the system to generateand comprehend backchannels and barge-ins,by employing the recent psycholinguistic hy-pothesis of information density (ID) (Jaeger,2010).
Results in terms of average rewardsand a human rating study show that our learntstrategy outperforms several baselines that arenot sensitive to ID by more than 23%.1 IntroductionRecent work on incremental systems has shownthat adapting a system?s turn-taking behaviour to bemore human-like can improve the user?s experiencesignificantly, based on incremental models of auto-matic speech recognition (ASR) (Baumann et al2011), dialogue management (Buss et al2010), andspeech generation (Skantze and Hjalmarsson, 2010).All of these approaches are based on the same gen-eral abstract architecture of incremental processing(Schlangen and Skantze, 2011).
While this archi-tecture offers inherently incremental mechanisms toupdate and revise input hypotheses, it is affectedby a number of drawbacks, shared by determinis-tic models of decision making in general: they relyon hand-crafted rules which can be time-consumingand expensive to produce, they do not provide amechanism to deal with uncertainty introduced byvarying user behaviour, they are unable to gener-alise and adapt flexibly to unseen situations, andthey do not use automatic optimisation.
Statisti-cal approaches to incremental processing that ad-dress some of these problems have been suggestedby Raux and Eskenazi (2009), who use a cost matrixand decision theoretic principles to optimise turn-taking in a dialogue system under the constraint thatusers prefer no gaps and no overlap at turn bound-aries.
Also, DeVault et al2009) use maximum en-tropy classification to support responsive overlap inan incremental system by predicting the completionsof user utterances.
Selfridge et al2011) use logis-tic regression models to predict the stability and ac-curacy of incremental speech recognition results toenhance performance without causing delay.
For re-lated work on (deterministic) incremental languagegeneration, please see (Kilger and Finkler, 1995;Purver and Otsuka, 2003).Recent years have seen a number of data-drivenapproaches to interactive systems that automaticallyadapt their decisions to the dialogue context us-ing Reinforcement Learning (Levin et al2000;Walker, 2000; Young, 2000; Singh et al2002;Pietquin and Dutoit, 2006; Henderson et al2008;Cuaya?huitl et al2010; Thomson, 2009; Young etal., 2010; Lemon, 2011; Janarthanam and Lemon,2010; Rieser et al2010; Cuaya?huitl and Dethlefs,822011; Dethlefs and Cuaya?huitl, 2011).
While theseapproaches have been shown to enhance the perfor-mance and adaptivity of interactive systems, unfor-tunately none of them has yet been combined withincremental processing.In this paper, we present a novel approach to in-cremental decision making for output planning thatis based on Hierarchical Reinforcement Learning(HRL).
In particular, we address the problem of op-timising IP strategies while allowing the system togenerate and comprehend backchannels and barge-ins based on a partially data-driven reward func-tion.
Generating backchannels can be beneficial forgrounding in interaction.
Similarly, barge-ins canlead to more efficient interactions, e.g.
when a sys-tem can clarify a bad recognition result immediatelybefore acting based on a misrecognition.A central concept to our approach is InformationDensity (ID) (Jaeger, 2010), a psycholinguistic hy-pothesis that human utterance production is sensitiveto a uniform distribution of information across theutterance.
This hypothesis has also been adopted forlow level output planning recently, see e.g.
Rajku-mar and White (2011).
Our results in terms of av-erage rewards and a human rating study show that alearning agent that is sensitive to ID can learn whenit is most beneficial to generate feedback to a user,and outperforms several other agents that are notsensitive to ID.2 Incremental Information Presentation2.1 Information Presentation StrategiesOur example domain of application is the Infor-mation Presentation phase in an interactive systemfor restaurant recommendations, extending previouswork by Rieser et al2010).
This previous workincrementally constructs IP strategies according tothe predicted user reaction, whereas our approachfocuses on whether and when to generate backchan-nels and barge-ins and how to react to user barge-ins in the context of dynamically changing input hy-potheses.
We therefore implement a simplified ver-sion of Rieser et al model.
Their system distin-guished two steps: the selection of an IP strategyand the selection of attributes to present to the user.We assume here that the choice of attributes is deter-mined by matching the types specified in the user in-put, so that our system only needs to choose a strat-egy for presenting its results.
Attributes include cui-sine, food quality, location, price range and servicequality of a restaurant.
The system then performs adatabase lookup and chooses among three main IPstrategies summary, comparison, recommendationand several ordered combinations of these.
Pleasesee Rieser et al2010) for details.
Table 1 showsexamples of the main types of IP strategies that wegenerate.2.2 Backchannels and Barge-insAn important advantage of incremental processingcan be the increased reactiveness of systems.
In thispaper, we focus on the phenomena of backchannelsand barge-ins that can act as feedback in an interac-tion for both user and system.
Figure 1 shows someexamples.
Backchannels can often be interpreted assignals of grounding.
Coming from the user, the sys-tem may infer that the user is following the presenta-tion of information or is confirming a piece of infor-mation without trying to take the turn.
Similarly, wecan allow a system to generate backchannels to theuser to confirm that it understands the user?s prefer-ences, i.e.
receives high confidence scores from theASR module.
An important decision for a dialoguesystem is then when to generate a backchannel?Barge-ins typically occur in different situations.The user may barge-in on the system to correct anASR error (such as ?Italian?
instead of ?Indian?
inFigure 1) or the system may want to barge-in on theuser to confirm a low-confidence ASR hypothesis soas to be able to start an immediate database look upfor results.
In the former case, the user barging-inon the system, we assume that the system has twochoices: yielding the turn to the user, or trying tokeep the turn.
In the latter case, the system barging-in on the user, the system would have to decide if andwhen it would be beneficial to barge-in on a user ut-terance.
In the following sections, we will developa model of dialogue optimisation that can addressthese question based on Hierarchical RL that opti-mises system behaviour based on trade-offs definedin terms of ID.83Type ExampleComparison The restaurant Roma is in the medium price range, but does not serve excellent food.The restaurants Firenze and Verona both have great food but are more expensive.
Therestaurant Verona has good service, too.Recommendation Restaurant Verona has the best overall match with your query.
It is a bit more expen-sive, but has great food and service.Summary I found 24 Italian restaurants in the city centre that match your query.
11 of them arein the medium price range, 5 are cheap and 8 are expensive.Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants inthe city centre that have a good price for value.Backchannel 1 (the system backchannels)USR I want Italian food [500 ms] in the city centre.
.
.SYS uh-huhSYS OK.
I found 24 Italian restaurants in the city centre.
Therestaurant Roma is in the medium price range, but does nothave great food.
The restaurants Firenze and Verona .
.
.Backchannel 2 (the user backchannels)USR I want Italian food in the centre of town .
.
.SYS OK.
I found 35 central Italian restaurants .
.
.USR OK.SYS The restaurant Verona has great food but is also a bitexpensive.
The Roma is cheaper, but not as central as Verona .
.
.Barge-ins 1 (the user barges-in on system)USR I want Italian food in the centre of town .
.
.SYS I found 35 Indian .
.
.USR Not Indian, I want Italian.SYS OK, Italian .
.
.SYS I have 24 Italian restaurants .
.
.Barge-ins 2 (the system barges-in on user)USR I need an Italian restaurant that is located .
.
.SYS I?m sorry.
Did you sayIndian or Italian?USR I said Italian.
And in the centre of town please.SYS OK, let me see.
I have 24 Italian restaurants .
.
.Figure 1: Example phenomena generated with the learntpolicy.
The agent has learnt to produce backchannelsand barge-ins at the appropriate moment and alternativestrategies to deal with user barge-ins.3 Information TheoryInformation Theory as introduced by Shannon(1948) is based on two main concepts: a communi-cation channel through which information is trans-ferred in bits and the information gain, i.e.
the in-formation load that each bit carries.
For natural lan-guage, the assumption is that people aim to com-municate according to the channel?s capacity, whichcorresponds to the hearer?s capacity in terms of cog-nitive load.
If they go beyond that, the cognitive loadof the listener gets too high.
If they stay (far) below,too little information is transferred per bit (i.e., theutterance is inefficient or uninformative).
The in-formation gain of each word, which is indicative ofhow close we are to the channel?s capacity, can becomputed using entropy measures.3.1 Information DensityPsycholinguistic research has presented evidence forusers distributing information across utterances uni-formly, so that each word is carrying roughly thesame amount of information.
This has been ob-served for phonetic phenomena based on words(Bell et al2003) and syllables (Aylett and Turk,2004), and for syntactic phenomena (Levy andJaeger, 2007; Jaeger, 2010).
Relating ID to likeli-hood, we can say that the less frequent a word is, themore information it is likely to carry (Jaeger, 2010).For example the word ?the?
often has a high corpusfrequency but a low ID.The ID is defined as the log-probability of anevent (i.e.
a word) (Shannon, 1948; Levy and Jaeger,2007), so that for an utterance u consisting of theword sequence w1 .
.
.
wi?1, we can compute the IDat each point during the utterance as:log 1P (u) =n?i=1log 1P (wi|w1 .
.
.
wi?1)(1)While typically the context of a word is given byall preceding words of the utterance, we follow Gen-zel and Charniak (2002) in restricting our computa-tion to tri-grams for computability reasons.
Given a84language model of the domain, we can therefore op-timise ID in system-generated discourse, where wetreat ID as ?an optimal solution to the problem ofrapid yet error-free communication in a noisy envi-ronment?
(Levy and Jaeger (2007), p.2).
We willnow transfer the notion of ID to IP and investigatethe distribution of information over user restaurantqueries.3.2 Information Density in User UtterancesWe aim to use ID for incremental IP in two ways:(1) to estimate the best moment for generatingbackchannels or barge-ins to the user, and (2) to de-cide whether to yield or keep the current system turnin case of a user barge-in.
While we do not have spe-cific data on human barge-in behaviour, we knowfrom the work of (Jaeger, 2010), e.g., that ID influ-ences human language production.
We therefore hy-pothesise a relationship between ID and incremen-tal phenomena.
A human-human data collection isplanned for the near future.To compute the ID of user and system utterancesat each time step, we estimated an n?gram lan-guage model (using Kneser-Ney smoothing) basedon a transcribed corpus of human subjects interact-ing with a system for restaurant recommendations ofRieser et al2011).1 The corpus contained user ut-terances as exemplified in Figure 1 and allowed us tocompute the ID at any point during a user utterance.2In this way, we can estimate points of low densitywhich may be eligible for a barge-in or a backchan-nel.
Figure 2 shows some example utterances drawnfrom the corpus and their ID including the first sen-tence from Figure 1.
These examples were typicalfor what could generally be observed from the cor-pus.
We see that while information is transmittedwith varying amounts of density, the main bits of in-formation are transmitted at a scale between 2 and7.Due to a lack of human data for the system utter-ances, we use the same corpus data to compute theID of system utterances.3 The learning agent can use1Available at http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/login.php.2Note that our model does not currently handle out-of-domain words.
In future work, we will learn when to seek clar-ification.3We plan a data collection of such utterances for the future,1 2 3 4 5 6 7 8 9 10 11 1202468101214161820InformationDensityTimeI want Italian food in the city centre.Yes, I need a moderately priced restaurant in the New Chesterton area.I need the address of a Thai restaurant.Figure 2: Information Density for example utterances,where peaks indicate places of high density.this information to consider the trade-off of yieldinga current turn to the user or trying to keep it, e.g., incase of a user barge-in given the ID of its own turnand of the user?s incoming turn.
Such decisions willbe made incrementally in our domain given dynam-ically changing hypotheses of user input.4 Incremental Utterance OptimisationTo optimise incremental decision making for an in-teractive system given the optimisation measure ofID, we formalise the dialogue module as a Hierar-chical Reinforcement Learning agent and learn anoptimal action policy by mapping states to actionsand optimising a long-term reward signal.
The di-alogue states can be seen as representing the sys-tem?s knowledge about the task, the user and theenvironment.
The dialogue actions correspond tothe system?s capabilities, such as present the re-sults or barge-in on the user.
They also handle in-cremental updates in the system.
In addition, weneed a transition function that specifies the waythat actions change the environment (as expressedin the state representation) and a reward functionwhich specifies a numeric value for each actiontaken.
In this way, decision making can be seenas a finite sequence of states, actions and rewards{s0, a0, r1, s1, a1, ..., rt?1, st}, where the goal is toinduce an optimal strategy automatically using Rein-forcement Learning (RL) (Sutton and Barto, 1998).We used Hierarchical RL, rather than flat RL, be-cause the latter is affected by the curse of dimen-sionality, the fact that the state space grows expo-nentially according to the state variables taken intoaccount.
This affects the scalability of flat RL agentsbut for now make the assumption that using the corpus data isinformative since they are from the same domain.85and limits their application to small-scale problems.Since timing is crucial for incremental approaches,where processing needs to be fast, we choose a hi-erarchical setting for better scalability.
We denotethe hierarchy of RL agents as M ij where the in-dexes i and j only identify an agent in a uniqueway, they do not specify the execution sequence ofsubtasks, which is subject to optimisation.
Eachagent of the hierarchy is defined as a Semi-MarkovDecision Process (SMDP) consisting of a 4-tuple< Sij , Aij , T ij , Rij >.
Here, Sij denotes the set ofstates, Aij denotes the set of actions, and T ij is aprobabilistic state transition function that determinesthe next state s?
from the current state s and the per-formed action a.
Rij(s?, ?
|s, a) is a reward functionthat specifies the reward that an agent receives fortaking an action a in state s lasting ?
time steps(Dietterich, 1999).
Since actions in SMDPs maytake a variable number of time steps to complete,the variable ?
represents this number of time steps.The organisation of the learning process into dis-crete time steps allows us to define incremental hy-pothesis updates as state updates and transitions inan SMDP.
Whenever conditions in the learning en-vironment change, such as the recogniser?s best hy-pothesis of the user input, we represent them as tran-sitions from one state to another.
At each time step,the agent checks for changes in its state represen-tation and takes the currently best action accordingto the new state.
The best action in an incrementalframework can also include generating a backchan-nel to the user to indicate the status of groundingor barging-in to confirm an uncertain piece of infor-mation.
Once information has been presented to theuser, it is committed or realised.
Realised informa-tion is represented in the agent?s state, so that it canmonitor its own output.Actions in a Hierarchical Reinforcement learnercan be either primitive or composite.
The formerare single-step actions that yield single rewards, andthe latter are multi-step actions that correspond toSMDPs and yield cumulative rewards.
Decisionmaking occurs at any time step of an SMDP: aftereach single-step action, we check for any updatesof the environment that require a system reaction orchange of strategy.
If no system action is required(e.g.
because the user is speaking), the system candecide to do nothing.
The goal of each SMDP is tofind an optimal policy pi?
that maximises the rewardfor each visited state, according topi?ij(s) = argmaxa?A Q?ij(s, a), (2)where Qij(s, a) specifies the expected cumulative re-ward for executing action a in state s and then fol-lowing pi?.
We use HSMQ-Learning to induce dia-logue policies, see (Cuaya?huitl, 2009), p. 92.5 Experimental Setting5.1 Hierarchy of Learning AgentsThe HRL agent in Figure 3 shows how the tasks of(1) dealing with incrementally changing input hy-potheses, (2) choosing a suitable IP strategy and (3)presenting information, are connected.
Note thatwe focus on a detailed description of models M10...3here, which deal with barge-ins and backchannelsand are the core of this paper.
Please see Dethlefs etal.
(2012) for details of an RL model that deals withthe remaining decisions.Briefly, model M00 deals with dynamic input hy-potheses.
It chooses when to listen to an incominguser utterance (M13 ) and when and how to presentinformation (M10...2) by calling and passing controlto a child subtask.
The variable ?incrementalStatus?characterises situations in which a particular (incre-mental) action is triggered, such as a floor holder ?letme see?, a correction or self-correction.
The variable?presStrategy?
indicates whether a strategy for IP hasbeen chosen or not, and the variable ?userReaction?shows the user?s reaction to an IP episode.
The?userSilence?
variable indicates whether the user isspeaking or not.
The detailed state and action spaceof the agents is given in Figure 4.
We distinguish ac-tions for Information Presentation (IP), actions forattribute presentation and ordering (Slot-ordering),and incremental actions (Incremental).Models M10...2 correspond to different ways ofpresenting information to the user.
They performattribute selection and ordering and then call thechild agents M20...4 for attribute realisation.
When-ever a user barges in over the system, these agentswill decide to either yield the turn to the user or totry and keep the turn based on information density.The variables representing the status of the cuisine,86RootSummary Comparison RecommendationObserveUserPresentCuisinePresentFoodPresentLocationPresentPricePresentServiceFigure 3: Hierarchy of learning agent for incremental In-formation Presentation and Slot Ordering.food, location, price and service of restaurants indi-cate whether the slot is of interest to the user (we as-sume that 0 means that the user does not care aboutthis slot), and what input confidence score is cur-rently associated with the value of the slot.
For ex-ample, if our current best hypothesis is that the useris interested in Indian restaurants, the variable ?sta-tusCuisine?
will have a value between 1-3 indicatingthe strength of this hypothesis.
Once slots have beenpresented to the user, they are realised and can onlybe changed through a correction or self-correction.Model M13 is called whenever the user is speak-ing.
The system?s main choice here is to remainsilent and listen to the user or barge-in to requestthe desired cuisine, location, or price range of arestaurant.
This can be beneficial in certain situa-tions, such as when the system is able to increase itsconfidence for a slot from ?low?
to ?high?
throughbarging-in with a direct clarification request, e.g.
?Did you say Indian??
(and thereby saving sev-eral turns that may be based on a wrong hypoth-esis).
This can also be harmful in certain situa-tions, though, assuming that users have a generalpreference for not being barged-in on.
The learningagent will need to learn to distinguish these situa-tions.
This agent is also responsible for generatingbackchannels and will over time learn the best mo-ments to do this.Models M20...4 choose surface forms for presenta-tion to the user from hand-crafted templates.
Theyare not the focus of this paper, however, and there-fore not presented in detail.
The state-action spacesize of this agent is roughly 1.5 million.4 The agent4Note that a flat RL agent, in contrast, would need 8?
1025million state-actions to represent this problem.States M00incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}observeUser {0=unfilled,1=filled}presStrategy {0=unfilled,1=filled}userReaction {0=none,1=select,2=askMore,3=other}userSilence={0=false,1=true}Actions M00IP: compare M11 , recommend M12 , summarise M10 , sum-mariseCompare, summariseRecommend, summariseCompar-eRecommend,Incremental: correct, selfCorrect, holdFloor, observeUserGoal State M00 0, 1, 1, 0, ?States M10...2IDSystem={0=low,1=medium, 2=high}statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}statusService {0=unfilled,1=low,2=medium,3=high,4=realised}turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}userBargeIn {0=false, 1=true}Actions M10...2Slot-ordering: presentCuisine M20 , presentQuality M21 ,presentLocation M22 , presentPrice M23 , presentService M24 ,Incremental: yieldTurn, keepTurnGoal State M10...2 ?, ?
4, 0 ?
4, 0 ?
4, 0 ?
4, 0 ?
4, ?, ?States M13bargeInOnUser={0=undecided,1=yes, 2=no}IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}Actions M13Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,bargeInPrice, backchannelGoal State M13 >0, ?, 0 ?
4, 0 ?
4, 0 ?
4States M20...4IDSystem={0=low,1=medium, 2=high}IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}surfaceForm {0=unrealised,1=realised}Actions M20...4Surface Realisation: [alternative surface realisations]e.g.
?$number$ restaurants serve $cuisine$ food?, ?$number$places are located in $area$, etc.Goal State M20...4 ?, ?, 1Figure 4: The state and action space of the HRL agent.The goal state is reached when all items (that the userspecified in the search query) have been presented.
Ques-tion marks mean that a variable does not affect the goalstate, which can be reached regardless of the variable?svalue.reaches its goal state (defined w.r.t.
the state vari-87ables in Fig.
4) when an IP strategy has been chosenand all information has been presented.5.2 The Simulated EnvironmentFor a policy to converge, a learning agent typicallyneeds several thousand interactions in which it is ex-posed to a multitude of different circumstances.
Forour domain, we designed a simulated environmentwith three main components addressing IP, incre-mental input hypotheses and ID.
Using this simula-tion, we trained the agent for 10 thousand episodes,where one episode corresponds to one recommenda-tion dialogue.5.2.1 Information PresentationTo learn a good IP strategy, we use a user simula-tion5 by Rieser et al2010) which was estimatedfrom human data and uses bi-grams of the formP (au,t|IPs,t), where au,t is the predicted user reac-tion at time t to the system?s IP strategy IPs,t in states at time t. We distinguish the user reactions of se-lect a restaurant, addMoreInfo to the current query toconstrain the search, and other.
The last category isusually considered an undesirable user reaction thatthe system should learn to avoid.
The simulationuses linear smoothing to account for unseen situa-tions.
In this way, we can predict the most likelyuser reaction to each system action.
Even thoughprevious work has shown that n-gram-based simu-lations can lead to dialogue inconsistencies, we as-sume that for the present study this does not presenta problem, since we focus on generating single utter-ances and on obtaining user judgements for single,independent utterances.5.2.2 Input Hypothesis UpdatesWhile the IP strategies can be used for incremen-tal and non-incremental dialogue, the second part ofthe simulation deals explicitly with the dynamic en-vironment updates that the system will need to besensitive to in an incremental setting.
We assumethat for each restaurant recommendation, the userhas the option of filling any or all of the attributescuisine, food quality, location, price range and ser-vice quality.
The possible values of each attributeand possible confidence scores for each value are5The simulation data are available from www.classic-project.org.shown in Table 2.
A score of 0 means that the userdoes not care about the attribute, 1 means that thesystem?s confidence in the attribute?s value is low, 2that the confidence is medium, and 3 means that theconfidence is high.
A value of 4 means that the at-tribute has already been realised, i.e.
communicatedto the user.
At the beginning of a learning episode,we assign each attribute a possible value and con-fidence score with equal probability.
For food andservice quality, we assume that the user is never in-terested in bad food or service.
Subsequently, con-fidence scores can change at each time step.
In fu-ture work these transition probabilities will be esti-mated from a data collection, though the followingassumptions are realistic based on our experience.We assume that a confidence score of 0 changes toany other value with a likelihood of 0.05.
A confi-dence score of 1 changes with a probability of 0.3,a confidence score of 2 with a probability of 0.1and a confidence score of 3 with a probability of0.03.
Once slots have been realised, their value isset to 4.
They cannot be changed then without an ex-plicit correction.
We also assume that realised slotschange with a probability of 0.1.
If they change,we assume that half of the time, the user is the ori-gin of the change (because they changed their mind)and half of the time the system is the origin of thechange (because of an ASR or interpretation error).Each time a confidence score is changed, it has aprobability of 0.5 for also changing its value.
Theresulting input to the system are data structures ofthe form present(cuisine=Indian), confidence=low.The probability of observing this data structure inour simulation is 0.1 (for Indian) ?
0.2 (for lowconfidence) = 0.02.
Its probability of changingto present(cuisine=italian), confidence=high is 0.1(for changing from low to medium) ?
0.05 (forchanging from Indian to Italian) = 0.005.5.2.3 Information Density UpdatesWe simulate ID of user utterances based on proba-bilistic context-free grammars (PCFG) that were au-tomatically induced from the corpus data in Section3.2 using the ABL algorithm (van Zaanen, 2000).This algorithm takes a set of strings as input andcomputes a context-free grammar as output by align-ing strings based on Minimum Edit Distance.
Weuse the n?gram language models trained earlier to88Attribute Values ConfidenceCuisine Chinese, French, German, In-, 0, 1, 2, 3, 4dian, Italian, Japanese, Mexi-can, Scottish, Spanish, ThaiQuality bad, adequate, good, very good 0, 1, 2, 3, 4Location 7 distinct areas of the city 0, 1, 2, 3, 4Price cheap, good-price-for-value,expensive, very expensive 0, 1, 2, 3, 4Service bad, adequate, good, very good 0, 1, 2, 3, 4Table 2: User goal slots for restaurant queries with possi-ble values and confidence scores.add probabilities to grammar rules.
We use thesePCFGs to simulate user utterances to which the sys-tem has to react.
They can be meaningful utter-ances such as ?Show me restaurants nearby?
or lessmeaningful fragments such as ?um let me see, doyou.
.
.
hm?.
The former type is more frequent inthe data, but both types can be simulated along withtheir ID (clearly, the first type is more dense than thesecond).In addition to simulating user utterances, wehand-crafted context-free grammars of system ut-terances and augmented them with probabilities es-timated using the same user corpus data as above(where again, we make the assumption that this isto some extent feasible given the shared domain).We use the simulated system utterances to computevarying degrees of ID for the system.Both measures, the ID of user and system utter-ances, can inform the system during learning to bal-ance the trade-off between them for generating andreceiving backchannels and barge-ins.5.3 A Reward Function for IncrementalDialogue Based on Information DensityTo train the HRL agent, we use a partially data-driven reward function.
For incremental IP, we userewards that are based on human intuition.
Theagent receivesR =??????????????????
?+100 if the user selects an item,0 if the user adds further con-straints to the search,-100 if the user does something elseor a self-correction,-0.5 for the system holding a turn,-1 otherwise.The agent is encouraged to choose those sequencesof actions that lead to the user selecting a restaurantas quickly as possible.
If the agent is not sure what tosay (because planning has not finished), it can gen-erate a floor holding marker, but should in any caseavoid a self-correction due to having started speak-ing too early.The remaining rewards are based on ID scorescomputed incrementally during an interaction.
Theagent receives the following rewards, where info-Density(Usr) and infoDensity(Sys) refer to the ID ofthe current user and system utterance, respectively,as defined in Equation 1.R =??????
?-infoDensity(Usr) for keeping a turn,barging-in ora backchannel,-infoDensity(Sys) for yielding a turn.These two measures encourage the agent to considerthe trade-offs between its own ID and the one trans-mitted by an incoming user utterance.
Barging-inon a user utterance at a low ID point then yields asmall negative reward, whereas barging-in on a userutterance at a high ID point yields a high negativereward.
Both rewards are negative because barging-in on the user always contains some risk.
Similarly,keeping a turn over a non-dense user utterance re-ceives a smaller negative reward than keeping it overa dense user utterance.
A reward of ?2 is assignedfor barging-in over a user utterance fragment with afalling ID to reflect results from a qualitative studyof our corpus data: humans tend to barge-in betweeninformation peaks, so that a barge-in to clarify a low-confidence slot appears immediately before the ID isrising again for a new slot.
The exact best momentfor barge-ins and backchannels to occur will be sub-ject to optimisation.896 Experimental ResultsThe agent learns to barge-in or generate backchan-nels to users at points where the ID is low but rising.In particular, the agent learns to barge-in right beforeinformation density peaks in an incoming user utter-ance to clarify or request slots that are still open fromthe previous information density peak.
If a user hasspecified their desired cuisine type but the systemhas received a low ASR confidence score for it, itmay barge-in to clarify the slot.
This case was illus-trated in the last example in Figure 1, where the sys-tem clarified the previous (cuisine) slot (which is as-sociated with a high ID) just before the user specifiesthe location slot (which again would have a high ID).The main benefit the system can gain through clar-ification barge-ins is to avoid self-corrections whenhaving acted based on a low ASR confidence, lead-ing to more efficient interactions.The system learns to generate backchannels afterinformation peaks to confirm newly acquired slotsthat have a high confidence.
An example is shownin the first dialogue fragment in Figure 1.In addition, the system learns to yield its currentturn to a user that is barging-in if its own ID is low,falling or rising, or if the ID of the incoming userutterance is high.
If the system?s own ID is high, butthe user?s is not, it will try to keep the turn.6 This isexemplified in the third dialogue fragment in Figure1.We compare our learnt policy against two base-lines.
Baseline 1 was designed to always generatebarge-ins after an information peak in a user utter-ance, i.e.
when ID has just switched from high tofalling.
We chose this baseline to confirm that usersindeed prefer barge-ins before information peaksrather than at any point of low ID.
Baseline 1 yieldsa turn to a user barge-in if its own ID is low and triesto keep it otherwise.
Baseline 2 generates barge-insand backchannels randomly and at any point duringa user utterance.
The decision of yielding or keepinga turn in case of a user barge-in is also random.
Bothbaselines also use HRL to optimise their IP strategy.We do not compare different IP strategies, which hasbeen done in detail by Rieser et al2010).
All re-6Incidentally, this also helps to prevent the system yieldingits turn to a user backchannel; cf.
Example 2 in Fig.
1.101 102 103 104?120?100?80?60?40?200204060AverageRewardEpisodesLearntBaseline1Baseline2Figure 5: Performance in terms of rewards (averaged over10 runs) for the HRL agent and its baselines.sults are summarised in Table 3.6.1 Average Rewards over TimeFigure 5 shows the performance of all systems interms of average rewards in simulation.
The learntpolicy outperforms both baselines.
While the learntpolicy and Baseline 1 appear to achieve similar per-formance, an absolute comparison of the last 1000episodes of each behaviour shows that the improve-ment of the HRL agent over Baseline 1 correspondsto 23.42%.
The difference between the learnt policyand its baselines is significant at p < 0.0001 accord-ing to a paired t-test and has a high effect size ofr = 0.85.The main reason for these different performancesis the moment each system will barge-in.
SinceBaseline 1 barges-in on users after an informationpeak, when ID may still be high, it continuously re-ceives a negative reward reflecting the user prefer-ence for late barge-ins.
As a result of this contin-uous negative reward, the agent will then learn toavoid barge-ins altogether, which may in turn leadto less efficient interactions because low confidenceASR scores are clarified only late in the interaction.The main problem of the random barge-ins ofBaseline 2 is that users may often have to restarta turn because the system barged-in too early orin the middle of an information peak.
In addition,Baseline 2 needs to occasionally self-correct its ownutterances because it started to present informationtoo early, when input hypotheses were not yet stableenough to act upon them.90Policy Average Reward User Rating (%)Learnt 55.54??,?
43%?
?Baseline 1 45.0??
26%Baseline 2 1.47 31%Table 3: Comparison of policies in terms of average re-wards and user ratings.
?
indicates a significant improve-ment over Baseline 1 and ??
over Baseline 2.6.2 Human Rating StudyTo confirm our simulation-based results, we con-ducted a user rating study on the CrowdFlowercrowd sourcing platform.7 Participants wereshown user utterances along with three options ofbarging-in over them.
For example: | I want[OPTION 1] Italian food [OPTION 2] in thecity [OPTION 3] centre|, where OPTION 1 cor-responds to the learnt policy, OPTION 2 to Baseline2 and OPTION 3 to Baseline 1.Users were asked to choose one option which theyconsidered the best moment for a barge-in.
Partici-pants in the study rated altogether 144 utterances.They preferred the learnt system 63 times (43%),Baseline 1 37 times (26%) and Baseline 2 44 times(31%).
This is statistically significant at p < 0.02according to a Chi-Square test (?2 = 7.542, df =2).
In a separate test, directly comparing the learntpolicy and Baseline 1, learnt was chosen signifi-cantly more often than Baseline 1; i.e.
79% of thetime (for 127 utterances, using a 1-tailed Sign test,p < 0.0001).
Finally, learnt was directly comparedto Baseline 2 and shown to be significantly more of-ten chosen; i.e.
59% of the time (138 utterances, 1-tailed Sign test, p < 0.025).
These results provideevidence that an optimisation of the timing of gener-ating barge-ins and backchannels in incremental di-alogue can be sensitive to fine-grained cues in evolv-ing ID and therefore achieve a high level of adaptiv-ity.
Such sensitivity is difficult to hand-craft as canbe concluded w.r.t.
the performance of Baseline 1,which received similar rewards to learnt in simula-tion, but is surprisingly beaten by the random Base-line 2 here.
This indicates a strong human dislikefor late barge-ins.
The bad performance of Base-line 2 in terms of average rewards was due to therandom barge-ins leading to less efficient dialogues.7www.crowdflower.comRegarding user ratings however, Baseline 2 was pre-ferred over Baseline 1.
This is most likely due to thetiming of barge-ins: since Baseline 2 has a chanceof barging-in at earlier occasions than Baseline 1,it may have received better ratings.
The evaluationshows that humans care about timing of a barge-inregarding the density of information that is currentlyconveyed and dislike late barge-ins.
ID is then usefulin determining when to barge-in.
We can thereforefurther conclude that ID can be a feasible optimisa-tion criterion for incremental decision making.7 Conclusion and Future WorkWe have presented a novel approach to incremen-tal dialogue decision making based on HierarchicalRL combined with the notion of information den-sity.
We presented a learning agent in the domain ofIP for restaurant recommendations that was able togenerate backchannels and barge-ins for higher re-sponsiveness in interaction.
Results in terms of av-erage rewards and a human rating study have shownthat a learning agent that is optimised based on apartially data-driven reward function that addressesinformation density can learn to decide when and ifit is beneficial to barge-in or backchannel on userutterances and to deal with backchannels and barge-ins from the user.
Future work can take several di-rections.
Given that ID is a measure influencinghuman language production, we could replace ourtemplate-based surface realiser by an agent that op-timises the information density of its output.
Cur-rently we learn the agent?s behaviour offline, be-fore the interaction, and then execute it statistically.More adaptivity towards individual users and situa-tions could be achieved if the agent was able to learnfrom ongoing interactions.
Finally, we can confirmthe human results obtained from an overhearer-styleevaluation in a real interactive setting and explicitlyextend our language model to discourse phenomenasuch as pauses or hesitations to take them into ac-count in measuring ID.AcknowledgementsThe research leading to this work has received fundingfrom EC?s FP7 programmes: (FP7/2011-14) under grantagreement no.
287615 (PARLANCE); (FP7/2007-13) un-der grant agreement no.
216594 (CLASSiC); (FP7/2011-14) under grant agreement no.
270019 (SPACEBOOK);91and (FP7/2011-16) under grant agreement no.
269427(STAC).
Many thanks to Michael White for discussionof the original idea of using information density as an op-timisation metric.ReferencesMatthew Aylett and Alice Turk.
2004.
The smooth signalredundancy hypothesis: A functional explanation forthe relationships between redundancy, prosodic promi-nence, and duration in spontaneous speech.
Languageand Speech, 47(1):31?56.Timo Baumann, Okko Buss, and David Schlangen.
2011.Evaluation and Optimisation of Incremental Proces-sors.
Dialogue and Discourse, 2(1).Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, CynthiaGirand, Michelle Gregory, and Daniel Gildea.
2003.Effects of disfluencies, predictability, and utteranceposition on word form variation in english conver-sation.
Journal of the Acoustic Society of America,113(2):1001?1024.Okko Buss, Timo Baumann, and David Schlangen.
2010.Collaborating on Utterances with a Spoken DialogueSysten Using an ISU-based Approach to IncrementalDialogue Management.
In Proceedings of 11th An-nual SIGdial Meeting on Discourse and Dialogue.Heriberto Cuaya?huitl and Nina Dethlefs.
2011.Spatially-aware Dialogue Control Using Hierarchi-cal Reinforcement Learning.
ACM Transactions onSpeech and Language Processing (Special Issue onMachine Learning for Robust and Adaptive SpokenDialogue System), 7(3).Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, andHiroshi Shimodaira.
2010.
Evaluation of a hierar-chical reinforcement learning spoken dialogue system.Computer Speech and Language, 24(2):395?429.Heriberto Cuaya?huitl.
2009.
Hierarchical ReinforcementLearning for Spoken Dialogue Systems.
PhD Thesis,University of Edinburgh, School of Informatics.Nina Dethlefs and Heriberto Cuaya?huitl.
2011.Combining Hierarchical Reinforcement Learning andBayesian Networks for Natural Language Generationin Situated Dialogue.
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation(ENLG), Nancy, France.Nina Dethlefs, Helen Hastie, Verena Rieser, and OliverLemon.
2012.
Optimising Incremental Genera-tion for Spoken Dialogue Systems: Reducing theNeed for Fillers.
In Proceedings of the InternationalConference on Natural Language Generation (INLG),Chicago, Illinois, USA.David DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
Learning when to respond to incrementalinterpretation result in interactive dialogue.
In Pro-ceedings of the 10th Annual SigDial Meeting on Dis-course and Dialogue, Queen Mary University, UK.Thomas G. Dietterich.
1999.
Hierarchical Reinforce-ment Learning with the MAXQ Value Function De-composition.
Journal of Artificial Intelligence Re-search, 13:227?303.Dmitriy Genzel and Eugene Charniak.
2002.
EntropyRate Constancy in Text.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, pages 199?206.James Henderson, Oliver Lemon, and Kallirroi Georgila.2008.
Hybrid Reinforcement/Supervised Learning ofDialogue Policies from Fixed Data Sets.
Computa-tional Linguistics, 34(4):487?511.T.
Florian Jaeger.
2010.
Redundancy and reduction:Speakers manage syntactic information density.
Cog-nitive Psychology, 61:23?62.Srini Janarthanam and Oliver Lemon.
2010.
Learningto Adapt to Unknown Users: Referring ExpressionGeneration in Spoken Dialogue Systems.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 69?78, July.Anne Kilger and Wolfgang Finkler.
1995.
Incremen-tal generation for real-time applications.
Technical re-port, DFKI Saarbruecken, Germany.Oliver Lemon.
2011.
Learning What to Say and How toSay It: Joint Optimization of Spoken Dialogue Man-agement and Natural Language Generation.Esther Levin, Roberto Pieraccini, and Wieland Eckert.2000.
A Stochastic Model of Computer-Human Inter-action for Learning Dialogue Strategies.
IEEE Trans-actions on Speech and Audio Processing, 8:11?23.Roger Levy and T. Florian Jaeger.
2007.
Speakers opti-mize information density through syntactic reduction.Advances in Neural Information Processing Systems,19.Olivier Pietquin and Dutoit.
2006.
A Probabilis-tic Framework for Dialogue Simulation and OptimalStrategy Learning.
IEEE Transactions on Speech andAudio Processing, 14(2):589?599.Matthew Purver and Masayuki Otsuka.
2003.
Incremen-tal Generation by Incremental Parsing.
In Proceedingsof the 6th UK Special-Interesting Group for Computa-tional Linguistics (CLUK) Colloquium.Rajakrishnan Rajkumar and Michael White.
2011.
Lin-guistically Motivated Complementizer Choice in Sur-face Realization.
In Proceedings of the EMNLP-11Workshop on Using Corpora in NLG, Edinburgh, Scot-land.Antoine Raux and Maxine Eskenazi.
2009.
A Finite-State Turn-Taking Model for Spoken Dialog Sys-tems.
In Proceedings of the 10th Conference of the92North American Chapter of the Association for Com-putational Linguistics?Human Language Technolo-gies (NAACL-HLT), Boulder, Colorado.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising Information Presentation for Spoken Di-alogue Systems.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics (ACL), Uppsala, Sweden.Verena Rieser, Simon Keizer, Xingkun Liu, and OliverLemon.
2011.
Adaptive Information Presentationfor Spoken Dialogue Systems: Evaluation with Hu-man Subjects.
In Proceedings of the 13th EuropeanWorkshop on Natural Language Generation (ENLG),Nancy, France.David Schlangen and Gabriel Skantze.
2011.
A General,Abstract Model of Incremental Dialogue Processing.Dialogue and Discourse, 2(1).Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-son Williams.
2011.
Stability and Accuracy in In-cremental Speech Recognition.
In Proceedings of the12th Annual SigDial Meeting on Discourse and Dia-logue, Portland, Oregon.Claude Shannon.
1948.
A Mathematical Theory ofCommunications.
Bell Systems Technical Journal,27(4):623?656.Satinder Singh, Diane Litman, Michael Kearns, and Mar-ilyn Walker.
2002.
Optimizing Dialogue Managementwith Reinforcement Learning: Experiments with theNJFun System.
Journal of Artificial Intelligence Re-search, 16:105?133.Gabriel Skantze and Anna Hjalmarsson.
2010.
TowardsIncremental Speech Generation in Dialogue Systems.In Proceedings of the 11th Annual SigDial Meeting onDiscourse and Dialogue, Tokyo, Japan.Richard Sutton and Andrew Barto.
1998.
ReinforcementLearning: An Introduction.
MIT Press, Cambridge,MA.Blaise Thomson.
2009.
Statistical Methods for Spo-ken Dialogue Management.
Ph.D. thesis, Universityof Cambridge.Menno van Zaanen.
2000.
Bootstrapping Syntax andRecursion using Alignment-Based Learning.
In Pro-ceedings of the Seventeenth International Conferenceon Machine Learning, ICML ?00, pages 1063?1070.Marilyn Walker.
2000.
An Application of ReinforcementLearning to Dialogue Strategy Selection in a SpokenDialogue System for Email.
Journal of Artificial In-telligence Research (JAIR), 12:387?416.Steve Young, Milica Gasic, Simon Keizer, FrancoisMairesse, Jost Schatzmann, Blaise Thomson, and KaiYu.
2010.
The Hidden Information State Model: APractical Framework for POMDP-based Spoken Dia-logue Management.
Computer Speech and Language,24(2):150?174.Steve Young.
2000.
Probabilistic Methods in SpokenDialogue Systems.
Philosophical Transactions of theRoyal Society (Series A), 358(1769):1389?1402.93
