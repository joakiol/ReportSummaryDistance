Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 295?303,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsCombining Incremental Language Generation andIncremental Speech Synthesis for Adaptive Information PresentationHendrik Buschmeier1, Timo Baumann3, Benjamin Dosch, Stefan Kopp1, David Schlangen21Sociable Agents Group, CITEC and Faculty of Technology, Bielefeld University2Dialogue Systems Group, Faculty of Linguistics and Literary Studies, Bielefeld University{hbuschme,bdosch,skopp,david.schlangen}@uni-bielefeld.de3Natural Language Systems Division, Department of Informatics, University of Hamburgbaumann@informatik.uni-hamburg.deAbstractParticipants in a conversation are normally re-ceptive to their surroundings and their inter-locutors, even while they are speaking and can,if necessary, adapt their ongoing utterance.
Typ-ical dialogue systems are not receptive and can-not adapt while uttering.
We present combin-able components for incremental natural lan-guage generation and incremental speech syn-thesis and demonstrate the flexibility they canachieve with an example system that adapts toa listener?s acoustic understanding problemsby pausing, repeating and possibly rephrasingproblematic parts of an utterance.
In an eval-uation, this system was rated as significantlymore natural than two systems representing thecurrent state of the art that either ignore theinterrupting event or just pause; it also has alower response time.1 IntroductionCurrent spoken dialogue systems often produce pre-scripted system utterances or use templates with vari-able substitution during language generation.
If adialogue system uses grammar-based generation atall, it produces complete utterances that are then syn-thesised and realised in one big chunk.
As systemsbecome increasingly more conversational, however,the need arises to make output generation1 more flex-ible.
In particular, capabilities for incrementally gen-erating output become desirable, for two kinds ofreasons.
(a) In situations where fast system responses areimportant, production of output can begin before the1We will use the term ?output generation?
here to cover bothnatural language generation and speech synthesis.content that is to be presented is fully specified ?
evenif what is being produced is just a turn-taking signal(Skantze and Hjalmarsson, 2010).
(b) A system that produces its output incrementallycan react to events happening while it is realising anutterance.
This can be beneficial in domains wherethe state of the world that the system relays informa-tion about can change mid-utterance, so that a needmay arise to adapt while speaking.
It should alsoimprove naturalness by allowing the system to reactto dialogue phenomena such as concurrent feedbacksignals from the user (Buschmeier and Kopp, 2011).We present work towards enabling such capabil-ities.
We have implemented and connected a com-ponent for incremental natural language genera-tion (iNLG) that works with specifications of sub-utterance-sized communicative intentions and a com-ponent for incremental speech synthesis (iSS) that canhandle sub-utterance-sized input and modificationsto not-yet-spoken parts of the utterance with very lowlatencies.
To explore whether such an output genera-tion capability can indeed be advantageous, we havecreated a test system that can react to random noiseevents that occur during a system utterance by repeat-ing and modifying the last sub-utterance chunk.
Inan evaluation, we found that this system is in generalmore reactive than a non-incremental variant and thathumans rate its behaviour to be more natural thantwo non-incremental and non-responsive systems.2 Related WorkPsycholinguistic research has identified incremen-tality as an important property of human languageproduction early on and it has been incorporated intoseveral models (e. g., Kempen and Hoenkamp, 1987;295Levelt, 1989).
Guhe (2007) presents a computationalmodel of incremental conceptualisation.
However,work on iNLG itself is rare, partly because NLG re-search focusses on text (instead of spoken language).Notable exceptions are the in-depth analysis ofrequirements for and properties of incremental gen-eration by Kilger and Finkler (1995), who describethe LTAG-based incremental syntactic generator VM-GEN.
It takes incremental input, processes it and pro-duces output as soon as at least a prefix of the finalsentence is syntactically complete.
If VM-GEN no-tices that it committed itself to a prefix too early, itcan initiate an overt repair.
More recently, Skantzeand Hjalmarsson (2010) presented a system that per-forms incremental generation in the context of a spo-ken dialogue system.
It can already start to produceoutput when the user has not yet finished speakingand only a preliminary interpretation exists.
By flexi-bly changing what to say and by being able to makeself-repairs the system can recover from situationswhere it selected and committed on an inadequatespeech plan.
Both systems, however, are not ableto flexibly adapt the language that they generate tochanging requirements due to changes in the situationor changing needs on the side of the user.Real-time on-the-fly control of speech synthesisis rare, especially the full integration into a dialoguesystem.
Matsuyama et al (2010) describe a systemthat feeds back to the dialogue system the word atwhich it has been interrupted by a barge-in.
Edlund(2008) additionally enables a system to continue atthe point where it was interrupted.
He also outlinessome requirements for incremental speech synthe-sis: to give constant feedback about what has beendelivered, to be interruptible (and possibly continuefrom that position), and to run in real time.
Edlund?ssystem, which uses diphone synthesis, performednon-incrementally before delivery starts.
We go be-yond this in also enabling changes during deliveryand conducting synthesis steps just-in-time.Dutoit et al (2011) present an incremental HMMoptimiser which allows to change pitch and tempoof upcoming phonemes.
However, as that system isfed from a (non-incrementally produced) label file, itcannot easily be used in an incremental system.A predecessor of our iSS component (which wasnot yet fully incremental on the HMM level) is de-scribed in detail in (Baumann and Schlangen, 2012a).3 Incremental and Adaptive NLG3.1 The SPUD microplanning frameworkThe NLG component presented here is based onthe SPUD microplanning framework (Stone et al,2003) and realised in DeVault?s (2008) implemen-tation ?Java SPUD?.
SPUD frames microplannig asa constraint satisfaction problem, solving the tasksthat are involved in generating a sentence (lexicaland syntactic choice, referring expression generationand aggregation) in an integrated manner.
Genera-tion starts from a communicative goal that specifiesconstraints for the final utterance.
The generation pro-cess is further shaped by (a) general constraints thatmodel pragmatic properties of language use such asthe Gricean maxims (a principle called ?textual econ-omy?
); (b) specific constraints imposed through thecommunicative status of the propositions to be com-municated (i. e., what knowledge can be presupposedand what needs to be communicated explicitly); and(c) linguistic resources (a context-free tree rewritingformalism based on LTAG; Stone, 2002).To deal efficiently with the infinite search spacespanned by the linguistic resources, SPUD uses aheuristic search algorithm to find an utterance thatsatisfies the imposed constraints (Stone et al, [2003]describe the heuristic function).
In each search step,the algorithm expands the ?provisional?
utterance byadding the linguistic resource that maximally reducesthe estimated distance to the final utterance.If the generation process runs into a dead-end state,it could in principle deal with the situation by track-ing back and expanding a different branch.
This,however, is impractical, as it becomes impossibleto project when ?
if at all ?
generation will finish.Hence, in that case, SPUD stops without providing aresult, delegating the problem back to the precedingcomponent in the generation pipeline.3.2 Partially incremental generationSPUD generates utterances incrementally in the sensethat the completeness of the provisional utteranceincreases monotonically with every step.
This, how-ever, does not mean that the surface structure of pro-visional utterances is constructed incrementally (i. e.,from left to right) as well, which would only be pos-sible, if (a) the search strategy would always expandthe leftmost non-lexicalised node in the provisional296Utterance IC1IC2ICn?UtteranceoutlineIMPT1IMPT2IMPTn?MCP?
{U1, ?}?
KB1?
{Ui, ?}?
KB2?
{Uk, ?}?
KBnMPP?statetFigure 1: Incremental microplanning consists of two pro-cesses, micro content planning (MCP) and microplanning-proper (MPP).
The former provides incremental microplan-ning tasks from an utterance outline to the latter, whichincrementally transforms them into communicative intentand intonation unit-sized chunks of natural language.utterance first and if (b) the linguistic resources arespecified (and ordered) in a way that allows left-to-right expansion of the trees in all possible situations.In practice, both requirements are difficult to meetand full word-by-word incrementality in natural lan-guage microplanning is not within reach in the SPUDframework.
Because of this, we take a slightly morecoarse grained approach to incremental microplan-ning and choose chunks of the size of intonationphrases instead of words as our incremental units.We say that our microplanner does ?partially incre-mental generation?.Our incremental microplanner comprises two inter-acting processes, micro content planning and micro-planning-proper (MCP and MPP; schematised in Fig-ure 1), each of which fulfils a distinct task and oper-ates on different structures.MCP takes as input utterance outlines that describethe communicative goal (a set of desired updates Ux)intended to be communicated in an utterance and thepresuppositions and private knowledge needed to doso.
Importantly, utterance outlines specify how thecommunicative goal can be decomposed into an or-dered list of incremental microplanning-tasks IMPTx.Each such task comprises (a) a subset of the commu-nicative goal?s desired updates that belong togetherand fit into one intonation unit sized chunk of speechand (b) knowledge KBx used in generation.MPP takes one incremental microplanning-task ata time and uses SPUD to generate the IMPT?s commu-nicative intent as well as its linguistic surface formICx.
The communiciative intent is added to a repre-sentation (?state?
in Figure 1) that is shared betweenthe two processes.
While processing the IMPTs ofan utterance outline, MCP can access this representa-tion, which holds information about all the desiredupdates that were achieved before, and thus knowsthat a desired update that is shared between subse-quent IMPTs has already been communicated.
MPPcan also take this information into account duringgeneration.
This makes it possible that an utteranceis coherent and adheres to pragmatic principles eventhough generation can only take local decisions.3.3 Adaptive generationBeing able to generate language in sub-utterancechunks makes it possible to dynamically adapt laterincrements of an utterance to changes in the situa-tion that occur while the utterance is being realised.Decisions about these adaptations need not be takenalmost until the preceding increment finishes, mak-ing the generation process very responsive.
This isimportant to be able to deal with interactive dialoguephenomena, such as communicative feedback of theinterlocutor (Allwood et al, 1992) or compound con-tributions (Howes et al, 2011), in a timely manner.Adaptation may happen in both parts of incremen-tal microplanning.
In MCP, adaptation takes the formof dynamically changing the choice of which IMPT togenerate next or changing the internal structure of anIMPT; adaptation in MPP changes the choices the gen-eration process makes while transforming IMPTs intocommunicative intent and surface form.
Adaptationin MCP is triggered top-down, by higher-level pro-cesses such as dialogue management.
Adaptation inMPP on the other hand depends on the task given andon the status of the knowledge used during generation.The details are then governed by global parametersettings MPP uses during generation.If there is, for example, reason for the system tobelieve that the current increment was not commu-nicated clearly because of noise in the transmissionchannel, the MCP process might delay future IMPTsand initiate a repair of the current one by re-insertingit at the beginning of the list of upcoming IMPTs ofthis utterance outline.
The MPP process?
next taskis then to re-generate the same IMPT again.
Due to297Table 1: Surface forms generated from the same IMPT (de-sired updates = {hasSubject(event6, ?VorlesungLinguistik?
)}; KB = {event6}) but with differentlevels of verbosity.Verbosity Generated sub-utterance chunk0 ?Vorlesung Linguistik?
(lecture Linguistcs)1 ?Betreff: Vorlesung Linguistik?
(subject: lecture Linguistics)2 ?mit dem Betreff Vorlesung Linguistik?
(with the subject: lecture Linguistics)changes in the state information and situation thatinfluence microplanning, the resulting communica-tive intent and surface form might then differ fromits previous result.3.4 Adaptation mechanismsAs a proof of concept, we integrated several adapta-tion mechanism into our NLG-microplanning system.The goal of these mechanisms is to respond to a dia-logue partner?s changing abilities to perceive and/orunderstand the information the system wants to con-vey.
Some of the mechanisms operate on the level ofMCP, others on the level of MPP.
The mechanisms areimplemented either with the knowledge and its con-versational status used in generation or by alteringthe decision structure of SPUD?s search algorithm?sheuristic function.
Similar to the approach of flexi-ble NLG described by Walker et al (2007), most ofthe mechanism are conditioned upon individual flags,that in our case depend on a numeric value that repre-sents the level of understanding the system attributesto the user.
Here we describe the two most relevantmechanisms used to adapt verbosity and redundancy.Verbosity The first mechanism aims at influenc-ing the length of a sub-utterance chunk by makingit either more or less verbose.
The idea is that actuallanguage use of human speakers seldom adheres tothe idealised principle of textual economy.
This isnot only the case for reasons of cognitive constraintsduring speech production, but also because wordsand phrases that do not contribute much to an utter-ance?s semantics can serve a function, for example bydrawing attention to specific aspects of an utteranceor by giving the listener time to process.To be able to vary utterance verbosity, we anno-tated the linguistic resources in our system with val-ues of their verbosity (these are hand-crafted similarto the rule?s annotation with production costs).
Dur-ing generation in MPP the values of all linguistic re-sources used in a (provisional) utterance are added upand used as one factor in SPUD?s heuristic function.When comparing two provisional utterances that onlydeviate in their verbosity value, the one that is nearerto a requested verbosity level is chosen.
Depend-ing on this level, more or less verbose constructionsare chosen and it is decided whether sub-utterancechunks are enriched with additional words.
Table 1shows the sub-utterance chunk ?Betreff: VorlesungLinguistik?
(subject: lecture Linguistics) generatedwith different levels of verbosity.Redundancy The second adaptation mechanism isredundancy.
Again, redundancy is something that anideal utterance does not contain and by design SPUDpenalises the use of redundancy in its heuristic func-tion.
Two provisional utterances being equal, the oneexhibiting less redundancy is normally preferred.
Butsimilar to verbosity, redundancy serves communica-tive functions in actual language use.
It can highlightimportant information, it can increase the probabilityof the message being understood (Reiter and Sripada,2002) and it is often used to repair misunderstanding(Baker et al, 2008).In incremental microplanning, redundant informa-tion can be present both within one sub-utterancechunk (e. g., ?tomorrow, March 26, .
.
.
?
vs. ?tomorrow.
.
.
?)
or across IMPTs.
For the former case, we modi-fied SPUD?s search heuristic in order to conditionallyeither prefer an utterance that contains redundant in-formation or an utterance that only contains what isabsolutely necessary.
In the latter case, redundancyonly becomes an option when later IMPTs enable thechoice of repeating information previously conveyedand therefore already established as shared knowl-edge.
This is controlled via the internal structure ofan IMPT and thus decided on the level of MCP.4 Incremental Speech SynthesisIn this section we describe our component for incre-mental speech synthesis.
We extend Edlund?s (2008)requirements specification cited in Section 2, requir-ing additionally that an iSS supports changes to as-yet298unspoken parts of an ongoing utterance.We believe that the iSS?s requirements of inter-ruptability, changeability, responsiveness, and feed-back are best resolved by a processing paradigm inwhich processing takes place just-in-time, i. e., tak-ing processing steps as late as possible such as toavoid re-processing if assumptions change.
Beforewe describe these ideas in detail, we give a shortbackground on speech synthesis in general.4.1 Background on speech synthesisText-to-speech (TTS) synthesis functions in a top-down processing approach, starting on the utterancelevel and descending onto words and phonemes, inorder to make good decisions (Taylor, 2009).
Forexample, top-down modelling is necessary to assignstress patterns and sentence-level intonation whichultimately lead to pitch and duration contours, and tomodel co-articulation effects.TTS systems start out assigning intonation patternsto the utterance?s words and then generate a targetphoneme sequence which is annotated with the tar-gets?
durations and pitch contour; all of this is calledthe linguistic pre-processing step.
The synthesis stepproper can be executed in one of several ways withHMM-based and unit-selection synthesis currentlyproducing the perceptually best results.In HMM-based synthesis, the target sequence isfirst turned into a sequence of HMM states.
A globaloptimisation then determines a stream of vocodingfeatures that optimise both HMM emission probabili-ties and continuity constraints (Tokuda et al, 2000).The stream may also be enhanced to consider globalvariance of features (Toda and Tokuda, 2007).
Theparameter frames are then fed to a vocoder whichgenerates the final speech audio signal.Unit-selection, in contrast, searches for the bestsequence of (variably sized) units of speech in alarge, annotated corpus, aiming to find a sequencethat closely matches the target sequence while havingfew and if possible smooth joints between units.We follow the HMM-based approach for our com-ponent for the following reasons: (a) even thoughonly global optimisation is optimal for both tech-niques, the influence of look-ahead on the continuityconstraints of HMM-based synthesis is linear leadingto a linear loss in optimality with smaller look-aheads(whereas unit-selection with limited look-ahead mayFigure 2: Hierarchical structure of incremental units de-scribing an example utterance as it is being producedduring delivery.jump erratically between completely different unit se-quences).
(b) HMM-based synthesis nicely separatesthe production of vocoding parameter frames fromthe production of the speech audio signal which al-lows for fine-grained concurrent processing (see nextsubsection).
(c) Parameters in the vocoding framesare partially independent.
This allows us to indepen-dently manipulate, e. g., pitch without altering otherparameters or deteriorating speech quality (in unit-selection, a completely different unit sequence mightbecome optimal even for slight changes of pitch).4.2 Incrementalising speech synthesisAs explained in the previous subsection, speech syn-thesis is performed top-down, starting at the utteranceand progressing down to the word, target and finally,in the HMM approach, vocoding parameter and signalprocessing levels.
It is, however, not necessary thatall details at one level of processing are worked outbefore starting to process at the next lower level.
Tobe precise, some syntactic structure is sufficient toproduce sentence-level intonation, but all words neednot be known.
Likewise, post-lexical phonologicalprocesses can be computed as long as a local contextof one word is available and vocoding parameter com-putation (which must model co-articulation effects)should in turn be satisfied with about one phoneme ofcontext.
Vocoding itself does not need any lookaheadat all (aside from audio buffering considerations).Thus, we generate our data structures incremen-tally in a top-down and left-to-right fashion with dif-ferent amounts of pre-planning and we do this usingseveral processing modules that work concurrently.This results in a ?triangular?
structure as shown in299Figure 2.
At the top stands a pragmatic plan for thefull utterance from which a syntactic plan can be de-vised.
This plan is filled with words, as they becomeavailable.
On the vocoding parameter level, only afew frames into the future have been computed sofar ?
even though much more context is already avail-able.
That is, we generate structure just-in-time, onlyshortly before it is needed by the next processor.
Thisholds very similarly for the vocoding step that pro-duces the speech signal just-in-time.The just-in-time processing approach, combinedwith the increasing temporal granularity of units to-wards the lower levels has several advantages: (a) lit-tle utterance-initial processing (only what is neces-sary to produce the beginning of the signal) allows forvery responsive systems; and (b) changes to the ini-tial plan result only in a modest processing overheadbecause little structure has to be re-computed.4.3 Technical overviewAs a basis, we use MaryTTS (Schr?der and Trouvain,2003), but replace Mary?s internal data structuresand processing strategies with structures from ourincremental SDS architecture, the INPROTK toolkit(Schlangen et al, 2010; Baumann and Schlangen,2012b), which implements the IU model for incre-mental dialogue processing (Schlangen and Skantze,2009).
The model conceptualises ?
and the toolkitimplements ?
incremental processing as the process-ing of incremental units (IUs), which are the smallest?chunks?
of information at a specific level (the boxesin Figure 2).
IUs are interconnected to form a network(e. g., words keep links to their associated phonemesand neighbouring words and vice-versa) which repre-sents the system?s information state.The component is fed with chunk IUs which con-tain some words to be synthesised (on their own orappended to an ongoing utterance).
For simplicity,all units below the chunk level are currently gener-ated using Mary?s (non-incremental) linguistic pre-processing capabilities to obtain the target phonemesequence.
For continuations, the preceding parts ofthe utterance are taken into account when generatingprosodic characteristics for the new chunk.
Also, ourcomponent is able to revoke and exchange chunks(or unspoken parts thereof) to change what is to bespoken; this capability however is not used in theexample system presented in Section 5.The lowest level module of our component is whatmay be called a crawling vocoder, which activelymoves along the phoneme IU layer and executes twoprocessing steps: (a) for each phoneme it generatesthe sequence of HMM parameter frames using a localoptimisation technique (using up to four neighbour-ing phonemes as context) similar to the one describedby Dutoit et al (2011); and (b) vocoding the HMMparameters into an audio stream which contains theactual speech signal.IUs have a ?progress?
field which is set by thecrawling vocoder to one of ?upcoming?, ?ongoing?,or ?completed?, as applicable.
IUs provide a genericupdate mechanism to support notification aboutprogress changes in delivery.
The next section de-scribes how this is used to drive the system.5 Integrating iNLG and iSS for AdaptiveInformation PresentationIntegrating incremental microplanning with incre-mental speech synthesis in one incremental outputgeneration architecture allows us to test and explorehow their capabilities act in a coordinated way.
As afirst example, we implemented a system that presentsinformation about events in an appointment database(e. g., new, conflicting or rescheduled appointments)and is able to cope with external noise burst events,as they might for example occur on a bad telephoneline or when using a dialogue system next to a busystreet.
The focus is on the incremental capabilities ofthe system which enable its adaptive behaviour.5.1 Component interplayiNLG and iSS are implemented as IU modules in theINPROTK architecture.
The control flow of the sys-tem (Figure 3) is managed without special couplingbetween the modules, relying only on the left-to-rightprocessing capabilities of INPROTK combined withits generic IU update mechanism for transportingfeedback from iSS to iNLG.
Both modules can be(and have been) combined with other IU modules.To communicate an appointment event, the iNLGmodule starts by generating two initial chunk IUs,the first to be expressed immediately, the second asadditional prosodic context (chunk lengths differ withan average of about 4 words).
The iNLG registers as a?progress listener?
on each chunkIU, which registers300Figure 3: Information flow (dashed lines) between iNLGand iSS components (rounded boxes) and incrementalunits (rectangular boxes).
The vocoder crawls along withtime and triggers the updates.as a progress listener on a phonemeIUnear its end.Shortly before iSS finishes speaking the chunk, iNLGis thus informed and can generate and send the nextchunk to iSS just-in-time.If adaptation to noise is needed, iNLG re-generatesand re-sends the previous chunk, taking altered pa-rameters into account.
Again, a subsequent chunkis immediately pre-generated for additional prosodiccontext.
This way of generating sub-utterance chunksensures that there is always one chunk lookahead toallow the iSS module to compute an adequate in-tonation for the current chunk, while maintainingthe single chunk as increment size for the systemand minimising redundant work on the side of iNLG(this lookahead is not required for iSS; but if it is un-available, sub-utterance chunks may be inadequatelyconnected prosodically).5.2 Responding to a noise eventA third module, the noise detector connects to bothiSS and iNLG.
On noise onset, it informs iSS to inter-rupt the ongoing utterance after the current word (thisworks by breaking the links between words so thatthe crawling vocoder finishes after the currently ongo-ing word).
Once a noise burst ends, iNLG is informed,re-generates the interrupted sub-utterance chunk withthe verbosity level decreased by one and the assumedunderstanding value increased by one (this degreeof adaptation results in a noticeable difference, it is,however, not based on empirical study).
The valuesare then reset, the following chunk is generated andboth chunks are sent to iSS.It should be noted, that we have not implementeda real noise source and noise detector.
Instead, ourrandom noise simulator generates bursts of noise of1000 ms after a random time interval (between 2 andTable 2: Processing time per processing step before deliv-ery can begin (in ms; averaged over nine stimuli taking themedian of three runs for each stimulus; calculated fromlog messages; code paths preheated for optimisation).non-incr.
incr.NLG-microplanning 361 52Synthesis (ling.
pre-processing) 217 4472Synthesis (HMM and vocoding) 1004 21total response time 1582 5195 seconds) and directly informs the system 300 msafter noise starts and ends.
We think it is reasonableto assume that a real noise detector should be able togive accurate information with a similar delay.6 Evaluation6.1 Quantitative evaluationOne important argument in favour of incrementalprocessing is the possibility of speeding up systemresponse time, which for non-incremental systemsis the sum of the times taken by all processors todo their work.
An incremental system, in contrast,can fold large amounts of its processing time into theongoing speech output; what matters is the sum ofthe onset times of each processor, i. e., the time untila first output becomes available to the next processor.Table 2 summarises the runtime for the three majorsteps in output production of our system using nineutterances from our domain.
Both NLG and speechsynthesis?
onset times are greatly reduced in the in-cremental system.2 Combined, they reduce systemresponse time by more than a second.
This is mostlydue to the almost complete folding of HMM opti-misation and vocoding times into the spoken utter-ance.
NLG profits from the fact that at the beginningof an utterance only two chunks have to be gener-ated (instead of an average of 6.5 chunks in the non-incremental system) and that the first chunk is oftenvery simple.6.2 Subjective evaluationTo further test whether the system?s behaviour innoisy situations resembles that of a human speaker2The iSS component by mistake takes the symbolic pre-processing step twice.
Unfortunately, we found this bug onlyafter creating the stimuli used in the subjective evaluation.301in a similar situation, we let humans rate utterancesproduced by the fully incremental, adaptive systemand utterances produced by two non-incrementaland less responsive variants (we have not used non-incremental TTS in combination with iNLG as anotherpossible base-line as pretests showed this to soundvery unnatural due to the missing prosodic linkage be-tween phrases).
The participants were to rate whetherthey agree to the statement ?I found the behaviour ofthe system in this situation as I would expect it froma human speaker?
on a 7-point Likert-scale.In condition A, full utterances were generated non-incrementally, synthesised non-incrementally andplayed without responding to noise-interruptions inthe channel (as if the system did not notice them).Utterances in condition B were generated and synthe-sised as in condition A, but playback responded to thenoisy channel, stopping when the noise was noticedand continuing when noise ended.
For condition C,utterances were generated with the fully incrementaland adaptive system described in Section 5.
Uponnoise detection, speech synthesis is interrupted and,when the noise ends, iNLG will re-generate the in-terrupted sub-utterance chunk ?
using the adaptationstrategy outlined in Section 5.2.
This then triggersiSS into action and shortly after, the system contin-ues speaking.
Nine system runs, each producing adifferent utterance from the calendar domain, wererecorded in each of the three conditions, resulting ina total of 27 stimuli.Before the actual stimuli were presented, partici-pants listened to two example stimuli without noiseinterruptions to get an impression of how an aver-age utterance produced by the system sounds.
Afterthe presentation of these two examples, the 27 stim-uli were presented in the same random order.
Par-ticipants listened once to each stimulus and rated itimmediately after every presentation.Twelve PhD-students (3 female, 9 male; mean age30.5 years; 11 with German as one of their first lan-guages; none with uncorrected hearing impairment)from Bielefeld University participated in our studyand listened to and rated the 27 stimuli.A Friedman rank sum test revealed a highly sig-nificant difference between the perceived human-likeness of the three systems (?2 = 151, p < .0001).Median values of stimulus ratings in the conditionsA, B and C were 2, 2 and 6 respectively, indicat-ing that the fully incremental system was rated con-siderably more human-like.
This was also shownthrough a post-hoc analysis with Wilcoxon signedrank tests which found no significant difference be-tween condition A and B (V = 1191.5, p = .91)3.Conditions A and C, however, differed highly signifi-cantly (V = 82, p < .0001), as did conditions B andC (V = 22.5, p < .0001) ?
even after applying a Bon-ferroni correction to correct for a possible cumulationof ?-errors.7 ConclusionWe have presented what is ?
to the best of our knowl-edge ?
the first integrated component for incrementalNLG and speech synthesis and demonstrated the flex-ibility that an incremental approach to output gener-ation for speech systems offers by implementing asystem that can repair understanding problems.From the evaluation we can conclude that incre-mental output generation (both iNLG and iSS in iso-lation or combined) is able to greatly speed up sys-tem response time and can be used as a means tospeed up system response even in an otherwise non-incremental system.
Furthermore, we showed that thebehaviour of our fully incremental and adaptive sys-tem was perceived as significantly more human-likethan the non-incremental and the non-incrementalbut responsive baseline systems.The understanding problem that our demonstra-tor system tackled was of the simplest kind, namelyacoustic non-understanding, objectively detectableas the presence of noise.
In principle, however, thesame mechanisms of stopping and rephrasing can beused to tackle more subjective understanding prob-lems as can be signalled by linguistic feedback.
Ourincremental output generation component gives us anideal basis to explore such problems in future work.Acknowledgements This research is partially sup-ported by the Deutsche Forschungsgemeinschaft(DFG) in the Center of Excellence in ?Cognitive Inter-action Technology?
(CITEC) and through an EmmyNoether Fellowship to the last author.3This suggests that it does not matter whether a system re-sponds to problems in the communication channel by waiting ortotally ignores these problems.
Notice, however, that we did nottest recall of the calendar events.
In that case, condition B shouldoutperform A, as some information was clearly inaudible in A.302ReferencesJens Allwood, Joakim Nivre, and Elisabeth Ahls?n.
1992.On the semantics and pragmatics of linguistic feedback.Journal of Semantics, 9:1?26.Rachel Baker, Alastair Gill, and Justine Cassell.
2008.Reactive redundancy and listener comprehension indirection-giving.
In Proceedings of the 9th SIGdialWorkshop on Discourse and Dialogue, pages 37?45,Columbus, OH.Timo Baumann and David Schlangen.
2012a.
INPRO_iSS:A component for just-in-time incremental speech syn-thesis.
In Proceedings of ACL System Demonstrations,Jeju, South Korea.Timo Baumann and David Schlangen.
2012b.
TheINPROTK 2012 release.
In Proceedings of the NAACL-HLT Workshop on Future directions and needs in theSpoken Dialog Community: Tools and Data, pages 29?32, Montr?al, Canada.Hendrik Buschmeier and Stefan Kopp.
2011.
Towardsconversational agents that attend to and adapt to com-municative user feedback.
In Proceedings of the 11thInternational Conference on Intelligent Virtual Agents,pages 169?182, Reykjavik, Iceland.David DeVault.
2008.
Contribution Tracking: Partici-pating in Task-oriented Dialogue Under Uncertainty.Ph.D.
thesis, Rutgers, The State University of New Jer-sey, New Brunswick, NJ.Thierry Dutoit, Maria Astrinaki, Onur Babacan, Nicolasd?Alessandro, and Benjamin Picart.
2011. pHTS forMax/MSP: A streaming architecture for statistical para-metric speech synthesis.
Technical Report 1, numediartResearch Program on Digital Art Technologies, Mons,Belgium.Jens Edlund.
2008.
Incremental speech synthesis.
InSecond Swedish Language Technology Conference,pages 53?54, Stockholm, Sweden, November.
SystemDemonstration.Markus Guhe.
2007.
Incremental Conceptualization forLanguage Production.
Lawrence Erlbaum, Mahwah,NJ.Christine Howes, Matthew Purver, Patrick G. T. Healey,Gregory Mills, and Eleni Gregoromichelaki.
2011.
Onincrementality in dialogue: Evidence from compoundcontributions.
Discourse & Dialogue, 2:279?311.Gerard Kempen and Edward Hoenkamp.
1987.
An incre-mental procedural grammar for sentence formulation.Cognitive Science, 11:201?258.Anne Kilger and Wolfgang Finkler.
1995.
Incremen-tal generation for real-time applications.
TechnicalReport RR-95-11, Deutsches Forschungszentrum f?rK?nstliche Intelligenz, Saarbr?cken, Germany.Willem J. M. Levelt.
1989.
Speaking: From Intention toArticulation.
The MIT Press, Cambridge, UK.Kyoko Matsuyama, Kazunori Komatani, Ryu Takeda,Toru Takahashi, Tetsuya Ogata, and Hiroshi G. Okuno.2010.
Analyzing user utterances in barge-in-able spo-ken dialogue system for improving identification accu-racy.
In Proceedings of INTERSPEECH 2010, pages3050?3053, Makuhari, Japan.Ehud Reiter and Somayajulu Sripada.
2002.
Human vari-ation and lexical choice.
Computational Linguistics,28:545?553.David Schlangen and Gabriel Skantze.
2009.
A general,abstract model of incremental dialogue processing.
InProceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 710?718, Athens, Greece.David Schlangen, Timo Baumann, Hendrik Buschmeier,Okko Bu?, Stefan Kopp, Gabriel Skantze, and RaminYaghoubzadeh.
2010.
Middleware for incrementalprocessing in conversational agents.
In Proceedings ofSIGdial 2010: the 11th Annual Meeting of the SpecialInterest Group in Discourse and Dialogue, pages 51?54, Tokyo, Japan.Marc Schr?der and J?rgen Trouvain.
2003.
The Ger-man text-to-speech synthesis system MARY: A toolfor research, development and teaching.
InternationalJournal of Speech Technology, 6:365?377.Gabriel Skantze and Anna Hjalmarsson.
2010.
Towardsincremental speech generation in dialogue systems.
InProceedings of SIGDIAL 2010: the 11th Annual Meet-ing of the Special Interest Group on Discourse andDialogue, pages 1?8, Tokyo, Japan.Matthew Stone, Christine Doran, Bonnie Webber, ToniaBleam, and Martha Palmer.
2003.
Microplanning withcommunicative intentions: The SPUD system.
Compu-tational Intelligence, 19:311?381.Matthew Stone.
2002.
Lexicalized grammar 101.
InProceedings of the ACL-02 Workshop on Effective Toolsand Methodologies for Teaching Natural LanguageProcessing and Computational Linguistics, pages 77?84, Philadelphia, PA.Paul Taylor.
2009.
Text-to-Speech Synthesis.
CambridgeUniv Press, Cambridge, UK.Tomoki Toda and Keiichi Tokuda.
2007.
A speech param-eter generation algorithm considering global variancefor HMM-based speech synthesis.
IEICE TRANSAC-TIONS on Information and Systems, 90:816?824.Keiichi Tokuda, Takayoshi Yoshimura, Takashi Masuko,Takao Kobayashi, and Tadashi Kitamura.
2000.Speech parameter generation algorithms for HMM-based speech synthesis.
In Proceedings of ICASSP2000, pages 1315?1318, Istanbul, Turkey.Marylin Walker, Amanda Stent, Fran?ois Mairesse, andRashmi Prasad.
2007.
Individual and domain adap-tation in sentence planning for dialogue.
Journal ofArtificial Intelligence Research, 30:413?456.303
