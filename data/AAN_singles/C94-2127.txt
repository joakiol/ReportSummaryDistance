Bottom-Up Earley DeductionGregor Erbach*University of the SaarlandComputational LinguisticsD-66041 Saarbrficken, Germanyerbach@coli.uni-sb.deAbst rac tWe propose a bottom-up variant of Earley de-duction.
Bottom-up deduction is preferable totop-down deduction because it allows incremen-tai processing (even for head-driven grammars),it is data-driven, o subsumption check is needed,and preference values attached to lexical items canbe used to guide best-first search.
We discuss thescanning step for bottom-up Earley deduction andindexing schemes that help avoid useless deduc-tion steps.1 IntroductionRecently, there has been a lot of interest in Earleydeduction \[10\] with applications to parsing andgeneration \[13, 6, 7, 3\].Earley deduction is a very attractive framworkfor natural anguage processing because it has thefollowing properties and applications.?
Memoization and reuse of partial results?
Incremental processing by addition of newitems?
Hypothetical reasoning by keeping track ofdependencies between items?
Best-first search by means of an agenda*This work was supported by the Deutsche Forschuugs-gemeinschaft through the project N3 "Bidirektionale Lin-guistische Deduktion (BiLD)" in the Sonderforschungsbe-reich 314 Ki lnst l iche Intel l igenz - -  Wissensbasierte Sy-sterne and by the Commission ofthe European Communi-ties through the project LRE-61-061 "Reusable Gramma-tical Resources."
I would like to thank Gfinter Neumann,Christer Samuelsson a d Mats Wirdn for comments onthispaper.Like Earley's algorithm, all of these approa-ches operate top-down (backward chaining).
Theinterest has naturally focussed on top-down me-thods because they are at least to a certain degreegoal-directed.In this paper, we present a bottom-up variantof Earley deduction, which we find advantageousfor the following reasons:Ineremental i ty :  Portions of an input string canbe analysed as soon as they are produced (orgenerated as soon as the what-to-say com-ponent has decided to verbalize them), evenfor grammars where one cannot assume thatthe left-corner has been predicted before itis scanned.Data-Dr lven Processing: Top-down al-gorithms are not well suited for processinggrammatical theories like Categoriai Gram-mar or nesG that would only allow verygeneral predictions because they make useof general schemata instead of construction-specific rules.
For these grammars data-driven bottom-up rocessing is more appro-priate.
The same is true for large-coveragerule-based grammars which lead to the crea-tion of very many predictions.Subsumpt ion Checking: Since the bottom-upalgorithm does not have a prediction step,there is no need for the costly operation ofsubsumption checking)Search Strategy:  In the case where lexical ent-ries \]lave been associated with preference in-1Subsumption checking may still be needed to filter outspurious ambiguities.796formation, this information can be exploitedto guide the heuristic search.2 Bottom-up Earley DeductionEarley deduction \[10\] is based on grammars en-coded as definite clauses.
The instantiation (pre-diction) rule of top-down Earley deduction is notneeded in bottom-up Earley deduction, becausethere is no prediction.
There is only one inferencerule, namely the reduction rule (1)3 In (1), X ,G and G t are literals, ~ is a (possibly empty) se-quence of literals, and a is the most general unifierof G and G'.
The leftmost literal in the.
lmdy of anon:unit clause is Mways the selected literal.X ~ (;' A ~(.~l (:__~(.x +- ~) (~)In 1)riuciple, this rule can be applied to anypair of unit clanses and non:unit clauses of theprogram to derive any consequences of the pro:gram.
In order to reduce this search space andachieve a more goal-directed behaviour, the ruleis not applied to any pair of clauses, but clausesare on\]y selected if they can contribute to a proofof the goal.
The set of selected clauses is (;ailedthe chart .
3 The selection of clauses is guided by ascanning step (section 2.1) an(l indexing of clauses(section 2.2).2.1 ScanningThe purpose of the scanning step, whic:h corre-sponds to lexical lookup in chart parsers, is tolook up base cases of recursive definitions to serveas a starting point for bottom-up rocessing.
Thescanning step selects clauses that can appear asleaves in the proof tree lbr a given goal C.Consider the following simple definition of anHPSG, with the recursive definition of the predi-cate sign/I.
42This rule is called combine by Earley, and is also re-ferred to as the flmdamental rule in the literature onchart parsing.aThc chart differs from the state of \[10\] in that clausesin the chart arc indexed (cf.
section 2.2).4 We use feature terms in dcfinitc clauses in addition toProlog terms, f:X denotes a feature structure where X isthe value of h:ature f, and X ~ Y denotes the conjunctionsign(X) <- phrasal_sign(X).sign(X) <- lex ica l  sign(X).phrasal sign(X ~ dtrs:(head dtr:RD &comp_dtr:CD)sign(RD),sign(CD),principles(X,HD,CD).)
<-principles(X,HD,CD) <-constituent_order_principle(X,HD,CD),head_featureprinciple(X,RD),constituent order principle(phon:X_Ph,phon:HD_Ph,phon:CD_Ph) <-sequence_union(CD_Ph,HD_Ph,X_Ph).The predicate sign/1 is defined recursi-vely, and the base case is the predicatelex ica l _s ign /1 .
But, clearly it is not restric-tive enough to find only the predicate name ofthe base case for a given goal.
The base casesmust also be instantiated in order to find thosethat are useful for proving a given goal.
In thecase of parsing, the lookup of base cases (lexi-cal items) will depend on the words that arepresent in the input string.
This is implied bythe first goal of the predicate pr inc ip les /3 ,  theconst i tuent  o rder  pr inc ip le ,  which determi-nes ihow the l'nON value of a constituent is con-strutted from the eflON values of its daughters.in general, we assume that the constituent orderprinciple makes use of a linear and non-erasingoI)eratkm tor combining str ingsJ  If this is thecase, then M1 the words contained in the PnON va-lue of the goal can have their lexical items selectedas unit clauses to start bottom-up processing.l%r generation, an analogous condition on logi-cal forms has been proposed by Shieber \[13\] as the"semantic monotonicity condition," which requi-res that the :logical form of every base case mustsubsume some portion of the goal's logical form.Base case lookup must be defined specificallytbr different grammatical  theories and directionsof processing by the predicate lookup/2 ,  whosefirst argument is the goal and whose second ar-gument is the selected base case.
The followingof the feature terms X and Y.r'There is an obvions connection to the Linear Context-Free Rewriting Systems (LCFRS) \[15, 16\].797clause defines the lookup relation for parsing withHPSG.7.
lookup(+Goal,-BaseCase)lookup (phon : PhonList,lexical_sign(phon: \[Word\] ~ synsem:X)) <-member (Word, PhonList ),lexicon(Word,X).Note that the base case clauses can becomefurther instantiated in this step.
If concatena-tion (of difference lists) is used as the operationon strings, then each base case clause can be in-stantiated with the string that follows it.
Thisavoids combination of items that are not adjacentin the input string.lookup (phon : PhonLis t,lexical_sign(phon: \[Word\[ Suf\] -Sufsynsem : Synsem)) <-append(_, \[Word I Suf\] , PhonList),lexicon (Word, Synsem).In bottom-up Earley deduction, the first steptowards proving a goal is perform lookup for thegoal, and to add all the resulting (unit) clauses tothe chart.
Also, all non-unit clauses of the pro-gram, which can appear as internal nodes in theproof tree of the goal, are added to the chart.The scanning step achieves a certain degree ofgoal-directedness for bottom-up algorithms bec-ause only those clauses which can appear as lea-ves in the proof tree of the goal are added to thechart.2.2 IndexingAn item in normal context-free chart parsing canbe regarded as a pair (R,S) consisting of a dottedrule R and the substring S that the item covers(a pair of starting and ending position).
The fun-damental rule of chart parsing makes use of thesestring positions to ensure that only adjacent sub-strings are combined and that the result is theconcatenation f the substrings.In grammar formalisms like DCG or IIPSG, thecomplex nonterminals have an argument or a fea-ture (PtION) that represents he covered substringexplicitly.
The combination of the substrings isexplicit in the rules of the grammar.
As a conse-quence, Earley deduction does not need to makeuse of string positions for its clauses, as Pereiraand Warren \[10\] point out.Moreover, the use of string positions knownfrom chart parsing is too inflexible because it el=lows only concatenation of adjacent contiguoussubstrings.
In linguistic theory, the interest hasshifted from phrase structure rules that combineadjacent and contiguous constituents o?
principle-based approaches to grammar thatstate general well-formedness conditions in-stead of describing particular constructions(e.g.
IIPSG)?
operations on strings that go beyond conca-tenation (head wrapping \[11\], tree adjoining\[15\], sequence uuion \[12\]).The string positions known from chart parsingare also inadequate for generation, as pointed outby Shieber \[13\] in whose generator all items gofrom position 0 to 0 so that any item can be com-bined with any item.Itowever, the string positions are useful as anindexing of the items so that it can be easily detec-ted whether their combination can contribute to aproof of the goal.
This is especially important fora bottom-up algorithm which is not goal-directedlike top-down processing.
Without indexing, thereare too many combinations ofitems which are use-less for a proof of the goal, in fact there may be in-finitely many items so that termination problemscan arise.For example, in an order-monotonic grammarformalism that uses sequence union as the opera-tion for combining strings, a combination of itemswould be useless which results in a sign in whichthe words are not in the same order as in the inputstring \[14\].We generalize the indexing scheme from chartparsing in order to allow different operations forthe combination of strings.
Indexing improves ef-ficiency by detecting combinations that would fallanyway and by avoiding combinations of itemsthat are useless for a proof of the goal.We define an item as a pair of a clause Cl andan index Idx, written as (Cl~ Idx}.798Below, we give some examples of possible in-dexing schemes.
Other indexing schemes can beused if they are needed.1.
Non- reuse  of  I tems:  This is useful forLCFRS, where no word of the input stringcan be used twice in a proof, or for genera-tion where no part of the goal logical formshould be verbalized twice in a derivation.2.
Non-ad jacent  combinat ion :  This indexingscheme is useful for order-monotonic gram-mars.3.
Non-d i rec t iona l  ad jacent  combinat ion :This indexing is used if only adjacent con-stituents can be combined, but the orderof comhination is not prescrihcd (e.g.
non-directional basic categorial grammars).4.
Direct iona l  ad jacent  combinat ion :This is used tbr grammars with a "context-flee backbone."5.
Free combinat ion :  Allows an item to beused several times in a proof, tor example forthe non-unit clauses of the program, whichwould be represented as items of the form<X ~-- (11 A .
.
.
h Gn, fret;).The following table summarizes the propertiesof these live coml)ination schemes.
Index 1 (11)is the index associated with the non-unit clause,Index 2 (12) is associated with the unit clause, andI1 * 12 is tit(; result of coml)ining tile indices.I Index 1 Index 2 Result \[ Nolc12 11,12 \[1.
X Y XUY I X rqY- -q )2.
X Y X( .
)Y  I3.
X+Y Y+Z X+Z IY+Z X+Y X+Z I. .
.
.
.
.
.
+4.
X -Y  Y -Z  X -Z  I5.
l--~-- 'free ~ .
.
.
.
~- -+In case 2 ("non-adjacent combinatiou"), theindices X and Y consist of a set of string positions,and tile operation (:) is the union of these stringpositions, provided that no two string positionsfi'om X and Y do overlap.In (2), the reduction rule is augmented tohandle indices.
X ,  Y denotes the combinationof the indices X and Y.
(X +- G Afl, l l )(c' +-,12/+- a), n .
12> (2)With the use of indices, the lookup relationbecomes a relation between goals and items.
Thefollowing specification of the lookup relation pro-vides indexing according to string positions as ina chart parser (usable for combination schemes 2,3, and 4).lookup (phon : PhonList,item(lexical sign(phon:\[Word\] &synsem:X),Begin-End)) <-nt h_member (Word, Begin, End, PhonLis t ),lexicon(Word,X) .nth member(X,O,l,\[X\[_\]).nth member(X,NI,N2,\[_lR\]) <-nth member(X,NO,Ni,R),N2 is N1 + 1.2.3 Goal TypesIn constraint-based grammars there are some pre-dicates that are not adequately dealt with bybottom-up Earley deduction, for example thellead Feature Principle and the SubcategorizationPrinciple of nrsG.
The Head Feature Principlejust unifies two variables, so that it can be exe-cuted at compile time and need not be called asa goal at runtime.
The Subcategorization Princi-ple involves an operation on lists (appond/3 ordo lo t+/3  in different formalizations) that doesnot need bottom-up processing, but can betterbe evaluated by top-down resolution if its argu-ments are sulficiently instantiated.
Creating andmanaging items for these proofs is too much of acomputational overhead, arid, moreover, a proofmay not terminate in the bottom-up case becauseinfinitely many consequences may be derived fi'omthe base case of a recursively defined relation.In order to (teal with such goals, we associatethe goals in the body of a clause with goal types.The goals that are relevant for bottom-up Earleydeduction are called wailing goals because theywait until they are activated by a unit clause thatunifies with the goalfi Whenever at unit clause is6The other goM types arc top-down goals (top-down799combined with a non-unit clause all goals up tothe first waiting goal of the resulting clause areproved according to their goal type, and then anew clause is added whose selected goal is the firstwaiting goal.In the following inference rule for clauses withmixed goal types, E is a (possibly empty) sequenceof goals without any waiting goals, and 9t is a(possibly empty) sequence of goals starting witha waiting goal.
(r is the most general unifier ofG and G ~, and the substitution v is the solutionwhich results from proving the sequence of goals~,(X+- -GA~A~, I1 )(a' ~-, x2)( ra (X  ~ f~), 11 * 12)(a)2.4 Correctness and CompletenessIn order to show the correctness of the system,we must show that the scanning step only addsconsequences of the program to the chart, andthat any items derived by the inference rule areconsequences of the program clauses.
The formeris easy to show because all clauses added by thescanning step are instances of program clauses,and the inference rule performs a resolution stepwhose correctness i  well-known in logic program-ming.
The other goal types are also proved byresolution.There are two potential sources of incomple-teness in the algorithm.
One is that the scanningstep may not add all the program clauses to timchart that are needed for proving a goal, and theother is that the indexing may prevent he deriva-tion of a clause that is needed to prove the goal.In order to avoid incompleteness, the scanningstep must add all program clauses that are neededfor a proof of the goal to the chart, and the combi-nation of indices may only fail for inference stepswhich are useless for a proof of the goal.
Thatdepth-first search), x-corner goals (which combine bottom-up and top-down processing like left-corner or head-corneralgorithms), Prolog goals (which are directly executed byProlog for efficiency or side-effects), and chart goals whichcreate a new, independent chart for the proof of the goal.DSrre \[3\] proposes a system with two goal types, namelytrigger goals, which lead to the creation of items and othcrgoals which don't.the lookup relation and the indexing scheme sa-tisfy this property must be shown for particulargrammar formalisms.In order to keep the search space small (andfinite to ensure termination) the scanning stepshould (ideally) add only those items that are nee-ded for proving the goal to the chart, and the in-dexing should be chosen in such a way that it ex-cludes derived items that are useless for a proof ofthe goal.3 Best-First SearchFor practical NL applications, it is desirable tohave a best-first search strategy, which follows themost promising paths in the search space first, and:finds preferred solutions before the less preferredones.There are often situations where the criteriato guide the search are available only for the basecases, for example?
weighted word hypotheses from a speech re-cognizer?
readings for ambigous words with probabili-ties, possibly assigned by a stochastic tagger(el.
\[2\])hypotheses for correction of string errorswhich should be delayed \[5\]Goals and clauses are associated with prefe-rence values that are intended to model the de-gree of confidence that a particular solution is the~correct' one.
Unit clauses are associated witha numerical preference value, and non-unit clau-ses with a formula that determines how its prefe-rence value is computed fi'om the preference va-lues of the goals in the body of the clause.
Prefe-rence values can (but need not) be interpreted asprobabilities.
7The preference values are the basis for givingpriorities to items, l'br unit clauses, the priority isidentified with the preference value, tibr non-unitclauses, where the preference formula may containuninstantiated variables, the priority is the valueof the formula with the free variables instantiatedto the highest possible preference value (in case7For further details and examples ee \[4\] and \[5\].800of an interpretation as probabilities: 1), so thatthe priority is equal to the maximal possible pre-ference valne for the clause, sThe implementation of best-first search doesnot combine new itelns with the chart immedia-tely, but makes use of an agenda \[8\], on which newitems are ordered in order of descending priority.The following is the algorithm for bottom-up best-first F, arley deduction.procedure prove( Goal):- initialize-agenda(Goal)- consume-agenda- for any item {G,I)- return mgu(Goal, G) as solution if it existsprocedure initialize-agenda(Goal):- f o r  every unit clause UCin lookup(Goal, UC)- create the index I for UC- add item (UC, I) to agenda- for every non-unit program clause H +- Body- add item (H ",-- 13ody.free) to agendaprocedure add item 1 to agenda- compute the priority of I- agenda := agenda 12 {I}procedure consume-agenda- while agenda is not empty- remove item I with highest priority from agenda- add item I to chartprocedure add item (C, It) to chart- chart := chart O {(C, I1)}- if 6' is a unit clause- for all items (H ~-- G A E A ~, 12)- if I = 12-k I1 existsand r, = mgu(C, G) existsand goals ~ are provable with solution rthen add item (ra(H ~- ~), 1) to agenda- i f  C = H ~- GAEA ~ is a non-unit clause- for all items (G' ~-, I2)- if I = I1 -k I2 existsand ~r = mgu(G, G') existsand goals ~ are provable with solution rthen add item (ra(lt +- ~2), I) to agendaThe algorithm is parametrized with respect othe relation lookup/2 and the choice of the inde-xing scheme, which are specific for difi'erent gram-matical theories and directions of processing.SThere are also other methods for mssigning priorities toitems.4 ImplementationThe bottom-up Earley deduction algorithm des-cribed here has been implemented in Quintus Pro-log as part of the GeLD system.
GeLD (Gene-raJized Linguistic Deduction) is an extension ofProlog which provides typed feature descriptionsand preference values as additions to the expressi-vity of the language, and partial evaluation, top-down, head-driven, and bottom-up Barley deduc-tion as processing strategies.
Tests of the systemwith small grammars have shown promising re-salts, and a medium-scale HPSG for German is pre-sently being implemented in GeLD.
The lookuprelation and the choice of an indexing scheme mustbe specified by the user of the system.5 Conclusion and thlture WorkWe have proposed bottom-up Earley deductionas a useful alternative to the top-down methodswhich require subsumption checking and restric-tion to avoid prediction loops.The proposed method should be improved intwo directions.
The first is that the lookup predi-(:ate should not have to be specified by the user,but automatically inferred from the program.The second problem is that all non-unit clau-ses of tile program are added to the chart.
Theaddition of non-unit clauses should be made de-pendent on the goal and the base cases in orderto go from a purely bottora-up algorithm to a di-rected algorithm that combines the advantages oftop-down and bottom-up rocessing.
It has beenrepeatedly noted \[8, 17, 1\] that directed methodsare more efficient than pure top-down or bottom-up methods.
However, it is not clear how wellthe directed methods are applicable to grammarswhich do not depend on concatenation and haveno unique 'left cornet" which should be connectedto the start symbol.It remains to I)e seeit how bottom-up Barleydeduction compares with (and can be combinedwith) the improved top-down Barley deduction ofl)hrre \[3\], Johnson \[7\] mud Neumann \[91, and tohead-driven methods with well-formed substringtables \[1\], and which methods are best suited forwhich kinds of problems (e.g.
parsing, generation,noisy input, incremental processing etc.
).807References\[1\] Gosse Bouma and Gertjan van Noord.
Head-driven parsing for lexicalist grammars: Expe-rimental results.
In EACL93, pages 71 - 80,Utrecht, NL, 1993.\[2\] Chris Brew.
Adding preferences to CUF.In Jochen D6rre, editor, DYANA-2 Deli-verable RI.2.A: Computational Aspects ofConstraint-Based Linguistic Descrotion I,pages 57 - 69.
Esprit Basic Research Project6852, 1993.\[3\] Jochen D6rre.
Generalizing Earley deductionfor constraint-based grammars.
In JochenD6rre, editor, DYANA-2 Deliverable RI.2.A:Computational Aspects of Constraint-BasedLinguistic Description I, pages 23 - 41.
Es-prit Basic Research Project 6852, 1993.\[4\] Gregor Erbach.
Using preference values in ty-ped feature structures to exploit non-absoluteconstraints for disambiguation.
In HaraldTrost, editor, Feature Formalisms and Lin-guistic Ambiguity.
E1Jis-Horwood, 1993.Gregor Erbach.
Towards a theory of degreesof grammaticality.
In Carlos Martfn-Vide,editor, Current Issues in Mathematical Lin-guistics.
North-Holland, Amsterdam, to ap-pear.
Also published as CLAUS Report 34,Universit?t des Saarlandes, 1993.\[6\] Dale Douglas Gerdemann.
Parsing and Ge-neration of Unification Grammars.
PhDthesis, University of Illinois at Urbana-Champaign, 1991.
Cognitive Science techni-cal report CS-91-06 (Language Series).\[7\] Mark Johnson.
Memoization i  constraint lo-gic programming.
Department of CognitiveScience, Brown University.
Presented at the1st International Conference on ConstraintProgramming, Newport, Rhode Island; to ap-pear in the proceedings, 1993.\[8\] Martin Kay.
Algorithm schemata nd datastructures in syntactic processing.
Techni-cal Report CSL-80-12, XEROX PARC, PaloAlto, CA, 1980.\[9\] Giinter Neumann.
A Uniform Tabular Al-gorithm for Natural Language Parsing andGeneration and its Use within Performance-based Methods.
PhD thesis, UniversitySaarbrficken.
forthcoming.\[10\] Fernando C.N.
Pereira and David H.D.
War-ren.
Parsing as deduction.
In ACL Procee-dings, 21st Annual Meeting, pages 137-144,1983.\[11\] Carl Pollard.
Generalized Context-FreeGrammars, Head Grammars, and NaturalLanguage.
PhD thesis, Stanford, 1984.\[12\] Mike Reape.
A theory of word order and dis-continuous constituency in West Germanic.In E. Engdahl and M. Reape, editors, Para-metric Variation in Germanic and Romance:Preliminary Investigations, pages 25-40.
~S-PRIT Basic Research Action 3175 DYANA, De-liverable RI.I.A, 1990.\[13\] Stuart M. Shieber.
A uniform architecturefor parsing and generation.
In Proceedings ofthe 12th International Conference on Com-putational Linguistics (COLING), Budapest,1988.\[14\] Gertjan van Noord.
Reversibility in NaturalLanguage Processing.
PhD thesis, Rijksuni-versiteit Utrecht, NL, 1993.\[15\] K. Vijay-Shanker, David J. Weir, and Ara-vind K. Joshi.
Characterizing structural de-scriptions produced by various grammaticalformalisms.
In 25th Annual Meeting, pages104-111, Stanford, CA, 1987.
Association forComputational Linguistics.\[16\] David J. Weir.
Characteri-zing Mildly Context-Sensitive Grammar For-malisms.
PhD thesis, Department of Com-puter and Information Science, University ofPennsylvania, 1988.\[17\] Mats Wir6n.
A comparison of rule-invocationstrategies in context-free chart parsing.
InACL Proceedings, Third European Confe-rence, pages 226-235, 1987.802
