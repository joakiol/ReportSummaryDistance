LFG Semant ics  v ia  Const ra in tsMary Dalrymple John Lamping Vijay Saraswat{dalrymple, lamping, saraswat}@parc.xerox.comXerox PARC3333 Coyote Hill RoadPalo Alto, CA 94304 USAAbstractSemantic theories of natural language as-sociate meanings with utterances by pro-viding meanings for lexical items andrules for determining the meaning oflarger units given the meanings of theirparts.
Traditionally, meanings are com-bined via function composition, whichworks well when constituent structuretrees are used to guide semantic com-position.
More recently, the functionalstructure of LFG has been used to pro-vide the syntactic information ecessaryfor constraining derivations of meaningin a cross-linguistically uniform format.It has been difficult, however, to recon-cile this approach with the combinationof meanings by function composition.
Incontrast o compositional pproaches, wepresent a deductive approach to assem-bling meanings, based on reasoning withconstraints, which meshes well with theunordered nature of information in thefunctional structure.
Our use of linearlogic as a 'glue' for assembling meaningsalso allows for a coherent reatment ofmodification as well as of the LFG re-quirements of completeness and coher-ence.1 In t roduct ionIn languages like English, the substantial scaffold-ing provided by surface constituent s ructure trees isoften a useful guide for semantic omposition, andthe h-calculus is a convenient formalism for assem-bling the semantics along that scaffolding \[Montague,1974\].
This is because the derivation of the mean-ing of a phrase can often be viewed as mirroring thesurface constituent structure of the English phrase.The sentence Bill kissed Hillary has the surface con-stituent structure indicated by the bracketing in 1:(1) \[s \[NF Bill\] \[ve kissed \[NP Hillary\]\]\]The verb is viewed as bearing a close syntactic rela-tion to the object and forming a constituent with it;this constituent then combines with the subject ofthe sentence.
Similarly, the meaning of the verb canbe viewed as a two-place function which is appliedfirst to the object, then to the subject, producing themeaning of the sentence.However, this approach is not as natural for lan-guages whose surface structure does not resemblethat of English.
For instance, a problem is presentedby VSO languages such as Irish \[McCloskey, 1979\].To preserve the hypothesis that surface constituentstructure provides the proper scaffolding for seman-tic interpretation i VSO languages, one of two as-sumptions must be made.
One must assume itherthat semantic omposition is nonuniform across lan-guages (leading to loss of explanatory power), or thatsemantic omposition proceeds not with reference tosurface syntactic structure, but instead with refer-ence to a more abstract (English-like) constituentstructure representation.
This second hypothesisseems to us to render vacuous the claim that surfaceconstituent structure is useful in semantic omposi-tion.Further problems are encountered in the seman-tic analysis of a free word order language such asWarlpiri \[Simpson, 1983; Simpson, 1991\], where sur-face constituent s ructure does not always give rise tounits that are semantically coherent or useful.
Here,an argument of a verb may not even appear as asingle unit at surface constituent structure; further,97arguments of a verb may appear in various differentplaces in the string.
In such cases, the appeal to anorder of composition different from that of Englishis particularly unattractive, since different orders ofcomposition would be needed for each possible wordorder sequence.The observation that surface constituent struc-ture does not always provide the optimal set of con-stituents or hierarchical structure to guide semanticinterpretation has led to efforts to use a more ab-stract, cross-linguistically uniform structure to guidesemantic omposition.
As originally proposed byKaplan and Bresnan \[1982\] and Halvorsen \[1983\], thefunctional structure or f-structure of LFG is a rep-resentation of such a structure.
However, as notedby Halvorsen \[1983\] and Reyle \[1988\], the A-calculusis not a very natural tool for combining meaningsof f-structure constituents.
The problem is that thesubconstituents of an f-structure are not assumed tobe ordered, and so the fixed order of combination ofafunctor with its arguments imposed by the A-calculusis no longer an advantage; in fact, it becomes a disad-vantage, since an artificial ordering must be imposedon the composition of meanings.
Furthermore, thecomponents of the f-structure may be not only com-plements but also modifiers, which contribute to thefinal semantics in a very different way.Related approaches.
In an effort to solve theproblem of the order-dependence imposed by stan-dard versions of the A-calculus, Reyle \[1988\] pro-poses to extend the A-calculus to reduce its sequen-tial bias, assembling meanings by an enhanced ap-plication mechanism.
However, it is not clear howReyle's ystem can be extended to treat modificationor complex predicates, phenomena which our use oflinear logic allows us to handle.Another means of overcoming the problem of theorder-dependence of the A-calculus is to adopt se-mantic terms whose structure resembles f-structures\[Fenstad et al, 1985; Pollard and Sag, 1987;Halvorsen and Kaplan, 1988\].
On these approaches,attribute-value matrices are used to encode seman-tic information, allowing the syntactic and semanticrepresentations to be built up simultaneously andin the same order-independent manner.
However,when expressions of the A-calculus are replaced withattribute-value matrices, other problems arise: inparticular, it is not clear how to view such attribute-value matrices as formulas, since issues such as therepresentation f variable binding and scope are nottreated precisely.These problems have been noted, and remedieshave been proposed.
Sometimes, for example, analgorithm is given which globally examines a se-mantic attribute-value matrix representation to con-struct a sentence in a well-defined logic; for in-stance, Halvorsen \[1983\] presents an approach inwhich attribute-value matrices are translated intoformulas of intensional logic.
However, the compu-ration involved is concerned with manipulating theserepresentations in procedural ways: it is hard to seehow these procedural mechanisms translate to mean-ing preserving manipulations on the formulas thatthe matrices represent.
In sum, such approachestend to sacrifice the semantic precision and declara-tive simplicity of logical approaches (e.g.
A-calculusbased approaches), and seem difficult o extend gen-erally or motivate convincingly.Our approach.
Our approach shares the order-independent features of approaches that representsemantic information using attribute-value matrices,while still allowing a well-defined treatment of vari-able binding and scope.
We do this by identifying(1) a language of meanings and (2) a language forassembling meanings.In principle, (1) can be any logic (e.g., Montague'shigher-order logic); for the purposes of this paper allwe need is the language of first-order terms.
Becausewe assemble the meaning out of semantically pre-cise components, our approach shares the precisionof the A-calculus based approaches.
For example, theassembled meaning has precise variable binding andscoping.We take (2) to be a fragment of first-order (linear)logic carefully chosen for its computational proper-ties, as discussed below.
In contrast o using theh-calculus to combine fragments of meaning via or-dered applications, we combine fragments of mean-ing through unordered conjunction, and implication.Rather than using )~-reduction to simplify mean-ings, we rely on deduction, as advocated by Pereira\[1990; 1991\].The elements of the f-structure provide an un-ordered set of constraints, expressed in the logic,governing how the semantics can fit together.
Con-straints for combining lexically-provided meaningscan be encoded in lexical items, as instructions forcombining several arguments into a result.
1In effect, then, our approach uses first order logicas the 'glue' with which semantic representations areassembled.
Once all the constraints are assembled,deduction in the logic is used to infer the mean-ing of the entire structure.
Throughout this processwe maintain a sharp distinction between assertionsabout he meaning (the glue) and the meaning itselfiTo better capture some linguistic properties, wemake use of first order linear logic as the glue withwhich meanings are assembled \[Girard, 1987\].
~ Onea Constraints may also be provided as rules govern-ing particular configurations.
Such rules are applicablewhen properties not of individual lexical items in the con-struction but of the construction as a whole are responsi-ble for its interpretation; these cases include the seman-tics of relative clauses.
We will not discuss examples ofconfigurationally-defined rules in this paper.2Specifically, we make use only of the tensor\]ragmentof linear logic.
The fragment isclosed under conjunction,universal quantification and implication (with atomic an-98way of thinking about linear logic is that it intro-duces accounting of premises and conclusions, o thatdeductions consume their premises to generate theirconclusions.
It turns out that this property of linearlogic nicely captures the LFG requirements of co-herence and consistency, and additionally providesa natural way to handle modifiers: a modifier con-sumes the unmodified meaning of the structure itmodifies and produces from it a new, modified mean-ing.In the following, we first illustrate our approachby discussing a simple example, and then presentmore complex examples howing how modifiers andvalence changing operations are handled.2 Theoret i ca l  p re l iminar iesIn the following, we describe two linguistic assump-tions that underlie this work.
First, we assume thatvarious aspects of linguistic structure (phonological,syntactic, semantic, and other aspects) are formallyrepresented as projections and are related to one an-other by means of functional correspondences.
Wealso assume that the relation between the thematicroles of a verb and the grammatical functions thatrealize them are specified by means of mapping prin-ciples which apply postlexically.Pro ject ions .
We adopt the projection architec-ture proposed by Kaplan \[1987\] and Halvorsen andKaplan \[1988\] to relate f-structures to representa-tions of their meaning: f-structures are put in func-tional correspondence with semantic representations,similar to the correspondence b tween odes of theconstituent structure tree and f-structures.
The se-mantic projection of an f-structure, written with asubscript ~r, is a representation of the meaning ofthat f-structure.Thus, the notation 'Ta' in the lexical entries givenin Figure 1 stands for the semantic projection of thef-structure 'T'; similarly, '(T svBJ)a' is the seman-tic projection of (i" StTBJ).
The equation T,= Billindicates that the semantic projection of 1", the f-structure introduced by the NP Bill, is Bill.
Thelexical entry for Hillary is analogous.
When a lexicalentry is used, the metavariable '~" is instantiated andreplaced with an actual variable corresponding to anf-structure f,~ \[Kaplan and Bresnan, 1982, page 183\].Similarly, the metavariable 'T~' is instantiated to alogic variable corresponding to the meaning of thef-structure.
In other words, the equation T~,= Billis instantiated as fna = Bill for some logic variablefna-tecedents).
It arises from transferring to linear logic theideas underlying the concurrent constraint programmingscheme of Saraswat \[1989\] - -  an explicit formulation forthe higher-order version of the linear concurrent con-straint programming scheme is given in Saraswat andLincoln \[1992\].
A nice tutorial introduction to linear logicitself may be found in Scedrov \[1990\].We have used the multiplicative conjunction ?
andlinear implication -o connectives of linear logic,rather than the analogous conjunction A and impli-cation ~ of classical logic.
For the present, we canthink of the linear and classical connectives as beingidentical.
Similarly, the of course connective '!'
oflinear logic can be ignored for now.
Below, we willdiscuss respects in which the linear logic connectiveshave properties that are crucially different from theircounterparts in classical logics.Mapp ing  principles.
We follow Bresnan andKanerva \[1989\], Alsina \[1993\], Butt \[1993\] and oth-ers in assuming that verbs specify an association be-tween each of their arguments and a particular the-matic role, and that mapping principles associatethese thematic roles with surface grammatical func-tions; this assumption, while not necessary for thetreatment of simple examples uch as the one dis-cussed in Section 3, is linguistically well-motivatedand enables us to provide a nice treatment of com-plex predicates, to be discussed in Section 5.The lexical entry for kiss specifies the denotationof (T PRED): it requires two arguments which wewill label agent and theme.
Mapping principles en-sure that each of these arguments i associated withsome grammatical function: here, the SUBa of kiss(Bill) is interpreted as the agent, and the OBa of kiss(Hillary) is interpreted as the theme.
The specificmapping principles that we assume are given in Fig-ure 2.The function of the mapping principles is to spec-ify the set of possible associations between gram-matical functions and thematic roles.
This is doneby means of implication.
Grammatical functions al-ways appear on the left side of a mapping princi-ple implication, and the thematic roles with whichthose grammatical functions are associated appearon the right side.
Mapping principle (1), for exam-ple, relates the thematic roles of agent and themedesignated by a two-argument verb like kiss to thegrammatical functions that realize these arguments:it states that if a suBJ and an osa are present, thispermits the deduction that the thematic role of agentis associated with the suBJ and the thematic role oftheme is associated with the oBJ.
(Other associa-tions are encoded by means of other mapping prin-ciples; the mapping principles given in Figure 2 en-codes only two of the possibilities.
)We make implicit appeal to an independently-given, fully-worked-out theory of argument mapping,from which mapping principles such as those givenin Figure 2 can be shown to follow.
It is impor-tant to note that we do not intend any claims aboutthe correctness of the specific details of the map-ping principles given in Figure 2; rather, our claimis that mapping principles hould be of the generalform illustrated there, specifying possible relationsbetween thematic roles and grammatical functions.In particular, no theoretical significance should be99BillkissedHillaryNP (T PRED) "- 'BILL'T.
= BillV (I" PP~ED)'- KISS'VX, Y. agent(( T PRED)a, X) ?
theme(( T FRED)o, Y) ---o To-- kiss(X, Y)NP (~ PRED) = 'HILLARY'T.
= Hil larvFigure 1: Lexical entries for Bill, kissed, l:Iillary(i) !
(Vf, X,Y.
((f SUBJ).
= X) ?
((f OBJ).
= Y) -0  agent(( I PRED).,X) ?theme((f PRED).,Y))(2) I(VI, X, Y, Z.
((f  SUB.I).
= X) ?
((f OBJ).
= Y) ?
((f onJ2).
= Z) -opermitter((f PRED)., X) ?
agellt((f PRED)., Z) ?
theme(( I PRED)., Y))Figure 2: Argument mapping principlesbill: (f2o = Bill)hil lary : (fa.
= Hillarv)kiss: (VX, Y.
agent(f1., X )  ?
theme(f1.,  Y )  --0 f4.
= kiss(X, g ) )mapping l  : (VX, Y.
(f2.
= X)  ?
(f3.
= Y)  --o agent(f1., X )  ?
theme(f1.
,  g ) ) )(bill ?
hil lary ?
kissed ?
mappingl )--o agent(ft .
,  Bill) ?
~heme(f l .
,  H illarv) ?
kissed--o f4.
= kiss(Bill, Hil larv)(Premises.
)(UI, Modus Ponens.
)(UI, Modus Ponens.
)Figure 3: Derivation of Bill kissed Hiilaryattached to the choice of thematic role labels usedhere; for the verb kiss, for example, labels such as'kisser' and 'kissed' would do as well.
We requireonly that the thematic roles designated in the lexicalentries of individual verbs are specified in enough de-tail for mapping principles uch as those illustratedin Figure 2 to apply successfully.3 A simple example of semanticcompositionConsider sentence 2 and the lexieM entries given inFigure 1:(2) Bill kissed Hillary.The f-structure for (2) is:(3) \[PRED fl :'KISS'f,: SUBJ f2: \[ PRED 'BILL'\]OSJ f3: \[ PRED 'HILLARY' \]The meaning associated with the f-structure may bederived by logical deduction, as shown in Figure 3. a?An alternative derivation, not using mapping princi-ples, is also possible.
In that case, the lexical entry forkissed would require a SUBJ and an OBJ rather than anagent and a theme, and the derivation would proceed inThe first three lines contain the information con-tributed by the lexical entries for Bill, tIillarv, andkissed, abbreviated as bill, hillary, and kissed.
Theverb kissed requires two pieces of information, anagent and a theme, in no particular order, to producea meaning for the sentence, f4,.
The mapping prin-ciple needed for associating the syntactic argumentsof transitive verbs with the agent/theme argumentstructure is given on the fourth line and abbreviatedas mapping1.
Mapping principles are assumed tobe a part of the background theory, rather than beingintroduced by particular lexical items.
Each map-ping principle can, then, be used as many or as fewtimes as necessary.The premises--i.e., the lexical entries and map-ping principle are restated as the first step of thederivation, labeled 'Premises'.
The second step isderived from the premises by Universal Instantia-tion and Modus Ponens.
The last step is then de-rived from this result by Universal Instantiation andModus Ponens.To summarize: a variable is introduced for themeaning corresponding to each f-structure in thethis way:((f~ = Bill)?
(13o = Hillary)?(?X,Y.I2.
= X ?
Ia.
= Y --o h,, = kiss(X, Y)))--0 f4~, = kiss(Bill, Hillary)100syntactic representation.
These variables form thescaffolding that guides the assembly of the meaning.Further information is then introduced: informationassociated with each lexical entry is made available,as are all the mapping rules.
Once all this informa-tion is present, we look for a logical deduction of ameaning of the sentence from that information.The use of linear logic provides certain advantages,since it allows us to capture the intuition that lexicalitems and phrases contribute uniquely to the mean-ing of a sentence.
As noted by Klein and Sag \[1985,page 172\]:Translation rules in Montague semanticshave the property that the translation ofeach component of a complex expressionoccurs exactly once in the translation ofthe whole .
.
.
.
That is to say, we do notwant the set S \[of semantic representa-tions of a phrase\] to contain all meaning-ful expressions of IL which can be builtup from the elements of S, but only thosewhich use each element exactly once.Similar observations underlie the work of Lambek\[1958\] on categorial grammars and the recent workof van Benthem \[1991\] and others on dynamic logics.It is this 'resource-conscious' property of naturallanguage semantics - a meaning is used once andonce only in a semantic derivation - that linear logicallows us to capture.
The basic insight underlyinglinear logic is to treat logical formulas as finite re-sources, which are consumed in the process of de-duction.
This gives rise to a notion of linear impli-cation --o which is resource-conscious: the formulaA --o B can be thought of as an action that can con-sume (one copy of) A to produce (one copy of) B.Thus, the formula A?
(A --o B) linearly implies B -but not A ?
B (because the deduction consumes A),and not (A --o B) ?
B (because the linear implica-tion is also consumed in doing the deduction).
Theresource consciousness not only disallows arbitraryduplication of formulas, but also arbitrary deletionof formulas.
This causes the notion of conjunction weuse (?)
to be sensitive to the multiplicity of formu-las: A?A is not equivalent to A (the former has twocopies of the formula A).
For example, the formulaA ?
A ?
(A -o B) does linearly imply A ?
B (there isstill one A left over) - -  but does not linearly imply B(there must still be one A present).
Thus, linear logicchecks that a formula is used once and only once ina deduction, reflecting the resource-consciousness ofnatural anguage semantics.
Finally, linear logic hasan of course connective !
which turns off accountingfor its formula.
That is, !A linearly implies an arbi-trary number copies of A, including none.
We usethis connective on the background theory of map-ping principles to indicate that they are not subjectto accounting; they can be used as often or seldomas necessary.A primary advantage of the use of linear logic isthat it enables a clean semantic definition of com-pleteness and coherence.
4 In the present setting, thefeature structure f corresponding to the utteranceis associated with the (?)
conjunction ?
of all theformulas associated with the lexical items in the ut-terance.
The conjunction is said to be complete andcoherent iff Th t- ?
--o fa = t (for some term t),where Th is the background theory containing, e.g.,the mapping principles.
Each t is to be thought ofas a valid meaning for the sentence.
This guaranteesthat the entries are used exactly once in building upthe denotation of the utterance: no syntactic or se-mantic requirements may be left unfulfilled, and nomeaning may remain unused.4 Mod i f i ca t ionAnother primary advantage of the use of linear logic'glue' in the derivation of meanings of sentences ithat it enables a clear treatment o f  modification.Consider the following sentence, containing the sen-tential modifier obviously:(4) Bill obviously kissed Hillary.We make the standard assumption that the verbkissed is the main syntactic predicate of this sen-tence.
The following is the f-structure for example4:(5) \[PRED fl :'KISS'SUBJ f~: \[ PRED 'BILL'\]h" OBJ /3: \[ PRED 'HILLARY' \]Mo,s  P ED 'OBVIOUSLY' \]}We also assume that the meaning of the sentence canbe represented by the following formula:(6) obviously(kiss(Bil l ,  Hi l lary))It is clear that there is a 'mismatch' of sorts betweenthe syntactic representation a d the meaning of thesentence; syntactically, the verb is the main functor;while the main semantic functor is the adverb.
5Consider now the lexical entry for obviously givenin Figure 4.
The semantic equation associated with4'An f-structure is locally complete if and only if itcontains all the governable grammatical functions thatits predicate governs.
An f-structure is complete if andonly if all its subsidiary f-structures are locally complete.An f-structure is locally coherent if and only if all the gov-ernable grammatical functions that it contains are gov-erned by a local predicate.
An f-structure is coherent ifand only if all its subsidiary f-structures are locally co-herent.'
\[Kaplan and Bresnan, 1982, pages 211-212\]5The related phenomenon of head switching, discussedin connection with machine translation by Kaplan et al\[1989\] and Kaplan and Wedekind \[1993\], is also amenableto treatment along the lines presented here.101Bill NP (T PLIED) ---- 'BILL'To = Billobviously ADV (T PliED) -- 'OBVIOUSLY'VP.
(MODS T)o = P --o (MODS T)?
= obviously(P)kissed V (T FliED).-- 'KISS'VX, Y. agent((T PliED)a, X) ?
therlle((T PliED)a, Y) --o To---- kiss(X, Y)Hillary NP (T PliED) = 'HILLAIIY'To = HillaryFigure 4: Lexical entries for Bill, obviously, kissed, ttillarybil l :  (f2a = Bill)hil lary : (f3o = Hillary)kiss : (VX, Y.
agent(f1?, X) ?
theme(rio, Y) --o f4a = kiss(X, Y))obvious ly:  (VP.
f4?
= P -o f4q -- obviously(P))mapping l  : (VX, Y.
(f2a - X) ?
(f3o = Y) -o agent(flu, X) ?
theme(rio, Y)))(bill ?
hil lary ?
kissed ?
obviously ?
mapping l )-o agent(flu, Bill) ?
theme(flu, gillary) ?
kissed ?
obviously-o f4a = kiss(Bill, Hillary) ?
obviously-o ho -" obviously(kiss(Bill, H illary) )Figure 5: Derivation of Bill obviously kissed Hillary(Premises.
)(UI, Modus Ponens.
)(UI, Modus Ponens.
)(UI, Modus Ponens.
)obviously makes use of 'inside-out functional uncer-tainty' \[Halvorsen and Kaplan, 1988\].
The expres-sion (MODS T) denotes an f-structure through whichthere is a path MODS leading to T. For example, ifT is the f-structure labeled f5 above, then (MODS T)is the f-structure labeled f4, and (MODS T)a is thesemantic projection of f4- Thus, the lexical entry forobviously specifies the semantic representation f thef-structure that it modifies, an f-structure in whichit is properly contained.Recall that linear logic enables acoherent notion ofconsumption and production of meanings.
We claimthat the semantic function of adverbs (and, indeed,of modifiers in general) is to consume the meaning ofthe structure they modify, producing anew, modifiedmeaning.
Note in particular that the meaning ofthe modified structure, (MOPS T)a, appears on bothsides of -o ; the unmodified meaning is consumed,and the modified meaning is produced.The derivation of the meaning of example 4 isshown in Figure 5.
The first part of the derivation isthe same as the derivation shown in Figure 3 for thesentence Bill kissed Hillary.
The crucial differenceis the presence of information introduced by obvi-ously, shown in the fourth line and abbreviated asobviously.
In the last step in the derivation, thelinear implication introduced by obviously consumesthe previous value for f4a and produces the new andfinal value.By using linear logic, each step of the derivationkeeps track of what 'resources' have been consumedby linear implications.
As mentioned above, thevalue for f4?
is a meaning for this sentence only ifthere is no other information left.
Thus, the deriva-tion could not stop at the next to last step, becausethe linear implication introduced by obviously wasstill left.
The final step provides the only completeand coherent meaning derivable for the utterance.5 Va lence-chang ing  operat ionsWe have seen that modifiers can be treated as 'con-suming' the meaning of the structure that they mod-ify, producing a new, modified meaning.
A simi-lar, although syntactically more complex, case ariseswith complex predicates, as Butt \[1990; 1993\] shows.Butt discusses the 'permissive construction' inUrdu, illustrated in 7:(7) Hillary-ne diyaa \[vP Bill-ko xatHillary-ERG let BilI-DAT letter-NOMlikhne \]write-PART'Hillary let Bill write a letter.
'She shows that although the permissive constructionis seemingly biclausal, it actually involves a com-plex predicate: a syntactically monoclausal predicateformed in the presence of the verb diyaa 'let'.
Inthe case at hand, the presence of diyaa requires an102Hillary NP (T PRED) ---- 'HILLARY'To = HillaryBill NP (T PRED) ---- 'BILL'To : Billxat N (~ PRED) = 'LETTER'T~ = letterlikhne V (T PRED)= 'WRITE'VX, Y. agent(  T PRED)o, X) ?
theme( ( T FRED)o, Y) --o To = write(X, Y)diyaa V VX, P. permitter((T PREO)o, X)?
To= P --o T~= let(X, P)Figure 6: Lexical entries for Hillary, Bill, zat, likhne, diyaahil lary : (f2~ = Hillary)bill: (f3o = Bill)letter  : (f4a = letter)write:  (VX, Y. agent(rio, X) ?
theme(rio, Y) --o/so = write(X, Y ) )let : (VX, P. permitter(fbo, X) ?
fso = P --o = f6~ = let(X, P)mapplng2 : (VX, Y, Z.
(f~o = X) ?
( f~ = Y) ?
(f4~ = Z) --opermitter(rio, X) ?
agent(rio, Y) ?
theme(rio, Z) )(bill ?
hil lary ?
letter ?
write ?
let ?
mapplng2)--o permitter(fx?, Hillary) ?
agent(rio, Bill) ?
theme(rio, letter) ?
write ?
let--o permitter(f~, H illary) ?
let ?
(fbo = write(Bill, letter))--o fb~ = let(Hillary, (write(Bill, letter))Figure 7: Derivation of Hillary let Bill write a letter(Premises.
)(UI, Modus Ponens.
)(UI, Modus Ponens.
)(UI, Modus Ponens.
)additional argument which we will label 'permitter',in addition to the arguments required by the verblikhne 'write'.
In general, the verb diyaa 'let' modi-fies the argument structure of the verb with which itcombines, requiring in addition to the original inven-tory of arguments he presence of a permitter.
Thef-structure for example 7 is:(8) rPRED fl: 'LET(WRITE)'I | SUBJ f2: \[ PRED 'HILLARY' \]fS:lOBj2l f3:\[PRED 'BILL'\]\[ OBJ f4: \[ PRED 'LETTER'\]As Butt points out, the verbs participating in theformation of the permissive construction eed notform a syntactic onstituent; in example 7, the verbslikhne and diyaa are not even next to each other.This shows that complex predicate formation can-not be analyzed as taking place in the lexicon; amethod of dynamically creating a complex predicatein the syntax is needed.
That is, sentences such as 7have, in essence, two syntactic heads, which dynami-cally combine to produce asingle syntactic argumentstructure.We claim that the function of a verb such as per-missive diyaa is somewhat analogous to that of amodifier: diyaa consumes the meaning of the origi-nal verb and its arguments, producing a new permis-sive meaning and requiring an additional argument,the permitter.
Mapping principles apply to this new,augmented argument structure to associate the newthematic argument structure with the appropriateset of syntactic roles.
We illustrate the derivation ofthe meaning of example 7 in Figure 7.The lexical entries necessary for example 7 can befound in Figure 6.
The instantiated information fromthese lexical entries appears in the first five lines ofFigure 7.
Mapping principle (2) in Figure 2, ab-breviated as mapping2, links the permitter, agent,and theme of the (derived) argument s ructure to thesyntactic arguments ofa permissive construction; themapping principle is given in the sixth line of Figure7.
6The premises of the derivation are, as above, infor-mation given by lexical entries and the mapping prin-ciple.
By means of mapping principle mapping2, in-formation about the possible array of thematic rolesrequired by the complex predicate let-write can beeRecall that in our framework, all the mapping prin-ciples are present to be used as needed.
In the derivationof the meaning of example 7, shown in Figure 7, we haveomnisciently provided the one that will be needed.103derived; this step uses Universal Instantiation andModus Ponens.Next, a (preliminary) meaning for f-structure fs,write(Bill, letter), is derived by Universal Instan-tiation and Modus Ponens.
At this point, the re-quirements imposed by diyaa 'let', labeled let, aremet: a permitter (Hillary) is present, and a com-plete meaning for f-structure f5 has been produced.These meanings can be consumed, and a new mean-ing produced, as represented in the final line of thederivation.
Again, this meaning is the only one avail-able, since completeness and coherence obtains onlywhen all requirements are fulfilled and no extra infor-mation remains.
As with the case of modifiers, thefinal step provides the only complete and coherentmeaning derivable for the utterance.Notice that the meaning of the complex predi-cate is not derived by composition of verb meanings:the permissive verb diyaa does not combine withthe verb likhne 'write' to form a new verb mean-ing.
Instead, permissive diyaa requires a (prelim-inary) sentence meaning, write(Bill, letter) in theexample above, in addition to the presence of a per-mitter argument.More generally, this approach treats linguistic phe-nomena such as modification and complex predicateformation function by operating on semantic enti-ties that have combined with all of their arguments,producing a modified meaning and (in the case ofcomplex predicate formation) introducing further ar-guments.
While it would be possible to extend ourapproach to operate on semantic entities that havenot combined with all their arguments, we have notyet encountered a compelling reason to do so.
Ourcurrent restriction is not so confining as it might ap-pear; most operations that can be performed on se-mantic entities that have not combined with all theirarguments have analogues that operate on fully com-bined entities.
In further esearch, we plan to explorethis characteristic of our analysis more fully.6 ConclusionOur approach results in a somewhat different view ofsemantic omposition, compared to h-calculus basedapproaches.
First of all, notice that both in ~-calculus based approaches and in our approach, thereis not only a semantic level of meanings of utterancesand phrases, but also a glue level or composition levelresponsible for assembling semantic level meanings ofconstituents oget a meaning for an entire utterance.In h-calculus based approaches, the semantic levelis higher order intensional logic.
The compositionlevel is the rules, often not stated in any formal sys-tem, that say what pattern of applications to do toassemble the constituent meanings.
The compositionlevel relies on function application in the semanticlevel to assemble meanings.
This forces some confla-tion of the levels, because it is using a semantic leveloperation, application, to carry out a compositionlevel task.
It requires functions at the semantic levelwhose primary purpose is to allow the compositionlevel to combine meanings via application.
For exam-ple, in order for the composition level to work right,the semantic level meaning of a transitive verb mustbe a function of two arguments, rather than a rela-tion.
This rather artificial requirement ~sa symptomof some of the work of the composition level beingdone at the semantic level.Our approach, on the other hand, better segre-gates the two levels of meaning, because the com-position level uses its own mechanism (substitution)to assemble semantic level meanings, rather than re-lying on semantic level operations.
Thus, the linearlogic operations of the composition level don't appearat the semantic level and the classical operations ofthe semantic level don't appear at the compositionlevel.
7Our system also expresses the composition levelrules in a formal system, first order linear logic.
Thecomposition rules are expressed by relations in thelexical entries and the mapping rules.
There is noseparate process of deciding how the meanings oflexical entries will be combined; the relations theyestablish, together with some background facts, justimply the high level meaning.
All the necessaryconnections between phrases are made at the com-position level when lexical entries are instantiated,through the shared variables of the sigma projec-tions.
From then on, logical inference at the com-position level assembles the semantic level meaning.These examples illustrate the capability of ourframework to handle the combination of predi-cates with their arguments, modification, and arity-affecting operations.
The use of linear logic providesa simple treatment of the requirements of complete-ness and consistency and of complex predicates.
Fur-ther, our deduction framework enables us to use lin-ear logic to state such operations in a formally well-defined and tractable manner.In future work, we plan to explore more fully thesemantics of modification, and to pursue the addi-tion of a type system to the logic to treat quantifiersanalogously to Pereira \[1990; 1991\].7 AcknowledgmentsWe are grateful to Ron Kaplan, Stanley Peters, JohnMaxwell, Joan Bresnan, and Stuart Shieber for help-ful comments on earlier versions of this paper.
Wewould particularly like to thank Fernando Pereira forextensive and very helpful discussion of the issuespresented here.rThis separation is not a necessary consequence ofusing deduction to assemble meanings; the compositionlogic could call for semantic level operations.
But wehave so far been able to maintain the separation, and thequestion of whether the separation can be maintainedseems to be linguistically interesting and worthy of fur-ther pursuit.104References\[Alsina, 1993\] Alex Alsina.
Predicate Composition: ATheory of Syntactic Function Alternations.
PhD the-sis, Stanford University, 1993.\[Bresnan and Kanerva, 1989\] Joan Bresnan andJonni M. Kanerva.
Locative inversion in Chiche@a:A case study of factorization i grammar.
LinguisticInquiry, 20(1):1-50, 1989.
Also in E. Wehrli and T.Stowell, eds., Syntax and Semantics 26: Syntax andthe Lexicon.
New York: Academic Press.\[Butt et ai., 1990\] Miriam Butt, Michio Isoda, and Pe-ter Sells.
Complex predicates in LFG.
MS, StanfordUniversity, 1990.\[Butt, 1993\] Miriam Butt.
The Structure of ComplexPredicates.
PhD thesis, Stanford University, 1993.
Inpreparation.\[Fenstad et ai., 1985\] Jens Erik Fenstad, Per-KristianHalvorsen, Tore Langholm, and Johan van Benthem.Equations, schemata nd situations: A frameworkfor linguistic semantics.
Technical Report 29, Centerfor the Study of Language and Information, StanfordUniversity, 1985.\[Girard, 1987\] J.-Y.
Girard.
Linear logic.
TheoreticalComputer Science, 45:1-102, 1987.\[Halvorsen and Kaplan, 1988\] Per-Kristian Halvorsenand Ronald M. Kaplan.
Projections and semantic de-scription in Lexical-Functional Grammar.
In Proceed-ings of the International Conference on Fifth Gener-ation Computer Systems, pages 1116-1122, Tokyo,Japan, 1988.
Institute for New Generation Systems.\[Halvorsen, 1983\] Per-Kristian Halvorsen.
Semanticsfor Lexical-Functional Grammar.
Linguistic Inquiry,14(4):567-615, 1983.\[Kaplan and Bresnan, 1982\] Ronald M. Kaplan andJoan Bresnan.
Lexical-Functional Grammar: Aformal system for grammatical representation.
InJoan Bresnan, editor, The Mental Representationof Grammatical Relations, pages 173-281.
The MITPress, Cambridge, MA, 1982.\[Kaplan and Wedeldnd, 1993\] Ronald M. Kaplan andJfirgen Wedekind.
Restriction and correspondence-based translation.
In Proceedings of the Sixth Meet-ing of the European ACL, University of Utrecht, April1993.
European Chapter of the Association for Com-putational Linguistics.\[Kaplan et al, 1989\] Ronald M. Kaplan, Klaus Netter,Jurgen Wedekind, and Annie Zaenen.
Translationby structural correspondences.
In Proceedings of theFourth Meeting of the European ACL, pages 272-281, University of Manchester, April 1989.
EuropeanChapter of the Association for Computational Lin-guistics.\[Kaplan, 1987\] Ronald M. Kaplan.
Three seductionsof computational psycholinguistics.
In Peter White-lock, Harold Somers, Paul Bennett, Rod Johnson,and Mary McGee Wood, editors, Linguistic The-ory and Computer Applications, pages 149-188.
Aca-demic Press, London, 1987.\[Klein and Sag, 1985\] Ewan Klein and Ivan A. Sag.Type-driven translation.
Linguistics and Philosophy,8:163-201, 1985.\[Lambek, 1958\] Joachim Lambek.
The mathematics ofsentence structure.
American Mathematical Monthly,65:154-170, 1958.\[McCloskey, 1979\] James McCloskey.
Transformationalsyntax and model theoretic semantics : a case studyin modern Irish.
D. Reidel, Dordrecht, 1979.\[Montague, 1974\] Richard Montague.
Formal Philoso-phy.
Yale University Press, New Haven, 1974.
Rich-mond Thomason, editor.\[Pereira, 1990\] Fernando C. N. Pereira.
Categorial se-mantics and scoping.
Computational Linguistics,16(1):1'10, 1990.\[Pereira, 1991\] Fernando C. N. Pereira.
Semantic inter-pretation as higher-order deduction.
In Jan van Eijck,editor, Logics in AI: European Workshop JELIA '90,pages 78-96, Amsterdam, Holland, 1991.
Springer-Verlag.\[Pollard and Sag, 1987\] Carl Pollard and Ivan A. Sag.Information.Based Syntax and Semantics, Volume \].Number 13 in CSLI Lecture Notes.
CSLI/The Uni-versity of Chicago Press, Stanford University, 1987.\[Reyle, 1988\] Uwe Reyle.
Compositional semantics forLFG.
In Uwe Reyle and Christian Rohrer, editors,Natural language parsing and linguistic theories.
D.Reidel, Dordrecht, 1988.\[Saraswat nd Lincoln, 1992\] Vijay A. Saraswat andPatrick Lincoln.
Higher-order, linear concurrent con-straint programming.
Technical report, Xerox PaloAlto Research Center, August 1992.\[Saraswat, 1989\] Vijay A. Saraswat.
ConcurrentConstraint Programming Languages.
PhD thesis,Carnegie-Mellon University, 1989.
To appear, Doc-toral Dissertation Award and Logic Programming Se-ries, MIT Press, 1993.\[Scedrov, 1990\] A. Scedrov.
A brief guide to linear logic.Bulletin of the European Assoc.
for Theoretical Com-puter Science, 41:154-165, June 1990.\[Simpson, 1983\] Jane Simpson.
Aspects of WarlpiriMorphology and Syntax.
PhD thesis, MIT, 1983.\[Simpson, 1991\] Jane Simpson.
Waripiri Morpho-Syntax.
Kluwer Academic Publishers, Dordrecht,1991.\[van Benthem, 1991\] Johan van Benthem.
Languagein Action: Categories, Lambdas and Dynamic Logic.North-Holland, Amsterdam, 1991.105
