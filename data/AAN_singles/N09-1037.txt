Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 326?334,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsJoint Parsing and Named Entity RecognitionJenny Rose Finkel and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{jrfinkel|manning}@cs.stanford.eduAbstractFor many language technology applications,such as question answering, the overall sys-tem runs several independent processors overthe data (such as a named entity recognizer, acoreference system, and a parser).
This eas-ily results in inconsistent annotations, whichare harmful to the performance of the aggre-gate system.
We begin to address this prob-lem with a joint model of parsing and namedentity recognition, based on a discriminativefeature-based constituency parser.
Our modelproduces a consistent output, where the namedentity spans do not conflict with the phrasalspans of the parse tree.
The joint represen-tation also allows the information from eachtype of annotation to improve performanceon the other, and, in experiments with theOntoNotes corpus, we found improvements ofup to 1.36% absolute F1 for parsing, and up to9.0% F1 for named entity recognition.1 IntroductionIn order to build high quality systems for complexNLP tasks, such as question answering and textualentailment, it is essential to first have high qualitysystems for lower level tasks.
A good (deep analy-sis) question answering system requires the data tofirst be annotated with several types of information:parse trees, named entities, word sense disambigua-tion, etc.
However, having high performing, low-level systems is not enough; the assertions of thevarious levels of annotation must be consistent withone another.
When a named entity span has crossingbrackets with the spans in the parse tree it is usuallyimpossible to effectively combine these pieces of in-formation, and system performance suffers.
But, un-fortunately, it is still common practice to cobble to-gether independent systems for the various types ofannotation, and there is no guarantee that their out-puts will be consistent.This paper begins to address this problem bybuilding a joint model of both parsing and namedentity recognition.
Vapnik has observed (Vapnik,1998; Ng and Jordan, 2002) that ?one should solvethe problem directly and never solve a more gen-eral problem as an intermediate step,?
implying thatbuilding a joint model of two phenomena is morelikely to harm performance on the individual tasksthan to help it.
Indeed, it has proven very diffi-cult to build a joint model of parsing and seman-tic role labeling, either with PCFG trees (Sutton andMcCallum, 2005) or with dependency trees.
TheCoNLL 2008 shared task (Surdeanu et al, 2008)was intended to be about joint dependency parsingand semantic role labeling, but the top performingsystems decoupled the tasks and outperformed thesystems which attempted to learn them jointly.
De-spite these earlier results, we found that combiningparsing and named entity recognition modestly im-proved performance on both tasks.
Our joint modelproduces an output which has consistent parse struc-ture and named entity spans, and does a better job atboth tasks than separate models with the same fea-tures.We first present the joint, discriminative modelthat we use, which is a feature-based CRF-CFGparser operating over tree structures augmented withNER information.
We then discuss in detail howwe make use of the recently developed OntoNotescorpus both for training and testing the model, andthen finally present the performance of the modeland some discussion of what causes its superior per-formance, and how the model relates to prior work.326NPDTtheNPNNP[DistrictPPINofNPNNPColumbia] GPE=?NPDTtheNamedEntity-GPE*NP-GPENNP-GPEDistrictPP-GPEIN-GPEofNP-GPENNP-GPEColumbiaFigure 1: An example of a (sub)tree which is modified for input to our learning algorithm.
Starting from the normalizedtree discussed in section 4.1, a new NamedEntity node is added, so that the named entity corresponds to a singlephrasal node.
That node, and its descendents, have their labels augmented with the type of named entity.
The * on theNamedEntity node indicates that it is the root of the named entity.2 The Joint ModelWhen constructing a joint model of parsing andnamed entity recognition, it makes sense to thinkabout how the two distinct levels of annotation mayhelp one another.
Ideally, a named entity should cor-respond to a phrase in the constituency tree.
How-ever, parse trees will occasionally lack some explicitstructure, such as with right branching NPs.
In thesecases, a named entity may correspond to a contigu-ous set of children within a subtree of the entireparse.
The one thing that should never happen is fora named entity span to have crossing brackets withany spans in the parse tree.For named entities, the joint model should helpwith boundaries.
The internal structure of the namedentity, and the structural context in which it ap-pears, can also help with determining the type ofentity.
Finding the best parse for a sentence can behelped by the named entity information in similarways.
Because named entities should correspondto phrases, information about them should lead tobetter bracketing.
Also, knowing that a phrase is anamed entity, and the type of entity, may help in get-ting the structural context, and internal structure, ofthat entity correct.2.1 Joint RepresentationAfter modifying the OntoNotes dataset to ensureconsistency, which we will discuss in Section 4, weaugment the parse tree with named entity informa-tion, for input to our learning algorithm.
In the caseswhere a named entity corresponds to multiple con-tiguous children of a subtree, we add a new Name-dEntity node, which is the new parent to those chil-dren.
Now, all named entities correspond to a singlephrasal node in the entire tree.
We then augment thelabels of the phrasal node and its descendents withthe type of named entity.
We also distinguish be-tween the root node of an entity, and the descendentnodes.
See Figure 1 for an illustration.
This repre-sentation has several benefits, outlined below.2.1.1 Nested EntitiesThe OntoNotes data does not contain any nested en-tities.
Consider the named entity portions of therules seen in the training data.
These will look, forinstance, like none ?
none person, and organization?
organization organization.
Because we only al-low named entity derivations which we have seen inthe data, nested entities are impossible.
However,there is clear benefit in a representation allowingnested entities.
For example, it would be beneficialto recognize that the United States Supreme Court isa an organization, but that it also contains a nestedGPE.1 Fortunately, if we encounter data which hasbeen annotated with nested entities, this representa-tion will be able to handle them in a natural way.In the given example, we would have a derivationwhich includes organization ?
GPE organization.This information will be helpful for correctly la-beling nested entities such as New Jersey SupremeCourt, because the model will learn how nested en-tities tend to decompose.2.1.2 Feature Representation for NamedEntitiesCurrently, named entity recognizers are usually con-structed using sequence models, with linear chain1As far as we know, GENIA (Kim et al, 2003) is the onlycorpus currently annotated with nested entities.327conditional random fields (CRFs) being the mostcommon.
While it is possible for CRFs to have linksthat are longer distance than just between adjacentwords, most of the benefit is from local features,over the words and labels themselves, and from fea-tures over adjacent pairs of words and labels.
Ourjoint representation allows us to port both types offeatures from such a named entity recognizer.
Thelocal features can be computed at the same time thefeatures over parts of speech are computed.
Theseare the leaves of the tree, when only the named en-tity for the current word is known.2 The pairwisefeatures, over adjacent labels, are computed at thesame time as features over binary rules.
Binariza-tion of the tree is necessary for efficient computa-tion, so the trees consist solely of unary and bi-nary productions.
Because of this, for all pairs ofadjacent words within an entity, there will be a bi-nary rule applied where one word will be under theleft child and the other word will be under the rightchild.
Therefore, we compute features over adjacentwords/labels when computing the features for the bi-nary rule which joins them.2.2 Learning the Joint ModelWe construct our joint model as an extension to thediscriminatively trained, feature-rich, conditionalrandom field-based, CRF-CFG parser of (Finkel andManning, 2008).
Their parser is similar to a chart-based PCFG parser, except that instead of puttingprobabilities over rules, it puts clique potentials overlocal subtrees.
These unnormalized potentials knowwhat span (and split) the rule is over, and arbitraryfeatures can be defined over the local subtree, thespan/split and the words of the sentence.
The inside-outside algorithm is run over the clique potentials toproduce the partial derivatives and normalizing con-stant which are necessary for optimizing the log like-lihood.2.3 Grammar SmoothingBecause of the addition of named entity annota-tions to grammar rules, if we use the grammaras read off the treebank, we will encounter prob-lems with sparseness which severely degrade per-formance.
This degradation occurs because of CFG2Note that features can include information about otherwords, because the entire sentence is observed.
The featurescannot include information about the labels of those words.rules which only occur in the training data aug-mented with named entity information, and becauseof rules which only occur without the named entityinformation.
To combat this problem, we added ex-tra rules, unseen in the training data.2.3.1 Augmenting the GrammarFor every rule encountered in the training data whichhas been augmented with named entity information,we add extra copies of that rule to the grammar.
Weadd one copy with all of the named entity informa-tion stripped away, and another copy for each otherentity type, where the named entity augmentationhas been changed to the other entity type.These additions help, but they are not sufficient.Most entities correspond to noun phrases, so we tookall rules which had an NP as a child, and madecopies of that rule where the NP was augmentedwith each possible entity type.
These grammar ad-ditions sufficed to improve overall performance.2.3.2 Augmenting the LexiconThe lexicon is augmented in a similar manner tothe rules.
For every part of speech tag seen with anamed entity annotation, we also add that tag withno named entity information, and a version whichhas been augmented with each type of named entity.It would be computationally infeasible to allowany word to have any part of speech tag.
We there-fore limit the allowed part of speech tags for com-mon words based on the tags they have been ob-served with in the training data.
We also augmenteach word with a distributional similarity tag, whichwe discuss in greater depth in Section 3, and al-low tags seen with other words which belong to thesame distributional similarity cluster.
When decid-ing what tags are allowed for each word, we initiallyignore named entity information.
Once we deter-mine what base tags are allowed for a word, we alsoallow that tag, augmented with any type of namedentity, if the augmented tag is present in the lexicon.3 FeaturesWe defined features over both the parse rules and thenamed entities.
Most of our features are over one orthe other aspects of the structure, but not both.Both the named entity and parsing features utilizethe words of the sentence, as well as orthographicand distributional similarity information.
For eachword we computed a word shape which encoded328information about capitalization, length, and inclu-sion of numbers and other non-alphabetic charac-ters.
For the distributional similarity information,we had to first train a distributional similarity model.We trained the model described in (Clark, 2000),with code downloaded from his website, on severalhundred million words from the British national cor-pus, and the English Gigaword corpus.
The modelwe trained had 200 clusters, and we used it to assigneach word in the training and test data to one of theclusters.For the named entity features, we used a fairlystandard feature set, similar to those described in(Finkel et al, 2005).
For parse features, we used theexact same features as described in (Finkel and Man-ning, 2008).
When computing those features, we re-moved all of the named entity information from therules, so that these features were just over the parseinformation and not at all over the named entity in-formation.Lastly, we have the joint features.
We included asfeatures each augmented rule and each augmentedlabel.
This allowed the model to learn that certaintypes of phrasal nodes, such as NPs are more likelyto be named entities, and that certain entities weremore likely to occur in certain contexts and have par-ticular types of internal structure.4 DataFor our experiments we used the LDC2008T04OntoNotes Release 2.0 corpus (Hovy et al, 2006).The OntoNotes project leaders describe it as ?alarge, multilingual richly-annotated corpus con-structed at 90% internanotator agreement.?
The cor-pus has been annotated with multiple levels of anno-tation, including constituency trees, predicate struc-ture, word senses, coreference, and named entities.For this work, we focus on the parse trees and namedentities.
The corpus has English and Chinese por-tions, and we used only the English portion, whichitself has been split into seven sections: ABC, CNN,MNB, NBC, PRI, VOA, and WSJ.
These sectionsrepresent a mix of speech and newswire data.4.1 Data InconsistenciesWhile other work has utilized the OntoNotes corpus(Pradhan et al, 2007; Yu et al, 2008), this is thefirst work to our knowledge to simultaneously modelthe multiple levels of annotation available.
Becausethis is a new corpus, still under development, it isnot surprising that we found places where the datawas inconsistently annotated, namely with crossingbrackets between named entity and tree annotations.In the places where we found inconsistent anno-tation it was rarely the case that the different lev-els of annotation were inherently inconsistent, butrather inconsistency results from somewhat arbitrarychoices made by the annotators.
For example, whenthe last word in a sentence ends with a period, suchas Corp., one period functions both to mark the ab-breviation and the end of the sentence.
The conven-tion of the Penn Treebank is to separate the final pe-riod and treat it as the end of sentence marker, butwhen the final word is also part of an entity, thatfinal period was frequently included in the namedentity annotation, resulting in the sentence terminat-ing period being part of the entity, and the entity notcorresponding to a single phrase.
See Figure 2 for anillustration from the data.
In this case, we removedthe terminating period from the entity, to produce aconsistent annotation.Overall, we found that 656 entities, out of 55,665total, could not be aligned to a phrase, or multiplecontiguous children of a node.
We identified andcorrected the following sources of inconsistencies:Periods and abbreviations.
This is the problemdescribed above with the Corp. example.
Wecorrected it by removing the sentence terminat-ing final period from the entity annotation.Determiners and PPs.
Noun phrases composed ofa nested noun phrase and a prepositional phrasewere problematic when they also consisted of adeterminer followed by an entity.
We dealt withthis by flattening the nested NP, as illustrated inFigure 3.
As we discussed in Section 2.1, thistree will then be augmented with an additionalnode for the entity (see Figure 1).Adjectives and PPs.
This problem is similar to theprevious problem, with the difference beingthat there are also adjectives preceding the en-tity.
The solution is also similar to the solutionto the previous problem.
We moved the adjec-tives from the nested NP into the main NP.These three modifications to the data solved most,but not all, of the inconsistencies.
Another sourceof problems was conjunctions, such as North andSouth Korea, where North and South are a phrase,329SNPNNP[Mr.NNPTodt]PERVPVBDhadVPVBNbeenNPNPNNpresidentPPINofNPNNP[InsilcoNNPCorp..]ORGFigure 2: An example from the data of inconsistently labeled named entity and parse structure.
The inclusion of thefinal period in the named entity results in the named entity structure having crossing brackets with the parse structure.NPNPDTtheNNP[DistrictPPINofNPNNPColumbia] GPENPDTtheNPNNP[DistrictPPINofNPNNPColumbia] GPE(a) (b)Figure 3: (a) Another example from the data of inconsistently labeled named entity and parse structure.
In thisinstance, we flatten the nested NP, resulting in (b), so that the named entity corresponds to a contiguous set of childrenof the top-level NP.but South Korea is an entity.
The rest of the er-rors seemed to be due to annotation errors and otherrandom weirdnesses.
We ended up unable to make0.4% of the entities consistent with the parses, so weomitted those entities from the training and test data.One more change we made to the data was withrespect to possessive NPs.
When we encounterednoun phrases which ended with (POS ?s) or (POS ?
),we modified the internal structure of the NP.
Origi-nally, these NPs were flat, but we introduced a newnested NP which contained the entire contents of theoriginal NP except for the POS.
The original NP la-bel was then changed to PossNP.
This change is mo-tivated by the status of ?s as a phrasal affix or clitic:It is the NP preceding ?s that is structurally equiva-lent to other NPs, not the larger unit that includes ?s.This change has the additional benefit in this contextthat more named entities will correspond to a singlephrase in the parse tree, rather than a contiguous setof phrases.4.2 Named Entity TypesThe data has been annotated with eighteen types ofentities.
Many of these entity types do not occurvery often, and coupled with the relatively smallamount of data, make it difficult to learn accurateentity models.
Examples are work of art, product,and law.
Early experiments showed that it was dif-ficult for even our baseline named entity recognizer,based on a state-of-the-art CRF, to learn these typesof entities.3 As a result, we decided to merge allbut the three most dominant entity types into intoone general entity type called misc.
The result wasfour distinct entity types: person, organization, GPE(geo-political entity, such as a city or a country), andmisc.3The difficulties were compounded by somewhat inconsis-tent and occasionally questionable annotations.
For example,the word today was usually labeled as a date, but about 10% ofthe time it was not labeled as anything.
We also found severalstrange work of arts, including Stanley Cup and the U.S.S.
Cole.330Training TestingRange # Sent.
Range # Sent.ABC 0?55 1195 56?69 199CNN 0?375 5092 376?437 1521MNB 0?17 509 18?25 245NBC 0?29 552 30?39 149PRI 0?89 1707 90?112 394VOA 0?198 1512 199?264 383Table 1: Training and test set sizes for the six datasets insentences.
The file ranges refer to the numbers within thenames of the original OntoNotes files.5 ExperimentsWe ran our model on six of the OntoNotes datasetsdescribed in Section 4,4 using sentences of length40 and under (approximately 200,000 annotated En-glish words, considerably smaller than the PennTreebank (Marcus et al, 1993)).
For each dataset,we aimed for roughly a 75% train / 25% test split.See Table 1 for the the files used to train and test,along with the number of sentences in each.For comparison, we also trained the parser with-out the named entity information (and omitted theNamedEntity nodes), and a linear chain CRF usingjust the named entity information.
Both the base-line parser and CRF were trained using the exactsame features as the joint model, and all were op-timized using stochastic gradient descent.
The fullresults can be found in Table 2.
Parse trees werescored using evalB (the extra NamedEntity nodeswere ignored when computing evalB for the jointmodel), and named entities were scored using entityF-measure (as in the CoNLL 2003 conlleval).5While the main benefit of our joint model is theability to get a consistent output over both types ofannotations, we also found that modeling the parse4These datasets all consistently use the new conventions fortreebank annotation, while the seventh WSJ portion is currentlystill annotated in the original 1990s style, and so we left theWSJ portion aside.5Sometimes the parser would be unable to parse a sentence(less than 2% of sentences), due to restrictions in part of speechtags.
Because the underlying grammar (ignoring the additionalnamed entity information) was the same for both the joint andbaseline parsers, it is the case that whenever a sentence is un-parseable by either the baseline or joint parser it is in fact un-parsable by both of them, and would affect the parse scores ofboth models equally.
However, the CRF is able to named entitytag any sentence, so these unparsable sentences had an effecton the named entity score.
To combat this, we fell back onthe baseline CRF model to get named entity tags for unparsablesentences.and named entities jointly resulted in improved per-formance on both.
When looking at these numbers,it is important to keep in mind that the sizes of thetraining and test sets are significantly smaller thanthe Penn Treebank.
The largest of the six datasets,CNN, has about one seventh the amount of trainingdata as the Penn Treebank, and the smallest, MNB,has around 500 sentences from which to train.
Parseperformance was improved by the joint model forfive of the six datasets, by up to 1.36%.
Lookingat the parsing improvements on a per-label basis,the largest gains came from improved identicationof NML consituents, from an F-score of 45.9% to57.0% (on all the data combined, for a total of 420NML constituents).
This label was added in the newtreebank annotation conventions, so as to identify in-ternal left-branching structure inside previously flatNPs.
To our surprise, performance on NPs only in-creased by 1%, though over 12,949 constituents, forthe largest improvement in absolute terms.
The sec-ond largest gain was on PPs, where we improved by1.7% over 3,775 constituents.
We tested the signif-icance of our results (on all the data combined) us-ing Dan Bikel?s randomized parsing evaluation com-parator6 and found that both the precision and recallgains were significant at p ?
0.01.Much greater improvements in performance wereseen on named entity recognition, where most ofthe domains saw improvements in the range of 3?4%, with performance on the VOA data improvingby nearly 9%, which is a 45% reduction in error.There was no clear trend in terms of precision ver-sus recall, or the different entity types.
The firstplace to look for improvements is with the bound-aries for named entities.
Once again looking at all ofthe data combined, in the baseline model there were203 entities where part of the entity was found, butone or both boundaries were incorrectly identified.The joint model corrected 72 of those entities, whileincorrectly identifying the boundaries of 37 entitieswhich had previously been correctly identified.
Inthe baseline NER model, there were 243 entities forwhich the boundaries were correctly identified, butthe type of entity was incorrect.
The joint model cor-rected 80 of them, while changing the labels of 39entities which had previously been correctly identi-fied.
Additionally, 190 entities were found whichthe baseline model had missed entirely, and 68 enti-6Available at http://www.cis.upenn.edu/ dbikel/software.html331Parse Labeled Bracketing Named Entities TrainingPrecision Recall F1 Precision Recall F1 TimeABC Just Parse 70.18% 70.12% 70.15% ?
25mJust NER ?
76.84% 72.32% 74.51%Joint Model 69.76% 70.23% 69.99% 77.70% 72.32% 74.91% 45mCNN Just Parse 76.92% 77.14% 77.03% ?
16.5hJust NER ?
75.56% 76.00% 75.78%Joint Model 77.43% 77.99% 77.71% 78.73% 78.67% 78.70% 31.7hMNB Just Parse 63.97% 67.07% 65.49% ?
12mJust NER ?
72.30% 54.59% 62.21%Joint Model 63.82$ 67.46% 65.59% 71.35% 62.24% 66.49% 19mNBC Just Parse 59.72% 63.67% 61.63% ?
10mJust NER ?
67.53% 60.65% 63.90%Joint Model 60.69% 65.34% 62.93% 71.43% 64.81% 67.96% 17mPRI Just Parse 76.22% 76.49% 76.35% ?
2.4hJust NER ?
82.07% 84.86% 83.44%Joint Model 76.88% 77.95% 77.41% 86.13% 86.56% 86.34% 4.2hVOA Just Parse 76.56% 75.74% 76.15% ?
2.3hJust NER ?
82.79% 75.96% 79.23%Joint Model 77.58% 77.45% 77.51% 88.37% 87.98% 88.18% 4.4hTable 2: Full parse and NER results for the six datasets.
Parse trees were evaluated using evalB, and named entitieswere scored using macro-averaged F-measure (conlleval).ties were lost.
We tested the statistical significanceof the gains (of all the data combined) using thesame sentence-level, stratified shuffling technique asBikel?s parse comparator and found that both preci-sion and recall gains were significant at p < 10?4.An example from the data where the joint modelhelped improve both parse structure and named en-tity recognition is shown in Figure 4.
The outputfrom the individual models is shown in part (a), withthe output from the named entity recognizer shownin brackets on the words at leaves of the parse.
Theoutput from the joint model is shown in part (b),with the named entity information encoded withinthe parse.
In this example, the named entity Egyp-tian Islamic Jihad helped the parser to get its sur-rounding context correct, because it is improbableto attach a PP headed by with to an organization.At the same time, the surrounding context helpedthe joint model correctly identify Egyptian IslamicJihad as an organization and not a person.
Thebaseline parser also incorrectly added an extra levelof structure to the person name Osama Bin Laden,while the joint model found the correct structure.6 Related WorkA pioneering antecedent for our work is (Miller etal., 2000), who trained a Collins-style generativeparser (Collins, 1997) over a syntactic structure aug-mented with the template entity and template rela-tions annotations for the MUC-7 shared task.
Theirsentence augmentations were similar to ours, butthey did not make use of features due to the gen-erative nature of their model.
This approach was notfollowed up on in other work, presumably becausearound this time nearly all the activity in namedentity and relation extraction moved to the use ofdiscriminative sequence models, which allowed theflexible specification of feature templates that arevery useful for these tasks.
The present model isable to bring together both these lines of work, byintegrating the strengths of both approaches.There have been other attempts in NLP to jointlymodel multiple levels of structure, with varying de-grees of success.
Most work on joint parsing and se-mantic role labeling (SRL) has been disappointing,despite obvious connections between the two tasks.Sutton and McCallum (2005) attempted to jointlymodel PCFG parsing and SRL for the CoNLL 2005shared task, but were unable to improve perfor-mance on either task.
The CoNLL 2008 shared task(Surdeanu et al, 2008) was joint dependency pars-ing and SRL, but the top performing systems de-coupled the tasks, rather than building joint models.Zhang and Clark (2008) successfully built a joint332VPVBDwereNPNPNNSmembersPPINofNPNPthe [Egyptian Islamic Jihad]PERPPINwithNPNPNNStiesPPTOtoNPNMLNNP[OsamaNNPBinNNPLaden]PER(a)VPVBDwereNPNNSmembersPPINofNPDTtheNamedEntity-ORG*Egyptian Islamic JihadPPINwithNPNPNNStiesPPTOtoNP-PER*NNP-PEROsamaNNP-PERBinNNP-PERLaden(b)Figure 4: An example for which the joint model helped with both parse structure and named entity recognition.
Theindividual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extrainternal structure to Osama Bin Laden.
The joint model (b) gets both the structure and the named entity correct.model of Chinese word segmentation and parts ofspeech using a single perceptron.An alternative approach to joint modeling is totake a pipelined approach.
Previous work on linguis-tic annotation pipelines (Finkel et al, 2006; Holling-shead and Roark, 2007) has enforced consistencyfrom one stage to the next.
However, these modelsare only used at test time; training of the compo-nents is still independent.
These models also havethe potential to suffer from search errors and are notguaranteed to find the optimal output.7 ConclusionWe presented a discriminatively trained joint modelof parsing and named entity recognition, which im-proved performance on both tasks.
Our modelis based on a discriminative constituency parser,with the data, grammar, and features carefully con-structed for the joint task.
In the future, we wouldlike to add other levels of annotation available inthe OntoNotes corpus to our model, including wordsense disambiguation and semantic role labeling.AcknowledgementsThe first author is supported by a Stanford Gradu-ate Fellowship.
This paper is based on work fundedin part by the Defense Advanced Research ProjectsAgency through IBM.
The content does not neces-sarily reflect the views of the U.S. Government, andno official endorsement should be inferred.
We alsowish to thank the creators of OntoNotes, withoutwhich this project would not have been possible.333ReferencesAlexander Clark.
2000.
Inducing syntactic categories bycontext distribution clustering.
In Proc.
of Conferenceon Computational Natural Language Learning, pages91?94, Lisbon, Portugal.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In ACL 1997.Jenny Rose Finkel and Christopher D. Manning.
2008.Efficient, feature-based conditional random field pars-ing.
In ACL/HLT-2008.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In ACL 2005.Jenny Rose Finkel, Christopher D. Manning, and An-drew Y. Ng.
2006.
Solving the problem of cascadingerrors: Approximate bayesian inference for linguisticannotation pipelines.
In EMNLP 2006.Kristy Hollingshead and Brian Roark.
2007.
Pipelineiteration.
In ACL 2007.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In HLT-NAACL 2006.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichiTsujii.
2003.
Genia corpus ?
a semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(suppl.1):i180?i182.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A novel use of statistical parsing toextract information from text.
In In 6th Applied Natu-ral Language Processing Conference, pages 226?233.Andrew Ng and Michael Jordan.
2002.
On discrimina-tive vs. generative classifiers: A comparison of logisticregression and naive bayes.
In Advances in Neural In-formation Processing Systems (NIPS).Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Un-restricted coreference: Identifying entities and eventsin ontonotes.
International Conference on SemanticComputing, 0:446?453.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL), Manchester, UK.Charles Sutton and Andrew McCallum.
2005.
Joint pars-ing and semantic role labeling.
In Conference on Nat-ural Language Learning (CoNLL).V.
N. Vapnik.
1998.
Statistical Learning Theory.
JohnWiley & Sons.Liang-Chih Yu, Chung-Hsien Wu, and Eduard Hovy.2008.
OntoNotes: Corpus cleanup of mistaken agree-ment using word sense disambiguation.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics (Coling 2008), pages 1057?1064.Yue Zhang and Stephen Clark.
2008.
Joint word segmen-tation and POS tagging using a single perceptron.
InACL 2008.334
