Named Entity Generation using Sampling-based Structured PredictionGuillaume BouchardXerox Research Centre Europe6 Chemin de Maupertuis38240 Meylan, Franceguillaume.bouchard@xerox.comAbstractThe problem of Named Entity Generationis expressed as a conditional probabilitymodel over a structured domain.
By defin-ing a factor-graph model over the men-tions of a text, we obtain a compact pa-rameterization of what is learned using theSampleRank algorithm.1 IntroductionThis document describes the participa-tion of the Xerox Research Centre Eu-rope team in the GREC-NEG?10 challenge(http://www.nltg.brighton.ac.uk/research/genchal10/grec/)2 ModelConditional random fields are conditional prob-ability models that define a distribution overa complex output space.
In the context ofthe Named-Entity Generation challenge, theoutput space is the set of possible referringexpressions for all the possible mentions of thetext.
For example, assuming that we have thefollowing text with holes (numbers are entity IDs):#1 was a Scottish mathematician,son of #2.
#1 is most rememberedas the inventor of logarithmsand Napier?s bones.Then the possibilities associated with the entity #1are:1.
John Napier of Merchistoun,2.
Napier,3.
he,4.
who,and the possibilities associated with the entity #2are:1.
Sir Archibald Napier of Merchiston,2.
he,3.
who.Then, the output space is Y = {1, 2, 3, 4} ?
{1, 2, 3} ?
{1, 2, 3, 4}, representing all the possi-ble combination of choices for the mentions.
Thesolution y = (1, 1, 3) corresponds to inserting thetexts ?John Napier of Merchiston?, ?Sir ArchibaldNapier of Merchiston?
and ?he?
in the holes of thetext in the same order.
This is the combinationthat is the closest to the original text, but a humancould also consider that solution y = (1, 1, 2) asbeing equally valid.Denoting x the input, i.e.
the text with the typedholes, the objective of the task is to find the combi-nation y ?
Y that is as close as possible to naturaltexts.We model the distribution of y given x by a fac-tor graph: p(y|x) ?
?c?C ?c(x, y), where C isthe set of factors defined over the input and outputvariables.
In this work, we considered 3 types ofexponential potentials:?
Unary potentials defined on each individualoutput yi.
They include more than 100 fea-tures corresponding to the position of themention in the sentence, the previous andnext part of speech (POS), the syntactic cat-egory and funciton of the mention, the typeand case of the corresponding referring ex-pression, etc.?
Binary potentials over contiguous mentionsinclude the distance between them, and thejoint distribution of the types and cases.?
Binary potentials that are activated only be-tween mentions and the previous time thesame entity was referred to by a name.
Thepurpose of this is to reduce the use of pro-nouns referring to a person when the men-tions are distant to each other.To learn the parameter of the factor graph, we usedthe SampleRank algorithm (Wick et al, 2009)which casts the prediction problem as a stochas-tic search algorithms.
During learning, an optimalranking function is estimated.3 ResultsUsing the evaluation software supplied by theGREC-NEG organizers, we obtained the folloingperformances:total slots : 907reg08 type matches : 693reg08 type accuracy : 0.764057331863286reg08 type matchesincluding embedded : 723reg08 type precision : 0.770788912579957reg08 type recall : 0.770788912579957total peer REFs : 938total reference REFs : 938string matches : 637string accuracy : 0.702315325248071mean edit distance : 0.724366041896362mean normalisededit distance : 0.279965348873838BLEU 1 score : 0.7206BLEU 2 score : 0.7685BLEU 3 score : 0.7702BLEU 4 score : 0.754NIST score : 5.1208ReferencesMichael Wick, Khashayar Rohanimanesh, Aron Cu-lotta, and Andrew McCallum.
2009.
SampleRank:Learning preferences from atomic gradients.
NeuralInformation Processing Systems (NIPS) Workshopon Advances in Ranking.
