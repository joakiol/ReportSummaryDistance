Towards Uniform Processing of Constraint-based CategorialGrammarsGert jan  van NoordLehrstuh l  f/it Computer l inguis t ikUniversit~t des Saar landesIm Stadtwald 15D-6600 Saarbrficken 11, FRGvannoord@col i .uni-sb.deAbst ractA class of constraint-based categorial grammars iproposed in which the construction ofboth logicalforms and strings is specified completely lexically.Such grammars allow the construction of a uni-form algorithm for both parsing and generation.Termination of the algorithm can be guaranteedif lexical entries adhere to a constraint, hat canbe seen as a computationally motivated versionof GB's projection principle.1 Mot ivat ionsIn constraint-based approaches to grammar thesemantic interpretation of phrases is often de-fined in the lexical entries.
These lexical en-tries specify their semantic interpretation, takinginto account he semantics of the arguments heysubcategorize for (specified in their subcat list).The grammar ules simply percolate the seman-tics upwards; by the selection of the arguments,this semantic formula then gets further instanti-ated (Moore, 1989).
Hence in such approaches itcan be said that all semantic formulas are 'pro-jected from the lexicon' (Zeevat et al, 1987).Such an organization ofa grammar is the startingpoint of a class of generation algorithms that havebecome popular recently (Calder et al, 1989;Shieber et al, 1989; Shieber el al., 1990).
Thesesemantic-head-driven algorithms are both gearedtowards the input semantic representation a dthe information contained in lexical entries.
Ifthe above sketched approach to semantic inter-pretation is followed systematically, it is possibleto show that such a semantic-head-driven g -eration algorithm terminates (Dymetman et al,1990).In van Noord (1991) I define a head-drivenparser (based on Kay (1989)) for a class ofconstraint-based grammars in which the con-struction of strings may use more complex op-erations that simple context-free concatenation.Again, this algorithm is geared towards the in-put (string) and the information found in lexi-cal entries.
In this paper I investigate an ap-proach where the construction of strings is de-fined lexically.
Grammar ules simply percolatestrings upwards.
Such an approach seems feasibleif we allow for powerful constraints o be defined.The head-corner parser knows about strings andperforms operations on them; in the types ofgrammars defined here these operations are re-placed by general constraint-solving techniques(HShfeld and Smolka, 1988; Tuda et al, 1989;Damas et al, 1991).
Therefore, it becomes pos-sible to view both the head-driven generator andthe head-driven parser as one and the same algo-rithm.For this uniform algorithm to terminate, wegeneralize the constraint proposed by Dymetmanet ai.
(1990) to both semantic interpretationsand strings.
That is, for each lexical entry werequire that its string and its semantics i largerthan the string and the semantics associated witheach of its arguments.
The following picture thenemerges.
The depth of a derivation tree is de-termined by the subcat list of the ultimate headof the tree.
Furthermore, the string and the se-mantic representation f each of the non heads inthe derivation tree is determined by the subcatlist as well.
A specific condition on the relationbetween elements in the subcat list and their se-12Lmantics and string representation e sures termi-nation.
This condition on lexical entries can beseen as a lexicalized !and computationally moti-vated version of GB's projection principle.Word-order  domains .
The string associatedwith a linguistic object (sign) is defined in termsof its word-order domain (Reape, 1989; Reape,1990a).
I take a word=order domain as a sequenceof signs.
Each of the?e signs is associated with aword-order domain recursively, or with a sequenceof words.
A word-order domain is thus a tree.Linear precedence rules are defined that constrainpossible orderings of  signs in such a word-orderdomain.
Surface strings are a direct function ofword-order domains.'
In the lexicon, the word-order domain of a lexical entry is defined by shar-ing parts of this domain with the arguments itsubcategorizes for.
Word-order domains are per-colated upward.
Hence word-order domains areconstructed in a derivation by gradual instantia-tions (hence strings are constructued in a deriva-tion by gradual instantiation as well).
Note thatthis implies that an unsaturated sign is not asso-ciated with one string, but merely with a set ofpossible strings (this is similar to the semantic in-terpretation of unsaturated signs (Moore, 1989)).In lexical entries, word order domains are definedusing Reape's sequence union operation (R.eape,1990a).
Hence the grammars are not only basedon context-free string concatenation.2 Constra lnt -based versionsof categorial  grammarThe formalism I assume consists of definite clau-ses over constraint languages in the manner ofHShfeld and Smolka (1988).
The constraint lan-guage at least consists of the path equationsknown from PATR II (Shieber, 1989), augmentedwith variables.
I write such a definite clause as:P : -q l  .
.
.qn,?.where p, qi are atoms and ?
is a (conjunction of)constraint(s).
The path equations are written asin PATR II, but each I path starts with a variable:(Xi i l .
.
.
l , )  =" cor(x, i , .
.
.
t . )
= (xj t l .
.
.
i?
)where Xt  are variables, c is a constant, I, l' areattributes.
I also use some more powerful con-straints that are written as atoms.This formalism is used to define what possible'signs' are, by the definition of the unary predi-cate s:i.gn/1.
There is only one nonunit clause forthis predicate.
The idea is that unit clauses fors ign/1 are lexical entries, and the one nonunitclause defines the (binary) application rule.
I as-sume that lexical entries are specified for theirarguments in their 'subcat list' (sc).
In the ap-plication rule a head selects the first ( f)  elementfrom its subcat list, and the tail (r) of the subcatlist is the subcat list of the mother; the semantics(sere) and strings (phon) are shared between thehead and the mother.sign(Xo) :- sign(X1), sign(X2),(Xo synsem sern) --" (X1 synsem sere),(Xo phon) ~ (X1 phon),(Xo synsem sc) ?
iX,  synsem sc r),(21 synsem sc f )  =" (X2).I write such rules using matrix notation as fol-lows; s t r ing(X)  represents the value Y, wheres t r ing(X ,?)
.synsem :Xo :phon : \[~\]synsem :X1 :phon : \[~\]X2 : \ [~The grammar also consists of a number of lex-ical entries.
Each of these lexical entries is speci-fied for its subcat list, and for each subcat elementthe semantics and word-order domain is specified,such that they satisfy a termination condition tobe defined in the following section.
For exam-ple, this condition is satisfied if the semantics ofeach element in the subcat list is a proper sub-part of the semantics of the entry, and each ele-ment of the subcat list is a proper subpart of theword-order domain of the entry.
The phonologyof a sign is defined with respect o the word-orderdomain with the predicate 'string'.
This predi-cate simply defines a left-to-right depth-first ra-versel of a word-order domain and picks up allthe strings at the terminals.
It should be notedthat the way strings are computed from the word-order domains implies that the string of a node13I syn : vp\[synsem : ~ 8c: @ synsem :sere : schla f en(~dom:~(~\] ,  dora: 0 )phon : (schlii ft)phon : str ing(D8eWR :Figure 1: The German verb 'schlKft'not necessarily is the concatenation f the stringsof its daughter nodes.
In fact, the relation be-tween the strings of nodes is defined indirectlyvia the word-order domains.The word-order domains are sequences of signs.One of these signs is the sign corresponding to thelexical entry itself.
However, the domain of thissign is empty, but other values can be shared.Hence the entry for an intransitive German verbsuch as 'schl~ft' (sleeps) is defined as in figure 1.I introduce some syntactic sugaring to makesuch entries readable.
Firstly, XPi will stand fors nsem: sem:\[Furthermore, in lexical entries the s~sem part isshared with the synsem part of an element of theword order domain, that is furthermore specifiedfor the empty domain and some string.
I willwrite: << string >> in a lexical entry to stand forthe sign whose synsem value is shared with thesynsem of the lexical entry itself; its dora valueis 0 and its phon value is string.
The foregoingentry is abreviated as:synsern : sere : sehla/en(\[T\]): ( \ [ \ ]N  P, )dora: \[\](\[\], << s hla/t >>)phon : str ing(DNote that in this entry we merely stipulate thatthe verb preceded by the subject constitutes theword-order domain of the entire phrase.
How-ever, we may also use more complex constraintsto define word-order constraints.
In particular,as already stated above, LP constraints are de-fined which holds for word-order domains.
I usethe sequence-union predicate (abbreviated su)defined by Reape as a possible constraint as well.This predicate is motivated by clause union andscrambling phenomena in German.
A linguisti-cally motivated example of the use of this con-straint can be found in section 4.
The predicatesu(A, B, C) is true in case the elements of the listC is the multi set union of the elements of the listsA and B; moreover, a < b in either A or B iff a <b in C. I also use the notation X U 0 Y to denotethe value Seq, where su(X,Y,$eq).
For exam-ple, su(\[a, d, e\], \[b, c, f\], \[a, b, c, d, e, f\]); \[a, e\] o 0 \[b\]stands for \[a, c, b\],\[a, b, c\] or \[b, a, c\].
In fact, I as-sume that this predicate is also used in the simplecases, in order to be able to spel out generaliza-tions in the linear precedence constraints.
Hencethe entry for 'schlafen' is defined as follows, whereI write lp(X) to indicate that the lp constraintsshould be satisfied for X. I have nothing to sayabout the definition of these constraints.synsem : sere: schla/en(E\])sc : (\[~\]N Pi)D ?m: (D  u0 (<< schla/t >>)phon: string(tp(\[ \[))In the following I (implicitly) assume that for eachlexical entry the following holds:dora: \ [ \ ]phon : string(lp(D) \]3 Un i fo rm Process ingIn van Noord (1991) I define a parsing strat-egy, called 'head-corner parsing' for a class of14Iigrammars allowing more complex constraints onstrings than context-free concatenation.
Reapedefines generalizations of the shift-reduce parserand the CYK parser (Reape, 1990b), for thesame class of grammars.
For generation head-driven generators can be used (van Noord, 1989;Calder et al, 1989; Shieber et al, 1990).
Alter-natively I propose a generalization of these head-driven parsing- and generation algorithms.
Thegeneralized algorithm can be used both for pars-ing and generation.
Hence we obtain a uniformalgorithm for both processes.
Shieber (1988) ar-gues for a uniform architecture for parsing ingeneration.
In his proposal, both processes are(different) instantiations of a parameterized algo-rithm.
The algoritthm I define is not parameter-ized in this sense, but really uses the same code inboth directions.
Some of the specific properties ofthe head-driven generator on the one hand, andthe head-driven parser on the other hand, followfrom general constraint-solving techniques.
Wethus obtain a uniform algorithm that is suitablefor linguistic processing.
This result should becompared with other uniform scheme's uch asSLD-resolution or some implementations of typeinference (Zajae, 1991, this volume) which clearlyare also uniform but facessevere problems in thecase of lexicalist grammars, as such scheme's donot take into account he specific nature of lexi-calist grammars (Shieber et al, 1990).A lgor i thm.
The algorithm is written in thesame formalism as the grammar and thus con-stitutes a meta-interpreter.
The definite clausesof the object-grammar e represented asiexical_entry(X) :- qLfor the unit clausessign(X) :- q~.andrule(H, M, A) :- ~.for the rulesign(M) :- sign(H), sign(A), oh.The associated interpreter is a Prolog like top-down backtrack interpreter where term unifi-cation is replaced by more general constraint-solving techniques~, (HShfeld and Smolka, 1988;Tuda et aL, 1989; Damas et al, 1991).
Themeta-interpreter defines a head-driven bottom-upstrategy with top-down prediction (figure 2), andis a generalization of the head-driven generator(van Noord, 1989; Calder et al, 1989; van Noord,1990a) and the head-corner parser (Kay, 1989;van Noord, 1991).prove(T) :-lexical_entry( L ), connect(L, T),(T phon) ~ (L phon),(T synsem sere) =" (L synsem sem).connect(T, T).connect(S, T) :-rule(S, M, A), prove(A),connect ( M, T).Figure 2: The uniform algorithmIn the formalism defined in the preceding sec-tion there are two possible ways where non-termination may come in, in the constraints orin the definite relations over these constraints.
Inthis paper I am only concerned with the secondtype of non-termination, that is, I simply assumethat the constraint language is decidable (HShfeldand Smolka, 1988).
1 For the grammar sketchedin the foregoing section we can define a very nat-ural condition on lexical entries that guaranteesus termination of both parsing and generation,provided the constraint language we use is decid-able.The basic idea is that for a given semantic rep-resentation or (string constraining a) word-orderdomain, the derivation tree that derives these rep-resentations has a finite depth.
Lexical entriesare specified for (at least) ae, phon and nero.
Theconstraint merely states that the values of theseattributes are dependent.
It is not possible forone value to 'grow' unless the values of the otherattributes grow as well.
Therefore the constraintwe propose can be compared with GB's projec-tion principle if we regard each of the attributesto define a 'level of description'.
Termination canthen be guaranteed because derivation trees arerestricted in depth by the value of the se at-tribute.In order to define a condition to guarantee ter-mination we need to be specific about the inter-1This is the  case  i f  we  only have PATH equations; butprobably not if we use t.J(), string/2smd lp/2 unlimited.15pretation of a lexical entry.
Following Shieber(1989) I assume that the interpretation of a setof path equations i  defined in terms of directedgraphs; the interpretation of a lexical entry is aset of such graphs.
The 'size' of a graph simply isdefined as the number of nodes the graph consistsof.
We require that for each graph in the interpre-tation of a lexical entry, the size of the subgraphat sere is strictly larger than each of the sizes ofthe sere part of the (subgraphs corresponding tothe) elements of the subcat list.
I require thatfor each graph in the interpretation of a lexicMentry, the size of phon is strictly larger than eachof the sizes of (subgraphs corresponding to) thephon parts of the elements of the subcat lists.Summarizing, all lexical entries hould satisfy thefollowing condition:Terminat ion condit ion.
For each interpreta-tion L of a lexical entry, if E is an element of L'ssubcat list (i.e.
(L synsem sc r* f)  ~ E), then:size\[(E phon)\] < size\[(L phon)\]size\[(E synsem sere)\] < size\[(L synsem sere)\]The most straightforward way to satisfy this con-dition is for an element of a subcat list to shareits semantics with a proper part of the semanticsof the lexical entry, and to include the elementsof the subcat list in its word-order domain.Possible inputs.
In order to prove terminationof the algorithm we need to make some assump-tions about possible inputs.
For a discussion cf.van Noord (1990b) and also Thompson (1991,this volume).
The input to parsing and gener-ation is specified as the goal?-- sign(Xo), ?.where ?
restricts the variable X0.
We re-quire that for each interpretation of X0 thereis a maximum for parsing of size\[{Xo phonl\] ,and that there is a maximum for generation ofsize\[(Xo synsem sem)\].If the input has a maximum size for either se-mantics or phonology, then the uniform algorithmterminates (assuming the constraint language isdecidable), because ach recursive call to 'prove'will necessarily be a 'smaller' problem, and asthe order on semantics and word-order domainsis well-founded, there is a 'smallest' problem.
Asa lexical entry specifies the length of its subcatlist, there is only a finite number of embeddingsof the 'connect' clause possible.4 Some examplesVerb raising.
First I show how Reape's anal-ysis of Dutch and German verb raising construc-tions can be incorporated in the current grammar(Reape, 1989; Reape, 1990a).
For a linguistic dis-cussion of verb-raising constructions the reader isreferred to Reape's papers.
A verb raiser such asthe German verb 'versprechen' (promise) selectsthree arguments, a vp, an object np and a subjectnp.
The word-order domain of the vp is unionedinto the word order domain of versprechen.
Thisis necessary because in German the arguments ofthe embedded vp can in fact occur left from theother arguments of versprechen, as in:esi ihmj jemandk zu leseni versprochenj hatk(it him someone to read promised hadi.e.
someome had promised him to read it.Hence, the lexical entry for the raising verb 'ver-sprechen' isdefined as in figure 3.
The word-orderdomain of 'versprechen' simply is the sequenceunion of the word-order domain of its vp object,with the np object, the subject, and ver~prechenitself.
This allows any of the permuations (al-lowed by the LP constraints) of the np object,versprechen, the subject, and the elements ofthe domain of the vp object (which may containsigns that have been unioned in recursively).Seperable prefixes.
The current frameworkoffers an interesting account of seperable prefixverbs in German and Dutch.
For an overview ofalternative accounts of such verbs, see Uszkoreit(1987)\[chapter 4\].
At first sight, such verbs mayseem problematic for the current approach be-cause their prefixes seem not to have any seman-tic content.
However, in my analysis a seperableprefix is lexically specified as part of the word-order domain of the verb.
Hence a particle is notidentified as an element of the subcat list.
Fig-ure 4 might be the encoding of the German verb'anrufen' (call up).
Note that this analysis con-forms to the condition of the foregoing section,because the particle is not on the subcat list.
Theadvantages of this analysis can be summarized asfollows.Firstly, there is no need for a feature systemto link verbs with the correct prefixes, as eg.in Uszkoreit's proposal.
Instead, the correspon-dence is directly stated in the lexical entry of theparticle verb which seems to me a very desirable:1.6:synsem : sc : (I sc : (NP4)dora : E \ ]do : (<< >>)uO E\]uo (DUo 0Figure 3: The German verb 'versprechen'result.
5 HPSG MarkersSecondly, the analysis predicts that particlescan 'move away' from the verb in case the verbis sequence-unioned into a larger word-order do-main.
This prediction is correct.
The clearestexamples are possibly from Dutch.
In Dutch, theparticle of a verb can be placed (nearly) anywherein the verb cluster, as long as it precedes its ma-trix verb:*dat jan marie piet heefft willen zien bellen opdat jan marie piet heeft willen zien op bellendat jan marie pier heeft willen op zien bellendat jan marie piet heeft op willen zien bellendat jan marie piet op heeft willen zien bellenthat john mary pete up has want see call(i.e.
john wanted to see mary call up pete)The fact that the particle is not allowed to followits head word is easily explained by the (indepen-dently motivated) LP constraint that argumentsof a verb precede the verb.
Hence these curiousfacts follow immediately in our analysis (the anal-ysis makes the same prediction for German, butbecause of the different order of German verbs,this prediction can not be tested).Thirdly, Uszkoreit argues that a theory ofseperable prefixes should also account for the'systematic orthog!aphic nsecurity felt by nativespeakers' i.e.
whether or not they should writethe prefix and the verb as one word.
The currentapproach can be seen as one such explanation: inthe lexical entry for a seperable prefix verb theverb and prefix are already there, on the otherhand each of the words is in a different part ofthe word-order domain.In newer versions of HPSG (Pollard and Sag,1991) a special 'marker' category is assumed forwhich our projection principle does not seem towork.
For example, complementizers are ana-lyzed as markers.
They are not taken to be thehead of a phrase, but merely 'mark' a sentencefor some features.
On the other hand, a spe-cial principle is assumed such that markers do infact select for certain type of constituents.
In thepresent framework a simple approach would be toanalyze such markers as functors, i.e.
heads, thathave one element in their subcat list:synseTTt  : $eTn  .
.se : (L~\] VP_F IN1)dora : (<< dass >>, \[~\])However, the termination condition defined inthe third section can not always be satisfied be-cause these markers usually do not have muchsemantic ontent (as in the preceding example).Furthermore these markers may also be phoneti-cally empty, for example in the HPSG-2 analysisof infinite vp's that occur independently such anempty marker is assumed.
Such an entry wouldlook presumably as follows, where it is assumedthat the empty marker constitutes no element ofits own domain:8ynseTn : $eTn  .
.sc : (L~J V P_I N F1)dom: QIt seems, then, that analyses that rely on suchmarker categories can not be defined in the cur-rent framework.
On the other hand, however,such markers have a very restricted istribution,and are never recursive.
Therefore, a slight mod-17syn : vp \]synsem: sere :anrufen(\[T\],c:  \]gPl)r>>) uo u(i ( \[ dora: (<< ruf f  \[synsem : syn : part 1dora: 0 \])phon : (an)Figure 4: Tile verb 'anrufen'ification of the termination condition can be de-fined that take into account such marker cate-gories.
To make this feasible we need a constraintthat markers can not apply arbitrarily.
In HPSG-2 the distribution of the English complementizer'that' is limited by the introduction of a specialbinary feature whose single purpose is to disallowsentences such as 'john said that that that maryloves pete'.
It is possible to generalize this to dis-allow any marker to be repeatedly applied in somedomain.
The 'seed' of a lexical entry is this entryitself; the seed of a rule is the seed of the head ofthis rule unless this head is a marker in which casethe seed is defined as the seed of the argument.In a derivation tree, no marker may be appliedmore than once to the same seed.
This 'don'tstutter' principle then subsumes the feature ma-chinery introduced in HPSG-2, and parsing andgeneration terminates for the resulting system.Given such a system for marker categories,we need to adapt our algorithm.
I assume lex-ical entries are divided (eg.
using some user-defined predicate) into markers and not mark-ers; markers are defined with the predicatemarker(Sign,Name) where Name is a uniqueidentifier.
Other lexical entries are encoded asbefore, marktypes(L) is the list of all markeridentifiers.
The idea simply is that markers areapplied top-down, keeping track of the markersthat have already been used.
The revised algo-rithm is given in figure 5.AcknowledgementsI am supported by SFB 314, Project N3 BiLD.prove(T):-marktypes( M), prove(T, M).prove(T, M) : -marker(L, Name), del(Name, M, M2),rule(L, T, A), prove(A, M2).prove(T, M) : -lezical_entry( L ), connect( L, T),(T phon) =" (L phon),(T synsem sere) ~ (L synsem sere).connect(T, ).connect(S, T) :-rule(S, M, A), prove(A),connect(M, T).Figure 5: The algorithm including markersReferencesJonathan Calder, Mike Reape, and Henk Zeevat.An algorithm for generation i unification cat-egorial grammar.
In Fourth Conference of theEuropean Chapter of the Association for Com-putational Linguistics, pages 233-240, Manch-ester, 1989.Luis Damas, Nelma Moreira, and Giovanni B.Varile.
The formal and processing models ofCLG.
In Fifth Conference of the EuropeanChapter of the Association for ComputationalLinguistics, Berlin, 1991.Marc Dymetman, Pierre Isabelle, and FrancoisPerrault.
A symmetrical pproach to parsingand generation.
In Proceedings of the 13th In-18iternational Conference on Computational Lin-guistics (COLING), Helsinki, 1990.Markus HShfeld and Gert Smolka.
Definite rela-tions over constraint languages.
Technical re-port, 1988.
LILOG Report 53; to appear inJournal of Logic Programming.Martin Kay.
Head driven parsing.
In Proceedingsof Workshop on Parsing Technologies, Pitts-burgh, 1989.Robert C. Moore.
Unification-based semanticinterpretation.
In 27th Annual Meeting ofthe Associationi for Computational Linguistics,Vancouver, 1989.Carl Pollard and ilvan Sag.
Information BasedSyntax and Semantics, Volume 2.
Center forthe Study of Language and Information Stan-ford, 1991. to appear.Mike Reape.
A ilogical treatment of semi-freeword order and bounded discontinuous con-stituency.
In Fourth Conference of the Euro-pean Chapter o\[ the Association for Computa-tional Linguistics, UMIST Manchester, 1989.Mike Reape.
Getting things in order.
In Proceed-ings of the Symposium on Discontinuous Con-stituency, ITK Tilburg, 1990.Mike Reape.
Parsing bounded discontinous con-stituents: Generalisations of the shift-reduceand CKY algorithms, 1990.
Paper presentedat the first CLIN meeting, October 26, OTSUtrecht.Stuart M. Shieber, Gertjan van Noord, Robert C.Moore, and Fernando C.N.
Pereira.
Asemantic-head-driven generation algorithm forunification based formalisms.
In 27th AnnualMeeting of the Association for ComputationalLinguistics, Vancouver, 1989.bStuart M. Shieber, Gertjan van Noord, Robert C.Moore, and Fernando C.N.
Pereira.
Semantic-head-driven gefieration.
Computational Lin-guistics, 16(1), 1990.Stuart M. Shieber.
A uniform architecture for Iparsing and generation.
In Proceedings of the12th International Conference on Computa-tional Linguistics (COLING), Budapest, 1988.Stuart M. Shieber.
Parsing and Type Inferencefor Natural and Computer Languages.
PhDthesis, Menlo Park, 1989.
Technical note 460.Henry S. Thompson.
Generation and transla-tion - towards a formalism-independent char-acterization.
In Proceedings of ACL workshopReversible Grammar in Natural Language Pro-cessing, Berkeley, 1991.Hirosi Tuda, K6iti Hasida, and Hidetosi Sirai.JPSG parser on constraint logic programming.In Fourth Conference of the European Chapterof the Association for Computational Linguis-tics, Manchester, 1989.Hans Uszkoreit.
Word Order and ConstituentStructure in German.
CSLI Stanford, 1987.Gertjan van Noord.
BUG: A directed bottom-up generator for unification based formalisms.Working Papers in Natural Language Process-ing, Katholieke Universiteit Leuven, StichtingTaaitechnologie Utrecht, 4, 1989.Gertjan van Noord.
An overview of head-driven bottom-up generation.
In Robert Dale,Chris Mellish, and Michael Zock, editors, Cur-rent Research in Natural Language Generation.Academic Press, 1990.Gertjan van Noord.
Reversible unification-basedmachine translation.
In Proceedings of the13th International Conference on Computa-tional Linguistics (COLING), Helsinki, 1990.Gertjan van Noord.
Head corner parsing for dis-continuous constituency.
In 29th Annual Meet-ing of the Association for Computational Lin-guistics, Berkeley, 1991.R~!mi Zajac.
A uniform architecture for parsing,generation and transfer.
In Proceedings of ACLworkshop Reversible Grammar in Natural Lan-guage Processing, Berkeley, 1991.Ilenk Zeevat, Ewan Klein, and Jo Calder.
Unifi-cation categorial grammar.
In Nicholas Had-dock, Ewan Klein, and Glyn Morrill, edi-tors, Categorial Grammar, Unification Gram-mar and Parsing.
Centre for Cognitive Science,University of Edinburgh, 1987.
Volume 1 ofWorking Papers in Cognitive Science.3.9
