Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 19?23,Prague, June 2007. c?2007 Association for Computational LinguisticsSemEval-2007 Task 5: Multilingual Chinese-English Lexical SamplePeng Jin, Yunfang Wu and Shiwen YuInstitute of Computational LinguisticsPeking University, Beijing China{jandp, wuyf, yusw}@pku.edu.cnAbstractThe Multilingual Chinese-English lexicalsample task at SemEval-2007 provides aframework to evaluate Chinese word sensedisambiguation and to promote research.This paper reports on the task preparationand the results of six participants.1 IntroductionThe Multilingual Chinese-English lexical sampletask is designed following the leading ideas of theSenseval-3 Multilingual English-Hindi lexicalsample task (Chklovski et al, 2004).
The ?sensetags?
for the ambiguous Chinese target words aregiven in the form of their English translations.The data preparation is introduced in the secondsection.
And then the participating systems arebriefly described and their scores are listed.In the conclusions we bring forward some sug-gestion for the next campaign.2 Chinese Word Sense Annotated CorpusAll the training and test data come from thePeople?s Daily in January, February and March of2000.
The People?s Daily is the most popularnewspaper in China and is open domain.
Beforemanually sense annotating, the texts have beenword-segmented and part of speech (PoS) taggedaccording to the PoS tagging scheme of Institute ofComputational Linguistics in Peking University(ICL/PKU).
The corpus had been used as one ofthe gold-standard data set for the secondinternational Chinese word segmentation bakeoffin 2005.12.1 Manual AnnotationThe sense annotated corpus is manually con-structed with the help of a word sense annotatinginterface developed in Java.
Three native annota-tors, two major in Chinese linguistics and one ma-jor in computer science took part in the construc-tion of the sense-annotated corpus.
A text generallyis first annotated by one annotator and then veri-fied by two checkers.
Checking is of course a nec-essary procedure to keep the consistency.
Inspiredby the observation that checking all the instancesof a word in a specific time frame will greatly im-prove the precision and accelerate the speed, asoftware tool is designed in Java to gather all theoccurrences of a word in the corpus into a check-ing file with the sense KWIC (Key Word in Con-text) format in sense tags order.
The inter-annotator agreement gets to 84.8% according toWu.
et al (2006).The sense entries are specified in the ChineseSemantic Dictionary (CSD) developed byICL/PKU.
The sense distinctions are made mainlyaccording to the Contemporary Chinese Dictionary,the most widely used dictionary in mandarin Chi-nese, with necessary adjustment and improvementis implemented according to words usage in realtexts.
Word senses are described using the feature-based formalism.
The features, which appear inthe form ?Attribute =Value?, can incorporate ex-tensive distributional information about a wordsense.
The feature set constitutes the representationof a sense, while the verbal definitions of meaning1 http://sighan.cs.uchicago.edu/bakeoff2005/19serve only as references for human use.
The Eng-lish translation is assigned to each sense in the at-tribute ?English translation?
in CSD.Based on the sense-annotated corpus, a sense isreplaced by its English translation, which mightgroup different senses together under the sameEnglish word.2.2 Instances selectionIn this task together 40 Chinese ambiguous words:19 nouns and 21 verbs are selected for the evalua-tion.
Each sense of one word is provided at least 15instances and at most 40 instances, in whicharound 2/3 is used as the training data and 1/3 asthe test data.
Table 1 presents the number of wordsunder each part of speech, the average number ofsenses for each PoS and the number of instancesrespectively in the training and test set.# Averagesenses# traininginstances# testinstances19nouns2.58 1019 36421verbs3.57 1667 571Table 1: Summary of the sense inventory andnumber of training data and test setIn order to escape from the sense-skewed distri-bution that really exists in the corpus of People?sDaily, many instances of some senses have beenremoved from the sense annotated corpus.
So thesense distribution of the ambiguous words in thistask does not reflect the usages in real texts.3 Participating SystemsIn order to facilitate participators to select the fea-tures, we gave a specification for the PoS-tag set.Both word-segmented and un-segmented contextare provided.Two kinds of precisions are evaluated.
One ismicro-average:?
?===NiiNiimir nmP11/N  is the number of all target word-types.
isthe number of labeled correctly to one specific tar-get word-type and  is the number of all test in-stances for this word-type.iminThe other is macro-average:?==Niimar NpP1/ ,  iii nmp /=All teams attempted all test instances.
So the re-call is the same with the precision.
The precisionbaseline is obtained by the most frequent sense.Because the corpus is not reflected the real usage,the precision is very low.Six teams participated in this word sense disam-biguation task.
Four of them used supervised learn-ing algorithms and two used un-supervised method.For each team two kinds of precision are given asin table 2.Team Micro-average Macro-averageSRCB-WSD 0.716578 0.749236I2R 0.712299 0.746824CITYU-HIF 0.710160 0.748761SWAT 0.657754 0.692487TorMd 0.375401 0.431243HIT 0.336898 0.395993baseline 0.4053 0.4618Table 2: The scores of all participating systemsAs follow the participating systems are brieflyintroduced.SRCB-WSD system exploited maximum entropymodel as the classifier from OpenNLP2 The fol-lowing features are used in this WSD system:?
All the verbs and nouns in the context, that is,the words with tags ?n, nr, ns, nt, nz, v, vd, vn??
PoS of the left word and the right word?noun phrase, verb phrase, adjective phrase,time phrase, place phrase and quantity phrase.These phrases are considered as constituents ofcontext, as well as words and punctuations whichdo not belong to any phrase.
?the type of these phrases which are around thetarget phrases2 http:// maxent.sourceforge.net/20?
word category information comes from Chi-nese thesaurusI2R system used a semi-supervised classificationalgorithm (label propagation algorithm) (Niu, et al,2005).
They used three types of features: PoS ofneighboring words with position information, un-ordered single words in topical context, and localcollocations.In the label propagation algorithm (LP) (Zhuand Ghahramani, 2002), label information of anyvertex in a graph is propagated to nearby verticesthrough weighted edges until a global stable stageis achieved.
Larger edge weights allow labels totravel through easier.
Thus the closer the examples,the more likely they have similar labels (the globalconsistency assumption).
In label propagationprocess, the soft label of each initial labeled exam-ple is clamped in each iteration to replenish labelsources from these labeled data.
Thus the labeleddata act like sources to push out labels throughunlabeled data.
With this push from labeled exam-ples, the class boundaries will be pushed throughedges with large weights and settle in gaps alongedges with small weights.
If the data structure fitsthe classification goal, then LP algorithm can usethese unlabeled data to help learning classificationplane.CITYU-HIF system was a fully supervised onebased on a Na?ve Bayes classifier with simple fea-ture selection for each target word.
The featuresused are as follows:?
Local features at specified positions:PoS of word at w-2, w-1, w1, w2Word at w-2, w-1, w1, w2?
Topical features within a given window:Content words appearing within w-10 to w10?
Syntactic features:PoS bi-gram at w-2w0 , w-1w0 , w0w1 , w0w2PoS tri-gram at w-2 w-1w0 and w0w1w2One characteristic of this system is the incorpo-ration of the intrinsic nature of each target word indisambiguation.
It is assumed that WSD is highlylexically sensitive and each word is best character-ized by different lexical information.
Humanjudged to consider for each target word the type ofdisambiguation information if they found useful.During disambiguation, they run two Na?ve Bayesclassifiers, one on all features above, and the otheronly on the type of information deemed useful bythe human judges.
When the probability of the bestguess from the former is under a certain threshold,the best guess from the latter was used instead.SWAT system uses a weighted vote from threedifferent classifiers to make the prediction.
Thethree systems are: a Na?ve Bayes classifier thatcompares similarities based on Bayes' Rule, a clas-sifier that creates a decision list of context features,and a classifier that compares the angles betweenvectors of the features found most commonly witheach sense.
The features include bigrams, and tri-grams, and unigrams are weighted by distancefrom the ambiguous word.TorMd used an unsupervised naive Bayes classi-fier.
They combine Chinese text and an Englishthesaurus to create a `Chinese word'--`Englishcategory' co-occurrence matrix.
This system gener-ated the prior-probabilities and likelihoods of aNa?ve Bayes word sense classifier not from sense-annotated (in this case English translation anno-tated) data, but from this word--category co-occurrence matrix.
They used the Macquarie The-saurus as very coarse sense inventory.They asked a native speaker of Chinese to mapthe English translations of the target words to ap-propriate thesaurus categories.
Once the Na?veBayes classifier identifies a particular category asthe intended sense, the mapping file is used to labelthe target word with the corresponding Englishtranslation.
They rely simply on the bag of wordsthat co-occur with the target word (window size of5 words on either side).HIT is a fully unsupervised WSD system, whichputs bag of words of Chinese sentences and theEnglish translations of target ambiguous word tosearch engine (Google and Baidu).
Then theycould get al kinds of statistic data.
The correcttranslation was found through comparing theircross entropy.4 ConclusionThe goal of this task is to create a framework toevaluate Chinese word sense disambiguation andto promote research.21Scores TargetWordSense #Training #Test#Base-line SRCB-WSDI2R CITYU-HIFSWAT-MPTORMDHIT?
3 63 20 .50 .70 .80 .75 .75 .55 .55??
3 73 27 .370 .778 .815 .741 .778 .481 .407?
4 69 23 .435 .696 .609 .696 .696 .174 .174?
9 222 77 .130 .506 .506 .481 .532 .169 .091?
8 197 67 .150 .567 .552 .537 .433 .119 .104?
4 58 20 .50 .60 .50 .55 .60 .30 .30??
2 47 16 .625 .875 .875 .875 .563 .50 .438?
5 105 36 .278 .694 .667 .611 .889 .25 .139?
3 56 18 .50 .667 .722 .667 .667 .389 .333?
4 106 39 .256 .718 .615 .641 .538 .256 .256?
5 132 44 .227 .659 .75 .727 .568 .25 .114??
2 56 20 .50 .90 .95 .95 .60 .50 .50?
4 103 34 .294 .765 .706 .765 .559 .294 .294??
2 20 8 .50 .75 .75 .75 .625 .375 .50?
2 46 16 .625 .938 .813 .813 .875 .563 .438??
2 60 18 .556 .667 .722 .778 .722 .444 .556?
2 40 14 .429 .571 .643 .571 .571 .143 .286??
2 29 10 .60 .80 .70 .90 .80 .30 .30?
2 37 13 .769 .769 .769 .769 .769 .462 .462?
4 110 37 .270 .730 .676 .676 .541 .216 .216??
2 38 14 .714 .930 1.0 .929 .786 .714 .571Ave.
3.571667 571 .342/.44.685/.728.676/.721.671/.723.618/.66.30/.355.263/.335Table 3: Performance on verbs.
Micro / macro average precisions are spitted by ?/?
at the last row.Together six teams participate in this WSD task,four of them adopt supervised learning methodsand two of them used unsupervised algorithms.
Allof the four supervised learning systems exceed ob-viously the baseline obtained by the most frequentsense.
It is noted that the performances of the firstthree systems are very close.
Two unsupervisedmethods?
scores are below the baseline.
Moreunlabeled data maybe improve their performance.Although the SRCB-WSD system got the high-est scores among the six participants, it does notperform always better than other system from table2 and table 3.
But to each word, the four super-vised systems always predict correctly more in-stances than the two un-supervised systems.Besides the corpus, we provide a specification ofthe PoS tag set.
Only SRCB-WSD system utilizedthis knowledge in feature selection.
We will pro-vide more instances in the next campaign.22Scores TargetWordSense #Training #Test#Base-line SRCB-WSDI2R CITYU-HIFSWAT-MPTORMDHIT?
3 68 25 .40 .88 .84 .88 .76 .72 .32??
2 53 18 .611 .611 .722 .722 .833 .556 .333?
2 56 19 .526 .842 .842 .684 .789 .474 .632??
3 48 21 .476 .571 .591 .619 .619 .429 .619??
2 50 17 .588 .824 .824 .824 .647 .706 .529?
3 53 18 .50 .778 .722 .778 .611 .50 .222??
3 64 22 .455 .591 .591 .636 .545 .318 .364??
2 60 20 .50 1.0 .95 1.0 1.0 .50 .50??
2 38 14 .714 1.0 1.0 1.0 1.0 .643 .571??
2 45 15 .533 .733 .733 .60 .467 .467 .467?
3 67 23 .435 .783 .783 .739 .696 .348 .696??
2 44 17 .353 .529 .589 .588 .588 .353 .529??
3 50 18 .556 .611 .611 .722 .722 .50 .111??
2 39 14 .714 .929 .786 .714 .786 .857 .571??
2 47 16 .625 .813 .813 .938 1.0 .438 .563??
3 88 32 .313 .656 .563 .625 .656 .281 .344??
3 65 25 .40 .88 1.0 .92 .60 .56 .44??
2 41 14 .714 .786 .714 .786 .643 .714 .50??
2 43 16 .625 .875 .938 1.0 .875 .438 .50Ave.
2.451019 364 .506/.528.766/.773.761/.769.772/.778.72/.728.50/.516.456/.464Table 4: Performance on nouns.
Micro / macro average precisions are spitted by ?/?
at the last row.5 AcknowledgementsThis research is supported by Humanity and SocialScience Research Project of China State EducationMinistry (No.
06JC740001) and National BasicResearch Program of China (No.
2004CB318102).We would like to thank Tao Guo and Yulai Peifor their hard work to guarantee the quality of thecorpus.
Huiming Duan provides us the corpuswhich has been word-segmented and PoS-taggedand gives some suggestions during the manual an-notation.ReferencesRada Mihalcea, Timothy Chklovski and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
Proceedings of SENSEVAL-3.
25-28.Timothy Chklovski, Rada Mihalcea, Ted Pedersen andAmruta Purandare.
2004.
The Senseval-3 Multilin-gual English-Hindi lexical sample task.
Proceedings ofSENSEVAL-3.
5-8.Xiaojin Zhu, Zoubin Ghahramani.
2002.
Learning fromLabeled and Unlabeled Data with Label Propagation.CMU CALD tech report CMU-CALD-02-107.Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu.2006.
A Chinese Corpus with Word Sense Annota-tion.
Proceedings of ICCPOL, Singapore, 414-421.Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan.
2005.Word Sense Disambiguation Using Label Propaga-tion Based Semi Supervised Learning.
Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics.395-40223
