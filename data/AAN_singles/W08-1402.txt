Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 2?9Manchester, August 2008Learning to Match Names Across LanguagesInderjeet ManiThe MITRE Corporation202 Burlington RoadBedford, MA 01730, USAimani@mitre.orgAlex YehThe MITRE Corporation202 Burlington RoadBedford, MA 01730, USAasy@mitre.orgSherri CondonThe MITRE Corporation7515 Colshire DriveMcLean, VA 22102, USAscondon@mitre.orgAbstractWe report on research on matchingnames in different scripts across languag-es.
We explore two trainable approachesbased on comparing pronunciations.
Thefirst, a cross-lingual approach, uses anautomatic name-matching program thatexploits rules based on phonologicalcomparisons of the two languages carriedout by humans.
The second, monolingualapproach, relies only on automatic com-parison of the phonological representa-tions of each pair.
Alignments producedby each approach are fed to a machinelearning algorithm.
Results show that themonolingual approach results in ma-chine-learning based comparison of per-son-names in English and Chinese at anaccuracy of over 97.0 F-measure.1 IntroductionThe problem of matching pairs of names whichmay have different spellings or segmentationarises in a variety of common settings, includingintegration or linking database records, mappingfrom text to structured data (e.g., phonebooks,gazetteers, and biological databases), and text totext comparison (for information retrieval,clustering, summarization, coreference, etc.
).For named entity recognition, a name from agazetteer or dictionary may be matched againsttext input; even within monolingual applications,the forms of these names might differ.
In multi-document summarization, a name may havedifferent forms across different sources.
Systems?
2008 The MITRE Corporation.
All rights reserved.
Licensed foruse in the proceedings of the Workshop on Multi-source, Multilin-gual Information Extraction and Summarization (MIMIES2) atCOLING?2008.that address this problem must be able to handlevariant spellings, as well as abbreviations,missing or additional name parts, and differentorderings of name parts.In multilingual settings, where the namesbeing compared can occur in different scripts indifferent languages, the problem becomesrelevant to additional practical applications,including both multilingual information retrievaland machine translation.
Here special challengesare posed by the fact that there usually aren?tone-to-one correspondences between soundsacross languages.
Thus the name Stewart,pronounced   / s t u w ?
r t / in IPA, can bemapped to Mandarin ?????
?, which isPinyin ?si tu er te?, pronounced /s i t?
u a ?
t?
e/,and the name Elizabeth / I l I z ?
b ?
?/ can mapto ?????
?, which is Pinyin ?yi li sha bai?,pronounced /I l I ?
?
p aI/.
Further, in a givenwriting system, there may not be a one-to-onecorrespondence between orthography and sound,a well-known case in point being English.
Inaddition, there may be a variety of variant forms,including dialectical variants, (e.g., Bourguibacan map to Abu Ruqayba), orthographicconventions (e.g., Anglophone Wasim can mapto Francophone Ouassime), and differences inname segmentation (Abd Al Rahman can map toAbdurrahman).
Given the high degree ofvariation and noise in the data, approaches basedon machine learning are needed.The considerable differences in possiblespellings of a name also call for approacheswhich can compare names based onpronunciation.
Recent work has developedpronunciation-based models for namecomparison, e.g., (Sproat, Tao and Zhai 2006)(Tao et al 2006).
This paper explores trainablepronunciation-based models further.2Table 1: Matching ?Ashburton?
and ?????
?Consider the problem of matching Chinesescript names against their English (Pinyin) Ro-manizations.
Chinese script has nearly 50,000characters in all, with around 5,000 characters inuse by the well-educated.
However, there areonly about 1,600 Pinyin syllables when tones arecounted, and as few as 400 when they aren?t.This results in multiple Chinese script represen-tations for a given Roman form name and manyChinese characters that map to the same Pinyinforms.
In addition, one can find multiple Romanforms for many names in Chinese script, andmultiple Pinyin representations for a Chinesescript representation.In developing a multilingual approach that canmatch names from any pair of languages, wecompare an approach that relies strictly on mo-nolingual knowledge for each language, specifi-cally, grapheme-to-phoneme rules for each lan-guage, with a method that relies on cross-lingualrules which in effect map between graphemicand/or phonemic representations for the specificpair of languages.The monolingual approach requires findingdata on the phonemic representations of a namein a given language, which (as we describe inSection 4) may be harder than finding moregraphemic representations.
But once thephonemic representation is found for names in agiven language, then as one adds more languagesto a system, no more work needs to be done inthat given language.
In contrast, with the cross-lingual approach, whenever a new language isadded, one needs to  go over all the existinglanguages already in the system and compareeach of them with the new language to developcross-lingual rules for each such language pair.The engineering of such rules requires bilingualexpertise, and knowledge of differences betweenlanguage pairs.
The cross-lingual approach isthus more expensive to develop, especially forapplications which require coverage of a largenumber of languages.Our paper investigates whether we can addressthe name-matching problem without requiringsuch a knowledge-rich approach, by carrying outa comparison of the performance of the twoapproaches.
We present results of large-scalemachine-learning for matching personal namesin Chinese and English, along with somepreliminary results for English and Urdu.2 Basic Approaches2.1 Cross-Lingual ApproachOur cross-lingual approach (called MLEV) isbased on (Freeman et al 2006), who used amodified Levenshtein string edit-distancealgorithm to match Arabic script person namesagainst their corresponding English versions.
TheLevenshtein edit-distance algorithm counts theminimum number of insertions, deletions orsubstitutions required to make a pair of stringsmatch.
Freeman et al (2006) used (1) insightsabout phonological differences between the twolanguages to create rules for equivalence classesof characters that are treated as identical in thecomputation of edit-distance and (2) the use ofnormalization rules applied to the English andtransliterated Arabic names based on mappingsbetween characters in the respective writingsystems.
For example, characters correspondingto low diphthongs in English are normalized as?w?, the transliteration for the Arabic??
?character, while high diphthongs are mappedto ?y?, the transliteration for the Arabic ??
?character.Table 1 shows the representation andcomparison of a Roman-Chinese name pair(shown in the title) obtained from the LinguisticData Consortium?s LDC Chinese-English namepairs corpus (LDC 2005T34).
This corpusprovides name part pairs, the first element inEnglish (Roman characters) and the second inChinese characters, created by the LDC fromXinhua Newswire's proper name and who's whodatabases.
The name part can be a first, middleor last name.
We compare the English form ofthe name with a Pinyin Romanization of theChinese.
(Since the Chinese is being comparedwith English, which is toneless, the tone part ofPinyin is being ignored throughout this paper.
)For this study, the Levenshtein edit-distancescore (where a perfect match scores zero) isRoman Chinese (Pinyin) Alignment ScoreLEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   ||   a   s   h   e   n   b  o  d    u   |0.67MLEV ashburton ashenbodu |  a   s   h   -   -   b   u   r    t   o   n  ||  a   s   h   e   n   b   o   -   d   u   -  |0.72MALINE asVburton aseCnpotu |   a   sV  -   b   <   u   r   t   o   |   n|   a   s   eC  n   p   o   -   t   u   |   -0.483normalized to a similarity score as in (Freeman etal.
2006), where the score ranges from 0 to 1,with 1 being a perfect match.
This edit-distancescore is shown in the LEV row.The MLEV row, under the Chinese Namecolumn, shows an ?Englishized?
normalizationof the Pinyin for Ashburton.
Certain characters orcharacter sequences in Pinyin are pronounceddifferently than in English.
We therefore applycertain transforms to the Pinyin; for example, thefollowing substitutions are applied at the start ofa Pinyin syllable, which makes it easier for anEnglish speaker to see how to pronounce it andrenders the Pinyin more similar to Englishorthography: ?u:?
(umlaut ?u?)
=> ?u?, ?zh?
=>?j?, ?c?
=> ?ts?, and ?q?
=> ?ch?
(so the Pinyin?Qian?
is more or less pronounced as if it werespelled as ?Chian?, etc.).
The MLEV algorithmuses equivalence classes that allow ?o?
and ?u?to match, which results in a higher score than thegeneric score using the LEV method.2.2 Monolingual ApproachInstead of relying on rules that require extensiveknowledge of differences between a languagepair2, the monolingual approach first builds pho-nemic representations for each name, and thenaligns them.
Earlier research by (Kondrak 2000)used dynamic programming to align strings ofphonemes, representing the phonemes as vectorsof phonological features, which are associatedwith scores to produce similarity values.
Hisprogram ALINE includes a ?skip?
function in thealignment operations that can be exploited forhandling epenthetic segments, and in addition to1:1 alignments, it also handles 1:2 and 2:1alignments.
In this research, we made extensivemodifications to ALINE to add the phonologicalfeatures for languages like Chinese and Arabicand to normalize the similarity scores, producinga system called MALINE.In Table 1, the MALINE row3 shows that theEnglish name has a palato-alveolar modification2As (Freeman et al, 2006) point out, these insights arenot easy to come by: ?These rules are based on firstauthor Dr. Andrew Freeman?s experience with read-ing and translating Arabic language texts for morethan 16 years?
(Freeman et al, 2006, p. 474).3For the MALINE row in Table 1, the ALINE docu-mentation explains the notation as follows: ?everyphonetic symbol is represented by a single lowercaseletter followed by zero or more uppercase letters.
Theinitial lowercase letter is the base letter most similarto the sound represented by the phonetic symbol.
Theremaining uppercase letters stand for the feature mod-on the ?s?
(expressed as ?sV?
), so that we get thesound corresponding to ?sh?
; the Pinyin nameinserts a centered ?e?
vowel, and devoices thebilabial plosive /b/ to /p/.
There are actuallysixteen different Chinese ?pinyinizations?
ofAshburton, according to our data prepared fromthe LDC corpus.3 Experimental Setup3.1 Machine Learning FrameworkNeither of the two basic approaches described sofar use machine learning.
Our machine learningframework is based on learning from alignmentsproduced by either approach.
To view the learn-ing problem as one amenable to a statistical clas-sifier, we need to generate labeled feature vectorsso that each feature vector includes an additionalclass feature that can have the value ?true?
or?false.?
Given a set of such labeled feature vec-tors as training data, the classifier builds a modelwhich is then used to classify unlabeled featurevectors with the right labels.A given set of attested name pairs constitutes aset of positive examples.
To create negativepairs, we have found that randomly selectingelements that haven?t been paired will createnegative examples in which the pairs of elementsbeing compared are so different that they can betrivially separated from the positive examples.The experiments reported here used the MLEVscore as a threshold to select negatives, so thatexamples below the threshold are excluded.
Asthe threshold is raised, the negative examplesshould become harder to discriminate frompositives (with the harder problems mirroringsome of the ?confusable name?
characteristics ofthe real-world name-matching problems thistechnology is aimed at).
Positive examples belowthe threshold are also eliminated.
Other criteria,including a MALINE score, could be used, butthe MLEV scores seemed adequate for thesepreliminary experiments.Raising the threshold reduces the number ofnegative examples.
It is highly desirable tobalance the number of positive and negativeexamples in training, to avoid the learning beingifiers which alter the sound defined by the base letter.By default, the output contains the alignments togeth-er with the overall similarity scores.
The aligned sub-sequences are delimited by '|' signs.
The '<' sign signi-fies that the previous phonetic segment has beenaligned with two segments in the other sequence, acase of compression/expansion.
The '-' sign denotes a?skip?, a case of insertion/deletion.
?4biased by a skewed distribution.
However, whenone starts with a balanced distribution of positiveand negatives, and then excludes a number ofnegative examples below the threshold, acorresponding number of positive examples mustalso be removed to preserve the balance.
Thus,raising the threshold reduces the size of thetraining data.
Machine learning algorithms,however, can benefit from more training data.Therefore, in the experiments below, thresholdswhich provided woefully inadequate training setsizes were eliminated.One can think of both the machine learningmethod and the basic name comparison methods(MLEV and MALINE) as taking each pair ofnames with a known label and returning asystem-assigned class for that pair.
Precision,Recall, and F-Measure can be defined in anidentical manner for both machine learning andbasic name comparison methods.
In such ascheme, a threshold on the similarity score isused to determine whether the basic comparisonmatch is a positive match or not.
Learning thebest threshold for a dataset can be determined bysearching over different values for the threshold.In short, the methodology employed for thisstudy involves two types of thresholds: theMLEV threshold used to identify negativeexamples and the threshold that is applied to thebasic comparison methods, MLEV andMALINE, to identify matches.
To avoidconfusion, the term negative threshold refers tothe former, while the term positive threshold isused for the latter.The basic comparison methods were used asbaselines in this research.
To be able to provide afair basic comparison score at each negativethreshold, we ?trained?
each basic comparisonmatcher at twenty different positive thresholdson the same training set used by the learner.
Foreach negative threshold, we picked the positivethreshold that gave the best performance on thetraining data, and used that to score the matcheron the same test data as used by the learner.3.2 Feature ExtractionConsider the MLEV alignment in Table 1.
It canbe seen that the first three characters are matchedidentically across both strings; after that, we getan ?e?
inserted, an ?n?
inserted, a ?b?
matchedidentically, a ?u?
matched to an ?o?, a ?r?
de-leted, a ?t?
matched to a ?d?, an ?o?
matched to a?u?, and an ?n?
deleted.
The match unigrams arethus ?a:a?, ?s:s?, ?h:h?, ?-:e?, ?-:n?, ?b:b?, ?u:o?,?r:-?, ?t:d?, ?o:u?, and ?n:-?.
Match bigramswere generated by considering any insertion, de-letion, and (non-identical) substitution unigram,and noting the unigram, if any, to its left, pre-pending that left unigram to it (delimited by acomma).
Thus, the match bigrams in the aboveexample include ?h:h,-:e?, ?-:e,-:n?, ?b:b,u:o?,?u:o,r:-?, ?r:-,t:d?, ?t:d,o:u?, ?o:u,n:-?.These match unigram and match bigramfeatures are generated from just a single MLEVmatch.
The composite feature set is the union ofthe complete match unigram and bigram featuresets.
Given the composite feature set, each matchpair is turned into a feature vector consisting ofthe following features: string1, string2, the matchscore according to each of the basic comparisonmatchers (MLEV and MALINE), and theBoolean value of each feature in the compositefeature set.3.3 Data SetOur data is a (roughly 470,000 pair) subset of theChinese-English personal name pairs in LDC2005T34.
About 150,000 of the pairs had morethan 1 way to pronounce the English and/or Chi-nese.
For these, to keep the size of the experi-ments manageable from the point of view oftraining the learners, one pronunciation was ran-domly chosen as the one to use.
(Even with thisrestriction, a minimum negative threshold resultsin over half a million examples).
Chinese charac-ters were mapped into Hanyu Pinyin representa-tions, which are used for MLEV alignment andstring comparisons.
Since the input to MALINEuses a phonemic representation that encodesphonemic features in one or more letters, bothPinyin and English forms were mapped into theMALINE notation.There are a number of slightly varying ways tomap Pinyin into an international pronunciationsystem like IPA.
For example, (Wikipedia 2006)and (Salafra 2006) have mappings that differfrom each other and also each of these twosources have changed its mapping over time.
Weused a version of Salafra from 2006 (but weignored the ejectives).
For English, the CMUpronouncing dictionary (CMU 2008) providedphonemic representations that were then mappedinto the MALINE notation.
The dictionary hadentries for 12% of our data set.
For the names notin the CMU dictionary, a simple grapheme tophoneme script provided an approximatephonemic form.
We did not use a monolingualmapping of Chinese characters (Mandarinpronunciation) into IPA because we did not findany.560657075808590951001050 0.2 0.4 0.6 0.8MXCMBXBCBNote that we could insist that all pairs in ourdataset be distinct, requiring that there be exactlyone match for each Roman name and exactly onematch for each Pinyin name.
This in our view isunrealistic, since large corpora will be skewedtowards names which tend to occur frequently(e.g., international figures in news) and occurwith multiple translations.
We included attestedmatch pairs in our test corpora, regardless of thenumber of matches that were associated with amember of the pair.4 ResultsA variety of machine learning algorithms weretested.
Results are reported, unless otherwise in-dicated, using SVM Lite, a Support Vector Ma-chine (SVM4) classifier5 that scales well to largedata sets.Testing with SVM Lite was done with a 90/10train-test split.
Further testing was carried outwith the weka SMO SVM classifier, which usedbuilt-in cross-validation.
Although the latter clas-sifier didn?t scale to the larger data sets we used,it did show that cross-validation didn?t changethe basic results for the data sets it was tried on.4.1 Machine Learning with Different Fea-ture SetsFigure 1:  F-measure with Different Fea-ture SetsFigure 1 shows the F-measure of learning formonolingual features (M, based on MALINE),cross-lingual features (X, based on MLEV), anda combined feature set (C) of both types of fea-tures6 at different negative thresholds (shown onthe horizontal axis).
Baselines are shown withthe suffix B, e.g., the basic MALINE withoutlearning is MB.
When using both monolingualand cross-lingual features (C), the baseline (CB)4We used a linear kernel function in our SVM expe-riments; using polynomial or radial basis kernels didnot improve performance.5 From svmlight.joachims.org.6In Figure 1, the X curve is more or less under the Ccurve.is set to a system response of ?true?
only whenboth the MALINE and MLEV baseline systemsby themselves respond ?true?.
Table 2 shows thenumber of examples at each negative thresholdand the Precision and Recall for these methods,along with baselines using the basic methodsshown in square brackets.The results show that the learning method (i)outperforms the baselines (basic methods), and(ii) the gap between learning and basic compari-son widens as the problem becomes harder (i.e.,as the threshold is raised).For separate monolingual and cross-linguallearning, the increase in accuracy of the learningover the baseline (non-learning) results7 was sta-tistically significant at all negative thresholdsexcept 0.6 and 0.7.
For learning with combinedmonolingual and cross-lingual features (C), theincrease over the baseline (non-learning) com-bined results was statistically significant at allnegative thresholds except for 0.7.In comparing the mono-lingual and cross-lingual learning approaches, however, the onlystatistically significant differences were that thecross-lingual features were more accurate thanthe monolingual features at the 0 to 0.4 negativethresholds.
This suggests that (iii) the mono-lingual learning approach is as viable as thecross-lingual one as the problem of confusablenames becomes harder.However, using the combined learning ap-proach (C) is better than using either one.
Learn-ing accuracy with both monolingual and cross-lingual features is statistically significantly betterthan learning with monolingual features at the0.0 to 0.4 negative thresholds, and better thanlearning with cross-lingual features at the 0.0 to0.2, and 0.4 negative thresholds.7Statistical significance between F-measures is notdirectly computable since the overall F-measure is notan average of the F-measures of the data samples.Instead, we checked the statistical significance of theincrease in accuracy (accuracy is not shown for rea-sons of space) due to learning over the baseline.
Thestatistical significance test was done by assuming thatthe accuracy scores were binomials that were approx-imately Gaussian.
When the Gaussian approximationassumption failed (due to the binomial being tooskewed), a looser, more general bound was used(Chebyshev?s inequality, which applies to all proba-bility distributions).
All statistically significant differ-ences are at the 1% level (2-sided).64.2 Feature Set AnalysesThe unigram features reflect common correspon-dences between Chinese and English pronuncia-tion.
For example, (Sproat, Tao and Zhai 2006)note that Chinese /l/ is often associated with Eng-lish /r/, and the feature l:r is among the most fre-quent unigram mappings in both the MLEV andMALINE alignments.
At a frequency of 103,361,it is the most frequent unigram feature in theMLEV mappings, and it is the third most fre-quent unigram feature in the MALINE align-ments (56,780).Systematic correspondences among plosivesare also captured in the MALINE unigram map-pings.
The unaspirated voiceless Chinese plo-sives /p,t,k/ contrast with aspirated plosives/p?,t?,k?/, whereas the English voiceless plosives(which are aspirated in predictable environments)contrast with voiced plosives /b,d,g/.
As a result,English /b,d,g/ phonemes are usually translite-rated using Chinese characters that are pro-nounced /p,t,k/, while English /p,t,k/ phonemesusually correspond to Chinese /p?,t?,k?/.
The ex-amples of Stewart and Elizabeth in Section 1illustrate the correspondence of English /t/ andChinese / t?/ and of English /b/ with Chinese /p/respectively.
All six of the unigram features thatresult from these correspondences occur amongthe 20 most frequent in the MALINE alignments,ranging in frequency from 23,602 to 53,535.Neg-ativeThre-sholdExam-plesMonolingual  (M) Cross-Lingual (X) Combined (C)P R P R P R0 538,621 94.69[90.6]95.73[91.0]96.5[90.0]97.15[93.4]97.13[90.8]97.65[91.0]0.1 307,066 95.28[87.1]96.23[83.4]98.06[89.2]98.25[89.9]98.4[87.6]98.64[84.1]0.2 282,214 95.82[86.2]96.63[84.4]97.91[88.4]98.41[90.3]98.26[86.7]98.82[84.7]0.3 183,188 95.79[80.6]96.92[85.3]98.18[86.3]98.8[90.7]98.24[80.6]99.27[84.8]0.4 72,176 96.31[77.1]98.69[82.3]97.89[91.8]99.61[86.2]98.91[77.1]99.64[80.9]0.5 17,914 94.62[64.6]98.63[84.3]99.44[89.4]100.0[91.9]99.46[63.8]99.89[84.7]0.6 2,954 94.94[66.1]100[77.0]98.0[85.2]98.66[92.8]99.37[61.3]100.0[73.1]0.7 362 95.24[52.8]100[100.0]94.74[78.9]100.0[78.9]100.0[47.2]94.74[100.0]Table 2:  Precision and Recall with Different Feature Sets(Baseline scores in square brackets)4.3 Comparison with other LearnersTo compare with other machine learning tools,we used the WEKA toolkit (fromwww.weka.net.nz).
Table 3 shows the compar-isons on the MLEV data for a fixed size at onethreshold.
Except for SVM Light, the resultsare based on 10-fold cross validation.
Theother classifiers appear to perform relativelyworse at that setting for the MLEV data, butthe differences in accuracy are not statisticallysignificant even at the 5% level.
A large con-tributor to the lack of significance is the smalltest set size of 66 pairs (10% of 660 examples)used in the SVM Light test.4.4 Other Language PairsSome earlier experiments for Arabic-Romancomparisons were carried out using a Condi-tional Random Field learner (CRF), using theCarafe toolkit (from source-forge.net/projects/carafe).
The method com-putes its own Levenshtein edit-distance scores,and learns edit-distance costs from that.
Thescores obtained, on average, had only a .6 cor-relation with the basic comparison Levenshteinscores.
However, these experiments did notreturn accuracy results, as ground-truth datawas not specified for this task.7Several preliminary machine learning expe-riments were also carried out on Urdu-Romancomparisons.
The data used were Urdu dataextracted from a parallel corpus recently pro-duced by the LDC (LCTL_Urdu.20060408).The results are shown in Table 4.
Here a .55MALINE score and a .85 MLEV score wereused for selecting positive examples by basiccomparison, and negative examples were se-lected at random.
Here the MALINE method(row 1) using the weka SMO SVM made useof a threshold based on a MALINE score.
Inthese earlier experiments, machine learningdoes not really improve the system perfor-mance (F-measure decreases with learning onone test and only increases by 0.1% on theother test).
However, since these earlier expe-riments did not benefit from the use of differ-ent negative thresholds, there was no controlover problem difficulty.5 Related WorkWhile there is a substantial literature employ-ing learning techniques for record linkagebased on the theory developed by Fellegi andSunter (1969), researchers have only recentlydeveloped applications that focus on namestrings and that employ methods which do notrequire features to be independent (Cohen andRichman 2002).
Ristad and Yianilos (1997)have developed a generative model for learn-ing string-edit distance that learns the cost ofdifferent edit operations during string align-ment.
Bilenko and Mooney (2003) extend Ris-tad?s approach to include gap penalties (wherethe gaps are contiguous sequences of mis-matched characters) and compare this genera-tive approach with a vector similarity approachthat doesn?t carry out alignment.
McCallum etal.
(2005) use Conditional Random Fields(CRFs) to learn edit costs, arguing in favor ofdiscriminative training approaches and againstgenerative approaches, based in part on thefact that the latter approaches ?cannot benefitfrom negative evidence from pairs of stringsthat (while partially overlapping) should beconsidered dissimilar?.
Such CRFs model theconditional probability of a label sequence (analignment of two strings) given a sequence ofobservations (the strings).A related thread of research is work on au-tomatic transliteration, where training sets aretypically used to compute probabilities formappings in weighted finite state transducers(Al-Onaizan and Knight 2002; Gao et al 2004)or source-channel models (Knight and Graehl1997; Li et al 2004).
(Sproat et al 2006) havecompared names from comparable and con-temporaneous English and Chinese texts, scor-ing matches by training a learning algorithm tocompare the phonemic representations of thenames in the pair, in addition to taking intoaccount the frequency distribution of the pairover time.
(Tao et al 2006) obtain similar re-sults using frequency and a similarity scorebased on a phonetic cost matrixThe above approaches have all developedspecial-purpose machine-learning architecturesto address the matching of string sequences.They take pairs of strings that haven?t beenaligned, and learn costs or mappings fromthem, and once trained, search for the bestmatch given the learned representationPositiveThresholdExamples Method P R F Accuracy.65 660 SVM Light 90.62 87.88 89.22 89.39.65 660 WEKA SMO 80.6 83.3 81.92 81.66.65 660 AdaBoost M1 84.9 78.5 81.57 82.27Table 3: Comparison of Different ClassifiersMethod PositiveThresholdExamples P R FWEKA SMO .55 (MALINE) 206 (MALINE) 84.8 [81.5] 86.4 [93.3] 85.6 [87.0]WEKA SMO .85 (MLEV) 584 (MLEV) 89.9 [93.2] 94.7 [91.2] 92.3 [92.2]Table 4: Urdu-Roman Name Matching Results with Random Negatives(Baseline scores in square brackets)8Our approach, by contrast, takes pairs ofstrings along with an alignment, and using fea-tures derived from the alignments, trains a learn-er to derive the best match given the features.This offers the advantage of modularity, in thatany type of alignment model can be combinedwith SVMs or other classifiers (we have pre-ferred SVMs since they offer discriminativetraining).
Our approach allows leveraging of anyexisting alignments, which can lead to startingthe learning from a higher baseline and less train-ing data to get to the same level of performance.Since the learner itself doesn?t compute thealignments, the disadvantage of our approach isthe need to engineer features that communicateimportant aspects of the alignment to the learner.In addition, our approach, as with McCallumet al (2005), allows one to take advantage ofboth positive and negative training examples,rather than positive ones alone.
Our data genera-tion strategy has the advantage of generatingnegative examples so as to vary the difficulty ofthe problem, allowing for more fine-grained per-formance measures.
Metrics based on such acontrol are likely to be useful in understandinghow well a name-matching system will work inparticular applications, especially those involvingconfusable names.6 ConclusionThe work presented here has established aframework for application of machine learningtechniques to multilingual name matching.
Theresults show that machine learning dramaticallyoutperforms basic comparison methods, with F-measures as high as 97.0 on the most difficultproblems.
This approach is being embedded in alarger system that matches full names using avetted database of full-name matches for evalua-tion.So far, we have confined ourselves to minimalfeature engineering.
Future work will investigatea more abstract set of phonemic features.
Wealso hope to leverage ongoing work on harvest-ing name pairs from web resources, in additionapplying them to less commonly taught languag-es, as and when appropriate resources for thembecome available.ReferencesAl-Onaizan, Y. and K. Knight, K. 2002.
MachineTransliteration of Names in Arabic Text.
Proceed-ings of the ACL Workshop on Computational Ap-proaches to Semitic Languages.Bilenko, M. and Mooney, R.J. 2003.
Adaptive dupli-cate detection using learnable string similaritymeasures.
In Proc.
of SIGKDD-2003.CMU.
2008.
The CMU Pronouncingnary.
ftp://ftp.cs.cmu.edu/project/speech/dict/Cohen, W. W., and Richman, J.
2002.
Learning tomatch and cluster large high-dimensional data setsfor data integration.
In Proceedings of The EighthACM SIGKDD International Conference on Know-ledge Discovery and Data Mining (KDD-2002).Fellegi, I. and Sunter, A.
1969.
A theory for recordlinkage.
Journal of the American Statistical Socie-ty, 64:1183-1210, 1969.Freeman, A., Condon, S. and Ackermann, C. 2006.Cross Linguistic Name Matching in English andArabic.
Proceedings of HLT.Gao, W., Wong, K., and Lam, W. 2004.
Phoneme-based transliteration of foreign names for OOVproblem.
In Proceedings of First InternationalJoint Conference on Natural Language Processing.Kondrak, G. 2000.
A New Algorithm for the Align-ment of Phonetic Sequences.
Proceedings of theFirst Meeting of the North American Chapter ofthe Association for Computational Linguistics(ANLP-NAACL 2000),  288-295.Knight, K. and Graehl, J., 1997.
Machine Translitera-tion, In Proceedings of the Conference of the Asso-ciation for Computation Linguistics (ACL).Li, H., Zhang, M., & Su, J.
2004.
A joint source-channel model for machine transliteration.
In Pro-ceedings of Conference of the Association forComputation Linguistics (ACL).McCallum, A., Bellare, K. and Pereira, F. 2005.
AConditional Random Field for Discriminatively-trained Finite-state String Edit Distance.
Confe-rence on Uncertainty in AI (UAI).Ristad, E. S. and Yianilos, P. N. 1998.
Learningstring edit distance.
IEEE Transactions on PatternRecognition and Machine Intelligence.Salafra.
2006. http://www.safalra.com /science/linguistics/pinyin-pronunciation/Sproat, R., Tao, T. and Zhai, C. 2006.
Named EntityTransliteration with Comparable Corpora.
In Pro-ceedings of the Conference of the Association forComputational Linguistics.
New York.Tao, T., Yoon, S. Fister, A., Sproat, R. and Zhai, C.2006.
Unsupervised Named Entity TransliterationUsing Temporal and Phonetic Correlation.
In Pro-ceedings of the ACL Empirical Methods in NaturalLanguage Processing Workshop.Wikipedia.
2006. http://en.wikipedia.org/wiki/Pinyin9
