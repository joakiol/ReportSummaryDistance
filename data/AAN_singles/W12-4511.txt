Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsA Multigraph Model for Coreference ResolutionSebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael StrubeNatural Language Processing GroupHeidelberg Institute for Theoretical Studies gGmbHHeidelberg, Germany(sebastian.martschat|jie.cai|michael.strube)@h-its.orgAbstractThis paper presents HITS?
coreference reso-lution system that participated in the CoNLL-2012 shared task on multilingual unrestrictedcoreference resolution.
Our system employs asimple multigraph representation of the rela-tion between mentions in a document, wherethe nodes correspond to mentions and theedges correspond to relations between thementions.
Entities are obtained via greedyclustering.
We participated in the closed tasksfor English and Chinese.
Our system rankedsecond in the English closed task.1 IntroductionCoreference resolution is the task of determiningwhich mentions in a text refer to the same entity.This paper describes HITS?
system for the CoNLL-2012 Shared Task on multilingual unrestricted coref-erence resolution, where the goal is to build a systemfor coreference resolution in an end-to-end multilin-gual setting (Pradhan et al, 2012).
We participatedin the closed tasks for English and Chinese and fo-cused on English.
Our system ranked second in theEnglish closed task.Being conceptually similar to and building uponCai et al (2011b), our system is based on a directedmultigraph representation of a document.
A multi-graph is a graph where two nodes can be connectedby more than one edge.
In our model, nodes rep-resent mentions and edges are built from relationsbetween the mentions.
The entities to be inferredcorrespond to clusters in the multigraph.Our model allows for directly representing anykind of relations between pairs of mentions in agraph structure.
Inference over this graph can har-ness structural properties and the rich set of encodedrelations.
In order to serve as a basis for furtherwork, the components of our system were designedto work as simple as possible.
Therefore our systemrelies mostly on local information between pairs ofmentions.2 ArchitectureOur system is implemented on top of the BARTtoolkit (Versley et al, 2008).
To compute the coref-erence clusters in a document, we first extract a setof mentions M = {m1, .
.
.
,mn} ordered accordingto their position in the text (Section 2.1).
We thenbuild a directed multigraph where the set of nodesis M and edges are induced by relations betweenmentions (Section 2.4).
The relations we use in oursystem are coreference indicators like string match-ing or alias (Section 3).
For every relation R, wecompute a weight wR using the training data (Sec-tion 2.3).
We then assign the weight wR to any edgethat is induced by the relation R. Depending on dis-tance and connectivity properties of the graph theweights may change (Section 2.4.1).
Given the con-structed graph with edge weights, we go through thementions according to their position in the text andperform greedy clustering (Section 2.6).
For Chi-nese, we employ spectral clustering (Section 2.5) asadopted in Cai et al (2011b) before the greedy clus-tering step to reduce the number of candidate an-tecedents for a mention.
The components of our sys-tem are described in the following subsections.1002.1 Mention ExtractionNoun phrases are extracted from the provided parseand named entity annotation layers.
For embeddedmentions with the same head, we only keep the men-tion with the largest span.2.1.1 EnglishFor English we identify eight different mentiontypes: common noun, proper noun, personal pro-noun, demonstrative pronoun, possessive pronoun,coordinated noun phrase, quantifying noun phrase(some of ..., 17 of ...) and quantified noun phrase(the armed men in one of the armed men).
The headfor a common noun or a quantified noun is com-puted using the SemanticHeadFinder from the Stan-ford Parser1.
The head for a proper noun starts atthe first token tagged as a noun until a punctuation,preposition or subclause is encountered.
Coordina-tions have the CC tagged token as head and quanti-fying noun phrases have the quantifier as head.In a postprocessing step we filter out adjectivaluse of nations and named entities with semanticclass Money, Percent or Cardinal.
We discard men-tions whose head is embedded in another mention?shead.
Pleonastic pronouns are identified and dis-carded via a modified version of the patterns usedby Lee et al (2011).2.1.2 ChineseFor Chinese we detect four mention types: com-mon noun, proper noun, pronoun and coordination.The head detection for Chinese is provided by theSunJurafskyChineseHeadFinder from the StandfordParser, except for proper nouns whose head is set tothe mention?s rightmost token.The remaining processing is similar to the men-tion detection for English.2.2 PreprocessingWe extract the information in the provided an-notation layers and transform the predicted con-stituent parse trees into dependency parse trees.We work with two different dependency represen-tations, one obtained via the converter implemented1http://nlp.stanford.edu/software/lex-parser.shtmlin Stanford?s NLP suite2, the other using LTH?sconstituent-to-dependency conversion tool3.
Forpronouns, we determine number and gender usingtables containing a mapping of pronouns to theirgender and number.2.2.1 EnglishFor English, number and gender for commonnouns are computed via a comparison of headlemma to head and using the number and genderdata of Bergsma and Lin (2006).
Quantified nounphrases are always plural.
We compute semanticclasses via a WordNet (Fellbaum, 1998) lookup.2.2.2 ChineseFor Chinese, we simply determine number andgender by searching for the corresponding desig-nators, since plural mentions mostly end with ?,while ??
(sir) and ??
(lady) often suggest gen-der information.
To identify demonstrative and defi-nite noun phrases, we check whether they start witha definite/demonstrative indicator (e.g.
?
(this) or?
(that)).
We use lists of named entities extractedfrom the training data to determine named entitiesand their semantic class in development and testingdata.2.3 Computing Weights for RelationsWe compute weights for relations using simple de-scriptive statistics on training documents.
Since thisis a robust approach to learning weights for the typeof graph model we employ (Cai et al, 2011b; Caiet al, 2011a), we use only a fraction of the availabletraining data.
We took a random subset consisting ofaround 20% for English and 15% for Chinese of thetraining data.
For every document in this set and ev-ery relation R, we go through the set M of extractedmentions and compute for every pair (mi,mj) withi > j whether R holds for this pair.
The weight wRfor R is then the number of coreferent pairs with Rdivided by the number of all pairs with R.2.4 Graph ConstructionThe set of relations we employ consists of two sub-sets: negative relations R?
which enforce that no2http://nlp.stanford.edu/software/stanford-dependencies.shtml3http://nlp.cs.lth.se/software/treebank_converter/101edge is built between two mentions, and positive re-lations R+ that induce edges.
Again, we go throughM in a left-to-right fashion.
If for two mentions mi,mj with i > j a negative relation R?
holds, no edgebetween mi and mj can be built.
Otherwise we addan edge from mi to mj for every positive relationR+ such that R+(mi,mj) is true.
The structure ob-tained by this construction is a directed multigraph.We handle copula relations similar to Lee et al(2011): if mi is this and the pair (mi,mj) is in acopula relation (like This is the World), we removemj and replace mj in all edges involving it by mi.For Chinese, we handle ?role appositives?
as intro-duced by Haghighi and Klein (2009) analogously.2.4.1 Assigning Weights to EdgesInitially, any edge (mi,mj) induced by the rela-tion R has the weight wR computed as describedin Section 2.3.
If R is a transitive relation, we di-vide the weight by the number of mentions con-nected by this relation.
This corresponds to the wayedge weights are assigned during the spectral em-bedding in Cai et al (2011b).
If R is a relation sen-sitive to distance like compatibility between a com-mon/proper noun and a pronoun, the weight is al-tered according to the distance between mi and mj .2.4.2 An ExampleWe demonstrate the graph construction by a sim-ple example.
Consider a document consisting of thefollowing three sentences.Barack Obama and Nicolas Sarkozy metin Toronto yesterday.
They discussed thefinancial crisis.
Sarkozy left today.Let us assume that our system identifies BarackObama (m1), Nicolas Sarkozy (m2), Barack Obamaand Nicolas Sarkozy (m3), They (m4) and Sarkozy(m5) as mentions.
We consider these mentions andthe relations N Number, P Nprn Prn, P Alias andP Subject described in Section 3.
The graph con-structed according to the algorithm described in thissection is displayed in Figure 1.Observe the effect of the negative relationN Number: while P Nprn Prn holds for the pairBarack Obama (m1) and They (m4), the mentionsdo not agree in number.
Hence N Number holds forthis pair and no edge from m4 to m1 can be built.m2 m5m3 m4P AliasP Nprn PrnP SubjectFigure 1: An example graph.
Nodes represent mentions,edges are induced by relations between the mentions.2.5 Spectral ClusteringFor Chinese we apply spectral clustering before thefinal greedy clustering phase.
In order to be able toapply spectral clustering, we make the graph undi-rected and merge parallel edges into one edge, sum-ming up all weights.
Due to the way edge weightsare computed, the resulting undirected simple graphcorresponds to the graph Cai et al (2011b) use asinput to the spectral clustering algorithm.
Spectralclustering is now performed as in Cai et al (2011b).2.6 Greedy ClusteringTo describe our clustering algorithm, we use someadditional terminology: if there exists an edge fromm to n we say that m is a parent of n and that n is achild of m.In the last step, we go through the mentions fromleft to right.
Let mi be the mention in focus.
ForEnglish, we consider all children of mi as possibleantecedents.
For Chinese we restrict the possible an-tecedents to all children that are in the same clusterobtained by spectral clustering.If mi is a pronoun, we determine mj such thatthe sum over all weights of edges from mi to mj ismaximized.
We then assign mi and mj to the sameentity.
In English, if mi is a parent of a noun phrasem that embeds mj , we instead assign mi and m tothe same entity.For Chinese, all other noun phrases are assignedto the same entity as all their children in the clusterobtained by spectral clustering.
For English, we aremore restrictive: definites and demonstratives are as-signed to the same cluster as their closest (accordingto the position of the mentions in the text) child.Negative relations may also be applied as con-straints in this phase.
Before assigning mi to thesame entity as a set of mentions C, we check for102every m ?
C and every negative relation R?that we want to incorporate as a constraint whetherR?
(mi,m) holds.
If yes, we do not assign mi to thesame entity as the mentions in C.2.7 ComplexityOur algorithms for weight computation, graph con-struction and greedy clustering look at all pairs ofmentions in a document and perform simple calcu-lations, which leads to a time complexity of O(n2)per document, where n is the number of mentionsin a document.
When performing spectral cluster-ing, this increases to O(n3).
Since we deal withat most a few hundred mentions per document, thecubic running time is not an issue.3 RelationsIn our system relations serve as templates for build-ing or disallowing edges between mentions.
Wedistinguish between positive and negative relations:negative relations disallow edges between mentions,positive relations build edges between mentions.Negative relations can also be used as constraintsduring clustering, while positive relations may alsobe applied as ?weak?
relations: in this case, we onlyadd the induced edge when the two mentions underconsideration are already included in the graph afterconsidering all the non-weak relations.Most of the relations presented here were alreadyused in our system for last year?s shared task (Cai etal., 2011b).
The set of relations was enriched mainlyto resolve pronouns in dialogue and to resolve pro-nouns that do not carry much information by them-selves like it and they.3.1 Negative Relations(1) N Gender, (2) N Number: Two mentions donot agree in gender or number.
(3) N SemanticClass: Two mentions do not agreein semantic class (only the Object, Date and Per-son top categories derived from WordNet (Fell-baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-tence distance to the antecedent is larger thanone.
(5) N BarePlural: Two mentions that are both bareplurals.
(6) N Speaker12Prn: Two first person pronounsor two second person pronouns with differentspeakers, or one first person pronoun and onesecond person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in directspeech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in thesubject and object positions of the same verb,and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-tic heads, and the anaphor has a pre- or post-modifier which does not occur in the antecedentand does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-beds the other, which is not a reflexive or posses-sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-nouns without speaker information and not in di-rect speech.3.2 Positive Relations(12) P StrMatch Npron, (13) P StrMatch Pron:After discarding stop words, if the strings ofmentions completely match and are not pro-nouns, the relation P StrMatch Npron holds.When the matched mentions are pronouns,P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-tions match.
(15) P Nprn Prn: If the antecedent is not a pro-noun and the anaphor is a pronoun.
This relationis restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other(i.e.
proper names with partial match, full namesand acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the secondperson pronoun is talking to the speaker of thefirst person pronoun.
The mentions contain onlyfirst or second person pronouns.
(18) P DSPrn: If one mention is subject of a speakverb, and the other mention is a first person pro-noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-noun, and the antecedent is the subject of thesentence.103(20) P PossPrn: If the anaphor is a possessive pro-noun, and the antecedent is the subject of thesentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-tity of GPE entity type and the anaphor is a def-inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is apossessive pronoun and is embedded in the an-tecedent.
(23) P VerbAgree: If the anaphor is a pronoun andhas the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentionsare subjects/objects (applies only if the anaphoris it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,the antecedent is not a pronoun, and both havesemantic class Person.For English, we used all relations except for (21) and(26).
Relations (1), (2) and (10) were incorporatedas constraints during greedy clustering.
For Chinese,we used relations (1) ?
(6), (12) ?
(15), (21) and (26).
(26) was incorporated as a weak relation.4 ResultsWe submitted to the closed tasks for English andChinese.
The results on the English developmentset and testing set are displayed in Table 1 and Table2 respectively.
To indicate the progress we achievedwithin one year, Table 3 shows the performance ofour system on the CoNLL ?11 development data setcompared to last year?s results (Cai et al, 2011b).The Overall number is the average of MUC, B3and CEAF (E), MD is the mention detection score.Overall, we gained over 5% F1 some of which canbe attributed to improved mention detection.Metric R P F1MD 73.96 75.69 74.81MUC 64.93 68.69 66.76B3 68.42 75.77 71.91CEAF (M) 61.23 61.23 61.23CEAF (E) 49.61 45.60 47.52BLANC 77.81 80.75 79.19Overall 62.06Table 1: Results on the English CoNLL ?12 developmentsetMetric R P F1MD 74.23 76.10 75.15MUC 65.21 68.83 66.97B3 66.50 74.69 70.36CEAF (M) 59.61 59.61 59.61CEAF (E) 48.64 44.72 46.60BLANC 73.29 78.94 75.73Overall 61.31Table 2: Results on the English CoNLL ?12 testing setMetric R P F1 2011 F1MD 70.84 73.08 71.94 66.28MUC 60.80 65.09 62.87 55.19B3 68.37 75.89 71.94 68.52CEAF (M) 60.42 60.42 60.42 54.44CEAF (E) 50.40 46.11 48.16 43.19BLANC 75.44 79.26 77.19 72.13Overall 60.99 55.63Table 3: Results on the English CoNLL ?11 developmentset compared to Cai et al (2011b)Table 4 and Table 5 display our results on Chinesedevelopment data and testing data respectively.Metric R P F1MD 52.45 71.50 60.51MUC 45.90 67.07 54.50B3 58.94 84.26 69.36CEAF (M) 53.60 53.60 53.60CEAF (E) 50.73 34.24 40.89BLANC 66.17 83.11 71.45Overall 54.92Table 4: Results on the Chinese CoNLL ?12 developmentsetMetric R P F1MD 48.49 74.02 58.60MUC 42.71 67.80 52.41B3 55.37 85.24 67.13CEAF (M) 51.30 51.30 51.30CEAF (E) 51.81 32.46 39.92BLANC 63.96 82.81 69.18Overall 53.15Table 5: Results on the Chinese CoNLL ?12 testing setBecause none of our team members has knowl-edge of the Arabic language we did not attempt to104run our system on the Arabic datasets and thereforeour official score for this language is considered tobe 0.
The combined official score of our submissionis (0.0 + 53.15 + 61.31)/3 = 38.15.
In the closedtask our system was the second best performing sys-tem for English and the eighth best performing sys-tem for Chinese.5 Error analysisWe did not attempt to resolve event coreference anddid not incorporate world knowledge which is re-sponsible for many recall errors our system makes.Since we use a simple greedy strategy for clus-tering that goes through the mentions left-to-right,errors in clustering propagate, which gives rise tocluster-level inconsistencies.
We observed a drop inperformance when using more negative relations asconstraints.
A more sophisticated clustering strat-egy that allows a more refined use of constraints isneeded.5.1 EnglishOur detection of copula and appositive relations isquite inaccurate, which is why we limit the incor-poration of copulas to cases where the antecedent isthis and left appositives out.We aim for high precision regarding the usage ofthe negative relation N Modifier.
This leads to someloss in recall.
For example, our system does not as-sign the just-completed Paralympics and the 12-dayParalympics to the same entity.
Such cases require amore involved reasoning scheme to decide whetherthe modifiers are actually contradicting each other.Non-referring pronouns constitute another sourceof errors.
While we improved detection of pleonas-tic it compared to last year?s system, a lot of themare not filtered out.
Our system also does not distin-guish well between generic and non-generic uses ofyou and we, which hurts precision.5.2 ChineseSince each Chinese character carries its own mean-ing, there are multiple ways to express the same en-tity by combining different characters into a word.Both syntactic heads and modifiers can be replacedby similar words or by abbreviated versions.
From???
(outside people) to ????
(outside eth-nic group) the head is replaced, while from ???
(Diana) to ??
??
?
??
(charming DiPrincess) the name is abbreviated.Modifier replacement is more difficult to copewith, our system does not recognize that ??
????
(starting-over counting-votes job) and????
(verifying-votes job) are coreferent.
It is alsonot trivial to separate characters from words (e.g.
byseparating ?
and ?)
to resolve such cases, sinceit will introduce too much noise as a consequence.In order to tackle this problem, a smart scheme topropagate similarities from partial words to the en-tire mentions and a knowledge base upon which re-liable similarities can be retrieved are necessary.In contrast to English there is no strict enforce-ment of using definite noun phrases when referringto an antecedent in Chinese.
Both ????
(thetalk) and??
(talk) can corefer with the antecedent???????????
(Clinton?s talk duringHanoi election).
This makes it very difficult to dis-tinguish generic expressions from referential ones.In the submitted version of our system, we simplyignore the nominal anaphors which do not start withdefinite articles or demonstratives.6 ConclusionsIn this paper we presented a graph-based model forcoreference resolution.
It captures pairwise relationsbetween mentions via edges induced by relations.Entities are obtained by graph clustering.
Discrim-inative information can be incorporated as negativerelations or as constraints during clustering.We described our system?s architecture and the re-lations it employs, highlighting differences and sim-ilarities to our system from last year?s shared task.Designed to work as a basis for further work, oursystem works mainly by exploring the relationshipbetween pairs of mentions.
Due to its modular archi-tecture, our system can be extended by componentstaking global information into account, for examplefor weight learning or clustering.We focused on the closed task for English inwhich our system achieved competitive perfor-mance, being ranked second out of 15 participants.Acknowledgments.
This work has been fundedby the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first and the second authors have beensupported by a HITS PhD.
scholarship.105ReferencesShane Bergsma and Dekang Lin.
2006.
Bootstrappingpath-based pronoun resolution.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, Sydney, Australia, 17?21 July 2006, pages 33?40.Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and MichaelStrube.
2011a.
Weakly supervised graph-based coref-erence resolution for clinical data.
In Proceedings ofthe 5th i2b2 Shared Tasks and Workshop on Chal-lenges in Natural Language Processing for ClinicalData, Washington, D.C., 20-21 October 2011.
To ap-pear.Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.2011b.
Unrestricted coreference resolution via globalhypergraph partitioning.
In Proceedings of the SharedTask of the 15th Conference on Computational Natu-ral Language Learning, Portland, Oreg., 23?24 June2011, pages 56?60.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,Mass.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, Singapore,6?7 August 2009, pages 1152?1161.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the CoNLL-2011 shared task.
In Proceedingsof the Shared Task of the 15th Conference on Compu-tational Natural Language Learning, Portland, Oreg.,23?24 June 2011, pages 28?34.Sameer Pradhan, Alessandro Moschitti, and NianwenXue.
2012.
CoNLL-2012 Shared Task: Modelingmultilingual unrestricted coreference in OntoNotes.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012.
This volume.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Companion Volume to the Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics, Columbus, Ohio, 15?20 June 2008, pages9?12.106
