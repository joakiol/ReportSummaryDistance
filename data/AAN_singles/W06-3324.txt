Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 124?125,New York City, June 2006. c?2006 Association for Computational LinguisticsA Pragmatic Approach to Summary Extraction in Clinical TrialsGraciela RosemblatNational Library of MedicineNIH, Bethesda, Marylandrosem@nlm.nih.govLaurel GrahamNational Library of MedicineNIH, Bethesda, Marylandlagraham@mail.nih.govBackground and IntroductionClinicalTrials.gov, the National Library ofMedicine clinical trials registry, is a monolingualclinical research website with over 29,000 recordsat present.
The information is presented in staticand free-text fields.
Static fields contain high-levelinformational text, descriptors, and controlled vo-cabularies that remain constant across all clinicalstudies (headings, general information).
Free-textdata are detailed and trial-specific, such as the Pur-pose section, which presents each trial?s goal, withlarge inter-trial variability in length as well as intechnical difficulty.
The crux of the trial purpose isgenerally found in 1-3 sentences, often introducedby clearly identified natural language markers.In the Spanish cross-language information re-trieval (CLIR) ClinicalTrials.gov prototype, indi-vidual studies are displayed as abridged Spanish-language records, with Spanish static field descrip-tors, and a manual Spanish translation for the free-text study title.
The Purpose section of these ab-breviated documents only contains a link (in Span-ish) to the full-text English record.
The premisewas that the gist could be obtained from the Span-ish title, the link to the English document, and theSpanish descriptors.
However, in a recently con-ducted user study on the Spanish CLIR prototype,Spanish-speaking consumers did not use the Pur-pose section link, as doing so entailed leaving aSpanish webpage to go to an English one.
Further,feedback from an earlier study indicated a need forsome Spanish text in the Purpose section to pro-vide the gist of the trial while avoiding the infor-mation overload in the full-text English record.Thus, in an alternative display format, extractivesummarization plus translation was used to en-hance the abbreviated Spanish document and sup-plement the link to the English record.
The trialpurpose--up to three sentences--was algorithmi-cally extracted from the English document Purposesection, and translated into Spanish via post-editedmachine translation for display in the Spanish re-cord Purpose section (Rosemblat et al, 2005).Our extraction technique, which combines sen-tence boundary detection, regular expressions, anddecision-based rules, was validated by the userstudy for facilitating user relevance judgment.
Allparticipants endorsed this alternative display for-mat over the initial schematic design, especiallywhen the Purpose extract makes up the entire Pur-pose section in the English document, as is the casein 48% of all trials.
For Purpose sections that spanmany paragraphs and exceed 1,000 words, humantranslation is not viable.
Machine translation isused to reduce the burden, and using excerpts ofthe original text as opposed to entire documentsfurther reduces the resource cost.
Human post-editing ensures the accuracy of translations.
Auto-mated extraction of key goal-describing text mayprovide relevant excerpts of the original text viatopic recognition techniques (Hovy, 2003).1 RegExp Detection and Pattern Match-ingLinguistic analysis of the natural language ex-pressions in the clinical trial records?
Purpose sec-tion was performed manually on a large sample ofdocuments.
Common language patterns acrossstudies introducing the purpose/goal of each trialserved as cue phrases.
These cue phrases containedboth quality features and the rhetorical role ofGOAL (Teufel and Moens, 1999).
The crux of thepurpose was generally condensed in 1-3 sentenceswithin the Purpose section, showing definite pat-terns and a limited set of productive, straightfor-ward linguistic markers.
From these commonpatterns, the ClinicalTrials.gov Purpose ExtractorAlgorithm (PEA) was devised, and developed inJava (1.5) using the native regexp package.124Natural language expressions in the purposesentences include three basic elements, makingthem well suited to regular expressions:a)   A small, closed set of verbs (determine, test)b)   Specific purpose triggers or cues (goal, aim)c)   Particular types of sentence constructs, as in:This study will evaluate two medications?PEA incorporates sentence boundary detection(A), purpose statement matching (B), and a seriesof decision steps (C) to ensure the extracted text issemantically and syntactically correct:A)   To improve regexp performance and en-sure that extraction occurred in complete sen-tences, sentence boundary detection wasimplemented.
Grok (OpenNLP), open source JavaNLP software, was used for this task, corpus-trained and validated, and supplemented withrules-based post-processing.B)  Regular expressions were rank orderedfrom most specific to the more general with a de-fault expression should all others fail to match.
Theregexp patterns allowed for possible tense and op-tional modal variations, and included a range of allpossible patterns that resulted from combiningverbs and triggers, controlled for case-sensitivity.The default for cases that differed from the stan-dard patterns relied solely on the verb set provided.C)  Checks were made for (a) length normali-zation (a maximum of 450 characters), with pur-pose-specific text in enumerated or bulleted listsoverriding this restriction; and (b) discourse mark-ers pointing to extra-sentential information for thesemantic processing of the text.
In this case, PEAdetermines the anchor sentence (main crux of thepurpose), and then whether to include a leadingand trailing sentence, or two leading sentences ortwo trailing ones, to reach the 3-sentence limit.RegExp Patterns Description  CasePURPOSE  Sentence label (purpose) YesTo VERB_SET Study action starts section NoIn THIS STUDY General actions in study NoTable 1.
Some purpose patterns used by PEA2 EvaluationManual PEA validation was done on a randomsample of 300 trials.
For a stricter test, the 13,110studies with Purpose sections short enough to in-clude in full without any type of processing or de-cision were not part of the random sample.Judgments were provided by the authors, one ofwhom was not involved in the development ofPEA code.
The 300 English extracts (before trans-lation) were compared against the full-text Purposesections in the clinical trials, with compression rateaveraging 30%.
Evaluation was done on a 3-pointscale: perfect extraction, appropriate, wrong text.Inter-annotator agreement using Cohen?s kappawas considered to be good (Kappa = 0.756987).Table 2 shows evaluation results after inter-raterdifferences were reconciled:CRITERIA TRIALS RATIOPerfect extraction 275   92%Appropriate extraction  18     6%Extraction of wrong text    7     2%Table 2: Results: 300 Clinical trials random sample3 ConclusionThis pragmatic approach to task-specific (pur-posive) summary extraction in a limited domain(ClinicalTrials.gov) using regular expressions hasshown a 92% precision.
Further research will de-termine if this method is appropriate for CLIR andquery language display via machine translation andsubsequent post-editing in clinical trials informa-tion systems for other registries and sponsors.AcknowledgementsThe authors thank Tony Tse and the anonymousreviewers for valuable feedback.
Work supportedby the NIH, NLM Intramural Research Program.ReferencesEduard Hovy.
2003.
Text Summarization.
In RuslanMitkov (Ed.
), The Oxford Handbook of Computa-tional Linguistics (pp.
583-598).
Oxford UniversityPress.Graciela Rosemblat, Tony Tse, Darren Gemoets, JohnE.
Gillen, and Nicholas C. Ide.
2005.
Supporting Ac-cess to Consumer Health Information Across Lan-guages.
Proceedings of the 8th International ISKOConference.
London, England.
pp.
315-321Grok part of the OpenNLP project.
[Accessed athttp://grok.sourceforge.net]Simone Teufel and Marc Moens.
1999.
Argumentativeclassification of extracted sentences as a step towardsflexible abstracting.
In Advances in Automatic TextSummarization, I. Mani and M.T.
Maybury (eds.),pp.
155-171.
MIT Press.125
