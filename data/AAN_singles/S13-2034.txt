Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 188?192, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsNRC: A Machine Translation Approach toCross-Lingual Word Sense Disambiguation(SemEval-2013 Task 10)Marine CarpuatNational Research CouncilOttawa, CanadaMarine.Carpuat@nrc.gc.caAbstractThis paper describes the NRC submission tothe Spanish Cross-Lingual Word Sense Dis-ambiguation task at SemEval-2013.
Since thisword sense disambiguation task uses Spanishtranslations of English words as gold annota-tion, it can be cast as a machine translationproblem.
We therefore submitted the output ofa standard phrase-based system as a baseline,and investigated ways to improve its sense dis-ambiguation performance.
Using only localcontext information and no linguistic analy-sis beyond lemmatization, our machine trans-lation system surprisingly yields top precisionscore based on the best predictions.
However,its top 5 predictions are weaker than thosefrom other systems.1 IntroductionThis paper describes the systems submitted by theNational Research Council Canada (NRC) for theCross-Lingual Word Sense Disambiguation task atSemEval 2013 (Lefever and Hoste, 2013).
As inthe previous edition (Lefever and Hoste, 2010), thisword sense disambiguation task asks systems to dis-ambiguate English words by providing translationsin other languages.
It is therefore closely related tomachine translation.
Our work aims to explore thisconnection between machine translation and cross-lingual word sense disambiguation, by providing amachine translation baseline and investigating waysto improve the sense disambiguation performance ofa standard machine translation system.Machine Translation (MT) has often been usedindirectly for SemEval Word Sense Disambiguation(WSD) tasks: as a tool to automatically create train-ing data (Guo and Diab, 2010, for instance) ; asa source of parallel data that can be used to trainWSD systems (Ng and Chan, 2007; van Gompel,2010; Lefever et al 2011); or as an applicationwhich can use the predictions of WSD systems de-veloped for SemEval tasks (Carpuat and Wu, 2005;Chan et al 2007; Carpuat and Wu, 2007).
This Se-mEval shared task gives us the opportunity to com-pare the performance of machine translation systemswith other submissions which use very different ap-proaches.
Our goal is to provide machine transla-tion output which is representative of state-of-the-artapproaches, and provide a basis for comparing itsstrength and weaknesses with that of other systemssubmitted to this task.
We submitted two systems tothe Spanish Cross-Lingual WSD (CLWSD) task:1.
BASIC, a baseline machine translation systemtrained on the parallel corpus used to define thesense inventory;2.
ADAPT, a machine translation system that hasbeen adapted to perform better on this task.After describing these systems in Sections 2 and3, we give an overview of the results in Section 4.2 BASIC: A Baseline Phrase-BasedMachine Translation SystemWe use a phrase-based SMT (PBSMT) architec-ture, and set-up our system to perform English-to-Spanish translation.
We use a standard SMT systemset-up, as for any translation task.
The fact that thisPBSMT system is intended to be used for CLWSDonly influences data selection and pre-processing.1882.1 Model and ImplementationIn order to translate an English sentence e into Span-ish, PBSMT first segments the English sentence intophrases, which are simply sequences of consecutivewords.
Each phrase is translated into Spanish ac-cording to the translations available in a translationlexicon called phrase-table.
Spanish phrases can bereordered to account for structural divergence be-tween the two languages.
This simple process canbe used to generate Spanish sentences, which arescored according to translation, reordering and lan-guage models learned from parallel corpora.
Thescore of a Spanish translation given an English inputsentence e segmented into J phrases is defined asfollows: score(s, e) =?i?j ?ilog(?i(sj , ej)) +?LM?LM (s)Detailed feature definitions for phrase-based SMTmodels can be found in Koehn (2010).
In our sys-tem, we use the following standard feature functions?
to score English-Spanish phrase pairs:?
4 phrase-table scores, which are conditionaltranslation probabilities and HMM lexicalprobabilities in both directions translation di-rections (Chen et al 2011)?
6 hierarchical lexicalized reordering scores,which represent the orientation of the currentphrase with respect to the previous block thatcould have been translated as a single phrase(Galley and Manning, 2008)?
a word penalty, which scores the length of theoutput sentence?
a word-displacement distortion penalty, whichpenalizes long-distance reorderings.In addition, fluency of translation is ensured by amonolingual Spanish language model ?LM , whichis a 5-gram model with Kneser-Ney smoothing.Phrase translations are extracted based on IBM-4 alignments obtained with GIZA++ (Och and Ney,2003).
The ?
weights for these features are learnedusing the batch lattice-MIRA algorithm (Cherry andFoster, 2012) to optimize BLEU-4 (Papineni et al2002) on a tuning set.
We use PORTAGE, our inter-nal PBSMT decoder for all experiments.
PORTAGEuses a standard phrasal beam-search algorithm withcube pruning.
The main differences between thisset-up and the popular open-source Moses system(Koehn et al 2007), are the use of hierarchical re-ordering (Moses only supports non-hierarchical lex-icalized reordering by default) and smoothed trans-lation probabilities (Chen et al 2011).As a result, disambiguation decisions for theCLWSD task are based on the following sources ofinformation:?
local source context, represented by sourcephrases of length 1 to 7 from the translation andreordering tables?
local target context, represented by the 5-gramlanguage model.Each English sentence in the CLWSD task istranslated into Spanish using our PBSMT system.We keep track of the phrasal segmentation used toproduce the translation hypothesis and identify theSpanish translation of the English word of interest.When the English word is translated into a multi-word Spanish phrase, we output the Spanish wordwithin the phrase that has the highest IBM1 transla-tion probability given the English target word.For the BEST evaluation, we use this processon the top PBSMT hypothesis to produce a singleCLWSD translation candidate.
For the Out-Of-Fiveevaluation, we produce up to five CLWSD transla-tion candidates from the top 1000 PBSMT transla-tion hypotheses.2.2 Data and PreprocessingTraining the PBSMT system requires a two-step pro-cess with two distinct sets of parallel data.First, the translation, reordering and languagemodels are learned on a large parallel corpus, thetraining set.
We use the sentence pairs extractedfrom Europarl by the organizers for the purpose ofselecting translation candidates for the gold annota-tion.
Training the SMT system on the exact sameparallel corpus ensures that the system ?knows?
thesame translations as the human annotators who builtthe gold standard.
This corpus consists of about900k sentence pairs.Second, the feature weights ?
in the PBSMT arelearned on a smaller parallel corpus, the tuning set.This corpus should ideally be drawn from the test189domain.
Since the CLWSD task does not provideparallel data in the test domain, we construct thetuning set using corpora publicly released for theWMT2012 translation task1.
Since sentences pro-vided in the trial data appeared to come from a widevariety of genres and domains, we decided to buildour tuning set using data from the news-commentarydomain, rather then the more narrow Europarl do-main used for training.
We selected the top 3000sentence pairs from the WMT 2012 developmenttest sets, based on their distance to the CLWSDtrial and test sentences as measured by cross-entropy(Moore and Lewis, 2010).All Spanish and English corpora were processedusing FreeLing (Padro?
and Stanilovsky, 2012).Since the CLWSD targets and gold translationsare lemmatized, we lemmatize all corpora.
WhileFreeLing can provide a much richer linguistic anal-ysis of the input sentences, the PBSMT sytem onlymakes use of their lemmatized representation.
Oursystems therefore contrast with previous approachesto CLWSD (van Gompel, 2010; Lefever et al 2011,for instance), which use richer sources of informa-tion such as part-of-speech tags.3 ADAPT: Adapting the MT system to theCLWSD taskOur ADAPT system simply consists of two modifi-cations to the BASIC PBSMT system.First, it uses a shorter maximum English phraselength.
Instead of learning a translation lexicons forphrases of length 1 to 7 as in the BASIC system,the ADAPT system only uses phrases of length 1and 2.
While this dramatically reduces the amountof source side context available for disambiguation,it also reduces the amount of noise due to incorrectword alignments.
In addition, there is more evidenceto estimate reliable translation probabilities for shortphrase, since they tend to occur more frequently thanlonger phrases.Second, the ADAPT system is trained on largerand more diverse data sets.
Since MT systems areknown to perform better when they can learn fromlarger amounts of relevant training data, we augmentour training set with additional parallel corpora fromthe WMT-12 evaluations.
We learn translation and1http://www.statmt.org/wmt12/translation-task.htmlreordering models for (1) the Europarl subset usedby the CLWSD organizers (900k sentence pairs, asin the BASIC system), and (2) the news commen-tary corpus from WMT12 (which comprises 150ksentence pairs).
For the language model, we use theSpanish side of these two corpora, as well as that ofthe full Europarl corpus from WMT12 (which com-prises 1.9M sentences).
Models learned on differentdata sets are combined using linear mixtures learnedon the tuning set (Foster and Kuhn, 2007).We also attempted other variations on the BASICsystem which were not as successful.
For instance,we tried to update the PBSMT tuning objective to bebetter suited to the CLWSD task.
When producingtranslation of entire sentences, the PBSMT systemis expected to produce hypotheses that are simulta-neously fluent and adequate, as measured by BLEUscore.
In contrast, CLWSD measures the adequacyof the translation of a single word in a given sen-tence.
We therefore attempted to tune for BLEU-1, which only uses unigram precision, and thereforefocuses on adequacy rather than fluency.
However,this did not improve CLWSD accuracy.4 ResultsTable 1 gives an overview of the results per tar-get word for both systems, as measured by all of-ficial metrics (see Lefever and Hoste (2010) for adetailed description.)
According to the BEST Pre-cision scores, the ADAPT system outperforms theBASIC system for almost all target words.
Usingonly the dominant translation picked by the humanannotators as a reference (Mode), the precision forBEST scores yield more heterogeneous results.
Thisis not surprising since the ADAPT system uses moreheterogeneous training data, which might make itharder to learn a reliable estimate of a single domi-nant translation.
When evaluating the precision outof the top 5 candidates (OOF), all systems improve,indicating that PBSMT systems can usually producesome correct alternatives to their top hypothesis.Table 2 lets us compare the average performanceof the BASIC and ADAPT systems with other par-ticipating systems.
The ADAPT system surprisinglyyields the top performance based on the PrecisionBEST evaluation setting, suggesting that, even withrelatively poor models of context, a PBSMT sys-190Precision: Best Best Best Mode Best Mode OOF OOF OOF Mode OOF ModeSystems: BASIC ADAPT BASIC ADAPT BASIC ADAPT BASIC ADAPTcoach 22.30 60.10 13.64 59.09 38.30 66.30 31.82 63.64education 36.07 38.01 73.08 84.62 42.36 42.80 84.62 84.62execution 41.07 41.07 32.00 32.00 41.57 41.57 36.00 36.00figure 23.43 29.02 33.33 37.04 31.15 36.12 37.04 44.44job 13.45 24.26 0.00 37.23 26.52 37.57 27.27 54.55letter 35.35 37.23 66.67 64.10 37.22 41.20 66.67 66.67match 15.07 16.53 2.94 2.94 20.70 20.90 5.88 8.82mission 67.98 67.98 85.29 85.29 67.98 67.98 85.29 85.29mood 7.18 8.97 0.00 0.00 26.99 29.90 11.11 11.11paper 31.33 44.59 29.73 40.54 50.45 55.61 45.95 51.35post 32.26 33.72 23.81 19.05 50.67 53.28 57.14 42.86pot 34.20 36.63 35.00 32.50 36.12 37.13 32.50 25.00range 5.41 7.56 10.00 0.00 10.39 17.47 10.00 20.00rest 20.91 23.44 12.00 8.00 27.44 25.89 16.00 16.00ring 15.87 10.10 18.92 10.81 42.80 43.14 48.65 45.95scene 15.86 23.42 43.75 62.50 38.35 37.53 81.25 81.25side 24.63 33.14 13.04 17.39 36.84 44.03 21.74 39.13soil 43.88 43.63 66.67 66.67 51.73 57.15 66.67 66.67strain 24.00 26.24 35.71 35.71 38.37 36.58 42.86 35.71test 34.45 37.51 50.00 28.57 43.61 40.86 50.00 28.57Average 27.24 32.16 32.28 36.20 37.98 41.65 42.92 45.38Table 1: Precision scores by target word for the BASIC and ADAPT systemsPrecision: Best Best Mode OOF OOF ModeSystemBest 32.16 37.11 61.69 57.35ADAPT 32.16 36.20 41.65 45.38BASIC 27.24 32.28 37.98 42.92Baseline 23.23 27.48 53.07 64.65Table 2: Overview of official results: comparison ofthe precision scores of the ADAPT and BASIC sys-tems with the best system according to each metricand with the official baselinetem can succeed in learning useful disambiguatinginformation for its top candidate.
Despite the prob-lems stemming from learning good dominant trans-lations from heterogeneous data, ADAPT ranks nearthe top using the Best Mode metric.
The rankings inthe out-of-five settings are strikingly different: thedifference between BEST and OOF precisions aremuch smaller for BASIC and ADAPT than for allother participating systems (including the baseline.
)This suggests that our PBSMT system only succeedsin learning to disambiguate one or two candidatesper word, but does not do a good job of a estimatingthe full translation probability distribution of a wordin context.
As a result, there is potentially much tobe gained from combining PBSMT systems with theapproaches used by other systems, which typicallyuse richer feature representation and context mod-els.
Further exploration of the role of context in PB-SMT performance and a comparison with dedicatedclassifiers trained on the same word-aligned paralleldata can be found in (Carpuat, 2013).5 ConclusionWe have described the two systems submitted bythe NRC to the Cross-Lingual Word Sense Disam-biguation task at SemEval-2013.
We used phrase-based machine translation systems trained on lem-matized parallel corpora.
These systems are unsu-pervised and do not use any linguistic analysis be-yond lemmatization.
Disambiguation decisions arebased on the local source context available in thephrasal translation lexicon and the target n-gramlanguage model.
This simple approach gives topperformance when measuring the precision of thetop predictions.
However, the top 5 predictions areinterestingly not as good as those of other systems.191(Carpuat, 2013)ReferencesMarine Carpuat and Dekai Wu.
2005.
Word Sense Dis-ambiguation vs. Statistical Machine Translation.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL-05), pages 387?394, Ann Arbor, Michigan.Marine Carpuat and Dekai Wu.
2007.
Improving Statis-tical Machine Translation using Word Sense Disam-biguation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL 2007), pages 61?72, Prague,June.Marine Carpuat.
2013.
A semantic evaluation of ma-chine translation lexical choice.
In Proceedings of the7th Workshop on Syntax, Semantics and Structure inStatistical Translation, Atlanta, USA, May.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word Sense Disambiguation improves Statistical Ma-chine Translation.
In 45th Annual Meeting of theAssociation for Computational Linguistics (ACL-07),Prague, June.Boxing Chen, Roland Kuhn, George Foster, and HowardJohnson.
2011.
Unpacking and transforming featurefunctions: New ways to smooth phrase tables.
In Pro-ceedings of Machine Translation Summit.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 427?436, Montre?al, Canada, June.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 848?856.Weiwei Guo and Mona Diab.
2010.
COLEPL and COL-SLM: An unsupervised wsd approach to multilinguallexical substitution, tasks 2 and 3 semeval 2010.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 129?133, Uppsala, Sweden,July.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In AnnualMeeting of the Association for Computational Linguis-tics (ACL), demonstration session, Prague, Czech Re-public, June.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Els Lefever and Ve?ronique Hoste.
2010.
Semeval-2010task 3: Cross-lingual word sense disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation, pages 15?20, Uppsala, Sweden,July.Els Lefever and Ve?ronique Hoste.
2013.
Semeval-2013task 10: Cross-lingual word sense disambiguation.In Proceedings of the 7th International Workshop onSemantic Evaluation (SemEval 2013), Atlanta, USA,May.Els Lefever, Ve?ronique Hoste, and Martine De Cock.2011.
Parasense or how to use parallel corpora forword sense disambiguation.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 317?322, Portland, Oregon, USA, June.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,USA.Hwee Tou Ng and Yee Seng Chan.
2007.
SemEval-2007Task 11: English Lexical Sample Task via English-Chinese Parallel Text.
In Proceedings of the 4th In-ternational Workshop on Semantic Evaluations (Se-mEval 2007), pages 54?58, Prague, Czech Republic.SIGLEX.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?52.Llu?
?s Padro?
and Evgeny Stanilovsky.
2012.
FreeLing3.0: Towards wider multilinguality.
In Proceedings ofthe Language Resources and Evaluation Conference(LREC 2012), Istanbul, Turkey, May.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, Philadelphia, PA, July.Maarten van Gompel.
2010.
Uvt-wsd1: A cross-lingualword sense disambiguation system.
In Proceedings ofthe 5th International Workshop on Semantic Evalua-tion, pages 238?241, Uppsala, Sweden, July.192
