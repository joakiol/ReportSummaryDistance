Workshop on Humans and Computer-assisted Translation, pages 16?21,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsEstimating Grammar Correctness for a Priori Estimationof Machine Translation Post-Editing EffortNicholas H. Kirk, Guchun ZhangAlpha Calligraphic Research Cambridge Ltd.St Andrew?s House, St Andrew?s Road,Cambridge CB4 1DL, UK{nkirk,gzhang}@alphacrc.comGeorg GrohFakultat f?ur InformatikTechnische Universit?at M?unchen,Germanygrohg@in.tum.deAbstractWe present a supervised learning pilot ap-plication for estimating Machine Transla-tion (MT) output reusability, in view ofsupporting a human post-editor of MTcontent.
We train our model on typeddependencies (labeled grammar relation-ships) extracted from human referenceand raw MT data, to then predict gram-mar relationship correctness values thatwe aggregate to provide a binary segment-level evaluation.
In view of scaling upto larger data, we provide implementedNa?
?ve Bayes and Stochastic Gradient De-scent with Support Vector Machine lossfunction approaches and their evaluation,and verify the correlation of predicted val-ues with human judgement.1 IntroductionCurrently the Machine Translation (MT) researchcommunity attempts to seamlessly integrate bothhumans and MT-instances in the workflow of tex-tual translation.
Efforts towards this integrationfocus, for instance, on automating a posteriori pro-cesses such as post-editing (Simard et al., 2007),or other format coherence maintenance (e.g.
date,spelling).
Our contribution addresses cases whena post-editor has to start a segment from scratch,because the MT raw output turns out to be a hin-drance rather than an aid, and the correspondingevaluation time between editing and manually re-translating a sequence is wasted a posteriori.
Rea-sons for unusable MT output in this context couldpotentially be a combination of the following ormore factors, with reference to the target segment:?
the word order, or grammar, are such that thesentence structure is unintelligible?
the lexical semantics of the words do not con-vey the meaning of the source segmentThese lexical or structural factors can be presentto various extents and their threshold of iden-tification can be subjective for each post-editor,but hypothetically any intervention on the latterpoints is quantifiable in terms of post-editing time,being this the most observable aspect of post-editing effort (Krings, 2001).
This paper proposesa supervised learning approach to discriminatetyped grammar relation instances that compose ahuman-written sentence from any other form, inorder to identify segments that can potentially leadto time loss on the basis of its incorrect grammaror word adjacency and delete them before post-editing.
The remainder presents the project?s as-sumptions and the nature of the adopted learn-ing features (Section 2), the high-level algorith-mic approach and the theory behind the adoptedprediction models (Section 3).
We then provideour implementation outline and the evaluation ap-proaches (Section 4).
In conclusion we presentcurrent limitations (Section 5), related and futurework (Section 6), and the conclusions (Section 7).2 ConceptWe will now provide some background and a se-ries of assertions as regards creating a classifi-cation method to estimate MT output grammarcorrectness, which mainly aims to support thepost-editor in assessing which segments will takelonger to post-edit than to translate from scratch.We assume the following post-editing behavioralphases:1.
Read source and/or target to various extents,in order to:?
check grammatical consistency of target?
check whether semantics have been con-veyed between source and target2.
Insert or delete text accordingly16Given the lack of robust adequacy understand-ing methods (i.e.
verifying meaning conveyance),we will perform analysis at a grammar and wordorder level, and for this we will seek a grammar-related formalism that is informative, scalable androbust to multiple, potentially unseen grammar in-stance variants.
We therefore exploit typed depen-dencies (De Marneffe and Manning, 2008), a la-beled, directed grammar relationship among pairsof words, which provides information on the orderof arguments and their relationship type.
Figure 1shows words of a segment instance, and for eachof these the unary and binary predicates of Part-Of-Speech tagging and typed dependency, respec-tively.Word order is important.JJ NN VBZ JJrootamodnsubjcopFigure 1: Example of words that compose a seg-ment instance, their typed dependencies (illus-trated as labeled directed edges), and the Part-Of-Speech (POS) tags.3 AlgorithmHaving discussed our aim and the required infor-mativeness, we present a pipeline for both train-ing a hypothesis model and the prediction itself(Figure 2).
The process comprises a typed de-pendency extraction module (M1) that, given aset of sentences from a test or training text,provides instances of grammar relationships inthe form of two word arguments (arg1, arg2)and a type label (deptype), which we will adoptas features.
In the training phase, a trainingmodule (M2) labels the feature data obtainedfrom {trainHuman} instances as 1, and from{trainHuman \ trainMT} as 0, where \ is theset difference operator.
We therefore consider anytyped dependency instance that does not appearin the human reference text as ?bad?.
This as-sumption holds when training on large datasetsthat comprise different grammar variants.
Fromsuch labeled dataset, M2 then formulates a hy-pothesis model.
More details on generating the hy-pothesis are provided in the next paragraph.
Dur-ing a test phase, various instances of a predic-tion module (M3) exploit such hypothesis modeltest dataMachineTranslationsource texttargethumanreferencetextTyped Gram-mar FeatureExtrac-tion (M1)Typed Gram-mar FeatureExtrac-tion (M1)ModelTraining (M2)Typed Gram-mar FeatureExtrac-tion (M1)Predicttdk,sn(M3)Predicttd1,s1(M3)TrainedHypothesisModelAggregation(sentence-levelpredic-tion) (M4)Pred.
Labelsps1.
.
.
psnFigure 2: Abstract implementation pipeline fortraining the hypothesis model (in bold), and forthe prediction itself of a typed dependency-basedsegment reusability estimator.to predict the grammar relationship goodness val-ues of td1,si.
.
.
tdk,sifor a sentence siand itstyped dependencies td1.
.
.
tdk, for all sentencessi?
{s1.
.
.
sn}.
A final phase (M4) aggregatesthe output predictions of grammar relationshipsfor each sentence, in order to construct segment-level estimations (Equation 1).Psi=1kk?j=1Ptdj,si(1)Hypothesis Model Given our aim to achievemethod robustness and breadth of applicability,and that context abstraction is potentially achiev-able since correctness of a grammar relationshipis not dependent on neighbouring dependencies,we train on a high number of diverse training in-stances.
In order to obtain a reduction in time andspace complexity, we approximate our hypothe-sis by making sample independence assumptions,such as the Na?
?ve Bayes (NB) approach (Good,1965).
NB is a model that assumes feature inde-pendence, i.e.
the ?naiveness?
implies that everyfeature Fiis conditionally independent of everyother feature Fjfor j 6= i given the class C. Equa-tion 2 describes the core of all current NB variants.17p (C|F1.
.
.
Fn) ?
p (C)n?i=1p (Fi|C) (2)Such generative model is efficient and requiresjust one linear iteration for training, hence its suit-ability for large input scaling.
Unfortunately, themodeling assumptions that enable efficient com-putability come at the expense of accuracy.
Moreprecisely, NB-based methods maximize likelihoodconditioned only over the class label C, and notover the set of all other remaining features, andas a result its effectiveness is often outperformedby discriminative classifiers such as Support Vec-tor Machines (SVM) (Crammer and Singer, 2002;Puurula, 2012).
However, given its high efficiencyand scalability, we will use NB as our primarymodel and compare it with an algorithm that hasdeeper modeling assumptions, but exploits infer-ence approximation to reduce complexity, namelySVM with Stochastic Gradient Descent for pa-rameter finding.
Approximated inference is oftenachieved via traditional gradient descent methods(see Equation 3 for linear classification or regres-sion approaches) that are largely used, first-order,stepwise optimization algorithms that seek min-ima of a problem with large dimensionality andunknown convexity status.wt+1= wt?
?t1nn?i=1?wQ(zt, wt) (3)where our objective is to iteratively minimize,given an initial parameterization (starting pointw0, number of iterations, step length ?0), and afunction Q(w), or more specifically when withinthe machine learning context, an empirical riskEn(f), defined as:Q(w) = En(f) =1nn?i=1l (f (xi) , yi) (4)where in turn l(y?, y) is a loss function that quan-tifies the cost of predicting y?
when the actual an-swer is y.
One advantage is computational effi-ciency, while the disadvantages include the inabil-ity to provide certainty of termination or result de-terminism.
Recent literature partially circumventsthese problems (Hager and Zhang, 2005), and theuse of such family of algorithms has been redis-covered for large scale data learning, whose appli-cations prefer approximate over exact inference,by using a stochastic variant (Equation 5) of thetraditional method (Zhang, 2004; Bottou, 2010),which samples a subset of the training data.wt+1= wt?
?t?wQ(zt, wt) (5)We adopt a hinge loss function for Support Vec-tor Machine (SVM) classification, a method whichhas proven to be reliable for elaborating highscale, sparsely distributed instance vector applica-tions that have dense concept vectors (Joachims,1998; Rosasco et al., 2004).4 Implementation & EvaluationAs a proof-of-concept and means of evaluation,we constructed a Java prototype that implementsthe pipeline in Figure 2, by making use of theMoses process (Koehn et al., 2007) for machinetranslation, the Stanford Parser (De Marneffe andManning, 2008) for typed dependency extraction,the API of the general-purpose machine learn-ing analysis platform Weka (Hall et al., 2009)for training and prediction, and ad-hoc imple-mentations of the remaining specified modules.Typed dependency extraction is possible by feed-ing a pre-trained language-specific ProbabilisticContext-Free Grammar (PCFG) into the parser,which is a model that defines probabilistic gram-mar production rules for such language.
Such amodel is trained beforehand on a large, syntacti-cally annotated text corpus (i.e.
a treebank) (Je-linek et al., 1992).4.1 ExperimentWe used our prototype on a subset of the Eu-roparl test data (Koehn, 2005), extracting 536404instances from 11270 human reference lines, andfrom 11270 machine-translated lines that share thesame source.
Our Naive Bayes algorithm (usingword frequencies, no pruning) required 1.08 sec-onds to formulate a hypothesis model, versus the28613.64 seconds required for the construction ofa Stochastic Gradient Descent model (hinge lossfunction for SVM, step length 0.01, 500 steps, nopruning).
A 10-fold cross validation on the train-ing set provided a correct instance classification of67.0168% for NB model, versus the 66.0118% ofthe SGD model.
Table 1 provides further statisticsof the latter evaluations.18Precision Recall F-1NB?good?
0.665 0.839 0.742?bad?
0.683 0.451 0.543SGD?good?
0.646 0.883 0.746?bad?
0.709 0.370 0.487Table 1: Precision and recall values of the 10-foldcross validation for both NB and SGD methods4.2 Correlation with Human JudgementWe organized a survey among post-editors togather human judgement values on a small setof 80 segments, for independent analysis andfor comparison with machine predicted labels.Four translators with different experience level (interms of years, 10+,5,1+, 1) evaluated a question-naire of 80 Europarl-domain segments.
Half ofthese were official human reference Europarl seg-ments, while the other half were MT processedFR ?
EN segments.
The process first involveda binary labeling task {aid|hindrance} (twoinstances of results are in Figure 3), and the sec-ond a phase assigning a mark {0. .
.10} to de-fine its level of usefulness.
Together with thelineNumber, we will consider these three fea-tures and their data as the human judgementdataset.
By aggregating the human binary evalu-ations with a majority vote and comparing thesewith the sentence-level prediction of our system,NB correctly classified 82.5% of the segments,while SGD classified 83.75%.Preliminary clustering analysis on the latterconfirmed the intuitive idea of the subjectivity ofthis kind of reusability evaluation for each trans-lator.
An unsupervised categorization was per-formed using an Expectation Maximization (EM)of Gaussian mixtures on the human judgementsurvey dataset, to understand distributional prop-erties and use this as a basis to evaluate how theprototype results correlate with human judgement.Starting with a human-only dataset and no ini-tial prior, the EM algorithm estimated by cross-validation only one homogeneous cluster.
By thenproviding the number of clusters (i.e.
the num-ber of human translators, 4), the cluster evaluationassignments were not aligned with the number ofinstances present for each translator, which im-plies post-editor behavior indistinguishability viathis method.
Once machine predicted samples areadded to the evaluation set, a further step is to ver-ify whether the cluster assignments are within anacceptable neighborhood of the previous clusterassignment values.
As shown in Table 2, clus-ter assignment percentages of NB predicted la-bels are closer to the original cluster assignmentsfrom the training set than SGD value-based clusterassignments.
The clustering approach describedwill be used as a preliminary method for effective-ness evaluation, i.e.
by evaluating the extent towhich machine predicted values are a mixture ofhuman behavior data based on the cluster assign-ment value distance from the generating modelvalues.label eval.
: human eval.
: NB eval.
: SGD0 179 (56%) 52 (65%) 58 (73%)1 24 (8%) 3 (4%) 2 (3%)2 52 (16%) 7 (9%) 2 (3%)3 65 (20%) 18 (23%) 18 (23%)Table 2: Evaluation data obtained with the clus-ter model generated from the human judgementdataset, with the number of clusters defined as 4.1) on the issue of Jerusalem , theyhave shown in a spirit of opennessand a capacity for listening hopeless .2) that is completely disproportionateand it does no favours for thepeace process .Figure 3: Examples of segments classified as ?bad?
(1) and ?good?
(2) by all the post-editors of theexperiment described in Section 4.2.5 LimitationsMethod robustness would imply that the grammarrelationships under test are known, or that the pre-diction algorithm reacts well to unseen data.
Fig-ure 4 presents an example that shows how typeddependencies of two related sentences (namelyreference and MT output of the same source) andthe word usage itself can be scarcely related, ornot overlap.
This highlights that we cannot as-sume training coverage of the typed dependenciesin the test segment, even if the contained wordsare present in the training set in multiple gram-matical contexts.
This stresses the importance ofthe scaling requirement and the complexity reduc-tion measures stated in Section 3, in order to traindiverse grammatical instance variants.
A further19aspect to consider with the Na?
?ve Bayes formula-tion is that the model defines a likelihood for eachentry conditioned on an unconditional class prob-ability, which is correlated to the ratio of ?bad?
and?good?
grammar relationships present in the train-ing set.
This information usage decreases robust-ness, as the model captures quality information ofthe MT instance, which can be subject to variabil-ity (e.g.
the language pair, MT instance setup).This is a lie : the accounts have been fiddled.DT VBZ DT NN : DT NNS VBP VBN VBNrootnsubjcopdetparataxisdetnsubjpassauxauxpassIn fact , it is not true because even accounting tricks.IN NN , PRP VBZ RB JJ IN RB NN NNSrootpobjprepnsubjcopneg preppobjadvmodnnFigure 4: Human reference and raw MT outputderived from the test subset of Europarl FR-ENcorpus, which show typed dependency relation-ships, the words, and related part-of-speech tagsthat highlight possible word usage variance6 Related & Future WorkIn order to estimate MT output quality, literaturein the past has traditionally compared an auto-matically translated sentence to one or more hu-man text references (Papineni et al., 2002), whileother work has exploited unlabeled dependenciesin order to take into account legitimate grammat-ical or lexical choice variations (Liu and Gildea,2005).
Other work improves the classificationeffectiveness of the latter by considering typeddependencies (Owczarzak et al., 2007).
Somedata-driven, referenceless evaluation approachesto learning human judgement have been intro-duced (Corston-Oliver et al., 2001), which ex-ploit syntactic features and linguistic indicators(Gamon et al., 2005), but have also been com-bined with typed dependency features (He andWay, 2009).
Estimation of post-editing effort isa growing concern addressed by Confidence Es-timation (CE) (Specia, 2011), but so far, to thebest of our knowledge, work within the domainperforms supervised learning of statistical linguis-tic features (Felice and Specia, 2012), but not ofdependency features, i.e.
the main focus of thiscontribution.
Previous quality estimation meth-ods differ in nature from the presented view, giventhat they attempt to predict a discrete level of post-editing effort (Bojar et al., 2013), more subject toannotation subjectivity, or to perform binary clas-sification (Hardmeier, 2011), but do not focus onsegment reusability estimation.
Future work willfocus on testing the hypothesis modeling and fea-ture extraction for scalability on larger context-abstract data, and verifying the distinguishabilityof predicted values from more human-annotatedjudgements using the method stated in Section 4.2.Time gain values have not yet been acquired giventhe unusability of the time productivity metricscurrently favored, which do not exhibit direct cor-relation with real PE time, and are also focus offuture investigations.
Futhermore, evaluation onthe WMT Quality Estimation Shared Task datasetswill be performed, for comparisons with state ofthe art methods of post-editing effort quantifica-tion (Bojar et al., 2013).7 ConclusionsThe presented pilot study proposes a grammar-based analysis for categorizing MT output in termsof whether it is an aid or a hindrance to the post-editor.
Our contributions are mainly the (i) use oftyped dependency learning for binary evaluationof confidence estimation and (ii) the analysis ofadequate algorithmic solutions to achieve its scal-ability and context abstraction.
Preliminary resultsshow that aggregation of predictions operated at atyped dependency level provide an evaluation thatresembles the segment-level judgement displayedby post-editors.
Futhermore, for the hypothesismodels created on the dataset tested, Na?
?ve Bayesoutperformed Stochastic Gradient Descent withhinge loss for Support Vector Machine in terms oftraining efficiency, and is on a par regarding clas-sification effectiveness.
We have showed the pre-liminary advantages of typed dependency-basedestimation in terms of context abstraction, whichprovides a novel type of assistance to human post-editors and correlates with post-editing cost ratherthan commonly analyzed linguistic metrics.ReferencesOnd?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-20shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.L?eon Bottou.
2010.
Large-scale machine learningwith stochastic gradient descent.
In Proceedings ofCOMPSTAT?2010, pages 177?186.
Springer.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A machine learning approach tothe automatic evaluation of machine translation.
InProceedings of the 39th Annual Meeting on Associa-tion for Computational Linguistics, pages 148?155.Association for Computational Linguistics.Koby Crammer and Yoram Singer.
2002.
On the learn-ability and design of output codes for multiclassproblems.
Machine Learning, 47(2-3):201?233.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.
Association for Com-putational Linguistics.Mariano Felice and Lucia Specia.
2012.
Linguis-tic features for quality estimation.
In Proceed-ings of the Seventh Workshop on Statistical MachineTranslation, pages 96?103.
Association for Compu-tational Linguistics.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level mt evaluation without refer-ence translations: Beyond language modeling.
InProceedings of EAMT, pages 103?111.Irving John Good.
1965.
The estimation of probabil-ities: An essay on modern Bayesian methods, vol-ume 30.
MIT press Cambridge, MA.William W Hager and Hongchao Zhang.
2005.
A newconjugate gradient method with guaranteed descentand an efficient line search.
SIAM Journal on Opti-mization, 16(1):170?192.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Christian Hardmeier.
2011.
Improving machine trans-lation quality prediction with syntactic tree kernels.In Proceedings of the 15th conference of the Euro-pean Association for Machine Translation (EAMT2011), pages 233?240.Yifan He and Andy Way.
2009.
Learning labelled de-pendencies in machine translation evaluation.Frederick Jelinek, John D Lafferty, and Robert L Mer-cer.
1992.
Basic methods of probabilistic contextfree grammars.
Springer.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: Learning with many rel-evant features.
Springer.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5.Hans P Krings.
2001.
Repairing texts: empirical in-vestigations of machine translation post-editing pro-cesses, volume 5.
Kent State University Press.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In Proceed-ings of the ACL Workshop on Intrinsic and Extrin-sic Evaluation Measures for Machine Translationand/or Summarization, pages 25?32.Karolina Owczarzak, Josef Van Genabith, and AndyWay.
2007.
Dependency-based automatic evalu-ation for machine translation.
In Proceedings ofthe NAACL-HLT 2007/AMTA Workshop on Syntaxand Structure in Statistical Translation, pages 80?87.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Antti Puurula.
2012.
Combining modifications tomultinomial naive bayes for text classification.
InInformation Retrieval Technology, pages 114?125.Springer.Lorenzo Rosasco, Ernesto De Vito, Andrea Capon-netto, Michele Piana, and Alessandro Verri.
2004.Are loss functions all the same?
Neural Computa-tion, 16(5):1063?1076.Michel Simard, Cyril Goutte, and Pierre Isabelle.2007.
Statistical phrase-based post-editing.Lucia Specia.
2011.
Exploiting objective annotationsfor measuring translation post-editing effort.
In Pro-ceedings of the 15th Conference of the European As-sociation for Machine Translation, pages 73?80.Tong Zhang.
2004.
Solving large scale linear predic-tion problems using stochastic gradient descent al-gorithms.
In Proceedings of the twenty-first inter-national conference on Machine learning, page 116.ACM.21
