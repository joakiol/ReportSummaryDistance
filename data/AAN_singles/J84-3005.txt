Strong Generative Capacity, Weak Generative Capacity,and Modern Linguistic TheoriesRobert C. BerwickMIT Artificial Intelligence LaboratoryCambridge, MA 02139IntroductionWhat makes a language a natural anguage?
A long-standing tradition in generative grammar holds that alanguage is natural just in case it is learnable under aconstellation of auxiliary assumptions about inputevidence available to children.
Yet another approachseeks some key mathematical property that distinguishesthe natural languages from all possible symbol-systems.With some exceptions - for example, Chomsky's demon-stration that a complete characterization f our grammat-ical knowledge lies beyond the power of finite statelanguages - the mathematical pproach as not providedclear-cut results.
For example, for a variety of reasons wecannot say that the predicate is context-free characterizesall and only the natural anguages.Still another use of mathematical nalysis in linguisticshas been to diagnose a proposed grammatical formalism astoo powerful (allowing too many grammars or languages)rather than as too weak.
Such a diagnosis was supposedby some to follow from Peters and Ritchie's demonstrationthat the theory of transformational grammar as describedin Chomsky's Aspects of the Theory of Syntax could speci-fy grammars to generate any recursively enumerable set.For some this demonstration marked a watershed in theformal analysis transformational grammar.
One generalreaction (not prompted by the Peters and Ritchie resultalone) was to turn to other theories of grammar designedto explicitly avoid the problems of a theory that couldspecify an arbitrary Turing machine computation.
Theproposals for generalized phrase structure grammar(GPSG) and lexical-functional grammar (LFG) haveexplicitly emphasized this point.
GPSG aims for gram-mars that generate context-free languages (though there issome recent wavering on this point; see Pullum 1984);LFG, for languages that are at worst context-sensitive.Whatever the merits of the arguments for this restrictionin terms of weak generative capacity - and they are farfrom obvious, as discussed at length in Berwick and Wein-berg (1983) - one point remains: the switch was promptedby criticism of the nearly two-decades old Aspects theory.Much has changed in transformational grammar intwenty years.
Modern transformational grammars nolonger contain swarms of individual rules such as Passive,Raising, or Dative.
The modern government-binding (GB)theory does not reconstruct a "deep structure", does notcontain powerful deletion rules, and has introduced awhole host of new constraints.
Given these sweepingchanges, it would seem appropriate, then, to re-examinethe Peters and Ritchie result, and compare the power ofthe newer GB-style theories to these other current linguis-tic theories.
That is the aim of this paper.
The basicpoints to be made are these:?
Since modern transformational grammars do notcontain the powerful deletion rules available in theAspects theory and need not explicitly reconstruct anunderlying deep structure, they are not immediatelysubject o the Peters and Ritchie results.
Thus the fearsrecently advanced by Bresnan and Kaplan (1982:xli-xlii) or Johnson-Laird (1983: 280) simply do nothold.?
Because modern transformational grammars use tracesto mark the site of displaced constituents, the size ofunderlying structures that need be recovered forlanguage recognition are just linearly larger than theircorresponding surface sentences.
Indeed, it appearsthat deep structures ("D-structures" in the currenttheory) need not be built at all to test grammaticality.?
Modern transformational grammars seem morerestricted than theories like LFG, not less restricted, inthe sense that the agreement predicates available in amodern transformational theory are defined solely overunordered sets of features, rather than, as in the lexi-cal-functional theory, over hierarchical trees.
Agree-ment ("unification") over trees adds extra power to theCopyright 1985 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted provided that thecopies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
To copy otherwise, orto republish, requires a fee and/or specific permission.0362-613X/84/030189-14503.00Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 189Robert C. Berwick Generative Capacity and Linguistic Theorylexical-functional formalism.
The result is that thereare some strikingly unnatural grammars that lexical-functional grammars can describe, but not GB gram-mars.
This result about strong generative capacityshows up on the weak generative capacity side: GBgrammars cannot generate some strictly context-sensi-tive languages that can be easily generated by lexical-functional grammars.This paper is organized as follows.
Section 1 reviews omeof the basic formal and linguistic examples demonstratingthat the excess power of the Aspects theory comes fromunbounded eletion.
It then shows why this power is notpermitted in current transformational theories.
Section 2turns to a general analysis of the power of government-binding grammars.
Section 3 compares the strong andweak generative capacity, of lexical-functional grammarand transformational grammars.
It aims to pinpoint justwhy lexical-functional grammars are more powerful thangovernment-binding grammars.
Section 4 concludes withsome speculations about the precise formal characteriza-tion of natural languages.
More generally, these resultssuggest a different role for the formal analysis of naturallanguages.
Instead of trying to fit natural languages intosome pre-defined mathematical or formal mold, thisrevised strategy aims to discover the properties of naturallanguages first, and then characterize them formally.
Theresults here may be regarded as the first fruits of this stra-tegy, applied to current linguistic theories.1.
Unbounded De let ion ,  Past  and PresentIt has long been recognized that the possibility ofunbounded deletion is at the root of the computationalpower of Aspects tyle transformational theories.
If what amachine must do to recognize whether or not a givensentence (surface string) is in the language generated bysome transformational grammar is to recover its deepstructure, and if deep structures can be arbitrarily largecompared to the surface strings derived from them, thenthe recognition procedures for such languages are not evenrecursive.Before describing Peters and Ritchie's formal charac-terization of this connection between deep structure lengthand the complexity of recognition, it would be valuable togive some insight into just why this connection shouldhold.
Recall that a recursive set is one where membershipin the set can be determined in some finite (thoughperhaps large) amount of time.
Here, the set we have inmind is the language generated by some transformationalgrammar, L(TG); given some sentence s, our job is tocalculate s ~ L(TG) and return a yes or no answer in somefinite amount of time.
Also recall that a set is recursivelyenumerable (r.e.)
if, whenever s is in fact in L(TG), there isa procedure such that the answer yes can come back insome finite amount of time, but if s is not in the language,we have no such guarantee; the procedure could just runforever.The key insight connecting length of deep structure torecursive numerability comes from examining the condi-tions under which a computation could run forever.
If weuse a standard Turing machine model of computation, onething that could happen is that the machine could justkeep using more and more new tape cells, moving a stepeach time.
This could go on forever.
So one way to obtainunbounded computation time is to use unbounded space.If we substitute for the "tape cells" of the Turing machinethe number of embedded s or np cycles in some arbitrarilylarge deep structure, and i f  we must recover this deep struc-ture in order to figure out whether or not the sentence is inthe grammar, then.
we have our correspondence b tweenunbounded deep structures and unbounded time forcomputation.But is this the only way to achieve unboundedly longcomputations?
Why not just have the machine shuttleback forth along some fixed sequence of tape cells, usingthe same space but looping forever?
This is certainlypossible, but in this case one can show that the number ofdistinct machine configurations i bounded above by thecross-product of a fixed number of possible moves times afixed number of possible cell contents.
But this means wecould "shut off" the machine after this number of timesteps (counting each Turing machine move as a tick of theclock), since the machine cannot do anything new afterthis number of moves, t In other words, given an upperbound on the space a machine uses, we can fix an upperbound on the length of time the machine can ever usewithout looping forever.
2In short then, the only way to get non-recursive compu-tations is by using unbounded space.
In the transforma-tional analog, Peters and Ritchie (1973) connectedrecognition complexity to the possible difference in lengthbetween deep structures and surface strings:Let G be a transformational grammar.
Let fc  be thecycling function of G, where fox  is 0 if x is not inL(G), and otherwise is the least number s such that Gassigns x a deep structure with s subsentences, l f f cis bounded by an elementary (primitive) recursivefunction, then L(G) is elementary (primitive) recur-sive.
(In fact, if f6  is linear, then L(G) is in a stillsmaller class.)
If the cycling function is not bounded,then L(G) is not even recursive.It is the possibility of arbitrary deletion that makes asurface sentence arbitrarily "shorter" than its correspond-ing underlying deep structure.
Lapointe (1977), in anexcellent review, sums up the situation:Putnam noted that early theories of transformationsallowed grammars which could generate any r.e.
language(whether ecursive or not).
The chief reason for this wasthat early theories allowed arbitrary deletions and substi-tutions in the course of a derivation.
Arbitrary permuta-tions or copying could never cause a grammar to generatea nonrecursive set, for if w i and ~i+l are  successive steps ina derivation such that Ti+ 1 arises through the applicationi The "clock" takes up a bit of extra space - log space - since it has tocount!2 This standard result may be found in Hoperoft and Ullman (1979:300-301 ).190 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic Theoryof a permutation or copying rule \[from\] ri then .
.
.
thenumber of terminal symbols in ~i+1 will be at least asgreat as \[the number of\] terminal symbols in ~i- But thisproperty, that successive steps in a derivation do not"shrink" in length, is the basic defining characteristic ofcontext-sensitive grammars.
Therefore only the applica-tion of rules which reduce length (that is, deletions andsubstitutions) could cause a grammar to generate a non-CS \[context-sensitive rcb\], and perhaps a nonrecursive,language.
(1977: 228)We can exhibit this result in a compact form.
It is wellknown that any r.e.
set can be described as the homo-morphic image of the intersection of two context-freelanguages (Ginsburg, Greibach\] and Harrison 1967).That is,LO = H(CFL 1 N CFL2)Recall that a homomorphism is simply a "respelling" ofthe symbols of a language.
The key point is that thehomomorphism required here permits the deletion ofunbounded strings of symbols, that is,H(z) =where ~ is the empty string.
In fact, all the proofs demon-strating the power of Aspects-style transformational gram-mars make use of this erasing power in one fashion oranother.
The remainder of this section reviews two ofthese demonstrations in the literature, one by Peters(1973), and one by Kimball (1967).
(Another demon-stration that Aspects-style TGs can generate any r.e.language, given by Salomma (1971), also uses unboundeddeletion, but is similar to Kimball's approach and will notbe discussed here.)
The point of going through the exam-ples in detail is to show exactly how each proof relies onunbounded eletion, and why it is that each does not gothrough under the assumptions of the current govern-ment-binding theory.
The basic reason for the change isthat unlimited erasing or deletion is no longer allowed.Indeed, as the next section will make clear, only a linearamount of erasing is permitted in current heories.
Thisinsight is the key to the analysis of the modern theory.We begin with Peters's 1973 demonstration.
Petersgives a specific example showing just how "large" deepstructures can be associated with "short" surfacesentences.
Again, copying and deletion are the culprits.Peters's example relies on the "Equi np deletion" analysisof sentences such as these:1.
Their sitting down promises to steady the canoe.On this account, such sentences have an underlyingstructure that explicitly reconstructs the missing npsubject of the embedded complement topromise:2.
\[NP Their sitting down\] promises \[s \[NP their sittingdown\] to steady the canoe\].Note that this sentence consists of three S phrases: the rootS and two embedded S phrases (the subject NP of thematrix clause and the subject NP of the complement of theVP).
The subject NP of the VP complement is deletedunder structural identity with the matrix subject NP.
Thisdeletion follows the "recoverability of deletion" constraint.Peters next builds a surface string that has a large associ-ated deep structure by embedding this sentence recursivelyin a construction of the same type, that is, a sentence thathas a matrix subject NP structurally identical to thesubject NP of a verb complement.
But the subject NP hasmore than two S phrases (three).
Given identity betweensubject NP of the matrix and the subject of the comple-ment, it follows that at the level of deep structure thesubject NP of the complement must have the same numberof subsentences as the subject NP of the matrix, here,three.
The hew sentence given below must have a deepstructure with more than 22 = 4 S phrases in all:3.
Their sitting down promising to steady the canoethreatens to spoil the joke.Clearly, as Peters notes, we can carry out this embeddingover and over again.
Each time the number of deep struc-ture subsentences i at least doubled, because of theassumption that the complement NP subject is identical tothat of the matrix subject.
If we let ds(n) be the size of thedeep structure corresponding to such a sentence of lengthn, then we have the inductive formula that ds(n) >2ds(n-1).
If we solve this formula, we find that thenumber of deep structure subsentences grows as an expo-nential function when compared to the length of thesurface string, exactly the sort of sentence that was to beconstructed.
If the sentence recognizer must reconstructthis entire deep structure in order to determine languagemembership, then at least this much space, and hence atleast this much time, will be required, just to write downthe deep structure.Interestingly, the argument does not work undercurrent versions of transformational theory.
The simplereason is that we no longer explicitly copy material toreconstruct a deep structure; in fact, we no longer rebuilddeep structure at all.
In place of the literally duplicatedsubject complement NPs, we have an empty categoryplaceholder, PRO, indexed to the proper antecedent NP asappropriate?4.
\[NP \[NP Their sitting down\], promising \[Pro/to steadythe canoe\]\]j threatens \[Proj to spoil the joke\]Crucially, the indexed Pros are not "nested".
What doesthis mean and why does this matter?
Proj is indexed to theentire matrix subject NP their sitting down promising tosteady the canoe.
But it does not contain as a subpart hePRO corresponding to their sitting down (although it maybe indexed to a subpart of a long antecedent s ring).
Theunderlying predicate-argument structure is fixed withoutbuilding up an explicit representation f antecedents in theembedded clause, what used to be called "deep structure.
"3 The other possibility is that the empty category is a trace, the result ofthe movement of an NP from an argument position like the direct objectof a transitive verb.
Here the empty category is PRO rather than t racebecause the subject NP position in the complement is not governed bytense or the matrix verb, but the reader may safely ignore this detail forour purposes here.Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 191Robert C. Berwick Generative Capacity and Linguistic TheoryBy changing what the representation looks like we haveavoided the problem of exponential space growth.
At eachstep we add just a single new element o the reconstructedstructure, the new PRO.
Our new inductive equation issimply ds(n)=ds(n-1)+l  - a linear increase in the size ofreconstructed forms, compared to the surface sentencelengths.
In fact, if we ignored the bracketing and justcounted the PROs, at each step we also add a new word(the new verb) and the underlying representations arealways just a fixed constant larger than the correspondingsurface sentences.One subtle point still remains.
At each step we add anindexed element, PRO/.
The index itself must grow as thenumber of PROs increases.
If we assume a standard bina-ry encoding, an index of size i will take log2i space to writedown, not the constant space implicitly assumed justabove.
Since log /< i, at worst the space added for eachembedding will be proportional to i. Summed over n possi-ble embeddings, this is at worst n 2 space, not exponentialspace.
In the next section we shall see how this represen-tation of so-called "empty categories" works in general?Peters's example centered on a "natural" example thatexhibited exponential deep structure growth.
We nextturn to a more artificial example, but one showing howarbitrarily large deep structures may be used to generateany r.e.
language.
Kimball (1967) does this by exhibitinga transformational grammar that meets a variant of theGinsburg, Greibach, and Harrison theorem (due toHaines, cited in Kimball 1967: 185).
In brief, Kimballsets up a base context-free grammar to generate two trees,rooted at S 1 and S 2, corresponding to the two context-freelanguages demanded by the homomorphism theorem(CFL i  and CFL2) and a third tree dominating these twothat eventually "simulates" the homomorphism H. S 1dominates a terminal string labeled x and S 2, a terminalstring labeled y.
Dominating these two subtrees is a thirdtree that, besides x and y, dominates a terminal string z.The idea is to use a single transformation to successivelycheck that the first member of x matches the first memberof y and z; if so, this element is erased in x and y.
If allelements match, and we are at the end of z (indicated by aspecial symbol), then the two strings are identical; thisstep carries out the intersection of the two context-freelanguages.
A final transformation performs the requiredhomomorphism.
Figure 1 depicts the overall scheme.
It isimportant to point out that both S 1 and S 2 generatecontext-free languages that are self-embedding, of thegeneral form aicaii.
Thus they must exhibit recursion onsome nonterminal node in the relevant context-free gram-mar.As Kimball notes, the strings x and y are deleted underidentity with z, so nothing is amiss here in the Aspectstheory.
X and y are arbitrarily long.
The underlying"deep structure" (the context-free base) is arbitrarily larg-er than the resulting surface string, namely, some part of zthat remains after the homomorphism does its work.What happens to this example in a modern transforma-tional theory?
The key point is that the modern theorydoes not have a deletion operation like the one justpresented.
Instead, a constituent is moved from one posi-tion to a "landing site" within its own cyclic domain or tothe next higher cyclic domain?
In English, the cyclicnodes are S and NP.
6 When a node is moved, it leavesbehind a trace, denoted e, of the same category as thedisplaced constituent, but with no phonological features.
(Thus the trace is not "pronounced" and does not show upin the surface sentence.)
The trace is co-indexed to thedisplaced constituent, as indicated by a subscript.
Forexample, given the sentence,5.
John bought whatwe could move what, yielding (after some adjustment withthe auxiliary verb),6.
What did John buy e iNow consider Kimball's tree structures again.
Since S 1and S 2 are true recursive sub-trees, in a trace-orientedtheory the way that we would get deletion would be tosuccessively move elements of x and y to higher and higherphrases, leaving behind traces (denoted by ei) as we go.Schematically, oar output structure would have to looksomething like that in figure 2, where R indicates a cyclicnode.
As it stands though, this structure is impossiblebecause it requires traces to be linked to elements that are"too far away": according to a key constraint of themodern transformational theory, the subjaceneyconstraint, the linking can cross at most one cyclic node.Since all recursion must eventually pass through S or NPnodes, subjacency must be violated by the trees pictured infigure 2.
Put another way, the rule that "erases", forexample, x 1, now must move x I across many S or NPnodes, and this movement is not directly possible.
Analternative is to move X i successive cyclically, up the chainof R nodes one step at a time.
But there are only two waysto do this: either we wind up moving more and more nodesat each step - at the n th step we move n nodes, which must"land" at n spots at the next higher cyclic domain - or wecollapse how many nodes we move by adjoining some ofthe moved elements together.
Figure 3 shows both possi-bilities.Both solutions are ruled out in current theories of trans-formational grammar.
The movement of an arbitrarynumber of nodes in a single cycle is impossible because itcalls for an arbitrary number of "landing sites" in domainn+l.
In fact, there can only be a finite number of suchpossibilities, as specified by a set of context-free base rules.For example, we can move an NP to a subject or object4 There is another solution to the index growth problem, one that will berequired later on.
Suppose that each indexed NP or PRO is in effect adistinct element of the grammar's vocabulary.
Thus the grammar allowsa denumerable infinity of "'pre-indexed" elements NPi, NP 2 .
.
.
.
.
Thisis not such a strange proposal, since the index is not used for any syntacticprocess, but simply for co-indexing.
As we shall see, this same proposal ismade, usually implicitly, in most current heories, for example, in thelexical-functional theory.5 In the next section we shall take a slightly different position and defineadmissible annotated surface structures without literal movement.6 For our purposes here, cyclic nodes are those that exhibit recursion.192 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic TheoryTransformation:structural condition (roughly):\[s, Xx\] Yy\] Zzx = y = z; .~, y, z are terminal symbolsstructural change:delete x and yS / \Sl $2 / \CT"LL CFT,2!
Ix yFigure 1.
Kimball's transformation to generate any r.e.
set.position, a wh phrase to a comp position (the position occu-pied by that in I know that Mary likes ice cream).
7But wecannot move n nodes in a single cycle, because there willnot be enough places tO put the moved constituents.The second solution is also ruled out.
Either theadjoined NPs linked to the traces violate subjacency (aspictured), or else we must also adjoin at each step i a copyof the i -1  st trace to the i th trace.
But this last method isalso barred, because we do not admit "nested" traces, ortree structures that dominate some arbitrarily deepsequence of nested empty elements.
In other words, atrace can be co-indexed at its "top-level" to a displacedconstituent, but is otherwise "opaque"; it has no interiorstructure.
There are in fact good linguistic reasons for aprinciple banning nested traces (see Hornstein 1984).Section 3 probes the formal implications of this constraintin more detail.It is hard then to see how the required trace structurelinking x and z could even be built.
But this is just the firststep in Kimball's proof.
Enforcing equality between x andy looks even harder.
Of course, this by no means showsthat there is no way to carry out Kimball's construction,but it does hint at some of the difficulties in a revisedgrammatical framework that does not permit the sameliberties with deletions as Aspects, and does not rely on anexplicitly reconstructed D-structure.2.
The Complexity of Modern TransformationalGrammarAs we have seen, the crux of the problem withAspects-style transformational grammars is deletion, and,more pointedly, the demand to recover unboundedly largedeep structures in order to determine sentence-hood.
Theproofs of intractability all hinge on the assumption thatthe job of the parser is to recover a literal copy of deletedelements.
If this assumption is not needed, then the job ofthe recognizer could well be easier.
The modern theoryrequires only the recovery of a trace-  or PRO-augmented7 See the next section for more on " land ing  sites.
"structure, an "annotated surface structure".
This makes adifference.
As Lapointe (1977) shows, it makes the recog-nition problem for such languages recursive.
Whateverthe merits of their arguments on other grounds, Lapointe'sresult renders moot Bresnan and Kaplan'sconcerns (1982:xli) about the non-recursiveness of transformational theo-ry, since their criticisms apply only to the older Aspectstheory.
This is our first conclusion.Much more than this can be said.
If the recognizerdoes not have to recover full deep structures, then its jobcould be much easier, as observed by Peters and Ritchie1973:Putnam proposed that the class of transformational gram-mars be defined so that they satisfy a "cut-elimination"theorem.
We can interpret his rather broadly to meanthat for for every grammar G 1 in a class there exists 2 suchthat (i) L(G 1) = L(G2) and (ii) there is a constant k with theproperty that for every x in L(G2), there is a deep hrase markerq~ underlying x with respect to G 2 such that l\[d(q~)\] < kx.
(1973: 81-82)Here, the notation l(x) stands for "length of", while d((~) isthe "debracketization" of the deep structure.
Thedebracketization consists of terminal elements sans rightand left brackets, but with traces and PROs.
As Petersand Ritchie go on to say:We now see that any grammar satisfying such a cut-elimi-nation theorem generates a language which more thanbeing recursive is context sensitive.
This is so because anondeterministic l near bounded automaton can determineboth that a labeled bracketing 4 is strongly generated bya context sensitive grammar and that it underlies a givenstring $x$ if the automaton has enough tape to write 4.
(1973:82)How would such a linearly-bounded recognizer work?Roughly, it would use a kind of "analysis by synthesis":given a sentence of length n, it would mark out a length ofinput tape kn, k a constant depending on the transforma-tional grammar.
The machine would be guaranteed thatannotated surface structures could not get larger than this.Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 193Robert C. Berwick Generative Capacity and Linguistic Theory/ S ~ I  z2  ?
?
.
ZnS1,, $2~R R/ \e.
/'e.R R/~en-  1 en-- 1. .
.
/  .
.
.
/R Rel ~etFigure 2.
Traces and the Kimball construc~tion.The machine would then use its nondeterministic power to"guess" all possible annotated surface structures less thanor equal to this length, now with the proviso that one ofthem must be a correct underlying structure if thesentence in question is in fact in the language generated bythe grammar.
Since the number of (NP or S) cycles ineach structure is bounded, we may simply try all possibletransformational rules (again nondeterministically) toproduce possible surface sentences, one at a time.
If thereis a match, then the sentence is in the language; if allstructures less than our bound are tested an fail, then thesentence is not in the language, sWhat then of modern transformational grammar?
Weclaim that D-structure need not be reconstructed at all todetermine grammaticality.
This may be a surprise forsome readers accustomed to the older picture of a transfor-mational grammar, where annotated surface structure isjust the result of mapping from D-structure under theoperation of Move a.
But it is nonetheless true.
Chomsky(1981: 91ff.)
observes that annotated surface structuresmay be simply defined with respect o certain admissibilityconditions (more on these shortly) without regard to anactual movement rule that maps from one level toanother.
9Our goal, then, will be to assume that only annotatedsurface structure is built to test grammaticality.
"~ Wemust now define more carefully just what annotatedsurface structure is in the current GB theory.
We thenshow that these representations are at most linearly largerthan their corresponding surface sentences.We begin simply by describing the set of admissibleannotated surface structures without reference to D-struc-ture.
That is, we define the set of annotated surface struc-tures statically, in the manner that Joshi and Levy (1977)define a set of admissible tree structures.
Roughly, theannotated surface structures of a given grammar are justthe set of all well-formed labeled bracketings produced bythe constraints of X theory plus the restrictions imposedby lexical subcategorization, plus bracketings where emptycategories appear in certain positions, governed by a fixedset of conditions.
In more detail, the well-formed anno-tated surface structures are defined inductively asfollows: ~(l): Following standard assumptions, constraints alongwith locality conditions on subcategorization togeth-er yield a system describable by a context-free gram-mar (see, e.g., Gazdar and Pullum 1981).
All NPsdominate some lexical material and correspond inone and only one way to the A positions, argumentssubcategorized by the relevant verbs, again followingthe method outlined by Joshi and Levy (1977); thepositions in English are: adjacent o the verb, for anobject NP; first NP under S, for subject NP; first NPunder PP for oblique PP, and so forth.
~2 Further, allsuch lexical NPs must appear in argument (A) posi-tions, where the notion of an argument position againdepends in a strictly local way on the verb (e.g., thesubject position of seem in English is not an argu-ment position).
Finally, we allow a finite number ofspecified lexical deletions (of particular words), suchs The details of the testing procedure are not given here, but may ofcourse add some fixed space to the kn bound required to write down theannotated surface structures.9 At least, this seems to be so for all cases in English.
But a note of quali-fication is required.
There may be subtle examples showing that D-struc-ture must be explicitly rebuilt in order to test grammaticality.
Suchexamples do not seem to arise in English, but they may in otherlanguages, uch as Italian.
So for example, it may be in Italian that thegrammaticality of such examples as was built a house may demandexplicit reference to D-structure, in order to determine whether a verb isa real passive or merely adjectival.
If so, then the conclusions in the maintext might not hold, since D-structure would have to be built.to Note that this is true of the Marcus parser (Marcus 1982).II Even this account is incomplete in some details, ignoring certain alter-native formulations of the theory.
But these defects can be repaired atthe cost of adding more or slightly different clauses to the definition.
Forexample, we omit a discussion of clitics, verb movement, governmentdefined as mutual c-command, or Subject-Verb agreement.
This lastconstraint may be defined via lexical insertion contexts, following Chom-sky (1965) as formalized by Joshi and Levy (1977).12 Note that all these constraints are readily checked in the manner ofJoshi and Levy (1977).194 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic TheoryCase 1: successive lements of individual nodesn + 1 st cyc le ::T I X2  ?
?
?
Xnnth cyc le : / /  /enCase 2: adjunctionn + 1 st cycle:RR, -1en / \] :rr(- l ~Xn.
2/Rien-2.
.
.
.
ilit /e lFigure 3.
Successive cyclic trace-based Kimball analyses.as of, you, in any single phrase, as long as no otherconstraints are violated.
All labeled bracketingsmeeting these conditions are well-formed annotatedsurface structures.
(II): Any of the structures of (I) with empty categories(e.c.
's) replacing NPs, subject o the following condi-tions, are well-formed annotated surface structures:(i) Every such e.c.
is an atomic constituent with anumerical index and with no internal bracket-ing;(ii) If the e.c is governed by some X ?
(lexicalelement such as verb, noun, and so on), whereX governs Y iff the first branching node domi-nating X dominates Y, and there is no interven-ing maximal projection (full phrase) between Xand Y then:1. the e.c.
must be c-commanded by an NPantecedent(= element with the same numer-ical index), where c-command is definedjust as government but dropping the clauseabout maximal projections; and2.
the antecedent is either a lexical NP oranother e.c.
in a non-argument (A) position(the complement of the A positions definedabove); and3.
the e.c.
must be subjacent o that antece-dent, where subjacency is defined as usual.
(iii) Else, the e.c.
is ungoverned (is a "sc PRO") andcan receive an arbitrary index.
~3(III): Any of the structures defined by (I) and (II), and, inaddition, with a wh phrase in COMP position c-com-manding a governed e.c., or another wh phrase inCOMP position and with the same index as thatother e.c.
or phrase, is a well-formed annotatedsurface structure.
(IV): Any of the structures defined by ( I ) - ( I I I ) ,  and, inaddition, with one of those structures with an e.c.having an index the same as that of an elementadjoined to VP (following Baltin 1982), and c-com-manded and subjacent o that element, is a well-formed annotated surface structure/4 There can beat most one such adjoined position.
(V): Any of the structures defined by ( I ) - ( IV)  conjoinedso as to meet Williams's (1978) Across the Board(ATB) conventions is a well-formed annotatedt3 Subject to constraints dictated by "control" theory, that is.
We ignorethis matter here by assuming an arbitrary index for PRO; this does notbear in any essential way on the description of the possible annotatedsurface structures.
Neither c-command nor subjacency seem required forcontrol; hence this may fall under whatever mechanism it is that inter-prets the indices of ordinary pronouns generally, a matter we leaveoutside scope of annotated surface structure.t4 Note that the position so adjoined to VP is not part of the obligatoryargument structure mentioned by the verb.Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 195Robert C. Berwick Generative Capacity and Linguistic Theorysurface structure.
~5 Without going into detail on theATB constraint, its effect is to place e.c.
's in aconjunct if its verb or verb phrase is missing; the e.c.is bound to a c-commanding antecedent, as before.In the case of more complex reduced conjunctions(The meat is ready to heat, serve, and eat) the miss-ing constituent sequence may be represented by asingle e.c.
in each missing position, though alterna-tive analyses are to be preferred here.
~5~ (VI):Nothing else is a well-formed annotated surfacestructure.We also need some technical constraints.
As pointed outabove, we must assume that the actual index of an NP (asdenoted by a subscript) takes up no extra space beyond theconstant storage required for a distinct nonterminal name.Otherwise the amount of space required to write down anannotated structure of length proportional to $n$ could beat worst proportional to n logn, where the logn factor isused to hold the number of the index n. To sidestep thisproblem we assume a denumerable infinity of distinct NP"names".This same assumption must be made explicitly orimplicitly in any theory that assumes co-indexing but stillstrives for linearity in the size of underlying structures.Consider Kaplan and Bresnan's sketch (1982:263-267)that lexical-functional language recognition uses only line-ar space.
In the full description of lexical-functionallanguages, names are distinguished as co-referential ornot.
Thus, two occurrences of, say, Mary  must be distinct.In the LFG formalism, this is indicated by subscripts.
(Seefor example Kaplan and Bresnan's ' examples1982:225-227.)
But then, this means that the sheer size ofa functional structure (f-structure), the lexical-functionalanalog of an annotated surface structure, could be of sizen logn, again with logn space for the indices.
Just writingdown one f-structure could take more than linear space.Kaplan and Bresnan do not say in any detail just how theyintend to check for sentence-hood using just linear space,but since all of their descriptions involve building anf-structure, we may assume that at least this much spacewill be required.
In short, in order to a linear space bound,Kaplan and Bresnan need to adopt exactly the proposalmade above.A second key assumption is that traces may not be nest-ed; a trace cannot contain another trace.
This ban isrequired because otherwise we could build a tree represen-tation containing just empty elements (the traces).
Sincea tree can be arbitrarily large, a single NP or S domaincould have an arbitrarily large but surface-empty struc-ture of elements, just what was to be avoided.
We are nowready to state just what we want to show.Theorem Let G be a government-binding grammar, andL(G) the language it generates.
Let AS i be the anno-tated surface structure) associated with sentence w i inL(G).
(If  there is more than one such annotated surfacestructure, then AS i is a set of annotated surface struc-tures; AS i is a singleton set if there is just one annotatedsurface structure.)
Then there is a constant k such thatfor al l  sentences w i in L(G), and for all annotatedsurface structures AS i underlying w i, \]ASi\[ <_ k\[wi\[.The proof proceeds by induction on the number of cycles(S or NP domains) in an annotated surface structurecorresponding to a sentence in L(G).
First we shall showthat the length of a one-cycle annotated surface structureis linearly proportional to its corresponding surfacesentence.
This will be easy, since within a single cycle (Sor NP domain), there can be movement to at most a fixednumber of "landing sites" as defined above: the ~ posi-tions, plus COMP, plus one adjunct to a VP.
The lexicalentry for a verb mentions only a finite number of sucharguments.
The one additional landing site adjoined to VPcan receive only one phrase, because in order to receivemore, additional phrases would have to be adjoined in themanner of the Kimball-type structures discussed in theprevious ection.
But these would violate subjacency?
6Once we have established linearity in the base case, wenow look at annotated surface structures i and i+ l  cyclesdeep.
Assuming that structures of depth i maintain linear-ity, we show that those of depth i+ l  do also.
This step istedious, since one must go through the possible ways toobtain the i+ l  cycle from the one preceding it, one by one.The landing site analysis is exploited here, as is subjacen-cy.
The empty category analysis is also used.
Subjacencyhelps because there is no way to "skip" a cycle, construct-ing structures of depth i+2 from those of depth i directly.ProofBasis step.
i = 1 (bottom cycle, no embedded sentences orNPs.)
Given a surface sentence wi, we consider the lengthof the corresponding annotated surface structure.
Let s =the length of the surface sentence.
There are four cases.Case 1.
No e.c.
's in the S or NP cycle, and no specifiedlexical deletions.
Assume a context-free base with nouseless nonterminals or cycles, and with rules where thelength of the longest righthand side is p. If m = thenumber of nonterminals in the derivation of a sentencein this grammar, then m _< cs for some fixed positiveinteger ?, as may be easily verified by induction.
Inaddition, to write down the annotated surface structure,we must add two bracket labels for each nonterminalsymbol.
Thus IASi\] = 2m + s _< 3cs.
Note that if wewanted to establish a relationship between debracketedannotated surface structures and surface strings, thenthis last step would be unnecessary.Case 2.
A finite number of specified lexical deletions with-in this cycle, e.g, of, as in, all o f  the people ~ all  thepeople, or an imperative (if a root sentence).
Let thet5 For a more recent formulation of the ATB conventions as the l inearunion of phrase markers, see Goodall (1983).
We note in passing that thephrase marker union also preserves l inearity of annotated surface struc-tures.t6 Recall that now we are applying subjacency as a static constraint onannotated surface structures.
In fact, since in the basis step we consideronly annotated surface structures one S or NP  cycle deep, this case doesnot arise.196 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic Theorymaximum number of these deletions be K. Then IASil_< 3cs + 3K.
For s >_ 1, 3cs + 3K _< 3cs + 3Ks =(3c+3K)s. Let c r = 3c + 3K.
Then IASil _< c's.
Again,we can omit the 2K factor for the debracketed case.Case 3.
Empty categories within this (S or NP) cycle, withantecedents in the same S or NP.
There are a finitenumber of such positions, as described earlier: only NPargument positions (thematically marked by the verb);or the adjoined position to VP.
Let C bound thisnumber from above.
Then IASil < cPs + C; using thesame approach as in case 2, the righthand side of thisinequality is less than c's.
Clearly, combinations ofcases 2 and 3 cause no problems because we can add aconstant number of deletions together with theconstants obtained from within cycle e.c.
's to obtain anew constant factor.Case 4.
Empty categories in the cycle without antecedentsin that domain.
If an empty category exists in an S orNP without an antecedent (NP, wh, etc.)
in thatdomain, then clearly the corresponding surface string isshorter than the corresponding annotated surface struc-ture, since it does not include the empty categorysymbols.
However, again the addition of each emptycategory symbol adds just one to the total annotatedsurface structure length, and there are at most a finitenumber of such positions (~'positions, uch as COMP,as described earlier).
Therefore, the correspondingannotated surface structure is just a constant longerthan the corresponding surface sentences, as in Case 3.Well-formed annotated surface structures exhibitingthe features of gapping, VP deletion, and conjunctionreduction do not show up at this step, since theycombine two i level cycles into an i+ l  domain.
Theyare considered in the induction step.
This completes thebasis step.Induction step.
Suppose that up through cycle i we havethat IASil _< ksi, where s i is the terminal ength at the ithcycle, and k is a constant.
We now must show that thisrelation holds for structures of depth i+1.
There are fivepossibilities.Case 1.
No empty categories athe top level of cycle i+1.Then the terminal string associated with this cycleconsists of two parts, whatever terminals are introduceddirectly by nonterminals in cycle i+ 1 and new elementsof cycle i+1 bound to e.c.
's in cycle i.
But there are afinite number of empty category sites for material inthe current domain, by the definition of a well-formedannotated surface structure.
Call this number C. Bythe inductive hypothesis, any of these constituentsthemselves meet the condition that their annotatedsurface structures are bounded above by a linear multi-ple of their terminal strings.
Thus the total annotatedsurface structure for the current cycle is at most Ctimes the bound on previous cycles, plus a constant oaccommodate he length of terminals introduced irect-ly in cycle i+1.i d t ASi+ 1 <_ d\]~j= 1 AS j  +?
i Zt\] IASj  < C ~j=l sjSubstituting, we obtain:i ttASi+ 1 < C Zj=I sj + k si+ 1-< ksi+ 1Case 2.
Specified deletions in cycle i+ l .
If there are afinite number of specified lexical deletions, this is justlike the basis case.
This case includes the introductionof PROs.
PRO can appear in a finite number of newpositions in cycle i+ l  (the Subject position, if ungov-erned).Case 3.
E.c.
's with antecedents within cycle i+1.
Thedemonstration proceeds as in the basis case.Case 4.
E.c.
's with antecedents in cycle i+2.
Again, likethe basis case.
This cannot change the linearity bound.Case 5.
Annotated surface structures with empty verb,verb phrase, and coordinate reduction positions.
This isthe only new situation that arises in the induction stepas opposed to the basis step.
Suppose we have aconjunct formed by deleting material from each of nconjuncts.
An example is the meat is ready to take outo f  the fridge, heat, and serve.
The example is fromRounds (1975:137) attributed to E. Bach.
If suchconstructions involved actual recovery of deleted deepstructure material, then problems could arise.
Theliteral material would have to be copied, and we couldget a linearity-violating Peters-type sentence.But this problem can be avoided with an interpretiveapproach governed by the "across the board"conventions of Williams (1978).
We supply indices, notactual copied material, for the well-formed annotatedsurface structure.
The ATB constraint lines up theconjuncts to be co-ordinated, one under the other.
Forexample, a sentence like the meat is ready to heat, serve,and eat is factored as follows, where we have deletedduplicate material in other conjuncts.The meat/is ready to heat e i1 serve 3eat2We can represent term (2) as an unordered set of lexi-cal items, for example, heat, eat, serve.
Plainly, thisrepresentation cannot be more than linearly larger thanthe surface sentence?
7Similar results hold for empty categories linked to verbsand verb phrases.
Each cyclic domain of the the associ-ated annotated surface structure contains a constantnumber of empty VP "gaps", denoted \[e\]; there can beat most one main verb, VP, or auxiliary verb sequence17 Again, the Goodall (1983) representation would be suitable here.Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 197Robert C. Berwick Generative Capacity and Linguistic Theoryper cyclic domain.
Therefore, the total number of gapsin the conjoined structure is bounded from above by aconstant imes Si+l, the length of the terminal string.This exhausts the range of possible cases, completing theinduction and the proof.The linearity demonstration shows restricting deletionshas a powerful effect on the weak generative capacity of atransformational grammar.
The implications of thatresult for linguistic description are discussed in the nextsection.3.
The Root of Complexity: kexicaI-FunctionalGrammars and GB grammarsThe results of the previous section show something aboutthe weak generative capacity of modern transformationalgrammars.
The study of weak generative capacity is notan end in itself, however.
In the best case, we would likeweak generative capacity to be a kind of diagnostic aid totell us that something is amiss with a linguistic theory.We would like our theory to be able to describe all andonly the natural languages.
A theory could fail to do thisin two ways, either in terms of weak generative capacity orin terms of strong generative capacity.
A theory that istoo powerful could generate ither unnatural tree struc-tures (and so be too powerful in terms of strong generativecapacity) or it could generate unnatural sentences (and betoo powerful in terms of weak generative capacity).
I f  weare interested in the rule systems (grammars) that underlylinguistic behavior, then it is ultimately strong generativecapacity that is of interest.
Still, weak generative capacitycan help here to point the way to excess strong generativecapacity.
We will also not want to stop at diagnosis.
Wealso want to determine just why a particular theory cangenerate too many languages - what the source of itsexcess power is.
We saw that with Aspects transforma-tional grammars the additional power lay with unbridleddeletion.
What of other recent heories of grammar?In this section we shall present an example of exactlythis kind.
This will be a language that is presumably not anatural anguage.
We will use this language as a "probe"into the power of current linguistic theories.
We shall seethat this language can be easily generated by lexical-func-tional grammar, but not by a GB grammar.
More impor-tant, this weak generative result has a strong generativecapacity reflex.
We can use this result to locate the excesspower of the LFG system.
This could be of value indiscovering restrictions for the LFG system.
In terms ofstrong generative capacity, the more important goal, weshall see that the LFG theory has the ability to defineunification predicates over hierarchical tree structures,something unavailable in the GB theory.
This extension ofthe traditional definition of linguistic predicates has impli-cations for the ability of LFG to describe unnatural gram-mars, not just unnatural languages.Here is what we mean to show in more detail.
LFGs usea particular kind of unification machinery (describedbelow) in order to account for well-formed sentence struc-tures of Dutch (Bresnan, Kaplan, Peters, Zaenen 1982).This unification procedure is central to the construction ofthe grammatical structures of lexical-functional theory.But it is also powerful enough to describe grammars quiteunlike any natural grammatical system.
By changing theDutch LFG only slightly we can produce a rule system thatallows "object control" via a preceding NP (as in Marypersuaded John to leave) just in case the NP in questionand the controlled position are equally deeply embedded.This we take to be an unnatural rule system.To begin, we present our artificial "diagnostic"language, the power of 2 language, L 2 = {aili is a power of2}.
L 2 is a lexical functional language, since the followinglexical-functional grammar generates it:1.
A --,- A A( t f~  = ~ ( i .
/ )  =2.
A~ a(I J) = 1The (I f )  = ,I, functional structure constraints on thenonterminals enforce the restriction that the same numberof A expansions be taken on each subtree; expansions aresymmetric all the way down the "words", the as.
Thisguarantees a power of 2 expansion; the details are left tothe reader.We can now ask deeper questions.
First, why can lexi-cal-functional grammars generate such languages?
Moreon this shortly.
Second, can L 2 be generated by a GBgrammar?
To answer the second question first, theanswer here seems to be no, because of a property of GBlanguages that is violated by L 2, namely, the constantgrowth property, defined and discussed for tree adjunctgrammars by Joshi (1983).
This property will only bebriefly explored below; for more complete remarks, seeBerwick and Weinberg (1984).I f  we arrange the sentences of L 2 in order of increasinglength, we see that they become farther and farther apart.In fact, for any fixed set of constants C, we can alwaysfind a sentence of Le, w i, say, such that there is no wj inL2, with Iwi\[ = Iwjl + c, for c ?
C. We state this propertyas follows:Definition.
A language L is said to possess the constantgrowth property (or be constant growth) if and only ifthere exists a constant M and a set of constants C suchthat for all sentences wk ?
L with Iwkl > M, there existsanother sentence in L, wkr, such that w k is at most aconstant longer than Wkt, Iwkl = IWk' I ?
c, for c ?
C. Agrammar is said to possess the constant growth propertyiff the language it generates i constant growth.
~8Lexical-functional grammars, then, are not constantgrowth.
In contrast, government-binding grammarscannot generate such languages because they are constantgrowth.
Intuitively, the demonstration works much likethe linearity proof.
For a full discussion, see Berwick and18 So far as it can be now determined, constant growth seems to be apurely mathematical property of natural languages that has no clear"'functional" reflection.
Presumably, constant growth is a derivative ofother, deeper properties of natural anguages.198 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic TheoryWeinberg (1984: Appendix A).
The point is that nogovernment-binding grammar can generate L 2 or anynonconstant growth language.E9What is it that gives lexical-functional grammars theirability to define languages like L2?
LFGs can testcomplete subtrees for compatibility.
At a dominatingnode we can check whether an entire hierarchical struc-ture is feature compatible with another structure?
Thisfollows from the account of functional structure unifica-tion defined by Kaplan and Bresnan (1982).
Functionalstructures are hierarchical in nature; they are directed,acyclic graphs.
Functional structure well-formedness idefined by the condition of functional structure unique-ness.
Roughly speaking, there can be no conflicts in theassignment of feature complexes, even if those features arein fact hierarchical structures.This kind of feature compatibility test goes well beyondthat required for the checking of "ordinary" agreement, asin subject-verb number agreement.
When we test asubject and verb for agreement, all that we do is check anunordered list of features for compatibility?
The number,gender, and so forth of the subject NP must agree withthat of the verb, as percolated through the VP.
It is a farcry from this kind of agreement checking to the"agreement" of two entire tree structures, but this is whatis implied by the lexical-functional unification procedure) ?As we saw in our earlier example, this unificationprocedure is sufficient to generate the power of 2language?
A natural question to ask then is whether thisability to compare ntire functional structures i necessary.For if all cases of functional structure unification can bereplaced by unordered feature agreement tests, then thereis no motivation for adopting the more powerful mech-anism, at least on these grounds.The lexical-functional theory is, perhaps, alreadycommitted to the ability to test hierarchical functionalstructures for compatibility.
For functional structures arecertainly hierarchical in nature.
They must encode thehierarchical relationships between root and embeddedpropositions, for example?
A functional structure is usedas the input to semantic interpretation, and so must reflecthierarchical dependencies.
Otherwise we cannot decipherthe relationships in a complex sentence like John expectedMary to persuade Bill to win.
The feature checkingmachinery must be designed to test for functional struc-ture compatibility because that is the only level of repre-sentation where features like the number of the subject areto be found?
But once we permit feature checking of func-tional structures at a single,'unembedded l vel for thenumber of a subject NP, it is hard to see how we can rule itout for a more complex functional structure.In fact, lexical-functional researchers have proposednatural anguage cases where one must check one complexfunctional structure for compatibility against another.19 Another example is the language of perfect squares.20 Several other theories also adopt a directed, acyclic graph notation forfeatures, among these, Kay's (1982) unification grammar and Shieber's(1983) PATR I1 formalism.
Interestingly, Sag et al (1984) adopt themore restricted view of features.Just such a case has been discussed by Bresnan, Kaplan,Peters, and Zaenen (1982), in the analysis of certainDutch sentences.
We will not review all the details of theirproposal here except o establish the point that hierarchi-cal functional structure comparisons are crucially impli-cated.
The data Bresnan et al want to account for is this.Dutch contains infinitely many sentences of the followingsort (examples from Bresnan et al 1982: 614):?
.
.
dat Jan de kinderen zag zwemmen?
.
.
that Jan the children saw swim?
.
.
that Jan saw the children swim?
.
.
dat Jan Piet Marie de kinderen zag helpen latenzwemmen?
.
.
that Jan Peter Marie the children saw help makeswim?
.
.
that Jan saw Piet help Marie make the childrenswimThese Dutch sentences must have a certain constituentstructure?
Their proposed structure consists of twobranching "spines", one a right branching tree of VPscontaining objects and complements, the other a rightbranching tree of V containing verbs without their objectsand complements.
Every verb uses its lexical argumentstructure to demand certain NP objects or that the verbcomplement's subject be controlled by the verb's object orsubject?
For example, the verb zag demands that its objectcontrol the subject of zag's verbal complement?
This isanalogous to the English case where the object of a verb,for example, persuade, controls the subject of persuade'scomplement, as in, We persuaded John to leave.The lexical-functional system encodes this agreement innumber of verbs and NPs by forcing an identificationbetween the functional structure of the object of zag andthe functional structure of the verb complement of zag(denoted VCOMP).
The "equation" is written (~ VCOMPSUB J) = (q OBJ).
The problem, of course, is that if wehave three verbs then we have three such constraints, butthe associated NPs that satisfy them lie along a distinct VP"spine" of the constituent structure tree that is separatedfrom the verbs along the V spine?
In other words, the"control" equations are built up along the rightmost, Vspine of the constituent structure tree, but the NPs thatsatisfy these equations lie along the left side.
How can weassemble the NP functional structures for proper checkingagainst the control equation demands?
Because featurechecking can occur only at some common dominatingmother node, the first place where all elements are"visible" to each other is at the first VP node completelydominating both right and left subtrees.
The way thatBresnan et al accomplish this task is to build up along therightmost subtree a functional structure representationthat encodes all of the control equations, in the form of ahierarchical functional structure with unfilled slots for thesubjects and objects mentioned by the controlling verbs.Note that the structure is indeed hierarchical, containingembedded components?
Along the lefthand side of theconstituent tree Bresnan et al build up a second hierarchi-Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 199Robert C. Berwick Generative Capacity and Linguistic Theorycal functional structure that "merges" successfully intothe righthand one just in case the number of NPs and theirassignment to controlled positions meshes with the "slots"left remaining in the righthand functional structure.One must build and check a hierarchy of featuresbecause in order to encode the possibility of an arbitrarynumber of controlled NPs below the dominating VP node,we must adopt some means of encoding a potentially arbi-trary number of features (denoting each of the NPs andtheir associated verbs).
But given that the functionalstructure "equations" annotating the underlying context-free grammar are fixed once the grammar is written down,the only way to do that is by building up some recursivestructure that mimics the constituent structure derivationas a chain.
(With only a finite number of features, we canonly encode an infinite number of different cases by meansof chains or trees.)
This means that Bresnan et al areforced to adopt hierarchical feature checking as the meansto describe the Dutch sentences.In contrast, the government-binding theory representsthe same pairing of NPs and verbs via a "flat" co-indexingscheme.
Jan de kinderen zag zwemmen would be roughly,NP/ NP 2 \[p" V 1 V2\] in annotated surface structure (seeEvers 1975 and Berwick and Weinberg 1984).
Asoutlined in Berwick and Weinberg (1984), potential co-in-dexings can be evaluated by non-erasing pushdown trans-ductions that test only single, unanalyzed nodes, neverbuilding up tree-structured features as in the lexical-func-tional grammar example.
(Note again that D-structuresare not reconstructed tocarry out this Check.
)The problem for the lexical-functional machinery isthat once hierarchical checking is admitted for this oneexample, there is nothing to bar it in other cases.
But thenthe power of 2 language can be generated.
One can alsobuild "unnatural" lexical-functionat grammars using justthe linguistically motivated control equation apparatusand phrase structure rules proposed in the lexical-func-tional theory.
The same linguistically motivated rulesused for Dutch, combined in slightly different ways, leadto grammars quite unlike anything ever attested or likelyto be attested in natural rule systems.
The example wegive uses almost precisely the Dutch control equations, anda slightly different context-free base.The idea behind our unnatural grammar is this.
Wewill build a grammar where a verb controls a higher objectNP just in case both the verb and that NP are essentiallyequally deeply embedded along different "spines" of theconstituent structure tree.
This we take to be a highlyunnatural system.
There is no natural language where acontrol property "counts".We need these context-free rules and their functionalstructure annotations:1.
VP --,- NP V (V)( f sub j )=+ ( fVcomp)=~ f=2.
VP --- NP(f obj) =m3.
V -~ V V(f Vcomp) = I,4.
V --- V5.
NP -,- NRules (2)-(5) are precisely those used by Bresnan et alRule (1) is different.
( I )  has the associated equation (fsubj) = ~ attached to the NP node instead of the equation(f obj) = 4.
We must also add new lexical entries for thefollowing "verbs":V3: (f Vcomp subj) =(f pred) =V2: (4' pred) =VI: (f pred) =(t' obj)V3((f subj)(t' ob)('l' Vcomp))"V2((4' subj)(q Vcomp))Vl(('t subj)('l' Vcomp))The effect of this modest change is a rule system that hasexactly the properties we claimed.
Consider first the func-tional structure built up along the lefthand VP branchingspine.
The last NP expansion will have the associatedequation (4' obj) = 4.
Each VP demands that the VCOMPfunctional structure component associated with the nodeabove it be identified with the functional structure built upat that VP.
The effect is to build up a hierarchicalarrangement of VCOMP functional structures, one forevery VP node that is generated except for the top and thebottommost vP.
In addition, a subject functional struc-ture component is passed up from all NPs but the last one.The object from the lefthand functional structure merg-es into this righthand structure successfully if and only if ithas one level of embedding less than the righthand struc-ture.
This is our desired result.
Otherwise the objectstructure cannot be laid on top of the righthand structureand overlap properly; it must coincide with the emptyobject slot on the righthand side.
2~4.
The Formal Characterization of NaturalLanguagesSummarizing the analysis so far, we have seen just howmodern transformational theories differ formally fromtheir older counterparts.
We have also seen that thatdifference is reflected as a weak and strong generativecapacity difference between the new theory and the lexi-cal-functional theory.Some questions are still unanswered.
In the previoussection, we came to a partial diagnosis of the source of theextra power of the lexical-functional theory.
In thissection we would like to pin down that diagnosis.
At thesame time we shall offer a different perspective on theformal characterization of natural anguages.
This analy-21 For example, suppose we interchanged V 2 and V 3.
Then the controlverb V 3 is less deeply embedded than the object NP it is supposed tocontrol.
This structure should be ruled out, and it is.
The lefthand func-tional structure will be as before.
But now the righthand functional struc-ture will not merge properly with the lefthand functional structurebecause that functional structure demands that the object be embeddedinside two VCOMPs, whereas the righthand structure calls for an objectembedded inside just one.
Similarly, if V 3 were embedded one more leveldown, the number of VCOMPs would not match.
Only when the numberof embeddings i  the same (plus one) on both left- and righthand sides isthe structure well-formed.200 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984Robert C. Berwick Generative Capacity and Linguistic Theorysis will necessarily be more speculative.
Still, it is hopedthat the discussion will provoke a fresh look at how to goabout the mathematical nalysis of natural anguages.To begin, let us recall that the suspected source of extrapower in the lexical-functional theory is the unificationprocedure defined over hierarchical structures (constituentstructures).
We also argued that nothing like this kind ofpower is required to describe natural languages.
In thissection we shall investigate this claim more deeply~ Weshall look at one case, co-ordination, that might seem torequire full unification, and see that in fact hierarchicalunification is not required.At first glance, co-ordination would seem to demandsome kind of unification predicate.
The reason is thatco-ordinate structures obey a familiar principle (Williams1978) that permits only "similar" conjuncts to be linked.
22One way to visualize the parallelism constraint is to imag-ine the two conjuncts being laid ~n top of one another.
Ifthey match, then the conjunction is permitted, otherwise,it is not permitted.
Williams (1978) formalizes this condi,tion.
This process is reminiscent of the lexical-functionalunification procedure.
(Compare it to the Dutch examplegiven earlier.)
Here too, we "overlaid" two hierarchicalstructures to determine well-formedness.
The Dutchsentences were legal just in case two hierarchical spinescould be so overlaid, or unified.
Is unification required?On closer inspection the analogy with unificationbreaks down.
It is true that the parallelism of co-ordinateconjuncts demands a match in terms of phrasal nodes.
Thekey difference between lexical-functional unification andthe co-ordination constraint is that co-ordinate parallelismneed only hold at the top level of a phrasal sequence.Internal details of the matched conjuncts do not matter.This is in contrast o the unification predicate, which, asthe Dutch example shows, can demand a hierarchicalmatch.
For example, the following conjunction is perfectlygrammatical, even though the conjoined VPs are internallydifferent, one containing an Adjectival Phrase and theother a Noun Phrase (example from Goodall 1983): thebouncer was muscular and was a guitarist.
One can evenconjoin active and passive sentences (John went to Bostonand was taken for  a ride).
As Goodall (1983) demon-strates, one way to describe this effect is as the union ofthe top level of phrasal nodes (actually, phrase markers).In contrast, the Kaplan and Bresnan unification proce-dure (1982: 272), as defined by their statement (190c),recursively defines a union over what may be an entiretree:(190) c. If e I e 2 are both f-structures, let A 1, A 2 be setsof attributes e!
and e2, respectively.
Then a new f-struc-ture e is constructed with e = {(a,v) I a ?
A 1 U A 2 and v= merge \[Locate I(el,a)\], Locate \[(e2,a)\] \]} (Locate is anoperator that actually finds the sub-f-structure withthe specified attribute structure.
)Here, (a, v) is the union of a hierarchical attribute set,since this last step is carried out recursively to all levels ofstructure.
This means that there is nothing to stop us fromwriting a co-ordination rule in the lexical-functionalsystem that demands equality in tree structure through alllevels of hierarchical detail, contrary to what is observed.
23We might speculate then that a general property ofconstraint statements in natural anguages i that they aredefined in terms of predicates on linear sequences of struc-tures (phrase markers), rather than by hierarchicallydefined unification predicates.
It remains to explore justwhat this restriction comes to, but  it is clear that this isexactly where and how lexical-functional grammardiverges from the "classical" view of generative grammar.The classic view, outlined in Chomsky's Logical Structureo f  Linguistic Theory, defined predicates in terms of aconcatenative algebra at each of several levels of repr6sen-tation (phonetic, syntactic, and so forth).
The details arenot essential here, but one property of these algebras is:they fixed predicates in terms of linear sequences ofelements, rather than trees.
24 The lexical-functionalsystem extends the power of representational descriptionto include the possibility of unification predicates definedover nonlinear constituent structures.
While this violationof the usual syntactic adjacency restrictions (observedfrom earliest days of generative grammar) is certainlysufficient to describe natural languages, the examplespresented here show that it is not necessary.This diagnosis also tells us one way to repair the lexi-cal-functional theory.
One could restrict he lexical-func-tional theory to ban hierarchical unification predicates.One way to do this is to simply eliminate the recursive stepof Kaplan and Bresnan's unification procedure (190c),(1982:272) excerpted earlier.
For example, feature merg-er could be restricted to operate over just two cyclic (S orNP) domains.
One would still need a way to handleconstructions like those in Dutch, or, should they be neces-sary, the ww constructions.
Of course, it may be that otherrestrictions suffice.Whatever the outcome of these changes, a more generalquestion for future work centers on the status of theconcatenation algebras underpinning traditional genera-tive grammar.
While there has been some formal work inthis area (see Borgida 1983 and Berwick 1982), it remainsto be seen whether the linear predicates presupposed bysuch a model do indeed characterize what it means to be anatural grammar.
If they do, then extensions to moregeneral unification predicates, as in LFG, unificationgrammar, or PATR-II, may well be unwarranted.AcknowledgmentsMuch of this research as been sparked by collaborationwith Amy S. Weinberg.
Thanks to her for manydiscussions on GB theory.
Portions of this work haveappeared in The Grammatical Basis o f  Linguistic Perform-22 See Sag et al (1984) for a different formulation of "similar".23 This is true also of respectively t pe constructions: evidently, only thelinear union of phrase markers is required to define these sentences, notthe tree-checking power used by Kaplan and Bresnan (1982: 269-271).24 This is true even of linear phrase markers ystems that directly admitdiscontinuous constituents, which Chomsky's system does not; theseinclude McCawley's (1982) proposal and Higgenbotham's (I 983) recentelaboration of McCawley (1982).Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 201Robert C. Berwick Generative Capacity and Linguistic Theoryance .
The research as been carried out at the MIT Arti-ficial Intelligence Laboratory.
Support for theLaboratory's work comes in part from the AdvancedResearch Projects Agency of the Department of Defenseunder Office of Naval Research contractN00014-80-C-0505.ReferencesBaltin, M. 1981 A Landing Site Theory of Movement Rules.
LinguisticInquiry 13: 1-38.Berwick, R. 1982 Locality Principles and the Acquisition of SyntacticKnowledge.
PhD dissertation, MIT Department of Electrical Engi-neering and Computer Science, Cambridge, Massachusetts.Berwick, R. and Weinberg, A.
1982 Parsing Efficiency, ComputationalComplexity, and the Evaluation of Grammatical Theories.
LinguisticInquiry 13: 165-191.Berwick, R. and Weinberg, A.
1984 The Grammatical Basis of Linguis-tic Performance.
MIT Press, Cambridge, Massachusetts.Borgida, A.
1983 Some Formal Results about Stratificational Gram-mars and Their Relevance to Linguistics.
Mathematical Systems Theo-ry 16: 29-56.Bresnan, J. and Kaplan, R. 1982 Introduction: Grammars as MentalRepresentations of Language.
In: Bresnan, J., Ed., The Mental Repre-sentation of Grammatical Relations.
MIT Press, Cambridge, Massa-chusetts: xvii-lii.Bresnan, J.; Kaplan, R.; Peters, S.; and Zaenen, A.
1982 Cross-serialDependencies in Dutch.
Linguistic Inquiry 13:613-636.Chomsky, N. 1955 The Logical Structure of Linguistic Theory.
PlenumPress, New York, New York, 1975.Chomsky, N. 1981 Lectures on Government and Binding.
Foris Publica-tions, Dordrecht, Holland.Evers, A.
1975 The Transformational Cycle in Dutch and German.PhD dissertation, Department of Linguistics, Rijksuniversiteit,Utrecht, Holland.Ginsburg, S.; Greibach, S.; and Harrison, M. 1967 One-way StackAutomata.
Journal of the Association for Computing Machinery 14:389-418.Goodall, G. 1983 Coordination.
Unpublished raft of thesis, Universityof California at San Diego.Higgenbotham, J.
1983 A Note on Phrase Markers.
Revue Qudbdcoisede Linguistique 13( 1 ): 147-166.Hopcroft, J. and Ullman, J.
1979 Introduction to Automata Theory,Languages, and Computation.
Addison-Wesley, Reading, Massachu-setts.Hornstein, N. 1984 Logic as Grammar.
MIT Press, Cambridge, Massa-chusetts.Johnson-Laird, P. 1983 Mental Models.
Harvard University Press,Cambridge, Massachusetts.Joshi, A.
1983 Some Formal Results about Tree Adjunct Grammars.Proceedings of the 21st Annual Meeting of the Association for Compu-tational Linguistics.Joshi, A. and Levy, L. 1977 Constraints on Local Transformations.?
SIAM Journal of Computing 6: 272-284.Kaplan, R. and Bresnan, J.
1982 Lexical Functional Grammar: AFormal System for Grammatical Representation.
In: Bresnan, J.,Ed., The Mental Representation f Grammatical Relations.
MIT Press,Cambridge, Massachusetts: 173-28 I.Kay, M. 1982 Unification Grammai'.
Xerox PARC unpublished ms.Kimball, J.
1967 Predicates Definable by Transformational Derivationsby Intersection with Regular Languages.
Information and Control I 1:177-195.Lapointe, S. 1977 Recursiveness and Deletion.
Linguistic Analysis 3:227-265.Marcus, M. 1982 A Theory of Syntactic Recognition for NaturalLanguage.
MIT Press, Cambridge, Massachusetts.McCawley, J.
1982 Parentheticals and Discontinuous ConstituentStructure.
Linguistic Inquiry 13: 1.Peters, S., 1973 On Restricting Deletion Transformations.
In: Gross,M.
; Halle, M.; and Schutzenberger, M., Eds,, The Formal Analysis ofLanguage.
Mouton, The Hague, Holland: 372-384.Peters, S. and Ritchie, R. 1973 On the Generative Power of Transfor-mational Grammars.
Infortaation Sciences 6: 49-83.Pullum, G. 1984 Syntactic and Semantic Parsability.
Proceedings ofColing84, Stanford, California: 112-122.Rounds, W. 1975 A Grammatical Characterization f the ExponentialTime Languages.
Proceedings of the 16th Annual Symposium onSwitching and Automata Theory: 135-143.Salomma, A.
1971 The Generative Capacity of TransformationalGrammars of Ginsburg and Partee.
Information and Control 18: 227-232.Sag, 1.; Gazdar, G.; Wasow, T.; and Weisler, S. 1984 Coordination andHow to Distinguish Categories.
CSLI Report CLSI-84-3.Shieber, S. 1983 Notes on the PATR-II formalism, SRI International,Menlo Park, California.Thiersch, C. 1978 Topics in German Syntax.
PhD dissertation, MITDepartment of Linguistics and Philosophy, Cambridge, Massachu-setts.Williams, E. 1978 Across-the-board Rule Application.
LinguisticInquiry 9:31-43.202 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
