Recent Progress in Robust Vocabulary-Independent Speech RecognitionHsiao-Wuen Hon and Kai.Fu LeeSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213AbstractThis paper eports recent efforts to improve the performanceof CMU's robust vocabulary-independent (VI) speech recog-nition systems on the DARPA speaker-independent r sourcemanagement task.
The improvements are evaluated on 320sentences that randomly selected from the DARPA June 88,February 89 and October 89 test sets.
Our first improvementinvolves more detailed acoustic modeling.
We incorporatedmore dynamic features computed from the LPC cepstra ndreduced error by 15% over the baseline system.
Our secondimprovement comes from a larger training database.
Withmore training data, our third improvement comes from a moredetailed subword modeling.
We incorporated the word bound-ary context into our VI subword modeling and it resulted in a30% error eduction.
Finally, we used decision-tree allophoneclustering to find more suitable models for the subword unitsnot covered in the training set and further educed error by17%.
All the techniques combined reduced the VI error rateon the resource management task from 11.1% to 5.4% (andfrom 15.4% to 7.4% when training and testing were under dif-ferent recording environment).
This vocabulary-independentperformance has exceeded our vocabulary-dependent p rfor-mance.IntroductionAs speech recognition flourishes and new applicationsemerge, the demand for vocabulary-specific training willbecome the bottleneck in building speech recognizers.
Ifsuccessful, a vocabulary-independent (VI) speech recogni-tion system trained on a large database alleviates the te-dious vocabulary-specific training process.
We have previ-ously demonstrated the feasibility of vocabulary-independentspeech recognition systems \[4, 5\].
Although the vocabulary-independent results improved as the training data increases,the best vocabulary-independent r sult previously reportedwas still about 30% worse than the vocabulary-dependent(VD) result.
In this paper, we will report recent efforts to fur-ther improve CMU's robust vocabulary-independent speechrecognition systems on the DARPA speaker-independent r -source management task.Our first improvement involves the incorporation of moredynamic features in theacoustic front-endprocessing \[7\].
Ourprevious vocabulary-independent experiments have used onlyfirst order differenced cepstra nd power.
Here, we add secondorder differenced cepstra nd power.
We also incorporateboth40 msec and 80 msec differenced cepstraa.
These new featuresyielded a 15% error rate reduction, about the same as wasachieved on vocabulary-dependent tasks \[7\].Our second improvement involves the collection of moregeneral English data, from which we can model more pho-netic variabilities, such as the word boundary context.
Ourexperiment shows that adding 5,000 sentences to an original15,000 sentence training set gives only a 3% error reduction.In this experiment, the set of models was fixed.Next, we incorporated word boundary context into our VIsubword modeling, which resulted in a surprising 30% errorreduction.
This compares with only a 20% error reductionobtained in the vocabulary-dependent case.
In the past, it hasbeen argued that between-word triphones may be learninggrammatical constraints instead of modeling acoustic vari-ations.
This result shows the contrary, since in vocabulary-independent experiments, grammars in the training and recog-nition are completely different.With more detailed models (such as between-word tri-phones), coverage on new tasks was reduced.
To deal withthis problem, we proposed a new decision-tree based sub-word clustering algorithm to find more suitable models forthe subword units not covered in the training set \[11\].
Thesequestions were first created using human speech knowledge,and the tree was automatically constructed by searching forsimple as well as composite questions.
Finally, the tree waspruned using cross validation.
When the algorithm termi-nated, the leaf nodes of the tree represented the generalizedallophones to be used.
This tree structure not only could findsuitable models for subword units never observed before, butit also enables moothing with all ancestor nodes instead ofonly the context-independent one.
In a preliminary experi-ment, we found that decision-tree based allophones made 17 %fewer errors than generalized triphones.We have found that different recording environments be-tween training and testing (CMU vs. TI) degrades the per-formance significantly \[4\], even when the same microphoneis used in each case.
In \[4\], we found the vocabulary-258independent system suffered much more from differencesin the recording environments at '17 versus CMU than thevocabulary-dependent system.
However, with the abovetechniques, the vocabulary-independent system became morerobust to the changes in recording environment than thevocabulary-dependent system.
Our results now show thevocabulary-independent system works about 11% better thanvocabulary-dependent sys em under cross-condition recogni-tion.These techniques implemented on the vocabulary-independent system led to more than 50% error reductionon both same recording and cross recording conditions.
Theymade our vocabulary-independent system 13% better than ourvocabulary-dependent sys em on the resource managementtask.In this paper, we will first describe our recent efforts onCMU's vocabulary- independent speech recognition system,including incorporating more dynamic acoustic feature, largertraining database, word boundary context, and decision treeallophone clustering.
Then we will describe our experimentsetup and present results.
Finally, we will close with someconcluding remark about his work and future work.More Detailed Acoustic ModelTemporal changes in the spectra re believed to play an im-portant role in human perception.
One way to capture thisinformation is to use differenced coefficients \[3, 12\] whichmeasure the change of coefficients over time.
Our previ-ous vocabulary-independent experiments have used only threecodebooks, the first codebook for cepstrum coefficients, thesecond codebook for differenced cepstrum coefficients (40msee) and the third codebook for power and differenced power(40 msec) \[4\].In additional to first order differencing, it has recently beenshown that adding second order differenced coefficients fur-ther enhances performance \[7\].
Thus, we added the fourthcodebook with second order cepstrum coefficients.
We alsoincorporated both 40 msec and 80 msec differenced cepstrumcoefficients into the second codebook and power, differencedpower (40 msec), and second order differenced power intothe third codebook.
(For detailed implementation, see \[7\]).These new features reduced the error rate on the vocabulary-independent system from 11.1% to 9.4 % and therefore yieldeda 15% error reduction, about he same as was achieved on thevocabulary-dependent sys em \[6\].Larger Training DatabaseIn previous work \[4\], we showed the vocabulary-independentresults improved ramatically as the vocabulary-independenttraining increased.
(The error rate was reduced 45% whenVI training database was increased from 5,000 sentences to15,000 sentences).
Therefore, we continue collecting moregeneral English database and hope to improve VI results frommore VI training.In addition to the TIM1T (3,300 sentences), Harvard(19,00 sentences) and old general English (10,000 sentences)databases used in the previous experiments, we add 5,000more general English data into our vocabulary-independenttraining set.
The database covered about 22,000 differentwords and 13,000 different triphones (not counting inter-wordtriphones).
While the word coverage on DARPA resourcemanagement task was only improved from 57% to 60%, theintra-word triphone coverage was improved from 90.0% to93.6%.
We first clustered the 13,000 different triphone mod-els down to about 2,200 by using an agglomerative clusteringalgorithm \[10\] and trained on those 2,200 generalized triphonemodels.
However, we only obtained a small improvement, re-ducing the error ate from 9.4 % to 9.1% (a 3 % error eduction ),when the training database increased from 15,000 sentencesto 20,000 sentences.We conjectured the current subword modeling techniquemay have reached an asymptote, so that additional sentencesare not giving much improvement.
If this is correct, we needto make our subword models more detailed with the growingdatabase.Between-Word TriphoneBecause our subword models are phonetic models, one wayto model more acoustic-phonetic detail is to incorporate morecontext information, e.g.
stress, word-boundary context, syl-lable position, etc.
We have already incorporated the stressinto vocabulary-dependent and vocabulary-independent sys-tems and did not get any improvement \[4, 11\].
It might bebecause lexical stress does not predict sentential stress well.As suggested by the incorporation of word boundary con-text into triphone modeling in vocabulary-dependent sys ems\[9, 14\], we decided to do between-word triphone modeling onour vocabulary-independent system by adding three more con-texts, word beginning, word ending and single-phone wordpositions.
The incorporation of word boundary context in-creased the number of triphones on the VI training set from13,000 to 33,500 and reduced the triphone coverage on re-source management task from 93.6% to 90.0%.
We used thesame clustering algorithm to cluster those 33,500 triphonesdown to 2,600 generalized triphones.
The between-word tri-phone modeling enables us to reduce the error rate of thevocabulary-independent system from 9.1% to 6.5%, which isabout 29% error reduction.This result is surprising when compared to only a 20% er-ror reduction in the vocabulary-dependent system \[8\].
Inthe past, it has been argued that between-word triphonesmight be learning grammatical constraints instead of mod-eling acoustic-phonetic variations.
This result shows the con-259trary, since in vocabulary-independent systems, grammars inthe training and recognition are completely different.Decision Tree A l lophone C lus ter ingAs indicated in the previous ection, with more detailed mod-els (between-word triphone models), coverage on new taskwas reduced.
For example, the triphone coverage on resourcemanagement was reduced from 93.6% (only intra-word tri-phones) to 90.0% (incorporated inter-word triphones).
Thismeans for 10% of the phones in the dictionary, we couldn'tfind suitable generalized triphone models, and were forcedto used the monophone model.
This could hurt the system'sperformance.To deal with this problem, we proposed a new decision treebased subword clustering algorithm \[11, 2, 1, 15\].
At the rootof the decision tree is the set of all triphones corresponding to aphone.
Each node has a binary "question" about heir contextsincluding left, right and word boundary contexts (e.g., "Is theright phoneme a back vowel?").
These questions are createdusing human speech knowledge and are designed to captureclasses of contextual effects.
To find the generalized triphonefor a triphone, the tree is traversed by answering the questionsattached to each node, until a leaf node is reached.
Figure 1 isan example of a decision tree for the phone/k / ,  along withsome actual questions. ?Left = vowel7 Left = Vowel?~wel?Right ~ ..~ Schwa?
y~c?Figure 1: An example of a decision tree that clusters theallophones of the phone / k /The metric for splitting is a information-theoretic distancemeasure based on the amount of entropy reduction when split-ting a node.
We want to find the question that divides node minto nodes a and b, such thatP(m) H(m) - P(a) H(a) - P(b) H(b) is maximizedCH(z) = - ~p(cl~r) log e(elz)Cwhere H(z) is the entropy of the distribution i HMM modelx, P(z) is the frequency (or count) of a model, and P(elz )is the output probability of codeword c in model x. Thealgorithm to generate a decision tree for a phone is givenbelow \[2\]:1.
Generate an HMM for every triphone.2.
Create a tree with one (roo0 node, consisting of alltriphones.3.
Find the best composite question for each node.
(a) Generate a tree with simple questions at eachnode.
(b) Cluster leaf nodes into two classes, represent-ing the composite question.4.
Split the node with the overall best question.5.
until some convergence riterion is met, go to step3.If only simple questions are allowed in the algorithm, thedata may be over-fragmented, resulting in similar leaves indifferent locations of the tree.
Therefore, We deal with thisproblem by using composite questions \[1, 13\] (questions thatinvolve conjunctive and disjunctive combinations ofall ques-tions and their negations).
A good composite question isformed by first growing a tree using simple questions only,and then clustering the leaves into two sets.
Figure 2 showsthe formation of one composite question.O OFigure 2: The use of simple-question clustering to form acomposite questionTo enhance the ability the decision tree clustering to predictthe suitable classes for new triphones, we grew the tree alittle further and pruned the tree by cross-validation with anindependent set \[2\].
Two thirds of the VI training data wasused to train the triphone models and the models were thenused to grow the trees.
Finally, the other set of triphonemodels trained from the remaining one third of training datawas used to prune the trees.The tree structure not only could find suitable mod-els for subword units never observed before, but also en-260able smoothing with all ancestor nodes instead of onlycontext-independent o e in the traditional generalized tri-phone scheme which used agglomerative clustering algo-rithm.
Thus we expected the decision tree based clusteringwould perform better than other algorithms in vocabulary-independent systems.
In a preliminary experiment, the useof decision tree based generalized triphones rather than tradi-tional generalized triphones reduced the error rate of the VIsystem from 6.5% to 5A% (a 17% error reduction).In \[4\], we found that decision tree based clustering workedonly marginally better than agglomerative clustering.
Thesignificant improvement here is due to three reasons: (1)improved tree growing and pruning techniques, and (2) ourmodels in this study are more detailed and consistent, whichmakes it easier to find appropriate and meaningful questions,and (3) triphone coverage is lower in this study, so decisiontree based clustering is able to find more suitable models.Experiments and ResultsAll the experiments are evaluated on the speaker-independentDARPA resource management task.
This task is a 991-wordcontinuous peech task and a standard word-pair grammarwith perplexity 60 was used throughout.
The test set con-sists of 320 sentences from 32 speakers (a random selectionfrom June 1988, February 1989 and October 1990 DARPAevaluation sets)For the vocabulary-dependent (VD)system, we used the thestandard DARPA speaker-independent database which con-sisted of 3,990 sentences from 109 speakers to train the sys-tem under different configurations.
The baseline vocabulary-independent (VI) system was trained from a total of 15,000 VIsentences.
5,000 of these were the 'lIMIT and Harvard sen-tences and I0,000 were General English sentences recorded atCMU.
We have shown that different recording environmentsbetween training and testing degrades the performance signif-icantly \[4\].
While the VD training set were recorded at TI, theVI training set were recorded at CMU.
Therefore, we recordedat CMU another exactly test set from 32 speakers (differentfrom TI speakers), each speaking 10 sentences ( ame as theTI sentences), toillustrate the influence of different recordingenvironments.
From now on, we use"CMU test set" to denotethe test set recorded at CMU and "TI test set" to denote thetest set recorded at TI.In the baseline systems, both the VD and VI systems onlyused 3 codebook and intra-word generalized triphones.
In thefirst experiment with more acoustic dynamic features, bothVD and VI used 4 codebook configuration and got roughlythe same improvements.
After that, we added 5,000 moregeneral English sentences to the VI training set.
We thenincorporated inter-word triphones into both VD and VI sys-tems.
The VI system was improved more than the VD system.Finally, we used decision tree based generalized triphones onboth VD and VI systems.
As we expected, the decision treeclustering further improved the VI system by finding moresuitable models for subword units never observed in VI train-ing set.
Decision tree clustering did not improve the VDsystem since all the triphones were covered in the VD system.Table 1 shov,s the recognition error ate for these xperimentswhen training and testing are under the same recording envi-ronments.
The VI systems was tested on CMU test set andthe VD systems was tested on TI test set.
Note that the lastcolumn showed the percentage ofthe increase of error ate forthe VI system in comparison with the VD system.
With theabove techniques, the final VI system was better than the VDsystem.Configuration VD VIBaseline 8.6% 11.1%+ 4 codebooks 7.5 % 9.4%+ 5,000 sentences (VI) 7.5% 9.1%+ inter-word triphones 6.0% 6.5%+ decision-tree clustering 6.2% 5.4%Increase inError Rate+29.0%+25.3%+21.3%+8.3%-12.9%Table 1: The VD and VI results under the same recordingconditionThe recording environment difference isunavoidable in thevocabulary-independent speech recognition system becausethe system is trained only once, and must be applied to anyapplications which could take place in other different environ-ments.
In \[4\], we found the VI system suffered much morefrom cross environment recognition than the VD system.
Ta-ble 2 showed the cross environment recognition for both theVD and VI systems.
That is, the VI systems were tested on TItest set and the VD systems were tested on CMU test set.
Wefind that the VI system became more robust o the changesin recording environment than the VD system when the VIsystem had more training data and better subword models.
Atlast, the VI system also performed better than the VD systemunder cross recording condition.Configuration VD VI Increase inError RateBaseline 10.8% 15.4% +42.6%+ 4 codebooks 10.1% 13.7% +35.6%+ 5,000 sentences (VI) 10.1% 12.7% +25.7%+ inter-word triphones 8.1% 8.0% - 1.2%+ decision-Wee clustering 8.3% 7.4% - 10.8%Table 2: The VD and VI results under the cross recordingconditionFinally, we tested the last two systems(incoporating inter-261word triphones and decision-tree clustering) on the no-grammar recognition.
Table 3 showed both results underthe same or cross recording conditions.
Like the recognitionwith word-pair grammar, the use of decision tree clusteringalgorithm reduced the error rate of VI system from 27.8% to22.8%(a 18% error eduction) under the same recording con-dition and also made the best VI system better than the besstVD system under the same and cross recording conditions.Configuration VD VIw/o decision-tree (same) 24.5% 27.8%w/o decision-tree (cross) 29.2% 30.8%w decision-tree (same) 25.2% 22.8%w decision-tree (cross) 29.9% 28.1%Increase inError Rate+13.5%+5.5%-9.5%-6.0%Table 3: The VD and VI results for no-grammar recognitionConclusionsIn this paper, we have presented several techniques that sub-stantially improve the performance of CMU's vocabulary-independent speech recognition system.
These techniques,including more dynamic features in acoustic modeling, moretraining data, more detailed subword modeling (incorporat-ing the word boundary contexts) and decision tree allophoneclustering, led to more than 50% error eduction on both samerecording and cross recording conditions.
This also made ourvocabulary-independent system better than our vocabulary-dependent system on the resource management task underboth conditions.In the future, we expect o further extend some of theseareas.
We will enhance our subword units by modeling moreacoustic-phonetic variations, e.g., contexts further than leftand right contexts, and function word contexts, etc.
Currently,since the use of composite questions might lead to some unrea-sonable combinations of simple questions, we would like torefine and constrain the type of questions which can be askedto split the decision tree.
We would also like to reduce thetraining data for the decision tree based generalized allophonesystem and demonstrate the smoothing power and generaliz-ability of decision tree because itwould reduce the coverageof the vocabulary-independent sys ems for new tasks.Although the vocabulary-independent recognition resultson cross recording condition were improved a lot when wehad more training data and better subword modeling, there isstill a non-negligible d gradation for cross recording condi-tion.
In the future, we will implement some environmentalnormalization techniques to further improve the performanceof cross environment conditions.
Moreover, we would alsolike to implement some rapid and non-intrusive task adapta-lion to make the vocabulary-independent system tailored tothe individual task.To make the speech recognition system more robust fornew vocabularies and new environments are essential to makethe speech recognition application feasible.
Our results haveshown that plentiful training data, careful subword model-ing, and decision tree based clustering have compensatedfor the lack of vocabulary and environment specific train-ing.
We hope with the additional help of environmental nor-realization and non-intrusive task adaptation, the vocabulary-independent system can be tailored to any task quickly andcheaply.AcknowledgementsThis research was sponsored in part by US West and in partby the Defense Advanced Research Projects Agency (DOD),Axpa Order No.
5167, under contract number N00039-85-C-0163.
The authors would like to express their gratitudeto Professor Raj Reddy and CMU speech research group fortheir support.References\[1\]\[2\]\[3\]\[4\]\[5\]\[6\]\[7\]Bahl, L., Brown, R, de Souze, R, and Mercer, R. A Tree-Based StatisticalLanguage Model for Natural LanguageSpeech Recognition.
IEEE Transactions on Acoustics,Speech, and Signal Processing, vol.
ASSP-37 (1989),pp.
1001-1008.Breiman, L., Friedman, J., Olshen, R., and Stone, C.Classification and Regression Trees.
Wadsworth, Inc.,Belmont, CA., 1984.Furui, S. Speaker-Independent Isolated Word Recogni-tion Using Dynamic Features of Speech Spectrum.
IEEETransactions on Acoustics, Speech, and Signal Pro-cessing, vol.
ASSP-34 (1986), pp.
52-59.Hon, H. and Lee, K. On Vocabulary-lndependentSpeechModeling.
in: ICASSP.
1990.Hon, H., Lee, K., and Weide, R. Towards Speech Recog-nition Without Vocabulary-Specific Training.
in: Pro-ceedings of Eurospeech.
1989.Huang, X., Alleva, E, Hayamizu, S., Hon, H., andLee, K. Improved Hidden Markov Modeling forSpeaker-Independent Continuous Speech Recognition.in: DARPA Speech and Language Workshop.
Mor-gan Kaufmann Publishers, San Mateo, CA, 1990.Huang, X., Lee, K., Hon, H., and Hwang, M. ImprovedAcoustic Modeling with the SPHINX Speech Recogni-tion System.
in: ICASSP.
1991.262[8] Hwang, M. Personal Communication.
unpublished,1988.
[9] Hwang, M., Hon, H., and Lee, K. Modeling Between-Word Coarticulationin Continuous Speech Recognition.in: Proceedings of Eurospeech.
1989.
[10] Lee, K. Context-Dependent Phonetic Hidden MarkovModels for Continuous Speech Recognition.
IEEETransactions on Acoustics, Speech, and Signal Pro-eessing, April 1990.
[11] Lee, K., Hayamizu, S., Hon, H., Huang, C., Swartz,J., and Weide, R. Allophone Clustering for ContinuousSpeech Recognition.
in: ICASSP.
1990.
[12] Lee, K., Hon, H., and Reddy, R. An Overview of theSPHINX Speech Recognition System.
IEEE Transac-tions on Acoustics, Speech, and Signal Processing,January 1990.
[13] L.R., B. and et.
al.
Large VocabularyNaturalLanguageContinuous Speech Recognition.
in: ICASSP.
1989.
[14] Pieraccini, R., Lee, C., Giachin, E., and Rabiner, L. Im-plementation Aspects of Large Vocabulary RecognitionBased on lntraword and Interword Phonetic Units.
in:DARPA Speech and Language Workshop.
1990.
[15] Sagayama, S. Phoneme Environment Clustering forSpeech Recognition.
in: ICASSP.
1989.263
