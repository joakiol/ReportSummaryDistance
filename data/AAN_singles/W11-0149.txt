Semantic Relatedness from Automatically Generated SemanticNetworksPia-Ramona Wojtinnek and Stephen PulmanOxford University Computing Laboratory{pia-ramona.wojtinnek,stephen.pulman}@comlab.ox.ac.ukAbstractWe introduce a novel approach to measuring semantic relatedness of terms based on an auto-matically generated, large-scale semantic network.
We present promising first results that indicatepotential competitiveness with approaches based on manually created resources.1 IntroductionThe quantification of semantic similarity and relatedness of terms is an important problem of lexicalsemantics.
Its applications include word sense disambiguation, text summarization and information re-trieval (Budanitsky and Hirst, 2006).
Most approaches to measuring semantic relatedness fall into oneof two categories.
They either look at distributional properties based on corpora (Finkelstein et al, 2002;Agirre et al, 2009) or make use of pre-existing knowledge resources such as WordNet or Roget?s The-saurus (Hughes and Ramage, 2007; Jarmasz, 2003).
The latter approaches achieve good results, but theyare inherently restricted in coverage and domain adaptation due to their reliance on costly manual acqui-sition of the resource.
In addition, those methods that are based on hierarchical, taxonomically structuredresources are generally better suited for measuring semantic similarity than relatedness (Budanitsky andHirst, 2006).
In this paper, we introduce a novel technique that measures semantic relatedness based onan automatically generated semantic network.
Terms are compared by the similarity of their contextsin the semantic network.
We present our promising initial results of this work in progress, which indi-cate the potential to compete with resource-based approaches while performing well on both, semanticsimilarity and relatedness.2 Similarity and Relatedness from semantic networksIn our approach to measuring semantic relatedness, we first automatically build a large semantic networkfrom text and then measure the similarity of two terms by the similarity of the local networks aroundtheir corresponding nodes.
The semantic network serves as a structured representation of the occurringconcepts, relations and attributes in the text.
It is built by translating every sentence in the text into anetwork fragment based on semantic analysis and then merging these networks into a large network bymapping all occurrences of the same term into one node.
Figure 1(a) contains a sample text snippetand the network derived from it.
In this way, concepts are connected across sentences and documents,resulting in a high-level view of the information contained.Our underlying assumption for measuring semantic relatedness is that semantically related nodes areconnected to a similar set of nodes.
In other words, we consider the context of a node in the networkas a representation of its meaning.
In contrast to standard approaches which look only at a type ofcontext directly found in the text, e.g.
words that occur within a certain window from the target word,our network-based context takes into account indirect connections between concepts.
For example, inthe text underlying the network in Fig.
2, dissertation and module rarely co-occurred in a sentence, butthe network shows a strong connection over student as well as over credit and work.3902.1 The Network StructureWe build the network incrementally by parsing every sentence, translating it into a small network frag-ment and then mapping that fragment onto the main network generated from all previous sentences.
Ourtranslation of sentences from text to network is based on the one used in the ASKNet system (Harringtonand Clark, 2007).
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004)and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C&C Toolkit1.The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 man-ually annotated sentences of the WSJ.
It is both robust and efficient.
Boxer is designed to convert theCCG parsed text into a logical representation based on Discourse Representation Theory (DRT).
Thisintermediate logical form representation presents an abstraction from syntactic details to semantic coreinformation.
For example, the syntactical forms progress of student and student?s progress have the sameBoxer representation as well as the student who attends the lecture and the student attending the lecture.In addition, Boxer provides some elementary co-reference resolution.The translation from the Boxer output into a network is straightforward and an example is givenin Figure 1(b).
The network structure distinguishes between object nodes (rectangular), relational nodes(diamonds) and attributes (rounded rectangles) and different types of links such as subject or object links.Students select modules from the published list and write a dissertation.
Modules usually provide 15 creditseach, but 30 credits are awarded for the dissertation.
The student must discuss the topic of the finaldissertation with their appointed tutor.Figure 1: (a) Sample text snippet and according network representation.
(b) Example of translation fromtext to network over Boxer semantic analysisThe large unified network is then built by merging every occurrence of a concept (e.g.
object node)into one node, thus accumulating the information on this concept.
In the second example (Figure ??
), thelecture node would be merged with occurrences of lecture in other sentences.
Figure 2 gives a subset of anetwork generated from a few paragraphs taken from Oxford Student Handbooks.
Multiple occurrencesof the same relation between two object nodes are drawn as overlapping.2.2 The Vector Space ModelWe measure the semantic relatedness of two concepts by measuring the similarity of the surroundings oftheir corresponding nodes in the network.
Semantically related terms are then expected to be connectedto a similar set of nodes.
We retrieve the network context of a specific node and determine the level1http://svn.ask.it.usyd.edu.au/trac/candc391Figure 2: Subgraph displaying selected concepts and relations from sample network.of significance of each node in the context using spreading activation2.
The target node is given aninitial activation of ax = 10?numberOfLinks(x) and is fired so that the activation spreads over its out-and ingoing links to the surrounding nodes.
They in turn fire if their received activation level exceeds acertain threshold.
The activation attenuates by a constant factor in every step and a stable state is reachedwhen no node in the network can fire anymore.
In this way, the context nodes receive different levels ofactivation reflecting their significance.We derive a vector representation ~v(x) of the network context of x including only object nodes andtheir activation levels.
The entries arevi(x) = actx,ax(ni) ni ?
{n ?
nodes | type(n) = object node}The semantic relatedness of two target words is then measured by the cosine similarity of their contextvectors.sim rel(x, y) = cos(~v(x), ~v(y)) = ~v(x) ?
~v(y)?~v(x)?
?~v(y)?As spreading activation takes several factors into account, such as number of paths, length of paths,level of density and number of connections, this method leverages the full interconnected structure of thenetwork.3 EvaluationWe evaluate our approach on the WordSimilarity-353 (Finkelstein et al, 2002) test collection, which isa commonly used gold standard for the semantic relatedness task.
It provides average human judgmentsscores of the degree of relatedness for 353 word pairs.
The collection contains classically similar word2The spreading activation algorithm is based on Harrington (2010)392Approach Spearman(Strube and Ponzetto, 2006) Wikipedia 0.19-0.48(Jarmasz, 2003) Roget?s 0.55(Hughes and Ramage, 2007) WordNet 0.55(Agirre et al, 2009) WordNet 0.56(Finkelstein et al, 2002) Web corpus, LSA 0.56(Harrington, 2010) Sem.
Network 0.62(Agirre et al, 2009) WordNet+gloss 0.66(Agirre et al, 2009) Web corpus 0.66(Gabrilovich and Markovitch, 2007) Wikipedia 0.75Network (all pairs) 0.38Network (>100 freq: 293 pairs) 0.46Network (>300 freq: 227 pairs) 0.50Similarity Relatednessall pairs 0.19 0.36(100 pairs) (250 pairs)>300 freq 0.50 0.52(60 pairs) (171 pairs)Table 1: (a) Spearman ranking correlation coefficient results for our approach and comparison withprevious approaches.
(b) Separate results for similarity and relatedness subset.pairs such as street - avenue and topically related pairs such as hotel - reservation.
However, no distinc-tion was made while judging and the instruction was to rate the general degree of semantic relatedness.As a corpus we chose the British National Corpus (BNC)3.
It is one of the largest standardizedEnglish corpora and contains approximately 5.9 million sentences.
Choosing this text collection enablesus to build a general purpose network that is not specifically created for the considered work pairs andensures a realistic overall connectedness of the network as well as a broad coverage.
In this paper wecreated a network from 2 million sentences of the BNC.
It contains 27.5 million nodes out of which635.000 are object nodes and the rest are relation and attribute nodes.
The building time includingparsing was approximately 4 days.Following the common practice in related work, we compared our scores to the human judgementsusing the Spearman rank-order correlation coefficient.
The results can be found in Table 1(a) with acomparison to previous results on the WordSimilarity-353 collection.Our first result over all word pairs is relatively low compared to the currently best performing sys-tems.
However, we noticed that many poorly rated word pairs contained at least one word with lowfrequency.
Excluding these considerably improved the result to 0.50.
On this reduced set of word pairsour scores are in the region of approaches which make use of the Wikipedia category network, the Word-Net taxonomic relations or Roget?s thesaurus.
This is a promising result as it indicates that our approachbased on automatically generated networks has the potential of competing with those using manuallycreated resources if we increase the corpus size.While our results are not competitive with the best corpus based methods, we can note that ourcurrent corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipediaarticles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009).
Theextent to which corpus size influences our results is subject to further research.We also evaluated our scores separately on the semantically similar versus the semantically relatedsubsets of WordSim-353 following Agirre et al (2009) (Table 1(b)).
Taking the same low-frequency cutas above, we can see that our approach performs equally well on both sets.
This is remarkable as differentmethods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009).
In particular,WordNet based measures are well known to be better suited to measure similarity than relatedness dueto its hierarchical, taxonomic structure (Budanitsky and Hirst, 2006).
The fact that our system achievesequal results on the subset indicates that it matches human judgement of semantic relatedness beyondspecific types of relations.
This could be due to the associative structure of the network.4 Related WorkOur approach is closely related to Harrington (2010) as our networks are built in a similar fashion and wealso use spreading activation to measure semantic relatedness.
In their approach, semantic relatedness3http://www.natcorp.ox.ac.uk/393of two terms a and b is measured by the activation b receives when a is fired.
The core difference of thismeasurement to ours is that it is path-based while ours is context based.
In addition, the corpus used wasretrieved specifically for the word pairs in question while ours is a general-purpose corpus.In addition, our approach is related to work that uses personalized PageRank or Random Walks onWordNet (Agirre et al, 2009; Hughes and Ramage, 2007).
Similar the spreading activation methodpresented here, personalized PageRank and Random Walks are used to provide a relevance distributionof nodes surrounding the target word to its meaning.
In contrast to the approaches based on resources,our network is automatically built and therefore does not rely on costly, manual creation.
In addition,compared to WordNet based measures, our method is potentially not biased towards relatedness due tosimilarity.5 Conclusion and OutlookWe presented a novel approach to measuring semantic relatedness which first builds a large-scale se-mantic network and then determines the relatedness of nodes by the similarity of their surrounding localnetwork.
Our preliminary results of this ongoing work are promising and are in the region of severalWordNet and Wikipedia link structure approaches.
As future work, there are several ways of improve-ment we are going to investigate.
Firstly, the results in Section 3 show the crucial influence of corpussize and occurrence frequency on the performance of our system.
We will be experimenting with largergeneral networks (e.g.
the whole BNC) as well as integration of retrieved documents for the low fre-quency terms.
Secondly, the parameters and specific settings for the spreading activation algorithm needto be tuned.
For example, the amount of initial activation of the target node determines the size of thecontext considered.
Thirdly, we will investigate different vector representation variants.
In particular, wecan achieve a more fine-grained representation by also considering relation nodes in addition to objectnodes.
We believe that with these improvements our automatic semantic network approach will be ableto compete with techniques based on manually created resources.ReferencesAgirre, E., E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca, and A. Soroa (2009).
A study on similarity and relatedness usingdistributional and wordnet-based approaches.
In NAACL ?09.Bos, J., S. Clark, M. Steedman, J. R. Curran, and J. Hockenmaier (2004).
Wide-coverage semantic representations from a ccgparser.
In COLING?04.Budanitsky, A. and G. Hirst (2006).
Evaluating wordnet-based measures of lexical semantic relatedness.
ComputationalLinguistics 32(1), 13?47.Clark, S. and J. R. Curran (2004).
Parsing the wsj using ccg and log-linear models.
In ACL?04.Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin (2002).
Placing search in context:the concept revisited.
ACM Trans.
Inf.
Syst.
20(1), 116?131.Gabrilovich, E. and S. Markovitch (2007).
Computing semantic relatedness using wikipedia-based explicit semantic analysis.In IJCAI?07.Harrington, B.
(2010).
A semantic network approach to measuring semantic relatedness.
In COLING?10.Harrington, B. and S. Clark (2007).
Asknet: automated semantic knowledge network.
In AAAI?07.Hughes, T. and D. Ramage (2007).
Lexical semantic relatedness with random graph walks.
In EMNLP-CoNLL?07.Jarmasz, M. (2003).
Roget?s thesaurus as a lexical resource for natural language processsing.
Master?s thesis, University ofOttawa.Strube, M. and S. P. Ponzetto (2006).
Wikirelate!
computing semantic relatedness using wikipedia.
In AAAI?06.394
