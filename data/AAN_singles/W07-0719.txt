Proceedings of the Second Workshop on Statistical Machine Translation, pages 159?166,Prague, June 2007. c?2007 Association for Computational LinguisticsContext-aware Discriminative Phrase Selectionfor Statistical Machine TranslationJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractIn this work we revise the applicationof discriminative learning to the problemof phrase selection in Statistical MachineTranslation.
Inspired by common tech-niques used in Word Sense Disambiguation,we train classifiers based on local contextto predict possible phrase translations.
Ourwork extends that of Vickrey et al (2005) intwo main aspects.
First, we move from wordtranslation to phrase translation.
Second, wemove from the ?blank-filling?
task to the ?fulltranslation?
task.
We report results on a setof highly frequent source phrases, obtaininga significant improvement, specially with re-spect to adequacy, according to a rigorousprocess of manual evaluation.1 IntroductionTranslations tables in Phrase-based Statistical Ma-chine Translation (SMT) are often built on the ba-sis of Maximum-likelihood Estimation (MLE), be-ing one of the major limitations of this approach thatthe source sentence context in which phrases occuris completely ignored (Koehn et al, 2003).In this work, inspired by state-of-the-art WordSense Disambiguation (WSD) techniques, we sug-gest using Discriminative Phrase Translation (DPT)models which take into account a wider featurecontext.
Following the approach by Vickrey et al(2005), we deal with the ?phrase translation?
prob-lem as a classification problem.
We use SupportVector Machines (SVMs) to predict phrase transla-tions in the context of the whole source sentence.We extend the work by Vickrey et al (2005) in twomain aspects.
First, we move from ?word transla-tion?
to ?phrase translation?.
Second, we move fromthe ?blank-filling?
task to the ?full translation?
task.Our approach is fully described in Section 2.
Weapply it to the Spanish-to-English translation of Eu-ropean Parliament Proceedings.
In Section 3, priorto considering the ?full translation?
task, we ana-lyze the impact of using DPT models for the iso-lated ?phrase translation?
task.
In spite of workingon a very specific domain, a large room for improve-ment, coherent with WSD performance, and resultsby Vickrey et al (2005), is predicted.
Then, in Sec-tion 4, we tackle the full translation task.
DPT mod-els are integrated in a ?soft?
manner, by making themavailable to the decoder so they can fully interactwith other models.
Results using a reduced set ofhighly frequent source phrases show a significantimprovement, according to several automatic eval-uation metrics.
Interestingly, the BLEU metric (Pap-ineni et al, 2001) is not able to reflect this improve-ment.
Through a rigorous process of manual eval-uation we have verified the gain.
We have also ob-served that it is mainly related to adequacy.
Theseresults confirm that better phrase translation proba-bilities may be helpful for the full translation task.However, the fact that no gain in fluency is reportedindicates that the integration of these probabilitiesinto the statistical framework requires further study.2 Discriminative Phrase TranslationIn this section we describe the phrase-based SMTbaseline system and how DPT models are built andintegrated into this system in a ?soft?
manner.1592.1 Baseline SystemThe baseline system is a phrase-based SMT sys-tem (Koehn et al, 2003), built almost entirely us-ing freely available components.
We use the SRILanguage Modeling Toolkit (Stolcke, 2002) for lan-guage modeling.
We build trigram language modelsapplying linear interpolation and Kneser-Ney dis-counting for smoothing.
Translation models arebuilt on top of word-aligned parallel corpora linguis-tically annotated at the level of shallow syntax (i.e.,lemma, part-of-speech, and base phrase chunks)as described by Gime?nez and Ma`rquez (2005).Text is automatically annotated, using the SVM-Tool (Gime?nez and Ma`rquez, 2004), Freeling (Car-reras et al, 2004), and Phreco (Carreras et al, 2005)packages.
We used the GIZA++ SMT Toolkit1 (Ochand Ney, 2003) to generate word alignments.
Weapply the phrase-extract algorithm, as described byOch (2002), on the Viterbi alignments output byGIZA++ following the ?global phrase extraction?strategy described by Gime?nez and Ma`rquez (2005)(i.e., a single phrase translation table is built on topof the union of alignments corresponding to dif-ferent linguistic data views).
We work with theunion of source-to-target and target-to-source align-ments, with no heuristic refinement.
Phrases up tolength five are considered.
Also, phrase pairs ap-pearing only once are discarded, and phrase pairsin which the source/target phrase is more than threetimes longer than the target/source phrase are ig-nored.
Phrase pairs are scored on the basis of un-smoothed relative frequency (i.e., MLE).
Regard-ing the argmax search, we used the Pharaoh beamsearch decoder (Koehn, 2004), which naturally fitswith the previous tools.2.2 DPT for SMTInstead of relying on MLE estimation to score thephrase pairs (fi, ej) in the translation table, wesuggest considering the translation of every sourcephrase fi as a multi-class classification problem,where every possible translation of fi is a class.We use local linear SVMs 2.
Since SVMs are bi-nary classifiers, the problem must be binarized.
We1http://www.fjoch.com/GIZA++.html2We use the SVMlight package, which is freely available athttp://svmlight.joachims.org (Joachims, 1999).have applied a simple one-vs-all binarization, i.e., aSVM is trained for every possible translation candi-date ej .
Training examples are extracted from thesame training data as in the case of MLE models,i.e., an aligned parallel corpus, obtained as describedin Section 2.1.
We use each sentence pair in whichthe source phrase fi occurs to generate a positive ex-ample for the classifier corresponding to the actualtranslation of fi in that sentence, according to theautomatic alignment.
This will be as well a negativeexample for the classifiers corresponding to the restof possible translations of fi.2.2.1 Feature SetWe consider different kinds of information, al-ways from the source sentence, based on standardWSD methods (Yarowsky et al, 2001).
As to thelocal context, inside the source phrase to disam-biguate, and 5 tokens to the left and to the right,we use n-grams (n ?
{1, 2, 3}) of: words, parts-of-speech, lemmas and base phrase chunking IOBlabels.
As to the global context, we collect topicalinformation by considering the source sentence as abag of lemmas.2.2.2 Decoding.
A Trick.At translation time, we consider every instance offi as a separate case.
In each case, for all possi-ble translations of fi, we collect the SVM score, ac-cording to the SVM classification rule.
We are infact modeling P (ej |fi).
However, these scores arenot probabilities.
We transform them into proba-bilities by applying the softmax function describedby Bishop (1995).
We do not constrain the decoderto use the translation ej with highest probability.
In-stead, we make all predictions available and let thedecoder choose.
We have avoided implementing anew decoder by pre-computing all the SVM pre-dictions for all possible translations for all sourcephrases appearing in the test set.
We input this in-formation onto the decoder by replicating the entriesin the translation table.
In other words, each distinctoccurrence of every single source phrase has a dis-tinct list of phrase translation candidates with theircorresponding scores.
Accordingly, the source sen-tence is transformed into a sequence of identifiers,160in our case a sequence of (w, i) pairs3, which allowus to uniquely identify every distinct instance of ev-ery word in the test set during decoding, and to re-trieve DPT predictions in the translation table.
Forthat purpose, source phrases in the translation tablemust comply with the same format.This imaginative trick4 saved us in the short runa gigantic amount of work.
However, it imposes asevere limitation on the kind of features which theDPT system may use.
In particular, features fromthe target sentence under construction and fromthe correspondence between source and target (i.e.,alignments) can not be used.3 Phrase TranslationAnalogously to the ?word translation?
definition byVickrey et al (2005), rather than predicting the senseof a word according to a given sense inventory, in?phrase translation?, the goal is to predict the correcttranslation of a phrase, for a given target language,in the context of a sentence.
This task is simpler thanthe ?full translation?
task, but provides an insight tothe gain prospectives.We used the data from the Openlab 2006 Initia-tive5 promoted by the TC-STAR Consortium6.
Thistest suite is entirely based on European ParliamentProceedings.
We have focused on the Spanish-to-English task.
The training set consists of 1,281,427parallel sentences.
Performing phrase extractionover the training data, as described in Section 2.1,we obtained translation candidates for 1,729,191source phrases.
We built classifiers for all the sourcephrases with more than one possible translation andmore than 10 occurrences.
241,234 source phrasesfulfilled this requirement.
For each source phrase,we used 80% of the instances for training, 10% fordevelopment, and 10% for test.Table 1 shows ?phrase translation?
results overthe test set.
We compare the performance, in termsof accuracy, of DPT models and the ?most fre-quent translation?
baseline (?MFT?).
The MFT base-3w is a word and i corresponds to the number of instancesof word w seen in the test set before the current instance.4We have checked that results following this type of decod-ing when translation tables are estimated on the basis of MLEare identical to regular decoding results.5http://tc-star.itc.it/openlab2006/6http://www.tc-star.org/phrase set model macro microall MFT 0.66 0.70DPT 0.68 0.76frequent MFT 0.76 0.75DPT 0.86 0.86Table 1: ?Phrase Translation?
Accuracy (test set).line is equivalent to selecting the translation candi-date with highest probability according to MLE.
The?macro?
column shows macro-averaged results overall phrases, i.e., the accuracy for each phrase countsequally towards the average.
The ?micro?
columnshows micro-averaged accuracy, where each test ex-ample counts equally.
The ?all?
set includes resultsfor the 241,234 phrases, whereas the ?frequent?
setincludes results for a selection of 41 very frequentphrases ocurring more than 50,000 times.A priori, DPT models seem to offer a significantroom for potential improvement.
Although phrasetranslation differs from WSD in a number of as-pects, the increase with respect to the MFT baselineis comparable.
Results are also coherent with thoseattained by Vickrey et al (2005).-1-0.500.510  50000  100000  150000  200000  250000  300000accuracy(DPT) -accuracy(MLE)#examplesFigure 1: Analysis of ?Phrase Translation?
Resultson the development set (Spanish-to-English).Figure 1 shows the relationship between the accu-racy7 gain and the number of training examples.
Ingeneral, with a sufficient number of examples (over10,000), DPT outperforms the MFT baseline.7We focus on micro-averaged accuracy.1614 Full TranslationIn the ?phrase translation?
task the predicted phrasedoes not interact with the rest of the target sentence.In this section we analyze the impact of DPT modelswhen the goal is to translate the whole sentence.For evaluation purposes we count on a set of 1,008sentences.
Three human references per sentence areavailable.
We randomly split this set in two halves,and use them for development and test, respectively.4.1 EvaluationEvaluating the effects of using DPT predictions, di-rected towards a better word selection, in the fulltranslation task presents two serious difficulties.In first place, the actual room for improvementcaused by a better translation modeling is smallerthan estimated in Section 3.
This is mainly due tothe SMT architecture itself which relies on a searchover a probability space in which several models co-operate.
For instance, in many cases errors causedby a poor translation modeling may be corrected bythe language model.
In a recent study, Vilar et al(2006) found that only around 25% of the errors arerelated to word selection.
In half of these cases er-rors are caused by a wrong word sense disambigua-tion, and in the other half the word sense is correctbut the lexical choice is wrong.In second place, most conventional automaticevaluation metrics have not been designed for thispurpose.
For instance, metrics such as BLEU (Pa-pineni et al, 2001) tend to favour longer n-grammatchings, and are, thus, biased towards word or-dering.
We might find better suited metrics, suchas METEOR (Banerjee and Lavie, 2005), which isoriented towards word selection8.
However, a newproblem arises.
Because different metrics are biasedtowards different aspects of quality, scores conferredby different metrics are often controversial.In order to cope with evaluation difficulties wehave applied several complementary actions:1.
Based on the results from Section 3, we focuson a reduced set of 41 very promising phrasestrained on more than 50,000 examples.
Thisset covers 25.8% of the words in the test set,8METEOR works at the unigram level, may consider wordstemming and, for the case of English is also able to perform alookup for synonymy in WordNet (Fellbaum, 1998).and exhibits a potential absolute accuracy gainaround 11% (See Table 1).2.
With the purpose of evaluating the changes re-lated only to this small set of very promis-ing phrases, we introduce a new measure, Apt,which computes ?phrase translation?
accuracyfor a given list of source phrases.
For everytest case, Apt counts the proportion of phrasesfrom the list appearing in the source sentencewhich have a valid9 translation both in the tar-get sentence and in any of the reference trans-lations.
In fact, because in general source-to-target algnments are not known, Apt calculatesan approximate10 solution.3.
We evaluate overall MT quality on the basisof ?Human Likeness?.
In particular, we usethe QUEEN11 meta-measure from the QARLAFramework (Amigo?
et al, 2005).
QUEEN op-erates under the assumption that a good trans-lation must be similar to all human referencesaccording to all metrics.
Given a set of auto-matic translations A, a set of similarity metricsX, and a set of human references R, QUEEN isdefined as the probability, over R?R?R, thatfor every metric in X the automatic translationa is more similar to a reference r than two otherreferences r?
and r??
to each other.
Formally:QUEENX,R(a) = Prob(?x ?
X : x(a, r) ?
x(r?, r??
))QUEEN captures the features that are commonto all human references, rewarding those auto-matic translations which share them, and pe-nalizing those which do not.
Thus, QUEEN pro-vides a robust means of combining several met-rics into a single measure of quality.
Followingthe methodology described by Gime?nez andAmigo?
(2006), we compute the QUEEN mea-sure over the metric combination with high-est KING, i.e., discriminative power.
We haveconsidered all the lexical metrics12 provided by9Valid translations are provided by the translation table.10Current Apt implementation searches phrases from left toright in decreasing length order.11QUEEN is available inside the IQMT package for MTEvaluation based on ?Human Likeness?
(Gime?nez and Amigo?,2006).
http://www.lsi.upc.edu/?nlp/IQMT12Consult the IQMT Technical Manual v1.3 for a detailed de-scription of the metric set.
http://www.lsi.upc.edu/?nlp/IQMT/IQMT.v1.3.pdf162QUEEN Apt BLEU METEOR ROUGEP (e) + PMLE(f |e) 0.43 0.86 0.59 0.77 0.42P (e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43P (e) + PDPT (e|f) 0.47 0.89 0.62 0.78 0.44Table 2: Automatic evaluation of the ?full translation?
results on the test set.IQMT.
The optimal set is:{ METEORwnsyn, ROUGEw 1.2 }which includes variants of METEOR, andROUGE (Lin and Och, 2004).4.2 Adjustment of ParametersModels are combined in a log-linear fashion:logP (e|f) ?
?lmlogP (e) + ?glogPMLE(f |e)+ ?dlogPMLE(e|f) + ?DPT logPDPT (e|f)P (e) is the language model probability.PMLE(f |e) corresponds to the MLE-basedgenerative translation model, whereas PMLE(e|f)corresponds to the analogous discriminative model.PDPT (e|f) corresponds to the DPT model whichuses SVM-based predictions in a wider featurecontext.
In order to perform fair comparisons,model weights must be adjusted.Because we have focused on a reduced set of fre-quent phrases, in order to translate the whole test setwe must provide alternative translation probabilitiesfor all the source phrases in the vocabulary whichdo not have a DPT prediction.
We have used MLEpredictions to complete the model.
However, inter-action between DPT and MLE models is problem-atic.
Problems arise when, for a given source phrase,fi, DPT predictions must compete with MLE pre-dictions for larger phrases fj overlapping with orcontaining fi (See Section 4.3).
We have alleviatedthese problems by splitting DPT tables in 3 subta-bles: (1) phrases with DPT prediction, (2) phraseswith DPT prediction only for subphrases of it, and(3) phrases with no DPT prediction for any sub-phrase; and separately adjusting their weights.Counting on a reliable automatic measure of qual-ity is a crucial issue for system development.
Opti-mal configurations may vary very significantly de-pending on the metric governing the optimizationprocess.
We optimize the system parameters overthe QUEEN measure, which has proved to lead tomore robust system configurations than BLEU (Lam-bert et al, 2006).
We exhaustively try all possibleparameter configurations, at a resolution of 0.1, overthe development set and select the best one.
In orderto keep the optimization process feasible, in terms oftime, the search space is pruned13 during decoding.4.3 ResultsWe compare the systems using the generative anddiscriminative MLE-based translation models to thediscriminative translation model which uses DPTpredictions for the set of 41 very ?frequent?
sourcephrases.
Table 2 shows automatic evaluation re-sults on the test set, according to several metrics.Phrase translation accuracy (over the ?frequent?
setof phrases) and MT quality are evaluated by meansof the Apt and QUEEN measures, respectively.
Forthe sake of informativeness, BLEU, METEORwnsynand ROUGEw 1.2 scores are provided as well.Interestingly, discriminative models outperformthe (noisy-channel) default generative model.
Im-provement in Apt measure also reveals that DPT pre-dictions provide a better translation for the set of?frequent?
phrases than the MLE models.
This im-provement remains when measuring overall transla-tion quality via QUEEN.
If we take into account thatDPT predictions are available for only 25% of thewords in the test set, we can say that the gain re-ported by the QUEEN and Apt measures is consistentwith the accuracy prospectives predicted in Table 1.METEORwnsyn and ROUGEw 1.2 reflect a slight im-provement as well.
However, according to BLEUthere is no difference between both systems.
Wesuspect that BLEU is unable to accurately reflect thepossible gains attained by a better ?phrase selection?over a small set of phrases because of its tendency13For each phrase only the 30 top-scoring translations areused.
At all times, only the 100 top-scoring solutions are kept.We also disabled distortion and word penalty models.
There-fore, translations are monotonic, and source and target tend tohave the same number of words (that is not mandatory).163to reward long n-gram matchings.
In order to clar-ify this scenario a rigorous process of manual evalu-ation has been conducted.
We have selected a subsetof sentences based on the following criteria:?
sentence length between 10 and 30 words.?
at least 5 words have a DPT prediction.?
DPT and MLE outputs differ.A total of 114 sentences fulfill these require-ments.
In each translation case, assessors must judgewhether the output by the discriminative ?MLE?
sys-tem is better, equal to or worse than the output bythe ?DPT?
system, with respect to adequacy, fluency,and overall quality.
In order to avoid any bias in theevaluation, we have randomized the respective posi-tion in the display of the sentences corresponding toeach system.
Four judges participated in the evalua-tion.
Each judge evaluated only half of the cases.Each case was evaluated by two different judges.Therefore, we count on 228 human assessments.Table 3 shows the results of the manual systemcomparison.
Statistical significance has been deter-mined using the sign-test (Siegel, 1956).
Accordingto human assessors, the ?DPT?
system outperformsthe ?MLE?
system very significantly with respect toadequacy, whereas for fluency there is a slight ad-vantage in favor of the ?MLE?
system.
Overall, thereis a slight but significant advantage in favor of the?DPT?
system.
Manual evaluation confirms our sus-picion that the BLEU metric is less sensitive thanQUEEN to improvements related to adequacy.Error AnalysisGuided by the QUEEN measure, we carefully inspectparticular cases.
We start, in Table 4, by show-ing a positive case.
The three phrases highlightedin the source sentence (?tiene?, ?sen?ora?
and ?unacuestio?n?)
find a better translation with the help ofthe DPT models: ?tiene?
translates into ?has?
insteadof ?i give?, ?sen?ora?
into ?mrs?
instead of ?lady?, and?una cuestio?n?
into ?a point?
instead of ?a ... motion?.In contrast, Table 5 shows a negative case.
Thetranslation of the Spanish word ?sen?ora?
as ?mrs?
isacceptable.
However, it influences very negativelythe translation of the following word ?diputada?,whereas the ?MLE?
system translates the phrase?sen?ora diputada?, which does not have a DPT pre-diction, as a whole.
Similarly, the translation ofAdequacy Fluency OverallMLE > DPT 39 84 83MLE = DPT 100 76 46MLE < DPT 89 68 99Table 3: Manual evaluation of the ?full translation?results on the test set.
Counts on the number oftranslation cases for which the ?MLE?
system is bet-ter than (>), equal to (=), or worse than (<) the?DPT?
system, with respect to adequacy, fluency,and overall MT quality, are presented.?cuestio?n?
as ?matter?, although acceptable, is break-ing the phrase ?cuestio?n de orden?
of high cohe-sion, which is commonly translated as ?point of or-der?.
The cause underlying these problems is thatDPT predictions are available only for a subset ofphrases.
Thus, during decoding, for these cases ourDPT models may be in disadvantage.5 Related WorkRecently, there is a growing interest in the appli-cation of WSD technology to MT.
For instance,Carpuat and Wu (2005b) suggested integratingWSD predictions into a SMT system in a ?hard?manner, either for decoding, by constraining the setof acceptable translation candidates for each givensource word, or for post-processing the SMT sys-tem output, by directly replacing the translation ofeach selected word with the WSD system predic-tion.
They did not manage to improve MT quality.They encountered several problems inherent to theSMT architecture.
In particular, they described whatthey called the ?language model effect?
in SMT:?The lexical choices are made in a way that heav-ily prefers phrasal cohesion in the output target sen-tence, as scored by the language model.?.
This prob-lem is a direct consequence of the ?hard?
interactionbetween their WSD and SMT systems.
WSD pre-dictions cannot adapt to the surrounding target con-text.
In a later work, Carpuat and Wu (2005a) ana-lyzed the converse question, i.e.
they measured theWSD performance of SMT models.
They showedthat dedicated WSD models significantly outper-form current state-of-the-art SMT models.
Conse-quently, SMT should benefit from WSD predictions.Simultaneously, Vickrey et al (2005) studied the164Source tiene la palabra la sen?ora mussolini para una cuestio?n de orden .Ref 1 mrs mussolini has the floor for a point of order .Ref 2 you have the floor , missus mussolini , for a question of order .Ref 3 ms mussolini has now the floor for a point of order .P (e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion .P (e) + PDPT (e|f) has the floor the mrs mussolini on a point of order .Table 4: Case of Analysis of sentence #422.
DPT models help.Source sen?ora diputada , e?sta no es una cuestio?n de orden .Ref 1 mrs mussolini , that is not a point of order .Ref 2 honourable member , this is not a question of order .Ref 3 my honourable friend , this is not a point of order .P (e) + PMLE(e|f) honourable member , this is not a point of order .P (e) + PDPT (e|f) mrs karamanou , this is not a matter of order .Table 5: Case of Analysis of sentence #434.
DPT models fail.application of discriminative models based on WSDtechnology to the ?blank-filling?
task, a simplifiedversion of the translation task, in which the targetcontext surrounding the word translation is avail-able.
They did not encounter the ?language modeleffect?
because they approached the task in a ?soft?way, i.e., allowing their WSD models to interactwith other models during decoding.
Similarly, ourDPT models are, as described in Section 2.2, softlyintegrated in the decoding step, and thus do not suf-fer from the detrimental ?language model effect?
ei-ther, in the context of the ?full translation?
task.
Be-sides, DPT models enforce phrasal cohesion by con-sidering disambiguation at the level of phrases.6 Conclusions and Further WorkDespite the fact that measuring improvements inword selection is a very delicate issue, we haveshowed that dedicated discriminative translationmodels considering a wider feature context providea useful mechanism in order to improve the qual-ity of current phrase-based SMT systems, speciallywith regard to adequacy.
However, the fact that nogain in fluency is reported indicates that the integra-tion of these probabilities into the statistical frame-work requires further study.Moreover, there are several open issues.
First, forpractical reasons, we have limited to a reduced set of?frequent?
phrases, and we have disabled reorderingand word penalty models.
We are currently studyingthe impact of a larger set of phrases, covering over99% of the words in the test set.
Experiments withenabled reordering and word penalty models shouldbe conducted as well.
Second, automatic evalua-tion of the results revealed a low agreement betweenBLEU and other metrics.
For system comparison, wesolved this through a process of manual evaluation.However, this is impractical for the adjustment ofparameters, where hundreds of different configura-tions are tried.
In this work we have relied on auto-matic evaluation based on ?Human Likeness?
whichallows for metric combinations and provides a sta-ble and robust criterion for the metric set selection.Other alternatives could be tried.
The crucial issue,in our opinion, is that the metric guiding the opti-mization is able to capture the changes.Finally, we argue that, if DPT models consideredfeatures from the target side, and from the corre-spondence between source and target, results couldfurther improve.
However, at the short term, the in-corporation of these type of features will force us toeither build a new decoder or extend an existing one,or to move to a new MT architecture, for instance,in the fashion of the architectures suggested by Till-mann and Zhang (2006) or Liang et al (2006).AcknowledgementsThis research has been funded by the Span-ish Ministry of Education and Science, projectsOpenMT (TIN2006-15307-C03-02) and TRAN-165GRAM (TIN2004-07925-C03-02).
We are recog-nized as a Quality Research Group (2005 SGR-00130) by DURSI, the Research Department of theCatalan Government.
Authors are thankful to theTC-STAR Consortium for providing such very valu-able data sets.ReferencesEnrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-lisa Verdejo.
2005.
QARLA: a Framework for theEvaluation of Automatic Sumarization.
In Proceed-ings of the 43th Annual Meeting of the Association forComputational Linguistics.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.Christopher M. Bishop.
1995.
6.4: Modeling conditionaldistributions.
In Neural Networks for Pattern Recog-nition, page 215.
Oxford University Press.Marine Carpuat and Dekai Wu.
2005a.
Evaluating theWord Sense Disambiguation Performance of Statisti-cal Machine Translation.
In Proceedings of IJCNLP.Marine Carpuat and Dekai Wu.
2005b.
Word Sense Dis-ambiguation vs. Statistical Machine Translation.
InProceedings of ACL.Xavier Carreras, Isaac Chao, Llu?
?s Padro?, and MuntsaPadro?.
2004.
FreeLing: An Open-Source Suite ofLanguage Analyzers.
In Proceedings of the 4th LREC.Xavier Carreras, Llu?
?s Ma?rquez, and Jorge Castro.
2005.Filtering-ranking perceptron learning for partial pars-ing.
Machine Learning, 59:1?31.C.
Fellbaum, editor.
1998.
WordNet.
An Electronic Lexi-cal Database.
The MIT Press.Jesu?s Gime?nez and Enrique Amigo?.
2006.
IQMT: AFramework for Automatic Machine Translation Eval-uation.
In Proceedings of the 5th LREC.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2004.
SVMTool: Ageneral POS tagger generator based on Support VectorMachines.
In Proceedings of 4th LREC.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2005.
CombiningLinguistic Data Views for Phrase-based SMT.
In Pro-ceedings of the Workshop on Building and Using Par-allel Texts, ACL.T.
Joachims.
1999.
Making large-Scale SVM LearningPractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
The MIT Press.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of HLT/NAACL.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proceedings of AMTA.Patrik Lambert, Jesu?s Gime?nez, Marta R. Costa-jussa?,Enrique Amigo?, Rafael E. Banchs, Llu?
?s Ma?rquez, andJ.A.
R. Fonollosa.
2006.
Machine Translation Sys-tem Development based on Human Likeness.
In Pro-ceedings of IEEE/ACL 2006 Workshop on Spoken Lan-guage Technology.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, , andBen Taskar.
2006.
An End-to-End DiscriminativeApproach to Machine Translation.
In Proceedings ofCOLING-ACL06.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatics.
In Proceedings of ACL.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Franz Josef Och.
2002.
Statistical Machine Transla-tion: From Single-Word Models to Alignment Tem-plates.
Ph.D. thesis, RWTH Aachen, Germany.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalua-tion of machine translation, rc22176.
Technical report,IBM T.J. Watson Research Center.Sidney Siegel.
1956.
Nonparametric Statistics for theBehavioral Sciences.
McGraw-Hill.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of ICSLP.Christoph Tillmann and Tong Zhang.
2006.
A Discrim-inative Global Training Algorithm for Statistical MT.In Proceedings of COLING-ACL06.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005.Word-Sense Disambiguation for Machine Translation.In Proceedings of HLT/EMNLP.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Trans-lation Output.
In Proceedings of the 5th LREC.David Yarowsky, Silviu Cucerzan, Radu Florian, CharlesSchafer, and Richard Wicentowski.
2001.
The JohnsHopkins Senseval2 System Descriptions.
In Proceed-ings of Senseval-2: Second International Workshop onEvaluating Word Sense Disambiguation Systems.166
