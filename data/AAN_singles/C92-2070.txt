Word-Sense DisambiguationUsing Statistical Models of Roget's CategoriesTrained on Large CorporaDavid YarowskyAT&T Bell  Laboratories600 Mountain AvenueMurray  Hil l  N J, 07974yarowsky@research.att .comAbst rac tThis paper describes a program that disambignatesEnglish word senses in unrestricted text using statisticalmodels of the major Roget's Thesaurus categories.Roget's categories serve as approximations of conceptualclasses.
The categories li ted for a word in Roger's indextend to correspond to sense distinctions; thus selectingthe most likely category provides a useful evel of sensedisambiguatiou.
The selection of categories isaccomplished by identifying and weighting words thatare indicative of each category when seen in context,using a Bayesian theoretical framework.Other statistical approaches have required special corporaor hand-labeled training examples for much of thelexicon.
Our use of class models overcomes thisknowledge acquisition bottleneck, enabling training onunresUicted monolingual text without humanintervention.
Applied to the 10 million word Grolier'sEncyclopedia, the system correctly disambiguated 92%of the instances of 12 polysemous words that have beenpreviously studied in the literature.1.
Problem FormulationThis paper presents an approach to word sensedisambiguation that uses classes of words to derivemodels useful for disambignating individual words incontext.
"Sense" is not a well defined concept; it hasbeen based on subjective and often subtle distinctions intopic, register, dialect, collocation, part of speech andvalency.
For the purposes of this study, we will define thesenses of a word as the categories li ted for that word inRoger's International Thesaurus (Fourth Edition -Chapman, 1977).
1Sense disambiguation will constitute1.
Note that his edition of Roger's Thesaurus is much more 0ttm$ivethan the 1911 vm'sion, though somewhat more difficult to obtain inelectronic form, One could me other other concept hlemrehics, suchas WordNet (Miller, 1990) or the LDOCE mbject codes (Slator,1991).
All that it necessary is ?
set of semamic categories and ?
listof the words in each category.selecting the listed category which is most probable giventhe surrounding context.
This may appear to be aparticularly crude approximation, but as shown in theexample below and in the table of results, it issurprisingly successful.I nput  OutputTvr.admillsauachedto cranu were used to lift heavy TOOLSfor supplying powe?
for cranes, hoists, and lift s. TOOLShovetl'fitheisht,atower crane is oftea med .SB TM* TOOLS?labocate oaumhip ribalds cranes build ?
nest of vegetafi ANIMALare  more closely tv.lated to cranes and rails .Sn They ran ANIMALlow tees ,PP At least five crane species are in danger of ANIMAl.Not only do the Roget categories succeed in partitioningthe major senses, but the sense tags they provide asoutput are far more mnemonic than a dictionarynumbering such as "crane 1.2".
Should such adictionary sense number be desired as output, section 5will outline how a linkage between Roget categories anddictionary definitions can be made.We will also focus on sense distinctions within a givenpart of speech.
Distinctions between parts of speech,should be based on local syntactic evidence.
We use astochastic part-of-speech tagger (Church, 1989) for thispurpose, run as a preprocessor.2, P roposed  MethodThe strategy proposed here is based on the followingthree observations: 1) Different conceptual classes ofwords, such as AmMALS or MACH~mS tend to appear inrecognizably different contexts.
2) Different word sensestend to belong to different conceptual c asses (crane canbe an ANIMAL or a MACHINE).
3) If one can build acontext discriminator for the conceptual c asses, one haseffectively built a context discriminator for the wordsenses that are members of those classes.
Furthermore,the context indicators for a Roget category (e.g.
gear,piston and engine for the category TOOLS/MACHINERY)will also tend to be context indicators for the members ofthat category (such as the machinery sense of crane).ACRES DE COLING-92.
NA~rrES, 23-28 AO~r 1992 4 5 4 PRec.
OF COLING-92, NANTES, AUG. 23-28, 1992We attempt to identify, weight and utilize th~e indicativewords &s follows.
For each of the 1042 Roget Categories:1.
Collect contexts which are representative of theRoget category2.
Identify salient words in the collective context anddetermine weights for each word, and3.
Use the resulting weights to predict he appropriatecategory for a polysemous word occurring in noveltext.2.1 Step 1: Col lect  Contexts  wh ich  areRepresentative of the  Roget  categoryThe goal of this step is to collect a set of words t/tat aretypically found in the context of a Roget category.
To dothis, we extract concordances of 100 surrounding wordsfor e2~h occurrence of each member of the category illthe corpus.
Below is a sample set of partial concordancesfor words in the category "IOOLS.tMACIIINERY (348).
Thecomplete set contains 30,924 lines, selected from theparticular training corpus used in this study, the 10million word.
June 1991 electronic version of Grolier'sEncyclopedia.CARVING .SB The gutteruipment such as a hydraulicon .SB Resembling apoweruipmant, valves for nuclear00 BC, flint-odged wooden1-penetrating c~bide-tipped~lt heightens the colors .SBlxaditionM ABC method andnter of ro~tion .PP A rowerrshy areas .SB The crownedadz has a concave blade for fonnshovel capable of lifting 26 cubicshovel mounted on a floating hulgenerators, oil-refinery turbinessickles were used to gather wilddrills forced manufacturers to fiDrills live in the forests of equadrill were unchanged, and dissacrane is an assembly of fabricatcrane, however, occasionallyFor optimal training, die concordance set should onlyinclude references to the given category.
But in practiceit will unavoidably include spurious examples ince mmtyof the words are polysemous (such its drill mid crane inlines 7, 8, and 10 above).While the level of noise introduced through polysemy issubstantial, it can usually be tolerated because thespurious senses are distributed through tile 1(}41 othercategories, whereas the signal is coneenwated in just one.Only if several words had secondary senses in the statecategory would context typical for the other categoryappear significant in this context.However, if one of these spurious senses was frexluentand dominated the set of examples, file situation could bedisastrous.
An attempt is made to weight theconcordance data to minim~e this effect and to make thesample representative of all tools attd tnachinery, not justthe more common ones.
If a word such as drill occurs ktinies in the coqms, all words ill the context of drillcontribnte weight 1/k to frequency sunos.Despite its flaws, this weighted matrix will serve as arepresentative, albeit noisy, sample of the typical contextof'IYOOI.S/MACItlNERY in Grolier's encyclopedia.2.2 Step 2: Identify ,salient words in thecol lect ive cmttext ,  and  we ight  appropr ia te lyIntuitively, a salient word 2 is one which appearssiguificantly more often in the context of a category thanat other txfints in the corpus, and hence is a better thanaverage indicator for the category.
We formalize thiswifl\] a nmtual-in formation-like estimate:Pr(wlRCat) / Pr(w), tile probability of a word (w)appearing in the context of a Roget category divided byits overall probability in rile corpus.It is imlmrtant to exerci~ some care in estimatingPr(wlRCat).
In principle, one could situply count tilenumber of times that w appears in the collective contexLHowever, this estimate, which is known as the tuaximnntlikelih(x',d estimate (MLE), can be unreliable, especiallywhen w does not apl~-~ar vely often in the collectivecoutexl.
We have smoothed file local estimates ofPr(wlRCat) with global estinmtes of Pr(w) to obtain amore reliable estimate.
Estimates obtained from the localcontext are subject to measurement errors whereasestimates obtained li'om the global context are subject obeing irrelevant.
By interpoiathlg between the two, weattempt to find a compromise between the two sources oferror, qllis procexlure is b~sed on recent work pioneewMby Willimn Gale, attd is explained in detail in anotherpaper (Gale, Church and Yarowsky, 1992).
Space doesnot permit a complete description here.Below are salient words tor Roget categories 348 and414.
*lllose ~lected are tile ntosl important 1o ritemodels, where importance is delined as the product ofsalience and local fi'equency.
That is to say importantwords ate distinctive and fi~equcat.The nnmhers in parentheses are the log of the salience(logPr(wlRCat) /Pr(w)) ,  which we will henceforthrefer to as the word's weight in the statistical model ofthe category.2.
Fo~ illustrative simplicity, we will refer to words in context, Inpnlctice, all op~\]lil~t$ ale ac~uMly p~rfonned onthe Iemma~ of thewords (eal/V = eat,eatg.elling,ate,elae~l), lind inflecdonml dlnincdonstire igltored.
While thi* achieves more concentrated and bclterestimated ttttiUics, it throws away uneful information which natty beext~loited in future work.AC'I'ES DE cOL1NG-92, NANTES, 23-28 Ao(rr 1992 4 S g PROC.
o1: COLING-92, NAN rJ',S, A\[Jo.
23-28, 1992ANIMAL,INSECT (Category 414):species (2.3), family (1.7), bird (2.6), fish (2.4),breed (2.2), cm (2.2), animal (1.7), tail (2.7), egg (2.2),wild (2.6), common (1.3), coat (2.5), female (2.0),inhabit (2.2), eat (2,2), nest (2.5) ....TOOLS/MACHINERY (Category 348):tool (3.1), machine (2.7), engine (2.6), blade (3.8),cut (2.6), saw (5.1), lever (4.1), pump (3.5), device(2.2), gear (3.5), knife(3.8), wheel (2.8), shaft(3.3),wood(2.0), tooth(2.5), piston(3.6) ....Notice that these are not a list of members of thecategory; they are the words which are likely to co-occurwith the members of the category.
The complete list forTOOLS/MACH1NFJI.Y includes a broad set of relations, suchas meronomy (blade, engine, gear, wheel, shaft, tooth,piston and cylinder), typical functions of machines (cut,rotate, move, turn, pull), typical objecls of those actions(wood, metal), as well as typical modifiers for machines(electric, mechanical, pneumatic).
The list for a categorytypically contains over 3000 words, and is far richer thancan be derived from a dictionary definition.2.3 Step 3: Use the resul t ing weights  to predictthe appropr ia te  category  for a word  in noveltextWhen any of the salient words derived in step 2 appear inthe context of an ambiguous word, there is evidence thatthe word belongs to the indicated category.
If severalsuch words appear, the evidence is compounded.
UsingBayes' rule, we sum their weights, over all words incontext, and determine the category for which the sum isgreatest ~.ARGMAX ~ log Pr(w\[RCat)  x Pr(RCat)Rca: w i~ co,,~1 Pr (w)The context is defined to extend 50 words to the left and50 words to the right of the polysemous word.
This rangewas shown by Gale, Church and Yarowsky (1992) to beuseful for this type of broad topic classification, incontrast to the relatively narrow (+3-6 word) windowused in previous studies (e.g.
Black, 1988).
The3.
The reader may have noticed that he Pr(w) factor can be omittedsince it will not change the results of the maximization.
It isincluded here for expository convenience sothat it is possible to~-,npare r sults across words with very different probabilities, 'naefactor also become?
impoc.ant when an incomplete t of indicatorsiJ stored be, cause of comlmtational spac~ constraints.
Currently weassume a uniform prior- probability for each Roget category(Pr(Rcal)).
i,e.
tense classification is based exclusively onotmte~tual information, i dependent of he underlying prd3abillt y ofa given Re?el category appearing at any point in the colpos.maximization over RCats is constrained to consider onlythose categories under which the polysemous word islisted, generally on the order of a half dozen or so.
4For example the word crane appears 74 times in Groliers;36 occurrences refer to the animal sense and 38 refer tothe heavy machinery sense.
The system correctlyclassified all but one of the machinery senses, yielding99% overall accuracy.
The one miselassified case had alow score for all models, indicating a lack of confidencein any classification.It is useful to look at one example in some more detail.Consider the following instance of crane and its contextof + l0 words: 5lift water and to grind grain .PP Treadmills attachedto cranes were used to lift heavy objects from Romantimes,The table below shows the strongest indicators identifiedfor the two categories in the sentence above.
The modelweights, as noted above, are equivalent tolog Pr(wlRCat  ) / Pr(w).
Several indicators werefound for the TOOLS/MACHtNE class.
There is very littleevidence for the ANIMAL sense of crane, with the possibleexception of water.
The preponderance of evidencefavors the former classification, which happens to becorrect.
The difference between the two total scoresindicate strong confidence in the answer.TOOLS/MACII.
Weight ANIMALINSECT Weightwater 0.76 lift 2.44lift 2.44grain 1.68used 1.32heavy 1.28Treadmills 1.16attached 0.58grind 0.29water 0,11TOTAL 11,30 TOTAL 0.764.
Although it is often useful to restrict the search in this way.
therestriction does ometimes l ad to uc~ble, especially when there aregaps in the thesaurus.
For example, the category AMIJSI~I,,g~-r (#876) lisa ?
number of card playin 8terms, lint for some reason, theword suit is not included in this list.
As it happens, the Grulier'sEncydopndia contains 54 instances ofthe card-playing sense of suit,all of which ale mislabeled if the search is limited to just thosecategories of suit that are listed in RogeCs.
However, if we open upthe search to consider all 1042 care?odes, then we find that all 54instances ofsu//are correctly abeled ils/o?/usE,~,~cr, andmo~over.the scca~ is large in all 54 instances, indicating great confidence inthe assignment.
I  is poJsiblc that the unrestricted search modemight be ?
good way to attemps tofill in omisfions in the ?ha?auras.In any case.
when suit is added to the ,oa~t/s~E~rr category, overallaccuracy improves from 68% to 92%.5, "Ibis narrower window is used for iaust rative simplicity.ACRES DE COLING-92, NANTES, 23-28 ^ OUT 1992 4 5 6 PREC.
OF COLING-92, NANTES, AUG. 23-28, 1992TABLE iSen~ R~etCat~o~ N Co~.STAR (Hirst, 1987: N/A)Space Object UNIVERSE 1422 96%Celebrity r~rcrm~TArNt.X 222 95%Staa" Shaped Object INSlOlqIA ..... _5_6_ ...... .82.%1700 96%MOLE (HirsL 1987: N/A *)Quantity OII!MlCALS 95 98%Mammal ANIMALJNSECI" 46 100%Skin Blemish DISEASE 13 100%Digging Machine SUPPORT 4 100~o160 99%GALLEY (LusL 1986: 50-70% overall)Ancient Slfip SIIIP,BOAT 35 97%Printer's Tray PRI~flNG 5 100%Ship's Kitchen .
COOKING 2 50%42 95%CONE (Lesk, 1986; 50-70% overall *)Part of Trec PLANT 71 99%Shape of Ohject ANGULARITY 89 61%Part of Eye VISION 13 69%173 77%BA&q (HearsL 1991: 100%; Speech Synthesis)Musical Senses MUSIC 158 99%Fish ANIMAL,INSECrf 69 100%227 99%BOW (Clear, 1989: < 67%; Speech Synthesis)Weapon ARMS 59 92%Front of Ship StllP,BOAT 34 94%Violin Part MUSICAL INSTR 30 100%Ribbon ORNAMENTAalON 4 25%Bend in Object CONV~.Xn'V 2 50eLowering Head RESPF.CT .______0_ .
.
.
.
.
.
.
5_-___129 91%~- -~Clear ,  1989: < 65%)Preference PAR'I\]CULA RIIY 228 93%Flavor SENSATION 80 93%308 93%INTEREST (Black, 1988: 72%; Zemik, 1990: > 70%)Curiosity REASONING 359 88%Advantage IN/UffI1CE 163 34%Financial DEBT 59 90%Share PROPERTY 21 38%602 72%ISSUE (Zemik, 1990: < 70%)Topic rotrtacs 831 94%Periodical BOOKS.PERIODI 28 89%Stock SECURn1Es 9 1OO%868 94%Sense Roget Category N Corr.DUTY (Gale et el, 1992: 96%)Obligation DUTY 347 96%Tax PRICE.Iq:~.
52 96%399 96%SENTENCE (Gale et al 1992', 90% *)Florishment t .EGALAC'I1ON 128 99%Set of Words GRAMMAR 213 98%341 98%SLUG (Hirsk 1987: N/A *)Animal ANI MAL,INSI!CT 24 100%Type Strip I,RINTtNO 8 100%Mass Unit WEIGItT 3 100%Fake Coin MONEY 2 50%Metallurgy 1MPUIZE.IMPAC-f I 100%Bullet ARMS 1 100%39 97%Notes:1) N refers to the total number of each sense obseawed inthe test corpus.
Corr .
indicates file percemage of thosetagged correctly.2) Because thexe is no independent ground truth to indicatewhich is the "correct" Roget category for a given word,the decision is a subjective judgement made by a singlehuman judge, in this case the author.3) As previously noted, the Roger index is incomplete, hifour cases, identified by *, one missing category has beenadded to the list of possibilities for a word.
Theseontissions in the lexicon have been identified as outlined inFootnote 4.
Without these additions, overall systemperformance would decrease by 5%.4) Uses which an English speaker may consider a singlesense are often realized by several Roget categories.
Forthe purposes of succinct representation, such categorieshave been merged, and the name of file dominant categoryused in the table.
As of this writing, the process has notbeen fully automated.For many applications such as speech synthesis andassignment to an established ictionary sense number orpossible French translations, this merging of Roget classesis not necessary.The primary criterion for success is that words arepartitioned into pure sense clusters.
Words having adifferent sense from the majority sense of a partition aregraded as errors.5) Examples with the ammtation 'speech synthesis' havemultiple pronunciations corresponding to sensedistinctions.
Their disambiguafion is important in speechprocessing.6) All results are based on 100% recall.AcrEs DE COLlNG-92, NAI~'ES, 23-28 AOt3T 1992 4 5 7 PROC.
OF COLING-92, NArCrES, AUG. 23-28, 19923.
Eva luat ionThe algorithm described above was applied to 12polysemous words previously discussed in the sensedisambignation literature.
Table 1 (previous l~lge) showsthe systenl's performance.
Authors who have discussedthese words are listed in parentheses, along with thereported accuracy of their systems.
Direct comparisonsof performance between researchers is difficult,compounded by variances in corpora nd grading criteria;using the same words is an attempt o minimize thesedifferences.Regrettably, most authors have reported their results inqualitative terms.
The exceptions include Zemik (1990)who cited "recall and precision of over 70%" for oneword (interest) and observed that results for other words,including /ssue, were "less positive."
Clear (1989)reported results for two words (65% and 67%),apparently at 85% recall.
Leak (1986) claimed overall"50-70%" accuracies, although it is unclear under whichparameters and constraints.
In a 5 word test set, Black(1988) observed 75% mean accuracy using his optimalmethod on high entropy, 4-way sense distinctions.
Hearst(1991) achieved 84% on simpler 2-way distinctions,editing out additional senses from the test set.
Gale,Church and Yarowsky (1992) reported 92% accuracy,also on 2-way distinctions.Out eun'ent work compares favorably with these results,with 92% accuracy on a mean 3-way sense distinction 6.The performance is especially promising iven that nohand tagging or special corpora were required in training,unlike all other systems considered.4.
Limitations of the MethodThe procedure described here is based on broad contextmodels.
It performs best on words with senses which canbe distinguished by their broad context.
These are mosttypically concrete nouns.
Performance is weaker on thefollowing:Topic Independent Distinctions: One of the reasons thatinterest is disambiguated poorly is that it can appear inalmost any context.
While its "curiosity" sense is oftenindicated by the presence of an academic subject orhobbie, the "advantage" sense (to be in one's interests)has few topic constraints.
Distinguishing between twosuch abstractions i difficult.
7 However, the financial6.
This result is a fair ra~lure of pedorr~nee on words used inp~vi{ms studies, and may he useful for comparison acms l  systems.However, as wolrd$ pmvioully discuJscd inthe literature may not het~preu~tafive of typical English polyk-my, mean performance on ?eomlTletely random u~ of words hould iffer,7.
Black (1988) has noted that his disfnction for interest is stronglycorrected with th?
~urality (~" the word, afuture we cura~ntly don'tutilize.sense of interest is readily identifiable, and can bedistinguished from the non-financial uses with 92%accuracy.
Other distinctions between topic independentand topic constrained senses appear successful as well(e.g.
taste, issue, duty and sentence).Minor Sense Distinctions within a Category: Distinctionsbetween the medicinal and narcotic senses of drug ate notcaptured by the system because they both belong to thesame Roget category (REMEDY).
Similar problems occurwith the musical senses of bass.
Roget's Thesaurusoffers a rich sub-hierarchy within each category,however.
Future implementations will likely use thisinformation, which is currently ignored.Verbs: Verbs have not been considered in this particularstudy, and it appears that they may benefit from morelocal models of their typical arguments.
The unmodifiedsystem does seem to perform well on verbs which showclear topic distinctions such as fire.
It's weapon, engine,furnace, employee, imagination and pottery senses havebeen disambiguated with 85% accuracy.Pre-Nominal Modifiers: The disambiguation of pre-nominal modifiers (adjectives and compound nominals)is heavily dependent on the noun modified, and much lessso on distant context.
While class-based Bayesiandiscrimination may be useful here as well, the optimalwindow size is much narrower.Idioms: These broad context, topic-based discriminatorsare also less successful in dealing with a word like hand,which is usually found in fixed expressions such us on theother hand and close at hand.
These fixed expressionshave more function than content, and therefore, they donot lend themselves to a method that depends ondifferences in content.
The situation is far from hopeless,as many idioms are listed directly in Roget's Thesaurusand can be associated with a category through simpletable lookup.
Other research, such as Smadja andMcKeown (1990), have shown more general ways ofidentifying and handling these fixed expressions andcollocations.Given the broad set of issues involved in sensedisambiguation, it is reasonable touse several specializedtools in cooperation.
We akeady handle part of speechdistinctions through other methods; an efficient idiomrecognizer would be an appropriate addition as well.5.
Linking Roget Categories with otherSense  RepresentationsThe Roget category names tend to be highly mnemonicand may well suffice as sense tags.
However, one maywant to link the Roget tags with an established referencesuch as the sense numbers one finds in a dictionary.
Weaccomplish this by applying the models described aboveto the text of the definitions in a dictionary, creating atable of correspondences between Roget categories andACRES DE COLING-92, NANTES, 23-28 AOUT 1992 4 5 8 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992sense numbers.
Results for the word crane are illustratedbelow for two dictionaries: (1) COBUILD (Sinclair,1987), and (2) Collins English Dictionary, First Edition(CED1) (Hanks, 1979).RCAT Sense # DefinitionTOOLS crane 1.1ANIMAL crane 1.2ANIMAL crane lANIMAL crane 2TOOLS crane 3"IDOLS crane 4a machine with a long movablelarge bird with a long neck andany large long-necked long-legany similar bird, such as a hera device for lifting and movinga large trolley carrying aboomIt may also be possible to link Roget category tags with"natural" sense tags, such as translations in a foreignlanguage.
We use a word-aligned parallel bilingualcorpus uch as the French-English Canadian Hansards forthis purpose.
For example, consider the polysemousword duty which can be translated into French its devoiror droit, depending on the sense (obligation or tax,respectively).
When the Grolier-trained models areapplied to the English side of the Hansards, the wordstagged PRICE.FI~ most commonly aligned with theFrench words droits (256), droit (96) and douane (67).Words labeled OUT'/(the Roget category for Obligation)most frequently aligned with devoir (205).
Thesecorrelations may have useful implications for machinetranslation and bilingual lexicography.6.
Other Sense Disambiguation Methods:The Knowledge  Acqu is i t ion  Bot t leneckWord sense disambiguation is a long-standing problem incomputational linguistics (Kaplan, 1950 ; Yngve, 1955;Bar-Hillel, 1960), with important implications for avariety of practical applications including speechsynthesis, information retrieval, and machine translation.Most approaches may be characterized by the lollowinggeneralizations: 1) They tend to focus on the search forsets of word-specific features or indicators (typicallywords in context) which can disambignate the senses of aword.
2) Efforts to acquire these indicators have faced aknowledge acquisition bottleneck, characterized byeithersubstantial human involvement for each word, and/orincomplete vocalmlary coverage.The AI community has enjoyed some success hand-coding detailed "word experts" (Small and Rieger, 1982;HirsL 1987), but this labor intensive process has severelylimited coverage beyond small vocabularies.Others such as Lesk (1986), Walker (1987), Veronis andIde (1990), and Guthrie et al (1991) have turned tomachine readable dictionaries (MRD's) in an effort toachieve broad vocabulary coverage.
MRD's have theuseful property that some indicative words for each senseare directly available in numbered definitions andexamples.
However, definitions arc often too short toprovide an adequate set of indicators, and those wordswhich are found lack significance weights to identifywhich are crucial and which are merely chaff.Dictionaries provide well structured but incompleteinformation.Recently, many have turned to text corpora to broadenthe range and volume of available examples.
Unlikedictionaries, however, raw corpora do not indicate whichsense of a word occurs at a given instance.
Severalresearchers (Kelly and Stone, 1975; Black, 1988) haveovercome this through and tagging of training examples,and were able to discover useful discriminatory patternsfrom the partitioned contexts.
This also has proved laborintensive.
Others (Weiss, 1973; Zeroik, 1990; Hearst,1991) have attempted to partially automate the hand-tagging process through bootstrapping.
Yet this has stillrequired significant human intervention for each word inthe vocabulary.Brown et al (1991), Dagan (1991), and Gale ct at.
(1992)have looked to parallel bilingual corpora to furtherautomate training set acquisition.
By identifying wordcorrespondences in a bilingual text such as the CanadianParliamentary Proceedings (Hansards), the translationsfound fur each English word may serve as sense tags.
Forexample, the senses of sentence may be identifiedthrough their correspondence in the French to phrase(grammatical sentence) or peine (legal sentence).
Whilethis method has been used successfully on a portion ofthe vocabulary, its coverage is also limited.
Currentlyavailable bilingual corpora lack size or diversity: overhulf of the words considered in this study either neverappear in the Hansards or lack examples of secondarysenses.
More fundamentally, many words are mutuallyambiguous across languages.
French would be of littleuse in disambiguating the word interest, as all majorsenses translate as int~rdt.
More promising is a non-lndoEuropean language such as Japanese, which should avoidsuch mutual ambiguity for etymological reasons.
Untilmore diverse, large bilingual corpora become available,the coverage of these methods will remain limited.Each of these approaches have faced a fundamentalobstacle: word sense is an abstract concept hat is notidentified in natural texL Hence any system which hopesto acquire discriminators for specific senses of a wordwill need to isolate samples of those senses.
While thisprocess has been partially automated, it appears to requiresubstantial human intervention to handle an unrestrictedvocabulary.7.
ConclusionThis paper has described an approach to word sensedisambiguation using statistical models of word classes.This method overcomes the knowledge acquisitionbottleneck faced by word-specific sense discriminators.By entirely circumventing the issue of polysemyAClXS DE COLING-92, NANqVS, 23-28 Aovr 1992 4 5 9 PRec.
OF COLING-92, NAI'~fES, AUG. 23-28.
1992resolution in training material acquisition, the system hasacquired an extensive set of sense discriminators fromunrestricted monolingnal texts withoat hamanintervention.
Class models also offer the additionaladvantages of smaller model storage requirements andincreased implementation efficiency due to reduceddimensionality.
Also, they can correctly identify a wordsense which occurs rarely or only once in the corpus - -performance unattainable by statistically trained word-specific models.
These advances are not without cost, asclass-based models have diluted discriminating powerand may not capture highly indicative collocationsspecific to only one word.
Despite the inherenthandicaps, the system performs better than severalprevious approaches, based on a direct comparison ofresults for the same words.8.
AcknowledgementsSpecial thanks are due to Ken Church and Barbara Groszfor their invaluable help in restructuring this paper, and toBill Gale for the theoretical fonndalions on which thiswork rests.
The author is also grateful to Marts Hearst,Femando Pemira, Donald Hindle, Richard Sproat, andMichael Riley for their comments and suggestions.ReferencesBar-Hillel (1960).
"Automatic Translation fLanguages," in Advancesin Coenpmera, Donald Booth and R. E. Meagher, eds., Academic, NewYork.Black, Ez~ (1988), "'An Experiment in Computational Discriminationof English Word Sea~s."
IBM Journal of Research and Dev?lopmenl,v 32. pp 185-194.Brown, Pr.mr, Stephen Della Pietra, Vincent Della Pinata, and RobertMercer (1991), "Word Sense Disambiguition using StatisticalMethods," Prooteding$ ofthe 2?th Annual Meeting of the Association\[or Computational Linguistics, pp 264-270.Brown, Peter, Vil,,c~at Delh Pintra, Peter deSouza, nd Rck~rt Mercer(1990), "clasa-based n-gram Modeht of Natural Language,"Proceedings of the IBM Natural Language ITL, Paris, Fnmce, pp 283-298.C~apman, Robert (1977).
Roget's International Thesaur~ (FourthEdition), Haq~r and Row, New York,Choueka, Ymmov, and Serge Lusignam (1985).
"'Disambiguation byShort Contexts," Computera and the ltwnanities, v 19. pp.
14%158.Omtch, Kmneth (1989), "A Stochastic Parts Program an Noun PhnseParser for Un~strict~d Text, '~ Proceeding, IEEE InternationalConference on Acovatics, Speech and Signal Processing, Glasgow.Clear, Jeremy (1989).
"'An Experiment in Automatic Word Senseld~RificJtlon."
Internal Doctwnent, Oxford Univerlity Press, Oxford.Courdl, Garriton (1989).
A Connectionist Appre.ach to Word SenseDisamblguatioa, Pitman, London.Dagan, 13o, Alon leaS, and Olrike Schwall (1991), "Two Languages amInfmmative than One," Proceedings ofthe 29th Annual Meetingof the Aesoclation for Computatiosal Linguistics.
pp 130-137.Gale, William, Kenneth (:hutch, and David Yarowsky (1992),"Ditcriminatlon Decisions for 100,000-Dimensional Sp ces" AT&TStatistical Retear?.h Report No.
103.Gale, William, Kenneth Church, and David Yarowsky (1992), "AMethod for Disarnbiguating Word S~ses in ?
Large Ca~.ts," to appearin Computers and llumdnitits,Gnmger, Richard (1977), "FOUL-UP A program that figures outmeanings ofwo~ from ?~ntext," HCAII-77, pp.
172-178.Guthile, J., L Guthrle, Y.
Walks, and H. Aidinejad (1991), "Subject-Dependent Co-oc.cunea~ and Word Sense Disambiguation,"Proceedings of the 29th Annual M, teeing of the Association forCompulmlanal Linguistics, pp 146-152.Hanks, Patrick (ed.)
(1979), Collins English Dictionary, Collins,London and Glasgow,He.am, Matti (1991), '*Noun Hctnograph Disambiguation Using LocalContext in Large Text Corpora," Using Corpora, Univenrity ofWaterloo, Waterloo, Ontario,\]tirst, Graerne.
(1987), Stmamic luterpretation a d the Resolution ofAmbiguity, Cambridge University Pl~ss, Cambridge.K~plan, Abraham 0950), "An Experimental Study of Ambiguity inContext," cited in Mechanical Translation, v. I, nos.
1-3.Kelly, ,Edward, and Phillip Stone (1975), Computer Recognition ofEnglish Word Senses, North-HoUand, Amsterdam.Lask, Michael (1986), "Automatic Sense Disambiguadoa: How to tell ?Pine Cone from an Ice Cream Cone," Proceeding of the 1986 SIGDOCConference, Association for Ct~nputing Machinery, New York.Miller, George (1990), "Woednea: An On-line Leaical Database,"InterncUionalJournal ofLexicography, 4(3), 1990.
(Special Issue).Moate\].ler, F edrick, and David Wallace (1964).
Inference and DisputedAuthorxhip: The Federalist, Addison-Wesley, Reading, Mastacinamtts.Salton, G. (1989), Automatic Text Processing, Addis0n-WesleyPublishing Co.Sinclair, J., Ilanks, P., Fox, G., Moon, R., Stock, P. et el.
(edl.)
(1987)Collin~ Cobuild English Language Dictionary, Collins, London andGlasgow.Slator, Brian (1991), "Using Context for Sense Prefenmce," in Zcmik(ed.)
Lexical AcqioMtion: E:9~ioitiog On-Line ResoW:ces to B~Id aL~icon, Lawrence Edbamn, Hillsdale, N'J.Smadja, F. and K. McKeown (1990), "'Automatically Ext~cting andRepresenting Collocations for Language Generation," Proceedings ofthe 21tth Annual Meeting of the Association for ComputationalLinguistics.Small, S. and C. Rieger (1982), "'Parring and Contprehending withWord Experts (A Theory and its Realization),'* in StrategiesfgrNaluralLanguage Processing, W, Lehnert and M, Ringle, eds., LawrenceErlbaum Associates, Hillsdale, NJ.Walker, Donald (1987), "Ka\]owledge R source Tools for Aeo~'ssingLarge Text Files," in Machine Translation: Theoretical andMethodological Issues, Serges Nirenberg, ed., Cambridge UniversityPm~s, Cambridge, England.Weiss, Stephen (1973), "Learning to Disamb/guate," InformationStorage and Rari~val.
v. 9. pp 33-4 I,Veronis, Jean and Nancy lde (1990), "Word Sense Disamliiguation wilhVery Large Neural Networks Extracted from Machine ReadableDictionaries," in Proceedings COLING-90 , pp 389-394.Yngve, Victor (1955), "Syntax and the Problem of Multiple Meaning,"in Machine Translation of Languages.
William Locke and DonaldBooth, eds., Wiley, New York.Zemik, Un (1990) "Tagging Word Senses in a Corpus: The Nee.die inthe Haystack Revisited," in Text-Bated Intelligenl Systems: CurremResearch in Text Analysis, Information Extraction, and Retrisval, P.S.Jacobs, ed., GE Research & Devdopmemt Center, Schetw..oady, NewYork.AcrEs DE COLING-92, NANTES.
23-28 AO~r 1992 4 6 0 PROC.
OF COL1NG-92, NANrFES, AUG. 23-28, 1992
