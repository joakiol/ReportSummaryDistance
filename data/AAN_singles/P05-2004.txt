Proceedings of the ACL Student Research Workshop, pages 19?24,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsJointly Labeling Multiple Sequences: A Factorial HMM ApproachKevin DuhDepartment of Electrical EngineeringUniversity of Washington, USAduh@ee.washington.eduAbstractWe present new statistical models forjointly labeling multiple sequences andapply them to the combined task of part-of-speech tagging and noun phrase chunk-ing.
The model is based on the FactorialHidden Markov Model (FHMM) with dis-tributed hidden states representing part-of-speech and noun phrase sequences.
Wedemonstrate that this joint labeling ap-proach, by enabling information sharingbetween tagging/chunking subtasks, out-performs the traditional method of tag-ging and chunking in succession.
Fur-ther, we extend this into a novel model,Switching FHMM, to allow for explicitmodeling of cross-sequence dependenciesbased on linguistic knowledge.
We reporttagging/chunking accuracies for varyingdataset sizes and show that our approachis relatively robust to data sparsity.1 IntroductionTraditionally, various sequence labeling problems innatural language processing are solved by the cas-cading of well-defined subtasks, each extracting spe-cific knowledge.
For instance, the problem of in-formation extraction from sentences may be brokeninto several stages: First, part-of-speech (POS) tag-ging is performed on the sequence of word tokens.This result is then utilized in noun-phrase and verb-phrase chunking.
Finally, a higher-level analyzerextracts relevant information based on knowledgegleaned in previous subtasks.The decomposition of problems into well-definedsubtasks is useful but sometimes leads to unneces-sary errors.
The problem is that errors in earliersubtasks will propagate to downstream subtasks, ul-timately deteriorating overall performance.
There-fore, a method that allows the joint labeling of sub-tasks is desired.
Two major advantages arise fromsimultaneous labeling: First, there is more robust-ness against error propagation.
This is especiallyrelevant if we use probabilities in our models.
Cas-cading subtasks inherently ?throws away?
the prob-ability at each stage; joint labeling preserves the un-certainty.
Second, information between simultane-ous subtasks can be shared to further improve ac-curacy.
For instance, it is possible that knowing acertain noun phrase chunk may help the model inferPOS tags more accurately, and vice versa.In this paper, we propose a solution to thejoint labeling problem by representing multiple se-quences in a single Factorial Hidden Markov Model(FHMM) (Ghahramani and Jordan, 1997).
TheFHMM generalizes hidden Markov models (HMM)by allowing separate hidden state sequences.
In ourcase, these hidden state sequences represent the POStags and phrase chunk labels.
The links between thetwo hidden sequences model dependencies betweentags and chunks.
Together the hidden sequencesgenerate an observed word sequence, and the task ofthe tagger/chunker is to invert this process and inferthe original tags and chunks.Previous work on joint tagging/chunking hasshown promising results.
For example, Xun et19Figure 1: Baseline FHMM.
The two hidden se-quences y1:t and z1:t can represent tags and chunks,respectively.
Together they generate x1:t, the ob-served word sequence.al.
(2000) uses a POS tagger to output an N-best listof tags, then a Viterbi search to find the chunk se-quence that maximizes the joint tag/chunk probabil-ity.
Florian and Ngai (2001) extends transformation-based learning tagger to a joint tagger/chunker bymodifying the objective function such that a trans-formation rule is evaluated on the classificationof all simultaneous subtasks.
Our work is mostsimilar in spirit to Dynamic Conditional RandomFields (DCRF) (Sutton et al, 2004), which alsomodels tagging and chunking in a factorial frame-work.
Some main differences between our modeland DCRF may be described as 1) directed graphicalmodel vs. undirected graphical model, and 2) gener-ative model vs. conditional model.
The main advan-tage of FHMM over DCRF is that FHMM requiresconsiderably less computation and exact inference iseasily achievable for FHMM and its variants.The paper is structured as follows: Section 2 de-scribes in detail the FHMM.
Section 3 presents anew model, the Switching FHMM, which representscross-sequence dependencies more effectively thanFHMMs.
Section 4 discusses the task and data andSection 5 presents various experimental results Sec-tion 6 discusses future work and concludes.2 Factorial HMM2.1 Basic Factorial HMMA Factorial Hidden Markov Model (FHMM) is ahidden Markov model with a distributed state rep-resentation.
Let x1:T be a length T sequence of ob-served random variables (e.g.
words) and y1:T andz1:T be the corresponding sequences of hidden statevariables (e.g.
tags, chunks).
Then we define theFHMM as the probabilistic model:p(x1:T , y1:T , z1:T ) (1)= pi0T?t=2p(xt|yt, zt)p(yt|yt?1, zt)p(zt|zt?1)where pi0 = p(x0|y0, z0)p(y0|z0)p(z0).
Viewedas a generative process, we can say that thechunk model p(zt|zt?1) generates chunks depend-ing on the previous chunk label, the tag modelp(yt|yt?1, zt) generates tags based on the previ-ous tag and current chunk, and the word modelp(xt|yt, zt) generates words using the tag and chunkat the same time-step.This equation corresponds to the graphical modelof Figure 1.
Although the original FHMM de-veloped by Ghahramani (1997) does not explicitlymodel the dependencies between the two hiddenstate sequences, here we add the edges between they and z nodes to reflect the interaction between tagand chunk sequences.
Note that the FHMM can becollapsed into a hidden Markov model where thehidden state is the cross-product of the distributedstates y and z.
Despite this equivalence, the FHMMis advantageous because it requires the estimation ofsubstantiatially fewer parameters.FHMM parameters can be calculated via maxi-mum likelihood (ML) estimation if the values of thehidden states are available in the training data.
Oth-erwise, parameters must be learned using approx-imate inference algorithms (e.g.
Gibbs sampling,variational inference), since exact Expectation-Maximization (EM) algorithm is computationallyintractable (Ghahramani and Jordan, 1997).
Givena test sentence, inference of the correspondingtag/chunk sequence is found by the Viterbi algo-rithm, which finds the tag/chunk sequence that max-imizes the joint probability, i.e.arg maxy1:T ,z1:Tp(x1:T , y1:T , z1:T ) (2)2.2 Adding Cross-Sequence DependenciesMany other structures exist in the FHMM frame-work.
Statistical modeling often involves the it-erative process of finding the best set of depen-dencies that characterizes the data effectively.
Asshown in Figures 2(a), 2(b), and 2(c), dependen-20cies can be added between the yt and zt?1, be-tween zt and yt?1, or both.
The model in Fig.
2(a)corresponds to changing the tag model in Eq.
1 top(yt|yt?1, zt, zt?1); Fig.
2(b) corresponds to chang-ing the chunk model to p(zt|zt?1, yt?1); Fig.
2(c),corresponds to changing both tag and chunk models,leading to the probability model:T?t=1p(xt|yt, zt)p(yt|yt?1, zt, zt?1)p(zt|zt?1, yt?1)(3)We name the models in Figs.
2(a) and 2(b) asFHMM-T and FHMM-C due to the added depen-dencies to the tag and chunk models, respectively.The model of Fig.
2(c) and Eq.
3 will be referred toas FHMM-CT.
Intuitively, the added dependencieswill improve the predictive power across chunk andtag sequences, provided that enough training dataare available for robust parameter estimation.
(a) (b) (c)Figure 2: FHMMs with additional cross-sequencedependencies.
The models will be referred to as (a)FHMM-T, (b) FHMM-C, and (c) FHMM-CT.3 Switching Factorial HMMA reasonable question to ask is, ?How exactly doesthe chunk sequence interact with the tag sequence?
?The approach of adding dependencies in Section 2.2acknowledges the existence of cross-sequence inter-actions but does not explicitly specify the type ofinteraction.
It relies on statistical learning to findthe salient dependencies, but such an approach isfeasable only when sufficient data are available forparameter estimation.To answer the question, we consider how thechunk sequence affects the generative process fortags: First, we can expect that the unigram distri-bution of tags changes depending on whether thechunk is a noun phrase or verb phrase.
(In a nounphrase, nouns and adjective tags are more com-mon; in a verb phrase, verbs and adverb tags aremore frequent.)
Similarly, a bigram distributionp(yt|yt?1) describing tag transition probabilities dif-fers depending on the bigram?s location in the chunksequence, such as whether it is within a noun phrase,verb phrase, or at a phrase boundary.
In other words,the chunk sequence interacts with tags by switchingthe particular generative process for tags.
We modelthis interaction explicitly using a Switching FHMM:p(x1:T , y1:T , z1:T ) (4)=T?t=1p(xt|yt, zt)p?(yt|yt?1)p?
(zt|zt?1)In this new model, the chunk and tag are now gen-erated by bigram distributions parameterized by ?and ?.
For different values of ?
(or ?
), we havedifferent distributions for p(yt|yt?1) (or p(zt|zt?1)).The crucial aspect of the model lies in a function?
= f(z1:t), which summarizes information in z1:tthat is relevant for the generation of y, and a func-tion ?
= g(y1:t), which captures information in y1:tthat is relevant to the generation of z.In general, the functions f(?)
and g(?)
partitionthe space of all tag or chunk sequences into sev-eral equivalence classes, such that all instances ofan equivalence class give rise to the same genera-tive model for the cross sequence.
For instance, allconsecutive chunk labels that indicate a noun phrasecan be mapped to one equivalence class, while labelsthat indicate verb phrase can be mapped to another.The mapping can be specified manually or learnedautomatically.
Section 5 discusses a linguistically-motivated mapping that is used for the experiments.Once the mappings are defined, the parametersp?
(yt|yt?1) and p?
(zt|zt?1) are obtained via max-imum likelihood estimation in a fashion similar tothat of the FHMM.
The only exception is that nowthe training data are partitioned according to themappings, and each ?- and ?- specific generativemodel is estimated separately.
Inference of the tagsand chunks for a test sentence proceeds similarly toFHMM inference.
We call this model a SwitchingFHMM since the distribution of a hidden sequence?switches?
dynamically depending on the values ofthe other hidden sequence.An idea related to the Switching FHMM is theBayesian Multinet (Geiger and Heckerman, 1996;21Bilmes, 2000), which allows the dynamic switchingof conditional variables.
It can be used to implementswitching from a higher-order model to a lower-order model, a form of backoff smoothing for deal-ing with data sparsity.
The Switching FHMM differsin that it switches among models of the same order,but these models represent different generative pro-cesses.
The result is that the model no longer re-quires a time-homogenous assumption for state tran-sitions; rather, the transition probabilities changedynamically depending on the influence across se-quences.4 POS Tagging and NP Chunking4.1 The TasksPOS tagging is the task of assigning words thecorrect part-of-speech, and is often the first stageof various natural language processing tasks.
Asa result, POS tagging has been one of the mostactive areas of research, and many statistical andrule-based approach have been tried.
The mostnotable of these include the trigram HMM tagger(Brants, 2000), maximum entropy tagger (Ratna-parkhi, 1996), transformation-based tagger (Brill,1995), and cyclic dependency networks (Toutanovaet al, 2003).Accuracy numbers for POS tagging are often re-ported in the range of 95% to 97%.
Althoughthis may seem high, note that a tagger with 97%accuracy has only a 63% chance of getting alltags in a 15-word sentence correct, whereas a 98%accurate tagger has 74% (Manning and Schu?tze,1999).
Therefore, small improvements can be sig-nificant, especially if downstream processing re-quires correctly-tagged sentences.
One of the mostdifficult problems with POS tagging is the handlingof out-of-vocabulary words.Noun-phrase (NP) chunking is the task of findingthe non-recursive (base) noun-phrases of sentences.This segmentation task can be achieved by assign-ing words in a sentence to one of three tokens: B for?Begin-NP?, I for ?Inside-NP?, or O for ?Outside-NP?
(Ramshaw and Marcus, 1995).
The ?Begin-NP?
token is used in the case when an NP chunkis immediately followed by another NP chunk.
Thestate-of-the-art chunkers report F1 scores of 93%-94% and accuracies of 87%-97%.
See, for exam-ple, NP chunkers utilizing conditional random fields(Sha and Pereira, 2003) and support vector machines(Kudo and Matsumoto, 2001).4.2 DataThe data comes from the CoNLL 2000 shared task(Sang and Buchholz, 2000), which consists of sen-tences from the Penn Treebank Wall Street Journalcorpus (Marcus et al, 1993).
The training set con-tains a total of 8936 sentences with 19k unique vo-cabulary.
The test set contains 2012 sentences and8k vocabulary.
The out-of-vocabulary rate is 7%.There are 45 different POS tags and 3 differentNP labels in the original data.
An example sentencewith POS and NP tags is shown in Table 1.The move could pose a challengeDT NN MD VB DT NNI I O O I ITable 1: Example sentence with POS tags (2nd row) and NPlabels (3rd row).
For NP, I = Inside-NP, O=Outside-NP.5 ExperimentsWe report two sets of experiments.
Experiment 1compares several FHMMs with cascaded HMMsand demonstrates the benefit of joint labeling.
Ex-periment 2 evaluates the Switching FHMM forvarious training dataset sizes and shows its ro-bustness against data sparsity.
All models areimplemented using the Graphical Models Toolkit(GMTK) (Bilmes and Zweig, 2002).5.1 Exp1: FHMM vs Cascaded HMMsWe compare the four FHMMs of Section 2 to thetraditional approach of cascading HMMs in succes-sion, and compare their POS and NP accuracies inTable 2.
In this table, the first row ?Oracle HMM?is an oracle experiment which shows what NP accu-racies can be achieved if perfectly correct POS tagsare available in a cascaded approach.
The secondrow ?Cascaded HMM?
represents the traditional ap-proach of doing POS tagging and NP chunking insuccession; i.e.
an NP chunker is applied to the out-put of a POS tagger that is 94.17% accurate.
Thenext four rows show the results of joint labeling us-ing various FHMMs.
The final row ?DCRF?
are22comparable results from Dynamic Conditional Ran-dom Fields (Sutton et al, 2004).There are several observations: First, it is im-portant to note that FHMM outperforms the cas-caded HMM in terms of NP accuracy for all but onemodel.
For instance, FHMM-CT achieves an NPaccuracy of 95.93%, significantly higher than boththe cascaded HMM (93.90%) and the oracle HMM(94.67%).
This confirms our hypothesis that joint la-beling helps prevent POS errors from propagating toNP chunking.
Second, the fact that several FHMMmodels achieve NP accuracies higher than the ora-cle HMM implies that information sharing betweenPOS and NP sequences gives even more benefit thanhaving only perfectly correct POS tags.
Thirdly, thefact that the most complex model (FHMM-CT) per-forms best suggests that it is important to avoid datasparsity problems, as it requires more parameters tobe estimated in training.Finally, it should be noted that although the DCRFoutperforms the FHMM in this experiment, theDCRF uses significantly more word features (e.g.capitalization, existence in a list of proper nouns,etc.)
and a larger context (previous and next 3tags), whereas the FHMM considers the word as itssole feature, and the previous tag as its only con-text.
Further work is required to see whether theaddition of these features in the FHMM?s genera-tive framework will achieve accuracies close to thatof DCRF.
The take-home message is that, in lightof the computational advantages of generative mod-els, the FHMM should not be dismissed as a poten-tial solution for joint labeling.
In fact, recent resultsin the discriminative training of FHMMs (Bach andJordan, 2005) has shown promising results in speechprocessing and it is likely that such advanced tech-niques, among others, may improve the FHMM?sperformance to state-of-the-art results.5.2 Exp2: Switching FHMM and Data SparsityWe now compare the Switching FHMM to the bestmodel of Experiment 1 (FHMM-CT) for varyingamounts of training data.
The Switching FHMMuses the following ?
and ?
mapping.
The mapping?
= f(z1:t) partitions the space of chunk history z1:tinto five equivalence classes based on the two mostrecent chunk labels:Model POS NPOracle HMM ?
94.67Cascaded HMM 94.17 93.90Baseline FHMM 93.82 93.56FHMM-T 93.73 94.07FHMM-C 94.16 95.76FHMM-CT 94.15 95.93DCRF 98.92 97.36Table 2: POS and NP Accuracy for Cascaded HMMand FHMM Models.Class1.
{z1:t : zt?1 = I, zt = I}Class2.
{z1:t : zt?1 = O, zt = O}Class3.
{z1:t : zt?1 = {I,B}, zt = O}Class4.
{z1:t : zt?1 = O, zt = {I,B}}Class5.
{z1:t : (zt?1, zt) = {(I,B), (B, I)}}Class1 and Class2 are cases where the tag is locatedstrictly inside or outside an NP chunk.
Class3 andClass4 are situations where the tag is leaving or en-tering an NP, and Class5 is when the tag transits be-tween consecutive NP chunks.
Class-specific tag bi-grams p?
(yt|yt?1) are trained by dividing the train-ing data according to the mapping.
On the otherhand, the mapping ?
= g(y1:t) is not used to en-sure a single point of comparison with FHMM-CT;we use FHMM-CT?s chunk model p(zt|zt?1, yt?1)in place of p?
(zt|zt?1).The POS and NP accuracies are plotted in Figures3 and 4.
We report accuracies based on the aver-age of five different random subsets of the trainingdata for datasets of sizes 1000, 3000, 5000, and 7000sentences.
Note that for the Switching FHMM, POSand NP accuracy remains relatively constant despitethe reduction in data size.
This suggests that a moreexplicit model for cross sequence interaction is es-sential especially in the case of insufficient train-ing data.
Also, for the very small datasize of 1000,the accuracies for Cascaded HMM are 84% for POSand 70% for NP, suggesting that the general FHMMframework is still beneficial.6 Conclusion and Future WorkWe have demonstrated that joint labeling with anFHMM can outperform the traditional approach ofcascading tagging and chunking in NLP.
The newSwitching FHMM generalizes the FHMM by allow-231000 2000 3000 4000 5000 6000 7000 8000 900086878889909192939495POSAccuracyNumber of training sentencesFHMM?CTSwitch FHMMFigure 3: POS Accuracy for varying data sizes1000 2000 3000 4000 5000 6000 7000 8000 900093.59494.59595.596NPAccuracyNumber of training sentencesFHMM?CTSwitch FHMMFigure 4: NP Accuracy for varying data sizesing dynamically changing generative models and isa promising approach for modeling the type of inter-actions between hidden state sequences.Three directions for future research are planned:First, we will augment the FHMM such that its ac-curacies are competitive with state-of-the-art taggersand chunkers.
This includes adding word features toimprove accuracy on OOV words, augmenting thecontext from bigram to trigram, and applying ad-vanced smoothing techniques.
Second, we plan toexamine the Switching FHMM further, especially interms of automatic construction of the ?
and ?
func-tion.
A promising approach is to learn the mappingsusing decision trees or random forests, which has re-cently achieved good results in a similar problem inlanguage modeling (Xu and Jelinek, 2004).
Finally,we plan to integrate the tagger/chunker in an end-to-end system, such as a Factored Language Model(Bilmes and Kirchhoff, 2003), to measure the over-all merit of joint labeling.AcknowledgmentsThe author would like to thank Katrin Kirchhoff, Jeff Bilmes,and Gang Ji for insightful discussions, Chris Bartels for supporton GMTK, and the two anonymous reviewers for their construc-tive comments.
Also, the author gratefully acknowledges sup-port from NSF and CIA under NSF Grant No.
IIS-0326276.ReferencesFrancis Bach and Michael Jordan.
2005.
Discriminative train-ing of hidden Markov models for multiple pitch tracking.
InProc.
Intl.
Conf.
Acoustics, Speech, Signal Processing.J.
Bilmes and K. Kirchhoff.
2003.
Factored language modelsand generalized parallel backoff.
In Proc.
of HLT/NACCL.J.
Bilmes and G. Zweig.
2002.
The Graphical Models Toolkit:An open source software system for speech and time-seriesprocessing.
In Intl.
Conf.
on Acoustics, Speech, Signal Proc.Jeff Bilmes.
2000.
Dynamic bayesian multi-networks.
In The16th Conference on Uncertainty in Artificial Intelligence.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tag-ger.
In Proceedings of the Applied NLP.Eric Brill.
1995.
Transformation-based error-driven learningand natural language processing: A case study in part ofspeech tagging.
Computational Linguistics, 21(4):543?565.Radu Florian and Grace Ngai.
2001.
Multidimensionaltransformation-based learning.
In Proc.
CoNLL.D.
Geiger and D. Heckerman.
1996.
Knowledge representationand inference in similarity netwrosk and Bayesian multinets.Artificial Intelligence, 82:45?74.Z.
Ghahramani and M. I. Jordan.
1997.
Factorial hiddenMarkov models.
Machine Learning, 29:245?275.T.
Kudo and Y. Matsumoto.
2001.
Chunking with support vec-tor machines.
In Proceedings of NAACL-2001.C.
D. Manning and H. Schu?tze, 1999.
Foundations of StatisticalNatural Language Processing, chapter 10.
MIT Press.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19:313?330.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunking usingtransformation-based learning.
In Proceedings of the ThirdWorkshop on Very Large Corpora (ACL-95).A.
Ratnaparkhi.
1996.
A maximum entropy model for part-of-speech tagging.
In Proceedings of EMNLP-1996.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Introduction tothe CoNLL-2000 shared task: Chunking.
In Proc.
CoNLL.Fei Sha and Fernando Pereira.
2003.
Shallow parsing withconditional random fields.
In Proceedings of HLT-NAACL.C.
Sutton, K. Rohanimanesh, and A. McCallum.
2004.
Dy-namic conditional random fields.
In Intl.
Conf.
MachineLearning (ICML 2004).K.
Toutanova, D. Klein, C. Manning, and Y.
Singer.
2003.Feature-rich part-of-speech tagging with a cyclic depen-dency network.
In Proc.
of HLT-NAACL.Peng Xu and Frederick Jelinek.
2004.
Random forests in lan-guage modeling.
In Proc.
EMNLP.E.
Xun, C. Huang, and M. Zhou.
2000.
A unified statisticalmodel for the identification of English BaseNP.
In Proc.ACL.24
