Evaluating Task Performance for a Unidirectional Controlled LanguageMedical Speech Translation SystemNikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma,Marianne StarlanderUniversity of Geneva, TIM/ISSCO40 bvd du Pont-d'Arve, CH-1211 Geneva 4, SwitzerlandNikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch,Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch,Marianne.Starlander@eti.unige.chBeth Ann HockeyUCSCNASA Ames Research CenterMoffett Field, CA 94035bahockey@email.arc.nasa.govAbstractWe present a task-level evaluation of theFrench to English version of MedSLT, amedium-vocabulary unidirectional con-trolled language medical speech transla-tion system designed for doctor-patientdiagnosis interviews.
Our main goal wasto establish task performance levels ofnovice users and compare them to expertusers.
Tests were carried out on eightmedical students with no previous expo-sure to the system, with each student us-ing the system for a total of threesessions.
By the end of the third session,all the students were able to use the sys-tem confidently, with an average taskcompletion time of about 4 minutes.1 IntroductionMedical applications have emerged as one of themost promising application areas for spoken lan-guage translation, but there is still little agreementabout the question of architectures.
There are inparticular two architectural dimensions which wewill address: general processing strategy (statisticalor grammar-based), and top-level translation func-tionality (unidirectional or bidirectional transla-tion).
Given the current state of the art inrecognition and machine translation technology,what is the most appropriate combination ofchoices along these two dimensions?Reflecting current trends, a common approachfor speech translation systems is the statistical one.Statistical translation systems rely on parallel cor-pora of source and target language texts, fromwhich a translation model is trained.
However, thisis not necessarily the best alternative in safety-critical medical applications.
Anecdotally, manydoctors express reluctance to trust a translationdevice whose output is not readily predictable, andmost of the speech translation systems which havereached the stage of field testing rely on varioustypes of grammar-based recognition and rule-basedtranslation (Phraselator, 2006; S-MINDS, 2006;MedBridge, 2006).
Even though statistical systemsexhibit many desirable properties (purely data-driven, domain independence), grammar-basedsystems utilizing probabilistic context-free gram-mar tuning appear to deliver better results whentraining data is sparse (Rayner et al, 2005a).One drawback of grammar-based systems is thatout-of-coverage utterances will be neither recog-nized nor translated, an objection that critics havesometimes painted as decisive.
It is by no meansobvious, however, that restricted coverage is sucha serious problem.
In text processing, work on sev-eral generations of controlled language systems hasdeveloped a range of techniques for keeping userswithin the bounds of system coverage (Kittredge,2003; Mitamura, 1999).
If these techniques workfor text processing, it is surely not inconceivablethat variants of them will be equally successful forspoken language applications.
Users are usuallyable to adapt to a controlled language system givenenough time.
The critical questions are how toprovide efficient support to guide them towards thesystem's coverage, and how much time they willthen need before they have acclimatized.With regard to top-level translation functional-ity, the choice is between unidirectional and bidi-rectional systems.
Bidirectional systems arecertainly possible today1, but the arguments in fa-vor of them are not as clear-cut as might first ap-pear.
Ceteris paribus, doctors would certainlyprefer bidirectional systems; in particular, medicalstudents are trained to conduct examination dia-logues using ?open questions?
(WH-questions),and to avoid leading the patient by asking YN-questions.The problem with a bidirectional system is,however, that open questions only really work wellif the system can reliably handle a broad spectrumof replies from the patients, which is over-optimistic given the current state of the art.
In prac-tice, the system's coverage is always more or lessrestricted, and some experimentation is requiredbefore the user can understand what language it iscapable of handling.
A doctor, who uses the systemregularly, will acquire the necessary familiarity.The same might be true for a few patients, if spe-cial circumstances mean that they encounterspeech translation applications reasonably fre-quently.
Most patients, however, will have had noprevious exposure to the system, and may be un-willing to use a type of technology which theyhave trouble understanding.A unidirectional system, in which the doctormostly asks YN-questions, will never be ideal.
If,1For example, the S-MINDS system (S-MINDS, 2006)offers bidirectional translation.however, the doctor can become proficient in usingit, it may still be very much better than the alterna-tive of no translation assistance at all.To summarize, today?s technology definitelylets us build unidirectional grammar-based medicalspeech translation systems which work for regularusers who have had time to adapt to their limita-tions.
While bidirectional systems are possible, thecase for them is less obvious, since users on thepatient side may not in practice be able to use themeffectively.In this paper, we will empirically investigate theability of medical students to adapt to the coverageof unidirectional spoken language translation sys-tem.
We report a series of experiments, carried outusing a French to English speech translation sys-tem, in which medical students with no previousexperience to the system were asked to use it tocarry out a series of verbal examinations on sub-jects who were simulating the symptoms of varioustypes of medical conditions.
Evaluation will befocused on usability.
We primarily want to knowhow quickly subjects learn to use the system, andhow their performance compares to that of expertusers.2 The MedSLT systemMedSLT (MedSLT, 2005; Bouillon et al, 2005)is a unidirectional, grammar-based medical speechtranslation system intended for use in doctor-patient diagnosis dialogues.
The system is built ontop of Regulus (Regulus, 2006), an Open Sourceplatform for developing grammar-based speechapplications.
Regulus supports rapid constructionof complex grammar-based language models usingan example-based method (Rayner et al, 2003;Rayner et al, 2006), which extracts most of thestructure of the model from a general linguisticallymotivated resource grammar.
Regulus-based rec-ognizers are reasonably easy to maintain, andgrammar structure is shared automatically acrossdifferent subdomains.
Resource grammars are nowavailable for several languages, including English,Japanese (Rayner et al, 2005b), French (Bouillonet al, 2006) and Spanish.MedSLT includes a help module, whose purposeis to add robustness to the system and guide theuser towards the supported coverage.
The helpmodule uses a second backup recognizer, equippedwith a statistical language model; it matches theresults from this second recognizer against a cor-pus of utterances, which are within system cover-age and have already been judged to give correcttranslations.
In previous studies (Rayner et al,2005a; Starlander et al, 2005), we showed that thegrammar-based recognizer performs much betterthan the statistical one on in-coverage utterances,and rather worse on out-of-coverage ones.
We alsofound that having the help module available ap-proximately doubled the speed at which subjectslearned to use the system, measured as the averagedifference in semantic error rate between the re-sults for their first quarter-session and their lastquarter-session.
It is also possible to recover fromrecognition errors by selecting one of the displayedhelp sentences; in the cited studies, we found thatthis increased the number of acceptably processedutterances by about 10%.The version of MedSLT used for the experi-ments described in the present paper was config-ured to translate from spoken French into spokenEnglish in the headache subdomain.
Coverage isbased on standard headache-related examinationquestions obtained from a doctor, and consistsmostly of yes/no questions.
WH-questions and el-liptical constructions are also supported.
A typicalshort session with MedSLT might be as follows:- is the pain in the side of the head?- does the pain radiate to the neck?- to the jaw?- do you usually have headaches in the morn-ing ?The recognizer?s vocabulary is about 1000 sur-face words; on in-grammar material, Word ErrorRate is about 8% and semantic error rate (per ut-terance) about 10% (Bouillon et al, 2006).
Boththe main grammar-based recognizer and the statis-tical recognizer used by the help system weretrained from the same corpus of about 975 utter-ances.
Help sentences were also taken from thiscorpus.3 Experimental SetupIn previous work, we have shown how to build arobust and extendable speech translation system.We have focused on performance metrics definedin terms of recognition and translation quality, andtested the system on na?ve users without any medi-cal background (Bouillon et al, 2005; Rayner etal., 2005a; Starlander et al, 2005).In this paper, our primary goal was rather to fo-cus on task performance evaluation using plausiblepotential users.
The basic methodology used iscommon in evaluating usability in software sys-tems in general, and spoken language systems inparticular (Cohen et.
al 2000).
We defined a simu-lated situation, where a French-speaking doctorwas required to carry out a verbal examination ofan English-speaking patient who claimed to be suf-fering from a headache, using the MedSLT systemto translate all their questions.
The patients wereplayed by members of the development team, whohad been trained to answer questions consistentlywith the symptoms of different medical conditionswhich could cause headaches.
We recruited eightnative French-speaking medical students to playthe part of the doctor.
All of the students had com-pleted at least four years of medical school; five ofthem were already familiar with the symptoms ofdifferent types of headaches, and were experiencedin real diagnosis situations.The experiment was designed to study how wellusers were able to perform the task using theMedSLT system.
In particular, we wished to de-termine how quickly they could adapt to the re-stricted language and limited coverage of thesystem.
As a comparison point, representing near-perfect performance, we also carried out the sametest on two developers who had been active in im-plementing the system, and were familiar with itscoverage.Since it seemed reasonable to assume that mostusers would not interact with the system on a dailybasis, we conducted testing in three sessions, withan interval of two days between each session.
Atthe beginning of the first session, subjects weregiven a standardized 10-minute introduction to thesystem.
This consisted of instruction on how to setup the microphone, a detailed description of theMedSLT push-to-talk interface, and a video clipshowing the system in action.
At the end of thepresentation, the subject was given four samplesentences to get familiar with the system.After the training was completed, subjects wereasked to play the part of a doctor, and conduct anexamination through the system.
Their task was toidentify the headache-related condition simulatedby the ?patient?, out of nine possible conditions.Subjects were given definitions of the simulatedheadache types, which included conceptual infor-mation about location, duration, frequency, onsetand possible other symptoms the particular type ofheadache might exhibit.Subjects were instructed to signal the conclusionof their examination when they were sure about thetype of simulated headache.
The time required toreach a conclusion was noted in the experimentprotocols by the experiment supervisor.The subjects repeated the same diagnosis task ondifferent predetermined sets of simulated condi-tions during the second and third sessions.
The ses-sions were concluded either when a time limit of30 minutes was reached, or when the subject com-pleted three headache diagnoses.
At the end of thethird session, the subject was asked to fill out aquestionnaire.4 ResultsPerformance of a speech translation system isbest evaluated by looking at system performanceas a whole, and not separately for each subcompo-nent in the systems processing pipeline (Rayner et.al.
2000, pp.
297-pp.
312).
In this paper, we conse-quently focus our analysis on objective and subjec-tive usability-oriented measures.In Section 4.1, we present objective usabilitymeasures obtained by analyzing user-system inter-actions and measuring task performance.
In Sec-tion 4.2, we present subjective usability figures anda preliminary analysis of translation quality.4.1 Objective Usability Figures4.1.1 Analysis of User InteractionsMost of our analysis is based on data from theMedSLT system log, which records all interactionsbetween the user and the system.
An interaction isinitiated when the user presses the ?Start Recogni-tion?
button.
The system then attempts to recog-nize what the user says.
If it can do so, it nextattempts to show the user how it has interpreted therecognition result, by first translating it into theInterlingua, and then translating it back into thesource language (in this case, French).
If the userdecides that the back-translation is correct, theypress the ?Translate?
button.
This results in thesystem attempting to translate the Interlingua rep-resentation into the target language (in this case,English), and speak it using a Text-To-Speech en-gine.
The system also displays a list of ?help sen-tences?, consisting of examples that are known tobe within coverage, and which approximatelymatch the result of performing recognition with thestatistical language model.
The user has the optionof choosing a help sentence from the list, using themouse, and submitting this to translation instead.We classify each interaction as either ?success-ful?
or ?unsuccessful?.
An interaction is defined tobe unsuccessful if eitheri) the user re-initiates recognition withoutasking the system for a translation, orii) the system fails to produce a correcttranslation or back translation.Our definition of ?unsuccessful interaction?
in-cludes instances where users accidentally press thewrong button (i.e.
?Start Recognition?
instead of?Translate?
), press the button and then say nothing,or press the button and change their minds aboutwhat they want to ask half way through.
We ob-served all of these behaviors during the tests.Interactions where the system produced a trans-lation were counted as successful, irrespective ofwhether the translation came directly from theuser?s spoken input or from the help list.
In at leastsome examples, we found that when the translationcame from a help sentence it did not corresponddirectly to the sentence the user had spoken; to oursurprise, it could even be the case that the help sen-tence expressed the directly opposite question tothe one the user had actually asked.
This type ofinteraction was usually caused by some deficiencyin the system, normally bad recognition or missingcoverage.
Our informal observation, however, wasthat, when this kind of thing happened, the userperceived the help module positively: it enabledthem to elicit at least some information from thepatient, and was less frustrating than being forcedto ask the question again.Table I to Table III show the number of total in-teractions per session, the proportion of successfulinteractions, and the proportion of interactionscompleted by selecting a sentence from the helplist.
The total number of interactions required tocomplete a session decreased over the three ses-sions, declining from an average of 98.6 interac-tions in the first session to 63.4 in the second (36%relative) and 53.9 in the third (45% relative).
It isinteresting to note that interactions involving thehelp system did not decrease in frequency, but re-mained almost constant over the first two sessions(15.5% and 14.0%), and were in fact most com-mon during the third session (21.7%).Session 1Subject Interactions % Successful % HelpUser 1 57 56.1% 0.0%User 2 98 52.0% 25.5%User 3 91 63.7% 15.4%User 4 156 69.9% 10.3%User 5 86 64.0% 22.1%User 6 134 47.0% 19.4%User 7 56 53.6% 5.4%User 8 111 63.1% 26.1%AVG 98.6 58.7% 15.5%Table I Total interaction rounds, percentage ofsuccessful interactions, and interactions involvingthe help system by subject for the 1st sessionSession 2Subject Interactions % Successful % HelpUser 1 50 74.0% 2.0%User 2 63 55.6% 27.0%User 3 34 88.2% 23.5%User 4 96 57.3% 17.7%User 5 64 65.6% 21.9%User 6 93 68.8% 10.8%User 7 48 60.4% 4.2%User 8 59 79.7% 5.1%AVG 63.4 68.7% 14.0%Table II Total interaction rounds, percentage ofsuccessful interactions, and interactions involvingthe help system by subject for the 2nd sessionSession 3Subject Interactions % Successful % HelpUser 1 33 90.9% 33.3%User 2 57 56.1% 22.8%User 3 48 72.9% 29.2%User 4 67 70.2% 16.4%User 5 68 73.5% 27.9%User 6 60 70.0% 6.7%User 7 41 65.9% 14.6%User 8 57 56.1% 22.8%AVG 53.9 69.5% 21.7%Table III Total interaction rounds, percentage ofsuccessful interactions, and interactions involvingthe help system by subject for the 3rd sessionIn order to establish a performance baseline, wealso analyzed interaction data for two expert users,who performed the same experiment.
The expertusers were two native French-speaking system de-velopers, which were both familiar with the diag-nosis domain.
Table IV summarizes the results ofthose users.
One of our expert users, listed as Ex-pert 2, is the French grammar developer, and hadno failed interactions.
This confirms that recogni-tion is very accurate for users who know the cov-erage.Session 1 / Expert UsersSubject Interactions % Successful % HelpExpert 1 36 77.8% 13.9%Expert 2 30 100.0% 3.3%AVG 33 88.9% 8.6%Table IV Number of interactions, and percentagesof successful interactions, and interactionsinvolving the help componentThe expert users were able to complete the ex-periment using an average of 33 interaction rounds.Similar performance levels were achieved by somesubjects during the second and third session, whichsuggests that it is possible for at least some newusers to achieve performance close to expert levelwithin a few sessions.4.1.2 Task Level PerformanceOne of the important performance indicators forend users is how long it takes to perform a giventask.
During the experiments, the instructors notedcompletion times required to reach a definite diag-nosis in the experiment log.
Table VI shows taskcompletion times, categorized by session (col-umns) and task within the session (rows).Session 1 Session 2 Session 3Diagnosis 1 17:00 min 11:00 min 7:54 minDiagnosis 2 11:00 min 6:18 min 5:34 minDiagnosis 3 7:54 min 4:10 min 4:00 minTable V Average time required by subjects tocomplete diagnosesIn the last two sessions, after subjects had ac-climatized to the system, a diagnosis takes an aver-age of about four minutes to complete.
Thiscompares to a three-minute average required tocomplete a diagnosis by our expert users.4.1.3 System coverageTable VI shows the percentage of in-coveragesentences uttered by the users on interactions thatdid not involve invocation of the help component.IN-COVERAGE SENTENCESSession 1 54.9%Session 2 60.7%Session 3 64.6%Table VI Percentage of in-coverage sentencesThis indicates that subjects learn and adapt tothe system coverage as they use the system more.The average proportion of in-coverage utterancesis 10 percent higher during the third session thanduring the first session.4.2 Subjective Usability Measures4.2.1 Results of QuestionnaireAfter finishing the third session, subjects wereasked to fill in a short questionnaire, where re-sponses were on a five-point scale ranging from 1(?strongly disagree?)
to 5 (?strongly agree?).
Theresults are presented in Table VIII.STATEMENT SCOREI quickly learned how to use the system.
4.4System response times were generallysatisfactory.4.5When the system did not understand me,the help system usually showed me an-other way to ask the question.4.6When I knew what I could say, the sys-tem usually recognized me correctly.4.3I was often unable to ask the questions Iwanted.3.8I could ask enough questions that I wassure of my diagnosis.4.3This system is more effective than non-verbal communication using gestures.4.3I would use this system again in a simi-lar situation.4.1Table VIII Subject responses to questionnaire.Scores are on a 5-point scale, averaged over allanswers.Answers were in general positive, and most ofthe subjects were clearly very comfortable with thesystem after just an hour and a half of use.
Interest-ingly, even though most of the subjects answered?yes?
to the question ?I was often unable to ask thequestions I wanted?, the good performance of thehelp system appeared to compensate adequately formissing coverage.4.2.2 Translation PerformanceIn order to evaluate the translation quality of thenewly developed French-to-English system, weconducted a preliminary performance evaluation,similar to the evaluation method described in(Bouillon 2005).We performed translation judgment in tworounds.
In the first round, an English-speakingjudge was asked to categorize target utterances ascomprehensible or not without looking at corre-sponding source sentences.
91.1% of the sentenceswere judged as comprehensible.
The remaining8.9% consisted of sentences where the terminologyused was not familiar to the judge and of sentenceswhere the translation component failed to producea sufficiently good translation.
An example sen-tence is- Are the headaches better when you experi-ence dark room?which stems from the French source sentence- Vos maux de t?te sont ils soulag?s par obs-curit?
?In the second round, English-speaking judges,sufficiently fluent in French to understand sourcelanguage utterances, were shown the French sourceutterance, and asked to decide whether the targetlanguage utterance correctly reflected the meaningof the source language utterance.
They were alsoasked to judge the style of the target language ut-terance.
Specifically, judges were asked to classifysentences as ?BAD?
if the meaning of the Englishsentence did not reflect the meaning of the Frenchsentence.
Sentences were categorized as ?OK?
ifthe meaning was transferred correctly and the sen-tence was comprehensible, but the style of the re-sulting English sentence was not perfect.
Sentenceswere judged as ?GOOD?
when they were compre-hensible, and both meaning and style were consid-ered to be completely correct.
Table VIIIsummarizes results of two judges.Good OK BadJudge 1 15.8% 73.80% 10.3%Judge 2 46.6% 47.1% 6.3%Table VIII Judgments of the quality of the transla-tions of 546 utterancesIt is apparent that translation judging is a highlysubjective process.
When translations were markedas ?bad?, the problem most often seemed to be re-lated to lexical items where it was challenging tofind an exact correspondence between French andEnglish.
Two common examples were ?troubles dela vision?, which was translated as ?blurred vi-sion?, and ?faiblesse musculaire?, which was trans-lated as ?weakness?.
It is likely that a more carefulchoice of lexical translation rules would deal withat least some of these cases.5 SummaryWe have presented a first end-to-end evaluationof the MedSLT spoken language translation sys-tem.
The medical students who tested it were allable to use the system well, with performance insome cases comparable to that of that of systemdevelopers after only two sessions.
At least for thefairly simple type of diagnoses covered by our sce-nario, the system?s performance appeared clearlyadequate for the task.This is particularly encouraging, since theFrench to English version of the system is quitenew, and has not yet received the level of attentionrequired for a clinical system.
The robustnessadded by the help system was sufficient to com-pensate for that, and in most cases, subjects wereable to find ways to maneuver around coverageholes and other problems.
It is entirely reasonableto hope that performance, which is already fairlygood, would be substantially better with anothercouple of months of development work.In summary, we feel that this study shows thatthe conservative architecture we have chosenshows genuine potential for use in medical diagno-sis situations.
Before the end of 2006, we hope tohave advanced to the stage where we can start ini-tial trials with real doctors and patients.AcknowledgmentsWe would like to thank Agnes Lisowska, AliaRahal, and Nancy Underwood for being impartialjudges over our system?s results.This work was funded by the Swiss NationalScience Foundation.ReferencesP.
Bouillon, M. Rayner, N. Chatzichrisafis, B.A.Hockey, M. Santaholma, M. Starlander, Y. Nakao, K.Kanzaki, and H. Isahara.
2005.
A generic multi-lingual open source platform for limited-domainmedical speech translation.
In Proceedings of the10th Conference of the European Association forMachine Translation (EAMT), Budapest, Hungary.P.
Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-taholma, M. Starlander, and N. Chatzichrisafis.
2006.Une grammaire multilingue partag?e pour la recon-naissance et la g?n?ration.
In Proceedings of TALN2006, Leuwen, Belgium.M.
Cohen, J. Giangola, and J. Balogh.
2004, Voice UserInterface Design.
Addison Wesley Publishing.R.
I. Kittredge.
2003.
Sublanguages and comtrolledlanguages.
In R. Mitkov, editor, The Oxford Hand-book of Computational Linguistics, pages 430?447.Oxford University Press.MedBridge, 2006. http://www.medtablet.com/.
As of15th March 2006.MedSLT, 2005. http://sourceforge.net/projects/medslt/.As of 15th March 2006.T.
Mitamura.
1999.
Controlled language for multilin-gual machine translation.
In Proceedings of MachineTranslation Summit VII, Singapore.Phraselator, 2006. http://www.phraselator.com.
As of15 February 2006.M.
Rayner, B.A.
Hockey, and J. Dowding.
2003.
Anopen source environment for compiling typed unifi-cation grammars into speech recognisers.
In Pro-ceedings of the 10th EACL (demo track), Budapest,Hungary.M.
Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao,H.
Isahara, K. Kanzaki, and B.A.
Hockey.
2005b.Japanese speech understanding using grammar spe-cialization.
In HLT-NAACL 2005: Demo Session,Vancouver, British Columbia, Canada.
Associationfor Computational Linguistics.M.
Rayner, P. Bouillon, N. Chatzichrisafis, B.A.Hockey, M. Santaholma,M.
Starlander, H. Isahara,K.
Kankazi, and Y. Nakao.
2005a.
A methodology forcomparing grammar-based and robust approaches tospeech understanding.
In Proceedings of the 9th In-ternational Conference on Spoken Language Process-ing (ICSLP), Lisboa, Portugal.M.
Rayner, D. Carter, P. Bouillon, V. Digalakis, and M.Wir?n.
2000.
The Spoken Language Translator,Cambridge University Press.M.
Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao,H.
Isahara, K. Kanzaki, and B.A.
Hockey.
2005b.Japanese speech understanding using grammar spe-cialization.
In HLT-NAACL 2005: Demo Session,Vancouver, British Columbia, Canada.
Associationfor Computational Linguistics.M.
Rayner, B.A.
Hockey, and P. Bouillon.
2006.
Put-ting Linguistics into Speech Recognition: TheRegulus Grammar Compiler.
CSLI Press, Chicago.Regulus, 2006. http://sourceforge.net/projects/regulus/.As of 15 March 2006.S-MINDS, 2006. http://www.sehda.com/.
As of 15March 2006.M.
Starlander, P. Bouillon, N. Chatzichrisafis, M. San-taholma, M. Rayner, B.A.
Hockey, H. Isahara, K.Kanzaki, and Y. Nakao.
2005.
Practicing controlledlanguage through a help system integrated into themedical speech translation system (MedSLT).
In Pro-ceedings of the MT Summit X, Phuket, Thailand
