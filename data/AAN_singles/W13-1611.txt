Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 81?86,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsFrom newspaper to microblogging: What does it take to find opinions?Wladimir Sidorenko and Jonathan Sonntag and Manfred StedeApplied Computational LinguisticsUniversity of Potsdam/Germanysidarenk|sonntag|stede@uni-potsdam.deNina Kru?ger and Stefan StieglitzDept.
of Information SystemsUniversity of Mu?nster/Germanynina.krueger|stefan.stieglitz@uni-muenster.deAbstractWe compare the performance of two lexicon-based sentiment systems ?
SentiStrength(Thelwall et al 2012) and SO-CAL (Taboadaet al 2011) ?
on the two genres of newspapertext and tweets.
While SentiStrength has beengeared specifically toward short social-mediatext, SO-CAL was built for general, longertext.
After the initial comparison, we suc-cessively enrich the SO-CAL-based analysiswith tweet-specific mechanisms and observethat in some cases, this improves the perfor-mance.
A qualitative error analysis then iden-tifies classes of typical problems the two sys-tems have with tweets.1 Introduction: Twitter, SentiStrength andSO-CALIn recent years, microblogging has been an attrac-tive new target for sentiment analysis.
The questionstudied in this paper is how the methods used for?standard?
newspaper text can be transferred to mi-croblogs.
We focused on the Twitter network be-cause of its widespread use, and because Twittercommunication, in response to emerging issues, isfast and especially ad hoc, making it an effectiveplatform for the sharing and discussion of crisis-related information (Bruns/Burgess, 2011).
Further-more, Twitter is characterized by a high topicality ofcontent (Milstein al., 2008).Specifically, we present experiments involvingtwo sentiment analysis systems that both employa combination of polarity lexicon and sentimentcomposition rules: (i) SentiStrength (Thelwall etal., 2012), a system that is geared toward shortsocial-media text, and (ii) SO-CAL (Taboada et al2011), ?Semantic Orientation Calculator?, a general-purpose system that was designed primarily to workon the level of complete texts.
While both arelexicon-based approaches, there are certain differ-ences in the roles of the various submodules.
For ourpurposes here, it is important that SentiStrength wasdesigned to cope specifically with ?user-generatedcontent?.
Among the features of the system, asstated by Thelwall et al the following four are espe-cially important for tweets: (i) a simple spelling cor-rection algorithm deletes repeated letters when theword is not found in the dictionary; (ii) repeated let-ters lead to a boost in sentiment value; (iii) an emoti-con list supplements the polarity lexicon; (iv) pos-itive sentences ending in an exclamation mark re-ceive an additional boost, and multiple exclamationmarks further strengthen the polarity.SO-CAL, on the other hand, does not includesocial-media-specific measures.
In contrast, it wasdesigned for determining semantic orientation onthe text level; in our experiments here, we are thususing it for the non-intended purpose of sentence-level sentiment, on tweet ?sentences?.Next, we review related work on twitter sentimentanalysis (Section 2), and describe the data sets forour experiments in Section 3.
Then we investigatethe relative performance of SentiStrength and SO-CAL on newspaper text and on tweets (Section 4),including experiments with preprocessing steps.
InSection 5, we present observations from a qualitativeevaluation, and we interpret the results and concludein Section 6.812 Related workFollowing the work on ?standard?
text, sentimentclassification on tweets is often treated as a two-steptask, e.g., (Barbosa/Feng, 2010): subjectivity classi-fication followed by polarity classification.
For sub-jectivity classification, (Pak/Paroubek, 2010) foundthat the distribution of POS tags is a useful feature,due to, for example, the presence of modal verbs insubjective tweets.For polarity assignment, one approach is to au-tomatically build large sets of training data andthen train classifiers on token n-grams; in this vein,(Pak/Paroubek, 2010) found that in their approach,bigrams outperform unigrams and trigrams, andthey report f-measures around 0.6 for the three-way pos/neg/neutral classification.
The other, non-learning, approach is to rely on a polarity wordlist(or a collection of several, as in (Joshi et al 2011;Mukherjee et al 2012)).
Mukherjee et alreportan accuracy of 66.69% for pos/neg, and 56.17% forpos/neg/neut classification.Typical preprocessing steps employed by theapproaches discussed are the correction of mis-spellings, the replacement of URLs and hashtags,the translation of emoticons and of slang words.Sometimes, stop word removal and stemming isused; sometimes deliberately not.
Few authors eval-uate the influence of the various measures; one ex-ception is (Mukherjee et al 2012).A recent branch of research deals with fine-grained target-specific analysis (as proposed re-cently by (Jiang et al 2011)).
In our work, how-ever, we tackle the more coarse-grained problemof assigning a single sentiment value to a completetweet.
However, we will return to the issue of target-specificity in our conclusions.An interesting result from analysing the state ofthe art is that apparently no consensus has beenreached yet on the question of ?extra difficulty?
oftweet sentiment analysis.
While everybody agreesthat tweets are noisy and can pose considerable diffi-culty to any standard linguistically-inspired analysistool, it is not clear to what extent this is a problemfor sentiment analysis.
Some authors argue that thenoise renders the task more difficult than the anal-ysis of longer text, while others maintain that thebrevity of tweets is in fact an advantage, because ?
as(Bermingham/Smeaton, 2010) put it, ?the short doc-ument length introduces a succinctness to the con-tent?, and thus ?the focused nature of the text andhigher density of sentiment-bearing terms may ben-efit automated sentiment analysis techniques.?
Intheir evaluation, the classification of microblogs in-deed yields better results than that of blogs.In correspondence with this open question, thereare only few investigations so far on the performancedifferences for existing sentiment tools operating onnewspaper versus social media text.
To shed morelight on the issue, we chose to run a set of com-parative experiments with the two aforementionedlexicon/rule-based systems, on both newspaper andtwitter corpora.3 Data setsMPQA The well-known MPQA corpus1 (Wiebeet al 2005) of newspaper text has fine-grained an-notations of ?private states?
at phrase level.
For ourpurposes these need to be reduced to a more coarse-grained labelling of sentence-level sentiment.
Toavoid ambiguity, we ignored those sentences that in-clude both positive and negative sentiment annota-tions.
From the remaining sentences, we selected100 positive and negative sentences each, where theformer target-specific sentiment is now taken to rep-resent sentence-level sentiment.
The data set is adifficult one, given that we are dealing with isolatedsentences from newspaper reports.Qantas To track Twitter data we used a self-developed prototype (see (Stieglitz/Kaufhold,2011)).
We concentrate our analysis on Qantas, anAustralian leading carrier for long-haul air travel,for which we assume substantial interest in publiccommunication.
We furthermore expect that ?caused by some management crises in 2011 ?
onlinecommunication around Qantas-related topics ischaracterized by a strong emotional investment ofstakeholders.The tracking tool captures all those tweets thatcontain the keyword ?Qantas?
in their content, in theusername of the sender, or in a URL.
After spam re-moval, we had a dataset of some 27,000 tweets, col-lected between mid-May and mid-November 2011.1http://mpqa.cs.pitt.edu/82Topic #pos #neut #neg #irrelevantApple 219 581 377 164Google 218 604 61 498Microsoft 93 671 138 513Twitter 68 647 78 611Table 1: Distribution of tweets and labels across subcor-poraFor evaluation purposes, 300 Tweets have been man-ually annotated by two annotators in parallel, usinga polarity scale ranging from -2 to 2.
190 Tweets ofthose (63%) received identical labels, and we usedonly this set in our experiments described below.That means we also discarded cases of ?minor?
dis-agreement such as a -1/-2 annotation.Sanders The Sanders corpus2 is a corpus consist-ing of 5513 tweets of various languages which havebeen annotated for sentiment.
The tweets have beensampled by the search terms ,,@apple?, ,,#google?,,,#microsoft?
and ,,#twitter?.
Each tweet is accom-panied by a date-time stamp and the target of its po-larity.
Possible polarity values are positive, negative,neutral (simple factual statements / questions with-out strong emotions / neither positive nor negative /both positive and negative), and irrelevant (spam /non-English).
The positive and negative tweets thuscontain judgements on the companies or their prod-ucts/services.
Along with the corpus comes an anno-tation scheme and statistics about the corpus.
Somenumbers of the size and distribution within the cor-pus are given in Table 1.According to the annotation guidelines, positiveand negative labels were only assigned to clear casesof sentiment.
Ambigious tweets have been anno-tated as neutral.4 Experiments and results4.1 Performance on MPQA sentencesIn order to establish a basis for the comparison, wefirst ran a small comparative evaluation on ?stan-dard?
text, i.e., on the sentences from the MPQAnewspaper corpus.
The results, given in Table 2,show that both systems perform considerably better2http://www.sananalytics.com/lab/twitter-sentiment/SentiStrength SO-CALacc pos 0.2727 0.4717acc neg 0.7071 0.6542weighted avg 0.4899 0.5634Table 2: Accuracy on MPQA sentencesSenti- SO-CAL SO-CALStrength preproc.Qantasacc 0.3754 0.3953 0.3887acc pos 0.3091 0.2545 0.2545acc neg 0.2857 0.2857 0.2857acc neut 0.6164 0.6781 0.6644avg sentiment 1.1075 1.2756 1.3316Sanders totalacc 0.5945 0.5899 0.5790acc pos 0.6171 0.5694 0.6032acc neg 0.4572 0.5301 0.5519acc neut 0.6230 0.6092 0.5802avg sentiment 0.8517 1.3761 1.5233Sanders twitteracc 0.4985 0.5804 0.5387acc pos 0.4286 0.3750 0.4821acc neg 0.4590 0.4754 0.5246acc neut 0.5099 0.6121 0.5245avg sentiment 0.8393 1.4054 1.6978Table 3: Accuracy on tweet corporaon negative than on positive sentences, and overallthere is a slight advantage for SO-CAL.4.2 Performance on Qantas and Sanders tweetsIn Table 3, we show the system performance on theTwitter corpora: Qantas, the complete Sanders cor-pus, and the Sanders subcorpus with target ?Twit-ter?.
We ran evaluations on all four separate sub-corpora, but only ?Twitter?
showed interesting dif-ferences from the results for the total corpus, andthat is why they are included in the table.
The ?acc?row gives the overall weighted accuracy.
?Avg senti-ment?
is the absolute value of the sentiment strengthdetermined by SentiStrength and SO-CAL; noticethat these should not be compared between the twosystems, as they do not operate on the same scale.
(We will return to the role of sentiment strength inSection 6.
)834.3 Preprocessing stepsSince SO-CAL was not intended for analyzing Twit-ter data, we implemented three preprocessing stepsto study whether noise effects of this text genre canbe reduced.
Similarly to the steps suggested by(Mukherjee et al 2012), we first unified all URLs,e-mail addresses and user names by replacing themwith unique tokens.
Additionally, in step 1 all hashmarks were stripped from words, and emoticonswere mapped to special tokens representing theiremotion categories.
These special tokens were thenadded to the polarity lexicons used by SO-CAL.In step 2, social media specific slang expressionsand abbreviations like ?2 b?
(for ?to be?)
or ?im-sry?
(for ?I am sorry?)
were translated to their ap-propriate standard language forms.
For this, we useda dictionary of 5,424 expressions that we gatheredfrom publicly available resources.3In the last step, we tackled two typical spellingphenomena: the omission of final g in gerund forms(goin), and elongations of characters (suuuper).
Forthe former, we appended the character g to wordsending with -in if these words are unknown to vo-cabulary,4 while the corresponding ?g?-forms are in-vocabulary words (IVW).
For the latter problem,we first tried to subsequently remove each repeat-ing character until we hit an IVW.
For cases re-sisting this treatment, we adopted the method sug-gested by (Brody/Diakopoulos, 2011) and generateda squeezed form of the prolongated word, subse-quently looking it up in a probability table that haspreviously been gathered from a training corpus.Altogether, SO-CAL does not benefit from pre-processing in the Qantas corpus, but it does help forthe pos/neg tweets from the Sanders corpus, espe-cially for the Twitter subcorpus.
The observationthat the accuracy on neutral tweets decreases whilethe average sentiment increases will be discussedin Section 6.
We also measured the effects of thethree individual steps in isolation, and the only note-worthy result is that SentiStrength, when subjectedto our ?extra?
preprocessing, benefits slightly fromslang normalization for the Qantas corpus, and from3http://www.noslang.com/dictionary/,http://onlineslangdictionary.com/, http://www.urbandictionary.com/4For vocabulary check, we used the open Hunspell dictio-nary (http://hunspell.sourceforge.net/).noise cleaning for some parts of the Sanders corpus.5 Qualitative evaluationHaving computed the success rates, we then per-formed a small qualitative evaluation: What are themain reasons for the misclassifications on tweets?
Inaddition, we wanted to know why the Qantas corpusyielded much worse results than the Sanders corpus,and thus we looked into its results.5.1 Problems for SO-CALWe chose SO-CAL?s judgements as the basis for thisevaluation and randomly selected 120 tweets fromthe Sanders corpus that were not correctly classi-fied.
The distribution across the manual annotationspos/neg/neut was 40/40/40.In Table 4, we provide a classification of the rea-sons for problems.
The first group are cases wherewe would not agree with the annotation and thuscannot blame SO-CAL.
The second group includesproblems that are beyond the scope of the systemand hence, strictly speaking, not its fault.
Among thetypos, there are cases of misspelled opinion words,but also a few where the typo leads to problems withSO-CALs linguistic analysis and in consequence toa misclassification.
The slang words include itemslike ?wow!?
but also shorthands such as ?thx?.
Mostimportant are ?domain formulae?
: expressions thatrequire inferences in order to identify the sentiment.An example is ?I now use X instead of TARGET?.We encounter these most often in negative tweets,where complaints are expressed, as in ?My phonecan send but not receive texts.
?In the third group, we find problems that are orcould be in the scope of SO-CAL.
Occasionally,negation or irrealis rules misfire.
Gaps in the lex-icon are noticeable especially on the positive side(examples: ?loving?, ?better?, ?thanks to?).
?Lex-ical ambiguity?
refers to words that may or maynot carry polarity; by far the most frequent examplehere is ?new?, which SO-CAL labels positive, but intechnology-related tweets often is neutral.
Also inneutral tweets, we often find high complexity, i.e.,cases where both positive and negative judgementsare mixed.
And finally, a fair number of problemsstems from sentiment expressed on the wrong targetof the tweet.84Problem Pos Neg NeutAnnotation ambig.
15% 0% 2%Typo 3% 5% 10%Slang words 12% 10% 0%Sarcasm 0% 2% 0%Domain formula 23% 60% 5%Wrong rule 3% 5% 3%Lexicon gap 30% 12% 0%Lexical ambiguity 5% 5% 50%Complexity 0% 0% 18%Wrong target 8% 0% 12%Table 4: SO-CAL error types on 120 Sanders tweetsProblem Pos Neg NeutAnnotation ambig.
45% 25% 12%Typo 18% 0% 0%Slang words 0% 0% 0%Sarcasm 0% 16% 0%Domain formula 9% 42% 4%Wrong rule 9% 0% 10%Lexicon gap 9% 16% 0%Lexical ambiguity 0% 0% 16%Complexity 9% 0% 16%Spam / news 0% 0% 41%Table 5: Error types on 75 Qantas tweets5.2 Observations on the Qantas corpusThe analysis of 75 Qantas tweets that have been mis-classified by both SentiStength and SO-CAL yieldedthe results in Table 5: Again, many annotation casesare ambiguous, and domain formulae are the ma-jor problem with negative tweets.
Sarcasm is muchmore frequent than in the Sanders corpus.
The cen-tral problem for neutral tweets stems from the factthat spam and tweets containing headlines and URLsof news messages have been annotated as neutral,but these may very well contain polarity-bearingwords, which are then detected by the systems.6 Interpretation and ConlusionsNews versus tweets.
Since the Sanders corpus ismuch larger than Qantas, we regard it as the tweetrepresentative for the comparison to MPQA (a dif-ficult data set, as argued above).
For positive text,both SentiStrength and SO-CAL yield better re-sults on tweets, while for negative texts, the resultson tweets are much lower than on news sentences.Within the news genre, however, both systems per-form much better on negative than on positive text.So we conclude a ?polarity flip?
in the performanceof both systems when going from news to tweets.Differences among tweets.
Based on the Sanderscorpus, the SentiStrength and SO-CAL results area little better than those reported by (Mukherjee etal., 2012), who achieved 56.17% accuracy for thethree-way classification.
As SO-CAL does not in-clude tweet-specific analysis, we may conclude thatthe utility of such genre-specific measures is in factlimited.
?
An interesting question is why the ?Twit-ter?
subcorpus of Sanders behaves so different fromthe others: While overall accuracy is the same, thefigures for the three categories differ widely.
Also,SO-CAL here benefits heavily from preprocessingon the non-neutral tweets.
One factor is the largeproportion of neutral tweets (see Table 1); besides,we find that these tweets are not as target-related asthose for Apple, Google, Microsoft; it seems thatusers often drop a ?#twitter?
without actually talkingabout Twitter or its service.Preprocessing.
Of the four measures taken bySentiStrength to account for tweet problems (seeSct.
1), SO-CAL already implements the exclama-tion mark boost; the other three were added in ourown preprocessing, but we did only minimal spell-checking.
Overall, SO-CAL does not profit as muchas we had expected, but we find a fair improvement(0.57?0.6) for the positive Sanders tweets.
For neu-tral tweets, performance actually decreases.The role of targets An interesting observation isthat adding preprocessing to SO-CAL leads to de-tecting ?more?
sentiment: The average sentimentvalues increase for all the corpora in Table 3.
At thesame time, the accuracy on neutral tweets decreases,which indicates that ?spurious?
sentiment is beingdetected.
The most likely reason is that SO-CAL in-deed profits from tweet-preprocessing but then de-tects sentiment that is unrelated to the target andtherefore not annotated in the gold data.
An im-portant direction for future work therefore is to paymore attention to target-specific sentiment identifi-cation, cf.
(Jiang et al 2011).85AcknowledgmentsThis work was funded by German Ministry for Edu-cation and Research (BMBF), grant 01UG1232D.ReferencesL.
Barbosa and J. Feng.
2010.
Robust sentiment detec-tion on twitter from biased and noisy data.
Proc.
ofCOLING (Posters), Beijing.A.
Bermingham and A. Smeaton.
2010.
ClassifyingSentiment in Microblogs: Is Brevity an Advantage?Proc.
of the 20th ACM Conference on Information andKnowledge Management (CIKM), Toronto.S.
Brody and N. Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
UsingWord Lengthening to Detect Sentiment in Microblogs.Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pp.
562?570, Edinburgh.A.
Bruns and J.E.
Burgess.
2011.
The Use of TwitterHashtags in the Formation of Ad Hoc Publics.
6thEuropean Consortium for Political Research GeneralConference, Reykjavik, Iceland, pp.
25-27.L.
Jiang, M. Yu, M. Zhou, X. Liu and T. Zhao.2011.
Target-dependent twitter sentiment classifica-tion.
Proc.
of the 49th Annual Meeting of the ACL,pp.
151-160, Portland/OR.A.
Joshi, Balamurali A R, P. Bhattacharyya and R. Mo-hanty.
2011.
C-Feel-It: a sentiment analyzer formicro-blogs.
Proc.
of the ACL-HLT 2011 SystemDemonstrations, pp.
127-132, Portland/OR.S.
Milstein, A. Chowdhury, G. Hochmuth, B. Lorica andR.
Magoulas.
2008.
Twitter and the Micro-MessagingRevolution: Communication, Connections, and Imme-diacy - 140 Characters at a Time.S.
Mukherjee, A. Malu, A.R.
Balamurali and P. Bhat-tacharyya.
2012.
TwiSent: a multistage system foranalyzing sentiment in twitter.
Proc.
of the 21st ACMConference on Information and Knowledge Manage-ment (CIKM).A.
Pak and P. Paroubek.
2010.
Twitter as a corpusfor sentiment analysis and opinion mining.
Proc.
ofLREC, Valletta/Malta.S.
Stieglitz and C. Kaufhold.
2011.
Automatic FullText Analysis in Public Social Media ?
Adoption ofa Software Prototype to Investigate Political Commu-nication.
Proc.
of the 2nd International Conference onAmbient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on MobileWeb Information Systems (MobiWIS 2011), ProcediaComputer Science 5, Elsevier, 776-781.M.
Taboada, J. Brooke, M. Tofiloski, K. Voll, and M.Stede.
2011.
Lexicon-based methods for sentimentanalysis.
Computational Linguistics, 37(2):267?307.M.
Thelwall, K. Buckley, and G. Paltoglou.
2012.
Sen-timent strength detection for the social Web.
Journalof the American Society for Information Science andTechnology, 63(1):163?173.J.
Wiebe, T. Wilson, and C. Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, 39(2?3):165?210.86
