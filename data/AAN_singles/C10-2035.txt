Coling 2010: Poster Volume, pages 303?311,Beijing, August 2010Building Systematic Reviews Using Automatic Text ClassificationTechniquesOana Frunza, Diana Inkpen, and Stan MatwinSchool of Information Technology and EngineeringUniversity of Ottawa{ofrunza,diana,stan}@site.uottawa.caAbstractThe amount of information in medicalpublications continues to increase at atremendous rate.
Systematic reviews helpto process this growing body of informa-tion.
They are fundamental tools for evi-dence-based medicine.
In this paper, weshow that automatic text classification canbe useful in building systematic reviewsfor medical topics to speed up the review-ing process.
We propose a per-questionclassification method that uses an ensem-ble of classifiers that exploit the particularprotocol of a systematic review.
We alsoshow that when integrating the classifierin the human workflow of building a re-view the per-question method is superiorto the global method.
We test severalevaluation measures on a real dataset.1 IntroductionSystematic reviews are the result of a tediousprocess which involves human reviewers to ma-nually screen references of papers to determinetheir relevance to the review.
This process oftenentails reading thousands or even tens of thou-sands of abstracts from prospective articles.
Asthe body of available articles continues to grow,this process is becoming increasingly difficult.Common systematic review practices stipu-late that two reviewers are used at the screeningphases of a systematic review to review each ab-stract of the documents retrieved after a simplequery-based search.
After a final decision ismade for each abstract (the two reviewers decideif the abstract is relevant or not to the topic ofreview), in the next phase further analysis (morestrict screening steps) on the entire article isdone.
A systematic review has to be complete,articles that are published on a certain topic andare clinically relevant need to be part of the re-view.
This requires near-perfect recall since theaccidental exclusion of a potentially relevant ab-stract can have a significantly negative impact onthe validity of the overall systematic review (Co-hen et al, 2006).
Our goal in this paper is to pro-pose an automatic system that can help humanjudges in the process of triaging articles by look-ing only at abstracts and not the entire docu-ments.
This decision step is known as the initialscreening phase in the protocol of building sys-tematic reviews, only the abstracts are used assource of information.One reviewer will still read the entire collec-tion of abstracts while the other will benefit fromthe help of the system; this reviewer will have tolabel only the articles that will be used to trainthe classifier (ideally a small proportion forworkload reduction), the rest of the articles willbe labeled by the classifier.In the systematic review preparation, if atleast one reviewer agrees to include an abstract,the abstract will have the labeled included and itwill pass to the next screening phase; otherwise,it will be discarded.
Therefore, the benefit ofdoubt plays an important role in the decisionprocess.
When we replace one reviewer with theautomatic classifier, because we keep one humanjudge in the process, the confidence and reliabil-ity of the review is still higher while the overallworkload is reduced.
The reduction is from thetime required for two passes through the collec-tion (for the two humans) to only one pass andthe smaller part labeled by the reviewer which isassisted by the classifier.
Figure 1 presents onoverview of our proposed workflow.303Figure 1.
Embedding automatic text classification inthe process of building a systematic review.The task that needs to be solved in order to helpthe systematic review process is a text classifica-tion task intended to classify an abstract as rele-vant or not relevant to the topic of review.The hypothesis that guides our research isthat it is possible to save time for the human re-viewers and obtain good performance levels,similar to the ones obtained by humans.
In thiscurrent study we show that we can achieve thisby building a classification model that is basedon the natural human workflow used for buildingsystematic reviews.
We show, on a real data set,that a human-machine system obtains the bestresults when an ensemble of classifiers is used asthe classification model.2 Related WorkThe traditional way to collect and triage the ab-stracts from a systematic review consists in usingsimple query search techniques based on MeSH1or keywords terms.
The queries are usual Boo-lean-based and are optimized either for precisionor for recall.
The studies done by Haynes et al(1994) show that it is difficult to obtain high per-formance for both measures.The research done by Aphinyanaphongs andAliferis (2005) is probably the first application ofautomatic text classification to the task of creat-1http://www.nlm.nih.gov/mesh/ing systematic reviews.
In that paper the authorsexperimented with a variety of text classificationtechniques using the data derived from the ACPJournal Club as their corpus.
They found thatsupport vector machine (SVM) was the best clas-sifier according to a variety of measures.
Furtherwork for systematic reviews was done by Cohenet al (2006).
Their work is mostly focused on theelimination of non relevant documents.
As theirmain goal is to save work for the reviewers in-volved in systematic review preparation, theydefine a measure, called work saved over sam-pling (WSS) that captures the amount of workthat the reviewers will save with respect to abaseline of just sampling for a given value ofrecall.
The idea is that a classifier returns, withhigh recall, a set of abstracts, and only those ab-stracts need to be read to weed out the non-relevant ones.
The savings are measured withrespect to the number of abstracts that wouldhave to be read if a random baseline classifierwas used.
Such baseline corresponds to uni-formly sampling a given percentage of abstracts(equal to the desired recall) from the entire set.
InCohen et al (2006), the WSS measure is appliedto report the reduction in reviewer's work whenretrieving 95% of the relevant documents; theprecision was very low.We focus on developing a classifier for sys-tematic review preparation, relying on character-istics of the data that were not included in theCohen et al?s (2006), because the questionsasked in the preparation of the reviews are notavailable, Therefore we cannot perform a directcomparison of results here.
Also, the data setsthat they used in their experiments are signifi-cantly smaller than the one that we used.3 The Data SetA set of 47,274 abstracts with titles were col-lected from MEDLINE2 as part of a systematicreview done by the McMaster University?s Evi-dence-Based Practice Center using TrialStatCorporation?s Systematic Review System 3 , aweb-based software platform used to conductsystematic reviews.The initial set of abstracts was collected usinga set of Boolean search queries that were run for2http://medline.cos.com3http://www.trialstat.com/304the specific topic of the systematic review: ?thedissemination strategy of health care servicesfor elderly people of age 65 and over?.In the protocol applied, two reviewers work inparallel.
They read the entire collection of 47,274abstracts and answer a set of questions to deter-mine if an abstract is relevant or not to the topicof review.
Examples of questions present in theprotocol: Is this article about a disseminationstrategy or a behavioral intervention?
; Is this aprimary study?
; Is this a review?
; etc.
An ab-stract is not considered to pass to the next screen-ing phase, when the entire article is available, ifthe two reviewers respond negative to the samequestion for a certain abstract.
All other cases ofpossible responses suggest that the abstract willbe part of the next screening phase.
In this paperwe focus on the initial screening phase, the onlysource of information is the abstract and the titleof the article, with the main goal to achieve anacceptable level of recall not to mistakenly ex-clude relevant abstracts.From the entire collection of labeled ab-stracts only 7,173 are relevant.
Usually in theprocess of building systematic reviews the num-ber of non-relevant documents is much higherthan the number of relevant ones.
The initial re-trieval query is purposefully very broad, so as notto miss any relevant papers.4 MethodsThe machine learning techniques that could beused in the process of automating the creation ofsystematic reviews need to take into accountsome issues that can arise when dealing withsuch tasks.
Imbalanced data sets are usuallywhat we deal with when building reviews, theproportion of relevant articles that end up beingpresent in the review is significantly lower com-pared with the original data set.
The benefit ofdoubt will affect the quality of the data used totrain the classifier, since a certain amount ofnoise is introduced: abstracts that are in fact non-relevant can be labeled as being relevant in thefirst screening process.
The relatively high num-ber of abstracts involved in the process will makethe classification algorithms deal with a highnumber of features and the representation tech-nique should try to capture aspects pertaining ofthe medical domain.4.1 Representation TechniquesIn our current research, we use three representa-tion techniques: bag-of-words (BOW), conceptsfrom the Unified Medical Language System(UMLS), and a combination of both.The bag-of-words representation is com-monly used for text classification and we havechosen to use binary feature values.
Binary fea-ture values were shown to out-perform weightedvalues for text classification tasks in the medicaldomain as shown by Cohen et al (2006) and bi-nary values tend to be more stable in results thanfrequency values for a task similar to ours, asshown by Ma (2007).We considered feature words delimitated byspace and simple punctuation marks that ap-peared at least three times in the training data,were not part of a stop words list4, and had alength greater than three characters.
30,000 wordfeatures were extracted.
No stemming was used.UMLS concepts which are part of the U.S.National Library of Medicine 5  (NLM) knowl-edge repository are identified and extracted formthe collection of abstracts using the MetaMap6system.
This conceptual representation helped usovercome some of the shortcomings of BOWrepresentation, and allowed us to use multi-wordfeatures, medical knowledge, and higher-levelmeanings of words in context.
As Cohen (2008)shows, multi-word and medical concept repre-sentations are suitable to use.4.2 Classification AlgorithmsAs a classification algorithm we have chosen touse the complement naive Bayes (CNB) (Frankand Bouckaert, 2006) classifier from the Weka7tool.
The reason for this choice is that the CNBclassifier implements state-of-the-art modifica-tions of the standard multinomial na?ve Bayes(MNB) classifier for a classification task withhighly skewed class distribution (Drummond andHolte, 2003).
As the systematic reviews datausually contain a large majority of not relevantabstracts, resulting in a skewness reaching evenbelow 1%, it is important to use appropriate clas-sifiers.
Other classifiers, such as decision tress,4http://www.site.uottawa.ca/~diana/csi5180/StopWords5http://www.nlm.nih.gov/pubs/factsheets/umls.html6http://mmtx.nlm.nih.gov/7www.cs.waikato.ac.nz/machine learning/weka/305support vector machine, instance-based learning,and boosting, were used but the results obtainedwith CNB were always better.4.3 Global Text Classification MethodThe first method that we propose in order tosolve the text classification task that is intendedto help a systematic review process is a straight-forward machine learning approach.
We trained aclassifier, CNB, on a collection of abstracts andthen evaluated the classifier?s performance on aseparate test data set.
The power of this classifi-cation technique stands in the ability to use asuitable classification algorithm and a good rep-resentation for the text classification task; Cohenet al (2006) also used this approach.
We ran-domly split the data set described in Section 3,into a training set and a test set.
The two possibleclasses are Included (relevant) or Excluded(non relevant).
We decided to work with a train-ing set smaller than the test set because ideallygood results need to be obtained without usingtoo much training data.
We have to take intoconsideration that training a classifier for a par-ticular topic, human effort is required for annota-tion.Table 1 presents a summary of the dataalong with the class distribution in the trainingand test data sets.
We randomly sampled the datato build the training and test data sets, and theoriginal distribution of 1:5.6 between the twoclasses holds in both sets.DatasetNo.
ofabstractsClass distributionIncluded : Excluded (ratio)Training 20,000 3,056 : 16,944 (1:5.6)Testing 27,274 4,117 : 23,157 (1:5.6)Table 1.
Training and test data sets.4.3.1 Feature SelectionUsing the global method, we performed experi-ments with several feature selection algorithms.We used only the BOW representation.Chi2 is a measure that evaluates the worth of anattribute by computing the value of the chi-squared statistic with respect to the class.
Weselected the top k1 CHI2 features that are exclu-sively included (appeared only in the trainingabstracts that are classified as Included) and thetop k2 CHI2 features that are exclusively excluded(appeared only in the training abstracts that areclassified as Excluded) and used them as a rep-resentation for our data set.
We varied the k1 pa-rameter from 10 to 150 and k2 from 5 to 150 Weused a minimum of 20 features and a maximumof 300.InfoGain evaluates the worth of an attributeby measuring the information gain with respectto the class.
We run experiments when we variedthe number of selected features from 50 to 500.We used a number of 50, 100, 150, 250, 300 and500 top features.Bi-Normal Separation (BNS) is a feature se-lection technique that measures the separationbetween the threshold occurrences of a feature inone of the two classes.
The latter measure is de-scribed in detail in Forman (2002).
We used aratio of features that varies from 10 to 150 for themost representative features for the Includedclass and from 5 to 150 for the Excluded class.For some experiments the number of features forthe Included class is higher than the number offeatures for the Excluded class.
We have chosento do so because we wanted to re-balance theimbalance of classes in the training data set.
Af-ter selecting the number of Included and Ex-cluded features, we used the combination to rep-resent our entire collection of abstracts.We used the implementation from the Wekapackage for the Chi2 and InfoGain and the BNSimplementation done by Ma (2007).4.4 Per-Question Classification MethodThe second method that we propose for solvingthe task takes into account the specifics of thesystematic review process.
It takes advantage ofthe set of questions the reviewers use in the proc-ess of deciding if an abstract is relevant or not.These questions are created in the design step ofthe systematic review and almost all systematicreviews have them.
By using these questions webetter emulate how the human judges work whenbuilding systematic reviews.We have chosen to use only the questionsthat have inclusion/exclusion criteria, there werealso some opened answer questions involved inthe review, because they are the ones that areimportant for reviewers to make a decision.
Tocollect training data for each question, we usedthe same training and test data set as in the pre-vious method (but note that not all the abstracts306have answers for all the questions; therefore thetraining set sizes differ for each question).
Table2 presents the questions and data sets used.When we created a training data set for eachquestion we removed the abstracts for which wehad a disagreement between the human experts ?two different answers for a specific question,they represent noise in the training data.
Foreach of the questions from Table 2, we trained aCNB classifier on the corresponding data set.Question(Training : Included class : Excluded class)Q1 - Is this article about a dissemination strat-egy or a behavioural intervention?
(14,057:1,145:12,912)Q2 - Is the population in this article made of indi-viduals 65-year old or older or does it compriseindividuals who serve the elderly population needs(i.e.
health care providers, policy makers, organi-zations, community)?
(15,005:7,360:7,645)Q3 - Is this a primary study?
(8,825:6,895:1,930)Q4 - Is this a review?
(6,429:5,640:789)Table 2.
Data sets for the per-question classificationmethod.We used the same representation for the per-question classifiers as we did for the global clas-sifier: BOW, UMLS (the concepts that appearedonly in the new question-oriented training datasets), and the combination BOW+UMLS.
Weused each trained model to obtain a predictionfor each instance from the test set; therefore eachtest instance was assigned four prediction valuesof 0 or 1.
To assign a final class for each test in-stance, from the prediction of all four classifiers,the class of a test instance is decided according toone of the following four schemes:1.
If any one vote is Excluded, the final classof a test instance is Excluded.
This is a 1-votescheme.2.
If any two votes are Excluded, the finalclass of a test instance is Excluded.
This is a 2-vote scheme.3.
If any three votes are Excluded, the finalclass of a test instance is Excluded.
This is a 3-vote scheme.4.
If all four votes are Excluded, the finalclass of a test instance is Excluded.
This is a 4-vote scheme.When we combined of the classifiers, wegave each classifier an equal importance.5 Evaluation Measures and ResultsWhen performing the evaluation for the task ofclassifying an abstract into one of the two classesIncluded (relevant) or Excluded (non rele-vant), two objectives are of great importance:Objective 1 - ensure the completeness of the sys-tematic review (maximize the number of relevantdocuments included); Objective 2 - reduce thereviewers' workload (maximize the number ofirrelevant documents excluded).We observe that objective 1 is more impor-tant than objective 2 and this is why we decidedto report recall and precision for the Includedclass.
We also report F-measure, since we aredealing with imbalanced data sets.Besides the standard evaluation measures,we report WSS8 measure as well in order to givea clearer view of the results we obtain.As baseline for our methods we consider:two extreme baselines and a random-baselineclassifier that takes into account the distributionof the two classes in the training data set.
Thebaselines results are: Include_All ?
a baselinethat classifies everything in the majority class:Recall = 100%, Precision = 15%, F-measure =26.2%; WSS = 0% Exclude_All ?
a baseline thatclassifies everything as Excluded: Recall = 0%,Precision = 100%, F-measure = 64.2%; WSS =0% Random baseline: Recall = 8.9%, Precision =15.4%, F-measure = 67.8%; WSS = 0.23%.5.1 Results for the Global MethodIn this subsection, we present the results obtainedusing our global method with the three represen-tation techniques and CNB as classification algo-rithm.
To get a clear image of the results weshow the confusion matrix in Table 3 for thereader to better understand the workload reduc-tion when using classifiers to help the process ofbuilding systematic reviews.BOW features were identified following theguidelines presented in Section 3.4 and a numberof 23,906 features were selected.
UMLS con-cepts were identified using the MetaMap system.8WSS = (TE + FE)/(TE + FE + TI + FI) ?
1+ TI/(TI + FE)where T stands for true; F ?
false I ?
Included class; E- Ex-cluded class.307BOW UMLS BOW+UMLSTrue Inc.  2,692 2,793 2,715False Inc. 5,022 8,922 5,086True Exc.
18,135 14,235 18,071False Exc.1,425 1,324 1,402Recall 65.3% 67.8% 65.9%Precision 34.9% 23.8% 34.8%F-measure 45.5% 35.2% 45.5%WSS 37.1% 24.9% 37.3%Table 3.
Results for the global method.From the whole training abstracts collection,a number of 459 UMLS features were identified.Analyzing the results from Table 5, in terms ofrecall, the UMLS representation obtained thebest recall results, 67.8% for the global methodbut much lower precision, 23.8% than BOW rep-resentation, 34.9%.
The hybrid representation,BOW+ UMLS features had similar results withthe BOW alone.
Recall increased a bit for thehybrid representation compared to BOW alone,0.6% but its value is still not acceptable.
Weconclude that the levels of recall, our main objec-tive for this task, were not acceptable for a classi-fier to be used as replacement of a human judgein the workflow of building a systematic review.The levels of precision that we obtained with theglobal method are acceptable but they cannotsubstitute the low level of recall.
Since our majorfocus is recall, we investigated more and we fur-ther improved our precision scores with the per-question classification method.5.1.1 Results for Feature SelectionTable 4 presents the results obtained with ourfeature selection techniques.
We decided to re-port only representative results using CNB as aclassifier and a specific representation setting.The number of features used in the experiment ispresented in the round brackets.
The first numberrepresents the number of features extracted fromthe Included class data set while the secondfrom the Excluded class data set.Similar experiments were performed whenusing Na?ve Bayes as classifier.
The results ob-tained were opposite to ones obtained for CNB,all abstracts were classified as Excluded.
Webelieve that this is the case because the CNBclassifier tries to compensate for the class imbal-ance and gives more credit to the minority class,Chi2(150:150)InfoGain(300)BNS(10:8)True Inc. 3,819 3,875 2,690False Inc. 19,233 19,638 13,905True Exc.
3,924 3,518 9,253False Exc.
298 242 1,427Recall 92.8% 94.1% 65.3%Precision 16.6% 16.5% 16.2%F-measure 28% 28% 25%WSS 8.2% 7.9% 4.5%Table 4.
Representative results obtained for variousfeature selection techniques.while the Na?ve Bayes classifier will let the ma-jority class overwhelm the classifier.Besides the results presented in Table 4, wealso tried to boost the representative features forthe Included class hoping to re-balance the im-balance present in the training data set.
To per-form these experiments we selected the top kCHI2 word features and then added to this set offeatures the top k1 CHI2 representative featuresonly for the Included class.
The parameter k var-ied from 50 to 100 and the parameter k1 from 30to 70.
We performed experiments when using theoriginal imbalanced training data set and using abalanced data set as well, with both CNB andNa?ve Bayes classifier.
The results obtained forthese experiments were similar to the ones whenwe used the previous feature selection tech-niques.
There was no significant difference in theresults compared to the ones in Table 5.5.2 Results for the Per-Question MethodThe results for our second method using the fourvoting schemes are presented in Table 5.Compared with the global method the resultsobtained by the per-question method, especiallythe ones for 2 votes are the best so far in terms ofthe balance between the two objectives.
A largenumber of abstracts that should be excluded areclassified as Excluded whereas wrongly exclud-ing very few abstracts that should have been in-cluded (a lot fewer than in the case of the globalclassification method).The 2-votes scheme performs better than the1-vote schemes because of potential classifica-tion errors.
When the classifiers for two differentquestions (that look at two different aspects ofthe systematic review topic) are confident thatthe abstract is not relevant, the chance of correct308prediction is higher; a balance between excludingan article and keeping it as relevant is achieved.When using the classifiers for 3 or 4 questionsthe performance goes down in terms of precision;a higher number of abstracts get classified as In-cluded - some abstracts do not address all targetquestion of the review topic.1-Vote  BOW UMLS BOW+UMLSTrue Inc. 1,262 1,222 1,264False Inc. 745 2,266 741True Exc.
22,412 20,891 22,416False Exc.
2,855 2,895 2,853Recall 30.6% 29.6% 30.7%Precision 62.8% 35.0% 63.0%F-measure 41.2% 32.1% 41.2%WSS 23.2% 16.8% 23.3%2-Vote BOW UMLS BOW+UMLSTrue Inc. 3,181 2,603 3,283False Inc. 9,976 9,505 10,720True Exc.
13,181 13,652 12,437False Exc.
936 1,514 834Recall 77.2% 63.2% 79.7%Precision 24.1% 21.5% 23.4%F-measure 36.8% 32.0% 36.2%WSS 29.0% 18.8% 28.4%3-Vote  BOW UMLS BOW+UMLSTrue Inc. 3,898 3,480 3,890False Inc. 18,915 16,472 18,881True Exc.
4,242 6,685 4,276False Exc.
219 637 227Recall 94.6% 84.5% 94.4%Precision 17.0% 17.4% 17.0%F-measure 28.9% 28.9% 28.9%WSS 11.0% 11.3% 11.0%4-Vote  BOW UMLS BOW+UMLSTrue Inc. 4,085 3,947 4,086False Inc. 21,946 20,869 21,964True Exc.
1,211 2,288 1,193False Exc.
32 170 31Recall 99.2% 95.8% 99.2%Precision 15.6% 15.9% 15.6%F-measure 27.1% 27.2% 27.0%WSS 3.7% 4.8% 3.7%Table 5.
Results for the per-question method for theIncluded class.For the per-question technique the recall valuepeaked at 99.2% with the 4-vote method BOWand BOW+UMLS representation technique.
Inthe same time the lowest values of precision forthe per-question technique, 15.6% is obtainedwith the same experimental setting.
It is impor-tant to aim for a high recall but not to dismiss theprecision values.
The difference of even less than2% in precision values can cause the reviewers toread additional thousands of documents, as ob-served in the confusion matrices for 2-vote, 3-vote and 4-vote methods in Table 5.From the confusion matrix in Table 5 for the2-vote method and the 3- and 4-vote method weobserve the high difference in the number ofdocuments a reviewer will have to read (thefalsely included documents).
The difference inprecision from 24.1% for the 2-vote method to15.6% for the 4-vote method makes the reviewergo through 11,988 additional abstracts.The best value for the WSS measure for theper-question method is achieved by the 2-votescheme.
The result is lower than the one obtainedby the global method but the recall level is highertherefore, we still keep as a potential winner the2-vote scheme.5.3 Results for Human-Machine WorkflowIn Figure 1, we envisioned the way we can usethe automatic classifier in the workflow of build-ing a systematic review.
In order to determine theperformance of the human-machine workflowthat we propose we computed the recall valueswhen the human reviewer?s labels are combinedwith the labels obtained from the classifier.
Thesame labeling technique is applied as for the hu-man-human workflow: if at least one decision foran abstract is to include it in the systematic re-view, then the final label is Included.We also calculated the evaluation measuresfor the two reviewers.
The evaluation measuresfor the human judge that is kept in the human-machine workflow, Reviewer 1 in Figure 1, are64.29% for recall and 15.20% for precision.
Theevaluation measures for the reviewer that is to bereplaced in the human-machine classification,Reviewer 2 in Figure 1 are 59.66% for recall and15.09% for precision.
The recall value for thetwo human judges combined is 85.26% and theprecision value is 100%.
As we can observe therecall value for the second reviewer, the one thatis replaced in the human-classifier workflow islow.
In Table 6 we present precision and recallresults for the symbiotic model for both our me-thods.
In these results we can clearly see that the2-vote technique is superior to the other votingtechniques and to the global method.
For almostthe same level of precision the level or recall it ismuch higher.
These observations support the fact309that the extra effort spent in identifying the mostsuitable methodology pays off.The fact that we keep a human in the loopmakes our method acceptable as a workflow forbuilding a systematic review.Method BOW UMLS BOW+UMLSGlobal    17.9/87.7% 17.0/88.6% 17.9/87.7%1-Vote 17.1/75.3% 16.5/74.8% 17.1/75.4%2-Vote 17.1/91.6% 16.4/86.6% 17.1/92.7%3-Vote 15.8/97.9% 15.8/94.2% 15.8/97.8%4-Vote 15.3/99.6% 15.4/98.3% 15.3/99.6%Table 6.
Precision/recall results for the human-classifier workflow for the Included class.6 DiscussionThe global method achieves good results in termsof precision while the best recall is obtained bythe per-question method.The best results for the task were obtainedusing the per-question method with the 2-votescheme with or without UMLS features.
The 3-vote scheme with UMLS representation is closeto the 2-vote scheme but looking at F-measureand WSS results the 2-vote scheme is better.
Theclear distinction between the methods comeswhen we combined the classifiers with the hu-man judge in the workflow of building reviews.The per-question technique is more robustand it offers the possibility to choose the desiredtype of performance.
If the reviewers are willingto read almost the entire collection of documents,knowing that the recall is high, then a 3 or 4-votescheme can be the set-up (though the 3 or 4-votemethod is not likely to achieve 100% recall be-cause it is very rare that an abstract contain an-swers to three or four of the questions associatedwith the systematic review).
If the reviewers willlike to read a small collection being confidentthat almost all the abstracts are relevant, then a 1-vote scheme can be the set-up required.
The per-question method confirms the fact that an en-semble of classifiers is better than one classifier;(Dietterich, 1997).When we combine the human and the systemresults we obtain a major improved in terms ofrecall.
We base our discussion for the human-machine results for the experiment that obtainedthe best results, the 2-vote scheme with aBOW+UMLS representation technique.
Whencombining the human and classifier decisions,the precision level decreased a bit compared tothe one that the machine obtained.
We believethat this is the case because some of the abstractsthat the classifier excluded were included by thefirst human reviewer and, with this decisionprocess in place, the level of precision dropped.Our goal of improving the recall level fromthe first level of screening is achieved, sincewhen both the classifier and the human judge areintegrated in the workflow, the recall level jumpsfrom 79.7% to 92.7%.We believe that the low level of precisionthat is obtained for the human reviewer, for thehuman-classifier workflow, and for the classifier,is due to the fact that we are running experimentsfor the first screening phase when we use onlythe abstracts as source of information and not theentire articles.We believe that further investigations are re-quired to fully replace a human reviewer with anautomatic classifier but the results obtained withthe per-question method encourage us to believethat this is a suitable solution for reaching ourfinal goal.7 Conclusions and Future WorkIn this paper, we looked at two methods bywhich we envision the way automatic text classi-fication techniques could help the workflow ofbuilding systematic reviews.The first method is a straight-forward appli-cation of the representations and learning algo-rithms that capture the specifics of the data: med-ical domain, huge number of features, misclassi-fication, and imbalanced classes.We showed that the specifics of the humanprotocol in which systematic reviews are builthave a positive effect when deployed in an auto-matic way.
We believe that the tedious processthat is currently used for building systematic re-views can be lightened by the use of a classifierin combination with only one human judge.
Byhaving a human judge in the loop, we ensure thatthe workflow is reliable and that the system canbe easily integrated in the workflow.In future work we would like to look intoways of improving the results by the way wechose the training data set and by integratingmore domain specific knowledge.
We would alsolike to investigate ways by witch we can updatesystematic reviews.310ReferencesAphinyanaphongs Y. and Aliferis C. Text Categoriza-tion Models for Retrieval of High Quality Articles.Journal of the American Medical Informatics As-sociation 2005; 12:207-216.Cohen A.M. Optimizing Feature Representation forAutomated Systematic Review Work Prioritization.Proceedings of the AMIA Annual Symposium2008; 6:121-126.Cohen A.M., Hersh W.R., Peterson K., Yen P.Y.
Re-ducing Workload in Systematic Review Prepara-tion Using Automated Citation Classification.Journal of the American Medical Informatics As-sociation 2006; 13:206-219.Dietterich, T. Machine-Learning Research: FourCurrent Directions.
Artificial Intelligence Maga-zine.
18(4): 97-136 (1997)Drummond C. and Holte R.C.
C4.5, Class Imbalance,and Cost Sensitivity: Why Under-Sampling beatsOver-Sampling.
Proceedings of the Twentieth In-ternational Conference on Machine Learning:Workshop on Learning from Imbalanced Data Sets(II), 2003.Forman G. Choose Your Words Carefully: An Empiri-cal Study of Feature Selection Metrics for TextClassification.
In the Joint Proceedings of the 13thEuropean Conference on Machine Learning andthe 6th European Conference on Principles andPractice of Knowledge Discovery in Databases(ECML/PKDD), 2002.Frank E. and Bouckaert R.R.
Naive Bayes for TextClassification with Unbalanced Classes.
In theProceedings of the 10th European Conference onPrinciples and Practice of Knowledge Discovery inDatabases, Berlin, Germany, 2006, pp.
503-510.Haynes R.B., Wilczynski N., McKibbon K.A., WalkerC.J., Sinclair J.C.
Developing optimal search strat-egies for detecting clinically sound studies inMEDLINE.
Journal of the American Medical In-formatics Association 1994; 1:447-58.Kohavi R. and Provost F. Glossary of Terms.
Editorialfor the Special Issue on Applications of MachineLearning and the Knowledge Discovery Process1998; 30:271-274.Ma Y.
2007.
Text classification on imbalanced data:Application to Systematic Reviews Automation.M.Sc.
Thesis.
University of Ottawa.311
