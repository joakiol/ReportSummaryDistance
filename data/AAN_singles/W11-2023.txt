Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 204?215,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsCommitments to Preferences in DialogueAnais Cadilhac*, Nicholas Asher*, Farah Benamara*, Alex Lascarides***IRIT, University of Toulouse, **School of Informatics, University of EdinburghAbstractWe propose a method for modelling how dialoguemoves influence and are influenced by the agents?preferences.
We extract constraints on preferencesand dependencies among them, even when they areexpressed indirectly, by exploiting discourse struc-ture.
Our method relies on a study of 20 dia-logues chosen at random from the Verbmobil cor-pus.
We then test the algorithms predictions againstthe judgements of naive annotators on 3 random un-seen dialogues.
The average annotator-algorithmagreement and the average inter-annotator agree-ment show that our method is reliable.1 IntroductionDialogues are structured by various moves that theparticipants make?e.g., answering questions, askingfollow-up questions, elaborating prior claims, and soon.
Such moves come with commitments to certain at-titudes such as intentions and preferences.
While map-ping utterances to their underlying intentions is wellstudied through the application of plan recognition tech-niques (e.g., Grosz and Sidner (1990), Allen and Litman(1987)), game-theoretic models of rationality generallysuggest that intentions result from a deliberation to findthe optimal tradeoff between one?s preferences and one?sbeliefs about possible outcomes (Rasmusen, 2007).
Somapping dialogue moves to preferences is an importanttask: for instance, they are vital in decisions on how tore-plan and repair should the agents?
current plan fail, forthey inform the agents about the relative importance oftheir various goals.
Classical game theory, however, de-mands a complete and cardinal representation of prefer-ences for the optimal intention to be defined.
This is notrealistic for modelling dialogue because agents often lackcomplete information about preferences prior to talking:they learn about the domain, each other?s preferences andeven their own preferences through dialogue exchange.For instance, utterance (1) implies that the speaker wantsto go to the mall given that he wants to eat, but we do notknow his preferences over ?go to the mall?
if he does notwant to eat.
(1) I want to go to the mall to eat something.Existing formal models of dialogue content either do notformalise a link between utterances and preferences (e.g.,Ginzburg (to appear)), or they encode such links in atyped feature structure, where desire is represented as afeature that takes conjunctions of values as arguments(e.g., Poesio and Traum (1998)), making the languagetoo restricted to express dependencies among preferencesof the kind we just described.
Existing implementeddialogue systems likewise typically represent goals assimple combinations of values on certain information?slots?
(e.g., He and Young (2005), Lemon and Pietquin(2007)); thus (1) yields a conjunction of preferences, togo to the mall and to eat something.
But such a systemcould lead to suboptimal dialogue moves?e.g., to helpthe speaker go to the mall even if he has already receivedfood.What?s required, then, is a method for extracting par-tial information about preferences and the dependenciesamong them that are expressed in dialogue, perhaps indi-rectly, and a method for exploiting that partial informa-tion to identify the next optimal action.
This paper pro-poses a method for achieving these tasks by exploitingdiscourse structure.We exploited the corpus of Baldridge and Lascarides(2005a), who annotated 100 randomly chosen sponta-neous face-to-face dialogues from the Verbmobil cor-pus (Wahlster, 2000) with their discourse structure ac-cording to Segmented Discourse Representation Theory(SDRT, Asher and Lascarides (2003))?these structuresrepresent the types of (relational) speech acts that theagents perform.
Here?s a typical fragment:(2) a.
A: Shall we meet sometime in the nextweek?b.
A: What days are good for you?c.
B: Well, I have some free time on almostevery day except Fridays.204d.
B: In fact, I?m busy on Thursday too.e.
A: So perhaps Monday?Across the corpus, more than 30% of the discourse unitsare either questions or assertions that help to elaborate aplan to achieve the preferences revealed by a prior partof the dialogue?these are marked respectively with thediscourse relations Q-Elab and Plan-Elab in SDRT, andutterances (2b) and (2e) and the segments (2c) and (2d)invoke these relations (see Section 2).
Moreover, 10% ofthe moves revise or correct prior preferences (like (2d)).We will model the interaction between dialogue con-tent and preferences in two steps.
The first maps ut-terances and their rhetorical connections into a partialdescription of the agents?
preferences.
The mapping iscompositional and monotonic over the dialogue?s logi-cal form (i.e., the description of preferences for an ex-tended segment is defined in terms of and always sub-sumes those for its subsegments): it exploits recursionover discourse structure.
The descriptions partially de-scribe ceteris paribus preference nets or CP-nets withBoolean variables (Boutilier et al, 2004).
We chose CP-nets over alternative logics of preferences, because theyprovide a compact, computationally efficient, qualitativeand relational representation of preferences and their de-pendencies, making them compatible with the kind ofpartial information about preferences that utterances re-veal.
Our mapping from the logical form of dialogueto partial descriptions of Boolean CP-nets proceeds in apurely linguistic or domain independent way (e.g., it ig-nores information such as Monday and Tuesday cannotco-refer) and will therefore apply to dialogue generallyand not just Verbmobil.In a second stage, we ?compress?
and refine our descrip-tion making use of constraints proper to CP-nets (e.g.,that preference is transitive) and constraints provided bythe domain?in this case constraints about times andplaces, as well as constraints from deep semantics.
Thissecond step reduces the complexity of inferring whichCP-net(s) satisfy the partial description and allows us toidentify the minimal CP-net that satisfies the domain-dependent description of preferences.
We can thus ex-ploit dependencies between dialogue moves and mentalstates in a compact, efficient and intuitive way.We start by motivating and describing the semantic repre-sentation of dialogue from which our CP-net descriptionsand then our CP-nets will be constructed.2 The Logical Form of DialogueOur starting point for representing dialogue con-tent is SDRT.
Like Hobbs et al (1993) andMann and Thompson (1987), it structures discourseinto units that are linked together with rhetorical re-lations such as Explanation, Question Answer Pair(QAP), Q-Elab, Plan-Elab, and so on.
Logical formsin SDRT consist of Segmented Discourse RepresentationStructures (SDRSs).
As defined in Asher and Lascarides(2003), an SDRS is a set of labels representing discourseunits, and a mapping from each label to an SDRS-formularepresenting its content?these formulas are based onthose for representing clauses or elementary discourseunits (EDUs) plus rhetorical relation symbols betweenlabels.
Lascarides and Asher (2009) argue that to makeaccurate predictions about acceptance and denial, bothof which can be implicated rather than linguisticallyexplicit, the logical form of dialogue should track eachagent?s commitments to content, including rhetoricalconnections.
They represent a dialogue turn (where turnboundaries occur whenever the speaker changes) as aset of SDRSs?one for each agent representing all hiscurrent commitments, from the beginning of the dialogueto the end of that turn.
The representation of the dialogueoverall?a Dialogue SDRS or DSDRS?is that of each ofits turns.
Each agent constructs the SDRSs for all otheragents as well as his own.
For instance, (2) is assignedthe DSDRS in Table 1, with the content of the EDUsomitted for reasons of space (see Lascarides and Asher(2009) for details).
We adopt a convention of indexingthe root label of the nth turn, spoken by agent d, asnd; and pi : ?
means that ?
describes pi?s content (we?llsometimes also write ?pi to identify this description).We now return to our example (2).
Intuitively, (2a) com-mits A to a preference for meeting next week but it doesso indirectly: the preference is not asserted, or equiva-lently entailed at the level of content from the semanticsof Q-Elab(a,b).
Accordingly, responding with "I do too"(meaning "I want to meet next week too") is correctly pre-dicted to be highly anomalous.
A?s SDRS for turn 1 in Ta-ble 1 commits him to the questions (2a) and (2b) becauseQ-Elab is veridical: i.e.
Q-Elab(a,b) entails the dynamicconjunction ?a ??b.
Since intuitively (2a) commits A tothe implicature that he prefers next week, our algorithmfor eliciting preferences from dialogue must ascribe thispreference to A on the basis of his move Q-Elab(a,b).Furthermore,Q-Elab(a,b) entails that any answer to (2b)must elaborate a plan to achieve the preference revealedby (2a); this makes ?b paraphrasable as ?What days nextweek are good for you?
?, which does not add new prefer-ences.B?s contribution in the second turn attaches to (2b) withQAP and also Plan-Elab?he answers with a non-emptyextension for what days.
Lascarides and Asher (2009) ar-gue that this means that B is also committed to the illo-cutionary contribution of (2b), as shown in Table 1 bythe addition of Q-Elab(a,b) to B?s SDRS.
This additioncommits B also to the preference of meeting next week,with his answer making the preferencemore precise: (2c)reveals that B prefers any day except Friday; by linking(2d) with Plan-Correction he retracts the preference forThursday.
This compels A to revise his inferences about205Turn A?s SDRS B?s SDRS1 pi1A : Q-Elab(a,b) /02 pi1A : Q-Elab(a,b) pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)pi : Plan-Correction(c,d)3 pi3A : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)?
pi2B : Q-Elab(a,b)?QAP(b,pi)?Plan-Elab(b,pi)Plan-Elab(pi,e) pi : Plan-Correction(c,d)Table 1: The DSDRS for Dialogue (2).B?s preference for meeting on Thursday.
A?s Plan-Elabmove (2e) in the third turn reveals another preference forMonday.
This may not match his preferred day when thedialogue started: perhaps that was Friday.
He may con-tinue to prefer that day.
But engaging in dialogue cancompel agents to revise their commitments to preferencesas they learn about the domain and each other.The above discussion of (2) exhibits how different typesof rhetorical relations between utterances rather thanSearle-like speech acts like question, construed as a prop-erty of an utterance, are useful for encoding how pref-erences evolve in a dialogue and how they relate toone another.
While the Grounding Acts dialogue model(Poesio and Traum, 1998) and the Question Under Dis-cussion (QUD) model (Ginzburg, to appear) both havemany attractive features, they do not encode as fine-grained a taxonomy of types of speech acts and their se-mantic effects as SDRT: in SDRT each rhetorical relationis a different kind of (relational) speech act, so that, forinstance, the speech act of questioning is divided into thedistinct types Q-Elab, Plan-Correction, and others.
Forthe QUD model to encode such relations would requireimplicit questions of all sorts of different types to be in-cluded in the taxonomy, in which case the result may beequivalent to the SDRT taxonomy of dialogue moves.
Wehave not explored this eventual equivalence here.3 CP-nets and CP-net descriptionsA preference is standardly understood as an ordering byan agent over outcomes; at the very least it entails a com-parison between one entity and another (outcomes beingone sort of entity among others).
As indicated in the in-troduction, we are interested in an ordinal definition ofpreferences, which consists in imposing an ordering overall (relevant) possible outcomes.
Among these outcomes,some are acceptable for the agent, in the sense that theagent is ready to act in such a way as to realize them;and some outcomes are not acceptable.
Amongst the ac-ceptable outcomes, the agent will typically prefer someto others.
Our method does not try to determine the mostpreferred outcome of an agent but follows rather the evo-lution of their commitments to certain preferences as thedialogue proceeds.
To give an example, if an agent pro-poses to meet on a certain day X and at a certain time Y,we infer that among the agent?s acceptable outcomes is ameeting on X at Y, even if this is not his most preferredoutcome (see earlier discussion of (2e)).A CP-net (Boutilier et al, 2004) offers a compact rep-resentation of preferences.
It is a graphical model thatexploits conditional preferential independence so as tostructure the decision maker?s preferences under a ceterisparibus assumption.Although CP-nets generally consider variables with a fi-nite range of values, to define the mapping from dialogueturns to descriptions of CP-nets in a domain indepen-dent and compositional way, we use Boolean proposi-tional variables: each variable describes an action that anagent can choose to perform, or not.
We will then refinethe CP-net description by using domain-specific informa-tion, transforming CP-nets with binary valued variablesto CP-nets with multiple valued variables.
This reducesthe complexity of the evaluation of the CP-net by a largefactor.More formally, let V be a finite set of propositional vari-ables and LV the description language built from V viaBoolean connectives and the constants ?
(true) and ?(false).
Formulas of LV are denoted by ?,?, etc.
2V is theset of interpretations for V , and as usual for M ?
2V andx?V , M gives the value true to x if x?M and false other-wise.
Where X ?V , let 2X be the set of X-interpretations.X-interpretations are denoted by listing all variables ofX , with a ?
symbol when the variable is set to false: e.g.,where X = {a,b,d}, the X-interpretation M = {a,d} isexpressed as abd.A preference relation  is a reflexive and transitive bi-nary relation on 2V with strict preference ?
defined inthe usual way (i.e., M  M?
but M?
6 M).
Note thatpreference orderings are not necessarily complete, sincesome candidates may not be comparable by a given agent.An agent is said to be indifferent between two optionsM,M?
?
2V , written M ?M?, if M M?
and M?
M.As we stated earlier, CP-nets exploit conditional pref-erential independence to compute a preferential rankingover outcomes:Definition 1 Let V be a set of propositional variablesand {X ,Y,Z} a partition of V .
X is conditionally pref-erentially independent of Y given Z if and only if ?z ?2Z , ?x1,x2 ?
2X and ?y1,y2 ?
2Y we have: x1y1z 206x2y1z iff x1y2z x2y2z.For each variable X , the agent specifies a set of parentvariables Pa(X) that can affect his preferences over thevalues of X .
Formally, X is conditionally preferentiallyindependent of V \ ({X}?Pa(X)).
This is then used tocreate the CP-net.Definition 2 Let V be a set of propositional variables.N = ?G ,T ?
is a CP-net on V , where G is a directedgraph over V , and T is a set of Conditional PreferenceTables (CPTs) with indifference.
That is, T = {CPT(X j):X j ?
V}, where CPT(X j) specifies for each instantiationp ?
2Pa(X j) either x j ?p x j, x j ?p x j or x j ?p x j.The following simple example illustrates these defini-tions.
Suppose our agent prefers to go from Paris toHong Kong by day rather than overnight.
If he takes anovernight trip, he prefers a non stop flight, but if he goesby day, he prefers a flight with a stop.
Figure 1 shows theassociated CP-net.
The variable T stands for the prefer-ence over the period of travel.
Its values are Td for a daytrip and Tn for a night one.
The variable St stands for thepreference over stops.
Its values are S for a trip with stopsand S without.TStCPT(T) = Td ?
TnCPT(St) = Td : S ?
STn : S?
SFigure 1: Travel CP-netWith CP-nets defined, we proceed to a description lan-guage for them.
The description language formula w ?y(CPT ) describes a CP-net where a CPT contains an en-try of the form w ?p y for some possibly empty list ofparent variables p. A CP-net description is a set of suchformulas.
The CP-net N |= x1, .
.
.xn : w?
y(CPT ) iff theCP-net N ?s CPT T contains an entry w?~u y?also writ-ten~u :w?
y?where x1, .
.
.xn figure in~u.
Satisfaction of adescription formula by a CP-net yields a notion of logicalconsequence between a CP-net descriptionD N and a de-scription formula in the obvious way.
Dialogue turns alsosometimes inform us that certain variables enter into pref-erence statements.
We?ll express the fact that the vari-ables x1, .
.
.
,xn are associated with discourse constituentpi by the formula x1, .
.
.
,xn(P(pi)), where P(pi) refers tothe partial description of the preferences expressed by thediscourse unit pi (see Section 4).The description language allows us to impose constraintson the CP-nets that agents commit to without specifyingthe CP-net completely, as is required for utterances like(1).
In section 6, we describe how to construct a min-imal CP-net from a satisfiable CP-net description.
Onecan then use the forward sweep procedure for outcomeoptimisation (Boutilier et al, 2004).
This is a proce-dure of linear complexity, which consists in instantiatingvariables following an order compatible with the graph,choosing for each variable (one of) its preferred valuesgiven the value of the parents.4 From EDUs to PreferencesEDUs are described in SDRT using essentially Booleanformulas over labels (Asher and Lascarides, 2003); thus?(pi)??
(pi) means that ?
and ?
describe aspects of pi?scontent.
Not(pi1,pi)?
?
(pi1) means that the logical formof the EDU pi is of the form ?pi1 and that pi1 is describedby ?
; so pi has the content ??.
Our task is to map suchdescriptions of content into descriptions of preferences.Our preference descriptions will use Boolean connectivesand operators over preference entries (e.g., of the formx ?
y): namely, &,?, 7?, and a modal operator ?.
Therules below explain the semantics of preference opera-tors (they are in effect defined in terms of the semanticsof buletic attitudes and Boolean connectives) and howto recursively calculate preference descriptions from theEDU?s logical structure.Simple EDUs can provide atomic preference statements(e.g., I want X or We need X).
This means that with thisEDU the speaker commits to a preference for X .
X willtypically involve a Boolean variable and a preference en-try for its CPT.
P(pi) is the label of the preference descrip-tion associated with discourse unit pi.
Hence for a sim-ple EDU pi, we have X(P(pi)) as its description.
SimpleEDUs also sometimes express preferences in an indirectway (see (2a)).More generally, P recursively exploits the logical struc-ture of an EDU?s logical form to produce an EDU pref-erence representation (EDUPR).
For instance, since thelogical form of the EDU I want fish and wine featuresconjunction, likewise so does its preference description:?&?
(P(pi)) means that among the preferences includedin pi, the agent prefers to have both ?
and ?
and prefers ei-ther one if he can?t have both.1 We also have disjunctions(let?s meet Thursday or Friday), and negations (I don?twant to meet on Friday), whose preferences we?ll expressrespectively as Thurs?Fri(P(pi)) and ?Fri(P(pi)).Some EDUs express commitments to dependenciesamong preferences.
For example, in the sentence Whatabout Monday, in the afternoon?, there are two prefer-ences: one for the day Monday, and, given the Mondaypreference, one for the time afternoon (of Monday), atleast on one syntactic disambiguation.
We represent thisdependency as Mon 7?
Aft(P(pi)).
Note that 7?
is notexpressible with just Boolean operators.
Finally, EDUscan express commitment to preferences via free choice1The full set of rules also includes a stronger conjunction ???
(P(pi))(the agent prefers both ?
and ?, but is indifferent if he can?t have both).207modalities; I am free on Thursday, or?Thurs(P(pi)), tellsus that Thursday is a possible day to meet.
??
says that ?is an acceptable outcome (as described earlier, this meansthe agent is ready to act so as to realize an outcome thatentails ?).
Thus, ??
(pi) entails ?
(pi), and ?-embeddedpreferences obey reduction axioms permitting ?
to beeliminated when combined with other preference oper-ators.
But a ?
preference statement does affect a prefer-ence description when is is conjoined in Boolean fashionwith another ?
preference statement in an EDU or com-bined via a discourse relation like Continuation.
This isbecause ?
is a free choice modality and obeys the equiv-alence (3) below, which in turn yields a disjunctive pref-erence ???
(P(pi)) from what appeared to be a conjunc-tion.2(3) (??(P(pi))???(P(pi)))??(???
)(P(pi))The variables introduced by a discourse segment pi areintegrated into the CP-net description D N via the oper-ation Commit(pi,D N ).
The following seven rules coverthe different possible logical structures for the EDU pref-erence representation.
In the following, X ,Y,Z,W denotepropositional variables and ?, ?
propositional formulasfrom EDUPR.
Var(?)
are the variables in ?, and ?Xthe preference relation describing CPT (X).
Sat(?)
(ornon-Sat(?))
is a conjunction of literals from Var(?)
thatsatisfy (or do not satisfy) ?.
Sat(?
)?X is the formula thatresults from removing the conjunct with X from Sat(?).1.
Where X(P(pi)) (X is a variable of P(pi), e.g., I wantX), Commit(pi,D N ) adds the description D N |=X ?
X(CPT (X)).32.
Where ?&?
(P(pi)), Commit(pi,D N ) adds descrip-tions as follows:?
For each X ?Var(?
), addVar(?)
to Pa(X) andmodifyCPT (X) as follows:If Sati(?
), Sat j(?)
?
X (resp.
X), then Sati(?
),Sat j(?
)?X : X ?
X (resp.
X ?
X), for all sat-isfiers i and j.?
Similarly for each Y ?Var(?
).If ?
and ?
are literals X and Y we get: D N |= Y ?Y (CPT (Y )) and D N |= X ?
X(CPT (X)).
Graph-ically, this yields the following preference relation(where one way arrows denote preference, two way2We provide here the reduction axioms over preference descriptions1.
?(?&?)(P(pi))?
(?&?)(P(pi))2.
?(?
7?
?)(P(pi))?
(?
7?
?)(P(pi))3.
?(???)?
(??
?)(P(pi))4.
???(P(pi))???
(P(pi))3Given our description language semantics, this means that anyCP-net which satisfies the description D N contains a preference ta-ble CPT (X) with an entry X ?
X with at least one instantiation of thevariables in Pa(X).arrows denote indifference or equal preference, andno arrow means the options are incomparable):XYXY XYXY3.
Where ??
?
(P(pi)) (the agent prefers to have atleast one of ?
and ?
satisfied).
If ?
and ?
are Xand Y , we get:?
Var(X) ?
Pa(Var(Y )) and D N |= X : Y ?Y (CPT (Y )), D N |= X : Y ?
Y (CPT (Y )).?
Var(Y ) ?
Pa(Var(X)) and D N |= Y : X ?X(CPT (X)), D N |= Y : X ?
X(CPT (X)).This corresponds to the following preference rela-tion:XYXY XY XYAs before, the use of indifference allows us to findthe best outcomes (XY , XY and XY ) easily.4.
Where ?
7?
?
(P(pi)) (the agent prefers that ?
is sat-isfied and if so that ?
is also satisfied.
If ?
is notsatisfied, it is not possible to define preferences on?).
If ?
and ?
are X and Y , we get:?
D N |= X ?
X(CPT (X))?
Var(X) ?
Pa(Var(Y)) andD N |= X : Y ?
Y (CPT (Y )).Note that this description is also produced byElab(pii,pi j) below where X(P(pii)) and Y (P(pi j))(see rule 8).
Thus the implication symbol 7?
is a"shortcut" in that it represents elaborations whosearguments are in the same EDU.5.
Where ??
(P(pi)) (the agent prefers a free choice of?).
Given the behaviour of?, this reduces to treating?(P(pi)).6.
Where ??(P(pi)).
We can apply rules 1-5 by con-verting ??
into conjunctive normal form.7.
Where ?(P(pi))??
(P(pi)), with ?
and ?
nonmodal,we simply apply the rule for ?
and that for ?.5 From Discourse Structure to PreferencesWe must now define how the agents?
preferences, repre-sented as a partial description of a CP-net, are built com-positionally from the discourse structure over EDUs.
Theconstraints are different for different discourse relations,reflecting the fact that the semantics of connections be-tween segments influences how their preferences relateto one another.We will add rules for defining Commit over la-bels pi whose content ?pi express rhetorical relationsR(pii,pi j)?indeed, we overload the notation and writeCommit(R(pii,pi j),D N ).
Since Commit applies com-positionally, starting with the EDUs and working up208the discourse structure towards the unique root la-bel of the SDRS, we can assume in our definition ofCommit(R(pii,pi j),D N ) that the EDUPRs are already de-fined.
We give rules for all the relations in the Verbmobilcorpus, though we will be very brief with those that areless prevalent.
A complete example using our rules is inappendix A.IExplanation, Elab, Plan-Elab, Q-elabIExplanation(pii, pi j): i.e., pi j?s preferences explain pii?s(e.g., see (1), where P(pii) would be going to the malland P(pi j) is eating something).
With Elab(pii, pi j) apreference in pii is elaborated on or developed in pi j,as in: I want wine.
I want white wine.
That is, apreference for white wine depends on a preference forwine.
Plan-Elab(pii,pi j) means that pi j describes a planfor achieving the preferences expressed by pii, and withQ-Elab we have a similar dependence between prefer-ences, but the second constituent is a question (so oftenin practice this means preference commitments from piitransfer from one agent to another).Plan-Elab(pi j,pii), Elab(pi j,pii) and IExplanation(pii,pi j)all follow the same two-step rule, and so from the pointof view of preference updates they are equivalent:8. i Firstly, preference description D N is up-dated according to P(pi j) by applyingCommit(pi j,D N ), if pi j expresses a newpreference.
If not go to step (ii).ii.
Secondly, description D N is modified so thateach variable in P(pii) depends on each vari-able in P(pi j): i.e., ?X ?
Var(P(pii)), ?Y ?Var(P(pi j)), Y ?
Pa(X).
Then, D N is enrichedaccording to P(pii), if pii expresses a preference.If it does not, then end.We now give some details concerning step (ii) above.
Tothis end, let ?
denote a formula with SDRS descriptionpredicates, ??
its corresponding boolean (preference) for-mula and ??
its negation.
Then for ?=Y , we define ?
?=Yand ??
= Y ; for ?
= Y 7?
Z we define ??
= Y ?
Z and??
= Y ?
Z; and for ?
= Y ?
Z and ?
= Y&Z, we have??
= Y ?Z and ??
= Y ?Z.a.
X(P(pii)) and ?
(P(pi j)).
The agent explains his pref-erences on X by ?.
So, if no preferences on X arealready defined, ?
is a reason to prefer X .
That is,D N |= ??
: X ?
X(CPT (X)).
However, it is not pos-sible to define preferences on X if ?
is false.
If, onthe other hand, preferences on X are already defined,the agent prefers X if ?
is satisfied, and does notmodify his preferences otherwise?i.e.,?X ,?
?= X ?X , ?X ,?
?=?X .44If we have ?X such that Z: X ?
X , Z: X ?
X , ?X ,??
representspreferences defined by Z???
and Z??
?, whereas ?X ,??
represents pref-erences defined by Z???
and Z??
?.For ?
= Y , if ?X is not already defined, we obtainthe following preference relation (no information onthe preference for X if Y is false makes XY and XYincomparable):XYXYXYXYb.
X?Z(P(pii)) and ?
(P(pi j)).
The agent explains hispreferences X?Z by ?
: he wants to satisfy X or Zif ?
is satisfied.First, we set Var(Z) ?
Pa(Var(X)), Var(X) ?Pa(Var(Z)).
If ?X is not already defined, we have:D N |= ??
?
Z: X ?
X(CPT (X)), D N |= ??
?
Z:X ?
X(CPT (X)).Otherwise, ?X ,?
?,Z= X ?
X , ?X ,?
?,Z= X ?
X ,?X ,?
?,Z= ?X ,?
?,Z= ?X .CPT (Z) is defined asCPT (X) by inverting X and Z.For ?
=Y , if ?X and?Z are not already defined, weobtain the following preference relation (again, thelack of preference information on X and Z when Yis false yields incomparability among states whereYis false):XYZXYZXYZXYZXYZXYZXYZXYZc.
X&Z(P(pii)) and ?
(P(pi j)).
The agent explains hispreferences on X&Z by ?.?
If ?X is not already defined, we have: D N |=??
: X ?
X(CPT (X)).Otherwise, ?X ,?
?= X ?
X , ?X ,?
?= ?X ,?
CPT (Z) is defined as CPT (X) by replacing Xby Z.d.
X 7?
Z(P(pii)) and ?
(P(pi j)).
The agent explains hispreferences on X 7?
Z by ?
: he wants to satisfy Xand after Z if ?
is satisfied.If ?X is not already defined, we have D N |= ??
:X ?
X(CPT (X)) and we setVar(X)?
Pa(Var(Z)).5If ?Z is not yet defined, we have : D N |= ??
?X :Z ?
Z(CPT (Z)), D N |= ??
?X : Z ?
Z(CPT (Z)).Else, ?Z,(??
?X)= Z ?
Z, ?Z,(??
?X)= Z ?
Z,?Z,(??
?X)= ?Z,(???X)=?Z.e.
?
(P(pii)) and ?
(P(pi j)).
We can apply rules 8 bydecomposing ?.5Otherwise, there is no need to modify ?X .
This is what we call a?partial elaboration?.
Variables that were evoked since preferences onX were introduced are parents of Z but not of X .
For example, if anagent commits to a preference for Monday then Afternoon, and later inthe discourse he commits to 2oclock, then Afternoon is 2oclock?s parentbut not Monday?s.209f.
?(?
)(P(pii)) and ?(?
)(P(pi j)).
We treat this like afree choice EDU (see rule 5).g.
?(?
)(P(pii)) and ?
(P(pi j)), where ?
is non modal.We treat this like ?
(P(pii)) and ?
(P(pi j)) (see rule8.e)Let?s briefly look at how the rule changes forQ-elabA(pi1,pi2) (where the subscript A identifies thespeaker of pi2):9.
Q-ElabA(pi1,pi2) implies that we update A?s CP-net description D N by applying the rule forElab(pi1,pi2), where if pi2 expresses no preferenceson their own, we simply make the P(pi2) descriptionequal to the P(pi1) description.
Thus A?s CP-net de-scription is updated with the preferences expressedby utterance pi1, regardless of who said pi1.QAPAnswers to questions affect preferences in complexways:10.
The first case concerns yes/no questions and thereare two cases, depending on whether B replies yesor no:Yes QAPB(pi1,pi2) where pi2 is yes.
B?s pref-erence descriptions are updated by apply-ing Commit(ElabB(pi1,pi2),D N ) (and so B?spreference description include preferences ex-pressed by pi1 and pi2).No QAPB(pi1,pi2) where pi2 is no.
If P(pi1)and P(pi2) are consistent, then B?s pref-erence descriptions are updated by ap-plying CommitB(ElabB(pi1,pi2),D N );otherwise, they are updated by applyingCommit(Correction(pi1,pi2),D N ) (see rule13).11.
When pi1 is a wh-question and QAPB(pi1,pi2), B?spreferences over variables in pi1 and pi2 are ex-actly the same as the ones defined for a yes/noquestion where the answer is yes.
Variables in pi2will refine preferences over variables in pi1.
So,B?s preference descriptions are updated by applyingCommitB(ElabB(pi1,pi2),D N ).In previous rules, it is relatively clear how to update thepreference commitments.
However, in some cases it?s notclear what the answer in a QAP targets: in Could we meetthe 25 in the morning?
No, I can?t., we do not know ifNo is about the 25 and the morning, or only about themorning.
So, we define the following rule for managingcases where the target is unknown :12.
If we know the target, we can change the descriptionof the CP-net.
Otherwise, we wait to learn more.Correction and Plan-Correction allow a speaker to rec-tify a prior commitment to preferences.
Self-correctionsalso occur in the corpus: I could do it on the 27th.
No Ican not make it on the 27th, sorry I have a seminar.
Cor-rection and Plan-Correction can have several effects onthe preferences.
For instance, they can correct preferenceentries.
That is, given Correction(pi1,pi2), some variablesin P(pi1) are replaced by variables in P(pi2) (in the self-correction example, every occurrence of 27 in P(pi1) isreplaced with 27 and vice versa).
We have a set of rulesof the form X ?
{Y1, .
.
.
,Ym}, which means that the vari-able X ?
Var(P(pi1)) is replaced by the set of variables{Y1, .
.
.
,Ym} ?
Var(P(pi2)).
We assume that X can?t de-pend on {Y1, .
.
.
,Ym} before the Correction is performed.Then replacement proceeds as follows:13.
If Pa(X) = /0, we add the description D N |= Yk ?Y k(CPT (Yk)) for all k ?
{1, .
.
.
,m} and remove X ?X(CPT (X)) (or X ?
X(CPT (X))).
Otherwise, wereplace every description ofCPT (X) with an equiv-alent statement using Yk (to describe CPT (Yk)), forall k ?
{1, .
.
.m}.The specific target of the correction behaves similarly tothe target of a QAP.
In some cases we don?t know thetarget, in which case we apply rule 12.Plan-Correction can also lead to the modification of anagent?s own plan because of other agent?s proposals.
Inthis case it corrects the list of parent variables on whicha preference depends.
We call that list of variables theoperative variables.
Once the operative variables arechanged, Plan-Correction can elaborate a plan if somenew preferences are expressed.
For example, all agentshave agreed to meet next week, so in their CP-net descrip-tion, there is the entry Week1?Week1.
Then discussionshows that their availabilities are not compatible and oneof them says "okay, that week is not going to work.".
Thatdoes not mean the agent prefersWeek1 toWeek1 becauseboth agreed on Week1 as preferable.
Rather, Week1 hasbeen removed as an operative variable in the followingdiscourse segments.
This leads us to the following rule:14.
For Plan-Correction(pi1,pi2) which correctsthe list of parent variables, the operative vari-able list becomes the intersection of all Pa(X)where X ?
Var(P(pi1)).
We can now applyCommit(Plan-Elab(pi1,pi2),D N ), if P(pi2) containssome new preferences ?.
If the CPT affected by arule has no entry for the current operative variablelist O , then O : ?
has to be added to D N .Continuation, Contrast and Q-Cont pattern with therule for Elab.
Alternation patterns with rule 8.b.6 Expla-nation, Explanation*,Result, Qclar (clarification ques-tion), Commentary, Summary and Acknowledgment6The rule for Alternative questions like Do you want fish or chicken?is a special case yielding ???
(P(pi)), but we don?t offer details here.210either do nothing or have the same effect on preferenceelicitation as Elab.
Sometimes, adding these preferencesvia the Elab rule may yield an unsatisfiable CP-net de-scription, because an implicit correction is involved.
If anevaluation of the CP-net (see next section) is performedafter a processing of one of these rules shows that theCP-net description is not satisfiable, then we apply therule 13, associated with Correction.6 From Descriptions to ModelsEach dialogue turn adds constraints monotonically to thedescriptions of the CP-nets to which the dialogue partic-ipants commit.
We have interpreted each new declaredvariable in our rules as independent, which allows us togive a domain independent description of preference elic-itation.
However, when it comes to evaluating a CP-netdescription for satisfiability, we need to take into accountvarious axioms about preference (irreflexivity and transi-tivity), and axioms for the domain of conversation: in ourcase, temporal designations (Wednesdays are not Tues-days and so on).
This typically adds dependencies amongthe variables in the description.
In the case of the Verb-mobil domain, since the variable Monday means essen-tially "to meet on Monday", Monday implies Meet , andthis must be reflected via a dependency in the CP-net: wemust view the variable Meet as filling a hidden slot inthe variable Monday in the preference description, Meet :Mon?Mon.
This likewise allows us to fill in the negativeclauses of the CP-net description: we can now infer thatMeet : Mon ?
Mon.
These axioms also predict certainpreference descriptions to be unsatisfiable.
For instance,if we have Mon ?Mon, our axioms imply Mon ?
Tues,Mon ?Wed, etc.
At this point we can calculate, ceterisparibus, inconsistencies on afternoons and mornings ofparticular days.Domain knowledge also allows us to collapse Booleanvalued variables that all denote, say, days or times of theday into multiple valued variables.
So for instance, ourdomain independent algorithm from dialogue moves topreference descriptions might yield:(4) Meet?31.01?30.01?02.02: am?
amDomain knowledge collapses all Boolean variables fordistinct days into one variable with values for days to get:(5) Meet?02.02: am?
pmThis leads to a sizeable reduction in the set of variablesthat are used in the CP-net.We can test any CP-net description for satisfiability byturning the description formulas into CP-net entries.
Ourdescription automatically produces a directed graph overthe parent variables.
We have to check that the ?
state-ments form an irreflexive and transitive relation and thateach variable introduced into the CP-net has a preferenceentry consistent given these constraints.
If the descriptiondoes not yield a preference entry for a given variable X ,we will add the indifference formula X ?
X as the entry.If our CP-net description meets these requirements, thisprocedure yields a minimal CP-net.
Testing for satisfia-bility is useful in eliciting preferences from several dis-course moves like Explanation, Qclar or Result, since inthe case of unsatisfiability, we will exploit the Correctionrule 13 with these moves.7 Evaluation of the proposed methodWe evaluate our method by testing it against the judg-ments of three annotators on three randomly chosen un-seen test dialogues from the Verbmobil corpus.
Thetest corpus contains 75 EDUs and the proportion of dis-course relations is the same as in the corpus overall.
Thethree annotators were naive in the sense that they werenot familiar with preference representations and prefer-ence reasoning strategies.
For each dialogue segment,we checked if the judges had the same intuitions that wedid on: (i) how commitments to preferences are extractedfrom EDUs, and (ii) how preferences evolve through dia-logue exchange.The judges were given a manual with all the instructionsand definitions needed to make the annotations.
For ex-ample, the manual defined preference to be "a notion ofcomparison between one thing at least one other".
Themanual also instructs annotators to label each EDU withthe following four bits of information: (1) preferences(if any) expressed in the EDU; (2) dependencies betweenpreferences expressed in the EDU; (3) dependencies be-tween preferences in the current EDU and previous ones;and (4) preference evolution (namely, the appearance ofa new factor that affects preferred outcomes, update topreferences over values for an existing factor, and so on).For each of these four components, example dialogueswere given for each type of decision they would need tomake, and instructions were given on the format in whichto code their judgements.
Appendix A shows an exampleof an annotated dialogue.Table 2 presents results of the evaluation of (i).
For eachEDU, we asked the annotator to list the preferences ex-pressed in the EDU and we compared the preferences ex-tracted by each judge with those extracted by our algo-rithm.
The triple (a, b, c) respectively indicates the pro-portion of common preferences (two preference sets ?iand ?
j are common if (?i = ?
j) or (?x ?
?i,y ?
?
j ,x?y)?for example, the preference MeetBefore2?MeetAt2implies MeetAt2 ?
MeetAt2), the proportion of prefer-ences that one judge extracts and the other judge or our al-gorithm misses and the proportion of preferences missedby one judge and extracted by the other judge or by ouralgorithm.
The average annotator-algorithm agreement(AAA) is 75.6% and the average inter-annotator agree-211Our algorithm J1 J2 J3 % of EDUs that commit to preferencesOur algorithm (83, 4, 13) (91, 0, 9) (91, 0, 9) 76%J1 (83, 13, 4) (85, 7, 8) (91, 4, 5) 80%J2 (91, 9, 0) (85, 8, 7) (92, 4, 4) 86%J3 (91, 9, 0) (91, 5, 4) (92, 4, 4) 84%Table 2: Evaluating how preferences are extracted from EDUs.Our algorithm J1 J2 J3Our algorithm (85, 71) (96, 100) (93, 86)J1 (85, 71) (89, 71) (91, 86)J2 (96, 100) (89, 71) (98, 86)J3 (93, 86) (91, 86) (98, 86)Table 3: Evaluating how preferences evolve through dialogue.ment (IAA) is 77.9%; this shows that our method for ex-tracting preferences from EDUs is reliable.The evaluation (ii) proceeds as follows.
For each EDU, weask the judge if the segment introduces new preferencesor if it updates, corrects or deletes preferences commitedin previous turns.
As in (i), judges have to justify theirchoices.
Table 3 presents the preliminary results wherethe couple (a,b) indicates respectively the proportion ofcommon elaborations (preference updates or new prefer-ences) and the proportion of common corrections.
Sinceelaboration is also applied in case of other discourse re-lations (e.g., Q-Elab), the measure a evaluates the rules8, 9, 10 (yes) and 11.
Similarly, the measure b evalu-ates the rules 10 (no), 13 and 14.
We obtain AAA=91%IAA=92.7% for elaboration and AAA=85.7% IAA=81%for correction.8 ConclusionWe have proposed a compositional method for elicit-ing preferences from dialogue consisting of a domain-independent algorithm for constructing a partial CP-netdescription of preferences, followed by a domain-specificmethod for identifying the minimal CP-net satisfying thepartial description and domain constraints.
The methodsupports qualitative and partial information about prefer-ences, with CP-nets benefiting from linear algorithms forcomputing the optimal outcome from a set of preferencesand their dependencies.
The need to compute intentionsfrom partially defined preferences is crucial in dialogue,since preferences are acquired and change through dia-logue exchange.Our work partially confirms that CP-nets have a certainnaturalness, as the map from dialogue moves to prefer-ences using the CP-net formalism is relatively intuitive.The next step is to implement our method.
This dependson extracting discourse structure from text, which, thoughdifficult, is becoming increasingly tractable for simpledomains (Baldridge and Lascarides, 2005b).
We plan toextract CP-net descriptions from EDUs and to evaluatethese descriptions using "multi-valued variables" auto-matically.
We will then evaluate our method on a largenumber of dialogues.Our work here is also and more generally a first step to-wards modelling the complex interaction between whatagents say, what their preferences are, and what they takethe preferences of other dialogue agents to be.
It leadsto a conception of dialogue that?s more general than onebased purely on Gricean cooperative principles (Grice,1975).
On a purely Gricean approach, conversation iscooperative in at least two ways: a basic level concern-ing the conventions that govern linguistic meaning (ba-sic cooperativity); and a level concerning shared attitudestowards what is said, including shared intentions (con-tent cooperativity).
While basic cooperation is neededfor communication to work at all, content cooperativ-ity involves strongly cooperative axioms like Coopera-tivity (interlocutors normally adopt the speaker?s inten-tions) (Allen and Litman, 1987, Grosz and Sidner, 1990,Lochbaum, 1998).
Our approach allows for divergentpreferences and divergent intentions, i.e.
conversationsthat aren?t based on content cooperativity.
This will al-low us to exploit information about conflicting agents?preferences and game-theoretic techniques that are inher-ent in the logics of CP-nets for computing optimal moves(Bonzon, 2007).
And in contrast to Franke et al (2009),who analyse conversations where content cooperativitydoesn?t hold using a game-theoretic framework, our ap-proach allows for partial and qualitative representationsof preferences rather than demanding complete and quan-titative representations of them.212ReferencesJ.
Allen and D. Litman.
A plan recognition model forsubdialogues in conversations.
Cognitive Science, 11(2):163?200, 1987.N.
Asher and A. Lascarides.
Logics of Conversation.Cambridge University Press, 2003.J.
Baldridge and A. Lascarides.
Annotating discoursestructures for robust semantic interpretation.
In Pro-ceedings of the Sixth International Workshop on Com-putational Semantics (IWCS), Tilburg, The Nether-lands, 2005a.J.
Baldridge and A. Lascarides.
Probabilistic head-drivenparsing for discourse structure.
In Proceedings ofthe Ninth Conference on Computational Natural Lan-guage Learning (CoNLL), pages 96?103, 2005b.E.
Bonzon.
Mod?lisation des Interactions entre AgentsRationnels: les Jeux Bool?ens.
PhD thesis, Universit?Paul Sabatier, Toulouse, 2007.C.
Boutilier, R.I. Brafman, C. Domshlak, H.H.
Hoos, andDavid Poole.
Cp-nets: A tool for representing andreasoning with conditional ceteris paribus preferencestatements.
Journal of Artificial Intelligence Research,21:135?191, 2004.M.
Franke, T. de Jager, and R. van Rooij.
Relevance incooperation and conflict.
Journal of Logic and Lan-guage, 2009.J.
Ginzburg.
The Interactive Stance: Meaning for Con-versation.
CSLI Publications, to appear.H.
P. Grice.
Logic and conversation.
In P. Cole andJ.
L. Morgan, editors, Syntax and Semantics Volume3: Speech Acts, pages 41?58.
Academic Press, 1975.B.
Grosz and C. Sidner.
Plans for discourse.
In J. Mor-gan P. R. Cohen and M. Pollack, editors, Intentions inCommunication, pages 365?388.
MIT Press, 1990.Y.
He and S. Young.
Spoken language understsandingusing the hidden vector state model.
Speech Commu-nication, 48(3-4):262?275, 2005.J.
R. Hobbs, M. Stickel, D. Appelt, and P. Martin.
Inter-pretation as abduction.
Artificial Intelligence, 63(1?2):69?142, 1993.A.
Lascarides and N. Asher.
Agreement, disputes andcommitment in dialogue.
Journal of Semantics, 26(2):109?158, 2009.O.
Lemon and O. Pietquin.
Machine learning for spokendialogue systems.
In Interspeech, 2007.K.
E. Lochbaum.
A collaborative planning model of in-tentional structure.
Computational Linguistics, 24(4):525?572, 1998.W.
C. Mann and S. A. Thompson.
Rhetorical structuretheory: A framework for the analysis of texts.
Interna-tional Pragmatics Association Papers in Pragmatics,1:79?105, 1987.M.
Poesio and D. Traum.
Towards an axiomatisation ofdialogue acts.
In J. Hulstijn and A. Nijholt, editors,Proceedings of the Twente Workshop on the Formal Se-mantics and Pragmatics of Dialogue.
1998.E.
Rasmusen.
Games and Information: An Introductionto Game Theory.
Blackwell Publishing, 2007.W.
Wahlster, editor.
Verbmobil: Foundations of Speech-to-Speech Translation.
Springer, 2000.213Appendix A : Treatment of an exampleWe illustrate in this section how our rules work on anexample.
Since this dialogue was also evaluated by ourjudges (cf section 7), we give where relevant some detailson those annotations.
The example is as follows:(6) pi1.
A: so, I guess we should have another meet-ingpi2.
A: how long do you think it should be for.pi3.
B: well, I think we have quite a bit to talkabout.pi4.
B: maybe, two hours?pi5.
B: how does that sound.pi6.
A: deadly,pi7.
A: but, let us do it anyways.pi8.
B: okay, do you have any time next week?pi9.
B: I have got, afternoons on Tuesday andThursday.pi10.
A: I am out of Tuesday Wednesday Thurs-day,pi11.
A: so, how about Monday or FridayTable 4 is the DSDRS associated with (6).Relation(pii, [pi j ?
pik]) indicates that a rhetorical re-lation holds between the segment pii and a segmentconsisting of pi j, pi j+1, .
.
.
, pikpi1 provides an atomic preference.
We apply the rule1 and so CommitA(pi1,D N A) adds the descriptionD N A |=M ?M(CPT (M)) where M means Meet.pi2 We have Q-Elab(pi1, pi2).
A continues to commit toM in pi2 and no new preferences are introduced bypi2.
We apply rule 9, which makes the P(pi2) de-scription the same as P(pi1)?s.pi3 is linked to pi2 with QAP.
B accepts A?s preferenceand we apply the rule 11 since pi2 is a wh-question.Thus CommitB(ElabB(pi2,pi3),D N B) adds the de-scription D N B |= M ?M(CPT (M)).
It is interest-ing to note that some judges consider that agent?sutterance in pi3 indicates a preference towards "talk-ing a long time" while other judges consider, as ourmethod predicts, that this segment does not conveyany preference.pi4 is linked to pi3 by Q-Elab.
B commits to a newpreference.
We apply rule 9, rule 8 and then rule8.a.
The preference on the hour is now dependenton the preference on meeting; i.e., D N B |= M :2h ?
2h(CPT (2h)), where the variable 2h meanstwo hours.pi5 is related to pi4 with the Q-Cont relation.
Wethen follow the same rule as the continued relation,namely Q-Elab.
We apply rule 9 which does notchange the CP-net description of B because pi5 doesnot convey any preference.pi6 is related to pi5 with QAP relation.
In this case, it?snot clear what is the QAP target and so we applyrule 12: we wait to learn more and we do not changeB?s CP-net description.All the Judges indicated that segments pi5 and pi6are ambiguous and therefore hesitated to say if theycommit to preferences.
For example in pi6, do wehave a preference for meeting more than 2 hoursor less than 2 hours?
This indecision is compatiblewith the predictions of rule 12.pi7 A accepts B?s preference.
We apply rule 9 and thenrule 8 to obtain:D N A |=M ?M(CPT (M)),D N A |=M : 2h?
2h(CPT (2h)).pi8 is linked to pi7 by Q-Elab.
B introduces a new pref-erence for meeting next week.We apply rule 9 and then 8 to obtain:D N B |=M ?M(CPT (M)),D N B |=M : 2h?
2h(CPT (2h)),D N B |=M?2h :NW ?NW (CPT (NW ))where thevariable NW means next week.pi9 is linked to pi8 by Plan-Elab.
pi9 expresses com-mitments to preference that already involve aCP-net description.
B introduces three prefer-ences: one for meeting on Tuesday, the otherfor meeting on Thursday and given the conjunc-tion of preferences Tues ?
Thurs, one for timeafternoon (of Tuesday and Thursday).
That is,((?(Tues)??
(Thurs)) 7?
Aft)(P(pi9)).
We applythe equivalence (3) and obtain :(?(Tues?Thurs)?
Aft)(P(pi9)).Then, we apply rules 8.g, 8.b and 8.d.
The CP-netdescription of B is thus updated as follows:D N B |= M ?
2h ?
NW ?
Tues : Thurs ?Thurs(CPT (Thurs)),D N B |= M ?
2h ?
NW ?
Tues : Thurs ?Thurs(CPT (Thurs)),D N B |= M ?
2h ?
NW ?
Thurs : Tues ?Tues(CPT (Tues)),D N B |= M ?
2h ?
NW ?
Thurs : Tues ?Tues(CPT (Tues)),D N B |= M ?
2h ?
NW ?
(Thurs ?
Tues) : Aft ?Aft(CPT (Aft)).Most judges express here a preference ranking overoutcomes.
For instance, if B elaborates by addingthe preference "I have got Monday morning too"(as it is in the test corpus), some consider the rank-ing "(Tuesday or Thursday afternoons) ?
(Monday214Turn A?s SDRS B?s SDRS1 pi1A : Q-Elab(pi1,pi2) /02 pi1A:is the same as in turn 1 pi2B : Q-Elab(pi1, [pi2?pi5])?QAP(pi2, [pi3?pi5])?Q-Elab(pi3,pi)pi : Q-Cont(pi4,pi5)3 pi3A : Q-Elab(pi1, [pi2?pi7])?QAP(pi2, [pi3?pi7])?
pi2B: is the same as in turn 2Q-Elab(pi3, [pi4,pi7])?QAP(pi,pi?
)pi : Q-Cont(pi4,pi5),pi?
: Contrast(pi6,pi7)4 pi3A: is the same as in turn 3 pi4B : Q-Elab(pi1, [pi2?pi9])?QAP(pi2, [pi3?pi9])?Q-Elab(pi3, [pi4?pi9])?QAP(pi, [pi6?pi9])?Q-Elab(pi?,pi??
)pi : Q-Cont(pi4,pi5),pi?
: Contrast(pi6,pi7)pi??
: Plan-Elab(pi8,pi9)5 pi5A : Q-Elab(pi1, [pi2?pi11])?QAP(pi2, [pi3?pi11])?
pi4B: is the same as in turn 4Q-Elab(pi3, [pi4?pi11])?QAP(pi, [pi6?pi11])?Q-Elab(pi?, [pi8?pi11])?QAP(pi??,pi???
)pi : Q-Cont(pi4,pi5),pi?
: Contrast(pi6,pi7)pi??
: Plan-Elab(pi8,pi9),pi???
: Q-Elab(pi10,pi11)Table 4: The DSDRS for Dialogue (6).morning)?
(other days)", while others consider theranking "(Tuesday or Thursday afternoon) or (Mon-day morning)?
(other days)".
We did not treat suchpreference ranking.pi10 is related to pi9 by QAP where A answers no to B?squestion asked in pi8.
We apply rule 10 (no).
SinceTues&Weds&Thurs(P(pi10)) is not consistent with((?
(Tues) ??
(Thurs)) 7?
Aft)(P(pi9)), we applyCommitA(Correction(pi9,pi10),D N A), which addsthe preference Weds to A?s description and thenthe rule 13 where Tues and Thurs are respectivelyreplaced by Tues and Thurs :D N A |=M?2h?NW : Tues?
Tues(CPT (Tues)),D N A |= M ?
2h ?
NW : Thurs ?Thurs(CPT (Thurs)),D N A |= M ?
2h ?
NW : Weds ?Weds(CPT (Weds)).pi11 Finally, this segment is linked to pi10 with Q-Elabwhere Mond?Fri(P(pi11)).
We apply rules 9 and8.b and update A?s CP-net description as follows:D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :Mond ?Mond(CPT (Mond)),D N A |=M?2h?NW ?Tues?Thurs?Weds?Fri :Mond ?Mond(CPT (Mond)),D N A |= M ?
2h ?
NW ?
Tues ?
Thurs ?Weds ?Mond : Fri?
Fri(CPT (Fri)),D N A |= M ?
2h ?
NW ?
Tues ?
Thurs ?Weds ?Mond : Fri?
Fri(CPT (Fri)).The evaluation of this dialogue also reveals to what extentnaive annotators reason with binary (Monday preferredto not Monday) or multi-valued variables (Monday pre-ferred to Tuesday).
Most judges use multi-valued vari-ables to express the preference extracted from an EDU,and the way in which our method exploits domain knowl-edge to yield the minimal CP-net satisfying the descrip-tion reflects this.
In addition, some judges use a smallset of variables (for example the variable time of meetingthat groups together the notion of week, day, hours, etc.
)while others use a distinct variable for each preference.Finally, we also noticed that judges do not describe thesame preference dependencies.
For example, in:(7) We could have lunch together and then have themeeting from one to three?some consider that the preference on having lunch is in-dependent from the preference on the meeting (in thiscase, they consider that the preference on the period oneto three is independent from the preference on meeting)while others consider that the two preferences are depen-dent.215
