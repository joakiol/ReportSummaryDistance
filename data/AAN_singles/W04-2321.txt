On the use of confidence for statistical decision in dialogue strategiesChristian Raymond1 Fre?de?ric Be?chet1 Renato de Mori1 Ge?raldine Damnati21 LIA/CNRS, University of Avignon France 2 France Telecom R&D, Lannion, Francechristian.raymond,frederic.bechet,renato.demori@lia.univ-avignon.frgeraldine.damnati@rd.francetelecom.comAbstractThis paper describes an interpretation and deci-sion strategy that minimizes interpretation er-rors and perform dialogue actions which maynot depend on the hypothesized concepts only,but also on confidence of what has been rec-ognized.
The concepts introduced here are ap-plied in a system which integrates languageand interpretation models into Stochastic FiniteState Transducers (SFST).
Furthermore, acous-tic, linguistic and semantic confidence mea-sures on the hypothesized word sequences aremade available to the dialogue strategy.
Byevaluating predicates related to these confi-dence measures, a decision tree automaticallylearn a decision strategy for rescoring a n-bestlist of candidates representing a user?s utter-ance.
The different actions that can be then per-formed are chosen according to the confidencescores given by the tree.1 IntroductionThere is a wide consensus in the scientific communitythat human-computer dialogue systems based on spokennatural language make mistakes because the AutomaticSpeech Recognition (ASR) component may not hypothe-size some of the pronounced words and the various levelsof knowledge used for recognizing and reasoning aboutconceptual entities are imprecise and incomplete.
In spiteof these problems, it is possible to make useful applica-tions with dialogue systems using spoken input if suit-able interpretation and decision strategies are conceivedthat minimize interpretation errors and perform dialogueactions which may not depend on the hypothesized con-cepts only, but also on confidence of what has been rec-ognized.This paper introduces some concepts developed fortelephone applications in the framework of stochasticmodels for interpretation and dialogue strategies, a goodoverview of which can be found in (Young, 2002).The concepts introduced here are applied in a systemwhich integrates language and interpretation models intoStochastic Finite State Transducers (SFST).
Furthermore,acoustic, linguistic and semantic confidence measures onthe hypothesized word sequences are made available tothe dialogue strategy.
A new way of using them in thedialogue decision process is proposed in this paper.Most of the Spoken language Understanding Systems(SLU) use semantic grammars with semantic tags as non-terminals (He and Young, 2003) with rules for rewritingthem into strings of words.The SFSTs of the system used for the experiments de-scribed here, represent knowledge for the basic buildingblocks of a frame-based semantic grammar.
Each blockrepresents a property/value relation.
Different SFSTsmay share words in the same sentence.
Property/valuehypotheses are generated with an approach described in(Raymond et al, 2003) and are combined into a sentenceinterpretation hypothesis in which the same word maycontribute to more than one property/value pair.
The di-alogue strategy has to evaluate the probability that eachcomponent of each pair has been correctly hypothesizedin order to decide to perform an action that minimizes therisk of user dissatisfaction.2 Overview of the decoding processThe starting point for decoding is a lattice of wordhypotheses generated with an n-gram language model(LM).
Decoding is a search process which detects com-binations of specialized SFSTs and the n-gram LM.
Theoutput of the decoding process consists of a n-best listof conceptual interpretations ?.
An interpretation ?
isa set of property/value pairs sj = (cj, vj) called con-cepts.
cj is the concept tag and vj is the concept valueof sj .
Each concept tag cj is represented by a SFST andcan be related either to the dialogue application (phonenumber, date, location expression, etc.)
or to the dia-logue management (confirmation, contestation, etc.).
Toeach string of words recognized by a given SFST cj isassociated a value vj representing a normalized valuefor the concept detected.
For example, to the wordphrase: on July the fourteenth, detected by aSFST dedicated to process dates, is associated the value:???
?/07/14.The n-best list of interpretations output by the decod-ing process is structured according to the different con-cept tag strings that can be found in the word lattice.
Toeach concept tag string is attached another n-best list onthe concept values.
This whole n-best is called a struc-tured n-best.
After presenting the statistical model usedin this study, we will describe the implementation of thisdecoding process.3 Statistical modelThe contribution of a sequence of words W to a concep-tual structure ?
is evaluated by the posterior probabilityP (?
| Y ), where Y is the description of acoustic features.Such a probability is computed as follows:P (?
| Y ) =?W?SW P (Y | W )P (?
| W )?P (W )?
?W?SW P (Y | W )P (W )?
(1)where P (Y | W ) is provided by the acoustic models,P (W ) is computed with the LM.
Exponents ?
and ?
arerespectively a semantic and a syntactic fudge factor.
SWcorresponds to the set of word strings that can be found inthe word lattice.
P (?
| W ) is computed by consideringthat thus:P (?
| W ) = P (s1 | W ).J?j=2P (sj | sj?11 W ) (2)P (sj | sj?11 W ) ?
P (sj | W )If the conceptual component sj is hypothesized witha sentence pattern pij(W ) recognized in W and pik(W )triggers a pair sk and there is a training set with whichthe probabilities P (pik(W ) | sk) ?k, can be estimated,then the posterior probability can be obtained as follows:P (sj | W ) =P (pij(W ) | sj)P (sj)?Kk=1 P (pik(W ) | sk)P (sk)(3)where P (sk) is a unigram probability of conceptualcomponents.4 Structured N-best listN-best lists are generally produced by simply enumerat-ing the n best paths in the word graphs produced by Au-tomatic Speech Recognition (ASR) engines.
The scoresused in such graphs are usually only a combination ofacoustic and language model scores, and no other linguis-tic levels are involved.
When an n-best word hypothesislist is generated, the differences between the hypothesisi and the hypothesis i+1 are often very small, made ofonly one or a few words.
This phenomenon is aggravatedwhen the ASR word graph contains a low confidencearea, due for example to an Out-Of-Vocabulary word, toa noisy input or to a speech disfluency.This is the main weakness of this approach in a SpokenDialogue context: not all words are important to the Di-alogue Manager, and all the n-best word hypotheses thatdiffer only between each other because of some speechdisfluency effects can be considered as equals.
That?swhy it is important to generate not only a n-best list ofword hypotheses but rather a n-best list of interpretations,each of them corresponding to a different meaning fromthe Dialogue Manager point of view.We propose here a method for directly extracting sucha structured n-best from a word lattice output by an ASRengine.
This method relies on operations between FiniteState Machines and is implemented thanks to the AT&TFSM toolkit (see (Mohri et al, 2002) for more details).4.1 Word-to-Concept transducerEach concept ck of the dialogue application is associatedwith an FSM.
These FMSs are called acceptors (Ak forthe concept ck).
In order to process strings of words thatdon?t belong to any concept, a filler model, called AFis used.
Because the same string of words can?t belongto both a concept model and the background text, all thepaths contained in the acceptors Ak are removed from thefiller model AF in the following way:AF = ?
?
?m?k=1Akwhere ?
is the word lexicon of the application and mis the number of concepts used.All these acceptors are now turned into transducersthat take words as input symbols and start or end con-cept tags as output symbols.
Indeed, all acceptors Ak be-come transducers Tk where the first transition emits thesymbol <Ck> and the last transition the symbol </Ck>.Similarly the filler model becomes the transducer Tbkwhich emits the symbols <BCK> and </BCK>.
Exceptthese start and end tags, no other symbols are emitted: allwords in the concept or background transducers emit anempty symbol.Finally all these transducers are linked together in asingle model called Tconcept as presented in figure 1.FILLERout=<BCK>in=<start>out=<>in=<> in=<>out=</BCK>in=<end>out=<>out=<>in=<start>out=<>in=w1in=w2out=<>in=<end>out=<>in=<>out=</C1>out=<C1>in=<>out=<C2>in=<>out=</C2>in=<>in=<>out=<Cn>in=<>out=</Cn>in=<>out=<>in=<>out=<>in=<>out=<>SFST C1SFST CnSFST C2Figure 1: Word-to-Concept Transducer4.2 Processing the ASR word latticeThe ASR word lattice is coded by an FSM: an acceptor Lwhere each transition emits a word.
The cost function fora transition corresponds to the acoustic score of the wordemitted.The first step in the word lattice processing consistsof rescoring each transition of L by means of a 3-gramLanguage Model (LM) in order to obtain the probabili-ties P (W ) of equation 1.
This is done by composing theword lattice with a 3-gram LM also coded as an FSM (see(Allauzen et al, 2003) for more details about statisticalLMs and FSMs).The resulting FSM is then composed with the trans-ducer TConcept in order to obtain the word-to-concepttransducer L?.
A path in L?
corresponds to a word stringif only the input symbols of the transducer are consideredand its score is the one expressed by equation 1; simi-larly by considering only the output symbols, a path in L?corresponds to a concept tag string.The structured n-best list is directly obtained from L?
:by extracting the n-best concept tag strings (output labelpaths) we obtain an n-best list on the conceptual interpre-tations.
The score of each conceptual interpretation is thesum of all the word strings (input label paths) in the wordlattice producing the same interpretation.Finally, for every conceptual interpretations C kept atthe previous step, a local n-best list on the word strings iscalculated by selecting in L?
the best paths outputting thestring C .The resulting structured n-best is illustrated by the fol-lowing example.
If we keep the 2 best conceptual in-terpretations C1, C2 of a transducer L?
and, for each ofthese, the 2 best word strings, we obtain:1 : C1 = <c1_1,c1_2,..,c1_x>1.1 : W1.1 = <v1.1_1,v1.1_2,..,v1.1_x>1.2 : W1.2 = <v1.2_1,v1.2_2,..,v1.2_x>2 : C2 = <c2_1,c2_2,..,c2_y>2.1 : W2.1 = <v2.1_1,v2.1_2,..,v2.1_y>2.2 : W2.2 = <v2.2_1,v2.2_2,..,v2.2_y>where <ci_1,ci_2,..,ci_y> is the conceptualinterpretation at the rank i in the n-best list; Wi.j is theword string ranked j of interpretation i; and vi.j_kis the concept value of the kth concept ci_k of the jthword string of interpretation i.5 Use of correctness probabilitiesIn order to select a particular interpretation ?
(concep-tual interpretation + concept values) from the structuredn-best list, we are now interested in computing the proba-bility that ?
is correct, given a set of confidence measuresM : P (?
| M ).
The choice of the confidence measuresdetermines the quality of the decision strategy.
Thoseused in this study are briefly presented in the next sec-tions.5.1 Confidence measures5.1.1 Acoustic confidence measure (AC)This confidence measure relies on the comparison ofthe acoustic likelihood provided by the speech recogni-tion model for a given hypothesis to the one that wouldbe provided by a totally unconstrained phoneme loopmodel.
In order to be consistent with the general model,the acoustic units are kept identical and the loop is overcontext dependent phonemes.
This confidence measureis used at the utterance level and at the concept level (see(Raymond et al, 2003) for more details).5.1.2 Linguistic confidence measure (LC)In order to assess the impact of the absence of ob-served trigrams as a potential cause of recognition errors,a Language Model consistency measure is introduced.This measure is simply, for a given word string candi-date, the ratio between the number of trigrams observedin the training corpus of the Language Model vs. the totalnumber of trigrams in the same word string.
Its computa-tion is very fast and the confidence scores obtained fromit give interesting results as presented in (Este`ve et al,2003).5.1.3 Semantic confidence measure (SC)Several studies have shown that text classification tools(like Support Vector Machines or Boosting algorithms)can be an efficient way of labeling an utterance transcrip-tion with a semantic label such as a call-type (Haffner etal., 2003) in a Spoken Dialogue context.
In our case, thesemantic labels attached to an utterance are the differentconcepts handled by the Dialogue Manager.
One classi-fier is trained for each concept tag in the following way:Each utterance of a training corpus is labeled with atag, manually checked, indicating if a given concept oc-curs or not in the utterance.
In order to let the classi-fier model the context of occurrence of a concept ratherthan its value we removed most of the concept headwordsfrom the list of criterion used by the classifier.During the decision process, if the interpretation eval-uated contains 2 concepts c1 and c2, then the classifierscorresponding to c1 and c2 are used to give to the utter-ance a confidence score of containing these two concepts.The text classifier used in the experimental sectionis a decision-tree classifier based on the Semantic-Classification-Trees introduced for the ATIS taskby (Kuhn and Mori, 1995) and used for semantic disam-biguation in (Be?chet et al, 2000).5.1.4 Rank confidence measure (R)To the previous confidence measures we added therank of each candidate in its n-best.
This rank containstwo numbers: the rank of the interpretation of the utter-ance and the rank of the utterance among those havingthe same interpretation.5.2 Decision Tree based strategyAs the dependencies of these measures are difficult to es-tablish, their values are transformed into symbols by vec-tor quantization (VQ) and conjunctions of these symbolsexpressing relevant statistical dependencies are obtainedby a decision tree which is trained with a developmentset of examples.
At the leaves probabilities P (M |?)
areobtained when ?
represents any correct hypothesis, thecase in which only the properties have been correctly rec-ognized or both properties and values have errors.
Withthese probabilities we are now able to estimate P (?
| M )in the following way:P (?
| M) = 11 + P (M |??
)P (??
)P (M |?
)P (?
)(4)where ??
indicates that the interpretation in question isincorrect and P (M |??)
= 1 ?
P (M |?
).6 From hypotheses to actionsOnce concepts have been hypothesized, a dialog systemhas to decide what action to perform.
Let A = aj bethe set of actions a system can perform.
Some of themcan be requests for clarification or repetition.
In partic-ular, the system may request the repetition of the entireutterance.
Performing an action has a certain risk and thedecision about the action to perform has to be the one thatminimizes the risk of user dissatisfaction.It is thus possible that some or all the hypothesizedcomponents of a conceptual structure ?
do not corre-spond to the user intention because the word sequenceW based on which the conceptual hypothesis has beengenerated contains some errors.
In particular, there arerequests for clarification or repetition which should beperformed right after the interpretation of an utterance inorder to reduce the stress of the user.
It is important tonotice that actions consisting in requests for clarificationor repetition mostly depend on the probability that the in-terpretation of an utterance is correct, rather than on theutterance interpretation.The decoding process described in section 2 providesa number of hypotheses containing a variable number ofpairs sj = (cj, vj) based on the score expressed by equa-tion 1.P (?
| M ) is then computed for these hypotheses.
Theresults can be used to decide to accept an interpretationor to formulate a clarification question which may implymore hypotheses.For simplification purpose, we are going to considerhere only two actions: accepting the hypothesis with thehigher P (?
| M ) or rejecting it.
The risk associated to theacceptation decision is called ?fa and corresponds to thecost of a false acceptation of an incorrect interpretation.Similarly the risk associated to the rejection decision iscalled ?fr and corresponds to the cost of a false rejectionof a correct interpretation.
In a spoken dialogue context,?fa is supposed to be higher than ?fr .The choice of the action to perform is determined bya threshold ?
on P (?
| M ).
This threshold is tuned ona development corpus by minimizing the total risk R ex-pressed as follows:R = ?fa ?NfaNtotal+ ?fr ?NfrNtotal(5)Nfa and Nfr are the numbers of false acceptation andfalse rejection decisions on the development corpus for agiven value of ?.
Ntotal is the total number of examplesavailable for tuning the strategy.The final goal of the strategy is to make negligible Nfaand the best set of confidence measures is the one thatminimizes Nfr .
In fact, the cost of these cases is lowerbecause the corresponding action has to be a request forrepetition.Instead of simply discarding an utterance if P (?
| M )is below ?, another strategy we are investigating consistsof estimating the probability that the conceptual interpre-tation alone (without the concept values) is correct.
Thisprobability can be estimated the same way as P (?
| M )and can be used to choose a third kind of actions: accept-ing the conceptual meaning of an utterance but asking forclarifications about the values of the concepts.A final decision about the strategy to be adopted shouldbe based on statistics on system performance to be col-lected and updated after deploying the system on the tele-phone network.7 Experiments7.1 Application domainThe application domain considered in this study is arestaurant booking application developed at France Tele-com R&D.
At the moment, we only consider in our strat-egy the most frequent concepts related to the applicationdomain: PLACE, PRICE and FOOD TYPE.
They can bedescribed as follows:?
PLACE: an expression related to a restaurant loca-tion (eg.
a restaurant near Bastille);?
PRICE: the price range of a restaurant (eg.
less thana hundred euros);?
FOOD TYPE: the kind of food requested by thecaller (eg.
an Indian restaurant).These entities are expressed in the training corpus byshort sequences of words containing three kinds of to-ken: head-words like Bastille, concept related words likerestaurant and modifier tokens like near.A single value is associated to each concept entitysimply be adding together the head-words and somemodifier tokens.
For example, the values associated tothe three contexts presented above are: Bastille ,less+hundred+euros and indian.In the results section a concept detected is considered asuccess only if the tag exists in the reference corpus and ifboth values are identical.
It?s a binary decision process:a concept can be considered as a false detection even ifthe concept tag is correct and if the value is partially cor-rect.
The measure on the errors (insertion, substitution,deletion) of these concept/value tokens is called in thispaper the Understanding Error Rate, by opposition to thestandard Word Error Rate measure where all words areconsidered equals.7.2 Experimental setupExperiments were carried out on a dialogue corpus pro-vided by France Telecom R&D.
The task has a vocabu-lary of 2200 words.
The language model used is madeof 44K words.
For this study we selected utterances cor-responding to answers to a prompt asking for the kindof restaurant the users were looking for.
This corpus hasbeen cut in two: a development corpus containing 511utterances and a test corpus containing 419 utterances.This development corpus has been used to train the deci-sion tree presented in section 5.2.
The Word Error Rateon the test corpus is 22.7%.7.3 Evaluation of the rescoring strategyTable 1 shows the results obtained with a rescoring strat-egy that selects, from the structured n-best list, the hy-pothesis with the highest P (?
| M ).
The baseline re-sults are obtained with a standard maximum-likelihoodapproach choosing the hypothesis maximizing the proba-bility P (?
| Y ) of equation 1.
No rejection is performedin this experiment.The size of the n-best lists was set to 12 items: the first4 candidates of the first 3 interpretations in the structuredn-best list.
The gain obtained after rescoring is very sig-nificant and justify our 2-step approach that first extractan n-best list of interpretations thanks to P (?
| Y ) andthen choose the one with the highest confidence accord-ing to a large set of confidence measures M .
This gaincan be compared to the one obtained on the Word ErrorRate measure: the WER drops from 21.6% to 20.7% af-ter rescoring on the development corpus and from 22.7%to 22.5% on the test corpus.
It is clear here that theWER measure is not an adequate measure in a SpokenDialogue context as a big reduction in the UnderstandingError Rate might have very little effect on the Word ErrorRate.Corpus baseline rescoring UER reduction %Devt.
15.0 12.4 17.3%Test 17.7 14.5 18%Table 1: Understanding Error Rate results with and with-out rescoring on structured n-best lists (n=12) (no rejec-tion)7.4 Evaluation of the decision strategyIn this experiment we evaluate the decision strategy con-sisting of accepting or rejecting an hypothesis ?
thanks toa threshold on the probability P (?
| M ).
Figure 2 showsthe curve UER vs. utterance rejection on the developmentand test corpora.
As we can see very significant improve-ments can be achieved with very little utterance rejection.For example, at a 5% utterance rejection operating point,the UER on the development corpus drops from 15.0% to8.6% (42.6% relative improvement) and from 17.7% to11.4% (35.6% relative improvement).By using equation 5 for finding the operating pointminimizing the risk fonction (with a cost ?fa = 1.5 ?
?fr) on the development corpus we obtain:?
on the development corpus: UER=6.5 utterance re-jection=13.1?
on the test corpus: UER=9.6 utterance rejec-tion=15.946810121416180 5 10 15 20understanding error rateutterance rejection (%)devttestFigure 2: Understanding Error Rate vs. utterance rejec-tion on the development and test corpora8 ConclusionThis paper describes an interpretation and decision strat-egy that minimizes interpretation errors and perform dia-logue actions which may not depend on the hypothesizedconcepts only, but also on confidence of what has beenrecognized.
The first step in the process consists of gen-erating a structured n-best list of conceptual interpreta-tions of an utterance.
A set of confidence measures isthen used in order to rescore the n-best list thanks to a de-cision tree approach.
Significant gains in UnderstandingError Rate are achieved with this rescoring method (18%relative improvement).
The confidence score given by thetree can also be used in a decision strategy about the ac-tion to perform.
By using this score, significant improve-ments in UER can be achieved with very little utterancerejection.
For example, at a 5% utterance rejection op-erating point, the UER on the development corpus dropsfrom 15.0% to 8.6% (42.6% relative improvement) andfrom 17.7% to 11.4% (35.6% relative improvement).
Fi-nally the operating point for a deployed dialogue systemcan be chosen by explicitly minimizing a risk function ona development corpus.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.Generalized algorithms for constructing statistical lan-guage models.
In 41st Annual Meeting of the Associa-tion for Computational Linguistics (ACL?03), Sapporo,Japan.Fre?de?ric Be?chet, Alexis Nasr, and Franck Genet.
2000.Tagging unknown proper names using decision trees.In 38th Annual Meeting of the Association for Compu-tational Linguistics, Hong-Kong, China, pages 77?84.Yannick Este`ve, Christian Raymond, Renato De Mori,and David Janiszek.
2003.
On the use of linguisticconsistency in systems for human-computer dialogs.IEEE Transactions on Speech and Audio Processing,(Accepted for publication, in press).Patrick Haffner, Gokhan Tur, and Jerry Wright.
2003.Optimizing SVMs for complex call classification.
InIEEE International Conference on Acoustics, Speechand Signal Processing, ICASSP?03, Hong-Kong.Y.
He and S. Young.
2003.
A data-driven spoken lan-guage understanding system.
In Automatic SpeechRecognition and Understanding workshop - ASRU?03,St.
Thomas, US-Virgin Islands.R.
Kuhn and R. De Mori.
1995.
The application of se-mantic classification trees to natural language under-standing.
IEEE Trans.
on Pattern Analysis and Ma-chine Intelligence, 17(449-460).Mehryar Mohri, Fernando Pereira, and Michael Ri-ley.
2002.
Weighted finite-state transducers inspeech recognition.
Computer, Speech and Language,16(1):69?88.Christian Raymond, Yannick Este`ve, Fre?de?ric Be?chet,Renato De Mori, and Ge?raldine Damnati.
2003.
Beliefconfirmation in spoken dialogue systems using confi-dence measures.
In Automatic Speech Recognition andUnderstanding workshop - ASRU?03, St. Thomas, US-Virgin Islands.Steve Young.
2002.
Talking to machines (statisti-cally speaking).
In International Conference on Spo-ken Language Processing, ICSLP?02, pages 113?120,Denver, CO.
