Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 434?442,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsImproving the Lexical Function Composition Modelwith Pathwise Optimized Elastic-Net RegressionJiming Li and Marco Baroni and Georgiana DinuCenter for Mind/Brain SciencesUniversity of Trento, Italy(jiming.li|marco.baroni|georgiana.dinu)@unitn.itAbstractIn this paper, we show that the lexicalfunction model for composition of dis-tributional semantic vectors can be im-proved by adopting a more advanced re-gression technique.
We use the pathwisecoordinate-descent optimized elastic-netregression method to estimate the compo-sition parameters, and compare the result-ing model with several recent alternativeapproaches in the task of composing sim-ple intransitive sentences, adjective-nounphrases and determiner phrases.
Experi-mental results demonstrate that the lexicalfunction model estimated by elastic-net re-gression achieves better performance, andit provides good qualitative interpretabil-ity through sparsity constraints on modelparameters.1 IntroductionVector-based distributional semantic models ofword meaning have gained increased attention inrecent years (Turney and Pantel, 2010).
Differ-ent from formal semantics, distributional seman-tics represents word meanings as vectors in a high-dimensional semantic space, where the dimen-sions are given by co-occurring contextual fea-tures.
The intuition behind these models lies inthe fact that words which are similar in meaningoften occur in similar contexts, e.g., moon andstar might both occur with sky, night and bright.This leads to convenient ways to measure similar-ity between different words using geometric meth-ods (e.g., the cosine of the angle between twovectors that summarize their contextual distribu-tion).
Distributional semantic models have beensuccessfully applied to many tasks in linguisticsand cognitive science (Griffiths et al., 2007; Foltzet al., 1998; Laham, 1997; McDonald and Brew,2004).
However, most of these tasks only dealwith isolated words, and there is a strong needto construct representations for longer linguisticstructures such as phrases and sentences.
In or-der to achieve this goal, the principle of com-positionality of linguistic structures, which statesthat complex linguistic structures can be formedthrough composition of simple elements, is ap-plied to distributional vectors.
Therefore, in recentyears, the problem of composition within distribu-tional models has caught many researchers?
atten-tion (Clark, 2013; Erk, 2012).A number of compositional frameworks havebeen proposed and tested.
Mitchell and Lapata(2008) propose a set of simple component-wiseoperations, such as multiplication and addition.Later, Guevara (2010) and Baroni and Zampar-elli (2010) proposed more elaborate methods, inwhich composition is modeled as matrix-vectormultiplication operations.
Particularly new to theirapproach is the proposal to estimate model param-eters by minimizing the distance of the composedvectors to corpus-observed phrase vectors.
For ex-ample, Baroni and Zamparelli (2010) consider thecase of Adjective-Noun composition and model itas matrix-vector multiplication: adjective matricesare parameters to be estimated and nouns are co-occurrence vectors.
The model parameter estima-tion procedure becomes a multiple response mul-tivariate regression problem.
This method, that,following Dinu et al.
(2013) and others, we termthe lexical function composition model, can alsobe generalized to more complex structures suchas 3rd order tensors for modeling transitive verbs(Grefenstette et al., 2013).Socher et al.
(2012) proposed a more complexand flexible framework based on matrix-vectorrepresentations.
Each word or lexical node in aparsing tree is assigned a vector (representing in-herent meaning of the constituent) and a matrix(controlling the behavior to modify the meaning of434Model Composition function ParametersAdd w1u?
+ w2v?
w1, w2Mult u?w1?
v?w2w1, w2Dil ||u?||22v?
+ (??
1)?u?, v??u?
?Fulladd W1u?
+ W2v?
W1,W2?
Rm?mLexfunc Auv?
Au?
Rm?mFulllex tanh([W1,W2][Auv?Avu?])
W1,W2,Au, Av?
Rm?mTable 1: Composition functions of inputs (u, v).neighbor words or phrases) simultaneously.
Theyuse recursive neural networks to learn and con-struct the entire model and show that it reachesstate-of-the-art performance in various evaluationexperiments.In this paper, we focus on the simpler, linearlexical function model proposed by Baroni andZamparelli (2010) (see also Coecke et al.
(2010))and show that its performance can be further im-proved through more advanced regression tech-niques.
We use the recently introduced elastic-net regularized linear regression method, whichis solved by the pathwise coordinate descent opti-mization algorithm along a regularization parame-ter path.
This new regression method can rapidlygenerate a sequence of solutions along the regular-ization path.
Performing cross-validation on thisparameter path should yield a much more accuratemodel for prediction.
Besides better prediction ac-curacy, the elastic-net method also brings inter-pretability to the composition procedure throughsparsity constraints on the model.The rest of this paper is organized as follows: InSection 2, we give details on the above-mentionedcomposition models, which will be used for com-parison in our experiments.
In Section 3, we de-scribe the pathwise optimized elastic-net regres-sion algorithm.
Experimental evaluation on threecomposition tasks is provided in Section 4.
In Sec-tion 5 we conclude and suggest directions for fu-ture work.2 Composition ModelsMitchell and Lapata (2008; 2010) present a set ofsimple but effective models in which each compo-nent of the output vector is a function of the cor-responding components of the inputs.
Given in-put vectors u?
and v?, the weighted additive model(Add) returns their weighted sum: p?
= w1u?
+w2v?.
In the dilation model (Dil), the output vectoris obtained by decomposing one of the input vec-tors, say v?, into a vector parallel to u?
and an or-thogonal vector, and then dilating only the parallelvector by a factor ?
before re-combining (formulain Table 1).
Mitchell and Lapata also propose asimple multiplicative model in which the outputcomponents are obtained by component-wise mul-tiplication of the corresponding input components.We use its natural weighted extension (Mult), in-troduced by Dinu et al.
(2013), that takes w1andw2powers of the components before multiplying,such that each phrase component piis given by:pi= uw1ivw2i.Guevara (2010) and Zanzotto et al.
(2010) ex-plore a full form of the additive model (Fulladd),where the two vectors entering a composition pro-cess are pre-multiplied by weight matrices beforebeing added, so that each output component isa weighted sum of all input components: p?
=W1u?
+ W2v?.Baroni and Zamparelli (2010) and Coecke etal.
(2010), taking inspiration from formal seman-tics, characterize composition as function applica-tion.
For example, Baroni and Zamparelli modeladjective-noun phrases by treating the adjectiveas a regression function from nouns onto (mod-ified) nouns.
Given that linear functions can beexpressed by matrices and their application bymatrix-by-vector multiplication, a functor (suchas the adjective) is represented by a matrix Auto be composed with the argument vector v?
(e.g.,the noun) by multiplication, returning the lexicalfunction (Lexfunc) representation of the phrase:p?
= Auv?.The method proposed by Socher et al.
(2012)can be seen as a combination and non-linear ex-tension of Fulladd and Lexfunc (that Dinu and col-leagues thus called Fulllex) in which both phraseelements act as functors (matrices) and arguments(vectors).
Given input terms u and v representedby (u?, Au) and (v?, Av), respectively, their com-position vector is obtained by applying first a lin-ear transformation and then the hyperbolic tangentfunction to the concatenation of the products Auv?and Avu?
(see Table 1 for the equation).
Socherand colleagues also present a way to construct ma-trix representations for specific phrases, neededto scale this composition method to larger con-stituents.
We ignore it here since we focus on thetwo-word case.Parameter estimation of the above compositionmodels follows Dinu et al.
(2013) by minimizingthe distance to corpus-extracted phrase vectors.
In435Figure 1: A sketch of the composition model train-ing and composing procedure.the case of the Fulladd and Lexfunc models thisamounts to solving a multiple response multivari-ate regression problem.The whole composition model training andphrase composition procedure is described with asketch in Figure 1.
To illustrate with an example,given an intransitive verb boom, we want to traina model for this intransitive verb so that we canuse it for composition with a noun subject (e.g.,export) to form an intransitive sentence (e.g., ex-port boom(s)).
We treat these steps as a composi-tion model learning and predicting procedure.
Thetraining dataset is formed with pairs of input (e.g.,activity) and output (e.g., activity boom) vectors.All composition models except Lexfunc also usethe functor vector (boom) in the training data.
Lex-func does not use this functor vector, but it wouldrather like to encode the learning target?s vectormeaning in a different way (see experimental anal-ysis in Section 4.3).
Then, this dataset is used forparameter estimation of models.
When a model(boom) is trained and given a new input seman-tic vector (e.g., export), it will output another vec-tor representing the concept for export boom.
Andthe concept export boom should be close to simi-lar concepts (e.g., export prosper) in meaning un-der some distance metric in semantic vector space.The same training and composition scheme is ap-plied for other types of functors (e.g., adjectivesand determiners).
All the above mentioned com-position models are evaluated within this scheme,but note that in the case of Add, Dil, Mult and Ful-ladd, a single set of parameters is obtained acrossall functors of a certain syntactic category.3 Pathwise Optimized Elastic-netAlgorithmThe elastic-net regression method (Zou andHastie, 2005) is proposed as a compromise be-tween lasso (Tibshirani, 1996) and ridge regres-sion (Hastie et al., 2009).
Suppose there are Nobservation pairs (xi, yi), here xi?
Rpis the ithtraining sample and yi?
R is the correspondingresponse variable in the typical regression setting.For simplicity, assume the xijare standardized:?Ni=1x2ij= 1, for j = 1, .
.
.
, p. The elastic-netsolves the following problem:min(?0,?)?Rp+1[1NN?i=1(yi?
?0?
xTi?
)2+ ?P?(?)](1)whereP?(?)
= ?((1?
?)12?
?
?2?2+???1)=p?j=1[12(1?
?
)?2j+ ?|?j|].P is the elastic-net penalty, and it is a compro-mise between the ridge regression penalty and thelasso penalty.
The merit of the elastic-net penaltydepends on two facts: the first is that elastic-net in-herits lasso?s characteristic to shrink many of theregression coefficients to zero, a property calledsparsity, which results in better interpretability ofmodel; the second is that elastic-net inherits ridgeregression?s property of a grouping effect, whichmeans important correlated features can be con-tained in the model simultaneously, and not beomitted as in lasso.For these linear-type regression problem (ridge,lasso and elastic-net), the determination of the ?value is very important for prediction accuracy.Efron et al.
(2004) developed an efficient algo-rithm to compute the entire regularization pathfor the lasso problem in 2004.
Later, Friedmanet al.
(Friedman et al., 2007; Friedman et al.,2010) proposed a coordinate descent optimization436method for the regularization parameter path, andthey also provided a solution for elastic-net.
Themain idea of pathwise coordinate descent is tosolve the penalized regression problem along anentire path of values for the regularization param-eters ?, using the current estimates as warm starts.The idea turns out to be quite efficient for elastic-net regression.
The procedure can be described asbelow: firstly establish an 100 ?
value sequencein log scale, and for each of the 100 regulariza-tion parameters, use the following coordinate-wiseupdating rule to cycle around the features for es-timating the corresponding regression coefficientsuntil convergence.??j?S(1N?Ni=1xij(yi?
y?
(j)i), ??
)1 + ?(1?
?)(2)where?
y?(j)i=??0+??
?=jxi???
?is the fitted value ex-cluding the contribution from xij, and henceyi?
y?
(j)ithe partial residual for fitting ?j.?
S(z, ?)
is the soft-thresholding operator withvalueS(z, ?)
= sign(z)(|z| ?
?)+=??
?z ?
?
if z > 0 and ?
< |z|z + ?
if z < 0 and ?
< |z|0 if ?
?
|z|Then solutions for a decreasing sequence of val-ues for ?
are computed in this way, starting at thesmallest value ?maxfor which the entire coeffi-cient vector??
= 0.
Then, 10-fold cross valida-tion on this regularization path is used to deter-mine the best model for prediction accuracy.
The?
parameter controls the model sparsity (the num-ber of coefficients equal to zero) and grouping ef-fect (shrinking highly correlated features simulta-neously).In what follows, we call the elastic-net regres-sion lexical function model EnetLex.
In Sec-tion 4, we will report the experiment results byEnetLex with ?
= 1.
It equals to pathwise co-ordinate descent optimized lasso, which favourssparser solutions and is often a better estimatorwhen the number of training samples is far greaterthan the number of feature dimensions, as in ourcase.
We also experimented with intermediate ?values (e.g., ?
= 0.5), that were, consistently, in-ferior or equal to the lasso setting.
?2 0 2 4200400600800log(Lambda)Mean?SquaredError50 50 50 50 50 50 50 50 50 48 29 21 12 7 4 2Model selection procedure for ?EnetLex?Figure 2: Example of model selection procedurefor elastic-net regression (?the?
model for deter-miner phrase experiment, SVD, 50 dimensions).Figure 2 is an example of the model selectionprocedure between different regularization param-eter ?
values for determiner ?the?
(experimentaldetails are described in section 4).
When ?
isfixed, EnetLex first generates a ?
sequence from?maxto ?min(?maxis set to the smallest valuewhich will shrink all the regression coefficientsto zero, ?min= 0.0001) in log scale (rightmostpoint in the plot).
The red points correspondingto each ?
value in the plot represent mean cross-validated errors and their standard errors.
To esti-mate a model corresponding to some ?
value ex-cept ?max, we use the solution from previous ?value as the initial coefficients (the warm startsmentioned before) for iteration with coordinatedescent.
This will often generate a stable solu-tion path for the whole ?
sequence very fast.
Andwe can choose the model with minimum cross-validation error on this path and use it for moreaccurate prediction.
In Figure 2, the labels on thetop are numbers of corresponding selected vari-ables (features), the right vertical dotted line is thelargest value of lambda such that error is within 1standard error of the minimum, and the left verti-cal dotted line corresponds to the ?
value whichgives minimum cross-validated error.
In this case,the ?
value of minimum cross-validated error is0.106, and its log is -2.244316.
In all of our ex-periments, we will select models corresponding tominimum training-data cross-validated error.4 Experiments4.1 DatasetsWe evaluate on the three data sets described below,that were also used by Dinu et al.
(2013), our most437direct point of comparison.Intransitive sentences The first dataset, intro-duced by Mitchell and Lapata (2010), focuseson the composition of intransitive verbs and theirnoun subjects.
It contains a total of 120 sentencepairs together with human similarity judgments ona 7-point scale.
For example, value slumps/valuedeclines is scored 7, skin glows/skin burns isscored 1.
On average, each pair is rated by 30participants.
Rather than evaluating against meanscores, we use each rating as a separate data point,as done by Mitchell and Lapata.
We report Spear-man correlations between human-assigned scoresand cosines of model-generated vector pairs.Adjective-noun phrases Turney (2012) intro-duced a dataset including both noun-noun com-pounds and adjective-noun phrases (ANs).
We fo-cus on the latter, and we frame the task as in Dinuet al.
(2013).
The dataset contains 620 ANs, eachpaired with a single-noun paraphrase.
Examplesinclude: upper side/upside, false belief/fallacy andelectric refrigerator/fridge.
We evaluate a modelby computing the cosine of all 20K nouns in oursemantic space with the target AN, and looking atthe rank of the correct paraphrase in this list.
Thelower the rank, the better the model.
We reportmedian rank across the test items.Determiner phrases The third dataset, intro-duced in Bernardi et al.
(2013), focuses on aclass of determiner words.
It is a multiple-choice test where target nouns (e.g., omniscience)must be matched with the most closely relateddeterminer(-noun) phrases (DPs) (e.g., all knowl-edge).
There are 173 target nouns in total, eachpaired with one correct DP response, as well as5 foils, namely the determiner (all) and noun(knowledge) from the correct response and threemore DPs, two of which contain the same noun asthe correct phrase (much knowledge, some knowl-edge), the third the same determiner (all prelimi-naries).
Other examples of targets/related-phrasesare quatrain/four lines and apathy/no emotion.The models compute cosines between target nounand responses and are scored based on their accu-racy at ranking the correct phrase first.4.2 SetupWe use a concatenation of ukWaC, Wikipedia(2009 dump) and BNC as source corpus, total-Model Reduction Dim CorrelationAdd NMF 150 0.1349Dil NMF 300 0.1288Mult NMF 250 0.2246Fulladd SVD 300 0.0461Lexfunc SVD 250 0.2673Fulllex NMF 300 0.2682EnetLex SVD 250 0.3239Table 2: Best performance comparison for intran-sitive verb sentence composition.ing 2.8 billion tokens.1Word co-occurrences arecollected within sentence boundaries (with a max-imum of a 50-words window around the targetword).
Following Dinu et al.
(2013), we use thetop 10K most frequent content lemmas as contextfeatures, Pointwise Mutual Information as weight-ing method and we reduce the dimensionality ofthe data by both Non-negative Matrix Factoriza-tion (NMF, Lee and Seung (2000)) and SingularValue Decomposition (SVD).
For both data di-mensionality reduction techniques, we experimentwith different numbers of dimension varying from50 to 300 with a step of 50.
Since the Mult modelworks very poorly when the input vectors containnegative values, as is the case with SVD, for thismodel we report result distributions across the 6NMF variations only.We use the DIStributional SEmantics Compo-sition Toolkit (DISSECT)2which provides imple-mentations for all models we use for comparison.Following Dinu and colleagues, we used ordinaryleast-squares to estimate Fulladd and ridge forLexfunc.
The EnetLex model is implemented in Rwith support from the glmnet package,3which im-plements pathwise coordinate descent elastic-netregression.4.3 Experimental Results and AnalysisThe experimental results are shown in Ta-bles 2, 3, 4 and Figures 3, 4, 5.
The best per-formances from each model on the three compo-sition tasks are shown in the tables.
The over-all result distributions across reduction techniquesand dimensionalities are displayed in the figure1http://wacky.sslmit.unibo.it;http://www.natcorp.ox.ac.uk2http://clic.cimec.unitn.it/composes/toolkit/3http://cran.r-project.org/web/packages/glmnet/438Model Reduction Dim RankAdd NMF 300 113Dil NMF 300 354.5Mult NMF 300 146.5Fulladd SVD 300 123Lexfunc SVD 150 117.5Fulllex SVD 50 394EnetLex SVD 300 108.5Table 3: Best performance comparison for adjec-tive noun composition (lower ranks mean betterperformance).Model Reduction Dim RankAdd NMF 100 0.3237Dil NMF 100 0.3584Mult NMF 300 0.2023Fulladd NMF 200 0.3642Lexfunc SVD 200 0.3699Fulllex SVD 100 0.3699EnetLex SVD 250 0.4046Table 4: Best performance comparison for deter-miner phrase composition.boxplots (NMF and SVD results are shown sep-arately).
From Tables 2, 3, 4, we can see thatEnetLex consistently achieves the best composi-tion performance overall, also outperforming thestandard lexical function model.
In the boxplotdisplay, we can see that SVD is in general morestable across dimensionalities, yielding smallervariance in the results than NMF.
We also observe,more specifically, larger variance in EnetLex per-formance on NMF than in Lexfunc, especially fordeterminer phrase composition.
The large vari-ance with EnetLex comes from the NMF low-dimensionality results, especially the 50 dimen-sions condition.
The main reason for this liesin the fast-computing tricks of the coordinate de-scent algorithm when cycling around many fea-tures with zero values (as resulting from NMF),which cause fast convergence at the beginning ofthe regularization path, generating an inaccuratemodel.
A subordinate reason might lie in the un-standardized larger values of the NMF features(causing large gaps between adjacent parametervalues in the regularization path).
Although datastandardization or other feature scaling techniquesare often adopted in statistical analysis, they areseldom used in semantic composition tasks due toAdd?nmf Dil?nmfMult?nmfFulladd?nmfLexfunc?nmfFulllex?nmfEnetLex?nmfAdd?svd Dil?svdMult?svdFulladd?svdLexfunc?svdFulllex?svdEnetLex?svd0.000.050.100.150.200.250.30Intransitive sentencesFigure 3: Intransitive verb sentence compositionresults.Add?nmf Dil?nmfMult?nmfFulladd?nmfLexfunc?nmfFulllex?nmfEnetLex?nmfAdd?svd Dil?svdMult?svdFulladd?svdLexfunc?svdFulllex?svdEnetLex?svd8006004002000 Adjective?noun phrasesFigure 4: Adjective noun phrase composition re-sults.the fact that they might negatively affect the se-mantic vector space.
A reasonable way out of thisproblem would be to save the mean and standarddeviation parameters used for data standardizationand use them to project the composed phrase vec-tor outputs back to the original vector space.On the other hand, EnetLex obtained a stablegood performance in SVD space, with the best re-sults achieved with dimensions between 200 and300.
A set of Tukey?s Honestly Significant Testsshow that EnetLex significantly outperforms theother models across SVD settings for determinerphrases and intransitive sentences.
The differenceis not significant for most comparisons in the ad-jective phrases task.For the simpler models for which it was com-putationally feasible, we repeated the experimentswithout dimensionality reduction.
The results ob-tained with (unweighted) Add and Mult using full-space representations are reported in Table 5.
Dueto computational limitations, we tuned full-spaceweights for Add model only, obtaining similar re-sults to those reported in the table.
The full-space439Add?nmf Dil?nmfMult?nmfFulladd?nmfLexfunc?nmfFulllex?nmfEnetLex?nmfAdd?svd Dil?svdMult?svdFulladd?svdLexfunc?svdFulllex?svdEnetLex?svd0.150.200.250.300.350.40 Determiner phrasesFigure 5: Determiner phrase composition results.model verb adjective determinerAdd 0.0259 957 0.2832Mult 0.1796 298.5 0.0405Table 5: Performance of Add and Mult modelswithout dimensionality reduction.results confirm that dimensionality reduction isnot only a computational necessity when work-ing with more complex models, but it is actuallyimproving the quality of the underlying semanticspace.Another benefit that elastic-net has brought tous is the sparsity in coefficient matrices.
Sparsityhere means that many entries in the coefficient ma-trix are shrunk to 0.
For the above three exper-iments, the mean adjective, verb and determinermodels?
sparsity ratios are 0.66, 0.55 and 0.18 re-spectively.
Sparsity can greatly reduce the spaceneeded to store the lexical function model, espe-cially when we want to use higher orders of repre-sentation.
Moreover, sparsity in the model is help-ful to interpret the concept a specific functor wordis conveying.
For example, we show how to an-alyze the coefficient matrices for functor contentwords (verbs and adjectives).
The verb burst andadjective poisonous, when estimated in the spaceprojected to 100 dimensions with NMF, have per-centages of sparsity 47% and 39% respectively,which means 47% of the entries in the burst ma-trix and 39% of the entries in the poisonous ma-trix are zeros.4Most of the (hopefully) irrelevantdimensions were discarded during model training.For visualization, we list the 6 most significant4We analyze NMF rather than the better-performing SVDfeatures because the presence of negative values in the lattermakes their interpretation very difficult.
And NMF achievescomparably good performance for interpretation when di-mension exceeds 100.columns and rows from verb burst and adjectivepoisonous in Table 6.
Each reduced NMF di-mension is represented by the 3 largest original-context entries in the corresponding row of theNMF basis matrix.
The top columns and rowsare selected by ordering sums of row entries andsums of column entries (the 10 most common fea-tures across trained matrices are omitted).
In thematrix-vector multiplication scenario, a larger col-umn contributes more to all the features of thecomposed output phrase vector, while one largerow corresponds to a large composition output fea-ture.
From these tables, we can see that the se-lected top columns and rows are mostly semanti-cally relevant to the corresponding functor words(burst and poisonous, in the displayed examples).A very interesting aspect of these experimentsis the role of the intercept in our regression model.The path-wise optimization algorithm starts witha lambda value (?max), which sets all the coef-ficients exactly to 0, and at that time the inter-cept is just the expected mean value of the train-ing phrase vectors, which in turn is of course quitesimilar to the co-occurrence vector of the cor-responding functor word (by averaging the poi-sonous N context distributions, we obtain a vec-tor that approximates the poisonous distribution).And, although the intercept also changes with dif-ferent lambda values, it still highly correlates withthe co-occurrence vectors of the functor wordsin vector space.
For adjectives and verbs, wecompared the initial model?s (?max) intercept andthe minimum cross-validation error model inter-cept with corpus-extracted vectors for the corre-sponding words.
That is, we used the word co-occurrence vector for a verb or an adjective ex-tracted from the corpus and projected onto thereduced feature space (e.g., NMF, 100 dimen-sions), then computed cosine similarity betweenthis word meaning representation and its corre-sponding EnetLex matrix initial and minimum-error intercepts, respectively.
Most of the simi-larities are still quite high after estimation: Themean cosine values for adjectives are 0.82 for theinitial intercept and 0.72 for the minimum-errorone.
For verbs, the corresponding values are 0.75and 0.69, respectively.
Apparently, the sparsityconstraint helps the intercept retaining informationfrom training phrases.Qualitatively, often the intercept encodes therepresentation of the original word meaning in440burst significant columns significant rowspoliceman, mob, guard hurricane, earthquake, disasterIraqi, Lebanese, Kurdish conquer, Byzantine, conquestjealousy, anger, guilt policeman, mob, guardhurricane, earthquake, disaster terminus, traffic, interchangedefender, keeper, striker convict, sentence, imprisonmentvolcanic, sediment, geological boost, unveil, campaignerpoisonous significant columns significant rowsbathroom, wc, shower ventilation, fluid, bacteriumignite, emit, reactor ignite, emit, reactorreptile, mammal, predator infectious, infect, infectedventilation, fluid, bacterium slay, pharaoh, tribeflowering, shrub, perennial park, lorry, pavementsauce, onion, garlic knife, pierce, brassTable 6: Interpretability for verbs and adjectives (exemplified by burst and poisonous).vector space.
For example, if we check the inter-cept for poisonous, the cosine between the origi-nal vector space representation (from corpus) andthe minimum-error solution intercept (from train-ing phrases) is at 0.7.
The NMF dimensions cor-responding with the largest intercept entries arerather intuitive for poisonous: ?ventilation, fluid,bacterium?, ?racist, racism, outrage?, ?reptile,mammal, predator?, ?flowering, shrub, perennial?,?sceptical, accusation, credibility?, ?infectious, in-fect, infected?.The mathematical reason for the above facts liesin the updating rule of the elastic-net?s intercept:?0=?y ?p?j=1?
?j?xj(3)Sparsity in the regression coefficients (?
?j) encour-ages intercept ?0to stay as close to the meanvalue of response?y as possible.
So the elastic-net lexical function composition model is de factoalso capturing the inherent meaning of the func-tor word, learning it from the training word-phrasepairs.
In future research, we would like to test ifthese lexical meaning representations are as goodor even better than standard co-occurrence vectorsfor single-word similarity tasks.5 ConclusionIn this paper, we have shown that the lexical func-tion composition model can be improved by ad-vanced regression techniques.
We use pathwisecoordinate descent optimized elastic-net, testingit on composing intransitive sentences, adjective-noun phrases and determiner phrases in compari-son with other composition models, including lex-ical function estimated with ridge regression.
Theelastic-net method leads to performance gains onall three tasks.
Through sparsity constraints on themodel, elastic-net also introduces interpretabilityin the lexical function composition model.
Theregression coefficient matrices can often be eas-ily interpreted by looking at large row and columnsums, as many matrix entries are shrunk to zero.The intercept of elastic-net regression also playsan interesting role in the model.
With the sparsityconstraints, the intercept of the model tends to re-tain the inherent meaning of the word by averagingtraining phrase vectors.Our approach naturally generalizes to similarcomposition tasks, in particular those involvinghigher-order tensors (Grefenstette et al., 2013),where sparseness might be crucial in producingcompact representations of very large objects.
Ourresults also suggest that the performance of thelexical function composition model might be fur-ther improved with even more advanced methods,such as nonlinear regression.
In the future, wewould also like to explore interpretability more indepth, by looking at grouping and interaction ef-fects between features.AcknowledgmentsWe acknowledge ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES), and wethank the reviewers for helpful feedback.441ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Raffaella Bernardi, Georgiana Dinu, Marco Marelli,and Marco Baroni.
2013.
A relatedness benchmarkto test the role of determiners in compositional dis-tributional semantics.
In Proceedings of ACL (ShortPapers), pages 53?57, Sofia, Bulgaria.Stephen Clark.
2013.
Vector space models of lexicalmeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, 2nd edition.Blackwell, Malden, MA.
In press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.Georgiana Dinu, Nghia The Pham, and Marco Baroni.2013.
General estimation and evaluation of com-positional distributional semantic models.
In Pro-ceedings of ACL Workshop on Continuous VectorSpace Models and their Compositionality, pages 50?58, Sofia, Bulgaria.Bradley Efron, Trevor Hastie, Iain Johnstone, andRobert Tibshirani.
2004.
Least angle regression.The Annals of statistics, 32(2):407?499.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence withLatent Semantic Analysis.
Discourse Processes,25:285?307.Jerome Friedman, Trevor Hastie, Holger H?ofling, andRobert Tibshirani.
2007.
Pathwise coordinateoptimization.
The Annals of Applied Statistics,1(2):302?332.Jerome Friedman, Trevor Hastie, and Rob Tibshirani.2010.
Regularization paths for generalized linearmodels via coordinate descent.
Journal of statisti-cal software, 33(1):1.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for composi-tional distributional semantics.
In Proceedings ofIWCS, pages 131?142, Potsdam, Germany.Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.2007.
Topics in semantic representation.
Psycho-logical Review, 114:211?244.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2009.
The Elements of Statistical Learning,2nd ed.
Springer, New York.Darrell Laham.
1997.
Latent Semantic Analysisapproaches to categorization.
In Proceedings ofCogSci, page 979.Daniel Lee and Sebastian Seung.
2000.
Algorithms forNon-negative Matrix Factorization.
In Proceedingsof NIPS, pages 556?562.Scott McDonald and Chris Brew.
2004.
A distribu-tional model of semantic context effects in lexicalprocessing.
In Proceedings of ACL, pages 17?24,Barcelona, Spain.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Rob Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 58(1):267?288.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.J.
Artif.
Intell.
Res.
(JAIR), 44:533?585.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.Hui Zou and Trevor Hastie.
2005.
Regularizationand variable selection via the elastic net.
Journalof the Royal Statistical Society: Series B (StatisticalMethodology), 67(2):301?320.442
