Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 684?691, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Translation Model for Sentence RetrievalVanessa Murdock and W. Bruce CroftCenter for Intelligent Information RetrievalComputer Science DepartmentUniversity of MassachusettsAmherst, MA 01003{vanessa,croft}@cs.umass.eduAbstractIn this work we propose a transla-tion model for monolingual sentenceretrieval.
We propose four methodsfor constructing a parallel corpus.
Ofthe four methods proposed, a lexi-con learned from a bilingual Arabic-English corpus aligned at the sentencelevel performs best, significantly im-proving results over the query likeli-hood baseline.
Further, we demon-strate that smoothing from the localcontext of the sentence improves re-trieval over the query likelihood base-line.1 IntroductionSentence retrieval is the task of retrieving a rel-evant sentence in response to a user?s query.Tasks such as question answering, novelty de-tection and summarization often incorporate asentence retrieval module.
In previous work weexamined sentence retrieval for question answer-ing (Murdock and Croft, 2004).
This involvesthe comparison of two well-formed sentences,one a question, one a statement.
In this work wecompare well-formed sentences to queries, whichcan be typical keyword queries of 1 to 3 terms,or a set of sentences or sentence fragments.
TheTREC Novelty Track provides this type of datain the form of topic titles and descriptions, andsentence-level relevance judgments for a smallsubset of the collection.We present a translation model specificallyfor monolingual data, and show that it signif-icantly improves sentence retrieval over query-likelihood.
Translation models train on a paral-lel corpus and in previous work we used a cor-pus of question/answer pairs.
No such corpusis available for the novelty data, so in this pa-per we present four ways to construct a parallelcorpus, to estimate a translation model.Many systems treat sentence retrieval as atype of document or passage retrieval.
In ourdata a sentence is an average of 18 words, mostof which occur once.
A document is an averageof 700 words, many of which are multiples ofthe same term.
It is much less likely for a wordand its synonym terms to appear in the samesentence than in the same document.Passages may be any length, either fixed orvariable, but are somewhat arbitrarily desig-nated.
Many systems that have a passage re-trieval module, on closer inspection have de-fined the passage to be a sentence.
What isneeded is a sentence retrieval mechanism thatretains the benefits of passage retrieval, wherea passage is longer than a sentence.
We pro-pose that smoothing from the local context ofthe sentence improves retrieval over the querylikelihood baseline, and the larger the context,the greater the improvement.We describe our translation model in sec-tion 2, along with our smoothing approach.
Insection 3 we discuss previous work in sentenceretrieval for the Novelty task, and translationmodels for information retrieval tasks.
Sec-tion 4 presents four ways to estimate a trans-lation model, in the absence of a parallel cor-pus, and presents our experimental results.
We684discuss the results in section 5, and present ourconclusions and future work in section 6.2 MethodologyOur data was provided by NIST, as part ofthe TREC Novelty Track1.
The documents forthe TREC Novelty Track in 2002 were takenfrom the TREC volumes 4 and 5, and consistof news articles from the Financial Times, theForeign Broadcast Information Service, and theLos Angeles Times from non-overlapping years.In 2003 and 2004, the documents were takenfrom the Aquaint Corpus, which is distributedby the Linguistic Data Consortium2 and con-sists of newswire text in English from the Xin-hua News Service, the New York Times, and theAssociated Press from overlapping years.We retrieved the top 1000 documents foreach topic from the TREC and Aquaint col-lections, and sentence segmented the docu-ments using MXTerminator (Reynar and Rat-naparkhi, 1997), which is a freely available sen-tence boundary detector.
Each topic was in-dexed separately and had an average of 30,000sentences.
It was impractical to do sentence-level relevance assessments for the complete setof 150,000 documents, so we used the relevanceassessments provided as part of the Noveltytask, recognizing that the results are a lowerbound on performance, because the relevanceassessments do not cover the collection.
Therelevance assessments cover 25 known relevantdocuments for each topic.We evaluated precision at N documents be-cause many systems using sentence retrieval em-phasize the results at the top of the ranked list,and are less concerned with the overall qualityof the list.2.1 Translation ModelsWe incorporated a machine translation modelin two steps: estimation and ranking.
In theestimation step, the probability that a term inthe sentence ?translates?
to a term in the queryis estimated using the implementation of IBM1http://trec.nist.gov2http://www.ldc.upenn.eduModel 1 (Brown et al, 1990) in GIZA++ (Al-Onaizan et al, 1999) out-of-the-box withoutalteration.
In the ranking step we incorpo-rate the translation probabilities into the query-likelihood framework.In Berger and Lafferty (1999), the IBM Model1 is incorporated thus:P (qi|S) =m?j=1P (qi|sj)P (sj |S) (1)where P (qi|sj) is the probability that term sj inthe sentence translates to term qi in the query.If the translation probabilities are modified suchthat P (qi|sj) = 1 if qi = sj and 0 otherwise,this is Berger and Lafferty?s ?Model 0?, and itis exactly the query-likelihood model (describedin section 2.2).A major difference between machine transla-tion and sentence retrieval is that machine trans-lation assumes there is little, if any, overlap inthe vocabularies of the two languages.
In sen-tence retrieval we depend heavily on the overlapbetween the two vocabularies.
With the Bergerand Lafferty formulation in equation 1, the prob-ability of a word translating to itself is estimatedas a fraction of the probability of the word trans-lating to all other words.
Because the probabil-ities must sum to one, if there are any othertranslations for a given word, its self-translationprobability will be less than 1.0.
To accommo-date this monolingual condition, we make thefollowing improvement.Let ti = 1 if there exists a term in the sentencesj such that qi = sj , and 0 otherwise:?1?j?np(qi|sj)p(sj |S) =?tip(qi|S) + (1 ?
ti)?1?j?n,sj 6=qip(qi|sj)p(sj |S)(2)The translation probabilities still sum to one.We determined empirically that this adjustmentimproved the results over IBM model 1, and overBerger and Lafferty model 0.6852.2 Document SmoothingQuery likelihood is a generative model that as-sumes that the sentence is a sample of a multino-mial distribution of terms.
Sentences are rankedaccording to the probability they generate thequery.
We estimate this probability by interpo-lating the term distribution in the sentence withthe term distribution in the collection:P (Q|S) = P (S)|Q|?i=1(?P (qi|S) + (1 ?
?
)P (qi|C))(3)where Q is the query, S is the sentence, P (S) isthe (uniform) prior probability of the sentence,P (qi|S) is the probability that term qi in thequery appears in the sentence, and P (qi|C) isthe probability that qi appears in the collection.In the experiments with document smoothing,we estimate the probability of a sentence gener-ating the query:P (Q|S) =P (S)|Q|?i=1(?P (qi|S) + ?P (qi|DS) + ?P (qi|C))(4)where ?+?
+?
= 1.0 and P (qi|DS) is the prob-ability that the term qi in the query appearsin the document the sentence came from.
Inour case, since the sentences for each topic areindexed separately, the collection statistics arein reference to the documents in the individualtopic index.3 Previous WorkThe TREC Novelty Track ran for three years,from 2002 to 2004.
Overviews of the trackcan be found in (Harman, 2002), (Soboroff andHarman, 2003) and (Soboroff, 2004).
A num-ber of systems use traditional information re-trieval techniques for sentence retrieval, usingvarious techniques to compensate for the sparseterm distributions in sentences.
The Univer-sity of Massachusetts (Larkey et al, 2002) andCarnegie Mellon University (Collins-Thompsonet al, 2002) both ranked sentences by the co-sine similarity of the sentence vector to thequery vector of tf.idf-weighted terms.
Amster-dam University (Monz et al, 2002) used tfc.nfxterm weighting which is a variant of tf.idf termweighting that normalizes the lengths of the doc-ument vectors.
Meiji University (Ohgaya et al,2003) expanded the query with concept groups,and then ranked the sentences by the cosine sim-ilarity between the expanded topic vector andthe sentence vector.Berger and Lafferty (1999) proposed the use oftranslation models for (mono-lingual) documentretrieval.
They used IBM Model 1 (Brown etal., 1990), to rank documents according to theirtranslation probability, given the query.
Theymake no adjustment for the fact that the queryand the document are in the same language, andinstead rely on the translation model to learnthe appropriate weights for word pairs.
Themodels are trained on parallel data artificiallyconstructed from the mutual information distri-bution of terms in the document.
The resultspresented either were not tested for statisticalsignificance, or they were not statistically signif-icant, because no significance results were given.Berger et al (2000) used IBM Model 1 to rankanswers to questions in call-center data.
In theirdata, there were no answers that were not inresponse to at least one of the questions, and allquestions had at least one answer.
Furthermore,there are multiples of the same question.
Thetask is to match questions and answers, giventhat every question has at least one match in thedata.
The translation models performed betterfor this task than the tf.idf baseline.4 Experiments and ResultsIn this section we describe four methods for es-timating a translation model in the absence ofa parallel corpus.
We describe experimental re-sults for each of the translation models, as wellas for document smoothing.4.1 Mutual Information and TRECAs in Berger and Lafferty (1999), a set ofdocuments was selected at random from theTREC collection, and for each document we686Query MT MTLikelihood (MI) (TREC)Prec@5 0.1176 0.1149 0.1392*Prec@10 0.1115 0.1047 0.1095Prec@15 0.1023 0.0928* 0.0977Prec@20 0.0973 0.0882* 0.0936Prec@30 0.0890 0.0865 0.0874Prec@100 0.0733 0.0680* 0.0705R-Prec 0.0672 0.0642* 0.0671Ave Prec 0.0257 0.0258 0.0264Table 1: Comparing translation model-based re-trieval with description queries.
?TREC?
and?MI?
are two ways to estimate a translationmodel.
Results with an asterisk are significantat the .05 level with a two-tailed t-test.constructed a distribution according to eachterm?s mutual information with the document,and randomly generated five queries of 8 wordsaccording to this distribution.
We were retriev-ing sentences rather than documents, so eachsentence in the document was ranked accordingto its probability of having generated the query,and then the query was aligned with the top 5sentences.
We call this approach ?MI?.The second approach uses the TREC topictitles and descriptions aligned with the top 5retrieved sentences from documents known tobe relevant to those topics, excluding topics thatwere included in the Novelty data.
We call thisapproach ?TREC?.Table 1 shows the results of incorporatingtranslations for topic descriptions.
Results inthe tables with an asterisk are significant at the.05 level using a two-tailed t-test.
The resultsfor sentence retrieval are lower than those typ-ically obtained for document retrieval.
Manualinspection of the results indicates that the ac-tual precision is much higher, and resembles theresults for document retrieval.
The lower resultsare an artifact of the way the relevance assess-ments were obtained.
The sentence-level judge-ments from the TREC Novelty Track are onlyfor 25 documents per topic.The Novelty data from 2003-2004 consists ofevent and opinion queries.
We observed thatEvent OpinionQuery MT Query MTLklhd (TREC) Lklhd (TREC)Prec@5 0.1149 0.1307 0.1234 0.1574Prec@10 0.1089 0.1079 0.1170 0.1128Prec@15 0.1036 0.1030 0.0993 0.0865Prec@20 0.0985 0.0980 0.0947 0.0840Prec@30 0.0901 0.0894 0.0865 0.0830Prec@100 0.0729 0.0719 0.0743 0.0674R-Prec 0.0658 0.0694 0.0701 0.0622Ave Prec 0.0275 0.0289 0.0219 0.0211Table 2: Comparing translation-based retrievalfor description queries, using the relevance judg-ments provided by NIST.
The translation modelwas trained from TREC topics.a number of the topic descriptions for eventtopics had a high degree of vocabulary overlapwith the sentences in our data.
This was nottrue of the opinion queries.
The results of us-ing a translation-based retrieval on descriptionqueries are given in table 2, broken down bythe sentiment of the query.
The Novelty queriesfrom 2002 were included in the ?event?
set.Not all of the sentences judged relevant toopinion topics express opinions.
To assessopinion-relevance we evaluated the top 10 sen-tences, and marked sentences that expressedopinions.
In our data approximately 10% ofsentences in the top 10 express opinions.
Ta-ble 3 shows the result of using a translationmodel trained on TREC data for descriptionqueries, broken down by sentiment, with thebaselines evaluated for this particular set of rel-evance judgments.
For opinion questions, thecolumn labeled ?topical?
indicates topical rele-vance.
The column labeled ?opinion?
indicatestopical relevance that also expresses an opinion.If we consider a sentence relevant to an opin-ion question only if it expresses an opinion, wesee improvement in the results at the top of theranked list for those queries, using a transla-tion model trained on TREC data.
Of the 150topics, only 50 were opinion topics, so althoughthe magnitude of the improvement in opinionqueries is large the results are not statistically687Topical Rel Express OpinQuery MT Query MTLklhd (TREC) Lklhd (TREC)Prec@5 .7289 .7111 .3300 .3900Prec@10 .7089 .6867 .3125 .3775Prec@15 .5363 .4919 .2350 .2717Prec@20 .4300 .4033 .1875 .2188Prec@30 .3170 .2970 .1408 .1617Prec@100 .1236 .1131* .0587 .0580R-Prec .4834 .4653 .2947 .3597*Ave Prec .4996 .4696 .2563 .3177Table 3: Comparison of translation retrieval onopinion queries, using truth data we created toevaluate opinion questions.
Translation modelswere trained with TREC data.
Results with anasterisk are significant at the .05 level using atwo-tailed t-test.significant with respect to the baseline.4.2 LexiconsExternal lexicons are often useful for transla-tion and query expansion.
The most obviousapproach was to incorporate a thesaurus intothe training process in GIZA as a dictionary,which affects the statistics in the first iterationof EM.
This is intended to improve the qual-ity of the alignments over subsequent iterations.We incorporated the thesauri into the trainingprocess of the data generated from the artificialmutual information distribution.
The dictionar-ies had almost no effect on the results.4.2.1 WordNetWe created a parallel corpus of synonym-termpairs from WordNet, and added this data to theartificial mutual information data to train thetranslation model.
The results of using this ap-proach to retrieve sentences using title queriesare in figure 1, labeled ?MI WN?.
Using Word-Net alne, without the mutual information data,is labeled ?WN Only?.
The results are statisti-cally significant using a Wilcoxon sign test at the.05 level for precision at .10, .20 and .60.
Querylikelihood retrieval is the baseline.
The resultsfor description queries are not shown, and werenot significantly different from the baseline.00.050.10.150.20.250  2  4  6  8  10"baseline""MI_WN""WN_Only"Figure 1: Comparing interpolated recall-precision for title queries using WordNet.
Theresults are statistically significant using aWilcoxon sign test at the .05 level, for precisionat .10, .20 and .60.4.2.2 Arabic-English corpusXu et al (2002) derive an Arabic thesaurusfrom a parallel corpus.
We derived an En-glish thesaurus using the same approach, from apair of English-Arabic/Arabic-English lexicons,learned from a parallel corpus.
We assumed thatif two English terms translate to the same Ara-bic term, the English terms are synonyms whoseprobability is given byP (e2|e1) =?a?AP (e2|a)P (a|e1) (5)Figure 2 shows the interpolated recall-precision of these results, for description queries.The English terms were not stemmed, and sothe baseline query-likelihood results are also notstemmed.
The results are statistically signifi-cant using a Wilcoxon sign test at the .05 level,for all retrieval levels.
Not shown is the averageprecision, which is also significantly better forthe Arabic-English lexicon than for the query-likelihood.
The results for title queries are notshown, but are similar to those for descriptions.68800.050.10.150.20.250.30  2  4  6  8  10"baseline""Arabic_English"Figure 2: Comparing interpolated recall-precision for description queries using a pair ofArabic-English, English-Arabic lexicons.
Theresults are statistically significant using aWilcoxon sign test at the .05 level, for precisionall recall levels.4.3 Document SmoothingSmucker and Allan (2005) demonstratedthat under certain conditions, Jelinek-Mercersmoothing is equivalent to Dirichlet smoothing,and that the advantage of Dirichlet smoothingis derived from the fact that it smoothes longdocuments less than shorter documents.
In ourdata there is much less variance in the lengthof a sentence than in the length of a document,thus we do not expect to see as great a benefitin performance from Dirichlet smoothing as hasbeen reported in Zhai and Lafferty (2001).
Infact we tried Absolute Discounting, Dirichlet,Jelinek-Mercer and Laplace smoothing andfound them to produce equivalent results.The vast majority of sentences in our dataare not stand-alone units, and the topic of thesentence is also the topic of surrounding sen-tences.
We took a context of the surround-ing 5 sentences, and the surrounding 11 sen-tences (about 1/3 of the whole document).
Thesentences were smoothed from the surroundingcontext, backing-off to the whole document, us-Query 5 Sents 11 SentsLklhdPrec@5 0.1203 0.1527* 0.1541*Prec@10 0.1122 0.1446* 0.1419*Prec@15 0.1018 0.1329* 0.1405*Prec@20 0.0973 0.1311* 0.1345*Prec@30 0.0890 0.1191* 0.1286*Prec@100 0.0732 0.0935* 0.1006*R-Prec 0.0672 0.0881* 0.0933*Ave Prec 0.0257 0.0410* 0.0485*Table 4: Comparison of smoothing context ondescription queries, retrieving sentences fromthe top 1000 documents.
Results with an aster-isk are significant at the .05 level using a two-tailed t-test.ing Jelinek-Mercer smoothing.
Table 4 shows acomparison of the amount of context.
Smooth-ing from the local context is clearly better thanthe baseline result.We investigated the effect of smoothing fromthe entire document.
Table 5 shows the results.Both topic titles and descriptions get signifi-cantly better results with document smoothing.4.4 Novelty Relevance TaskIn the TREC Novelty Track, participants aregiven a set of 25 documents most of which arerelevant for each topic.
If we believe that a doc-ument is relevant because it has relevant sen-tences in it, then a ?good?
sentence would comefrom a ?good?
document.
This would suggestthat smoothing from the document the sentencecame from would improve retrieval.
We foundthat for title queries document smoothing im-proved precision in the top 5 documents by12.5%, which is statistically significant using atwo-tailed t-test at the .05 level.
Precision inthe top 10 - 100 documents also improved re-sults by an average of 5%, but the result is notstatistically significant.
For description queries,smoothing from the document had no effect.For title queries, translation models improvethe average precision, and R-Precision.
For bothtitle and description queries, the number of rel-evant documents that are retrieved is also im-proved with translation models.689Title DescriptionQuery Doc Query DocLklhd Smth Lklhd SmthPrec@5 .0765 .2268* .1203 .2362*Prec@10 .0805 .2262* .1122 .2128*Prec@15 .0814 .2192* .1018 .2000*Prec@20 .0765 .2124* .0973 .1893*Prec@30 .0765 .2007* .0890 .1743*Prec@100 .0675 .1638* .0732 .1335*R-Prec .0646 .1379* .0672 .1226*Ave Prec .0243 .0796* .0257 .0749*Table 5: Comparison of document smoothing toquery likelihood retrieving sentences from thetop 1000 documents.
Results with an asteriskare significant at the .05 level using a two-tailedt-test.5 DiscussionThe results for sentence retrieval are low, incomparison to results we would expect for doc-ument retrieval.
We might think that althoughwe show improvements, nothing is working well.In reality, the relevance assessments provided byNIST as part of the Novelty Track only cover25 documents per topic.
Evaluating the top 10sentences by hand shows that the systems give aperformance comparable to document retrievalsystems, and the low numbers are the result ofa lack of coverage in the assessments.
Unfor-tunately, there is no collection of documents ofsignificant size, where the relevance assessmentsat the sentence level cover the collection.
Con-structing such a corpus would be a major un-dertaking, outside of the scope of this paper.The best performing method of constructinga parallel corpus used a bilingual lexicon derivedfrom a sentence-aligned Arabic-English parallelcorpus.
This suggests that data in which sen-tences are actually translations of one another,as opposed to sentences aligned with key termsfrom the document, yield a higher quality lexi-con.
The model trained on the parallel corpus ofTREC topics and relevant sentences performedbetter than the MI corpus, but not as well as theArabic-English corpus.
The TREC corpus con-sisted of approximately 15,000 sentence pairs,whereas the Arabic-English corpus was trainedon more than a million sentence pairs.
This mayaccount in part for the higher quality results.
Inaddition, the TREC corpus was created by re-trieving the top 5 sentences from each relevantdocument.
Even when the document is knownto be relevant, the retrieval process is noisy.
Fur-thermore, although there were 15,000 sentencepairs, there were only 450 unique queries, limit-ing the size of the source vocabulary.Opinion topics have much less vocabularyoverlap with relevant sentences than do eventtopics.
Translation models would be expectedto perform better when retrieving sentences thatcontain synonym or related terms.
For sentencesthat have exact matches in the query, query like-lihood will perform better.We find that smoothing from the local con-text of the sentence performs significantly bet-ter than the baseline retrieval.
The sentences areall about the same length, so there is no perfor-mance advantage to using Dirichlet smoothing,whose smoothing parameter is a function of thedocument length.
The smoothing parametersgave very little weight to the collection.
As sen-tences have few terms, relative to documents,matching a term in the query is a good indica-tion of relevance.6 Conclusions and Future WorkWe have shown that translation models improveretrieval for title and opinion queries, and thata translation model derived from a high-qualitybilingual lexicon significantly improves retrievalfor title and description queries.
Smoothingfrom the local context of a sentence dramati-cally improves retrieval, with smoothing fromthe document that contains the sentence per-forming the best.We evaluated sentences based on lexical sim-ilarity, but structural similarity is also an im-portant measure, which we plan to investigatein the future.
The translation model we usedwas the most basic model.
We used this modelbecause it had been shown effective in docu-ment retrieval, and was easily incorporated inthe query-likelihood framework, but we intend690to explore more sophisticated translation mod-els, and better alignment mechanisms.
Prelimi-nary results suggest that sentence retrieval canbe used to improve document retrieval, but weplan a more extensive investigation of evaluat-ing document similarity and relevance based onsentence-level similarity.7 AcknowledgementsThe authors would like to thank Leah Larkeyfor her Arabic-English lexicons.
This workwas supported in part by the Center for In-telligent Information Retrieval, in part by Ad-vanced Research and Development Activity andNSF grant #CCF-0205575 , and in part bySPAWARSYSCEN-SD grant number N66001-02-1-8903.
Any opinions, findings and conclu-sions or recommendations expressed in this ma-terial are the author(s) and do not necessarilyreflect those of the sponsor.ReferencesY.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.Smith, and D. Yarowsky.
1999.
Statistical ma-chine translation, final report, JHU workshop.Adam Berger and John Lafferty.
1999.
Informationretrieval as statistical translation.
In Proceedingsof the 22nd Annual Conference on Research andDevelopment in Information Retrieval (ACM SI-GIR).Adam Berger, Rich Caruana, David Cohn, DayneFreitag, and Vibhu Mittal.
2000.
Bridging thelexical chasm: Statistical approaches to answer-finding.
In Proceedings of the 23rd Annual Con-ference on Research and Development in Informa-tion Retrieval (ACM SIGIR), pages 192?199.Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. Della Pietra, Fredrick Jelineck,John D. Lafferty, Robert L. Mercer, and Paul S.Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.Kevyn Collins-Thompson, Paul Ogilvie, Yi Zhang,and Jamie Callan.
2002.
Information filtering,novelty detection and named-page finding.
In Pro-ceedings of the Eleventh Text Retrieval Conference(TREC).Donna Harman.
2002.
Overview of the TREC 2002novelty track.
In Proceedings of the Eleventh TextRetrieval Conference (TREC).Leah Larkey, James Allan, Margie Connell, AlvaroBolivar, and Courtney Wade.
2002.
UMass atTREC 2002: Cross language and novelty tracks.In Proceedings of the Eleventh Text Retrieval Con-ference (TREC), page 721.Christof Monz, Jaap Kamps, and Maarten de Rijke.2002.
The University of Amsterdam at TREC2002.
In Proceedings of the Eleventh Text Re-trieval Conference (TREC).Vanessa Murdock and W. Bruce Croft.
2004.
Simpletranslation models for sentence retrieval in factoidquestion answering.
In Proceedings of the Infor-mation Retrieval for Question Answering Work-shop at SIGIR 2004.Ryosuke Ohgaya, Akiyoshi Shimmura, and TomohiroTakagi.
2003.
Meiji University web and noveltytrack experiments at TREC 2003.
In Proceedingsof the Twelth Text Retrieval Conference (TREC).Jeffrey C. Reynar and Adwait Ratnaparkhi.1997.
A maximum entropy approach toidentifying sentence boundaries.
In Pro-ceedings of the 5th Conference on Ap-plied Natural Language Processing (ANLP).http://www.cis.upenn.edu/a?dwait/statnlp.html.Mark Smucker and James Allan.
2005.
An investi-gation of dirichlet prior smoothing?s performanceadvantage.
Technical Report IR-391, The Univer-sity of Massachusetts, The Center for IntelligentInformation Retrieval.Ian Soboroff and Donna Harman.
2003.
Overview ofthe TREC 2003 novelty track.
In Proceedings ofthe Twelfth Text Retrieval Conference (TREC).Ian Soboroff.
2004.
Overview of the TREC 2004novelty track.
In Proceedings of the ThirteenthText Retrieval Conference (TREC).
forthcoming.Jinxi Xu, Alexander Fraser, and Ralph Weischedel.2002.
Empirical studies in strategies for arabic re-trieval.
In Proceedings of the 25th Annual Confer-ence on Research and Development in InformationRetrieval (ACM SIGIR).ChengXiang Zhai and John Lafferty.
2001.
A studyof smoothing methods for language models appliedto ad hoc information retrieval.
In Proceedings ofthe 24th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 334?342.691
