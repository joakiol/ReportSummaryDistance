Proceedings of the ACL Student Research Workshop, pages 67?72,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsPhrase Linguistic Classification and Generalization for Improving StatisticalMachine TranslationAdria` de GispertTALP Research CenterUniversitat Polite`cnica de Catalunya (UPC)Barcelonaagispert@gps.tsc.upc.esAbstractIn this paper a method to incorporate lin-guistic information regarding single-wordand compound verbs is proposed, as afirst step towards an SMT model basedon linguistically-classified phrases.
Bysubstituting these verb structures by thebase form of the head verb, we achievea better statistical word alignment perfor-mance, and are able to better estimate thetranslation model and generalize to unseenverb forms during translation.
Preliminaryexperiments for the English - Spanish lan-guage pair are performed, and future re-search lines are detailed.1 IntroductionSince its revival in the beginning of the 1990s, statis-tical machine translation (SMT) has shown promis-ing results in several evaluation campaigns.
Fromoriginal word-based models, results were further im-proved by the appearance of phrase-based transla-tion models.However, many SMT systems still ignore anymorphological analysis and work at the surface levelof word forms.
For highly-inflected languages, suchas German or Spanish (or any language of the Ro-mance family) this poses severe limitations both intraining from parallel corpora, as well as in produc-ing a correct translation of an input sentence.This lack of linguistic knowledge in SMT forcesthe translation model to learn different transla-tion probability distributions for all inflected formsof nouns, adjectives or verbs (?vengo?, ?vienes?,?viene?, etc.
), and this suffers from usual data sparse-ness.
Despite the recent efforts in the community toprovide models with this kind of information (seeSection 6 for details on related previous work), re-sults are yet to be encouraging.In this paper we address the incorporation ofmorphological and shallow syntactic information re-garding verbs and compound verbs, as a first steptowards an SMT model based on linguistically-classified phrases.
With the use of POS-tags andlemmas, we detect verb structures (with or withoutpersonal pronoun, single-word or compound withauxiliaries) and substitute them by the base form1of the head verb.
This leads to an improved statisti-cal word alignment performance, and has the advan-tages of improving the translation model and gen-eralizing to unseen verb forms, during translation.Experiments for the English - Spanish language pairare performed.The organization of the paper is as follows.
Sec-tion 2 describes the rationale of this classificationstrategy, discussing the advantages and difficultiesof such an approach.
Section 3 gives details ofthe implementation for verbs and compound verbs,whereas section 4 shows the experimental settingused to evaluate the quality of the alignments.
Sec-tion 5 explains the current point of our research, aswell as both our most-immediate to-do tasks and ourmedium and long-term experimentation lines.
Fi-nally, sections 6 and 7 discuss related works that canbe found in literature and conclude, respectively.1The terms ?base form?
or ?lemma?
will be used equivalentlyin this text.672 Morphosyntactic classification oftranslation unitsState-of-the-art SMT systems use a log-linear com-bination of models to decide the best-scoring tar-get sentence given a source sentence.
Amongthese models, the basic ones are a translation modelPr(e|f) and a target language model Pr(e), whichcan be complemented by reordering models (if thelanguage pairs presents very long alignments intraining), word penalty to avoid favoring short sen-tences, class-based target-language models, etc (Ochand Ney, 2004).The translation model is based on phrases; wehave a table of the probabilities of translating a cer-tain source phrase f?j into a certain target phrasee?k.
Several strategies to compute these probabili-ties have been proposed (Zens et al, 2004; Crego etal., 2004), but none of them takes into account thefact that, when it comes to translation, many differ-ent inflected forms of words share the same transla-tion.
Furthermore, they try to model the probabilityof translating certain phrases that contain just aux-iliary words that are not directly relevant in trans-lation, but play a secondary role.
These words area consequence of the syntax of each language, andshould be dealt with accordingly.For examples, consider the probability of translat-ing ?in the?
into a phrase in Spanish, which does notmake much sense in isolation (without knowing thefollowing meaning-bearing noun), or the modal verb?will?, when Spanish future verb forms are writtenwithout any auxiliary.Given these two problems, we propose a classifi-cation scheme based on the base form of the phrasehead, which is explained next.2.1 Translation with classified phrasesAssuming we translate from f to e, and defining e?i,f?j a certain source phrase and a target phrases (se-quences of contiguous words), the phrase translationmodel Pr(e?i|f?j) can be decomposed as:?TPr(e?i|T, f?j)Pr(E?i|F?j , f?j)Pr(F?j , f?j) (1)where E?i, F?j are the generalized classes of thesource and target phrases, respectively, and T =(E?i, F?j) is the pair of source and target classes used,which we call Tuple.
In our current implementation,we consider a classification of phrases that is:?
Linguistic, ie.
based on linguistic knowledge?
Unambiguous, ie.
given a source phrase thereis only one class (if any)?
Incomplete, ie.
not all phrases are classified,but only the ones we are interested in?
Monolingual, ie.
it runs for every language in-dependentlyThe second condition implies Pr(F?
|f?)
= 1,leading to the following expression:Pr(e?i|f?j) = Pr(E?i|F?j)Pr(e?i|T, f?j) (2)where we have just two terms, namely a standardphrase translation model based on the classifiedparallel data, and an instance model assigning aprobability to each target instance given the sourceclass and the source instance.
The latter helps uschoose among target words in combination with thelanguage model.2.2 AdvantagesThis strategy has three advantages:Better alignment.
By reducing the number ofwords to be considered during first word alignment(auxiliary words in the classes disappear and noinflected forms used), we lessen the data sparsenessproblem and can obtain a better word alignment.In a secondary step, one can learn word alignmentrelationships inside aligned classes by realigningthem as a separate corpus, if that is desired.Improvement of translation probabilities.
Byconsidering many different phrases as differentinstances of a single phrase class, we reduce the sizeof our phrase-based (now class-based) translationmodel and increase the number of occurrences ofeach unit, producing a model Pr(E?|F? )
with lessperplexity.68Generalizing power.
Phrases not occurring inthe training data can still be classified into a class,and therefore be assigned a probability in the trans-lation model.
The new difficulty that rises is how toproduce the target phrase from the target class andthe source phrase, if this was not seen in training.2.3 DifficultiesTwo main difficulties2 are associated with thisstrategy, which will hopefully lead to improvedtranslation performance if tackled conveniently.Instance probability.
On the one hand, when aphrase of the test sentence is classified to a class,and then translated, how do we produce the instanceof the target class given the tuple T and the sourceinstance?
This problem is mathematically expressedby the need to model the term of the Pr(e?i|T, f?j) inEquation 2.At the moment, we learn this model from relativefrequency across all tuples that share the samesource phrase, dividing the times we see the pair(f?j, e?i) in the training by the times we see f?j .Unseen instances.
To produce a target instancef?
given the tuple T and an unseen e?, our idea is tocombine both the information of verb forms seen intraining and off-the-shelf knowledge for generation.A translation memory can be built with all the seenpairs of instances with their inflectional affixesseparated from base forms.For example, suppose we translate from Englishto Spanish and see the tuple T=(V[go],V[ir]) intraining, with the following instances:I will go ire?PRP(1S) will VB VB 1S Fyou will go ira?sPRP(2S) will VB VB 2S Fyou will go vasPRP(2S) will VB VB 2S P2A third difficulty is the classification task itself, but we takeit for granted that this is performed by an independent systembased on other knowledge sources, and therefore out of scopehere.where the second row is the analyzed form in termsof person (1S: 1st singular, 2S: 2nd singular andso on) and tense (VB: infinitive and P: present, F:future).
From these we can build a generalized ruleindependent of the person ?
PRP(X) will VB ?
thatwould enable us to translate ?we will go?
to twodifferent alternatives (present and future form):we will go VB 1P Fwe will go VB 1P PThese alternatives can be weighted according tothe times we have seen each case in training.
An un-ambiguous form generator produces the forms ?ire-mos?
and ?vamos?
for the two Spanish translations.3 Classifying Verb FormsAs mentioned above, our first and basic implemen-tation deals with verbs, which are classified unam-biguously before alignment in training and beforetranslating a test.3.1 Rules usedWe perform a knowledge-based detection of verbsusing deterministic automata that implement a fewsimple rules based on word forms, POS-tags andword lemmas, and map the resulting expression tothe lemma of the head verb (see Figure 1 for somerules and examples of detected verbs).
This is doneboth in the English and the Spanish side, and beforeword alignment.Note that we detect verbs containing adverbs andnegations (underlined in Figure 1), which are or-dered before the verb to improve word alignmentwith Spanish, but once aligned they are reorderedback to their original position inside the detectedverb, representing the real instance of this verb.4 ExperimentsIn this section we present experiments with theSpanish-English parallel corpus developed in theframework of the LC-STAR project.
This corpusconsists of transcriptions of spontaneously spokendialogues in the tourist information, appointmentscheduling and travel planning domain.
Therefore,sentences often lack correct syntactic structure.
Pre-processing includes:69PP + V(L=have) {+RB} {+been} +V{G}V(L=have) {+not} +PP {+RB} {+been} +V{G}PP +V(L=be) {+RB} +VGV(L=be) {+not} +PP {+RB} +VGPP + MD(L=will/would/...) {+RB} +VMD(L=will/would/...) {+not} +PP {+RB} +VPP {+RB} +VV(L=do) {+not} +PP {+RB} +VV(L=be) {+not} +PPPP: Personal PronounV / MD / VG / RB: Verb / Modal / Gerund/ Adverb (PennTree Bank POS)L: Lemma (or base form){ } / ( ): optionality / instantiationExamples:leavesdo you havedid you comehe hasnotattendedhave youeverbeenI will haveshe is going to bewe would arriveFigure 1: Some verb phrase detection rules and detected forms in English.?
Normalization of contracted forms for English(ie.
wouldn?t = would not, we?ve = we have)?
English POS-tagging using freely-availableTnT tagger (Brants, 2000), and lemmatizationusing wnmorph, included in the WordNet pack-age (Miller et al, 1991).?
Spanish POS-tagging using FreeLing analysistool (Carreras et al, 2004).
This software alsogenerates a lemma or base form for each inputword.4.1 Parallel corpus statisticsTable 1 shows the statistics of the data used, whereeach column shows number of sentences, number ofwords, vocabulary, and mean length of a sentence,respectively.sent.
words vocab.
LmeanTrain setEnglish 419113 5940 14.0Spanish 29998 388788 9791 13.0Test setEnglish 7412 963 14.8Spanish 500 6899 1280 13.8Table 1: LC-Star English-Spanish Parallel corpus.There are 116 unseen words in the Spanish testset (1.7% of all words), and 48 unseen words in theEnglish set (0.7% of all words), an expected big dif-ference given the much more inflectional nature ofthe Spanish language.4.2 Verb Phrase Detection/ClassificationTable 2 shows the number of detected verbs usingthe detection rules presented in section 3.1, and thenumber of different lemmas they map to.
For the testset, the percentage of unseen verb forms and lemmasare also shown.verbs unseen lemmas unseenTrain setEnglish 56419 768Spanish 54460 911Test setEnglish 1076 5.2% 146 4.7%Spanish 1061 5.6% 171 4.7%Table 2: Detected verb forms in corpus.In average, detected English verbs contain 1.81words, whereas Spanish verbs contain 1.08 words.This is explained by the fact that we are includingthe personal pronouns in English and modals for fu-ture, conditionals and other verb tenses.4.3 Word alignment resultsIn order to assess the quality of the word alignment,we randomly selected from the training corpus 350sentences, and a manual gold standard alignmenthas been done with the criterion of Sure and Pos-sible links, in order to compute Alignment ErrorRate (AER) as described in (Och and Ney, 2000) andwidely used in literature, together with appropriatelyredefined Recall and Precision measures.
Mathe-matically, they can be expressed thus:recall = |A ?
S||S| , precision =|A ?
P ||A|AER = 1 ?
|A ?
S| + |A ?
P ||A| + |S|70where A is the hypothesis alignment and S is theset of Sure links in the gold standard reference, andP includes the set of Possible and Sure links in thegold standard reference.We have aligned our data using GIZA++ (Och,2003) from English to Spanish and vice versa (per-forming 5 iterations of model IBM1 and HMM, and3 iterations of models IBM3 and IBM4), and haveevaluated two symmetrization strategies, namely theunion and the intersection, the union always ratingthe best.
Table 3 compares the result when aligningwords (current baseline), and when aligning classi-fied verb phrases.
In this case, after the alignmentwe substitute the class for the original verb form andeach new word gets the same links the class had.
Ofcourse, adverbs and negations are kept apart fromthe verb and have separate links.Recall Precision AERbaseline 74.14 86.31 20.07with class.
verbs 76.45 89.06 17.37Table 3: Results in statistical alignment.Results show a significant improvement in AER,which proves that verbal inflected forms and auxil-iaries do harm alignment performance in absence ofthe proposed classification.4.4 Translation resultsWe have integrated our classification strategy in anSMT system which implements:?
Pr(e?i|f?k) as a tuples language model (Ngram),as done in (Crego et al, 2004)?
Pr(e) as a standard Ngram language model us-ing SRILM toolkit (Stolcke, 2002)Parameters have been optimised for BLEU scorein a 350 sentences development set.
Three refer-ences are available for both development and testsets.
Table 4 presents a comparison of English toSpanish translation results of the baseline systemand the configuration with classification (withoutdealing with unseen instances).
Results are promis-ing, as we achieve a significant mWER error re-duction, while still leaving about 5.6 % of the verbforms in the test without translation.
Therefore, weexpect a further improvement with the treatment ofunseen instances.mWER BLEUbaseline 23.16 0.671with class.
verbs 22.22 0.686Table 4: Results in English to Spanish translation.5 Ongoing and future researchOngoing research is mainly focused on developingan appropriate generalization technique for unseeninstances and evaluating its impact in translationquality.Later, we expect to run experiments with a muchbigger parallel corpus such as the European Parlia-ment corpus, in order to evaluate the improvementdue to morphological information for different sizesof the training data.
Advanced methods to computePr(e?i|T, f?j) should also be tested (based on sourceand target contextual features).The next step will be to extend the approach toother potential classes such as:?
Nouns and adjectives.
A straightforward strat-egy would classify all nouns and adjectives totheir base form, reducing sparseness.?
Simple Noun phrases.
Noun phrases with orwithout article (determiner), and with or with-out preposition, could also be classified to thebase form of the head noun, leading to a fur-ther reduction of the data sparseness, in a sub-sequent stage.
In this case, expressions like atnight, the night, nights or duringthe night would all be mapped to the class?night?.?
Temporal and numeric expressions.
As they areusually tackled in a preprocessing stage in cur-rent SMT systems, we did not deal with themhere.More on a long-term basis, ambiguous linguisticclassification could also be allowed and included inthe translation model.
For this, incorporating statis-tical classification tools (chunkers, shallow parsers,phrase detectors, etc.)
should be considered, andevaluated against the current implementation.716 Related WorkThe approach to deal with inflected forms presentedin (Ueffing and Ney, 2003) is similar in that it alsotackles verbs in an English ?
Spanish task.
How-ever, whereas the authors join personal pronounsand auxiliaries to form extended English units anddo not transform the Spanish side, leading to an in-creased English vocabulary, our proposal aims at re-ducing both vocabularies by mapping all differentverb forms to the base form of the head verb.An improvement in translation using IBM model1 in an Arabic ?
English task can be found in (Lee,2004).
From a processed Arabic text with all pre-fixes and suffixes separated, the author determineswhich of them should be linked back to the wordand which should not.
However, no mapping to baseforms is performed, and plurals are still differentwords than singulars.In (Nie?en and Ney, 2004) hierarchical lexiconmodels including base form and POS informationfor translation from German into English are intro-duced, among other morphology-based data trans-formations.
Finally, the same pair of languages isused in (Corston-Oliver and Gamon, 2004), wherethe inflectional normalization leads to improvementsin the perplexity of IBM translation models and re-duces alignment errors.
However, compound verbsare not mentioned.7 ConclusionA proposal of linguistically classifying translationphrases to improve statistical machine translationperformance has been presented.
This classificationallows for a better translation modeling and a gen-eralization to unseen forms.
A preliminary imple-mentation detecting verbs in an English ?
Spanishtask has been presented.
Experiments show a sig-nificant improvement in word alignment, and in pre-liminary translation results.
Ongoing and future re-search lines are discussed.ReferencesT.
Brants.
2000.
TnT ?
a statistical part-of-speech tag-ger.
In Proc.
of the Sixth Applied Natural LanguageProcessing (ANLP-2000), Seattle, WA.X.
Carreras, I. Chao, L. Padr o?, and M. Padr o?.
2004.Freeling: An open-source suite of language analyzers.4th Int.
Conf.
on Language Resources and Evaluation,LREC?04, May.S.
Corston-Oliver and M. Gamon.
2004.
Normalizinggerman and english inflectional morphology to im-prove statistical word alignment.
Proc.
of the 6thConf.
of the Association for Machine Translation inthe Americas, pages 48?57, October.J.M.
Crego, J. Marin?o, and A. de Gispert.
2004.
Finite-state-based and phrase-based statistical machine trans-lation.
Proc.
of the 8th Int.
Conf.
on Spoken LanguageProcessing, ICSLP?04, pages 37?40, October.Y.S.
Lee.
2004.
Morphological analysis for statisticalmachine translation.
In Daniel Marcu Susan Dumaisand Salim Roukos, editors, HLT-NAACL 2004: ShortPapers, pages 57?60, Boston, Massachusetts, USA,May.
Association for Computational Linguistics.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross,K.
Miller, and R. Tengi.
1991.
Five papers on word-net.
Special Issue of International Journal of Lexicog-raphy, 3(4):235?312.S.
Nie?en and H. Ney.
2004.
Statistical machine trans-lation with scarce resources using morpho-syntacticinformation.
Computational Linguistics, 30(2):181?204, June.F.J.
Och and H. Ney.
2000.
Improved statistical align-ment models.
38th Annual Meeting of the Associationfor Computational Linguistics, pages 440?447, Octo-ber.F.J.
Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449, December.F.J.
Och.
2003.
Giza++ software.
http://www-i6.informatik.rwth-aachen.de/?och/ soft-ware/giza++.html.A.
Stolcke.
2002.
Srilm - an extensible language mod-eling toolkit.
Proc.
of the 7th Int.
Conf.
on SpokenLanguage Processing, ICSLP?02, September.N.
Ueffing and H. Ney.
2003.
Using pos information forsmt into morphologically rich languages.
10th Conf.of the European Chapter of the Association for Com-putational Linguistics, pages 347?354, April.R.
Zens, F.J. Och, and H. Ney.
2004.
Improvementsin phrase-based statistical machine translation.
Proc.of the Human Language Technology Conference, HLT-NAACL?2004, pages 257?264, May.72
