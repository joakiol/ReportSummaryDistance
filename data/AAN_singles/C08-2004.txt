Coling 2008: Companion volume ?
Posters and Demonstrations, pages 15?18Manchester, August 2008The power of negative thinking:Exploiting label disagreement in the min-cut classification frameworkMohit BansalDept.
of Computer Science & EngineeringIndian Institute of Technology Kanpurmbansal47@gmail.comClaire Cardie and Lillian LeeDept.
of Computer ScienceCornell University{cardie,llee}@cs.cornell.eduAbstractTreating classification as seeking minimumcuts in the appropriate graph has proven ef-fective in a number of applications.
Thepower of this approach lies in its abil-ity to incorporate label-agreement prefer-ences among pairs of instances in a prov-ably tractable way.
Label disagreementpreferences are another potentially richsource of information, but prior NLP workwithin the minimum-cut paradigm has notexplicitly incorporated it.
Here, we re-port on work in progress that examinesseveral novel heuristics for incorporatingsuch information.
Our results, producedwithin the context of a politically-orientedsentiment-classification task, demonstratethat these heuristics allow for the additionof label-disagreement information in a waythat improves classification accuracy whilepreserving the efficiency guarantees of theminimum-cut framework.1 IntroductionClassification algorithms based on formulating theclassification task as one of finding minimum s-tcuts in edge-weighted graphs ?
henceforth min-imum cuts or min cuts ?
have been successfullyemployed in vision, computational biology, andnatural language processing.
Within NLP, appli-cations include sentiment-analysis problems (Pangand Lee, 2004; Agarwal and Bhattacharyya, 2005;Thomas et al, 2006) and content selection for textgeneration (Barzilay and Lapata, 2005).c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.As a classification framework, the minimum-cut approach is quite attractive.
First, it providesa principled, yet flexible mechanism for allowingproblem-specific relational information ?
includ-ing several types of both hard and soft constraints?
to influence a collection of classification deci-sions.
Second, in many important cases, such aswhen all the edge weights are non-negative, find-ing a minimum cut can be done in a provably effi-cient manner.To date, however, researchers have restricted thesemantics of the constraints mentioned above toencode pair-wise ?agreement?
information only.There is a computational reason for this restriction:?agreement?
and ?disagreement?
information arearguably most naturally expressed via positive andnegative edge weights, respectively; but in general,the inclusion of even a relatively small number ofnegative edge weights makes finding a minimumcut NP-hard (McCormick et al, 2003).To avoid this computational issue, we proposeseveral heuristics that encode disagreement infor-mation with non-negative edge weights.
We in-stantiate our approach on a sentiment-polarity clas-sification task ?
determining whether individualconversational turns in U.S. Congressional floordebates support or oppose some given legislation.Our preliminary results demonstrate promising im-provements over the prior work of Thomas et al(2006), who considered only the use of agreementinformation in this domain.2 Method2.1 Min-cut classification frameworkBinary classification problems are usually ap-proached by considering each classification deci-sion in isolation.
More formally, let Xtest=15{x1, x2, .
.
.
, xn} be the test instances, drawn fromsome universe X , and let C = {c1, c2} bethe two possible classes.
Then, the usual ap-proach can often be framed as labeling each xiaccording to some individual-preference functionInd:X ?
C?<, such as the signed distance tothe dividing hyperplane according to an SVM orthe posterior class probability assigned by a NaiveBayes classifier.But when it is difficult to accurately classify aparticular xiin isolation, there is a key insightthat can help: knowing that xihas the same la-bel as an easily-categorized xjmakes labeling xieasy.
Thus, suppose we also have an association-preference function Assoc:X ?X?< express-ing a reward for placing two items in the sameclass; an example might be the output of an agree-ment classifier or a similarity function.
Then, wecan search for a classification function c(xi|Xtest)?
note that all of Xtestcan affect an instance?s la-bel ?
that minimizes the total ?pining?
of the testitems for the class they were not assigned to due toeither their individual or associational preferences:?iInd(xi, c(xi|Xtest)) +??
?i,j:c(xi|Xtest)=c(xj|Xtest)Assoc(xi, xj),where c(xi|Xtest) is the class ?opposite?
toc(xi|Xtest), and the free parameter ?
regulatesthe emphasis on agreement information.
Solutionsto the above minimization problem correspond tominimum s-t cuts in a certain graph, and if bothInd and Assoc are non-negative functions, then,surprisingly, minimum cuts can be found in poly-nomial time; see Kleinberg and Tardos (2006, Sec-tion 7.10) for details.
But, as mentioned above,allowing negative values makes finding a solutionintractable in the general case.2.2 Prior work discards some negative valuesThe starting point for our work is Thomas etal.
(2006) (henceforth TPL).
The reason for thischoice is that TPL used minimum-cut-based classi-fication wherein signed distances to dividing SVMhyperplanes were employed to define Ind(x, c)and Assoc(x, x?).
It was natural to use SVMs,since association was determined by classificationrather than similarity ?
specifically, categorizingreferences by one congressperson to another as re-flecting agreement or not ?
but as a result, neg-ative association-preferences (e.g., negative dis-tance to a hyperplane) had to be accounted for.We formalize TPL?s approach at a highlevel as follows.
Let Ind?
:X ?
C?< andAssoc?
:X ?X?< be initial individual- andassociation-preference functions, such as thesigned distances mentioned above.
TPL create twonon-negative conversion functions f :<?
[0, 1]and g:<?
[0, 1], and then defineInd(xi, c) := f(Ind?
(xi, c))Assoc(xi, xj) := g(Assoc?
(xi, xj))so that an optimal classification can be found inpolynomial time, as discussed above.
We omit theexact definitions of f and g in order to focus onwhat is important here: roughly speaking, f andg normalize values and handle outliers1, with thefollowing crucial exception.
While negative initialindividual preferences for one class can be trans-lated into positive individual preferences for theother, there is no such mechanism for negative val-ues of Assoc?
; so TPL resort to defining g to be0 for negative arguments.
They thus discard po-tentially key information regarding the strength oflabel disagreement preferences.2.3 Encoding negative associationsInstead of discarding the potentially crucial label-disagreement information represented by negativeAssoc?
values, we propose heuristics that seek toincorporate this valuable information, but that keepInd and Assoc non-negative (by piggy-backing offof TPL?s pre-existing conversion-function strat-egy2) to preserve the min-cut-classification effi-ciency guarantees.We illustrate our heuristics with a runningexample.
Consider a simplified setting with onlytwo instances x1and x2; f(z) = z; g(z) = 0 ifz < 0, 1 otherwise; and Ind?
values (numberslabeling left or right arrows in the diagrams below,e.g., Ind?
(x1, c1) = .7) and Assoc?
value (the -2labeling the up-and-down arrow) as depicted here:?
[.7]?
x1?
[.3]?c1m [?2] c2?
[.6]?
x2?
[.4]?Then, the resulting TPL Ind and Assoc values are1Thus, strictly speaking, f and g also depend on Ind?,Assoc?, and Xtest, but we suppress this dependence for nota-tional compactness.2Our approach also applies to definitions of f and g dif-ferent from TPL?s.16?
[.7]?
x1?
[.3]?c1m [0] c2?
[.6]?
x2?
[.4]?Note that since the initial -2 association value isignored, c(x1|Xtest) = c(x2|Xtest) = c1appearsto be a good classification according to TPL.The Scale all up heuristic Rather than discarddisagreement information, a simple strategy is tojust scale up all initial association preferences by asufficiently large positive constant N :Ind(xi, c) := f(Ind?
(xi, c))Assoc(xi, xj) := g(Assoc?
(xi, xj) + N)For N = 3 in our example, we get?
[.7]?
x1?
[.3]?c1m [1] c2?
[.6]?
x2?
[.4]?This heuristic ensures that the more negative theAssoc?
value, the lower the cost of separating therelevant item pair (whereas TPL don?t distinguishbetween negative Assoc?
values).
The heuristicbelow tries to be more proactive, forcing suchpairs to receive different labels.The SetTo heuristic We proceed throughx1, x2, .
.
.
in order.
Each time we encounteran xiwhere Assoc?
(xi, xj) < 0 for somej > i, we try to force xiand xjinto dif-ferent classes by altering the four relevantindividual-preferences affecting this pair of in-stances, namely, f(Ind?
(xi, c1)), f(Ind?
(xi, c2)),f(Ind?
(xj, c1)), and f(Ind?
(xj, c2)).
Assumewithout loss of generality that the largest ofthese values is the first one.
If we respectthat preference to put xiin c1, then accordingto the association-preference information, itfollows that we should put xjin c2.
We caninstantiate this chain of reasoning by settingInd(xi, c1) := max(?, f(Ind?
(xi, c1)))Ind(xi, c2) := min(1?
?, f(Ind?
(xi, c2)))Ind(xj, c1) := min(1?
?, f(Ind?
(xj, c1)))Ind(xj, c2) := max(?, f(Ind?
(xj, c2)))for some constant ?
?
(.5, 1], and making nochange to TPL?s definition of Assoc.
For ?
= .8in our example, we get?
[.8]?
x1?
[.2]?c1m [0] c2?
[.2]?
x2?
[.8]?Note that as we proceed through x1, x2, .
.
.
inorder, some earlier changes may be undone.The IncBy heuristic A more conservative ver-sion of the above heuristic is to increment anddecrement the individual-preference values so thatthey are somewhat preserved, rather than com-pletely replace them with fixed constants:Ind(xi, c1) := min(1, f(Ind?
(xi, c1)) + ?
)Ind(xi, c2) := max(0, f(Ind?
(xi, c2))?
?
)Ind(xj, c1) := max(0, f(Ind?
(xj, c1))?
?
)Ind(xj, c2) := min(1, f(Ind?
(xj, c2)) + ?
)For ?
= .1, our example becomes?
[.8]?
x1?
[.2]?c1m [0] c2?
[.5]?
x2?
[.5]?3 EvaluationFor evaluation, we adopt the sentiment-classification problem tackled by TPL: clas-sifying speech segments (individual conversationalturns) in a U.S. Congressional floor debate asto whether they support or oppose the legis-lation under discussion.
TPL describe manyreasons why this is an important problem.
Forour purposes, this task is also very convenientbecause all of TPL?s computed raw and convertedInd?
and Assoc?
data are publicly available atwww.cs.cornell.edu/home/llee/data/convote.html.Thus, we used their calculated values to imple-ment our algorithms as well as to reproduce theiroriginal results.3One issue of note is that TPL actually in-ferred association preferences between speakers,not speech segments.
We do the same when ap-plying SetTo or IncBy to a pair {xi, xj} by con-sidering the average of f(Ind?
(xk, c1)) over allxkuttered by the speaker of xi, instead of justf(Ind?
(xi, c1)).
The other three relevant individ-ual values are treated similarly.
We also makeappropriate modifications (according to SetTo andIncBy) to the individual preferences of all such xksimultaneously, not just xi, and similarly for xj.A related issue is that TPL assume that allspeech segments by the same speaker should havethe same label.
To make experimental compar-isons meaningful, we follow TPL in consideringtwo different instantiations of this assumption.
Insegment-based classification, Assoc(xi, xj) is setto an arbitrarily high positive constant if the samespeaker uttered both xiand xj.
In speaker-basedclassification, Ind?
(xi, c) is produced by running3For brevity, we omit TPL?s ?high-threshold?
variants.1760626466687072747678SetTo(.6)SVM SetTo(1) IncBy(.25)IncBy(.15)TPL IncBy(.05)Scale all up SetTo(.8)percent correctALGORITHMSTest-set classification accuracies, using held-out parameter estimationsegment-based, testspeaker-based, testbest TPL, testFigure 1: Experimental results.
?SVM?
: classification using only individual-preference information.Values of ?
are indicated in parentheses next to the relevant algorithm names.an SVM on the concatenation of all speeches ut-tered by xi?s speaker.Space limits preclude inclusion of further de-tails; please see TPL for more information.3.1 Results and future plansThe association-emphasis parameter ?
was trainedon held-out data, with ties broken in favor of thelargest ?
in order to emphasize association in-formation.
We used Andrew Goldberg?s HIPRcode (http://avglab.com/andrew/soft.html) to com-pute minimum cuts.
The resultant test-set classifi-cation accuracies are presented in Figure 1.We see that Scale all up performs worsethan TPL, but the more proactive heuristics(SetTo, IncBy) almost always outperform TPL onsegment-based classification, sometimes substan-tially so, and outperform TPL on speaker-basedclassification for half of the variations.
We there-fore conclude that label disagreement informa-tion is indeed valuable; and that incorporating la-bel disagreement information on top of the (posi-tive) label agreement information that TPL lever-aged can be achieved using simple heuristics; andthat good performance enhancements result with-out any concomitant significant loss of efficiency.These results are preliminary, and the diver-gence in behaviors between different heuristicsin different settings requires investigation.
Ad-ditional future work includes investigating moresophisticated (but often therefore less tractable)formalisms for joint classification; and lookingat whether approximation algorithms for findingminimum cuts in graphs with negative edge capac-ities can be effective.Acknowledgments We thank Jon Kleinberg and thereviewers for helpful comments.
Portions of this workwere done while the first author was visiting Cornell Uni-versity.
This paper is based upon work supported in partby the National Science Foundation under grant nos.
IIS-0329064, BCS-0624277, and IIS-0535099, a Cornell Univer-sity Provost?s Award for Distinguished Scholarship, a Yahoo!Research Alliance gift, an Alfred P. Sloan Research Fellow-ship, and by DHS grant N0014-07-1-0152.
Any opinions,findings, and conclusions or recommendations expressed arethose of the authors and do not necessarily reflect the viewsor official policies, either expressed or implied, of any spon-soring institutions, the U.S. government, or any other entity.ReferencesA.
Agarwal, P. Bhattacharyya.
2005.
Sentiment analysis: Anew approach for effective use of linguistic knowledge andexploiting similarities in a set of documents to be classi-fied.
ICON.R.
Barzilay, M. Lapata.
2005.
Collective content selection forconcept-to-text generation.
HLT/EMNLP, pp.
331?338.J.
Kleinberg, ?E.
Tardos.
2006.
Algorithm Design.
AddisonWesley.S.
T. McCormick, M. R. Rao, G. Rinaldi.
2003.
Easy and dif-ficult objective functions for max cut.
Mathematical Pro-gramming, Series B(94):459?466.B.
Pang, L. Lee.
2004.
A sentimental education: Sentimentanalysis using subjectivity summarization based on mini-mum cuts.
ACL, pp.
271?278.M.
Thomas, B. Pang, L. Lee.
2006.
Get out the vote: De-termining support or opposition from Congressional floor-debate transcripts.
EMNLP, pp.
327?335.18
