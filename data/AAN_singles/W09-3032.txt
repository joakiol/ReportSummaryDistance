Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 170?173,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAnnotating Wall Street Journal Texts Using a Hand-Crafted DeepLinguistic GrammarValia Kordoni & Yi ZhangDFKI GmbH and Dept.
of Computational Linguistics, Saarland University66041 Saarbru?cken, GERMANY{kordoni,yzhang}@coli.uni-sb.deAbstractThis paper presents an on-going effortwhich aims to annotate the Wall StreetJournal sections of the Penn Treebank withthe help of a hand-written large-scale andwide-coverage grammar of English.
In do-ing so, we are not only focusing on thevarious stages of the semi-automated an-notation process we have adopted, but weare also showing that rich linguistic anno-tations, which can apart from syntax alsoincorporate semantics, ensure that the tree-bank is guaranteed to be a truly sharable,re-usable and multi-functional linguisticresource?.1 IntroductionThe linguistic annotation of a corpus is the prac-tice of adding interpretative linguistic informationin order to give ?added value?
to the corpus.
Lin-guistically annotated corpora have been shown tohelp in many kinds of automatic language pro-cessing or analysis.
For example, corpora whichhave been POS-tagged can automatically yield fre-quency lists or frequency dictionaries with gram-matical classification.
Another important use forlinguistically annotated corpora is in the area ofautomatic parsing.
In terms of re-usability of lin-guistic annotations, what is to be advocated here isthat ?
as long as the annotation provided is a kinduseful to many users - an annotated corpus gives?value added?
because it can be readily shared byothers, apart from those who originally added theannotation.
In short, a linguistically annotated cor-pus is a sharable resource, an example of the elec-tronic resources increasingly relied on for researchand study in the humanities and social sciences.In this paper, we present an on-going projectwhose aim is to produce rich syntactic and se-?We thank Dan Flickinger and Stephan Oepen for theirsupport with the grammar and treebanking software used inthis project.
The second author is supported by the GermanExcellence Cluster: Multimodal Computing & Interaction.mantic annotations for the Wall Street Journal(henceforward WSJ) sections of the Penn Tree-bank (henceforward PTB; Marcus et al (1993)).The task is being carried out with the help of theEnglish Resource Grammar (henceforward ERG;Flickinger (2002)), which is a hand-written gram-mar for English in the spirit of the framework ofHead-driven Phrase Structure Grammar (hence-forward HPSG; Pollard and Sag (1994)).2 Background & MotivationThe past two decades have seen the developmentof many syntactically annotated corpora.
There isno need to defend the importance of treebanks inthe study of corpus linguistics or computationallinguistics here.
Evidently, the successful devel-opment of many statistical parsers is attributedto the development of large treebanks.
But forparsing systems based on hand-written grammars,treebanks are also important resources on the baseof which statistical parse disambiguation modelshave been developed.The early treebanking efforts started with man-ual annotations which are time-consuming anderror-prone procedures.
For instance, the WSJsections of the PTB has taken many person yearsto get annotated.
Similar efforts have been car-ried out in many more languages, as can be seenin the cases of the German Negra/Tiger Treebank(Brants et al, 2002), the Prague Dependency Tree-bank (Hajic?
et al, 2000), Tu?Ba-D/Z1, etc.
Al-though many of these projects have stimulated re-search in various sub-fields of computational lin-guistics where corpus-based empirical methodsare used, there are many known shortcomings ofthe manual corpus annotation approach.Many of the limitations in the manual treebank-ing approach have led to the development of sev-eral alternative approaches.
While annotating lin-guistically rich structures from scratch is clearlyinpractical, it has been shown that the different1http://www.sfs.nphil.uni-tuebingen.de/en tuebadz.shtml170structures in various linguistic frameworks can beconverted from annotated treebanks to a differ-ent format.
And the missing rich annotations canbe filled in incrementally and semi-automatically.This process usually involves careful design ofthe conversion program, which is a non-trivialtask.
In very recent years, based on the treebankconversion approach and existing manually anno-tated treebanks, various ?new?
annotations in dif-ferent grammar frameworks have been producedfor the same set of texts.
For example, for theWSJ sections of the PTB, annotations in the styleof dependency grammar, CCG, LFG and HPSGhave become available.
Such double annotationshave helped the cross-framework development andevaluation of parsing systems.
However, it mustbe noted that the influence of the original PTB an-notations and the assumptions implicit in the con-version programs have made the independence ofsuch new treebanks at least questionable.
To ourknowledge, there is no completely independentannotation of the WSJ texts built without conver-sion from the original PTB trees.Another popular alternative way to aid treebankdevelopment is to use automatic parsing outputsas guidance.
Many state-of-the-art parsers areable to efficiently produce large amount of anno-tated syntactic structures with relatively high ac-curacy.
This approach has changed the role ofhuman annotation from a labour-intensive task ofdrawing trees from scratch to a more intelligence-demanding task of correcting parsing errors, oreliminating unwanted ambiguities (cf., the Red-woods Treebank (Oepen et al, 2002)).
It is ouraim in this on-going project to build a HPSG tree-bank for the WSJ sections of the PTB based on thehand-written ERG for English.3 The Annotation Scheme3.1 Grammars & ToolsThe treebank under construction in this projectis in line with the so-called dynamic treebanks(Oepen et al, 2002).
We rely on the HPSG anal-yses produced by the ERG, and manually dis-ambiguate the parsing outputs with multiple an-notators.
The development is heavily based onthe DELPH-IN2 software repository and makesuse of the English Resource Grammar (ERG;Flickinger (2002), PET (Callmeier, 2001), an ef-ficient unification-based parser which is used in2http://www.delph-in.net/our project for parsing the WSJ sections of thePTB, and [incr tsdb()] (Oepen, 2001), the gram-mar performance profiling system we are using,which comes with a complete set of GUI-basedtools for treebanking.
Version control system alsoplays an important role in this project.3.2 PreprocessingThe sentences from the Wall Street Journal Sec-tions of the Penn Treebank are extracted with theiroriginal tokenization, with each word paired witha part-of-speech tag.
Each sentence is given aunique ID which can be used to easily look up itsorigin in the PTB.3.3 Annotation CyclesThe annotation is organised into iterations ofparsing, treebanking, error analysis and gram-mar/treebank update cycles.Parsing Sentences from the WSJ are first parsedwith the PET parser using the ERG.
Up to500 top readings are recorded for each sentence.The exact best-first parsing mode guarantees thatthese recorded readings are the ones that have?achieved?
highest disambiguation scores accord-ing to the current parse selection model, withoutenumerating through all possible analyses.Treebanking The parsing results are then man-ually disambiguated by the annotators.
However,instead of looking at individual trees, the annota-tors spend most of their effort making binary de-cisions on either accepting or rejecting construc-tions.
Each of these decisions, called discrim-inants, reduces the number of the trees satisfy-ing the constraints (see Figure 1).
Every time adecision is made, the remaining set of trees anddiscriminants are updated simultaneously.
Thiscontinues until one of the following conditions ismet: i) if there is only one remaining tree and itrepresents a correct analysis of the sentence, thetree is marked as gold; ii) if none of the remain-ing trees represents a valid analysis, the sentencewill be marked as ?rejected?, indicating an errorin the grammar3; iii) if the annotator is not sureabout any further decision, a ?low confidence?3In some cases, the grammar does produce a valid read-ing, but the disambiguation model fails to rank it among thetop 500 recorded candidates.
In practice, we find such er-rors occuring frequently during the first annotation circle, butthey diminish quickly when the disambiguation model getsupdated.171Figure 1: Treebanking Interface with an example sentence, candidate readings, discriminants and the MRS.
The top row ofthe interface is occupied by a list of functional buttons, followed by a line indicating the sentence ID, number of remainingreadings, number of eliminated readings, annotator confidence level, and the original PTB bracket annotation.
The left partdisplays the candidate readings, and their corresponding IDs (ranked by the disambiguation model).
The right part lists all thediscriminants among the remaining readings.
The lower part shows the MRS of one candicate reading.state will be marked on the sentence, saved to-gether with the partial disambiguation decisions.Generally speaking, given n candidate trees, onaverage log2 n decisions are needed in order tofully disambiguate.
Given that we set a limit of500 candidate readings per sentence, the wholeprocess should require no more than 9 decisions.If both the syntactic and the MRS analyses lookvalid, the tree will be recorded as the gold read-ing for the sentence.
It should be noted here thatthe tree displayed in the treebanking window isan abbreviated representation of the actual HPSGanalysis, which is much more informative than thephrase-structure tree shown here.Grammar & Treebank Update While thegrammar development is independent to the tree-banking progress, we periodically incorporate therecent changes of the grammar into the treebankannotation cycle.
When a grammar update is in-corporated, the treebank will be updated accord-ingly by i) parsing all the sentences with the newgrammar; ii) re-applying the recorded annotationdecisions; iii) re-annotating those sentences whichare not fully disambiguated after step ii.
The ex-tra manual annotation effort in treebank update isusually small when compared to the first round an-notation.Another type of update happens more fre-quently without extra annotation cost.
When anew portion of the corpus is annotated, this is usedto retrain the parse disambiguation model.
Thisimproves the parse selection accuracy and reducesthe annotation workload.3.4 Grammar coverage & robust parsingNot having been specifically tuned for the newspa-per texts, the ERG achieved out-of-box coverageof over 80% on the WSJ dataset.
While this is a re-spectably high coverage for a hand-written preci-sion grammar, the remaining 20% of the data is notcovered by the first round of annotation.
We planto parse the remaining data using a less-restrictiveprobabilistic context-free grammar extracted fromthe annotated part of the treebank.
The PCFGparser will produce a pseudo-derivation tree, withwhich robust unifications can be applied to con-struct the semantic structures (Zhang and Kordoni,172theDETnationNNNNP?sDETDETlargestADJpensionNfund,NNNNwhichNNPoverseesV/NPV/NP$NNNP80ADJbillionADJADJNNNPVP/NPforPcollegeNNemployees,NNNNNNPPPVP/NPS/NPSNNNPplansVVVPtoPofferVVtwoADJDETnewAPinvestmentNoptionsNNNNNPVPtoPitsDET1.2ADJmillionADJADJparticipants.NNNNNPPPVPPPVPSFigure 2: An example tree including a ?heavy?
NP-subject, a relative clause, and noun-noun compounds2008).3.5 Multiple annotationsTo speed up the annotation, the project employsthree annotators.
They are assigned with slightlyoverlapping sections of the WSJ dataset.
Theoverlapping part allows us to measure the inter-annotator agreement for the purpose of qualitycontrol.
To estimate the agreement level, the WSJSection 02 has been completely annotated by allthree annotators.
Analysis shows that the annota-tors reach exact match agreement for around 50%of the sentences.
Many disagreements are re-lated to subtle variations in the linguistic analy-ses.
The agreement level shows improvement af-ter several treebanker meetings.
For future devel-opment, a more fine-grained disagreement assess-ment is planned.4 DiscussionThe WSJ section of the PTB is not only a chal-lenging corpus to parse with a hand-written gram-mar.
It also contains various interesting and chal-lenging linguistic phenomena.
Figure 2, for in-stance, shows the syntactic analysis that the ERGproduces for a sentence which includes a ?heavy?NP (noun phrase) containing a relative clause in-troduced by which in the subject position, as wellas many interesting compound nouns whose inter-pretations are missing from the PTB annotation.The newly annotated data will be also very im-portant for the cross-framework parser develop-ment and evaluation.
While almost all of the state-of-the-art statistical parsers for English use PTBannotations for training and testing, it would beinteresting to see whether a comparable level ofparsing accuracy can be reproduced on the sametexts when re-annotated independently.ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The tiger treebank.
InProceedings of the workshop on treebanks and linguistictheories, pages 24?41.Ulrich Callmeier.
2001.
Efficient parsing with large-scaleunification grammars.
Master?s thesis, Universita?t desSaarlandes, Saarbru?cken, Germany.Dan Flickinger.
2002.
On building a more efficient grammarby exploiting types.
In Stephan Oepen, Dan Flickinger,Jun?ichi Tsujii, and Hans Uszkoreit, editors, CollaborativeLanguage Engineering, pages 1?17.
CSLI Publications.Jan Hajic?, Alena Bo?hmova?, Eva Hajic?ova?, and Barbora Vi-dova?-Hladka?.
2000.
The Prague Dependency Treebank:A Three-Level Annotation Scenario.
In A.
Abeille?, editor,Treebanks: Building and Using Parsed Corpora, pages103?127.
Amsterdam:Kluwer.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof english: The penn treebank.
Computational Linguis-tics, 19(2):313?330.Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christo-pher Manning, Dan Flickinger, and Thorsten Brants.2002.
The LinGO Redwoods treebank: motivation andpreliminary applications.
In Proceedings of COLING2002: The 17th International Conference on Computa-tional Linguistics: Project Notes, Taipei, Taiwan.Stephan Oepen.
2001.
[incr tsdb()] ?
competenceand performance laboratory.
User manual.
Technicalreport, Computational Linguistics, Saarland University,Saarbru?cken, Germany.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of Chicago Press,Chicago, USA.Yi Zhang and Valia Kordoni.
2008.
Robust Parsing with aLarge HPSG Grammar.
In Proceedings of the Sixth Inter-national Language Resources and Evaluation (LREC?08),Marrakech, Morocco.173
