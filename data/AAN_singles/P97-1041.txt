A Trainable Rule-based Algorithm for Word SegmentationDav id  D .
Pa lmerThe  MITRE Corporat ion202 Bur l ington  Rd.Bedford ,  MA 01730, USApalmer@mitre, orgAbst ractThis paper presents a trainable rule-basedalgorithm for performing word segmen-tation.
The algorithm provides a sim-ple, language-independent alternative tolarge-scale l xicai-based segmenters equir-ing large amounts of knowledge ngineer-ing.
As a stand-alone segmenter, we showour algorithm to produce high performanceChinese segmentation.
In addition, weshow the transformation-based algorithmto be effective in improving the output ofseveral existing word segmentation algo-rithms in three different languages.1 In t roduct ionThis paper presents a trainable rule-based algorithmfor performing word segmentation.
Our algorithm iseffective both as a high-accuracy stand-alone seg-menter and as a postprocessor that improves theoutput of existing word segmentation algorithms.In the writing systems of many languages, includ-ing Chinese, Japanese, and Thai, words are not de-limited by spaces.
Determining the word bound-aries, thus tokenizing the text, is usually one of thefirst necessary processing steps, making tasks such aspart-of-speech tagging and parsing possible.
A vari-ety of methods have recently been developed to per-form word segmentation and the results have beenpublished widely.
1A major difficulty in evaluating segmentation al-gorithms is that there are no widely-accepted guide-lines as to what constitutes a word, and there istherefore no agreement on how to "correctly" seg-ment a text in an unsegmented language.
It is1Most published segmentation work has been done forChinese.
For a discussion of recent Chinese segmentationwork, see Sproat et al (1996).frequently mentioned in segmentation papers thatnative speakers of a language do not always agreeabout the "correct" segmentation a d that the sametext could be segmented into several very different(and equally correct) sets of words by different na-tive speakers.
Sproat et a1.
(1996) and Wu and Fung(1994) give empirical results howing that an agree-ment rate between ative speakers as low as 75% iscommon.
Consequently, an algorithm which scoresextremely well compared to one native segmentationmay score dismally compared to other, equally "cor-rect" segmentations.
We will discuss some other is-sues in evaluating word segmentation i  Section 3.1.One solution to the problem of multiple correctsegmentations might be to establish specific guide-lines for what is and is not a word in unsegmentedlanguages.
Given these guidelines, all corpora couldtheoretically be uniformly segmented according tothe same conventions, and we could directly compareexisting methods on the same corpora.
While thisapproach as been successful in driving progress inNLP tasks such as part-of-speech tagging and pars-ing, there are valid arguments against adopting itfor word segmentation.
For example, since word seg-mentation is merely a preprocessing task for a widevariety of further tasks such as parsing, informationextraction, and information retrieval, different seg-mentations can be useful or even essential for thedifferent asks.
In this sense, word segmentation issimilar to speech recognition, in which a system mustbe robust enough to adapt to and recognize the mul-tiple speaker-dependent "correct" pronunciations ofwords.
In some cases, it may also be necessary toallow multiple "correct" segmentations of the sametext, depending on the requirements of further pro-cessing steps.
However, many algorithms use exten-sive domain-specific word lists and intricate namerecognition routines as well as hard-coded morpho-logical analysis modules to produce a predeterminedsegmentation output.
Modifying or retargeting an321existing segmentation algorithm to produce a differ-ent segmentation can be difficult, especially if it isnot clear what and where the systematic differencesin segmentation are.It is widely reported in word segmentationpapers, 2 that the greatest barrier to accurate wordsegmentation is in recognizing words that are not inthe lexicon of the segmenter.
Such a problem is de-pendent both on the source of the lexicon as well asthe correspondence (in vocabulary) between the textin question and the lexicon.
Wu and Fung (1994)demonstrate that segmentation accuracy is signifi-cantly higher when the lexicon is constructed usingthe same type of corpus as the corpus on which itis tested.
We argue that rather than attempting toconstruct a single exhaustive l xicon or even a seriesof domain-specific lexica, it is more practical to de-velop a robust trainable means of compensating forlexicon inadequacies.
Furthermore, developing suchan algorithm will allow us to perform segmentationin many different languages without requiring ex-tensive morphological resources and domain-specificlexica in any single language.For these reasons, we address the problem of wordsegmentation from a different direction.
We intro-duce a rule-based algorithm which can produce anaccurate segmentation f a text, given a rudimentaryinitial approximation to the segmentation.
Recog-nizing the utility of multiple correct segmentations ofthe same text, our algorithm also allows the outputof a wide variety of existing segmentation algorithmsto be adapted to different segmentation schemes.
Inaddition, our rule-based algorithm can also be usedto supplement the segmentation of an existing al-gorithm in order to compensate for an incompletelexicon.
Our algorithm is trainable and language in-dependent, so it can be used with any unsegmentedlanguage.2 T rans format ion-basedSegmentat ionThe key component of our trainable segmenta-tion algorithm is Transformation-based Error-drivenLearning, the corpus-based language processingmethod introduced by Brill (1993a).
This techniqueprovides a simple algorithm for learning a sequenceof rules that can be applied to various NLP tasks.It differs from other common corpus-based methodsin several ways.
For one, it is weakly statistical, butnot probabilistic; transformation-based approachesconseo,:~,tly require far less training data than mosto;a~is~ical pproaches.
It is rule-based, but relies on2See, for example, Sproat et al (1996).machine learning to acquire the rules, rather thanexpensive manual knowledge ngineering.
The rulesproduced can be inspected, which is useful for gain-ing insight into the nature of the rule sequence andfor manual improvement and debugging of the se-quence.
The learning algorithm also considers theentire training set at all learning steps, rather thandecreasing the size of the training data as learningprogresses, such as is the case in decision-tree in-duction (Quinlan, 1986).
For a thorough discussionof transformation-based l arning, see Ramshaw andMarcus (1996).Brill's work provides a proof of viability oftransformation-based techniques in the form ofa number of processors, including a (widely-distributed) part-of-speech tagger (Brill, 1994),a procedure for prepositional phrase attachment(Brill and Resnik, 1994), and a bracketing parser(Brill, 1993b).
All of these provided performancecomparable to or better than previous attempts.Transformation-based l arning has also been suc-cessfully applied to text chunking (Ramshawand Marcus, 1995), morphological disambiguation(Oflazer and Tur, 1996), and phrase parsing (Vilainand Day, 1996).2.1 Tra in ingWord segmentation can easily be cast as atransformation-based problem, which requires aninitial model, a goal state into which we wish totransform the initial model (the "gold standard"),and a series of transformations to effect his improve-ment.
The transformation-based algorithm involvesapplying and scoring all the possible rules to train-ing data and determining which rule improves themodel the most.
This rule is then applied to all ap-plicable sentences, and the process is repeated untilno rule improves the score of the training data.
Inthis manner asequence of rules is built for iterativelyimproving the initial model.
Evaluation of the rulesequence is carried out on a test set of data which isindependent of the training data.If we treat the output of an existing segmentationalgorithm 3 as the initial state and the desired seg-mentation as the goal state, we can perform a seriesof transformations onthe initial state - removing ex-traneous boundaries and inserting new boundaries -to obtain a more accurate approximation of the goalstate.
We therefore need only define an appropriaterule syntax for transforming this initial approxima-3The "existing" algorithm does not need to be a largeor even accurate system; the algorithm can be arbi-trarily simple as long as it assigns ome form of initialsegmentation.322tion and prepare appropriate training data.For our experiments, we obtained corpora whichhad been manually segmented by native or near-native speakers of Chinese and Thai.
We divided thehand-segmented data randomly into training andtest sets.
Roughly 80% of the data was used totrain the segmentation algorithm, and 20% was usedas a blind test set to score the rules learned fromthe training data.
In addition to Chinese and Thai,we also performed segmentation experiments usinga large corpus of English in which all the spaces hadbeen removed from the texts.
Most of our Englishexperiments were performed using training and testsets with roughly the same 80-20 ratio, but in Sec-tion 3.4.3 we discuss results of English experimentswith different amounts of training data.
Unfortu-nately, we could not repeat these experiments withChinese and Thai due to the small amount of hand-segmented ata available.2.2 Ru le  syntaxThere are three main types of transformations whichcan act on the current state of an imperfect segmen-tation:?
Insert - place a new boundary between two char-acters?
Delete - remove an existing boundary betweentwo characters?
Slide - move an existing boundary from its cur-rent location between two characters to a loca-tion 1, 2, or 3 characters to the left or right 4In our syntax, Insert and Delete transformationscan be triggered by any two adjacent characters (abigram) and one character to the left or right of thebigram.
Slide transformations can be triggered by asequence of one, two, or three characters over whichthe boundary is to be moved.
Figure 1 enumeratesthe 22 segmentation transformations we define.3 Resu l tsWith the above algorithm in place, we can use thetraining data to produce a rule sequence to augmentan initial segmentation approximation i order toobtain a better approximation f the desired segmen-tation.
Furthermore, since all the rules are purelycharacter-based, a sequence can be learned for anycharacter set and thus any language.
We used ourrule-based algorithm to improve the word segmen-tation rate for several segmentation algorithms inthree languages.4Note that a Slide transformation is equivalent to aDelete plus an Insert.3.1 Eva luat ion  o f  segmentat ionDespite the number of papers on the topic, the eval-uation and comparison of existing segmentation al-gorithms is virtually impossible.
In addition to theproblem of multiple correct segmentations of thesame texts, the comparison of algorithms is diffi-cult because of the lack of a single metric for re-porting scores.
Two common measures of perfor-mance are recall and precision, where recall is de-fined as the percent of words in the hand-segmentedtext identified by the segmentation algorithm, andprecision is defined as the percentage of words re-turned by the algorithm that also occurred in thehand-segmented text in the same position.
The com-ponent recall and precision scores are then used tocalculate an F-measure (Rijsbergen, 1979), whereF = (1 +/~)PR/ (~P + R).
In this paper we willreport all scores as a balanced F-measure (precisionand recall weighted equally) with/~ = 1, such thatF = 2PR/(P + R)3.2 Ch ineseFor our Chinese experiments, the training set con-sisted of 2000 sentences (60187 words) from a Xin-hun news agency corpus; the test set was a separateset of 560 sentences (18783 words) from the samecorpus.
5 We ran four experiments using this corpus,with four different algorithms providing the startingpoint for the learning of the segmentation transfor-mations.
In each case, the rule sequence learnedfrom the training set resulted in a significant im-provement in the segmentation of the test set.3.2.1 Character -as -word  (CAW)A very simple initial segmentation for Chinese isto consider each character a distinct word.
Sincethe average word length is quite short in Chinese,with most words containing only 1 or 2 characters, 6this character-as-word segmentation correctly iden-tified many one-character words and produced aninitial segmentation score of F=40.3.
While this isa low segmentation score, this segmentation algo-rithm identifies enough words to provide a reason-able initial segmentation approximation.
In fact, theCAW algorithm alone has been shown (Buckley etal., 1996; Broglio et al, 1996) to be adequate to beused successfully in Chinese information retrieval.Our algorithm learned 5903 transformations fromthe 2000 sentence training set.
The 5903 transfor-mations applied to the test set improved the scorefrom F=40.3 to 78.1, a 63.3% reduction in the error5The Chinese texts were prepared by Tom Keenan.6The average length of a word in our Chinese datawas 1.60 characters.323Boundary TriggeringAction Context RulexABC y ~ x ABCyAB ?==~ A B Insert (delete) between A and B anyxB ?=:?, x B Insert (delete) before any B anyAy ~ A y Insert (delete) after any A anyABC ~ A B C Insert (delete) between A and B anyAND Insert (delete) between B and CJAB ~ JAB  Insert (delete) between A and B J to left of A--JAB ~ -~JA B Insert (delete) between A and B no J to left of AABK ~ A BK Insert (delete) between A and B K to right of BAB~K ~ A B-~K Insert (delete) between A and B no K to right of BxA y ~ x Ay Move from after A to before A anyxAB y ~==e, x ABy Move from after bigram AB to before AB anyMove from after trigram ABC to before ABC anyFigure 1: Possible transformations.
A, B, C, J, and K are specific characters; x and y can be any character.~J and ~K can be any character except J and K, respectively.rate.
This is a very surprising and encouraging re-sult, in that, from a very naive initial approximationusing no lexicon except hat implicit from the train-ing data, our rule-based algorithm is able to producea series of transformations with a high segmentationaccuracy.3.2.2 Max imum match ing  (greedy)a lgor i thmA common approach to word segmentation is touse a variation of the maximum atching algorithm,frequently referred to as the "greedy algorithm.
"The greedy algorithm starts at the first characterin a text and, using a word list for the language be-ing segmented, attempts to find the longest word inthe list starting with that character.
If a word isfound, the maximum-matching algorithm marks aboundary at the end of the longest word, then be-gins the same longest match search starting at thecharacter following the match.
If no match is foundin the word list, the greedy algorithm simply skipsthat character and begins the search starting at thenext character.
In this manner, an initial segmen-tation can be obtained that is more informed thana simple character-as-word approach.
We appliedthe maximum matching algorithm to the test setusing a list of 57472 Chinese words from the NMSUCHSEG segmenter (described in the next section).This greedy algorithm produced an initial score ofF=64.4.A sequence of 2897 transformations was learned ?from the training set; applied to the test set, theyimproved the score from F=64.4 to 84.9, a 57.8%error reduction.
From a simple Chinese word list,the rule-based algorithm was thus able to produce a-segmentation score comparable to segmentation al-gorithms developed with a large amount of domainknowledge (as we will see in the next section).This score was improved further when combin-ing the character-as-word (CAW) and the maximummatching algorithms.
In the maximum matching al-gorithm described above, when a sequence of char-acters occurred in the text, and no subset of thesequence was present in the word list, the entiresequence was treated as a single word.
This of-ten resulted in words containing 10 or more char-acters, which is very unlikely in Chinese.
In thisexperiment, when such a sequence of characters wasencountered, each of the characters was treated asa separate word, as in the CAW algorithm above.This variation of the greedy algorithm, using thesame list of 57472 words, produced an initial scoreof F=82.9.
A sequence of 2450 transformations waslearned from the training set; applied to the testset, they improved the score from F=82.9 to 87.7,a 28.1% error reduction.
The score produced usingthis variation of the maximum matching algorithmcombined with a rule sequence (87.7) is nearly equalto the score produced by the NMSU segmenter seg-menter (87.9) discussed in the next section.3.2 .3  NMSU segmenterThe previous three experiments showed that ourrule sequence algorithm can produce excellent seg-mentation results given very simple initial segmen-tation algorithms.
However, assisting in the adapta-tion of an existing algorithm to different segmenta-tion schemes, as discussed in Section 1, would mostlikely be performed with an already accurate, fully-developed algorithm.
In this experiment we demon-324strate that our algorithm can also improve the out-put of such a system.The Chinese segmenter CHSEG developed at theComputing Research Laboratory at New MexicoState University is a complete system for high-accuracy Chinese segmentation (Jin, 1994).
In ad-dition to an initial segmentation module that findswords in a text based on a list of Chinese words,CHSEG additionally contains specific modules forrecognizing idiomatic expressions, derived words,Chinese person names, and foreign proper names.The accuracy of CHSEG on an 8.6MB corpus hasbeen independently reported as F=84.0 (Ponte andCroft, 1996).
(For reference, Ponte and Croft re-port scores of F=86.1 and 83.6 for their probabilis-tic Chinese segmentation algorithms trained on over100MB of data.
)On our test set, CHSEG produced a segmentationscore of F=87.9.
Our rule-based algorithm learned asequence of 1755 transformations from the trainingset; applied to the test set, they improved the scorefrom 87.9 to 89.6, a 14.0% reduction in the error rate.Our rule-based algorithm is thus able to produce animprovement to an existing high-performance sys-tem.Table 1 shows a summary of the four Chinese ex-periments.3.3 Tha iWhile Thai is also an unsegmented language, theThai writing system is alphabetic and the averageword length is greater than Chinese.
~ We wouldtherefore xpect that our character-based transfor-mations would not work as well with Thai, since acontext of more than one character is necessary inmany cases to make many segmentation decisions inalphabetic languages.The Thai corpus consisted of texts from the ThaiNews Agency via NECTEC in Thailand.
For ourexperiment, he training set consisted of 3367 sen-tences (40937 words); the test set was a separateset of 1245 sentences (13724 words) from the samecorpus.The initial segmentation was performed using themaximum matching algorithm, with a lexicon of9933 Thai words from the word separation filterin ctte~,a Thai language Latex package.
Thisgreedy algorithm gave an initial segmentation scoreof F=48.2 on the test set.7The average length of a word in our Thai data was5.01 characters.8The Thai texts were manually segmented by 3oTyler.Our rule-based algorithm learned a sequence of731 transformations which improved the score from48.2 to 63.6, a 29.7% error reduction.
While thealphabetic system is obviously harder to segment,we still see a significant reduction in the segmentererror rate using the transformation-based algorithm.Nevertheless, it is doubtful that a segmentation witha score of 63.6 would be useful in too many appli-cations, and this result will need to be significantlyimproved.3.4 De-segmented  Engl ishAlthough English is not an unsegmented language,the writing system is alphabetic like Thai and theaverage word length is similar.
9 Since English lan-guage resources (e.g.
word lists and morphologicalanalyzers) are more readily available, it is instruc-tive to experiment with a de-segmented English cor-pus, that is, English texts in which the spaces havebeen removed and word boundaries are not explic-itly indicated.
The following shows an example ofan English sentence and its de-segmented version:About 20,000 years ago the last ice age ended.About20,000yearsagothelasticeageended.The results of such experiments can help us deter-mine which resources need to be compiled in order todevelop a high-accuracy segmentation algorithm inunsegmented alphabetic languages such as Thai.
Inaddition, we are also able to provide a more detailederror analysis of the English segmentation (since theauthor can read English but not Thai).Our English experiments were performed using acorpus of texts from the Wall Street Journal (WSJ).The training set consisted of 2675 sentences (64632words) in which all the spaces had been removed; thetest set was a separate set of 700 sentences (16318words) from the same corpus (also with all spacesremoved).3.4.1 Max imum match ing  exper imentFor an initial experiment, segmentation was per-formed using the maximum matching algorithm,with a large lexicon of 34272 English words com-piled from the WSJ.
l?
In contrast o the low initialThai score, the greedy algorithm gave an initial En-glish segmentation score of F=73.2.
Our rule-basedalgorithm learned a sequence of 800 transformations,9The average length of a word in our English datawas 4.46. characters, compared to 5.01 for Thai and 1.60for Chinese.1?Note that the portion of the WSJ corpus used tocompile the word list was independent of both the train-ing and test sets used in the segmentation experiments.325InitialalgorithmCharacter-as-wordMaximum matchingMaximum matching + CAWNMSU segmenterl Initial I Rulesscore learned40.3 590364.4 289782.9 245087.9 1755Improved Iscore78.184.987.789.6Errorreduction63.3%57.8%28.1%14.0%Table 1: Chinese results.which improved the score from 73.2 to 79.0, a 21.6%error reduction.The difference in the greedy scores for English andThai demonstrates the dependence on the word listin the greedy algorithm.
For example, an exper-iment in which we randomly removed half of thewords from the English list reduced the performanceof the greedy algorithm from 73.2 to 32.3; althoughthis reduced English word list was nearly twice thesize of the Thai word list (17136 vs. 9939), thelongest match segmentation utilizing the list wasmuch lower (32.3 vs. 48.2).
Successive xperimentsin which we removed ifferent random sets of halfthe words from the original list resulted in greedyalgorithm performance of 39.2, 35.1, and 35.5.
Yet,despite the disparity in initial segmentation scores,the transformation sequences effect a significant er-ror reduction in all cases, which indicates that thetransformation sequences are effectively able to com-pensate (to some extent) for weaknesses in the lexi-con.
Table 2 provides a summary of the results usingthe greedy algorithm for each of the three languages.3.4.2 Basic morpholog ica l  segmentat ionexper imentAs mentioned above, lexical resources are morereadily available for English than for Thai.
Wecan use these resources to provide an informed ini-tial segmentation approximation separate from thegreedy algorithm.
Using our native knowledge ofEnglish as well as a short list of common Englishprefixes and suffixes, we developed a simple al-gorithm for initial segmentation of English whichplaced boundaries after any of the suffixes and beforeany of the prefixes, as well as segmenting punctua-tion characters.
In most cases, this simple approachwas able to locate only one of the two necessaryboundaries for recognizing full words, and the ini-tial score was understandably low, F=29.8.
Never-theless, even from this flawed initial approximation,our rule-based algorithm learned a sequence of 632transformations which nearly doubled the word re-call, improving the score from 29.8 to 53.3, a 33.5%error reduction.3.4.3 Amount of training dataSince we had a large amount of English data, wealso performed a classic experiment to determine theeffect the amount of training data had on the abil-ity of the rule sequences to improve segmentation.We started with a training set only slightly largerthan the test set, 872 sentences, and repeated themaximum matching experiment described in Section3.4.1.
We then incrementally increased the amountof training data and repeated the experiment.
Theresults, summarized in Table 3, clearly indicate (notsurprisingly) that more training sentences produceboth a longer rule sequence and a larger error re-duction in the test data.Trainingsentences8721731267535724522Ruleslearned4366538009021015Improved Errorscore reduction78.2 18.9%78.9 21.3%79.0 21.6%79.4 23.1%80.3 26.5%Table 3: English training set sizes.
Initial score oftest data (700 sentences) was 73.2.3.4.4 Er ror  analysisUpon inspection of the English segmentation er-rors produced by both the maximum matching algo-rithm and the learned transformation sequences, onemajor category of errors became clear.
Most appar-ent was the fact that the limited context ransforma-tions were unable to recover from many errors intro-duced by the naive maximum matching algorithm.For example, because the greedy algorithm alwayslooks for the longest string of characters which canbe a word, given the character sequence "economicsi-tuation", the greedy algorithm first recognized "eco-nomics" and several shorter words, segmenting thesequence as "economics it u at io n".
Since ourtransformations consider only a single character ofcontext, the learning algorithm was unable to patchthe smaller segments back together to produce thedesired output "economic situation".
In some cases,326LexiconLanguage sizeChinese 57472Chinese (with CAW) 57472Thai 9939English 34272,oitial I I Imp.oved 11 score learned score64.4 2897 84.982.9 2450 87.748.2 731 63.673.2 800 79.0Errorreduction57.8%28.1%29.7%21.6%Table 2: Summary of maximum matching results.the transformations were able to recover some of theword, but were rarely able to produce the full desiredoutput.
For example, in one case the greedy algo-rithm segmented "humanactivity" as "humana c tivi ty".
The rule sequence was able to transform thisinto "humana ctivity", but was not able to producethe desired "human activity".
This suggests thatboth the greedy algorithm and the transformationlearning algorithm need to have a more global wordmodel, with the ability to recognize the impact ofplacing a boundary on the longer sequences of char-acters surrounding that point.4 D iscuss ionThe results of these experiments demonstrate thata transformation-based rule sequence, supplement-ing a rudimentary initial approximation, can pro-duce accurate segmentation.
In addition, they areable to improve the performance of a wide range ofsegmentation algorithms, without requiring expen-sive knowledge ngineering.
Learning the rule se-quences can be achieved in a few hours and requiresno language-specific knowledge.
As discussed in Sec-tion 1, this simple algorithm could be used to adaptthe output of an existing segmentation algorithm todifferent segmentation schemes as well as compen-sating for incomplete segmenter lexica, without re-quiring modifications to segmenters themselves.The rule-based algorithm we developed to improveword segmentation is very effective for segment-ing Chinese; in fact, the rule sequences combinedwith a very simple initial segmentation, such asthat from a maximum matching algorithm, produceperformance comparable to manually-developed s g-menters.
As demonstrated by the experiment withthe NMSU segmenter, the rule sequence algorithmcan also be used to improve the output of an alreadyhighly-accurate segmenter, thus producing one ofthe best segmentation results reported in the litera-ture.In addition to the excellent overall results in Chi-nese segmentation, we also showed the rule sequencealgorithm to be very effective in improving segmen-tation in Thai, an alphabetic language.
While thescores themselves were not as high as the Chineseperformance, the error reduction was neverthelessvery high, which is encouraging considering the sim-ple rule syntax used.
The current state of our algo-rithm, in which only three characters are consideredat a time, will understandably perform better witha language like Chinese than with an alphabetic lan-guage like Thai, where average word length is muchgreater.
The simple syntax described in Section 2.2can, however, be easily extended to consider largercontexts to the left and the right of boundaries; thisextension would necessarily come at a correspondingcost in learning speed since the size of the rule spacesearched uring training would grow accordingly.
Inthe future, we plan to further investigate the ap-plication of our rule-based algorithm to alphabeticlanguages.Acknowledgements  This work would not havebeen possible without the assistance and encour-agement of all the members of the MITRE NaturalLanguage Group.
This paper benefited greatly fromdiscussions with and comments from Marc Vilain,Lynette Hirschman, Sam Bayer, and the anonymousreviewers.Re ferencesEric Brill and Philip Resnik.
1994.
A rule-based ap-proach to prepositional phrase attachment disam-biguation.
In Proceedings of the Fifteenth Interna-tional Conference on Computational Linguistics(COLING-1994).Eric Brill.
1993a.
A corpus-based approach to lan-guage learning.
Ph.D. Dissertation, University ofPennsylvania, Department of Computer and In-formation Science.Eric Brill.
1993b.
Transformation-based rror-driven parsing.
In Proceedings of the Third In-ternational Workshop on Parsing Technologies.Eric Brill.
1994.
Some advances in transformation-based part of speech tagging.
In Proceedings of~he Twelfth National Conference on Artificial In-telligence, pages 722-727.327John Broglio, Jamie Callan, and W. Bruce Croft.1996.
Technical issues in building an informationretrieval system for chinese.
CIIR Technical Re-port IR-86, University of Massachusetts, Amherst.Chris Buckley, Amit Singhal, and Mandar Mitra.1996.
Using query zoning and correlation withinsmart: Trec 5.
In Proceedings of the Fifth TextRetrieval Conference (TREC-5).Wanying Jin.
1994.
Chinese segmentation disam-biguation.
In Proceedings of the Fifteenth Interna.tional Conference on Computational Linguistics(COLING-94), Japan.Judith L. Klavans and Philip P~snik.
1996.
TheBalancing Act: Combining Symbolic and Statis-tical Approaches to Language.
MIT Press, Cam-bridge, MA.Kemal Oflazer and Gokhan Tur.
1996.
Combin-ing hand-crafted rules and unsupervised learn-ing in constraint-based morphological disambigua-tion.
In Proceedings of the Conference on Empir-ical Methods in Language Processing (EMNLP).Jay M. Ponte and W. Bruce Croft.
1996.
Useg:A retargetable word segmentation procedure forinformation retrieval.
In Proceedings of SDAIR96,Las Vegas, Nevada.J.R.
Quinlan.
1986.
Induction of decision trees.
Ma-chine Learning, 1(1):81-106.Lance Ramshaw and Mitchell Marcus.
1995.
Textchunking using transformation-based learning.
InProceedings of the Third Workshop on Very LargeCorpora (WVLC-3), pages 82-94.Lance A. Ramshaw and Mitchell P. Marcus.
1996.Exploring the nature of transformation-basedlearning.
In Klavans and Resnik (1996).C.
J.
Van Rijsbergen.
1979.
Information Retrieval.Butterworths, London.Giorgio Satta and Eric Brill.
1996.
Efficienttransformation-based parsing.
In Proceedings ofthe Thirty-fourth Annual Meeting of the Associa-tion for Computational Linguistics (ACL-96).Richard W. Sprout, Chilin Shih, William Gale, andNancy Chang.
1996.
A stochastic finite-stateword-segmentation algorithm for chinese.
Com-putational Linguistics, 22(3):377-404.Marc Vilain and David Day.
1996.
Finite-statephrase parsing by rule sequences.
In Proceed-ings of the Sixteenth International Conference onComputational Linguistics (COLING-96).Marc Vilain and DavidPalmer.
1996.
Transformation-based bracketing:Fast algorithms and experimental results.
In Pro-ceedings of the Workshop on Robust Parsing, heldat ESSLLI 1996.Dekai Wu and Pascale Fung.
1994.
Improving chi-nese tokenization with linguistic filters on sta-tistical exical acquisition.
In Proceedings of theFourth ACL Conference on Applied Natural Lan-guage Processing (ANLP94), Stuttgart, Germany.Zimin Wu and Gwyneth Tseng.
1993.
Chinese textsegmentation fortext retrieval: Achievements andproblems.
Journal of the American Society forInformation Science, 44(9):532-542.328
