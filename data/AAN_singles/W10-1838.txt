Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 235?242,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Proposal for a Configurable Silver StandardUdo Hahn, Katrin Tomanek, Elena Beisswanger and Erik FaesslerJena University Language & Information Engineering (JULIE) LabFriedrich-Schiller-Universita?t JenaFu?rstengraben 30, 07743 Jena, Germanyhttp://www.julielab.deAbstractAmong the many proposals to promote al-ternatives to costly to create gold stan-dards, just recently the idea of a fully au-tomatically, and thus cheaply, to set up sil-ver standard has been launched.
However,the current construction policy for such asilver standard requires crucial parameters(such as similarity thresholds and agree-ment cut-offs) to be set a priori, based onextensive testing though, at corpus com-pile time.
Accordingly, such a corpus isstatic, once it is released.
We here proposean alternative policy where silver stan-dards can be dynamically optimized andcustomized on demand (given a specificgoal function) using a gold standard as anoracle.1 IntroductionTraining natural language systems which rely on(semi-)supervised machine learning algorithms,or measuring the systems?
performance requiressome standardized ground truth from which onecan learn or against which one evaluate, respec-tively.
Usually, a manually crafted gold stan-dard is provided that is generated by human lan-guage or domain experts after lots of iterative,guideline-based training rounds.
This procedure isexpensive, slow and yields only small, yet highlytrustable, amounts of meta data ?
because humanexperts are in the loop.In the CALBC project,1 an alternative ap-proach is currently under investigation (Rebholz-Schuhmann et al, 2010a).
The basic idea is togenerate the much needed ground truth automati-cally.
This is achieved by letting a flock of namedentity taggers run on a corpus, without impos-ing any restriction on the type(s) being annotated.1http://www.calbc.euThe (most likely) heterogeneous results are auto-matically homogenized subsequently, thus yield-ing a consensus-based, machine-generated groundtruth.
Considering the possible benefits (e.g., thepositive experience from boosting-style machinelearners (Freund, 1990)), but also being aware ofthe possible drawbacks (varying quality of the dif-ferent systems, skewed coverage of entity types,different types of guidelines on which they weretrained, etc.
), the CALBC consortium refers tothe outcome of this process as a silver standard(Rebholz-Schuhmann et al, 2010a).
This proce-dure is inexpensive, fast, yields huge amounts ofmeta data ?
because computers are in the loop ?but after all its applicability and validity has yet tobe determined experimentally.The first silver standard corpus (SSC) that cameout of the CALBC project was generated by thefour main partners?
named entity taggers.2 Thevarious contributions covered, among others, an-notations for genes and proteins, chemicals, dis-eases, etc (Rebholz-Schuhmann et al, 2010b).
Af-ter the submission of their runs, the SSC was gen-erated by, first, harmonizing stretches of text interms of entity mention identification and, second,by mapping these normalized mentions to agreed-upon type systems (such as the MESH SemanticGroups as described by Bodenreider and McCray(2003) for entity type normalization).
Basically,the harmonization steps included rules when en-tity mentions were considered to match or overlap(using a cosine-based similarity criterion) and en-tity types referred to the same class.
For consensusgeneration, finally, simple rules for majority voteswere established.The CALBC consortium is fully aware of thefact that the value of an SSC can only be assessed2The CALBC consortium consists the Rebholz Groupfrom EBI (Hinxton, U.K.), the Biosemantics Group fromErasmus (Rotterdam, The Netherlands), the JULIE Lab (Jena,Germany), and LINGUAMATICS (Cambridge, U.K.).235by comparing, e.g., systems trained on such a sil-ver standard with systems trained on a gold stan-dard (preferably, though not necessarily, one thatis a subset of the document set which makes up theSSC).In the absence of such a gold standard, theCALBC consortium has spent enormous efforts tofind out the most reasonable parameter settingsfor, e.g., the cosine threshold (setting similar men-tions apart from dissimilar ones) or the consen-sus constraint (where a certain number of entitytypes equally assigned by different taggers makesone type the consensual silver one and discards allalternative annotations).
Once these criteria aremade effective, the SSC is completely fixed.As an alternative, we are looking for a moreflexible solution.
Our investigation was fuelled bythe following observations:?
The idiosyncrasies of guidelines (on which(some) taggers were trained) do not necessar-ily lead to semantically totally different enti-ties although they differ literally to some de-gree.
Some guidelines prefer, e.g., ?humanIL-7 protein?, others favor ?human IL-7?,and some lean towards ?IL-7?.
As the cosinemeasure tends to penalize a pair such as ?hu-man IL-7 protein?
and ?IL-7?, we intendedto avoid such a prescriptive mode and justlook at the type assignment for single tokensas (parts of) entity mentions.
thus avoidinginconclusive mention boundary discussions.?
While we were counting, for all tokens ofthe document set, the votes a single token re-ceived from different taggers in terms of an-notating this token with respect to some type,we generated confidence data for meta dataassignments.
Incorporating the distributionof confidence values into the configurationprocess, this allows us to get rid of a pri-ori fixed majority criteria (e.g., two or threeout of five systems must agree on this token)which are hard to justify in an absolute way.Summarizing, we believe that the nature of di-verging tasks to be solved, the levels of entity typespecificity to be reached, the sort of guidelines be-ing preferred, etc.
should allow prospective usersof a silver standard to customize one on their ownand not stick to one that is already prefabricatedwithout concrete application in mind.33There may be tasks where a ?long?
entity such as ?hu-As such an enterprise would be quite arbitrarywithout a reference standard, we even go one stepfurther.
We determine the suitability of, say, dif-ferent voting scores and varying lexical extensionsof mentions by comparison to a gold standard sothat the ?optimal?
configuration of a silver stan-dard, given a set of goal-derived requirements,can be automatically learned.
In real-world ap-plications, such gold standard annotations wouldbe delivered only for a fraction of the documentscontained in the entire corpus being tagged by aflock of taggers.
The gold standard is used to op-timize parameters which are subsequently appliedto the aggregation of automatically annotated data.Note that the gold standard is used for optimiza-tion only, not for training.
We call such a flexible,dynamically adjustable silver standard a config-urable Silver Standard Corpus (conSSC).
In a sec-ond step, we split the various conSSCs, re-trainedour NER tagger on these data sets and, by compar-ison with the gold standard, were able to identifythe optimal conSSC for this task (which is not theone (SSC I) made available by the CALBC consor-tium for the first challenge round).42 Optimizing Silver StandardsIn this section, we describe the constituent param-eters of a wide spectrum of SSCs.
Mostly, theseparameters were taken over from the design of theSSC as developed by the CALBC project members.Differing from that fixed SSC, we investigate theimpact of different parameter settings on the con-struction of a collection of SSCs, and, first, eval-uate their direct usefulness on a gold standard forprotein-gene annotations.
Second, we also assesstheir indirect usefulness by training NER classi-fiers on these SSCs and evaluate the NERs?
perfor-mance on the gold standard.
Thus, our approachis entirely data-driven without the need for humanintervention in terms of choosing suitable param-eter settings.Technically, we first aggregate the votes fromthe flock of taggers (in our experiments, we usedthe four taggers from the CALBC project membersplus a second tagger of one of the members) foreach text token (for confidence-based decisions)or at the entity level (for cosine-based decisions),then we determine the confidence values of theseman IL-7 protein?
may be appropriate, while for another taska short one such as ?IL-7?
is entirely sufficient.4http://www.ebi.ac.uk/Rebholz-srv/CALBC/challenge.html236aggregated votes, and, finally, we compute thesimilarity of the various SSCs with the gold stan-dard data in terms of F-scores (both exact and openboundaries) and accuracy on the token level.2.1 Calibrating ConsensusThe metrical interpretation of consensus will bebased on thresholded votes for semantic groups atthe token level (cf.
Section 2.1.1) and a cosine-based measure to determine contiguous stretchesof entity mentions in the text (cf.
Section 2.1.2).2.1.1 Type Confidence and Type VotingFor each text token, we determine the entity typeassignment as generated by each NER taggerwhich is part of the flock of CALBC taggers.5 Wecount and aggregate these votes such that each en-tity type has an associated type count value.We then compute the ratio of systems agree-ing on the same single type assignment and callthis the confidence attributed to a particular typefor some token.
The confidence value will sub-sequently be interpreted against the confidencethreshold [0, 1] that defines a measure of certaintya type assignment should have in order to be ac-cepted as consensual.2.1.2 Cosine-based Similarity of PhrasalEntity MentionsAs the above policy of token-wise annotation de-couples contiguous entity mentions spanning overmore than one token, we also want to restitute thisphrasal structure.
This is achieved by constructingcontiguous sequences of tokens that characterize aphrasal entity mention at the text level to which thesame type label has been assigned.
Since differ-ent taggers tend to identify different spans of textfor the same entity type (as shown in the exam-ple from Section 1) we have to account for similarphrasal forms of named entity mentions.This is achieved by constructing vectors whichrepresent entity mentions and by computing thecosine between the different entity mention vec-tors.
Let E1 = T1T2T3 be an entity mention com-prised of three tokens T1 to T3.
Let E2 = T2T3 be5Due to time constraints when we performed our experi-ments, we make an extremely simplifying assumption: Fromthe whole range of possible entity types NER taggers may as-sign to some token (cf.
(Bodenreider and McCray, 2003)) wehave chosen the PRotein/GEne group for testing.
Still, thisassumption does not do harm to the core of our hypotheses.See also our discussion in Section 5.an entity mention overlapping with E1 in the to-kens T2 and T3.
To decide whether E1 and E2 areconsidered similar, we first construct two vectorsrepresenting the entity mentions:v(E1) = (f1, f2, f3)Twith fi = IDF (Ti) being the inverse documentfrequency of the token Ti.
We compute the in-verse document frequency of tokens based on thecorpus which is subject to analysis.
Analogously,we construct the vector for E2v(E2) = (0, f2, f3)Tfilling in a zero for the IDF of T1 since it is notcovered by E2.
The entity mentions E1 and E2are considered equal or similar, if the cosine ofthe two vectors is greater or equal a given thresh-old, cos(v(E1), v(E2)) ?
threshold.6 We thencompute the number of systems considering an en-tity annotation as similar in the manner describedabove.
The annotation is accepted and thus en-tered into the SSC, if a particular number of sys-tems agree on one annotation.
This approach waspreviously developed by the CALBC project part-ners (Rebholz-Schuhmann et al, 2010a).The number of agreeing systems and the thresh-old are the free parameters of this method and thussubject to optimization.2.2 Optimization of Silver Standard CorporaIn the experiments described in the next section,we will consider alternative parametrizations forSilver Standard Corpora, i.e., the required confi-dence threshold or cosine threshold and the num-ber of agreeing systems.
We will then discuss twovariants for optimizing this collection of SSCs.The first one directly uses the gold standard for op-timization.
The task will be to find that particularparameter setting for an SSC which best fits thedata contained in the gold standard.
Once theseparameters are determined they can be applied tothe complete CALBC document set (composed of100,000 documents) to produce the final, quasi-optimal SSC.In another variant, we insert a classifier into thisloop.
First, we train a classifier on a particular6For final corpus creation, it must be decided which of thematching entity mentions is entered into the reference SSC,e.g.
the longest or shortest entity annotation.
In our exper-iments, we always chose the shortest entity mention.
How-ever, preliminary experiments showed that the differences totaking the longest entity mention were marginal.237SSC that is built from a particular parameter com-bination.
Next, this classifier is tested against thegold standard.
This is iterated through all parame-ter combinations.
Obviously, the best performingclassifier relative to the gold standard selects theoptimal SSC.3 Experimental Setting3.1 Gold StandardWe generated a new broad-coverage corpus com-posed of 3,236 MEDLINE abstracts (35,519 sen-tences or 941,890 tokens) dealing with geneand protein mentions.
Altogether, it comprises57,889 named entity type annotations annotatedby one expert biologist.
We created this new re-source to have a consistent and (as far as pos-sible) subdomain-independent protein-annotatedcorpus.7MEDLINE abstracts were annotated with (pro-tein coding) genes, mRNAs and proteins.
Adistinction was made between dedicated proteinsas they are recorded in the protein databaseUNIPROT,8 protein complexes consisting of sev-eral protein subunits (e.g., IL-2 receptor consist-ing of ?, ?, and ?
chain), and protein families orgroups (e.g., ?transcription factors?).
Also enu-merations of proteins and protein variants were an-notated.
Discontinuous annotations were avoidedas well as nested annotations (annotations embed-ded in other annotations).
However, gene/proteinmentions nested in terms other than gene/proteinmentions were annotated (e.g., protein mentionsnested in protein function descriptions such as?ligase?
in ?ligase activity?).
Modifiers such asspecies designators were excluded from annota-tions whenever possible.
Gene segments or pro-tein fragments were also not annotated.For our experiments, we did not distinguish be-tween the different annotation classes (see Table1) but merged all available annotations into oneclass, viz.
PRotein/GEne (PRGE).3.2 Automatic Annotation of the Gold StandardWe then asked all four sites participating in theCALBC project to automatically annotate the givengold standard (made available without gold data,7We are aware of other gene/protein-annotated corporasuch as PENNBIOIE (http://bioie.ldc.upenn.edu/) or GENIA (http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi) that willhave to be taken into account in future studies as well.8http://www.uniprot.org/semantic type descriptionT028 Gene or GenomeT086 Nucleotide SequenceT087 Amino Acid Sequence,Amino Acid, PeptideT116 ProteinT126 EnzymeT192 ReceptorTable 1: Semantic types defining the PRGE group(semantic type codes refer to the UMLS).of course) using the same type of named entity tag-ging machinery as was used to annotate CALBC?scanonical SSC.
The performance results of eachgroup?s system evaluated against the gold standardare reported in Table 2.
The data of each systemconstitute the reference data sets and raw data forall subsequent experiments on the configurationand optimization of the silver standard.The resulting raw material does thus not onlycontain gene/protein annotations but also anyother entity types as supplied by the partners.For our experiments on the gold standard, how-ever, only the entity types subsumed by the PRGEgroup (see Table 1) were considered and annota-tions of all other types were discarded.
The def-inition of the PRGE group is identical to the oneproposed by Rebholz-Schuhmann et al (2010a).For the experiments, the specific semantic types(e.g., the UMLS concepts)9 were not considered,only the semantic group PRGE was.3.3 Evaluation MetricsThe following metrics were used to evaluate howgood the silver standard(s) fit(s) the provided goldstandard:?
segment-level recall, precision, and F-scorevalues with exact boundaries, the standardway to evaluate NER taggers,?
segment-level recall, precision, and F-score,but with relaxed boundary constraints.
Thismeans that two entity mentions are consid-ered to match when they overlap with at leastone token and have the same entity type as-signed to them,?
accuracy measured on the token level.These metrics can be considered as optimizationcriteria.9http://www.nlm.nih.gov/research/umls/2383.4 TokenizationThe CALBC partners?
data do not necessarilycome with tokenization information and, more-over, different partners/systems might have differ-ent tokenizations.
Since a common ground forcomparison is thus lacking we added a new, con-sistent tokenization based on the JULIE Lab tok-enizer (Tomanek et al, 2007b).
This tokenizer isoptimized for biomedical documents with intrinsicfocus to keep complex biological terminologicalunits (such as ?IL-2?)
unsegmented, but to splitup tokens that are not terminologically connected(such as dividing ?IL-2-related?
up into ?IL-2?,?-?
and ?related?).
As a matter of fact, entityboundaries do not necessarily coincide with tokenboundaries.
Our solution to this problem is as fol-lows: Whenever a token partially overlaps with anentity name, the full form of that token is consid-ered to be associated with this entity.
All data onwhich we report here (silver and gold standards)obey to this tokenization scheme.3.5 Parameters Being TestedThe following parameter settings were consideredin our experiments:?
Four different values for confidence thresh-olds indicating that 20% (0.2), 40% (0.4),60% (0.6) or 80% (0.8) of all taggers agreedon the same type annotation, viz.
PRGE,?
Five different values for cosine thresholdsto identify overlapping entity mentions, viz.
(0.7, 0.8, 0.9, 0.95, 0.975), and two differentvalues for the number n of agreeing taggers,viz.
n ?
2 and n ?
3,?
Two tagger crowd scenarios, viz.
one whereall five systems were involved, the otherwhere subsets of cardinality 2 of thesecrowds were re-combined.104 ResultsAs already described in Section 2.2, we performedtwo types of experiments.
In the first experiment(Section 4.1), we intend to find proper calibrationsof parameters for an optimal SSC as described inSection 3.5.
In the second experiment (Section4.2), we incorporate an extrinsic task, training anNER classifier on different parameter settings, asa selector for the optimal SSC.10We refrained from also testing combinations of 3 and 4systems due to time constraints.4.1 Intrinsic Calibration of ParametersFull Merger of All Taggers.
In this scenario,we tested the merged results of the entire crowd ofCALBC taggers when compared to the gold stan-dard and determined their performance scores (seeTable 3).
We will discuss the results with respectto the overlapping F-score, if not explicitly statedotherwise.Looking at the results of the runs involving dif-ferent cosine thresholds, we witness a systematicdrawback when more than two systems are re-quired to agree.
Although precision is boosted inthis setting, recall is decreasing strongly which re-sults in overall lower F-scores.
When only twosystems are required to agree a comparativelyhigher recall comes at the cost of lower preci-sion.
Yet, the F-score (both under exact as wellas overlap conditions) is always superior (rangingbetween 75% and 73%) when compared to the 3-agreement scenario.
Note that the 2-agreementcondition for the highest threshold being testedyields, without exception, better scores than thebest single system (cf.
Table 2).The best performing run in terms of F-score forthe confidence method results from a threshold of0.2 with an F-score of 76%.
Note that this F-score lies 4 percentage points above the best per-formance of a single system (cf.
Table 2).A threshold of 0.2 with five contributing sys-tems results in a union of all annotations.
Conse-quently, this run benefits from a high recall com-pared with the other runs.
However, the run ex-hibits the lowest precision rating (both for the ex-act and overlap condition), which is due to the lowthreshold being chosen.
As can also be seen withthe confidence method at a threshold of 0.80, avery high precision can be reached (99%) but atthe cost of extremely low recall.11 The methodsperforming best in terms of overlapping F-scorealso perform best in terms of exact F-score.Selected Tagger Combinations: Twin Taggers.In this scenario, we evaluated all twin combina-tions of taggers against the gold standard regard-ing the confidence criterion.
In Table 4 we contrastthe two best performing and the two worst per-forming tagger pairs for the confidence method.The table reveals that there are some cases wherethe taggers seem to complement each other, e.g.,the twins SYS-1 and SYS-3, as well as SYS-3 and11Exactly these kinds of alternatives offer flexibility forchoosing the most appropriate SSC given a specific task.239exactR exactP exactF overlapR overlapP overlapF systems0.55 0.74 0.63 0.63 0.84 0.72 SYS-10.36 0.53 0.43 0.46 0.68 0.55 SYS-20.48 0.77 0.59 0.59 0.95 0.72 SYS-30.44 0.83 0.58 0.49 0.91 0.64 SYS-40.34 0.61 0.44 0.41 0.74 0.53 SYS-5Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (bestperformance scores in bold face).
Measurements are taken both for exact as well as overlapping recall(R), precision (P) and F-score (F).method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr.
systemscosine 0.94 0.53 0.71 0.61 0.66 0.87 0.75 0.70 2.00cosine 0.93 0.40 0.79 0.53 0.49 0.96 0.65 0.70 3.00cosine 0.94 0.54 0.71 0.61 0.65 0.87 0.74 0.80 2.00cosine 0.93 0.41 0.80 0.54 0.48 0.95 0.64 0.80 3.00cosine 0.94 0.54 0.72 0.62 0.65 0.86 0.74 0.90 2.00cosine 0.93 0.41 0.81 0.54 0.48 0.95 0.64 0.90 3.00cosine 0.94 0.54 0.73 0.62 0.64 0.86 0.74 0.95 2.00cosine 0.93 0.41 0.83 0.55 0.47 0.95 0.63 0.95 3.00cosine 0.94 0.55 0.75 0.64 0.64 0.86 0.73 0.97 2.00cosine 0.93 0.42 0.85 0.56 0.47 0.95 0.63 0.97 3.00confidence 0.95 0.58 0.73 0.65 0.68 0.85 0.76 0.20confidence 0.94 0.44 0.83 0.58 0.50 0.94 0.66 0.40confidence 0.93 0.32 0.88 0.47 0.35 0.97 0.52 0.60confidence 0.91 0.16 0.91 0.27 0.17 0.99 0.30 0.80Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param-eter setting in bold face).
Parameters: threshold (confidence or cosine) and number of agreeing systems(agr.
systems).SYS-4.
In both cases, a confidence threshold of0.2 yields the best F-score.
Additionally, these F-scores (81% and 78%) are even higher than thesingle system?s F-scores (+9% up to +14%).
Thiscomes with a significant increase in recall overboth systems (+13% to +28%) though at the costof lowered precision relative to the system withthe higher precision (?1% to ?10%).
These re-sults also outperform the best results of the exper-imental runs where all systems were involved (seeTable 3).
This indicates that a subset of all systemsmight yield a better SSC than a combination of allsystems?
outputs.4.2 Extrinsic Calibration of ParametersWe employed a standard named entity tagger to as-sess the impact of the different merging strategieson a scenario near to a real-world application.1212This tagger is based on Conditional Random Fields (Laf-ferty et al, 2001) and employs a standard feature set used forEach SSC variant (and thus each parameter com-bination) was evaluated with this tagger in a 10-fold cross validation.
The SSC and the gold corpuswere split into ten parts of equal size.
Nine parts ofthe SSC constituted the training data of one crossvalidation round, the corresponding tenth part ofthe gold standard was used for evaluation.
Thisway, we tested how adequate a merged corpus waswith respect to the training of a classifier.
Becausethe cross validation has been very time consum-ing, we did not consider specific combinations ofsystems but always merged the annotations of allfive systems.
The results are displayed in Table 5.Interestingly, the highest recall, precision, andF-score values (both for the exact and overlap con-dition) are shared by the same parameter combi-nations which also performed best in Section 4.1.Hence, the use of a named entity tagger supportsthe evaluation results when comparing the variousbiomedical entity recognition (Settles, 2004).240ACC exactR exactP exactF overlapR overlapP overlapF systems threshold0.95 0.62 0.69 0.65 0.76 0.85 0.81 SYS-1 + SYS-3 0.200.92 0.22 0.69 0.34 0.26 0.81 0.39 SYS-2 + SYS-5 0.600.95 0.55 0.75 0.63 0.67 0.91 0.78 SYS-3 + SYS-4 0.200.92 0.30 0.85 0.45 0.34 0.94 0.50 SYS-4 + SYS-5 0.60Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairsobtained by the confidence method.method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr.
systemscosine 0.94 0.46 0.69 0.56 0.58 0.86 0.69 0.70 2.00cosine 0.93 0.32 0.77 0.45 0.39 0.94 0.55 0.70 3.00cosine 0.94 0.46 0.69 0.56 0.57 0.86 0.69 0.80 2.00cosine 0.93 0.32 0.78 0.46 0.39 0.94 0.55 0.80 3.00cosine 0.94 0.46 0.70 0.56 0.57 0.85 0.68 0.90 2.00cosine 0.93 0.32 0.79 0.46 0.38 0.93 0.54 0.90 3.00cosine 0.94 0.47 0.71 0.56 0.56 0.85 0.68 0.95 2.00cosine 0.93 0.33 0.80 0.47 0.38 0.93 0.54 0.95 3.00cosine 0.94 0.47 0.73 0.57 0.56 0.85 0.67 0.97 2.00cosine 0.93 0.33 0.82 0.47 0.38 0.93 0.54 0.97 3.00confidence 0.94 0.50 0.72 0.59 0.60 0.85 0.70 0.20confidence 0.93 0.36 0.82 0.50 0.41 0.93 0.56 0.40confidence 0.92 0.25 0.87 0.39 0.28 0.95 0.43 0.60confidence 0.91 0.12 0.89 0.20 0.12 0.96 0.22 0.80Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.Parameters: threshold (confidence or cosine) and number of agreeing systems (agr.
systems).SSCs directly to the gold standard corpus.
How-ever, this result may be due to our particular exper-imental setting and should not be taken as a gen-eral rule.
Instead, this issue should be studied onadditional gold standard corpora (cf.
Section 5).5 Discussion and ConclusionsThe experiments reported in this paper strengthenthe empirical basis of the novel idea of a silverstandard corpus (SSC).
While the originators ofthe SSC have come up with a fixed SSC, our ex-periments show that different parametrizations ofSSCs allow to dynamically configure or select anoptimal one given a gold standard for comparisonduring this optimization.Our experimental data reveals that the boostinghypothesis (the combination of several classifiersoutperforms weaker single ones in terms of perfor-mance) is confirmed for complete mergers as wellas selected twin pairs of taggers.
We also haveevidence that boosting within the SSC paradigmtends to increase precision whereas it seems to de-crease recall.
This general observation becomesstronger and stronger when the size of the commit-tees (i.e., the number of submitting classifiers) in-creases.
It is also particularly interesting that boththe intrinsic evaluation (groups of classifiers vs.gold standard), as well as the extrinsic evaluationof SSCs (groups of classifiers trained and tested onmutually exclusive partitions of the gold standard)reveal parallel patterns in terms of performance ?this indicates a surprising level of stability of theentire SSC approach.In our view, the strongest finding from our ex-periments is the possibility to calibrate an SSC ac-cording to requirements derived from the goal ofannotation campaigns.
In particular, one can adaptparameters to a specific use case, e.g., building acorpus with high precision when compared to thegold standard.
Through the evaluation of the pa-rameter space, one can assess the costs of reach-ing a specific goal.
For instance, a precision of99% can be reached, yet at the cost of the F-scoreplunging to 30%; only slightly lowering the preci-sion to 97% boosts the F-score by 22 points (seelast two rows in Table 3).241Also, when increasingly more annotation setsbecome available (e.g., through the CALBC chal-lenges) the problem of adversarial or extremelybad performing systems is no longer a pressing is-sue since with the optimization approach such sys-tems are automatically sorted out when optimizingover the set of possible system combinations.While our experiments are but a first step to-wards the consolidation of the SSC paradigmsome obvious limitations of our work have to beovercome:?
experiments with different gold standardshave to be run as one might hypothesize thatdifferent gold standards require different pa-rameter settings for the optimal SSC,?
experiments with different NER taggers haveto be run (e.g., we plan to use an NER tag-ger which prefers recall over precision, whilethe one used for these experiments generallyyields higher precision than recall scores),?
test with crowds of taggers which generatehigher recall than precision.13In our approach, a gold standard is needed tofind good parameters to build an SSC.
A ques-tion not addressed so far is how huge such a goldstandard must be to offer an appropriate size forthe optimization step.
Finally, it might be particu-larly rewarding to join efforts in reducing the de-velopment costs for such a gold standards ?
ActiveLearning (e.g., Tomanek et al (2007a)) might beone promising approach to break this bottleneck.Since effective calibration of SSCs is in need ofreasonably sized and densely populated gold stan-dards, by combining these lines of research weclaim that additional benefits for SSCs become vi-able.6 AcknowledgmentsWe wish to thank Kerstin Hornbostel for stim-ulating and corrective remarks on the biologicalgrounding of this investigation.
This research waspartially funded by the EC?s 7th Framework Pro-gramme within the CALBC project (FP7-231727)and the GERONTOSYS research initiative from the13We used a gold standard in which some unusual entities(e.g., protein families) had been annotated for which mostnamed entity taggers have not been trained.
This might alsoexplain the generally overall low recall among the crowd oftaggers yielded in our experiments.German Federal Ministry of Education and Re-search (BMBF) under grant 0315581D within theJENAGE project.ReferencesOlivier Bodenreider and Alexa T. McCray.
2003.
Ex-ploring semantic groups through visual approaches.Journal of Biomedical Informatics, 36(6):414?432.Yoav Freund.
1990.
Boosting a weak learning algo-rithm by majority.
In COLT?90 ?
Proceedings of the3rd Annual Workshop on Computational LearningTheory, pages 202?216.John D. Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In ICML?01 ?
Proceedings of the18th International Conference on Machine Learn-ing, pages 282?289.Dietrich Rebholz-Schuhmann, Antonio Jose?
JimenoYepes, Erik van Mulligen, Ning Kang, Jan Kors,David Milward, Peter Corbett, Ekaterina Buyko,Elena Beisswanger, and Udo Hahn.
2010a.
CALBCSilver Standard Corpus.
Journal of Bioinformaticsand Computational Biology, 8:163?179.Dietrich Rebholz-Schuhmann, Antonio Jose?
JimenoYepes, Erik M. van Mulligen, Ning Kang, JanKors, Peter Milward, David Corbett, EkaterinaBuyko, Katrin Tomanek, Elena Beisswanger, andUdo Hahn.
2010b.
The CALBC Silver StandardCorpus for biomedical named entities: A study inharmonizing the contributions from four indepen-dent named entity taggers.
In LREC 2010 ?
Pro-ceedings of the 7th International Conference onLanguage Resources and Evaluation.Burr Settles.
2004.
Biomedical named entity recog-nition using conditional random fields and rich fea-ture sets.
In NLPBA/BioNLP 2004 ?
COLING2004 International Joint Workshop on Natural Lan-guage Processing in Biomedicine and its Applica-tions, pages 107?110.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007a.
An approach to text corpus constructionwhich cuts annotation costs and maintains corpusreusability of annotated data.
In EMNLP-CoNLL?07?
Proceedings of the 2007 Joint Conference on Em-pirical Methods in Natural Language Processingand Computational Language Learning, pages 486?495.Katrin Tomanek, Joachim Wermter, and Udo Hahn.2007b.
A reappraisal of sentence and token splittingfor life sciences documents.
In K. A. Kuhn, J. R.Warren, and T. Y. Leong, editors, MEDINFO?07 ?Proceedings of the 12th World Congress on MedicalInformatics, number 129 in Studies in Health Tech-nology and Informatics, pages 524?528.
IOS Press.242
