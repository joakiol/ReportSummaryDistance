FeaMble Learnabil ity of Formal Grammars and\[~?he Theory of Natm'al Language Acquis it ionNaoki ABEl)epartment of Computer and Information ScienceUniversity of Pem~sylvaniaPhiladelphia, PA 19104-6389A bst;ractWe propo;:e to apply a. complexity theoretic notion of feasiblelearnability called "polynomial earnability" to the evaluationof grammatical formalisms for linguistic de.~;criptiol).
Polylm-.mil l  h;arnability was originally defined by Valiant in the con-text of bo,llean concept t(!arniiig and sul)scquetltly generalizedhy Blumec el, al.
to i~llinita.cy domains.
We give a clear, intuitiveexposition el' this notion (/l' k'arnability au(l what characteristicsof a collection of hmguages may or many not help feasible learn--ability under this paradigm.
In particular, we preset,t a novel,nontrivJal ::onstraint on the degree of "locality" of grammarswhich allows a r i& class of mildly context sensitive languagesto be feasibly learnable.
We discuss pos,';ihle implications of thisobservati(m to the theory of natm'al anguage acquisition.t.
Introduct, ionA central i~sue o\[ linguistic theory is the "t)~'ojectio~l prohhml",which was origblally prol)osed by Noam Chomsky \[?\] and subsequ(mtly l.?d to much of the development in modern linguistics.This probh,.m pose~ the question: "i\[ow is it posslbk~ for humaninfants to acquire thei,' native language on the basis of casualexposure to limited data in a short amount of t, ime?"
The pro-posed solulion is that the human infant in ell\;ct "knows" whatthe natura{ language that it is trying to learn could possiblybe.
Another way to look at it is that there is a re.latively smallset of possible grammars that it would be able to learn, andits learmng stratergy, implicitly or explicitly, takes adwmtage ofthis apriori knowledge.
The goal of linguistic theory, then, isto &aractedze this set of possible grammars, by specifiying theconstraints, often cMled the "Uniwwsal ( Irammar".
Tile theoryof inductiw~' inference oilers a precise solution to this problem,by characterizing exactly what collections of (or its dual "con-straints ou") languages atisfy tile requirement for being the setof possible grammars, i e. are learnable?
A theory of "feasible"inference is particularly interesting because the language acqui-sitkm process of a human infant is feasible, not to mention itsrelewmce to the technological counterpart of such a pwbh'.m.In this paper, we investigate the learuability of formal gram-mars for linguistic description with respect to a complexity the-oretic notion of feasible lea.rnability called 'polynomial learnabil-ity'.
Polynomial earnabillty was originally developed by Valiant\[?\], \[?\] in the context of learning boolean coitcei)t from exam-ples, artd subsequently generalized by I llumer et al for arbitraryconcepts \[?\].
We apply this criterion of feasible lcarnability tosubclasses of formal grammars thai, are of considerable linguisticinterest.
Specifically, we present a novel, nontrivial constrainton gramma,:s called "k. locality", which ena\])k~s a rich ehlss ofmildly context sensitive grammars called l{ank<~d Node Rewrit-ing G'rammars (RNI{.
( 0 to be limsibly lear1~able.
\'Vc discusspossible implications of this result to thc Lheory of natural Innguagc acqui:~ition.2 Polynomial  Learnabil ity2ol Formal Model ing of LearningWhat constitutes a good model of tile learning behavior?
Belowwe list tlve basic elements that any formal model of learningmust con<, .
(c.f.
\[13\])1.
Objects to be learned: l,ct us call them ~knacks' for fullgenerality.
The question of learnability is asked of a col-lection of knacks.2.
Environment: The way in whidl 'data'  are available to tilelearner.3.
I\[ypotheses: I)escriptious t))r 'knacks', usually CXl)ressedin a certain language.4.
/,earners: Ill general functions from data to hypotheses.5.
Criterion of l,earning: \])efines precisely what is meant bythe question; When is a learner said to 'learn' a giwmcollection of 'knacks' on the basis of data obtained throughthe enviromnent ?In most cases 'knacks' can be thought of as subsets of someuniverse (set) of objects, from which examples are drawn.
1 (Sucha set is often called the 'domain' of the learning problem.)
Theobvions example is the definition of what a language is in thetheory of natural language syntax.
Syntactically, the Englishlanguage is nothing but the set of all grammatical sentences,although this is subject to much philosophical controversy.
Thecorresponding mathematical notion of a formal language is onethat is fi'ee of such a controversy.
A formal language is a subsetof the set of all strings in .E* for some alphabet E. Clearly E*is tile domMn.
The characterization of a kna& as a subset of auniverse is in fact a very general one.
For example, a booleanconcept of n variables is a subset of the set of all assignments othose n variables, often written 2 '~.
Positive examples in this caseare assignments to the n variables which 'satisfy' the concept inquestion.When the 'knacks' under consideration can in fact be thoughtof as subsets of some domain, the overall picture of a learningmodel looks like the one given in Figure 1.2.2 Po lynomia l  Learnab i l i tyPolynomial learnability departs from the classic paradigm of lan-guage learning, 'idenitification in the limit', ~ in at least twoimportant aspects, lilt enforces a higher demand oil tile time1First order structures are an example in which langtlages arc more thanjust subsets of some set \[14\].2Identification i  the limit w?~s originally proposed and studied by Gold\[8\], and has subsequently been generalized in many diflbrent ways.
See forexample \[13\] for a comprehensive treatment of this and related paradigms.The KnacksThe DomainThe EnvironmentoThe HypothesesThe LearnerThe CrilerionyFigure 1: A Learning Modelcomplexity by requiring that the learner converge in time poly-nomial, but on the other hand relaxes the criterion of what con-stitutes a 'correct' grammar by employing an approximate, andprobabilistic notion of correctness, or aecraey to be'precise.
Fur-thermore, this notion of correctness is intricately tied to boththe time complexity requirement and the way in which the en-vironment presents examples to the learner, Specifically, theenvironment is assumed to present o the learner examples fromthe domain with respect to an unknown (to the learner) butfixed probability distribution, and the accuracy of a hypothesisis measured with respect to that same probability distribution.This way, the learner is, so to speak, protected from 'bad' pre-sentations of a knack.
We now make these ideas precise by spec-ifying the five essential parameters of this learning paradigm.1.
Objects to be learned are languages or subsets of ?2" forsome fixed alphabet E. Although we do not specify apri-ori the language in which to express these grammars a, foreach collection of languages Z; of which we ask the learn-ability, we fix a class of grammars G (such that L(~) = ?where we write L(~) to mean {L(G) I G E ~}) with re-spect to which we will define the notion of 'complexity' or'size' of a language.
We take the number of bits it takes towrite down a grammar under a reasonable 4, fixed encod-ing scheme to be the size of the grammar.
The size of alanguage is then defined as the size of a minimal grammarfor it.
(For a language L, we write size(L) for its size.)2.
The environment produces a string in E* with a time-invariant probability distribution unknown to the learnerand pairs it with either 0 or 1 depending on whether thestring is in the language in question or not, gives it to thelearner.
It repeats this process indefinitely.3.
The hypotheses axe expressed as grammars.
The class ofgrammars allowed as hypotheses, say "H, is not necessarilyrequired to generate xactly the class Z; of languages to belearned.
In general, when a collection ?
can be learned bya learner which only outputs hypotheses from a class 7"/,we say that ?
is learnable by Tl, and in particular, whenZ; = L(~)) is learnable by ~, the class of representations Gis said to be properly learnable.
(See \[6\].)4.
Learners passively receive an infinite sequence of positiveand negative xamples in the manner described above, andaPotentAally any 'l?urning program could be a hypothesis~By a reasonblc encoding, we mean one which can represent n ditrerent.grannnars using O(log*~) bits.5.at each initial (finite) segment of such a sequence, output ahypothesis.
In other words, they are functions from finitesequences of positive and negative xamples 5 to grammars.A learning function is said to polynomially learn a col-lection of languages just in case it is computable in timepolynomial ill the length of the input sample, and for anarbitrary degrees of accuracy e and confidence 5, its outputon a sample produced by the environment by the mannerdescribed above for any language L in that collection, willbe an e-approximation of the unknown language L withconfidence probability at least 1 -- a, no matter what theunknown distribution is, as long as the number of stringsin the sample exceeds p(e -~, 5 -~, size (L)) for some fixedplynomial p. Here, grammar G is an e-approximation oflanguage L, if the probability distribution over the sym-metric difference 6 of L and I,(G) is at most e.2.3 Occam Algor i thmBlumer et al \[5\] have shown an extremely interesting resultrevealing a connection between reliable data compression andpolynomial learnability.
Occam's l~azor is a principle in thephilosophy of science which stipulates that a shorter theory istobe  preferred as long as it remains adequate.
B\]umel" el; al.define a precise version of such a notion in the present contextof learning which they call Occam Algorithm, and establishes arelation between the existence of such an algorithm and poly-nomiM learnability: If there exists a polynomial time algorithmwhich reliably "compresses" any sample of any language in agiven collection to a provably small consistent grammar for it,then such an Mogorithm polynomially learns that collection inthe limit.
We state this theorem in a slightly weaker form.Def in i t ion  2.1 Let ?
be a language collection with associatedrepresenation ~ with size function "size".
(Define a sequenceof subclasses of ~ by 7~n = {G e 7-\[ \] size(G) _< n}.)
Then Ais an Occar(~ algorithm for ?
with range size f(m, ~z) if and onlyif!VLE?VS C graph(L)if size(L) = n and \] S I= m thenA(S) is consistent with Sand A(S)) e 7~I(,~,m )and .A runs in time polynomial in the length of S.Theorem 2.1 (B lumer  et al) If A is an Occam algorithmfor f~ with range size f (n,m) = O(nk~ ~) for some k >_ ;0 < c~ < 1 then .4 polynomially learns ?
in the limit.We give below an intuitive explication of why an 0cesta Algo-r ithm polynomiMly learns in the limit.
Suppose A is an OccamAlgorithm for ?, and let L ~ l: be the language to be learned,and n its size.
Then for an arbitrary sample for L of an arbi-trary size, a minimal consistent language for it will never havesize larger than size(L) itself.
Hence A's output on a sample ofsize m will always be one of the hypotheses in H\](m,~), whosecardinality is at most 2\](~,n).
As the sample size m grows, its ef-fect on the probability that any consistent hypothesis in 7~i(,~,, 0is accurate will (polynomially) soon dominate that of the growthof the eardinality of the hypothesis class, which is less than linearin the sample size.Sin the sequel, we shall call them 'labeled samples'SThe symmetric difference between two sets A and B is (A-B)U(B-A).rFor any langugage L, ~jraph(L) = {(x, O} I x C-: L} U {{a:, I) \] a: ~ L}.3 Rar~ked Node Rewr i t ing  GrammarsIn this section, we define l, hc class of nrihlly context sensitivegrammars under consideration, or Ranked Node Rewriting (\]ram.-mars (RNR(~'s).
\[{NR(\]'s are based on the underlying ideas ofTree Adjoining Grammars (TArt's) s and are also a specicalcase of context fi'ee tree grammars \[15\] in which unres~,ricteduse of w~rial)les for moving, copying and deleting, is not per-mitted, in other words each rewriting in this system replaces a"ranked" noclterminal node of say rank j with an "incomplete"tree containing exactly j edges that have no descendants.
Ifwe define a hierarchy of languages generated by subclasses ofRNRG's having nodes and rules with hounded rank j (RNRLj),then RNRL0 = CFL, and RNRLa :: TAL.
9 We formally definethese grammars below.Def in i t ion  'LI (P re l im inar ies )  77ze following definitions arenecessar!l Jb'," the ,~equel.
(i) The set of labeled directed trees over an alphabet E is denoted7;>(ii) r\['ll.e Ta.'ll.'.
of an "incomplete" tree is the number of outgoingedges with no descendents.
(iii) The rarth oj'a node is the.
number of outgoing edges.
(iv) The ~u& 4 'a  symbol is defined if the rank of any nodelabeled by it is always the same, and equal~ that rank.
(v) A ranked alphabet is one in which every symbol has a rank.
(vi) I,l)r writ,': rank(x) for the rank of a~ything x, if it is defined.Def in i t ion 3.2 (Ranked  Node  Rewr i t ing  Grammars)  Aronl;ed nodt; re'writing rammar C is a q'uinl,ph' {>',,v, E'e, ~, It,., Re;)where:(i) EN is a ranked nonterminal alphabet.
(ii) );'r is a germinal alphabet di4oint fi'om F~N.
We let ~; =}-;N U 2T.
(iii) ~ is a distinguished symbol distinct from any member of E,indicating "a'a outgoing edge with no descendent", m(iv) It; is a finite set of labeled trees over E. We refer ~o I(; as~he "initial trees" of the grammar.
(v) Ra is a finite set of rewriting rules: R<~ C {(A,a} I A eY,'N & a C T~u{.}
& rank(A) = rank(re)}.
(In the sequel, wewrite A --.
o for rewriting rule {A, ce).
)(vO ,'a,,V(c) = ,ha, {,-~,4.
(A) I A e EN}.We emphasize that the nonterminM vs. terminal distinction abovedoes not coiadde with the internal node vs. frontier node dis-tinction.
(See examples 2.1 - 2.3.)
tiaving defined the notionsof 'rewriting' and 'derivation' in the obvious manner, the treelanguage of a grammar is then defiimd as the set of trees overthe terminal alphabet, whid~ can be derived fi'om the grammar.
11This is analogous to the way the string language of a rewritinggrammar in the Chomsky hierarchy is defined.Def in i t ion  3.:"1 ('IYee Languages  and  St r ing  Languages)The tree language and string Iang~tagc of a RNRG G, denoteds'\]?ree adjoitdng rammars were introduced a.s a formalism for linguis-tic description by aoshi et al \[10\], \[9\].
Various formal and computationalproperties of TAG's were studied in \[17\].
Its linguistic relevance was demon-s~rated in \[12\].9This hierar,:hy is different fi'om the hierarchy of "meta-TAL's" inventedand studied exl.ensively by Weir in \[20\].l?ln context free t.ree grammars iu \[15\], variables are used in place of ~J.
'l'hese variables can then be used in rewriting rules to move, copy, or erasesubtrees.. \[t is i;his restriction of avoiding such use of variables Hint keepsRNR,G's within the class of etlicient, ly recognizable r writing systems called"Linear context fi'ee rewriting systems" (\[18\]).II'Phis is how an "obligatory adjunction constraint" in the tree adjoiningnunar formalism can be sintulated.a S b9:Sa S dIj\[--.b # c7:SIV .
.a 8 fS $b # c d # cderived :sa s fa s fs sb s c d s eb )v c d )~ eFigurc 2: a, fl, 7 and deriving 'aabbccddeeff' by G:~T((;) and Leo  repectively, are defined as follows;/~(c') = {.,ji~ld(~) t ~ ~ T(O)}.If we now define a hierarchy of languages generated by sub-classes of RNRG's with bounded ranks, context fi'ee languages((',FL) and tree adjoining languages (TAt) constitute the firsttwo members of the hierarchy.Def in i t ion  3.4 l;br each j ~ N RNI~Gj = {GIG C RNRG &rank(G) < J}.
l;br each j ~ N, I{NIU, j = {L(C) I O e: antiC; ;}Theorem 3.1 I{NI~Lo - CFL  ~tn.d l~N I~\[.1 : !I'AL.We now giw; some examples of grammars in this laierarchy, J2which also illustrate the way in which the weak generative ca-pacity of different levels of this hierarchy increases progressively.13Example  3.1.
1), = {3% ~ \[ n. C N} C Gl' , is generated by thefollowing l?~Nl~(_7o 9rammar~ where o' is shown in Figure 2.6', = ({s}, {,,a,b},L {s'}, {,5'--~ ~,,~ + s(~)})Example  3.2 I)2 -- {a'W~c'~d '~ \] n G N} C- TAL  is ocher, ted bythe following \]~N I~G1 grammar, where/~ is shown in Figure 2.C ;~=({S},{s ,a ,b ,e ,d},~,{(S( ,~) )} ,{S ' - ,  ,'<S' +s(~)}>Example  3.3 L3 = {a'%'*c'~d'~e'~f '~ I n C N} ?
TAL is gen-erated by the following RNI?,G2 grnmn~ar, where 7 is shown 5*t,'igure 2.C;':~ = ({S'}, {s, a, b, ,.
', d, c, f} ,  ~, {(,5'(A, A))}, {5'-- ,  7, ,5'-~ ,~(~, I1)})4 K-Loca l  Grammarsq'he notion of qocality' of a grammar we define in this paper isa measure of how much global dependency there is within thegrammar.
By global dependency within a gramnlar, we.
meanthe interactions that exist between different rules and nonter-minals in the grammar.
As it is intuitively clear, allowing un-bounded amont of global interaction is a major, though notonly, cause of a combinatorial explosion in a search for a rightgrammar.
K-locality limits the amount of such interaction, bytSSimpler trees are represented as term struct.ures, whereas lnore involvedtrees are shown in the figure.
Also note that we rise uppercase l tters fornonterminals and lowercase for terminals.IaSome linguistic motiwltions of this extension of'lDkG's are argagned forby the author in \[1\].bounding the number of different rules that can participate inany slngle derivation.Pormally, the notion of "k-locality" of a grammar is definedwith respect o a formulation of derivations due originally toVijay-Shankar, Weir, and 3oshi (\[\[9\]), which is a generalizationof the notion of parse trees for CFO's.
In their formulation,a derivation is a tree recording the tfistory of rewritings.
Theroot of a derivation tree is labeled with an initial tree, and therest of the nodes with rewriting rules.
Each edge correspondsto a rewriting; the edge from a rule (host rule) to auother ule(applied rule) is labeled with the address of the node in the hostl, ree at which the rewriting takes place.The degree of locality of a derivation is the number of distinctkinds of rewritings that appear in it.
In terms of a derivationtree, the degree of locality is the number of different kinds ofedges in it, where two edges are equivalent just in ease the twoend nodes are labeled by the same rules, and the edges them-selves are labeled by the same node address.Definit ion 4.1 Let 7)(G) denote the set of all derivation treesof G, and let r 6 D(G).
Then, the degree of locality of t ,  writtenlocality(r), is d4ned as follows, locality(r) = card{(p,q,,t) Ithere is an edge in r from a node labeled with p to another labeledwith q, and is itself labeled with 77}The degree of locality of a gramm,~r is the maximum of those ofall its derivations.Def init ion 4.2 a RNRG G is called k-local if max{locality(r) \]r e ~(C)}  _< k.We write k-Local-I~NRO - {(7 I G (5 RNRG and G is k-Local}and k-Local-t2Nl~L = { L(G) I G C k-Local-i~NR(: }, etc..Example 4.1 L1 = {a"bna"b '' I n,m C N} ~ /t-Local-RNRLosince all the derivations of G, - ({S} ,  {s,a,b}, ~, {s(S,S)},{S -+ sea, S,b), S --~ A}) generating Lt have deflree of localityat most 4. l,br example, the derivation for the string a3b3ab hasdegree of locality 4 as shown in Figure 8.Because locality of a derivation is the number of distinctkinds of rewritings, inclusive of the positions at which they takcplace, k-locality also puts a bound on the number of nonterminaloccurrences in any rule.
In fact, had we defined the notion of k-locality by the two conditins: (i) at most k rules take part in anyderivation, (if) each rule is k-bounded, t4, the analogous learn-ability result would follow essentially by the same argument.
So,k-locality in effect forces a grammar to be an unbounded unionof boundedly simple grammar, with bounded number of ruleseach boundedly small, with a bounded number of nonterminals.This fact is captured formally by the existence of the followingnormal form with only a polynomial expansion factor.Le lnma 4.1 (K-Local Normal  Form) For every k-Local-RNRGjG, if we let n = size(G), then there is a RNRGj G' such that~.
L( C') = r,,( a).2. c '  is in k-local normal form, i.c.
O' = U{1\]~ I i C -rG,}such that:(a) each lIi has a nonterminal set that is: disjoint fromany other IIj.
(b) each tI~ is k-sire, pie, that isi.
each Ili contains exactly i initial tree.14'K-bounded' here means k nontermineJ occurrences in each rule, \[4\].For instance, a context free grammar in Chomsky Normal l%rm has only2-bounded rules., / - :s s 2A 2..?- s - *A .
- - -  sS S a SbIs s 2 s 2s-./l',, s ---../1XmaS b a S b a Sblocality(~-) = 4s 2 s s sA s--*A A m s.. AS S a Sb S S a Sbs s ss -'/1",, s.oaS b a S b a SbFigure 3: Degree of locality of a derivation of a3b3ab by G1if.
each Hi contains at most k rules.iii.
each IIi contains at most k nonterminal occur-rences.s.
~i~e(c~") = o(~+').Crucially, the constraint of k-locality on RNRG's is an interest-ing one because not only each k-local subclass is an exponentialclass containing infinitely many infinite languages, but also k-local subclasses of the RNRG hierarchy become progressivelymore complex as we go higher in the hierarchy.
In particular,for each j, IlNP~Gj can "count up to" 2(j + 1) and for each k > 2,k-local-RN\[4Gj can also count up to 2(j + 1)) 5 We summarizethese properties of k-loeal-RNRL's below.Theorem 4.1 Pbr every k E N,1.
Vj E N UkeN k-local-RNRLj = RNRLj.~.
Vj C N Vk > 3 k-local-RNRLj+l is incomparable withRNRLp3.
Vj, k ~ N k-local:RNRLj is a p~oper subset of (k+I)-loeal-t~NRLj.4.
Vj Vk > 2 E N k-local-RNRLj contains infinitely manyinfinite languages.hfformal t'roof:1 is obvious because for each grammar in RNRLj, the degreeof locality o~" the grannnar is finite.As for 2, we note that the sequence of the languages (for thefirst three of which we gave example grammars) L~ = {a~*a~...a~ Iu ~ N} are each in 3-1ocal-RNRLI_I but not in RNRLi_2.To verii} 3, we give the following sequence of languages Lj,ksuch that for each j and k, Lj, k is in k-local-RNRLj but not in(k-1)-local-RNRL/.
Intuitively this is because k-local-languagescan have at most O(k) mutually independent dependencies in asingle sentence.Example 4.2 For each j, k ~ N, let Lj,k = { ~ '~ 2,~2 2~, al ...a20+1 ) al ...a2(j+l)knk kn~ ... a 1 ...a2(j~t) \]nl,n2,...,nk e N}.is obvious because Zoo = Uwe~.Lw where Lt~ = {w" \] n e N}are a subset of 2-1ocal-I~NRL0, and hence is a subset of k:local-RNl~Lj for every j and k >_ 2.
???
clearly contains inifinitelymany infinite languages.
\[\]5 K-Loca l  Languages  Are  Learnab leIt turns out that each k-loeal subclass of each RNRLj is poly-nomially lear~lable.Theorem 5. t For each j and k, k-local-RNRLj is polynomiallyIcarnable.This theorem can be proved by exhibiting an Occam Algorithmi(c.f, for this class with size which is Subsection 2.3), a rangel logarithmic in the sample size, and polynomial in the size of aminimal consistent grammar.
We ommit a detailed proof andigiw~ an informal outline of the proof.
:1.
By the Normal Form Lemma, for any k-local-RNRG G,there is a language quivalent k-local-RNR.G H in k-localnormal form whose size is only polynomially larger thanthe size of G.t~A class of grammars G is said to be able to "count up to" j, just incase {a?a'~...a\] \] n e N} e {L(G) \[ G (~ G} but {ai'a'~...a~+ 1 \[ n e N} ?
{c(G) I a e 6}.2.
The number of k-simple grammars with is apriori infinite,but for a given positive sample, the number of such gram-mars that are 'relevant' o that sample (i.e.
which couldhave been used to derive any of the examples) is polyno-mially bounded in the length of the sample.
This followsessentially by the non-erasure and non-copying propertiesof RNRG's.
(See \[3\] for detail.)3.
Out of the set of k-simple grammars in the normal formthus obtained, the ones that are inconsistent with the neg-ative sample are eliminated.
Such a filtering can be seen tobe performable in polynomial time, appealing to the resultof Vijay-Shankar, Weir and Joshi \[18\] that Linear ContextFree Rewriting Systems (LCFRS's) are polynomial timerecognizable.
That R.NRG's are indeed LCFRS's followalso from the non-erasure and non-copying properties.4.
What we have at this stage is a polynomially bounded setof k-simple grammars of varying sizes which are all con-sistent with the input sample.
The 'relevant' part 10 ofa minimal consistent grammar in k-local normal form isguaranteed to be a subset of this set of grammars.
Whatan Oceam algorithm needs to do, then, is to find some sub-set of this set of k-simple grammars that "covers" all thepoints in the positive sample, and has a total size that isprovably only polynomially larger than the minimal totalsize of a subset hat covers the positive sample and is lessthan linear in the sample size.5.
We formalize this as a variant of "Set Cover" problemwhich we call "Weighted Set Cover" (WSC), and prove (in\[2 D the existence of an approximation algorithm with aperformance guarantee which suffices to ensure that theoutput of ,4 will be a basis set consistent with the samplewhich is provably only polynomially larger than a mini-mal one, and is less than linear in the sample size.
Thealgorithm runs in time polynomial in the size of a minimalconsistent grammar and the sample length.6 Discussion: Possible Implicationsto the Theory of Natural LanguageAcqu is i t ionWe have shown that a single, nontrivial constraint of 'k-locality'allows a rich class of mildly context sensitive languages, whichare argued by some \[9\] to be an upperbound of weak genera-tive capacity that may be needed by a hnguistic formalism, tobe learnable.
Let us recall that k-locality puts a bound on theamount of global interactions between different parts (rules) of agrammar.
Although the most concise discription of natrual an-guage might require almost unbounded amount of such interac-tions, it is conceivable that the actual grammar that is acquiredby humans have a bounded degree of interactions, and thus insome cases may involve some inefficiency and redundancy.
Toillustrate the nature of inefficiecy introduced by 'forcing' agram-mar to be k-loeal, consider the following.
The syntactic ategoryof a noun phrase seems to be essentially context independent inthe sense that a noun phrase in a subject position and a nounphrase in an object positionare more or less syntactically equiv-alent.
Such a 'generalization' contributes to the 'global' inter-action in a grammar.
Thus, for a k-local grammar (for somerelatively small k) to account for it, it may have to repeat hesame set of noun phrase rules for different constructions.t?This ,lotion is to be made precise.As is stated in Section 4, for each fixed k, there are clearlya lot of languages (in a given class) which could not be gener-ated by a k-local grammar.
However, it is also the case thatmany languages, for which the most concise grammar is not ak-local grammar, can be generated by a less concise (and thusperhaps less explanatory) grammar, which is k-locah In somesense, this is similar to the well-known distinction of 'compe-tence' and 'performance'.
It is conceivable that performancegrammars which are actually acquired by humans are in somesense much less efficient and less explanatory than a competencegrammar for the same language.
After all when the 'projectionproblem' asks: 'How is it possible for human infants to acquiretheir native languages...', it does not seem necessary that it beasking the question with respect o 'competence grammars', forwhat we know is that the set of 'performance grammars' is fea-sibly learnable.
The possibility that we are suggesting here isthat 'k-locality ~is not visible in competence grammars, however,it is implicitly there so that the languages generated by the classof competence grammars, which are not necessarily k-local, areindeed all k-local languages for some fixed 'k'.7 Conc lus ionsWe have investigated the use of complexity theory to the evalu-ation of grammatical systems as linguistic formalisms from thepoint of view of feasible learnability.
In particular, we havedemonstrated that a single, natural and non-trivial constraintof "locality" on the grammars allows a rich class of mildly con-text sensitive languages to be feasibly learnable, in a well-definedcomplexity theoretic sense.
Our work differs from recent workson efficient learning of formal languages, for example by An-gluin (\[4\]), in that it uses only examples and no other powerfuloracles.
We hope to have demonstrated that learning formal--  grammars need not be doomed to be necessarily computation-ally intractable, and the investigation ofalternative formulationsof this problem is a worthwhile ndeavonr.8 AcknowledgmentThe research reported here in was in part supported by an IBMgraduate fellowship awarded to the author.
The author grate-fully acknowledges his advisor, Scott Weinstein, for his guidanceand encouragement throughout this research.
He has also ben-efitted from valuable discussions with Aravind Joshi and DavidWeir.
Finally he wishes to thank Haim Levkowitz and EthelSchuster for their kind help in formatting this paper.References\[1\] Naoki Abe.
Generalization of tree adjunction as rankednode rewriting.
1987.
Unpublished manuscript.\[2\] Naoki Abe.. Polynomial learnability and locality of formalgrammars.
In 26th Meeting of A.C.L., June 1988.\[3\] Naoki Abe.
Polynomially learnable subclasses of mildy con-text sensitive languages.
1987.
Unpublished manuscript.\[4\] Dana Angluin.
Leafing k-bounded context-free grammars.Technical Report YALEU/DCS/TR-557, Yale University,August 1987.\[5\] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth.Classifying learnable geometric oncepts with the vapnik-chervonenkis dimension.
In Proc.
18th ACM Syrup.
on The-ory of Computation, pages 243 - 282, 1986.\[6\] A. Blumer, A. Ehrenfeueht, D. Hausslor, and M. War-muth.
Learnability and the Vapnik-Chervonenkis D men-sion.
Technical Report UCSC CI~L-87-20, University ofCalifornia at Santa Cruz, Novermber 1987.\[7\] Noam Chomsky.
Aspects of the Theory of Syntax.
The MITPress, 1965.\[8\] E. Mark Gold.
Language identification i  the limit.
Infor-mation and Control, 10:447-474, 1967.\[9\] A. K. Joshi.
How much context-sensitivity is necessary forcharacterizing structural description - tree adjoining ram-mars.
In D. Dowty, L. Karttunen, and A. Zwicky, edi-tors, Natural Language Processing - Theoretical, Computa-tional~ and Psychological Perspectives, Cambridege Univer-sity Press, 1983.\[10\] Aravind K. Joshi, Leon Levy, and Masako Takahashi.
Treeadjunct grammars.
Journal of Computer and System Sci-ences, 10:136-163, 1975.\[11\] M. Kearns, M. Li, L. Pitt, and L. Valiant.
On the learn-ability of boolean formulae.
In Proc.
19th ACM Syrup.
onTheory of Comoputation, pages 285 - 295, 1987.\[12\] A. Kroch and A. K. Joshi.
Linguistic relevance of tree ad-joining grammars.
1989.
To appear in Linguistics and Phi-losophy.\[13\] Daniel N. Osherson, Michael Stob, and Scott Weinstein.Systems That Learn.
The MIT Press, 1986.\[14\] Daniel N. Osherson and Scott Weinstein.
Identification ithe limit of first order structures.
JouT"aal of PhilosophicalLogic, 15:55 - 81, 1986.\[15\] William C. Rounds.
Context-free grammars on trees.
InA CM Symposium on Theory of Computing, pages 143-148,1969.\[16\] Leslie G. Valiant.
A theory of the learnable.
Communica-tions of A.C.M., 27:1134-1142, 1984.\[17\] K. Vijay-Shanker and A. K. Joshi.
Some computationalproperties of tree adjoining grammars.
In 23rd Meeting ofA.C.L., 1985.\[18\] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi.
Character-izing structural descriptions produced by various grarmnat-ieal formalisms.
In 25th Meeting of A.C.L., 1987.\[19\] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi.
On theprogression from context-freo to tree adjoining languages.In A. Manaster-Ramer, editor, Mathematics of Language,John Benjamins, 1986.\[20\] David J. Weir.
From Context-Free Grammars to Tree Ad-joining Grammars and Beyond - A dissertation proposal.Technical Report MS-CIS-87-42, University of Pennsylva-nia, 1987.
