Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1943?1947,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsWell-argued recommendation:adaptive models based on words in recommender systemsJulien GaillardUniversity of AvignonAgoranticAvignon, Francejulien.gaillard@univ-avignon.frMarc El-BezeUniversity of AvignonAgoranticAvignon, Francemarc.elbeze@univ-avignon.frEitan AltmanINRIA Sophia AntipolisAgoranticSophia-Antipolis, Franceeitan.altman@inria.frEmmanuel EthisUniversity of AvignonAgoranticAvignon, Franceemmanuel.ethis@univ-avignon.frAbstractRecommendation systems (RS) take advan-tage of products and users information in orderto propose items to consumers.
Collaborative,content-based and a few hybrid RS have beendeveloped in the past.
In contrast, we proposea new domain-independent semantic RS.
Byproviding textually well-argued recommenda-tions, we aim to give more responsibility to theend user in his decision.
The system includesa new similarity measure keeping up both theaccuracy of rating predictions and coverage.We propose an innovative way to apply a fastadaptation scheme at a semantic level, provid-ing recommendations and arguments in phasewith the very recent past.
We have performedseveral experiments on films data, providingtextually well-argued recommendations.1 IntroductionRecommender systems aim at suggesting appropri-ate items to users from a large catalog of products.Those systems are individually adapted by using aspecific profile for each user and item, derived fromthe analysis of past ratings.
The last decade hasshown a historical change in the way we consumeproducts.
People are getting used to receive recom-mendations.
Nevertheless, after a few bad recom-mendations, users will not be convinced anymore bythe RS.
Moreover, if these suggestions come withoutexplanations, why people should trust it?
Numbersand figures cannot talk to people.To answer these key issues, we have designed anew semantic recommender sytem (SRS) includingat least two innovative features:?
Argumentation: each recommendation relieson and comes along with a textual argumenta-tion, providing the reasons that led to that rec-ommendation.?
Fast adaptation: the system is updated in a con-tinuous way, as each new review is posted.In doing so, the system will be perceived as lessintrusive thanks to well-chosen words and its fail-ures will be smoothed over.
It is therefore necessaryto design a new generation of RS providing textu-ally well-argued recommendations.
This way, theend user will have more elements to make a well-informed choice.
Moreover, the system parametershave to be dynamically and continuously updated,in order to provide recommendations and argumentsin phase with the very recent past.
To do so, wehave adapted the algorithms we described in Gail-lard (Gaillard et al 2013), by including a semanticlevel, i.e words, terms and phrases as they are natu-rally expressed in reviews.This paper is structured as follows.
In the nextsection, we present the state of the art in recom-mendation systems and introduce some of the im-provements we have made.
Then, we present ourapproach and define the associated methods in sec-tion 3.
We describe the evaluation protocol and howwe have performed some experiments in section 4.Finally we report results including a comparison toa baseline in section 5.2 Related work and choice of a baselineWe present here some methods used in the litera-ture.
Collaborative Filtering (CF) systems use logs1943of users, generally user ratings on items (Burke,2007; Sarwar et al 1998).
In these systems, thefollowing assumption is made: if user a and userb rate n items similarly, they will rate other itemsin the same way (Deshpande and Karypis., 2004).This technique has many well-known issues suchas the ?cold start?
problem, i.e when new items orusers appear, it is impossible to make a recommen-dation, due to the absence of rating data (Schein etal., 2002).
Other limitations of RS are sparsity, scal-ability, overspecialization and domain-dependencyproblems.In Content Based Filtering (CBF) systems, users aresupposed to be independent (Mehta et al 2008).Hence for a given user, recommendations rely onlyon items he previously rated.Some RS incorporate semantic knowledge to im-prove quality.
Generally, they apply a concept-based approach to enhance the user modeling stageand employ standard vocabularies and ontology re-sources.
For instance, ePaper (scientific-paper rec-ommender), computes the matching between theconcepts constituting user interests and the conceptsdescribing an item by using hierarchical relation-ships of domain concepts (Maidel et al 2008).
Cod-ina and Ceccaroni (2010) propose to take advantageof semantics by using an interest-prediction methodbased on user ratings and browsing events.However, none of them are actually based on theuser opinion as it is expressed in natural language.2.1 Similarity measuresSimilarity measures are the keystone of RS (Her-locker et al 2005).
Resnick (1997) was one of thefirst to introduce the Pearson correlation coefficientto derive a similarity measure between two entities.Other similarity measures such as Jaccard and Co-sine have been proposed (Meyer, 2012).
Let Su bethe set of items rated by u, Ti the set of users whohave rated item i, ru,i the rating of user u on item iand rx the mean of x (user or item).
PEA(i,j) standsfor the Pearson similarity between items i and j andis computed as follows:?u?Ti?Tj (ru,i ?
ri)(ru,j ?
rj)?
?u?Ti?Tj (ru,i ?
ri)2?u?Ti?Tj (ru,j ?
rj)2(1)In the remainder, the Pearson similarity measure willbe used as a baseline.
The Manhattan Weighted andCorrected similarity (MWC), that we introduced in(Gaillard et al 2013), will be used as a point ofcomparison as well1.
Again, for none of them, tex-tual content is taken into account.2.2 Rating predictionLet i be a given item and u a given user.
We supposethe pair (u, i) is unique.
Indeed, most of social net-works do not allow multiple ratings by the same userfor one item.
In this framework, two rating predic-tion methods have to be defined: one user orientedand the other item oriented.
Sim stands for somesimilarity function in the following formula.rating(u, i) =?v?Ti Sim(u, v)?
rv,i?v?Ti |Sim(u, v)|(2)A symmetrical formula for items rating(i, u) is de-rived from and combined with (2).r?u,i = ?
?rating(u, i)+(1??
)?rating(i, u) (3)3 MethodsIn this section, we describe the methods we haveused and propose some of the enhancements wehave elaborated in our system.
In formula (2),Sim can be replaced by several similarity such asPearson, Cosine or MWC similarity (Tan et al2005).
All these methods provide a measurement ofthe likeness between two objects.
We then concludeif two users (or items) are ?alike?
or not.
One hasto define what ?alike?
should mean in this case.
Iftwo users rate the same movies with equals ratings,then these similarities will be maximal.
However,they may have rated identically but for completelydifferent reasons, making them not alike at all.Moreover, none of these similarity measures canexpress why two users or items are similar.
This isdue to the fact that they rely on ratings only.3.1 New similarity based on wordsWe propose a new similarity method, taking into ac-count words used by users in their past reviews aboutitems.
In the remainder, we call it the Word BasedSimilarity (WBS).
Each user x (or item) has a vo-cabulary set Vx and each word w in it is associated1Details on MWC can be found in supplementary material.1944with a set of ratings Rw,x and an average usage rat-ing rw.
In order to balance the contribution of eachword, we define a weight function Fw, mixing thewell-known Inverse Document Frequency IDF (w)with the variance ?2w.
Common words and words wassociated with very heterogenous ratings Rw,x (i.ea high variance) will have a smaller weight in thesimilarity.
Nw is the number of items in which theword w appears.
Ntot is the total number of items.D is the maximum difference between two ratings.Note that Fw has to be updated at each iteration.Fw = ?log(NwNtot)?
1?2w(4)WBS(x, y) =?w?Vx?Vy(D ?
|rw,x ?
rw,y|)FwD ?
|Vx ?
Vy|?w?Vx?Vy Fw(5)3.2 AdaptationAn adaptive framework proposed in (Gaillard et al2013) allows the system to have a dynamic adapta-tion along time, overcoming most of the drawbacksdue to the cold-start.
The authors have designed adynamic process following the principle that everyupdate (u, i) needs to be instantly taken into accountby the system.
Consequently, we have to update the?2w and IDF(w) at each iteration, for every word.Paying attention to avoid a whole re-estimation ofthese two variables, we derived an iterative relationfor the two of them2.
We thus reduced the complex-ity by one degree, keeping our system very well-fitted to dynamic adaptation.3.3 Textual recommendationThe main innovative feature of our proposal is topredict what a user is going to write on an itemwe recommend.
More precisely, we can tell theuser why he is expected to like or dislike the rec-ommended item.
This is possible thanks to the newsimilarity measure we have introduced (WBS).
Letus consider a user u and an item i.
To keep it sim-ple, the system takes into account what u has writtenon other items in the past and what other users havewritten on item i, by using WBS.
The idea consistsin extracting what elements of i have been liked ordisliked by other users, and what u generally likes.2More details can be found in the supplementary material.At the intersection of these two pieces of informa-tion, we extract a set of matching words that wesort by relevance using Fw.
Then, by taking intoaccount the ratings associated with each word, wedefine two sub-sets Pw and Nw.
Pw contains whatuser u is probably going to like in i and Nw what umay dislike.
Finally, we provide the most relevantarguments contained in both Pw and Nw, and eachof them is given in the context they have been usedfor item i.
As an example, some outputs are shownin section 5.2.4 Evaluation criteriaWe present here the evaluation protocol we de-signed.
It should be noted that we are not ableto make online experiments.
Therefore, we cannot measure the feedback on our recommendations.However, the cornerstone of recommender system isthe accuracy of rating predictions (Herlocker et al2004).
From this point of view, one could argue thatthe quality of a recommender engine could be as-sessed by its capacity to predict ratings.
It is thuspossible to evaluate our system comparing the pre-diction r?u,i for a given pair (u, i), with the actualreal rating ru,i.The classical metrics3 (Bell et al 2007) Root MeanSquare Error (RMSE) and Mean Absolute Error(MAE) will be used to evaluate our RS.Last but not least, we make the following assump-tion: if WBS results are as good as MWC?s, thewords presented by the system to users as argumentsare likely to be relevant.5 ExperimentsThis work has been carried out in partnership withthe website Vodkaster 4, a Cinema social network.Researchers have used other datasets such as the fa-mous Netflix.
Unfortunately, the latter does not in-clude textual reviews.
It is therefore strictly impos-sible to experiment a SRS on such a dataset.5.1 CorpusThe corpus has been extracted from Vodkaster?sdatabase.
Users post micro-reviews (MR) to ex-press their opinion on a movie and rate it, within a3Details on metrics are given in the supplementary material.4www.vodkaster.com1945140 characters Twitter-like length limit.
We dividedthe corpus into three parts, chronologically sorted:training (Tr), development (D) and test (T).
Note thatin our experiments, the date is taken into accountsince we also work on dynamic adaptation.Tr D Tr+D TSize 55486 9892 65378 9729Nb of Films 8414 3184 9130 3877Nb of Users 1627 675 1855 706Table 1: Statistics on the corpus5.2 ResultsFigure 1 compares four different methods: theclassical Pearson (PEA) method that does notallow quick adaptation, the MWC method with andwithout quick adaptation MNA and ours (WBS).Within the confidence interval, in terms of accuracy,800 1000 1200 1400 1600 1800 20000.860.880.900.920.940.96Accuracy as a function of Coverage on DEVCoverageAccuracyWBSMWCMNAPEAFigure 1: Evolution of accuracy as a function of coveragefor PEA, MWC and WBS methods on D corpus.the same performances are obtained by MWC andWBS.
Both outperform5 PEA and MNA.
Our wordbased approach is thus able to offer the arguments5Note that the key point here is the comparison of results ob-tained with the baseline and with the method we propose.
Bothof them have been evaluated with the same protocol: RMSE iscomputed with respect to rating predictions above some empir-ical threshold as done in (Gaillard et al 2013).feature without any loss of perfomances withrespect to any others RS methods that we know of.In Table 2, we set a constant coverage (2000 pre-dictions) in order to be able to compare results ob-tained with different methods.Corp.
Met.
RMSE MAE %Acc.
CID PEA 0.99 0.76 86.41 1.49E MNA 0.93 0.72 90.75 1.26V MWC 0.89 0.69 92.95 1.12WBS 0.89 0.70 92.45 1.16T PEA 1.01 0.78 86.02 1.51E MNA 0.98 0.75 90.04 1.30S MWC 0.92 0.71 91.46 1.22T WBS 0.94 0.72 91.15 1.24Table 2: Results with Pearson (PEA), MWC, MWCwith-out Adaptation (MNA), WBS.
CI is the radius confidenceinterval estimated in % on accuracy (Acc.
).MNA (MWC without adaptation) being betterand more easily updated than Pearson (PEA), wehave decided to use the adaptive framework only forMWC.
Moreover, for Pearson dynamic adaptation,the updating algorithm complexity is increased byone degree.We want to point out that the results are the same forboth MWC and WBS methods, within a confidenceinterval (CI) radius of 1.16%.
From a qualitativepoint of view, these results can be seen as anassessment of our approach based on words.Example of outputs: The movie ApocalypseNow is recommended to user Theo6 with a ratingprediction equal to 4.3.
Why he might like: somebrillant moments (0.99), among the major master-piece (0.91), Vietnam?s hell (0.8); dislike: did notunderstand everything but... (0.71).The data we have does not contain the informa-tion on the reaction of the user to the recommen-dation.
In particular, we do not know if the textualargumentation would have been sufficient for con-vincing Theo6 to see the film.
But we know thatafter seeing it, he put a good rating (4.5/5) on thismovie.19466 Conclusion and perspectivesWe have presented an innovative proposal for de-signing a domain-independent SRS relying on aword based similarity function (WBS), providingtextually well-argued recommendations to users.Moreover, this system has been developed in a dy-namic and adaptive framework.
This might be thefirst step really made towards an anthromorphic andevolutive recommender.
As future work, we plan toevaluate how the quality is impacted by the time di-mension (adaptation delay, cache reset,etc.
).AcknowledgmentThe authors would like to thank Vodkaster for pro-viding the data.This work has been partly supported by the Eu-ropean Commission within the framework of theCONGAS Project (FP7- ICT-2011-8-317672), seewww.congas-project.eu.ReferencesR.
Bell, Y. Koren and C. Volinsky.
2007.
The BellKor2008 Solution to the Netflix Prize.
The Netflix Prize.R.
Burke.
2007.
Hybrid Web Recommender Systems.The Adaptive Web, 377?408.V.
Codina and Luigi Ceccaroni.
2010.
Taking Advan-tage of Semantics in Recommendation Systems.
Pro-ceedings of the 13th International Conference of theCatalan Association for A.I.,163?172M.
Deshpande and G. Karypis.
2004.
Item based top-N recommendation algorithms.
ACM Transactions onInformation and System Security.J.
Gaillard, M. El-Beze, E. Altman and E. Ethis.
2013.Flash reactivity: adaptive models in recommendersystems.
International Conference on Data Mining(DMIN), WORLDCOMP.J.
Herlocker, J.A Konstan, L. Terveen and J. Riedl.
2004.Evaluating collaborative filtering recommender sys-tems.
ACM Transactions on Information Systems(TOIS).V.
Maidel, P. Shoval, B. Shapira, M. Taieb-Maimon.2008.
Evaluation of an ontology-content based filter-ing method for a personalized newspaper.
RecSys?08:Proceedings, 91?98.B.
Mehta, T. Hofmann, and W. Nejdl.
2008.
Robust col-laborative filtering.
In RecSysF.
Meyer.
2012.
Recommender systems in industrial con-texts.
PhD thesis, University of Grenoble, France.P.
Resnick and R. Varian Hal.
1997.
Recommender sys-tems (introduction to special section.)
Communica-tions of the ACMB.M Sarwar, J.A Konstan, A. Borchers,J.
Herlocker, B.Miller, J. Riedl 1998.
Using filtering agents to im-prove prediction quality in the groupLens researchcollaborative filtering system.
Proceedings of theACM Conference on Computer Supported Coopera-tive WorkA.I Schein, A. Popescul and L.H Ungar.
2002.
Methodsand metrics for cold-start recommendations.
ACM SI-GIR Conference on Research and Development in In-formation Retrieval.P.
Tan, M. Steinbach and V. Kumar.
2005 Introductionto Data Mining.
Addison-Wesley, 500?524.C.
Ziegler, S.M McNee, J.A Konstan and G. Lausen.2005.
Improving recommendation lists through topicdiversification.
Fourteenth International World WideWeb Conference1947
