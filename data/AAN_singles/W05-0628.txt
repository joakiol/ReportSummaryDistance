Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 193?196, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsSemantic Role Labeling as Sequential TaggingLlu?
?s Ma`rquez, Pere Comas, Jesu?s Gime?nez and Neus Catala`TALP Research CentreTechnical University of Catalonia (UPC){lluism,pcomas,jgimenez,ncatala}@lsi.upc.eduAbstractIn this paper we present a semantic rolelabeling system submitted to the CoNLL-2005 shared task.
The system makesuse of partial and full syntactic informa-tion and converts the task into a sequen-tial BIO-tagging.
As a result, the label-ing architecture is very simple .
Build-ing on a state-of-the-art set of features, abinary classifier for each label is trainedusing AdaBoost with fixed depth decisiontrees.
The final system, which combinesthe outputs of two base systems performedF1=76.59 on the official test set.
Addi-tionally, we provide results comparing thesystem when using partial vs. full parsinginput information.1 Goals and System ArchitectureThe goal of our work is twofold.
On the one hand,we want to test whether it is possible to implementa competitive SRL system by reducing the task to asequential tagging.
On the other hand, we want toinvestigate the effect of replacing partial parsing in-formation by full parsing.
For that, we built two dif-ferent individual systems with a shared sequentialstrategy but using UPC chunks-clauses, and Char-niak?s parses, respectively.
We will refer to thosesystems as PPUPC and FPCHA, hereinafter.Both partial and full parsing annotations providedas input information are of hierarchical nature.
Oursystem navigates through these syntactic structuresin order to select a subset of constituents organizedsequentially (i.e., non embedding).
Propositions aretreated independently, that is, each target verb gen-erates a sequence of tokens to be annotated.
We callthis pre-processing step sequentialization.The sequential tokens are selected by exploringthe sentence spans or regions defined by the clauseboundaries1.
The top-most syntactic constituentsfalling inside these regions are selected as tokens.Note that this strategy is independent of the inputsyntactic annotation explored, provided it containsclause boundaries.
It happens that, in the case offull parses, this node selection strategy is equivalentto the pruning process defined by Xue and Palmer(2004), which selects sibling nodes along the path ofancestors from the verb predicate to the root of thetree2.
Due to this pruning stage, the upper-bound re-call figures are 95.67% for PPUPC and 90.32% forFPCHA.
These values give F1 performance upperbounds of 97.79 and 94.91, respectively, assumingperfect predictors (100% precision).The nodes selected are labeled with B-I-O tagsdepending if they are at the beginning, inside, or out-side of a verb argument.
There is a total of 37 argu-ment types, which amount to 37*2+1=75 labels.Regarding the learning algorithm, we used gen-eralized AdaBoost with real-valued weak classifiers,which constructs an ensemble of decision trees offixed depth (Schapire and Singer, 1999).
We con-sidered a one-vs-all decomposition into binary prob-1Regions to the right of the target verb corresponding to an-cestor clauses are omitted in the case of partial parsing.2With the unique exception of the exploration inside siblingPP constituents proposed by (Xue and Palmer, 2004).193lems to address multi-class classification.AdaBoost binary classifiers are used for labelingtest sequences in a left-to-right tagging scheme us-ing a recurrent sliding window approach with infor-mation about the tag assigned to the preceding to-ken.
This tagging module ensures some basic con-straints, e.g., BIO correct structure, arguments donot cross clause boundaries nor base chunk bound-aries, A0-A5 arguments not present in PropBankframes for a certain verb are not allowed, etc.
Wealso tried beam search on top of the classifiers?
pre-dictions to find the sequence of labels with highestsentence-level probability (as a summation of indi-vidual predictions).
But the results did not improvethe basic greedy tagging.Regarding feature representation, we used allinput information sources, with the exception ofverb senses and Collins?
parser.
We did not con-tribute with significantly original features.
Instead,we borrowed most of them from the existing liter-ature (Gildea and Jurafsky, 2002; Carreras et al,2004; Xue and Palmer, 2004).
Broadly speaking, weconsidered features belonging to four categories3:(1) On the verb predicate:?
Form; Lemma; POS tag; Chunk type and Type ofverb phrase in which verb is included: single-word ormulti-word; Verb voice: active, passive, copulative, in-finitive, or progressive; Binary flag indicating if the verbis a start/end of a clause.?
Subcategorization, i.e., the phrase structure rule expand-ing the verb parent node.
(2) On the focus constituent:?
Type; Head: extracted using common head-word rules;if the first element is a PP chunk, then the head of the firstNP is extracted;?
First and last words and POS tags of the constituent.?
POS sequence: if it is less than 5 tags long; 2/3/4-gramsof the POS sequence.?
Bag-of-words of nouns, adjectives, and adverbs in theconstituent.?
TOP sequence: sequence of types of the top-most syn-tactic elements in the constituent (if it is less than 5 ele-ments long); in the case of full parsing this corresponds tothe right-hand side of the rule expanding the constituentnode; 2/3/4-grams of the TOP sequence.?
Governing category as described in (Gildea and Juraf-sky, 2002).3Features extracted from partial parsing and Named Enti-ties are common to PPUPC and FPCHA models, while featurescoming from Charniak parse trees are implemented exclusivelyin the FPCHA model.?
NamedEnt, indicating if the constituent embeds orstrictly-matches a named entity along with its type.?
TMP, indicating if the constituent embeds or strictlymatches a temporal keyword (extracted from AM-TMP ar-guments of the training set).
(3) Context of the focus constituent:?
Previous and following words and POS tags of the con-stituent.?
The same features characterizing focus constituents areextracted for the two previous and following tokens,provided they are inside the clause boundaries of the cod-ified region.
(4) Relation between predicate and constituent:?
Relative position; Distance in words and chunks; Levelof embedding with respect to the constituent: in numberof clauses.?
Constituent path as described in (Gildea and Jurafsky,2002); All 3/4/5-grams of path constituents beginning atthe verb predicate or ending at the constituent.?
Partial parsing path as described in (Carreras et al,2004); All 3/4/5-grams of path elements beginning at theverb predicate or ending at the constituent.?
Syntactic frame as described by Xue and Palmer (2004)2 Experimental Setting and ResultsWe trained the classification models using the com-plete training set (sections from 02 to 21).
Once con-verted into one sequence per target predicate, the re-sulting set amounts 1,049,049 training examples inthe PPUPC model and 828,811 training examples inthe FPCHA model.
The average number of labels perargument is 2.071 and 1.068, respectively.
This factmakes ?I?
labels very rare in the FPCHA model.When running AdaBoost, we selected as weakrules decision trees of fixed depth 4 (i.e., each branchmay represent a conjunction of at most 4 basic fea-tures) and trained a classification model per label forup to 2,000 rounds.We applied some simplifications to keep trainingtimes and memory requirements inside admissiblebounds.
First, we discarded all the argument la-bels that occur very infrequently and trained onlythe 41 most frequent labels in the case of PPUPCand the 35 most frequent in the case of FPCHA.The remaining labels where joined in a new label?other?
in training and converted into ?O?
when-ever the SRL system assigns a ?other?
label dur-ing testing.
Second, we performed a simple fre-quency filtering by discarding those features occur-ring less than 15 times in the training set.
As an194exception, the frequency threshold for the featuresreferring to the verb predicate was set to 3.
The finalnumber of features we worked with is 105,175 in thecase of PPUPC and 80,742 in the case of FPCHA.Training with these very large data and featuresets becomes an issue.
Fortunately, we could splitthe computation among six machines in a Linuxcluster.
Using our current implementation combin-ing Perl and C++ we could train the complete mod-els in about 2 days using memory requirements be-tween 1.5GB and 2GB.
Testing with the ensemblesof 2,000 decision trees per label is also not very effi-cient, though the resulting speed is admissible, e.g.,the development set is tagged in about 30 minutesusing a standard PC.The overall results obtained by our individualPPUPC and FPCHA SRL systems are presented in ta-ble 1, with the best results in boldface.
As expected,the FPCHA system significantly outperformed thePPUPC system, though the results of the later canbe considered competitive.
This fact is against thebelief, expressed as one of the conclusions of theCoNLL-2004 shared task, that full-parsing systemsare about 10 F1 points over partial-parsing systems.In this case, we obtain a performance difference of2.18 points in favor of FPCHA.Apart from resulting performance, there are addi-tional advantages when using the FPCHA approach.Due to the coarser granularity of sequence tokens,FPCHA sequences are shorter.
There are 21% lesstraining examples and a much lower quantity of ?I?tags to predict (the mapping between syntactic con-stituents and arguments is mostly one-to-one).
Asa consequence, FPCHA classifiers train faster withless memory requirements, and achieve competitiveresults (near the optimal) with much less rounds ofboosting.
See figure 1.
Also related to the tokengranularity, the number of completely correct out-puts is 4.13 points higher in FPCHA, showing thatthe resulting labelings are structurally better thanthose of PPUPC.Interestingly, the PPUPC and FPCHA systemsmake quite different argument predictions.
For in-stance, FPCHA is better at recognizing A0 and A1arguments since parse constituents corresponding tothese arguments tend to be mostly correct.
Compar-atively, PPUPC is better at recognizing A2-A4 argu-ments since they are further from the verb predicate6466687072747678200  400  600  800  1000  1200  1400  1600  1800  2000Overall F1Number of roundsPP-upcFP-chaPP bestFP-cha bestFigure 1: Overall F1 performance of individual sys-tems on the development set with respect to the num-ber of learning roundsPerfect props Precision Recall F?=1PPUPC 47.38% 76.86% 70.55% 73.57FPCHA 51.51% 78.08% 73.54% 75.75Combined 51.39% 78.39% 75.53% 76.93Table 1: Overall results of the individual systems onthe development set.and tend to accumulate more parsing errors, whilethe fine granularity of the PPUPC sequences still al-low to capture them4.
Another interesting observa-tion is that the precision of both systems is muchhigher than the recall.The previous two facts suggest that combining theoutputs of the two systems may lead to a significantimprovement.
We experimented with a greedy com-bination scheme for joining the maximum number ofarguments from both solutions in order to increasecoverage and, hopefully, recall.
It proceeds depart-ing from an empty solution by: First, adding all thearguments from FPCHA in which this method per-forms best; Second, adding all the arguments fromPPUPC in which this method performs best; andThird, making another loop through the two meth-ods adding the arguments not considered in the firstloop.
At each step, we require that the added argu-ments do not overlap/embed with arguments in thecurrent solution and also that they do not introducerepetitions of A0-A5 arguments.
The results on the4As an example, the F1 performance of PPUPC on A0 andA2 arguments is 79.79 and 65.10, respectively.
The perfor-mance of FPCHA on the same arguments is 84.03 and 62.36.195Precision Recall F?=1Development 78.39% 75.53% 76.93Test WSJ 79.55% 76.45% 77.97Test Brown 70.79% 64.35% 67.42Test WSJ+Brown 78.44% 74.83% 76.59Test WSJ Precision Recall F?=1Overall 79.55% 76.45% 77.97A0 87.11% 86.28% 86.69A1 79.60% 76.72% 78.13A2 69.18% 67.75% 68.46A3 76.38% 56.07% 64.67A4 79.78% 69.61% 74.35A5 0.00% 0.00% 0.00AM-ADV 59.15% 52.37% 55.56AM-CAU 73.68% 57.53% 64.62AM-DIR 71.43% 35.29% 47.24AM-DIS 77.14% 75.94% 76.54AM-EXT 63.64% 43.75% 51.85AM-LOC 62.74% 54.27% 58.20AM-MNR 54.33% 52.91% 53.61AM-MOD 96.16% 95.46% 95.81AM-NEG 99.13% 98.70% 98.91AM-PNC 53.49% 40.00% 45.77AM-PRD 0.00% 0.00% 0.00AM-REC 0.00% 0.00% 0.00AM-TMP 77.68% 78.75% 78.21R-A0 86.84% 88.39% 87.61R-A1 75.32% 76.28% 75.80R-A2 54.55% 37.50% 44.44R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 0.00% 0.00% 0.00R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-TMP 69.81% 71.15% 70.48V 99.16% 99.16% 99.16Table 2: Overall results (top) and detailed results onthe WSJ test (bottom).development set (presented in table 1) confirm ourexpectations, since a performance increase of 1.18points over the best individual system was observed,mainly caused by recall improvement.
The final sys-tem we presented at the shared task performs exactlythis solution merging procedure.
When applied onthe WSJ test set, the combination scheme seems togeneralize well, since an improvement is observedwith respect to the development set.
See the offi-cial results of our system, which are presented in ta-ble 2.
Also from that table, it is worth noting that theF1 performance drops by more than 9 points whentested on the Brown test set, indicating that the re-sults obtained on the WSJ corpora do not generalizewell to corpora with other genres.
The study of thesources of this lower performance deserves furtherinvestigation, though we do not believe that it is at-tributable to the greedy combination scheme.3 ConclusionsWe have presented a simple SRL system submit-ted to the CoNLL-2005 shared task, which treatsthe SRL problem as a sequence tagging task (us-ing a BIO tagging scheme).
Given the simplic-ity of the approach, we believe that the results arevery good and competitive compared to the state-of-the-art.
We also provided a comparison betweentwo SRL systems sharing the same architecture, butbuild on partial vs. full parsing, respectively.
Al-though the full parsing approach obtains better re-sults and has some implementation advantages, thepartial parsing system shows also a quite competi-tive performance.
The results on the developmentset differ in 2.18 points, but the outputs generatedby the two systems are significantly different.
Thefinal system, which scored F1=76.59 in the officialtest set, is a combination of both individual systemsaiming at increasing coverage and recall.AcknowledgementsThis research has been partially supported by theEuropean Commission (CHIL project, IP-506909).Jesu?s Gime?nez is a research fellow from the Span-ish Ministry of Science and Technology (ALIADOproject, TIC2002-04447-C02).
We would like tothank also Xavier Carreras for providing us withmany software components and Mihai Surdeanu forfruitful discussions on the problem and feature engi-neering.ReferencesX.
Carreras, L. Ma`rquez, and G. Chrupa?a.
2004.
Hierarchicalrecognition of propositional arguments with perceptrons.
InProceedings of CoNLL-2004.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.R.
E. Schapire and Y.
Singer.
1999.
Improved Boosting Algo-rithms Using Confidence-rated Predictions.
Machine Learn-ing, 37(3).N.
Xue and M. Palmer.
2004.
Calibrating features for semanticrole labeling.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing (EMNLP).196
