Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 54?59Manchester, August 2008Speech to Speech translation for Nurse Patient InteractionFarzad Ehsani, Jim Kimzey, ElaineZuber, Demitrios MasterFluential Inc./ 1153 Bordeaux Dr.,Sunnyvale, CA 94089{farzad,jkimzey,elaine,dlm}@fluentialinc.comKaren SudreTeleNav, Inc./1130 Kifer Road,Sunnyvale CA, 94086karens@telenav.comAbstractS-MINDS is a speech translation system,which allows an English speaker to commu-nicate with a limited English proficiencyspeaker easily within a question-and-answer,interview-style format.
It can handle dialogsin specific settings such as nurse-patient in-teraction, or medical triage.
We have builtand tested an English-Spanish system for ena-bling nurse-patient interaction in a number ofdomains in Kaiser Permanente achieving a to-tal translation accuracy of 92.8% (for bothEnglish and Spanish).
We will give an over-view of the system as well as the quantitativeand qualitatively system performance.1 IntroductionThere has been a lot of work in the area ofspeech to speech translation by CMU, IBM, SRI,University of Geneva and others.
In a health caresetting, this technology has the potential to givenurses and other clinical staff immediate accessto consistent, easy-to-use, and accurate medicalinterpretation for routine patient encounters.
Thiscould greatly improve safety and quality of carefor patients who speak a different language fromthat of the healthcare provider.This paper describes the building and testing of aspeech translation system, S-MINDS (SpeakingMultilingual Interactive Natural Dialog System),built in less than 3 months with Kaiser Perma-nente Hospital in San Francisco, CA.
The sys-tem was able to gain fairly robust results for thedomains that it was designed for, and we believe?
2008.
Licensed under the Creative Commons At-tribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.that it does demonstrate that building and deploy-ing a successful speech translation system is be-coming possible and even commercially viable.2 BackgroundThe number of people in the U.S. who speak alanguage other than English is large and grow-ing, and Spanish is the most commonly spokenlanguage next to English.
According to the 2000census, 18% of the U.S. population over age 5(47 million people) did not speak English athome?a 48% increase from 1990.
In 2000, 8%of the population (21 million) was LEP (LimitedEnglish Proficiency), with more than 65% of thatpopulation (almost 14 million people) speakingSpanish.A body of research shows that language barriersimpede access to care, compromise quality, andincrease the risk of adverse outcomes.
Althoughtrained medical interpreters and bilingual health-care providers are effective in overcoming suchlanguage barriers, the use of semi-fluent health-care professionals and ad hoc interpreters (suchas family members and friends) cause more in-terpreter errors and lower quality of care (Flores2005).When friends and family interpret, they are proneto omit, add, and substitute information.
Oftenthey inject their own opinions and observations,or impose their own values and judgments, ratherthan interpreting what the patient actually said.Frequently these ad hoc interpreters have limitedEnglish capabilities themselves and areunfamiliar with medical terminology.Furthermore, many patients are reluctant todisclose private or sensitive information in frontof a family member, thus giving the doctor anincomplete picture; this sometimes prevents a54correct diagnosis.
For example, a batteredwoman is unlikely to reveal the true cause of herinjuries if her husband is being used as theinterpreter.The California Academy of Family PhysiciansFoundation conducted practice visits in 2006 andfound that, ?Although they realize the use offamily members or friends as interpreters isprobably not the best means of interpretation, allpractice teams use them.?
(Chen et al2007)3 System DescriptionFluential?s speech translation system, S-MINDS1, has a hybrid architecture (Figure 1)that combines multiple ASR engines and multi-ple translation engines.
This approach onlyslightly increases the development cost of newtranslation applications, but it greatly improvesthe accuracy and the coverage of the system byleveraging the strengths of both statistical andgrammar/rules-based systems.
Furthermore, thishybrid approach enables rapid integration of newspeech recognition and translation engines asthey become available.Figure 1.
The hybrid system architecture of S-MINDS combines multiple ASR engines with an in-terpretation engine and an SMT engine.
Note that thisfigure describes the interaction in English to-secondlanguage direction only.
The 2nd language-to-Englishdirection has only the small ASR engine and the in-terpretation engine.3.1 Components of Speech TranslationSystemS-MINDS has a modular architecture with thecomponents described below.
All of these com-ponents already exist, so they will not need to be1Speaking Multilingual Interactive Natural DialogSystemdeveloped to conduct the research proposed inPhase I.3.1.1 ASR EngineS-MINDS employs multiple acoustic engines sothe best engine can be chosen for each language.Within each language, two separate languagemodels are active at the same time, telling theASR engines which words and phrases to recog-nize.
A smaller, more directed language modelwith higher accuracy is used to capture importantand frequently used concepts.
For less frequentlyused concepts, a larger language model that gen-erally has broader coverage but somewhat loweraccuracy is used.
The combination of these twoprovides high accuracy for responses that can beanticipated and slightly lower accuracy butbroader coverage for everything else.
Thismethod also allows development of new domainswith very little data?for each domain, only anew domain-specific small language modelneeds to be built.3.1.2 Interpretation EngineFluential has created an interpretation engine thatis an alternative to an SMT engine.
The S-MINDS interpretation engine uses informationextracted from the output of the ASR engine andthen performs a paraphrase translation in seman-tic space.
This process is similar to what humaninterpreters do when they convey the essentialmeaning without providing a literal translation.The advantage of an interpretation engine is thatnew domains can be added more quickly andwith less data than is possible with an SMT en-gine.
For high?volume, routine interactions, aninterpretation engine can be extremely fast andhighly accurate; however, the translation maylose some of the nuance.
Again, this means thathighly accurate target applications can be builtwith very little data?only a few examples ofeach concept are needed to train the interpreta-tion engine.3.1.3 Statistical Machine Translation En-gineFor the S-MINDS SMT engine, Fluential is de-veloping a novel approach that has generally im-proved the accuracy of speech translation sys-tems.2 This approach capitalizes on the intuitionthat language is broadly divided into two levels:2This effort is ongoing; it has not yet been fullyimplemented.55structure and vocabulary.
Traditional statisticalapproaches force the system to learn both typesof information simultaneously.
However, if theacquisition of structural information is kept sepa-rate from the acquisition of vocabulary, the re-sulting system should learn both levels more ef-ficiently.
And by modifying the existing corpusto separate structure and vocabulary, we havebeen able to take full advantage of all the infor-mation in the bilingual corpus, producing higher-quality MT without requiring large bodies oftraining data.
The most recent modification tothis approach was the use of distance-based or-dering (Zens and Ney, 2003) and lexicalized or-dering (Tillmann and Zhang, 2005) to allow formultiple language models, including non-wordmodels such as part-of-speech improved searchalgorithm, in order to improve its speed and effi-ciency.3.1.4 VUI+GUI SystemS-MINDS has a flexible user interface that canbe configured to use VUI only or VUI+GUI foreither the English speaker or the second-language speaker.
Also, the English speaker canexperience a different user interface than the sec-ond-language speaker.
The system has the flexi-bility to use multiple types of microphones, in-cluding open microphones, headsets, and tele-phone headsets.
Speech recognition can be con-firmed by VUI, GUI, or both, and it can be con-figured to verify all utterances, no utterances, orjust utterances that fall below a certain confi-dence level.3.1.5 Synthesis EngineS-MINDS can use text-to-speech (TTS) synthesisthroughout the system; alternatively, it can useTTS in its SMT-based system and chunk-basedrecordings that are spliced together in its inter-pretation engine.
Fluential licenses its TTS tech-nology from Cepstral, and other vendors.
Ingeneral we do not expect to be doing any re-search and development activities in this area, asCepstral can easily create good synthesis modelsfrom the 10 hours of provided speech data(Schultz and Black, 2006, Peterson, 2007).4 System BuildingFluential conducted fiver activities in order tobuild the system.
They included: (1) Definingthe task, (2) Collecting speech data to modelnurse-patient interactions, (3) Building and test-ing a speech translation system in English andSpanish, (4) Using the system with patients andnurses and collecting data to measure systemperformance, and (5) Analyzing the results.To define the task, Fluential conducted a two-hour focus group with six registered nurses fromMed/Surg unit of Kaiser Medical Center in SanFrancisco.
In this focus group, the nurses identi-fied six nurse-patient encounter types that theywanted to use for the evaluation.
These were:(1) Greeting/Goodbye, (2) Vital Signs, (3) PainAssessment, (4) Respiratory Assessment, (5)Blood Sugar, (6) Placement of an I.V.Fluential then collected speech data over a four-week period by recording nurse-patient interac-tions involving 11 nurses and 25 patients.
Fluen-tial also recruited 59 native Spanish speakerswho provided speech data using an automatedsystem that walked them through hypotheticalscenarios and elicited their responses in Spanish.The English recognizer had a vocabulary of2,003 and it was trained with 9,683 utterances.The Spanish recognizer had a vocabulary of 822,and it was trained with 1,556 utterances.
Wesuspect that the vocabulary size in Spanish wouldhave been much bigger if we had more data.5 System EvaluationAfter building and testing the speech translationsystem, Fluential conducted a two-hour trainingsession for each of the nurses before using thesystem with patients.
A bilingual research assis-tant explained the study to patients, obtainedtheir consent, and trained them for less than fiveminutes on the system.
Nurses then used the sys-tem with Spanish-speaking patients for the sixnurse-patient encounters that were built into thesystem.
The system was used by three nurseswith eleven patients for a total of 95 nurse-patient encounters creating a total of 500 conver-sation segments.3To protect patients from a mistranslation, eachencounter was remotely monitored by a bilingualinterpreter, who immediately notified the nurseany time the system mistranslated.
Each encoun-ter was recorded, transcribed, and translated by ahuman.3.1 Scoring Accuracy3A conversation segment is a single con-tinuous sequence of speech in a single languageplus the translation of what was said.56The human translations were compared to thesystem?s translations and given a score using theLaws Methodology of either Good, Fair, Poor,Mistranslated, or Not Translated.
(Laws, 2004).If a translation were scored as Good or Fair, itwas considered accurate.
If the translation werescored as Poor, Mistranslated, or Not Translated,it was considered inaccurate.Table 2 and 3 give examples of how we haveused Law?s method to grade actual interactionresults from nurses and patients.Table 2: Nurse Scoring ExamplesWhatNurse SaidS-MINDSTMTranslationHumanTranslationS-MINDSAccuracyI will giveyou an I.V.Voy a colo-carle uncateter paraliquidosintraveno-sos.Voy a colo-carle uncateter deliquidosintravenosos.GoodLet mecheck if Ican giveyou medi-cation forthat.Dejemechequear sipuedo darlealgun medi-camento.Permitamereviso sipuedo darlealgun medi-camento paraeso.FairI willcheckyour?Yo voy arevisarle losvendajesVoy a revisarsu ?PoorDid some-one takeyour vi-tals?
?Le tomaresus signosvitals?
?Alguientomo sussignos vi-tals?Mis-translatedYour heartrate isnormal.---Su frecuen-cia cardiacaes normal.NotTrans-latedTable 3: Patient Scoring ExamplesWhatPatientSaidS-MINDSTranslationHumanTranslationS-MINDSAccuracyNo, notengodificultaden respi-rar.I don't havedifficultybreathing.No, I don'thave diffi-culty breath-ing.GoodEn laparte bajadel esto-mago.The lowerpart of mystomach.In the lowerpart of mystomach.FairN/A N/A N/A PoorN/A N/A N/A MistranslatedLoshuesos.
--- My bones.Not Trans-lated6 Results- Our internal milestones for Phase I was toachieve 80% accuracy using the Laws Method-ology.
Out of 500 conversation segments, thespeech translation system had an overall accu-racy rate of 93% combining both nurse- and pa-tient-conversation segments,91.80%1.00% 0.60%4.20% 2.40%0%10%20%30%40%50%60%70%80%90%100%Good Fair Poor Mistranslated Not TranslatedFigure 2: Total results for both nurses andpatients.6.1 Nurse Translation ResultsLooking at just nurse conversation segments, thespeech translation system had higher accuracythan for patient segments.
Out of 404 nurse seg-ments, the speech translation system had a 94%accuracy rate.0%10%20%30%40%50%60%70%80%90%100%Good Fair Poor Mistranslated Not TranslatedFigure 3: Accuracy for Nurse ConversationalSegmentsThe biggest problem with system performancewith nurses was with mistranslations.
Whennurses tried to say things that were not in the sys-tem, the system tried to map their utterances tosomething that was in the system.
In each case ofmistranslation, the system told the nurse what itwas about to translate, gave the nurse a chance tostop the translation, and then translated thewrong thing when the nurse did not respond.
Webelieve that system performance can be greatlyimproved in by collecting more speech data frompatients and nurses, making changes to the userinterface, and improving our training program.576.2  Patient Translation ResultsLooking at just patient conversation segments,the speech translation system had lower overallaccuracy than for nurse segments.
Out of 96 pa-tient segments, the speech translation system hada 90% accuracy rate.0%10%20%30%40%50%60%70%80%90%100%Good Fair Poor Mistranslated Not TranslatedFigure 4: Results for PatientsAll of the problems with system performancewith patients were with responses that the systemwas not able to translate.
The system never gavea Poor translation or Mistranslated.
So there weretimes when the nurse knew that the patient triedto say something that the system could not trans-late, but there was never a time when the systemgave the nurse false information.
However, thispercentage is quite high, and in a large context, itmight cause additional problems.6.3 Nurse Survey ResultsAfter each time using the system, the nursescompleted a user satisfaction survey that had fivestatements and asked them assign a 1-to-5 Likertscore to each statement with 1 meaning?Strongly Disagree?
and 5 meaning ?StronglyAgree.?
Average scores for each question were:4.7  The speech translator was easyto use.4.5  The English voice was fluentand easy to understand.4.4  I understood the patient betterbecause of the speech translator.4.5  I feel that I am providing bettermedical care because of the speech translator.4.7  I would like to use the speechtranslator with my patients in the future.6.4 Patient Survey ResultsThe patients also completed a similar user satis-faction survey, translated to Spanish, after usingthe system.
Their average scores for each ques-tion were:4.6  The speech translator was easyto use.4.8  The Spanish voice was fluentand easy to understand.4.7  I understood my nurse better be-cause of the speech translator.5.0  I feel that I am receiving bettermedical care because of the speech translator.4.9  I would like to use the speechtranslator with my nurse in the future.6.5 ANOVA TestingWe conducted Analysis of Variance (ANOVA)testing to evaluate whether there were any sig-nificant variations in translation accuracy by pa-tient, nurse, or encounter type.
There were nosignificant differences.7 DiscussionWe were able to build and evaluate a system in 3months and show its utility by nurses and pa-tients in clinical setting.
The system seemed towork and was liked by both nurses and patients.The next question is whether such a system canscale and cover a much larger domain, and howmuch data and training is required to accomplishthis.ReferencesChen A., et al (2007), Addressing Language andCulture?A Practice Assessment for HealthCare Professionals, p3.Flores Glenn, (2005), ?The Impact of MedicalInterpreter Services on the Quality of HealthCare: A Systematic Review,?
Medical CareResearch and Review, Vol.
62, No.
3, pp.
255-29Laws, MB, Rachel Heckscher, Sandra Mayo,Wenjun.
Li, Ira  Wilson, (2004), ?A NewMethod for Evaluating the Quality of MedicalInterpretation,?
Medical Care.
42(1):71-80,January 2004Peterson Kay (2007).
Senior Linguist, CepstralLLC, Personal Communication.Schultz, Tanja and A. W Black (2006),?Challenges with Rapid Adaptation of SpeechTranslation Systems to New Language Pairs?58Proceedings of the IEEE InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP-2006), Toulouse, France,May 15-19, 2006.Tillmann, Christoph and T. Zhang, (2005), ?Alocalized prediction model for statisticalmachine translation,?
in Proceedings of the43rd Annual Meeting of the ACL, pp.
557-564,Ann Arbor, June 2005.Zens, Richard, and H. Ney, (2003), ?A compara-tive study of reordering constraints in statisti-cal machine translation,?
in Proceedings of the41st Annual Meetings of the ACL, pp.
144-151, Sapporo, Japan, July 2003Association forComputing Machinery.
1983.
Computing Re-views, 24(11):503-512.59
