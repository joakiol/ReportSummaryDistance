Information Extraction for Question Answering:Improving Recall Through Syntactic PatternsValentin Jijkoun and Maarten de RijkeInformatics InstituteUniversity of Amsterdam{jijkoun,mdr}@science.uva.nlJori MurInformation ScienceUniversity of Groningenmur@let.rug.nlAbstractWe investigate the impact of the precision/recalltrade-off of information extraction on the per-formance of an offline corpus-based questionanswering (QA) system.
One of our findings isthat, because of the robust final answer selectionmechanism of the QA system, recall is more im-portant.
We show that the recall of the extrac-tion component can be improved using syntac-tic parsing instead of more common surface textpatterns, substantially increasing the number offactoid questions answered by the QA system.1 IntroductionCurrent retrieval systems allow us to locate docu-ments that might contain the pertinent information,but most of them leave it to the user to extract theuseful information from a ranked list of documents.Hence, the (often unwilling) user is left with a rel-atively large amount of text to consume.
There isa need for tools that reduce the amount of text onemight have to read to obtain the desired informa-tion.
Corpus-based question answering is designedto take a step closer to information retrieval ratherthan document retrieval.
The question answering(QA) task is to find, in a large collection of data,an answer to a question posed in natural language.One particular QA strategy that has proved suc-cessful on large collections uses surface patterns de-rived from the question to identify answers.
For ex-ample, for questions like When was Gandhi born?,typical phrases containing the answer are Gandhiwas born in 1869 and Gandhi (1869?1948).
Theseexamples suggest that text patterns such as ?namewas born in birth date?
and ?name (birthyear?death year)?
formulated as regular ex-pressions, can be used to select the answer phrase.Similarly, such lexical or lexico-syntactic pat-terns can be used to extract specific information onsemantic relations from a corpus offline, before ac-tual questions are known, and store it in a repositoryfor quick and easy access.
This strategy allows oneto handle some frequent question types: Who is.
.
.
,Where is.
.
.
, What is the capital of.
.
.
etc.
(Fleis-chman et al, 2003; Jijkoun et al, 2003).A great deal of work has addressed the problemof extracting semantic relations from unstructuredtext.
Building on this, much recent work in QAhas focused on systems that extract answers fromlarge bodies of text using simple lexico-syntacticpatterns.
These studies indicate two distinct prob-lems associated with using patterns to extract se-mantic information from text.
First, the patternsyield only a small subset of the information that maybe present in a text (the recall problem).
Second, afraction of the information that the patterns yield isunreliable (the precision problem).
The precision ofthe extracted information can be improved signif-icantly by using machine learning methods to filterout noise (Fleischman et al, 2003).
The recall prob-lem is usually addressed by increasing the amountof text data for extraction (taking larger collections(Fleischman et al, 2003)) or by developing moresurface patterns (Soubbotin and Soubbotin, 2002).Some previous studies indicate that in the settingof an end-to-end state-of-the-art QA system, withadditional answer finding strategies, sanity check-ing, and statistical candidate answer re-ranking, re-call is more of a problem than precision (Bernardi etal., 2003; Jijkoun et al, 2003): it often seems use-ful to have more data rather than better data.
Theaim of this paper is to address the recall problemby using extraction methods that are linguisticallymore sophisticated than surface pattern matching.Specifically, we use dependency parsing to extractsyntactic relations between entities in a text, whichare not necessarily adjacent on the surface level.
Asmall set of hand-built syntactic patterns allows usto detect relevant semantic information.
A com-parison of the parsing-based approach to a surface-pattern-based method on a set of TREC questionsabout persons shows a substantial improvement inthe amount of the extracted information and num-ber of correctly answered questions.In our experiments we tried to understandwhether linguistically involved methods such asparsing can be beneficial for information extraction,where rather shallow techniques are traditionallyemployed, and whether the abstraction from surfaceto syntactic structure of the text does indeed help tofind more information, at the same time avoiding thetime-consuming manual development of increasingnumbers of surface patterns.The remainder of the paper is organized as fol-lows.
In Section 2 we discuss related work onextracting semantic information.
We describe ourmain research questions and experimental setting inSection 3.
Then, in Section 4 we provide detailson the extraction methods used (surface and syntac-tic).
Sections 5 and 6 contain a description of ourexperiments and results, and an error analysis, re-spectively.
We conclude in Section 7.2 Related WorkThere is a large body of work on extracting seman-tic information using lexical patterns.
Hearst (1992)explored the use of lexical patterns for extractinghyponym relations, with patterns such as ?such as.
?Berland and Charniak (1999) extract ?part-of?
rela-tions.
Mann (2002) describes a method for extract-ing instances from text by means of part-of-speechpatterns involving proper nouns.The use of lexical patterns to identify answers incorpus-based QA received lots of attention after ateam taking part in one of the earlier QA Tracksat TREC showed that the approach was competi-tive at that stage (Soubbotin and Soubbotin, 2002;Ravichandran and Hovy, 2002).
Different aspects ofpattern-based methods have been investigated since.E.g., Ravichandran et al (2003) collect surface pat-terns automatically in an unsupervised fashion us-ing a collection of trivia question and answer pairsas seeds.
These patterns are then used to generateand assess answer candidates for a statistical QAsystem.
Fleischman et al (2003) focus on the preci-sion of the information extracted using simple part-of-speech patterns.
They describe a machine learn-ing method for removing noise in the collected dataand showed that the QA system based on this ap-proach outperforms an earlier state-of-the-art sys-tem.
Similarly, Bernardi et al (2003) combine theextraction of surface text patterns with WordNet-based filtering of name-apposition pairs to increaseprecision, but found that it hurt recall more than ithelped precision, resulting in fewer questions an-swered correctly when the extracted information isdeployed for QA.The application of deeper NLP methods has alsoreceived much attention in the QA community.
Theopen-domain QA system by LCC (Moldovan et al,2002) uses predicate-argument relations and lexicalchaining to actually prove that a text snippet pro-vides an answer to a question.
Katz and Lin (2003)use syntactic dependency parsing to extract rela-tions between words, and use these relations ratherthan individual words to retrieve sentences relevantto a question.
They report a substantial improve-ment for certain types of questions for which theusual term-based retrieval performs quite poorly,but argue that deeper text analysis methods shouldbe applied with care.3 Experimental SettingWe set up experiments to address two related issues.First, we wanted to understand how the usual pre-cision/recall trade-off shows up in off-line corpus-based QA, and specifically, whether extracting moredata of lower quality (i.e., favoring recall) givesa QA system a better performance than extractingsmaller amounts of more accurate data (i.e., favor-ing precision).
Second, we tried to verify the hy-pothesis that syntactic parsing for information ex-traction does increase the extraction recall by iden-tifying relations between entities not adjacent on thesurface layer but connected syntactically.There are different approaches to the evaluationof information extraction modules.
The usual recalland precision metrics (e.g., how many of the inter-esting bits of information were detected, and howmany of the found bits were actually correct) requireeither a test corpus previously annotated with therequired information, or manual evaluation (Fleis-chman et al, 2003).
Although intrinsic evaluationof an IE module is important, we were mainly inter-ested in measuring the performance of this modulein context, that is, working as a sub-part of a QAsystem.
We used the number of questions answeredcorrectly as our main performance indicator.3.1 QA SystemFor the experiments described below we used anopen-domain corpus-based QA system QUARTZ(Jijkoun et al, 2004).
The system implementsa multi-stream approach, where several differentstrategies are used in parallel to find possible an-swers to a question.
We ran the system turning ononly one stream, Table Lookup, which implementsan off-line strategy for QA.The Table Lookup stream uses a number ofknowledge bases created by pre-processing a doc-ument collection.
Currently, QUARTZ?
knowledgebases include 14 semi-structured tables containingvarious kinds of information: birth dates of persons,dates of events, geographical locations of differentobjects, capitals and currencies of countries, etc.
Allthis information is extracted from the corpus off-line, before actual questions are known.An incoming question is analyzed and assignedto one of 37 predefined question types.
Based onthe question type, the Table Lookup stream identi-fies knowledge bases where answers to the questioncan potentially be found.
The stream uses keywordsfrom the question to identify relevant entries in theselected knowledge bases and extracts candidate an-swers.
Finally, the QA system reranks and sanitychecks the candidates and selects the final answer.3.2 Questions and CorpusTo get a clear picture of the impact of using dif-ferent information extraction methods for the off-line construction of knowledge bases, similarly to(Fleischman et al, 2003), we focused only onquestions about persons, taken from the TREC-8 through TREC 2003 question sets.
The ques-tions we looked at were of two different types:person identification (e.g., 2301.
What composerwrote ?Die Go?tterda?mmerung??)
and person defi-nition (e.g., 959. Who was Abraham Lincoln?).
Theknowledge base relevant for answering questions ofthese types is a table with several fields containinga person name, an information bit about the per-son (e.g., occupation, position, activities), the con-fidence value assigned by the extraction modulesto this information bit (based on its frequency andthe reliability of the patterns used for extraction),and the source document identification.
The TableLookup finds the entries whose relevant fields bestmatch the keywords from the question.We performed our experiments with the 336TREC questions about persons that are known tohave at least one answer in the collection.
Thecollection used at TREC 8, 9 and 10 (referred toas TREC-8 in the rest of the paper) consists of1,727,783 documents, with 239 of the correspond-ing questions identified by our system as askingabout persons.
The collection used at TREC 2002and 2003 (AQUAINT) contains 1,033,461 docu-ments and 97 of the questions for these editions ofTREC are person questions.4 Extraction of Role InformationIn this section we describe the two extraction meth-ods we used to create knowledge bases containinginformation about persons: extraction using surfacetext patterns and using syntactic patterns.Clearly, the performance of an information ex-traction module depends on the set of language phe-nomena or patterns covered, but this relation is notstraightforward: having more patterns allows one tofind more information, and thus increases recall, butit might introduce additional noise that hurts preci-sion.
Since in our experiments we aimed at com-paring extraction modules based on surface text vs.syntactic patterns, we tried to keep these two mod-ules parallel in terms of the phenomena covered.First, the collections were tagged with a NamedEntity tagger based on TnT (TnT, 2003) and trainedon CoNLL data (CoNLL, 2003).
The Named Entitytagger was used mainly to identify person names asseparate entities.
Although the tagging itself wasnot perfect, we found it useful for restricting oursurface text patterns.Below we describe the two extraction methods.4.1 Extraction with Surface Text PatternsTo extract information about roles, we used the setof surface patterns originally developed for the QAsystem we used at TREC 2003 (Jijkoun et al, 2004).The patterns are listed in Table 1.In these patterns, person is a phrase that istagged as person by the Named Entity tagger, roleis a word from a list of roles extracted from theWordNet (all hyponyms of the word ?person,?
15703entries),1 role-verb is from a manually con-structed list of ?important?
verbs (discovered, in-vented, etc.
; 48 entries), leader is a phrase identify-ing leadership from a manually created list of lead-ers (president, minister, etc.
; 22 entries).
Finally,superlat is the superlative form of an adjectiveand location is a phrase tagged as location bythe Named Entity tagger.4.2 Extraction with Syntactic PatternsTo use the syntactic structure of sentences for roleinformation extraction, the collections were parsedwith Minipar (Lin, 1998), a broad coverage depen-dency parser for English.
Minipar is reported toachieve 88% precision and 80% recall with respectto dependency relations when evaluated on the SU-SANNE corpus.
We found that it performed wellon the newpaper and newswire texts of our collec-tions and was fairly robust to fragmented and notwell-formed sentences frequent in this domain.
Be-fore extraction, Minipar?s output was cleaned andmade more compact.
For example, we removedsome empty nodes in the dependency parse to re-solve non-local dependencies.
While not loosingany important information, this made parses easierto analyse when developing patterns for extraction.Table 2 lists the patterns that were used to ex-tract information about persons; we show syntacticdependencies as arrows from dependents to heads,with Minipar?s dependency labels above the arrows.As with the earlier surface patterns, role is oneof the nouns in the list of roles (hyponyms of person1The list of roles is used to increase precision by filteringout snippets that may not be about roles; in some of the experi-ments below, we turn this filtering mechanism off.Pattern Example... role, person The British actress, Emma Thompson... (superlat|first|last)..., person The first man to set foot on the moon, Armstrongperson,... role... Audrey Hepburn, goodwill ambassador for UNICEF.person,... (superlat|first|last)... Brown, Democrats?
first black chairman.person,... role-verb... Christopher Columbus, who discovered America,... role person District Attoney Gil Garcettirole... person The captain of the Titanic Edward John Smithperson,... leader... location Tony Blair, the prime minister of Englandlocation... leader, person The British foreign secretary , Jack StrawTable 1: Surface patterns.Pattern ExampleApposition person appo???
?role a major developer, Joseph BeardApposition person appo???
?role Jerry Lewis, a Republican congressmanClause person subj???
?role-verb Bell invented the telephonePerson person person???
?role Vice President Al GoreNominal modifier person nn???
?role businessman Bill ShockleySubject person subj???
?role Alvarado was chancellor from 1983 to 1984Conjunction person conj???
?role Fu Wanzhong, director of the Provincial Department of Foreign Trade(this is a frequent parsing error)Table 2: Syntactic patterns.in WordNet), role-verb is one of the ?importantverbs.?
The only restriction for person was that itshould contain a proper noun.When an occurence of a pattern was found ina parsed sentence, the relation (person; info-bit) was extracted, where info-bit is a se-quence of all words below role or role-verbin the dependency graph (i.e., all dependents alongwith their dependents etc.
), excluding the per-son.
For example, for the sentence Jane Goodall,an expert on chimps, says that evidence for so-phisticated mental performances by apes has be-come ever more convincing, that matches the pat-tern person appo???
?role, the extracted informa-tion was (Jane Goodall; an expert on chimps).5 Experiments and ResultsWe ran both surface pattern and syntactic patternextraction modules on the two collections, with aswitch for role filtering.
The performance of the Ta-ble Lookup stream of our QA system was then eval-uated on the 336 role questions using the answerpatterns provided by the TREC organizers.
An earlyerror analysis showed that many of the incorrectanswers were due to the table lookup process (seeSection 3) rather than the information extractionmethod itself: correct answers were in the tables,but the lookup mechanism failed to find them orpicked up other, irrelevant bits of information.
Sincewe were interested in evaluating the two extractionmethods rather than the lookup mechanism, we per-formed another experiment: we reduced the sizesof the collections to simplify the automatic lookup.For each TREC question with an answer in the col-lection, NIST provides a list of documents that areknown to contain an answer to this question.
We puttogether the document lists for all questions, whichleft us with much smaller sub-collections (16.4 MBfor the questions for the TREC-8 collection and 3.2MB for the AQUAINT collection).
Then, we ran thetwo extraction modules on these small collectionsand evaluated the performance of the QA system onthe resulting tables.
All the results reported belowwere obtained with these sub-collections.
Compari-son of the extraction modules on the full TREC col-lections gave very similar relative results.Table 3 gives the results of the different runs forthe syntactic pattern extraction and the surface pat-tern extraction on the TREC-8 collection: the num-ber of correct answers (in the top one and the topthree answer candidates) for the 239 person ques-tions.
The columns labeled Roles+ show the resultsfor the extraction modules using the list of possibleroles from WordNet (Section 4), and the columns la-beled Roles ?
show the results when the extractionmodules consider any word as possibly denoting arole.
The results of the runs on the AQUAINT col-lection with 97 questions are shown in Table 4.The syntactic pattern module without role filter-ing scored best of all, with more than a third of theSyntactic patterns Surface patternsRank Roles ?
Roles + Roles ?
Roles +1 80 (34%) 73 (31%) 59 (25%) 54 (23%)1?3 90 (38%) 79 (33%) 68 (29%) 59 (25%)Table 3: Correct answers for the TREC-8 collection(239 questions).Syntactic patterns Surface patternsRank Roles ?
Roles + Roles ?
Roles +1 16 (17%) 14 (14%) 9 (9%) 6 (6%)1?3 20 (21%) 14 (14%) 11 (11%) 6 (6%)Table 4: Correct answers for the AQUAINT collec-tion (97 questions).questions answered correctly for the TREC-8 col-lection.
Another interesting observation is that in allexperiments the modules based on syntactic patternsoutperformed the surface-text-based extraction.Furthermore, there is a striking difference be-tween the results in Table 3 (questions fromTREC 8, 9 and 10) and the results in Table 4(questions from TREC 2002 and 2003).
The ques-tions from the more recent editions of TREC areknown to be much harder: indeed, the Table Lookupstream answers only 21% of the questions fromTREC 2002 and 2003, vs. 38% for earlier TRECs.In all experiments, both for syntactic and surfacepatterns, using the list of roles as a filtering mecha-nism decreases the number of correct answers.
Us-ing lexical information from WordNet improves theprecision of the extraction modules less than it hurtsthe recall.
Moreover, in the context of our knowl-edge base lookup mechanism, low precision of theextracted information does not seem to be an ob-stacle: the irrelevant information that gets into thetables is either never asked for or filtered out duringthe final sanity check and answer selection stage.This confirms the conclusions of (Bernardi et al,2003): in this specific task having more data seemsto be more useful than having better data.To illustrate the interplay between the precisionand recall of the extraction module and the perfor-mance of the QA system, Table 5 gives the com-parison of the different extraction mechanisms (syn-tactic and surface patterns, using or not using thelist of roles for filtering).
The row labelled # factsshows the size of the created knowledge base, i.e.,the number of entries of the form (person, info), ex-tracted by each method.
The row labelled Preci-sion shows the precision of the extracted informa-tion (i.e., how many entries are correct, according toa human annotator) estimated by random samplingand manual evaluation of 1% of the data for each ta-ble, similar to (Fleischman et al, 2003).
The row la-belled Corr.
answers gives the number of questionscorrectly answered using the extracted information.Syntactic patterns Surface patternsRoles ?
Roles + Roles ?
Roles +# facts 29890 9830 28803 6028Precision 54% 61% 23% 68%Corr.
answers 34% 31% 25% 23%Table 5: Comparison of the tables built with differ-ent extraction methods on the TREC-8 collection.The results in Table 5 indicate that role filtering af-fects the syntactic and surfaces modules quite dif-ferently.
Filtering seems almost essential for thesurface-pattern-based extraction, as it increases theprecision from 23% to 68%.
This confirms the re-sults of Fleischman et al (2003): shallow methodsmay benefit significantly from the post-processing.On the other hand, the precision improvement forthe syntactic module is modest: from 54% to 61%.The data from the syntactic module containsmuch less noise, although the sizes of the extractedtables before role filtering are almost the same.
Af-ter filtering, the number of valid entries from thesyntactic module (i.e., the table size multiplied bythe estimated precision) is about 6000.
This is sub-stantially better than the recall of the surface module(about 4100 valid entries).6 Error AnalysisIn theory, all relatively simple facts extracted by thesurface pattern module should also be extracted bythe syntactic pattern module.
Moreover, the syn-tactic patterns should extract more facts, especiallyones whose structure deviates from the patterns pre-defined in the surface pattern module, e.g., whereelements adjacent in the syntactic parse tree are farapart on the surface level.
To better understand thedifferences between the two extraction approachesand to verify the conjecture that syntactic parsingdoes indeed increase the recall of the extracted in-formation, we performed a further (manual) erroranalysis, identifying questions that were answeredwith one extraction method but not with the other.Tables 6 and 7 gives the breakdown of the per-formance of the two modules, again in terms of thequestions answered correctly.
We show the resultsfor the 239 questions on the TREC-8 collection; forthe 97 questions on the AQUAINT corpus the rela-tive scores are similar.
As Tables 6 and 7 indicate,not all questions answered by the surface patternmodule were also answered by the syntactic patternmodule, contrary to our expectations.
We took acloser look at the questions for which the two mod-ules performed differently.Syntactic patternsSurfacepatterns correct incorrectcorrect 47 12incorrect 32 148Table 6: Performance analysis for the TREC-8 col-lection with role filtering.Syntactic patternsSurfacepatterns correct incorrectcorrect 51 17incorrect 39 132Table 7: Performance analysis for the TREC-8 col-lection without role filtering.6.1 Syntactic Patterns vs.
Surface PatternsThere were three types of errors responsible for pro-ducing an incorrect answer by the syntactic patternmodule for questions correctly answered with sur-face patterns.
The most frequent errors were pars-ing errors.
For 6 out of 12 questions (see Table 6)the answer was not extracted by the syntactic pat-tern method, because the sentences containing theanswers were not parsed correctly.
The next mostfrequent error was caused by the table lookup pro-cess.
For 4 questions out of the 12, the requiredinformation was extracted but simply not selectedfrom the table as the answer due to a failure of thelookup algorithm.
The remaining errors (2 out of12) were of a different type: for these 2 cases thesurface pattern extraction did perform better thanthe syntactic method.
In both cases this was becauseof wildcards allowed in the surface patterns.
E.g.,for the sentence .
.
.
aviator Charles Lindbergh mar-ried Anne Spencer Morrow.
.
.
the syntactic patternmethod extracted only the relation(Charles Lindbergh; aviator),whereas the surface pattern method also extracted(Anne Spencer Morrow; aviator Charles Lindberghmarried),because of the pattern ?role.
.
.
person?
withrole instantiated with aviator and person withAnne Spencer Morrow.
In fact, the extracted in-formation is not even correct, because Anne is notan aviator but Lindbergh?s wife.
However, due tothe fuzzy nature of the lookup mechanism, this newentry in the knowledge base allows the QA sys-tem to answer correctly the question 646. Who wasCharles Lindbergh?s wife?, which is not answeredwith the syntactic pattern extraction module.To summarize, of the 12 questions where the sur-face patterns outperformed the syntactic patterns?
6 questions were not answered by the syntacticmethod due to parsing errors,?
4 were not answered because of the tablelookup failure and?
for 2 the surface-based method was more ap-propriate.6.2 Surface Patterns vs. Syntactic PatternsWe also took a closer look at the 32 questions forwhich the syntactic extraction performed better thanthe surface patterns (see Table 6).
For the sur-face pattern extraction module there were also threetypes of errors.
First, some patterns were miss-ing, e.g., person role-verb....
The onlydifference from one of the actually used patterns(person,... role-verb...) is that thereis no comma between person and role-verb.This type of incompleteness of the set of the surfacepatterns was the cause for 16 errors out of 32.The second class of errors was caused by theNamed Entity tagger.
E.g., Abraham Lincoln wasalways tagged as location, so the name nevermatched any of the surface patterns.
Out of 32 ques-tions, 10 were answered incorrectly for this reason.Finally, for 6 questions out of 32, the syntacticextraction performed better because the informationcould not be captured on the surface level.
For ex-ample, the surface pattern module did not extractthe fact that Oswald killed Kennedy from the sen-tence .
.
.
when Lee Harvey Oswald allegedly shotand killed President John F. Kennedy.
.
.
, becausenone of the patterns matched.
Indeed, Lee HarveyOswald and the potentially interesting verb killedare quite far apart in the text, but there is an imme-diate relation (subject) on the syntactic level.It is worth pointing out that there were no lookuperrors for the surface pattern method, even thoughit used the exact same lookup mechanism as theapproach based on syntactic patterns (that did ex-perience various lookup errors, as we have seen).It seems that the increased recall of the syntacticpattern approach caused problems by making thelookup process harder.To summarize, out of 32 questions answered us-ing syntactic extraction method but not by the sur-face pattern approach?
16 questions would have required extendingthe set of surface patterns,?
10 questions were not answered because of NEtagging error, and?
6 questions required syntactic analysis for ex-traction of the relevant information.6.3 Adding Patterns?We briefly return to a problem noted for extrac-tion based on surface patterns: the absence of cer-tain surface patterns.
The surface pattern personrole-verb... was not added because, we felt,it would introduce too much noise in the knowledgebase.
With dependency parsing this is not an is-sue as we can require that person is the subjectof role-verb.
So in this case the syntactic pat-tern module has a clear advantage.
More generally,while we believe that extraction methods based onhand-crafted patterns are necessarily incomplete (inthat they will fail to extract certain relevant facts),these observations suggest that coping with the in-completeness is a more serious problem for the sur-face patterns than for the syntactic ones.7 ConclusionsWe described a set of experiments aimed at com-paring different information extraction methods inthe context of off-line corpus-based Question An-swering.
Our main finding is that a linguisticallydeeper method, based on dependency parsing and asmall number of simple syntactic patterns, allows anoff-line QA system to correctly answer substantiallymore questions than a traditional method based onsurface text patterns.
Although the syntactic methodshowed lower precision of the extracted facts (61%vs.
68%), in spite of parsing errors the recall washigher than that of the surface-based method, judg-ing by the number of correctly answered questions(31% vs. 23%).
Thus, the syntactic analysis can infact be considered as another, intensive way of im-proving the recall of information extraction, in ad-dition to successfully used extensive ways, such asdeveloping larger numbers of surface patterns or in-creasing the size of the collection.Moreover, we confirmed the claim that for a com-plex off-line QA system, with statistical as well asknowledge-intensive sanity checking answer selec-tion modules, recall of the information extractionmodule is more important than precision, and a sim-ple WordNet-based method for improving precisiondoes not help QA.
In our future work we plan to in-vestigate the effect of more sophisticated and, prob-ably, more accurate filtering methods (Fleischmanet al, 2003) on the QA results.8 AcknowledgementsValentin Jijkoun and Maarten de Rijke were sup-ported by a grant from the Netherlands Organiza-tion for Scientific Research (NWO) under projectnumber 220-80-001.
De Rijke was also sup-ported by NWO under project numbers 365-20-005, 612.069.006, 612.000.106, 612.000.207, and612.066.302.ReferencesM.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Proceedings of the 37th AnnualMeeting of the ACL.R.
Bernardi, V. Jijkoun, G. Mishne, and M. de Rijke.2003.
Selectively using linguistic resources through-out the question answering pipeline.
In Proceedingsof the 2nd CoLogNET-ElsNET Symposium.M.
Fleischman, E. Hovy, and A. Echihabi.
2003.
Offlinestrategies for online question answering: answeringquestions before they are asked.
In Proceedings of the41st Annual Meeting of the ACL.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thInternational Conference on Computational Linguis-tics (COLING-92).V.
Jijkoun, G. Mishne, and M. de Rijke.
2003.
Prepro-cessing Documents to Answer Dutch Questions.
InProceedings of the 15th Belgian-Dutch Conference onArtificial Intelligence (BNAIC?03).V.
Jijkoun, G. Mishne, C. Monz, M. de Rijke,S.
Schlobach, and O. Tsur.
2004.
The University ofAmsterdam at the TREC 2003 Question AnsweringTrack.
In Proceedings of the TREC-2003 Conference.B.
Katz and J. Lin.
2003.
Selectively using relations toimprove precision in question answering.
In Proceed-ings of the EACL-2003 Workshop on Natural Lan-guage Processing for Question Answering.D.
Lin.
1998.
Dependency-based evaluation of Minipar.In Proceedings of the Workshop on the Evaluation ofParsing Systems.G.
Mann.
2002.
Fine-grained proper noun ontologiesfor question answering.
In SemaNet?02: Building andUsing Semantic Networks.D.
Moldovan, S. Harabagiu, R. Girju, P. Morarescu,A.
Novischi F. Lacatusu, A. Badulescu, and O. Bolo-han.
2002.
LCC tools for question answering.
In Pro-ceedings of the TREC-2002.TnT Statistical Part of Speech Tagging.
2003.URL: http://www.coli.uni-sb.de/?thorsten/tnt/.CoNLL: Conference on Natural Language Learn-ing.
2003.
URL: http://cnts.uia.ac.be/signll/shared.html.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
In Pro-ceedings of the 40th Annual Meeting of the ACL.D.
Ravichandran, A. Ittycheriah, and S. Roukos.
2003.Automatic derivation of surface text patterns for amaximum entropy based question answering system.In Proceedings of the HLT-NAACL Conference.M.M.
Soubbotin and S.M.
Soubbotin.
2002.
Use of pat-terns for detection of likely answer strings: A system-atic approach.
In Proceedings of the TREC-2002 Con-ference.
