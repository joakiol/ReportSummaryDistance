INVITED TALKEye Movements and Spoken Language ComprehensionMichael K. Tanenhaus*Department of Brain and Cognitive SciencesUniversity of RochesterRochester, NY 14627mtan@bcs, rochester, eduMichael J. Spivey-KnowltonDepartment of PsychologyCornell UniversityIthaca, NY 14583mj sk@cornel  i. eduKathleen M. EberhardDepartment of PsychologyUniversity of Notre DameNotre Dame, IN 46556kathleen .m.
eberhard, l@nd.
eduJulie C. SedivyDepartment of LinguisticsUniversity of RochesterRochester, NY 14627sedivy@bcs, rochester,  eduPaul D. AllopennaDepartment of Brain and Cognitive SciencesUniversity of RochesterRochester, NY 14627al lopenObcs, rochester,  eduJames S. MagnusonDepartment of Brain and Cognitive SciencesUniversity of RochesterRochester, NY 14627magnuson@bcs,  rochester,  eduAbstractWe present an overview of recent work in which eyemovements are monitored as people follow spokeninstructions to move objects or pictures in a visual workspace.Subjects naturally make saccadic eye-movements to objectsthat are closely time-locked to relevant information in theinstruction.
Thus the eye-movements provide a window intothe rapid mental processes that underlie spoken languagecomprehension.
We review studies of reference resolution,word recognition, and pragmatic effects on syntacticambiguity resolution.
Our studies show that people seek toestablish reference with respect o their behavioral goalsduring the earliest moments of linguistic processing.Moreover, eferentially relevant non-linguistic informationimmediately affects how the linguistic input is initiallystructured.IntroductionMany important questions about language comprehensioncan only be answered by examining processes that areclosely time-locked to the linguistic input.
These processestake place quite rapidly and they are largely opaque tointrospection.
As a consequence, psycholinguists haveincreasingly turned to experimental methods designed to tapreal-time language processing.
These include a variety ofreading time measures as well as paradigms in whichsubjects monitor the incoming speech for targets or respondto visually presented probes.
The hope is that these "on-line" measures can provide information that can be used toinform and evaluate explicit computational models oflanguage processing.Although on-line measures have provided increasinglyfine-grained information about the time-course of languageprocessing, they ,are also limited in some important respects.Perhaps the most serious limitation is that they cannot beused to study language in natural tasks with real-worldreferents.
This makes it difficult to study how interpretationdevelops.
Moreover, the emphasis on processing"decontextualized" language may be underestimating theimportance of interpretive processes in immediate languageprocessing.Recently, we have been exploring a new paradigm forstudying spoken language comprehension.
Participants inour experiments follow spoken instructions to touch ormanipulate objects in a visual workspace while we monitortheir eye-movements u ing a lightweight camera mountedon a headband.
The camera, manufactured by AppliedScientific Laboratories, provides an infrared image of theeye at 60Hz.
The center of the pupil and the cornealreflection are tracked to determine the orbit of the eyerelative to file head.
Accuracy is better than one degree ofarc, with virtually unrestricted head and body movements\[Ballard, Hayhoe, and Pelz, 1995\].
Instructions are spokeninto a microphone connected to a Hi-8 VCR.
The VCR alsorecords the participant's field of view from a "scene"camera mounted on the headband.
The participant's gazefixation is superimposed on the video image We analyzeeach frame of the instructions to determine the location andtiming of eye movements with respect o critical words inthe instruction.We find that subjects make eye-movements to objects inthe visual workspace that are closely time-locked to relevantinformation in the instruction.
Thus the timing and patterns48of the eye movements provide a window intocomprehension processes as the speech unfolds.
Unlikemost of the on-line measures that have been used to studyspoken language processing in the past, our procedure canbe used to examine comprehension during natural tasks withreal-world referents \[Tanenhaus, M. K., Spivey-Knowlton,M.
J., Eberhard, K. M., & Sedivy, J. C., 1996\].In the remainder of this paper, we review some of ourrecent work using the visual world paradigm.
We will focuson three areas: (a) reference resolution; (b) wordrecognition, and (c) the interaction of referential context andsyntactic ambiguity resolution.Reference ResolutionEvidence for Ineremental InterpretationIn order to investigate the time course with which peopleestablish reference we use different displays to manipulatewhere in an instruction the referent of a definite noun phrasebecomes unique.
The timing and patterns of the eye-movements clearly show that people establish referenceincrementally by continuously evaluating the information inthe instruction against the alternatives in the visualworkspace.
For example, in one experiment \[Eberhard,Spivey-Knowlton, Sedivy & Tanenhaus, 1995\], participantswere told to touch one of four blocks.
The blocks variedalong three dimensions: marking (plain or starred), color(pink, yellow, blue and red) and shape (square or rectangle).The instructions referred to the block using a definite nounphrase with adjectives (e.g., "Touch the starred yellowsquare.").
The display determined which word in the nounphrase disambiguated the target block with respect o thevisual alternatives For example, the earliest point ofdisambiguation would be after "starred" if only one of theblocks was starred, after "yellow" if only one of the starredblocks was yellow, and after "square" if there were twostarred yellow blocks, only one of which was a square(Instructions with definite noun phrases always had a uniquereferen0.An instruction began with subjects looking at a fixationcross.
We then measured the latency from the beginning ofthe noun phrase until the onset of the eye-movement to thetarget object.
Subjects made eye-movements beforetouching the target block on about 75% of the trials.Eye-movement latencies increased monotonically as thepoint of disambiguation shifted from the marking adjectiveto the color adjective to the head noun.
Moreover, eye-movements were launched within 300 milliseconds of theend of the disambiguating word.
It takes about 200milliseconds from the point that an eye-movement isprogrammed until when the eye actually begins to move.On average then, participants began programming an eye-movement to the target block once they had heard thedisambiguating word and before they had finished hearingthe next word in the instruction.We used the same logic in an experiment with displayscontaining more objects and syntactically more complexinstructions \[Eberhard et al 1995\].
Participants wereinstructed to move miniature playing cards placed on slotson a 5X5 vertical board.
Seven cards were displayed oneach trial, A trial consisted of a sequence of threeinstructions.
On the instructions of interest, there were twocards of the same suit and denomination i  the display.
Thetarget card was disambiguated using a restrictive relativeclause, e.g.
"Put tile five of hearts that is below the eight ofclubs above the three of diamonds."
Figure 1 shows one ofthe displays for this instruction.10~ + 8~KO"Put the five of hearts that is below the eieht of clubsabove the tht~ee of diamonds.
"Figure 1: Display of cards in which their are two fives ofhearts.
As each five of heart is below a different numberedcard, the above instruction becomes unambiguous at "eight".The display determined tile point of disambiguation in theinstruction.
For the display in Figure 1, the point ofdisambiguation occurs after the word "eight" because onlyone of tile fives is below an eight.
We also used an earlypoint of disambiguation display in which only one of thepotential target cards was immediately below a" "context"card and a l..al?
point of disambiguation display in which thedenomination f the "context" card disambiguated the target(i.e., one five was below an eight of spades and the otherwas below and eight of clubs).Participants always made an eye-movement to the targetcard before reaching for it.
We again found a clear point ofdisambiguation effect.
The mean latency of the eye-movement that preceded tile hand movement to the targetcard (measured from a common point in the instruction)increased monotonically with the point of disambiguation.In addition, participants made sequences of eye-movements which made it clear that interpretation wastaking place continuously.
We quantified this by examiningthe probability that the subject would be looking at (fixatingon) particular classes of cards during segments of theinstruction.
For example, during the noun phrase thatintroduced the potential targets, "the five of hearts", nearlyall of tile fixations were on one of the potential target cards.49During the beginning of the relative clause "...that is belowthe...", most of the fixations were to one of the contextcards (i.e.
the card that was above or below a potentialtarget card).
Shortly after the disambiguating word, thefixations hifted to the target card.Contrast ive focusThe presence of a circumscribed set of referents in avisual model makes it possible to use eye-movements toexamine how presuppositional information associated withintonation is used in on-line comprehension.
\[Sedivy,Tanenhaus, Spivey-Knowlton, Eberhard & Carlson, 1995\]For example, semantic analyses of contrast have convergedon a representation f contrastive focus which involves theintegration of presupposed and asserted information \[e.g.,Rooth, 1992; Kratzer, 1991; Krifka, 1991\].
Thus a speakeruttering "Computational linguists give good talks" ismaking an assertion about computational linguists.However, a speaker who says "COMPUTATIONALlinguists give good talks."
is both complimenting thecommunity of computational linguists and making aderogatory comparison with a presupposed set ofcontrasting entities (perhaps the community of non-computationally oriented linguists).We explored whether contrast sets are computed on-lineby asking whether contrastive focus could be used todisambiguate among potential referents, using a variationon the point of disambiguation manipulation describedearlier.
We used displays with objects that could differalong three dimensions: size (large or small), color (red,blue and yellow), and shape (circles, triangles and squares).Each display contained four objects \[see Sedivy et al, 1995for details\].Consider now the display illustrated in Figure 2 whichcontains a small yellow triangle, a large blue circle and twored squares, one large and one small.
With the instruction"Touch the large red square."
the point of dismnbiguationcomes after "red".
After "large" there are still two possiblereferents: the large red square and the large blue circle.After "red" only the large red square is a possible referent..However, with the instruction "Touch the LARGE redsquare", contrastive focus on "large" restricts felicitousreference to objects that have a contrast member differingalong the dimension indicated by the contrast (size).
In thedisplay in Figure 2, the small red square contrasts with thelarge red square.
However, the display does not contain acontrast element for the large blue circle.
Thus, if peopleuse contrastive stress to compute acontrast set on-line, thenthey should have sufficient information to determine thetarget object after hearing the size adjective Thus eye-movements to the target object should be faster withcontrastive stress.
That is, in fact, what we found.Latencies to launch a saccade to the target were faster withcontrastive stress than with neutral stress.However, there is a possible objection to an interpretatiouinvoking contrasts ets.
One could argue that stress shnplyfocused participants' attention on the size dimension,allowing them to restrict attention to the large objects, Torule out this alternative, we also included isplays with twocontrast sets: e.g., two red squares, one large and one small,and two blue circles, one large and one small.
With a twocontrast display, contrastive focus is still felicitous.However, the point of disambiguation now does not comeuntil after the color adjective for instructions withcontrastive stress and with neutral stress.
Under theseconditions, we found no effect of contrast.
The interactionbetween type of display and stress provides clear evidencethat participants were computing contrast sets rapidlyenough to select among potential referents.11o+ hiAFigure 2: Display with one large and one smallred square.
The large circle is blue; the smalltriangle is yellow.Word RecognitionThe time course of spoken word recognition is stronglyinfluenced by both the properties of the word itself (e.g., itsfrequency) and the set of words to which it is phoneticallysimilar.
Recognition of a spoken word occurs shortly afterthe auditory input uniquely specifies a lexical candidate\[Marslen-Wilson, 1987\].
For polysyllabic words, this isoften prior to the end of the word.
For example, the word"elephant" would be recognized shortly after the "phoneme"If/.
Prior to that, the auditory input would be consistent withthe beginnings of several words, including "elephant","elegant", eloquent" and "elevator".Most models of spoken word recognition account forthese data by proposing that multiple lexical candidates areactivated a~s the speech stream unfolds.
Recognition thentakes place with respect o the set of competing activatedcandidates.
However, models differ in how the candidateset is defined.
In some models, such as Marslen-Wilson'sclassic Cohort model, competition takes place in a strictly"left-to right" fashion.
\[Marslen-Wilson, 1987\].
Thus thecompetitor set for "paddle" would contain "padlock", whichhas the same initial phonemes as "paddle", but would notinclude a phonetically similar word that did not overlap in50its initial phonemes, uch as a rhyming word like "saddle".In contrast, activation models uch as TRACE \[McClelland& Elman, 1986\] assume that competition can occurthroughout the word and thus rhyming words would alsocompete for activation.Our initial experiments used real objects and instructionssuch as "Pick up the candy".
We manipulated whether ornot the display contained an object with a name that beganwith the same phonetic sequence as the target object\[Tanenhaus, Spivey-Knowlton, Eberhard & Sedivy, 1995;Spivey-Knowlton, Tanenhaus, Eberhard & Sedivy, 1995\].Examples of objects with overlapping initial phonemes were"candy" and "candle", and "doll" and "dolphin".
An eye-movement to the target object ypically began shortly afterthe word ended, indicating that programming of the eye-movement often began before the end of the word.
Thepresence of a competitor increased the latency of eye-movements othe target and induced frequent false launchesto the competitor.
The timing of these eye-movementsindicated that they were programmed during the"ambiguous" segment of the target word.
These resultsdemonstrated that the two objects with similar names were,in fact, competing as the target word unfolded.
Moreover,they highlight the sensitivity of the eye-movementparadigm.In ongoing work, we are exploring more fine-grainedquestions about the t/me-course of lexical activation.
Forexample, in an experiment in progress \[Allopenna,Magnuson & Tanenhaus, 1996\], the stimuli are linedrawings of objects presented on a computer screen (seeFigure 3).
On each trial, participants are shown a set of fourobjects and asked to "pick up" one of the objects with themouse and move it to a specified location on the grid.
Thepaddle was the target object for the trial shown in Figure 3.The display includes a "cohort" competitor sharing initialphonemes with the target (padlock) a rhyme competitor(saddle) and an unrelated object (castle).tFigure 3: Sample Display for the Instxuction:"Pick up the paddle.
"Figure 4 shows the probability that the eye is fixating onthe target and the cohort competitor as the spoken targetword unfolds.
Early on in the speech stream, the eye is onthe fixation cross, where subjects are told to look at thebeginning of the trial.
The probability of a fixation to thetarget word and the cohort competitor then increases.
Asthe target word unfolds, the probability that the eye isfixated on the target increases compared to the cohortcompetitor.
These data replicate our initial experiments andshow how eye-movements can be used to trace the timecourse of spoken word recognition.
Our preliminary dataalso make it clear that rhyme competitors attract fixations,as predicted by activation models.,.o j - -0.9 " Target0,8o.7i 0.60.50.40.30.2o, .
\ :  oo o,,O.O5 O0 1000 1500 2000 2500Tlmetme)Figure 4: Probabilities of eye-fixations in acompetitor t ial.Reference and Syntactic Ambiguity ResolutionThere has been an unresolved ebate in the languageprocessing community about whether there are initial stagesin syntactic processing that are strictly encapsulated frominfluences of referential and pragmatic context.
Thestrongest evidence for encapsulated processing modules hascome from studies using sentences with brief syntactic"attachment" ambiguities in which readers have clearpreferences for interpretations, associated with particularsyntactic onfigurations.
For example, in the.
instruction"put the apple on the towel...," people prefer to attach theprepositional phrase "on the towel" to the verb "put", ratherthan the noun phrase "the apple", thus interpreting it as theargument of the verb (encoding the thematic relation ofGoal), rather than as a modifier of the noun.If the instruction continues "Put the apple on the towelinto the box", the initial preference for a verb-phraseattachment is revealed by clear "garden-path" effects when"into" is encountered.
Encapsulated models account for thispreference in terms of principles uch as pursue the simplest51attachment first, or initially attach a phrase as an argumentrather than as an adjunct.
In contrast, constraint-basedmodels attribute these preferences tothe strength of multipleinteracting constraints, including those provided bydiscourse context.
\[For a recent review, see Tanenhaus andTrueswell, 1995\]An influential proposal, most closely associated withCrain and Steedman \[1985\], is that pragmatically drivenexpectations about reference are an important source ofdiscourse constraint.
For example, a listener hearing "putthe apple..." might reasonably assume that there is a singleapple and thus expect o be told where to put the apple (theverb-phrase attachment).
However, in a context in whichthere was more than one apple, the listener might expect obe told which of the apples is the intended referent and thusprefer the noun phrase attachment.Numerous experiments have investigated whether or notthe referential context established by a discourse context canmodify attachment preferences.
These studies typicallyintroduce the context in a short paragraph and examine ye-movements to the disambiguating words in a targetsentences containing the temporary ambiguity.
While somestudies have shown effects of discourse context, others havenot.
In particular, strong syntactic preferences persistmomentarily, even when the referential context introducedby the discourse supports the normally less-preferredattachment.
For example, the preference toinitially attach aprepositional phrase to a verb requiring a goal argument(e.g., "put") cannot be overridden by linguistic context.These results have been taken as strong evidence for anencapsulated syntactic processing system.However, typical psycholinguistic experiments may bestrongly biased against finding pragmatic effects onsyntactic processing.
For example, the context may not beimmediately accessible because it has to be represented inmemory.
Moreover, readers may not consider the context tobe relevant when the ambiguous region of the sentence isbeing processed.We reasoned that a relevant visual context that wasavailable for the listener to interrogate as the linguistic inputunfolded might influence initial syntactic analysis eventhough the same information might not be effective whenintroduced linguistically.Sample instructions are illustrated by the examples in (1).1. a.
Put the apple on the towel in the box.b.
Put the apple that's on the towel in the box.In sentence (la), the first prepositional phrase "on thetowel", is ambiguous as to whether it modifies the nounphrase ("the apple") thus specifying the location of theobject o be picked up, or whether it modifies the verb, thusintroducing the goal location.
In example (lb) the word"that's" disambiguates the phrase as a modifier, serving asan unambiguous control condition.These instructions were paired with three types of displaycontexts.
Each context contained four sets of real objectsplaced on a horizontal board.
Sample displays for theinstructions presented in (1) are illustrated in Figures 5, 6,and 7 Three of file objects were the same across all of thedisplays.
Each display contained the target object (an appleon a towel) the correct goal, (a box) and an incorrect goal(another towel).
In the one referent display (Figure 4) therewas only one possible referent for the definite noun phrase"the apple", the apple on the towel.
Upon hearing the phrase"the apple", participants can immediately identify the objectto be moved because there is only one apple and thus theyare likely to assume that "on the towel" is specifying thegoal.
In the two-referent display (Figure 5), there was asecond possible referent (an apple on a napkin).
Thus, "theapple", could refer to either of the two apples and the phrase"on the towel" provides modifying information that specifieswhich apple is the correct referent.
Under these conditions alistener seeking to establish reference should interpret theprepositional phrase "on the towel" as providingdisambiguating information about he location of the apple.In the three and one display, we added an apple cluster.
Theuniqueness presupposition associated with the definite nounphrase should bias the listener to assume that the singleapple (the apple on the towel) is the intended referent for thetheme argument.
However, it is more felicitous to use amodifier with this instruction.
This display was used to testif even a relatively subtle pragmatic effects will influencesyntactic processingStrikingly different fixation patterns among the visualcontexts revealed that the ambiguous phrase "on the towel"was initially interpreted as the goal in the one-referentcontext but as a modifier in the two-referent contexts andthe three-and-one contexts \[for details see Spivey-Knowltonet al 1995; Tanenhaus et al, 1995\] In the one-referentcontext, subjects looked at the incorrect goal (e.g., theirrelevant towel) on 55% of the trials shortly after hearingthe ambiguous prepositional phrase, whereas they neverlooked at the incorrect goal with the unambiguousinstruction.
In contrast, when the context contained twopossible referents, subjects rarely looked at the incorrectgoal, and there were no differences between the ambiguous,mid unambiguous instructions.
Similar results obtained forthe three-and-one context.Figures 5 and 6 summarize the most typical sequences ofeye-movements and their timing in relation to words in themnbiguous instructions for the one-referent and the two-referent contexts, respectively.
In the one-referent context,subjects first looked at the target object (the apple) 500 msafter hearing "apple" then looked at the incorrect goal (thetowel) 484 ms after hearing "towel".
In contrast, with theunmnbiguous instruction, the first look to a goal did notoccur until 500 ms after the subject heard the word "box".In the two-referent context, subjects often looked at bothapples, reflecting the fact that the referent of "the apple" wastemporarily mnbiguous.
Subjects looked at the incorrectobject on 42% of the unambiguous trials and on 61% of themnbiguous trials.
In contrast, in the one-referent context,subjects rarely looked at the incorrect object (0% and 6% ofdie trials for die ambiguous and unambiguous instructions,respectively).
In the two-referent context, subjects elected52the correct referent as quickly for the ambiguous instructionas for the unambiguous instruction providing additionalevidence that the first prepositional phrase was immediatelyinterpreted as a modifier.The three-and-one context provided additionalinformation.
Typical sequences of eye-movements for thiscontext are presented in Figure 7.
Participants rarely lookedat the apple cluster, making their initial eye-movement tothe apple on the towel.
The next eye-movement was to thebox for both the ambiguous and unambiguous instruction.These data also rule out a possible objection to the resultsfrom the two referent condition.
One could argue thatparticipants were, in fact, temporarily misparsing theprepositional phrase as the goal.
However, this misanalysismight not be reflected in eye-movements to the towelbecause the eye was already in transit, moving between thetwo apples.
However, in the three-and-one condition, theeye remains on the referent hroughout the prepositionalphrase.
Given the sensitivity of eye-movements oprobabilistic information, e.g., false launches to cohort andrhyme competitors, it is difficult to argue that theparticipants experienced a temporary garden-path t at wastoo brief to influence ye-movements.
"Put the apple on the towel in the box.
"A B C D =,,._I I , I , I , I ,e , .
-ms  0 500 1000 1500 2000 2500"Put the apple that's on the towel in the box.
"A' B' I I , I I , I Irns 0 500 1000 1500 2000 2500=p,,,.-Figure 5: Typical sequence of eye movements in the one-referent context for the ambiguous and unambiguousinstructions.
Letters on the timeline show when in theinstruction each eye movement occurred, as determined bymean latency of that type of eye movement (A' and B'correspond tothe unambiguous instruction).
"Put the apple on the towel in the box.
"A B C re=I I I I I I .e - -ms  0 500 1000 1500 2000 2500"Put the apple that's on the towel in the box.
"I I I A .
I B ; C ,rns 0 500 1000 1500 2000 2500 "~"-Figure 6: Typical sequence of eye movements in the two-referent context.
Note that he sequence and the timing of eyemovements, relative to the nouns in the speech stream, did notdiffer for the ambiguous and unambiguous instructions.A B???
"Put the apple on the towel in the box.
"A B i=~I , I , I , I , I I , rms 0 500 1000 1500 2000 2500"Put the apple that's on the towel in the box.
"I .
I I A .
I I , B Ims 0 500 1000 1500 2000 2500Figure 7: Typical sequence of eye movements in the three-and-one context.
Note that he sequence and the timing of eyemovements, relative to the nouns in the speech stream, did notdiffer for the ambiguous and unambiguous instructions.53ConclusionWe have reviewed results establishing that, with well-defined tasks, eye-movements can be used to observe undernatural conditions the rapid mental processes that underliespoken language comprehension.
We believe that thisparadigm will prove valuable for addressing questions on afull spectrum of topics in spoken language comprehension,ranging from the uptake of acoustic information duringword recognition to conversational interactions duringcooperative problem solving.Our results demonstrate hat in natural contexts peopleinterpret spoken language continuously, seeking to establishreference with respect o their behavioral goals during theearliest moments of linguistic processing.
Thus our resultsprovide strong support for models that support continuousinterpretation.
Our experiments also show that referentiallyrelevant non-linguistic nformation immediately affects howthe linguistic input is initially structured.
Given theseresults, approaches to language comprehension thatemphasize fully encapsulated processing modules areunlikely to prove fruitful.
More promising are approachesin which grammatical constraints are integrated intoprocessing systems that coordinate linguistic and non-linguistic information as the linguistic input is processed.Acknowledgments* This paper summarizes work that the invited talk by thefirst author (MKT) was based upon.
Supported by NIHresource grant 1-P41-RR09283; NIH HD27206 to MKT;NIH F32DC00210 to PDA, NSF Graduate ResearchFellowships to MJS-K and JSM and a Canadian SocialScience Research Fellowship to JCS.ReferencesAllopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K.(1996).
Watching spoken language perception: Usingeye-movements to track lexical access.
Proceedings ofthe Eighteenth Annual Conference of the CognitiveScience Society.
Mahwah, NJ: Erlbaum.Ballard, D., Hayhoe, M. & Pelz, J.
(1995).
Memoryrepresentations in natural tasks.
Journal of CognitiveNeuroscience, 7, 68-82.Crain, S. & Steedman, M. (1985).
On not being led up thegarden path.
In Dowty, Kartunnen & Zwicky (eds.
),Natural Language Parsing.
Cambridge, MA: CambridgeU.
Press.Eberhard, K., Spivey-Knowlton, M., Sedivy, J.
&Tanenhaus, M. (1995).
Eye movements a  a window intoreal-time spoken language comprehension i naturalcontexts.
Journal of Psycholinguistic Research, 24, 409-436.Kratzer, J.
(1991).
Representation f focus.
In A. yonStechow & D. Wunderlich (Eds.
), Semantik: EinInternationales Hundbuch der ZeitgenossichenForschung.
Berlin: Walter de Guyter.Krifka, M. (1991).
A compositional semantics for multiplefocus constructions.
Proceedings of Semantics andLinguistic Theory (SALT) I, Cornell Working Papers, 11.Marslen-Wilson, W.D.
(1987).
Functional Parallelism inspoken word-recognition.
Cognition, 25, 71-102.McClelland, J. L., & Elman, J .L.
(1986).
The TRACEmodel of speech perception.
Cognitive Psychology, 18, 1-86.Rooth, M. (1992).
A theory of focus interpretation, NaturalLanguage Interpretation, 1, 75-116.Sedivy, J., Tanenhaus, M., Spivey-Knowlton, M., Eberhard,K.
& Carlson, G. (1995).
Using intonationally-markedpresuppositional information in on-line languageprocessing: Evidence from eye movements to a visualmodel.
Proceedings of the 17th Annual Conference of theCognitive Science Society (pp.375-380).
Hillsdale, NJ:Erlbaum.Spivey-Knowlton, M., Tanenhaus, M., Eberhard, K. &Sedivy, J.
(1995).
Eye-movements accompanyinglanguage and action in a visual context: Evidence againstmodularity.
Proceedings of the 17th Annual Conferenceof the Cognitive Science Society (pp.25-30).
Hillsdale,NJ: Edbaum.Tanenhaus, M. K., Spivey-Knowlton, M.-J., Eberhard, K.M., & Sedivy, J. C. (1995).
Integration of visual andlinguistic infonnation in spoken language-comprehension.Science, 268, 1632-1634.Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard, K.M., & Sedivy, J. C. (1996).
Using eye-movements tostudy spoken language comprehension: Evidence forvisually mediated incremental interpretation.
In T Inui &J.L.
McClelland (eds.).
Attention and Performance XVI:Information integration in perception andcomnmnication., 457-478.
Cambridge Mass: MIT Press.Tanenhaus, M. & Trueswell, J.
(1995).
Sentencecomprehension.
In J. Miller & P. Eimas (Eds.
).Handbook of Perception and Cognition: Volume 11:Speech, Language and Communication.
Academic Press.,217-262.
New York: Academic Press.54
