Improving Statistical Natural Language Translation withCategories and RulesFranz  Jose f  Och  and  Hans  WeberFAU Er langen - Computer  Science Inst i tute,IMMD VI I I  - Artificial Intelligence,Am Weichselgarten 9, 91058 Erlangen - Tennenlohe, Germany{faoch, weber}@immd8, inf ormatik, uni-erlangen, deAbst rac tThis paper describes an all level approach onstatistical natural anguage translation (SNLT).Without any predefined knowledge the systemlearns a statistical translation lexicon (STL),word classes (WCs) and translation rules (TRs)from a parallel corpus thereby producing a gen-eralized form of a word alignment (WA).
Thetranslation process itself is realized as a beamsearch.
In our method example-based tech-niques enter an overall statistical approach lead-ing to about 50 percent correctly translatedsentences applied to the very difficult English-German VERBMOBIL  spontaneous speech cor-pus.1 In t roduct ionIn SNLT the transfer itself is realized as a max-imization process of the formTrans(d) = argmax e P(e\[d) (1)Here d is a given source language (SL) sentencewhich has to be translated into a target lan-guage (TL) sentence .
In order to model thedistributions P(e\[d) all approaches in SNLT usea "divide and conquer" strategy of approximat-ing P(e\[d) by a combination of simpler models.The problem is to reduce parameters in a suffi-cient way but end up with a model still able todescribe the linguistic facts of natural anguagetranslation.The work presented here uses two approxi-mations for P(e\[d).
One approximation is usedfor to gain the relevant parameters in trainingwhile a modified formula is subject of decodingtranslations.
In detail, we impose the followingmodifications with respect o approaches pub-lished in the last decade: 1.
A refined distanceweight for the STL probabilities is used whichallows for a good modeling of the effects causedby syntactic phrases.
2.
In order to account forcollocations a WA technique is used, where one-to-n and n-to-one WAs are allowed.
3.
Forthe translation WCs are used which are con-structed using clustering techniques, where theSTL forms a part of the optimization criterion.4.
A set of TRs is learned mapping sequencesof SL WCs to sequences of TL WCs.Throughout the paper the four topics aboveare described in more detail.
Finally we reporton experimental results produced on the VERB-MOBIL corpus.2 Learn ing  o f  the  Trans la t ionLex iconIn order to determine the STL, we use a sta-tistical model for translation and the EM algo-r ithm to adjust its model parameters.
The sim-ple model 1 (Brown et al, 1993) for the trans-lation of a SL sentence d = d l .
.
.d t  in a TLsentence = e l .
.
.
em assumes that every TLword is generated independently as a mixtureof the SL words:m lP(e\[d) ,,~ H ~ t(ej\[di) (2)j= l  i=OIn the equation above t(ej\[di) stands for theprobability that ej is generated by di.The assumption that each SL word influencesevery TL word with the same strength appearsto be too simple.
In the refined model 2 (Brownet al, 1993) alignment probabilities a(ilj , l, m)are included to model the effect that the po-sition of a word influences the position of itstranslation.The phrasal organization of natural anguagesis well known and has been described by (Jack-endorff, 1977) among many others.
The tra-985ditional alignment probabilities depend on ab-solute positions and do not take that into ac-count, as has already been noted by (Vogel etal., 1996).
Therefore we developed a kind ofrelative weighting probability.
The followingmodel - -  which we will call the model 2 ~ - -makes the weight between the words di and ejdependent on the relative distances between thewords dk which generated the previous worde j -1  :ls(i\]j, ej_z,d) ~ ~ d( i -  k\]l).t(ej_z\]dk) (3)k=0Here d(i - kll ) is the probability that word diinfluences a word ej if the previous word ej-1 isinfluenced by dk.
As an effect of such a weighta (phrase-)cluster of words being moved over along distance receives additional 'cost' only atthe ends of the cluster.
So we have the finaltranslation probability for model 2~:m lP(eld) ~" I I  ~ t(ejldi)s(i\[j, e j - l ,d )  (4)j= l  i=0The parameters involved can be determined us-ing the EM algorithm (Baum, 1972).
The ap-plication of this algorithm to the basic prob-lem using a parallel bilingual corpus aligned onthe sentence l vel is described in (Brown et al,1993).3 Determin ing  a Word  A l ignmentThe kind of WA we use is more general thanthe often used WA through a vector, where ev-ery TL word is generated by exactly one SLword.
We use a matrix Z for every sentencepair, whose fields describe whether or not twowords are aligned.
In this approach, multiplewords can be aligned to one TL word, which ismotivated by collocation phenomena as for in-stance German compound nouns.
Alignmentsmay look like the one in figure 1 according to ourmethod.
The matrix Z contains i + 1 lines andj rows with binary values.
The value zij = 1(zij = 0) means that the word i influences (not)the word j.
In figure 1 every link stands forz i j  = l .The models 1, 2 and 2 ~ and some similar mod-~ ~  tmontagFigure 1: Alignment example.els can be described in the formm lP(eld) "" 1-I ~ xij (5)j= l  i=0where the value xij is the strength of the influ-ence of word di to word ej.
We use a thresh-old 0 < 1 in such a way that while the sum~=o xi~j of the first s values is smaller thanO.
~tk= oXkj we set zi~j = O.
The other valuesare set to 1.
The permutation i0, .
.
.
,  il sorts thexij so that Xioj < ... < Xilj.Interestingly using such a WA technique doesnot in general ead to the same results whenapplied from TL to SL and vice versa.
If weuse P(e\[d) or P(dle ) we receive different WAsz~ d and z d-e.
Intuitively the relation between thewords of the sentences should be symmetric andthere should be the same WA.
It is possible toenforce the symmetry with zij = zed.
zdeij, inorder to make a link between two words only ifthere is a link in both WAs.It is possible to include the WA into the EMalgorithm for the estimation of the model prob-abilities.
This can be done by replacing t(ej Idi)by t (e j ld i ) .z i  j.
The resulting STL becomesmuch cleaner in the sense that it does not con-tain so many wrong entries (see section 7).4 Learn ing  of  T rans la t ion  Ru lesThe incorporation of TRs adds an "example-based" touch to the statistical approach.
In avery naive approach a TR could be representedby a translation example.
The obvious advan-tage is an expectable good quality of the trans-lated sentences.
The disadvantage is the factthat almost no sentence can be translated be-cause every corpus would have too few examples- -  the generalization capability of the naive ap-proach is very limited.We desired a general kind of TR which doesnot use explicit linguistic properties of the usedlanguages.
In addition the rules should general-ize from very sparse data.
Therefore it seemed986natural to use WCs and shorter sequences toend up with a set of rather general rules.
In or-der to achieve a good learning performance, allthe WCs of a language are pairwise disjoint (seesection 5).
The function C(.)
gives the class ofa word or the sequence of WCs of a sequence ofwords.Our TRs axe triples (D, E, Z) where D is asequence of SL WCs, E is a sequence of TL WCsand Z is a WA matrix between D and E. Forusing one rule in the translation process we firstrewrite the probability P(eld):P(e ld ) = ~ P(E,  Zld ) ?
P(elE,  Z ,d  ) (6)E,ZIn order to simplify the maximization (equation1) we use only the TR which gives the maximumprobability.During the learning of those TRs we count allextractable rules occurring in the aligned cor-pus and define the probability p(E, ZlC(d))P(E,  Zld ) in terms of the relative frequency.We approximate P(elE,  Z ,d  ) by simplerprobabilities, o that we finally need a languagemodel p(ejle~-l), a translation model p(ej Id, Z)and a probability p(ejlEj).
For p(ejle~ -1) weuse a class-based polygram language model(Schukat-Talamazzini, 1994).
For the transla-tion probability p(ej Id, Z) we use model 1 andinclude the information of the WA:lp(ejld ,Z) := ~ t(ejldi) .
zi j (7)i=0Figure 2 shows how the application of thoserules works in principle.
We arrive at a list ofword hypotheses with probabilities for each po-sition.
Neglecting the language model, the bestdecision would be to independently choose themost probable word for every position.In general the translation of a sentence in-volves more than one rule and usually there aremany rules applicable.
An applicable rule is onewhere the sequence of SL WCs matches a se-quence of WCs in the sentence.
So in the gen-eral case we have to decide for a set of rules wewant to apply.
This set of rules has to cover thesentence, this means that every word is used ina rule and that no word is used twice or moretimes.
The next step is to decide how to ar-range the generated units to get the translatedsentence.
Finally we have to decide for everyposition which word to use.
We want all thosedecisions to be optimal in the sense that thefollowing product is maximized:Lp(e (jl) o .
.
.o  e(JD) ?
1-I P(z(k), E(k)IC(d(k))k=l?
p(e (jk) IZ (k) , E (k) , d (k)) (8)Here L is the number of SL units, d (k) is the k-thSL unit, e (k) is the k-th TL unit and j l , .
.
.
, j iis a permutation of the numbers 1 , .
.
.
,  L.5 Learn ing  o f  Category  Sys temsDuring the last decade some publications havediscussed the problem of learning WCs usingclustering techniques based on maximum like-lihood criteria applied to single language cor-pora.
The question which we pose in additionis: Which WCs are suitable for translation?
Itseems to make sense to require that the usedWCs in the two languages are correlated, sothat the information about the class of a SLword gives much information about the class ofthe generated TL word.
Therefore it has beenargued in (Fung and Wu, 1995) that indepen-dently generated WCs are not good for the usein translation.For the automatic generation of class systemsexists a well known procedure (see (Kneser andNey, 1993), (Och, 1995)) which maximizes theperplexity of the language model for a trainingcorpus by moving one word from a class to an-other in an iterative procedure.
The functionML(CINw_~w, ) which has to be optimized de-pends only on the count function Nw~w, whichcounts the frequency that the word w' comesafter the word w.Using two sets of WCs for the TL and SLwhich are independent (method INDEP) doesnot guarantee that those WCs are much cor-related.
The resulting WCs have only the prop-erty that the information about the class of aword w has much information about the classof the following word w'.
We want for theWCs used for translation that the informationabout the WC of a word has much informationabout the WC of the translation.
For the useof the standard method for optimizing WCs weneed only define a count function Nd-+e, whichwe do by Nd-.e(d,e) := t(eld)" n(e).
In the987source text translation rule \ [2  word hypothesesr-=-I V-r-1\ [~  translated textFigure 2: Application of a Rule.same way a count function Ne-.d can be deter-mined and we get the new optimization criterionM L ( Cd t~Ce I Nd--+e-J- Need).
The resulting classesare strongly correlated, but rarely contain wordswith similar syntactic/semantic properties.
Toarrive at WCs having both (method COMB), wedetermine TL WCs with the first method andafterwards we determine SL WCs with the sec-ond method.So we can use the well known iterativemethod to end up with WCs in different lan-guages which are correlated.
From those WCswe expect hat they are more suitable for build-ing the TRs from section 4 and finally result ina better overall translation performance.6 T rans la t ion  as a Search  Prob lemThe problem of finding the translation of a sen-tence can be viewed as a search problem for apath with minimal cost in a tree.
If we applythe negative logarithm to the product of proba-bilities in equation 8 we arrive at a sum of costswhich has to be minimized.
The costs stem fromthe language model, the rule probabilities andthe translation probabilities.
In the search treeevery node represents a partial translation forthe first words or a full translation.
The leavesof the tree are the nodes where the applied rulesdefine a complete cover of the SL sentence.
Toreduce the search space we use additional costsfor changing the order of the fragments.We use a beam search strategy (Greer et al,1982) to find a good path in this tree.
To makethe search feasible we had to implement someproblem specific heuristics.7 Resu l tsThe experiments in this section have all beencarried out on the bilingual German-EnglishVERBMOBIL corpus.
This corpus consists ofspontaneous utterances from negotiation di-alogs which had originally been produced inGerman.
For training we used 11 500 randomlychosen sentence pairs.The first experiment shall be understood asan illustration for our improved technique ingenerating a STL using the WA in the EM-algorithm.
We generated a STL using 10 EM-iterations for model 1 and 10 iterations formodel 2q The whole process took about 4 hoursfor our corpus.
Below are given some STL en-tries for German words.
The probabilities t(eld )are written in parentheses.?
Tuesday--+Dienstag (0.83), den (0.05),COMMA (0.042), am (0.038), dienstags(0.018), der (0.009), also (0.0069), passen(0.0019), diesem (0.0013), steht (0.0012)?
Frankfurt--+Frankfurt (0.67), nach (0.12),in (0.081), mit (0.068), um (0.031),habe (0.02), besuchen (0.0078), wiederum(0.0036)The top positions are always plausible trans-lations.
But there are many improper transla-tions produced.
When we include the WA in theEM algorithm as described in section 3 we canproduce fewer lexicon entries of a much betterquality:?
Tuesday-+Dienstag (0.97), dienstags(0.029)?
Frankfurt--+Frankfurt (1)The following two corresponding WCs (out of600) show a typical result of the method COMBto determine correlated WCs:?
Mittwoch, Donnerstag, Freitag,Sonnabend, Friihlingsanfang, Karsamstag,Volkstrauertag, Weihnachtsferien, Som-merschule, Thomas, einschlieflen?
Wednesday, Thursday, Friday, Thursdays,Fridays, Thomas, Veterans', mourning, na-tional, spending, spring, summer-school988To evaluate the complete system we translated200 randomly chosen sentences drawn from anindependent test corpus and checked manuallyhow many of them constituted acceptable trans-lations.
Since we used a spontaneous speechcorpus many sentences were grammatically in-correct.
A translation is classified 'correct' ifthe translation is an error-free (spontaneaousspeech) utterance and classified 'understand-able' if the intention of the utterance is trans-lated.
The 100 sentences had a mean sentencelength of 10 words.
The used STL was gener-ated using model 2' (see section 2).correct understandableINDEP 46.5 % 64 %COMB 52 % 71%Table h Quality of Translation.Some example translations:?
was h~iltst du von zweiter Februar nachmit-tags, nach fiinfzehn Uhr --4 what do youthink about the second of February in theafternoon, after three o'clock?
I wanted to fix a time with you for a five-day business trip to Stuttgart --4 ich wolltemit Ihnen einen Termin ausmachen fiir einef/inft~igige Gesch?ftsreise nach Stuttgart8 ConclusionsWe have presented a couple of improvementsto SNLT.
The most important changes are thetranslation model 2', the representation f WAusing a matrix, a method to determine corre-lated WCs and the use of TRs to constrainsearch.
In the future, the rule mechanismshould be extended.
So far the rules learnedare only loop-free finite state transducers.
Stillmany translation errors stem from the inabilityto model ong distance dependencies.
We intendto move to finite state cascades or context freegrammars in future work.
With respect o thecategory sets we feel that an additional morpho-logical model could further improve the transla-tion quality.
As it stands the system still makesmany errors concerning the number of nominalsand verbs.
This is especially important whenthe language pairs differ with respect o the pro-ductivity of their inflectional systems.9 AcknowledgementsWe have to thank Stefan Vogel from the RWTHAachen explicitly, for the material he providedand G/inther G5rz for general promotion.
Thework is part of the German Joint Project VERB-MOBIL.
This work was funded by the GermanFederal Ministry for Research and Technology(BMBF) in the framework of the VerbmobilProject under Grant BMBF 01 IV 701 K 5.
Theresponsibility for the contents of this study lieswith the authors.ReferencesL.E.
Baum.
1972.
An Inequality and Asso-ciated Maximization Technique in Statisti-cal Estimation for Probabilistic Functions ofMarkov Processes.
Inequalities, 3:1-8.P.
F. Brown, S. A. Della Pietra, V. J.Della Pietra, and R. L. Mercer.
1993.
Themathematics of statistical machine transla-tion: Parameter estimation.
ComputationalLinguistics, 19(2):263-311.P.
Fung and D. Wu.
1995.
Coerced markovmodels for cross-lingual lexical-tag relations.In The Sixth Int.
Conf on Theor.
and Method-ological Issues in Machine Translation, pages240-255, Leuven, Belgium, July.K.
Greer, B. Lowerre, and L. Wilcox.
1982.Acoustic Pattern Matching and Beam Search-ing.
In Proc.
Int.
Conf.
on Acoustics, Speech,and Signal Processing, pages 1251-1254,Paris.R.
Jackendorff.
1977.
X-bar-syntax: A studyof phrase structure.
In Linguistic InquiryMonograph 2.R.
Kneser and H. Ney.
1993.
Improved Clus-tering Techniques for Class-Based StatisticalLanguage Modelling.
In Eurospeech, pages973-976.F.
J. Och.
1995.
Maximum-Likelihood-Sch~itzung von Wortkategorien mit Verfahrender kombinatorischen Optimierung.
Studien-arbeit, FAU Erlangen-Niirnberg.E.G.
Schukat-Talamazzini.
1994.
AutomatischeSpracherkennung.
Vieweg, Wiesbaden.S.
Vogel, H. Ney, and C. Tillmann.
1996.HMM-Based Word Alignment in StatisticalTranslation.
In Proc.
Int.
Conf.
on Compu-tational Linguistics, pages 836-841, Kopen-hagen, August.989
