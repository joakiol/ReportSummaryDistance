EVALUATING TEXT UNDERSTANDING SYSTEMSBeth M. SundheimNaval Command, Control and Ocean Surveillance Center(formerly Naval Ocean Systems Center)Naval Research, Development, Test and Evaluation Division, Code 444San Diego, CA 92152-5000PROJECT GOALSThe objectives of this project are to advance ourunderstanding of the merits of current textanalysis  techniques,  as appl ied to theperformance of realistic text analysis tasks, andto achieve this understanding by means of asound performance valuation methodology.
Theperformance data can be interpreted in light ofinformation known about the text interpretationtechniques for the various systems to yieldqualitative insight into the relative validity ofthose techniques for the text analysis task.
Thedata can also be used as a means for determiningwhich research areas are most critical to thesuccessful performance of the task.The most recent performance evaluation wasconducted in May, 1991, on systems contributedby 15 R&D sites.
The evaluation task wasintended to yield insight into text analysistechnology, including the use of informationretrieval technology (document retrieval andcategorization) instead of or in concert withlanguage understanding technology.
Theevaluation concluded with the Third MessageUnderstanding Conference (MUC-3).MUC-3 RESULTS* Signif icant performance benchmarksthat show substantial capability for the top-scoring systems, given the extreme difficulty ofthe task: the systems demonstrated an ability toextract up to approximately 40-50% of theinformation expected and to extract informationwith at least 50-60% accuracy.
* A rich set of data on each of thesystems for further analysis, a highly improvedset of performance metrics embedded in aflexible, semiautomated scoring program, and alarge database of texts and extractedinformation that can be used to support futureresearch in computational inguistics.
* Useful insights into the merits of thevarious technologies, including these high-levelones: (1) although the top-scoring systemsreflected a diversity of overall approaches, theyall employed robust parsing methods anddomain-specific knowledge; (2) systems usingonly nonlinguistic techniques were not able toscore as well as those that included linguistictechniques.
* Ident i f i cat ion  of  the need forscientific breakthroughs in at least one majorresearch area, namely discourse (tracking theflow of an incident description across sentencesand paragraphs).CURRENT EFFORTS* Modify the evaluation task and testingtechniques to improve the rel iabi l i ty andutility of the results.
* Explore new ways of using the existingframework to gain insight into particularaspects of performance.
* Measure progress in the field byrunning the evaluation again in June, 1992,using a new test set, an improved scoringprogram, the refined basic evaluation task, andany newly-defined subtasks.
* Use the evaluation methodology as thebasis for developing an evaluation plan for theinformation extraction portion of the DARPATipster natural language program.478
