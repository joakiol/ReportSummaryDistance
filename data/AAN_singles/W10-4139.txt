Chinese Word Segmentation based on Mixing Multiple Preprocessorand CRFJianping Shen, XuanWang, Hainan Zhao,Wenxiao ZhangComputer Application Research Center, Shenzhen Graduate SchoolHarbin Institute of Technology Shenzhen, China, 518055Email: {jpshen, wangxuan,hnzhao@cs.hitsz.edu.cn}  Email: { xiaohit@126.com}AbstractThis paper describes the Chinese WordSegmenter for our participation in CIPS-SIGHAN-2010 bake-off task of Chineseword segmentation.
We formalize the tasksas sequence tagging problems, andimplemented them using conditional randomfields (CRFs) model.
The system contains twomodules: multiple preprocessor and basicsegmenter.
The basic segmenter is designed asa problem of character-based tagging, andusing named entity recognition and chunkrecognition based on boundary to preprocess.We participated in the open training onSimplified Chinese Text and TraditionalChinese Text, and our system achieved oneRank#5 and four Rank#2 best in all fourdomain corpus.1 IntroductionWord is a logical semantic and syntactic unitin natural language (Zhenxing Wang, 2008).Chinese word segmentation is very important forChinese language processing, which aims torecognize the implicit word boundaries inChinese text.
It is the foundation of mostChinese NLP tasks.
In past decades, greatsuccess has been achieved in Chinese wordsegmentation (Nie, et al 1995; Wang et al2000;Zhang, et al 2002).
But there still existmany problems, such as cross-domainperformance of Chinese word segmentationalgorithms.
As the development of the internet,more and more new word has been appearing,Improving the  performance of Chinese wordsegmentation algorithms on OOV (Out-Of-Vocabulary Word, is a word which occurs in thereference corpus but does not occur in thelabeled training corpus) is the important researchdirection.
Our system participated in the CIPS-SIGHAN-2010 bake-off task of Chinese wordsegmentation.
And we have done work in dealingwith two main sub-tasks: (1) Word Segmentationfor Simplified Chinese Text, (2) WordSegmentation for Traditional Chinese Text.
Oursystem formalizes these tasks as consecutivesequence tagging problems, and learns thesegmentation using conditional random fieldsapproach.
Our system contains two modules, amultiple preprocessor and a basic segmenter.
Themultiple preprocessor first finds chunks based onboundary dictionary and then uses named entityrecognition technology to extract the person,location, organization and special time.
The basicsegmenter using CRF model is trained tosegment the sentence to word which contains oneor more characters.
The basic segmenter followsthe study of Zhenxing Wang, Changning Huangand Jingbo Zhu (2008), but applies more refinedfeatures and tags.The reminder of the paper is organized asfollows.
In section 2, we briefly describe the taskand the details of our system.
The experimentalresults are discussed in section 3.
In section 4 weput forward our conclusion.2 System DescriptionIn this section we describe our system in moredetail.
The Figure1 is the frame of our system.
Itcontains two modules: multiple preprocessor andbasic segmenter.2.1 Multiple PreprocessorThe preprocessor contain two modules:chunking based on boundary dictionary and NEReorganization.2.1.1 ChunkingIn one sentence, there are always somecharacters or  words, such as ??
?, ???,???,???
?, the character adjacent them can nottogether with them.
We define these charactersor words as boundary word.
We built a boundarydictionary manual, which contains about 100words.
Once aFigure1.
Chinese Word Segmentersentence input, our system finds boundary wordsin the sentence first.
For example, such as, ??????????????????
15??
16?????????????.
In this sentencewe can find the boundary word???
???
???
???
?
???.
Then chunking result is shownbelow in Figure 2.
Using ?
[ ]?
to mark up thechunks.[???]
?
[?????]
?
[????]??
?
[15?]
?
[16?]
?
[????][??????
]Figure 2. a sentence with chunk in data setChunking is very useful to improve the precisionof segmentation.
Especially when lackingenough training corpus for training CRF model.It can improve the out-of-vocabulary (OOV)word Recall and Precision on cross-domainChinese word segmentation.2.1.2 NE RecognitionWe will recognize the named entities such aspersons, locations organizations.
We perform aprocess of the named entities recognition withforward-backward maximum matching algorithmbased on entity dictionary.
The dictionarycontain location dictionary, person dictionary,family name dictionary, organization dictionary,country dictionary (Jianping Shen and XuanWang, 2010).
For example, a sentence, ??????????????????????????????????.
And  the processorwill find out the location????,???
?, thefamily name ???
and person ????.
The NErecognition result is shown below in Figure 3.StringSentenceMultiple preprocessChunkingNE RecognitionTraining dataTrain systemCRF modelSystem CRFmodelTaggingsegmenterResult[??
]/loc  ?????
[?
]/fam  [??
]/per  ????????
[??
]/  [??
]/org  ?????????
?.Figure 3.sentence with NE recognition in data setThe location tag with ?
[ ]/loc?, family name tagwith ?
[ ]/fam?, person tag with ?
[ ]/per?,organization tag with ?
[ ]/org?.2.2 Basic SegmenterWe model the segment task as the consecutivesequence labeling problems, such as chunking,and named entity recognition, and train the basicsegmenter using conditional random fieldsapproach (Lafferty et al, 2001).2.2.1 Conditional Random FieldsCRF models are conditional probabilisticsequence and undirected graphical modelsCRF models hold two natures.
First is theconditional nature, second the exponential nature.The conditional nature of the distribution overlabel sequence allows CRF models to modelreal-world data in which the conditionalprobability of a label sequence can depend onnon-independent and interacting features of theobservation sequence.
The exponential nature ofthe distribution enables features of differentstates to be traded off against each other,weighting some states in a sequence as beingmore important than other states.
FollowingLafferty et al and Hanna Wallach, theexponential distribution chosen by John Laffertyet al is shown as follow:( ) (,| exp( , | ,k k ee E kp y x f e y x?
???
)?
( ),, | , )k k vv V kg v y x?
?+ ?1exp( ( , , )k k i ii kf y y x?
?= ?
?,( )k k ii kg y x?
+??
)(1)Where( )', 1 ', , 0 u vy y u vif y y and y yf y y xotherwise=   =?= ?
?And( ), 1, 0 v vy x vif y y and x xg y xotherwirse=   =?= ?
?In this situation, the parameters ',y y?and,y x?
corresponding to these features areequivalent to the logarithms of the HMMtransition and emission probabilitiesand .
The parameter of themodel can be estimated in many ways, such asGIS, IIS, L-BFGS etc.
( )' |p y y ( )|p x y2.2.2 Segment base on CRF modelWhen a sentence or chunk (which get from thepreprocessor) input, it will be split to thesequences shown in Figure 4.chunk sequence?????????
?Figure  4.
Chunk and sequenceEvery character in input sentences will begiven a label which indicates whether thischaracter is a word boundary.
Our basicsegmenter is almost the same as the systemdescribed in (Zhao et al, 2006) which is learnedfrom training corpus.
The CRF model we use isimplemented with CRF++ 0.51.
The parametersof the CRF segmenter are set as defaults.Under the CRF tagging scheme, eachcharacter in one sentence will be given a label byCRF model to indicate which position thischaracter occupies in a word.
In our system, CRFtag set is proposed to distinguish differentpositions in the multi-character words when theword length is less than 6, namely 6-tag set {B,B2, B3, M, E, O}( Zhenxing Wang ,2008).Wedefined that B stands for the first and  E standsfor  the last position in a multi-character word.
Sstands up a single-character word.
B2 and B3stand for the second and the third position in amulti-character word.
M stands for the fourth ormore rear position in a multi-character word,whose length is larger than four-character.
Thenwe add the entity tag set {B-entity, I-entity, E-entity}.
B-entity stands for the first character in anamed entity, E-entity stands for the lastcharacter in a named entity, and I-entity standsfor the other character in a named entity.We use a greedy forward procedure to select abetter feature sets for the segementer accordingto the evaluation results in the development set.We first start from a basic feature set, and thenadd each feature outside the basic set and removeeach feature inside the basic set one by one tocheck the effectiveness of each feature by theperformance change in the development set.
Thisprocedure is repeated until no feature is added orremoved or the performance is not improved.The selected features are listed below:?
Cn (n=-2,-1, 0, 1, 2)?
CnCn+1 (n=-1,0)?
Cn-1CnCn+1 (n=-1,0,1)?
Cn-2Cn-1CnCn+1  (n=0,1)Where C refer to the tag of each character, andC0 denotes current character and Cn(C-n)denotes the character n positions to the right (left)of current character.2.2.3 Post-processingWe can obtain the preliminary results throughthe CRF model-based Segment, but there aresome missed or incorrect cases for the digit,English word.
For example ?the sighan?
maybesegment to ?th e sig han?, so we will re-segmentthe ?th e sig han?
as ?the sighan?.3 Performance and AnalysisIn this section we will present ourexperimental results for these two subtasks.
Forthe Word Segmentation for Simplified ChineseText subtask, comparing the performance ofthese four domains, we find that the performanceof computer and finance are better than literatureand medical.
We can find that the OOV RR ofliterature and medical are lower than thecomputer and finance.
In the test data set, thereare many Out-of-vocabulary(OOV), especiallythe disease.
In medical domain, there are manydiseases which do not appear in the corpus, andthere is the proper name.
The segment oftencan?t recognize disease well, so we add a post-processing procedure, using domain dictionaryfor medicine, is used to increase the recallmeasure.
The result for medical is shown inTable 2.domainRPF1OOVRRIVRRliterature 0.836 0.841 0.838 0.609 0.853computer0.9510.9510.9320.770.983medical0.8390.8320.8360.7960.866finance0.8930.8960.8940.7960.902Table  1: Performance of the four domainSimplified Chinese  test data setR P F1OOVRR IV RR0.894 0.882 0.888 0.683 0.901Table 2: Performance of medical test data setwith post-processing using domain dictionaryWord Segmentation for Traditional ChineseText subtask.
We use a Traditional andSimplified Dictionary to translate the namedentity dictionary, boundary dictionary fromSimplified to Traditional.
And then we use oursystem to segment the traditional test data set.The results are shown in Table 3.domainRPF1OOVRRIVRRliterature 0.868 0.802 0.834 0.503 0.905computer0.8750.8290.8510.5940.904medical0.8790.8140.8460.4800.912finance0.8320.7600.7940.3560.866Table 3: Performance of four domain TraditionalChinese test data set4 ConclusionThrough the CIPS-SIGHAN bakeoff, we findour system is effective.
And at the same time, wealso find some problems of us.
Our system stillcan?t performance very good in cross-domain.Especially the Out-of-vocabulary (OOV)recognition.
From the experiment we can see thatusing domain dictionary is a good idea.
In thefuture we will do more work in post-processing.The bakeoff points out the direction for us toimprove our system.ReferencesHuipeng Zhang, Ting Liu, Jinshan Ma, XiantaoLiao,Chinese Word Segmentation with MultiplePostprocessors  in HIT-IRLab, Proceedings of theFourth SIGHAN Workshop on Chinese LanguageProcessing, Jeju Island, Republic of KoreaOctober 11-13, 2005Hai Zhao, Changning Huang et al 2006.
Effective TagSet Selection in Chinese Word Segmentation viaConditional Random Field Modeling.
InProceedings of PACLIC-20.
pages 87-94.
Wuhan,China, Novemeber.Zhenxing Wang; Changning Huang; Jingbo Zhu.
TheCharacter-based CRF Segmenter of MSRA&NEUfor the 4th Bakeoff.
The Sixth SIGHAN Workshopfor Chinese Language was be held in conjunctionwith IJCNLP 2008, in Hyderabad, India, January11-12, 2008.Wei Jiang Jian Zhao Yi Guan Zhiming Xu.
ChineseWord Segmentation based on Mixing Model.Proceedings of the Fourth SIGHAN Workshop onChinese Language Processing, Jeju Island,Republic of Korea  October 11-13, 2005Nie, Jian-Yuan, M.-L. Hannan and W.-Y.
Jin.1995.Unknown word detection and segmentationofChinese using statistical and heuristicknowledge.Communication of COLIPS, 5(1&2):47-57.Wang, Xiaolong, Fu Guohong, Danial S.Yeung,James N.K.Liu, and Robert Luk.
2000.
Models andalgorithms of Chinese word segmentation.
In:Proceedings of the International Conference onArtificial Intelligence (IC-AI?2000), Las Vegas,Nevada, USA, 1279-1284.Lafferty, J. and McCallum, A. and Pereira, F.Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.MACHINE LEARNING-INTERNATIONALWORKSHOP THEN CONFERENCE.
2001, 282-289Hanna Wallach, Efficient Training of ConditionalRandom Fields, In Proceedings of the 6th AnnualCLUK Research Colloquium, 2002.
