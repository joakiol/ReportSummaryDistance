The influence of data homogeneity on NLP system performanceEtienne DenoualATR Spoken Language Communication Research Labs,2-2-2 Keihanna Science City, Kyoto 619-0288, JapanLaboratoire CLIPS - GETA - IMAG, Universite?
Joseph Fourier, Grenoble, Franceetienne.denoual@atr.jpAbstractIn this work we study the influence ofcorpus homogeneity on corpus-basedNLP system performance.
Experi-ments are performed on both stochas-tic language models and an EBMT sys-tem translating from Japanese to En-glish with a large bicorpus, in orderto reassess the assumption that usingonly homogeneous data tends to makesystem performance go up.
We de-scribe a method to represent corpushomogeneity as a distribution of sim-ilarity coefficients based on a cross-entropic measure investigated in previ-ous works.
We show that beyond min-imal sizes of training data the exces-sive elimination of heterogeneous dataproves prejudicial in terms of both per-plexity and translation quality : exces-sively restricting the training data to aparticular domain may be prejudicialin terms of In-Domain system perfor-mance, and that heterogeneous, Out-of-Domain data may in fact contribute tobetter sytem performance.1 IntroductionHomogeneity of large corpora is still an unclearnotion.
In this study we make a link between thenotions of similarity and homogeneity : a largecorpus is made of sets of documents to whichmay be assigned a score in similarity defined bycross-entropic measures (similarity is implicitlyexpressed in the data).
The distribution of thesimilarity scores of such subcorpora may then beinterpreted as a representation of the homogeneityof the main corpus, which can in turn be used toperform corpus adaptation to tune a corpus basedNLP system to a particular domain.
(Cavaglia` 2002) makes the assumption that acorpus based NLP system generally yields bet-ter results with homogeneous training data ratherthan heterogeneous, and experiments on a textclassifier system (Rainbow1), to mixed conclu-sions.
We reassess this assumption by experi-menting on language model perplexity, and on anEBMT system translating from Japanese to En-glish.2 A framework for corpus homogeneity2.1 Previous work on corpus similarity andhomogeneityA range of measures for corpus similarity havebeen put forward in past literature : (Kilgarriffand Rose 98; Kilgarriff 2001) investigated onthe similarity of corpora and compared ?KnownSimilarity Corpora?
(KSC) using perplexity andcross-entropy on words, word frequency mea-sures, and a ?2-test which they found to bethe most robust.
However(as acknowledged in(Kilgarriff and Rose 98)), using KSC requiresthat the two corpora chosen for comparison aresufficiently similar that the most frequent lex-emes in them almost perfectly overlap.
How-ever (Liebscher 2003) showed by comparing fre-quency counts of different large Google Group1See http://www.cs.cmu.edu/mccallum/bow .226corpora that it is not usually the case.Measuring homogeneity by countingword / lexeme frequencies introduces addi-tional difficulties as it assumes that the word is anobvious, well-defined unit, which is not the casein the Chinese (Sproat and Emerson 2003) orJapanese language (Matsumoto et al, 2002), forinstance, where word segmentation is not trivial.
(Denoual 2004) showed that similarity betweencorpora could be quantified with a coefficientbased on the cross-entropies of probabilistic mod-els built upon reference data.
The approachneeded no explicit selection of features and waslanguage independent, as it relied on characterbased models (as opposed to word based models)thus bypassing the word segmentation issue andmaking it applicable on any electronic data.The cross-entropy HT (A) of an N-gram modelp constructed on a training corpus T , on a testcorpus A = {s1, .., sQ} of Q sentences with si ={ci1..ci|si|} a sentence of |si| characters is:HT (A) =?Qi=1[?|si|j=1?logpij ]?Qi=1 |si|(1)where pij = p(cij |cij?N+1..cij?1).We therefore define a scale of similarity be-tween two corpora on which to rank any thirdgiven one.
Two reference corpora T1 and T2 areselected by the user, and used as training sets tocompute N-gram character models.
The cross-entropies of these two reference models are es-timated on a third test set T3, and respectivelynamed HT1(T3) and HT2(T3) as in the notation inEq.
1.
Both model cross-entropies are estimatedaccording to the other reference , i .e ., HT1(T2)and HT1(T1), HT2(T1) and HT2(T2) so as to ob-tain the weights W1 and W2 of references T1 andT2 :W1 = HT1(T3)?HT1(T1)HT1(T2)?HT1(T1)(2)and :W2 = HT2(T3)?HT2(T2)HT2(T1)?HT2(T2)(3)After which W1 and W2 are assumed to bethe weights of the barycentre between the user-chosen references.
ThusI(T3) = W1W1 +W2 =11 + W2W1(4)is defined to be the similarity coefficient betweenreference sets 1 and 2, which are respectively cor-pus T1 and corpus T2 .
Given the previous as-sumptions, I(T1) = 0 and I(T2) = 1 ; further-more, any given corpus T3 yields a score betweenthe extrema I(T1) = 0 and I(T2) = 1This framework may be applied to the quantifi-cation of the similarity of large corpora, by pro-jecting them to a scale defined implicitly via thereference data selection.
In this study we shallspecifically focus on a scale of similarity boundedby a sublanguage of spoken conversation on theone hand, and a sublanguage of written style me-dia on the other.We build upon this previous work in order torepresent intra-corpus homogeneity.2.2 Representing corpus homogeneityCorpora are collected sets of documents usuallyoriginating from various sources.
Whether a cor-pus is homogeneous in content or not is scarcelyknown besides the knowledge of the nature ofthe sources.
As homogeneity is multidimensional(see (Biber 1988) and (Biber 1995) for consid-erations on the dimensions in register variationfor instance), one cannot trivially say that a cor-pus is homogeneous or heterogeneous : differentsublanguages show variations that are lexical, se-mantic, syntactic, and structural (Kittredge andLehrberger 1982).In this study we wish to implicitly capture suchvariations by applying the previously describedsimilarity framework to the representation of ho-mogeneity.
Coefficients of similarity may becomputed for all smaller sets in a corpus, the dis-tribution of which shall depict the homogeneityof the corpus relatively to the scale defined im-plicitly by the choice of the reference data.Homogeneity as depicted here is relative to thechoice of reference training data, which implic-itly embrace lexical and syntactic variations in asublanguage (which are by any means not unidi-mensional, as argued previously).
We focus as in(Denoual 2004) on a scale of similarity boundedby a sublanguage of spoken conversation on theone hand, and a sublanguage of written style me-dia on the other.2273 A study of the homogeneity of a largebicorpus3.1 DataReference data is needed to set up a scale of sim-ilarity, and implicitly bound it.For the sublanguage of spoken conversation weused for both English and Japanese the SLDB(Spontaneous Speech Database) corpus, a multi-lingual corpus of raw transcripts of dialogues de-scribed in (Nakamura et al, 1996).For the sublanguage of written style media, weused for English a part of the Calgary2 corpus,containing several contemporary English litera-ture pieces3, and for Japanese a corpus of col-lected articles from the Nikkei Shinbun newspa-per4.The large multilingual corpus that is used inour study is the C-STAR5 Japanese / English partof an aligned multilingual corpus, the Basic Trav-eller?s Expressions Corpus (BTEC).A prerequisite of the method is that levels ofdata transcriptions are strictly normalized, so thatthe comparison is not made on the transcriptionmethod but on the underlying signal itself.3.2 Homogeneity in the BTECThe BTEC is a collection of sentences originat-ing from 197 sets (one set originating from onephrasebook) of basic travel expressions.
Here weexamine the distribution of the similarity coeffi-cients assigned to its subsets.The corpus may be segmented in a variety ofmanners, however we wish to proceed in two intu-itive ways : firstly, by keeping the original subdi-vision, i .e ., one phrasebook per subset ; secondly,at the level of the sentence, i .e ., one sentence persubset .
Figure 1 shows the similarity coefficientdistributions for Japanese and English at the sen-tence and subset level, and Table 1 shows theirmeans and standard deviations.The difference in means and standard deviation2The Calgary Corpus is available via anonymous ftp atftp.cpcs.ucalgary.ca/pub/projects/text.compression.corpus .3Parts are entitled book1, book2 and book3.4The use of classical Japanese literature is not appropri-ate as (older) copyright-free works make use of a consider-ably different language.
In order to maintain a certain homo-geneity, we limit our study to contemporary language.5See http://www.c-star.org .Coefficient Japanese EnglishPhrasebook 0.330?0.020 0.288?0.027Line 0.315?0.118 0.313?0.156Table 1: Means ?
standard deviations of the simi-larity coefficient distributions in Japanese and En-glish.values can be explained by the fact that all phrase-books do not have the same size in lines6.
The dis-tribution of similarity coefficients at the line level,however similar to the distribution at the phrase-book level, suggests in its irregularities that it isindeed safer to use a larger unit to estimate cross-entropies.
Moreover, we wish not to tamper withthe integrity of the original subsets, that is to keepthe integrity of phrasebook contents as much aspossible.On the phrasebook level, the similarity coef-ficient has a low correlation on both the aver-age phrasebook length (0.178) and the averageline length (0.278) (which does not make it atoo ?shallow?
profiling method).
On the otherhand, correlation is high between the coefficientsin Japanese and English (0.781), which is only tobe expected intuitively.4 Experiments4.1 MethodThis work wishes to reassess the assumption that,for a same amount of training data, a corpus-based NLP system performs better when its datatends to be homogeneous.
Here we use the rep-resentation of homogeneity defined by the sim-ilarity coefficient scale to select data that tendsto be homogeneous to an expected task.
Exper-iments shall be performed both on randomly se-lected data, and on data selected according to theirsimilarity coefficient.
The closer the coefficientof the training data is to the coefficient of the ex-pected task, the better.We assume that the task is sufficiently repre-sented by a set of data from the same domain asthe large bicorpus used, the BTEC.
Experimentsare performed on a test set of 510 Japanese sen-tences which are not included in the ressource.6The BTEC phrasebooks have an average size of 824lines with a standard deviation in size of 594 lines.2280 0.2 0.4 0.6 0.8 10100200300400500600700800Japanese BTEC CoefficientsOccurrences0 0.2 0.4 0.6 0.8 10100200300400500600700800English BTEC CoefficientsOccurrencesLine levelPhrasebook levelLine levelPhrasebook levelFigure 1: Distributions of similarity coefficients at the sentence level (thin line) and at the phrasebooklevel (thick line), respectively for Japanese and English.0 50 10000.20.40.60.81BLEUscore(interpolated)0 50 10046810Percent of the original BTEC in sizeNISTscore(interpolated)0 50 10000.20.40.60.81mWERscore (interpolated)Randomly selected dataHomogeneous dataFigure 2: BLEU, NIST and mWER scores for EBMT systems built on increasing amounts of randomlychosen and homogeneous BTEC data.These sentences shall first be used for languagemodel perplexity estimation, then as input sen-tences for the EBMT system.
The task is found tohave a coefficient of I0 = 0.331.
The average co-efficient for a BTEC phrasebook being 0.330, thetask is found to be particularly in the domain ofthe ressource.
We examine the influence of train-ing data size first on language model perplexity,then on the quality of translation from Japaneseto English by an example-based MT system.4.1.1 Language model perplexityEven if perplexity does not always yield a highcorrelation with NLP systems performance, it isstill an indicator of language model complexity asit gives an estimate of the average branching fac-tor in a language model.
The measure is popularin the NLP community because admittedly, whenperplexity decreases, the performance of systemsbased on stochastic models tends to increase.We compute perplexities of character languagemodels built on variable amounts of training datafirst randomly taken from the Japanese part ofthe BTEC, and then selected around the expectedtask coefficient I0 (thresholds are determined bythe amount of training data to be kept).
Cross-entropies are estimated on the test set, and all es-timations are performed five times for the ran-dom data selections and averaged.
Figure 3shows the character perplexity values for increas-ing amounts of data from 0.5% to 100% of theBTEC and interpolated.
As was expected, per-plexity decreases as training data increases andtends to have an asymptotic behaviour when moredata is being used as training.2290 10 20 30 40 50 60 70 80 90 100051015202530Percent of the original BTEC in sizeCharacter perplexity (interpolated)Randomly selected dataHomogeneous dataFigure 3: Perplexity of character language modelsbuilt on increasing amounts of randomly chosenBTEC and homogeneous Japanese data.While homogeneous data yield lower perplex-ity scores for small amounts of training data (upto 15% of the ressource - roughly 1.5 Megabytesof data), beyond this value perplexity is slightlyhigher than for a model trained on randomly se-lected data.
Except for the smaller amounts ofdata, there seems to be no benefit in using homo-geneous rather than random heterogeneous train-ing data for model perplexity.
On the contrary,excessively restricting the domain seems to yieldhigher model perplexities.4.1.2 Automatic evaluation of the translationqualityIn this section we experiment on a Japanese toEnglish grammar-based EBMT system, HPATR(described in (Imamura 2001)), which parses a bi-corpus with grammars for both source and targetlanguage, and translates by automatically gener-ating transfer patterns from bilingual trees con-structed on the parsed data.
Not being a MTsystem based on stochastic methods, it is usedhere as a task evaluation criterion complemen-tary to language model perplexity.
Systems arelikewise constructed on variable amounts of train-ing data, and evaluated on the previous task of510 Japanese sentences, to be translated fromJapanese to English.Because it is not feasible here to have humansjudge the quality of many sets of translated data,we rely on an array of well known automatic eval-uation measures to estimate translation quality :?
BLEU (Papineni et al 2002) is the geomet-ric mean of the n-gram precisions in the out-put with respect to a set of reference trans-lations.
It is bounded between 0 and 1, bet-ter scores indicate better translations, and ittends to be highly correlated with the fluencyof outputs ;?
NIST (Doddington 2002) is a variant ofBLEU based on the arithmetic mean ofweighted n-gram precisions in the outputwith respect to a set of reference translations.It has a lower bound of 0, no upper bound,better scores indicate better translations, andit tends to be highly correlated with the ade-quacy of outputs ;?
mWER (Och 2003) or Multiple Word ErrorRate is the edit distance in words betweenthe system output and the closest referencetranslation in a set.
It is bounded between 0and 1, and lower scores indicate better trans-lations.Figure 2 shows BLEU, NIST and mWERscores for increasing amounts of data from 0.5%to 100% of the BTEC and interpolated.
As wasexpected, MT quality increases as training dataincreases and tends to have an asymptotic be-haviour when more data is being used in training.Here again except for the smaller amounts of data(up to 3% of the BTEC in BLEU, up to 18% inNIST and up to 2% in mWER), using the threeevaluation methods, translation quality is equal orhigher when using random heterogenous data.
Ifwe perform a mean comparison of the 510 pairedscore values assigned to sentences, for instance at50% of training data, this difference is found to bestatistically significant between BLEU, NIST, andmWER scores with confidence levels of 88.49%,99.9%, and 73.24% respectively.5 Discussion and future workThe contribution of this work is twofold :We describe a method of representing homo-geneity according to a cross-entropic measure ofsimilarity to reference sublanguages, that can beused to profile language ressources.
A corpusis represented by the distribution of the similar-ity coefficients of the smaller subsets it contains,230and atypical therefore heterogeneous data may becharacterized by the lower occurrences of theirvalues.We further observe that marginalizing suchatypical data in order to restrict the domain onwhich a corpus-based NLP system operates doesnot yield better performance, either in terms ofperplexity when the system is based on stochasticlanguage models, or in terms of objective transla-tion quality when the system is a grammar-basedEBMT system.An objective for future work is thereforeto study corpus adaptation with Out-of-Domaindata.
While (Cavaglia` 2002) also acknowledgedthat for minimal sizes of training data, the bestNLP system performance is reached with ho-mogeneous ressources, we would like to knowmore precisely why and to what extent mixingIn-Domain and Out-of-Domain data yields betteraccuracy.
Concerning the representation of ho-mogeneity, other experiments are needed to tacklethe multidimensionality of sublanguage varietiesless implicitly.
We would like to consider multi-ple sublanguage references to untangle the dimen-sions of register variation in spoken and writtenlanguage.AcknowledgementsThis research was supported in part by the Na-tional Institute of Information and Communica-tions Technology.ReferencesDouglas Biber.
1988.
Variation across speech andwriting.
Cambridge University Press.Douglas Biber.
1995.
Dimensions in Register Varia-tion.
Cambridge University Press.Gabriela Cavaglia`.
2002.
Measuring corpus homo-geneity using a range of measures for inter-document distance.
Proceedings of LREC, pp.
426-431.Etienne Denoual.
2004.
A method to quantify corpussimilarity and its application to quantifying the de-gree of literality in a document.
Proceedings of theInternational Workshop on Human Language Tech-nology, Hong Kong, pp.28-31.George Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
Proceedings of Human Lang.Technol.
Conf.
(HLT-02), pp.138-145.Kenji Imamura.
2001.
Hierarchical Phrase AlignmentHarmonized with Parsing.
Proceedings of NLPRS,pp.377-384.Adam Kilgarriff and Tony Rose.
1998.
Measures forcorpus similarity and homogeneity.
Proceedings ofthe 3rd conference on Empirical Methods in NaturalLanguage Processing, Granada, Spain, pp.
46 - 52.Adam Kilgarriff.
2001.
Comparing corpora.
Interna-tional Journal of Corpus Linguistics 6:1, pp.
1-37.Richard Kittredge and John Lehrberger.
1982.
Sublan-guage.
Studies of language in restricted semanticdomains Walter de Gruyter, editor.Robert A. Liebscher.
2003.
New corpora, new tests,and new data for frequency-based corpus compar-isons.
Center for Research in Language Newsletter,15:2Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Hiroshi Matsuda,KazumaTakaoka and Masayuki Asahara.
2002.
Morpholog-ical Analysis System ChaSen version 2.2.9 Manual.Nara Institute of Science and Technology.Atsushi Nakamura, Shoichi Matsunaga, TohruShimizu, Masahiro Tonomura and Yoshinori Sag-isaka 1996.
Japanese speech databases for robustspeech recognition.
Proceedings of the ICSLP?96,Philadelphia, PA, pp.2199-2202, Volume 4Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
Proceedings ofACL 2003, pp.160-167.Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofACL 2002, pp.311-318.Richard Sproat and Thomas Emerson.
2003.
The FirstInternational Chinese Word Segmentation Bakeoff.The Second SIGHAN Workshop on Chinese Lan-guage Processing, Sapporo, Japan.231
