Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185?192,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsJoint WMT 2013 Submission of the QUAERO Project?Stephan Peitz, ?Saab Mansour, ?Matthias Huck, ?Markus Freitag, ?Hermann Ney,?Eunah Cho, ?Teresa Herrmann, ?Mohammed Mediani, ?Jan Niehues, ?Alex Waibel,?Alexandre Allauzen, ?Quoc Khanh Do,?Bianka Buschbeck, ?Tonio Wandmacher?RWTH Aachen University, Aachen, Germany?Karlsruhe Institute of Technology, Karlsruhe, Germany?LIMSI-CNRS, Orsay, France?SYSTRAN Software, Inc.?surname@cs.rwth-aachen.de?firstname.surname@kit.edu?firstname.lastname@limsi.fr ?surname@systran.frAbstractThis paper describes the joint submis-sion of the QUAERO project for theGerman?English translation task of theACL 2013 Eighth Workshop on Statisti-cal Machine Translation (WMT 2013).The submission was a system combina-tion of the output of four different transla-tion systems provided by RWTH AachenUniversity, Karlsruhe Institute of Technol-ogy (KIT), LIMSI-CNRS and SYSTRANSoftware, Inc.
The translations werejoined using the RWTH?s system com-bination approach.
Experimental resultsshow improvements of up to 1.2 points inBLEU and 1.2 points in TER compared tothe best single translation.1 IntroductionQUAERO is a European research and develop-ment program with the goal of developing multi-media and multilingual indexing and managementtools for professional and general public applica-tions (http://www.quaero.org).
Research in ma-chine translation is mainly assigned to the fourgroups participating in this joint submission.
Theaim of this submission was to show the quality ofa joint translation by combining the knowledge ofthe four project partners.
Each group develop andmaintain their own different machine translationsystem.
These single systems differ not only intheir general approach, but also in the preprocess-ing of training and test data.
To take advantageof these differences of each translation system, wecombined all hypotheses of the different systems,using the RWTH system combination approach.This paper is structured as follows.
First, thedifferent engines of all four groups are introduced.In Section 3, the RWTH Aachen system combina-tion approach is presented.
Experiments with dif-ferent system selections for system combinationare described in Section 4.
This paper is concludedin Section 5.2 Translation SystemsFor WMT 2013, each QUAERO partner trainedtheir systems on the parallel Europarl (EPPS),News Commentary (NC) corpora and the web-crawled corpus.
All single systems were tuned onthe newstest2009 and newstest2010 developmentset.
The newstest2011 development set was usedto tune the system combination parameters.
Fi-nally, on newstest2012 the results of the differentsystem combination settings are compared.
In thisSection, all four different translation engines arepresented.2.1 RWTH Aachen Single SystemFor the WMT 2013 evaluation, RWTH utilized aphrase-based decoder based on (Wuebker et al2012) which is part of RWTH?s open-source SMTtoolkit Jane 2.1 1.
GIZA++ (Och and Ney, 2003)was employed to train a word alignment, languagemodels have been created with the SRILM toolkit(Stolcke, 2002).After phrase pair extraction from the word-aligned parallel corpus, the translation probabil-ities are estimated by relative frequencies.
Thestandard feature set al includes an n-gram lan-guage model, phrase-level IBM-1 and word-,phrase- and distortion-penalties, which are com-bined in log-linear fashion.
Furthermore, we usedan additional reordering model as described in(Galley and Manning, 2008).
By this model six1http://www-i6.informatik.rwth-aachen.de/jane/185additional feature are added to the log-linear com-bination.
The model weights are optimized withstandard Mert (Och, 2003a) on 200-best lists.
Theoptimization criterion is BLEU.2.1.1 PreprocessingIn order to reduce the source vocabulary size trans-lation, the German text was preprocessed by split-ting German compound words with the frequency-based method described in (Koehn and Knight,2003).
To further reduce translation complexityfor the phrase-based approach, we performed thelong-range part-of-speech based reordering rulesproposed by (Popovic?
et al 2006).2.1.2 Translation ModelWe applied filtering and weighting for domain-adaptation similarly to (Mansour et al 2011) and(Mansour and Ney, 2012).
For filtering the bilin-gual data, a combination of LM and IBM Model1 scores was used.
In addition, we performedweighted phrase extraction by using a combinedLM and IBM Model 1 weight.2.1.3 Language ModelDuring decoding a 4-gram language model is ap-plied.
The language model is trained on the par-allel data as well as the provided News crawl,the 109 French-English, UN and LDC GigawordFourth Edition corpora.2.2 Karlsruhe Institute of Technology SingleSystem2.2.1 PreprocessingThe training data was preprocessed prior to thetraining.
Symbols such as quotes, dashes andapostrophes are normalized.
Then the first wordsof each sentence are smart-cased.
For the Ger-man part of the training corpus, the hunspell2 lex-icon was used, in order to learn a mapping fromold German spelling to new German writing rules.Compound-splitting was also performed as de-scribed in Koehn and Knight (2003).
We also re-moved very long sentences, empty lines, and sen-tences which show big mismatch on the length.2.2.2 FilteringThe web-crawled corpus was filtered using anSVM classifier as described in (Mediani et al2011).
The lexica used in this filtering task wereobtained from Giza alignments trained on the2http://hunspell.sourceforge.net/cleaner corpora, EPPS and NC.
Assuming that thiscorpus is very noisy, we biased our classifier moretowards precision than recall.
This was realizedby giving higher number of false examples (80%of the training data).This filtering technique ruled out more than38% of the corpus (the unfiltered corpus containsaround 2.4M pairs, 0.9M of which were rejectedin the filtering task).2.2.3 System OverviewThe in-house phrase-based decoder (Vogel, 2003)is used to perform decoding.
Optimization withregard to the BLEU score is done using MinimumError Rate Training (MERT) as described in Venu-gopal et al(2005).2.2.4 Reordering ModelWe applied part-of-speech (POS) based reorderingusing probabilistic continuous (Rottmann and Vo-gel, 2007) and discontinuous (Niehues and Kolss,2009) rules.
This was learned using POS tags gen-erated by the TreeTagger (Schmid, 1994) for shortand long range reorderings respectively.In addition to this POS-based reordering, wealso used tree-based reordering rules.
Syntacticparse trees of the whole training corpus and theword alignment between source and target lan-guage are used to learn rules on how to reorder theconstituents in a German source sentence to makeit match the English target sentence word orderbetter (Herrmann et al 2013).
The training corpuswas parsed by the Stanford parser (Rafferty andManning, 2008).
The reordering rules are appliedto the source sentences and the reordered sentencevariants as well as the original sequence are en-coded in a word lattice which is used as input tothe decoder.Moreover, our reordering model was extendedso that it could include the features of lexicalizedreordering model.
The reordering probabilities foreach phrase pair are stored as well as the origi-nal position of each word in the lattice.
Duringthe decoding, the reordering origin of the wordsis checked along with its probability added as anadditional score.2.2.5 Translation ModelsThe translation model uses the parallel data ofEPPS, NC, and the filtered web-crawled data.
Asword alignment, we used the Discriminative WordAlignment (DWA) as shown in (Niehues and Vo-186gel, 2008).
The phrase pairs were extracted usingdifferent source word order suggested by the POS-based reordering models presented previously asdescribed in (Niehues et al 2009).In order to extend the context of source lan-guage words, we applied a bilingual languagemodel (Niehues et al 2011).
A DiscriminativeWord Lexicon (DWL) introduced in (Mauser etal., 2009) was extended so that it could take thesource context also into the account.
For this,we used a bag-of-ngrams instead of representingthe source sentence as a bag-of-words.
Filteringbased on counts was then applied to the featuresfor higher order n-grams.
In addition to this, thetraining examples were created differently so thatwe only used the words that occur in the n-best listbut not in the reference as negative example.2.2.6 Language ModelsWe build separate language models and combinedthem prior to decoding.
As word-token basedlanguage models, one language model is built onEPPS, NC, and giga corpus, while another one isbuilt using crawled data.
We combined the LMslinearly by minimizing the perplexity on the de-velopment data.
As a bilingual language model weused the EPPS, NC, and the web-crawled data andcombined them.
Furthermore, we use a 5-gramcluster-based language model with 1,000 wordclusters, which was trained on the EPPS and NCcorpus.
The word clusters were created using theMKCLS algorithm.2.3 LIMSI-CNRS Single System2.3.1 System overviewLIMSI?s system is built with n-code (Crego et al2011), an open source statistical machine transla-tion system based on bilingual n-gram3.
In thisapproach, the translation model relies on a spe-cific decomposition of the joint probability of asentence pair using the n-gram assumption: a sen-tence pair is decomposed into a sequence of bilin-gual units called tuples, defining a joint segmen-tation of the source and target.
In the approach of(Marin?o et al 2006), this segmentation is a by-product of source reordering which ultimately de-rives from initial word and phrase alignments.2.3.2 An overview of n-codeThe baseline translation model is implemented asa stochastic finite-state transducer trained using3http://ncode.limsi.fr/a n-gram model of (source,target) pairs (Casacu-berta and Vidal, 2004).
Training this model re-quires to reorder source sentences so as to matchthe target word order.
This is performed bya stochastic finite-state reordering model, whichuses part-of-speech information4 to generalize re-ordering patterns beyond lexical regularities.In addition to the translation model, eleven fea-ture functions are combined: a target-languagemodel; four lexicon models; two lexicalized re-ordering models (Tillmann, 2004) aiming at pre-dicting the orientation of the next translation unit;a ?weak?
distance-based distortion model; andfinally a word-bonus model and a tuple-bonusmodel which compensate for the system prefer-ence for short translations.
The four lexicon mod-els are similar to the ones use in a standard phrasebased system: two scores correspond to the rel-ative frequencies of the tuples and two lexicalweights estimated from the automatically gener-ated word alignments.
The weights associated tofeature functions are optimally combined using adiscriminative training framework (Och, 2003b).The overall search is based on a beam-searchstrategy on top of a dynamic programming algo-rithm.
Reordering hypotheses are computed in apreprocessing step, making use of reordering rulesbuilt from the word reorderings introduced in thetuple extraction process.
The resulting reorderinghypotheses are passed to the decoder in the formof word lattices (Crego and Mario, 2006).2.3.3 Continuous space translation modelsOne critical issue with standard n-gram translationmodels is that the elementary units are bilingualpairs, which means that the underlying vocabu-lary can be quite large, even for small translationtasks.
Unfortunately, the parallel data available totrain these models are typically order of magni-tudes smaller than the corresponding monolingualcorpora used to train target language models.
It isvery likely then, that such models should face se-vere estimation problems.
In such setting, usingneural network language model techniques seemall the more appropriate.
For this study, we fol-low the recommendations of Le et al(2012), whopropose to factor the joint probability of a sen-tence pair by decomposing tuples in two (sourceand target) parts, and further each part in words.This yields a word factored translation model that4Part-of-speech labels for English and German are com-puted using the TreeTagger (Schmid, 1995).187can be estimated in a continuous space using theSOUL architecture (Le et al 2011).The design and integration of a SOUL model forlarge SMT tasks is far from easy, given the com-putational cost of computing n-gram probabilities.The solution used here was to resort to a two passapproach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in thesecond pass, the k-best list is reordered using theprobabilities of m-gram SOUL translation models.In the following experiments, we used a fixed con-text size for SOUL of m= 10, and used k = 300.2.3.4 Corpora and data pre-processingAll the parallel data allowed in the constrainedtask are pooled together to create a single par-allel corpus.
This corpus is word-aligned usingMGIZA++5 with default settings.
For the Englishmonolingual training data, we used the same setupas last year6 and thus the same target languagemodel as detailed in (Allauzen et al 2011).For English, we also took advantage of our in-house text processing tools for the tokenizationand detokenization steps (Dchelotte et al 2008)and our system is built in ?true-case?.
As Ger-man is morphologically more complex than En-glish, the default policy which consists in treat-ing each word form independently is plagued withdata sparsity, which is detrimental both at trainingand decoding time.
Thus, the German side wasnormalized using a specific pre-processing scheme(described in (Allauzen et al 2010; Durgar El-Kahlout and Yvon, 2010)), which notably aims atreducing the lexical redundancy by (i) normalizingthe orthography, (ii) neutralizing most inflectionsand (iii) splitting complex compounds.2.4 SYSTRAN Software, Inc.
Single SystemIn the past few years, SYSTRAN has been focus-ing on the introduction of statistical approachesto its rule-based backbone, leading to Hybrid Ma-chine Translation.The technique of Statistical Post-Editing(Dugast et al 2007) is used to automatically editthe output of the rule-based system.
A StatisticalPost-Editing (SPE) module is generated from abilingual corpus.
It is basically a translation mod-ule by itself, however it is trained on rule-based5http://geek.kyloo.net/software6The fifth edition of the English Gigaword(LDC2011T07) was not used.translations and reference data.
It applies correc-tions and adaptations learned from a phrase-based5-gram language model.
Using this two-stepprocess will implicitly keep long distance re-lations and other constraints determined by therule-based system while significantly improvingphrasal fluency.
It has the advantage that qualityimprovements can be achieved with very littlebut targeted bilingual data, thus significantlyreducing training time and increasing translationperformance.The basic setup of the SPE component is identi-cal to the one described in (Dugast et al 2007).A statistical translation model is trained on therule-based translation of the source and the targetside of the parallel corpus.
Language models aretrained on each target half of the parallel corporaand also on additional in-domain corpora.
More-over, the following measures - limiting unwantedstatistical effects - were applied:?
Named entities are replaced by special tokenson both sides.
This usually improves wordalignment, since the vocabulary size is sig-nificantly reduced.
In addition, entity trans-lation is handled more reliably by the rule-based engine.?
The intersection of both vocabularies (i.e.
vo-cabularies of the rule-based output and thereference translation) is used to produce anadditional parallel corpus (whose target isidentical to the source).
This was added to theparallel text in order to improve word align-ment.?
Singleton phrase pairs are deleted from thephrase table to avoid overfitting.?
Phrase pairs not containing the same numberof entities on the source and the target sideare also discarded.?
Phrase pairs appearing less than 2 times werepruned.The SPE language model was trained on 2Mphrases from the news/europarl and Common-Crawl corpora, provided as training data for WMT2013.
Weights for these separate models weretuned by the Mert algorithm provided in the Mosestoolkit (Koehn et al 2007), using the providednews development set.188015:that/17:this/323:is/38:was/130:*EPS*/34:it/140:*EPS*/32:in/150:*EPS*/36:the/160:*EPS*/11:future/3Figure 1: Confusion network of four different hypotheses.3 RWTH Aachen System CombinationSystem combination is used to produce consen-sus translations from multiple hypotheses gener-ated with different translation engines.
First, aword to word alignment for the given single sys-tem hypotheses is produced.
In a second step aconfusion network is constructed.
Then, the hy-pothesis with the highest probability is extractedfrom this confusion network.
For the alignmentprocedure, each of the given single systems gen-erates one confusion network with its own as pri-mary system.
To this primary system all other hy-potheses are aligned using the METEOR (Lavieand Agarwal, 2007) alignment and thus the pri-mary system defines the word order.
Once thealignment is given, the corresponding confusionnetwork is constructed.
An example is given inFigure 1.
The final network for one source sen-tence is the union of all confusion networks gen-erated from the different primary systems.
Thatallows the system combination to select the wordorder from different system outputs.Before performing system combination, eachtranslation output was normalized by tokenizationand lowercasing.
The output of the combinationwas then truecased based on the original truecasedoutput.The model weights of the system combinationare optimized with standard Mert (Och, 2003a)on 100-best lists.
We add one voting feature foreach single system to the log-linear framework ofthe system combination.
The voting feature firesfor each word the single system agrees on.
More-over, a word penalty, a language model trained onthe input hypotheses, a binary feature which pe-nalizes word deletions in the confusion networkand a primary feature which marks the systemwhich provides the word order are combined inthis log-linear model.
The optimization criterionis 4BLEU-TER.4 Experimental ResultsIn this year?s experiments, we tried to improve theresult of the system combination further by com-bining single systems tuned on different develop-Table 1: Comparison of single systems tuned onnewstest2009 and newstest2010.
The results arereported on newstest2012.single systems tuned on newstest2012newstest BLEU TERKIT 2009 24.6 58.42010 24.6 58.6LIMSI 2009 22.5 61.52010 22.6 59.8SYSTRAN 2009 20.9 63.32010 21.2 62.2RWTH 2009 23.7 60.82010 24.4 58.8ment sets.
The idea is to achieve a more stableperformance in terms of translation quality, if thesingle systems are not optimized on the same dataset.
In Table 1, the results of each provided singlesystem tuned on newstest2009 and newstest2010are shown.
For RWTH, LIMSI and SYSTRAN,it seems that the performance of the single systemdepends on the chosen tuning set.
However, thetranslation quality of the single systems providedby KIT is stable.As initial approach and for the final submis-sion, we grouped single systems with dissimilarapproaches.
Thus, KIT (phrase-based SMT) andSYSTRAN (rule-based MT) tuned their system onnewstest2010, while RWTH (phrase-based SMT)and LIMSI (n-gram) optimized on newstest2009.To compare the impact of this approach, all pos-sible combinations were checked (Table 2).
How-ever, it seems that the translation quality can not beimproved by this approach.
For the test set (new-stest2012), BLEU is steady around 25.6 points.Even if the single system with lowest BLEU arecombined (KIT 2010, LIMSI 2009, SYSTRAN2010, RWTH 2009), the translation quality interms of BLEU is comparable with the combina-tion of the best single systems (KIT 2009, LIMSI2010, SYSTRAN 2010, RWTH 2010).
However,we could gain 1.0 point in TER.Due to the fact, that for the final submission theinitial grouping was available only, we kept this189Table 2: Comparison of different system combination settings.
For each possible combination of systemstuned on different tuning sets, a system combination was set up, re-tuned on newstest2011 and evaluatedon newstest2012.
The setting used for further experiments is set in boldface.single systems system combinationsKIT LIMSI SYSTRAN RWTH newstest2011 newstest2012tuned on newstest BLEU TER BLEU TER2009 2009 2009 2009 24.6 58.0 25.6 56.82010 2010 2010 2010 24.2 58.1 25.6 57.72010 2009 2009 2009 24.5 57.9 25.7 57.42009 2010 2009 2009 24.4 58.3 25.7 57.02009 2009 2010 2009 24.5 57.9 25.6 57.02009 2009 2009 2010 24.5 58.0 25.6 56.82009 2010 2010 2010 24.1 57.5 25.4 56.42010 2009 2010 2010 24.3 57.6 25.6 56.92010 2010 2009 2010 24.2 58.0 25.6 57.32010 2010 2010 2009 24.3 57.9 25.5 57.62010 2010 2009 2009 24.4 58.1 25.6 57.52009 2009 2010 2010 24.4 57.8 25.5 56.62009 2010 2010 2009 24.4 58.2 25.5 57.02009 2010 2009 2010 24.2 57.8 25.5 56.82010 2009 2009 2010 24.4 57.9 25.6 57.42010 2009 2010 2009 24.4 57.7 25.6 57.4Table 3: Results of the final submission (bold-face) compared with best single system on new-stest2012.newstest2011 newstest2012BLEU TER BLEU TERbest single 23.2 60.9 24.6 58.4system comb.
24.4 57.7 25.6 57.4+ IBM-1 24.6 58.1 25.6 57.6+ bigLM 24.6 57.9 25.8 57.2combination.
To improve this baseline further, twoadditional models were added.
We applied lexi-cal smoothing (IBM-1) and an additional languagemodel (bigLM) trained on the English side of theparallel data and the News shuffle corpus.
The re-sults are presented in Table 3.The baseline was slightly improved by 0.2points in BLEU and TER.
Note, this system com-bination was the final submission.5 ConclusionFor the participation in the WMT 2013 sharedtranslation task, the partners of the QUAEROproject (Karlsruhe Institute of Technology, RWTHAachen University, LIMSI-CNRS and SYSTRANSoftware, Inc.) provided a joint submission.
Byjoining the output of four different translation sys-tems with RWTH?s system combination, we re-ported an improvement of up to 1.2 points inBLEU and TER.Combining systems optimized on different tun-ing sets does not seem to improve the translationquality.
However, by adding additional model, thebaseline was slightly improved.All in all, we conclude that the variability interms of BLEU does not influence the final result.It seems that using different approaches of MT ina system combination is more important (Freitaget al 2012).AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agencyfor innovation.ReferencesAlexandre Allauzen, Josep M. Crego, I?lknur Durgar El-Kahlout, and Franc?ois Yvon.
2010.
LIMSI?s statis-tical translation systems for WMT?10.
In Proc.
of190the Joint Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 54?59, Uppsala, Swe-den.Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-Maynard, Josep M. Crego, Hai-Son Le, Aure?lienMax, Adrien Lardilleux, Thomas Lavergne, ArtemSokolov, Guillaume Wisniewski, and Franc?oisYvon.
2011.
LIMSI @ WMT11.
In Proceedings ofthe Sixth Workshop on Statistical Machine Transla-tion, pages 309?315, Edinburgh, Scotland, July.
As-sociation for Computational Linguistics.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Josep M. Crego and Jose?
B. Mario.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Josep M. Crego, Franois Yvon, and Jos B. Mario.2011.
N-code: an open-source Bilingual N-gramSMT Toolkit.
Prague Bulletin of Mathematical Lin-guistics, 96:49?58.Lo?
?c Dugast, Jean Senellart, and Philipp Koehn.
2007.Statistical post-editing on systran?s rule-based trans-lation system.
In Proceedings of the Second Work-shop on Statistical Machine Translation, StatMT?07, pages 220?223, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Ilknur Durgar El-Kahlout and Franois Yvon.
2010.The pay-offs of preprocessing for German-EnglishStatistical Machine Translation.
In Marcello Fed-erico, Ian Lane, Michael Paul, and Franois Yvon, ed-itors, Proceedings of the seventh International Work-shop on Spoken Language Translation (IWSLT),pages 251?258.Daniel Dchelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,and Franois Yvon.
2008.
LIMSI?s statisticaltranslation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Markus Freitag, Stephan Peitz, Matthias Huck, Her-mann Ney, Teresa Herrmann, Jan Niehues, AlexWaibel, Alexandre Allauzen, Gilles Adda, BiankaBuschbeck, Josep Maria Crego, and Jean Senellart.2012.
Joint wmt 2012 submission of the quaeroproject.
In NAACL 2012 Seventh Workshop on Sta-tistical Machine Translation, pages 322?329, Mon-treal, Canada, June.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 847?855, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Teresa Herrmann, Jan Niehues, and Alex Waibel.2013.
Combining Word Reordering Methods ondifferent Linguistic Abstraction Levels for Statisti-cal Machine Translation.
In Proceedings of the Sev-enth Workshop on Syntax, Semantics and Structurein Statistical Translation, Altanta, Georgia, USA,June.
Association for Computational Linguistics.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL, Bu-dapest, Hungary.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantine, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.pages 177?180, Prague, Czech Republic, June.Alon Lavie and Abhaya Agarwal.
2007.
ME-TEOR: An Automatic Metric for MT Evaluationwith High Levels of Correlation with Human Judg-ments.
pages 228?231, Prague, Czech Republic,June.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
Structuredoutput layer neural network language model.
In Pro-ceedings of ICASSP?11, pages 5524?5527.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In NAACL ?12: Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguisticson Human Language Technology.Saab Mansour and Hermann Ney.
2012.
A sim-ple and effective weighted phrase extraction for ma-chine translation adaptation.
In International Work-shop on Spoken Language Translation, pages 193?200, Hong Kong, December.Sab Mansour, Joern Wuebker, and Hermann Ney.2011.
Combining Translation and Language ModelScoring for Domain-Specific Data Filtering.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation (IWSLT), San Francisco, CA,December.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego,Adria` de Gispert, Patrick Lambert, Jose?
A.R.
Fonol-losa, and Marta R. Costa-Jussa`.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.Extending Statistical Machine Translation with Dis-criminative and Trigger-based Lexicon Models.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1 - Volume 1, EMNLP ?09, Singapore.191Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The KITEnglish-French Translation Systems for IWSLT2011.
In Proceedings of the Eighth Interna-tional Workshop on Spoken Language Translation(IWSLT).Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proc.
of Third ACL Workshop on Statistical Ma-chine Translation, Columbus, USA.Jan Niehues, Teresa Herrmann, Muntsin Kolss, andAlex Waibel.
2009.
The Universita?t KarlsruheTranslation System for the EACL-WMT 2009.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, UK.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003a.
Minimum Error Rate Train-ing in Statistical Machine Translation.
In Proc.
ofthe 41th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 160?167, Sap-poro, Japan, July.Franz Josef Och.
2003b.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Proc.of the 41st Annual Meeting on Association for Com-putational Linguistics, pages 160?167.M.
Popovic?, D. Stein, and H. Ney.
2006.
StatisticalMachine Translation of German Compound Words.In FinTAL - 5th International Conference on Nat-ural Language Processing, Springer Verlag, LNCS,pages 616?624.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three German treebanks: lexicalized and un-lexicalized baselines.
In Proceedings of the Work-shop on Parsing German.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI, Sko?vde,Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, UK.Helmut Schmid.
1995.
Improvements in part-of-speech tagging with an application to German.In Evelyne Tzoukermann and SusanEditors Arm-strong, editors, Proceedings of the ACL SIGDAT-Workshop, pages 47?50.
Kluwer Academic Publish-ers.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Int.
Conf.
on Spo-ken Language Processing, volume 2, pages 901?904, Denver, Colorado, USA.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL 2004, pages 101?104.
As-sociation for Computational Linguistics.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Be-yond (WPT-05), Ann Arbor, MI.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural LanguageProcessing and Knowledge Engineering, Beijing,China.Joern Wuebker, Matthias Huck, Stephan Peitz, MalteNuhn, Markus Freitag, Jan-Thorsten Peter, SaabMansour, and Hermann Ney.
2012.
Jane 2: Opensource phrase-based and hierarchical statistical ma-chine translation.
In International Conference onComputational Linguistics, pages 483?491, Mum-bai, India, December.192
