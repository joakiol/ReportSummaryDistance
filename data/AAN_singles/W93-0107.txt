HIERARCHICAL CLUSTERING OF VERBSRoberto Basili (*)Maria Teresa Pazienza (*)Paola Velardi (**)(*) Universita' diRoma T~x Vergata, Italy(**) Universita' di Ancona, ItalyAbstract.In this paper we present an unsupervisedlearning algorithm for incremental conceptformation, based on an augmented versionof COBWEB.
The algorithm is applied tothe task of acquiring a verb taxonomythrough the systematic observation of verbusages in corpora.Using a Machine Learning methodology fora Natural language problem requiredadjustments on both sides.
In fact, conceptformation algorithms assume the inputinformation as being stable, unambiguousand complete.
At the opposite, linguisticdata are ambiguous, incomplete, andpossibly erroneous.A NL processor is used to extract semi-automatically from corpora the thematicroles of verbs and derive a feature-vectorrepresentation f verb instances.
In orderto account for multiple instances of thesame verb, the measure of category utility,defined in COBWEB, has been augmentedwith the notion of memory inertia.
Memoryinertia models the influence that previouslyclassified instances of a given verb have onthe classification of subsequent instances ofthe same verb.
Finally, a method is definedto identify the basic-level classes of anacquired hierarchy, i.e.
those bringing themost predictive information about theirmembers.1.
IntroductionThe design of word-sense taxonornies iacknowledged as one of the most difficult(and frustrating) tasks in NLP systems.The decision to assign a word to a categoryis far from being straightforward(Nirenburg and Raskin (1987)) and oftenthe lexicon builders do not use consistentclassification pfincipia.Automatic approaches tothe acquisition ofword taxonomies have generally made useof machine readable dictionaries (MRD),for the typical definitory nature of MRDtexts.
For example, in Byrd et al, (1987)and other similar studies the category of aword is acquired from the first few wordsof a dictionary definition.
Besides the wellknown problems of inconsistency andcircularity of definitions, an inherentdifficulty with this approach is that verbscan hardly be defined in terms of genus anddifferentiae.
Verb semantics resides in thenature of the event they describe, that isbetter expressed by the roles played by itsarguments in a sentence.
Psycholinguistiestudies on verb semantics outline therelevance of thematic roles, especially ineategorisation activities Keil, (1989),Jackendoff (1983) and indicate theargument structure of verbs as playing acentral role in language acquisition Pinker(1989).
In NLP, representing verbsemantics with their thematic roles is aconsolidated practice, even thoughtheoretical researches (Pustejovski (1991))propose more rich and formalrepresentation frameworks.More recent papers Hindle (1990), Pereiraand Tishby (1992) proposed to clusternouns on the basis of a metric derived fromthe distribution of subject, verb and objectin the texts.
Both papers use as a source ofinformation large corpora, but differ in thetype of statistical approach used todetermine word similarity.
These studies,though valuable, leave several openproblems:701) A metric of conceptual c oseness basedon mere syntactic similarity isquestionable, particularly if applied toverbs.
In fact, the argument structureof verbs is variegated and poorlyoverlapping.
Furthermore, subject andobject relations do not fullycharacterize many verbs.2) Many events accumulate statisticalevidence only in very large corpora,even though in Pereira and Tishby(1992) the adopted notion ofdistributional similarity in part avoidsthis problem.3) The description of a word is an"agglomerate" of its occurrences in thecorpus, and it is not possible todiscriminate different senses.4) None of the aforementioned studiesprovide a method to describe andevaluate the derived categories.As a result, the acquired classificationsseem of little use for a large-scale NLPsystem, and even for a linguist hat is incharge of deriving the taxonomy.Our research is an attempt to overcome inpart the aforementioned limitations.
Wepresent a corpus-driven unsupervisedlearning algorithm based on a modifiedversion of COBWEB Fisher (1987),Gennari et al (1989).
The algorithm learnsverb classifications through the systematicobservation of verb usages in sentences.The algorithm has been tested on twodomains with very different linguisticstyles, a commercial and a legal corpus ofabout 500,000 words each.In section 2 we highlight he advantagesthat concept formation algorithms, likeCOBWEB, have over "agglomerate"statistical approaches.
However, using aMachine Learning methodology for aNatural Language Processing problemrequired adjustments on both sides.
Rawtexts representing instances of verb usageshave been processed to fit the feature-vectorlike representation needed for conceptformation algorithms.
The NL processorused for this task is briefly summarized insection2.1.
Similarly, it was necessary toadapt COBWEB to the linguistic nature ofthe classification activity, since, forexample, the algorithm does notdiscriminate different instances of the sameentity, i.e.
polysernic verbs, nor identicalinstances of different entities, i.e.
verbswith the same pattern of use.
Thesemodifications are discussed in sections 2.1trough 2.3.
Finally, in section 3 we presenta method to identify the basic-levelcategories of a classification, i.e.
those thatare repository of most of the lex ica linformation about heir members.Class descriptions and basic-levelcategories, as derived by our clusteringalgorithm, are in our view greatly helpful ataddressing the intuition of a linguisttowards the relevant taxonomic relations ina Oven language domain.2.
C IAULAI :  An algorithm toacquire word clustersIncremental example-based learningalgorithms, like COBWEB Fisher (1987),seem more adequate than other MachineLearning and Statistical methods to the taskof acquiring word taxonomies fromcorpora.
COBWEB has several desirablefeatures:a) Incrementality, since whenever new dataare available, the system updates itsclassification;b) A formal description of the acquiredclusters;c) The notion of category utility, used toselect among competing classifications.b) and e) are particularly relevant o ourlinguistic problem, as remarked in theIntroduction.On the other side, applying COBWEB toverb classification is not straightforward.First, there is a knowledge representationproblem, that is common to most MachineLearning algorithms: Input instances mustbe pre-coded (manually) using a feature-I Ciaula stands for Concept formation AlgorithmUsed for Language Acquisition, and has beeninspired by the tale "Ciaula scopre la luna" byLuigi Pirandello (1922).71vector like representation.
This limited theuse of such algorithms in many real worldproblems.
In the specific case we areanalyzing, a manual codification of verbinstances i not realistic on a large scale.Second, the algorithm does not distinguishmultiple usages of the same verb, nordifferent verbs that are found with thesame pattern of use, since differentinstances with the same feature vector aretaken as identical.
The motivation is thatconcept formation algorithms as COBWEBassume the input information as beingstable, unambiguous, and complete.
At theopposite, our data do not exhibit a stablebehaviour,  they are ambiguous,incomplete, and possibly misleading, sinceerrors in codification of verb instances maywell be possible.In the following sections we will discussthe methods by which we attempted toovercome these obstacles.2.1 Representing verb instancesThis section describes the formalrepresentation f verb instances and verbclusters in CIAULA.Verb usages input to the clusteringalgorithm are represented bytheir thematicroles, acquired semi-automatically fromcorpora by a process that has beendescribed in Basili, (1992a), (1992b), (inpress).
In short, sentences including verbsare processed as follows:First, a (general-purpose) morphologic anda partial syntactic analyzer Basili, (1992b)extracts from the sentences in the corpus allthe elementary syntactic relations (esl) inwhich a word participates.
Syntacticrelations are word pairs and triplesaugmented with a syntactic information,e.g.
for the verb to carry: N_V(company,car ry )  V_N(car ry , food)V_N(carry,goods)  V_prep_N(carry,with;truck), etc.Each syntactic relation is stored with itsfrequency of occurrence in the corpus.Ambiguous relations are weighted by a 1/kfactor, where k is the number of competingesl in a sentence.Second, the verb arguments are tagged byhand using 10-12 "naive" conceptualtypes (semantic tags), such as: ACT,PLACE, HUMAN_ENTITY, GOOD, etc.Conceptual types are not the same for everydomain, even though the commercial andlegal domains have many common types.Syntactic relations between words arevalidated in terms of semantic relationsbetween word classes using a set of semi-automatically acquired selectional rulesBasi l i ,  (1992a).
For example ,V_prep_N(carry,with,truck) is accepted asan istance of the high-level selectional rule\[ACT\]-->(INSTRUMENT)->\[MACHINE\].
The relation: \[carry\]->(INSTRUMENT)->\[truck\] is acquired aspart of the argument s ructure of the verb tocarry.
In other published papers wedemonstrated that the use of semantic tagsgreatly increase the statistical stabifity of thedata, and add predictive power to theacquired information on word usages, atthe price of a limited manual work (thesemantic tagging).For the purpose of this paper, theinteresting aspect is that single instances ofverb usages (local 2 meanings) are validatedon the basis of a global analysis of thecorpus.
This considerably reduces (thoughdoes not eliminate) the presence oferroneous instances.The detected thematic roles of a verb v in asentence are represented by the feature-vector:(1) v / (Rit:Catjt) it~ I, jt~ J t=l,2 ..... nwhere Rit are the thematic roles (AGENT,INSTRUMENT etc.)
3 and Cat j t  are theconceptual types of the words to which v isrelated semantically.
For example, the2 i.e.
meanings that are completely described withina single sentence of the corpus3 The roles used are an extension fSowa'sconceptual relations \[Sowa 1984\].
Details on theset of conceptual re ations used and a corpus-basedmethod to select adomain-approprime set, areprovided inother papers.72following sentence in the commercialdomain:"... ia ditta produce beni di consumo conmacchinari elettromeccanici..""... the company produces goods withelectroraechanical machines.."originates the instance:produce/(AGENT:HUMAN~ENTITY,OBJECT:GOODS,INSTRUMENT:MACHINE)Configurations in which words of the sameconceptual type play the same roles arestrong suggestion of semantic similaritybetween the related events.
Thecategorisation process must capture thissimilarity among local meanings of verbs.The representation f verb clusters followsthe scheme adopted in COBWEB.
Eachtarget class is represented by the probabilitythat its members (i.e.
verbs) are seen with aset of typical roles.
Given the set {Ri}i~Iof thematic roles and the set {Catj }je j ofconceptual types, a target class ~ for ourclustering system is given by the following(2) cE = < cog,, \[x\]ij, Vc E, S~ >or equivalently by(2)' < c, \[x\]ij, V, S >A class is represented in COBWEB by thematrix \[x\]ij, showing the distribution ofprobability among relations (Ri) andconceptual types (Cat j).
The additionalparameters V~,and cog, are introduced toaccount for multiple instances of the sameverb in a class, c~ is the cardinality (i.e.the number of different instance membersof cE), and V~ is the set of pairs <v, v#>such that it exists at least one instancev / (Ri:Caztj)classified in ~, and v# is the number ofsuch instances.Finally, S~,iS the set of CEsubtypes.
Thedefinitions of the empty class (3.1) and ofthe top node of the taxonomy (3.2) followsfrom (2)(3.1) <0,\[xlij,{O},lO}>with xij=0 for each i,j(3.2) <Ntot, \[x\]ij, V, S >where Ntot is the number of availableinstances in the corpus, V is the set ofverbs with their absolute occurrences.An excerpt of a class acquired from thelegal domain is showed in Fig.
1.
Thesemantic types used in this domain arelisted in the figure.Special type of classes are those in whichonly a verb has been classified, that we willcall singleton classes.
A singleton class is aclass cE=<c,\[x\] i j ,V,S> for whichcard(V)= 1.
It will be denoted by { v } wherev is the only member of (whatever itsoccurrences) ~.
For a singleton class it isclearly true that S={0}.
Note that asingleton class is different from an instancebecause any number of instances of theverb v can be classified in {v }.2.2 Measuring the utility of aclassificationAs remarked in the introduction, a usefulproperty of concept formation algorithms,with respect to agglomerate statisticalapproaches, is the use of formal methodsthat guide the classification choices.Quantitative approaches to model humanchoices in categorisation have been adoptedin psychological models of conceptualdevelopment.
In her seminal work, Rosch(1976) introduced a metrics of preference,the category cue validity, expressed by thesum of expectations of observing somefeature in the class members.
This value ismaximum for the so-called basic levelcategories.
A later development, used inCOBWEB, introduces the notion ofcategory utility, derived from theapplication of the Bayes law to theexpression of the predictive power of agiven classification.
Given a classification73into K classes, the category utility is givenby:(4.
)K 2 E prob(C z)~.prob(attr ffval}Ck" l  JJIn COBWEB, a hill climbing algorithm isdefined to maximize the category utility of aresulting classification.
The followingexpression is used to discriminate amongconflicting clusters:K 2 2 prob(C ~.prob(attr pval~C~ -~,prob(attr pva l j )(5) k-I j$ ,IKThe clusters that maximize the abovequantity provide the system with thecapability of deriving the best predictivetaxonomy with respect to the set of iattributes and j values.
This evaluationmaximizes infra-class imilarity and intra-class dissimilarity.Class: 123I Cardina I ity :AAGEN: 0.00AFF: 0.00FI_S: 0.00MANN: 0.00FI_D: 0.00FI_L: 0.00REF: 0.00REC: 0".00CAUSE: 0.00LOC: 0.00Father class: 718 (5) Level: 2D0.00 00.00 00.00 01.00 00.00 00.00 00.00 00.00 00.00 00.00 0Tau: 1.00RE G HE AE S00 0.00 1.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.0000 0.00 0.00 0.00 0.00Omega : 0.28RE TE P AM Q M0 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.000 00 0.00 0.00 0.00 0.00 0.00Heads:- approvare (occ 8) %to approvestabl l i re (occ 7) %to establish, to decide- prevedere (occ i) %to foresee- d isporre (occ i) %to dispose- d ich iarare (occ I) %to declareLE, QEd~/2/k (semantic types for the legal domain):A=ACT, D=DOCUMENT, RE=REAL ESTATE, G=GOODS, HE=HUMANENTITY ,AE=ABSTRACT_ENTITY,  S=STATE, AM=AMOUNT, TE=TEMPORAL_ENTITY,P=PLACES, Q=QUALITY, M=MANNER- Fig 1.
Example of cluster produced by the system -The notion of category utility adopted inCOBWEB, however, does not fully copewith our linguistic problem.
As remarked inthe previous section, multiple instances ofthe same entity are not considered inCOBWEB.
In order to account for multipleinstances of a verb, we introduced thenotion of mnemonic inertia.
The mnemonicinertia models an inertial trend attracting anew instance of an already classified verbin the class where it was previouslyclassified.Given the incoming instancev / (Ri:Ca, tj)and a current classification in the set ofclasses ~,  for each k the mnemonic inertiais modelled by:(6) gk(v) = #v/Ckwhere #v is the number of instances of theverb v already classified in 5~: and Ck is thecardinality of c~:.
(6) expresses a fuzzy membership of v tothe class 5~k.
The more instances of v areclassified into 5fk, the more futureobservations of v will be attracted by ~.
Asuitable combination of the mnemonic74inertia and the category utility provides oursystem with generalization capabilitiesalong with the "conservative" policy ofleaving different verb instances eparate.The desired effect within the data is thatslightly different usages of a verb areclassified in the same cluster, whileremarkable differences result in differentclassifications.The global measure of category utility, usedby the CIAULA algorithm duringclassification, can now be defined.
Let v /(Ri:Catj) be the incoming instance, 56k bethe set of classes, and let cu(v,k) be thecategory utility as defined in (5), themeasure It, given by(7) It = vcu(v,k) + (1-v)Itk(v) v~ \[0,1\]expresses the global ut i l i ty of theclassification obtained by assigning theinstance v to the class ~?k.
(7) is a distancemetrics among instances and classes.2.3 The incrementa l  c luster ingalgor i thm.The algorithm for the incremental clusteringof verb instances follows the approach usedin COBWEB.
Given a new incominginstance I and a current valid classification{5~k}ke K, the system evaluates the utilityof the new classification obtained byinserting I in each class.
The maximumutility value corresponds to the bestpredictive configuration of classes.
Afurther attempt is made to change thecurrent configuration (introducing a newclass, merging the two best candidate forthe classification orsplitting the best classesin the set of its son) to improve thepredictivity.
The main difference withrespect o COBWEB, due to the linguisticnature of the problem at hand, concern theprocedure to evaluate the utility of atemporary classification and the MERGEoperator, as it applies to singleton classes.The description of the algorithm is given inAppendix 1.
Auxiliary procedures areomitted for brevity.According to (7), the procedureG_UTILITY(x, I, ~, ..~, v) evaluates theutility of the classification as a combinationof the category utility and the inertial factorintroduced in (6).
Current valuesexperimented for v are 0.90-0.75.Figure 2 shows the difference between thestandard MERGE operation, identical tothat used in COBWEB, and the elementaryMERGE between two singleton classes, asdefined in CIAULA.- Fig.
2: Merge (a) vs.
Elementary Merge (b) -3.
Experimental Results.The algorithm has been experimented ontwo corpora of about 500,000 words each,a legal and a commercial domain, thatexhibit very different linguistic styles andverb usages.
Only verbs for which at least65 instances in each corpus have beenconsidered, in order to further reduceparsing errors.
Notice however that the useof semantic tags in corpus parsing reducesconsiderably the noise, with respect oother corpus-based approaches.75In the first experiment, CIAULA classifies3325 examples of 371 verbs, from the legalcorpus.
In the second, it receives 1296examples of 41 verbs from the commercialcorpus.
Upon a careful analysis of theclusters obtained from each domain, theresulting classifications were judged quiteexpressive, and semantically biased fromthe target linguistic domains, a part fromsome noise due to wrong semanticinterpretation of elementary syntacticstructures Basili et al, (1992a).
However,the granularity of the description ofthe finaltaxonomy is too fine, to be usefullyimported in the type hierarchy of a NLPsystem.
Furthermore, the order ofpresentation of the different examplesstrongly influences the final result 4.
Inorder to derive reliable results we must findsome invariant with respect to thepresentation order.
An additionalrequirement is to define some objectivemeasure of the quality of the acquiredclassification, other than the personaljudgement ofthe authors.In this section we define a measure of theclass informative power, able to capture themost relevant levels of the hierarchy.
Theidea is to extract from the hierarchy thebasic level classes, or classes that arerepository of the most relevant lexicalinformation about their members.
Wedefine basic level classes of theclassification those bringing most predictiveand stable information with respect to thepresentation rder.The notion of basic level classes has beenintroduced in Rosch (1978).
Sheexperimentally demonstrated that someconceptual categories are more meaningfulthan others as for the quantity ofinformation they bring about theirmembers.
Membership to such classesimplies a grater number of attributes to beinherited by instances of the domain.
Theseclasses appear at the intermediate levels of ataxonomy: for example within the vaguenotion of animal, classes uch dog or cat4 This is an inherent problem with conceptformation algorithmsseem to concentrate the major part ofinformation about their members, withrespect for example to the class ofmammals Lakoff (1987).But what is a basic-level c ass for verbs?
Aformal definition for these morerepresentative classes, able to guide theintuition of the linguist in the categorisationactivity has been attempted, and will bediscussed inthe next section.3.1.
Basic level categories ofverbs.The information conveyed by the derivedclusters, c~=<c,\[x\]ij,V,S>, is in thedistributions of the matrices \[x\]ij, and in theset V. Two examples may be helpful atdistinguishing classes that are moreselective, from other more vague clusters.Let C~?l be a singleton class, withWI=<I,\[xl\],VI,{O}>.
This clearly impliesthat \[xl\] is binary.
This class is highlytypical, as it is strongly characterized byitsonly instance, but it has no generalizationpower.
Given, for example, a classqb?l=<10,\[x2\],V2,S> for which thecardinality of a V2 is 10, and let \[x2\] besuch that for each couple <ij> for whichx2ij~0, it follows x2i'=I/10j .
This class isscarcely typical but has a s t ronggeneralization power, as it clusters verbsthat show no overlaps between the thematicroles they are represented by.
We can saythat ypicality is signaled by high values ofro les- types probabi l i t ies  ( i .e.xij=prob((Ri:Catj) I c g)  ), while thegeneralization power to of a classW=<c,\[x\]i j ,V,S>, is related to thefollowing quantity:(8) co = card(V)/cTo quantify the typicality of a classcg=<c,\[x\]ij,V,S>, the following definitionsare useful.
Given a threshold ae \[0,1\], thetypicality of Cgis given by:76(9) xW = ~<i,j>e TW xij / card(T~where T~,is the typicality set of ~, i.e.I<i,j> I xij >a}.DEF (Basic-level verb category).
Giventwo thresho lds  T, 8 e \[ 0 ,1  \ ] ,c~?=<e,\[x\]ij,V,S> is a basic-level categoryfor the related taxonomy iff:(10.1) co < T (generalization power)(10.2) 'cog > 8 (typicality)Like all the classes derived by the algorithmof section 2.3, each basic-level category~=<c,\[x\]ij,V,S> determines two fuzzymembership values of the verb v includedin V. The local membership of v to ~,I.t l~(v), is defined by:(11) gtlW(V)= #v/max{# I <w, #w>~ V}The global membership ofv to ~, \].t2~(v),is :(12) l.t2~(v) = #v / nv,where nv is the number of differentinstances of v in the learning set.
(11)depends on the contribution of v to thedistribution of probabilities \[x\]i',j i.e.
itmeasures the adherence of v to theprototype.
(12) determines how typical isthe classification of v in ~, with respect toall the observations ofv in the corpus.
Lowvalues of the global membership are usefulat identifying instances of v that are likelyto be originated by parsing errors.Given a classification ,.qbf extended sets oflinguistic instances, the definition (10)identifies all the basic-level classes.Repeated experiment over the two corporademonstrated that these classes aresubstantially invariant with respect o thepresentation rder of the instances.The values y=0.6 and 8=0.75 have beenempirically selected as producing the moststable results in both corpora.4 DiscussionThe Appendix 2 shows all the basic levelcategories derived from a small learningset, named DPR633, that belongs to thelegal corpus.
CIAULA receives in input293 examples of 30 verbs.
The reason forshowing DPR633 rather than an excerpt ofthe results derived from the full corpus isthat there was no objective way to selectamong the over 300 basic level classes.
InAppendix 2, the relatively low values of gtland I,t 2 are due to the exiguity of theexample set, rather than to errors inparsing, as remarked in the previoussection.
Of corse, the basic-level classesextracted from the larger corpora exhibit amore striking similarity among theirmembers, indicated by highest values ofglobal and local membership.
An exampleof cluster extracted from the whole legalcorpus was shown in Figure 1.The example shown in Appendix 2 ishowever "good enough" to highlight someinteresting property of our clusteringmethod.
Each cluster has a semanticdescription, and the degree of local andglobal membership of verbs give anobjective measure of the similarity amongcluster members.
It is interesting toobservethat the algorithm classifies in distinctclusters different verb usages.
Forexample, the cluster 4 and the cluster 6classify two different usages of the verbindicare, e.g.
indicare un'ammontare (toindicate an amount) and indicare un motive(to specify a motivation), where"ammontare" is a type of AMOUNT(AM)and " mot ive"  is a type ofABSTRACT_ENTITY (AE).The two clusters 13 and 14 capture thephysical and abstract use of eseguire, e.g.eseguire un'opera (to build abuilding(=REAL_ESTATE) yrs.
eseguireun pagamento  (to make apayment(= AMOUNT,A CT) ).77The clusters 3 and 6 classify two uses ofthe verb tenere, i.e.
tenere un registro (tokeep a record(=DOCUMENT) yrs.
tenereun d i scorso  (to ho ld  aspeech(=ABSTRACT_ENTITY)).
Manyother (often domain-dependent) examplesare reflected in the derived classification.To sum up, we believe that CIAULA hasseveral advantages over other clusteringalgonthrns presented in literature.
(1) The derived clusters have a semanticdescription, i.e.
the predicted thematicroles of its members.
(2) The clustering algorithm incrementallyassigns instances to classes, evaluatingits choices on the basis of a formalcfitefium, the global utility.
(3) The defined measures of typicality andgeneralization power make it possibleto select the basic-level classes of ahierarchy, i.e.
those that are repositoryof most lexical information about heirmembers.
These classes demonstratedsubstantially stable with respect o theorder of presentation ofption, i.e.
thepredicted thematic roles of itsmembers.
(4) It is possible to discriminate differentusages of verbs, since verb instancesare considered individually.The hierarchy, as obtained by CIAULA, isnot usable tout court by a NLP system,however class descriptions and basic-levelcategories appear to be greatly useful ataddressing the intuition of the linguist.References.Natural Language Procesing: Utilizing theGrammar COding System of LDOCE", InComputational Linguistics, December 1987Dubois, D., Prade, H. (1988), "Possibility Theory:an Approach to Computerized Processing ofUncertainty", Plenum Press, New York, 1988.Fisher, D., (1987), Knowledge acquisition viaincremental conceptual clustering, MachineLearning, 2, 1987.J.
Gennari, P. Langley, D. Fisher, (1989), Modelof incremental Concept Formation, in ArtificialIntelligence n.l-3, 1989.Jacobs, P., (1991), "Integrating language andmeaning in structured inheritance networks", in"Principles of Semantic Networks", J. Sowa Ed.,Morgan Kauffmann, 1991.R.
Jackendoff, (1983), "Semantics and cognition",MIT Press, 1983.D.
Hindle, (1990), Noun classification frompredicate argument structures, in Proc.
of ACL,1990F.
Keil, (1989),Concepts, kinds and cognitivedevelopment, The MIT press, 1989.G.
Lakoff, (1987),Woman, fire and dangerousthings, University of Chicago Press, 1987.S.
Nirenburg, V. Raskin, (1987), The subworldconcept lexicon and the lexicon managementsystem, In Computational Linguistics, n. 13,December 1987F.Pereira, H. Tishby, (1992), "Distributionalsimilarity, Phase Transition and HierarchicalClustering"~ in Proc.
of AAAI Fall SymposiumSeries, Probabilistic Approaches to NaturalLanguage, Cambridge, October, 1992.S.Pinker, (1989), Learnability and Cognition - TheAcquisition of Argument Structure, MIT Press,1989.Luigi Pirandello, (1922), Novelle per un anno,Editore R. Bemporad, Mondadon, 1922.Pustejovsky, J., (1991), "The Generative Lexicon",Computational Linguistics, vol.
17, n. 4, 1991.E.
Rosch, (1978), Principle of categorization, inCognition and Categorization, Erlbaum 1978.R.
Basili, M.T.
Pazienza, P. Velardi, (1992a)Computational Lexicons: the neat examples and theodd exemplars, Proc.
of 3rd.
Conf.
on AppfiedNLP, 1992.R.
Basili, M.T.
Pazienza, P. Velardi, (1992b) "Ashallow Syntax to extract word associations fromcorpora', in Literary and Linguistic Computing,vol.
2, 1992R.
Basili, M.T.
Pazienza, P. Velardi, (in press)Semi-automatic extraction of linguistic informationfor syntactic disambiguation, Applied ArtificialIntelligence.R.
Byrd, N. Calzolari, M. Chodorow, I. Klavans,M.
Neff, O. Rizk, (1987), "Large lexicons for78Appendix 1: The Algorithm for Conceptual Clustering of Verb SemanticInstancesInput: ,~  root node of the current taxonomyI, Unclassified verb semantic instancev(I), verb head of the instance IOumut: An exhaustive conceptual c assification ofthe incoming instances.Y.~.I.
?.~: c~ ,~.
,.,~, /g",,~ classes of the taxonomyx, p, s, n, m, q measures of global utility of a classificationCIAULA( ,~ ,  I, v),~  is a terminalYdt?d~L~ is the singleton {v(I)}THENINCORPORATE(,~'~, I)ELSENEW_TERMINAL(g, I)INCORPORATE(,~, I)INCORPORATE(,~"~, I)subtype C~of~G_UTILITYx, I, ~, ,~ ,  v)Let: p the best score x for classifying I in the class ,~s the second best score x for classifying I in the classS an the score x for classifyng I in a new node,.Af~"subtype of ,~m the score x of classifying I in node ~ merge between ,.~andq the score x in classifying I in a classification obtained removing ,~fi'om thecurrent level and picking up the set of its son to the previous ,~levelX.E P is the highest score~d~tFd2CIAULA( ,~e, I, v).LE n is the highest scoreTHEN initialize,.~/'with values hown by IELSE /.\[ m is the highest scoreTHENMERGE( ,.,4\[, ~, ~, ,~  I)CIAULA( ,.4~ ?,I, v)Fda..q.F,_ .I.E q is the highest scoreTHENsezrr(CIAULA( ~, I ,  V)79Appendix 2: Basic level classes derived from the DPR633 CorpusClass :  1 Card :  3 Omega=0.67  Tau:  1 .00PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (F IG_DEST)  -- \[A\]-- (REC IP IENT)  -- \[HE\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :d ich ia rare  (0 .50  - 0 .20)applicare ( i .00  - 0 .07)C lass :  2 Card :  i0 Omega=0.50  Tau:  1 .00PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (OBJ) -- \[AM\]-- (REC IP IENT)  -- \[HE\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :r i ch iedere  (0.33 - 0 .25)esegu i re  (0.33 - 0 .06)app l i care  ( i .00  - 0 .
i i )versare  (0 .66 - 0 .13)pagare  ( i .00  - 0 .30)C lass :  3 Card :  14 Omega=0.50  Tau:  1 .00PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (OBJ) -- \[D\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :tenere  (0 .25  - 0 .
I i )app l i care  (0 .25  - 0 .03)a l legare  (0 .50  - 0 .33)p revedere  ( i .00  - 0 .44)emet tere  (0 .25  - 0 .07)ind icare  (0 .50  - 0 .03)esegu i re  (0 .75  - 0 .20)C lass :  4 Card :  9 Omega=0.33  Tau:  1 .00PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (OBJ) -- \[AM\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :app l i care  ( I .00  - 0 .19)ind icare  (0 .60  - 0 .05)p revedere  (0 .20  - 0 .
I I )C lass :  5 Card :  3 Omega=0.67  Tau:  0 .89PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (OBJ) -- \[A, AM\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :ammettere  ( i .00  - 0 .66)mod i f i care  (0 .50  - 0 .50)C lass :  7 Card :  3 Omega=0.67  Tau:  0 .78PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :-- (OBJ) -- \[RE\]-- (F IG_LOC)  -- \[HE\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :comprendere  (0 .50 - 0 .05)ind icate  (1 .00  - 0 .03)C lass :  8 Card :  18 Omega=0.28  Tau:  1 .00PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :-- (SUBJ)  -- \[HE\]- -  (MANNER)  - -  \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :p revedere  (0 .12 - 0 .11)d i spor re  (0 .12 - 0 .33)approvare  ( i .00  - 0 .88)s tab i l i re  (0.87 - 0 .38)d ich ia rare  (0 .12 - 0 .20)C lass :  9 Card :  3 Omega=0.67  Tau:  0 .78PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :-- (SUBJ)  -- \[HE\]-- (MANNER)  -- \[A\]-- ( LOCATION)  -- \[_\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :rendere  (0 .50  - 0 .20)operare  ( I .00 - 0 .18)C lass :  i0 Card :  3 Omega=0.67  Tau:  0 .78PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :?- (OBJ) -- \[AM\]-- (F IG_DEST)  -- \[A\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :versare  (0 .50 - 0 .06)operare  ( I .00  - 0 .18)C lass :  i i  Card :  6 Omega=0.67  Tau:  1 .00PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :-- (OSJ) -- \[D\]-- (F IG_DEST)  -- \[A\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :p resentare  (0.33 - 0 .06)tenere  (0.33 - 0 .11)emet tere  (1 .00 - 0 .23)ind icate  (0.33 - 0 .01)C lass :  6 Card :  8 Omega=0.62  Tau:  1 .00PROTOTYPE ( i .e .
,  P red ic ted  Ro les ) :-- (OBJ) -- \[AE\]-- (REFERENCE)  -- \[D\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :operare  (0 .25  - 0 .09)e f fe t tuare  (0 .25  - 0 .01)r i ch iedere  (0 .25  - 0 .25)tenere  (0 .25  - 0 .
I i )ind icate  ( i .00  - 0 .07)C lass :  12 Card :  6 Omega=0.50  Tau:  0 .78PROTOTYPE ( i .e.
,  P red ic ted  Ro les ) :-- (OBJ) -- \[A, AM\]-- (MANNER)  -- \[A\]Verbs  ( loca l  - g loba l  degree  membersh ip ) :versare  (0 .25  - 0 .06)esegu i re  (0 .25  - 0 .06)e f fe t tuare  (1 .00  - 0 .07)80Class :  13 Card :  3 Omega=0.67  Tau: 1 .00PROTOTYPE (i.e., P red ic ted  Ro les) :-- (OBJ) -- \[RE\]-- (MANNER) -- \[A\]Verbs  ( local  - g loba l  degree  membersh ip ) :esegu i re  (0.50 - 0.06)comprendere  ( i .00 - 0. i0)C lass :  14 Card:  ii Omega=0.55  Tau: 1.00PROTOTYPE (i.e., P red ic ted  Ro les) :-- (OBJ) -- \[A, AM\]Verbs  ( local  - g loba l  degree  membersh ip ) :p rodur re  (0.16 - 0. i i )cons iderare  (0.16 - 0.16)app l i care  (0.16 - 0.03)esegu i re  (0.16 - 0.06)e f fe t tuare  ( i .00 - 0.
I i )ind icate  (0.16 - 0.01)81
