Coling 2010: Poster Volume, pages 1310?1317,Beijing, August 2010Automatic Extraction of Cue Phrases forCross-Corpus Dialogue Act ClassificationNick Webb and Michael FergusonILS Institute, SUNY Albanynwebb@albany.edu, ferguson@cs.albany.eduAbstractIn this paper, we present an investiga-tion into the use of cue phrases as a ba-sis for dialogue act classification.
We de-fine what we mean by cue phrases, and de-scribe how we extract them from a manu-ally labelled corpus of dialogue.
We de-scribe one method of evaluating the use-fulness of such cue phrases, by applyingthem directly as a classifier to unseen ut-terances.
Once we have extracted cuephrases from one corpus, we determineif these phrases are general in nature, byapplying them directly as a classificationmechanism to a different corpus to thatfrom which they were extracted.
Finally,we experiment with increasingly restric-tive methods for selecting cue phrases,and demonstrate that there are a smallnumber of core cue phrases that are use-ful for dialogue act classification.1 MotivationIn this paper we present a recent investigation intothe role of linguistic cues in dialogue act (DA)classification.
Dialogue acts (Bunt, 1994) are an-notations over segments of dialogue that charac-terise the function of those segments.
Linguisticcues, which can take many forms including lexi-cal and syntactic structures, are features that canserve as useful indicators of discourse structure(Hirschberg and Litman, 1993; Grosz and Sidner,1986).
In prior work, several researchers haveshown that cue phrases can be a powerful fea-ture for DA classification (Samuel et al, 1999;Webb et al, 2005a).
Webb and Liu (2008) havepreviously shown that cue phrases automaticallyextracted from one corpus can be used to clas-sify utterances from a new corpus.
We take thisapproach and apply it to two established corporawith manually encoded dialogue act annotations,to investigate both the existence and the useful-ness of cue phrases shared between the two cor-pora.2 Related WorkIn parallel with the increased availability of man-ually annotated dialogue corpora there has beena proliferation of literature detailing dialogue actlabelling as a classification task.
Prior work de-scribes the selection of features from the corpus(including word n-grams, cue phrases, syntacticstructures, dialogue history and prosodic cues)which are then passed to some machine learn-ing algorithm.
Most studies have concentrated ona single corpus, and optimised feature selectionand learning algorithm accordingly.
In this workwe focus on two corpora, Switchboard and ICSI-MRDA, and discuss prior classification efforts re-lating to these two corpora.2.1 Switchboard CorpusThe Switchboard corpus contains a large numberof approximately 5-minute conversations betweentwo people who are unknown to each other, whowere asked to converse about a range of every-day topics with little or no constraint.
The DA an-notated portion of the Switchboard corpus (Juraf-sky et al, 1997) consists of 1155 annotated con-versations, containing some 225,000 utterances,of which we use 200,000 utterances, the rest be-ing held out for separate experiments.
The dia-logues are annotated with a non-hierarchical vari-ant of the DAMSL annotation scheme (Core et al,1999).
The resulting Switchboard-DAMSL an-notation was a set of more than 220 distinct la-bels.
To obtain enough data per class for statis-tical modelling purposes, a clustered tag set wasdevised, which distinguishes 42 mutually exclu-1310sive DA types.
Classification over the Switch-board corpus has been demonstrated using Deci-sion Trees (Verbree et al, 2006), Memory-BasedLearning (Rotaru, 2002) and Hidden MarkovModels (HMM) (Stolcke et al, 2000).
The workof Stolcke et al (2000) is often cited as the bestperforming, achieving a classification accuracy of71% over the 42 labels, although there is no cross-validation of these results.
The approach of Stol-cke et al (2000) combines HMM modelling of ut-terances with a tri-gram model of DA sequences.Webb et al (2005a) report a slightly lower cross-validated score (of 69%) containing an individ-ual classification high of 72%, using an intra-utterance, cue-based classification model.2.2 ICSI-MRDA CorpusLike the Switchboard corpus, the ICSI MeetingRoom DA (MRDA) corpus (Shriberg et al, 2004)was annotated using a variant of the DAMSL tag-set, similar but not identical to the Switchboard-DAMSL annotation.
The differences (and atranslation between the two sets) can be seen inShriberg et al (2004).
The underlying domainof the dialogues in the ICSI-MRDA corpus wasthat of multi-party meetings, with multiple partic-ipants discussing an agenda of items in a struc-tured meeting.
This application required the in-troduction of new tags specifically for this sce-nario, such as a label introduced to indicate whenan utterance was used to take control of the meet-ing.
The ICSI-MRDA corpus comprises 75 nat-urally occurring meetings, each around an hourin length.
The section of the corpus we use con-sists of around 105,000 utterances.
For each utter-ance in the corpus, one general tag was assigned,with zero or more additional specific tags.
Ex-cluding non-labelled cases, there are 11 generaltags and 39 specific tags resulting in 1,260 uniquedialogue acts used in the annotation.
As with theSwitchboard corpus, processing steps were intro-duced that compressed the number of unique DAsto 55.
In later work, the dimensionality was fur-ther reduced, resulting in a subset of just 5 labels.Over the ICSI-MRDA corpus, we also see DAclassification efforts using Decision Trees (Ver-bree et al, 2006) and Memory-Based Learning(Lendvai and Geertzen, 2007), in addition toGraph Models (Ji and Bilmes, 2006) and Maxi-mum Entropy (Ang et al, 2005).
Comparativelyfew approaches have been applied to the 55-labelannotated corpus, with most choosing to focus onthe 5-label clustering, presumably for the result-ing increase in score.
When Ji and Bilmes (2005)apply a Graph Model to the 55 category corpus,they achieve a classification accuracy of 66%.However, when they apply the exact same methodto the 5-label corpus (Ji and Bilmes, 2006), clas-sification accuracy is boosted to 81%.
The bestreported classification score on the the 5-label ver-sion of the corpus is reported by Verbree et al(2006), who achieve 89% classification accuracyby modelling the words of the utterance, the DAhistory and some orthographic information (suchas the presence of question marks).It remains very difficult to directly compare ap-proaches, even when applied to the same corpus,so cross-corpora comparisons must be carefullyconsidered.
There are issues of the DA label setused, the labels considered and those ignored, thepre-processing of the corpus, the use of ortho-graphic information, or prosody and so on.
Whatseems clear is that there are no obvious leadingcontender for algorithm best suited to the DA clas-sification task.
Instead, we focus on the featuresused for DA classification.3 Automatic Cue ExtractionWhen examining prior approaches, we noticedthat they used a range of different features for theDA classification task, including lexical, syntac-tic, prosodic and dialogue context features.
Mostclassifiers used some lexical features (the wordsin the utterances under consideration), frequentlyemploying some kind of Hidden Markov Mod-elling to every utterance (Levin et al, 2003; Stol-cke et al, 2000; Reithinger and Klesen, 1997), atechnique popular in speech processing.
We wereinspired by the work of Samuel et al (1999), whoinstead of modelling entire utterances, extract sig-nificant cue phrases from the VerbMobil corpusof dialogues.
We use a method for cue extractionunused by Samuel et al (1999).What defines a good cue phrase?
We are look-ing for words or phrases in a corpus that regularlyco-occur with individual dialogue acts.
We use1311the term predictivity to indicate how predictive aphrase is of a particular DA.
We want to selectphrases that are highly indicative, and so concernourselves with the highest predictivity of a par-ticular cue phrase.
We call this score the maxi-mal predictivity.
There are several other thresh-olds that should also be apparent.
First, belowsome maximal predictivity score, we assume thatphrases will no longer be discriminative enough tobe useful for labelling DAs.
Second, the numberof occurrences of each phrase in the corpus as awhole is important.
In their experiments, Samuelet al (1999) constructed all n-grams of lengths1 through 3 from the corpus, and then applied arange of measures which pruned the n-gram listuntil only candidate cue phrases remained.
In or-der to test the effectiveness of these automaticallyacquired cue phrases, Samuel et al (1999) passedthem as features to a machine learning method, intheir case transformation-based learning.More formally, we can describe our criteria,predictivity, for selecting cue phrases from the setof all possible cue phrases in the following way.The predictivity of phrase c for DA d is the condi-tional probability P (d|c), where:P (d|c) = #(c&d)#(c)We represent the set of all possible cue phrases(all n-grams length 1?4 from the corpus) as C,so given c ?
C : c represents some possible cuephrase.
Similarly, D is the set of all dialogue actlabels, and d ?
D : d represents some dialogueact label.
Therefore #(c) is the count of (pos-sible) cue phrase c in corpus, and #(c&d) is thecount of occurrences of phrase c in utterances withdialogue act d in the training data.
The maximalpredictivity of a cue phrase c, written as mp(c), isdefined as:mp(c) = maxd?DP (d|c)In their experiments, Samuel et al (1999) alsoexperimented with conditional probability, usingP (c|d), or the probability of some phrase occur-ring given some Dialogue Act.
For our exper-iments, the word n-grams used as potential cuephrases during are automatically extracted fromtraining data.
All word n-grams of length 1?4 within the data are considered as candidates.The maximal predictivity of each cue phrase canbe computed directly from the corpus.
We canuse this value as one threshold for pruning po-tential cue phrases from our model.
Removingn-grams below some predictivity threshold willimprove the compactness of the model produced.Another reasonable threshold would appear to bethe frequency count of each potential cue phrase.Phrases which have a low frequency score arelikely to have very high predictivity scores, pos-sibly skewing the model as a whole.
For example,any potential cue phrase which occurs only oncewill de-facto have a 100% predictivity score.
Wecan use a minimal count value (t#) and minimalpredictivity thresholds (tmp) to prune the set C?of ?useful?
cue phrases derived from the trainingdata, as defined by:C?
= {c ?
C |mp(c) ?
tmp ?#(c) ?
t#}The n-grams that remain after this thresholdingprocess are those we identify as cue phrases.
Forour initial experiments, we used a predictivity of30% and a frequency of 2 as our thresholds for cueextraction.4 Cue-Based DA ClassificationHaving defined our mechanism to extract cuephrases from a corpus, we need some way toevaluate their effectiveness.
Samuel et al (1999)passed their cue phrases as a feature to a machinelearning method.
We chose instead a methodwhere the cue phrases extracted from a corpuscould be used directly as a method of classifi-cation.
If our extracted cues are indeed reliablepredictors of dialogue acts, then a classifier thatuses these cues directly should perform reason-ably well.
If, on the other hand, this mechanismdid not work, it would not necessarily mean thatour cue phrases are not effective, only that weneed to pass them to a subsequent machine learn-ing process as others had done.
The benefit of ourdirect classification approach is that it is very fastto evaluate, and gives us immediate feedback asto the possible effectiveness of our automaticallyextracted cue phrases.1312The predictivity of a cue phrase can be ex-ploited directly in a simple model of DialogueAct classification.
We can extract potential cuephrases as described in Section 3.
The resultingcue phrases selected using our measure of predic-tivity are then used directly to classify unseen ut-terances in the following manner.
We identify allthe potential cue phrases a target utterance con-tains, and determine which has the highest predic-tivity of some dialogue act category, then assignthat category.
Given the notation we define earlier,we can obtain the DA predicted by a particular cue(dp(c)) by:dp(c) = argmaxd?DP (d|c)If multiple cue phrases share the same maxi-mal predictivity, but predict different categories,we select the DA category for the phrase whichhas the higher number of occurrences (that is, then-gram with the highest frequency).
If the combi-nation of predictivity and occurrence count is in-sufficient to determine a single DA, then a randomchoice is made amongst the remaining candidateDAs.
If ng(u) defines the set of ngrams of length1..4 in utterance u, and C?u is the set of n-grams inthe utterance u that are also in the threshold modelC?
then C?u is defined as:C?u = ng(u) ?
C?Given our thresholds, the mpu(u) (the utter-ance maximal prediction, or mp value for thehighest scoring cue in utterance u) is defined as:mpu(u) = maxc?C?ump(c)The maximally predictive cues of an utterance(mpcu(u)) are:mpcu(u) = {c ?
C?u |mp(c) = mpu(u)}Then the maximal cue of utterance (mcu(u)),i.e.
one of its maximally predictive cues that has amaximal count (from within that set), is:mcu(u) = argmaxc?mpcu(u)#(c)Finally, for our classification model, dpu(u) ut-terance DA prediction ?
the DA predicted bymodel for utterance u, is defined as:dpu(u) = dp(mcu(u))If no cue phrases are present in the utteranceunder consideration, then a default tag is assigned.To this basic model, we added three fur-ther elaborations.
The first used models sensi-tive to utterance length.
When examining theICSI-MRDA corpus, Ji and Bilmes (2006) foundthat the mean length of <STATEMENT> utter-ances was 8.60 words, <BACKCHANNEL> utter-ances were 1.04 words, <PLACE-HOLDERS> ut-terances were 1.31 words and <QUESTIONS> ut-terances were 6.50 words.
Taking this as a startpoint, we grouped utterances into those of length1 (i.e.
short, or one word utterances), those withlengths 2?4 (we call medium length utterances),and those of length 5+ (the long length model, thatcomprises everything else), and produced separatecue-based models for each group.Second, we introduced <start> and <finish>tags to each utterance (independent of the calcula-tion of utterance length), to capture position spe-cific information for particular cues.
For exam-ple ?<start> okay?
identifies the occurrence ofthe word ?okay?
as the first word in the utterance.Finally, in the Switchboard annotation, there areother markers dealing with various linguistic is-sues, as outlined in Meteer (1995).
A primary ex-ample is the label <+>, which indicated the pres-ence of overlapping speech.
One approach to bet-ter utilise this data is to ?reconnect?
the dividedutterances, i.e.
appending any utterance assignedtag <+> to the last utterance by the same speaker.We base the selection of these model elaborationsand the values for the parameters of frequency andpredictivity on prior research (cf.
(Webb et al,2005a; Webb et al, 2005b; Webb et al, 2005c)).5 Cue-Based Classification ResultsUltimately, we want to compare classificationperformance of a set of automatically extractedcue phrases across the two corpora, Switchboardand ICSI-MRDA.
Both are annotated with sim-ilar variants of the DAMSL annotation scheme,1313Condition Cue Source Cue Count Accuracy(1) Switchboard training data 136,942 80.72%(2) ICSI-MRDA training data 48,856 70.78%(3) Intersection of Switchboard and ICSI-MRDA Training Data 25,053 72.34%(4) As above, discard <STATEMENT> cue phrases 577 72.62%(5) As above, retain only cue phrases containing <start> tags 242 72.52%(6) As above, retain only cue phrases appearing in every training intersection 148 72.09%Table 1: Switchboard Classification Resultsbut there are differences.
For example, theICSI-MRDA corpus introduces several new la-bels that do not exist in the Switchboard annota-tion.
Some labels in the Switchboard annotationare clustered into a single corresponding label inthe ICSI-MRDA corpus, such as the two labelsfrom Switchboard, <STATEMENT-OPINION> and<STATEMENT-NON-OPINION>, which are repre-sented by a single label <STATEMENT> in theICSI-MRDA corpus.
To facilitate cross-corpusclassification, we will cluster these labels as de-scribed in Shriberg et al (2004).
Of course, anyclustering of labels has an impact on classifier per-formance, usually resulting in an increase.
Webbet al (2005c) indicate that clustering statement la-bels in the Switchboard corpus should improveperformance by 8-10% percentage points.5.1 Baseline ResultsWe need to establish baseline classification per-formance for both corpora.
Our baseline for thisclassification task is to the most frequently oc-curring label for all utterances.
For a numberof dialogue corpora, the most frequently occur-ring label is some sort of statement or asser-tion, which is true for both the Switchboard andICSI-MRDA corpora, where <STATEMENT> isthe most frequent label.
For the Switchboardcorpus, selecting this label results in 51.05% ac-curacy.
Remember that we are working witha version of the Switchboard corpus where wehave clustered the original labels <STATEMENT-OPINION> and <STATEMENT-NON-OPINION>into a single label.
In the original Switchboardannotation, the most frequently occurring la-bel is <STATEMENT-NON-OPINION>, which oc-curs 36% of the time.
Further analysis on theSwitchboard corpus by Webb et al (2005c) high-lights that a significant number of <STATEMENT-OPINION> utterances in Switchboard are mis-labelled as <STATEMENT-NON-OPINION> byhuman annotators.
For the ICSI-MRDA corpus,an accuracy of 31.77% is achieved by labellingeach utterance as <STATEMENT>.Now we have established a simple baseline ofperformance, we want to know how well our cue-based classification method works applied to thesecorpora, as an evaluation of how well our cue ex-traction method works for each of these corpora.We ran a 10-fold stratified cross-validation ex-ercise (referred to as Condition (1) in Tables 1and 2) using the cue-based extraction mecha-nism described in Section 3, selecting cue phrasesfrom the training data (which averaged 180k ut-terances for Switchboard, and 95k utterances forICSI-MRDA), resulting in an average of 135k cuephrases from Switchboard and 50k cue phrasesfrom ICSI-MRDA.
We then applied these cue-based models to the held out test data as describedin Section 4, applying Switchboard extracted cuephrases to Switchboard test data, and likewisewith the ICSI-MRDA data.
This establishes thebest performance by our algorithm over these datasets.
For Switchboard, we achieve 80.72% accu-racy, as predicted by the work reported in Webb etal.
(2005c).
For ICSI-MRDA we obtain an accu-racy of 58.14%.
Remember, this model is appliedto the 55-label annotated ICSI-MRDA corpus.Best reported classification accuracy for this cor-pus is the 66% reported by Ji and Bilmes (2005),using a graph-based model that models both ut-terances and sequences of DA labels.
For bothcorpora, the cue-based model of classification out-performs the baseline, using no dialogue contextwhatsoever.1314Condition Cue Source Cue Count Accuracy(1) ICSI-MRDA training data 48,856 58.14%(2) Switchboard training data 136,942 47.07%(3) Intersection of Switchboard and ICSI-MRDA Training Data 25,053 47.86%(4) As above, discard <STATEMENT> cue phrases 577 48.05%(5) As above, retain only cue phrases containing <start> tags 242 47.30%(6) As above, retain only cue phrases appearing in every training intersection 148 46.34%Table 2: ICSI-MRDA Classification Results5.2 Cross-Corpus ResultsThe focus of our effort is not to maximise rawperformance over individual corpora, but to ex-amine the effectiveness of our automatically ex-tracted cue phrases, and one mechanism to do thisis to compare classification cross-corpora.
If ourcue phrases are sufficiently general predictors ofDA labels across corpora, we believe that to be apowerful claim for cue phrases as a DA classifica-tion feature.
Therefore, our next step was to takethe cue-phrases generated from each fold of theSwitchboard experiment, and apply them to theheld out test data from the corresponding fold ofthe ICSI-MRDA experiment, and vice-versa.
Thisis a test to see how generally applicable are the cuephrases extracted from each corpus.When we take cues extracted from the Switch-board corpus, and apply them to the held out por-tion of the ICSI-MRDA corpus, we achieve an av-erage classification accuracy (over our 10-folds)of 47.07%.
This score represents 81% of the ac-curacy achieved by our prior result when ICSI-MRDA test data is classified using ICSI-MRDAtraining data.
It also represents 71% of the bestpublished score on this corpus (Ji and Bilmes,2005).
When we classify held out Switchboardtest data with cue phrases extracted from the ICSI-MRDA corpus, we achieve an average classifica-tion accuracy of 70.78%, which corresponds to88% of our best score on this corpus using Switch-board training data.
These results correspond toCondition (2) in Tables 1 and 2.These are very positive results for both di-rections of classification, indicating that the cuephrases we automatically extract from our corporaare generally applicable as a feature for DA clas-sification.5.3 Cue Phrase ReductionWe have successfully shown that we can use cuephrases extracted from one corpus to classify ut-terances from a different corpus.
We used an in-clusive approach, using all cue phrases extractedfrom the source corpus training data.
Intuitivelyhowever, we might expect to get comparable per-formance by using only those cue phrases that ap-pear in both corpora.
For these intersection cuephrases, we require a strict overlap.
Once the cuesphrases are extracted from each individual train-ing fold for each corpus, they are compared andretained if and only if:?
the cue phrase itself is a direct match, includ-ing any position specific label?
the DA the phrases predicts is a match?
the model number (as defined in Section 4) isa matchFor each fold of our cross-validation, we takecues phrases extracted from the training data thatappear in both corpora, pruning out cue phrasesthat only appear in one of the corpora.
We thenretain only those cue phrases that meet these cri-teria from both corpora for each specific fold, andapply them to the held out test data from that foldfor each corpus.Average classification performance for bothcorpora rises very slightly in comparison to us-ing all extracted cue phrases.
These results can beseen as Condition (3) in Tables 1 and 2.
When ap-plying the intersection cue phrases to the Switch-board test data, we achieve an average classifica-tion accuracy score of 72.34%.
When we applythe intersection cues to the ICSI-MRDA test data,the average score is 47.86%.
The average num-ber of cue phrases that are used in this experiment1315(i.e that appear in all training folds for both cor-pora, with matching model information) is around25k.
This represents 50% of the average num-ber of cues extracted from the ICSI-MRDA cor-pus, and only 19% of the average number of cuephrases extracted from the Switchboard corpus.We describe earlier that our default label as ap-plied by our classifier when no cue phrase can befound is the <STATEMENT> label, the most fre-quent single label in both corpora.
Given this,we can safely remove cue phrases that predict<STATEMENT> labels from our cue phrase set.The absence of such cue phrases should haveno impact on our classification performance, butshould reduce our total number of cue phrases.As can be seen in Condition (4) in Tables 1 and2, this is indeed the case, with no statistical sig-nificance between the results with and without<STATEMENT> cue phrases.
However, there isa drop in the number of cue phrases.
When weremove all <STATEMENT> cue phrases from theintersection of cue phrases, we are left with an av-erage of 577 cue phrases.Further analysis of classifier performance indi-cates that a high percentage of actual labelling isperformed using a subset of even the cue phrasesextracted under Condition (4).
We observed thatcue phrases that contain a <start> tag (as de-scribed in Section 4) were used in the majority ofcases.
Our final experiment was to extract, fromthe 577 cue phrases, only those phrases that con-tain the <start> tag.
This reduced the averagenumber of cue phrases to 242.
Classification per-formance remains unaffected, scoring an averageof 72.52% for Switchboard and 47.30% for ICSI-MRDA, as seen in Condition (5) in the results ta-bles.
We note that of those 242 phrases, 148 ap-pear in the intersection of every training fold ofour 10-fold cross-validation.
When we use onlythose 148 cue phrases for classification, as seenin Condition (6), average classification accuracyremains the same; 72.09% for Switchboard, and46.34% for ICSI-MRDA.6 ConclusionsIn this paper, we investigate a cue-based ap-proach to DA classification, applied to two cor-pora, Switchboard and ICSI-MRDA.
We automat-ically extracted cue phrases from both corpora,and used them directly to classify unseen utter-ances from the corresponding corpus, demonstrat-ing that our automatically discovered cue phrasesare a sufficiently useful feature for this task.We then explored the generality of our cuephrases, by applying them directly as a classifierto data from the alternate corpus.
Whilst therewas some expected drop in performance, the clas-sification accuracy for both experiments is good,given such a small number of features and the sim-ple design of the classifier.
The result indicatesthat cue phrases are a highly useful feature forDA classification, and can be used to classify datafrom new corpora, possibly as some part of somequasi-automatic first annotation effort.We experimented with reducing the set of cuephrases, using increasingly restrictive measuresof retaining our automatically discovered cues.We found that we did not have a drop in per-formance compared to the cross-corpus classifica-tion accuracy, even when the cue set is drasticallyreduced (to 0.001% of the original Switchboardcue phrases, and 0.003% of the ICSI-MRDA cuephrases).
This appears to be a strong indicator ofthe discriminative power of some small number ofautomatically discovered core cue phrases.ReferencesAng, J., Y. Liu, and E. Shriberg.
2005.
Automatic Di-alog Act Segmentation and Classification in Multi-party Meetings.
In Proceedings of the InternationalConference on Acoustics, Speech, and Signal Pro-cessing, pages 1061?1064, Philadelphia.Bunt, H. 1994.
Context and Dialogue Control.THINK, 3:19?31.Core, M., M. Ishizaki, J. Moore, and C. Nakatani.1999.
The Report of the Third Workshop of the Dis-course Resource Initiative.
Chiba University andKazusa Academia Hall.Grosz, B. and C. Sidner.
1986.
Attention, Intentions,and the Structure of Discourse.
Computational Lin-guistics, 19(3).Hirschberg, J. and D. Litman.
1993.
Empirical Stud-ies on the Disambiguation of Cue Phrases.
Compu-tational Linguistics, 19(3):501?530.Ji, G. and J. Bilmes.
2005.
Dialog Act Tagging Us-ing Graphical Models.
In Proceedings of the IEEE1316International Conference on Acoustics, Speech, andSignal Processing, Philadelphia, PA.Ji, G. and J. Bilmes.
2006.
Backoff ModelTraining using Partially Observed Data: Applica-tion to Dialog Act Tagging.
In Proceedings ofthe Human Language Technology/ American chap-ter of the Association for Computational Linguis-tics(HLT/NAACL?06).Jurafsky, D., R. Bates, N. Coccaro, R. Martin,M.
Meteer, K. Ries, E. Shriberg, A. Stolcke, P. Tay-lor, and C. Van Ess-Dykema.
1997.
Automatic De-tection of Discourse Structure for Speech Recogni-tion and Understanding.
In Proceedings of the 1997IEEE Workshop on Speech Recognition and Under-standing, Santa Barbara.Lendvai, P. and J. Geertzen.
2007.
Token-BasedChunking of Turn-Internal Dialogue Act Sequences.In Proceedings of the 8th SIGDIAL Workshop onDiscourse and Dialogue, pages 174?181, Antwerp,Belgium.Levin, L., C. Langley, A. Lavie, D. Gates, and D. Wal-lace.
2003.
Domain Specific Speech Acts for Spo-ken Language Translation.
In Proceedings of 4thSIGdial Workshop on Discourse and Dialogue.Meteer, M. 1995.
Dysfluency Annotation Stylebookfor the Switchboard Corpus.
Working paper, Lin-guistic Data Consortium.Reithinger, N. and M. Klesen.
1997.
Dialogue ActClassification Using Language Models.
In Proceed-ings of EuroSpeech-97.Rotaru, M. 2002.
Dialog Act Tagging using Memory-Based Learning.
Term project, University of Pitts-burgh.Samuel, K., S. Carberry, and K. Vijay-Shanker.
1999.Automatically Selecting Useful Phrases for Dia-logue Act Tagging.
In Proceedings of the FourthConference of the Pacific Association for Computa-tional Linguistics, Waterloo, Ontario, Canada.Shriberg, E., R. Dhillon, S. Bhagat, J. Ang, and H. Car-vey.
2004.
The ICSI Meeting Recorder Dialog Act(MRDA) Corpus.
In Special Interest Group on Dis-course and Dialogue (SIGdial), Boston, USA.Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer.
2000.
Dialogue ActModeling for Automatic Tagging and Recognitionof Conversational Speech.
In Computational Lin-guistics 26(3), 339?373.Verbree, D., R. Rienks, and D. Heylen.
2006.
Dia-logue Act Tagging using Smart Feature Selection;Results on Multiple Corpora.
Spoken LanguageTechnology Workshop, 2006.
IEEE, pages 70?73.Webb, N. and T. Liu.
2008.
Investigating the Porta-bility of Corpus-Derived Cue Phrases for DialogueAct Classification.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (COLING 2008), Manchester, United Kingdom.Webb, N., M. Hepple, and Y. Wilks.
2005a.
Dia-logue Act Classification Based on Intra-UtteranceFeatures.
In Proceedings of the AAAI Workshopon Spoken Language Understanding, at the Twen-tieth National Conference on Artificial Intelligence,Pittsburgh, PA.Webb, N., M. Hepple, and Y. Wilks.
2005b.
Empir-ical Determination of Thresholds for Optimal Di-alogue Act Classification.
In Proceedings of theNinth Workshop on the Semantics and Pragmaticsof Dialogue.Webb, N., M. Hepple, and Y. Wilks.
2005c.
Er-ror Analysis of Dialogue Act Classification.
InProceedings of the 8th International Conference onText, Speech and Dialogue, Carlsbad, Czech Repub-lic.1317
