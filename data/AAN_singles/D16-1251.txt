Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2295?2299,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsMeasuring the behavioral impact of machine translation qualityimprovements with A/B testingBenjamin Russell and Duncan GillespieEtsy{brussell, dgillespie}@etsy.comAbstractIn this paper we discuss a process for quan-tifying the behavioral impact of a domain-customized machine translation system de-ployed on a large-scale e-commerce platform.We discuss several machine translation sys-tems that we trained using aligned text fromproduct listing descriptions written in mul-tiple languages.
We document the qual-ity improvements of these systems as mea-sured through automated quality measures andcrowdsourced human quality assessments.
Wethen measure the effect of these quality im-provements on user behavior using an au-tomated A/B testing framework.
Throughtesting we observed an increase in key e-commerce metrics, including a significant in-crease in purchases.1 IntroductionQuality evaluation is an essential task when train-ing a machine translation (MT) system.
While au-tomatic evaluation methods like BLEU (Papineni etal., 2002) can be useful for estimating translationquality, a higher score is no guarantee of qualityimprovement (Callison-Burch et al, 2006).
Previ-ous studies (e.g.
Coughlin, 2003) have comparedhuman evaluations of MT to metrics like BLEUand found close correspondence between the two.Koehn (2004) argued that relatively small differ-ences in BLEU can indicate significant MT qual-ity differences and suggested that human evaluation,the traditional alternative to automated metrics likeBLEU, is therefore unnecessarily time-consumingand costly.
Callison-Burch (2009) explored the useof crowdsourcing platforms for evaluating MT qual-ity, with good results.
However, we are not awareof any research that investigates the effect of im-proved MT on human behavior.
In a commercialapplication, like an e-commerce platform, it is de-sirable to have a high degree of confidence in thematerial effect of MT quality differences: any MTsystem change should positively impact user experi-ences.Etsy is an online marketplace for handmade andvintage items, with over 40 million active listingsand a community of buyers and sellers locatedaround the world.
Visitors can use MT to translatethe text of product descriptions, product reviews,and private messages, making it possible for mem-bers to communicate effectively with one another,even when they speak different languages.
Thesemultilingual interactions facilitated by MT, such asreading nonnative listing descriptions or conversingwith a foreign seller, are integral to the user experi-ence.However, due to the unique nature of the productsavailable in the marketplace, a generic third partyMT system1 often falls short when translating user-generated content.
One challenging lexical item is?clutch.?
A generic engine, trained on commonlyavailable parallel text, translates clutch as an ?auto-motive clutch.?
In this marketplace, however, clutchalmost always means ?purse.?
A mistake like this isproblematic: a user who sees this incorrect machinetranslation may lose confidence in that listing andpossibly in the marketplace as a whole.1We use Microsoft?s Bing Translator for our machine trans-lations.2295Figure 1: An example review translation on the website.To improve the translation quality for terms likeclutch, we used an interface provided by a thirdparty machine translation service2 to train a cus-tom MT engine for English to French translations.To validate that the retrained MT systems werematerially improved, we used a two step valida-tion process, first using crowd-sourced evaluationswith Amazon?s Mechanical Turk, and secondly us-ing A/B testing, a way of conducting randomizedexperiments on web sites, to measure the effect ofthe trained system on user behavior.2 Data CollectionOur online marketplace contains millions of listingdescriptions posted by tens of thousands of multilin-gual sellers.
We conducted an MT system trainingusing aligned texts from these product listings.
Weused our third-party translation service?s automatedretraining framework to train multiple MT systemsthat were specifically tuned to the marketplace?s cor-pus.
To gather this data, we used a Hadoop job toparse through 130 million active and expired prod-uct listings to find listing descriptions that were writ-ten in both English and French.
Once we foundthese listing descriptions, we tokenized the text onsentence boundary.
We removed any descriptionswhere there was a mismatch in the number of sen-tences between the source and target descriptions.Next, we used a language detection service to en-sure the source and target strings were the correctlanguages (source: English, target: French).
Afterlanguage detection, we removed all sentences where2http://hub.microsofttranslator.comthe ratio of alphabetic characters to total characterswas below 70%.
This 70% threshold was deter-mined through manual assessment of the result set,and was used to eliminate strings with low numbersof alphabetic characters, such as ?25.5 in x 35.5 in?.After these preliminary filtering steps, our train-ing set consisted of 885,732 aligned sentences.To supplement the aligned text, we also collected2,625,162 monolingual French sentences for thetraining.
The monolingual text was parsed andcleaned in the same manner as the aligned sentences.The commercial MT system?s automatic trainingframework provides tools for the upload of bilingualand monolingual training data, tuning data, and test-ing data for customization of the underlying statis-tical MT system.
Bilingual training data is used tomodify the base translation model; monolingual datacustomizes the language model; the system is opti-mized for the tuning data; and the testing data is usedto calculate a BLEU score.
We trained over a dozensystems with a variety of datasets and selected thethree systems that had the highest BLEU scores.System 1 was trained using the aligned sentences,along with the 2.6 million monolingual sentences.The system was tuned using 2,500 sentences auto-matically separated from the training sentences bythe third party?s training system, and used an ad-ditional 2,500 automatically separated sentences fortesting.For System 2, we used a variation of the Gale-Church alignment algorithm (1993) to remove sen-tences predicted to be misaligned based on theirlength differences.
The subject of sentence align-ment in parallel texts has been researched exten-2296Training Data Sys.
1 Sys.
2 Sys.
3886K aligned sentences x766K aligned sentencesafter Gale-Church applied x x2.6M monolingualsegments x x xAuto tuning* x xTuning with 2Kin-domain sentences xTable 1: Data sets used for three MT system retrainings.
*Thethird party?s training platform automatically sets aside data touse for the parameter tuning.sively (e.g.
Brown et al, 1991; Gale and Church,1993).
Although more sophisticated methods ex-ist (e.g.
Chen, 1993; Wu, 1994; Melamed, 1996;Munteanu and Marcu, 2005), we used Gale-Churchdue to its relatively high accuracy and low im-plementation overhead.
Misalignment between thesame listing descriptions written in multiple lan-guages could be caused by several factors, the mostcommon problem being that sellers do not trans-late descriptions sentence for sentence from one lan-guage to the next.
We detected possible misalign-ments in 13.5% of the original 886K aligned sen-tences, leaving 776K sentences to use for trainingSystem 2.
We used auto-tuning and auto-testing forthis engine, as we did for System 1.System 3 was trained using the same training dataas the second engine, but was tuned using 2,000professionally-translated sentences taken from list-ing descriptions.
Two hundred of these sentenceswere drawn semi-randomly to represent a generalsample of listing description text; the remaining1,800 contained terms, like ?clutch,?
that were be-ing mistranslated by the generic system.
This sys-tem used the same automatically-generated testingdata as the other two to calculate a BLEU score.
Ta-ble 1 shows the training and tuning data used for thethree systems.3 Crowdsourced EvaluationFor evaluation of the trained translation systems, wegenerated translations of sentences drawn randomlyfrom our monolingual English corpus (product list-ings that sellers had not translated into languagesother than English).
We excluded segments thatwere translated the same by both the trained andBLEU ScoreBLEU ScoreImprovement OverGeneric SystemSystem 1 48.16 +9.82System 2 50.36 +12.02System 3 46.85 +8.51Table 2: BLEU score improvements for three translation sys-tems over a baseline BLEU for the generic system of 38.34.generic systems.
(For System 1, 48 of 2,000 testsentences had the same translation as the genericsystem, for System 2 that number was 42, and forSystem 3 that number was 148.
)To obtain judgments about the quality of thesetranslations, we used Mechanical Turk to obtain hu-man evaluations of our candidate translation sys-tems (Callison-Burch, 2009).
To recruit Mechan-ical Turk workers with bilingual competence, werequired workers to achieve at least 80% accuracyin a binary translation judgment task (workers wereasked to judge whether each of 20 translations was?Good?
or ?Bad?
; their answers were compared withthose of professional translators).Qualified workers completed a survey indicatingtheir preference for the translation of a particulartrained system compared to the generic commercialtranslation system.
Translation pairs were presentedin random order with no indication of whether atranslation was produced by a human, a generictranslation system, or an untrained translation sys-tem.
Workers were asked to choose the better ofthe two translations or to indicate, ?Neither is bet-ter?.
Workers were offered $2.00 to complete a 50-question survey.
Each survey contained five hid-den questions with known answers (translation pairsjudged by professional translators) for quality con-trol (we excluded responses from workers who didnot answer the hidden questions with at least 80%accuracy).4 Results4.1 BLEU EvaluationWe used the automated BLEU calculation providedby the third-party translation service to obtain scoresfor each of the three translation systems.All three systems had significant BLEU improve-ments after retraining, as shown in Table 2.
We be-2297Trained Generic Neither RatioSys.
1 129 (34%) 109 (29%) 138 (36%) 1.18Sys.
2 71 (25%) 85 (31%) 123 (44%) 0.84Sys.
3 203 (36%) 150 (27%) 205 (37%) 1.35Table 3: Results from crowdsourced evaluations of three trans-lation systems.
Columns labeled Trained, Generic, and Nei-ther include the number of responses and percentage of totalresponses for each response type.
The Ratio column shows thenumber of responses that favored the trained system to the num-ber of responses that favored the generic system.lieve System 3 has a lower BLEU score than the oth-ers because it was tuned on a different data set: theprofessionally-translated, in-domain sentences fromproduct listing descriptions.
This made the system?soutput less like the automatically-selected test setthan the others, but closer, presumably, to the high-quality, low-noise tuning translations sourced fromprofessional translators.4.2 Crowdsourced evaluationThe crowdsourced evaluation of the three systemsfavored System 3.
Table 3 provides a summary ofthe results.
Neither System 1 nor System 2 showeda significant difference between selection of transla-tions provided by the trained or untrained system:chi-squared tests did not detect a significant dif-ference between number of responses favoring thetrained system and number of responses favoring thegeneric system (p = 0.1948 and p = 0.26, respec-tively, for the two systems).
However, a chi-squaredtest indicated a significant preference for System 3,which was chosen 35% more often than the genericsystem (p = 0.0048).
Based on the crowd-sourcedresults, we proceeded to A/B test System 3 againstthe generic translation system baseline.The lack of improvements for System 1 and Sys-tem 2 detected using the crowd-sourcing methodswas somewhat surprising, given the large BLEUscore improvements observed for all three systems.We believe this lends further support to Callison-Burch, et al?s (2006) critiques of BLEU as a stand-alone machine translation quality metric.
In thiscase, it is possible that Systems 1 and 2 achievedhigh BLEU improvements due to over-fitting thetraining data from which the test set was drawn.
Wemight speculate that this is due to the presence oflow-quality translations from limited-bilingual sell-ers, or the presence of MT generated by a differentonline tool in some sellers?
translations.
By tun-ing the system using a high-quality, professionally-translated test set, we reduced overall BLEU but in-creased quality as judged by bilingual evaluators.4.3 A/B testingA/B testing is a strategy for comparing two differ-ent versions of a website to see which one performsbetter.
Traditionally, one of these experiences is theexisting, A, control experience, and the other expe-rience is a new, B, variant experience.
By randomlygrouping users into one of the two experiences, andmeasuring the on-site behavior (e.g., clicks on alisting or items purchased) of each group, we canmake data-driven decisions about whether new ex-periences are actually an improvement for our users.For our use case, the control experience is showingusers content machine translated with the genericengine, and the variant experience is showing con-tent translated with the retrained engine.
A/B test-ing allows us to answer the following question: willusers who read a product description translated bya domain-customized translation engine be more orless likely to purchase a product?To test the effects of the quality improvement ob-tained, we used our in-house automated A/B test-ing framework to compare the behavioral effects onusers who translated text using the generic engineand those who translated using System 3.
Visitors tothe online marketplace were randomly ?bucketed?into an experimental group or a control group.
Ran-dom bucketing was achieved via a hash of a user?sbrowser ID, which allows users who return to thesite during the experimental period to be bucketedconsistently across visits.
For visitors who requestedtranslations from English into French, the genericsystem?s translations were displayed to visitors inthe control group, and System 3 translations weredisplayed to visitors in the experimental group.The experiment ran for 66 days for a total of88,106 visitors (43,306 control and 44,800 experi-mental).
The key metrics tracked were pages pervisit (the number of pages seen in one user session),conversion rate (the percent of visits that include atleast one purchase), and add-to-cart rate (the per-cent of visits in which a user adds an item to theirshopping cart).
We observed a significant positive2298Metric Trained engineConversion rate +8.72%Visit add-to-cart rate +2.92%Pages per visit +3.37%Table 4: The trained translation system?s (System 3) improve-ment over the generic engine on key business metrics.
All dif-ferences are statistically significant (p < 0.05).
Base rates areomitted for data privacy reasons.effect of the trained system on all three metrics, asshown in Table 4: a 3.37% increase in pages pervisit (p = 0.00153 95% CI [1.29, 5.46]), an 8.72%increase in purchase rate (p = 0.00513 95% CI[2.61, 14.82]), and a 2.92% increase in add-to-cart(p = 0.04689 95% CI [0.04, 5.8]).5 ConclusionNumerous studies have shown that automatic ma-chine translation quality estimates, such as BLEU,are correlated with human evaluations of translationquality.
Our work shows that those improvementsin translation quality can have a positive effect onuser behavior in a commercial setting, as measuredthrough conversion rate.
These considerations sug-gest that, in domains where machine translation con-veys information upon which individuals base deci-sions, the effort needed to gather and process datato customize a machine translation system can beworthwhile.
Additionally, our experiments showA/B testing can be a valuable tool to evaluate ma-chine translation quality.
A/B testing goes beyondmeasuring the quality of translation improvements:it allows us to see the positive impact that quality im-provements are having on users?
purchase behaviorin a measurable way.ReferencesPeter F. Brown, Jennifer C. Lai, and Robert L Mercer.1991.
Aligning sentences in parallel corpora.
In Pro-ceedings of the 29th Annual Meeting on Associationfor Computational Linguistics, pages 169?176.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of BLEU in ma-chine translation research.
In Proceedings of EACL,volume 6, pages 249?256.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: evaluating translation quality using Amazon?sMechanical Turk.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, volume 1, pages 286?295.Stanley F. Chen.
1993.
Aligning sentences in bilingualcorpora using lexical information.
In Proceedings ofthe 31st Annual Meeting on Association for Computa-tional Linguistics, pages 9?16.Deborah Coughlin.
2003.
Correlating automated andhuman assessments of machine translation quality.
InProceedings of MT Summit IX, pages 63?70.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 388?395.I Dan Melamed.
1996.
A geometric approach to map-ping bitext correspondence.
In Proceedings of theFirst Conference on Empirical Methods in NaturalLanguage Processing.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 311?318.Dekai Wu.
1994.
Aligning a parallel English-Chinesecorpus statistically with lexical criteria.
In Proceed-ings of the 32nd Annual Meeting on Association forComputational Linguistics, pages 80?87.2299
