Integrating a Large-scale, Reusable Lexicon with a NaturalLanguage GeneratorHongyan 3 ingDepartment of Computer  ScienceColumbia UniversityNew York, NY 10027, USAhjing@cs.columbia.eduYael Dahan NetzerDepartment of Computer  ScienceBen-Gurion UniversityBe'er-Sheva, 84105, Israelyaeln@cs.bgu.ac.ilMichael ElhadadDepartment  of Computer  ScienceBen-Gurion UniversityBe'er-Sheva, 84105, Israelelhadad@cs.bgu.ac.i lKathleen R. McKeownDepartment of Computer  ScienceColumbia UniversityNew York, NY 10027, USAkathy@cs.columbia.eduAbst rac tThis paper presents the integration of a large-scale, reusable lexicon for generation with theFUF/SURGE unification-based syntactic realizer.The lexicon was combined from multiple xisting re-sources in a semi-automatic process.
The integra-tion is a multi-step unification process.
This inte-gration allows the reuse of lexical, syntactic, andsemantic knowledge ncoded in the lexicon in thedevelopment of lexical chooser module in a genera-tion system.
The lexicon also brings other benefitsto a generation system: for example, the ability togenerate many lexical and syntactic paraphrases andthe ability to avoid non-grammatical output.1 In t roduct ionNatural language generation requires lexical, syn-tactic, and semantic knowledge in order to producemeaningful and fluent output.
Such knowledge isoften hand-coded anew when a different applicationis developed.
We present in this paper the integra-tion of a large-scale, reusable lexicon with a naturallanguage generator, FUF/SURGE (Elhadad, 1992;Robin, 1994); we show that by integrating the lexi-con with FUF/SURGE as a tactical component, wecan reuse the knowledge ncoded in the lexicon andautomate to some extent he development of the lex-ical realization component in a generation applica-tion.The integration of the lexicon with FUF/SURGEalso brings other benefits to generation, includingthe possibility to accept a semantic input at thelevel of WordNet synsets, the production of lexicaland syntactic paraphrases, the prevention of non-grammatical output, reuse across applications, andwide coverage.We present he process of integrating the lexiconwith FUF/SUR(;E. including how to represenl thelexicon in FUF format, how to unify input with thelexicon incrementally to generate more sophisticatedand informative representations, and how to designan appropriate semantic input format so that theintegration of the lexicon and FUF/SURGE can bedone easily.This paper is organized as follows.
In Section 2,we explain why a reusable lexical chooser for gen-eration needs to be developed.
In Section 3, wepresent he large-scale, reusable lexicon which wecombined from multiple resources, and illustrate itsbenefits to generation by examples.
In Section 4, wedescribe the process of integrating the lexicon withFUF/SURGE, which includes four unification steps,with each step adding additional lexical or syntac-tic information.
Other applications and comparisonwith related work are presented inSection 5.
Finally,we conclude by discussing future work.2 Bu i ld ing  a reusab le  lex ica l  chooserfor generat ionWhile reusable components have been widely used ingeneration applications, the concept of a "reusablelexical chooser" for generation remains novel.There are two main reasons why such a lexicalchooser has not been developed in the past:1.
In the overall architecture of a generator, thelexical chooser is an internal component thatdepends on the semantic representation a d for-.
:malism and onthe syntactic realizer used by theapplication.2.
The lexical chooser links conceptual e ements tolexical items.
Conceptual elements are by defi-nition domain and application dependent ( heyare the primitive concepts used in an applica-tion knowledge base).
These primitives are noteasily ported from application to application.209The emergence of standard architectures for gen-erators (RAGS, (Reiter, 1994))and the possibilityto use a standard syntactic realizer answer the firstissue.To address the second issue, one must realize thatif the whole lexical chooser can not be made domain-independent, major parts can be made reusable.The main argument is that lexical knowledge is mod-ular.
Therefore, while choice of words is constrainedby domain-specific conceptual knowledge (what in-formation the sentences are to represent) on the onehand, it is also affected by several other dimensions:* inter-lexical constraints: collocations amongwordso pragmatic onstraints: connotations of wordso stylistic constraints: familiarity of words* syntactic constraints: government patterns ofwords, e.g., thematic structure of verbs.We show in this paper how the separation of thesyntactic and conceptual interfaces of lexical itemdefinitions allows us to reuse a large amount of lex-ical knowledge across appli.cations.3 The  lex icon  and  i t s  benef i t s  togenerat ion3.1  A large-scale,  reusab le  lexicon forgenerat ionNatural Language generation starts from semanticconcepts and then finds words to realize such seman-tic concepts.
Most existing lexical resources, how-ever, are indexed by words rather than by semanticconcepts.
Such resources, therefore, can not be usedfor generation directly.
Moreover, generation eedsdifferent ypes of knowledge, which typically are en-coded in different resources.
However, the differentrepresentation formats used by these resources makeit impossible to use them simultaneously in a singlesystem.To overcome these limitations, we built a large-scale, reusable lexicon for generation by combiningmultiple existing resources.
The resources that arecombined include:o Tile WordNet Lexical Database (Miller et al,1990).
WordNet is the largest lexical databaseto date, consisting of over 120,000 unique words(version 1.6).
It also encodes many types oflexical relations between words, including syn-onytny, antonymy, and many more.o English Verb Classes and Alternations(EVCA) (Levin, 1993).
It categorized 3.104verbs into classes based on their syntacticproperties and studied verb alternations.
Analternation is a variation in the realization ofverb arguments.
For example, the alternation"there-insertion" transforms A ship appeared~-on..the horizon_to There,appeared a ship..o~....thehorizon.
A total of 80 alternations for 3,104verbs were studied.The COMLEX syntax dictionary (Grishman etal., 1994).
COMLEX contains syntactic infor-mation for over 38,000 English words.The Brown Corpus tagged with WordNet senses(Miller et al, 1993).
We use this corpus forfrequency measurement.
.In combining these resources, we focused on verbs,since they play a more important role in decidingsentence structures.
The combined lexicon includesrich lexical and syntactic knowledge for 5,676 verbs.It is indexed by WordNet synsets(which are at thesemantic oncept level) as required by the generationtask.
The knowledge in the lexicon includes:Q A complete list of subcategorizations for eachsense of a verb.o A large variety of alternations for each sense ofa verb.o Frequency of lexical items and verb subcatego-rizations in the tagged Brown corpusRich lexicat relations between wordsThe sample entry for the verb "appear" is shownin Figure 1.
It shows that the verb appear has eightsenses (the sense distinctions come from WordNet).For each sense, the lexicon lists all the applicablesubcategorization for that particular sense of theverb.
The subcategorizations are represented usingthe same format as in COMLEX.
For each sense,the lexicon also lists applicable alternations, whichwe encoded based on the information in EVCA.
Inaddition, for each subcategorization a d alternation,the lexicon lists the semantic ategory constraints onverb arguments.
In the figure, we omitted the fre-quency information derived from Brown Corpus andlexical relations (the lexical relations are encoded inWordNet).The construction of the lexicon is semi-automatic.First, COMLEX and EVCA were merged, produc-ing a list of syntactic subcategorizations and alter-nations for each verb.
Distinctions in these syntac-tic restrictions according to each sense of a verbare achieved in the second stage, where WordNetis merged with the result of the first step.
Finally,the corpus information is added, complementing thestatic resources with actual usage counts for eachsyntactic pattern.
For a detailed description of thecombination process, refer to (Jing and Mchieown,1998).210appear:sense  1 give an impress ion((PP-TO-INF-gS :PVAL ("to") :SO ((sb,  - ) ) )(TO-INF-RS :S0 ((sb, --)))(NP-PRED-RS :S0 ((sb,  --)))(ADJP-PRED-RS :SO ((sb,  - )  (sth, - - ) ) ) ) )sense 2 become v is ib le((PP-T0-INF-KS :PVAL ("to"):S0 ((sb, -) (sth, -)))(INTRANS TIIERE-V-SUB J. .
.
.
.
.
.
.
.
.
.
_: ALT there-insertion:S0 ((sb, --) (sth, --))))sense 8 have an outward express ion((NP-PRED-RS :SO ((sth, --)))(ADJP-PRED-RS :S0 ((sb, --) (sth, --))))Figure I: Lexicon entry for the verb appear3.2 The  benefits of  the  lex iconThere are a number of benefits that this combinedlexicon can bring to language generation.First, the use of synsets as semantic tags canhelp map an application conceptual model to lexi-cal items.
Whenever application concepts are repre-sented at the abstraction level of a WordNet synset,they can be directly accepted as input to the lexi-con.
By this way, the lexicon can actually lead tothe generation of many lexical paraphrases.
For ex-ample, (look, seem, appear} is a WordNet synset; itincludes a list of words that can convey the seman-tic concept ' 'g ive  an impression o f '  '.
We canuse synsets to find words that can lexicalize the se-mantic concepts in the semantic input.
By choosingdifferent words in a synset, we can therefore gen-erate lexical paraphrases.
For instance, using theabove synset, the system can generate the followingparaphrases:"He seems happy.
""He looks happy.
""He appears happy.
'"Secondly, the subcategorization information i  thelexicon prevents generating a non-grammatical out-put.
As shown in Figure 1, the lexicon lists appli-cable subcategorizations for each sense of a verb.
Itwill not allow the generation of sentences like"*He convinced me in his innocence"(wrong preposition)"*He convinced to go to the party"(missing object)"*Th.e bread cuts"(missing adverb (e.g., "'easily" ))"*The book consists three parts"( m issing t)reposit.ion)In addition, alternation information can help gen-erate .syntactic paraphrases.
For instance, usingthe "simple reciprocal intransitive" alternation, thesystem can generate the following syntactic para-phrases: ?
,"Brenda agreed with Molly.
""Brenda and Molly agreed?
""Brenda and Molly agreed with each other.
"Finally, the corpus frequency information can help............... _the.lexicat.. -~ice.proeesa~.,When:multiple .words canbe used to realize a semantic oncept, the systemcan use corpus frequency information in additionto other constraints to choose the most appropriateword.The knowledge ncoded in the lexicon is general,thus it can be used in different applications.
Thelexicon has wide coverage: the final lexicon consistsof 5,676 verbs in total, over 14,100 senses (on average2.5 senses/verb), and over 11,000 semantic oncepts(synsets).
It uses 147 patterns to represent the sub-categorizations and includes 80 alternations.To exploit the lexicon's many benefits, its formatmust be made compatible with the architecture of agenerator.
We have integrated the lexicon with theFUF/SURGE syntactic realizer to form a combinedlexico-grammar.4 Integration ProcessIn this section, we first explain how lexical choosersare interfaced with FUF/SURGE.
We then describestep by step how the lexicon is integrated withFUF/SURGE and show that this integration pro-cess helps to automate the development of a lexicalrealization component.4.1 FUF /SURGE and the lexical chooserFUF (Elhadad, 1992) uses a functional unificationformalism for generation.
It unifies the input that auser provides with a grammar to generate sentences.SURGE (Elhadad and Robin, 1996) is a comprehen-sive English Grammar written in FUF.
Tile role ofa lexical realization component is to map a semanticrepresentation drawn from the application domainto an input format acceptable by SURGE, addingnecessary lexical and syntactic information duringthis process.Figure 2 shows a sample semantic input (a), thelexicalization module that is used to map this se-mantic input to SURGE input (b), and 'thefinalSURGE input (c) - -  taken from a real applicationsystem(Passoneau et al, 1996).
The functions of thelexicalization module include selecting words thatcan be used to realize the semalltic oncepts in theinput, adding syntactic features, and mapping tilearguments in tile semantic input to the thematicroles in SURGE.211Sentence :  / t  has 24 activities, including 20 tasks and four decisions.conceptargstotal-node-counttheme conceptrefconceptrheme argspronounPr?cess-fl?wgraph \]elaborationconcepttheme argsexpansion conceptargscardinality \]\[ theme \[1\] \] /t value \[21 l -I. s.ubset-node-countJconcept flownode \]\[1\] = ref fullconceptprocparticcatprocpartic\[2\] =concept cardinal \]cardinal 24ref full(a) The semantic input (i.e., input of lexicalization module)#(under  total-node-count)type possessive \]possessor cat pronoun /icat commoncardinal \[ valuedefinite noheadpossessedqualifier\[,l\]lex "activity" \]cat clausemood present-participletype locativeproc lex "include"partic location \[ catk(b) Tile lexicalization module\]clausetype possessive \]possessor cat pronoun /Icat  COn l l l l oncardinal \[ value 24 \]definite nohead lex "activhy" \]possessed cat clausemood present-participletype locative \]qualifier proc lex "include"(c) Tile SURGE input (ie., output of lexicalization module)1IIIIIFigure 2: A samph~ lexicalization component212The development of the lexicalizer component wasdone by hand in the past.
Furthermore, for.
eachnew application, a new lexicatizer component hadto be written despite the fact that some lexical andsyntactic information is repeatedly used in differentapplications.
The integration process we describe,however, partially automates this process.4.2 The  in tegrat ion  s tepsThe integration of the lexicon with FUF/SURGEis done through incremental unification, using fourunification steps as shown in Figure 3.
Each  stepadds information to the semantic input, and at theend of the four unification steps, the semantic inputhas been mapped to the SURGE input format.
(1) The semantic inputDifferent generation systems usually use differentrepresentation formats for semantic input.
Somesystems use case roles ; some systems use flatattribute-value r presentation (Kukich et al, 1994).For the integrated lexicon and FUF/SURGE pack-age to be easily pluggable in applications, we need todefine a standard semantic input format.
It shouldbe designed in such a way that applications can eas-ily adapt their particular semantic inputs to thisstandard format.
It should also be easily mappedto the SURGE input format.In this paper, we only consider the issue of seman-tic input format for the expression of the predicate-argument relation.
Two questions need to be an-swered in the design of the standard semantic inputformat: one, how to represent semantic oncepts;and two, how to represent he predicate-argumentrelation.We use WordNet synsets to represent semanticconcepts.
The input can refer to synsets in severalways: either using a globally unique synset num-ber I or by specifying a word and its sense numberin WordNet.The representation of verb arguments is a morecomplicated issue.
Case roles are frequently used ingeneration systems to represent verb arguments insemantic inputs.
For example, (Dorr et al, 1998)used 20 case roles in their lexical conceptual struc-ture corresponding to underlying positions in a com-positional lexical structure.
(Langkilde and Knight.1998) use a list of case roles in their interlingua rep-resentations.We decided to use numbered arguments (similar tothe DSyntR in MTT (Mel'cuk and Perstov, 1987))instead of case roles.
The difference between the two1Since there are a huge number of synsets in WordNet, wewill provide a searchable database of synsets o that users canlook up a synset and its index number easily.
For a part icularappl ication, users can adapt  the synsets to their specific do-main, such as removing non-relevant synsets, merging synsets.and relabel ing the synsets for convenience, as discussed in(,ling, 1998).is not critical but the numbered argument approach?
avoids the need?
to commit: the: lexicon to a specificontology and seems to be easier to learn 2.Figure 4 shows a sample semantic input.
For easyunderstanding, we refer to  the semantic conceptsusing their definitions rather than numerical indexnumbers.
There are two arguments in the input.The intended output sentence for this semantic in-put is "A boat appeared on the horizon" or its para-phrases.
(2) Lexical unificationIn this step, we map the semantic oncepts in the "semantic input to concrete words.
To do this, we usethe synsets in WordNet.
All the words in the samesynset can be used to convey the same semantic on-cept.
For the above example, the semantic oncepts"become visible" and "a small vessel for travel onwater" can be realized by the the verb appear andthe noun boat respectively.
This is the step that canproduce lexical paraphrases.
Note that when thesystem chooses a word, it also determines the par-ticular sense number of the word, since a word asit belongs to a synset has a unique sense number inWordNet.We represented all the synsets in Wordnet in FUFformat.
Each synset includes its numerical indexnumber and the list of word senses included in thesynsets.
This lexical unification, works for bothnouns and verbs.
(3) Structural unificationAfter the system has chosen a verb (actually aparticular sense of a verb), it uses that informationas an index to unify with the subcategorization a dalternations the particular verb sense has.
This stepadds additional syntactic information to the origi-nal input and has the capacity to produce syntacticparaphrases using alternation information.
(4) Constraints on the number of argumentsNext, we use the constraints that a subcategoriza-tion has on the number of arguments it requires torestrict unification with subcategorization patterns.\~k~ use 147 possible patterns.
For example, the in-put in Figure 4 has two arguments.
Although IN-TRANS (meaning intransitive) is listed as a possi-ble subcategorization pattern for "appear" (see sense2 in Figure 1), the input will fail to unify with itsince INTRANS requires a single argument only.This prevents the generation of non-grammatic'Asentences.
This step adds a feature which specifiesthe transitivity of the verb to FUF/SURGE input,selecting one from the lexicon when there is morethan one possibility for the given verb.2The difference between numbered arguments and labeledroles is s imi lar  to that between amed semantic primit ives andsynsets in \.VordNet.
Verb classes share the same definitionof which argument is denoted by l, 2 etc.
if they share somesyntact ic properties as far as argument aking properties areconcerned.213Semantic input Synsets verbs lexicon si~ucts Input for SURGEFigure 3: The integration process\[rel-- i--ept --evisible J 1\]1 \[ concept  "a  smal l  vesse l  fo r  t rave l  on  water ' '  \]args 2 \[ concept ' ' the  l i ne  at  which the sky and Earth appear to  meet' '  \]Figure 4: The semantic input using numbered arguments(5) Mapping structures to SURGE inputIn the last step, the subcategorization a d alter-nations are mapped to SURGE input format.
Themapping from subcategorizations to SURGE inputwas manually encoded in the lexicon for each oneof the 147 patterns.
This mapping information canbe reused for all applications, which is more effi-cient than composing SURGE input in the lexical-ization component of each different application.
Fig-ure 5 shows how the subcategorization NP-WITH-NP (e.g., The clown amused the children with hisantics) is mapped to the SURGE input format.
Thismapping mainly involves matching the numbered ar-guments in the semantic input to appropriate l xicalroles and syntactic ategories so that FIJF/SURGEcan generate them in the correct order.The final SURGE input for the sentence ",4 boatappeared on the horizon" is shown in Figure 6.
Us-ing the "THERE-INSERTION" alternation that theverb "appear" (sense 2) authorizes, the system canalso generate the syntactic paraphrase "There ap-peared a boat on the hor izon".
The SURGE inputthe system generates for "There appeared a boat onthe horizon" is very different .from that for "A boatappeared on the horizon".It is possible that for a given application somegenerated paraphrases are not appropriate.
In thiscase, users can edit the synsets and the alternationsto filter out tile paraphrases tile) do not want.Tile four unification steps are completely auto-matic.
Tile system can send feedback upon failurestructrelationargsproclex-rolesnp-with-np1 \[21<...>2 \[al<...>3 \[41<...>type lexicallex Illt12subcat 23\ [1  \[all 2 \[3\]3 \[41cat np \]121\[rat .p \]\[alcat ipprep lexnp \[41"with" \] 1Figure 5: Mapping subcategorization "NP-\VITH-NP" to SURGE inputof unification.5 Re la ted  WorkThe lexicon, after it is integrated withFUF/SURGE, can also be used for other tasks inlanguage generation.
For example, revision (Robin,1994) is a technique for building semantic inputsincrementally.
The revision process decides whetherit is appropriate to attach a new constituent to thecurrent semantic input, for example, by adding an214relationargsstructarglcatlexical-rolesconceptword1 conceptwordconcept2 wordppb2 ~ givenc lause cdc 'become ~is ib le '  ' \]\] "appear"a'a small  vessel  for travel on water'' \]J "boa~"a'Cthe l ine  at  which the sky and Earth appear to meet \]"hor,izon ''a \]"Enriched in first stepbEnriched in second stepCEnriched in third stepdEnriched in fourth stepFigure 6: SURGE input for "A boat appeared on the horizon"object or an adverb.
Such decisions are constrainedby syntactic properties of verbs.
The integratedlexicon is useful to verify these properties.Nitrogen (Langkilde and Knight, 1998), a naturallanguage generation system developed at ISI, alsoincludes a large-scale l xicon to support the genera-tion process.
Given that Nitrogen and FUF/SURGEuse very different methods for generation, the waythat we integrate the lexicon with the generation sys-tem is also very different.
Nitrogen combines ym-bolic rules with statistics learned from text corpora,while FUF/SURGE is based on Functional Unifica-tion Grammar.
Other related work includes (Stede,1998), which suggests a lexicon structure for multi-lingual generation in a knowledge-based generationsystem.
The main idea is to handle multilingual gen-eration in the same way as paraphrasing of the samelanguage.
Stede's work concerns mostly the lexicalsemantics of the transitivity alternations.6 Conc lus ionWe have presented in this paper the integration ofa large-scale, reusable lexicon for generation withFUF/SURGE, a unification-based natural languagegenerator.
This integration makes it possible toreuse major parts of a lexical chooser, which is tilecomponent in a generation system that is responsi-ble for mapping semantic inputs to surface genera-tor inputs.
We show that although the whole lexical "chooser can not be made domain-independent, it ispossible to reuse a large amount of lexical, syntactic,and semantic knowledge across applications.In addition, tile lexicon other benefits to a genera-tion system, inchiding the abilities to generate nlanylexical paraphrases automatically, generate syntac -tic paraphrases, av(fid n(m-grammatical output, andchoose the most frequently used word when there ismore than one candidate words.
Since the lexical,syntactic, and semantic knowledge ncoded in thelexicon is general and the lexicon has a wide cover-age, it can be reused for different applications.In the future, we plan to validate the paraphrasesthe lexicon can generate by asking human subjects toread the generated paraphrases and judge whetherthey are acceptable.
We would like to investigateways that can systematically filter out paraphrasesthat are considered unacceptable.
We are also inter-ested in exploring the usage of this system in multi-lingual generation.Re ferencesB.
J. Doff, N. Habash,A thematic hierarchyfrom lexical-conceptual.and D. Traum.
1998.for efficient generationTechnical Report CS-TR-3934, Institute for Advanced Computer Stud-ies, Department of Computer Science, Universityof Maryland, October.M.
Elhadad and J. Robin.
1996.
An overview ofSURGE: a re-usable comprehensive syntactic re-alization component.
In INLG'96, Brighton, UK.
(demonstration session).M.
Elhadad.
1992.
Using Argumentation to ControlLezical Choice: A Functional Unification-BasedApproach.
Ph.D. thesis, Department of ComputerScience, Columbia University.R.
Grishman, C. Macleod, and A. Meyers.
1994.COMLEX syntax: Building a computationallexicon.
In Proceedings of COLING'94, Kyoto,,Japan.H.. l ing and K. McKeown.
1998.
Combining mul-tiple, large-scale resources in a reusable lexiconfor natural language generation.
In Proceedings215of the 36th Annual Meeting of the Association forComputational Linguistics and the .17th Interna-tional Conference on Computational Linguistics,volume 1, pages 607-613, Universit(~ de MontrEal,Quebec, Canada, August.H.
Jing.
1998.
Applying wordnet o natural an-guage generation.
In Proceedings of COLING-ACL'98 workshop on the Usage of WordNet inNatural Language Processing Systems, Universityof Montreal, Montreal, Canada, August.K.
Kukich, K. McKeown, J. Shaw, J. Robin, N. Mor-gan, and J. Phillips.
"1994.
User-needs analysisand design methodology for an automated oc-ument generator.
In A. Zampolli, N. Calzolari,and M. Palmer, editors, Current Issues in Com-putational Linguistics: In Honour of Don Walker.Kluwer Academic Press, Boston.I.
Langkilde and K. Knight.
1998.
The practicalvalue of n-grams in generation.
In INLG'98, pages248-255, Niagara-on-the-Lake, Canada, August.B.
Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
University ofChicago Press, Chicago, Illinois.I.A.
Mel'cuk and N.V. Perstov.
1987.
Surface-syntax of English, a formal model in theMeaning Text Theory.
Benjamins, Amster-dam/Philadelphia.G.
Miller, R. Beckwith C. Fellbaum, and D. Gross K.Miller.
1990.
Introduction to WordNet: An on-line lexical database.
International Journal ofLexicography (special issue), 3 (4) :235-312.G.A.
Miller, C. Leacock, R. Tengi, and R.T. Bunker.1993.
A semantic oncordance.
Cognitive ScienceLaboratory, Princeton University.R.
Passoneau, K. Kukich, J. Robin, V. Hatzivas-siloglou, L. Lefkowitz, and H. Jing.
1996.
Gen-erating summaries of workflow diagrams.
In Pro-ceedings of the International Conference on Nat-ural Language Processing and Industrial Appli-cations (NLP-IA'96), Moncton, New Brunswick,Canada.E.
Reiter.
1994.
Has a consensus nl generation ar-chitecture appeared, and is it psyeholinguisticallyplausible?
In Proceedings of the Seventh Interna-tional Workshop on Natural Language Generation(INLGW-1994), pages 163-170, Kennebunkport,Maine, USA.
available from the cmp-lg archive aspaper cmp-lg/9411032.J.
Robin.
1994.
Revision-Based Generation of Nat-.ural Language Summaries Providing HistoricalBackground: Corpus-Based Analysis, Design, Im-plementation, and Evaluation.
Ph.D. thesis, De-partment of Computer Science, Cohnnbia Univer-sity.
Also Technical Report CU-CS-034-94.M.
Stede.
1998.
A generative l)ersl}ective on vert} al-ternations.
Computational Lin.quistics.
24(3):4{}1-_430-,September"216
