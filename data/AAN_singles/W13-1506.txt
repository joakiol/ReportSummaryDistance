Proceedings of the 2th Workshop of Natural Language Processing for Improving Textual Accessibility (NLP4ITA), pages 49?58,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsA Two-Stage Approach for Generating UnbiasedEstimates of Text ComplexityKathleen M. Sheehan Michael Flor Diane NapolitanoEducational Testing ServicePrinceton, NJ, USA{ksheehan, mflor, dnapolitano}@ets.orgAbstractMany existing approaches for measuring textcomplexity tend to overestimate the complexi-ty levels of informational texts while simulta-neously underestimating the complexity levelsof literary texts.
We present a two-stage esti-mation technique that successfully addressesthis problem.
At Stage 1, each text is classi-fied into one or another of three possible ge-nres:  informational, literary or mixed.
Next,at Stage 2, a complexity score is generated foreach text by applying one or another of threepossible prediction models:  one optimized forapplication to informational texts, one opti-mized for application to literary texts, and oneoptimized for application to mixed texts.Each model combines lexical, syntactic anddiscourse features, as appropriate, to best rep-licate human complexity judgments.
We dem-onstrate that resulting text complexitypredictions are both unbiased, and highly cor-related with classifications provided by expe-rienced educators.1 IntroductionAutomated text analysis systems, such as reada-bility metrics, are frequently used to assess theprobability that texts with varying combinations oflinguistic features will be more or less accessible toreaders with varying levels of reading comprehen-sion skill (Stajner, Evans, Orasan and Mitkov,2012).
This paper introduces TextEvaluator, a ful-ly-automated text analysis system designed to faci-litate such work.1Our approach for addressing these differencescan be summarized as follows.
First, a large set oflexical, syntactic and discourse features is ex-tracted from each text.
Next, either human raters,or an automated genre classifier is used to classifyeach text into one or another of three possible ge-nre categories: informational, literary, or mixed.Finally, a complexity score is generated for eachtext by applying one or another of three possibleprediction models: one optimized for application toinformational texts, one optimized for applicationto literary texts, and one optimized for applicationto mixed texts.
We demonstrate that resultingcomplexity measures are both unbiased, and highlycorrelated with text grade level (GL) classificationsprovided by experienced educators.TextEvaluator successfullyaddresses an important limitation of many existingreadability metrics:  the tendency to over-predictthe complexity levels of informational texts, whilesimultaneously under-predicting the complexitylevels of literary texts (Sheehan, Kostin & Futagi,2008; Sheehan, Kostin, Futagi & Flor, 2010).
Weillustrate this phenomenon, and argue that it resultsfrom two fundamental differences between infor-mational and literary texts:  (a) differences in theway that common every-day words are used andcombined; and (b) differences in the rate at whichrare words are repeated.1 TextEvaluator was previously called SourceRater.49Our paper is organized as follows.
Section 2summarizes related work on readability assess-ment.
Section 3 describes the two corpora assem-bled for use in this study, and outlines how genreand GL classifications were assigned.
Section 4illustrates the problem of genre bias by consideringthe specific biases detected in two widely-usedreadability metrics.
Section 5 describes the Text-Evaluator features, methods and results.
Section 6presents a summary and discussion.2    Related WorkDespite the large numbers of text features that maypotentially contribute to the ease or difficulty ofcomprehending complex text, many widely-usedreadability metrics are based on extremely limitedfeature sets.
For example, the Flesch-Kincaid GLscore (Kincaid, et al 1975), the FOG Index (Gun-ning, 1952), and the Lexile Framework (Stenner, etal., 2006) each consider just two features: a singlemeasure of syntactic complexity (average sentencelength) and a single measure of lexical difficulty(either average word length in syllables, averagefrequency of multi-syllable words, or average wordfamiliarity estimated via a word frequency, WF,index).Recently, more computationally sophisticatedmodeling techniques such as Statistical LanguageModels (Si and Callan, 2001; Collins-Thompsonand Callan, 2004, Heilman, et al 2007, Pitler andNenkova, 2008), Support Vector Machines(Schwarm and Ostendorf, 2005), Principal Com-ponents Analyses (Sheehan, et al 2010) and Mul-ti-Layer Perceptron classifiers (Vajjala andMeurers, 2012) have enabled researchers to inves-tigate a broader range of potentially useful fea-tures.
For example: Schwarm and Ostendorf(2005) demonstrated that vocabulary measuresbased on trigrams were effective at distinguishingarticles targeted at younger and older readers; Pit-ler and Nenkova (2008) reported improved validityfor measures based on the likelihood of vocabularyand the likelihood of discourse relations; and Vaj-jala and Meurers (2012) demonstrated that featuresinspired by Second Language Acquisition researchalso contributed to validity improvements.
Impor-tantly, however, while this research has contributedto our understanding of the types of text featuresthat may cause texts to be more or less compre-hensible, evaluations focused on the presence anddegree of genre bias have not been reported.3   CorporaTwo text collections are considered in this re-search.
Our training corpus includes 934 passagesselected from a set of previously administeredstandardized assessments constructed to providevalid and reliable feedback about the types of ver-bal reasoning skills described in U.S. state and na-tional assessment frameworks.
Human judgmentsof genre (informational, literary or mixed) and GL(grades 3-12) were available for all texts.
Genreclassifications were based on established guide-lines which place texts structured to inform or per-suade (e.g., newspaper text, excerpts from scienceor social studies textbooks) in the informationalcategory, and texts structured to provide a reward-ing literary experience (e.g., folk tales, short sto-ries, excerpts from novels) in the literary category(see American Institutes for Research, 2008).
Weadded a Mixed category to accommodate textsclassified as incorporating both informational andliterary elements.
Nelson, Perfetti, Liben and Li-ben (2012) describe an earlier, somewhat smallerversion of this dataset.
We added additional pas-sages downloaded from State Department of Edu-cation web sites, and from the NationalAssessment of Educational Progress (NAEP).
Ineach case, GL classifications reflected the GLs atwhich passages were administered to students.Thus, all passages classified at Grade 3 appearedon high-stakes assessments constructed to provideevidence of student performance relative to Grade3 reading standards.Two important characteristics of this datasetshould be noted.
First, unlike many previous cor-pora, (e.g., Stenner, et al 2006; Zeno, et al 2005)accurate paragraph markings are included for alltexts.
Second, while many of the datasets consi-dered in previous readability research were com-prised entirely of informational text (e.g., Pitlerand Nenkova, 2008; Schwarm and Ostendorf,2005;  Vajjala and Meurers, 2012) the current da-taset covers the full range of text types consideredby teachers and students in U.S. classrooms.Table 1 shows the numbers of informational, li-terary and mixed training passages at each targetedGL.
Passage lengths ranged from 112 words atGrade 3, to more than 2000 words at Grade 12.50Average passage lengths were 569 words and 695words in the informational and literary subsets,respectively.GradeLevelGenreTotal Inf.
Lit.
Mixed3 46 60 8 1144 51 74 7 1325 44 46 12 1026 41 40 6 877 36 58 6 1008 70 63 18 1519 23 23 2 4810 26 49 2 7711 15 24 0 3912 47 15 22 84Total 399 452 83 934Table 1.
Numbers of passages in the model develop-ment/training dataset, by grade level and genre.A validation dataset was also constructed.
It in-cludes the 168 texts that were published as Appen-dix B of the new Common Core State Standards(CCSSI, 2010), a new standards document that hasnow been adopted in 46 U.S. states.
Individualtexts were contributed by teachers, librarians, cur-riculum experts, and reading researchers.
GL clas-sifications are designed to illustrate the ?staircaseof increasing complexity?
that teachers and testdevelopers are being encouraged to replicate whenselecting texts for use in K-12 instruction and as-sessment in the U.S.
The staircase is specified interms of five grade bands:  Grades 2-3, Grades 4-5,Grades 6-8, Grades 9-10 or Grades 11+.
Table 2shows the numbers of informational, literary and?Other?
texts (includes both Mixed and speeches)included at each grade band.GradeBandGenreTotal Inf.
Lit.
Other2-3 6 10 4 204-5 16 10 4 306-8 12 16 13 419-10 12 10 17 3911+ 8 10 20 38Total 54 56 58 168Table 2.
Numbers of passages in the validation dataset,by grade band and genre.4   Genre BiasThis section examines the root causes of genre bi-as.
We focus on two fundamental differences be-tween informational and literary texts: differencesin the types of vocabularies employed, and differ-ences in the rate at which rare words are repeated.These differences have been examined in severalprevious studies.
For example, Lee (2001) docu-mented differences in the use of ?core?
vocabularywithin a corpus of informational and literary textsthat included over one million words downloadedfrom the British National Corpus.
Core vocabularywas defined in terms of a list of 2000 commonwords classified as appropriate for use in the dic-tionary definitions presented in the Longman Dic-tionary of Contemporary English.
The analysesdemonstrated that core vocabulary usage was high-er in literary texts than in informational texts.
Forexample, when literary texts such as fiction, poetryand drama were considered, the percent of totalwords classified as ?core?
vocabulary ranged from81% to 84%.
By contrast, when informationaltexts such as science and social studies texts wereconsidered, the percent of total words classified as?core?
vocabulary ranged from 66% to 71%.
Ininterpreting these results Lee suggested that thecreativity and imaginativeness typically associatedwith literary writing may be less closely tied to thetype or level of vocabulary employed and moreclosely tied to the way that core words are usedand combined.
Note that this implies that an indi-vidual word detected in a literary text may not beindicative of the same level of processing chal-lenge as that same word detected in an informa-tional text.Differences in the vocabularies employed withininformational and literary texts, and subsequentimpacts on readability metrics, are also discussedin Appendix A of the Common Core State Stan-dards (CCSSI, 2010).
The tendency of many exist-ing readability metrics to underestimate thecomplexity levels of literary texts is described asfollows: ?The Lexile Framework, like traditionalformulas, may underestimate the difficulty of textsthat use simple, familiar language to convey so-phisticated ideas, as is true of much high-qualityfiction written for adults and appropriate for olderstudents?
(p. 7).Genre bias may also result from genre-specificdifferences in word repetition rates.
Hiebert and51Mesmer (2013, p.46) describe this phenomenon asfollows:  ?Content area texts often receive inflatedreadability scores since key concept words that arerare (e.g., photosynthesis, inflation) are often re-peated which increases vocabulary load, eventhough repetition of content words can supportstudent learning (Cohen & Steinberg, 1983)?.Table 3 provides empirical evidence of thesetrends.
The table presents mean GL classificationsestimated conditional on mean WF scores, for theinformational (n = 399) and literary (n = 452) pas-sages in our training dataset.
WF scores were gen-erated via an in-house WF index constructed froma corpus of more than 400 million word tokens.The corpus includes more than 17,000 completebooks, including both fiction and nonfiction titles.Avg.
WFInformational LiteraryN GL SD N GL SD51.0?52.5 2 12.0 0.0 0 -- --52.5?54.0 16 10.8 1.9 0 -- --54.0?55.5 68 9.6 2.0 1 10.0 --55.5?57.0 89 7.8 2.7 18 9.9 1.957.0?58.5 96 6.6 2.3 46 9.2 2.058.5?60.0 78 5.3 1.8 92 7.6 2.460.0?61.5 44 4.6 1.8 142 6.2 2.461.5?63.0 6 3.7 0.8 119 5.5 2.163.0?64.5 0 -- -- 31 4.5 1.964.5?66.0 0 -- -- 3 4.0 1.7Total 399 57.4 2.1 452 60.6 1.9Table 3.
Mean GL classifications, by Average WFscore, for informational and literary passages targeted atreaders in grades 3 through 12.The results in Table 3 confirm that, consistentwith expectations, texts with lower average WFscores are more likely to appear on assessmentstargeted at older readers, while texts with higheraverage WF scores are more likely to appear onassessments targeted at younger readers.
But notethat large genre differences are also present.
Figure1 provides a graphical representation of thesetrends.
Results for informational texts are plottedwith a solid line; those for literary texts are plottedwith a dashed line.
Note that the literary curve ap-pears above the informational curve throughout theentire observed range of the data.
This suggeststhat a given value of the Average WF measure isindicative of a higher GL classification if the textin question is a literary text, and a lower GL classi-fication if the text in question is an informationaltext.
Since a readability measure that includes thisfeature (or a feature similar to this feature) withoutalso accounting for genre effects will tend to yieldpredictions that fall between the two curves, result-ing GL predictions will tend to be too high for in-formational texts (positive bias) and too low forliterary texts (negative bias).Figure 1.
Mean text GL plotted conditional on averageWF score.
(One literary mean score based on evidencefrom a single text is not plotted.
)Figure 2 confirms that this evidence-based pre-diction holds true for two widely-used readabilitymetrics: the Flesch-Kincaid GL score and the Lex-ile Framework22 All Lexile scores were obtained via the Lexile Analyzeravailable at www.lexile.com.
Scores are only available for asubset of texts since our training corpus included just 548passages at the time that these data were collected.
Corres-ponding human GL classifications were approximately evenlydistributed across grades 3 through 12.. Each individual plot comparesFlesch-Kincaid GL scores (top row), or Lexilescores (bottom row) to the human GL classifica-tions stored in our training dataset, i.e., classifica-tions that were developed and reviewed byexperienced educators, and were subsequently usedto make high-stakes decisions about students andteachers, e.g., requiring students to repeat a graderather than advancing to the next GL.
The plotsconfirm that, in each case, the predicted pattern ofover- and under-estimation is present.
That is, onaverage, both Flesch-Kincaid scores and Lexilescores tend to be slightly too high for informationaltexts, and slightly too low for literary texts, therebycalling into doubt any cross-genre comparisons.Average ETS Word FrequencyMeanGradeLevel52 54 56 58 60 62 64 664681012 LiteraryInformational52Human Grade LevelLexileScore0 5 10 15600800100012001400Informational (n = 243)Human Grade LevelLexileScore0 5 10 15600800100012001400Literary (n = 305)Human Grade LevelFlesch-KincaidGradeLevel0 5 10 15051015Informational (n = 399)Human Grade LevelFlesch-KincaidGradeLevel0 5 10 15051015Literary (n = 452)Figure 2.
Passage complexity scores generated via theFlesch-Kincaid GL score (top) and the Lexile Frame-work (bottom) compared to GL classifications providedby experienced educators.5  Features, Components and Results5.1 FeaturesThe TextEvaluator feature set is designed tomeasure the ease or difficulty of implementingfour types of processes believed to be criticallyinvolved in comprehending complex text: (1)processes involved in word recognition and decod-ing, (2) processes associated with using relevantsyntactic knowledge to assemble words into mea-ningful propositions, (3) processes associated withinferring connections across propositions or largersections of text, and (4) processes associated withusing relevant prior knowledge and experience todevelop a more complete, more integrated mentalrepresentation of a text.
(See Kintsch, 1998).A total of 43 candidate features were developed.Since many of these were expected to be moderate-ly inter-correlated, a Principal Components Analy-sis (PCA) was used to locate clusters of featuresthat exhibited high within-cluster correlation andlow between-cluster correlation.
Linear combina-tions defined in terms of the resulting feature clus-ters provided the independent variables consideredin subsequent investigations.
Biber and his col-leagues (2004) justify this approach by noting that,because many important aspects of text variationare not well captured by individual linguistic fea-tures, investigation of such characteristics requiresa focus on ?constellations of co-occurring linguis-tic features?
as opposed to individual features (p.45).The PCA suggested that more than 60% of thevariation captured by the full set of 43 featurescould be accounted for via a set of eight compo-nent scores, where each component is estimated asa linear combination of multiple correlated fea-tures, and only 3 of the 43 features had moderatelyhigh loadings on more than one component, andmost loadings exceeded 0.70.
The individual fea-tures comprising each component are describedbelow.Component #1:  Academic Vocabulary.
Tenfeatures loaded heavily on this component.
Twoare based on the Academic Word List described inCoxhead (2000).
These include:  the frequency perthousand words of all words on the AcademicWord List, and the ratio of listed words to totalwords.
In a previous study, Vajjala and Meurers(2012)  demonstrated that the ratio of listed wordsto total wards was very effective at distinguishingtexts at lower and higher levels in the WeeklyReader corpus.
Two additional features focus onthe frequency of nominalizations, including oneestimated from token counts and one estimatedfrom type counts.
Four additional features arebased on word lists developed by Biber and hiscolleagues.
These include the frequency per thou-sand words of academic verbs, abstract nouns, top-ical adjectives and cognitive process nouns (seeBiber, 1986, 1988; and Biber, et al 2004).
Twomeasures of word length also loaded on this di-mension:  average word length measured in syl-lables, and the frequency per thousand words ofwords containing more than 8 characters.Component #2:  Syntactic Complexity.
Sevenfeatures loaded heavily on this component.
Theseinclude features determined from the output of theStanford  Parser (Klein and Manning, 2003), aswell as more easily computed measures such asaverage sentence length, average frequency of longsentences (>= 25 words), and average number of53words between punctuation marks (commas, semi-colons, etc.).
Parse-based features include averagenumber of dependent clauses, and an automatedversion of the word ?depth?
measure introduced byYngve (1960).
This last feature, called AverageMaximum Yngve Depth, is designed to capturevariation in the memory load imposed by sentenceswith varying syntactic structures.
It is estimated byfirst assigning a depth classification to each wordin the text, then determining the maximum depthrepresented within each sentence, and then averag-ing over resulting sentence-level estimates to ob-tain a passage-level estimate.
Several studies ofthis word depth measure have been reported.
Forexample, Bormuth (1964) reported a correlation of-0.78 between mean word depth scores and clozefill-in rates provided by Japanese EFL learners.Component #3:  Concreteness.
Words that aremore concrete are more likely to evoke meaningfulmental images, a response that has been shown tofacilitate comprehension (Coltheart, 1981).
Alder-son (2000) argued that the level of concretenesspresent in a text is a useful feature to considerwhen evaluating passages for use on reading as-sessments targeted at L2 readers.
A total of fiveconcreteness and imageability measures loadedheavily on this dimension.
All five measures arebased on concreteness and imageability ratingsdownloaded from the MRC psycholinguistic data-base (Coltheart, 1981).
Ratings are expressed on a7 point scale with 1 indicating least concrete, orleast imageable, and 7 indicating most concrete ormost imageable.Component #4:  Word Unfamiliarity.
This com-ponent summarizes variation detected via six dif-ferent features.
Two features are measures ofaverage word familiarity: one estimated via our in-house WF Index, and one estimated via the TASAWF Index (see Zeno, et al 1995).
Both featureshave negative loadings, suggesting that the com-ponent is measuring vocabulary difficulty as op-posed to vocabulary easiness.
The other featureswith high loadings on this component are all meas-ures of rare word frequency.
These all have posi-tive loadings since texts with large numbers of rarewords are expected to be more difficult.
Two typesof rare word indices are included: indices based ontoken counts and indices based on type counts.Vocabulary measures based on token counts vieweach new word as an independent comprehensionchallenge, even when the same word occurs re-peatedly throughout the text.
By contrast, vocabu-lary measures based on type counts assume that apassage containing five different unfamiliar wordsmay be more challenging than a passage contain-ing the same unfamiliar word repeated five times.This difference is consistent with the notion thateach repetition of an unknown word provides anadditional opportunity to connect to prior know-ledge (Cohen & Steinberg, 1983).Component #5:  Interactive/ConversationalStyle.
This component includes the frequency perthousand words of:  conversation verbs, fictionverbs, communication verbs, 1st person plural pro-nouns, contractions, and words enclosed in quotes.Verb types were determined from one or more ofthe following studies: Biber (1986),  Biber (1988),and Biber, et al(2004).Component #6:  Degree of Narrativity.
Threefeatures had high positive loadings on this dimen-sion:  Frequency of past perfect aspect verbs, fre-quency of past tense verbs and frequency of 3rdperson singular pronouns.
All three features havepreviously been classified as providing positiveevidence of the degree of narrativity exhibited in atext (see Biber, 1986 and Biber, 1988).Component #7:  Cohesion.
Cohesion is thatproperty of a text that enables it to be interpreted asa ?coherent message?
rather than a collection ofunrelated clauses and sentences.
Halliday and Ha-san (1976) argued that readers are more likely tointerpret a text as a ?coherent message?
when cer-tain observable features are present.
These includerepeated content words and explicit connectives.The seventh component extracted in the PCA in-cludes three different types of cohesion features.The first two features measure the frequency ofcontent word repetition across adjacent sentenceswithin paragraphs.
These measures differ from thecohesion measures discussed in Graesser et al(2004) and in Pitler and Nenkova (2008) in that apsychometric linking procedure is used to ensurethat results for different texts are reported on com-parable scales (See Sheehan, in press).
The fre-quency of causal conjuncts (therefore,consequently, etc.)
also loads on this dimension.Component #8:  Argumentation.
Two featureshave high loadings on this dimension:  the fre-quency of concessive and adversative conjuncts(although, though, alternatively, in contrast, etc.
),and the frequency of negations (no, neither, etc.
),Just and Carpenter, (1987).545.2  An Automated Genre ClassifierA preliminary automated genre classifier wasdeveloped by training a logistic regression modelto predict the probability that a text is classified asinformational  as opposed to literary.
A signifi-cant positive coefficient was obtained for the Aca-demic Vocabulary component defined above,suggesting that a high score on this componentmay be interpreted as an indication that the text ismore likely to be informational.
Significant nega-tive coefficients were obtained for Narrativity, In-teractive/Conversational Style, and SyntacticComplexity, indicating that a high score on any ofthese components may be interpreted as an indica-tion that the text is more likely to be literary.
Twoindividual features that were not included in thePCA were also significant:  the proportion of adja-cent sentences containing at least one overlappingstemmed content word, and the frequency of 1stperson singular pronouns.
These features were notincluded in the PCA because they are not reliablyindicative of differences in text complexity (SeeSheehan, in press; Pitler and Nenkova, 2008.)
Re-sults confirmed, however, that these features areuseful for predicting a text?s genre classification.Alternative decision rules based on this modelwere investigated.
Table 4 summarizes the levelsof precision (P), recall (R) and F1 = 2RP/(R+P)obtained for the selected decision rule which wasdefined as follows: Classify as informational ifP(Inf) >= 0.52, classify as literary if P(inf) < 0.48,else classify as mixed.
This decision rule is definedsuch that few texts are classified into the mixedcategory since, at present, the training dataset in-cludes very few mixed texts.
The table shows de-creased precision in the Validation dataset sincemany more mixed texts are included, and the ma-jority of these were classified as informational.Dataset Genre N R P F1Training Inf 399 .84 .79 .81Training Lit 452 .88 .79 .83Training Mixed 83 .01 .09 .01Validation Inf 67 .91 .56 .69Validation Lit 56 .80 .80 .80Validation Mixed 45 .07 1.0 .13Table 4.
Levels of Precision, Recall and F1 obtained for1, 089 texts in the training and validation datasets.Speeches are not included in this summary.5.3  Prediction EquationsWe use separate genre-specific regression mod-els to generate GL predictions for texts classifiedas informational, literary, or mixed.
The coeffi-cients estimated for informational and literary textsare shown in Table 5.
Note that each component issignificant in one or both models.
The table alsohighlights key genre differences.
For example, notethat the Interactive/Conv.
Style score is significantin the Inf.
model but not in the Literary model.This reflects the fact that, while literary texts at allGLs tend to exhibit relatively high interactivity,similarly high interactivity among inf.
texts tendsto only be present at the lowest GLs.
Thus, a highInteractivity is an indication of low complexity ifthe text in question is an informational text, butprovides no statistically significant evidence aboutcomplexity if the text in question is a literary text.Component Informational LiteraryAcademic Voc.
1.126* .824*Word Unfamiliarity .802* .793*Word Concreteness -.610* -.483*Syn.
Complexity .983* 1.404*Lexical Cohesion -.266* -.440*Interactive/Conv.
Style -.518* nsDegree of Narrativity ns -.361*Argumentation .431* nsTable 5.
Regression coefficients estimated from trainingtexts.
*p < .01, ns = not significant.5.4  Validity EvidenceTwo aspects of system validity are of interest:(a) whether genre bias is present, and (b) whethercomplexity scores correlate well with judgmentsprovided by professional educators, i.e., the educa-tors involved in selecting texts for use on high-stakes state reading assessments.
The issue of ge-nre bias is addressed in Figure 3.
Each plot com-pares GL predictions generated via TextEvaluatorto GL predictions provided by experienced educa-tors.
Note that no evidence of a systematic tenden-cy to under-predict the complexity levels ofliterary texts is present.
This suggests that ourstrategy of developing distinct prediction modelsfor informational and literary texts has succeededin overcoming the genre biases present amongmany key features.55Human Grade LevelTextEvaluatorGradeLevel0 5 10 15051015Informational (n = 399)Human Grade LevelTextEvaluatorGradeLevel0 5 10 15051015Literary (n = 452)Figure 3.
TextEvaluator GL predictions compared tohuman GL classifications for informational and literarytexts.TestEvaluator performance relative to the goal ofpredicting the human grade band classifications inthe validation dataset was also examined.
Resultsare summarized in Table 6 along with correspond-ing results for the Lexile Framework (Stenner, etal., 2006) and the REAP system (Heilman, et al2007).
All results are reprinted, with permission,from Nelson, et al (2012).
In each case, perfor-mance is summarized in terms of the Spearmanrank order correlation between the readabilityscores generated for each text, and correspondinghuman grade band classifications.
95% confidencelimits estimated via the Fisher r to z transformationare also listed.SystemLower95%BoundCorrelationCoefficientUpper95%BoundTextEvaluator 0.683 0.76 0.814REAP 0.427 0.54 0.641Lexile 0.380 0.50 0.607Table 6.
Correlation  between readability scores andhuman grade band classifications for the 168 CommonCore texts in the validation dataset.The comparison suggests that, relative to the taskof  predicting the human grade band classificationsassigned to the informational, literary and mixedtexts in Appendix B of the new Common CoreState Standards, TextEvaluator is significantlymore effective than both the Lexile Frameworkand the REAP system.6  Summary and DiscussionIn many recent studies, proposed readability me-trics have been trained and validated on text collec-tions composed entirely of informational text, e.g.,Wall Street Journal articles (Pitler and Nenkova,2008), Encyclopedia Britannica articles (Schwarmand Ostendorf, 2005) and Weekly Reader articles(Vajjala and Meurers, 2012).
This paper considersthe more challenging task of predicting human-assigned GL classifications in a corpus of textsconstructed to be representative of the broad rangeof reading materials considered by teachers andstudents in U.S. classrooms.Two approaches for modeling the complexitycharacteristics of these passages were compared.In Approach #1, a single, non-genre specific pre-diction equation is estimated, and that equation isthen applied to texts in all genres.
Two measuresdeveloped via this approach were evaluated:  theLexile Framework and the REAP system.Approach #2 differs from Approach #1 in thatgenre-specific prediction equations are used, there-by ensuring that important genre effects are ac-commodated.
This approach is currently onlyavailable via the TextEvaluator system.Measures developed via each approach wereevaluated on a held-out sample.
Results confirmedthat complexity classifications obtained viaTextEvaluator are significantly more highly corre-lated with the human grade band classifications inthe held-out sample than are classifications ob-tained via the Lexile Framework or REAP system.This study also demonstrated that, when genreeffects are ignored, readability scores for informa-tional texts tend to be overestimated, while thosefor literary texts tend to be underestimated.
Notethat this finding significantly complicates theprocess of using readability metrics to generatevalid cross-genre comparisons.
For example,Stajner, et al(2012) conclude that SimpleWikimay not serve as a ?gold standard?
of high acces-sibility because comparisons based on readabilitymetrics suggest that it is more complex than Fic-tion.
We intend to further investigate this findingusing TextEvaluator since conclusions that are notimpacted by genre bias can then be reported.
Addi-tional planned work involves investigating addi-tional measures of genre, and incorporating theseinto our genre classifier.56ReferencesAlderson, J. C. (2000).
Assessing reading.
Cam-bridge: Cambridge University Press.American Institutes for Research (2008).
Readingframework for the 2009 National Assessment ofEducational Progress.
Washington, DC: Na-tional Assessment Governing Board.Biber, D. (1986).
Spoken and written textual di-mension in English: Resolving the contradictoryfindings.
Language, 62: 394-414.Biber, D. (1988).
Variation across Speech andWriting.
Cambridge: Cambridge UniversityPress.Biber, D., Conrad, S., Reppen, R., Byrd, P., Helt,M., Clark, V., et al (2004).
Representing lan-guage use in the university:  Analysis of theTOEFL 2000 Spoken and Written AcademicLanguage Corpus.
TOEFL Monograph Series,MS-25, January 2004.
Princeton, NJ: Educa-tional Testing Service.Bormuth, J.R. (1964).
Mean word depth as a pre-dictor of comprehension difficulty.
CaliforniaJournal of Educational Research, 15, 226-231.Cohen, S. A.
& Steinberg, J. E. (1983).
Effects ofthree types of vocabulary on readability of in-termediate grade science textbooks:  An applica-tion of Finn?s transfer feature theory.
ReadingResearch Quarterly, 19(1), 86-101.Collins-Thompson, K. and Callan, J.
(2004).
Alanguage modeling approach to predicting read-ing difficulty.
In Proceedings of HLT/NAACL2004, Boston, USA.Coltheart, M. (1981).
The MRC psycholinguisticdatabase, Quarterly Journal of ExperimentalPsychology, 33A, 497-505.Common Core State Standards Initiative (2010).Common core state standards for English lan-guage arts & literacy in history/social studies,science and technical subjects.
Washington,DC: CCSSO & National Governors Association.Coxhead, A.
(2000)  A new academic word list.TESOL Quarterly, 34(2), 213-238.Gunning, R. (1952).
The technique of clear writ-ing.
McGraw-Hill: New York.Graesser, A.C., McNamara, D. S., Louwerse,M.W.
and Cai, Z.
(2004).
Coh-Metrix:  Analy-sis of text on cohesion and language.
BehaviorResearch Methods, Instruments & Computers,36(2), 193-202.Halliday, M. A.K.
& Hasan, R. (1976) Cohesion inEnglish.
Longman, London.Hiebert, E. H. & Mesmer, H. A. E. (2013).
Uppingthe ante of text complexity in the Common CoreState Standards: Examining its potential impacton young readers.
Educational Researcher,42(1), 44-51.Heilman, M., Collins-Thompson, K., Callan, J.
&Eskenazi, M. (2007).
Combining lexical andgrammatical features to improve readabilitymeasures for first and second language texts.
InHuman Language Technologies 2007: The Con-ference of the North American Chapter of theAssociation for Computational Linguistics(HLT-NAACL?07), 460-467.Just, M. A.
& Carpenter, P. A.
(1987).
The psy-chology of reading and language comprehen-sion.
Boston: Allyn & Bacon.Kincaid, J.P., Fishburne, R.P, Rogers, R.L.
&Chissom, B.S.
(1975).
Derivation of new reada-bility formulas (automated readability index,Fog count and Flesch reading ease formula) fornavy enlisted personnel.
Research Branch Re-port 8-75.
Naval Air Station, Memphis, TN.Kintsch, W. (1998).
Comprehension: A paradigmfor cognition.
Cambridge, UK: Cambridge Uni-versity Press.Klein, D. & Manning, C. D. (2003).
Accurate Un-lexicalized Parsing.
In Proceedings of the 41stMeeting of the Association for ComputationalLinguistics, 423-430.Lee, D. Y. W. (2001)  Defining core vocabularyand tracking its distribution across spoken andwritten genres.
Journal of English Linguistics.29, 250-278.Nelson, J., Perfetti, C., Liben, D. and Liben, M.(2012).
Measures of text difficulty: Testing theirpredictive value for grade levels and studentperformance.
Technical Report, The Council ofChief State School Officers.57Pitler, E. & Nenkova, A (2008).
Revisiting reada-bility:  A unified framework for predicting textquality.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural LanguageProcessing, Association for Computational Lin-guistics, 186-195.Schwarm, S. & Ostendorf, M. (2005).
Readinglevel assessment using support vector machinesand statistical language models.
In Proceedingsof the 43rd Annual Meeting of the Association forComputational Linguistics (ACL?05), 523-530.Sheehan, K.M.
(in press).
Measuring cohesion: Anapproach that accounts for differences in the de-gree of integration challenge presented by dif-ferent types of sentences.
EducationalMeasurement: Issues and Practice.Sheehan, K.M., Kostin, I & Futagi, Y.
(2008).When do standard approaches for measuringvocabulary difficulty, syntactic complexity andreferential cohesion yield biased estimates oftext difficulty?
In B.C.
Love, K. McRae, &V.M.
Sloutsky (Eds.
), Proceedings of the 30thAnnual Conference of the Cognitive ScienceSociety, Washington D.C.Sheehan, K.M., Kostin, I., Futagi, Y.
& Flor, M.(2010).
Generating automated text complexityclassifications that are aligned with targetedtext complexity standards.
(ETS RR-10-28).Princeton, NJ: ETS.Si, L. & Callan, J.
(2001).
A statistical model forscientific readability.
In Proceedings of the 10thInternational Conference on Information andKnowledge Management (CIKM), 574-576.?tajner, S., Evans, R., Orasan, C., & Mitkov, R.(2012).
What Can Readability Measures ReallyTell Us About Text Complexity?.
In NaturalLanguage Processing for Improving Textual Ac-cessibility (NLP4ITA) Workshop Programme(p. 14).Stenner, A. J., Burdick, H., Sanford, E. & Burdick,D.
(2006).
How accurate are Lexile text meas-ures?
Journal of Applied Measurement, 7(3),307-322.Vajjala, S. & Meurers, D. (2012).
On improvingthe accuracy of readability classification usinginsights from second language acquisition.
InProceedings of the 7th Workshop on the Innova-tive Use of NLP for Building Educational Appli-cations, 163-173.Yngve, V.H.
(1960).
A model and an hypothesisfor language structure.
Proceedings of theAmerican Philosophical Society, 104, 444-466.Zeno, S. M., Ivens, S. H., Millard, R. T., Duvvuri,R.
(1995).
The educator?s word frequencyguide.
Brewster, NY: Touchstone AppliedScience Associates.58
