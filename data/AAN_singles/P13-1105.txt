Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063?1072,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBilingually-Guided Monolingual Dependency Grammar InductionKai Liu?
?, Yajuan Lu?
?, Wenbin Jiang?, Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn?Centre for Next Generation LocalisationFaculty of Engineering and Computing, Dublin City Universityqliu@computing.dcu.ie?University of Chinese Academy of SciencesAbstractThis paper describes a novel strategy forautomatic induction of a monolingual de-pendency grammar under the guidanceof bilingually-projected dependency.
Bymoderately leveraging the dependency in-formation projected from the parsed coun-terpart language, and simultaneously min-ing the underlying syntactic structure ofthe language considered, it effectively in-tegrates the advantages of bilingual pro-jection and unsupervised induction, so asto induce a monolingual grammar muchbetter than previous models only usingbilingual projection or unsupervised in-duction.
We induced dependency gram-mar for five different languages under theguidance of dependency information pro-jected from the parsed English translation,experiments show that the bilingually-guided method achieves a significantimprovement of 28.5% over the unsuper-vised baseline and 3.0% over the best pro-jection baseline on average.1 IntroductionIn past decades supervised methods achieved thestate-of-the-art in constituency parsing (Collins,2003; Charniak and Johnson, 2005; Petrov et al,2006) and dependency parsing (McDonald et al,2005a; McDonald et al, 2006; Nivre et al, 2006;Nivre et al, 2007; Koo and Collins, 2010).
Forsupervised models, the human-annotated corporaon which models are trained, however, are expen-sive and difficult to build.
As alternative strate-gies, methods which utilize raw texts have been in-vestigated recently, including unsupervised meth-ods which use only raw texts (Klein and Man-ning, 2004; Smith and Eisner, 2005; William etal., 2009), and semi-supervised methods (Koo etal., 2008) which use both raw texts and annotat-ed corpus.
And there are a lot of efforts have alsobeen devoted to bilingual projection (Chen et al,2010), which resorts to bilingual text with one lan-guage parsed, and projects the syntactic informa-tion from the parsed language to the unparsed one(Hwa et al, 2005; Ganchev et al, 2009).In dependency grammar induction, unsuper-vised methods achieve continuous improvementsin recent years (Klein and Manning, 2004; Smithand Eisner, 2005; Bod, 2006; William et al, 2009;Spitkovsky et al, 2010).
Relying on a predefineddistributional assumption and iteratively maximiz-ing an approximate indicator (entropy, likelihood,etc.
), an unsupervised model usually suffers fromtwo drawbacks, i.e., lower performance and high-er computational cost.
On the contrary, bilin-gual projection (Hwa et al, 2005; Smith and Eis-ner, 2009; Jiang and Liu, 2010) seems a promis-ing substitute for languages with alarge amount of bilingual sentences and an exist-ing parser of the counterpart language.
By project-ing syntactic structures directly (Hwa et al, 2005;Smith and Eisner, 2009; Jiang and Liu, 2010)across bilingual texts or indirectly across multi-lingual texts (Snyder et al, 2009; McDonald etal., 2011; Naseem et al, 2012), a better depen-dency grammar can be easily induced, if syntacticisomorphism is largely maintained between targetand source languages.Unsupervised induction and bilingual projec-tion run according to totally different principles,the former mines the underlying structure of themonolingual language, while the latter leveragesthe syntactic knowledge of the parsed counter-1063Bilingual corpus Joint OptimizationBilingually-guidedParsing modelUnsupervisedobjectiveProjectionobjectiveRandomTreebankEvolvedtreebankTargetsentencesSourcesentences projectionFigure 1: Training the bilingually-guided parsing model by iteration.part language.
Considering this, we propose anovel strategy for automatically inducing a mono-lingual dependency grammar under the guidanceof bilingually-projected dependency information,which integrates the advantage of bilingual pro-jection into the unsupervised framework.
Arandomly-initialized monolingual treebankevolves in a self-training iterative procedure, andthe grammar parameters are tuned to simultane-ously maximize both the monolingual likelihoodand bilingually-projected likelihood of the evolv-ing treebank.
The monolingual likelihood is sim-ilar to the optimization objectives of convention-al unsupervised models, while the bilingually-projected likelihood is the product of the projectedprobabilities of dependency trees.
By moderatelyleveraging the dependency information projectedfrom the parsed counterpart language, and simul-taneously mining the underlying syntactic struc-ture of the language considered, we can automat-ically induce a monolingual dependency grammarwhich is much better than previous models onlyusing bilingual projection or unsupervised induc-tion.
In addition, since both likelihoods are fun-damentally factorized into dependency edges (ofthe hypothesis tree), the computational complexi-ty approaches to unsupervised models, while withmuch faster convergence.
We evaluate the finalautomatically-induced dependency parsing mod-el on 5 languages.
Experimental results showthat our method significantly outperforms previ-ous work based on unsupervised method or indi-rect/direct dependency projection, where we seean average improvement of 28.5% over unsuper-vised baseline on all languages, and the improve-ments are 3.9%/3.0% over indirect/direct base-lines.
And our model achieves the most signif-icant gains on Chinese, where the improvementsare 12.0%, 4.5% over indirect and direct projec-tion baselines respectively.In the rest of the paper, we first describe the un-supervised dependency grammar induction frame-work in section 2 (where the unsupervised op-timization objective is given), and introduce thebilingual projection method for dependency pars-ing in section 3 (where the projected optimiza-tion objective is given); Then in section 4 wepresent the bilingually-guided induction strategyfor dependency grammar (where the two objec-tives above are jointly optimized, as shown in Fig-ure 1).
After giving a brief introduction of previ-ous work in section 5, we finally give the experi-mental results in section 6 and conclude our workin section 7.2 Unsupervised Dependency GrammarInductionIn this section, we introduce the unsupervised ob-jective and the unsupervised training algorithmwhich is used as the framework of our bilingually-guided method.
Unlike previous unsupervisedwork (Klein and Manning, 2004; Smith and Eis-ner, 2005; Bod, 2006), we select a self-trainingapproach (similar to hard EM method) to trainthe unsupervised model.
And the framework ofour unsupervised model builds a random treebankon the monolingual corpus firstly for initializationand trains a discriminative parsing model on it.Then we use the parser to build an evolved tree-bank with the 1-best result for the next iterationrun.
In this way, the parser and treebank evolve inan iterative way until convergence.
Let?s introducethe parsing objective firstly:Define ei as the ith word in monolingual sen-tence E; deij denotes the word pair dependency re-lationship (ei ?
ej).
Based on the features arounddeij , we can calculate the probability Pr(y|deij )that the word pair deij can form a dependency arc1064as:Pr(y|deij ) =1Z(deij )exp(?n?n ?
fn(deij , y)) (1)where y is the category of the relationship of deij :y = + means it is the probability that the wordpair deij can form a dependency arc and y = ?means the contrary.
?n denotes the weight for fea-ture function fn(deij , y), and the features we usedare presented in Table 1 (Section 6).
Z(deij) is anormalizing constant:Z(deij ) =?yexp(?n?n ?
fn(deij , y)) (2)Given a sentence E, parsing a dependency treeis to find a dependency tree DE with maximumprobability PE :PE = argmaxDE?deij?DEPr(+|deij ) (3)2.1 Unsupervised ObjectiveWe select a simple classifier objective function asthe unsupervised objective function which is in-stinctively in accordance with the parsing objec-tive:?(?)
=?de?DEPr(+|de)?de?D?EPr(?|de) (4)where E is the monolingual corpus and E ?
E,DE is the treebank that contains all DE in the cor-pus, and D?E denotes all other possible dependen-cy arcs which do not exist in the treebank.Maximizing the Formula (4) is equivalent tomaximizing the following formula:?1(?)
=?de?DElogPr(+|de)+?de?D?ElogPr(?|de)(5)Since the size of edges between DE and D?E isdisproportionate, we use an empirical value to re-duce the impact of the huge number of negativeinstances:?2(?)
=?de?DElogPr(+|de)+ |DE ||D?E |?de?D?ElogPr(?|de)(6)where |x| is the size of x.Algorithm 1 Training unsupervised model1: build random DE2: ??
train(DE , D?E)3: repeat4: for each E ?
E do ?
E step5: DE ?
parse(E,?
)6: ??
train(DE , D?E) ?
M step7: until convergenceBush held talk with Sharonabushi yu juxingshalong huitanle?
???
?
????
??
?Figure 2: Projecting a Chinese dependency treeto English side according to DPA.
Solid arrowsare projected dependency arcs; dashed arrows aremissing dependency arcs.2.2 Unsupervised Training AlgorithmAlgorithm 1 outlines the unsupervised training inits entirety, where the treebank DE and unsuper-vised parsing model with ?
are updated iteratively.In line 1 we build a random treebank DE onthe monolingual corpus, and then train the parsingmodel with it (line 2) through a training proceduretrain(?, ?)
which needs DE and D?E as classifica-tion instances.
From line 3-7, we train the unsu-pervised model in self training iterative procedure,where line 4-5 are similar to the E-step in EM al-gorithm where calculates objective instead of ex-pectation of 1-best tree (line 5) which is parsedaccording to the parsing objective (Formula 3) byparsing process parse(?, ?
), and update the treebank with the tree.
Similar to M-step in EM, thealgorithm maximizes the whole treebank?s unsu-pervised objective (Formula 6) through the train-ing procedure (line 6).3 Bilingual Projection of DependencyGrammarIn this section, we introduce our projection objec-tive and training algorithm which trains the modelwith arc instances.Because of the heterogeneity between dif-ferent languages and word alignment errors, pro-jection methods may contain a lot of noises.
TakeFigure 2 as an example, following the DirectProjection Algorithm (DPA) (Hwa et al, 2005)(Section 5), the dependency relationships betweenwords can be directly projected from the source1065Algorithm 2 Training projection model1: DP , DN ?
proj(F ,DF , A,E)2: repeat ?
train(DP , DN )3: ???
grad(DP , DN , ?(?
))4: ??
climb(?,?
?, ?
)5: until maximizationlanguage to the target language.
Therefore, wecan hardly obtain a treebank with complete treesthrough direct projection.
So we extract projecteddiscrete dependency arc instances instead of tree-bank as training set for the projected grammar in-duction model.3.1 Projection ObjectiveCorrespondingly, we select an objective which hasthe same form with the unsupervised one:?(?)
=?de?DPlog Pr(+|de)+?de?DNlogPr(?|de)(7)where DP is the positive dependency arc instanceset, which is obtained by direct projection methods(Hwa et al, 2005; Jiang and Liu, 2010) and DN isthe negative one.3.2 Projection AlgorithmBasically, the training procedure in line 2,7 of Al-gorithm 1 can be divided into smaller iterativesteps, and Algorithm 2 outlines the training stepof projection model with instances.
F in Algo-rithm 2 is source sentences in bilingual corpus,and A is the alignments.
Function grad(?, ?, ?
)gives the gradient (??)
and the objective is op-timized with a generic optimization step (such asan LBFGS iteration (Zhu et al, 1997)) in the sub-routine climb(?, ?, ?
).4 Bilingually-Guided DependencyGrammar InductionThis section presents our bilingually-guided gram-mar induction model, which incorporates unsuper-vised framework and bilingual projection modelthrough a joint approach.According to following observation: unsuper-vised induction model mines underlying syntacticstructure of the monolingual language, however, itis hard to find good grammar induction in the ex-ponential parsing space; bilingual projection ob-tains relatively reliable syntactic knowledge of theparsed counterpart, but it possibly contains a lotof noises (e.g.
Figure 2).
We believe that unsu-pervised model and projection model can comple-ment each other and a joint model which takes bet-ter use of both unsupervised parse trees and pro-jected dependency arcs can give us a better parser.Based on the idea, we propose a nov-el strategy for training monolingual grammar in-duction model with the guidance of unsuper-vised and bilingually-projected dependency infor-mation.
Figure 1 outlines our bilingual-guidedgrammar induction process in its entirety.
In ourmethod, we select compatible objectives for unsu-pervised and projection models, in order to theycan share the same grammar parameters.
Thenwe incorporate projection model into our iterativeunsupervised framework, and jointly optimize un-supervised and projection objectives with evolv-ing treebank and constant projection informationrespectively.
In this way, our bilingually-guidedmodel?s parameters are tuned to simultaneous-ly maximizing both monolingual likelihood andbilingually-projected likelihood by 4 steps:1.
Randomly build treebank on target sentencesfor initialization, and get the projected arc in-stances through projection from bitext.2.
Train the bilingually-guided grammar induc-tion model by multi-objective optimizationmethod with unsupervised objective and pro-jection objective on treebank and projectedarc instances respectively.3.
Use the parsing model to build new treebankon target language for next iteration.4.
Repeat steps 1, 2 and 3 until convergence.The unsupervised objective is optimized by theloop?
?tree bank?optimized model?new treebank?.
The treebank is evolved for runs.
Theunsupervised model gets projection constraint im-plicitly from those parse trees which contain in-formation from projection part.
The projection ob-jective is optimized by the circulation?
?projectedinstances?optimized model?, these projected in-stances will not change once we get them.The iterative procedure proposed here is not aco-training algorithm (Sarkar, 2001; Hwa et al,2003), because the input of the projection objec-tive is static.10664.1 Joint ObjectiveFor multi-objective optimization method, we em-ploy the classical weighted-sum approach whichjust calculates the weighted linear sum of the ob-jectives:OBJ =?mweightmobjm (8)We combine the unsupervised objective (For-mula (6)) and projection objective (Formula (7))together through the weighted-sum approach inFormula (8):?(?)
= ??2(?)
+ (1 ?
?)?(?)
(9)where ?(?)
is our weight-sum objective.
And ?is a mixing coefficient which reflects the relativeconfidence between the unsupervised and projec-tion objectives.
Equally, ?
and (1??)
can be seenas the weights in Formula (8).
In that case, we canuse a single parameter ?
to control both weightsfor different objective functions.
When ?
= 1 itis the unsupervised objective function in Formula(6).
Contrary, if ?
= 0, it is the projection objec-tive function (Formula (7)) for projected instances.With this approach, we can optimize the mixedparsing model by maximizing the objective in For-mula (9).
Though the function (Formula (9)) isan interpolation function, we use it for traininginstead of parsing.
In the parsing procedure, ourmethod calculates the probability of a dependencyarc according to the Formula (2), while the inter-polating method calculates it by:Pr(y|deij) =?Pr1(y|deij )+ (1 ?
?
)Pr2(y|deij )(10)where Pr1(y|deij ) and Pr2(y|deij ) are the proba-bilities provided by different models.4.2 Training AlgorithmWe optimize the objective (Formula (9)) via agradient-based search algorithm.
And the gradi-ent with respect to ?k takes the form:??
(?k) = ???2(?)?
?k+ (1 ?
?)??(?)?
?k(11)Algorithm 3 outlines our joint training proce-dure, which tunes the grammar parameter ?
simul-taneously maximize both unsupervised objectiveAlgorithm 3 Training joint model1: DP , DN ?
proj(F,DF , A,E)2: build random DE3: ??
train(DP , DN )4: repeat5: for each E ?
E do ?
E step6: DE ?
parse(E,?
)7: ??(?)?
grad(DE, D?E , DP , DN , ?(?
))8: ??climb(?(?),??(?
), ?)
?
M step9: until convergenceand projection objective.
And it incorporates un-supervised framework and projection model algo-rithm together.
It is grounded on the work whichuses features in the unsupervised model (Berg-Kirkpatrick et al, 2010).In line 1, 2 we get projected dependency in-stances from source side according to projec-tion methods and build a random treebank (step1).
Then we train an initial model with projectioninstances in line 3.
From line 4-9, the objective isoptimized with a generic optimization step in thesubroutine climb(?, ?, ?, ?, ?).
For each sentence weparse its dependency tree, and update the tree intothe treebank (step 3).
Then we calculate the gra-dient and optimize the joint objective according tothe evolved treebank and projected instances (step2).
Lines 5-6 are equivalent to the E-step of theEM algorithm, and lines 7-8 are equivalent to theM-step.5 Related workThe DMV (Klein and Manning, 2004) is a single-state head automata model (Alshawi, 1996) whichis based on POS tags.
And DMV learns the gram-mar via inside-outside re-estimation (Baker, 1979)without any smoothing, while Spitkovsky et al(2010) utilizes smoothing and learning strategyduring grammar learning and William et al (2009)improves DMV with richer context.The dependency projection method DPA (H-wa et al, 2005) based on Direct CorrespondenceAssumption (Hwa et al, 2002) can be describedas: if there is a pair of source words with a de-pendency relationship, the corresponding alignedwords in target sentence can be considered as hav-ing the same dependency relationship equivalent-ly (e.g.
Figure 2).
The Word Pair Classification(WPC) method (Jiang and Liu, 2010) modifies theDPA method and makes it more robust.
Smithand Eisner (2009) propose an adaptation methodfounded on quasi-synchronous grammar features1067Type Feature TemplateUnigram wordi posi wordi ?
posiwordj posj wordj ?
posjBigram wordi ?
posj wordj ?
posi posi ?
posjwordi ?
wordj wordi ?
posi ?
wordj wordi ?
wordj ?
posjwordi ?
posi ?
posj posi ?
wordj ?
posjwordi ?
posi ?
wordj ?
posjSurrounding posi?1 ?
posi ?
posj posi ?
posi+1 ?
posj posi ?
posj?1 ?
posjposi ?
posj ?
posj+1 posi?1 ?
posi ?
posj?1 posi ?
posi+1 ?
posj+1posi?1 ?
posj?1 ?
posj posi+1 ?
posj ?
posj+1 posi?1 ?
posi ?
posj+1posi ?
posi+1 ?
posj?1 posi?1 ?
posj ?
posj+1 posi+1 ?
posj?1 ?
posjposi?1 ?
posi ?
posj?1 ?
posj posi ?
posi+1 ?
posj ?
posj+1posi ?
posi+1 ?
posj?1 ?
posj posi?1 ?
posi ?
posj ?
posj+1Table 1: Feature templates for dependency parsing.
For edge deij : wordi is the parent word and wordjis the child word, similar to ?pos?.
?+1?
denotes the preceding token of the sentence, similar to ?-1?.for dependency projection and annotation, whichrequires a small set of dependency annotated cor-pus of target language.Similarly, using indirect information from mul-tilingual (Cohen et al, 2011; Ta?ckstro?m et al,2012) is an effective way to improve unsupervisedparsing.
(Zeman and Resnik, 2008; McDonald etal., 2011; S?gaard, 2011) employ non-lexicalizedparser trained on other languages to process atarget language.
McDonald et al (2011) adaptstheir multi-source parser according to DCA, whileNaseem et al (2012) selects a selective sharingmodel to make better use of grammar informationin multi-sources.Due to similar reasons, many works are devotedto POS projection (Yarowsky et al, 2001; Shen etal., 2007; Naseem et al, 2009), and they also suf-fer from similar problems.
Some seek for unsu-pervised methods, e.g.
Naseem et al (2009), andsome further improve the projection by a graph-based projection (Das and Petrov, 2011).Our model differs from the approaches abovein its emphasis on utilizing information from bothsides of bilingual corpus in an unsupervised train-ing framework, while most of the work above onlyutilize the information from a single side.6 ExperimentsIn this section, we evaluate the performance of theMST dependency parser (McDonald et al, 2005b)which is trained by our bilingually-guided modelon 5 languages.
And the features used in our ex-periments are summarized in Table 1.6.1 Experiment SetupDatasets and Evaluation Our experiments arerun on five different languages: Chinese(ch),Danish(da), Dutch(nl), Portuguese(pt) andSwedish(sv) (da, nl, pt and sv are free data setsdistributed for the 2006 CoNLL Shared Tasks(Buchholz and Marsi, 2006)).
For all languages,we only use English-target parallel data: we takethe FBIS English-Chinese bitext as bilingual cor-pus for English-Chinese dependency projectionwhich contains 239K sentence pairs with about8.9M/6.9M words in English/Chinese, and forother languages we use the readily available datain the Europarl corpus.
Then we run tests on thePenn Chinese Treebank (CTB) and CoNLL-X testsets.English sentences are tagged by the implemen-tations of the POS tagger of Collins (2002), whichis trained on WSJ.
The source sentences are thenparsed by an implementation of 2nd-ordered MSTmodel of McDonald and Pereira (2006), which istrained on dependency trees extracted from PennTreebank.As the evaluation metric, we use parsing accu-racy which is the percentage of the words whichhave found their correct parents.
We evaluate onsentences with all length for our method.Training Regime In experiments, we use theprojection method proposed by Jiang and Liu(2010) to provide the projection instances.
Andwe train the projection part ?
= 0 first for initial-ization, on which the whole model will be trained.Availing of the initialization method, the modelcan converge very fast (about 3 iterations is suffi-cient) and the results are more stable than the onestrained on random initialization.Baselines We compare our method againstthree kinds of different approaches: unsupervisedmethod (Klein and Manning, 2004); single-source direct projection methods (Hwa et al,2005; Jiang and Liu, 2010); multi-source in-direct projection methods with multi-sources (M-106860.061.5ch50.351.2da59.560.5accuracy%nl70.574.5pt61.565.00.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1alphasvFigure 3: The performance of our model with re-spect to a series of ratio ?cDonald et al, 2011; Naseem et al, 2012).6.2 ResultsWe test our method on CTB and CoNLL-X freetest data sets respectively, and the performance issummarized in Table 2.
Figure 3 presents the per-formance with different ?
on different languages.Compare against Unsupervised Baseline Ex-perimental results show that our unsupervisedframework?s performance approaches to the DMVmethod.
And the bilingually-guided model canpromote the unsupervised method consisten-cy over all languages.
On the best results?
aver-age of four comparable languages (da, nl, pt, sv),the promotion gained by our model is 28.5% overthe baseline method (DMV) (Klein and Manning,2004).Compare against Projection Baselines Forall languages, the model consistent-ly outperforms on direct projection baseline.On the average of each language?s best result, ourmodel outperforms all kinds of baselines, yielding3.0% gain over the single-source direct-projectionmethod (Jiang and Liu, 2010) and 3.9% gain overthe multi-source indirect-projection method (Mc-Donald et al, 2011).
On the average of all resultswith different parameters, our method also gain-s more than 2.0% improvements on all baselines.Particularly, our model achieves the most signif-icant gains on Chinese, where the improvementsare 4.5%/12.0% on direct/indirect projection base-Accuracy%Model ch da nl pt sv avgDMV 42.5?
33.4 38.5 20.1 44.0 ?.
?DPA 53.9 ?.?
?.?
?.?
?.?
?.
?WPC 56.8 50.1 58.4 70.5 60.8 59.3Transfer 49.3 49.5 53.9 75.8 63.6 58.4Selective 51.2 ?.?
55.9 73.5 61.5 ?.
?unsuper 22.6 41.6 15.2 45.7 42.4 33.5avg 61.0 50.7 59.9 72.0 63.1 61.3max 61.3 51.1 60.1 74.2 64.6 62.3Table 2: The directed dependency accuracy withdifferent parameter of our model and the base-lines.
The first section of the table (row 3-7)shows the results of the baselines: a unsupervisedmethod baseline (Klein and Manning, 2004)(D-MV); a single-source projection method baseline(Hwa et al, 2005) (DPA) and its improve-ment (Jiang and Liu, 2010)(WPC); two multi-source baselines (McDonald et al, 2011)(Trans-fer) and (Naseem et al, 2012)(Selective).
Thesecond section of the table (row 8) presents theresult of our unsupervised framework (unsuper).The third section gives the mean value (avg) andmaximum value (max) of our model with different?
in Figure 3.
*: The result is based on sentences with 10words or less after the removal of punctuation, itis an incomparable result.lines.The results in Figure 3 prove that our unsuper-vised framework ?
= 1 can promote the grammarinduction if it has a good start (well initialization),and it will be better once we incorporate the infor-mation from the projection side (?
= 0.9).
Andthe maximum points are not in ?
= 1, which im-plies that projection information is still availablefor the unsupervised framework even if we employthe projection model as the initialization.
So wesuggest that a greater parameter is a better choicefor our model.
And there are some random factorsin our model which make performance curves withmore fluctuation.
And there is just a little improve-ment shown in da, in which the same situation isobserved by (McDonald et al, 2011).6.3 Effects of the Size of Training CorpusTo investigate how the size of the training corpusinfluences the result, we train the model on ex-tracted bilingual corpus with varying sizes: 10K,50K, 100K, 150K and 200K sentences pairs.As shown in Figure 4, our approach continu-1069535455565758596061626310K 50K 100K 150K 200Kaccuracy%size of training setour modelbaselineFigure 4: Performance on varying sizes (averageof 5 languages, ?
= 0.9)515253545556575859606162630  0.05  0.1  0.15  0.2  0.25  0.3  0.35accuracy%noise rateour modelbaselineFigure 5: Performance on different projectionquality (average of 5 languages, ?
= 0.9).
Thenoise rate is the percentage of the projected in-stances being messed up.ously outperforms the baseline with the increasingsize of training corpus.
It is especially noteworthythat the more training data is utilized the more su-periority our model enjoys.
That is, because ourmethod not only utilizes the projection informa-tion but also avails itself of the monolingual cor-pus.6.4 Effect of Projection QualityThe projection quality can be influenced by thequality of the source parsing, alignments, projec-tion methods, corpus quality and many other fac-tors.
In order to detect the effects of varying pro-jection qualities on our approach, we simulate thecomplex projection procedure by messing up theprojected instances randomly with different noiserates.
The curves in Figure 5 show the perfor-mance of WPC baseline and our bilingual-guidedmethod.
For different noise rates, our model?s re-sults consistently outperform the baselines.
Whenthe noise rate is greater than 0.2, our improvement49.5...54.6...58.258.659.059.459.860.20 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3accuracy%alphaour modelbaseline(58.5)Figure 6: The performance curve of our model(random initialization) on Chinese, with respect toa series of ratio ?.
The baseline is the result ofWPC model.increases with the growth of the noise rate.
The re-sult suggests that our method can solve some prob-lems which are caused by projection noise.6.5 Performance on Random InitializationWe test our model with random initialization ondifferent ?.
The curve in Figure 6 shows the per-formance of our model on Chinese.The results seem supporting our unsupervisedoptimization method when ?
is in the range of(0, 0.1).
It implies that the unsupervised structureinformation is useful, but it seems creating a nega-tive effect on the model when ?
is greater than 0.1.Because the unsupervised part can gain constraintsfrom the projection part.
But with the increase of?, the strength of constraint dwindles, and theunsupervised part will gradually lose control.
Andbad unsupervised part pulls the full model down.7 Conclusion and Future WorkThis paper presents a bilingually-guided strate-gy for automatic dependency grammar induction,which adopts an unsupervised skeleton and lever-ages the bilingually-projected dependency infor-mation during optimization.
By simultaneous-ly maximizing the monolingual likelihood andbilingually-projected likelihood in the EM proce-dure, it effectively integrates the advantages ofbilingual projection and unsupervised induction.Experiments on 5 languages show that the novelstrategy significantly outperforms previous unsu-pervised or bilingually-projected models.Since its computational complexity approaches tothe skeleton unsupervised model (with much few-er iterations), and the bilingual text aligned to1070resource-rich languages is easy to obtain, such ahybrid method seems to be a better choice for au-tomatic grammar induction.
It also indicates thatthe combination of bilingual constraint and unsu-pervised methodology has a promising prospectfor grammar induction.
In the future work we willinvestigate such kind of strategies, such as bilin-gually unsupervised induction.AcknowledgmentsThe authors were supported by NationalNatural Science Foundation of China, Con-tracts 61202216, 863 State Key Project (No.2011AA01A207), and National Key TechnologyR&D Program (No.
2012BAH39B03), KeyProject of Knowledge Innovation Program of Chi-nese Academy of Sciences (No.
KGZD-EW-501).Qun Liu?s work is partially supported by ScienceFoundation Ireland (Grant No.07/CE/I1142) aspart of the CNGL at Dublin City University.
Wewould like to thank the anonymous reviewers fortheir insightful comments and those who helpedto modify the paper.ReferencesH.
Alshawi.
1996.
Head automata for speech transla-tion.
In Proc.
of ICSLP.James K Baker.
1979.
Trainable grammars for speechrecognition.
The Journal of the Acoustical Societyof America, 65:S132.T.
Berg-Kirkpatrick, A.
Bouchard-Co?te?, J. DeNero,and D. Klein.
2010.
Painless unsupervised learn-ing with features.
In HLT: NAACL, pages 582?590.Rens Bod.
2006.
An all-subtrees approach to unsu-pervised parsing.
In Proc.
of the 21st ICCL and the44th ACL, pages 865?872.S.
Buchholz and E. Marsi.
2006.
Conll-x shared taskon multilingual dependency parsing.
In Proc.
of the2002 Conference on EMNLP.
Proc.
CoNLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative r-eranking.
In Proc.
of the 43rd ACL, pages 173?180,Ann Arbor, Michigan, June.W.
Chen, J. Kazama, and K. Torisawa.
2010.
Bi-text dependency parsing with bilingual subtree con-straints.
In Proc.
of ACL, pages 21?29.S.B.
Cohen, D. Das, and N.A.
Smith.
2011.
Unsu-pervised structure prediction with non-parallel mul-tilingual guidance.
In Proc.
of the Conference onEMNLP, pages 50?61.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proc.
of the2002 Conference on EMNLP, pages 1?8, July.Michael Collins.
2003.
Head-driven statistical mod-els for natural language parsing.
In ComputationalLinguistics.D.
Das and S. Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proc.
of ACL.K.
Ganchev, J. Gillenwater, and B. Taskar.
2009.
De-pendency grammar induction via bitext projectionconstraints.
In Proc.
of IJCNLP of the AFNLP: Vol-ume 1-Volume 1, pages 369?377.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.Evaluating translational correspondence using anno-tation projection.
In Proc.
of ACL, pages 392?399.R.
Hwa, M. Osborne, A. Sarkar, and M. Steedman.2003.
Corrected co-training for statistical parsers.In ICML-03 Workshop on the Continuum from La-beled to Unlabeled Data in Machine Learning andData Mining, Washington DC.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, andO.
Kolak.
2005.
Bootstrapping parsers via syntacticprojection across parallel texts.
Natural languageengineering, 11(3):311?325.W.
Jiang and Q. Liu.
2010.
Dependency parsingand projection based on word-pair classification.
InProc.
of ACL, pages 12?20.D.
Klein and C.D.
Manning.
2004.
Corpus-based in-duction of syntactic structure: Models of dependen-cy and constituency.
In Proc.
of ACL, page 478.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of the 48th ACL,pages 1?11, July.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
pages 595?603.R.
McDonald and F. Pereira.
2006.
Online learningof approximate dependency parsing algorithms.
InProc.
of the 11th Conf.
of EACL.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProc.
of ACL, pages 91?98.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.2005b.
Non-projective dependency parsing using s-panning tree algorithms.
In Proc.
of EMNLP, pages523?530.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Mul-tilingual dependency analysis with a two-stage dis-criminative parser.
In Proc.
of CoNLL, pages 216?220.1071R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.In Proc.
of EMNLP, pages 62?72.
ACL.T.
Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.2009.
Multilingual part-of-speech tagging: Two un-supervised approaches.
Journal of Artificial Intelli-gence Research, 36(1):341?385.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective sharing for multilingual dependencyparsing.
In Proc.
of the 50th ACL, pages 629?637,July.J.
Nivre, J.
Hall, J. Nilsson, G. Eryig?it, and S. Mari-nov. 2006.
Labeled pseudo-projective dependencyparsing with support vector machines.
In Proc.
ofCoNLL, pages 221?225.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(02):95?135.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of the 21st ICCL& 44th ACL, pages 433?440, July.A.
Sarkar.
2001.
Applying co-training methods to sta-tistical parsing.
In Proc.
of NAACL, pages 1?8.L.
Shen, G. Satta, and A. Joshi.
2007.
Guided learningfor bidirectional sequence classification.
In AnnualMeeting-, volume 45, page 760.N.A.
Smith and J. Eisner.
2005.
Contrastive estima-tion: Training log-linear models on unlabeled data.In Proc.
of ACL, pages 354?362.D.A.
Smith and J. Eisner.
2009.
Parser adapta-tion and projection with quasi-synchronous gram-mar features.
In Proc.
of EMNLP: Volume 2-Volume2, pages 822?831.B.
Snyder, T. Naseem, and R. Barzilay.
2009.
Unsu-pervised multilingual grammar induction.
In Proc.of IJCNLP of the AFNLP: Volume 1-Volume 1, pages73?81.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Proc.of the 49th ACL: HLT, pages 682?686.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2010.
From baby steps to leapfrog: How?less is more?
in unsupervised dependency parsing.In HLT: NAACL, pages 751?759, June.O.
Ta?ckstro?m, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer of lin-guistic structure.William, M. Johnson, and D. McClosky.
2009.
Im-proving unsupervised dependency parsing with rich-er contexts and smoothing.
In Proc.
of NAACL,pages 101?109.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In Proc.
of HLT,pages 1?8.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In Proc.
of the IJCNLP-08.
Proc.
CoNLL.Ciyou Zhu, Richard H Byrd, Peihuang Lu, and JorgeNocedal.
1997.
Algorithm 778: L-bfgs-b: Fortransubroutines for large-scale bound-constrained opti-mization.
ACM Transactions on Mathematical Soft-ware (TOMS), 23(4):550?560.1072
