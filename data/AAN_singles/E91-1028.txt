Generating Referring Expressions Involving RelationsRobert  DaleDepar tment  of Artif icial Intell igenceand Centre for Cognit ive ScienceUniversity of Ed inburghEd inburgh EH8 9LWScot landR.
Dale~uk.
ac.
edinburghAbstractIn this paper, we review Dale's \[1989\] algorithmfor determining the content of a referring expres-sion.
The algorithm, which only permits the useof one-place predicates, is revised and extendedto deal with n-ary predicates.
We investigate theproblem of blocking 'recursion' in complex nounphrases and propose a solution in the context ofour algorithm.IntroductionIn very simple language generation systems, thereis typically a one-to-one relationship between en-tities known to the system and the linguistic formsavailable for describing those entities; in effect,each entity has a canonical name.
In such sys-tems, deciding upon the form of reference r quiredin a given context requires at most choosing be-twc(,n a pronoun and the canonical name.
1As soon as a generation system has access to aknowledge base which contains richer knowledgeabout the entities in the domain, the system hasto face the problem of deciding what particularproperties of an entity should be used in describ-ing it in a given context?
Producing a descrip-tion which includes all of the known propertiesof Lhe entity is likely to be both inefficient andt~.Ve do not  mean to imply,  of  course,  that  the  de-c is ion as to whether  or not  to use a pronoun is s imple .2Th is  p rob lem ex is ts  qu i te  independent ly  of  anycons iderat ions  of  the  d i f ferent  perspect ives  thatmight  be taken  upon  an ent i ty ,  where,  for examplepone ent i ty  can  be viewed f rom the  perspect ive  of  be-ing a fa ther ,  a b icyc l is t  and  a teacher ,  w i th  separatec lus ters  of  p roper t ies  in each case.
Even  if the  sys temis res t r i c ted  to a s ing le  perspect ive  upon  each ent i ty(as a lmost  all l anguage  generat ion  sys tems are) ,  in anysoph is t i ca ted  knowledge  base  there  will sti l l  be morein fo rmat ion  ava i lab le  about  the  ent i ty  than  it is sen-s ible to inc lude in a descr ip t ion .Nicholas HaddockHewlett  Packard Laborator iesF i l ton RoadStoke GiffordBristol Bs l2  6QZEnglandnjh@com, hp.
hpl.
hplbmisleading.The core of the problem is finding a way of de-scribing the intended referent hat distinguishesit from other potential referents with which itmight be confused.
We refer to this problem asthe content determinat ion task.
In this paper,we point out some limitations in an earlier solu-tion proposed in Dale \[1988, 1989\], and discussthe possibilites of extending this solution by in-corporating a use of constraints motivated by thework of Haddock \[1987, 1988\].Generating ReferringExpressionsThe Pr inc ip les  o f  Re ferenceDale \[1988, 1989\] presents a solution to the con-tent determination task which is motivated bythree principles of refcrence.
These are cssen-tinily Gricean conversational maxims rephrasedfrom the perspective of generating referring ex-pressions:1.
The principle of sensit ivity states that thereferring expression chosen should take accountof the state af the hearer's knowledge.2.
The principle of adequacy states that thereferring expression chosen should be sufficientto identify the intended referent.3.
The principle of efficiency states that thereferring expression chosen should provide nomore information than is necessary for the iden-tification of the intended referent.The solution proposed in Dale \[1988, 1989\] fo-cuses on the second and third of these principlesof reference as constraints on the content deter-mination task.161 -Dis t ingu ish ing  Descr ip t ionsOther researchers ( ee, for example, \[Davey 1978;Appclt 1985a\]) have suggested that the processol: determining the content of a referring expres-sion should be governed by principles like thosejust described.
Detailed algorithms for satisfyingthese requirements are rarely provided, however.Suppose that we have a set of entities C (calledthe context  set)  such that C = {a l ,a2 , .
.
.
,an}and our task is to distinguish from this context setsome intended referent r where r E C. Suppose,also, that each entity ak is described in the sys-tem's knowledge base by means of a set of prop-ertics, pk~, Pk2, ?
?
?, Pk,,.In order to distinguish our intended referent rfrom the other entities in C, we need to find someset of properties which are together true of r, butof no other entity in C. 3 The linguistic realisa-tion of this set of properties constitutes a d ist in-gu ish ing  descr ip t ion  (DD) of r with respect othe context C. A min ima l  d i s t ingu ish ing  de-sc r ip t ion  is then the linguistic realisation of thesmallest such set of properties.An  A lgor i thm to  ComputeD is t ingu ish ing  Descr ip t ionsI,eL Lr be the set of properties to be realised inour description; and let t~ be the set of proper-tics known to be true of our intended referent r(we assume that Dr is non-empty).
The initialconditions are thus as follows:?
C,.
= {(all entities in the knowledge base)};?
Pr = {(all properties true of r)};?In order to describe the intended referent r withrespect o the context set Cr, we do the following:1.
Check Successif \[Cr I = 1 then  return Lr as a DDelsei f  Pr = 0 then  return Lr as a non-DDelse goto  Step 2."2.
Choose Propertyfor each  Pi E P~ do: Cr, ~-- C~ f3 {x\]pi(x)}Chosen property is pj, where Crj is the small-(;st se t fgoto  Step 3.3A sirnilar approach is being pursued by Leavitt(personal communication) at CMU.4In the terminology of Dale \[1988, 1989\], this isequivalent o finding the property with the greatestdiscriminatory power..
Extend Description (wrt the chosen pj)Lr *-- L rU {pj}P~ *-- Pr - {Pj}goto  Step 1.If we have a distinguishing description, a definitedeterminer can be used, since the intended refer-ent is described uniquely in context.
If the resultis:a non-distinguishing description, all is not lost:we can realise the description by means of a nounphrase of the form one of the Xs, where X is therealisation of the properties in Lr.
5 For simplic-ity, the remainder of this paper concentrates onthe generation of distinguishing descriptions only;the extended algorithm presented later will sim-ply fail if it is not possible to produce a DD.The abstract process described above requiressome slight modifications before it can be usedeffectively for noun phrase generation.
In partic-ular, we should note that, in noun phrases, thehead noun typically appears even in cases whereit does not have any discriminatory power.
Forexample, suppose there are six entities on a table,all of which are cups although only one is red: weare then likely to describe that particular cup asas the red cup rather than simply the red or thered thing.
Thus, in order to implement the abovealgorithm, we always first add to L that propertyof the entity that would typically be denoted bya head noun.
?
In many cases, this means that nofurther properties need be added.Note also that Step 2 of our algorithm is non-deterministic, in that several properties may inde-pendently ield a context set of the same minimalsize.
For simplicity, we assume that one of theseequally viable properties is chosen at random.Some Prob lemsThere are some problems with the algorithm justdescribed.As Reiter \[1990:139\] has pointed out, the algo-rithm does not guarantee to find a minimal dis-tinguishing description: this is equivalent o theminimal set cover problem and is thus intractableas stated.Second, the mechanism doesn't necessarily pro-duce a useful description: consider the exampleSOne might be tempted to suggest hat a straight-forward indefinite, as in an X, could be used in suchcases; this is typically not what people do, however.SFor simplicity, we can assume that this is thatproperty of the entity that would be denoted by whatP~sch \[1978\] calls the entity's basic category.162 -offered by Appelt \[1985b:6\], where a speaker tellsa hearer (whom she has just met on the bus)which bus stop to get off at by saying Get off onestop before I do.
This may be a uniquely iden-tifying description of the intended referent, butit is of little use without a supplementary offerto indicate the stop; ultimately, we require somecomputational treatment of the Principle of Sen-sitivity here.Third, as has been demonstrated by work inpsycholinguistics (for a recent summary, see Lev-elt \[1989:129-13d\]), the algorithm does not rep-resent what people seem to do when construct-ing a referring expression: in particular, peopletypically produce referring expressions which areredundant (over and above the inclusion of thehead noun as discussed above).
This fact can, ofcourse, be taken to nullify the impact of the firstproblem described above.We do not intend to address any of these prob-lems in the present paper.
Instead, we consider anextension of our basic algorithm to deal with rela-tions, and focus on an orthogonal problem whichbesets any algorithm for generating DDS involvingrelations.Relat ions and the Problem of'Recursion'Suppose that our knowledge base consists of a setof facts, as follows:{cup(c\]), cup(c2), cup(c3), bowl(bx), bowl(b2),table(t\]), table(t2), floor(I\] ), in(cl, bl),in(c2, b2), on(c3, fl), on(b\], fl), on(b2, Q),on(t \ ] , f l ) ,on(t~,f l )}Thus we have three cups, two bowls, two tablesand a floor: Cup c\] is in bowl bl, and bowl b\]is on the floor, as are the tables and cup ca; andso on.
The algorithm described above deals onlywith one-place predicates, and says nothing aboutusing relations such as on(bl,fl) as part of adistinguishing description.
How can we extendtile basic algorithm to handle relations?
It turnsout that this is not as simple as it might seem:problems arise because of the potential for infiniteregress in the construction of the description.A natural strategy to adopt for generating ex-prcssions with relations is that used by Appelt\[1985a:108-112\].
For example, to describe theentity c3, our planner might determine that thepredicate to be realized in our referring expres-sion is the abstraction Ax\[cup(x)Aon(x, f l ) \ ] ,  sincethis complex predicate is true of only one entity,namely ca.
In Appelt's TELEGRAM, this resultsfirst in the choice of the head noun cup, followedby a recursive call to the planner to determinehow fl  should be described.
The resulting nounphrase is then the cup on the floor.In many cases this approach will do what isrequired.
However, in certain situations, it willattempt o describe a referent in terms of itselfand generate an infinite description.For example, consider a very specific instanceof the problem, which arises in a scenario f thekind discussed in Haddock \[1987, 19881 from theperspective of interpretation.
Such a scenario ischaracterised in the above knowledge base: wehave two bowls and two tables, and one of thebowls is on one of the tables.
Given this situa-tion, it is felicitous to refer to b~ as the bowl onthe table.
However, the use of the definite arti-cle in the embedded NP the table poses a problemfor purely compositional pproaches to interpre-tation, which would expect he embedded NP torefer uniquely in isolation.Naturally, this same scenario will be problem-atic for a purely compositional pproach to gen-eration of the kind alluded to at the beginning ofthis section.
Taken literally, this algorithm couldgenerate an infinite NP, such as: zthe  bowl on the table which supports the bowlon the  tab le  which supports ...Below, we present an algorithm for generatingrelational descriptions which deals with this spe-cific instance of the problem of repetition.
Had-dock \[1988\] observes the problem can be solvedby giving both determiners scope over the entireNP, thus:(3tx)(:l!y)bowl(m) A on(x, y) A table(y)In Haddock's model of interpretation, this treat-ment falls out of a scheme of incremental, left-to-right reference valuation based on an incremen-tal accumulation of constraints.
Our generationalgorithm follows Haddock \[1988\], and Mellish\[1985\], in using constraint-network consistency todetermine the entities relating to a description(see Mackworth \[1977\]).
This is not strictly nec-essary, since any evaluation procedure such asgenerate-and-test or backtracking, can producethe desired result; however, using network consis-tency provides a natural evolution of the existingalgorithm, since this already models the problemin terms of incremental refinement ofcontext sets.
?We Ignore the question of determiner choice in thepresent paper, and assume for simplicity that definitedeterminers are chosen here.- 163  -We conclude the paper by investigating the im-plications of our approach for the more generalproblem of recursive repetition.A Const ra in t -Based  A lgor i thmData  St ructuresWe assume three global kinds of data structure.1.
The Referent  S tack  is a stack of referents weare trying to describe.
Initially this stack is setto contain just the top-level referent: s\[Describe(b2, x)\]This means that the goal is to describe the ref-erent b2 in terms of predicates over the variableX.2.
The Proper ty  Set for the intended referentr is the set of facts, or predications, in theknowledge base relating to r; we will notatethis as Pr.
For example, given the knowledgebase introduced in the previous section, thefloor f l  has the following Property Set:PA = {floor(f1), on(e3,/1), on(b1,/1),on(tl, f l) ,  on(t2, f l )  }3.
A Const ra in t  Network  N will be viewed ab-stractly as a pair consisting of (a) a set of con-straints, which corresponds to our descriptionL, and (b) the context sets for the variablesmentioned in L. The following is an exampleof a constraint network, viewed in these terms:i.
(x, u)},{c: = {ca, = {bl, b2}\])The A lgor i thmFor brevity, our algorithm uses the notation N~pto signify the result of adding the constraint pto the network N. Whenever a constraint p isadded to a network, assume the following actionsoccur: (a) p is added to the set of constraintsL; and (b) the context sets for variables in L arerefined until their values are consistent with thenew constraint.
9 Assume that every variable is~\Ve represent  he  s tack  here as a list, w i th  the  topof  the  s tack  be ing  the  le f t -most  i tem in the  list.9We do not  address  the  degree of network  consis-tency  requ i red  by our  a lgor i thm.
However,  for theexamples  t reated  in th is  paper ,  a node and  arc  con-s i s tcncy  a lgor i thm,  such  as Mackwor th ' s  \[1977\] AC-3, will suffice.
(Haddock  \[1991\] invest igates  the  suffi-c iency of  such  low-power  techn iques  for noun  phrasein terpreta t ion . )
We assume that  our  a lgor i thm han-dles constants  as well as var iab les  w i th in  const ra in ts .initially associated with a context set containingall entities in the knowledge base.In addition, we use the notation It\r ip to sig-nify the result of replacing every occurence of theconstant r in p by the variable v. For instance,\[c3\x\]on(c3, f l )  = on(x, f l )The initial conditions are as follows:?
Stack = \[Describe(r,v)\]?
Pr = {(all facts true of r)}?
N = ({}, \[C. = {(all entities)}\])Thus, initially there are no properties in L. Asbefore, the problem of finding a description L in-volves three steps which are repeated until a suc-cessful description has been constructed:1.
We first check whether the description we haveconstructed so far is successful in picking outthe intended referent.2.
If the description is not sufficient to pick outthe intended referent, we choose the most use-ful fact that will contribute to the description.3.
We then extend the description with a con-straint representing this fact, and add Describegoals for any constants relating to the con-straint.The essential use of constraints occurs in Step 2and 3; the detail of the revised algorithm is shownin Figure 1.An  ExampleThere is insufficient space to go through an exam-ple in detail here; however, we summarise somesteps for the problematic ase of referring to b2 asthe the bowl on the table.
1?
For simplicity here,we assume our algorithm will always choose thehead category first.
Thus, we have the followingconstraint network after one iteration through thealgorithm:N = ({bowl(x)}, \[Cx = {bl, b~}\])Let  us suppose that the second iteration chooseson(b2, t l)  as the predication with which to extendour description.
When integrated into the con-straint network, we havel ?Aga in ,  we ignore the  quest ion  of  determiner  choiceand  assume def in i tes  are chosen.- 164  -Note that in Steps 1, 2 and 3, r and v relate to thecurrent Describe(r, v) on top of the stack.1.
Check Successif Stack is empty then  return L as a rODelseif  ICy\] = 1 then  pop Stack &goto  Step 1elseif  Pr = ~ then  \[aftelse goto  Step 22.
Choose Propertyfor each propert,y Pi E P,- dop' ~-\[r \ ,vb,N, ,-- N (2)I",('bosch predicatiou is Pa, where Nj containsthe smallest sew C,, for v.goto  Step 33.
I':xtcnd l)escriptio,~ (w.r.t.
the chosen p)1',.
~-  1'~ - {p}t, , - \ [ r \~bfor  every t)thcr cor lstant  r '  in p doassoc iate  r '  with a new, unique variable v'~) ~- \ [ / \v 'bpush Describe('r', v') onto Stackinitialisc a sct 1~, of facts true of r':'V ,-- N ?
pgoto Step 1I,'igure 1: A Constraint-l{ased AlgorithmI1\; = ({bowl (x ) ,on(x ,  y )} ,\[C= = {b,,b,~},C~ = {/1 ,h}\ ] )Note that the network has determined a set forg which does not include the second table t2 be-ca.llse it is not known to support anything.
(liven our head-category-first strategy, the thirditcratiorl through the algorithm adds table(t1) asa coI istra int  to N,  to form l,h(; new networkA' = ({ bowl(x), on(x, y), table(y)},\[C, = {b~},C~ = {t ,} \ ] )Ahcr adding this new constraint, f l  is eliminatedI'rt)nl ~y.
This leads to the revision of to Cx,which must remove every vahm which is not onI i .On the fourth iteration, we exit with the firstcorn p,ment of this network, L, as our description;w(: can then realize this content as the bowl onll., lath'.The Prob lem Rev is i tedThe task of referring to b2 in our knowledge baseis something of a special case, and does not illus-trate the nature of the general problem of recur-sion.
Consider the task of referring to el.
Dueto the non-determinism in Step 2, our algorithmmight either generate the DD corresponding to thecup in the bowl on the floor, or it might insteadget into an infinite loop corresponding to the cupin the bowl containing the cup in the bowl con-taining .
.
.
The initial state of the referent slackand O's property set will be:Stack = \[Describe(cl,z)\]P~ = {cup(o), in(cl ,bi)}At the beginning of the fourth iteration the al-gorithm will have produced a partial descriptioncorresponding to the cup in the bowl, with thetop-level goal to uniquely distinguish bl:Stack = \[Describe(bt,y), Describe(ca,x)\]Pc, = OPb, = { in (o ,b l ) ,on(b l , f l )}N = ({cup(x),in(x,y),bowl(y)},to= = {cl,o~},C~ = {b, ,b~) \ ] )Step 2 of the fourth iteration computes two net-works, for the two facts in Pb, :Nl = N ~ in(o ,y )= ({cup(x), in(x,y), bowl(y),in(cl, y)},\[c~ = {cx }, c~ = {b, }l)N2 = N ~on(y ,  f l )= ({cup(x), in(x, y), bowl(y), on(y, f l )} ,\[c= = {c,},c, = {b,}\]>Since both networks yield singleton sets lbr Cu,the algorithm might choose the property in(el, bl).This means extending the current description witha constraint in(z,y), and stacking an additionalcommitment to describe cl in terms of the vari-able z.
Hence at the end of the fourth iteration,the algorithm is in the stateStack = \[Oescribe(cl,z),Describe(bl,y),Describe(o, x)\]P~,, = 0ebl = {on(bl, f l )}Pea.
= {cup(o) , in (c l ,b , )}N = ({cup(x), in(x,y),bowl(y), in(z,y)},\[...\])and may continue to loop in this manner.The general problem of inlinite repetition hasbeen noted before in the generation literature.For example, Novak \[1988:83\] suggests that- 165  -\[i\]f a two-place predicate is lined to generate therc.~trictive relative clause, the second object ofthis predicate ischaracterized simply by its prop-crties to avoid recursivc reference as in the carwhich was overtaken by the truck which overtookthe car.Davey \[1979\], on the other hand, introducesthe notion of a CANLIST (the Currently ActiveNode List) for those entities which have alreadybeen mentioned in the noun phrase currently un-der construction.
The generator is then prohib-ited from describing an cntity in tetras of entitiesalready in the CANLIST.in the general case, these proposals appear tob(: too strong.
Davey's restriction would seemt.o b(: the weaker of the two, but if taken liter-ally, it will nevertheless prevent legitimate casesof bound-variable anaphora within an NP, such asthe mani who ale the cake which poisoned himi.We suggest he following, possibly more generalheuristic: do not express a given piece of infor-mation more than once within the same NP.
Forour simplified representation f contextual knowl-c.dgc, exernplified above, we could encode thisheuristic by stipulating that any fact in the knowl-edge base can only be chosen once within a givencall to the algorithm.
So in the above example,once the relation in(el, bl) has been chosen fromthe initial set \ [~, - - in  order to constrain the vari-able x---it is no longer available as a viable con-textual constraint to distinguish b~ later on.
Thisheuristic will therefore block the infinite descrip-tion of cl.
But as desired, it will admit the bound-variable anaphora mentioned above, since this NPis not based on repeated inforrnation; the phraseit mcrcly self-referential.Conclus ionWc have shown how tile referring expression gen-eration algorithm presented in Dale \[1988, 1989\]can bc extended to encompass the use of rela-tions, by making use of constraint network con-sistency.
In the context of this revised genera-tion procedure we have investigated the problemof blocking the production of infinitely recursivenoun phrase,  and suggested an improvement onsome existing approaches to the problem.
Ar-eas lbr further research include the relationshipof our approach to existing algorithms in otherfields, such as machine learning, and also its re-lationship to observed characteristics of humandiscourse production.AcknowledgementsThe work reported here ?
was prompted by a con-versation with Breck Baldwin.
Both authors wouldlike to thank colleagues at each of their institu-tions for numerous comments that have improvedthis paper.ReferencesAppelt, Douglas E \[1985a\] Planning English Sentences.Cambridge: Cambridge University Press.Appelt, Douglas E \[1985b\] Planning English l~ferringExpressions.
Artificial Intelligence, 26, 1-33.Dale, Robert \[1988\] Generating Referring Expressionsin a Domain of Objects and Processes.
PhD The-sis, Centre for Cognitive Science, University of Ed-inburgh.Dale, Robert \[1989\] Cooking up Iteferring Expressions.In Proceedings of the ~Tth Annual Meeting of theAssociation for Computational Linguistics, Vancou-ver BC, pp68-75.Davey, Anthony \[1978\] Discourse Production.
F_ziin-burgh: Edinburgh University Press.Haddock, Nicholas J \[1987\] Incremental Interpretationand Combinatory Categorial Grammar.
In Proceed-ings of the Tenth International Joint Conference onArtificial Intelligence, Milan, Italy, pp.Haddock, Nicholas J \[1988\] Incremental Semantics andInteractive Syntactic Processing.
PhD Thesis, Cen-tre for Cognitive Science, University of Edinburgh.Haddock, Nicholas 3 \[1991\] Linear-Time Reference Eval-uation.
Technical Report, ttewlett Packard Labora-tories, Bristol.Levelt, Willem J M \[1989\] Speaking: b?om Intention toArticulation.
Cambridge, Mass.
: MIT Press.Mac&worth, Alan K \[1977\] Consistency in Networks ofRelations.
Artificial Intelligence, 8, 99-118.Mellish, Christopher S \[1985\] Computer Interpretationof Natural Language Descriptions.
Chichester: EllisHorwood.Novak, Hans-Joachim \[1988\] Generating Referring Phrasesin a Dynamic Environment.
Chapter 5 in M Zockand G Sabah (eds), Advances in Natural LanguageGeneration, Volume 2, pp76--85.
London: PintcrPublishers.Reiter, Ehud \[1990\] Generating Appropriate Natural Lan-guage Object Descriptions.
PhD thesis, Aiken Com-putation Laboratory, Harvard University.Rosch, Eleanor 11978\] Principles of Categorization.
InE Rosch and B Lloyd (eds), Cognition and Catego-rization, pp27--48.
Hillsdale, N J: Lawrence ErlbaumAssociates.166 -
