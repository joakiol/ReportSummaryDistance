Proceedings of the 6th Workshop on Statistical Machine Translation, pages 135?139,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsMANY improvements for WMT?11Lo?
?c BarraultLIUM, University of Le MansLe Mans, France.FirstName.LastName@lium.univ-lemans.frAbstractThis paper describes the development op-erated into MANY for the 2011 WMTsystem combination evaluation campaign.Hypotheses from French/English and En-glish/French MT systems were combinedwith a new version of MANY, an opensource system combination software basedon confusion networks decoding currentlydeveloped at LIUM.
MANY has been up-dated in order to optimize decoder pa-rameters with MERT, which proves tofind better weights.
The system combi-nation yielded significant improvements inBLEU score when applied on system com-bination data from two languages.1 IntroductionThis year, the LIUM computer science laboratoryparticipated in the French-English system combi-nation task at WMT?11 evaluation campaign.
Thesystem used for this task is MANY1 (Barrault,2010), an open source system combination soft-ware based on Confusion Networks (CN).For this year evaluation, rather more technicalthan scientific improvements have been added toMANY.
The tuning process has been improvedby using MERT (Och, 2003) as a replacementof the numerical optimizer Condor (Berghen andBersini, 2005).
The impact of such change is de-tailed in section 3.After the evaluation period, some experimentshave been performed on the English-French sys-tem combination task.
The results are presentedin the section 5.
Before that, a quick descriptionof MANY, including recent developments, can befound in section 2.1MANY is available at the following address http://www-lium.univ-lemans.fr/?barrault/MANY2 System descriptionMANY is a system combination software (Bar-rault, 2010) based on the decoding of a latticemade of several Confusion Networks (CN).
Thisis a widespread approach in MT system combina-tion (Rosti et al, 2007; Shen et al, 2008; Karakoset al, 2008; Rosti et al, 2009).
MANY can bedecomposed in two main modules.
The first oneis the alignment module which actually is a modi-fied version of TERp (Snover et al, 2009).
Its roleis to incrementally align the hypotheses against abackbone in order to create a confusion network.Those confusion networks are then connected to-gether to create a lattice.
This module uses dif-ferent costs (which corresponds to a match, an in-sertion, a deletion, a substitution, a shift, a syn-onym and a stem) to compute the best alignmentand incrementally build a confusion network.
Inthe case of confusion network, the match (substi-tution, synonyms, and stems) costs are consideredwhen the word in the hypothesis matches (is a sub-stitution, a synonyms or a stems of) at least oneword of the considered confusion sets in the CN.System 0System 1TERpalignmentLMoutput1-bestoutput1-bestoutputTERpalignmentDECODEMergeSystem M1-bestoutputTERpalignment{best hyponbest listLatticeCNCNCNFigure 1: System combination based on confusionnetwork decoding.The second module is the decoder.
This decoderis based on the token pass algorithm and it acceptsas input the lattice previously created.
The proba-bilities computed in the decoder can be expressedas follow :135log(PW ) =?i?i log(hi(t))(1)where t is the hypothesis, the ?i are the weightsof the feature functions hi.
The following featuresare considered for decoding:?
The language model probability: the proba-bility given by a 4-gram language model.?
The word penalty: penalty depending on thesize (in words) of the hypothesis.?
The null-arc penalty: penalty depending onthe number of null-arcs crossed in the latticeto obtain the hypothesis.?
System weights: each word receive a weightcorresponding to the sum of the weights of allsystems which proposed it.3 TuningAs mentioned before, MANY is made of two mainmodules: the alignment module based on a modi-fied version of TERp and the decoder.
Consideringa maximum of 24 systems for this year evaluation,33 parameters in total have to be optimized.
Bydefault, TERp costs are set to 0.0 for match and1.0 for everything else.
These costs are not correct,since a shift in that case will hardly be possible.TERp costs are tuned with Condor (a numericaloptimizer based on Powell?s algorithm, (Berghenand Bersini, 2005)).
Decoder feature functionsweights are optimized with MERT (Och, 2003).The 300-best list created at each MERT iterationis appended to the n-best lists created at previousiterations.
This proves to be a more reliable tuningas shown in the following experiments.During experiments, data from WMT?09 eval-uation campaign are used for testing the tuningapproach.
news-dev2009a is used as developmentset, and news-dev2009b as internal test, these cor-pora are described in Table 1.NAME #sent.
#words #toknews-dev2009a 1025 21583 24595news-dev2009b 1026 21837 24940Table 1: WMT?09 corpora : number of sentences,words and tokens calculated on the reference.For the sake of simplicity, the five best systems(ranking given by score on dev) are consideredonly.
Baseline systems performances on dev andtest are presented in Table 2.Corpus Sys0 Sys1 Sys2 Sys3 Sys4Dev 18.20 17.83 20.14 21.06 17.72Test 18.53 18.33 20.43 21.35 18.15Table 2: Baseline systems performance onWMT?09 data (%BLEU).The 2-step tuning protocol applied on news-dev2009a, when using MERT to optimize decoderfeature functions weights provides the set of pa-rameters presented in Table 3.Costs: Del Stem Syn Ins Sub Shift0.87 0.91 0.94 0.90 0.98 1.21Dec.
: LM weight Word pen.
Null pen.0.056 0.146 0.042Wghts.
: Sys0 Sys1 Sys2 Sys3 Sys4-0.03 -0.21 -0.23 -0.28 -0.02Table 3: Parameters obtained with tuning decoderparameters with MERT.Results on development corpus of WMT?09(used as test set) are presented in Table 4.
We canSystem Dev TestBest single 21.06 21.35MANY (2010) 22.08 22.28MANY-2steps (2010) 21.94 22.09MANY-2steps/MERT (2011) 23.05 23.07Table 4: System Combination results on WMT?09data (%BLEU-cased).observe that 2-step tuning provides almost +0.9BLEU point improvement on development corpuswhich is well reflected on test set with a gain ofmore than 0.8 BLEU.
By using MERT, this im-provement is increased to reach almost +2 BLEUpoint on dev corpus and +1.7 BLEU on test.There are two main reasons for this improve-ment.
The first one is the use of MERT whichmake use of specific heuristics to better opti-mize toward BLEU score.
The second one is thefully log-linear interpolation of features functionsscores operated into the decoder (previously, theword and null penalties were applied linearly).1364 2011 evaluation campaignA development corpus, newssyscombtune2011,and a test set, newssyscombtest2011, described inTable 5, were provided to participants.NAME #sent.
#words #toknewssyscombtune2011 1003 23108 26248newssyscombtest2011 2000 42719 48502Table 5: Description of WMT?11 corpora.Language model: The English target languagemodels has been trained on all monolingual dataprovided for the translation tasks.
In addition,LDC?s Gigaword collection was used for both lan-guages.
Data corresponding to the developmentand test periods were removed from the Gigawordcollections.Sys.
# BLEU TER Sys.
# BLEU TERSys0 29.86 52.46 Sys11 27.23 53.48Sys1 29.74 51.74 Sys12* 26.82 54.23Sys2 29.73 52.90 Sys13 26.25 55.60Sys3 29.58 52.73 Sys14* 26.13 55.65Sys4* 29.39 52.91 Sys15 25.90 55.69Sys5 28.89 53.74 Sys16 25.45 56.92Sys6 28.53 53.27 Sys17 25.23 56.09Sys7* 28.31 54.22 Sys18 23.63 60.25Sys8* 28.08 54.47 Sys19 21.90 63.65Sys9* 27.98 53.92 Sys20 21.77 60.78Sys10 27.46 54.60 Sys21 20.97 64.00Sys22 16.63 65.83MANY-5sys 31.83 51.27MANY-10sys 31.75 51.91MANY-allsys 30.75 54.33Table 6: Systems performance on newssyscomb-tune2011 development data (%BLEU-cased).
(*indicate a contrastive run)Choosing the right number of systems to com-bine: Table 6 shows the performance of the in-put systems (ordered by BLEU score computed onnewssyscombtune2011) and the result of 3 systemcombination setups.
The difference in these se-tups only reside on the number of inputs to use forcombination (5, 10 and all system outputs).
Noticethat the contrastive runs have not been used whencombining 5 and 10 systems.
The motivation forthis is to benefit from the multi-site systems de-velopment which more likely provide varied out-puts (i.e.
different ngrams and word choice).
Theresults show that combining 5 systems is slightlybetter than 10, but give more than 1 BLEU pointimprovement compared to combining all systems.Still, the combination always provide an improve-ment, which was not the case in last year evalua-tion.The results obtained by combining 5 and 10 sys-tems are presented in Table 7.Sys.
# BLEU TER Sys.
# BLEU TERSys0 29.43 52.01 Sys6 28.08 53.19Sys1 29.15 51.30 Sys11 27.24 53.74Sys2 28.87 52.82 Sys13 26.74 52.92Sys3 28.82 52.57 Sys15 26.31 54.61Sys5 28.08 53.19 Sys16 25.23 55.38MANY (5sys) 30.74 51.17MANY (10sys) 30.60 51.39Table 7: Baseline systems performance onWMT?11 syscomb test data (%BLEU-cased).Optimizing MANY on newssyscombtune2011corpus produced the parameter set presented in Ta-ble 8.
We can see that the weights of all system arenot proportional to the BLEU score obtained onthe development corpus.
This suggest that a bet-ter system selection could be found.
This is evenmore probable since the weight of system Sys2 ispositive (which imply a negative impact on eachword proposed by this system), which means thatwhen an hypothesis contains a word coming fromthis system, then its score is decreased.Costs: Del Stem Syn Ins Sub Shift0.90 0.88 0.96 0.97 1.01 1.19Dec.
: LM weight Null pen.
Len pen.0.0204 0.26 0.005Wghts.
:Sys0 Sys1 Sys2 Sys3 Sys5-0.16 -0.30 0.008 -0.16 -0.09Table 8: Parameters obtained after tuning the sys-tem parameter using 5 hypotheses.Table 9 contains the BLEU scores computed be-tween the outputs of the five systems used duringcombination.
An interesting observation is that thesystem which receive the bigger weight is the onewhich ?distance?2 against all other system outputs2This ?distance?
is expressed in terms of ngrams agree-ment137Sys0 Sys1 Sys2 Sys3 Sys5 meanSys0 - 53.59 62.67 64.60 62.50 60.84Sys1 53.51 - 54.19 52.42 51.69 52.95Sys2 62.72 54.28 - 65.49 63.09 61.40Sys3 64.63 52.51 65.47 - 61.35 60.99Sys5 62.55 51.78 63.10 61.37 - 59.70mean 60.85 53.04 61.36 60.97 59.66Table 9: Cross-system BLEU scores computedon WMT?11 French-English test corpus outputs(%BLEU-cased).is the highest, whereas the ?closest?
system get thesmallest weight.
This suggests that systems closerto other systems tends to be less useful for sys-tem combination.
This is an interesting behaviourwhich has to be explored deeper and validated onother tasks and corpora.5 MANY for french outputsAfter the evaluation period, some experimentshave been conducted in order to combine frenchoutputs.
The main difference lie in the fact thatlinguistic resources are not easily or freely avail-able for that kind of language.
Therefore, insteadof using TERp with relax3 shift constraint, thestrict constraint was used (shifts occur only whena match is found).The available data are detailed in the Table 10.NAME #sent.
#words #toksyscombtune 1003 24659 29171syscombtest 2000 45372 53970Table 10: Description of WMT?11 corpora forsystem combination in french.The results obtained are presented in Table 11.The BLEU score increase by more than 0.8 pointbut the TER score decrease by 0.58.
The metrictargeted during tuning is BLEU, which can ex-plain the improvement in that metric.
When deal-ing with english text, the only case where such be-haviour is observed is when combining all systems(see Table 6.6 MANY technical newsSeveral improvements have been performed onMANY.
The decoder is now based on a fully log-3Shifts can occur when a match, a stem, a synonym or aparaphrase is found.Corpus syscombtune2011 syscombtest2011BLEU TER BLEU TERSys0 35.99 49.16 34.36 49.78Sys1 32.99 51.90 30.73 52.52Sys2 32.41 52.77 29.85 53.61Sys3 32.40 51.26 30.48 52.20Sys4 32.30 52.21 31.02 52.49MANY 36.81 49.74 34.51 50.54Table 11: Systems and combination performanceon WMT?11 french data (%BLEU-cased).linear model (whereas before, the word and nullpenalties were applied linearly).
Using MERT totune the decoder parameters is therefore possibleand allows to reach bigger improvement comparedto using Condor.
This is probably due to the factthat MERT uses several heuristics useful for tun-ing on BLEU score.In order to facilitate the use of MANY, it hasbeen integrated in the Experiment ManagementSystem, EMS - (Koehn, 2010).
An experiment cannow be setup/modified/re-run easily by modifyinga single configuration file.
The default behavior ofthis framework is to perform 3 runs of MERT inparallel (using torque) and take the best optimiza-tion run.
Apart from avoiding local maximum, theprocedure allows to see the variability of the opti-mization process and report more realistic results(for example, by taking the average).7 Conclusion and future workFor WMT?11 system combination evaluation cam-paign, several rather technical improvements havebeen performed into MANY.
By homogenizingthe log-linear model used by the decoder and uti-lizing MERT for tuning, MANY achieves im-provements of more than 2 BLEU points onWMT?09 data and about 1.3 BLEU point onnewssyscombtest2011 relatively to the best singlesystem.
Moreover, a dry-run operated on frenchdata shows a promising result with an improve-ment of more than 0.8 BLEU points.
This will befurther explored in the future.MANY can benefit from various information.At the moment, the decision taken by the decodermainly depends on a target language model.
Thisis clearly not enough to achieve greater perfor-mances.
The next issues which will be addressedwithin the MANY framework is to estimate goodconfidence measure to use in place of the systems138priors.
These confidences measures have to be re-lated to the system performances, but also to thecomplementarity of the systems considered.8 AcknowledgementThis work has been partially funded by the Eu-ropean Union under the EuroMatrix Plus project(http://www.euromatrixplus.net, IST-2007.2.2-FP7-231720)References[Barrault, 2010] Barrault, L. (2010).
MANY :Open source machine translation system com-bination.
Prague Bulletin of Mathematical Lin-guistics, Special Issue on Open Source Tools forMachine Translation, 93:147?155.
[Berghen and Bersini, 2005] Berghen, F. V. andBersini, H. (2005).
CONDOR, a new parallel,constrained extension of Powell?s UOBYQAalgorithm: Experimental results and compari-son with the DFO algorithm.
Journal of Com-putational and Applied Mathematics, 181:157?175.
[Karakos et al, 2008] Karakos, D., Eisner, J.,Khudanpur, S., and Dreyer, M. (2008).
Ma-chine translation system combination usingITG-based alignments.
In 46th Annual Meetingof the Association for Computational Linguis-tics: Human Language Technologies., pages81?84, Columbus, Ohio, USA.
[Koehn, 2010] Koehn, P. (2010).
An experimentalmanagement system.
The Prague Bulletin ofMathematical Linguistics, 94:87?96.
[Och, 2003] Och, F. (2003).
Minimum error ratetraining in statistical machine translation.
InACL, Sapporo, Japan.
[Rosti et al, 2007] Rosti, A.-V., Matsoukas, S.,and Schwartz, R. (2007).
Improved word-levelsystem combination for machine translation.In Association for Computational Linguistics,pages 312?319.
[Rosti et al, 2009] Rosti, A.-V., Zhang, B., Mat-soukas, S., , and Schwartz, R. (2009).
In-cremental hypothesis alignment with flexi-ble matching for building confusion networks:BBN system description for WMT09 systemcombination task.
In EACL/WMT, pages 61?65.
[Shen et al, 2008] Shen, W., Delaney, B., An-derson, T., and Slyh, R. (2008).
The MIT-LL/AFRL IWSLT-2008 MT System.
In Inter-national Workshop on Spoken Language Trans-lation, Hawaii, U.S.A.[Snover et al, 2009] Snover, M., Madnani, N.,Dorr, B., and Schwartz, R. (2009).
TER-Plus:Paraphrase, semantic, and alignment enhance-ments to translation edit rate.
Machine Trans-lation Journal.139
