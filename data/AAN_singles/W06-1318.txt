Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 126?133,Sydney, July 2006. c?2006 Association for Computational LinguisticsMeasuring annotator agreement in a complex hierarchical dialogue actannotation schemeJeroen Geertzen and Harry BuntLanguage and Information ScienceTilburg University, P.O.
Box 90153NL-5000 LE Tilburg, The Netherlands{j.geertzen,h.bunt}@uvt.nlAbstractWe present a first analysis of inter-annotator agreement for the DIT++ tagsetof dialogue acts, a comprehensive, lay-ered, multidimensional set of 86 tags.Within a dimension or a layer, subsets oftags are often hierarchically organised.
Weargue that especially for such highly struc-tured annotation schemes the well-knownkappa statistic is not an adequate measureof inter-annotator agreement.
Instead, wepropose a statistic that takes the structuralproperties of the tagset into account, andwe discuss the application of this statisticin an annotation experiment.
The exper-iment shows promising agreement scoresfor most dimensions in the tagset and pro-vides useful insights into the usability ofthe annotation scheme, but also indicatesthat several additional factors influenceannotator agreement.
We finally suggestthat the proposed approach for measuringagreement per dimension can be a goodbasis for measuring annotator agreementover the dimensions of a multidimensionalannotation scheme.1 IntroductionThe DIT++ tagset (Bunt, 2005) was designed tocombine in one comprehensive annotation schemethe communicative functions of dialogue acts dis-tinguished in Dynamic Interpretation Theory (DIT,(Bunt, 2000; Bunt and Girard, 2005)), and manyof those in DAMSL (Allen and Core, 1997) and inother annotation schemes.
An important differ-ence between the DIT++ and DAMSL schemes is themore elaborate and fine-grained set of functionsfor feedback and other aspects of dialogue controlthat is available in DIT, partly inspired by the workof Allwood (Allwood et al, 1993).
As it is oftenthought that more elaborate and fine-grained anno-tation schemes are difficult for annotators to applyconsistently, we decided to address this issue in anannotation experiment on which we report in thispaper.
A frequently used way of evaluating hu-man dialogue act classification is inter-annotatoragreement.
Agreement is sometimes measured aspercentage of the cases on which the annotatorsagree, but more often expected agreement is takeninto account in using the kappa statistic (Cohen,1960; Carletta, 1996), which is given by:?
= po ?
pe1 ?
pe(1)where po is the observed proportion of agreementand pe is the proportion of agreement expected bychance.
Ever since its introduction in general (Co-hen, 1960) and in computational linguistics (Car-letta, 1996), many researchers have pointed outthat there are quite some problems in using ?
(e.g.
(Di Eugenio and Glass, 2004)), one of which isthe discrepancy between p0 and ?
for skewed classdistribution.Another is that the degree of disagreement isnot taken into account, which is relevant for anynon-nominal scale.
To address this problem, aweighted ?
has been proposed (Cohen, 1968) thatpenalizes disagreement according to their degreerather than treating all disagreements equally.
Itwould be arguable that in a similar way, charac-teristics of dialogue acts in a particular taxonomyand possible pragmatic similarity between themshould be taken into account to express annotatoragreement.
For dialogue act taxonomies which arestructured in a meaningful way, such as those that126express hierarchical relations between concepts inthe taxonomy, the taxonomic structure can be ex-ploited to express how much annotators disagreewhen they choose different concepts that are di-rectly or indirectly related.
Recent work that ac-counts for some of these aspects is a metric forautomatic dialogue act classification (Lesch et al,2005) that uses distance in a hierarchical structureof multidimensional labels.In the following sections of this paper, we willfirst briefly consider the dimensions in the DIT++scheme and highlight the taxonomic characteris-tics that will turn out to be relevant in later stage.We will then introduce a variant of weighted ?
forinter-annotator agreement called ?tw that adoptsa taxonomy-dependent weighting, and discuss itsuse.2 Annotation using DITDIT is a context-change (or information-state up-date) approach to the analysis of dialogue, whichdescribes utterance meaning in terms of contextupdate operations called ?dialogue acts?.
A dia-logue act in DIT has two components: (1) the se-mantic content, being the objects, events, proper-ties, relations, etc.
that are considered; and (2)the communicative function, that describes howthe addressee is intended to use the semantic con-tent for updating his context model when he un-derstands the utterance correctly.
DIT takes a mul-tidimensional view on dialogue in the sense thatspeakers may use utterances to address several as-pects of the communication simultaneously, as re-flected in the multifunctionality of utterances.
Onesuch aspect is the performance of the task or ac-tivity for which the dialogue takes place; anotheris the monitoring of each other?s attention, under-standing and uptake through feedback acts; othersinclude for instance the turn-taking process andthe timing of communicative actions, and finallyyet another aspect is formed by the social obli-gations that may arise such as greeting, apologis-ing, or thanking.
The various aspects of commu-nication that can be addressed independently arecalled dimensions (Bunt and Girard, 2005; Bunt,2006).
The DIT++ tagset distinguishes 11 dimen-sions, which all contain a number of communica-tive functions that are specific to that dimension,such as TURN GIVING, PAUSING, and APOLOGY.Besides dimension-specific communicativefunctions, DIT also distinguishes a layer ofcommunicative functions that are not specific toany particular dimension but that can be usedto address any aspect of communication.
Thesefunctions, which include questions, answers,statements, and commissive as well as directiveacts, are called general purpose functions.
Adialogue act falls within a specific dimensionif it has a communicative function specific forthat dimension or if it has a general-purposefunction and a semantic content relating to thatdimension.
Dialogue utterances can in principlehave a function (but never more than one) in eachof the dimensions, so annotators using the DIT++scheme can assign at most one tag for each of the11 dimensions to any given utterance.Both within the set of general-purpose com-municative function tags and within the sets ofdimension-specific tags, tags can be hierarchicallyrelated in such a way that a label lower in a hier-archy is more specific than a label higher in thesame hierarchy.
Tag F1 is more specific than tagF2 if F1 defines a context update operation that in-cludes the update operation corresponding to F2.For instance, consider a part of the taxonomy forgeneral purpose functions (Figure 1).INFO.SEEKINGIND-YNQYNQCHECKPOSI NEGAIND-WHQWHQ.
.
.Figure 1: Two hierarchies in the information seek-ing general purpose functions.For an utterance to be assigned a YN-QUESTION,we assume the speaker believes that the addresseeknows the truth value of the proposition presented.For an utterance to be assigned a CHECK, we as-sume the speaker additionally has a weak be-lief that the proposition that forms the seman-tic content is true.
And for a POSI-CHECK, thereis the additional assumption that the speaker be-lieves (weakly) that the hearer also believes thatthe proposition is true.1Similar to the hierarchical relations betweenYN-Question, CHECK, and POSI-CHECK, other parts1For a formal description of each function in the DIT++tagset see http://ls0143.uvt.nl/dit/127of the annotation scheme contain hierarchically re-lated functions.The following example illustrates the use ofDIT++ communicative functions for a very simpletranslated) dialogue fragment2.1 S at what time do you want to travel today?TASK = WH-Q, TURN-MANAGEMENT = GIVE2 U at ten.TASK = WH-A, TURN-MANAGEMENT = GIVE3 S so you want to leave at ten in the morning?TASK = POSI-CHECK, TURN-MANAGEMENT = GIVE4 U yes that is right.TASK = CONFIRM, TURN-MANAGEMENT = GIVE3 Agreement using ?3.1 Related workInter-annotator agreements have been calculatedwith the purpose of qualitatively evaluating tagsetsand individual tags.
For DAMSL, the first agree-ment results were presented in (Core and Allen,1997), based on the analysis of TRAINS 91-93 dialogues (Gross et al, 1993; Heeman andAllen, 1995).
In this analysis, 604 utteranceswere tagged by mostly two annotators.
Follow-ing the suggestions in (Carletta, 1996), Core etal.
consider kappa scores above 0.67 to indi-cate significant agreement and scores above 0.8reliable agreement.
Another more recent analy-sis was performed for 8 dialogues of the MON-ROE corpus (Stent, 2000), counting 2897 utter-ances in total, processed by two annotators for 13DAMSL dimensions.
Other analyses apply DAMSLderived schemes (such as SWITCHBOARD-DAMSL)to various corpora (e.g.
(Di Eugenio et al, 1998;Shriberg et al, 2004) ).
For the comprehensiveDIT++ taxonomy, the work reported here repre-sents the first investigation of annotator agree-ment.3.2 Experiment outlineAs noted, existing work on annotator agreementanalysis has mostly involved only two annotators.It may be argued that especially for annotation ofconcepts that are rather complex, an odd numberof annotators is desirable.
First, it allows havingmajority agreement unless all annotators chooseentirely different.
Second, it allows to deal bet-ter with the undesirable situation that one annota-tor chooses quite differently from the others.
The2Drawn from the OVIS corpus (Strik et al, 1997):OVIS2:104/001/001:008-011agreement scores reported in this paper are all cal-culated on the basis of the annotations of threeannotators, using the method proposed in (Daviesand Fleiss, 1982).The dialogues that were annotated are task-oriented and are all in Dutch.
To account fordifferent complexities of interaction, both human-machine and human-human dialogues are consid-ered.
Moreover, the dialogues analyzed are drawnfrom different corpora: OVIS (Strik et al, 1997),DIAMOND (Geertzen et al, 2004), and a collec-tion of Map Task dialogues (Caspers, 2000); seeTable 1, where the number of annotated utterancesis also indicated.corpus domain type #uttOVIS TRAINS like interactions H-M 193on train connectionsDIAMOND1 interactions on how to H-M 131operate a fax deviceDIAMOND2 interactions on how to H-H 114operate a fax deviceMAPTASK HCRC Map Task like H-H 120interaction558Table 1: Characteristics of the utterances consid-eredSix undergraduate students annotated the se-lected dialogue material.
They had been intro-duced to the DIT++ annotation scheme and the un-derlying theory while participating in a course onpragmatics.
During this course they were exposedto approximately four hours of lecturing and fewsmall annotation exercises.
For all dialogues, theaudio recordings were transcribed and the annota-tors annotated presegmented utterances for whichfull agreement was established on segmentationlevel beforehand.
During the annotation sessionsthe annotators had ?
apart from the transcribedspeech ?
access to the audio recordings, to theon-line definitions of the communicative functionsin the scheme and to a very brief, 1-page set of an-notation guidelines3 .
The task was facilitated bythe use of an annotation tool that had been builtfor this occasion; this tool allowed the subjects toassign each utterance one DIT++ tag for each di-mension without any further constraints.
In total1,674 utterances were annotated.3.3 Problems with standard ?If we were to apply the standard ?
statistic toDIT++ annotations, we would not do justice to animportant aspect of the annotation scheme con-cerning the differences between alternative tags,3See http://ls0143.uvt.nl/dit128and hence the possible differences in the dis-agreement between annotators using alternativetags.
An aspect in which the DIT++ scheme dif-fers from other taxonomies for dialogue acts isthat, as noted in Section 2, communicative func-tions (CFs) within a dimension as well as general-purpose CFs are often structured into hierarchiesin which a difference in level represents a relationof specificity.
When annotators differ in that theyassign tags which both belong to the same hier-archy, they may differ in the degree of specificitythat they want to express, but they agree to the ex-tent that these tags inherit the same elements fromtags higher in the hierarchy.
Inter-annotator dis-agreement is in such a case much less than if theywould choose two unrelated tags.
This is for in-stance obvious in the following example of the an-notations of two utterances by two annotators:1 S what do you want to know?
WHQ YNQ2 U can I print now?
YNQ CHECKWith utterance 1, the annotators should be saidsimply to disagree (in fact, annotator 2 incorrectlyassigns a YNQ function).
Concerning utterance 2the annotators also disagree, but Figure 1 and thedefinitions given in Section 2 tell us that the dis-agreement in this case is quite small, as a CHECK in-herits the properties of a YNQ.
We therefore shouldnot use a black-and-white measure of agreement,like the standard ?, but we should have a measurefor partial annotator agreement.In order to measure partial (dis-)agreement be-tween annotators in an adequate way, we shouldnot just take into account whether two tags are hi-erarchically related or not, but also how far theyare apart in the hierarchy, to reflect that two tagswhich are only one level apart are semanticallymore closely related than tags that are several lev-els apart.
We will take this additional requirementinto account when designing a weighted disagree-ment statistic in the next section.4 Agreement based on structuraltaxonomic propertiesThe agreement coefficient we are looking forshould in the first place be weighted in the sensethat it takes into account the magnitude of dis-agreement.
Two such coefficients are weightedkappa (?w, (Cohen, 1968)) and alpha (Krippen-dorff, 1980).
For our purposes, we adopt ?w forits property to take into account a probability dis-tribution typical for each annotator, generalize it tothe case for multiple annotators by taking the aver-age over the scores of annotator pairs, and definea function to be used as distance metric.4.1 Cohen?s weighted ?Assuming the case of two annotators, let pij de-note the proportion of utterances for which the firstand second annotator assigned categories i and j,respectively.
Then Cohen defines ?w in terms ofdisagreement rather than agreement where qo =1 ?
po and qe = 1 ?
pe such that Equation 1 canbe rewritten to:?
= 1 ?
qoqe(2)To arrive at ?w, the proportions qo and qe in Equa-tion 2 are replaced by weighted functions over allpossible category pairs:?w = 1 ??
vij ?
poij?
vij ?
peij(3)where vij denotes the disagreement weight.
Tocalculate this weight we need to specify a distancefunction as metric.4.2 A taxonomic metricThe task of defining a function in order to calcu-late the difference between a pair of categories re-quires us to determine semantic-pragmatic related-ness between the CFs in the taxonomy.
For any an-notation scheme, whether it is hierarchically struc-tured or not, we could assign for each possible pairof categories a value that expresses the semantic-pragmatic relatedness between the two categoriescompared to all other possible pairs.
However, itseems quite difficult to find universal characteris-tics for CFs to be used to express relatedness on arational scale.
When we consider a taxonomy thatis structured in a meaningful way, in this case onethat expresses hierarchical relations between CFbased on their effect on information states, the tax-onomic structure can be exploited to express in asystematic fashion how much annotators disagreewhen they choose different concepts that are di-rectly or indirectly related.The assignment of different CFs to a specific ut-terance by two annotators represents full disagree-ment in the following cases:1. the two CFs belong to different dimensions;1292. one of the two CFs is general-purpose; theother is dimension-specific;43. the two CFs belong to the same dimensionbut not to the same hierarchy;4. the two CFs belong to the same hierarchybut are not located in the same branch.
TwoCFs are said to be located in the same branchwhen one of the two CFs is an ancestor of theother.If, by contrast, the two CFs take part in a parent-child relation within a hierarchy (either within adimension or among the general-purpose CFs),then the CFs are related and this assignment repre-sents partial disagreement.
A distance metric thatmeasures this disagreement, which we denote as?, should have the following properties:1. ?
should be a real number normalized in therange [0 .
.
.
1];2.
Let C be the (unordered) set of CFs.5 For ev-ery two CFs c1, c2 ?
C , ?
(c1, c2) = 0 whenc1 and c2 are not related;3.
Let C be the (unordered) set of CFs.
For ev-ery communicative function c ?
C , ?
(c, c) =1;4.
Let C be the (unordered) set of CFs.
Forevery two CFs c1, c2 ?
C , ?
(c1, c2) =?
(c2, c1).Furthermore, when c1 and c2 are related, weshould specify how distance between them in thehierarchy should be expressed in terms of partialdisagreement.
For this, we should take the follow-ing aspects into account:1.
The distance in levels between c1 and c2 inthe hierarchy is proportional to the magnitudeof the disagreement;4This is in fact a simplification.
For instance, an INFORMact of which the semantic content conveys that the speakerdid not understand the previous utterance forms an act in theAuto-Feedback dimension (see Note 6), and a tagging to thiseffect should perhaps not be considered to express full dis-agreement with the assignment of the dimension-specific tagAUTO-FEEDBACK-Int?.
See also the next footnote.5Strictly speaking, in DIT a dialogue act annotation tag iseither (a) the name of a dimension-specific function, or (b) apair consisting of the name of a general-purpose function andthe name of a dimension.
However, in view of the simplifica-tion mentioned in the previous note, for the sake of this paperwe may as well consider tags containing a general-purposefunction as simply consisting of that function.Auto FeedbackPerc?Int?Eval?Exec?Perc+Int+Eval+Exec+Figure 2: Hierarchical structures in the auto feed-back dimension.2.
The magnitude of disagreement between c1and c2 being located in two different levels ofdepths n and n+1 might be considered to bemore different than that between to levels ofdepth n + 1 and n + 2.
If this would be thecase, the deeper two levels are located in thetree, the smaller the differences between thenodes on those levels.
For the hierarchies inDIT, we keep the magnitude of disagreementlinear with the difference in levels, and inde-pendent of level depth;Given the considerations above, we propose thefollowing metric:?
(ci, cj) = a?
(ci,cj) ?
b?
(ci,cj) (4)where:?
a is a constant for which 0 < a < 1, express-ing how much distance there is between twoadjacent levels in the hierarchy; a plausiblevalue for a could be 0.75;?
?
is a function that returns the difference indepth between the levels of ci and cj;?
b is a constant for which 0 < b ?
1, express-ing in what rate differences should becomesmaller when the depth in the hierarchy getslarger.
If there is no reason to assume thatdifferences on a higher depth in the hierarchyare of less magnitude than differences on alower depth, then b = 1;?
?
(ci, cj) is a function that returns the mini-mal depth of ci and cj .To provide some examples of how ?
would becalculated, let us consider the general purposefunctions in Figure 1.
Consider also Figure 2,that represents two hierarchies of CFs in the auto130feedback dimension6, and let us assume the valuesof the various parameters those that are suggestedabove.
We then get the following calculations:?
(IND ?
Y NQ,CHECK) = 0.752 ?
1 = 0.563?
(Y NQ,CHECK) = 0.751 ?
1 = 0.75?
(Perc+, P erc+) = 0.750 ?
1 = 1?
(Perc+, Eval+) = 0.752 ?
1 = 0.563?
(Int?, Int+) = 0?
(POSI,NEGA) = 0To conclude, we can simply take ?
to be theweighting in Cohen?s ?w and come to a coefficientwhich we will call taxonomically weighted kappa,denoted by ?tw:?tw = 1 ??
(1 ?
?
(i, j)) ?
poij?
(1 ?
?
(i, j)) ?
peij(5)4.3 ?tw statistics for DITConsidering the DIT++ taxonomy, it may be arguedthat due to the many hierarchies in the topologyof the general-purpose functions, this is the partwhere most is to be gained by employing ?tw.Table 2 shows the statistics for each dimension,averaged over all annotation pairs.
With anno-tation pair is understood the pair of assignmentsan utterance received by two annotators for a par-ticular dimension.
The figures in the table arebased on those cases in which both annotators as-signed a function to a specific utterance for a spe-cific dimension.
Cases where either one annotatordoes not assign a function while the other does,or where both annotators do not assign a function,are not considered.
Scores for standard ?
and ?twcan be found in the first two columns.
The column#pairs indicates on how many annotation pairs thestatistics are based.
The last column shows theap-ratio.
This figure indicates which fraction ofall annotated functions in that dimension are rep-resented by annotation pairs.
When #ap denotesthe number of annotation pairs and #pa denotesthe number of partial annotations (annotations inwhich one annotator assigned a function and theother did not), then the ap-ratio is calculated as#ap/(#pa + #ap).
We can observe that due tothe use of the taxonomic weighting both feedbackdimensions and the task dimension gained sub-stantially in annotator agreement.6Auto-feedback: feedback on the processing (perception,understanding, evaluation,..) of previous utterances by thespeaker.
DIT also distinguishes allo-feedback, where thespeaker provides or elicits information about the addressee?sprocessing.Dimension ?
?tw #pairs ap-ratiotask 0.47 0.71 848 0.87task:action discussion 0.61 0.61 91 0.37auto feedback 0.21 0.57 127 0.34allo feedback 0.42 0.58 17 0.14turn management 0.82 0.82 115 0.18time management 0.58 0.58 68 0.72contact management 1.00 1.00 8 0.17topic management nav nav 2 0.08own com.
management 1.00 1.00 2 0.08partner com.
management nav nav 1 0.07dialogue struct.
management 0.74 0.74 15 0.31social obl.
management 1.00 1.00 61 0.80Table 2: Scores for corrected ?
and ?tw per DITdimension.When we look at the agreement statistics andconsider ?
scores above 0.67 to be significantand scores above 0.8 considerably reliable, as isusual for ?
statistics, we can find the dimensionsTURN-MANAGEMENT, CONTACT MANAGEMENT, andSOCIAL-OBLIGATIONS-MANAGEMENT to be reliableand DIALOGUE STRUCT.
MANAGEMENT to be signif-icant.
For some dimensions, the occurences offunctions in these dimensions in the annotated di-alogue material were too few to draw conclusions.When we also take the ap-ratio into account,only the dimensions TASK, TIME MANAGEMENT,and SOCIAL-OBLIGATIONS-MANAGEMENT combinea fair agreement on functions with fair agreementon whether or not to annotate in these dimensions.Especially for the other dimensions, the questionshould be raised for which cases and for what rea-sons the ap-ratio is low.
This question asks forfurther qualitative analysis, which is beyond thescope of this paper7.5 DiscussionIn the previous sections, we showed how the tax-onomically weighted ?tw that we proposed can bemore suitable for taxonomies that contain hierar-chical structures, like the DIT++) taxonomy.
How-ever, there are some specific and general issuesthat deserve more attention.A question that might be raised in using ?tw asopposed to ordinary ?, is if the assumption that theinterpretations of ?
proposed in literature in termsof reliability is also valid for ?tw statistics.
Thisis ultimately an empirical issue, to be decided bywhich ?tw scores researchers find to correspond tofair or near agreement between annotators.Another point of discussion is the arbitrarinessof the values of the parameters that can be cho-sen in ?.
In this paper we proposed a = 0.75 and?
= 0.5.
Choosing different values may change7See (Geertzen, 2006) for more details.131the disagreement of two distinct CFs located in thesame hierarchy considerably.
Still, we think thatby interpolating smoothly between the intuitivelyclear cases at the two extreme ends of the scale,it is possible to choose reasonable values for theparameters that scale well, given the average hier-archy depth.A more general problem, inherent in almostany (dialogue act) annotation activity is that whenwe consider the possible factors that influence theagreement scores, we find that they can be nu-merous.
Starting with the tagset, unclear defini-tions and vague concepts are a major source ofdisagreement.
Other factors are the quality and ex-tensiveness of annotation instructions, and the ex-perience of the annotators.
These were kept con-stant throughout the experiment reported in thispaper, but clearly the use of more experienced orbetter trained annotators could have a great influ-ence.
Then there is the influence that the use of anannotation tool can have.
Does the tool gives hintson annotation consistency (e.g.
an ANSWER shouldbe preceded by a QUESTION), does it enforce con-sistency, or does it not consider annotation consis-tency at all?
Are the possible choices for anno-tators presented in such a way that each choice isequally well visible and accessible?
Clearly, whenwe do not control these factors sufficiently, we runthe risk that what we measure does not expresswhat we try to quantify: (dis)agreement amongannotators about the description of what happensin a dialogue.6 Conclusion and future workIn this paper we have presented agreement scoresfor Cohen?s unweighted ?
and claimed that forannotation schemes with hierarchically relatedtags, a weighted ?
gives a better indication of(dis)agreement than unweighted ?.
The ?
scoresfor some dimensions seem not particularly spec-tacular but become more interesting when look-ing at semantic-pragmatic differences between di-alogue acts or CFs.
Even though there are some-what arbitrary aspects in weighting, when parame-ters are carefully chosen a weighted metric gives abetter representation of the inter-annotator agree-ments.
More generally, we propose that semantic-pragmatic relatedness between taxonomic con-cepts should be taken into account when calculat-ing inter-annotator (dis)agreement.
While we usedDIT++ as tagset, the weighting function we pro-posed can be employed in any taxonomy contain-ing hierarchically related concepts, since we onlyused structural properties of the taxonomy.We have also quantitatively8 evaluated theDIT++ tagset per dimension, and obtained an in-dication of its usability.
We focussed on agree-ment per dimension, but when we desire a globalindication of the difference in semantic-pragmaticinterpretation of a complete utterance it requiresus to consider other aspects.
A truly multidimen-sional study of inter-annotator agreement shouldnot only take intra-dimensional aspects into ac-count but also relate the dimensions to each other.In (Bunt and Girard, 2005; Bunt, 2006) it is arguedthat dimensions should be orthogonal, meaningthat an utterance can have a function in one dimen-sion independent of functions in other dimensions.This is a somewhat utopical condition, since thereare some functions that show correlations and de-pendencies with across dimensions.
For this rea-son it makes sense to try to express the effect of thepresence of strong correlations, dependencies andpossible entailments in a multidimensional notionof (dis)agreement.
Additionally, it may be desir-able to take into account the importance that a CFcan have.
It is widely acknowledged that utter-ances are often multifunctional, but it could be ar-gued that in many cases an utterance has a primaryfunction and secondary functions; for instance, ifan utterance has both a task-related function andone or more other functions, the task-related func-tion is typically felt to be more important than theother functions, and disagreement about the task-related function is therefore felt to be more seri-ous than disagreement about one of the other func-tions.
This might be taken into account by addinga weighting function when combining agreementmeasures over multiple dimensions.Other future work we plan is more methodolog-ical in nature, quantifying the relative effect of thefactors that may have influenced the scores that wehave found.
This would create a situation in whichthere is more insight in what exactly is evaluated.As for evaluating the tagset, we for instance planto further analyze co-occurence matrices to iden-tify frequent misannotations, and to have annota-tors thinking aloud while performing the annota-tion task.8Kappa statistics are indicative.
To get a full understand-ing of what the figures represent, qualitative analysis by usinge.g.
co-occurence matrices is required, which is beyond thescope of this paper.132AcknowledgementsThe authors thank three anonymous reviewers fortheir helpful comments on an earlier version of thispaper.ReferencesJames Allen and Mark Core.
1997.
Draft of DAMSL:Dialog act markup in several layers.
Unpublishedmanuscript.J.
Allwood, J. Nivre, and E. Ahlse?n.
1993.
Manual forcoding interaction management.
Technical report,Go?teborg University.
Project report: Semantik ochtalspra?k.Harry C. Bunt and Yann Girard.
2005.
Designing anopen, multidimensional dialogue act taxonomy.
InProceedings of the 9th Workshop on the Semanticsand Pragmatics of Dialogue (DIALOR 2005), pages37?44, Nancy, France, June.Harry C. Bunt.
2000.
Dialogue pragmatics and con-text specification.
In Harry C. Bunt and WilliamBlack, editors, Abduction, Belief and Context in Di-alogue; Studies in Computational Pragmatics, pages81?150.
John Benjamins, Amsterdam, The Nether-lands.Harry C. Bunt.
2005.
A framework for dialogue actspecification.
In Joint ISO-ACL Workshop on theRepresentation and Annotation of Semantic Infor-mation, Tilburg, The Netherlands, January.Harry C. Bunt.
2006.
Dimensions in dialogue annota-tion.
In Proceedings of the 5th International Confer-ence on Language Resources and Evaluation (LREC2006), Genova, Italy, May.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Johanneke Caspers.
2000.
Pitch accents, boundarytones and turn-taking in dutch map task dialogues.In Proceedings of the 6th International Conferenceon Spoken Language Processing (ICSLP 2000), vol-ume 1, pages 565?568, Beijing, China.Jacob Cohen.
1960.
A coefficient of agreement fornominal scales.
Education and Psychological Mea-surement, 20:37?46.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement with provision for scaled disagreement orpartial credit.
Psychological Bulletin, 70:213?220.Mark G. Core and James F. Allen.
1997.
Coding di-alogues with the DAMSL annotation scheme.
InDavid Traum, editor, Working Notes: AAAI FallSymposium on Communicative Action in Humansand Machines, pages 28?35, Menlo Park, CA, USA.American Association for Artificial Intelligence.Mark Davies and J.L.
Fleiss.
1982.
Measuring agree-ment for multinomial data.
Biometrics, 38:1047?1051.Barbara Di Eugenio and Michael Glass.
2004.
Thekappa statistic: a second look.
Computational Lin-guistics, 30(1):95?101.Barbara Di Eugenio, Pamela W. Jordan, Johanna D.Moore, and Richmond H. Thomason.
1998.
An em-pirical investigation of proposals in collaborative di-alogues.
In Proceedings of the 17th InternationalConference on Computational Linguistics and the36th Annual Meeting of the Association for Com-putational Linguistics (COLING-ACL 1998), pages325?329, Montreal, Canada.Jeroen Geertzen, Yann Girard, Roser Morante, Ielkavan der Sluis, Hans Van Dam, Barbara Suijkerbuijk,Rintse van der Werf, and Harry Bunt.
2004.
Thediamond project (poster,project description).
In The8th Workshop on the Semantics and Pragmatics ofDialogue (Catalog?04).
Barcelona, Spain.Jeroen Geertzen.
2006.
Inter-annotator agreementwithin dit++ dimensions.
Technical report, TilburgUniversity, Tilburg, The Netherlands.Derek Gross, James F. Allen, and David R. Traum.1993.
The TRAINS 91 dialogues.
Technical Re-port TN92-1, University of Rochester, Rochester,NY, USA.Peter A. Heeman and James F. Allen.
1995.
TheTRAINS 93 dialogues.
Technical Report TN94-2,University of Rochester, Rochester, NY, USA.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to its Methodology.
Sage Publications,Beverly Hills, CA, USA.Stephan Lesch, Thomas Kleinbauer, and Jan Alexan-dersson.
2005.
A new metric for the evaluationof dialog act classification.
In Proceedings of the9th Workshop on the Semantics and Pragmatics ofDialogue (DIALOR 2005), pages 143?146, Nancy,France, june.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, JeremyAng, and Hannah Carvey.
2004.
The ICSI meetingrecorder dialog act (MRDA) corpus.
In Proceedingsof the 5th SIGdial Workshop on Discourse and Dia-logue, pages 97?100, Boston, USA, April-May.Amanda J. Stent.
2000.
The monroe corpus.
Techni-cal Report TR728/TN99-2, University of Rochester,Rochester, UK.Helmer Strik, Albert Russel, Henk van den Heuvel, Ca-tia Cucchiarini, and Lou Boves.
1997.
A spoken di-alog system for the dutch public transport informa-tion service.
International Journal of Speech Tech-nology, 2(2):119?129.133
