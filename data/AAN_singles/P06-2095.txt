Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739?746,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing comparable corporato solve problems difficult for human translatorsSerge Sharoff, Bogdan Babych, Anthony HartleyCentre for Translation StudiesUniversity of Leeds, LS2 9JT UK{s.sharoff,b.babych,a.hartley}@leeds.ac.ukAbstractIn this paper we present a tool that usescomparable corpora to find appropriatetranslation equivalents for expressions thatare considered by translators as difficult.For a phrase in the source language thetool identifies a range of possible expres-sions used in similar contexts in target lan-guage corpora and presents them to thetranslator as a list of suggestions.
In thepaper we discuss the method and presentresults of human evaluation of the perfor-mance of the tool, which highlight its use-fulness when dictionary solutions are lack-ing.1 IntroductionThere is no doubt that both professional andtrainee translators need access to authentic dataprovided by corpora.
With respect to polyse-mous lexical items, bilingual dictionaries list sev-eral translation equivalents for a headword, butwords taken in their contexts can be translatedin many more ways than indicated in dictionar-ies.
For instance, the Oxford Russian Dictionary(ORD) lacks a translation for the Russian expres-sion ?????????????
?????
(?comprehensive an-swer?
), while the Multitran Russian-English dic-tionary suggests that it can be translated as ir-refragable answer.
Yet this expression is ex-tremely rare in English; on the Internet it occursmostly in pages produced by Russian speakers.On the other hand, translations for polysemouswords are too numerous to be listed for all pos-sible contexts.
For example, the entry for strongin ORD already has 57 subentries and yet it failsto mention many word combinations frequent inthe British National Corpus (BNC), such as strong{feeling, field, opposition, sense, voice}.
Strongvoice is also not listed in the Oxford French, Ger-man or Spanish Dictionaries.There has been surprisingly little research oncomputational methods for finding translationequivalents of words from the general lexicon.Practically all previous studies have concerneddetection of terminological equivalence.
For in-stance, project Termight at AT&T aimed to de-velop a tool for semi-automatic acquisition oftermbanks in the computer science domain (Da-gan and Church, 1997).
There was also a studyconcerning the use of multilingual webpages todevelop bilingual lexicons and termbanks (Grefen-stette, 2002).
However, neither of them concernedtranslations of words from the general lexicon.
Atthe same time, translators often experience moredifficulty in dealing with such general expressionsbecause of their polysemy, which is reflected dif-ferently in the target language, thus causing thedependency of their translation on the correspond-ing context.
Such variation is often not capturedby dictionaries.Because of their importance, words from thegeneral lexicon are studied by translation re-searchers, and comparable corpora are increas-ingly used in translation practice and training(Varantola, 2003).
However, such studies aremostly confined to lexicographic exercises, whichcompare the contexts and functions of potentialtranslation equivalents once they are known, forinstance, absolutely vs. assolutamente in Italian(Partington, 1998).
Such studies do not pro-vide a computational model for finding appropri-ate translation equivalents for expressions that arenot listed or are inadequate in dictionaries.Parallel corpora, conisting of original texts and739their exact translations, provide a useful supple-ment to decontextualised translation equivalentslisted in dictionaries.
However, parallel corporaare not representative.
Many of them are in therange of a few million words, which is simply toosmall to account for variations in translation ofmoderately frequent words.
Those that are a bitlarger, such as the Europarl corpus, are restrictedin their domain.
For instance, all of the 14 in-stances of strong voice in the English section ofEuroparl are used in the sense of ?the opinion ofa political institution?.
At the same time the BNCcontains 46 instances of strong voice covering sev-eral different meanings.In this paper we propose a computationalmethod for using comparable corpora to find trans-lation equivalents for source language expressionsthat are considered as difficult by trainee or pro-fessional translators.
The model is based on de-tecting frequent multi-word expressions (MWEs)in the source and target languages and finding amapping between them in comparable monolin-gual corpora, which are designed in a similar wayin the two languages.The described methodology is implemented inASSIST, a tool that helps translators to find solu-tions for difficult translation problems.
The toolpresents the results as lists of translation sugges-tions (usually 50 to 100 items) ordered alphabeti-cally or by their frequency in target language cor-pora.
Translators can skim through these lists andidentify an example which is most appropriate ina given context.In the following sections we outline our ap-proach, evaluate the output of the prototype of AS-SIST and discuss future work.2 Finding translations in comparablecorporaThe proposed model finds potential translationequivalents in four steps, which include1.
expansion of words in the original expressionusing related words;2. translation of the resultant set using existingbilingual dictionaries;3. further expansion of the set using relatedwords in the target language;4. filtering of the set according to expressionsfrequent in the target language corpus.In this study we use several comparable cor-pora for English and Russian, including large ref-erence corpora (the BNC and the Russian Refer-ence Corpus) and corpora of major British andRussian newspapers.
All corpora used in the studyare quite large, i.e.
the size of each corpus is inthe range of 100-200 million words (MW), so thatthey provide enough evidence to detect such col-locations as strong voice and clear defiance.Although the current study is restricted to theEnglish-Russian pair, the methodology does notrely on any particular language.
It can be ex-tended to other languages for which large com-parable corpora, POS-tagging and lemmatisationtools, and bilingual dictionaries are available.
Forexample, we conducted a small study for transla-tion between English and German using the Ox-ford German Dictionary and a 200 MW Germancorpus derived from the Internet (Sharoff, 2006).2.1 Query expansionThe problem with using comparable corpora tofind translation equivalents is that there is no ob-vious bridge between the two languages.
Unlikealigned parallel corpora, comparable corpora pro-vide a model for each individual language, whiledictionaries, which can serve as a bridge, are inad-equate for the task in question, because the prob-lem we want to address involves precisely transla-tion equivalents that are not listed there.Therefore, a specific query needs first to begeneralised in order to then retrieve a suitablecandidate from a set of candidates.
One wayto generalise the query is by using similarityclasses, i.e.
groups of words with lexically simi-lar behaviour.
In his work on distributional sim-ilarity (Lin, 1998) designed a parser to identifygrammatical relationships between words.
How-ever, broad-coverage parsers suitable for process-ing BNC-like corpora are not available for manylanguages.
Another, resource-light approach treatsthe context as a bag of words (BoW) and detectsthe similarity of contexts on the basis of colloca-tions in a window of a certain size, typically 3-4words, e.g.
(Rapp, 2004).
Even if using a parsercan increase precision in identification of contextsin the case of long-distance dependencies (e.g.
tocook Alice a whole meal), we can find a reason-able set of relevant terms returned using the BoWapproach, cf.
the results of human evaluation forEnglish and German by (Rapp, 2004).740For each source word s0 we produce a list ofsimilar words: ?
(s0) = s1, .
.
.
, sN (in our toolwe use N = 20 as the cutoff).
Since lists of dis-tributionally words can contain words irrelevant tothe source word, we filter them to produce a morereliable similarity class S(s0) using the assump-tion that the similarity classes of similar wordshave common members:?w ?
S(s0), w ?
?
(s0)&w ???
(si)This yields for experience the following similar-ity class: knowledge, opportunity, life, encounter,skill, feeling, reality, sensation, dream, vision,learning, perception, learn.1 Even if there is norequirement in the BoW approach that words inthe similarity class are of the same part of speech,it happens quite frequently that most words havethe same part of speech because of the similarityof contexts.2.2 Query translation and further expansionIn the next step we produce a translation class bytranslating all words from the similarity class intothe target language using a bilingual dictionary(T (w) for the translation of w).
Then for Step 3we have two options: a full translation class (TF )and a reduced one (TR).TF consists of similarity classes produced forall translations: S(T (S(s0))).
However, thiscauses a combinatorial explosion.
If a similarityclass contains N words (the average figure is 16)and a dictionary lists on average M equivalentsfor a source word (the average figure is 11), thisprocedure outputs on average M ?
N2 words inthe full translation class.
For instance, the com-plete translation class for experience contains 998words.
What is worse, some words from the fulltranslation class do not refer to the domain im-plied in the original expression because of the am-biguity of the translation operation.
For instance,the word dream belongs to the similarity class ofexperience.
Since it can be translated into Rus-sian as ??????
(?fairy-tale?
), the latter Russian wordwill be expanded in the full translation class withwords referring to legends and stories.
In the laterstages of the project, word sense disambiguationin corpora could improve precision of translationclasses.
However at the present stage we attemptto trade the recall of the tool for greater precisionby translating words in the source similarity class,1Ordered according to the score produced by the SingularValue Decomposition method as implemented by Rapp.and generating the similarity classes of transla-tions only for the source word:TR(s0) = S(T (s0)) ?
T (S(s0)).This reduces the class of experience to 128 words.This step crucially relies on a wide-coveragemachine readable dictionary.
The bilingual dictio-nary resources we use are derived from the sourcefile for the Oxford Russian Dictionary, providedby OUP.2.3 Filtering equivalence classesIn the final step we check all possible combina-tions of words from the translation classes for theirfrequency in target language corpora.The number of elements in the set of theoreti-cally possible combinations is usually very large:?Ti, where Ti is the number of words in the trans-lation class of each word of the original MWE.This number is much larger than the set of wordcombinations which is found in the target lan-guage corpora.
For instance, daunting experiencehas 202,594 combinations for the full translationclass of daunting experience and 6,144 for the re-duced one.
However, in the target language cor-pora we can find only 2,256 collocations with fre-quency > 2 for the full translation class and 92 forthe reduced one.Each theoretically possible combination is gen-erated and looked up in a database of MWEs(which is much faster than querying corpora forfrequencies of potential collocations).
The MWEdatabase was pre-compiled from corpora using amethod of filtering, similar to part-of-speech fil-tering suggested in (Justeson and Katz, 1995): incorpora each N-gram of length 2, 3 and 4 tokenswas checked against a set of filters.However, instead of pre-defined patterns for en-tire expressions our filtering method uses sets ofnegative constraints, which are usually applied tothe edges of expressions.
This change boosts re-call of retrieved MWEs and allows us to use thesame set of patterns for MWEs of different length.The filter uses constraints for both lexical andpart-of-speech features, which makes configura-tion specifications more flexible.The idea of applying a negative feature filterrather than a set of positive patterns is based onthe observation that it is easier to describe unde-sirable features than to enumerate complete lists ofpatterns.
For example, MWEs of any length end-ing with a preposition are undesirable (particles in741British news Russian newsno of words 217,394,039 77,625,002REs in filter 25 182-grams 6,361,596 5,457,8483-grams 14,306,653 11,092,9084-grams 19,668,956 11,514,626Table 1: MWEs in News Corporaphrasal verbs, which are desirable, are tagged dif-ferently by the Tree Tagger, so there is no problemwith ambiguity here).
Our filter captures this factby having a negative condition for the right edge ofthe pattern (regular expression /_IN$/), rather thanenumerating all possible configurations which donot contain a preposition at the end.
In this sensethe filter is permissive: everything that is not ex-plicitly forbidden is allowed, which makes the de-scription more economical.The same MWE database is used for check-ing frequencies of multiword collocates for cor-pus queries.
For this task, candidate N-grams inthe vicinity of searched patterns are filtered us-ing the same regular expression grammar of MWEconstraints, and then their corpus frequency ischecked in the database.
Thus scores for mul-tiword collocates can be computed from contin-gency tables similarly to single-word collocates.In addition, only MWEs with a frequencyhigher than 1 are stored in the database.
This fil-ters out most expressions that co-occur by chance.Table 1 gives an overview of the number of MWEsfrom the news corpus which pass the filter.
Othercorpora used in ASSIST (BNC and RRC) yieldsimilar results.
MWE frequencies for each corpuscan be checked individually or joined together.3 EvaluationThere are several attributes of our system whichcan be evaluated, and many of them are crucialfor its efficient use in the workflow of professionaltranslators, including: usability, quality of final so-lutions, trade-off between adequacy and fluencyacross usable examples, precision and recall of po-tentially relevant suggestions, as well as real-textevaluation, i.e.
?What is the coverage of difficulttranslation problems typically found in a text thatcan be successfully tackled?
?In this paper we focus on evaluating the qualityof potentially relevant translation solutions, whichis the central point for developing and calibrat-ing our methodology.
The evaluation experimentdiscussed below was specifically designed to as-sess the usefulness of translation suggestions gen-erated by our tool ?
in cases where translatorshave doubts about the usefulness of dictionary so-lutions.
In this paper we do not evaluate otherequally important aspects of the system?s func-tionality, which will be the matter of future re-search.3.1 Set-up of the experimentFor each translation direction we collected ten ex-amples of possibly recalcitrant translation prob-lems ?
words or phrases whose translation is notstraightforward in a given context.
Some of theseexamples were sent to us by translators in responseto our request for difficult cases.
For each exam-ple, which we included in the evaluation kit, theword or phrase either does not have a translation inORD (which is a kind of a baseline standard ref-erence for Russian translators), or its translationhas significantly lower frequency in a target lan-guage corpus in comparison to the frequency ofthe source expression.
If an MWE is not listed inavailable dictionaries, we produced compositional(word-for-word) translations using ORD.
In orderto remove a possible anti-dictionary bias from ourexperiment, we also checked translations in Mul-titran, an on-line translation dictionary, which wasoften quoted as one of the best resources for trans-lation from and into Russian.For each translation problem five solutions werepresented to translators for evaluation.
One or twoof these solutions were taken from a dictionary(usually from Multitran, and if available and dif-ferent, from ORD).
The other suggestions weremanually selected from lists of possible solutionsreturned by ASSIST.
Again, the criteria for se-lection were intuitive: we included those sugges-tions which made best sense in the given context.Dictionary suggestions and the output of ASSISTwere indistinguishable in the questionnaires to theevaluators.
The segments were presented in sen-tence context and translators had an option of pro-viding their own solutions and comments.
Ta-ble 2 shows one of the questions sent to evalua-tors.
The problem example is ??????
?????????
(?precise programme?
), which is presented in thecontext of a Russian sentence with the following(non-literal) translation This team should be puttogether by responsible politicians, who have a742Problem example??????
????????
?, as in???????
???
???????
??????
????????????????
?, ???????
??????
?????????
??????
????????
?.Translation suggestions Scoreclear planclear policyclear programmeclear strategyconcrete planYour suggestion ?
(optional)Table 2: Example of an entry in questionnaireclear strategy for resolving the current crisis.
Thethird translation equivalent (clear programme) inthe table is found in the Multitran dictionary (ORDoffers no translation for ??????
?????????).
Theexample was included because clear programmeis much less frequent in English (2 examples in theBNC) in comparison to ??????
?????????
in Rus-sian (70).
Other translation equivalents in Table 2are generated by ASSIST.We then asked professional translators affiliatedto a translator?s association (identity witheld at thisstage) to rate these five potential equivalents usinga five-point scale:5 = The suggestion is an appropriate translationas it is.4 = The suggestion can be used with some minoramendment (e.g.
by turning a verb into a par-ticiple).3 = The suggestion is useful as a hint for an-other, appropriate translation (e.g.
suggestionelated cannot be used, but its close synonymexhilarated can).2 = The suggestion is not useful, even though it isstill in the same domain (e.g.
fear is proposedfor a problem referring to hatred).1 = The suggestion is totally irrelevant.We received responses from eight translators.Some translators did not score all solutions, butthere were at least four independent judgementsfor each of the 100 translation variants.
An exam-ple of the combined answer sheet for all responsesto the question from Table 2 is given in Table 3 (t1,Translation t1 t2 t3 t4 t5 ?clear plan 5 5 3 4 4 0.84clear policy 5 5 3 4 4 0.84clear programme 5 5 3 4 4 0.84clear strategy 5 5 5 5 5 0.00concrete plan 1 5 3 3 5 1.67Best Dict 5 5 3 4 4 0.84Best Syst 5 5 5 5 5 0.00Table 3: Scores to translation equivalentst2,.
.
.
denote translators; the dictionary translationis clear programme).3.2 Interpretation of the resultsThe results were surprising in so far as for the ma-jority of problems translators preferred very differ-ent translation solutions and did not agree in theirscores for the same solutions.
For instance, con-crete plan in Table 3 received the score 1 fromtranslator t1 and 5 from t2.In general, the translators very often picked upon different opportunities presented by the sug-gestions from the lists, and most suggestions wereequally legitimate ways of conveying the intendedcontent, cf.
the study of legitimate translation vari-ation with respect to the BLEU score in (Babychand Hartley, 2004).
In this respect it may be unfairto compute average scores for each potential solu-tion, since for most interesting cases the scores donot fit into the normal distribution model.
So aver-aging scores would mask the potential usability ofreally inventive solutions.In this case it is more reasonable to evaluatetwo sets of solutions ?
the one generated by AS-SIST and the other found in dictionaries ?
but noteach solution individually.
In order to do that foreach translation problem the best scores given byeach translator in each of these two sets were se-lected.
This way of generalising data characterisesthe general quality of suggestion sets, and exactlymeets the needs of translators, who collectively getideas from the presented sets rather than from in-dividual examples.
This also allows us to mea-sure inter-evaluator agreement on the dictionaryset and the ASSIST set, for instance, via computingthe standard deviation ?
of absolute scores acrossevaluators (Table 3).
This appeared to be a veryinformative measure for dictionary solutions.In particular, standard deviation scores for thedictionary set (threshold ?
= 0.5) clearly split743Agreement: ?
for dictionary ?
0.5Example Dict ASSISTAve ?
Ave ?political upheaval 4.83 0.41 4.67 0.82Disagreement: ?
for dictionary >0.5Example Dict ASSISTAve ?
Ave ?clear defiance 4.14 0.90 4.60 0.55Table 4: Examples for the two groupsAgreement: ?
for dictionary ?
0.5Sub-group Dict ASSISTAve ?
Ave ?Agreement E?R 4.73 0.46 4.47 0.80Agreement R?E 4.90 0.23 4.52 0.60Agreement?All 4.81 0.34 4.49 0.70Disagreement: ?
for dictionary >0.5Sub-group Dict ASSISTAve ?
Ave ?Disagreement E?R 3.63 1.08 3.98 0.85Disagreement R?E 3.90 1.02 3.96 0.73Disagreement?All 3.77 1.05 3.97 0.79Table 5: Averages for the two groupsour 20 problems into two distinct groups: the firstgroup below the threshold contains 8 examples,for which translators typically agree on the qual-ity of dictionary solutions; and the second groupabove the threshold contains 12 examples, forwhich there is less agreement.
Table 4 shows someexamples from both groups and Table 5 presentsaverage evaluation scores and standard deviationfigures for both groups.Overall performance on all 20 examples is thesame for the dictionary responses and for the sys-tem?s responses: average of the mean top scoresis about 4.2 and average standard deviation of thescores is 0.8 in both cases (for set-best responses).This shows that ASSIST can reach the level ofperformance of a combination of two authoritativedictionaries for MWEs, while for its own transla-tion step it uses just a subset of one-word transla-tion equivalents from ORD.
However, there is an-other side to the evaluation experiment.
In fact, weare less interested in the system?s performance onall of these examples than on those examples forwhich there is greater disagreement among trans-lators, i.e.
where there is some degree of dissatis-faction with dictionary suggestions.012345impingepolitical upheavalcontroversial plandefuse tensions?????????????
??????????????????????????????????????????????????????????????????
?Figure 1: Agreement scores: dictionaryInterestingly, dictionary scores for the agree-ment group are always higher than 4, which meansthat whenever translators agreed on the dictionaryscores they were usually satisfied with the dictio-nary solution.
But they never agreed on the inap-propriateness of the dictionary: inappropriatenessrevealed itself in the form of low scores from sometranslators.This agreement/disagreement threshold can besaid to characterise two types of translation prob-lems: those for which there exist generally ac-cepted dictionary solutions, and those for whichtranslators doubt whether the solution is appropri-ate.
Best-set scores for these two groups of dic-tionary solutions ?
the agreement and disagree-ment group ?
are plotted on the radar charts inFigures 1 and 2 respectively.
The identifiers onthe charts are problematic source language expres-sions as used in the questionnaire (not translationsolutions to these problems, because a problemmay have several solutions preferred by differentjudges).
Scores for both translation directions arepresented on the same chart, since both follow thesame pattern and receive the same interpretation.Figure 1 shows that whenever there is littledoubt about the quality of dictionary solutions, theradar chart approaches a circle shape near the edgeof the chart.
In Figure 2 the picture is different:the circle is disturbed, and some scores frequentlyapproach the centre.
Therefore the disagreementgroup contains those translation problems wheredictionaries provide little help.The central problem in our evaluation experi-ment is whether ASSIST is helpful for problemsin the second group, where translators doubt thequality of dictionary solutions.Firstly, it can be seen from the charts that judge-744012345????????????????????????????????????
??????
???????????????????????????????????????????????
???????
?due processnegotiatedsettlementcleardefiancedaunting experiencepassionatelyseekrecreationalfearFigure 2: Disagreement scores: dictionary012345????????????????????????????????????
??????
???????????????????????????????????????????????
???????
?due processnegotiatedsettlementcleardefiancedaunting experiencepassionatelyseekrecreationalfearFigure 3: Disagreement scores: ASSISTments on the quality of the system output are moreconsistent: score lines for system output are closerto the circle shape in Figure 1 than those for dic-tionary solutions in Figure 2 (formally: the stan-dard deviation of evaluation scores, presented inTable 4, is lower).Secondly, as shown in Table 4, in this group av-erage evaluation scores are slightly higher for AS-SIST output than for dictionary solutions (3.97 vs3.77) ?
in the eyes of human evaluators ASSISToutperforms good dictionaries.
For good dictio-nary solutions ASSIST performance is slightlylower: (4.49 vs 4.81), but the standard deviationis about the same.Having said this, solutions from our system arereally not in competition with dictionary solutions:they provide less literal translations, which oftenemerge in later stages of the translation task, whentranslators correct and improve an initial draft,where they have usually put more literal equiva-lents (Shveitser, 1988).
It is a known fact in trans-lation studies that non-literal solutions are harderto see and translators often find them only uponlonger reflection.
Yet another fact is that non-literal translations often require re-writing othersegments of the sentence, which may not be ob-vious at first glance.4 Conclusions and future workThe results of evaluation show that the tool issuccessful in finding translation equivalents for arange of examples.
What is more, in cases wherethe problem is genuinely difficult, ASSIST consis-tently provides scores around 4 ?
?minor adapta-tions needed?.
The precision of the tool is low, itsuggests 50-100 examples with only 2-4 useful forthe current context.
However, recall of the outputis more relevant than precision, because transla-tors typically need just one solution for their prob-lem, and often have to look through reasonablylarge lists of dictionary translations and examplesto find something suitable for a problematic ex-pression.
Even if no immediately suitable trans-lation can be found in the list of suggestions, itfrequently contains a hint for solving the problemin the absence of adequate dictionary information.The current implementation of the model is re-stricted in several respects.
First, the majority oftarget language constructions mirror the syntacticstructure of the source language example.
Evenif the procedure for producing similarity classesdoes not impose restrictions on POS properties,nevertheless words in the similarity class tend tofollow the POS of the original word, because ofthe similarity of their contexts of use.
Further-more, dictionaries also tend to translate wordsusing the same POS.
This means that the ex-isting method finds mostly NPs for NPs, verb-object pairs for verb-object pairs, etc, even if themost natural translation uses a different syntacticstructure, e.g.
I like doing X instead of I do Xgladly (when translating from German ich macheX gerne).Second, suggestions are generated for the queryexpression independently from the context it isused in.
For instance, the words judicial, militaryand religious are in the similarity class of politi-cal, just as reform is in the simclass of upheaval.So the following exampleThe plan will protect EC-based investors in Russiafrom political upheavals damaging their business.creates a list of ?possible translations?
evokingvarious reforms and transformations.745These issues can be addressed by introduc-ing a model of the semantic context of situation,e.g.
?changes in business practice?
as in the ex-ample above, or ?unpleasant situation?
as in thecase of daunting experience.
This will allowless restrictive identification of possible transla-tion equivalents, as well as reduction of sugges-tions irrelevant for the context of the current ex-ample.Currently we are working on an option to iden-tify semantic contexts by means of ?semantic sig-natures?
obtained from a broad-coverage seman-tic parser, such as USAS (Rayson et al, 2004).The semantic tagset used by USAS is a language-independent multi-tier structure with 21 major dis-course fields, subdivided into 232 sub-categories(such as I1.1- = Money: lack; A5.1- = Eval-uation: bad), which can be used to detect thesemantic context.
Identification of semanticallysimilar situations can be also improved by theuse of segment-matching algorithms as employedin Example-Based MT (EBMT) and translationmemories (Planas and Furuse, 2000; Carl andWay, 2003).The proposed model looks similar to some im-plementations of statistical machine translation(SMT), which typically uses a parallel corpus forits translation model, and then finds the best possi-ble recombination that fits into the target languagemodel (Och and Ney, 2003).
Just like an MT sys-tem, our tool can find translation equivalents forqueries which are not explicitly coded as entriesin system dictionaries.
However, from the userperspective it resembles a dynamic dictionary orthesaurus: it translates difficult words and phrases,not entire sentences.
The main thrust of our sys-tem is its ability to find translation equivalents fordifficult contexts where dictionary solutions do notexist, are questionable or inappropriate.AcknowledgementsThis research is supported by EPSRC grantEP/C005902.ReferencesBogdan Babych and Anthony Hartley.
2004.
Ex-tending the BLEU MT evaluation method with fre-quency weightings.
In Proceedings of the 42d An-nual Meeting of the Association for ComputationalLinguistics, Barcelona.Michael Carl and Andy Way, editors.
2003.
Re-cent advances in example-based machine transla-tion.
Kluwer, Dordrecht.Ido Dagan and Kenneth Church.
1997.
Ter-might: Coordinating humans and machines in bilin-gual terminology acquisition.
Machine Translation,12(1/2):89?107.Gregory Grefenstette.
2002.
Multilingual corpus-based extraction and the very large lexicon.
In LarsBorin, editor, Language and Computers, Parallelcorpora, parallel worlds, pages 137?149.
Rodopi.John S. Justeson and Slava M. Katz.
1995.
Techninalterminology: some linguistic properties and an al-gorithm for identification in text.
Natural LanguageEngineering, 1(1):9?27.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Joint COLING-ACL-98, pages768?774, Montreal.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Alan Partington.
1998.
Patterns and meanings: usingcorpora for English language research and teach-ing.
John Benjamins, Amsterdam.Emmanuel Planas and Osamu Furuse.
2000.
Multi-level similar segment matching algorithm for trans-lation memories and example-based machine trans-lation.
In COLING, 18th International Conferenceon Computational Linguistics, pages 621?627.Reinhard Rapp.
2004.
A freely available automaticallygenerated thesaurus of related words.
In Proceed-ings of the Forth Language Resources and Evalua-tion Conference, LREC 2004, pages 395?398, Lis-bon.Paul Rayson, Dawn Archer, Scott Piao, and TonyMcEnery.
2004.
The UCREL semantic analysissystem.
In Proc.
Beyond Named Entity RecognitionWorkshop in association with LREC 2004, pages 7?12, Lisbon.Serge Sharoff.
2006.
Creating general-purposecorpora using automated search engine queries.In Marco Baroni and Silvia Bernardini, editors,WaCky!
Working papers on the Web as Corpus.Gedit, Bologna.A.D.
Shveitser.
1988.
??????
????????
: ?????
?, ???-????
?, ???????.
Nauka, Moskow.
(In Russian:Theory of Translation: Status, Problems, Aspects).Krista Varantola.
2003.
Translators and disposablecorpora.
In Federico Zanettin, Silvia Bernardini,and Dominic Stewart, editors, Corpora in Transla-tor Education, pages 55?70.
St Jerome, Manchester.746
