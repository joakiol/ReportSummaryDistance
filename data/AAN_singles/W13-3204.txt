Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 30?39,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsLetter N-Gram-based Input Encoding for Continuous Space LanguageModelsHenning Sperr?, Jan Niehues?
and Alexander Waibel?Institute of AnthropomaticsKIT - Karlsruhe Institute of TechnologyKarlsruhe, Germany?
firstname.lastname@kit.edu?
henning.sperr@student.kit.eduAbstractWe present a letter-based encoding forwords in continuous space language mod-els.
We represent the words completely byletter n-grams instead of using the wordindex.
This way, similar words will au-tomatically have a similar representation.With this we hope to better generalizeto unknown or rare words and to alsocapture morphological information.
Weshow their influence in the task of machinetranslation using continuous space lan-guage models based on restricted Boltz-mann machines.
We evaluate the trans-lation quality as well as the training timeon a German-to-English translation task ofTED and university lectures as well as onthe news translation task translating fromEnglish to German.
Using our new ap-proach a gain in BLEU score by up to 0.4points can be achieved.1 IntroductionLanguage models play an important role in naturallanguage processing.
The most commonly usedapproach is n-gram-based language models (Chenand Goodman, 1999).In recent years Continuous Space LanguageModels (CSLMs) have gained a lot of atten-tion.
Compared to standard n-gram-based lan-guage models they promise better generalizationto unknown histories or n-grams with only fewoccurrences.
Since the words are projected intoa continuous space, true interpolation can be per-formed when an unseen sample appears.
The stan-dard input layer for CSLMs is a so called 1-of-n coding where a word is represented as a vectorwith a single neuron turned on and the rest turnedoff.
In the standard approach it is problematic toinfer probabilities for words that are not inside thevocabulary.
Sometimes an extra unknown neu-ron is used in the input layer to represent thesewords (Niehues and Waibel, 2012).
Since all un-seen words get mapped to the same neuron, no realdiscrimination between those words can be done.Furthermore, rare words are also hard to model,since there is too few training data available to es-timate their associated parameters.We try to overcome these shortcomings byusing subword features to cluster similar wordscloser together and generalize better over unseenwords.
We hope that words containing similar let-ter n-grams will yield a good indicator for wordsthat have the same function inside the sentence.Introducing a method for subword units also hasthe advantage that the input layer can be smaller,while still representing nearly the same vocabularywith unique feature vectors.
By using a smaller in-put layer, less weights need to be trained and thetraining is faster.
In this work we present the lettern-gram approach to represent words in an CSLM,and compare it to the word-based CSLM presentedin Niehues and Waibel (2012).The rest of this paper is structured as follows:First we will give an overview of related work.After that we give a brief overview of restrictedBoltzmann machines which are the basis of theletter-based CSLM presented in Section 4.
Thenwe will present the results of the experiments andconclude our work.2 Related WorkFirst research on neural networks to predict wordcategories has been done in Nakamura et al(1990) where neural networks were used to pre-dict word categories.
Xu and Rudnicky (2000)proposed a language model that has an input con-sisting of one word and no hidden units.
Thisnetwork was limited to infer unigram and bigramstatistics.
There has been research on feed for-ward neural network language models where they30achieved a decrease in perplexity compared tostandard n-gram language models (Bengio et al2003).
In Schwenk and Gauvain (2005) and laterin Schwenk (2007) research was performed ontraining large scale neural network language mod-els on millions of words resulting in a decrease ofthe word error rate for continuous speech recog-nition.
In Schwenk et al(2006) they use theCSLM framework to rescore n-best lists of a ma-chine translation system during tuning and testingsteps.
Usually these networks use short lists toreduce the size of the output layer and to makecalculation feasible.
There have been approachesto optimize the output layer of such a network,so that vocabularies of arbitrary size can be usedand there is no need to back off to a smaller n-gram model (Le et al 2011).
In this StructuredOutput Layer (SOUL) neural network languagemodel a hierarchical output layer was chosen.
Re-current Neural Networks have also been used totry and improve language model perplexities inMikolov et al(2010), concluding that RecurrentNeural Networks potentially improve over classi-cal n-gram language models with increasing dataand a big enough hidden unit size of the model.In the work of Mnih and Hinton (2007) and Mnih(2010) training factored restricted Boltzmann ma-chines yielded no gain compared to Kneser-Neysmoothed n-gram models.
But it has been shownin Niehues and Waibel (2012), that using a re-stricted Boltzmann machine with a different layoutduring decoding can yield an increase in BLEUscore.
There has also been a lot of research inthe field of using subword units for language mod-eling.
In Shaik et al(2011) linguistically moti-vated sub-lexical units were proposed to improveopen vocabulary speech recognition for German.Research on morphology-based and subword lan-guage models on a Turkish speech recognition taskhas been done by Sak et al(2010).
The ideaof Factored Language models in machine transla-tion has been introduced by Kirchhoff and Yang(2005).
Similar approaches to develop joint lan-guage models for morphologically rich languagesin machine translation have been presented bySarikaya and Deng (2007).
In Emami et al(2008)a factored neural network language model for Ara-bic was built.
They used different features such assegmentation, part-of-speech and diacritics to en-rich the information for each word.3 Restricted Boltzmann Machine-basedLanguage ModelIn this section we will briefly review the con-tinuous space language models using restrictedBoltzmann machines (RBM).
We will focus onthe parts that are important for the implementa-tion of the input layers described in the next sec-tion.
A restricted Boltzmann machine is a gener-ative stochastic neural network which consists ofa visible and a hidden layer of neurons that haveunidirectional connections between the layers butno inner layer connections as shown in Figure 1.VisibleHiddenFigure 1: Restricted Boltzmann Machine.The activation of the visible neurons will be de-termined by the input data.
The standard inputlayer for neural network language models uses a1-of-n coding to insert a word from the vocabularyinto the network.
This is a vector, where only theindex of the word in the vocabulary is set to oneand the rest to zero.
Sometimes this is also referredto as a softmax layer of binary units.
The activa-tion of the hidden units is usually binary and willbe inferred from the visible units by using sam-pling techniques.
In Niehues and Waibel (2012)an n-gram Boltzmann machine language model isproposed using such a softmax layer for each con-text.
In this work, we want to explore differentways of encoding the word observations in the in-put layer.
Figure 2 is an example of the originalmodel with three hidden units, two contexts anda vocabulary of two words.
In this example thebigram my house is modeled.To calculate the probability of a visible config-uration v we will use the definition of the free en-ergy in a restricted Boltzmann machine with bi-nary stochastic hidden units, which isF (v) = ?
?iviai ?
?jlog(1 + exj ) (1)where ai is the bias of the ith visible neuron vi and31Visible<s> </s> my house <s> </s> my houseHiddenFigure 2: RBMLM with three hidden units and avocabulary size of two words and two word con-texts, where activated units are marked as black.xj is the activation of the jth hidden neuron.
Theactivation xj is defined asxj = bj +?iviwij (2)where bj is the bias of the jth hidden neuron andwij is the weight between visible unit vi and hid-den unit xj .
Using these definitions, the probabil-ity of our visible configuration v isp(v) =1Ze?F (v) (3)with the partition function Z =?v e?F (v) beingthe normalization constant.
Usually this normal-ization constant is not easy to compute since it isa sum over an exponential amount of values.
Weknow that the free energy will be proportional tothe true probability of our visible vector, this isthe reason for using the free energy as a featurein our log-linear model instead of the true prob-ability.
In order to use it as a feature inside thedecoder we actually need to be able to computethe probability for a whole sentence.
As shown inNiehues and Waibel (2012) we can do this by sum-ming over the free energy of all n-grams containedin the sentence.3.1 TrainingFor training the restricted Boltzmann machine lan-guage model (RBMLM) we used the ContrastiveDivergence (CD) algorithm as proposed in Hinton(2010).
In order to do this, we need to calculate thederivation of the probability of the example giventhe weights?
log p(v)?wij= <vihj>data ?<vihj>model (4)where <vihj>model is the expected value of vihjgiven the distribution of the model.
In otherwords we calculate the expectation of how oftenvi and hj are activated together, given the dis-tribution of the data, minus the expectation ofthem being activated together given the distribu-tion of the model, which will be calculated us-ing Gibbs-Sampling techniques.
Usually manysteps of Gibbs-Sampling are necessary to get anunbiased sample from the distribution, but in theContrastive Divergence algorithm only one step ofsampling is performed (Hinton, 2002).4 Letter-based Word EncodingIn this section we will describe the proposed in-put layers for the RBMLM.
Compared to the wordindex-based representation explained above, wetry to improve the capability to handle unknownwords and morphology by splitting the word intosubunits.4.1 MotivationIn the example mentioned above, the word indexmodel might be able to predict my house but itwill fail on my houses if the word houses is not inthe training vocabulary.
In this case, a neuron thatclassifies all unknown tokens or some other tech-niques to handle such a case have to be utilized.In contrast, a human will look at the single let-ters and see that these words are quite similar.
Hewill most probably recognize that the appended sis used to mark the plural form, but both words re-fer to the same thing.
So he will be able to inferthe meaning although he has never seen it before.Another example in English are be the wordshappy and unhappy.
A human speaker who doesnot know the word unhappy will be able to knowfrom the context what unhappy means and he canguess that both of the words are adjectives, thathave to do with happiness, and that they can beused in the same way.In other languages with a richer morphology,like German, this problem is even more important.The German word scho?n (engl.
beautiful) can have16 different word forms, depending on case, num-ber and gender.Humans are able to share information aboutwords that are different only in some morphemeslike house and houses.
With our letter-based inputencoding we want to generalize over the commonword index model to capture morphological infor-32mation about the words to make better predictionsfor unknown words.4.2 FeaturesIn order to model the aforementioned morpholog-ical word forms, we need to create features forevery word that represent which letters are usedin the word.
If we look at the example of house,we need to model that the first letter is an h, thesecond is an o and so on.If we want to encode a word this way, we havethe problem that we do not have a fixed size offeatures, but the feature size depends on the lengthof the word.
This is not possible in the frameworkof continuous space language models.
Therefore,a different way to represent the word is needed.An approach for having a fixed size of featuresis to just model which letters occur in the word.In this case, every input word is represented by avector of dimension n, where n is the size of thealphabet in the text.
Every symbol, that is usedin the word is set to one and all the other featuresare zero.
By using a sparse representation, whichshows only the features that are activated, the wordhouse would be represented byw1 = e h o s uThe main problem of this representation is thatwe lose all information about the order of the let-ters.
It is no longer possible to see how the wordends and how the word starts.
Furthermore, manywords will be represented by the same feature vec-tor.
For example, in our case the words house andhouses would be identical.
In the case of housesand house this might not be bad, but the wordsshortest and others or follow and wolf will alsomap to the same input vector.
These words haveno real connection as they are different in mean-ing and part of speech.Therefore, we need to improve this approachto find a better model for input words.
N-gramsof words or letters have been successfully used tomodel sequences of words or letters in languagemodels.
We extend our approach to model notonly the letters that occur in the in the word, butthe letter n-grams that occur in the word.
Thiswill of course increase the dimension of the fea-ture space, but then we are able to model the orderof the letters.
In the example of my house the fea-ture vector will look likew1 = my <w>m y</w>w2 = ho ou se us <w>h e</w>We added markers for the beginning and end ofthe word because this additional information is im-portant to distinguish words.
Using the exampleof the word houses, modeling directly that the lastletter is an s could serve as an indication of a pluralform.If we use higher order n-grams, this will in-crease the information about the order of the let-ters.
But these letter n-grams will also occur morerarely and therefore, the weights of these featuresin the RBM can no longer be estimated as reliably.To overcome this, we did not only use the n-gramsof order n, but all n-grams of order n and smaller.In the last example, we will not only use the bi-grams, but also the unigrams.This means my house is actually represented asw1 = m y my <w>m y</w>w2 = e h o s u ho ou se us <w>h e</w>With this we hope to capture many morpholog-ical variants of the word house.
Now the represen-tations of the words house and houses differ onlyin the ending and in an additional bigram.houses = ... es s</w>house = ... se e</w>The beginning letters of the two words will con-tribute to the same free energy only leaving theending letter n-grams to contribute to the differentusages of houses and house.The actual layout of the model can be seen inFigure 3.
For the sake of clarity we left out theunigram letters.
In this representation we now donot use a softmax input layer, but a logistic inputlayer defined asp(vi = on) =11 + e?xi(5)where vi is the ith visible neuron and xi is the in-put from the hidden units for the ith neuron de-fined asxi = ai +?jhjwij (6)with ai being the bias of the visible neuron vi andwij being the weight between the hidden unit hjand vi.33Visible<i>>e/myho ou myhH dssemyho ou umnyh semyhH?ds umnyh?Figure 3: A bigram letter index RBMLM withthree hidden units and two word contexts, whereactivated units are marked as black.4.3 Additional InformationThe letter index approach can be extended byseveral features to include additional informationabout the words.
This could for example be part-of-speech tags or other morphological informa-tion.
In this work we tried to include a neuronto capture capital letter information.
To do this weincluded a neuron that will be turned on if the firstletter was capitalized and another neuron that willbe turned on if the word was written in all capitalletters.
The word itself will be lowercased after weextracted this information.Using the example of European Union, the newinput vector will look like thisw1 =a e n o p r u an ea eu op pe ro ur<w>e n</w><CAPS>w2 =u i n o un io ni on<w>u n</w><CAPS>This will lead to a smaller letter n-gram vocab-ulary since all the letter n-grams get lowercased.This also means there is more data for each of theletter n-gram neurons that were treated differentlybefore.
We also introduced an all caps featurewhich is turned on if the whole word was writtenin capital letters.
We hope that this can help detectabbreviations which are usually written in all cap-ital letters.
For example EU will be representedasw1 = e u eu <w>e u</w><ALLCAPS>5 EvaluationWe evaluated the RBM-based language modelon different statistical machine translation (SMT)tasks.
We will first analyze the letter-based wordrepresentation.
Then we will give a brief descrip-tion of our SMT system.
Afterwards, we de-scribe in detail our experiments on the German-to-English translation task.
We will end with addi-tional experiments on the task of translating Eng-lish news documents into German.5.1 Word RepresentationIn first experiments we analyzed whether theletter-based representation is able to distinguishbetween different words.
In a vocabulary of27,748 words, we compared for different letter n-gram sizes how many words are mapped to thesame input feature vector.Table 1 shows the different models, their inputdimensions and the total number of unique clus-ters as well as the amount of input vectors con-taining one, two, three or four or more words thatget mapped to this input vector.
In the word indexrepresentation every word has its own feature vec-tor.
In this case the dimension of the input vectoris 27,748 and each word has its own unique inputvector.If we use only letters, as done in the unigrammodel, only 62% of the words have a unique repre-sentation.
Furthermore, there are 606 feature vec-tors representing 4 or more words.
This type ofencoding of the words is not sufficient for the task.When using a bigram letter context nearly eachof the 27,748 words has a unique input represen-tation, although the input dimension is only 7%compared to the word index.
With the three let-ter vocabulary context and higher there is no inputvector that represents more than three words fromthe vocabulary.
This is good since we want similarwords to be close together but not have exactly thesame input vector.
The words that are still clus-tered to the same input are mostly numbers or typ-ing mistakes like ?YouTube?
and ?Youtube?.5.2 Translation System DescriptionThe translation system for the German-to-Englishtask was trained on the European Parliament cor-pus, News Commentary corpus, the BTEC cor-pus and TED talks1.
The data was preprocessedand compound splitting was applied for German.Afterwards the discriminative word alignment ap-proach as described in Niehues and Vogel (2008)was applied to generate the alignments betweensource and target words.
The phrase table was1http://www.ted.com34#Vectors mapping toModel Caps VocSize TotalVectors 1 Word 2 Words 3 Words 4+ WordsWordIndex - 27,748 27,748 27,748 0 0 0Letter 1-gram No 107 21,216 17,319 2,559 732 606Letter 2-gram No 1,879 27,671 27,620 33 10 8Letter 3-gram No 12,139 27,720 27,701 10 9 0Letter 3-gram Yes 8,675 27,710 27,681 20 9 0Letter 4-gram No 43,903 27,737 27,727 9 1 0Letter 4-gram Yes 25,942 27,728 27,709 18 1 0Table 1: Comparison of the vocabulary size and the possibility to have a unique representation of eachword in the training corpus.built using the scripts from the Moses package de-scribed in Koehn et al(2007).
A 4-gram languagemodel was trained on the target side of the paralleldata using the SRILM toolkit from Stolcke (2002).In addition, we used a bilingual language model asdescribed in Niehues et al(2011).
Reordering wasperformed as a preprocessing step using part-of-speech (POS) information generated by the Tree-Tagger (Schmid, 1994).
We used the reorder-ing approach described in Rottmann and Vogel(2007) and the extensions presented in Niehues etal.
(2009) to cover long-range reorderings, whichare typical when translating between German andEnglish.
An in-house phrase-based decoder wasused to generate the translation hypotheses andthe optimization was performed using the MERTimplementation as presented in Venugopal et al(2005).
All our evaluation scores are measured us-ing the BLEU metric.We trained the RBMLM models on 50K sen-tences from TED talks and optimized the weightsof the log-linear model on a separate set of TEDtalks.
For all experiments the RBMLMs have beentrained with a context of four words.
The devel-opment set consists of 1.7K segments containing16K words.
We used two different test sets toevaluate our models.
The first test set containsTED talks with 3.5K segments containing 31Kwords.
The second task was from an in-housecomputer science lecture corpus containing 2.1Ksegments and 47K words.
For both tasks we usedthe weights optimized on TED.For the task of translating English news textsinto German we used a system developed for theWorkshop on Machine Translation (WMT) eval-uation.
The continuous space language modelswere trained on a random subsample of 100K sen-tences from the monolingual training data used forthis task.
The out-of-vocabulary rates for the TEDtask are 1.06% while the computer science lec-tures have 2.73% and nearly 1% on WMT.5.3 German-to-English TED TaskThe results for the translation of German TED lec-tures into English are shown in Table 2.
The base-line system uses a 4-gram Kneser-Ney smoothedlanguage model trained on the target side paralleldata.
We then added a RBMLM, which was onlytrained on the English side of the TED corpus.If the word index RBMLM trained for one iter-ation using 32 hidden units is added, an improve-ment of about 1 BLEU can be achieved.
The let-ter bigram model performs about 0.4 BLEU pointsbetter than no additional model, but significantlyworse then the word index model or the other let-ter n-gram models.
The letter 3- to 5-gram-basedmodels obtain similar BLEU scores, varying onlyby 0.1 BLEU point.
They also achieve a 0.8 to0.9 BLEU points improvement against the base-line system and a 0.2 to 0.1 BLEU points decreasethan the word index-based encoding.System Dev TestBaseline 26.31 23.02+WordIndex 27.27 24.04+Letter 2-gram 26.67 23.44+Letter 3-gram 26.80 23.84+Letter 4-gram 26.79 23.93+Letter 5-gram 26.64 23.82Table 2: Results for German-to-English TEDtranslation taskUsing the word index model with the first base-line system increases the BLEU score nearly asmuch as adding a n-gram-based language modeltrained on the TED corpus as done in the base-35line of the systems presented in Table 3.
In theseexperiments all letter-based models outperformedthe baseline system.
The bigram-based languagemodel performs worst and the 3- and 4-gram-based models perform only slightly worse than theword index-based model.System Dev TestBaseline+ngram 27.45 24.06+WordIndex 27.70 24.34+Letter 2-gram 27.45 24.15+Letter 3-gram 27.52 24.25+Letter 4-gram 27.60 24.30Table 3: Results of German-to-English TED trans-lations using an additional in-domain languagemodel.A third experiment is presented in Table 4.
Herewe also applied phrase table adaptation as de-scribed in Niehues et al(2010).
In this experimentthe word index model improves the system by 0.4BLEU points.
In this case all letter-based modelsperform very similar.
They are again performingslightly worse than the word index-based system,but better than the baseline system.To summarize the results, we could always im-prove the performance of the system by addingthe letter n-gram-based language model.
Further-more, in most cases, the bigram model performsworse than the higher order models.
It seems to beimportant for this task to have more context infor-mation.
The 3- and 4-gram-based models performalmost equal, but slightly worse than the wordindex-based model.System Dev TestBL+ngram+adaptpt 28.40 24.57+WordIndex 28.55 24.96+Letter 2-gram 28.31 24.80+Letter 3-gram 28.31 24.71+Letter 4-gram 28.46 24.65Table 4: Results of German-to-English TED trans-lations with additional in-domain language modeland adapted phrase table.5.3.1 Caps FeatureIn addition, we evaluated the proposed caps fea-ture compared to the non-caps letter n-gram modeland the baseline systems.
As we can see in Ta-ble 5, caps sometimes improves and sometimesdecreases the BLEU score by about ?0.2 BLEUpoints.
One reason for that might be that mostEnglish words are written lowercased, thereforewe do not gain much information.System Dev TestBaseline 26.31 23.02+Letter 3-gram 26.80 23.84+Letter 3-gram+caps 26.67 23.85Baseline+ngram 27.45 24.06+Letter 3-gram 27.52 24.25+Letter 3-gram+caps 27.60 24.47BL+ngram+adaptpt 28.40 24.57+Letter 3-gram 28.31 24.71+Letter 3-gram+caps 28.43 24.66Table 5: Difference between caps and non-capsletter n-gram models.5.4 German-to-English CSL TaskAfter that, we evaluated the computer science lec-ture (CSL) test set.
We used the same system asfor the TED translation task.
We did not performa new optimization, since we wanted so see howwell the models performed on a different task.The results are summarized in Table 6.
In thiscase the baseline is outperformed by the word in-dex approach by approximately 1.1 BLEU points.Except for the 4-gram model the results are similarto the result for the TED task.
All systems couldagain outperform the baseline.System TestBaseline 23.60+WordIndex 24.76+Letter 2-gram 24.17+Letter 3-gram 24.36+Letter 4-gram 23.82Table 6: Results the baseline of the German-to-English CSL task.The system with the additional in-domain lan-guage model in Table 7 shows that both lettern-gram language models perform better than thebaseline and the word index model, improving thebaseline by about 0.8 to 1 BLEU.
Whereas theword index model only achieved an improvementof 0.6 BLEU points.The results of the system with the additionalphrase table adaption can be seen in Table 8.
The36System TestBaseline+ngram 23.81+WordIndex 24.41+Letter 2-gram 24.37+Letter 3-gram 24.66+Letter 4-gram 24.85Table 7: Results on German-to-English CSL cor-pus with additional in-domain language model.word index model improves the baseline by 0.25BLEU points.
The letter n-gram models improvethe baseline by about 0.3 to 0.4 BLEU points alsoimproving over the word index model.
The letterbigram model in this case performs worse than thebaseline.System TestBL+ngram+adaptpt 25.00+WordIndex 25.25+Letter 2-gram 24.68+Letter 3-gram 25.43+Letter 4-gram 25.33Table 8: Results on German-to-English CSL withadditional in-domain language model and adaptedphrase table.In summary, again the 3- and 4-gram letter mod-els perform mostly better than the bigram version.They both perform mostly equal.
In contrast to theTED task, they were even able to outperform theword index model in some configurations by up to0.4 BLEU points.5.5 English-to-German News TaskWhen translating English-to-German news wecould not improve the performance of the base-line by using a word index model.
In contrast, theperformance dropped by 0.1 BLEU points.
If weuse a letter bigram model, we could improve thetranslation quality by 0.1 BLEU points over thebaseline system.System Dev TestBaseline 16.90 17.36+WordIndex 16.79 17.29+Letter 2-gram 16.91 17.48Table 9: Results for WMT2013 task English-to-German.5.6 Model Size and Training TimesIn general the letter n-gram models perform al-most as good as the word index model on Englishlanguage tasks.
The advantage of the models up tothe letter 3-gram context model is that the trainingtime is lower compared to the word index model.All the models were trained using 10 cores anda batch size of 10 samples per contrastive diver-gence update.
As can be seen in Table 10 the letter3-gram model needs less than 50% of the weightsand takes around 75% of the training time of theword index model.
The four letter n-gram modeltakes longer to train due to more parameters.Model #Weights TimeWordIndex 3.55 M 20 h 10 minLetter 2-gram 0.24 M 1h 24 minLetter 3-gram 1.55 M 15 h 12 minLetter 4-gram 5.62 M 38 h 59 minTable 10: Training time and number of parametersof the RBMLM models.6 ConclusionsIn this work we presented the letter n-gram-basedinput layer for continuous space language models.The proposed input layer enables us to encode thesimilarity of unknown words directly in the inputlayer as well as to directly model some morpho-logical word forms.We evaluated the encoding on different trans-lation tasks.
The RBMLM using this encod-ing could always improve the translation qual-ity and perform similar to a RBMLM based onword indices.
Especially in the second configu-ration which had a higher OOV rate, the letter n-gram model performed better than the word indexmodel.
Moreover, the model based on letter 3-grams uses only half the parameters of the wordindex model.
This reduced the training time of thecontinuous space language model by a quarter.AcknowledgmentsThis work was partly achieved as part of theQuaero Programme, funded by OSEO, FrenchState agency for innovation.
The research lead-ing to these results has received funding fromthe European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementn?
287658.37ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A Neural Probabilistic Lan-guage Model.
Journal of Machine Learning Re-search, 3:1137?1155.Stanley F. Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques for lan-guage modeling.
Computer Speech & Language,13(4):359?393.Ahmad Emami, Imed Zitouni, and Lidia Mangu.
2008.Rich morphology based n-gram language models forarabic.
In INTERSPEECH, pages 829?832.
ISCA.Geoffrey E. Hinton.
2002.
Training products of ex-perts by minimizing contrastive divergence.
NeuralComput., 14(8):1771?1800, August.Geoffrey Hinton.
2010.
A Practical Guide to TrainingRestricted Boltzmann Machines.
Technical report,University of Toronto.Katrin Kirchhoff and Mei Yang.
2005.
Improved Lan-guage Modeling for Statistical Machine Translation.In Proceedings of the ACL Workshop on Buildingand Using Parallel Texts, pages 125?128, Ann Ar-bor, Michigan, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In ACL 2007, Demonstration Session,Prague, Czech Republic.Hai Son Le, Ilya Oparin, Abdelkhalek Messaoudi,Alexandre Allauzen, Jean-Luc Gauvain, andFranc?ois Yvon.
2011.
Large vocabulary soul neuralnetwork language models.
In INTERSPEECH 2011,12th Annual Conference of the International SpeechCommunication Association, Florence, Italy.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH 2010, 11th Annual Conference of theInternational Speech Communication Association,Makuhari, Chiba, Japan.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th international conferenceon Machine Learning, ICML ?07, pages 641?648,New York, NY, USA.
ACM.Andriy Mnih.
2010.
Learning distributed representa-tions for statistical language modelling and collabo-rative filtering.
Ph.D. thesis, University of Toronto,Toronto, Ont., Canada.
AAINR73159.Masami Nakamura, Katsuteru Maruyama, TakeshiKawabata, and Kiyohiro Shikano.
1990.
Neuralnetwork approach to word category prediction forenglish texts.
In Proceedings of the 13th conferenceon Computational linguistics - Volume 3, COLING?90, pages 213?218, Stroudsburg, PA, USA.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues and Stephan Vogel.
2008.
DiscriminativeWord Alignment via Alignment Matrix Modeling.In Proceedings of the Third Workshop on Statisti-cal Machine Translation, StatMT ?08, pages 18?25,Stroudsburg, PA, USA.Jan Niehues and Alex Waibel.
2012.
ContinuousSpace Language Models using Restricted Boltz-mann Machines.
In Proceedings of the InternationalWorkshop for Spoken Language Translation (IWSLT2012), Hong Kong.Jan Niehues, Mohammed Mediani, Teresa Herrmann,Michael Heck, Christian Herff, and Alex Waibel.2010.
The KIT Translation system for IWSLT 2010.In Proceedings of the Seventh International Work-shop on Spoken Language Translation (IWSLT),Paris, France.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, UK.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI, Sko?vde,Sweden.Hasim Sak, Murat Saraclar, and Tunga Gungor.
2010.Morphology-based and sub-word language model-ing for Turkish speech recognition.
In 2010 IEEEInternational Conference on Acoustics Speech andSignal Processing (ICASSP), pages 5402?5405.Ruhi Sarikaya and Yonggang Deng.
2007.
JointMorphological-Lexical Language Modeling for Ma-chine Translation.
In Human Language Technolo-gies 2007: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics; Companion Volume, Short Papers, pages145?148, Rochester, New York, USA.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, UK.Holger Schwenk and Jean-Luc Gauvain.
2005.
Train-ing neural network language models on very largecorpora.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages201?208, Stroudsburg, PA, USA.38Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-vain.
2006.
Continuous Space Language mModelsfor Statistical Machine T ranslation.
In Proceedingsof the COLING/ACL on Main conference poster ses-sions, COLING-ACL ?06, pages 723?730, Strouds-burg, PA, USA.Holger Schwenk.
2007.
Continuous Space LanguageModels.
Comput.
Speech Lang., 21(3):492?518,July.M.
Ali Basha Shaik, Amr El-Desoky Mousa, RalfSchlu?ter, and Hermann Ney.
2011.
Hybrid languagemodels using mixed types of sub-lexical units foropen vocabulary german lvcsr.
In INTERSPEECH2011, 12th Annual Conference of the InternationalSpeech Communication Association, Florence, Italy.Andreas Stolcke.
2002.
SRILM - an extensiblelanguage modeling toolkit.
In 7th InternationalConference on Spoken Language Processing, IC-SLP2002/INTERSPEECH 2002, Denver, Colorado,USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluation Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-driven Machine Translation and Be-yond (WPT-05), Ann Arbor, MI, USA.Wei Xu and Alex Rudnicky.
2000.
Can artificial neuralnetworks learn language models?
In Sixth Interna-tional Conference on Spoken Language Processing,ICSLP 2000 / INTERSPEECH 2000, Beijing, China,pages 202?205.
ISCA.39
