Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 449?453,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsUser Embedding for Scholarly Microblog RecommendationYang Yu, Xiaojun Wan and Xinjie ZhouInstitute of Computer Science and Technology, The MOE Key Laboratory of ComputationalLinguistics, Peking University, Beijing 100871, China{yu.yang, wanxiaojun, xinjiezhou}@pku.edu.cnAbstractNowadays, many scholarly messages areposted on Chinese microblogs and moreand more researchers tend to find schol-arly information on microblogs.
In orderto exploit microblogging to benefit scien-tific research, we propose a scholarly mi-croblog recommendation system in thisstudy.
It automatically collects and minesscholarly information from Chinese mi-croblogs, and makes personalized rec-ommendations to researchers.
We pro-pose two different neural network modelswhich learn the vector representations forboth users and microblog texts.
Then therecommendation is accomplished basedon the similarity between a user?s vectorand a microblog text?s vector.
We alsobuild a dataset for this task.
The two em-bedding models are evaluated on the da-taset and show good results compared toseveral baselines.1 IntroductionOnline social networks such as microblogs havedrawn growing attention in recent years, andmore and more researchers are involved in mi-croblogging websites.
Besides expressing theirown emotions and exchanging their life experi-ences just like other users, these researchers alsowrite from time to time about their latest findingsor recommend useful research resources on theirmicroblogs, which may be insightful to otherresearchers in the same field.
We call such mi-croblog texts scholarly microblog texts.
The vol-ume of scholarly microblog texts is huge, whichmakes it time-consuming for a researcher tobrowse and find the ones that he or she is inter-ested in.In this study, we aim to build a personalizedrecommendation system for recommendingscholarly microblogs.
With such a system a re-searcher can easily obtain the scholarly mi-croblogs he or she has interests in.
The systemfirst collects the latest scholarly microblogs bycrawling from manually selected microblog usersor by applying scholarly microblog classificationmethods, as introduced in (Yu and Wan, 2016).Second, the system models the relevance of eachscholarly microblog to a researcher and makepersonalized recommendation.
In this study, wefocus on the second step of the system and aim tomodel the interest and preference of a researcherby embedding the researcher into a dense vector.We also embed each scholarly microblog into adense vector, and thus the relevance of a scholar-ly microblog to a researcher can be estimatedbased on their vector representations.In this paper, we propose two neural embed-ding algorithms for learning the vector represen-tations for both users (researchers) and mi-croblog texts.
By extending the paragraph vectorrepresentation method proposed by (Le andMikolov, 2014), the vector representations arejointly learned in a single framework.
By model-ing the user preferences into the same vectorspace with the words and texts, we can obtain thesimilarity between them in a straightforward way,and use this relevance for microblog recommen-dation.
We build a real evaluation dataset fromSina Weibo.
Evaluation results on the datasetshow the efficacy of our proposed methods.2 Related WorkThere have been a few previous studies focusingon microblog recommendation.
Chen et al (2012)proposed a collaborative ranking model.
Theirapproach takes advantage of collaborative filter-ing based recommendation by collecting prefer-ence information from many users.
Their ap-proach takes into account the content of the tweet,user?s social relations and certain other explicitlydefined features.
Ma et al (2011) generated  rec-ommendations by adding additional social regu-larization terms in MF to constrain the user latentfeature vectors to be similar to his or her friends'average latent features.
Bhattacharya et al (2014)449proposed a method benefiting from knowing theuser?s topics of interest, inferring the topics ofinterest for an individual user.
Their idea is toinfer them from the topical expertise of the userswhom the user follows.
Khater and Elmongu(2015) proposed a dynamic personalized tweetrecommendation system capturing the user?s in-terests, which change over the time.
Their sys-tem shows the messages that correspond to suchdynamic interests.
Kuang et al (2016) consid-ered three major aspects in their proposed tweetrecommending model, including the popularityof a tweet itself, the intimacy between the userand the tweet publisher, and the interest fields ofthe user.
They also divided the users into threetypes by analyzing their behaviors, using differ-ent weights for the three aspects when recom-mending tweets for different types of users.Most of the above studies make use of the re-lationships between users, while in this study, wefocus on leveraging only the microblog texts foraddressing the task.3 Our Approach3.1 Task DefinitionWe denote a set of users by ?
?1 2 , , , mu u u u?
?
,and a set of microblog texts by?
?1 2 , , , nd d d d?
?.
We assume that a usertweeting, retweeting or commenting on a mi-croblog text reflects that the user is interested inthat microblog.
Giventu u?
, we denote the setof microblogs thattu  is interested in by ?
?td u .In our task, the entire sets of d  and u  are given,while given a usertu u?
, only a subset of?
?td u  is known.
This subset is used as the train-ing set, denoted as ?
?td u .
Our task aims to re-trieve a subset 'd  of d , that 'd  is as similar to?
?
?
?t td u d u?
as possible.In this section, we introduce one baselinemethod and then propose two different neuralnetwork methods for user and microblog embed-ding.
The baseline averages the vector represen-tation of microblog texts into a user vector repre-sentation.
Our proposed two methods learn uservector representations jointly with word and textvectors, either indirectly or directly from wordvectors.3.2 Paragraph VectorAs our methods are mainly based on the Para-graph Vector model proposed by (Le andMikolov, 2014), we start by introducing thisframework first.Paragraph Vector is an unsupervised frame-work that learns continuous distributed vectorrepresentations for pieces of texts.
In this ap-proach, every paragraph is mapped to a uniquevector, represented by a column in matrix D  andevery word is also mapped to a unique vector,represented by a column in matrix W .
This ap-proach is similar to the Word2Vec approach pro-posed in (Mikolov et al, 2013), except that aparagraph token is added to the paragraph and istreated as a special word.
The paragraph vector isasked to contribute to the prediction work in ad-dition to the word vectors in the context of theword to be predicted.
The   paragraph vector andword vectors are averaged to predict the nextword in a context.Formally speaking, given a paragraph?
?1 2,  , , ,i Td w w w?
with id  as the paragraphtoken, k  as the window size, the Paragraph Vec-tor model applies hierarchical softmax to maxim-ize the average log probability1 log ( | , ,..., )t i t k t ktp w d w wT ?
?
?3.3 Averaging Microblog Text Vectors asUser VectorAn intuitive baseline approach to map a mi-croblog user into a vector space is to build suchrepresentation from the vector representations ofthe microblogs he or she likes.We treat microblog texts as paragraphs, andthen apply the Paragraph Vector model intro-duced in Section 3.2 to learn vector representa-tions of the microblog texts.
After learning allvector representations of microblog texts, foreach user, we average all vectors of microblogFigure 1.
The proposed User2Vec#1 framework forlearning user vector representation.
In this frame-work, the word vectors do not directly contribute tothe user vectors.wtwt-3 wt-2 wt-1diui1 ui2 ui3Word Matrix WMicroblog textMatrix DUser Matrix UAverageAverage450texts he or she likes in the training set as the uservector.3.4 Learning User Vectors Indirectly FromWord VectorsBesides the above-mentioned baseline approachwe further consider to jointly learn the vectors ofusers and microblog texts.
In this framework,every user is mapped to a vector represented in acolumn in matrix U , in addition to the mi-croblog text matrix D  and the word matrixW .Given a microblog text?
?1 2,  , , ,i Td w w w?
, be-sides predicting words in the microblog textsusing the microblog tokenid  and words in thesliding window, we also try to predictid   usingthe users related to it.
Denoting the set of all us-ers related toid  in the training set as1 2( ) { , ..., }hi i i iu d u u u?, we maximize the aver-age log probability11 [log ( | , ,..., ) log ( | ,..., )]ht i t k t k i i it p w d w w p d u uT ?
?
?
?The structure of this framework is shown inFigure 1.
We name this framework User2Vec#1.3.5 Learning User Vectors Directly FromWord VectorsIn the above framework, the user vectors arelearned only from microblog text vectors, notdirectly from word vectors.
Another frameworkwe proposed for learning user vector representa-tion is to put user vectors and microblog vectorsin the same layer.
Unlike User2Vec#1, we do notuse user vectors to predict microblog text vector.Instead, we directly add user vectors into the in-put layer of word vector prediction task, alongwith the microblog text vector.In this framework, the average log probabilitywe want to maximize is11 ( log ( | , ,..., , ,..., )ht i t k t k i it p w d w w u uT ?
?
?In practical tasks, we modify the dataset bycopying each microblog once for each user in???(??
), and make each copied microblog text onlyrelate to one user.
All copies of the same mi-croblog text share a same vector representation.The structure of the framework is shown inFigure 2.
We name this framework User2Vec#2.3.6 Recommending MicroblogsWhen recommending microblogs, given a mi-croblogjdand a userku  , we compute the cosinedistance between their vector representations,and use the cosine distance to determine whetherjd  should be recommended to ku  or not.4 Evaluation4.1 Data PreparationTo evaluate our proposed user embedding meth-ods in a scholarly microblog recommending sys-tem, we built a dataset by crawling from thewebsite Machine Learning Daily1.The Machine Learning Daily is a Chinesewebsite which focuses on collecting and labelingscholarly microblogs related to machine learning,natural language processing, information retriev-al and data mining on Sina Weibo.
These mi-croblog texts were collected by a combination ofmanual and automatic methods, and each mi-croblog text is annotated with multiple tags byexperts, yielding an excellent dataset for our ex-periment.
The microblog texts in our dataset canbe written in a mixture of both Chinese and Eng-lish.
We removed stop words from   the raw texts,leaving 16,797 words in our corpus.
The textswere then segmented with the Jieba Chinese textsegmentation tool2.1 http://ml.memect.com/2 https://github.com/fxsjy/jiebaFigure 2.
The proposed User2Vec#2 framework forlearning user vector representation.
In this frame-work, the word vectors contribute directly to theuser vectors, along with the microblog text vectors.wtwt-3 wt-2 wt-1diuiWord Matrix WMicroblog textMatrix DUserMatrix UWord Matrix WAveragek4510.3400.3600.3800.4000.4200.44050 100 150 200 250 300Average Embedding User2Vec#1User2Vec#2After crawling the microblogs from the Ma-chine Learning Daily, we used Sina Weibo APIto retrieve the list of users who retweeted orcommented on those microblogs.
These retweet-ing and commenting actions indicated that thoseusers have interests in the microblogs they re-tweeted or commented, and such microblogswere considered the gold-standard (positive) mi-croblogs for the users in the recommendationsystem.
Then we filtered out the users who haveless than two hundred positive samples to avoidthe data sparseness problem.
This left us with711 users and 10,620 microblog texts in our cor-pus.
Each user was associated with 282.3 posi-tive microblogs on average.4.2 Evaluation SetupBecause there is no API that can directly grant usthe access to the follower and followee list foreach user without authorization on Sina Weibo,when evaluating the effectiveness of our methods,we randomly choose one hundred positive sam-ples and another four hundred negative samplesrandomly selected from the crawled microblogs,to simulate the timeline of a user, and use thissimulated timeline as the test dataset.
The re-maining positive samples are used for training.We adopt two additional baselines: Bag-of-Words and SVM on Bag-of-Words.
For the Bag-of-Words baseline, we use the Bag-of-Wordsvector of each microblog text as the microblogtext vector, and average them to obtain user vec-tors.
For the SVM on Bag-of-Words baseline, werandomly choose the same amount of negativesamples as that of positive samples for training.We use the Bag-of-Words vector of each mi-croblog text as the features, and run the SVMalgorithm implemented in LibSVM 3  once forevery user.
Note that the Average Embedding3 https://www.csie.ntu.edu.tw/~cjlin/libsvm/method introduced in Section 3.3 is considered astrong baseline for comparison.For each method and each user, we sort themicroblog texts according to their similarity withthe user and select the top k microblog texts asrecommendation results, where k varies from 10to 100.Besides precision and recall values, we alsocompute mean reciprocal rank (MRR) to meas-ure the recommendation results in our experi-ments, which is the average of the multiplicativeinverse of the rank of the positive samples in theoutput of the recommending system, and thenaveraged again across all users.
Note that when kis set to 100, the precision and recall value willbe equal to each other.4.3 Evaluation ResultsThe comparison results with respect to differ-ent k are shown in Table 1.
As we can see, thetwo proposed joint learning methods outperformthe simple average embedding method and thetwo other baselines, indicating the effectivenessof the proposed methods.
Moreover, User2Vec#2yields better results than User2Vec#1.We believethis is because in User2Vec#2, the word vectorshave a direct contribution  to the user vectors,which improves the learning effect of the userk=10 k=20 k=50 k=100Preci-sionRecall MRRPreci-sionRecall MRRPreci-sionRecall MRRPreci-sionRecall MRRBag-of-Words0.5036 0.0504 0.0153 0.4917 0.0983 0.0185 0.4461 0.2231 0.0223 0.3204 0.3204 0.0246SVM onBoW0.5774 0.0577 0.0172 0.5662 0.1132 0.0212 0.5122 0.2561 0.0256 0.3675 0.3675 0.0282AverageEmbedding0.5963 0.0596 0.0183 0.5824 0.1165 0.0219 0.5266 0.2633 0.0264 0.3793 0.3793 0.0291User2Vec#1 0.6246 0.0625 0.0189 0.6055 0.1211 0.0228 0.5511 0.2756 0.0275 0.3953 0.3953 0.0304User2Vec#2 0.6652 0.0665 0.0201 0.6498 0.1300 0.0244 0.5883 0.2942 0.0295 0.4231 0.4231 0.0325Figure 3.
Precision/Recall@k=100 w.r.t.
vector di-mension.Table 1.
Overview of results.452vectors learnt in the framework.
Furthermore, theprecision/recall scores of the embedding methods(k=100) with respect to different vector dimen-sions are shown in Figure 3.
We can see that thedimension size has little impact on the recom-mendation performance, and our proposed twomethods always outperform the strong baseline.5   ConclusionIn this paper, we proposed two neural embeddingmethods for learning the vector representationsfor both the users and the microblog texts.
Wetested their performance by applying them torecommending scholarly microblogs.
In futurework, we will investigate leveraging user rela-tionships and temporal information to furtherimprove the recommendation performance.AcknowledgmentsThe work was supported by National NaturalScience Foundation of China (61331011), Na-tional Hi-Tech Research and Development Pro-gram (863 Program) of China (2015AA015403)and IBM Global Faculty Award Program.
Wethank the anonymous reviewers and mentor fortheir helpful comments.
Xiaojun Wan is the cor-responding author.ReferencesMichal Barla.
2011.
Towards social-based user mod-eling and personalization.
Information Sciencesand Technologies Bulletin of the ACM Slovakia,3(1).Parantapa Bhattacharya, Muhammad Bilal Zafar, Ni-loy Ganguly, Saptarshi Ghosh, and Krishna P.Gummadi.
2014.
Inferring user interests in the twit-ter social network.
In Proceedings of the 8th ACMConference on Recommender systems.
ACM.Kailong Chen, Tianqi Chen, Guoqing Zheng, Ou Jin,Enpeng Yao, and Yong Yu.
2012.
Collaborativepersonalized tweet recommendation.
In Proceed-ings of the 35th international ACM SIGIR confer-ence on Research and development in informationretrieval.
ACM.Shaymaa Khater and Hicham G. Elmongui.
2015.Tweets You Like: Personalized Tweets Recom-mendation based on Dynamic Users Interests.
In2014 ASE Conference.Li Kuang, Xiang Tang, Meiqi Yu, Yujian Huang andKehua Guo.
2016.
A comprehensive ranking modelfor tweets big data in online social network.
EUR-ASIP Journal on Wireless Communications andNetworking, 2016(1).Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyuand Irwin King.
2011.
Recommender systems withsocial regularization.
In Proceedings of the fourthACM international conference on Web search anddata mining.
ACM.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S.Corrado and Jeff Dean.
2013.
Distributed represen-tations of words and phrases and their composi-tionality.
In Advances in neural information pro-cessing systems.Hongzhi Yin, Bin Cui, Ling Chen, Zhiting Hu andXiaofang Zhou.
2015.
Dynamic user modeling insocial media systems.
ACM Transactions on In-formation Systems (TOIS), 33(3).Jianjun Yu, Yi Shen and Zhenglu Yang.
2014.
Topic-STG: Extending the session-based temporal graphapproach for personalized tweet recommendation.In Proceedings of the companion publication of the23rd international conference on World Wide Webcompanion.
International World Wide Web Con-ferences Steering Committee.Yang Yu and Xiaojun Wan.
2016.
MicroScholar:Mining Scholarly Information from Chinese Mi-croblogs.
In Thirtieth AAAI Conference on Artifi-cial Intelligence.453
