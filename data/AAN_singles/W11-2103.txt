Proceedings of the 6th Workshop on Statistical Machine Translation, pages 22?64,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsFindings of the 2011 Workshop on Statistical Machine TranslationChris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins UniversityPhilipp KoehnSchool of InformaticsUniversity of EdinburghChristof MonzInformatics InstituteUniversity of AmsterdamOmar F. ZaidanCenter for Language and Speech ProcessingJohns Hopkins UniversityAbstractThis paper presents the results of the WMT11shared tasks, which included a translationtask, a system combination task, and a task formachine translation evaluation metrics.
Weconducted a large-scale manual evaluation of148 machine translation systems and 41 sys-tem combination entries.
We used the rank-ing of these systems to measure how stronglyautomatic metrics correlate with human judg-ments of translation quality for 21 evaluationmetrics.
This year featured a Haitian Creoleto English task translating SMS messages sentto an emergency response service in the af-termath of the Haitian earthquake.
We alsoconducted a pilot ?tunable metrics?
task to testwhether optimizing a fixed system to differ-ent metrics would result in perceptibly differ-ent translation quality.1 IntroductionThis paper presents the results of the shared tasksof the Workshop on statistical Machine Translation(WMT), which was held at EMNLP 2011.
Thisworkshop builds on five previous WMT workshops(Koehn and Monz, 2006; Callison-Burch et al,2007; Callison-Burch et al, 2008; Callison-Burchet al, 2009; Callison-Burch et al, 2010).
The work-shops feature three shared tasks: a translation taskbetween English and other languages, a task to com-bine the output of multiple machine translation sys-tems, and a task to predict human judgments oftranslation quality using automatic evaluation met-rics.
The performance for each of these shared tasksis determined through a comprehensive human eval-uation.
There were a two additions to this year?sworkshop that were not part of previous workshops:?
Haitian Creole featured task ?
In addition totranslation between European language pairs,we featured a new translation task: translatingHaitian Creole SMS messages that were sentto an emergency response hotline in the im-mediate aftermath of the 2010 Haitian earth-quake.
The goal of this task is to encourage re-searchers to focus on challenges that may arisein future humanitarian crises.
We invited WillLewis, Rob Munro and Stephan Vogel to pub-lish a paper about their experience developingtranslation technology in response to the crisis(Lewis et al, 2011).
They provided the dataused in the Haitian Creole featured translationtask.
We hope that the introduction of this newdataset will provide a testbed for dealing withlow resource languages and the informal lan-guage usage found in SMS messages.?
Tunable metric shared task ?
We conducteda pilot of a new shared task to use evaluationmetrics to tune the parameters of a machinetranslation system.
Although previous work-shops have shown evaluation metrics other thanBLEU are more strongly correlated with humanjudgments when ranking outputs from multiplesystems, BLEU remains widely used by systemdevelopers to optimize their system parameters.We challenged metric developers to tune theparameters of a fixed system, to see if their met-rics would lead to perceptibly better translationquality for the system?s resulting output.22The primary objectives of WMT are to evaluatethe state of the art in machine translation, to dissem-inate common test sets and public training data withpublished performance numbers, and to refine eval-uation methodologies for machine translation.
Aswith previous workshops, all of the data, transla-tions, and collected human judgments are publiclyavailable.1 We hope these datasets form a valuableresource for research into statistical machine transla-tion, system combination, and automatic evaluationof translation quality.2 Overview of the Shared Translation andSystem Combination TasksThe recurring task of the workshop examines trans-lation between English and four other languages:German, Spanish, French, and Czech.
We created atest set for each language pair by translating newspa-per articles.
We additionally provided training dataand two baseline systems.2.1 Test dataThe test data for this year?s task was created byhiring people to translate news articles that weredrawn from a variety of sources from early Decem-ber 2010.
A total of 110 articles were selected, inroughly equal amounts from a variety of Czech, En-glish, French, German, and Spanish news sites:2Czech: aktualne.cz (4), Novinky.cz (7), iH-Ned.cz (4), iDNES.cz (4)French: Canoe (5), Le Devoir (5), Le Monde (5),Les Echos (5), Liberation (5)Spanish: ABC.es (6), Cinco Dias (6), El Period-ico (6), Milenio (6), Noroeste (7)English: Economist (4), Los Angeles Times (6),New York Times (4), Washington Post (4)German: FAZ (3), Frankfurter Rundschau (2), Fi-nancial Times Deutschland (3), Der Spie-gel (5), Su?ddeutsche Zeitung (3)The translations were created by the professionaltranslation agency CEET.3 All of the translations1http://statmt.org/wmt11/results.html2For more details see the XML test files.
The docid taggives the source and the date for each document in the test set,and the origlang tag indicates the original source language.3http://www.ceet.eu/were done directly, and not via an intermediate lan-guage.Although the translations were done profession-ally, in some cases errors still cropped up.
For in-stance, in parts of the English-French translations,some of the English source remains in the Frenchreference as if the translator forgot to delete it.2.2 Training dataAs in past years we provided parallel corpora to traintranslation models, monolingual corpora to train lan-guage models, and development sets to tune systemparameters.
Some statistics about the training mate-rials are given in Figure 1.2.3 Baseline systemsTo lower the barrier of entry for newcomers tothe field, we provided two open source toolkits forphrase-based and parsing-based statistical machinetranslation (Koehn et al, 2007; Li et al, 2010).2.4 Submitted systemsWe received submissions from 56 groups across 37institutions, as listed in Tables 1, 2 and 3.
We alsoincluded two commercial off-the-shelf MT systems,two online statistical MT systems, and five onlinerule-based MT systems.
(Not all systems supportedall language pairs.)
We note that these nine compa-nies did not submit entries themselves, and are there-fore anonymized in this paper.
Rather, their entrieswere created by translating the test data via their webinterfaces.4 The data used to construct these systemsis not subject to the same constraints as the sharedtask participants.
It is possible that part of the refer-ence translations that were taken from online newssites could have been included in the online systems?models, for instance.
We therefore categorize allcommercial systems as unconstrained when evalu-ating the results.2.5 System combinationIn total, we had 148 primary system entries (includ-ing the 46 entries crawled from online sources), and60 contrastive entries.
These were made available to4We would like to thank Ondr?ej Bojar for harvesting thecommercial entries (2), Christian Federmann for the statisticalMT entries (14), and Herve?
Saint-Amand for the rule-based MTentries (30)!23Europarl Training CorpusSpanish?
English French?
English German?
English Czech?
EnglishSentences 1,786,594 1,825,077 1,739,154 462,351Words 51,551,370 49,411,045 54,568,499 50,551,047 45,607,269 47,978,832 10,573,983 12,296,772Distinct words 171,174 113,655 137,034 114,487 362,563 111,934 152,788 56,095News Commentary Training CorpusSpanish?
English French?
English German?
English Czech?
EnglishSentences 132,571 115,562 136,227 122,754Words 3,739,293 3,285,305 3,290,280 2,866,929 3,401,766 3,309,619 2,658,688 2,951,357Distinct words 73,906 53,699 59,911 50,323 120,397 53,921 130,685 50,457United Nations Training CorpusSpanish?
English French?
EnglishSentences 10,662,993 12,317,600Words 348,587,865 304,724,768 393,499,429 344,026,111Distinct words 578,599 564,489 621,721 729,233109 Word Parallel CorpusFrench?
EnglishSentences 22,520,400Words 811,203,407 668,412,817Distinct words 2,738,882 2,861,836CzEng Training CorpusCzech?
EnglishSentences 7,227,409Words 72,993,427 84,856,749Distinct words 1,088,642 522,770Europarl Language Model DataEnglish Spanish French German CzechSentence 2,032,006 1,942,761 2,002,266 1,985,560 479,636Words 54,720,731 55,105,358 57,860,307 48,648,697 10,770,230Distinct words 119,315 176,896 141,742 376,128 154,129News Language Model DataEnglish Spanish French German CzechSentence 30,888,595 3,416,184 11,767,048 17,474,133 12,333,268Words 777,425,517 107,088,554 302,161,808 289,171,939 216,692,489Distinct words 2,020,549 595,681 1,250,259 3,091,700 2,068,056News Test SetEnglish Spanish French German CzechSentences 3003Words 75,762 79,710 85,999 73,729 65,427Distinct words 10,088 11,989 11,584 14,345 16,922Figure 1: Statistics for the training and test sets used in the translation task.
The number of words and the number ofdistinct words (case-insensitive) is based on the provided tokenizer.24ID ParticipantALACANT University of Alicante (Sa?nchez-Cartagena et al, 2011)CEU-UPV CEU University Cardenal Herrera& Polytechnic University of Valencia (Zamora-Martinez and Castro-Bleda, 2011)CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)CMU-DYER Carnegie Mellon University - Dyer (Dyer et al, 2011)CMU-HANNEMAN Carnegie Mellon University - Hanneman (Hanneman and Lavie, 2011)COPENHAGEN Copenhagen Business SchoolCST Centre for Language Technology @ Copenhagen University (Rish?j and S?gaard, 2011)CU-BOJAR Charles University - Bojar (Marec?ek et al, 2011)CU-MARECEK Charles University - Marec?ek (Marec?ek et al, 2011)CU-POPEL Charles University - Popel (Popel et al, 2011)CU-TAMCHYNA Charles University - Tamchyna (Bojar and Tamchyna, 2011)CU-ZEMAN Charles University - Zeman (Zeman, 2011)DFKI-FEDERMANN Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Federmann(Federmann and Hunsicker, 2011)DFKI-XU Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Xu (Xu et al, 2011b)HYDERABAD IIIT-HyderabadILLC-UVA Institute for Logic, Language and Computation @ University of Amsterdam(Khalilov and Sima?an, 2011)JHU Johns Hopkins University (Weese et al, 2011)KIT Karlsruhe Institute of Technology (Herrmann et al, 2011)KOC Koc University (Bicici and Yuret, 2011)LATL-GENEVA Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)LIA-LIG Laboratoire Informatique d?Avignon @ The University of Avignon& Laboratoire d?Informatique de Grenoble @ University of Grenoble (Potet et al, 2011)LIMSI LIMSI (Allauzen et al, 2011)LINGUATEC Linguatec Language Technologies (Aleksic and Thurmair, 2011)LIU Linko?ping University (Holmqvist et al, 2011)LIUM University of Le Mans (Schwenk et al, 2011)PROMT ProMTRWTH-FREITAG RWTH Aachen - Freitag (Huck et al, 2011)RWTH-HUCK RWTH Aachen - Huck (Huck et al, 2011)RWTH-WUEBKER RWTH Aachen - Wu?bker (Huck et al, 2011)SYSTRAN SYSTRANUEDIN University of Edinburgh (Koehn et al, 2007)UFAL-UM Charles University and University of Malta (Corb?
?-Bellot et al, 2005)UOW University of Wolverhampton (Aziz et al, 2011)UPM Technical University of Madrid (Lo?pez-Luden?a and San-Segundo, 2011)UPPSALA Uppsala University (Koehn et al, 2007)UPPSALA-FBK Uppsala University& Fondazione Bruno Kessler (Hardmeier et al, 2011)ONLINE-[A,B] two online statistical machine translation systemsRBMT-[1?5] five online rule-based machine translation systemsCOMMERCIAL-[1,2] two commercial machine translation systemsTable 1: Participants in the shared translation task (European language pairs; individual system track).
Not all teamsparticipated in all language pairs.
The translations from commercial and online systems were crawled by us, notsubmitted by the respective companies, and are therefore anonymized.25ID ParticipantBBN-COMBO Raytheon BBN Technologies (Rosti et al, 2011)CMU-HEAFIELD-COMBO Carnegie Mellon University (Heafield and Lavie, 2011)JHU-COMBO Johns Hopkins University (Xu et al, 2011a)KOC-COMBO Koc University (Bicici and Yuret, 2011)LIUM-COMBO University of Le Mans (Barrault, 2011)QUAERO-COMBO Quaero Project?
(Freitag et al, 2011)RWTH-LEUSCH-COMBO RWTH Aachen (Leusch et al, 2011)UOW-COMBO University of Wolverhampton (Specia et al, 2010)UPV-PRHLT-COMBO Polytechnic University of Valencia (Gonza?lez-Rubio and Casacuberta, 2011)UZH-COMBO University of Zurich (Sennrich, 2011)Table 2: Participants in the shared system combination task.
Not all teams participated in all language pairs.?
The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.participants in the system combination shared task.Continuing our practice from last year?s workshop,we separated the test set into a tuning set and a finalheld-out test set for system combinations.
The tun-ing portion was distributed to system combinationparticipants along with reference translations, to aidthem set any system parameters.In the European language pairs, the tuning setconsisted of 1,003 segments taken from 37 docu-ments, whereas the test set consisted of 2,000 seg-ments taken from 73 documents.
In the Haitian Cre-ole task, the split was 674 segments for tuning and600 for testing.Table 2 lists the 10 participants in the system com-bination task.3 Featured Translation TaskThe featured translation task of WMT11 was totranslate Haitian Creole SMS messages into En-glish.
These text messages were sent by people inHaiti in the aftermath of the January 2010 earth-quake.
In the wake of the earthquake, much of thecountry?s conventional emergency response servicesfailed.
Since cell phone towers remained stand-ing after the earthquake, text messages were a vi-able mode of communication.
Munro (2010) de-scribes how a text-message-based emergency report-ing system was set up by a consortium of volunteerorganizations named ?Mission 4636?
after a freeSMS short code telephone number that they estab-lished.
The SMS messages were routed to a systemfor reporting trapped people and other emergencies.Search and rescue teams within Haiti, including theUS Military, recognized the quantity and reliabil-ity of actionable information in these messages andused them to provide aid.The majority of the SMS messages were writ-ten in Haitian Creole, which was not spoken bymost of first responders deployed from overseas.A distributed, online translation effort was estab-lished, drawing volunteers from Haitian Creole- andFrench-speaking communities around the world.The volunteers not only translated messages, butalso categorized them and pinpointed them on amap.5 Collaborating online, they employed their lo-cal knowledge of locations, regional slang, abbre-viations and spelling variants to process more than40,000 messages in the first six weeks alone.
Firstresponders indicated that this volunteer effort helpedto save hundreds of lives and helped direct the firstfood and aid to tens of thousands.
Secretary of StateClinton described one success of the Mission 4636program:?The technology community has set up in-teractive maps to help us identify needs and targetresources.
And on Monday, a seven-year-old girland two women were pulled from the rubble of acollapsed supermarket by an American search-and-rescue team after they sent a text message callingfor help.?
Ushahidi@Tufts described another:?TheWorld Food Program delivered food to an informalcamp of 2500 people, having yet to receive food orwater, in Diquini to a location that 4636 had identi-5A detailed map of Haiti was created by a crowdsourcingeffort in the aftermath of the earthquake (Lacey-Hall, 2011).26ID ParticipantBM-I2R Barcelona Media& Institute for Infocomm Research (Costa-jussa` and Banchs, 2011)CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)CMU-HEWAVITHARANA Carnegie Mellon University - Hewavitharana (Hewavitharana et al, 2011)HYDERABAD IIIT-HyderabadJHU Johns Hopkins University (Weese et al, 2011)KOC Koc University (Bicici and Yuret, 2011)LIU Linko?ping University (Stymne, 2011)UMD-EIDELMAN University of Maryland - Eidelman (Eidelman et al, 2011)UMD-HU University of Maryland - Hu (Hu et al, 2011)UPPSALA Uppsala University (Hardmeier et al, 2011)Table 3: Participants in the featured translation task (Haitian Creole SMS into English; individual system track).
Notall teams participated in both the ?Clean?
and ?Raw?
tracks.fied for them.
?In parallel with Rob Munro?s crowdsourcingtranslation efforts, the Microsoft Translator team de-veloped a Haitian Creole statistical machine transla-tion engine from scratch in a compressed timeframe(Lewis, 2010).
Despite the impressive numberof translations completed by volunteers, machinetranslation was viewed as a potentially useful toolfor higher volume applications or to provide trans-lations of English medical documents into HaitianCreole.
The Microsoft Translator team quickly as-sembled parallel data from a number of sources,including Mission 4636 and from the archives ofCarnegie Mellon?s DIPLOMAT project (Frederkinget al, 1997).
Through a series of rapid prototyp-ing efforts, the team improved their system to dealwith non-standard orthography, reduced pronouns,and SMS shorthand.
They deployed a functionaltranslation system to relief workers in the field inless than 5 days ?
impressive even when measuredagainst previous rapid MT development efforts likeDARPA?s surprise language exercise (Oard, 2003;Oard and Och, 2003).We were inspired by the efforts of Rob Munro andWill Lewis on translating Haitian Creole in the af-termath of the disaster, so we worked with them tocreate a featured task at WMT11.
We thank them forgenerously sharing the data they assembled in theirown efforts.
We invited Rob Munro, Will Lewis,and Stephan Vogel to speak at the workshop on thetopic of developing translation technology for futurecrises, and they recorded their thoughts in an invitedpublication (Lewis et al, 2011).3.1 Haitian Creole DataFor the WMT11 featured translation task, weanonymized the SMS Haitian Creole messagesalong with the translations that the Mission 4636volunteers created.
Examples of these messages aregiven in Table 4.
The goal of anonymizing the SMSdata was so that it may be shared with researcherswho are developing translation and mapping tech-nologies to support future emergency relief effortsand social development.
We ask that any researcherworking with these messages to be aware that theyare actual communications sent by people in need ina time of crisis.
Researchers who use this data areasked to be cognizant of the following:?
Some messages may be distressing in content.?
The people who sent the messages (and whoare discussed in them) were victims of a naturaldisaster and a humanitarian crisis.
Please treatthe messages with the appropriate respect forthese individuals.?
The primary motivation for using this datashould be to understand how we can better re-spond to future crises.Participants who received the Haitian Creole datafor WMT11 were given anonymization guidelines27mwen se [FIRSTNAME] mwen gen twaset ki mouri mwenmande nou ed pou nou edem map tan reponsI am [FIRSTNAME], I have three sisters who have died.
Iask help for us, I await your response.Ki kote yap bay manje Where are they giving out food?Eske lekol kolej marie anne kraze?mesi Was the College Marie Anne school destroyed?
Thank you.Nou pa ka anpeche moustik yo mo`de nou paske yo anpil.
We can?t prevent the mosquitoes from biting because thereare so many.tanpri ke`m ap kase mwen pa ka pran nouvel manmanm.
Please heart is breaking because I have no news of mymother.4636:Opital Medesen san Fwontie` delmas 19 la fe`men.Opital sen lwi gonzag nan delma 33 pran an chaj gratwit-man tout moun ki malad ou blese4636: The Doctors without Borders Hospital in Delmas 19is closed.
The Saint Louis Gonzaga hospital in Delmas 33is taking in sick and wounded people for freeMwen re?se?voua mesaj nou yo 5 sou 5 men mwen ta vle diyon bagay kile` e koman nap kapab fe`m jwin e`d sa yo poumoune b la kay mwen ki sinistwe?
adre`s la se?I received your message 5/5 but I would like to ask onething when and how will you be able to get the aid to me forthe people around my house who are victims of the earth-quake?
The address isSil vous plait map chehe [LASTNAME][FIRSTNAME].diyo relem nan [PHONENUMBER].mwen se [LAST-NAME] [FIRSTNAME]I?m looking for [LASTNAME][FIRSTNAME].
Tell himto call me at [PHONENUMBER] I am [LASTNAME][FIRSTNAME]Bonswa mwen rele [FIRSTNAME] [LASTNAME] kaymwen krase mwen pagin anyin poum mange ak fanmi-mtampri di yon mo pou mwen fem jwen yon tante tou akmange.
.mrete nHello my name is [FIRSTNAME] [LASTNAME]my housefell down, I?ve had nothing to eat and I?m hungry.
Pleasehelp me find food.
I liveMwen viktim kay mwen kraze e`skem ka ale sendomengmwen gen paspo`I?m a victim.
My home has been destroyed.
Am I allowedto go to the Dominican Republic?
I have a Passport.KISAM DWE FE LEGEN REPLIK,ESKE MOUN SAINTMARC AP JWENN REPLIK.What should I do when there is an aftershock?
Will thepeople of Saint Marc have aftershocks?MWEN SE YON JEN ETIDYAN AN ASYANS ENFO-MATIK KI PASE ANPIL MIZE NAN TRANBLEMANDE TE 12 JANVYE A TOUT FANMIM FIN MOURIMWEN SANTIM SEL MWEN TE VLE ALE VIVI?m a young student in computer science, who has suffereda lot during and after the earthquake of January 12th.
Allmy family has died and I feel alone.
I wanted to go live.Mw rele [FIRSTNAME], mw fe` mason epi mw abitelaple`n.
Yo dim minustah ap bay djob mason ki kote poumw ta pase si mw ta vle jwenn nan djob sa yo.My name is [FIRSTNAME], I?m a construction worker andI live in La Plaine.
I heard that the MINUSTAH was givingjobs to construction workers.
What do I have to go to findone of these jobs?Souple mande lapolis pou fe on ti pase nan magloire am-broise prolonge zone muler ak cadet jeremie ginyin jen ga-son ki ap pase nan zone sa yo e ki agresiplease ask the police to go to magloire ambroise going to-wards the ?muler?
area and cadet jeremie because there arevery aggressive young men in these areasKIBO MOUN KA JWENN MANJE POU YO MANJEANDEYO KAPITAL PASKE DEPI 12 JANVYE YOVOYE MANJE POU PEP LA MEN NOU PA JANMJWENN ANYEN.
NAP MOURI AK GRANGOUWhere can people get food to eat outside of the capital be-cause since January 12th, they?ve sent food for the peoplebut we never received anything.
We are dying of hungerMwen se [FIRSTNAME][LASTNAME] mwen nan akenmwen se yon je`n ki ansent mwen te genyen yon paran ki tapede li mouri po`toprens, mwen pral akouye nan ko`mansmanfeviyeI am [FIRSTNAME][LASTNAME] I am in Aquin I am apregnant young person I had a parent who was helping me,she died in Port-au-Prince, I?m going to give birth at thestart of FebruaryTable 4: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along withtheir translations into English.
Translations were done by volunteers who wanted to help with the relief effort.
Priorto being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc.
Theanonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.28Training set Parallel Wordssentences per langIn-domain SMS data 17,192 35kMedical domain 1,619 10kNewswire domain 13,517 30kGlossary 35,728 85kWikipedia parallel sentence 8,476 90kWikipedia named entities 10,499 25kThe bible 30,715 850kHaitisurf dictionary 3,763 4kKrengle dictionary 1,687 3kKrengle sentences 658 3kTable 5: Training data for the Haitian Creole-English fea-tured translation task.
The in-domain SMS data consistsprimarily of raw (noisy) SMS data.
The in-domain datawas provided by Mission 4636.
The other data is out-of-domain.
It comes courtesy of Carnegie Mellon Univer-sity, Microsoft Research, Haitisurf.com, and Krengle.net.alongside the SMS data.
The WMT organizers re-quested that if they discovered messages with incor-rect or incomplete anonymization, that they notifyus and correct the anonymization using the versioncontrol repository.To define the shared translation task, we dividedthe SMS messages into an in-domain training set,along with designated dev, devtest, and test sets.
Wecoordinated with Microsoft and CMU to make avail-able additional out-of-domain parallel corpora.
De-tails of the data are given in Table 5.
In additionto this data, participants in the featured task wereallowed to use any of the data provided in the stan-dard translation task, as well as linguistic tools suchas taggers, parsers, or morphological analyzers.3.2 Clean and Raw Test DataWe provided two sets of testing and developmentdata.
Participants used their systems to translate twotest sets consisting of 1,274 unseen Haitian CreoleSMS messages.
One of the test sets contains the?raw?
SMS messages as they were sent, and theother contains messages that were cleaned up by hu-man post-editors.
The English side is the same inboth cases, and the only difference is the HaitianCreole input sentences.The post-editors were Haitian Creole languageinformants hired by Microsoft Research.
They pro-vided a number of corrections to the SMS messages,including expanding SMS shorthands, correctingspelling/grammar/capitalization, restoring diacriticsthat were left out of the original message, andcleaning up accented characters that were lost whenthe message was transmitted in the wrong encoding.Original Haitian Creole messages:Sil vou ple?
e?de mwen avek moun ki vik-tim yo nan tranbleman de te?
a,ki kite?
poto-prins ki vini nan provins- mwen ede ak tikob mwen te ginyin kounie?
a4636: Manje vin pi che nan PaP apre tran-bleman te-a.
mamit diri ap van?n 250gdkounye, sete 200gd avan.
Mayi-a 125gd,avan sete 100gdEdited Haitian Creole messages:Silvouple ede mwen ave`k moun ki viktimyo nan tranblemannte` a, ki kite Po`toprenski vini nan pwovens, mwen ede ak ti ko`bmwen te genyen kounye a4636: Manje vin pi che` nan PaP apre tran-blemannte` a. Mamit diri ap vann 250gdkounye a, sete 200gd avan.
Mayi-a 125gd,avan sete 100gd.For the test and development sets the informantsalso edited the English translations.
For instance,there were cases where the original crowdsourcedtranslation summarized the content of the messageinstead of translating it, instances where parts ofthe source were omitted, and where explanatorynotes were added.
The editors improved the trans-lations so that they were more suitable for machinetranslation, making them more literal, correctingdisfluencies on the English side, and retranslatingthem when they were summaries.Crowdsourced English translation:We are in the area of Petit Goave, wewould like .... we need tents and medi-cation for flu/colds...Post-edited translation:We are in the area of Petit Goave, wewould like to receive assistance, however,29it should not be the way I see the Minus-tah guys are handling the people.
We needlots of tents and medication for flu/colds,and feverThe edited English is provided as the reference forboth the ?clean?
and the ?raw?
sets, since we intendthat distinction to refer to the form that the sourcelanguage comes in, rather than the target language.Tables 47 and 48 in the Appendix show a signifi-cant difference in the translation quality between theclean and the raw test sets.
In most cases, systems?output for the raw condition was 4 BLEU pointslower than for the clean condition.
We believe thatthe difference in performance on the raw vs. cleanedtest sets highlight the importance of handling noisyinput data.All of the in-domain training data is in the raw for-mat.
The original SMS messages are unaltered, andthe translations are just as the volunteered providedthem.
In some cases, the original SMS messages arewritten in French or English instead of Haitian Cre-ole, or contain a mixture of languages.
It may bepossible to further improve the quality of machinetranslation systems trained from this data by improv-ing the quality of the data itself.3.3 Goals and ChallengesThe goals of the Haitian Creole to English transla-tion task were:?
To focus researchers on the problems presentedby low resource languages?
To provide a real-world data set consisting ofSMS messages, which contain abbreviations,non-standard spelling, omitted diacritics, andother noisy character encodings?
To develop techniques for building translationsystems that will be useful in future crisesThere are many challenges in translating noisydata in a low resource language, and there are a vari-ety of strategies that might be considered to attemptto tackle them.
For instance:?
Automated cleaning of the raw (noisy) SMSdata in the training set.?
Leveraging a larger French-English model totranslate out of vocabulary Haitian words, bycreating a mapping from Haitian words ontoFrench.?
Incorporation of morphological and/or syntac-tic models to better cope with the low resourcelanguage pair.It is our hope that by introducing this data as ashared challenge at WMT11 that we will establish auseful community resource so that researchers mayexplore these challenges and publish about them inthe future.4 Human EvaluationAs with past workshops, we placed greater empha-sis on the human evaluation than on the automaticevaluation metric scores.
It is our contention thatautomatic measures are an imperfect substitute forhuman assessment of translation quality.
Therefore,we define the manual evaluation to be primary, anduse the human judgments to validate automatic met-rics.Manual evaluation is time consuming, and it re-quires a large effort to conduct on the scale ofour workshop.
We distributed the workload acrossa number of people, including shared-task partici-pants, interested volunteers, and a small number ofpaid annotators (recruited by the participating sites).More than 130 people participated in the manualevaluation, with 91 people putting in more than anhour?s worth of effort, and 29 putting in more thanfour hours.
There was a collective total of 361 hoursof labor.We asked annotators to evaluate system outputsby ranking translated sentences relative to eachother.
This was our official determinant of trans-lation quality.
The total number of judgments col-lected for the different ranking tasks is given in Ta-ble 6.We performed the manual evaluation of the indi-vidual systems separately from the manual evalua-tion of the system combination entries, rather thancomparing them directly against each other.
Lastyear?s results made it clear that there is a large (ex-pected) gap in performance between the two groups.This year, we opted to reduce the number of pairwise30comparisons with the hope that we would be morelikely to find statistically significant differences be-tween the systems in the same groups.
To that sameend, we also eliminated the editing/acceptabilitytask that was featured in last year?s evaluation, in-stead we had annotators focus solely on the systemranking task.4.1 Ranking translations of sentencesRanking translations relative to each other is a rea-sonably intuitive task.
We therefore kept the instruc-tions simple:You are shown a source sentence followedby several candidate translations.Your task is to rank the translations frombest to worst (ties are allowed).Each screen for this task involved judging trans-lations of three consecutive source segments.
Foreach source segment, the annotator was shown theoutputs of five submissions, and asked to rank them.With the exception of a few tasks in the systemcombination track, there were many more than 5systems participating in any given task?up to 23for the English-German individual systems track.Rather than attempting to get a complete orderingover the systems, we instead relied on random se-lection and a reasonably large sample size to makethe comparisons fair.We use the collected rank labels to assign eachsystem a score that reflects how highly that systemwas usually ranked by the annotators.
The score forsome systemA reflects how frequently it was judgedto be better than or equal to other systems.
Specif-ically, each block in which A appears includes fourimplicit pairwise comparisons (against the other pre-sented systems).
A is rewarded once for each of thefour comparisons in which A wins or ties.
A?s scoreis the number of such winning (or tying) pairwisecomparisons, divided by the total number of pair-wise comparisons involving A.The system scores are reported in Section 5.
Ap-pendix A provides detailed tables that contain pair-wise head-to-head comparisons between pairs ofsystems.4.2 Inter- and Intra-annotator agreement inthe ranking taskWe were interested in determining the inter- andintra-annotator agreement for the ranking task, sincea reasonable degree of agreement must exist to sup-port our process as a valid evaluation setup.
To en-sure we had enough data to measure agreement, wepurposely designed the sampling of source segmentsand translations shown to annotators in a way thatensured some items would be repeated, both withinthe screens completed by an individual annotator,and across screens completed by different annota-tors.We did so by ensuring that 10% of the generatedscreens are exact repetitions of previously gener-ated screen within the same batch of screens.
Fur-thermore, even within the other 90%, we ensuredthat a source segment appearing in one screen ap-pears again in two more screens (though with differ-ent system outputs).
Those two details, intentionalrepetition of source sentences and intentional repeti-tion of system outputs, ensured we had enough datato compute meaningful inter- and intra-annotatoragreement rates.We measured pairwise agreement among anno-tators using Cohen?s kappa coefficient (?)
(Cohen,1960), which is defined as?
=P (A)?
P (E)1?
P (E)where P (A) is the proportion of times that the anno-tators agree, and P (E) is the proportion of time thatthey would agree by chance.
Note that ?
is basicallya normalized version of P (A), one which takes intoaccount how meaningful it is for annotators to agreewith each other, by incorporating P (E).
Note alsothat ?
has a value of at most 1 (and could possiblybe negative), with higher rates of agreement result-ing in higher ?.The above definition of ?
is actually used by sev-eral definitions of agreement measures, which differin how P (A) and P (E) are computed.We calculate P (A) by examining all pairs ofsystems which had been judged by two or morejudges, and calculating the proportion of time thatthey agreed that A > B, A = B, or A < B. Inother words, P (A) is the empirical, observed rate at31Inividual System Track System Combination TrackLanguage Pair # Systems Label Labels # Systems Label LabelsCount per System Count per SystemCzech-English 8 2,490 276.7 4 1,305 261.0English-Czech 10 8,985 816.8 2 2,700 900.0German-English 20 4,620 220.0 8 1,950 216.7English-German 22 6,540 284.4 4 2,205 441.0Spanish-English 15 2,850 178.1 6 2,115 302.1English-Spanish 15 5,595 349.7 4 3,000 600.0French-English 18 3,540 186.3 6 1,500 214.3English-French 17 4,590 255.0 2 900 300.0Haitian (Clean)-English 9 3,360 336.0 3 1,200 300.0Haitian (Raw)-English 6 1,875 267.9 2 900 300.0Urdu-English 8 3,165 351.7 N/A N/A N/A(tunable metrics task)Overall 148 47,610 299.4 41 17,775 348.5Table 6: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected ineach of the individual and system combination tracks.
The system count does not include the reference translation,which was included in the evaluation, and so a value under ?Labels per System?
can be obtained only after adding 1to the system count, before dividing the label count (e.g.
in German-English, 4, 620/21 = 220.0).which annotators agree, in the context of pairwisecomparisons.
P (A) is computed similarly for intra-annotator agreement (i.e.
self-consistency), but overpairwise comparisons that were annotated more thanonce by a single annotator.As for P (E), it should capture the probability thattwo annotators would agree randomly.
Therefore:P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2Note that each of the three probabilities in P (E)?sdefinition are squared to reflect the fact that we areconsidering the chance that two annotators wouldagree by chance.
Each of these probabilities is com-puted empirically, by observing how often annota-tors actually rank two systems as being tied.
Wenote here that this empirical computation is a depar-ture from previous years?
analyses, where we hadassumed that the three categories are equally likely(yielding P (E) = 19 +19 +19 =13 ).
We believe thatthis is a more principled approach, which faithfullyreflects the motivation of accounting for P (E) in thefirst place.66Even if we wanted to assume a ?random clicker?
model,setting P (E) = 13 is still not entirely correct.
Given thatTable 7 gives ?
values for inter-annotator andintra-annotator agreement across the various evalu-ation tasks.
These give an indication of how oftendifferent judges agree, and how often single judgesare consistent for repeated judgments, respectively.There are some general and expected trends thatcan be seen in this table.
First of all, intra-annotatoragreement is higher than inter-annotator agreement.Second, reference translations are noticeably betterthan other system outputs, which means that anno-tators have an artificially high level of agreement onpairwise comparisons that include a reference trans-lation.
For this reason, we also report the agreementlevels when such comparisons are excluded.The exact interpretation of the kappa coefficient isdifficult, but according to Landis and Koch (1977),0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moder-ate, 0.6?
0.8 is substantial, and 0.8?
1.0 is almostperfect.
Based on these interpretations, the agree-ment for sentence-level ranking is moderate to sub-stantial for most tasks.annotators rank five outputs at once, P (A = B) = 15 , not13 , since there are only five (out of 25) label pairs that satisfyA = B.
Working this back into P (E)?s definition, we haveP (A > B) = P (A < B) = 25 , and therefore P (E) = 0.36rather than 0.333.32INTER-ANNOTATOR AGREEMENT (I.E.
ACROSS ANNOTATORS)ALL COMPARISONS NO REF COMPARISONSP (A) P (E) ?
P (A) P (E) ?European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409INTRA-ANNOTATOR AGREEMENT (I.E.
SELF-CONSISTENCY)ALL COMPARISONS NO REF COMPARISONSP (A) P (E) ?
P (A) P (E) ?European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580Table 7: Inter- and intra-annotator agreement rates, for the various manual evaluation tracks of WMT11.
See Tables 49and 50 below for a detailed breakdown by language pair.However, one result that is of concern is thatagreement rates are noticeably lower for Europeanlanguage pairs, in particular for the individual sys-tems track.
When excluding reference comparisons,the inter- and intra-annotator agreement levels are0.320 and 0.512, respectively.
Not only are thosenumbers lower than for the other tasks, but theyare also lower than last year?s numbers, which were0.409 and 0.580.We investigated this result a bit deeper.
Tables 49and 50 in the Appendix break down the results fur-ther, by reporting agreement levels for each lan-guage pair.
One observation is that the agreementlevel for some language pairs deviates in a non-trivial amount from the overall agreement rate.Let us focus on inter-annotator agreement ratesin the individual track (excluding reference compar-isons), in the top right portion of Table 49.
The over-all ?
is 0.320, but it ranges from 0.264 for German-English, to 0.477 for Spanish-English.What distinguishes those two language pairs fromeach other?
If we examine the results in Table 8,we see that Spanish-English had two very weak sys-tems, which were likely easy for annotators to agreeon comparisons involving them.
(This is the con-verse of annotators agreeing more often on com-parisons involving the reference.)
English-French issimilar in that regard, and it too has a relatively highagreement rate.On the other hand, the participants in German-English formed a large pool of more closely-matched systems, where the gap separating the bot-tom system is not as pronounced.
So it seems thatthe low agreement rates are indicative of a morecompetitive evaluation and more closely-matchedsystems.5 Results of the Translation TasksWe used the results of the manual evaluation to an-alyze the translation quality of the different systemsthat were submitted to the workshop.
In our analy-sis, we aimed to address the following questions:?
Which systems produced the best translationquality for each language pair??
Which of the systems that used only the pro-vided training materials produced the besttranslation quality?33Czech-English1023?1166 comparisons/systemSystem C?
?othersUEDIN ??
Y 0.69ONLINE-B ?
N 0.68CU-BOJAR N 0.60JHU N 0.57UPPSALA Y 0.57SYSTRAN N 0.51CST Y 0.47CU-ZEMAN Y 0.44Spanish-English583?833 comparisons/systemSystem C?
?othersONLINE-B ?
N 0.72ONLINE-A ?
N 0.72KOC ?
Y 0.67SYSTRAN ?
N 0.66ALACANT ?
N 0.66RBMT-1 N 0.63RBMT-3 N 0.61RBMT-2 N 0.60RBMT-4 N 0.60RBMT-5 N 0.51UEDIN Y 0.51UPM Y 0.50UFAL-UM Y 0.47HYDERABAD Y 0.17CU-ZEMAN Y 0.16French-English608?883 comparisons/systemSystem C?
?othersONLINE-A ?
N 0.66LIMSI ??
Y+G 0.66ONLINE-B ?
N 0.66LIA-LIG Y 0.64KIT ??
Y+G 0.64LIUM Y+G 0.63CMU-DENKOWSKI ?
Y 0.62JHU Y+G 0.61RWTH-HUCK Y+G 0.58RBMT-1 ?
N 0.58CMU-HANNEMAN Y+G 0.58RBMT-3 N 0.55SYSTRAN N 0.54RBMT-4 N 0.53RBMT-2 N 0.52UEDIN Y 0.50RBMT-5 N 0.45CU-ZEMAN Y 0.37English-Czech3126?3397 comparisons/systemSystem C?
?othersONLINE-B ?
N 0.65CU-BOJAR N 0.64CU-MARECEK ?
N 0.63CU-TAMCHYNA N 0.62UEDIN ?
Y 0.59CU-POPEL ?
Y 0.58COMMERCIAL2 N 0.51COMMERCIAL1 N 0.51JHU N 0.49CU-ZEMAN Y 0.43English-Spanish1300?1480 comparisons/systemSystem C?
?othersONLINE-B ?
N 0.74ONLINE-A ?
N 0.72RBMT-3 ?
N 0.71PROMT ?
N 0.70CEU-UPV ?
Y 0.65UEDIN ?
Y 0.64UPPSALA ?
Y 0.61RBMT-4 N 0.61RBMT-1 N 0.60UOW Y 0.59RBMT-2 N 0.57KOC Y 0.56RBMT-5 N 0.54CU-ZEMAN Y 0.49UPM Y 0.34English-French868?1121 comparisons/systemSystem C?
?othersLIMSI ??
Y+G 0.73ONLINE-B ?
N 0.70KIT ??
Y+G 0.69RWTH-HUCK Y+G 0.65LIUM Y+G 0.64RBMT-1 N 0.61ONLINE-A N 0.60UEDIN Y 0.58RBMT-3 N 0.58RBMT-5 N 0.55UPPSALA Y 0.55JHU Y 0.55UPPSALA-FBK Y 0.54RBMT-4 N 0.49RBMT-2 N 0.46LATL-GENEVA N 0.39CU-ZEMAN Y 0.20German-English741?998 comparisons/systemSystem C?
?othersONLINE-B ?
N 0.72CMU-DYER ??
Y+G 0.66ONLINE-A ?
N 0.66RBMT-3 N 0.64LINGUATEC N 0.63RBMT-4 N 0.61RBMT-1 N 0.60DFKI-XU N 0.60RWTH-WUEBKER ?
Y+G 0.59KIT Y+G 0.57LIU Y 0.57LIMSI Y+G 0.56RBMT-5 N 0.56UEDIN Y 0.55RBMT-2 N 0.54CU-ZEMAN Y 0.47UPPSALA Y 0.47KOC Y 0.45JHU Y+G 0.43CST Y 0.37English-German1051?1230 comparisons/systemSystem C?
?othersRBMT-3 ?
N 0.73ONLINE-B ?
N 0.73RBMT-1 ?
N 0.70DFKI-FEDERMANN ?
N 0.68DFKI-XU N 0.67RBMT-4 ?
N 0.66RBMT-2 ?
N 0.66ONLINE-A ?
N 0.65LIMSI ?
Y+G 0.65KIT ?
Y 0.64UEDIN Y 0.60LIU Y 0.59RBMT-5 N 0.58RWTH-FREITAG Y 0.56COPENHAGEN ?
Y 0.56JHU Y 0.54KOC Y 0.53UOW Y 0.53CU-TAMCHYNA Y 0.50UPPSALA Y 0.49ILLC-UVA Y 0.48CU-ZEMAN Y 0.38C?
indicates whether system is constrained: trained only using supplied training data, standard monolingual linguis-tic tools, and, optionally, LDC?s English Gigaword.
Eentries that used the Gigaword are marked with +G.?
indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.?
indicates a constrained win: no other constrained system is statistically better.Table 8: Official results for the WMT11 translation task.
Systems are ordered by their ?others score, reflecting howoften their translations won or tied pairwise comparisons.
For detailed head-to-head comparisons, see Appendix A.34Czech-English1036?1042 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.64BBN-COMBO ?
0.62JHU-COMBO 0.58UPV-PRHLT-COMBO 0.47English-Czech1788?1792 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.48UPV-PRHLT-COMBO 0.41German-English811?927 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.70RWTH-LEUSCH-COMBO 0.65BBN-COMBO 0.61UZH-COMBO ?
0.60JHU-COMBO 0.56UPV-PRHLT-COMBO 0.52QUAERO-COMBO 0.46KOC-COMBO 0.45English-German1746?1752 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.61UZH-COMBO ?
0.58UPV-PRHLT-COMBO 0.56KOC-COMBO 0.46Spanish-English1132?1249 comparisons/comboSystem ?othersRWTH-LEUSCH-COMBO ?
0.71CMU-HEAFIELD-COMBO ?
0.67BBN-COMBO ?
0.64UPV-PRHLT-COMBO 0.64JHU-COMBO 0.62KOC-COMBO 0.56English-Spanish2360?2378 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.69UOW-COMBO 0.63UPV-PRHLT-COMBO 0.59KOC-COMBO 0.58French-English820?916 comparisons/comboSystem ?othersBBN-COMBO ?
0.67RWTH-LEUSCH-COMBO ?
0.63CMU-HEAFIELD-COMBO 0.62JHU-COMBO ?
0.59LIUM-COMBO 0.53UPV-PRHLT-COMBO 0.53English-French586?587 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.51UPV-PRHLT-COMBO 0.43?
indicates a win: no other system combination is statistically significantly better at p-level?0.10 in pairwisecomparison.Table 9: Official results for the WMT11 system combination task.
Systems are ordered by their ?others score,reflecting how often their translations won or tied pairwise comparisons.
For detailed head-to-head comparisons, seeAppendix A.35Haitian Creole (Clean)-English(individual systems)1256?1435 comparisons/systemSystem ?othersBM-I2R ?
0.71CMU-DENKOWSKI 0.66CMU-HEWAVITHARANA 0.64UMD-EIDELMAN 0.63UPPSALA 0.57LIU 0.55UMD-HU 0.52HYDERABAD 0.43KOC 0.31Haitian Creole (Raw)-English(individual systems)1065?1136 comparisons/systemSystem ?othersBM-I2R ?
0.65CMU-HEWAVITHARANA 0.60CMU-DENKOWSKI 0.59LIU 0.55UMD-EIDELMAN 0.52JHU 0.41Haitian Creole (Clean)-English(system combinations)896?898 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO ?
0.52UPV-PRHLT-COMBO 0.48KOC-COMBO 0.38Haitian Creole (Raw)-English(system combinations)600?600 comparisons/comboSystem ?othersCMU-HEAFIELD-COMBO 0.47UPV-PRHLT-COMBO 0.43?
indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.Table 10: Official results for the WMT11 featured translation task (Haitian Creole SMS into English).
Systems areordered by their ?others score, reflecting how often their translations won or tied pairwise comparisons.
For detailedhead-to-head comparisons, see Appendix A.36Tables 8?10 show the system ranking for eachof the translation tasks.
For each language pair,we define a system as ?winning?
if no other systemwas found statistically significantly better (using theSign Test, at p ?
0.10).
In some cases, multiple sys-tems are listed as winners, either due to a large num-ber of participants or a low number of judgments persystem pair, both of which are factors that make itdifficult to achieve statistical significance.We start by examining the results for the individ-ual system track for the European languages (Ta-ble 8).
In Spanish?English and German?English,unconstrained systems are observed to perform bet-ter than constrained systems.
In other languagepairs, particularly French?English, constrainedsystems are found to be able to be on the same levelor outperform unconstrained systems.
It also seemsthat making use of the Gigaword corpora is likelyto yield better systems, even when translating out ofEnglish, as in English-French and English-German.For English-German the rule-based MT systems per-formed well.Of the participating teams, there is no individ-ual system clearly outperforming all other systemsacross the different language pairs.
However, oneof the crawled systems, ONLINE-B, performs con-sistently well, being one of the winners in all eightlanguage pairs.As for the system combination track (Table 9),the CMU-HEAFIELD-COMBO entry performed quitewell, being a winner in seven out of eight languagepairs.
This performance is carried over to the HaitianCreole task, where it again comes out on top (Ta-ble 10).
In the individual track of the Haitian Creoletask, BM-I2R is the sole winner in both the ?clean?and ?raw?
tracks.6 Evaluation TaskIn addition to allowing us to analyze the translationquality of different systems, the data gathered duringthe manual evaluation is useful for validating auto-matic evaluation metrics.
Our evaluation shared taskis similar to the MetricsMATR workshop (Metricsfor MAchine TRanslation) that NIST runs (Przy-bocki et al, 2008; Callison-Burch et al, 2010).
Ta-ble 11 lists the participants in this task, along withtheir metrics.A total of 21 metrics and their variants were sub-mitted to the evaluation task by 9 research groups.We asked metrics developers to score the outputs ofthe machine translation systems and system com-binations at the system-level and at the segment-level.
The system-level metrics scores are given inthe Appendix in Tables 39?48.
The main goal of theevaluation shared task is not to score the systems,but instead to validate the use of automatic metricsby measuring how strongly they correlate with hu-man judgments.
We used the human judgments col-lected during the manual evaluation for the transla-tion task and the system combination task to calcu-late how well metrics correlate at system-level andat the segment-level.This year the strongest metric was a new metricdeveloped by Columbia and ETS called MTeRater-Plus.
MTeRater-Plus is a machine-learning-basedmetric that use features from ETS?s e-rater, an auto-mated essay scoring engine designed to assess writ-ing proficiency (Attali and Burstein, 2006).
The fea-tures include sentence-level and document-level in-formation.
Some examples of the e-rater featuresinclude:?
Preposition features that calculate the proba-bility of prepositions appearing in the givencontext of a sentence (Tetreault and Chodorow,2008)?
Collocation features that indicate whether thecollocations in the document are typical of na-tive use (Futagi et al, 2008).?
A sentence fragment feature that counts thenumber of ill-formed sentences in a document.?
A feature that counts the number of words withinflection errors?
A feature that counts the the number of articleerrors in the sentence citeHan2006.MTeRater uses only the e-rater features, and mea-sures fluency without any need for reference transla-tions.
MTeRater-Plus is a meta-metric that incorpo-rates adequacy by combining MTeRater with otherMT evaluation metrics and heuristics that take thereference translations into account.Please refer to the proceedings for papers provid-ing detailed descriptions of all of the metrics.37Metric IDs ParticipantAMBER, AMBER-NL, AMBER-IT National Research Council Canada (Chen and Kuhn, 2011)F15, F15G3 Koc?
University (Bicici and Yuret, 2011)METEOR-1.3-ADQ, METEOR-1.3-RANK Carnegie Mellon University (Denkowski and Lavie, 2011a)MTERATER, MTERATER-PLUS Columbia / ETS (Parton et al, 2011)MP4IBM1, MPF, WMPF DFKI (Popovic?, 2011; Popovic?
et al, 2011)PARSECONF DFKI (Avramidis et al, 2011)ROSE, ROSE-POS The University of Sheffield (Song and Cohn, 2011)TESLA-B, TESLA-F, TESLA-M National University of Singapore (Dahlmeier et al, 2011)TINE University of Wolverhampton (Rios et al, 2011)BLEU provided baseline (Papineni et al, 2002)TER provided baseline (Snover et al, 2006)Table 11: Participants in the evaluation shared task.
For comparison purposes, we include the BLEU and TER metricsas baselines.EN-CZ-10SYSTEMSEN-DE-22SYSTEMSEN-ES-15SYSTEMSEN-FR-17SYSTEMSAVERAGEAVERAGEW/OCZSystem-level correlation for translation out of EnglishTESLA-M .90 .95 .96 .94TESLA-B .81 .90 .91 .87MPF .72 .63 .87 .89 .78 .80WMPF .72 .61 .87 .89 .77 .79MP4IBM1 -.76 -.91 -.71 -.61 .75 .74ROSE .65 .41 .90 .86 .71 .73BLEU .65 .44 .87 .86 .70 .72AMBER-TI .56 .54 .88 .84 .70 .75AMBER .56 .53 .87 .84 .70 .74AMBER-NL .56 .45 .88 .83 .68 .72F15G3 .50 .30 .89 .84 .63 .68METEORrank .65 .30 .74 .85 .63 .63F15 .52 .19 .86 .85 .60 .63TER -.50 -.12 -.81 -.84 .57 .59TESLA-F .86 .80 -.83 .28Table 12: System-level Spearman?s rho correlation of theautomatic evaluation metrics with the human judgmentsfor translation out of English, ordered by average abso-lute value.
We did not calculate correlations with the hu-man judgments for the system combinations for the out ofEnglish direction, because none of them had more than 4items.6.1 System-Level Metric AnalysisWe measured the correlation of the automatic met-rics with the human judgments of translation qualityat the system-level using Spearman?s rank correla-tion coefficient ?.
We converted the raw scores as-signed to each system into ranks.
We assigned a hu-man ranking to the systems based on the percent oftime that their translations were judged to be betterthan or equal to the translations of any other systemin the manual evaluation.
The reference was not in-cluded as an extra translation.When there are no ties, ?
can be calculated usingthe simplified equation:?
= 1?6?d2in(n2 ?
1)where di is the difference between the rank forsystemi and n is the number of systems.
The pos-sible values of ?
range between 1 (where all systemsare ranked in the same order) and?1 (where the sys-tems are ranked in the reverse order).
Thus an auto-matic evaluation metric with a higher absolute valuefor ?
is making predictions that are more similar tothe human judgments than an automatic evaluationmetric with a lower absolute ?.The system-level correlations are shown in Ta-ble 13 for translations into English, and Table 12out of English, sorted by average correlation acrossthe language pairs.
The highest correlation foreach language pair and the highest overall averageare bolded.
This year, nearly all of the metrics38CZ-EN-8SYSTEMSDE-EN-20SYSTEMSDE-EN-8COMBOSES-EN-15SYSTEMSES-EN-6COMBOSFR-EN-18SYSTEMSFR-EN-6COMBOSAVERAGE(EUROPEANLANGS)HT-EN(CLEAN)-9SYSTEMSHT-EN(RAW)-6SYSTEMSAVERAGE(ALLLANGS)System-level correlation for metrics scoring translations into EnglishMTERATER-PLUS -.95 -.90 -.93 -.91 -.94 -.93 -.77 .90 -.82 -.54 .85TINE-SRL-MATCH .95 .69 .95 .95 1.00 .87 .66 .87TESLA-F .95 .70 .98 .96 .94 .90 .60 .86 .93 .83 .87TESLA-B .98 .88 .98 .91 .94 .91 .31 .84 .93 .83 .85MTERATER -.91 -.88 -.91 -.88 -.89 -.79 -.60 .83 .13 .77 .55METEOR-1.3-ADQ .93 .68 .91 .91 .83 .93 .66 .83 .95 .77 .84TESLA-M .95 .94 .95 .82 .94 .87 .31 .83 .95 .83 .84METEOR-1.3-RANK .91 .71 .91 .88 .77 .93 .66 .82 .95 .83 .84AMBER-NL .88 .58 .91 .88 .94 .94 .60 .82AMBER-TI .88 .63 .93 .85 .83 .94 .60 .81AMBER .88 .59 .91 .86 .83 .95 .60 .80MPF .95 .69 .91 .83 .60 .87 .54 .77 .95 .77 .79WMPF .95 .66 .86 .83 .60 .87 .54 .76 .93 .77 .78F15 .93 .45 .88 .96 .49 .87 .60 .74F15G3 .93 .48 .83 .94 .49 .88 .60 .74ROSE .88 .59 .83 .92 .60 .86 .26 .70 .93 .77 .74BLEU .88 .48 .83 .90 .49 .85 .43 .69 .90 .83 .73TER -.83 -.33 -.64 -.89 -.37 -.77 -.89 .67 -.93 -.83 .72MP4IBM1 -.91 -.56 -.50 -.12 -.43 -.08 .14 .35DFKI-PARSECONF .31 .52Table 13: System-level Spearman?s rho correlation of the automatic evaluation metrics with the human judgmentsfor translation into English, ordered by average absolute value for the European languages.
We did not calculatecorrelations with the human judgments for the system combinations for Czech to English and for Haitian Creole toEnglish, because they had too few items (?
4) for reliable statistics.39FR-EN(6337PAIRS)DE-EN(8950PAIRS)ES-EN(5974PAIRS)CZ-EN(3695PAIRS)AVERAGESegment-level correlation for translations into EnglishMTERATER-PLUS .30 .36 .45 .36 .37TESLA-F .28 .24 .39 .32 .31TESLA-B .28 .26 .36 .29 .30METEOR-1.3-RANK .23 .25 .38 .28 .29METEOR-1.3-ADQ .24 .25 .37 .27 .28MPF .25 .23 .34 .28 .28AMBER-TI .24 .26 .33 .27 .28AMBER .24 .25 .33 .27 .27WMPF .24 .23 .34 .26 .27AMBER-NL .24 .24 .30 .27 .26MTERATER .19 .26 .33 .24 .26TESLA-M .21 .23 .29 .23 .24TINE-SRL-MATCH .20 .19 .30 .24 .23F15G3 .17 .15 .29 .21 .21F15 .16 .14 .27 .22 .20MP4IBM1 .15 .16 .18 .12 .15DFKI-PARSECONF n/a .24 n/a n/aTable 14: Segment-level Kendall?s tau correlation of theautomatic evaluation metrics with the human judgmentsfor translation into English, ordered by average correla-tion.had stronger correlation with human judgments thanBLEU.
The metrics that had the strongest correlationthis year included two metrics, MTeRater and TINE,as well as metrics that have demonstrated strong cor-relation in previous years like TESLA and Meteor.6.2 Segment-Level Metric AnalysisWe measured the metrics?
segment-level scores withthe human rankings using Kendall?s tau rank corre-lation coefficient.
The reference was not included asan extra translation.We calculated Kendall?s tau as:?
=num concordant pairs - num discordant pairstotal pairswhere a concordant pair is a pair of two translationsof the same segment in which the ranks calculatedfrom the same human ranking task and from the cor-responding metric scores agree; in a discordant pair,they disagree.
In order to account for accuracy- vs.EN-FR(6934PAIRS)EN-DE(10732PAIRS)EN-ES(8837PAIRS)EN-CZ(11651PAIRS)AVERAGESegment-level correlation for translations out of EnglishAMBER-TI .32 .22 .31 .21 .27AMBER .31 .21 .31 .22 .26MPF .31 .22 .30 .20 .26WMPF .31 .22 .29 .19 .25AMBER-NL .30 .19 .29 .20 .25METEOR-1.3-RANK .31 .14 .26 .19 .23F15G3 .26 .08 .22 .13 .17F15 .26 .07 .22 .12 .17MP4IBM1 .21 .13 .13 .06 .13TESLA-B .29 .20 .28 n/aTESLA-M .25 .18 .27 n/aTESLA-F .30 .19 .26 n/aTable 15: Segment-level Kendall?s tau correlation of theautomatic evaluation metrics with the human judgmentsfor translation out of English, ordered by average corre-lation.error-based metrics correctly, counts of concordantvs.
discordant pairs were calculated specific to thesetwo metric types.
The possible values of ?
rangebetween 1 (where all pairs are concordant) and ?1(where all pairs are discordant).
Thus an automaticevaluation metric with a higher value for ?
is mak-ing predictions that are more similar to the humanjudgments than an automatic evaluation metric witha lower ?
.We did not include cases where the human rank-ing was tied for two systems.
As the metrics produceabsolute scores, compared to five relative ranks inthe human assessment, it would be potentially un-fair to the metric to count a slightly different met-ric score as discordant with a tie in the relative hu-man rankings.
A tie in automatic metric rank fortwo translations was counted as discordant with twocorresponding non-tied human judgments.The correlations are shown in Table 14 for trans-lations into English, and Table 15 out of English,sorted by average correlation across the four lan-guage pairs.
The highest correlation for each lan-guage pair and the highest overall average are40ID Participant Metric NameCMU-METEOR Carnegie Mellon University METEOR (Denkowski and Lavie, 2011a)CU-SEMPOS-BLEU Charles University SemPOS/BLEU (Macha?c?ek and Bojar, 2011)NUS-TESLA-F National University of Singapore TESLA-F (Dahlmeier et al, 2011)RWTH-CDER RWTH Aachen CDER (Leusch and Ney, 2009)SHEFFIELD-ROSE The University of Sheffield ROSE (single reference) (Song and Cohn, 2011)STANFORD-DCP Stanford DCP (based on Liu and Gildea (2005))BLEU provided baseline BLEUBLEU-SINGLE provided baseline BLEU (single reference)Table 16: Participants in the tunable-metric shared task.
For comparison purposes, we included two BLEU-optimizedsystems in the evaluation as baselines.bolded.
There is a clear winner for the metrics thatscore translations into English: the MTeRater-Plusmetric (Parton et al, 2011) has the highest segmentlevel correlation across the board.
For metrics thatscore translation into other languages, there is notsuch a clear-cut winner.
The AMBER metric variantsdo well, as do MPF and WMPF.7 Tunable Metrics TaskThis year we introduced a new shared task that fo-cuses on using evaluation metrics to tune the param-eters of a statistical machine translation system.
Theintent of this task was to get researchers who de-velop automatic evaluation metrics for MT to workon the problem of using their metric to optimizethe parameters of MT systems.
Previous workshopshave demonstrated that a number of metrics performbetter than BLEU in terms of having stronger cor-relation with human judgments about the rankingsof multiple machine translation systems.
However,most MT system developers still optimize the pa-rameters of their systems to BLEU.
Here we aimto investigate the question of whether better metricswill result in better quality output when a system isoptimized to them.Because this was the first year that we ran thetunable metrics task, participation was limited to afew groups on an invitation-only basis.
Table 16lists the participants in this task.
Metrics developerswere invited to integrate their evaluation metric intoa MERT optimization routine, which was then usedto tune the parameters of a fixed statistical machinetranslation system.
We evaluated whether the sys-tem tuned on their metrics produced higher-qualityoutput than the baseline system that was tuned toBLEU, as is typically done.
In order to evaluatewhether the quality was better, we conducted a man-ual evaluation, in the same fashion that we evalu-ate the different MT systems submitted to the sharedtranslation task.We provide the participants with a fixed MT sys-tem for Urdu-English, along with a small parallelset to be used for tuning.
Specifically, we providedevelopers with the following components:?
Decoder - the Joshua decoder was used in thispilot.?
Decoder configuration file - a Joshua configu-ration file that ensures all systems use the samesearch parameters.?
Translation model - an Urdu-to-English trans-lation model, with syntax-based SCFG rules(Baker et al, 2010).?
Language model - a large 5-gram languagemodel trained on the English Gigaword corpus?
Development set - a development set, with 4English reference sets, to be used to optimizethe system parameters.?
Test set - a test set consisting of 883 Urdu sen-tences, to be translated by the tuned system (noreferences provided).?
Optimization routine - we provide an imple-mentation of minimum error rate training thatallows new metrics to be easily integrated asthe objective function.41Tunable Metrics Task1324?1484 comparisons/systemSystem ?others >othersBLEU ?
0.79 0.28BLEU-SINGLE ?
0.77 0.27CMU-METEOR ?
0.76 0.27RWTH-CDER 0.76 0.26CU-SEMPOS-BLEU ?
0.74 0.29STANFORD-DCP ?
0.73 0.27NUS-TESLA-F 0.68 0.28SHEFFIELD-ROSE 0.05 0.00?
indicates a win: no other system combination is sta-tistically significantly better at p-level?0.10 in pair-wise comparison.Table 17: Official results for the WMT11 tunable-metrictask.
Systems are ordered by their ?others score, re-flecting how often their translations won or tied pairwisecomparisons.
The > column reflects how often a systemstrictly won a pairwise comparison.We provided the metrics developers with OmarZaidan?s Z-MERT software (Zaidan, 2009), whichimplements Och (2003)?s minimum error rate train-ing procedure.
Z-MERT is designed to be modularwith respect to the objective function, and allowsBLEU to be easily replaced with other automaticevaluation metrics.
Metric developers incorporatedtheir metrics into Z-MERT by subclassing the Eval-uationMetric.java abstract class.
They ran Z-MERTon the dev set with the provided decoder/models,and created a weight vector for the system param-eters.Each team produced a distinct final weight vec-tor, which was used to produce English translationsof sentences in the test set.
The different transla-tions produced by tuning the system to different met-rics were then evaluated using the manual evaluationpipeline.77.1 Results of the Tunable Metrics TaskThe results of the evaluation are in Table 18.
Thescores show that the entries were quite close to eachother, with the notable exception of the SHEFFIELD-ROSE-tuned system, which produced overly-long7We also recased and detokenized each system?s output, toensure the outputs are more readable and easier to evaluate.REFBLEUBLEU-SINGLECMU-METEORCU-SEMPOS-BLEUNUS-TESLA-FRWTH-CDERSHEFFIELD-ROSESTANFORD-DCPREF ?
.15?
.11?
.13?
.09?
.09?
.10?
.00?
.11?BLEU .78?
?
.15 .11 .20 .19?
.13?
.01?
.14BLEU-SINGLE .82?
.20 ?
.11 .16 .21 .11 .00?
.20CMU-METEOR .84?
.09 .15 ?
.21 .20 .19 .00?
.19CU-SEMPOS-BLEU .82?
.23 .21 .21 ?
.12?
.18 .00?
.21NUS-TESLA-F .80?
.32?
.31 .28 .28?
?
.31 .00?
.28RWTH-CDER .79?
.22?
.16 .16 .22 .23 ?
.00?
.15SHEFFIELD-ROSE .98?
.93?
.93?
.96?
.95?
.95?
.93?
?
.94?STANFORD-DCP .82?
.17 .18 .26 .27 .28 .15 .00?
?> others .83 .28 .27 .27 .29 .28 .26 .00 .27>= others .90 .79 .77 .76 .74 .68 .76 .05 .73Table 18: Head to head comparisons for the tunable met-rics task.
The numbers indicate how often the system inthe column was judged to be better than the system inthe row.
The difference between 100 and the sum of thecorresponding cells is the percent of time that the twosystems were judged to be equal.and erroneous output (possibly due to an implemen-tation issue).
This is also evident from the fact that38% of pairwise comparisons indicated a tie be-tween the two systems, with the tie rate increasingto a full 47% when excluding comparisons involvingthe reference.
This is a very high tie rate ?
the cor-responding figure in, say, European language pairs(individual systems) is only 21%.What makes the different entries appear evenmore closely-matched is that the ranking changessignificantly when ordering systems by their>others score rather than the ?others score (i.e.when rewarding only wins, and not rewarding ties).NUS-TESLA-F goes from being a bottom entry to be-ing a top entry, with CU-SEMPOS-BLEU also bene-fiting, changing from the middle to the top rank.Either way, we see that a BLEU -tuned systemis performing just as well as systems tuned to theother metrics.
This might be an indication that somework remains to be done before a move away fromBLEU-tuning is fully justified.
On the other hand,the close results might be an artifact of the languagepair choice.
Urdu-English translation is still a rel-atively difficult problem, and MT outputs are stillof a relatively low quality.
It might be the case thathuman annotators are simply not very good at distin-42guishing one bad translation from another bad trans-lation, especially at such a fine-grained level.It is worth noting that the designers of the TESLAfamily replicated the setup of this tunable metric taskfor three European language pairs, and found thathuman judges did perceive a difference in qualitybetween a TESLA-tuned system and a BLEU -tunedsystem (Liu et al, 2011).7.2 Anticipated Changes Next YearThis year?s effort was a pilot of the task, so we in-tentionally limited the task to some degree, to makeit easier to iron out the details.
Possible changes fornext year include:?
More language pairs / translations into lan-guages other than English.
This year we fo-cus on Urdu-English because the language pairrequires a lot of reordering, and our syntacticmodel has more parameters to optimize thanthe standard Hiero and phrase-based models.?
Provide some human judgments about themodel?s output, so that people can experimentwith regression models.?
Include a single reference track along with themultiple reference track.
Some metrics may bebetter at dealing with the (more common) caseof there being only a single reference transla-tion available for every source sentence.?
Allow for experimentation with the MIRA op-timization routine instead of MERT.
MIRA canscale to a greater number of features, but re-quires that metrics be decomposable.8 SummaryAs in previous editions of this workshop we car-ried out an extensive manual and automatic evalua-tion of machine translation performance for translat-ing from European languages into English, and viceversa.The number of participants grew slightly com-pared to previous editions of the WMT workshop,with 36 groups from 27 institutions participating inthe translation task of WMT11, 10 groups from 10institutions participating in the system combinationtask, and 10 groups from 8 institutions participatingin the featured translation task (Haitian Creole SMSinto English).This year was also the first time that we included alanguage pair (Haitian-English) with non-Europeansource language and with very limited resources forthe source language side.
Also the genre of theHaitian-English task differed from previous WMTtasks as the Haitian-English translations are SMSmessages.WMT11 also introduced a new shared task focus-ing on evaluation metrics to tune the parameters ofa statistical machine translation system in which 6groups have participated.As in previous years, all data sets generated bythis workshop, including the human judgments, sys-tem translations and automatic scores, are publiclyavailable for other researchers to analyze.8AcknowledgmentsThis work was supported in parts by the Euro-MatrixPlus project funded by the European Com-mission (7th Framework Programme), the GALEprogram of the US Defense Advanced ResearchProjects Agency, Contract No.
HR0011-06-C-0022,the US National Science Foundation under grantIIS-0713448, and the CoSyne project FP7-ICT-4-248531 funded by the European Commission.
Theviews and findings are the authors?
alone.
A bigthank you to Ondr?ej Bojar, Simon Carter, Chris-tian Federmann, Will Lewis, Rob Munro and Herve?Saint-Amand, and to the shared task participants.ReferencesVera Aleksic and Gregor Thurmair.
2011.
PersonalTranslator at WMT2011.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-SonLe, Aure?lien Max, Guillaume Wisniewski, Franc?oisYvon, Gilles Adda, Josep Maria Crego, AdrienLardilleux, Thomas Lavergne, and Artem Sokolov.2011.
LIMSI @ WMT11.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Yigal Attali and Jill Burstein.
2006.
Automated es-say scoring with e-rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3):159?174.Eleftherios Avramidis, Maja Popovic?, David Vilar, andAljoscha Burchardt.
2011.
Evaluate with confidence8http://statmt.org/wmt11/results.html43estimation: Machine ranking of translation outputs us-ing grammatical features.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Wilker Aziz, Miguel Rios, and Lucia Specia.
2011.
Shal-low semantic trees for SMT.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Kathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie Dorr, Scott Miller, Christine Pi-atko, Nathaniel W. Filardo, and Lori Levin.
2010.Semantically-informed syntactic machine translation:A tree-grafting approach.
In Proceedings of AMTA.Lo?
?c Barrault.
2011.
MANY improvements forWMT?11.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Ergun Bicici and Deniz Yuret.
2011.
RegMT system formachine translation, system combination, and evalua-tion.
In Proceedings of the Sixth Workshop on Statisti-cal Machine Translation.Ondr?ej Bojar and Ales?
Tamchyna.
2011.
Improvingtranslation model by monolingual data.
In Proceed-ings of the Sixth Workshop on Statistical MachineTranslation.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion (WMT07), Prague, Czech Republic.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation (WMT08), Colmbus, Ohio.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009 work-shop on statistical machine translation.
In Proceedingsof the Fourth Workshop on Statistical Machine Trans-lation (WMT09), Athens, Greece.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on statisti-cal machine translation and metrics for machine trans-lation.
In Proceedings of the Fourth Workshop on Sta-tistical Machine Translation (WMT10), Uppsala, Swe-den.Boxing Chen and Roland Kuhn.
2011.
Amber: A mod-ified bleu, enhanced ranking metric.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measur-ment, 20(1):37?46.Antonio M.
Corb?
?-Bellot, Mikel L. Forcada, Sergio Ortiz-Rojas, Juan Antonio Pe?rez-Ortiz, Gema Ram?
?rez-Sa?nchez, Felipe Sa?nchez-Mart?
?nez, In?aki Alegria,Aingeru Mayor, and Kepa Sarasola.
2005.
An open-source shallow-transfer machine translation engine forthe romance languages of Spain.
In Proceedings of theEuropean Association for Machine Translation, pages79?86.Marta R. Costa-jussa` and Rafael E. Banchs.
2011.
TheBM-I2R Haitian-Cre?ole-to-English translation systemdescription for the WMT 2011 evaluation campaign.In Proceedings of the Sixth Workshop on StatisticalMachine Translation.Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng.
2011.TESLA at WMT 2011: Translation evaluation and tun-able metric.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Michael Denkowski and Alon Lavie.
2011a.
Meteor 1.3:Automatic metric for reliable optimization and evalu-ation of machine translation systems.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion.Michael Denkowski and Alon Lavie.
2011b.
METEOR-Tuned Phrase-Based SMT: CMU French-English andHaitian-English Systems for WMT 2011.
TechnicalReport CMU-LTI-11-011, Language Technologies In-stitute, Carnegie Mellon University.Chris Dyer, Kevin Gimpel, Jonathan H. Clark, andNoah A. Smith.
2011.
The CMU-ARK German-English translation system.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Vladimir Eidelman, Kristy Hollingshead, and PhilipResnik.
2011.
Noisy SMS machine translation in low-density languages.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.Christian Federmann and Sabine Hunsicker.
2011.Stochastic parse tree selection for an existing RBMTsystem.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation.Robert Frederking, Alexander Rudnicky, and ChristopherHogan.
1997.
Interactive speech translation in theDIPLOMAT project.
In Proceedings of the ACL-1997Workshop on Spoken Language Translation.Markus Freitag, Gregor Leusch, Joern Wuebker, StephanPeitz, Hermann Ney, Teresa Herrmann, Jan Niehues,Alex Waibel, Alexandre Allauzen, Gilles Adda,Josep Maria Crego, Bianka Buschbeck, Tonio Wand-macher, and Jean Senellart.
2011.
Joint WMT sub-mission of the QUAERO project.
In Proceedings ofthe Sixth Workshop on Statistical Machine Translation.Yoko Futagi, Paul Deane, Martin Chodorow, and JoelTetreault.
2008.
A computational approach to de-tecting collocation errors in the writing of non-nativespeakers of English.
Computer Assisted LanguageLearning Journal.Jesu?s Gonza?lez-Rubio and Francisco Casacuberta.
2011.The UPV-PRHLT combination system for WMT 2011.44In Proceedings of the Sixth Workshop on StatisticalMachine Translation.Greg Hanneman and Alon Lavie.
2011.
CMU syntax-based machine translation at WMT 2011.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,Marcello Federico, and Mathur Prashant.
2011.
TheUppsala-FBK systems at WMT 2011.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion.Kenneth Heafield and Alon Lavie.
2011.
CMU systemcombination in WMT 2011.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Teresa Herrmann, Mohammed Mediani, Jan Niehues,and Alex Waibel.
2011.
The Karlsruhe Institute ofTechnology translation systems for the WMT 2011.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation.Sanjika Hewavitharana, Nguyen Bach, Qin Gao, VamshiAmbati, and Stephan Vogel.
2011.
CMU HaitianCreole-English translation system for WMT 2011.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation.Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.2011.
Experiments with word alignment, normaliza-tion and clause reordering for SMT between Englishand German.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-delman, Olivia Buzek, and Benjamin B. Bederson.2011.
The value of monolingual crowdsourcing ina real-world translation scenario: Simulation usingHaitian Creole emergency SMS messages.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Matthias Huck, Joern Wuebker, Christoph Schmidt,Markus Freitag, Stephan Peitz, Daniel Stein, ArnaudDagnelies, Saab Mansour, Gregor Leusch, and Her-mann Ney.
2011.
The RWTH Aachen machine trans-lation system for WMT 2011.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Maxim Khalilov and Khalil Sima?an.
2011.
ILLC-UvAtranslation system for EMNLP-WMT 2011.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweenEuropean languages.
In Proceedings of NAACL 2006Workshop on Statistical Machine Translation, NewYork, New York.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the ACL-2007 Demo and Poster Sessions,Prague, Czech Republic.Oliver Lacey-Hall.
2011.
The guardian?s poverty mattersblog: How remote teams can help the rapid responseto disasters, March.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Gregor Leusch and Hermann Ney.
2009.
Edit distanceswith block movements and error rate confidence esti-mates.
Machine Translation, 23:129?140.Gregor Leusch, Markus Freitag, and Hermann Ney.2011.
The RWTH system combination system forWMT 2011.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.William Lewis, Robert Munro, and Stephan Vogel.
2011.Crisis MT: Developing a cookbook for MT in crisissituations.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.William D. Lewis.
2010.
Haitian Creole: How tobuild and ship an MT engine from scratch in 4 days,17hours, & 30 minutes.
In Proceedings of EAMT2010.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation with syn-tax, semirings, discriminative training and other good-ies.
In Proceedings of the Joint Fifth Workshop on Sta-tistical Machine Translation and MetricsMATR, Upp-sala, Sweden, July.Ding Liu and Daniel Gildea.
2005.
Syntactic featuresfor evaluation of machine translation.
In Proceedingsof the ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 25?32.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011.Better evaluation metrics lead to better machine trans-lation.
In Proceedings of EMNLP.Vero?nica Lo?pez-Luden?a and Rube?n San-Segundo.
2011.UPM system for the translation task.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion.Matous?
Macha?c?ek and Ondr?ej Bojar.
2011.
Approxi-mating a deep-syntactic metric for MT evaluation andtuning.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation.45David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, andOndr?ej Bojar.
2011.
Two-step translation with gram-matical post-processing.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Robert Munro.
2010.
Crowdsourced translation foremergency response in Haiti: the global collabora-tion of local knowledge.
In Proceedings of the AMTAWorkshop on Collaborative Crowdsourcing for Trans-lation.Douglas W. Oard and Franz Josef Och.
2003.
Rapid-response machine translation for unexpected lan-guages.
In Proceedings of MT Summit IX.Douglas W. Oard.
2003.
The surprise language exer-cises.
ACM Transactions on Asian Language Infor-mation Processing, 2(2):79?84.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL-2003), Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automatic evalua-tion of machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL-2002), Philadelphia, Pennsylvania.Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-tin Chodorow.
2011.
E-rating machine translation.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation.Martin Popel, David Marec?ek, Nathan Green, andZdene?k Z?abokrtsky?.
2011.
Influence of parser choiceon dependency-based MT.
In Proceedings of the SixthWorkshop on Statistical Machine Translation.Maja Popovic?, David Vilar, Eleftherios Avramidis, andAljoscha Burchardt.
2011.
Evaluation without ref-erences: IBM1 scores as evaluation metrics.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Maja Popovic?.
2011.
Morphemes and POS tags for n-gram based evaluation metrics.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Marion Potet, Raphae?l Rubino, Benjamin Lecouteux,Ste?phane Huet, Laurent Besacier, Herve?
Blanchon,and Fabrice Lefe`vre.
2011.
The LIGA (LIG/LIA)machine translation system for WMT 2011.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Mark Przybocki, Kay Peterson, and Sebastian Bron-sart.
2008.
Official results of the NIST 2008 ?Met-rics for MAchine TRanslation?
challenge (Metrics-MATR08).
In AMTA-2008 workshop on Metrics forMachine Translation, Honolulu, Hawaii.Miguel Rios, Wilker Aziz, and Lucia Specia.
2011.TINE: A metric to assess MT adequacy.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Christian Rish?j and Anders S?gaard.
2011.
Factoredtranslation with unsupervised word clusters.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, andRichard Schwartz.
2011.
Expected BLEU training forgraphs: BBN system description for WMT11 systemcombination task.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.V?
?ctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-Mart?
?nez,and Juan Antonio Pe?rez-Ortiz.
2011.
The Univer-sitat d?Alacant hybrid machine translation system forWMT 2011.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Holger Schwenk, Patrik Lambert, Lo?
?c Barrault,Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,and Kashif Shah.
2011.
LIUM?s SMT machine trans-lation systems for WMT 2011.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Rico Sennrich.
2011.
The UZH system combination sys-tem for WMT 2011.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Americas(AMTA-2006), Cambridge, Massachusetts.Xingyi Song and Trevor Cohn.
2011.
Regression andranking based optimisation for sentence level MT eval-uation.
In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.
Ma-chine translation evaluation versus quality estimation.Machine Translation, 24(1):39?50.Sara Stymne.
2011.
Spell checking techniques for re-placement of unknown words and data cleaning forHaitian Creole SMS translation.
In Proceedings of theSixth Workshop on Statistical Machine Translation.Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of preposition error detection.
In Proceedingsof COLING, Manchester, UK.Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez.
2011.
Joshua3.0: Syntax-based machine translation with the Thraxgrammar extractor.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.Eric Wehrli, Luka Nerima, and Yves Scherrer.
2009.Deep linguistic multilingual translation and bilingualdictionaries.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, pages 90?94.46Daguang Xu, Yuan Cao, and Damianos Karakos.
2011a.Description of the JHU system combination schemefor WMT 2011.
In Proceedings of the Sixth Workshopon Statistical Machine Translation.Jia Xu, Hans Uszkoreit, Casey Kennington, David Vilar,and Xiaojun Zhang.
2011b.
DFKI hybrid machinetranslation system for WMT 2011 - on the integrationof SMT and RBMT.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Francisco Zamora-Martinez and Maria Jose Castro-Bleda.
2011.
CEU-UPV English-Spanish system forWMT11.
In Proceedings of the Sixth Workshop onStatistical Machine Translation.Daniel Zeman.
2011.
Hierarchical phrase-based MT atthe Charles University for the WMT 2011 shared task.In Proceedings of the Sixth Workshop on StatisticalMachine Translation.47A Pairwise System Comparisons by Human JudgesTables 19?38 show pairwise comparisons between systems for each language pair.
The numbers in each ofthe tables?
cells indicate the percentage of times that the system in that column was judged to be better thanthe system in that row.
Bolding indicates the winner of the two systems.
The difference between 100 andthe sum of the complementary cells is the percent of time that the two systems were judged to be equal.Because there were so many systems and data conditions the significance of each pairwise comparisonneeds to be quantified.
We applied the Sign Test to measure which comparisons indicate genuine differences(rather than differences that are attributable to chance).
In the following tables ?
indicates statistical signif-icance at p ?
0.10, ?
indicates statistical significance at p ?
0.05, and ?
indicates statistical significance atp ?
0.01, according to the Sign Test.B Automatic ScoresTables 39?48 give the automatic scores for each of the systems.C Meta-evaluationTables 49 and 50 give a detailed breakdown of intra- and inter-annotator agreement rates for all of manualevaluation tracks of WMT11, broken down by language pair.48REFCSTCU-BOJARCU-ZEMANJHUONLINE-BSYSTRANUEDINUPPSALAREF ?
.02?
.04?
.01?
.04?
.04?
.04?
.05?
.04?CST .88?
?
.49?
.36 .49?
.59?
.41 .58?
.44?CU-BOJAR .91?
.27?
?
.27?
.30 .48?
.28?
.41?
.41CU-ZEMAN .94?
.31 .49?
?
.47?
.67?
.47?
.64?
.49?JHU .89?
.29?
.39 .28?
?
.47?
.36 .41?
.36ONLINE-B .84?
.20?
.27?
.19?
.28?
?
.24?
.30 .27?SYSTRAN .91?
.31 .49?
.30?
.39 .59?
?
.56?
.37UEDIN .89?
.16?
.25?
.16?
.27?
.36 .23?
?
.25?UPPSALA .84?
.28?
.40 .24?
.37 .49?
.38 .45?
?> others .89 .23 .36 .23 .33 .46 .31 .43 .33>= others .96 .47 .60 .44 .57 .68 .51 .69 .57Table 19: Ranking scores for entries in the Czech-English task (individual system track).REFCOMMERCIAL-1COMMERCIAL-2CU-BOJARCU-MARECEKCU-POPELCU-TAMCHYNACU-ZEMANJHUONLINE-BUEDINREF ?
.05?
.04?
.04?
.04?
.05?
.05?
.04?
.03?
.04?
.04?COMMERCIAL-1 .91?
?
.36 .53?
.50?
.47?
.44?
.33?
.33?
.55?
.45?COMMERCIAL-2 .87?
.42 ?
.52?
.47?
.47?
.50?
.30?
.40 .50?
.43CU-BOJAR .89?
.31?
.31?
?
.29 .41 .21?
.19?
.27?
.42?
.31?CU-MARECEK .88?
.31?
.37?
.27 ?
.35?
.28 .21?
.30?
.39 .28?CU-POPEL .85?
.33?
.29?
.43 .45?
?
.41 .27?
.31?
.50?
.39CU-TAMCHYNA .87?
.34?
.35?
.30?
.32 .40 ?
.22?
.25?
.45?
.32CU-ZEMAN .91?
.47?
.52?
.56?
.56?
.55?
.55?
?
.44?
.64?
.54?JHU .91?
.43?
.41 .50?
.47?
.51?
.51?
.31?
?
.52?
.48?ONLINE-B .86?
.27?
.32?
.33?
.39 .33?
.29?
.18?
.23?
?
.31?UEDIN .85?
.34?
.40 .40?
.37?
.42 .36 .24?
.25?
.44?
?> others .88 .33 .34 .39 .39 .40 .36 .23 .28 .44 .35>= others .96 .51 .51 .64 .63 .58 .62 .43 .49 .65 .59Table 20: Ranking scores for entries in the English-Czech task (individual system track).49REFCMU-DYERCSTCU-ZEMANDFKI-XUJHUKITKOCLIMSILINGUATECLIUONLINE-AONLINE-BRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5RWTH-WUEBKERUEDINUPPSALAREF ?
.05?
.02?
.03?
.04?
.00?
.08?
.04?
.00?
.07?
.05?
.07?
.14?
.02?
.08?
.00?
.06?
.08?
.02?
.10?
.08?CMU-DYER .95?
?
.18?
.17?
.33 .26?
.22?
.12?
.29?
.43 .23?
.43 .54 .32 .20?
.40 .43 .48 .31 .19?
.18?CST .96?
.74?
?
.42 .62?
.35 .68?
.44?
.47?
.78?
.62?
.77?
.73?
.81?
.70?
.74?
.67?
.53?
.65?
.47 .51CU-ZEMAN .97?
.67?
.22 ?
.56?
.26?
.41 .22?
.48 .66?
.46 .60?
.62?
.73?
.57?
.60?
.62?
.53?
.40 .44 .48DFKI-XU .94?
.44 .06?
.24?
?
.10?
.26 .17?
.49?
.47 .21?
.42 .45 .52 .42 .45 .51 .39 .40 .48 .29JHU 1.00?.61?
.33 .55?
.64?
?
.59?
.45 .51?
.59 .52?
.68?
.63?
.62?
.64?
.65?
.58?
.46 .61?
.44 .38KIT .87?
.65?
.12?
.21 .44 .23?
?
.34 .40 .54 .30 .43 .57?
.44 .43 .47 .50 .53 .40 .28 .17?KOC .96?
.64?
.09?
.49?
.66?
.36 .43 ?
.43 .69?
.57?
.69?
.63?
.62?
.41 .63?
.59 .52?
.51 .59?
.40LIMSI .96?
.54?
.24?
.30 .22?
.25?
.38 .27 ?
.63?
.52 .43 .55?
.43 .43 .59?
.47 .40 .41 .32 .44LINGUATEC .91?
.45 .13?
.24?
.38 .32 .34 .18?
.27?
?
.26?
.45 .62?
.46 .20?
.49 .53 .36 .41 .32?
.29?LIU .89?
.49?
.14?
.29 .54?
.25?
.48 .24?
.31 .64?
?
.47 .61?
.52 .46 .48 .50 .23?
.48 .37 .36ONLINE-A .88?
.47 .12?
.25?
.42 .18?
.41 .19?
.39 .39 .30 ?
.32 .26?
.28 .46 .36 .35 .42 .19?
.27?ONLINE-B .78?
.38 .16?
.23?
.33 .28?
.26?
.16?
.26?
.29?
.22?
.38 ?
.23?
.23?
.29?
.29?
.22?
.27 .22?
.18?RBMT-1 .96?
.42 .09?
.18?
.35 .21?
.51 .23?
.43 .41 .38 .56?
.62?
?
.31 .46 .39 .13 .48 .50 .30?RBMT-2 .86?
.54?
.15?
.28?
.48 .29?
.43 .41 .39 .55?
.44 .51 .64?
.43 ?
.55?
.47 .54?
.44 .41 .29?RBMT-3 .92?
.42 .11?
.27?
.32 .23?
.47 .18?
.19?
.34 .38 .49 .55?
.38 .26?
?
.36 .29?
.34 .33 .28?RBMT-4 .88?
.36 .19?
.24?
.38 .29?
.43 .38 .45 .32 .37 .44 .56?
.33 .34 .45 ?
.35 .29?
.51 .24?RBMT-5 .92?
.45 .27?
.27?
.45 .32 .37 .27?
.47 .47 .61?
.55 .67?
.26 .24?
.53?
.46 ?
.45 .47 .39RWTH-WUEBKER .93?
.50 .23?
.26 .33 .20?
.24 .36 .41 .44 .39 .47 .55 .44 .38 .53 .56?
.45 ?
.21 .39UEDIN .88?
.59?
.24 .28 .28 .33 .50 .24?
.45 .65?
.40 .67?
.62?
.34 .39 .52 .41 .36 .43 ?
.48UPPSALA .92?
.64?
.27 .29 .39 .44 .58?
.32 .41 .66?
.53 .68?
.69?
.59?
.59?
.58?
.61?
.54 .36 .31 ?> others .92 .50 .17 .28 .40 .26 .40 .26 .38 .51 .40 .51 .57 .43 .38 .49 .47 .39 .41 .36 .32>= others .95 .66 .37 .47 .60 .43 .57 .45 .56 .63 .57 .66 .72 .60 .54 .64 .61 .56 .59 .55 .47Table 21: Ranking scores for entries in the German-English task (individual system track).50REFCOPENHAGENCU-TAMCHYNACU-ZEMANDFKI-FEDERMANNDFKI-XUILLC-UVAJHUKITKOCLIMSILIUONLINE-AONLINE-BRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5RWTH-FREITAGUEDINUOWUPPSALAREF ?
.08?
.06?
.00?
.13?
.02?
.05?
.05?
.02?
.02?
.16?
.06?
.11?
.07?
.14?
.14?
.19?
.11?
.11?
.16?
.07?
.07?
.08?COPENHAGEN .85?
?
.31 .09?
.60?
.39 .25 .32 .41 .27 .36 .34 .49?
.61?
.56?
.61?
.64?
.64?
.60 .26 .49 .30 .16CU-TAMCHYNA .92?
.37 ?
.13?
.61?
.48?
.30 .38 .58?
.33 .39 .41?
.55?
.57?
.72?
.69?
.81?
.49 .59?
.47 .39 .40 .43CU-ZEMAN 1.00?.60?
.41?
?
.76?
.78?
.51?
.47?
.64?
.53?
.66?
.49?
.77?
.68?
.69?
.64?
.70?
.64?
.72?
.55?
.47 .44 .50DFKI-FEDERMANN .72?
.19?
.17?
.16?
?
.39 .25?
.38 .38 .24?
.32 .29 .35 .40 .43 .33 .39 .19 .33?
.22?
.31 .11?
.30DFKI-XU .84?
.31 .21?
.08?
.37 ?
.25?
.32 .34 .12?
.37 .30 .35 .47 .54?
.30 .51?
.43 .37 .20?
.22?
.25?
.14?ILLC-UVA .90?
.39 .37 .25?
.63?
.50?
?
.41?
.58?
.35 .56?
.38 .55?
.63?
.61?
.63?
.71?
.75?
.62?
.33 .56?
.38 .41JHU .91?
.45 .27 .27?
.41 .40 .20?
?
.37 .27 .43 .50?
.58?
.59?
.43 .55?
.72?
.50 .50 .50?
.47 .46 .22?KIT .87?
.24 .23?
.17?
.41 .43 .26?
.37 ?
.16?
.51 .27?
.37 .45?
.47 .39 .58?
.53 .47 .23?
.24 .21?
.17?KOC .95?
.35 .35 .13?
.61?
.65?
.38 .42 .57?
?
.47?
.33 .47?
.62?
.61?
.53?
.64?
.63?
.45 .20 .38 .37 .18?LIMSI .77?
.31 .26 .11?
.48 .35 .18?
.30 .33 .23?
?
.36 .39 .50?
.52 .47 .48 .39 .42 .18?
.22?
.28 .14?LIU .84?
.32 .20?
.25?
.51 .38 .26 .21?
.51?
.35 .39 ?
.51 .49?
.63?
.52?
.56 .48?
.56 .29 .38 .25 .25ONLINE-A .75?
.21?
.24?
.09?
.48 .41 .22?
.30?
.37 .25?
.37 .37 ?
.46 .37 .41 .47 .33 .44 .27?
.28 .22?
.16?ONLINE-B .91?
.17?
.15?
.13?
.44 .22 .17?
.16?
.20?
.15?
.24?
.25?
.27 ?
.43 .35 .48 .33 .17?
.17?
.26 .12?
.20?RBMT-1 .80?
.23?
.11?
.20?
.37 .28?
.18?
.29 .38 .25?
.36 .30?
.41 .38 ?
.34 .45 .36 .02?
.17?
.17?
.28?
.24?RBMT-2 .80?
.20?
.10?
.16?
.43 .38 .20?
.27?
.45 .22?
.36 .30?
.38 .51 .43 ?
.48 .40 .42 .31?
.28?
.16?
.25?RBMT-3 .65?
.18?
.14?
.15?
.37 .29?
.17?
.22?
.25?
.20?
.27 .33 .33 .29 .30 .31 ?
.34 .16?
.24?
.35 .20?
.11?RBMT-4 .80?
.21?
.28 .22?
.19 .26 .09?
.32 .29 .27?
.39 .27?
.43 .44 .38 .38 .45 ?
.42 .29?
.36 .27?
.31?RBMT-5 .88?
.35 .31?
.15?
.54?
.51 .26?
.34 .36 .36 .44 .35 .44 .59?
.37?
.33 .62?
.38 ?
.29 .45 .38 .30RWTH-FREITAG .80?
.31 .27 .17?
.62?
.55?
.19 .25?
.56?
.30 .49?
.41 .53?
.59?
.56?
.53?
.62?
.57?
.45 ?
.36 .38 .24UEDIN .82?
.27 .27 .27 .46 .47?
.17?
.28 .36 .33 .48?
.27 .47 .43 .75?
.55?
.52 .50 .43 .21 ?
.35 .27UOW .86?
.39 .21 .23 .74?
.53?
.36 .38 .64?
.20 .38 .41 .74?
.61?
.56?
.64?
.57?
.65?
.38 .26 .41 ?
.31UPPSALA .79?
.32 .35 .29 .54 .57?
.34 .51?
.51?
.45?
.53?
.43 .73?
.70?
.55?
.64?
.77?
.57?
.55 .43 .33 .41 ?> others .84 .29 .24 .17 .48 .42 .24 .31 .42 .27 .40 .34 .46 .51 .51 .47 .56 .46 .41 .29 .34 .29 .25>= others .91 .56 .50 .38 .68 .67 .48 .54 .64 .53 .65 .59 .65 .730 .70 .66 .732 .66 .58 .56 .60 .53 .49Table 22: Ranking scores for entries in the English-German task (individual system track).REFALACANTCU-ZEMANHYDERABADKOCONLINE-AONLINE-BRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5SYSTRANUEDINUFAL-UMUPMREF ?
.03?
.02?
.00?
.02?
.03?
.12?
.15?
.04?
.07?
.05?
.02?
.03?
.03?
.03?
.07?ALACANT .86?
?
.07?
.08?
.30 .52 .31 .27?
.29?
.54 .49 .32?
.51 .27?
.26?
.26?CU-ZEMAN .98?
.89?
?
.48 .84?
.85?
.94?
.90?
.83?
.87?
.85?
.78?
.97?
.79?
.79?
.91?HYDERABAD .98?
.86?
.27 ?
.88?
.95?
.92?
.85?
.96?
.74?
.82?
.80?
.88?
.91?
.80?
.86?KOC .93?
.48 .06?
.06?
?
.28 .39 .40 .34 .44 .38 .26?
.59?
.22?
.20?
.18?ONLINE-A .90?
.28 .02?
.02?
.48 ?
.32 .34 .34 .26?
.34 .19?
.35 .20?
.11?
.20?ONLINE-B .79?
.33 .04?
.00?
.47 .30 ?
.24?
.31?
.31?
.27?
.25?
.33 .27?
.21?
.07?RBMT-1 .81?
.52?
.05?
.11?
.50 .57 .62?
?
.50 .36 .34 .17 .40 .39 .34 .30?RBMT-2 .96?
.61?
.09?
.04?
.52 .47 .59?
.37 ?
.39 .46 .27 .58?
.29?
.24?
.45RBMT-3 .88?
.31 .09?
.13?
.44 .56?
.60?
.53 .37 ?
.47 .14?
.52 .40 .23?
.31RBMT-4 .90?
.38 .08?
.16?
.50 .53 .60?
.41 .43 .38 ?
.43 .52 .33?
.18?
.22?RBMT-5 .94?
.61?
.06?
.10?
.54?
.70?
.63?
.37 .45 .59?
.41 ?
.66?
.42 .50 .43SYSTRAN .92?
.33 .02?
.10?
.25?
.53 .53 .42 .30?
.36 .38 .27?
?
.21?
.41 .24?UEDIN .95?
.63?
.13?
.02?
.63?
.67?
.59?
.47 .61?
.53 .59?
.42 .53?
?
.32?
.45UFAL-UM .94?
.63?
.10?
.11?
.56?
.70?
.74?
.51 .61?
.59?
.74?
.36 .47 .61?
?
.44UPM .85?
.54?
.02?
.03?
.62?
.61?
.81?
.59?
.45 .55 .68?
.40 .60?
.42 .38 ?> others .91 .51 .07 .10 .52 .56 .59 .48 .48 .47 .48 .35 .54 .39 .34 .36>= others .96 .66 .16 .17 .67 .723 .723 .63 .60 .61 .60 .51 .66 .51 .47 .50Table 23: Ranking scores for entries in the Spanish-English task (individual system track).51REFCEU-UPVCU-ZEMANKOCONLINE-AONLINE-BPROMTRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5UEDINUOWUPMUPPSALAREF ?
.06?
.03?
.09?
.09?
.09?
.05?
.03?
.06?
.04?
.08?
.02?
.08?
.02?
.03?
.04?CEU-UPV .84?
?
.21?
.20?
.43 .36 .42 .37 .34?
.50?
.31 .34 .32 .21?
.13?
.22CU-ZEMAN .87?
.56?
?
.38?
.56?
.56?
.58?
.46?
.40 .70?
.46?
.49?
.51?
.45?
.19?
.49?KOC .84?
.41?
.22?
?
.56?
.51?
.48?
.54?
.39 .55?
.42 .35 .51?
.44 .11?
.34ONLINE-A .72?
.31 .24?
.15?
?
.36 .37 .28?
.23?
.35 .25?
.20?
.29?
.25?
.08?
.09?ONLINE-B .72?
.30 .17?
.18?
.26 ?
.29 .23?
.20?
.37 .20?
.19?
.19?
.22?
.02?
.23?PROMT .76?
.29 .21?
.25?
.42 .43 ?
.24?
.24 .19 .27?
.26?
.32 .25?
.18?
.21?RBMT-1 .85?
.37 .29?
.23?
.51?
.54?
.48?
?
.35 .45?
.40?
.05?
.47 .39 .25?
.39RBMT-2 .86?
.50?
.35 .38 .51?
.48?
.35 .39 ?
.41?
.34 .36 .45 .36 .23?
.41RBMT-3 .86?
.26?
.18?
.22?
.40 .35 .19 .20?
.22?
?
.25?
.23?
.24?
.33 .10?
.22?RBMT-4 .80?
.45 .29?
.34 .53?
.51?
.43?
.21?
.38 .43?
?
.24?
.34 .30 .20?
.45?RBMT-5 .96?
.43 .29?
.42 .57?
.61?
.46?
.22?
.38 .49?
.47?
?
.50 .46 .27?
.47UEDIN .74?
.28 .20?
.21?
.46?
.48?
.43 .37 .31 .49?
.45 .35 ?
.20?
.14?
.23UOW .90?
.44?
.18?
.32 .46?
.52?
.56?
.39 .39 .44 .45 .36 .38?
?
.10?
.32UPM .93?
.65?
.53?
.67?
.74?
.71?
.69?
.59?
.51?
.74?
.60?
.51?
.64?
.68?
?
.62?UPPSALA .84?
.36 .21?
.32 .49?
.42?
.45?
.39 .35 .45?
.29?
.41 .35 .30 .15?
?> others .83 .38 .24 .30 .47 .46 .41 .33 .32 .43 .35 .29 .38 .33 .14 .31>= others .94 .65 .49 .56 .72 .74 .70 .60 .57 .71 .61 .54 .64 .59 .34 .61Table 24: Ranking scores for entries in the English-Spanish task (individual system track).REFCMU-DENKOWSKICMU-HANNEMANCU-ZEMANJHUKITLIA-LIGLIMSILIUMONLINE-AONLINE-BRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5RWTH-HUCKSYSTRANUEDINREF ?
.10?
.18?
.06?
.03?
.14?
.15?
.14?
.14?
.12?
.05?
.12?
.09?
.05?
.06?
.05?
.05?
.07?
.02?CMU-DENKOWSKI .79?
?
.35 .12?
.34 .32 .41 .35 .21?
.47?
.46 .49 .32 .33 .36 .35 .25 .45 .29CMU-HANNEMAN .79?
.35 ?
.17?
.29 .44?
.43 .52?
.45 .45 .49 .51 .39 .44 .38 .35 .35 .43 .37CU-ZEMAN .94?
.61?
.67?
?
.54?
.66?
.66?
.58?
.60?
.59?
.88?
.62?
.59?
.63?
.60?
.56 .68?
.64?
.40JHU .82?
.34 .29 .22?
?
.26 .54?
.40 .36 .43 .40 .49 .42 .40 .34 .35 .36 .47 .20?KIT .79?
.39 .20?
.16?
.40 ?
.26?
.46 .34 .38 .52 .38 .35 .39 .28 .38 .15?
.32 .30LIA-LIG .75?
.24 .31 .28?
.24?
.59?
?
.49 .27 .40 .46 .35 .26 .31?
.29 .32 .32 .33?
.35LIMSI .86?
.30 .25?
.21?
.31 .26 .26 ?
.38 .40 .42 .35 .18?
.43 .34 .16?
.34 .34 .33LIUM .78?
.45?
.33 .16?
.38 .34 .44 .40 ?
.38 .30 .44 .26?
.33?
.38 .28 .29 .33 .28ONLINE-A .80?
.23?
.21 .22?
.37 .35 .36 .33 .46 ?
.43 .35 .16?
.33 .24?
.20?
.26 .34 .27?ONLINE-B .86?
.37 .31 .04?
.46 .22 .36 .33 .43 .26 ?
.40 .20?
.16?
.44 .20?
.41 .38 .22?RBMT-1 .87?
.44 .35 .23?
.46 .44 .54 .48 .44 .53 .54 ?
.39 .37 .33 .11?
.39 .17?
.35RBMT-2 .84?
.47 .37 .26?
.40 .50 .45 .52?
.54?
.58?
.67?
.45 ?
.51 .35 .22?
.51 .57 .41RBMT-3 .89?
.44 .42 .19?
.40 .43 .54?
.46 .61?
.50 .71?
.37 .32 ?
.42 .35 .42 .47 .40RBMT-4 .85?
.53 .36 .26?
.51 .47 .55 .52 .46 .59?
.40 .43 .50 .42 ?
.34 .46 .44 .41RBMT-5 .93?
.58 .55 .33 .54 .54 .59 .70?
.56 .66?
.65?
.36?
.54?
.46 .37 ?
.50 .54?
.54RWTH-HUCK .92?
.43 .38 .14?
.36 .59?
.41 .44 .29 .53 .48 .46 .30 .46 .32 .38 ?
.37 .17?SYSTRAN .93?
.39 .38 .24?
.44 .48 .60?
.50 .40 .55 .57 .45?
.36 .29 .44 .21?
.49 ?
.36UEDIN .93?
.48 .41 .40 .51?
.48 .54 .49 .46 .60?
.57?
.52 .37 .47 .39 .39 .51?
.52 ?> others .85 .39 .36 .21 .39 .41 .46 .46 .41 .46 .50 .41 .33 .39 .35 .28 .37 .39 .32>= others .91 .62 .58 .37 .61 .64 .64 .661 .63 .661 .66 .58 .52 .55 .53 .45 .58 .54 .50Table 25: Ranking scores for entries in the French-English task (individual system track).52REFCU-ZEMANJHUKITLATL-GENEVALIMSILIUMONLINE-AONLINE-BRBMT-1RBMT-2RBMT-3RBMT-4RBMT-5RWTH-HUCKUEDINUPPSALAUPPSALA-FBKREF ?
.07?
.06?
.25?
.07?
.13?
.20?
.15?
.20?
.10?
.09?
.18?
.11?
.12?
.14?
.18?
.16?
.16?CU-ZEMAN .92?
?
.83?
.86?
.63?
.85?
.90?
.86?
.81?
.89?
.70?
.75?
.75?
.61?
.78?
.79?
.81?
.81?JHU .91?
.07?
?
.55?
.30?
.60?
.50?
.55?
.59?
.45 .41 .34?
.30?
.50 .40 .42 .42 .44KIT .63?
.04?
.29?
?
.18?
.47 .37 .30?
.37 .38 .30?
.37 .24?
.34 .28 .34 .24?
.13?LATL-GENEVA .86?
.29?
.54?
.73?
?
.77?
.67?
.71?
.79?
.55?
.39 .66?
.52 .58?
.58?
.51 .52 .58?LIMSI .75?
.04?
.21?
.29 .13?
?
.23?
.28?
.37 .27?
.27?
.24?
.24?
.21?
.27?
.28?
.25?
.31LIUM .76?
.04?
.26?
.44 .24?
.46?
?
.33 .52 .48 .25?
.36 .25?
.28?
.43 .40 .35 .32ONLINE-A .78?
.10?
.31?
.51?
.22?
.51?
.46 ?
.44 .39 .36 .41 .30?
.41 .41 .32?
.46 .33ONLINE-B .70?
.06?
.27?
.41 .13?
.39 .32 .30 ?
.47 .22?
.26?
.13?
.28?
.32 .26?
.33 .27?RBMT-1 .83?
.07?
.38 .46 .23?
.56?
.39 .41 .42 ?
.17?
.34 .36 .13 .52 .33?
.40 .40RBMT-2 .88?
.25?
.47 .59?
.37 .65?
.63?
.51 .57?
.54?
?
.58?
.39 .54?
.63?
.61?
.47 .42RBMT-3 .80?
.19?
.54?
.42 .20?
.60?
.47 .44 .52?
.42 .18?
?
.21?
.43 .51 .55 .41 .39RBMT-4 .82?
.22?
.54?
.63?
.33 .63?
.64?
.54?
.59?
.41 .44 .46?
?
.47 .68?
.53 .42 .39RBMT-5 .86?
.18?
.46 .53 .20?
.62?
.56?
.46 .61?
.22 .33?
.40 .34 ?
.43 .52 .40 .53?RWTH-HUCK .76?
.08?
.33 .38 .21?
.60?
.40 .38 .43 .36 .18?
.37 .21?
.38 ?
.39 .22?
.29UEDIN .78?
.15?
.37 .46 .34 .49?
.38 .53?
.58?
.56?
.33?
.35 .36 .37 .47 ?
.38 .31UPPSALA .77?
.07?
.36 .53?
.36 .49?
.46 .46 .56 .46 .38 .42 .39 .55 .57?
.39 ?
.47UPPSALA-FBK .80?
.10?
.40 .71?
.27?
.50 .47 .51 .53?
.42 .48 .41 .52 .29?
.50 .47 .40 ?> others .80 .12 .39 .51 .25 .55 .48 .45 .52 .43 .32 .41 .33 .39 .46 .43 .39 .38>= others .86 .20 .55 .69 .39 .73 .64 .60 .70 .61 .46 .58 .49 .55 .65 .58 .55 .54Table 26: Ranking scores for entries in the English-French task (individual system track).REFBM-I2RCMU-DENKOWSKICMU-HEWAVITHARANAHYDERABADKOCLIUUMD-EIDELMANUMD-HUUPPSALAREF ?
.03?
.01?
.03?
.02?
.01?
.00?
.01?
.01?
.02?BM-I2R .91?
?
.28?
.27?
.13?
.08?
.19?
.30?
.30?
.24?CMU-DENKOWSKI .93?
.44?
?
.25 .22?
.15?
.28?
.33 .29?
.31?CMU-HEWAVITHARANA .91?
.40?
.31 ?
.21?
.16?
.29?
.35 .39 .30HYDERABAD .96?
.71?
.59?
.58?
?
.27?
.56?
.57?
.42 .52?KOC .94?
.78?
.75?
.64?
.55?
?
.65?
.69?
.62?
.64?LIU .92?
.56?
.42?
.44?
.27?
.24?
?
.43 .41 .39UMD-EIDELMAN .94?
.44?
.35 .35 .17?
.17?
.34 ?
.37 .31?UMD-HU .90?
.50?
.57?
.45 .35 .21?
.46 .45 ?
.42UPPSALA .93?
.48?
.47?
.39 .31?
.20?
.40 .43?
.37 ?> others .93 .49 .42 .39 .25 .17 .35 .40 .36 .35>= others .98 .71 .66 .64 .43 .31 .55 .63 .52 .57Table 27: Ranking scores for entries in the Haitian Creole (Clean)-English task (individual system track).53REFBM-I2RCMU-DENKOWSKICMU-HEWAVITHARANAJHULIUUMD-EIDELMANREF ?
.05?
.03?
.04?
.02?
.02?
.03?BM-I2R .83?
?
.29?
.25?
.22?
.30?
.30?CMU-DENKOWSKI .89?
.44?
?
.37?
.23?
.37 .30?CMU-HEWAVITHARANA .86?
.43?
.26?
?
.27?
.37 .32JHU .96?
.62?
.53?
.49?
?
.52?
.47?LIU .92?
.48?
.38 .34 .31?
?
.36UMD-EIDELMAN .92?
.48?
.44?
.42 .29?
.41 ?> others .90 .43 .34 .33 .23 .34 .30>= others .97 .65 .59 .60 .41 .55 .52Table 28: Ranking scores for entries in the Haitian Creole (Raw)-English task (individual system track).REFBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBOUPV-PRHLT-COMBOREF ?
.01?
.02?
.01?
.01?BBN-COMBO .91?
?
.25 .18?
.16?CMU-HEAFIELD-COMBO .90?
.24 ?
.17?
.12?JHU-COMBO .92?
.27?
.29?
?
.20?UPV-PRHLT-COMBO .94?
.41?
.42?
.36?
?> others .92 .23 .24 .18 .12>= others .99 .62 .64 .58 .47Table 29: Ranking scores for entries in the Czech-English task (system combination track).REFCMU-HEAFIELD-COMBOUPV-PRHLT-COMBOREF ?
.04?
.04?CMU-HEAFIELD-COMBO .86?
?
.17?UPV-PRHLT-COMBO .88?
.30?
?> others .87 .17 .11>= others .96 .48 .41Table 30: Ranking scores for entries in the English-Czech task (system combination track).54REFBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBOKOC-COMBOQUAERO-COMBORWTH-LEUSCH-COMBOUPV-PRHLT-COMBOUZH-COMBOREF ?
.11?
.09?
.04?
.09?
.10?
.14?
.05?
.09?BBN-COMBO .79?
?
.45?
.32 .21?
.28?
.39 .31?
.36CMU-HEAFIELD-COMBO .84?
.23?
?
.21?
.17?
.19?
.25?
.19?
.31JHU-COMBO .85?
.42 .55?
?
.25?
.28?
.40?
.28?
.47?KOC-COMBO .83?
.56?
.62?
.45?
?
.41 .54?
.40?
.51?QUAERO-COMBO .86?
.52?
.64?
.45?
.36 ?
.54?
.49?
.48RWTH-LEUSCH-COMBO .83?
.28 .41?
.22?
.20?
.22?
?
.22?
.38UPV-PRHLT-COMBO .85?
.47?
.57?
.42?
.25?
.26?
.48?
?
.49?UZH-COMBO .86?
.34 .38 .31?
.29?
.32 .41 .30?
?> others .84 .36 .46 .30 .22 .26 .39 .27 .39>= others .91 .61 .70 .56 .45 .46 .65 .52 .60Table 31: Ranking scores for entries in the German-English task (system combination track).REFCMU-HEAFIELD-COMBOKOC-COMBOUPV-PRHLT-COMBOUZH-COMBOREF ?
.11?
.09?
.10?
.11?CMU-HEAFIELD-COMBO .81?
?
.19?
.23?
.32KOC-COMBO .84?
.48?
?
.38?
.47?UPV-PRHLT-COMBO .81?
.36?
.23?
?
.37?UZH-COMBO .80?
.34 .24?
.31?
?> others .81 .320 .19 .25 .318>= others .90 .61 .46 .56 .58Table 32: Ranking scores for entries in the English-German task (system combination track).REFBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBOKOC-COMBORWTH-LEUSCH-COMBOUPV-PRHLT-COMBOREF ?
.05?
.09?
.05?
.07?
.06?
.08?BBN-COMBO .81?
?
.34 .27 .21?
.27 .26CMU-HEAFIELD-COMBO .84?
.31 ?
.18?
.15?
.29 .20JHU-COMBO .83?
.25 .32?
?
.27 .35?
.25KOC-COMBO .84?
.39?
.39?
.32 ?
.39?
.31?RWTH-LEUSCH-COMBO .81?
.24 .23 .16?
.17?
?
.14?UPV-PRHLT-COMBO .77?
.30 .26 .27 .22?
.35?
?> others .82 .25 .27 .21 .18 .28 .21>= others .93 .64 .67 .62 .56 .71 .64Table 33: Ranking scores for entries in the Spanish-English task (system combination track).55REFCMU-HEAFIELD-COMBOKOC-COMBOUOW-COMBOUPV-PRHLT-COMBOREF ?
.10?
.07?
.09?
.08?CMU-HEAFIELD-COMBO .70?
?
.15?
.21?
.17?KOC-COMBO .76?
.35?
?
.36?
.19UOW-COMBO .72?
.29?
.22?
?
.25?UPV-PRHLT-COMBO .76?
.35?
.16 .35?
?> others .73 .27 .15 .25 .17>= others .91 .69 .58 .63 .59Table 34: Ranking scores for entries in the English-Spanish task (system combination track).REFBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBOLIUM-COMBORWTH-LEUSCH-COMBOUPV-PRHLT-COMBOREF ?
.04?
.04?
.06?
.06?
.06?
.02?BBN-COMBO .82?
?
.35 .25 .18?
.21?
.21?CMU-HEAFIELD-COMBO .90?
.29 ?
.30 .20?
.29 .25?JHU-COMBO .83?
.35 .40 ?
.31?
.36 .21?LIUM-COMBO .83?
.42?
.40?
.44?
?
.38?
.35RWTH-LEUSCH-COMBO .83?
.34?
.29 .30 .22?
?
.21?UPV-PRHLT-COMBO .91?
.49?
.40?
.34?
.30 .40?
?> others .85 .32 .31 .28 .21 .28 .21>= others .95 .67 .62 .59 .53 .63 .53Table 35: Ranking scores for entries in the French-English task (system combination track).REFCMU-HEAFIELD-COMBOUPV-PRHLT-COMBOREF ?
.11?
.11?CMU-HEAFIELD-COMBO .74?
?
.23?UPV-PRHLT-COMBO .77?
.38?
?> others .76 .24 .17>= others .89 .51 .43Table 36: Ranking scores for entries in the English-French task (system combination track).56REFCMU-HEAFIELD-COMBOKOC-COMBOUPV-PRHLT-COMBOREF ?
.01?
.01?
.01?CMU-HEAFIELD-COMBO .94?
?
.29?
.21?KOC-COMBO .96?
.48?
?
.41?UPV-PRHLT-COMBO .94?
.34?
.29?
?> others .95 .28 .20 .21>= others .99 .52 .38 .48Table 37: Ranking scores for entries in the Haitian Creole (Clean)-English task (system combination track).REFCMU-HEAFIELD-COMBOUPV-PRHLT-COMBOREF ?
.02?
.02?CMU-HEAFIELD-COMBO .83?
?
.24UPV-PRHLT-COMBO .86?
.29 ?> others .84 .16 .13>= others .98 .47 .43Table 38: Ranking scores for entries in the Haitian Creole (Raw)-English task (system combination track).57AMBERAMBER-NLAMBER-TIBLEUF15F15G3MTERATERMTERATER-PLUSROSETERTINE-SRL-MATCHMETEOR-1.3-ADQMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFCzech-English News TaskBBN-COMBO 0.24 0.24 0.25 0.29 0.31 0.19 ?9627 ?10667 1.97 0.53 0.49 0.61 0.34 ?65 44 0.48 0.03 0.51 43CMU-HEAFIELD-COMBO 0.24 0.24 0.24 0.28 0.3 0.18 ?9604 ?10933 1.97 0.54 0.5 0.60 0.33 ?65 43 0.48 0.03 0.52 42CST 0.19 0.19 0.2 0.16 0.21 0.10 ?27410 ?27880 1.94 0.64 0.40 0.5 0.28 ?65 34 0.38 0.02 0.42 33CU-BOJAR 0.21 0.21 0.22 0.19 0.24 0.13 ?23441 ?22289 1.95 0.64 0.44 0.55 0.30 ?65 37 0.42 0.02 0.46 36CU-ZEMAN 0.20 0.2 0.21 0.14 0.21 0.11 ?33520 ?30938 1.93 0.66 0.38 0.52 0.29 ?66 31 0.37 0.02 0.40 30JHU 0.22 0.21 0.22 0.2 0.25 0.13 ?21278 ?20480 1.95 0.60 0.43 0.55 0.30 ?65 37 0.42 0.02 0.46 36JHU-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?12563 ?12688 1.97 0.53 0.5 0.60 0.33 ?65 44 0.48 0.03 0.52 43ONLINE-B 0.24 0.23 0.24 0.29 0.31 0.19 ?10673 ?11506 1.97 0.52 0.50 0.60 0.33 ?65 44 0.49 0.03 0.52 43SYSTRAN 0.20 0.2 0.21 0.18 0.22 0.11 ?23996 ?24570 1.94 0.63 0.42 0.52 0.29 ?65 36 0.4 0.02 0.45 34UEDIN 0.22 0.22 0.23 0.22 0.26 0.14 ?14958 ?15342 1.96 0.59 0.45 0.57 0.31 ?65 40 0.44 0.03 0.48 39UPPSALA 0.21 0.20 0.21 0.20 0.23 0.12 ?22233 ?22509 1.95 0.62 0.43 0.53 0.29 ?65 37 0.41 0.02 0.46 36UPV-PRHLT-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?13904 ?15260 1.97 0.54 0.49 0.60 0.33 ?65 44 0.48 0.03 0.52 43Table 39: Automatic evaluation metric scores for systems in the WMT11 Czech-English News Task(newssyscombtest2011)AMBERAMBER-NLAMBER-TIBLEUF15F15G3MTERATERMTERATER-PLUSROSETERTINE-SRL-MATCHDFKI-PARSECONFMETEOR-1.3-ADQMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFGerman-English News TaskBBN-COMBO 0.23 0.22 0.23 0.25 0.28 0.16 ?17103 ?17837 1.97 0.56 0.46 0.06 0.59 0.32 ?43 42 0.46 0.03 0.49 41CMU-DYER 0.21 0.21 0.22 0.22 0.25 0.13 ?26089 ?29214 1.95 0.59 0.44 0.04 0.56 0.31 ?45 39 0.43 0.03 0.47 38CMU-HEAFIELD-COMBO 0.23 0.22 0.23 0.24 0.27 0.15 ?12868 ?16156 1.96 0.57 0.47 0.07 0.58 0.32 ?44 41 0.46 0.03 0.51 40CST 0.19 0.18 0.19 0.17 0.22 0.11 ?61131 ?60157 1.94 0.63 0.39 0.03 0.5 0.27 ?46 34 0.37 0.02 0.41 33CU-ZEMAN 0.2 0.19 0.20 0.14 0.22 0.11 ?64860 ?61329 1.93 0.65 0.37 0.06 0.51 0.28 ?47 31 0.37 0.02 0.4 30DFKI-XU 0.21 0.20 0.21 0.21 0.25 0.14 ?40171 ?39455 1.95 0.58 0.44 0.03 0.54 0.3 ?45 38 0.42 0.02 0.46 37JHU 0.19 0.19 0.2 0.17 0.22 0.11 ?62997 ?58673 1.94 0.64 0.39 0.03 0.51 0.28 ?45 34 0.38 0.02 0.41 33JHU-COMBO 0.22 0.22 0.23 0.24 0.27 0.15 ?30492 ?27016 1.96 0.57 0.46 0.04 0.57 0.31 ?44 41 0.45 0.03 0.48 39KIT 0.21 0.21 0.22 0.22 0.25 0.13 ?31064 ?31930 1.95 0.6 0.44 0.05 0.55 0.31 ?44 39 0.43 0.02 0.47 37KOC 0.2 0.2 0.20 0.18 0.23 0.12 ?52337 ?50231 1.94 0.63 0.41 0.05 0.52 0.29 ?45 35 0.39 0.02 0.43 34KOC-COMBO 0.21 0.21 0.21 0.22 0.26 0.14 ?40002 ?38374 1.96 0.59 0.44 0.03 0.54 0.3 ?44 38 0.42 0.02 0.46 37LIMSI 0.21 0.20 0.21 0.20 0.24 0.13 ?39419 ?38297 1.95 0.61 0.43 0.04 0.54 0.3 ?44 38 0.42 0.02 0.46 36LINGUATEC 0.19 0.19 0.2 0.16 0.22 0.11 ?26064 ?31116 1.94 0.68 0.42 0.15 0.53 0.29 ?46 35 0.42 0.02 0.47 34LIU 0.21 0.20 0.21 0.2 0.24 0.13 ?40281 ?40496 1.95 0.62 0.43 0.04 0.53 0.29 ?44 37 0.41 0.02 0.45 36ONLINE-A 0.22 0.21 0.22 0.21 0.26 0.14 ?25411 ?25675 1.95 0.6 0.45 0.06 0.57 0.31 ?44 39 0.45 0.03 0.48 38ONLINE-B 0.22 0.22 0.23 0.23 0.27 0.15 ?15149 ?19578 1.96 0.58 0.46 0.06 0.57 0.32 ?44 41 0.46 0.03 0.5 39QUAERO-COMBO 0.21 0.21 0.22 0.22 0.26 0.14 ?34486 ?33449 1.96 0.58 0.45 0.03 0.55 0.30 ?44 39 0.43 0.03 0.47 38RBMT-1 0.20 0.2 0.21 0.16 0.21 0.11 ?32960 ?34972 1.94 0.67 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.46 34RBMT-2 0.19 0.19 0.2 0.15 0.2 0.1 ?40842 ?43413 1.94 0.69 0.4 0.11 0.50 0.28 ?45 34 0.4 0.02 0.44 33RBMT-3 0.20 0.2 0.21 0.17 0.22 0.11 ?32476 ?33417 1.94 0.65 0.42 0.09 0.53 0.29 ?44 36 0.42 0.02 0.47 35RBMT-4 0.20 0.2 0.21 0.17 0.22 0.11 ?34287 ?34604 1.94 0.66 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.47 35RBMT-5 0.19 0.19 0.20 0.15 0.20 0.10 ?49097 ?46635 1.94 0.68 0.40 0.07 0.50 0.28 ?46 34 0.4 0.02 0.44 33RWTH-LEUSCH-COMBO 0.22 0.22 0.23 0.24 0.28 0.16 ?22878 ?22089 1.96 0.56 0.46 0.03 0.58 0.32 ?44 41 0.45 0.03 0.49 40RWTH-WUEBKER 0.21 0.20 0.21 0.21 0.24 0.13 ?35973 ?37140 1.95 0.60 0.44 0.04 0.54 0.3 ?45 38 0.42 0.02 0.45 37UEDIN 0.21 0.20 0.21 0.19 0.23 0.12 ?32791 ?34633 1.95 0.63 0.43 0.07 0.54 0.3 ?45 37 0.42 0.02 0.46 36UPPSALA 0.20 0.2 0.21 0.2 0.23 0.12 ?40448 ?41548 1.95 0.63 0.42 0.06 0.53 0.29 ?45 37 0.41 0.02 0.44 36UPV-PRHLT-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?33413 ?31778 1.96 0.58 0.45 0.03 0.57 0.31 ?44 40 0.44 0.03 0.48 39UZH-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?16326 ?20831 1.96 0.58 0.45 0.07 0.57 0.31 ?44 40 0.45 0.03 0.48 39Table 40: Automatic evaluation metric scores for systems in the WMT11 German-English News Task(newssyscombtest2011)58AMBERAMBER-NLAMBER-TIBLEUF15F15G3MTERATERMTERATER-PLUSROSETERTINE-SRL-MATCHMETEOR-1.3-ADQMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFFrench-English News TaskBBN-COMBO 0.25 0.25 0.26 0.31 0.32 0.21 ?19552 ?22107 1.98 0.48 0.51 0.64 0.36 ?43 47 0.49 0.03 0.54 46CMU-DENKOWSKI 0.24 0.24 0.24 0.26 0.29 0.17 ?34357 ?37807 1.97 0.53 0.48 0.61 0.34 ?45 43 0.46 0.03 0.50 42CMU-HANNEMAN 0.24 0.23 0.24 0.27 0.29 0.17 ?33662 ?37698 1.97 0.52 0.49 0.60 0.33 ?45 44 0.46 0.03 0.51 42CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.30 0.31 0.2 ?18365 ?22937 1.98 0.5 0.51 0.63 0.35 ?44 46 0.49 0.03 0.54 45CU-ZEMAN 0.22 0.22 0.23 0.17 0.24 0.13 ?67586 ?64688 1.94 0.6 0.41 0.56 0.31 ?47 34 0.39 0.02 0.42 33JHU 0.24 0.24 0.24 0.25 0.29 0.17 ?41567 ?39578 1.96 0.53 0.47 0.61 0.34 ?45 42 0.46 0.03 0.5 41JHU-COMBO 0.25 0.25 0.25 0.31 0.32 0.20 ?32785 ?31712 1.98 0.49 0.50 0.63 0.35 ?43 47 0.48 0.03 0.53 45KIT 0.25 0.24 0.25 0.29 0.31 0.19 ?22678 ?28283 1.98 0.51 0.50 0.63 0.35 ?44 46 0.49 0.03 0.53 44LIA-LIG 0.25 0.24 0.25 0.29 0.3 0.18 ?34063 ?34716 1.97 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.52 44LIMSI 0.25 0.24 0.25 0.28 0.29 0.18 ?26269 ?29363 1.97 0.52 0.5 0.62 0.34 ?44 45 0.48 0.03 0.52 44LIUM 0.25 0.24 0.25 0.29 0.30 0.19 ?29288 ?36137 1.98 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.53 44LIUM-COMBO 0.25 0.24 0.25 0.31 0.31 0.2 ?30678 ?35365 1.98 0.50 0.5 0.62 0.34 ?44 46 0.48 0.03 0.53 45ONLINE-A 0.25 0.24 0.25 0.27 0.3 0.18 ?38761 ?34096 1.97 0.52 0.49 0.62 0.34 ?44 44 0.48 0.03 0.52 43ONLINE-B 0.25 0.24 0.25 0.29 0.31 0.19 ?19157 ?25284 1.98 0.50 0.51 0.62 0.35 ?45 46 0.49 0.03 0.54 44RBMT-1 0.24 0.23 0.24 0.23 0.26 0.15 ?49115 ?39153 1.96 0.59 0.46 0.60 0.33 ?43 42 0.46 0.03 0.51 41RBMT-2 0.23 0.22 0.23 0.21 0.24 0.13 ?59549 ?50466 1.95 0.63 0.44 0.57 0.32 ?43 40 0.43 0.02 0.48 39RBMT-3 0.23 0.23 0.23 0.22 0.25 0.14 ?52047 ?45073 1.96 0.59 0.46 0.58 0.32 ?44 41 0.45 0.02 0.50 40RBMT-4 0.23 0.22 0.24 0.22 0.25 0.14 ?54507 ?42933 1.96 0.63 0.45 0.59 0.33 ?43 40 0.44 0.02 0.49 39RBMT-5 0.23 0.22 0.23 0.21 0.24 0.13 ?55545 ?48332 1.95 0.62 0.45 0.57 0.32 ?44 40 0.44 0.02 0.49 38RWTH-HUCK 0.24 0.24 0.25 0.28 0.3 0.18 ?44018 ?42549 1.97 0.52 0.49 0.61 0.34 ?44 44 0.47 0.03 0.51 43RWTH-LEUSCH-COMBO 0.26 0.25 0.26 0.31 0.32 0.20 ?21914 ?21746 1.98 0.49 0.51 0.64 0.35 ?43 47 0.50 0.03 0.54 46SYSTRAN 0.24 0.23 0.24 0.25 0.27 0.16 ?34321 ?40119 1.96 0.54 0.48 0.59 0.33 ?44 43 0.46 0.03 0.51 41UEDIN 0.23 0.23 0.24 0.25 0.27 0.16 ?47202 ?47955 1.96 0.56 0.47 0.59 0.33 ?45 42 0.45 0.03 0.49 40UPV-PRHLT-COMBO 0.25 0.25 0.26 0.31 0.32 0.20 ?26947 ?28689 1.98 0.5 0.51 0.63 0.35 ?43 47 0.49 0.03 0.54 46Table 41: Automatic evaluation metric scores for systems in the WMT11 French-English News Task(newssyscombtest2011)AMBERAMBER-NLAMBER-TIBLEUF15F15G3MTERATERMTERATER-PLUSROSETERTINE-SRL-MATCHMETEOR-1.3-ADQMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFSpanish-English News TaskALACANT 0.24 0.23 0.24 0.27 0.28 0.17 ?30135 ?29622 1.97 0.53 0.46 0.61 0.34 ?45 43 0.46 0.03 0.50 42BBN-COMBO 0.25 0.25 0.25 0.32 0.33 0.21 ?15284 ?16192 1.98 0.48 0.5 0.64 0.35 ?44 47 0.49 0.03 0.53 46CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.32 0.31 0.20 ?13456 ?16113 1.98 0.5 0.5 0.64 0.35 ?44 47 0.5 0.03 0.54 46CU-ZEMAN 0.20 0.20 0.21 0.16 0.22 0.12 ?49428 ?48440 1.93 0.61 0.36 0.51 0.28 ?49 32 0.35 0.02 0.38 31HYDERABAD 0.20 0.20 0.21 0.17 0.21 0.11 ?47754 ?47059 1.94 0.61 0.39 0.50 0.28 ?47 34 0.36 0.02 0.41 33JHU-COMBO 0.25 0.25 0.25 0.32 0.32 0.20 ?23939 ?22685 1.98 0.49 0.49 0.63 0.35 ?44 47 0.48 0.03 0.52 46KOC 0.24 0.24 0.24 0.26 0.29 0.17 ?22724 ?25857 1.96 0.53 0.46 0.61 0.34 ?45 42 0.46 0.03 0.49 41KOC-COMBO 0.25 0.24 0.25 0.28 0.30 0.19 ?22678 ?22267 1.97 0.52 0.48 0.62 0.34 ?44 44 0.48 0.03 0.52 43ONLINE-A 0.25 0.24 0.25 0.28 0.3 0.18 ?19017 ?20120 1.97 0.52 0.48 0.63 0.35 ?44 45 0.48 0.03 0.52 43ONLINE-B 0.24 0.24 0.24 0.29 0.30 0.19 ?11980 ?18589 1.97 0.50 0.49 0.62 0.34 ?45 45 0.49 0.03 0.53 44RBMT-1 0.24 0.24 0.25 0.28 0.28 0.17 ?31202 ?26151 1.97 0.57 0.46 0.61 0.34 ?44 45 0.47 0.03 0.51 43RBMT-2 0.23 0.23 0.24 0.24 0.25 0.15 ?35157 ?31405 1.96 0.6 0.44 0.59 0.33 ?44 42 0.44 0.02 0.49 41RBMT-3 0.23 0.23 0.24 0.25 0.26 0.15 ?28289 ?26082 1.97 0.59 0.45 0.6 0.33 ?43 43 0.46 0.03 0.51 42RBMT-4 0.24 0.23 0.24 0.25 0.26 0.16 ?27892 ?25546 1.97 0.59 0.46 0.60 0.33 ?43 43 0.46 0.03 0.52 42RBMT-5 0.24 0.23 0.24 0.27 0.26 0.16 ?36770 ?31613 1.96 0.58 0.45 0.6 0.33 ?45 43 0.45 0.03 0.50 42RWTH-LEUSCH-COMBO 0.25 0.25 0.26 0.32 0.32 0.21 ?15172 ?15261 1.98 0.49 0.5 0.64 0.35 ?43 48 0.50 0.03 0.54 47SYSTRAN 0.24 0.23 0.24 0.27 0.28 0.17 ?20129 ?26051 1.97 0.53 0.47 0.60 0.33 ?46 44 0.46 0.03 0.51 42UEDIN 0.22 0.22 0.23 0.22 0.25 0.14 ?25462 ?31678 1.96 0.58 0.45 0.57 0.32 ?47 40 0.44 0.03 0.48 39UFAL-UM 0.23 0.22 0.23 0.23 0.24 0.14 ?42123 ?37765 1.96 0.60 0.43 0.58 0.32 ?43 41 0.43 0.02 0.48 40UPM 0.22 0.22 0.23 0.22 0.24 0.14 ?39748 ?38433 1.95 0.58 0.43 0.57 0.32 ?45 40 0.42 0.02 0.46 38UPV-PRHLT-COMBO 0.25 0.25 0.26 0.32 0.32 0.20 ?16094 ?17723 1.98 0.50 0.49 0.64 0.35 ?43 47 0.5 0.03 0.54 46Table 42: Automatic evaluation metric scores for systems in the WMT11 Spanish-English News Task(newssyscombtest2011)59AMBERAMBER-NLAMBER-TIBLEUF15F15G3ROSETERMETEOR-1.3-RANKMP4IBM1MPFWMPFEnglish-Czech News TaskCMU-HEAFIELD-COMBO 0.2 0.19 0.20 0.19 0.22 0.12 2.03 0.62 0.24 ?62 29 27COMMERCIAL1 0.16 0.15 0.16 0.11 0.16 0.08 2.01 0.70 0.19 ?65 22 21COMMERCIAL2 0.12 0.10 0.13 0.09 0.15 0.06 2.00 0.73 0.18 ?65 21 19CU-BOJAR 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.23 ?63 26 24CU-MARECEK 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.22 ?63 26 24CU-POPEL 0.17 0.16 0.18 0.14 0.19 0.1 2.02 0.66 0.21 ?64 25 23CU-TAMCHYNA 0.18 0.17 0.18 0.15 0.2 0.1 2.02 0.65 0.22 ?63 26 24CU-ZEMAN 0.17 0.16 0.17 0.13 0.18 0.09 2.02 0.66 0.21 ?63 23 22JHU 0.18 0.18 0.18 0.16 0.21 0.11 2.02 0.63 0.22 ?63 26 24ONLINE-B 0.2 0.19 0.20 0.2 0.22 0.12 2.03 0.62 0.24 ?63 29 27UEDIN 0.19 0.18 0.19 0.17 0.21 0.11 2.03 0.63 0.23 ?63 27 26UPV-PRHLT-COMBO 0.2 0.19 0.20 0.20 0.23 0.13 2.03 0.61 0.24 ?63 29 28Table 43: Automatic evaluation metric scores for systems in the WMT11 English-Czech News Task(newssyscombtest2011)AMBERAMBER-NLAMBER-TIBLEUF15F15G3ROSETERMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFEnglish-German News TaskCMU-HEAFIELD-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.39 ?46 36 0.41 0.03 0.45 35COPENHAGEN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 33 0.38 0.02 0.42 32CU-TAMCHYNA 0.17 0.17 0.18 0.11 0.18 0.09 1.94 0.70 0.36 ?48 31 0.36 0.02 0.4 30CU-ZEMAN 0.16 0.15 0.16 0.05 0.17 0.08 1.92 0.71 0.34 ?51 25 0.31 0.02 0.34 25DFKI-FEDERMANN 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32DFKI-XU 0.18 0.17 0.18 0.15 0.19 0.1 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34ILLC-UVA 0.15 0.14 0.15 0.12 0.18 0.08 1.95 0.68 0.33 ?49 32 0.36 0.02 0.4 31JHU 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32KIT 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34KOC 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.69 0.35 ?47 32 0.36 0.02 0.40 31KOC-COMBO 0.18 0.17 0.18 0.15 0.2 0.1 1.95 0.67 0.37 ?47 34 0.38 0.02 0.42 33LIMSI 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.36 ?47 35 0.39 0.03 0.44 33LIU 0.17 0.17 0.18 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.38 0.02 0.43 33ONLINE-A 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.37 ?47 35 0.40 0.03 0.45 33ONLINE-B 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.65 0.38 ?46 36 0.42 0.03 0.46 35RBMT-1 0.17 0.17 0.18 0.13 0.18 0.08 1.95 0.7 0.35 ?46 34 0.39 0.03 0.45 33RBMT-2 0.16 0.16 0.17 0.12 0.16 0.08 1.94 0.73 0.33 ?47 32 0.37 0.03 0.43 31RBMT-3 0.18 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?46 35 0.39 0.03 0.46 34RBMT-4 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.70 0.34 ?47 33 0.38 0.03 0.45 32RBMT-5 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32RWTH-FREITAG 0.17 0.17 0.17 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.37 0.02 0.41 33UEDIN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 34 0.38 0.02 0.42 33UOW 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.7 0.35 ?47 33 0.37 0.02 0.42 32UPPSALA 0.17 0.16 0.17 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32UPV-PRHLT-COMBO 0.18 0.18 0.19 0.17 0.20 0.10 1.96 0.66 0.38 ?46 36 0.4 0.03 0.44 35UZH-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.38 ?46 36 0.40 0.03 0.44 35Table 44: Automatic evaluation metric scores for systems in the WMT11 English-German News Task(newssyscombtest2011)60AMBERAMBER-NLAMBER-TIBLEUF15F15G3ROSETERMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFEnglish-French News TaskCMU-HEAFIELD-COMBO 0.25 0.25 0.26 0.34 0.35 0.23 2.02 0.5 0.57 ?41 52 0.54 ?0.01 0.60 50CU-ZEMAN 0.18 0.17 0.18 0.13 0.19 0.09 1.96 0.68 0.39 ?46 35 0.34 ?0.03 0.40 33JHU 0.23 0.23 0.24 0.27 0.31 0.19 2.01 0.53 0.52 ?43 47 0.49 ?0.01 0.55 45KIT 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.52 0.53 ?42 49 0.51 ?0.01 0.57 47LATL-GENEVA 0.20 0.2 0.21 0.19 0.23 0.12 1.99 0.62 0.44 ?43 41 0.44 ?0.02 0.51 39LIMSI 0.24 0.24 0.24 0.3 0.31 0.19 2.01 0.53 0.53 ?41 49 0.51 ?0.01 0.58 48LIUM 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.53 0.53 ?42 49 0.51 ?0.01 0.57 47ONLINE-A 0.24 0.23 0.24 0.27 0.3 0.18 2.01 0.53 0.52 ?42 47 0.5 ?0.01 0.56 46ONLINE-B 0.25 0.25 0.25 0.33 0.35 0.23 2.02 0.5 0.56 ?42 51 0.53 ?0.01 0.59 50RBMT-1 0.23 0.22 0.23 0.24 0.27 0.16 2.00 0.56 0.5 ?41 45 0.48 ?0.02 0.56 44RBMT-2 0.22 0.21 0.22 0.22 0.25 0.14 1.99 0.58 0.47 ?42 44 0.46 ?0.02 0.53 42RBMT-3 0.23 0.22 0.23 0.25 0.28 0.16 2.00 0.56 0.5 ?41 46 0.48 ?0.02 0.56 44RBMT-4 0.22 0.21 0.22 0.23 0.26 0.15 1.99 0.58 0.47 ?42 43 0.45 ?0.02 0.51 42RBMT-5 0.22 0.22 0.23 0.23 0.27 0.15 2 0.57 0.49 ?41 45 0.47 ?0.02 0.55 43RWTH-HUCK 0.23 0.23 0.24 0.29 0.30 0.18 2.01 0.54 0.52 ?42 48 0.5 ?0.01 0.56 47UEDIN 0.23 0.22 0.23 0.27 0.3 0.18 2.01 0.54 0.51 ?42 47 0.49 ?0.01 0.55 46UPPSALA 0.23 0.22 0.23 0.27 0.29 0.17 2.00 0.55 0.51 ?42 46 0.48 ?0.01 0.55 45UPPSALA-FBK 0.23 0.23 0.23 0.28 0.29 0.18 2.01 0.55 0.51 ?42 47 0.49 ?0.01 0.55 46UPV-PRHLT-COMBO 0.25 0.24 0.25 0.32 0.34 0.22 2.02 0.50 0.55 ?41 51 0.53 ?0.01 0.59 49Table 45: Automatic evaluation metric scores for systems in the WMT11 English-French News Task(newssyscombtest2011)AMBERAMBER-NLAMBER-TIBLEUF15F15G3ROSETERMETEOR-1.3-RANKMP4IBM1MPFTESLA-BTESLA-FTESLA-MWMPFEnglish-Spanish News TaskCEU-UPV 0.24 0.24 0.24 0.29 0.3 0.18 2.01 0.51 0.55 ?45 46 0.45 0.01 0.45 45CMU-HEAFIELD-COMBO 0.26 0.25 0.26 0.35 0.34 0.22 2.02 0.47 0.58 ?44 50 0.49 0.01 0.49 49CU-ZEMAN 0.23 0.22 0.23 0.22 0.27 0.15 1.99 0.55 0.52 ?48 39 0.41 0.00 0.41 38KOC 0.23 0.23 0.23 0.25 0.27 0.16 2 0.54 0.52 ?46 43 0.42 0.00 0.43 42KOC-COMBO 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.5 0.56 ?44 47 0.46 0.01 0.47 46ONLINE-A 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.49 0.56 ?44 48 0.46 0.01 0.47 46ONLINE-B 0.25 0.25 0.25 0.33 0.32 0.2 2.02 0.50 0.57 ?44 49 0.47 0.01 0.47 48PROMT 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?45 45 0.44 0.01 0.46 43RBMT-1 0.23 0.23 0.23 0.25 0.27 0.16 2 0.55 0.51 ?45 43 0.42 0.00 0.44 42RBMT-2 0.23 0.22 0.23 0.25 0.26 0.15 1.99 0.55 0.5 ?44 43 0.41 0.00 0.42 41RBMT-3 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?44 45 0.43 0.00 0.45 43RBMT-4 0.23 0.22 0.23 0.26 0.26 0.16 1.99 0.54 0.51 ?44 44 0.42 0.00 0.43 42RBMT-5 0.23 0.22 0.23 0.24 0.26 0.15 1.99 0.57 0.49 ?45 42 0.41 0.00 0.43 41UEDIN 0.24 0.24 0.24 0.31 0.3 0.18 2.01 0.51 0.55 ?45 47 0.45 0.01 0.45 46UOW 0.23 0.23 0.24 0.28 0.28 0.16 2.00 0.53 0.53 ?45 45 0.42 0.01 0.43 44UOW-COMBO 0.25 0.25 0.25 0.33 0.32 0.2 2.01 0.50 0.56 ?44 49 0.47 0.01 0.47 47UPM 0.21 0.21 0.21 0.21 0.22 0.12 1.98 0.61 0.47 ?47 39 0.37 0.00 0.37 38UPPSALA 0.24 0.24 0.24 0.3 0.29 0.18 2.01 0.51 0.54 ?45 46 0.44 0.01 0.44 45UPV-PRHLT-COMBO 0.25 0.25 0.25 0.33 0.32 0.21 2.02 0.49 0.57 ?44 49 0.47 0.01 0.48 48Table 46: Automatic evaluation metric scores for systems in the WMT11 English-Spanish News Task(newssyscombtest2011)61BLEUMTERATERMTERATER-PLUSROSETERMETEOR-1.3-ADQMETEOR-1.3-RANKMPFTESLA-BTESLA-FTESLA-MWMPFHaitian Creole (clean)-English Haitian Creole SMS Emergency Response Featured Translation TaskBM-I2R 0.33 ?6798 ?4575 1.96 0.51 0.62 0.34 43 0.44 0.03 0.46 43CMU-DENKOWSKI 0.29 ?6849 ?6172 1.95 0.53 0.58 0.32 40 0.39 0.02 0.40 39CMU-HEAFIELD-COMBO 0.32 ?6188 ?4347 1.96 0.51 0.61 0.34 42 0.43 0.03 0.45 42CMU-HEWAVITHARANA 0.28 ?6523 ?6341 1.95 0.57 0.57 0.32 39 0.38 0.02 0.40 38HYDERABAD 0.14 ?7548 ?8502 1.92 0.66 0.50 0.28 26 0.3 0.02 0.30 26KOC 0.23 ?6490 ?9020 1.94 0.67 0.49 0.27 36 0.32 0.02 0.34 35KOC-COMBO 0.29 ?4901 ?5349 1.95 0.57 0.56 0.31 39 0.38 0.02 0.4 39LIU 0.27 ?6526 ?6078 1.95 0.59 0.56 0.31 38 0.38 0.02 0.39 37UMD-EIDELMAN 0.26 ?4407 ?6215 1.95 0.57 0.55 0.31 38 0.37 0.02 0.4 37UMD-HU 0.22 ?6379 ?7460 1.94 0.59 0.51 0.28 35 0.36 0.02 0.39 34UPPSALA 0.27 ?5497 ?6754 1.95 0.59 0.54 0.3 38 0.36 0.02 0.39 37UPV-PRHLT-COMBO 0.32 ?6896 ?5968 1.96 0.53 0.6 0.33 42 0.41 0.02 0.43 41Table 47: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (clean)-English HaitianCreole SMS Emergency Response Featured Translation Task (newssyscombtest2011)BLEUMTERATERMTERATER-PLUSROSETERMETEOR-1.3-ADQMETEOR-1.3-RANKMPFTESLA-BTESLA-FTESLA-MWMPFHaitian Creole (raw)-English Haitian Creole SMS Emergency Response Featured Translation TaskBM-I2R 0.29 ?3885 ?3017 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38CMU-DENKOWSKI 0.25 ?3965 ?3905 1.95 0.60 0.53 0.3 35 0.38 0.02 0.4 35CMU-HEAFIELD-COMBO 0.28 ?3057 ?2588 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38CMU-HEWAVITHARANA 0.25 ?3701 ?3824 1.95 0.61 0.53 0.3 35 0.37 0.02 0.39 35JHU 0.14 ?3207 ?4279 1.92 0.74 0.43 0.24 26 0.30 0.02 0.32 26LIU 0.25 ?3447 ?3445 1.95 0.60 0.54 0.30 36 0.38 0.02 0.4 35UMD-EIDELMAN 0.24 ?2826 ?3754 1.94 0.64 0.52 0.29 34 0.36 0.02 0.39 34UPV-PRHLT-COMBO 0.28 ?3591 ?3370 1.95 0.58 0.56 0.32 38 0.4 0.02 0.42 38Table 48: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (raw)-English Haitian CreoleSMS Emergency Response Featured Translation Task (newssyscombtest2011)62INTER-ANNOTATOR AGREEMENT (I.E.
ACROSS ANNOTATORS)ALL COMPARISONS NO REF COMPARISONSP (A) P (E) ?
P (A) P (E) ?Czech-English, individual systems 0.591 0.354 0.367 0.535 0.343 0.293English-Czech, individual systems 0.608 0.359 0.388 0.552 0.350 0.312German-English, individual systems 0.562 0.377 0.298 0.536 0.370 0.264English-German, individual systems 0.564 0.352 0.327 0.528 0.348 0.276Spanish-English, individual systems 0.695 0.398 0.493 0.683 0.393 0.477English-Spanish, individual systems 0.574 0.343 0.352 0.548 0.339 0.317French-English, individual systems 0.616 0.367 0.393 0.584 0.361 0.349English-French, individual systems 0.631 0.382 0.403 0.603 0.376 0.363European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320Czech-English, system combinations 0.700 0.334 0.549 0.577 0.369 0.329English-Czech, system combinations 0.812 0.348 0.711 0.696 0.392 0.500German-English, system combinations 0.675 0.353 0.498 0.629 0.341 0.437English-German, system combinations 0.608 0.346 0.401 0.547 0.334 0.320Spanish-English, system combinations 0.638 0.335 0.456 0.604 0.359 0.382English-Spanish, system combinations 0.657 0.335 0.485 0.603 0.371 0.369French-English, system combinations 0.654 0.336 0.479 0.608 0.336 0.410English-French, system combinations 0.678 0.352 0.503 0.595 0.339 0.388European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389Haitian (Clean)-English, individual systems 0.693 0.364 0.517 0.640 0.353 0.443Haitian (Raw)-English, individual systems 0.689 0.357 0.517 0.639 0.344 0.450Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446Haitian (Clean)-English, system combinations 0.770 0.367 0.636 0.645 0.333 0.468Haitian (Raw)-English, system combinations 0.745 0.345 0.611 0.753 0.361 0.613Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437WMT10 (European languages, individual vs. individual) 0.663 0.394 0.445 0.620 0.385 0.382WMT10 (European languages, combo vs. combo) 0.728 0.344 0.586 0.629 0.334 0.443WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.634 0.360 0.428WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409Table 49: Inter-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down bylanguage pair.
The highlighted rows correspond to rows in the top half of Table 7.
See Table 50 below for detailedintra-annotator agreement rates.63INTRA-ANNOTATOR AGREEMENT (I.E.
SELF-CONSISTENCY)ALL COMPARISONS NO REF COMPARISONSP (A) P (E) ?
P (A) P (E) ?Czech-English, individual systems 0.762 0.354 0.632 0.713 0.343 0.564English-Czech, individual systems 0.743 0.359 0.598 0.700 0.350 0.539German-English, individual systems 0.675 0.377 0.478 0.670 0.370 0.475English-German, individual systems 0.704 0.352 0.543 0.700 0.348 0.541Spanish-English, individual systems 0.750 0.398 0.585 0.719 0.393 0.537English-Spanish, individual systems 0.644 0.343 0.458 0.601 0.339 0.396French-English, individual systems 0.829 0.367 0.730 0.816 0.361 0.712English-French, individual systems 0.716 0.382 0.541 0.681 0.376 0.488European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512Czech-English, system combinations 0.756 0.334 0.633 0.657 0.369 0.457English-Czech, system combinations 0.923 0.348 0.882 0.842 0.392 0.740German-English, system combinations 0.732 0.353 0.586 0.716 0.341 0.569English-German, system combinations 0.722 0.346 0.575 0.676 0.334 0.513Spanish-English, system combinations 0.783 0.335 0.673 0.720 0.359 0.562English-Spanish, system combinations 0.741 0.335 0.610 0.711 0.371 0.540French-English, system combinations 0.772 0.336 0.657 0.659 0.336 0.487English-French, system combinations 0.841 0.352 0.755 0.714 0.339 0.568European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571Haitian (Clean)-English, individual systems 0.758 0.364 0.619 0.686 0.353 0.515Haitian (Raw)-English, individual systems 0.783 0.357 0.663 0.756 0.344 0.628Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539Haitian (Clean)-English, system combinations 0.882 0.367 0.813 0.778 0.333 0.667Haitian (Raw)-English, system combinations 0.882 0.345 0.820 0.802 0.361 0.690Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774WMT10 (European languages, individual vs. individual) 0.757 0.394 0.599 0.728 0.385 0.557WMT10 (European languages, combo vs. combo) 0.783 0.344 0.670 0.719 0.334 0.578WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.746 0.360 0.603WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580Table 50: Intra-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down bylanguage pair.
The highlighted rows correspond to rows in the bottom half of Table 7.
See Table 49 above for detailedinter-annotator agreement rates.64
