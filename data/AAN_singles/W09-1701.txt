Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1?9,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAcquiring Applicable Common Sense Knowledge from the WebHansen A. Schwartz and Fernando GomezSchool of Electrical Engineering and Computer ScienceUniversity of Central FloridaOrlando, FL 32816, USA{hschwartz, gomez}@cs.ucf.eduAbstractIn this paper, a framework for acquiring com-mon sense knowledge from the Web is pre-sented.
Common sense knowledge includesinformation about the world that humans usein their everyday lives.
To acquire this knowl-edge, relationships between nouns are re-trieved by using search phrases with automat-ically filled constituents.
Through empiricalanalysis of the acquired nouns over Word-Net, probabilities are produced for relation-ships between a concept and a word ratherthan between two words.
A specific goal ofour acquisition method is to acquire knowl-edge that can be successfully applied to NLPproblems.
We test the validity of the acquiredknowledge by means of an application to theproblem of word sense disambiguation.
Re-sults show that the knowledge can be used toimprove the accuracy of a state of the art un-supervised disambiguation system.1 IntroductionCommon sense knowledge (CSK) is the knowledgewe use in everyday life without necessarily beingaware of it.
Panton et al (2006) of the Cyc project,define common sense as ?the knowledge that everyperson assumes his neighbors also possess?.
Al-though the term common sense may be understoodas a process such as reasoning, we are referring onlyto knowledge.
It is CSK that tells us keys are kept inone?s pocket and keys are used to open a door, butCSK does not hold that keys are kept in a kitchensink or that keys are used to turn on a microwave,although all are possible.To show the need for this information moreclearly we provide a couple sentences:She put the batter in the refrigerator.
(1)He ate the apple in the refrigerator.
(2)In (1), we are dealing with lexical ambiguity.
Thereis little doubt for us to determine just what the ?bat-ter?
is (food/substance used in baking).
However, acomputer must determine that it is not someone whoswings a bat in baseball that is being put into a re-frigerator, although it is entirely possible to do (de-pending on the size of the refrigerator).
This demon-strates how CSK can be useful in solving word sensedisambiguation.
We know it is common for food tobe found in a refrigerator and so we easily resolvebatter as a food/substance rather than a person.CSK can also help to solve syntactic ambiguity.The problem of prepositional phrase attachment oc-curs in sentences similar to (2).
In this case, it isdifficult for a computer to determine if ?he?
is in therefrigerator eating an apple or if the ?apple?
whichhe ate was in the refrigerator.
Like the previous ex-ample, the knowledge that food is commonly foundin a refrigerator and people are not, leads us to un-derstand that ?in the refrigerator?
should be attachedto the noun phrase ?the apple?
and not as a modifierof the verb phrase ?ate?.Unfortunately, there are not many sources of CSKreadily available for use in computer algorithms.Those sets of knowledge that are available, suchas the CYC project (Lenat, 1995) or ConceptNet(Liu and Singh, 2004) rely on manually providedor crafted data.
Our aim is to develop an auto-matic approach to acquire CSK1 by turning to thevast amount of unannotated text that is available onthe Web.
In turn, we present a method to automat-ically retrieve and analyze phrases from the Web.1data available at: http://eecs.ucf.edu/?hschwartz/CSK/1We employ the use of a syntactic parser to accu-rately match syntactic patterns of phrases acquiredfrom the Web.
The data is analyzed over WordNet(Miller et al, 1993) in order to induce knowledgeabout word senses or concepts rather than words.
Fi-nally, we evaluate whether the knowledge by apply-ing it to the problem of word sense disambiguation.2 BackgroundThe particular type of CSK that we experiment within this paper is described formally as follows:A relationship, e1Re2, exists between entitiese1 and e2 if one finds ?e1 is R e2.
?Some examples include: ?a cup is on a table?
and?food is in a refrigerator?, which would result in re-lationships: cupontable and foodinrefrigerator.
Thenext section attempts to make the relationship moreclear, as we provide a brief linguistic background ofprepositions and relationships.2.1 Prepositions and RelationshipsPrepositions state a relationship between two enti-ties (Quirk et al, 1985).
One of the entities is typ-ically a constituent of the sentence while the otheris the complement to the preposition.
For exam-ple, consider the relationship between ?furniture?and ?house?
in the following sentences:The furniture is......at the house....on the house....in the house.
?The furniture?
is the subject of the sentence, while?the house?
is a prepositional complement.
Noticethat the meaning is different for each sentence de-pending on the actual preposition (?at?, ?on?, or ?in?
),and thus furniture relates to house in three differentways.
Although each relationship between furnitureand house is possible, only one would be consideredCSK to most people: furnitureinhouse.We focus on prepositions which indicate a posi-tive spacial relationship given by Quirk et al (1985).There are three types of such relationships: ?at apoint?, ?on a line or surface?, and ?in an area or vol-ume?.
In particular, we concentrate on the 1 to 3dimensional relationships given in Table 1, denotedon and in throughout the paper.
At, the 0 dimen-sional relationship, occurred far less frequently.
Thedims description prepositions1 or 2 on surface or line on, onto, atop, upon,on top of, down on2 or 3 in area or volume in, into, inside,within, inside ofTable 1: Spatial dimensions (dims) and correspondingprepositions.sentences below exemplify each of the 1 to 3 dimen-sional relationships:on surface The keyboard is on the table.on line The beach is on US 1.in area The bank is in New York.in volume The vegetables are in the bowl.2.2 Related WorkAs a prevalent source of lexical knowledge, dictio-nary definitions may be regarded as common sense.However, some definitions may be considered expertknowledge rather than CSK.
The scope of definitionscertainly do not provide all necessary information(such as keys are commonly kept in one?s pocket).We examine WordNet in particular because the hy-pernym relation has been developed extensively fornouns.
The noun ontology is used in our work tohelp induce relationships involving concepts (sensesof nouns) rather than just among words.
This notionof inducing CSK among concepts, rather than words,is a key difference between our work and similar re-search.The work on VerbOcean is similar to our researchin the use of the Web for acquiring relationships(Chklovski and Pantel, 2004).
They used patternsof phrases in order to search the Web for semanticrelations among verbs.
The knowledge they acquirefalls into the category of CSK, but the specific re-lationships are different than ours in that they areamong verb word forms and senses are not resolved.ConceptNet was created based on the OpenMindCommonsense project (Liu and Singh, 2004).
Theproject acquired knowledge through an interface onthe Web by having users play games and answerquestions about words.
A contribution of Concept-Net is that it has a wide range of relations.
WhileWordNet provides connections between concepts(senses of words), ConceptNet only provides rela-tionships between word forms.2Concept AnalysisNoun Acquisitionweb searchparse and matchWordprobabilities:nounA[in|on]nounBWordNetOntologydetermineconceptprobabilitiesconceptprobabilities:conceptA[in|on]nounBa chosennounBcreateweb queriessearchphrasesfor CSKFigure 1: The overall common sense knowledge acquisition framework under the assumption that one is acquiringconcepts (WordNet synsets) in a relationship with a given nounB (word).A project in progress for over twenty years, CYChas been acquiring common sense knowledge abouteveryday objects and actions stored in 106 axioms(Lenat, 1995).
The axioms, handcrafted by workersat CYCcorp, represent knowledge rooted in propo-sitions.
There are three layers of information: thefirst two, access and physical, contain meta data,while the third, logical layer, stores high level im-plicit meanings.
Only a portion of CYC is availableto the public.Our method for acquiring knowledge is somewhatsimilar to that of (Hearst, 1992).
Patterns are builtmanually.
However, we do not use our manuallyconstructed patterns (referred to as search phrases)to query the Web.
Instead the search phrases are ab-stract patterns that are used to automatically gener-ate more specific web queries by filling constituentsbased on lists of words.The SemEval-2007 Task 4 presents a goodoverview of work in noun-noun relationships (Girjuet al, 2007).
Our work is related in that the rela-tionships we acquire are between nominals, and inorder to build their corpus Girju et al queried theweb with patterns like that of Hearst?s work (Hearst,1992).
The SemEval task was to choose or clas-sify relationships, rather than acquire and apply rela-tionships.
Additionally, the relationship classes theyuse are not necessarily within the scope of commonsense knowledge.Similar to our research, in (Agirre et al, 2001)knowledge is acquired about WordNet concepts.They find topics signatures, sets of related words,based on data from the Web and use them for wordsense disambiguation.
However, the type of rela-tionship between words of a topic signature and theWordNet concept is not made explicit, and the au-thors find the topic signatures are not very effectivefor word sense disambiguation.Finally, we note one approach to using the Webfor NLP applications is to acquire knowledge on thefly.
Previous work has approached solutions to wordsense disambiguation by acquiring words or phrasesdirectly based on the sentences or words being dis-ambiguated (Martinez et al, 2006; Schwartz andGomez, 2008).
These methods dynamically acquirethe data at runtime, rather than automatically createa common sense database of relations that is readilyavailable.
Additionally, in our current approach, weare able to acquire explicit CSK relationships.3 Common Sense AcquisitionThe two major phases of our framework, ?Noun Ac-quisition?
and ?Concept Analysis?, are outlined inFigure 1 and described within this section.3.1 Noun AcquisitionThe first step of our method is to acquire nouns(as words) from the Web which are in a relation-ship with other nouns.
A Web search is performedin order to retrieve samples of text matching a webquery created from a search phrase for the relation-ship.
Each sample is syntactically parsed to verifya match with the corresponding web query, and thenoun(s) filling a missing constituent of the parse arerecorded.The framework itself is very flexible, and it canhandle the acquisition of words from other parts ofspeech.
However, to be clear, we focus the explana-tion on the use of the framework to acquire specifictypes of relationships between nouns.
Below we de-scribe the procedures in more detail.33.1.1 Creating Web QueriesWeb queries are created semi-automatically bydefining these parameters of a search phrase:nounA the first noun phrasenounB the second noun phraseprep preposition, if any, used in the phraseverb verb, if any, used in the phrase.Table 2 lists all of the search phrases we use, one ofwhich we use as an example throughout this section:place nounA prep nounBThe verb, ?place?
in this case, is statically defined aspart of the search phrase.Prepositions were chosen to describe the type ofrelationship we were seeking to acquire as describedin the background section.
We limited ourselves tothe ?on?
and ?in?
relationships since these were themost common.on = (on, onto, atop, upon, on top of, down on)in = (in, into, inside, within, inside of )When noun parameters are provided, determinersor possessive pronouns selected from the list beloware included.
This provides greater accuracy in oursearch results.det = (the, a/an, this, that, my, your, his, her)Finally, the undefined parameters are replacedwith a ?*?.
Below is a web query created from oursearch phrase where nounB is ?refrigerator?, prep is?in?, det is ?the?, and nounA is undefined:place * in the refrigerator3.1.2 Searching the WebGiven a nounB, The search algorithm can be sum-marized through the pseudocode below.for each search phrasefor each prepfor each detquery = create query(search phrase,prep, det, nounB));samples = websearch(query);The searches were carried out through the GoogleSearch API2, or the Yahoo!
Search Web Services3.Each search phrase, listed in Table 2, was run untila maximum of 2000 results were returned.
Dupli-cate samples were removed to reduce the effects ofwebsites replicating the text of one another.2no longer supported by Google3http://developer.yahoo.com/search/relation search phrase voicenounA is located prep nounBon, in nounA is found prep nounB passivenounA is situated prep nounBnounA is prep nounBput nounA prep nounBplace nounA prep nounBon, in lay nounA prep nounB activeset nounA prep nounBlocate nounA prep nounBposition nounA prep nounBhang nounA prep nounBon mount nounA prep nounB activeattach nounA prep nounBTable 2: Search phrases and relationships used for acqui-sition of CSK.3.1.3 Parse and MatchThe results we want to achieve in this step shoulddescribe a relationship:nounA is [in | on] nounBWe use Charniak?s parser (Charniak, 2000) on boththe web query and the results returned from the webin order to ensure accuracy.
To demonstrate this pro-cess, we extend our example, ?place * in the refrig-erator?.First, we get a parse with * (nounA) representedas ?something?.
(VP (VB place)(NP (NN something))(PP (IN in) (NP (DT the) (NN refrigerator))))We now know the constituent(s) which replace ?(NNsomething)?
will be our nounA.
For example, in thefollowing parse ?batter?
is resolved as nounA.
(S1 (S (NP (PRP He))(VP (AUX was) (VP (VBN told) (S (VP (TO to)(VP (VB place)(NP (DT the) (JJ mixed) (NN batter))(PP (IN in) (NP (DT the) (NN refrigerator))))]The head noun of the matching phrase is determined,which is ?batter?
in the phrase ?
(DT the) (JJ mixed)(NN batter)?.
Words are only recorded if they arepresent as a noun in WordNet.
If the noun phrasecontains a compound noun found in WordNet, thenthe compound noun is recorded instead.The parse also helps to eliminate bad results.
Forthe following sentence, the verb phrase does not4match the parse of the web query due to an extra PP,and therefore we do not pull out ?for several hours?as nounA.
(S1 (S (VP (VP (VB Mix)(NP (DT the) (NN sugar))(PRT (RP in))(PP (TO to) (NP (DT the) (NN dough))))(CC and)(VP (VB place)(PP (IN for) (NP (JJ several) (NNS hours)))(PP (IN in) (NP (DT the) (NN refrigerator)))))))One may note that this malformed sentence is com-municating that ?dough?
is placed in the refrigerator,but the method does not handle this.At the end of the noun acquisition phase, we areleft with frequency counts of nouns being retrievedfrom a context matching the syntactic structure ofa web query.
This can easily be represented as theprobability of a noun, nA, being returned to a queryfor the relationship, R, with noun nB.pw(nA,R, nB)This value along with the other steps we have goneover are stored in a MySQL relational database4.One could trace a relationship probability betweennouns back to the web results which were matchedto a web query, and even determine the abstractsearch phrase which produced the web query.3.2 Concept AnalysisA focus of this work is on going beyond relation-ships between words.
We would like to acquireknowledge about specific concepts in WordNet.
Inparticular, we are trying to induce:conceptA is [in | on] nounB.where conceptA is a concept in WordNet (such as asense of nounA), and nounB remains simply a word.For the analysis, we rely on the vast amount ofnouns we are able to acquire in order to create proba-bilities for relationships of conceptARnounB.
To geta grasp of the idea in general, consider ?table?
as anounB of interest.
By examining all possible hyper-nyms of all senses of each nounA one will find itis common for abstract entities to be ?in a table?(i.e.
data in a table), artifacts to be ?on a table?
(i.e.4http://www.mysql.comcup on a table), and physical things (including livingthings) to be ?at a table?
(i.e.
the employees at thetable).
The same idea could be applied in reverse ifone acquires knowledge for a set of nounAs.
How-ever, this paper only focuses on acquiring knowl-edge for the nounB constituent in a search phrase.To begin with, one should note that concepts inWordNet are represented as synsets.
A synset isa group of word-senses that have the same mean-ing.
For example, (batter-1, hitter-1, slugger-1,batsman-1) is a synset with the meaning ?
(baseball)a ballplayer who is batting?.
We use WordNet ver-sion 3.0 in order to take advantage of the latest up-dates and corrections to the noun ontology.
Since aword has multiple senses, we represent the probabil-ity that a word-sense, nAs, resulted from a query fora relationship, R with nounB as:pns(nAs,R, nB) = pw(lemma(nAs),R, nB)senses(lemma(nAs))where senses returns the number of senses of theword (lemma) within the word-sense nAs.
Wecan then extend the probability to apply to a synset,syns, as:psyn(syns,R, nB) =?nAs?synspns(nAs,R, nB)Finally, we define a recursive function based onthe idea that a concept subsumes all concepts belowit (hyponyms) in the WordNet ontology:Pc(cA,R, nB) = psyn(syns(cA),R, nB)+ ?h?hypos(cA)Pc(h,R, nB)where cA is a concept/node in WordNet, syns re-turns the synset which represents the concept, andhypos returns the set of all direct hyponyms withinthe WordNet ontology.
For example, (money-3) isa (currency-1), so Pc(currency-1,R, nB) receivespsyn((money-3),R, nB) among others.
This typeof calculation over WordNet follows much likethat of Resnik?s (1999) information-content calcu-lation.
Note that the function no longer recurswhen reaching a concept with no hyponyms and thatPc(entity-1,R, nB) is always 1 (entity-1 is the rootnode).
Pc now represents a probability for the rela-tionship: conceptARnounB.5nounB #nounAs nounB #nounAsbasket 3300 boat 2787bookcase 260 bottle 4742bowl 5252 cabin 720cabinet 1474 canoe 163car 5534 ceiling 1187city 1432 desk 4770drawer 1638 dresser 698floor 2850 house 4627jar 4462 kitchen 2948pocket 4771 refrigerator 2897road 5493 room 5023shelf 2581 ship 1469sink 296 sofa 509table 5312 truck 528van 301 wall 2285Table 3: List of nouns which fill the nounB constituentin a search phrase, and the corresponding occurrences ofnounAs acquired for each.4 EvaluationOur evaluation focuses on the applicability of theacquired CSK.
We acquired relationships for the 30nouns listed in Table 3.
These nouns represent allpossible words to fill the nounB constituent of asearch phrase.
The corresponding #nounAs indi-cates the number of nounAs that were acquired fromthe Web for each nounB.
For example, 4771 nounAswere acquired for ?pocket?.
This means 4771 resultsfrom the web matched the parse of a web query for?pocket?
and contained a nounA in WordNet (keep-ing in mind duplicates Web text were removed).Delving deeper into our example, below arethe top 20 nounAs found for the relationshipnounAinpocket.money, hand, cash, firework, something, dol-lar, ball, hands, key, coin, pedometer, card,battery, item, phone, penny, music, buck, im-plant, walletAs described in the concept analysis section, occur-rences of each nounA for a given nounB lead to pwvalues, which in turn are used to produce Pc valuesfor concepts in WordNet.
The application of CSKutilizes these probabilities rather than simply lists ofwords or even lists of concepts.
However, challengeswere encountered during the noun acquisition stepbefore the probabilities were produced.Many challenges of the noun acquisition stepwere overcome through the use of a parser.
For ex-ample, phrases such as ?Palestine is on the road tobecoming...?
could be eliminated since the parsermarks the prepositional phrase ?to becoming?
as be-ing attached to ?the road?.
Thus, the parse of theweb sample does not match the parse of the webquery used to acquire it.
Other times, noun-noun re-lationships were common simply because many webpages seem to copy the text of others.
This prob-lem was handled through the elimination of dupli-cate text samples from the Web.
In the end, onlyabout one in four results from the Web were actuallyused.
Numbers in Table 3 reflect the result of theseeliminations.Some issues of the acquisition step were not di-rectly addressed in this paper.
A domain may tendto be more prevalent on the Internet and skew theCSK, such as fireworkinpocket.
Another example,babyinbasket was very common due to biblical ref-erences.
Fictional works and metaphors also pro-vided uncommon relationships dispersed within theresults.
Additionally, the parser makes mistakes.
Itwas the hope that the concept analysis step wouldhelp to mitigate some noise from these problems.A final issue was the bottleneck of limited queriesper day by the search engines, which restricted us totesting on only the 30 nouns listed.4.1 Disambiguation SystemThe CSK is not intended to be used by itself for dis-ambiguation.
It would be far from accurate to as-sume the sense of a noun can be disambiguated sim-ply by observing its relationship with one other nounin the sentence.
For example, one of the test sen-tences incorporated the relationship noteinpocket.Multiple senses of note are likely to be found in apocket (i.e.
the senses referring to ?a brief writtenrecord?, ?a short personal letter?, or ?a piece of pa-per money?).
In other cases, a relationship may notbe found for any sense of a target word.
Therefore,our knowledge is intended to be used as a reference,consulted by a disambiguation system.We integrate our knowledge into a state of the art?all-words?
word sense disambiguation algorithm.These algorithms are considered unsupervised or6minimally supervised, because they do not requirespecific training data that is designed for instancesof words in the testing data.
In other words, thesesystems are designed to handle any word they comeacross.
Our knowledge can supplement such a sys-tem, because the data can be acquired automaticallyfor an unlimited number of nouns, assuming limit-less web query restrictions.The basis of our disambiguation system is thepublicly available GWSD system (Sinha and Mihal-cea, 2007).
Sinha and Mihalcea report higher re-sults on the Senseval-2 and Senseval-3 datasets thanany of the participating unsupervised system.
Ad-ditionally, GWSD is compatible with WordNet 3.0and its output made it easy to integrate our knowl-edge.
Sense predictions from four different graphmetrics are produced, and we are able to incorporateour knowledge as another prediction within a votingscheme.Considering the role of our knowledge as a refer-ence, in some cases we would like the CSK to sug-gest multiple senses and in others none.
For eachtarget noun instance in the corpus, we lookup thePc(c,R, nB) value, where c is the WordNet conceptthat corresponds to a sense of the target noun.
Wechoose nB by matching the phrase ?in|on det nB?within the sentence.
The system suggests all senseswith a Pc value greater than 0.75 of the maximum Pcvalue over all senses.
If no senses have a Pc valuethen no senses are suggested.During voting, tallies of predictions and sugges-tions are taken for each sense of a noun.
Ties arebroken by choosing the lowest sense number amongall those involved in the tie.
Note that this is differ-ent than choosing the most frequent sense (i.e.
thelowest sense number from all senses), in that onlythe top predicted senses are considered.
This sametype of voting is used with and without the CSK sug-gestions.4.2 Experimental CorpusA goal of our work was to acquire data which couldbe applied to NLP problems.
We focus particularlyon the difficult problem of word sense disambigua-tion.
Due to the lack of sense tagged data, we wereunable to find an annotated corpus with instancesof all the nouns in Table 3 as prepositional com-plements.
This was not surprising considering oneof the reasons that minimally supervised approacheshave become more popular is that they do not requirehand-tagged training data (Mihalcea, 2002; Diab,2004; McCarthy et al, 2004).We created a corpus from sentences in Wikipediawhich contained the phrase ?in|on det lemma?,where det is a determiner or possessive pronoun,lemma is a noun from Table 3, and in|on is a prepo-sition for either relationship described earlier.
Be-low we have provided an example from our corpuswhere the knowledge from ?pocket?
can be appliedto disambiguate ?key?.Now Tony?s key to the flat is in the pocket of hisraincoat, so on returning to his flat some timelater he realizes that he cannot get inside.The corpus5 contained a total of 342 sen-tences, with one target noun annotated per sen-tence.
The target nouns were selected to poten-tially fill the nounA constituent in the relationshipnounARnounB, and they were assigned all appro-priate WordNet 3.0 senses.
Considering the fine-grained nature of WordNet (Ide and Wilks, 2006),26.3% of the instances were annotated with multi-ple senses.
We also restricted the corpus to onlyinclude polysemous nouns, or nouns which had anadditional sense beyond the senses assigned to it.Inter-annotator agreement was used to validatethe corpus.
Because the corpus was built by anauthor of the work, we asked a non-author to re-annotate the corpus without knowledge of the orig-inal annotations.
This second annotator was told tochoose all appropriate senses just as did the originalannotator.
Agreement was calculated as:agree =(?i?C|S1i ?
S2i||S1i ?
S2i|)?
342where S1 and S2 are the two sets of sense annota-tions, and i is an instance of the corpus, C.The agreement and other data concerning corpusannotation can be found in Table 4.
As a point ofcomparison, the Senseval 3 all-words task had a 75%agreement on nouns (Snyder and Palmer, 2004).
Asecond evaluation of agreement was also done.
Thenon-author annotations were treated as if they came5available at: http://eecs.ucf.edu/?hschwartz/CSK/7insts agree F1h F1rnd F1MFSon 131 79.9 84.7 28.2 71.0in 211 80.8 91.9 27.2 67.8both 342 80.5 89.2 27.6 69.0Table 4: Experimental corpus data for each relation-ship (on, in).
insts: number of annotated instances;agree: inter-annotator agreement %; F1 values (precision= recall): h: human annotation, rnd: random baseline,MFS: most frequent sense baseline.without CSK with CSKF1all F1indeg F1all F1indegon 62.6 63.4 64.9 67.2in 68.7 69.7 71.6 72.5both 66.4 67.3 69.0 70.5ties 37 0 66 72Table 5: F1 values (precision = recall) on our experimen-tal corpus with and without CSK.
F1all: using all 4 graphmetrics; F1indeg: using only the indegree metric; ties:number of instances where tie votes occurred.from a disambiguation system in order to get a hu-man upper-bound of performance.
Just as the auto-matic system handled tie votes, when one word hadmultiple sense annotations, the annotation with thelowest sense number was used.
This performanceupper-bound is shown as F1h in Table 4.4.3 ResultsOur disambiguation results are presented in Table5.
We found that, in all cases, including CSK im-proved results.
It turned out that 54.7% of the nouninstances received at least one suggestion from theCSK, and 24.5% of the instances received multiplesuggestions.
It is not clear why the on results wereslightly below that for in.
We suspect the on por-tion of the corpus was slightly more difficult be-cause the human annotation (F1h) found a similarphenomenon.One observation we made when setting up thetest was that the indegree metric alone performedslightly better than using the votes of all four met-rics.
This was not surprising considering Sinha andMihalcea found the indegree metric by itself to per-form only slightly below a combination of metricson the senseval data (Sinha and Mihalcea, 2007).Therefore, Table 5 also reports the use of the inde-gree metric by itself or with CSK, F1indeg.
In thesecases we saw the greatest improvements of usingCSK, producing an an error reduction of about 4.5%and outperforming the F1MFS value.Several additional experiments were performed.Note that even during ties, the chosen sense wastaken from the predictions and suggestions.
Whenwe instead incorporated an MFS backoff strategy forties, our top results for F1indeg with CSK dropped to70.2.
We also ran a precision test with no predictionsmade for tie votes, and found a precision of 71.9%on the 270 instances that did not have a tie for topvotes (this also used the indegree metric with CSK).All results supported our goal of acquiring CSK thatwas applicable to word sense disambiguation.5 ConclusionWe found our acquired CSK to be useful when incor-porated into a word sense disambiguation system,finding an error reduction of around 4.5% for top re-sults.
Relationships between nouns were acquiredfrom the Web through a unique search method offilling constituents in a search phrase.
Samples re-turned from the Web were restricted by a require-ment to match the syntactic parse of a web query.The resulting data was analyzed over WordNet toproduce probabilities of relationships in the form ofconceptARnounB, where conceptA is a concept inWordNet rather than an ambiguous noun.In our effort to validate the knowledge through ap-plication, many steps along the way were left openfor future investigations.
First, there is a need to ex-haustively search for CSK of all nouns and to acquireother forms of CSK.
With this improvement CSKcould be tested on a standard corpus, rather thana corpus focused on select nouns and prepositionalphrases.
Looking into acquisition improvements, astudy of the effectiveness of the parse would be ben-eficial.
Finally, the applicability of the knowledgemay be increased through a more complex conceptanalysis or utilizing a more advanced voting scheme.6 AcknowledgementThis research was supported by the NASA Engi-neering and Safety Center under Grant/CooperativeAgreement NNX08AJ98A.8ReferencesEneko Agirre, Olatz Ansa, and David Martinez.
2001.Enriching wordnet concepts with topic signatures.
InIn Proceedings of the NAACL workshop on WordNetand Other Lexical Resources: Applications, Exten-sions and Customizations, Pittsburg, USA.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the first conference on NorthAmerican chapter of the Association for Computa-tional Linguistics, pages 132?139, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of Conference on EmpiricalMethods in Natural Language Processing (EMNLP-04), Barcelona, Spain.Mona Diab.
2004.
Relieving the data acquisition bottle-neck in word sense disambiguation.
In Proceedings ofthe 42nd Annual Meeting of the Association for Com-putational Linguistics (ACL?04), pages 303?310.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic rela-tions between nominals.
In Proceedings of SemEval-2007, pages 13?18, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In In Proceedings ofthe 14th International Conference on ComputationalLinguistics (COLING-92), pages 539?545.Nancy Ide and Yorick Wilks, 2006.
Word Sense Dis-ambiguation: Algorithms And Applications, chapter 3:Making Sense About Sense.
Springer.Douglas B. Lenat.
1995.
CYC: a large-scale investmentin knowledge infrastructure.
Communications of theACM, 38(11):33?38.H.
Liu and P Singh.
2004.
Conceptnet: A practical com-monsense reasoning toolkit.
BT Technology Journal,22:211?226.David Martinez, Eneko Agirre, and Xinglong Wang.2006.
Word relatives in context for word sense dis-ambiguation.
In Proceedings of the 2006 AustralasianLanguage Technology Workshop, pages 42?50.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant word sensesin untagged text.
In Proceedings of the 42nd Meet-ing of the Association for Computational Linguistics,pages 279?286, Barcelona, Spain, July.
Associationfor Computational Linguistics.Rada Mihalcea.
2002.
Bootstrapping large sense taggedcorpora.
In Proceedings of the 3rd InternationalConference on Languages Resources and EvaluationsLREC 2002, Las Palmas, Spain, May.George Miller, R. Beckwith, Christiane Fellbaum,D.
Gross, and K. Miller.
1993.
Five papers on word-net.
Technical report, Princeton University.Kathy Panton, Cynthia Matuszek, Douglas Lenat, DaveSchneider, Michael Witbrock, Nick Siegel, and BlakeShepard.
2006.
Common sense reasoning : From cycto intelligent assistant.
In Y. Cai and J. Abascal, edi-tors, Ambient Intelligence in Everyday Life, pages 1?31.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Grammaerof the English Language.
Longman.Philip Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journal ofArtificial Intelligence Research, 11:95?130.Hansen A. Schwartz and Fernando Gomez.
2008.
Ac-quiring knowledge from the web to be used as selec-tors for noun sense disambiguation.
In CoNLL 2008:Proceedings of the Twelfth Conference on Computa-tional Natural Language Learning, pages 105?112,Manchester, England, August.Ravi Sinha and Rada Mihalcea.
2007.
Unsupervisedgraph-based word sense disambiguation using mea-sures of word semantic similarity.
Irvine, CA, Septem-ber.Benjamin Snyder and Martha Palmer.
2004.
The En-glish all-words task.
In ACL Senseval-3 Workshop,Barcelona, Spain, July.9
