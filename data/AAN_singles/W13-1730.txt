Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 232?241,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsFeature Engineering in the NLI Shared Task 2013:Charles University Submission ReportBarbora Hladka?, Martin Holub and Vincent Kr?
?z?Charles University in PragueFaculty of Mathematics and PhysicsInstitute of Formal and Applied LinguisticsPrague, Czech Republic{hladka, holub,kriz}@ufal.mff.cuni.czAbstractOur goal is to predict the first language (L1)of English essays?s authors with the help ofthe TOEFL11 corpus where L1, prompts (top-ics) and proficiency levels are provided.
Thuswe approach this task as a classification taskemploying machine learning methods.
Outof key concepts of machine learning, we fo-cus on feature engineering.
We design fea-tures across all the L1 languages not makinguse of knowledge of prompt and proficiencylevel.
During system development, we experi-mented with various techniques for feature fil-tering and combination optimized with respectto the notion of mutual information and infor-mation gain.
We trained four different SVMmodels and combined them through majorityvoting achieving accuracy 72.5%.1 IntroductionLearner corpora are collections of texts written bysecond language (L2) learners, e.g.
English as L2?
ICLE (Granger et al 2009), Lang-8 (Tajiri et al2012), Cambridge Learner Corpus,1 German as L2?
FALKO (Reznicek et al 2012), Czech as L2 ?CzeSL (Hana et al 2010).
They are a valuableresource for second language acquisition research,identifying typical difficulties of learners of a cer-tain proficiency level (e.g.
low/medium/high) orlearners of a certain native language (L1 learners ofL2).
Research on the learner corpora does not con-centrate on text collections only.
Studying the er-rors in learner language is undertaken in the form1http://www.cambridge.org/gb/eltof error annotation like in the projects (Hana et al2012), (Boyd et al 2012), (Rozovskaya and Roth,2010), (Tetreault and Chodorow, 2008).
Once theerrors and other relevant data are recognized in thelearner corpora, automatic procedures for e.g.
errorcorrection, author profiling, native language identi-fication etc.
can be designed.Our attention is focused on the task of automaticNative Language Identification (NLI), namely withEnglish as L2.In this report, we summarize the involment of theCharles University team in the first shared task inNLI co-located with the 8th Workshop on Innova-tive Use of NLP for Building Educational Appli-cations in June 2013 in Atlanta, USA.
The reportis organized as follows: we briefly review relatedworks in Section 2.
The data sets to experiment withare characterized in Section 3.
Section 4 lists themain concepts we pursue during the system devel-opment.
Our approach is entirely focused on featureengineering and thus Section 5 is the most impor-tant one.
We present there our main motivation formaking such a decision, describe patterns accordingto which the features are generated and techniquesthat manipulate the features.
We revise our ideas ex-perimentally as documented in Section 6.
In total,we submitted five systems to the sub-task of closed-training.
In Sections 7 and 8, we describe these sys-tems and discuss their results in detail.
We summa-rize our two month effort in the shared task in Sec-tion 9.2322 Related workWe understand the task of native language identifica-tion as a subtask of natural language processing andwe consider it as still a young task since the veryfirst attempt to address it occurred eight years ago in2005, as evident from the literature, namely (Koppelet al 2005b), (Koppel et al 2005a).We appreciate all the previous work concernedwith the given topic but we focus on the latest threepapers only, all of them published at the 24th In-ternational Conference on Computational Linguis-tics held in December 2012 in Bombay, India,namely (Brooke and Hirst, 2012), (Bykh and Meur-ers, 2012), and (Tetreault et al 2012).
They providea comprehensive review of everything done since thevery first attempts.
We do not want to replicate theirchapters.
Rather, we summarize them from the as-pects we consider the most important ones in anymachine learning system, namely the data, the fea-ture design, the feature manipulation, and the ma-chine learning methods - see Table 1.3 Data setsA new publicly available corpus of non-native En-glish writing called TOEFL112 consists of essays oneight different topics written by non-native speakersof three proficiency levels (low/medium/high); theessays?
authors have 11 different native languages.The corpus contains 1,100 essays per language withan average of 348 word tokens per essay.
A corpusdescription and motivation to build such corpus canbe found in (Blanchard et al 2013).The texts from TOEFL11 were released for thepurpose of the shared task as three subsets, namelyTrain for training, DevTest for testing while sys-tem development, and EvalTest for final testing.The texts were already tokenized and we processedthem with the Standford POS tagger (Toutanova etal., 2003).4 System settings1.
Task: Having a collection of English essayswritten by non-native speakers, the goal is topredict a native language of the essays?
authors.2Source: Derived from data provided by ETS.
Copyright c?2013 ETS.
www.ets.org.Languages L1 are known in advance.
Since wehave a collection of English essays for whichL1 is known (TOEFL11) at our disposal, weformulate this task as a classification task ad-dressed by using supervised machine learningmethods.2.
Feature set: A setA = {A1, A2, ..., Am} ofmfeatures where m changes as we perform var-ious feature combinations and filtering steps.We prefer to work with binary features.
Wedo not include two extra features, proficiencylevel and prompt, provided with the data.
Inaddition, we design features across all 11 lan-guages, i.e.
we do not design features sepa-rately for a particular L1.
Doing so, we ad-dress the task of predicting L1 from the textonly, without any additional knowledge.3.
Input data: A set X of instances being textsfrom TOEFL11 corpus represented as featurevectors, x = ?x1, x2, ..., xm?
?
X,xi ?
Ai.4.
Output classes: A set C of L1 languages, C= {ARA, CHIN, FRE, GER, HIN, ITA, JPN,KOR, SPA, TEL, TUR}, |C| = 11.5.
True prediction: A set D = {< x, y >:x ?
X , y ?
C}, |D| = 12, 100 and its pairwisedisjoint subsets Train, DevTest, EvalTestwhere Train ?
DevTest ?
EvalTest = D,|Train| = 9, 900, |DevTest| = 1, 100,|EvalTest| = 1, 100.6.
Training data: Train ?
DevTest.
No othertype of training data is used.7.
Learning mechanism: Since we focus on fea-ture engineering, we do not study appropriate-ness of particular machine learning methods toour task in details.
Instead, reviewing the re-lated works, we selected the Support VectorMachine algorithm to experiment with.8.
Evaluation: 10-fold cross-validation with thesample Train ?
DevTest.
Accuracy, Pre-cision, Recall.
Proficiency-based evaluation.Topic-based evaluation.233PAPER DATA FEATURE FEATURE MLDESIGN MANIPULATION METHOD[1] Lang-8,ICLE,CambridgeLearnerCorpusfunction words, charac-ter n-grams, POS n-grams,POS/function n-grams, context-free-grammar productions,dependencies, word n-gramsfrequency-basedfeature selectionSVM, MaxEnt[2] ICLE binary features spanning word-based recurring n-grams, func-tion words, recurring POS basedn-grams and combination ofthemno special featuretreatmentlogistic regression[3] ICLE,TOEFL11character n-grams, functionwords, POS, spelling errors,writing qualityno special featuretreatmentlogistic regressionTable 1: A summary of latest related works [1](Brooke and Hirst, 2012), [2](Bykh and Meurers, 2012), [3](Tetreaultat al., 2012)5 Feature engineeringWe split the process of feature engineering into twomutually interlinked steps.
The first step aims at anunderstanding of the task projected into features de-scribing properties of entities we experiment with.These experiments represent the second step wherewe find out how the features interact with each otherand how they interact with a chosen machine learn-ing algorithm.We compose a feature family as a group of pat-terns that are relevant for a particular task.
The fea-tures are then extracted from the data according tothem.
Since we experiment with English texts writ-ten by non-native speakers, we have to search forspecific and identifiable text properties, i.e.
tenden-cies of certain first language writers, based on theerrors caused by the difference between L1 and L2.In addition, we look for phenomena that are not nec-essarily incorrect in written English but they provideclear evidence of characteristics typical for L1.
Ourfeature family is built from chunks of various lengthin the texts, formally lexically and part-of-speechbased n-grams.
In total, the feature family containseight patterns described in Table 2 - six for binaryfeatures l,n,p,s1,s2,sp and two for continuous fea-tures a,r.
Outside the feature family, its patterns canbe combined into joint patterns, like l+sp, n+sp+r.Considering the key issues of machine learning,Figure 1: Feature engineeringwe mainly pay attention to overfitting.
We are awareof many aspects that may cause overfitting, likecomplexity of the model trained, noise in trainingdata, a small amount of training data.
Features canlead to overfitting as well, thus we address it us-ing elaborated feature engineering visualised in Fig-ure 1.
We can see there the data components and theprocess components having the features in common.The scheme can be traced either with individual pat-terns from the feature family or with joint patterns.Both basic feature filtering and advanced featuremanipulation apply selected concepts from informa-234FEATURE DESCRIPTION EXAMPLESFAMILY n=1,2,3PATTERNl n-grams of lemmas picture; to see; you, be, notn n-grams of words picture; to see; you, are, notp n-grams of function words and POS tags of contentwords, i.e.
nouns, verbs, adjectives, cardinal num-bersnot; PRP; you, VBP; JJ, to, VBs1 skipgrams of words: bigram wi?2, wi and trigramswi?3, wi?1, wi, wi?3, wi?2, wi extracted from a se-quence of words wi?3 wi?2 wi?1 wiyou,not; able, see; to, see,in; tothings, ins2 skipgrams of words: bigrams wi?3, wi, wi?4, wiand trigrams wi?4, wi?3, wi, wi?4, wi?2, wi,wi?4, wi?1, wi extracted from a sequence of wordswi?4 wi?3 wi?2 wi?1 wiare,see; you,see; you,are,see;you,able,see; you,to,see;sp n-grams of function words and shrunken POS tagsof content words: POS tags N* are shrunken into atag N, V* into V, J* into Jnot; PRP; you V; J to Va relative frequency of POS tags and function wordsr relative frequency of POS tagsTable 2: A feature family.
Examples are taken from the file 498.txt, namely from the sentence You are not able tosee things in a big picture.
tagged as follows: (You/you/PRP are/be/VBP not/not/RB able/able/JJ to/to/TO see/see/VBthings/thing/NNS in/in/IN a/a/DT big/big/JJ picture/picture/NN ././.
)tion theory.5.1 Concepts from information theoryConsider a random variable A having two possiblevalues 0 and 1 where the probability of 1 is p and0 is 1 ?
p. A degree of uncertainty we deal withwhen predicting the value of the variable dependson p. If p is close to zero or one, then we are almostconfident about the value and our uncertainty is low.If the values are equally likely (i.e.
p = 0.5), ouruncertainty is maximal.The entropy H(A) measures the uncertainty.
Inother words, it quantifies the amount of informationneeded to predict the value of the variable.
The for-mula 1 for the entropy treats variables with N ?
1possible values.H(A) = ?N?i=1p(A = ai) log2 p(A = ai) (1)The conditional entropy H(A|B) quantifies theamount of information needed to predict the valueof the random variable A given that the value of an-other random variable B is known, see Formula 2.Then H(A|B) ?
H(A) holds.H(A|B) =?b?Bp(B = b)H(A|B = b) (2)The amount H(A) ?
H(A|B) by which H(A)decreases reflects additional information about Aprovided by B and is called mutual informationI(A;B) - see Formula 3.
In other words, I(A;B)quantifies the mutual dependence of two randomvariables A and B.I(A;B) = H(A)?H(A|B) (3)Proceeding from statistics to machine learning,independent random variables correspond to fea-tures.
Thus we can directly speak about the entropyof a feature, the conditional entropy of a featuregiven another feature and the mutual information oftwo features.235Information gain of feature Ak - IG(Ak) - mea-sures the expected reduction in entropy caused bypartitioning the data set Data according to the val-ues of the feature Ak (Quinlan, 1987):IG(Ak) = H(Data)?c?i=j|Dvj ||Data|H(Dvj ), (4)where Avk = {v1, v2, ..., vc} is a set of possible val-ues of feature Ak and Dvi is a subset of Data con-tainig instances with the feature value xk = vj .C being a target feature, H(Data) = H(C).Thus the mutual information between C and Ak -I(C;Ak) - is the information gain of the feature Ak,i.e.I(C;Ak) = IG(Ak).
(5)All mentioned concepts are visualized in Figure 2for our settings:?
Our target feature C has eleven possible val-ues (i.e.
L1 languages).
These values areuniformly distributed in the data D, thusH(C) = ?
?11i=1111 log2111 = log2 11.=3.46.
Sample features (only for illustration)A1, A2, A3, A4 ?
A are binary features soH(Ai) ?
1 < H(C) = 3.46, i = 1, ..., 4.The circle areas correspond to the entropy offeatures.?
The black areas correspond to mutual informa-tion I(Ai;Ak).?
The striped areas correspond to the mutual in-formation I(C;Ak) between C and Ak.?
Features A1 and A3 are independent, soI(A1;A3) = 0.?
A2 has the highest mutual dependence with C,?
H(A2) = H(A3) and IG(A2) > IG(A3)In addition to the concepts from information the-ory, we introduce another measure to quantify fea-tures: the document frequency of feature Ak ?df(Ak) is the number of texts in which Ak occurs,i.e.
df(Ak) ?
0.Figure 2: Information gain and mutual information visu-alization5.2 Discussion on featuresWe impose a fundamental requirement on features:they should be both informative (i.e.
useful for theclassification task) and robust (i.e.
not sensitive totraining data).
We control the criterion of being in-formative by information gain maximization.
Thecriterion of being robust is quantified by documentfrequency.
If df(Ak) is high enough, then we canexpect that Ak will occur in test data frequently.
Wepropose two techniques to increase df : (i) filteringout features with low df ; (ii) feature combinationdriven by IG.The fulfillment of both criteria is always depen-dent on training data, i.e.
the final feature set tendsto fit training data and our goal is to weaken this ten-dency in order to get a more robust feature set.
Bothbasic feature filtering and advanced feature combi-nation help us to address this issue.5.3 Basic feature filteringWe obtained the feature setA0 by extracting featuresaccording to the feature family patterns) from thetraining data.
Basic feature filtering removes fea-tures from A0 in two steps that result in a primaryfeature set A1:1.
Remove binary feature Ak if df(Ak) < ?df .Remove continous feature Ak ifrelative frequency(Ak) < ?rf ordf(relative frequency(Ak) ?
?rf ) < ?df .2.
Remove binary feature Ak if IG(Ak) ?
?IG.2365.4 Advanced feature manipulationThe process of advanced feature manipulation han-dles m input features from the primary feature setA1 in two different ways, filter them and combinethem, in order to generate a final feature set Afready to train the model:?
Filter them.
We use Fast Correlation-BasedFilter (FCBF; (Fleuret, 2004), (Yu and Liu,2003)) that addresses the correlation betweenfeatures.
It first ranks the features accord-ing to their information gain, i.e.
IG(A1) ?IG(A2) ?
... ?
IG(Am).
In the second step,it iteratively removes any featureAk if there ex-ists a feature Aj such that IG(Aj) ?
IG(Ak)and I(Ak;Aj) ?
IG(Ak), i.e.
Aj is bet-ter as a predictor of C and Ak is more sim-ilar to Aj than to C. In the situation visu-alized in Figure 2, the feature A4 will be fil-tered out because there is a featureA3 such thatIG(A3) ?
IG(A4) and I(A3;A4) ?
IG(A4)?
Combine them.
We combine (COMB) binaryfeatures using logical operations (AND, OR,XOR, AND NOT, etc.)
getting a new binaryfeature.For example, if we combine two features A1and A2 using the OR operator, we get a newbinary feature Y = A1 OR A2 for which theinequalities df(Y ) > df(A1) and df(Y ) >df(A2) hold.
Thus we get a feature that ismore robust than the two input features.
Toknow whether it is more informative, we needto know how high IG(Y ) is with respect toIG(A1) and IG(A2).
Without loss of gen-erality, assume that IG(A1) > IG(A2).
IfIG(Y ) > IG(A1) > IG(A2), then Y is moreinformative than A1 and A2, but both of thesefeatures could be informative enough as well.It depends on the threshold we set up for beinginformative.
We can easily iterate this process -let Y1 = A1 ORA2 and Y2 = A3 ORA4.
Thenwe can combine Y3 = Y1 OR A5 or Y4 = Y1OR Y2, etc.Then, advanced feature manupilation runs ac-cording to scenarios formed as a series of FCBFand COMB, for example A1 ?
FCBF ?
COMB?
FCBF?
Af or A1 ?
COMB?
FCBF?
Af .6 System developmentDuring system development, we formulated hy-potheses how to avoid overfitting and get features ro-bust and informative enough.
In parallel, we run theexperiments with parameters using which we con-trolled this requirement.Basic feature filtering We set the thresholds ?df ,?IG, ?rf empirically to the values 4, 0 and 0.02, re-spectively.
Table 3 shows the changes in the size ofthe initial feature set after the basic feature filtering.It is evident that even such trivial filtering reducesthe number of features substantially.FEATURE INITIAL AFTER AFTERFAMILY FEATURE df IGPATTERN SET FILTERING FILTERING(i.e.
|A0|) (i.e.
|A1|)l 2,078,105 156,722 2,827n 2,411,516 163,939 2,840p 1,116,986 161,681 2,467s1 4,794,702 242,969 1,877s2 7,632,011 382,881 4,566sp 781,018 123,431 933a 181 111 111r 48 48 48Table 3: Volumes of initial feature sets extracted fromTrain ?
DevTest (1st column).
Volumes of primaryfeature sets after basic filtering of A0 (3rd column).Learning mechanisms Originally, we startedwith two learning algorithms, Random Forests (RF)and Support Vector Machines (SVM), running themin the R system.3The Random forests4 algorithm joins random-ness with classification decision trees.
They iteratethe process of two random selections and training adecision tree k-times on a subset ofm features.
Eachof them classifies a new input instance x and theclass with the most votes becomes the output classof x.Support Vector Machines (Vapnik, 1995) effi-ciently perform both linear and non-linear classi-fication employing different Kernel functions and3http://www.r-project.org4http://www.stat.berkeley.edu/?breiman/237avoiding the overfitting by two parameters, cost andgamma.We run a number of initial experiments with thefollowing settings: the feature family pattern n; thebasic feature filtering, RF with different values ofparameters k and m, SVM with different values ofparameters kernel, gamma and costCross-validation on the data set Train performedwith SVM showed significantly better results thanthose obtained with RF.
We were quite suprised thatRF ran with low performance so that we decidedto stop experimenting with this algorithm.
Step bystep, we added patterns into the feature family andcarried out experiments with SVM only on the dataset Train ?
DevTest.
We fixed the values of theSVM parameters kernel, degree, gamma, cost afterseveral experiments as follows kernel = polynomial,degree = 1, gamma = 0.0004, cost = 1.
Then weincluded the advanced feature manipulation into theexperiments according to the scenariosA1 ?
FCBF?
COMB ?
FCBF ?
Af and A1 ?
COMB ?FCBF?
Af .
COMB was composed using the ORoperator only.
Unfortunately, none of them outper-formed the initial experiments with the basic filter-ing only.Table 4 contains candidates for the final submis-sion.
The highlighted candidates were finally se-lected for the submission.FEATURE CROSS-VALIDATION Acc (%)PATTERNS on Train on DevTestl + a 72.97 ?
0.76 71.09n + a 72.45 ?
0.98 63.00l + sp + a 72.00 ?
0.72 70.64l+sp 71.09 ?
0.72 71.45n+sp 70.38 ?
0.69 52.27l 71.67 ?
0.57 70.18n 71.27 ?
0.84 68.72l+p 71.17 ?
2.41 71.27n+s1 69.90 ?
1.04 66.72n+s2 68.75 ?
1.50 67.63n+s1+s2 67.97 ?
0.96 66.81Table 4: Candidates for the final submission.
Candidatesin bold were submitted.MODEL FEATURE FAMILY AccPATTERN (%)CUNI-closed-1 majority votingof CUNI-closed-[2-5] 72.5CUNI-closed-2 l+a 71.6CUNI-closed-3 l+p 71.6CUNI-closed-5 l+sp+a 71.1CUNI-closed-4 l+sp 69.7Table 5: An overview of models submitted.MODEL Acc (%)CUNI-closed-1 74.2CUNI-closed-2 73.4CUNI-closed-3 73.9CUNI-closed-4 73.1CUNI-closed-5 72.9Table 6: Cross-validation results for all submitted CUNI-closed systems.7 Submission to the shared taskIn total, we submitted five systems to the closed-training sub-task - see their overview in Table 5.
Theresults correspond to our expectations that we madebased on the results of cross-validation presented inTable 4.
The best system, CUNI-closed-1, was theoutcome of majority voting of the remaining foursystems.
The performance of this system per lan-guage is presented in Table 7.Table 6 reports accuracy results when doing 10-fold cross-validation on Train ?
DevTest.
Thefolds for this experiment were provided by the or-ganizers to get more reliable comparison of the NLIsystems.It is interesting to analyse the complementarity ofthe CUNI-closed-[2-5] systems that affects the per-formance of CUNI-closed-1.
In Table 8, we list thenumerical characteristics of five possible situationsthat can occur when comparing the outputs of twosystems i and j.
Situations 2 and 3 capture howcomplementary the systems are.
The numbers forour systems are presented in Table 9.We grouped languages according to the thresholdsof F-measure.
First we did it across the data, no mat-ter what the proficiency level and prompt are - seethe first row of Table 10.
Second we did grouping238Acc(%) P(%) R(%) F(%)ARA 72 67 72 69,6CHI 78 71 78 74,3FRE 73 74 73 73,7GER 83 83 83 83,0HIN 75 68 75 71,4ITA 83 85 83 83,8JPN 70 65 70 67,6KOR 64 70 64 67,0SPA 66 70 66 68,0TEL 68 72 68 69,7TUR 65 72 65 68,4Table 7: CUNI-closed-1 on EvalTest: Acc, P, R, F1.
the number of instances both systems pre-dicted correctly;2. the number of instances both systems pre-dicted incorrectly;3. the number of instances the systems pre-dicted differently: i system correctly and jsystem incorrectly;4. the number of instance the systems pre-dicted differently: i system incorrectly and jsystem correctly;5. the number of instances the systems pre-dicted differently and both incorrectly.Table 8: Pair of two systems i and j and their predictions.pair of CUNI-closed-iand CUNI-closed-j systems2-3 2-4 2-5 3-4 3-5 4-51 707 717 745 701 710 7322 161 215 242 183 181 2503 81 71 43 87 78 354 81 50 37 66 72 505 70 47 33 63 59 33Table 9: CUNI-closed-[2-5]: complementary rates.?
90% ?
80% ?
70% < 70%overall GER,ITACHI,FRE,HINTEL,ARA,TUR,SPA,JPN,KORhigh GER,ITACHI,HIN,FREKOR,TUR,SPA,TEL,ARA,JPNmedium ITA,GER,FRE,TELCHI,ARA,SPA,TURJPN,KOR,HINlow GER ITA,FRE,JPNARA KOR,TEL,HIN,TUR,SPA,CHI,FRETable 10: CUNI-closed-1 on EvalTest: Groups of lan-guages sorted according to F-measure w.r.t.
proficiencylevel.for a particular proficiency level - see the remainingrows in Table 10.
We can see that both GER andITA are languages with the highest F-measure on alllevels.
Third we grouped by a particular prompt -see Table 11.
We can see there diversed numbers forL1 languages despite the fact that prompts are for-mulated generally.
Even more, we observe a topicsimilarity between prompts P2, P3, and P8, betweenP4 and P5, and between P1 and P7.8 Future plansIn our future research, w want to elaborate ideas thatconcern the feature engineering.
We plan to workwith the feature family that we designed in our ini-tial experiments.
However, we will think about morespecific patterns in the essays, like the average countof tokens/punctuation/capitalized nouns/articles persentence.
As Table 12 shows, there is only one can-didate, namely the number of tokens in sentence, tobe taken into considerations since there is the largestdifference between minimum and maximum.We confronted Ken Lackman,5 an Englishteacher, with the task of manual native languageidentification by English teachers.
He says: ?I think5http://kenlackman.com239?
90% ?
80% ?
70% < 70%P1 GER,ITAFRE,HIN,ARA,TELCHI,KOR,TURSPA,JPNP2 GER,FRE,ITA,TELARA,HIN,JPNSPA,KOR,CHITURP3 GER CHI,KORHIN,ITAFRE,JPN,TUR,ARA,SPA,TELP4 ITA CHI,TUR,HIN,FRETEL,SPA,GER,JPN,ARA,KORP5 ITA TUR,JPN,GERFRE,TEL,KORHIN,CHI,SPA,ARAP6 ITA,CHI,SPAKOR,ARA,JPNHIN,FRE,TEL,GER,TURP7 ITA,CHI,TURSPA,GER,HIN,FREARA,JPN,KOR,TELP8 ARA GER,TEL,SPA,ITAFREHIN,KOR,JPN,TUR,CHITable 11: CUNI-closed-1 on EvalTest: Groups of lan-guages sorted according to F-measure w.r.t.
prompt.AVG COUNT TrainPER MIN (L1) - MAX (L1)SENTENCETOKEN 18 (JPN) -25.8 (SPA)PUNCTUATION 1.5 (HIN, TEL) - 2.1 (SPA)CAPITALIZED 0.1 (CHI) - 0.3 (HIN)NOUNthe 0.6 (KOR) - 1.2 (ITA, SPA, TEL)a/an 0.3 (JPN, KOR) - 0.7 (ITA, SPA)Table 12: Data counts on Train.it?s quite possible to do but you would need a set ofguidelines to supply teachers with.
The guidelineswould list tendancies of certain first language writ-ers, based on errors caused by difference betweenL1 and L2.
For example, Germans tend to capital-ize too many nouns, since there are far more nounscapitalized in their language, Asians tend to leaveout articles and Arab students tend to use the verb?to be?
inappropriately before other verbs.?
Look-ing into the data, we observe the phenomena Ken isspeaking about, but the quantity of them is not sta-tistically significant to distinguish L1s.We formulate an idea of a bootstrapped featureextraction that has not been published yet, at leastto our knowledge.
Let us assume a set of opera-tions that can be performed over a feature set (so far,we have proposed two possible operations with thefeatures, filtering them out and their combinations).Determining whether a condition to perform a givenoperation holds is done on the high number of ran-dom samples.
If the condition holds on the majorityof them, then the operation is performed.
The onlyparameter that must be set up is the majority.
In-stead of setting a threshold that is adjusted for all thefeatures, bootstrapped feature extraction deals withfitting the data individually for each feature.9 ConclusionIt was the very first experience for our team to ad-dress the task of NLI.
We assess it as very stimu-lating and we understand our participation as settingthe baseline for applying other ideas.
An overall ta-ble of results (Tetreault et al 2013) for all the teamsinvolved in the NLI 2013 Shared Task shows thatthere is still space for improvement of our baseline.We really appreciate all the work done by the or-ganizers.
They?ve made an effort to prepare thehigh-quality data and set up the framework by whichthe use of various NLI systems can be reliably com-pared.AcknowledgmentsThe authors would like to thank Eva Hajic?ova?
andJirka Hana for their valuable comments.
We alsothank Ken Lackman and Leslie Ryan6 for sharing6http://lesliestreet.cz240their teaching experience.
This research was sup-ported by the Czech Science Foundation, grant no.P103/12/G084 and the Technology Agency of theCzech Republic, grant no.
TA02010182.ReferencesDaniel Blanchard, Joel Tetreault, Derrick Higgins, AoifeCahill, and Martin Chodorow.
2013.
TOEFL11: ACorpus of Non-Native English.
Technical report, Ed-ucational Testing Service.Adriane Boyd, Marion Zepf, and Detmar Meurers.
2012.Informing Determiner and Preposition Error Correc-tion with Hierarchical Word Clustering.
In Proceed-ings of the 7th Workshop on Innovative Use of NLPfor Building Educational Applications (BEA7), pages208?215, Montreal, Canada.
Association for Compu-tational Linguistics.Julian Brooke and Graeme Hirst.
2012.
Robust, Lexical-ized Native Language Identification.
In Proceedingsof COLING 2012, pages 391?408, Mumbai, India, De-cember.Serhiy Bykh and Detmar Meurers.
2012.
Native Lan-guage Identification using Recurring n-grams ?
Inves-tigating Abstraction and Domain Dependence.
In Pro-ceedings of COLING 2012, pages 425?440, Mumbai,India, December.F.
Fleuret.
2004.
Fast Binary Feature Selection withConditional Mutual Information.
Journal of MachineLearning Research (JMLR), 5:1531?1555.Sylviane Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009. International Corpus ofLearner English v2 (Handbook + CD-ROM).
Pressesuniversitaires de Louvain, Louvain-la-Neuve.Jirka Hana, Alexandr Rosen, Svatava S?kodova?, andBarbora S?tindlova?.
2010.
Error-tagged Learner Cor-pus of Czech.
In Proceedings of the Fourth Lin-guistic Annotation Workshop (LAW IV), pages 11?19, Stroudsburg, USA.
Association for ComputationalLinguistics.Jirka Hana, Alexandr Rosen, Barbora S?tindlova?, andPetr Ja?ger.
2012.
Building a learner corpus.
InProceedings of the 8th International Conference onLanguage Resources and Evaluation (LREC 2012),I?stanbul, Turkey.
European Language Resources As-sociation.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005a.Automatically determining an anonymous author?s na-tive language.
Intelligence and Security Informatics,pages 41?76.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005b.Determining an author?s native language by mininga text for errors.
In Proceedings of the 11th ACMSIGKDD, pages 624?628, Chicago, IL.
ACM.John Ross Quinlan.
1987.
Simplifying decision trees.International Journal of ManMachine Studies, 27,221-234.Marc Reznicek, Anke Ludeling, Cedric Krummes,Franziska Schwantuschke, Maik Walter, KarinSchmidt, Hagen Hirschmann, and Torsten Andreas.2012.
Das Falko-Handbuch.
Korpusaufbau undAnnotationen Version 2.01.
Technical report, Depart-ment of German Studies and Linguistics, HumboldtUniversity, Berlin, Germany.Alla Rozovskaya and Dan Roth.
2010.
Annotating ESLErrors: Challenges and Rewards.
In Proceedings ofthe NAACL HLT 2010 Fifth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 28?36, Los Angeles, California, June.
Associ-ation for Computational Linguistics.Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-sumoto.
2012.
Tense and Aspect Error Correction forESL Learners Using Global Context.
In In Proceed-ings of the 50th ACL: Short Papers, pages 192?202.Joel R. Tetreault and Martin Chodorow.
2008.
Nativejudgments of non-native usage: experiments in prepo-sition error detection.
In Proceedings of the Work-shop on Human Judgements in Computational Lin-guistics, HumanJudge ?08, pages 24?32, Stroudsburg,PA, USA.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-tin Chodorow.
2012.
Native Tongues, Lost andFound: Resources and Empirical Evaluations in Na-tive Language Identification.
In Proceedings of COL-ING 2012, pages 2585?2602, Mumbai, India, Decem-ber.
The COLING 2012 Organizing Committee.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013.A Report on the First Native Language IdentificationShared Task.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, Atlanta, GA, USA, June.
Association forComputational Linguistics.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of HLT-NAACL 2003, pages 252?259.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, New York.L.
Yu and H. Liu.
2003.
Feature Selection for High-Dimensional Data: A Fast Correlation-Based FilterSolution.
In Proceedings of The Twentieth Interna-tional Conference on Machine Leaning (ICML-03),pages 856?863, Washington, D.C., USA.
Associationfor Computational Linguistics.241
