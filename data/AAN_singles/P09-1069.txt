Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 611?619,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPLearning a Compositional Semantic Parserusing an Existing Syntactic ParserRuifang Ge Raymond J. MooneyDepartment of Computer SciencesUniversity of Texas at AustinAustin, TX 78712{grf,mooney}@cs.utexas.eduAbstractWe present a new approach to learning asemantic parser (a system that maps natu-ral language sentences into logical form).Unlike previous methods, it exploits an ex-isting syntactic parser to produce disam-biguated parse trees that drive the compo-sitional semantic interpretation.
The re-sulting system produces improved resultson standard corpora on natural languageinterfaces for database querying and sim-ulated robot control.1 IntroductionSemantic parsing is the task of mapping a natu-ral language (NL) sentence into a completely for-mal meaning representation (MR) or logical form.A meaning representation language (MRL) is aformal unambiguous language that supports au-tomated inference, such as first-order predicatelogic.
This distinguishes it from related taskssuch as semantic role labeling (SRL) (Carrerasand Marquez, 2004) and other forms of ?shallow?semantic analysis that do not produce completelyformal representations.
A number of systems forautomatically learning semantic parsers have beenproposed (Ge and Mooney, 2005; Zettlemoyer andCollins, 2005; Wong and Mooney, 2007; Lu et al,2008).
Given a training corpus of NL sentencesannotated with their correct MRs, these systemsinduce an interpreter for mapping novel sentencesinto the given MRL.Previous methods for learning semantic parsersdo not utilize an existing syntactic parser that pro-vides disambiguated parse trees.1 However, ac-curate syntactic parsers are available for many1Ge and Mooney (2005) use training examples withsemantically annotated parse trees, and Zettlemoyer andCollins (2005) learn a probabilistic semantic parsing modelwhich initially requires a hand-built, ambiguous CCG gram-mar template.
(a) If our player 2 has the ball,then position our player 5 in the midfield.
((bowner (player our {2}))(do (player our {5}) (pos (midfield))))(b) Which river is the longest?answer(x1,longest(x1,river(x1)))Figure 1: Sample NLs and their MRs in theROBOCUP and GEOQUERY domains respectively.languages and could potentially be used to learnmore effective semantic analyzers.
This paperpresents an approach to learning semantic parsersthat uses parse trees from an existing syntacticanalyzer to drive the interpretation process.
Thelearned parser uses standard compositional seman-tics to construct alternative MRs for a sentencebased on its syntax tree, and then chooses the bestMR based on a trained statistical disambiguationmodel.
The learning system first employs a wordalignment method from statistical machine trans-lation (GIZA++ (Och and Ney, 2003)) to acquirea semantic lexicon that maps words to logicalpredicates.
Then it induces rules for composingMRs and estimates the parameters of a maximum-entropy model for disambiguating semantic inter-pretations.
After describing the details of our ap-proach, we present experimental results on stan-dard corpora demonstrating improved results onlearning NL interfaces for database querying andsimulated robot control.2 BackgroundIn this paper, we consider two domains.
Thefirst is ROBOCUP (www.robocup.org).
In theROBOCUP Coach Competition, soccer agentscompete on a simulated soccer field and receivecoaching instructions in a formal language calledCLANG (Chen et al, 2003).
Figure 1(a) shows asample instruction.
The second domain is GEO-QUERY, where a logical query language based onProlog is used to query a database on U.S. geog-raphy (Zelle and Mooney, 1996).
The logical lan-611CONDITION(bowner PLAYER )(player TEAMour{UNUM})2(a)P BOWNERP PLAYERP OUR P UNUM(b)SNPPRP$ourNPNNplayerCD2VPVBhasNPDETtheNNball(c)Figure 2: Parses for the condition part of the CLANG in Figure 1(a): (a) The parse of the MR. (b) Thepredicate argument structure of (a).
(c) The parse of the NL.PRODUCTION PREDICATERULE?
(CONDITION DIRECTIVE) P RULECONDITION?
(bowner PLAYER) P BOWNERPLAYER?
(player TEAM {UNUM}) P PLAYERTEAM?our P OURUNUM?2 P UNUMDIRECTIVE?
(do PLAYER ACTION) P DOACTION?
(pos REGION) P POSREGION?
(midfield) P MIDFIELDTable 1: Sample production rules for parsing theCLANG example in Figure 1(a) and their corre-sponding predicates.guage consists of both first-order and higher-orderpredicates.
Figure 1(b) shows a sample query inthis domain.We assume that an MRL is defined by an un-ambiguous context-free grammar (MRLG), so thatMRs can be uniquely parsed, a standard require-ment for computer languages.
In an MRLG, eachproduction rule introduces a single predicate in theMRL, where the type of the predicate is given inthe left hand side (LHS), and the number and typesof its arguments are defined by the nonterminals inthe right hand side (RHS).
Therefore, the parse ofan MR also gives its predicate-argument structure.Figure 2(a) shows the parse of the conditionpart of the MR in Figure 1(a) using the MRLGdescribed in (Wong, 2007), and its predicate-argument structure is in Figure 2(b).
SampleMRLG productions and their predicates for pars-ing this example are shown in Table 1, where thepredicate P PLAYER takes two arguments (a1 anda2) of type TEAM and UNUM (uniform number).3 Semantic Parsing FrameworkThis section describes our basic framework, whichis based on a fairly standard approach to computa-tional semantics (Blackburn and Bos, 2005).
Theframework is composed of three components: 1)an existing syntactic parser to produce parse treesfor NL sentences; 2) learned semantic knowledge(cf.
Sec.
5), including a semantic lexicon to assignpossible predicates (meanings) to words, and a setof semantic composition rules to construct possi-ble MRs for each internal node in a syntactic parsegiven its children?s MRs; and 3) a statistical dis-ambiguation model (cf.
Sec.
6) to choose amongmultiple possible semantic constructs as definedby the semantic knowledge.The process of generating the semantic parsefor an NL sentence is as follows.
First, the syn-tactic parser produces a parse tree for the NLsentence.
Second, the semantic lexicon assignspossible predicates to each word in the sentence.Third, all possible MRs for the sentence are con-structed compositionally in a recursive, bottom-upfashion following its syntactic parse using com-position rules.
Lastly, the statistical disambigua-tion model scores each possible MR and returnsthe one with the highest score.
Fig.
3(a) showsone possible semantically-augmented parse tree(SAPT) (Ge and Mooney, 2005) for the conditionpart of the example in Fig.
1(a) given its syntac-tic parse in Fig.
2(c).
A SAPT adds a semanticlabel to each non-leaf node in the syntactic parsetree.
The label specifies the MRL predicate forthe node and its remaining (unfilled) arguments.The compositional process assumes a binary parsetree suitable for predicate-argument composition;parses in Penn-treebank style are binarized usingCollins?
(1999) method.Consider the construction of the SAPT inFig.
3(a).
First, each word is assigned a semanticlabel.
Most words are assigned an MRL predicate.For example, the word player is assigned the pred-icate P PLAYER with its two unbound arguments,a1 and a2, indicated using ?.
Words that do notintroduce a predicate are given the label NULL,like the and ball.2 Next, a semantic label is as-2The words the and ball are not truly ?meaningless?
sincethe predicate P BOWNER (ball owner) is conveyed by the612P BOWNERP PLAYERP OURour?a1P PLAYER?
?a1?a2?P PLAYERplayerP UNUM2?a1P BOWNER?a1P BOWNERhasNULLNULLtheNULLball(a) SAPT(bowner (player our {2}))(player our {2})ourour?a1 (player a1 {2})??a1?a2?
(player a1 {a2} )player22?a1(bowner a1)?a1(bowner a1)hasNULLNULLtheNULLball(b) Semantic DerivationFigure 3: Semantic parse for the condition part of the example in Fig.
1(a) using the syntactic parse inFig.
2(c): (a) A SAPT with syntactic labels omitted for brevity.
(b) The semantic derivation of the MR.signed to each internal node using learned compo-sition rules that specify how arguments are filledwhen composing two MRs (cf.
Sec.
5).
The label?a1P PLAYER indicates that the remaining argu-ment a2 of the P PLAYER child is filled by the MRof the other child (labeled P UNUM).Finally, the SAPT is used to guide the composi-tion of the sentence?s MR. At each internal node,an MR for the node is built from the MRs of itschildren by filling an argument of a predicate, asillustrated in the semantic derivation shown in Fig.3(b).
Semantic composition rules (cf.
Sec.
5) areused to specify the argument to be filled.
For thenode spanning player 2, the predicate P PLAYERand its second argument P UNUM are composed toform the MR: ?a1 (player a1 {2}).
Composingan MR with NULL leaves the MR unchanged.
AnMR is said to be complete when it contains no re-maining ?
variables.
This process continues up thephrase has the ball.
For simplicity, predicates are intro-duced by a single word, but statistical disambiguation (cf.Sec.
6) uses surrounding words to choose a meaning for aword whose lexicon entry contains multiple possible predi-cates.tree until a complete MR for the entire sentence isconstructed at the root.4 Ensuring Meaning CompositionThe basic compositional method in Sec.
3 onlyworks if the syntactic parse tree strictly followsthe predicate-argument structure of the MR, sincemeaning composition at each node is assumed tocombine a predicate with one of its arguments.However, this assumption is not always satisfied,for example, in the case of verb gapping and flex-ible word order.
We use constructing the MR forthe directive part of the example in Fig.
1(a) ac-cording to the syntactic parse in Fig.
4(b) as anexample.
Given the appropriate possible predicateattached to each word in Fig.
5(a), the node span-ning position our player 5 has children, P POS andP PLAYER, that are not in a predicate-argument re-lation in the MR (see Fig.
4(a)).To ensure meaning composition in this case,we automatically create macro-predicates thatcombine multiple predicates into one, so thatthe children?s MRs can be composed as argu-613P DOP PLAYERP OUR P UNUMP POSP MIDFIELD(a)VPADVPRBthenVPVPVBpositionNPour player 5PPINinNPDTtheNNmidfield(b)Figure 4: Parses for the directive part of the CLANG in Fig.
1(a): (a) The predicate-argument structureof the MR. (b) The parse of the NL (the parse of the phrase our player 5 is omitted for brevity).ments to a macro-predicate.
Fig.
5(b) showsthe macro-predicate P DO POS (DIRECTIVE?
(doPLAYER (pos REGION))) formed by merging theP DO and P POS in Fig.
4(a).
The macro-predicatehas two arguments, one of type PLAYER (a1)and one of type REGION (a2).
Now, P POS andP PLAYER can be composed as arguments to thismacro-predicate as shown in Fig.
5(c).
However,it requires assuming a P DO predicate that hasnot been formally introduced.
To indicate this, alambda variable, p1, is introduced that ranges overpredicates and is provisionally bound to P DO, asindicated in Fig.
5(c) using the notation p1:do.Eventually, this predicate variable must be boundto a matching predicate introduced from the lexi-con.
In the example, p1:do is eventually bound tothe P DO predicate introduced by the word then toform a complete MR.Macro-predicates are introduced as needed dur-ing training in order to ensure that each MR inthe training set can be composed using the syn-tactic parse of its corresponding NL given reason-able assignments of predicates to words.
For eachSAPT node that does not combine a predicate witha legal argument, a macro-predicate is formed bymerging all predicates on the paths from the childpredicates to their lowest common ancestor (LCA)in the MR parse.
Specifically, a child MR be-comes an argument of the macro-predicate if itis complete (i.e.
contains no ?
variables); other-wise, it also becomes part of the macro-predicateand its ?
variables become additional argumentsof the macro-predicate.
For the node spanning po-sition our player 5 in the example, the LCA of thechildren P PLAYER and P POS is their immedi-ate parent P DO, therefore P DO is included in themacro-predicate.
The complete child P PLAYERbecomes the first argument of the macro-predicate.The incomplete child P POS is added to the macro-predicate P DO POS and its ?
variable becomesanother argument.For improved generalization, once a predicatein a macro-predicate becomes complete, it is re-moved from the corresponding macro-predicatelabel in the SAPT.
For the node spanning positionour player 5 in the midfield in Fig.
5(a), P DO POSbecomes P DO once the arguments of pos arefilled.In the following two sections, we describe thetwo subtasks of inducing semantic knowledge anda disambiguation model for this enhanced compo-sitional framework.
Both subtasks require a train-ing set of NLs paired with their MRs. Each NLsentence also requires a syntactic parse generatedusing Bikel?s (2004) implementation of Collinsparsing model 2.
Note that unlike SCISSOR (Geand Mooney, 2005), training our method does notrequire gold-standard SAPTs.5 Learning Semantic KnowledgeLearning semantic knowledge starts from learningthe mapping from words to predicates.
We usean approach based on Wong and Mooney (2006),which constructs word alignments between NLsentences and their MRs.
Normally, word align-ment is used in statistical machine translation tomatch words in one NL to words in another; hereit is used to align words with predicates based ona ?parallel corpus?
of NL sentences and MRs. Weassume that each word alignment defines a possi-ble mapping from words to predicates for buildinga SAPT and semantic derivation which composethe correct MR. A semantic lexicon and compo-sition rules are then extracted directly from the614P DO?
?a1?a2?P DOthen?p1P DO POS = ?p1P DO?
?p1?a2?P DO POS?a1P POSpositionP PLAYERour player 5P MIDFIELDNULLinP MIDFIELDNULLtheP MIDFIELDmidfield(a) SAPTP DOa1:PLAYER P POSa2:REGION(b) Macro-Predicate P DO POS(do (player our {5}) (pos (midfield)))??a1?a2?
(do a1a2)then?p1(p1:do (player our {5}) (pos (midfield)))??p1?a2?
(p1:do (player our {5}) (pos a2))?a1(pos a1)position(player our {5})our player 5(midfield)NULLin(midfield)NULLthe(midfield)midfield(c) Semantic DerivationFigure 5: Semantic parse for the directive part of the example in Fig.
1(a) using the syntactic parse inFig.
4(b): (a) A SAPT with syntactic labels omitted for brevity.
(b) The predicate-argument structure ofmacro-predicate P DO POS (c) The semantic derivation of the MR.nodes of the resulting semantic derivations.Generation of word alignments for each train-ing example proceeds as follows.
First, each MRin the training corpus is parsed using the MRLG.Next, each resulting parse tree is linearized to pro-duce a sequence of predicates by using a top-down, left-to-right traversal of the parse tree.
Thenthe GIZA++ implementation (Och and Ney, 2003)of IBM Model 5 is used to generate the five bestword/predicate alignments from the corpus of NLsentences each paired with the predicate sequencefor its MR.After predicates are assigned to words usingword alignment, for each alignment of a trainingexample and its syntactic parse, a SAPT is gener-ated for composing the correct MR using the pro-cesses discussed in Sections 3 and 4.
Specifically,a semantic label is assigned to each internal nodeof each SAPT, so that the MRs of its children arecomposed correctly according to the MR for thisexample.There are two cases that require special han-dling.
First, when a predicate is not aligned to anyword, the predicate must be inferred from context.For example, in CLANG, our player is frequentlyjust referred to as player and the our must be in-ferred.
When building a SAPT for such an align-ment, the assumed predicates and arguments aresimply bound to their values in the MR. Second,when a predicate is aligned to several words, i.e.
itis represented by a phrase, the alignment is trans-formed into several alignments where each predi-cate is aligned to each single word in order to fitthe assumptions of compositional semantics.Given the SAPTs constructed from the resultsof word-alignment, a semantic derivation for eachtraining sentence is constructed using the methodsdescribed in Sections 3 and 4.
Composition rules615are then extracted from these derivations.Formally, composition rules are of the form:?1.P1 + ?2.P2 ?
{?p.Pp, R} (1)where P1, P2 and Pp are predicates for the leftchild, right child, and parent node, respectively.Each predicate includes a lambda term ?
ofthe form ?
?pi1 , .
.
.
, ?pim , ?aj1 , .
.
.
, ?ajn?, an un-ordered set of all unbound predicate and argumentvariables for the predicate.
The component Rspecifies how some arguments of the parent predi-cate are filled when composing the MR for the par-ent node.
It is of the form: {ak1=R1, .
.
.
, akl=Rl},where Ri can be either a child (ci), or a child?scomplete argument (ci, aj) if the child itself is notcomplete.For instance, the rule extracted for the node forplayer 2 in Fig.
3(b) is:?
?a1?a2?.P PLAYER + P UNUM ?
{?a1.P PLAYER, a2=c2},and for position our player 5 in Fig.
5(c):?a1.P POS + P PLAYER ?
{?
?p1?a2?.P DO POS, a1=c2},and for position our player 5 in the midfield:?
?p1?a2?.P DO POS + P MIDFIELD?
{?p1.P DO POS, {a1=(c1,a1), a2=c2}}.The learned semantic knowledge is necessaryfor handling ambiguity, such as that involvingword senses and semantic roles.
It is also used toensure that each MR is a legal string in the MRL.6 Learning a Disambiguation ModelUsually, multiple possible semantic derivations foran NL sentence are warranted by the acquired se-mantic knowledge, thus disambiguation is needed.To learn a disambiguation model, the learned se-mantic knowledge (see Section 5) is applied toeach training example to generate all possible se-mantic derivations for an NL sentence given itssyntactic parse.
Here, unique word alignments arenot required, and alternative interpretations com-pete for the best semantic parse.We use a maximum-entropy model similarto that of Zettlemoyer and Collins (2005) andWong and Mooney (2006).
The model defines aconditional probability distribution over semanticderivations (D) given an NL sentence S and itssyntactic parse T :Pr(D|S, T ; ??)
= exp?i ?ifi(D)Z??
(S, T )(2)where f?
(f1, .
.
.
, fn) is a feature vector parame-terized by ?
?, and Z??
(S, T ) is a normalizing fac-tor.
Three simple types of features are used inthe model.
First, are lexical features which countthe number of times a word is assigned a particu-lar predicate.
Second, are bilexical features whichcount the number of times a word is assigned aparticular predicate and a particular word precedesor follows it.
Last, are rule features which countthe number of times a particular composition ruleis applied in the derivation.The training process finds a parameter ???
that(approximately) maximizes the sum of the condi-tional log-likelihood of the MRs in the training set.Since no specific semantic derivation for an MR isprovided in the training data, the conditional log-likelihood of an MR is calculated as the sum of theconditional probability of all semantic derivationsthat lead to the MR.
Formally, given a set of NL-MR pairs {(S1,M1), (S2,M2), ..., (Sn,Mn)} andthe syntactic parses of the NLs {T1, T2, ..., Tn},the parameter ???
is calculated as:???
= argmax?
?n?i=1log Pr(Mi|Si, Ti; ??)
(3)= argmax?
?n?i=1log?D?iPr(D?i |Si, Ti; ??
)where D?i is a semantic derivation that producesthe correct MR Mi.L-BFGS (Nocedal, 1980) is used to estimate theparameters ???.
The estimation requires statisticsthat depend on all possible semantic derivationsand all correct semantic derivations of an exam-ple, which are not feasibly enumerated.
A vari-ant of the Inside-Outside algorithm (Miyao andTsujii, 2002) is used to efficiently collect the nec-essary statistics.
Following Wong and Mooney(2006), only candidate predicates and composi-tion rules that are used in the best semantic deriva-tions for the training set are retained for testing.No smoothing is used to regularize the model; Wetried using a Gaussian prior (Chen and Rosenfeld,1999), but it did not improve the results.7 Experimental EvaluationWe evaluated our approach on two standard cor-pora in CLANG and GEOQUERY.
For CLANG,300 instructions were randomly selected fromthe log files of the 2003 ROBOCUP Coach616Competition and manually translated into En-glish (Kuhlmann et al, 2004).
For GEOQUERY,880 English questions were gathered from vari-ous sources and manually translated into Prologqueries (Tang and Mooney, 2001).
The averagesentence lengths for the CLANG and GEOQUERYcorpora are 22.52 and 7.48, respectively.Our experiments used 10-fold cross validationand proceeded as follows.
First Bikel?s imple-mentation of Collins parsing model 2 was trainedto generate syntactic parses.
Second, a seman-tic parser was learned from the training set aug-mented with their syntactic parses.
Finally, thelearned semantic parser was used to generate theMRs for the test sentences using their syntacticparses.
If a test example contains constructs thatdid not occur in training, the parser may fail to re-turn an MR.Wemeasured the performance of semantic pars-ing using precision (percentage of returned MRsthat were correct), recall (percentage of test exam-ples with correct MRs returned), and F-measure(harmonic mean of precision and recall).
ForCLANG, an MR was correct if it exactly matchedthe correct MR, up to reordering of argumentsof commutative predicates like and.
For GEO-QUERY, an MR was correct if it retrieved the sameanswer as the gold-standard query, thereby reflect-ing the quality of the final result returned to theuser.The performance of a syntactic parser trainedonly on the Wall Street Journal (WSJ) can de-grade dramatically in new domains due to cor-pus variation (Gildea, 2001).
Experiments onCLANG and GEOQUERY showed that the perfor-mance can be greatly improved by adding a smallnumber of treebanked examples from the corre-sponding training set together with the WSJ cor-pus.
Our semantic parser was evaluated usingthree kinds of syntactic parses.
Listed togetherwith their PARSEVAL F-measures these are:gold-standard parses from the treebank (GoldSyn,100%), a parser trained on WSJ plus a smallnumber of in-domain training sentences requiredto achieve good performance, 20 for CLANG(Syn20, 88.21%) and 40 for GEOQUERY (Syn40,91.46%), and a parser trained on no in-domaindata (Syn0, 82.15% for CLANG and 76.44% forGEOQUERY).We compared our approach to the following al-ternatives (where results for the given corpus werePrecision Recall F-measureGOLDSYN 84.73 74.00 79.00SYN20 85.37 70.00 76.92SYN0 87.01 67.00 75.71WASP 88.85 61.93 72.99KRISP 85.20 61.85 71.67SCISSOR 89.50 73.70 80.80LU 82.50 67.70 74.40Table 2: Performance on CLANG.Precision Recall F-measureGOLDSYN 91.94 88.18 90.02SYN40 90.21 86.93 88.54SYN0 81.76 78.98 80.35WASP 91.95 86.59 89.19Z&C 91.63 86.07 88.76SCISSOR 95.50 77.20 85.38KRISP 93.34 71.70 81.10LU 89.30 81.50 85.20Table 3: Performance on GEOQUERY.available): SCISSOR (Ge and Mooney, 2005), anintegrated syntactic-semantic parser; KRISP (Kateand Mooney, 2006), an SVM-based parser usingstring kernels; WASP (Wong and Mooney, 2006;Wong and Mooney, 2007), a system based onsynchronous grammars; Z&C (Zettlemoyer andCollins, 2007)3, a probabilistic parser based on re-laxed CCG grammars; and LU (Lu et al, 2008),a generative model with discriminative reranking.Note that some of these approaches require ad-ditional human supervision, knowledge, or engi-neered features that are unavailable to the othersystems; namely, SCISSOR requires gold-standardSAPTs, Z&C requires hand-built template gram-mar rules, LU requires a reranking model usingspecially designed global features, and our ap-proach requires an existing syntactic parser.
TheF-measures for syntactic parses that generate cor-rect MRs in CLANG are 85.50% for syn0 and91.16% for syn20, showing that our method canproduce correct MRs even when given imperfectsyntactic parses.
The results of semantic parsersare shown in Tables 2 and 3.First, not surprisingly, more accurate syntac-tic parsers (i.e.
ones trained on more in-domaindata) improved our approach.
Second, in CLANG,all of our methods outperform WASP and KRISP,which also require no additional information dur-ing training.
In GEOQUERY, Syn0 has signifi-cantly worse results than WASP and our other sys-tems using better syntactic parses.
This is not sur-prising since Syn0?s F-measure for syntactic pars-ing is only 76.44% in GEOQUERY due to a lack3These results used a different experimental setup, train-ing on 600 examples, and testing on 280 examples.617Precision Recall F-measureGOLDSYN 61.14 35.67 45.05SYN20 57.76 31.00 40.35SYN0 53.54 22.67 31.85WASP 88.00 14.37 24.71KRISP 68.35 20.00 30.95SCISSOR 85.00 23.00 36.20Table 4: Performance on CLANG40.Precision Recall F-measureGOLDSYN 95.73 89.60 92.56SYN20 93.19 87.60 90.31SYN0 91.81 85.20 88.38WASP 91.76 75.60 82.90SCISSOR 98.50 74.40 84.77KRISP 84.43 71.60 77.49LU 91.46 72.80 81.07Table 5: Performance on GEO250 (20 in-domainsentences are used in SYN20 to train the syntacticparser).of interrogative sentences (questions) in the WSJcorpus.
Note the results for SCISSOR, KRISP andLU on GEOQUERY are based on a different mean-ing representation language, FUNQL, which hasbeen shown to produce lower results (Wong andMooney, 2007).
Third, SCISSOR performs betterthan our methods on CLANG, but it requires extrahuman supervision that is not available to the othersystems.
Lastly, a detailed analysis showed thatour improved performance on CLANG comparedto WASP and KRISP is mainly for long sentences(> 20 words), while performance on shorter sen-tences is similar.
This is consistent with theirrelative performance on GEOQUERY, where sen-tences are normally short.
Longer sentences typ-ically have more complex syntax, and the tradi-tional syntactic analysis used by our approach re-sults in better compositional semantic analysis inthis situation.We also ran experiments with less training data.For CLANG, 40 random examples from the train-ing sets (CLANG40) were used.
For GEOQUERY,an existing 250-example subset (GEO250) (Zelleand Mooney, 1996) was used.
The results areshown in Tables 4 and 5.
Note the performanceof our systems on GEO250 is higher than thaton GEOQUERY since GEOQUERY includes morecomplex queries (Tang and Mooney, 2001).
First,all of our systems gave the best F-measures (ex-cept SYN0 compared to SCISSOR in CLANG40),and the differences are generally quite substantial.This shows that our approach significantly im-proves results when limited training data is avail-able.
Second, in CLANG, reducing the trainingdata increased the difference between SYN20 andSYN0.
This suggests that the quality of syntacticparsing becomes more important when less train-ing data is available.
This demonstrates the advan-tage of utilizing existing syntactic parsers that arelearned from large open domain treebanks insteadof relying just on the training data.We also evaluated the impact of the word align-ment component by replacing Giza++ by gold-standard word alignments manually annotated forthe CLANG corpus.
The results consistentlyshowed that compared to using gold-standardword alignment, Giza++ produced lower seman-tic parsing accuracy when given very little trainingdata, but similar or better results when given suf-ficient training data (> 160 examples).
This sug-gests that, given sufficient data, Giza++ can pro-duce effective word alignments, and that imper-fect word alignments do not seriously impair oursemantic parsers since the disambiguation modelevaluates multiple possible interpretations of am-biguous words.
Using multiple potential align-ments from Giza++ sometimes performs even bet-ter than using a single gold-standard word align-ment because it allows multiple interpretations tobe evaluated by the global disambiguation model.8 Conclusion and Future workWe have presented a new approach to learning asemantic parser that utilizes an existing syntacticparser to drive compositional semantic interpre-tation.
By exploiting an existing syntactic parsertrained on a large treebank, our approach producesimproved results on standard corpora, particularlywhen training data is limited or sentences are long.The approach also exploits methods from statisti-cal MT (word alignment) and therefore integratestechniques from statistical syntactic parsing, MT,and compositional semantics to produce an effec-tive semantic parser.Currently, our results comparing performanceon long versus short sentences indicates that ourapproach is particularly beneficial for syntacticallycomplex sentences.
Follow up experiments us-ing a more refined measure of syntactic complex-ity could help confirm this hypothesis.
Rerankingcould also potentially improve the results (Ge andMooney, 2006; Lu et al, 2008).AcknowledgmentsThis research was partially supported by NSFgrant IIS?0712097.618ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Patrick Blackburn and Johan Bos.
2005.
Represen-tation and Inference for Natural Language: A FirstCourse in Computational Semantics.
CSLI Publica-tions, Stanford, CA.Xavier Carreras and Luis Marquez.
2004.
Introduc-tion to the CoNLL-2004 shared task: Semantic rolelabeling.
In Proc.
of 8th Conf.
on ComputationalNatural Language Learning (CoNLL-2004), Boston,MA.Stanley F. Chen and Ronald Rosenfeld.
1999.
A Gaus-sian prior for smoothing maximum entropy model.Technical Report CMU-CS-99-108, School of Com-puter Science, Carnegie Mellon University.Mao Chen, Ehsan Foroughi, Fredrik Heintz, SpirosKapetanakis, Kostas Kostiadis, Johan Kummeneje,Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-fens, Yi Wang, and Xiang Yin.
2003.
Usersmanual: RoboCup soccer server manual for soccerserver version 7.07 and later.
Available at http://sourceforge.net/projects/sserver/.Michael Collins.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Ruifang Ge and Raymond J. Mooney.
2005.
A statisti-cal semantic parser that integrates syntax and seman-tics.
In Proc.
of 9th Conf.
on Computational NaturalLanguage Learning (CoNLL-2005), pages 9?16.Ruifang Ge and Raymond J. Mooney.
2006.
Dis-criminative reranking for semantic parsing.
In Proc.of the 21st Intl.
Conf.
on Computational Linguis-tics and 44th Annual Meeting of the Associationfor Computational Linguistics (COLING/ACL-06),Sydney, Australia, July.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proc.
of the 2001 Conf.
on EmpiricalMethods in Natural Language Processing (EMNLP-01), Pittsburgh, PA, June.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProc.
of the 21st Intl.
Conf.
on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics (COLING/ACL-06),pages 913?920, Sydney, Australia, July.Greg Kuhlmann, Peter Stone, Raymond J. Mooney, andJude W. Shavlik.
2004.
Guiding a reinforcementlearner with natural language advice: Initial resultsin RoboCup soccer.
In Proc.
of the AAAI-04 Work-shop on Supervisory Control of Learning and Adap-tive Systems, San Jose, CA, July.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProc.
of the Conf.
on Empirical Methods in Natu-ral Language Processing (EMNLP-08), Honolulu,Hawaii, October.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proc.of Human Language Technology Conf.
(HLT-2002),San Diego, CA, March.Jorge Nocedal.
1980.
Updating quasi-Newton matri-ces with limited storage.
Mathematics of Computa-tion, 35(151):773?782, July.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Lappoon R. Tang and Raymond J. Mooney.
2001.
Us-ing multiple clause constructors in inductive logicprogramming for semantic parsing.
In Proc.
of the12th European Conf.
on Machine Learning, pages466?477, Freiburg, Germany.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proc.
of Human LanguageTechnology Conf.
/ N. American Chapter of theAssociation for Computational Linguistics AnnualMeeting (HLT-NAACL-2006), pages 439?446.Yuk Wah Wong and Raymond J. Mooney.
2007.Learning synchronous grammars for semantic pars-ing with lambda calculus.
In Proc.
of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL-07), pages 960?967.Yuk Wah Wong.
2007.
Learning for Semantic Pars-ing and Natural Language Generation Using Statis-tical Machine Translation Techniques.
Ph.D. the-sis, Department of Computer Sciences, University ofTexas, Austin, TX, August.
Also appears as Artifi-cial Intelligence Laboratory Technical Report AI07-343.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proc.
of 13th Natl.
Conf.
on Artifi-cial Intelligence (AAAI-96), pages 1050?1055.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proc.
of the 21th Annual Conf.
on Un-certainty in Artificial Intelligence (UAI-05).Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In Proc.
of the 2007 Joint Conf.
onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing (EMNLP-CoNLL-07), pages 678?687, Prague,Czech Republic, June.619
