Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 440?449,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPGraded Word Sense AssignmentKatrin ErkUniversity of Texas at Austinkatrin.erk@mail.utexas.eduDiana McCarthyUniversity of Sussexdianam@sussex.ac.ukAbstractWord sense disambiguation is typicallyphrased as the task of labeling a word incontext with the best-fitting sense from asense inventory such as WordNet.
Whilequestions have often been raised over thechoice of sense inventory, computationallinguists have readily accepted the best-fitting sense methodology despite the factthat the case for discrete sense bound-aries is widely disputed by lexical seman-tics researchers.
This paper studies gradedword sense assignment, based on a recentdataset of graded word sense annotation.1 IntroductionThe task of automatically characterizing wordmeaning in text is typically modeled as word sensedisambiguation (WSD): given a list of senses fortarget lemma w, the task is to pick the best-fittingsense for a given occurrence of w. The list ofsenses is usually taken from an online dictionaryor thesaurus.
However, clear cut sense boundariesare sometimes hard to define, and the meaning ofwords depends strongly on the context in whichthey are used (Cruse, 2000; Hanks, 2000).
Someresearchers in lexical semantics have suggestedthat word meanings lie on a continuum betweeni) clear cut cases of ambiguity and ii) vaguenesswhere clear cut boundaries do not hold (Tuggy,1993).
Certainly, it seems that a more complexrepresentation of word sense is needed with asofter, graded representation of meaning ratherthan a fixed listing of senses (Cruse, 2000).A recent annotation study ((Erk et al, 2009),hereafter GWS) marked a target word in contextwith graded ratings (on a scale of 1-5) on sensesfrom WordNet (Fellbaum, 1998).
Table 1 showsan example of a sentence with the target wordin bold, and with the annotator judgments givento each sense.
The study found that annotatorsmade ample use of the intermediate ratings on thescale, and often gave high ratings to more than oneWordNet sense for the same occurrence.
It wasfound that the annotator ratings could not easilybe transformed to categorial judgments by makingmore coarse-grained senses.
If human word sensejudgments are best viewed as graded, it makessense to explore models of word sense that canpredict graded sense assignments.In this paper we look at the issue of graded ap-plicability of word sense from the point of viewof automatic graded word sense assignment, us-ing the GWS graded word sense dataset.
We makethree primary contributions.
Firstly, we proposeevaluation metrics that can be used on gradedword sense judgments.
Some of these metrics, likeSpearman?s ?, have been used previously (Mc-Carthy et al, 2003; Mitchell and Lapata, 2008),but we also introduce new metrics based on thetraditional precision and recall.
Secondly, we in-vestigate how two classes of models perform onthe task of graded word sense assignment: onthe one hand classical WSD models, on the otherhand prototype-based vector space models thatcan be viewed as simple one-class classifiers.
Westudy supervised models, training on traditionalWSD data and evaluating against a graded scale.Thirdly, the evaluation metrics we use also pro-vides a novel analysis of annotator performanceon the GWS dataset.2 Related WorkWSD has to date been a task where word senses areviewed as having clear cut boundaries.
However,there are indications that word meanings do notbehave in this way (Kilgarriff, 2006).
Researchersin the field of WSD have acknowledged these prob-lems but have used existing lexical resources inthe hope that useful applications can be built withthem.
However, there is no consensus on which440SensesSentence 1 2 3 4 5 6 7 AnnotatorThis can be justified thermodynamically in this case, andthis will be done in a separate paper which is beingprepared.2 3 3 5 5 2 3 Ann.
11 3 1 3 5 1 1 Ann.
21 5 2 1 5 1 1 Ann.
31.3 3.7 2 3 5 1.3 1.7 AvgTable 1: A sample annotation in the GWS experiment.
The senses are: 1 material from cellulose 2 report3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical objectinventory is suitable for which application, otherthan cross-lingual applications where the inven-tory can be determined from parallel data (Carpuatand Wu, 2007; Chan et al, 2007).
For monolin-gual applications however it is less clear whethercurrent state-of-the-art WSD systems for taggingtext with dictionary senses are able to have an im-pact on applications.One way of addressing the problem of low inter-annotator agreement and system performance is tocreate an inventory that is coarse-grained enoughfor humans and computers to do the job reli-ably (Ide and Wilks, 2006; Hovy et al, 2006;Palmer et al, 2007).
Such coarse-grained invento-ries can be produced manually from scratch (Hovyet al, 2006) or by automatically relating (Mc-Carthy, 2006) or clustering (Navigli, 2006; Nav-igli et al, 2007) existing word senses.
While thereduction in polysemy makes the task easier, wedo not know which are the right distinctions to re-tain.
In fact, fine-grained distinctions may be moreuseful than coarse-grained ones for some applica-tions (Stokoe, 2005).
Furthermore, Hanks (2000)goes further and argues that while the ability todistinguish coarse-grained senses is indeed desir-able, subtler and more complex representations ofword meaning are necessary for text understand-ing.In this paper, instead of focusing on issues ofgranularity we try to predict graded judgments ofword sense applicability, using a recent datasetwith graded annotation (Erk et al, 2009).
Ourhope is that models which can mimic graded hu-man judgments on the same task should better re-flect the underlying phenomena of word mean-ing compared to a system that focuses on mak-ing clear cut distinctions.
Also, we hope that suchmodels might prove more useful in applications.There is one existing study of graded sense as-signment (Ramakrishnan et al, 2004).
It tries toestimate a probability distribution over senses byconverting all of WordNet into a huge BayesianNetwork, and reports improvements in a QuestionAnswering task.
However, it does not test its pre-diction against human annotator data.We concentrate on supervised models in thispaper since they generally perform better thantheir unsupervised or knowledge-based counter-parts (Navigli, 2009).
We compare them againsta baseline model which simply uses the train-ing data to obtain a probability distribution oversenses regardless of context, since marginal distri-butions are highly skewed making a prior distribu-tion very informative (Chan and Ng, 2005; Lapataand Brew, 2004).Along with standard WSD models, we evalu-ate vector space models that use the training datato locate a word sense in semantic space.
Wordsense and vector space models have been related intwo ways.
On the one hand, vector space modelshave been used for inducing word senses (Sch?utze,1998; Pantel and Lin, 2002).
The different mean-ings of a word are obtained by clustering vectors.The clusters must then be mapped to an inven-tory if a standard WSD dataset is used for eval-uation.
In contrast, we use sense tagged train-ing data with the aim of building models of givenword senses, rather than clustering occurrencesinto word senses.
The second way in which wordsense and vector space models have been related isto assign disambiguated feature vectors to Word-Net concepts (Pantel, 2005; Patwardhan and Ped-ersen, 2006).
However those works do not usesense-tagged data and are not aimed at WSD, ratherthe applications are to insert new concepts into anontology and to measure the relatedness of con-cepts.We are not concerned in this paper with argu-ing for or against any particular sense inventory.WordNet has been criticized for being overly fine-grained (Navigli et al, 2007; Ide and Wilks, 2006),we are using it here because it is the sense inven-tory used by Erk et al (2009).
That annotationstudy used it because it is sufficiently fine-grainedto allow for the examination of subtle distinctionsbetween usages and because it is publicly available441lemma # # training(PoS) senses SemCor SE-3add (v) 6 171 238argument (n) 7 14 195ask (v) 7 386 236different (a) 5 106 73important (a) 5 125 11interest (n) 7 111 160paper (n) 7 46 207win (v) 4 88 53total training sentences 1047 1173Table 2: Lemmas used in this studywith various sense-tagged datasets (e.g.
(Miller etal., 1993; Mihalcea et al, 2004)) for comparison.3 DataIn this paper, we use a subset of the GWSdataset (Erk et al, 2009) where three annotatorssupplied ordinal judgments of the applicability ofWordNet (v3.0) senses on a 5 point scale: 1 ?completely different, 2 ?
mostly different, 3 ?
sim-ilar, 4 ?
very similar and 5 ?
identical.
Table 1shows a sample annotation.
The sentences thatwe use from the GWS dataset were originally ex-tracted from the English SENSEVAL-3 lexical sam-ple task (Mihalcea et al, 2004) (hereafter SE-3)and SemCor (Miller et al, 1993).1For 8 lem-mas, 25 sentences were randomly sampled fromSemCor and 25 randomly sampled from SE-3, giv-ing a total of 50 sentences per lemma.
The lem-mas, their PoS and number of senses from Word-Net are shown in table 2.The annotation study found that annotatorsmade ample use of the intermediate levels of ap-plicability (2-4), and they often gave positive rat-ings (3-5) to more than one sense for a single oc-currence.
The example in Table 1 is one suchcase.
An analysis of the annotator ratings foundthat they could not easily be explained in catego-rial terms by making more coarse-grained sensesbecause senses that were not positively correlatedoften had high ratings for the same instance.The GWS dataset contains a sequence of judg-ments for each occurrence of a target word in asentence context: one judgment for each Word-Net sense of the target word.
To obtain a sin-gle judgment for each sense in each sentence weuse the average judgment from the three annota-tors.
As models typically assign values between1The GWS data also contains data from the English Lex-ical Substitution Task (McCarthy and Navigli, 2007) but wedo not use that portion of the data for these experiments.0 and 1, we normalize the annotator judgmentsfrom the GWS dataset to fall into the same range byusing normalized judgment = (judgment ?1.0)/4.0.
This maps an original judgment of 5 toa normalized judgment of 1.0, it maps an original1 to 0.0, and intermediate judgments are mappedaccordingly.As the GWS dataset is too small to accommodateboth training and testing of a supervised model, weuse all the data from GWS for testing our models,and train our models on traditional word sense an-notation data.
We use as training data all sentencesfrom SemCor and the training portion of SE-3 thatare not included in GWS.
The quantity of trainingdata available is shown in the last two columns oftable 2.4 Evaluating Graded Word SenseAssignmentThis section discusses measures for evaluatingsystem performance for the case where gold judg-ments are graded rather than categorial.Correlation.
The standard method for compar-ing a list of graded gold judgments to a list ofgraded predicted judgments is by testing for corre-lation.
In our case, as we cannot assume a normaldistribution of the judgments, a non-parametrictest such as Spearman?s ?
will be appropriate.Spearman?s ?
uses the formula of Pearson?s coef-ficient, defined as?
(X,Y ) =cov(X,Y )?X?YPearson?s coefficient computes the correlation oftwo random variables X and Y as their covari-ance divided by the product of their standard devi-ations.
In the computation of Spearman?s ?, valuesare transformed to rankings before the formula isapplied.2As Spearman?s ?
compares the rank-ings of two sets of judgments, it abstracts from theabsolute values of the judgments.
It is useful tohave a measure that abstracts from absolute valuesof judgments and magnitude of difference becausethe GWS dataset contains annotator judgments ona fixed scale, and it is quite possible that humanjudges will differ in how they use such a scale.Each judgment in the gold-standard can berepresented as a 4-tuple ?lemma, sense no, sen-tence no, gold judgment?.
For example, ?add.v,2Mitchell and Lapata (2008) note that Spearman?s ?
tendsto yield smaller coefficients than its parametric counterpartssuch as Pearson?s coefficient.4421, 1, 0.8?
is the first sentence for target add.v, firstWordNet sense, with a (normalized) judgment of0.8.
Likewise, each prediction by the model canbe represented as a 4-tuple ?lemma, sense no, sen-tence no, predicted judgment?.
We writeG for theset of gold tuples, A for the set of assigned tuples,L for the set of lemmas, S`for the set of sensenumbers that exist for lemma `, and T for the setof sentence numbers (there are 50 sentences foreach lemma).
We writeG|lemma=`for the gold setrestricted to those tuples with lemma `, and anal-ogously for other set restrictions and for A. Thereare several possibilities for measuring correlation:by lemma: for each lemma ` ?
L, compute cor-relation between G|lemma=`and A|lemma=`by lemma+sense: for each lemma ` and eachsense number i ?
S`, compute cor-relation between G|lemma=`,senseno=iandA|lemma=`,senseno=iby lemma+sentence: for each lemma ` and sen-tence number t ?
T , compute cor-relation between G|lemma=`,sentence=tandA|lemma=`,sentence=tComparison by lemma tests for the consis-tent use of judgments for the same target lemma.A comparison by lemma+sense ranks all occur-rences of the same target lemma by how stronglythey evoke a given word sense.
A comparisonby lemma+sentence ranks different senses by howstrongly they apply to a given target lemma oc-currence.
In reporting correlation by lemma (bylemma+sense, by lemma+sentence), we averageover all lemmas (lemma+sense, lemma+sentencecombinations), and we report the percentage oflemmas (combinations) for which the correlationwas significant.
We report averaged correlation bylemma rather than one overall correlation over alljudgments in order not to give more weight to lem-mas with more senses.Divergence.
Another possibility for measuringthe performance of a graded sense assignmentmodel is to use Jensen/Shannon divergence (J/S),which is a symmetric version of Kullback/Leiblerdivergence.
Given two probability distributionsp, q, the Kullback/Leibler divergence of q from pisD(p||q) =?xp(x) logp(x)q(x)and their J/S isJS(p, q) =12(D(p||p+ q2) +D(q||p+ q2)We will use J/S for an evaluation bylemma+sentence: for each lemma ` ?
Land sentence number t ?
T , we normalizeG|lemma=`,sentence=t, the set of judgments forsenses of ` in t, by the sum of sense judgments for` and t. We do the same for A|lemma=`,sentence=t.Then we compute J/S.
In doing so, we are nottrying to interpret G|lemma=`,sentence=tas somekind of probability distribution over senses, ratherwe use J/S as a measure that abstracts fromabsolute judgments but not from the magnitude ofdifferences between judgments.Precision and Recall.
We have discussed ameasure that abstracts from both absolute judg-ments and magnitude of differences (Spearman?s?
), and a measure that abstracts from absolutejudgments but not the magnitude of differences(J/S).
What is still missing is a measure that teststo what degree a model conforms to the absolutejudgments given by the human annotators.To obtain a measure for performance in predict-ing absolute gold judgments, we generalize preci-sion and recall.
In the categorial case, precision isdefined as P =true positivestrue positives+false positives, true pos-itives divided by system-assigned positives, andrecall is R =true positivestrue positives+false negatives, true posi-tives divided by gold positives.
Writing gold`,i,tfor the judgment j associated with lemma ` andsense number i for sentence t in the gold data (i.e.,?`, i, t, j?
?
G), and analogously assigned`,i,t, weextend precision and recall to the graded case asfollows:P`=?i?S`,t?Tmin(gold`,i,t, assigned`,i,t)?i?S`,t?Tassigned`,i,tandR`=?i?S`,t?Tmin(gold`,i,t, assigned`,i,t)?i?S`,t?Tgold`,i,twhere ` is a lemma.
We compute precision and re-call by lemma, then macro-average them in ordernot to give more weight to lemmas that have moresenses.
The formula for F-score as the harmonicmean of precision and recall remains unchanged:F = 2 P R/(P +R).If the data is categorial, the graded precision andrecall measures coincide with ?classical?
precision443Cx/2 until, IN, soft, JJ, remaining, VBG, ingredient,NNSCx/50 for, IN, sweet-sour, NN, sauce, NN, .
.
.
, to, TO,a, DT, boil, NNCh OA, OA/ingredient/NNSTable 3: Sample features for add in BNC occur-rence For sweet-sour sauce, cook onion in oil un-til soft.
Add remaining ingredients and bring toa boil.
Cx/2 (Cx/50): context of size 2 (size 50)either side of the target.
Ch: children of target.and recall, which can be seen as follows.
Gradedsense assignment is represented by assigning eachsense a score between 0.0 and 1.0.
The categorialcase can be represented in the same way, the dif-ference being that one single sense will receive ascore of 1.0 while all other senses get a score of0.0.
With this representation for categorial senseassignment, consider a fixed token t of lemma `.
?i?S`min(assigned`,i,t, gold`,i,t) will be 1 if theassigned sense is the gold sense, and 0 otherwise.5 Models for Graded Word SenseAssignmentIn this section we discuss the computational mod-els for graded word sense that are tested in thispaper.Single-best-sense WSD.
The first model that wetest is a standard WSD model that assigns, to eachtest occurrence of a target word, a single best-fitting word sense.
The system thus attributes aconfidence score of 1 to the assigned sense and aconfidence score of 0 for all other senses for thatsentence.
We refer to it as WSD/single.
The modeluses standard features: lemma and part of speechin a narrow context window (2 words either side)and a wide context window (50 words either side),as well as dependency labels leading to parent,children, and siblings of the target word, and lem-mas and part of speech of parent, child, and siblingnodes.
Table 3 shows sample model features for anoccurrence of add in the British National Corpus(BNC) (Leech, 1992).
The model uses a maxi-mum entropy learner3, training one binary classi-fier per sense.
(With n-ary classifiers, the model?sperformance is slightly worse.)
The model is thusnot highly optimized, but fairly standard.WSD confidence level as judgment.
Our secondmodel is the same WSD system as above, but we3http://maxent.sourceforge.net/use it to predict a judgment for each sense of atarget occurrence, taking the confidence level re-turned by each sense-specific binary classifier asthe predicted judgment.
We refer to this model asWSD/conf .Word senses as points in semantic space.
Theresults of the GWS annotation study raise the ques-tion of how word senses are best conceptualized,given that annotators assigned graded judgmentsof applicability of word senses, and given that theyoften combined high judgments for multiple wordsenses.
One way of modeling these findings isto view word senses as prototypes, where someuses of a word will be typical examples of a givensense, for some uses the sense will clearly not ap-ply, and to some uses the sense will be borderlineapplicable.We use a very simple model of word senses asprototypes, representing them as points in a se-mantic space.
Graded sense applicability judg-ments can then be modeled using vector similarity.The dimensions of the vector space are the featuresof the WSD system above (including dimensionslike Cx2/until, Cx2/IN, Ch/OA/ingredient/NNS forthe example in Table 3), and the coordinates areraw feature counts.
We compute a single vectorfor each sense s, the centroid of all training oc-currences that have been labeled with s. The pre-dicted judgment for a test sentence and sense sis then the similarity of the sentence?s vector tothe centroid vector for s, computed using cosine.We call this model Prototype.
Like instance-basedlearners (Daelemans and den Bosch, 2005), thePrototype model measures the distance betweenfeature vectors in space.
Unlike instance-basedlearners, it only uses data from a single categoryfor training.As it is to be expected that the vectors in thisspace will be very sparse, we also test a variantof the Prototype model with Sch?utze-style second-order vectors (Sch?utze, 1998), called Prototype/2.Given a (first-order) feature vector, we computea second-order vector as the centroid of vectorsfor all lemma features (omitting stopwords) in thefirst-order vector.
For the feature vector in Table 3,this is the centroid of vectors~sweet-sour,~sauce,.
.
.
,~boil.
We compute the vectors~sweet-sour etc.as dependency vectors (Pad?o and Lapata, 2007)4over a Minipar parse (Lin, 1993) of the BNC.4We use the DV package, http://www.nlpado.de/?sebastian/dv.html, to compute the vector space.444We transform raw co-occurrence counts in theBNC-based vectors using pointwise mutual in-formation (PMI), a common transformation func-tion (Mitchell and Lapata, 2008).5Another way of motivating the use of vectorspace models of word sense is by noting that weare trying to predict graded sense assignment bytraining on traditional word sense annotated data,where each target word occurrence is typicallymarked with a single word sense.
Traditional wordsense annotation, when used to predict GWS judg-ments, will contain spurious negative data: sup-pose a human annotator is annotating an occur-rence of target word t and views senses s1, s2ands3as somewhat applicable, with sense s1applyingmost clearly.
Then if the annotation guidelines askfor the best-fitting sense, the annotator should onlyassign s1.
The occurrence is recorded as havingsense s1, but not senses s2and s3.
This, then, con-stitutes spurious negative data for senses s2and s3.The simple vector space model of word sense thatwe use implements a radical solution to this prob-lem of spurious negative data: it only uses positivedata for a single sense, thus forgoing competitionbetween categories.
It is to be expected that notusing competition between categories will hurt thevector space model?s performance, but this designgives us the chance to compare two model classesthat use opposing strategies with respect to spuri-ous negative data: the WSD models fully trust thenegative data, while the vector space models ig-nore it.6 ExperimentsThis section reports on experiments for the taskof graded word sense assignment.
As data, weuse the GWS dataset described in Sec.
3.
We testthe models discussed in Sec.
5, evaluating with themethods described in Sec.
4.To put the models?
performance into perspec-tive, we first consider the human performance onthe task, shown in Table 4.
The first three linesof the table show the performance of each annota-tor evaluated against the average of the other two.The fourth line averages over the previous threelines to provide an average human ceiling for thetask.
In the correlation of rankings by lemma, cor-relation is statistically significant for all lemmas at5We also tested PMI transformation for the first-order vec-tors, but will not report the results here as they were worseacross the board than without PMI.p ?
0.01.
For correlation by lemma+sense and bylemma+sentence, the percentage of pairs with sig-nificant correlation is lower: 73.6 of lemma/sensepairs and 29.0 of lemma/sentence pairs reach sig-nificance at p ?
0.05.
For p ?
0.01, the per-centage is 58.3 and 12.2, respectively.
The higher?
but lower proportion of significant values forlemma+sentence pairs compared to lemma+senseis due to the fact that there are far fewer dat-apoints (sample size) for each calculation of ?
(#senses for lemma+sentence vs 50 sentences forlemma+sense).At 0.131, J/S for Annotator 1 is considerablylower than for Annotators 2 and 3.6In termsof precision and recall, Annotator 1 again differsfrom the other two.
At 87.5, her recall is higherthan her precision (50.6), while the other annota-tors have considerably higher precision (75.5 and82.4) than recall (62.4 and 52.3).
This indicatesthat Annotator 1 tended to assign higher ratingsthroughout, an impression that is confirmed by Ta-ble 6.
The left two columns show average rat-ings for each annotator over all senses of all to-kens (normalized to values between 0.0 and 1.0 asdescribed in Sec.
3).
The three annotators differwidely in their average ratings, which range from0.285 for Ann.3 to 0.540 for Ann.1.Standard WSD.
We tested the performance ofthe WSD/single model on a standard WSD task,using the same training and testing data as inour subsequent experiments, as described in sec-tion 3.7The model?s accuracy when trained andtested on SemCor was A=77.0%, with a most fre-quent sense baseline of 63.5%.
When trainedand tested on SE-3, the model achieved A=53.0%against a baseline of 44.0%.
When trained andtested on SemCor plus SE-3, the model reached anaccuracy 58.2%, with a baseline of 56.0%.
So onthe combined dataset, the baseline is the averageof the baselines on the individual datasets, whilethe model?s performance falls below the averageperformance on the individual datasets.WSD models for graded sense assignment.Table 5 shows the performance of different mod-els in the task of graded word sense assignment.The first line in Table 5 lists results for the maxi-mum entropy model when used to assign a singlebest sense.
The second line lists the results for6Low J/S implies a closer agreement between two sets ofjudgments.7Note that this constitutes less training data than in theSE-3 task.445by lemma by lemma+sense by lemma+sentenceAnn ?
* ** ?
* ** ?
* ** J/S P R FAnn.1 0.517 100.0 100.0 0.407 75.0 58.3 0.482 27.3 11.5 0.131 50.6 87.5 64.1Ann.2 0.587 100.0 100.0 0.403 68.8 58.3 0.612 38.1 17.2 0.153 75.5 62.4 68.3Ann.3 0.528 100.0 100.0 0.41 77.1 58.3 0.51 21.8 7.8 0.165 82.4 52.3 64.0Avg 0.544 100.0 100.0 0.407 73.6 58.3 0.535 29.0 12.2 0.149 69.5 67.4 65.5Table 4: Human ceiling: one annotator vs. average of the other two annotators.
?, ??
: percentagesignificant at p ?
0.05, p ?
0.01.
Avg: average annotator performanceby lemma by lemma+sense by lemma+sentenceModel ?
* ** ?
* ** ?
* ** J/S P R FWSD/single 0.267 87.5 75.0 0.053 6.3 4.2 0.28 2.8 1.8 0.39 58.7 25.5 35.5WSD/conf 0.396 87.5 87.5 0.177 33.3 18.8 0.401 10.8 3.0 0.164 81.8 37.1 51.0Prototype 0.245 62.5 62.5 0.053 20.8 8.3 0.396 15.3 2.5 0.173 58.4 78.3 66.9Prototype/2 0.292 87.5 87.5 0.086 14.6 4.2 0.478 22.8 7.5 0.164 68.2 63.3 65.7Prototype/N 0.396 100.0 100.0 0.137 22.9 14.6 0.396 15.3 2.5 0.173 82.2 29.9 43.9Prototype/2N 0.465 100.0 100.0 0.168 29.8 23.4 0.478 22.8 7.5 0.164 82.6 30.9 45.0baseline 0.338 87.5 87.5 0.0 0.0 0.0 0.355 10.3 3.0 0.167 79.9 34.5 48.2Table 5: Evaluation: computational models, and baseline.
?, ??
: percentage significant at p ?
0.05,p ?
0.01the same maximum entropy model when classifierconfidence is used as predicted judgment.
The lastline shows the baseline, an adaptation of the mostfrequent sense baseline to the graded case.
Forthis baseline, we computed the relative frequencyof each sense in the training corpus and used thisrelative frequency as the prediction for each testsentence and sense combination.
The WSD/singlemodel remains below the baseline in all evalua-tions except correlation by lemma+sense, whereno rank-based correlation could be computed forthe baseline because it always assigns the samejudgment for a given sense.
WSD/conf shows aperformance slightly above the baseline in all eval-uation measures.
Table 6 lists average ratings, av-eraged over all lemmas, senses, and occurrences,for each model in the two right-hand columns.Prototype models.
Lines 3-6 in Tables 5 and6 show results for Prototype variants.
While eachPrototype and Prototype/2 model only sees pos-itive data annotated for a single sense, the vari-ants with /N (lines 5 and 6) make very limited useof information coming from all senses of a givenlemma.
They normalize judgments for each sen-tence, withassignednorm`,i,t=assigned`,i,t?j?S`assigned`,j,tLine 3 evaluates the Prototype model with first-order vectors.
Its correlation with the gold data issomewhat lower than that of WSD/conf in almostall cases.8The Prototype model deviates strongly8The reason why the average ?
for correlation byfrom both WSD/conf and baseline in having a verygood recall, at 78.3, with lower precision at 58.4,for an overall F-score that is 16 points higher thanthat of WSD/conf .
Both Prototype and Prototype/2have average ratings (Table 6) far above thoseof the WSD models and of the /N variants.
Thesecond-order vector model Prototype/2 has rela-tively low correlation by lemma+sense, while cor-relation by lemma+sentence shows the best per-formance of all models (along with Prototype/2N).Its correlation by lemma+sentence is similar to thelowest correlation by lemma+sentence achievedby a human annotator.
In terms of J/S, thismodel also shows the best performance along withWSD/conf and Prototype/2N.
Both /N variantsachieve very high correlation by lemma.
Corre-lation by lemma+sense for the /N models is be-tween those of Prototype and WSD/conf .
The cor-relation by lemma+sentence is the same with orwithout normalization, as normalization does notchange the ranking of senses of an individual sen-tence.
While Prototype has higher recall than pre-cision, normalization turns it into a model witheven higher precision than WSD/conf but evenlower recall.DiscussionHuman performance.
The evaluation of humanannotators in Table 4 provides a novel analysis ofthe GWS dataset over and above that by Erk et allemma+sense is the same for Prototype and WSD/singlewhile the significance percentage differs greatly is that thePrototype shows negative correlation for some of the senses.446Ann.
avg Model avgAnn.1 0.540 WSD/single 0.163Ann.2 0.345 WSD/conf 0.173Ann.3 0.285 Prototype 0.558Prototype/N 0.143Prototype/2 0.375Prototype/2N 0.143baseline 0.167Table 6: Average judgment for individual annota-tors (transformed) and average rating for models(2009).
Human annotators show very strong cor-relation of their rankings by lemma.
They also hadstrong agreement on rankings by lemma+sense,which ranks occurrences of a lemma by howstrongly they evoke a given sense.
The relativelylow precision and recall in Table 4 confirm thatdifferent annotators use the 5-point scale in differ-ent ways.
A comparison of precision and recallbetween the annotators reflects the fact that An-notator 1 tended to give considerably higher rat-ings than the other two, which is also apparentin the average ratings in Table 6.
Given the rela-tively low F-score achieved by human annotators,judgments by additional annotators could makethe GWS dataset more useful, in that the averagejudgments would not be influenced so strongly byidiosyncrasies in the use of the 5-point scale.
(Psy-cholinguistic experiments using fixed scales typi-cally elicit judgments from 10 or more participantsper item.
)Evaluation measures.
Given the degree of dif-ferences in the absolute values of the human an-notator judgments (Table 4), a rank-based evalu-ation of graded sense assignment models, com-plemented by J/S to evaluate the magnitude ofdifferences between ratings, seems most appro-priate to the data.
Rankings by lemma+senseand by lemma+sentence are especially interest-ing for their potential use in systems that mightuse graded sense assignment as part of a largerpipeline.
Still, the new graded precision and re-call measures allow for a more fine-grained anal-ysis of the performance of models, showing fun-damental differences in the behavior of WSD/confand the Prototype model.
Graded precision andrecall could become even more informative mea-sures with a gold set containing judgments of moreannotators, since then the absolute gold judgmentswould be more reliable.Standard WSD models and vector space mod-els.
The results in Table 5 reflect the compromisebetween the advantage of having competition be-tween categories and the disadvantage of spuriousnegative data: WSD/conf , Prototype/N and Proto-type/2N achieve the highest correlation by lemma,and high precision, while Prototype has much bet-ter recall for an overall higher F-score.
However,as Table 6 shows, Prototype tends to assign highratings across the board, leading to high recall.The much lower average ratings of the /N mod-els explain their higher precision and lower recall:they overshoot less and undershoot more.
The im-provement in correlation for the /N models alsoindicates that Prototype assigns some sentenceshigh ratings for all senses, impacting rankings bylemma and by lemma+sense.The comparison of Prototype and Prototype/2gives us a chance to study effects of feature sparse-ness.
Prototype/2, using second-order vectors thatshould be much less sparse, yields better rankingsthan Prototype.
The average ratings of model Pro-totype/2 (Table 6) are lower than those of Pro-totype (and closer to human average ratings), re-sulting in higher precision and lower recall.
Onepossible reason for the high average ratings ofPrototype is that in sparser (and shorter) vectors,matches in dimensions for high-frequency, rela-tively uninformative context items have greaterimpact.It is interesting to see that WSD/conf performsslightly above the sense frequency baseline in allevaluations, since this is a very familiar picturefrom standard WSD.Prototype/2N shows the overall most favorableperformance in terms of correlation as it i) paysminimal attention to the negative data ii) uses nor-malization to avoid overshooting and iii) compen-sates for sparse data by using second order vectors.For J/S, WSD/conf , Prototype/2, Prototype/2Nand the sense frequency baseline just outperformthe score of the lowest-scoring of the three anno-tators.
In terms of F-score, Prototype shows re-sults very close to human performance.
Interest-ingly, the Prototype model resembles Annotator 1in its precision and recall, while WSD/conf moreresembles Annotators 2 and 3.
None of the mod-els come close to human performance in rankingby lemma+sense, which requires an identificationof the ?typical?
occurrence of a given sense.
Thelow ratings in correlation by lemma+sense indi-cate that the models might be limited by the lackof training data for many of the rarer senses.
In fu-447ture work, we will test how the frequency of sensesin the training data affects the different models.7 ConclusionIn this paper we have done a first study on mod-eling graded annotator judgments on sense appli-cability.
We have discussed evaluation measuresfor models of graded sense assignment, includ-ing new extensions of precision and recall to thegraded case.
A combination of rank-based correla-tion at the level of lemmas, senses, and sentences,Jensen/Shannon divergence, and precision and re-call provided a nuanced picture of the strengthsand weaknesses of different models.
We havetested two types of models: on the one hand astandard binary WSD model using classifier con-fidence as predicted judgments, and on the otherhand several vector space models which compute aprototype vector for each sense in semantic space.These two types of model differ strongly in theirbehavior.
The WSD model shows a similar behav-ior as the baseline, with high precision but low re-call, while the unnormalized version of the vectorspace model has higher recall at lower precision.The results show both the benefits of having com-petition between categories, for improved rank-based correlation and precision, and the problemof spurious negative data in the training set arisingfrom the best-sense methodology.The last two correlation measures, bylemma+sense and by lemma+sentence, yieldmaybe the most insight into the question of theusability of a computational model for gradedword sense assignment: a graded word senseassignment model that is a component of a largersystem could provide useful sense informationeither by ranking occurrences by how stronglythey evoke a sense, or by ranking senses by howstrongly they apply to a given occurrence.
Thereis room for improvement however as systemperformance is well below that of humans.
Inthe future we plan to investigate features that aremore informative for making graded judgments.Second, the vector space model we used wasvery simple; it might be worthwhile to test moresophisticated one-class classifiers (Marsland,2003; Sch?olkopf et al, 2000).Acknowledgments.
We acknowledge supportfrom the UK Royal Society for a Dorothy HodgkinFellowship to the second author.ReferencesM.
Carpuat and D. Wu.
2007.
Improving statisticalmachine translation using word sense disambigua-tion.
In Proceedings of EMNLP-CoNLL 2007, pages61?72, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Y.
S. Chan and H. T. Ng.
2005.
Word sense disam-biguation with distribution estimation.
In Proceed-ings of IJCAI 2005, pages 1010?1015, Edinburgh,Scotland.Y.
S. Chan, H. T. Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In Proceedings of ACL?07, Prague, Czech Re-public, June.D.
A. Cruse.
2000.
Aspects of the microstructure ofword meanings.
In Y. Ravin and C. Leacock, edi-tors, Polysemy: Theoretical and Computational Ap-proaches, pages 30?51.
OUP, Oxford, UK.W.
Daelemans and A.
Van den Bosch.
2005.
Memory-Based Language Processing.
Cambridge UniversityPress, Cambridge, UK.K.
Erk, D. McCarthy, and N. Gaylord.
2009.
Inves-tigations on word senses and word usages.
In Pro-ceedings of ACL-09, Singapore.C.
Fellbaum, editor.
1998.
WordNet, An ElectronicLexical Database.
The MIT Press, Cambridge, MA.P.
Hanks.
2000.
Do word meanings exist?
Computersand the Humanities, 34(1-2):205?215(11).E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90% solu-tion.
In Proceedings of the HLT-NAACL 2006 work-shop on Learning word meaning from non-linguisticdata, New York City, USA.
Association for Compu-tational Linguistics.N.
Ide and Y. Wilks.
2006.
Making sense aboutsense.
In E. Agirre and P. Edmonds, editors,Word Sense Disambiguation, Algorithms and Appli-cations, pages 47?73.
Springer.A.
Kilgarriff.
2006.
Word senses.
In E. Agirreand P. Edmonds, editors, Word Sense Disambigua-tion, Algorithms and Applications, pages 29?46.Springer.M.
Lapata and C. Brew.
2004.
Verb class disambigua-tion using informative priors.
Computational Lin-guistics, 30(1):45?75.G.
Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.D.
Lin.
1993.
Principle-based parsing without over-generation.
In Proceedings of ACL?93, Columbus,Ohio, USA.448S.
Marsland.
2003.
Novelty detection in learning sys-tems.
Neural computing surveys, 3:157?195.D.
McCarthy and R. Navigli.
2007.
SemEval-2007task 10: English lexical substitution task.
In Pro-ceedings of SemEval-2007, pages 48?53, Prague,Czech Republic.D.
McCarthy, B. Keller, and J. Carroll.
2003.
De-tecting a continuum of compositionality in phrasalverbs.
In Proceedings of the ACL 03 Workshop:Multiword expressions: analysis, acquisition andtreatment, pages 73?80.D.
McCarthy.
2006.
Relating WordNet senses forword sense disambiguation.
In Proceedings of theACL Workshop on Making Sense of Sense: Bring-ing Psycholinguistics and Computational Linguis-tics Together, pages 17?24, Trento, Italy.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.The Senseval-3 English lexical sample task.
In Pro-ceedings of SensEval-3, Barcelona, Spain.G.
A. Miller, C. Leacock, R. Tengi, and R. T Bunker.1993.
A semantic concordance.
In Proceedings ofthe ARPA Workshop on Human Language Technol-ogy, pages 303?308.
Morgan Kaufman.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL?08- HLT, pages 236?244, Columbus, Ohio.R.
Navigli, K. C. Litkowski, and O. Hargraves.
2007.SemEval-2007 task 7: Coarse-grained English all-words task.
In Proceedings of SemEval-2007, pages30?35, Prague, Czech Republic.R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-mance.
In Proceedings of COLING-ACL 2006,pages 105?112, Sydney, Australia.R.
Navigli.
2009.
Word sense disambiguation: a sur-vey.
ACM Computing Surveys, 41(2):1?69.S.
Pad?o and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.M.
Palmer, H. Trang Dang, and C. Fellbaum.
2007.Making fine-grained and coarse-grained sense dis-tinctions, both manually and automatically.
NaturalLanguage Engineering, 13:137?163.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proceedings of KDD?02.P.
Pantel.
2005.
Inducing ontological co-occurrencevectors.
In Proceedings of ACL?05, Ann Arbor,Michigan.S.
Patwardhan and T. Pedersen.
2006.
Using wordnet-based context vectors to estimate the semantic relat-edness of concepts.
In Proceedings of the EACL 06Workshop: Making Sense of Sense: Bringing Psy-cholinguistics and Computational Linguistics To-gether, Trento, Italy.G.
Ramakrishnan, B.P.
Prithviraj, A. Deepa, P. Bhat-tacharyya, and S. Chakrabarti.
2004.
Soft wordsense disambiguation.
In Proceedings of GWC 04,Brno, Czech Republic.B.
Sch?olkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt.
2000.
Support vector methodfor novelty detection.
Advances in neural informa-tion processing systems, 12.H.
Sch?utze.
1998.
Automatic word sense discrimina-tion.
Computational Linguistics, 24(1).C.
Stokoe.
2005.
Differentiating homonymy and pol-ysemy in information retrieval.
In Proceedings ofHLT/EMNLP-05, pages 403?410, Vancouver, B.C.,Canada.D.
H. Tuggy.
1993.
Ambiguity, polysemy and vague-ness.
Cognitive linguistics, 4(2):273?290.449
