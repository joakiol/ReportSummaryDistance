Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 13?21,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsImproving Arabic Dependency Parsingwith Lexical and Inflectional Morphological FeaturesYuval Marton, Nizar Habash and Owen RambowCenter for Computational Learning Systems (CCLS)Columbia University{ymarton,habash,rambow}@ccls.columbia.eduAbstractWe explore the contribution of different lexi-cal and inflectional morphological features todependency parsing of Arabic, a morpholog-ically rich language.
We experiment with allleading POS tagsets for Arabic, and introducea few new sets.
We show that training theparser using a simple regular expressive ex-tension of an impoverished POS tagset withhigh prediction accuracy does better than us-ing a highly informative POS tagset with onlymedium prediction accuracy, although the lat-ter performs best on gold input.
Using con-trolled experiments, we find that definiteness(or determiner presence), the so-called phi-features (person, number, gender), and undi-acritzed lemma are most helpful for Arabicparsing on predicted input, while case andstate are most helpful on gold.1 IntroductionParsers need to learn the syntax of the modeled lan-guage, in order to project structure on newly seensentences.
Parsing model design aims to come upwith features that best help parsers to learn the syn-tax and choose among different parses.
One aspectof syntax, which is often not explicitly modeled inparsing, involves morphological constraints on syn-tactic structure, such as agreement.
In this paper, weexplore the role of morphological features in pars-ing Modern Standard Arabic (MSA).
For MSA, thespace of possible morphological features is fairlylarge.
We determine which morphological featureshelp and why, and we determine the upper bound fortheir contribution to parsing quality.We first present the corpus we use (?2), then rel-evant Arabic linguistic facts (?3); we survey relatedwork (?4), describe our experiments (?5), and con-clude with analysis of parsing error types (?6).2 CorpusWe use the Columbia Arabic Treebank (CATiB)(Habash and Roth, 2009).
Specifically, we use theportion converted from part 3 of the Penn ArabicTreebank (PATB) (Maamouri et al, 2004) to theCATiB format, which enriches the CATiB depen-dency trees with full PATB morphological informa-tion.
CATiB?s dependency representation is basedon traditional Arabic grammar and emphasizes syn-tactic case relations.
It has a reduced POS tagset(with six tags only), but a standard set of eightdependency relations: SBJ and OBJ for subjectand (direct or indirect) object, respectively, (whetherthey appear pre- or post-verbally); IDF for the idafa(possessive) relation; MOD for most other modifica-tions; and other less common relations that we willnot discuss here.
For more information, see (Habashand Roth, 2009).
The CATiB treebank uses the wordsegmentation of the PATB.1 It splits off several cat-egories of orthographic clitics, but not the definitearticle ?
@ Al.
In all of the experiments reported inthis paper, we use the gold segmentation.
An exam-ple CATiB dependency tree is shown in Figure 1.23 Relevant Linguistic ConceptsMorphemes: At a shallow level, Arabic words canbe described in terms of their morphemes.
In ad-dition to concatenative prefixes and suffixes, Ara-1Tokenization involves further decisions on the segmentedtoken forms, such as spelling normalization.2All Arabic transliterations are presented in the HSBtransliteration scheme (Habash et al, 2007).13Figure 1: CATiB.
?JJ???
@ ?J.
J???
@ ??
?J?Y?
@ I.
KA??@?k.
?
P ??
?Kt?ml zwj~ AlkAtb Al?ky~ fy Almktb~ AlwTny~ ?The writer?ssmart wife works at the national library.?
(Annotation example)VRB??
?K t?ml?works?SBJNOM?k.
?
P zwj~?wife?IDFNOMI.
KA??
@ AlkAtb?the-writer?MODNOM?J?Y?
@ Al?ky~?smart?MODPRT??
fy?in?OBJNOM?J.
J???
@ Almktb~?library?MODNOM?JJ???
@ AlwTny~?national?bic has templatic morphemes called root and pat-tern.
For example, the word 	??J.
KA?Kyu+kAtib+uwn?they correspond?
has one prefix and one suffix, inaddition to a stem composed of the root H. H?
k-t-b?writing related?
and the pattern 1A2i3.
3Lexeme and Features: At a deeper level, Arabicwords can be described in terms of sets of inflec-tional and lexical morphological features.
We firstdiscuss lexical features.
The set of word forms thatonly vary inflectionally among each other is calledthe lexeme.
A lemma is a particular word form usedto represent, or cite, the lexeme word set.
For ex-ample, verb lemmas are third person masculine sin-gular perfective.
We explore using both diacritizedlemma, and undiacritized lemma (lmm).
Just as thelemma abstracts over inflectional morphology, theroot abstracts over both inflectional and derivationalmorphology and thus provides a deeper level of lex-ical abstraction than the lemma.
The pattern featureis the pattern of the lemma of the lexeme, not of theword form.The inflectional morphological features4 definethe dimensions of Arabic inflectional morphology,or the space of variations of a particular word.PATB-tokenized words vary along nine dimensions:3The digits in the pattern correspond to the positions rootradicals are inserted.4The inflectional features we use in this paper are form-based (illusory) as opposed to functional features (Smr?, 2007).We plan to work with functional features in the future.GENDER and NUMBER (for nominals and verbs);PERSON, ASPECT, VOICE and MOOD (for verbs);and CASE, STATE, and the attached definite articleproclitic DET (for nominals).
The inflectional fea-tures abstract away from the specifics of morphemeforms, since they can affect more than one mor-pheme in Arabic.
For example, changing the valueof the aspect feature in the example above from im-perfective to perfective yields the word form @?J.
KA?kAtab+uwA ?they corresponded?, which differs interms of prefix, suffix and pattern.Inflectional features interact with syntax in twoways.
First, there are agreement features: twowords in a sentence which are in a specific syn-tactic configuration have the same value for a spe-cific set of features.
In MSA, we have subject-verb agreement on PERSON, GENDER, and NUMBER(but NUMBER only if the subject precedes the verb),and we have noun-adjective agreement in PERSON,NUMBER, GENDER, and DET.5 Second, morphol-ogy can show a specific syntactic configuration ona single word.
In MSA, we have CASE and STATEmarking.
Different types of dependents have differ-ent CASE; for example, verbal subjects are alwaysmarked NOMINATIVE.
CASE and STATE are rarelyexplicitly manifested in undiacritized MSA.Lexical features do not participate in syntacticconstraints on structure as inflectional features do.Instead, bilexical dependencies are used in parsingto model semantic relations which often are the onlyway to disambiguate among different possible syn-tactic structures; lexical features provide a way ofreducing data sparseness through lexical abstraction.We compare the effect on parsing of different sub-sets of lexical and inflectional features.
Our hypoth-esis is that the inflectional features involved in agree-ment and the lexical features help parsing.The core POS tagsets: Words also have associ-ated part-of-speech (POS) tags, e.g., ?verb?, whichfurther abstract over morphologically and syntac-tically similar lexemes.
Traditional Arabic gram-mars often describe a very general three-way dis-tinction into verbs, nominals and particles.
In com-parison, the tagset of the Buckwalter Morphologi-cal Analyzer (Buckwalter, 2004) used in the PATBhas a core POS set of 44 tags (before morphologi-5We do not explicitly address here agreement phenomenathat require more complex morpho-syntactic modeling.
Theseinclude adjectival modifiers of irrational (non-human) pluralnominals, and pre-nominal number modifiers.14cal extension).
Henceforth, we refer to this tagsetas CORE44.
Cross-linguistically, a core set con-taining around 12 tags is often assumed, including:noun, proper noun, verb, adjective, adverb, preposi-tion, particles, connectives, and punctuation.
Hence-forth, we reduce CORE44 to such a tagset, and dubit CORE12.
The CATIB6 tagset can be viewed asa further reduction, with the exception that CATIB6contains a passive voice tag; however, it constitutesonly 0.5% of the tags in the training.Extended POS tagsets: The notion of ?POStagset?
in natural language processing usually doesnot refer to a core set.
Instead, the Penn EnglishTreebank (PTB) uses a set of 46 tags, including notonly the core POS, but also the complete set of mor-phological features (this tagset is still fairly smallsince English is morphologically impoverished).
Inmodern standard Arabic (MSA), the correspondingtype of tagset (core POS extended with a completedescription of morphology) would contain upwardsof 2,000 tags, many of which are extremely rare (inour training corpus of about 300,000 words, we en-counter only 430 of such POS tags with completemorphology).
Therefore, researchers have proposedtagsets for MSA whose size is similar to that of theEnglish PTB tagset, as this has proven to be a use-ful size computationally.
These tagsets are hybridsin the sense that they are neither simply the corePOS, nor the complete morphological tagset, but in-stead they choose certain morphological features toinclude along with the core POS tag.The following are the various tagsets we comparein this paper: (a) the core POS tagsets CORE44 andthe newly introduced CORE12; (b) CATiB treebanktagset (CATIB6) (Habash and Roth, 2009); and itsnewly introduced extension, CATIBEX, by greedyregular expressions indicating particular morphemessuch as the prefix ?
@ Al+ or the suffix 	??
+wn.6(c) the PATB full tagset (BW), size ?2000+ (Buck-walter, 2004); and two extensions of the PATB re-duced tagset (PENN POS, a.k.a.
RTS, size 24), bothoutperforming it: (d) Kulick et al (2006)?s tagset(KULICK), size ?43, one of whose most impor-tant extensions is the marking of the definite arti-cle clitic, and (e) Diab and BenAjiba (2010)?s EX-TENDED RTS tagset (ERTS), which marks gender,number and definiteness, size ?134; Besides usingmorphological information to extend POS tagsets,6Inspired by a similar extension in Habash and Roth (2009).we explore using it in separate features in parsingmodels.
Following this exploration, we also extendCORE12, producing (f) CORE12EX (see Section 5for details).4 Related WorkMuch work has been done on the use of morpho-logical features for parsing of morphologically richlanguages.
Collins et al (1999) report that an op-timal tagset for parsing Czech consists of a basicPOS tag plus a CASE feature (when applicable).This tagset (size 58) outperforms the basic CzechPOS tagset (size 13) and the complete tagset (size?3000+).
They also report that the use of gender,number and person features did not yield any im-provements.
We get similar results for CASE in thegold experimental setting but not when using pre-dicted POS tags (POS tagger output).
This may bea result of CASE tagging having a lower error ratein Czech (5.0%) (Hajic?
and Vidov?-Hladk?, 1998)compared to Arabic (?14.0%, see Table 3).
Simi-larly, Cowan and Collins (2005) report that the useof a subset of Spanish morphological features (num-ber for adjectives, determiners, nouns, pronouns,and verbs; and mode for verbs) outperforms othercombinations.
Our approach is comparable to theirwork in terms of its systematic exploration of thespace of morphological features.
We also find thatthe number feature helps for Arabic.
Looking at He-brew, a Semitic language related to Arabic, Tsarfatyand Sima?an (2007) report that extending POS andphrase structure tags with definiteness informationhelps unlexicalized PCFG parsing.As for work on Arabic, results have been reportedon PATB (Kulick et al, 2006; Diab, 2007), thePrague Dependency Treebank (PADT) (Buchholzand Marsi, 2006; Nivre, 2008) and the ColumbiaArabic Treebank (CATiB) (Habash and Roth, 2009).Besides the work we describe in ?3, Nivre (2008)reports experiments on Arabic parsing using hisMaltParser (Nivre et al, 2007), trained on the PADT.His results are not directly comparable to ours be-cause of the different treebanks representations andtokenization used, even though all our experimentsreported here were performed using the MaltParser.Our results agree with previous published work onArabic and Hebrew in that marking the definite ar-ticle is helpful for parsing.
However, we go beyondprevious work in that we also extend this morpho-logically enhanced feature set to include additional15lexical and inflectional morphological features.
Pre-vious work with MaltParser in Russian, Turkish andHindi showed gains with case but not with agree-ment features (Nivre et al, 2008; Eryigit et al, 2008;Nivre, 2009).
Our work is the first to show gainsusing agreement in MaltParser and in Arabic depen-dency parsing.5 Experiments5.1 Experimental SpaceWe examined a large space of settings including thefollowing: (a) the contribution of POS tagsets to theparsing quality, as a function of the amount of in-formation encoded in the tagset; (b) parsing perfor-mance on gold vs. predicted POS and morphologi-cal feature values for all models; (c) prediction accu-racy of each POS tagset and morphological feature;(d) the contribution of numerous morphological fea-tures in a controlled fashion; and (e) the contributionof certain feature and POS tagset combinations.
Allresults are reported mainly in terms of labeled at-tachment accuracy (parent word and the dependencyrelation to it).
Unlabeled attachment accuracy andlabel accuracy are also given, space permitting.5.2 ParserFor all experiments reported here we used the syn-tactic dependency parser MaltParser v1.3 (Nivre,2003; Nivre, 2008; K?bler et al, 2009) ?
atransition-based parser with an input buffer and astack, using SVM classifiers to predict the nextstate in the parse derivation.
All experiments weredone using the Nivre "eager" algorithm.7 Wetrained the parser on the training portion of PATBpart 3 (Maamouri et al, 2004).
We used the samesplit as in Zitouni et al (2006) for dev/test, and keptthe test unseen during training.There are five default attributes, in the MaltParserterminology, for each token in the text: word ID(ordinal position in the sentence), word form, POS7Nivre (2008) reports that non-projective and pseudo-projective algorithms outperform the "eager" projective algo-rithm in MaltParser; however, our training data did not containany non-projective dependencies, so there was no point in us-ing these algorithms.
The Nivre "standard" algorithm is alsoreported to do better on Arabic, but in a preliminary experimen-tation, it did slightly worse than the "eager?
one.
This couldbe due to high percentage of right branching (left headed struc-tures) in our Arabic training set, an observation already notedin Nivre (2008).tag, head (parent word ID), and deprel (the depen-dency relation between the current word and its par-ent).
There are default MaltParser features (in themachine learning sense),8 which are the values offunctions over these attributes, serving as input tothe MaltParser internal classifiers.
The most com-monly used feature functions are the top of the in-put buffer (next word to process, denoted buf[0]), ortop of the stack (denoted stk[0]); following items onbuffer or stack are also accessible (buf[1], buf[2],stk[1], etc.).
Hence MaltParser features are de-fined as POS tag at top of the stack, word form attop of the buffer, etc.
K?bler et al (2009) de-scribe a ?typical?
MaltParser model configurationof attributes and features.9 Starting with it, ina series of initial controlled experiments, we set-tled on using buf[0], buf[1], stk[0], stk[1] for thewordform, and buf[0], buf[1], buf[2], buf[3], stk[0],stk[1], stk[2] for the POS tag.
For features of allnew MaltParser-attributes (discussed later), we usedbuf[0] and stk[0].
We did not change the featuresfor the deprel.
This new MaltParser configurationresulted in gains of 0.3-1.1% in labeled attachmentaccuracy (depending on the POS tagset) over thedefault MaltParser configuration.
We also exper-imented with using normalized word forms (AlifMaqsura conversion to Ya, and hamza removal fromeach Alif ) as is common in parsing and statisticalmachine translation literature.
This resulted in asmall decrease in performance (0.1-0.2% in labeledattachment accuracy).
We settled on using the non-normalized word form.
All experiments reported be-low were conducted using this new configuration.5.3 Parsing quality as a function of POS tagrichnessWe turn first to the contribution of POS informationto parsing quality, as a function of the amount of in-formation encoded in the POS tagset.
A first roughestimation for the amount of information is the ac-tual tagset size, as it appears in the training data.For this purpose we compared POS tagsets basedon, or closely inspired by, previously publishedwork.
These sets are typically morphologically-enriched (marking the existence of a determiner inthe word, person, gender, number, etc.).
The num-8The terms ?feature?
and ?attribute?
are over loaded in theliterature.
We use them in the linguistic sense, unless specifi-cally noted otherwise, e.g., ?MaltParser feature(s)?.9It is slightly different from the default configuration.16ber of tag types occurring in the training data fol-low each tagset in parentheses: BW (430 tags), ERTS(134 tags), KULICK (32 tags), and the smallest POStagset published: CATIB6 (6 tags).
In optimal con-ditions (using gold POS tags), the richest tagset(BW) is indeed the best performer (84.02%), and thepoorest (CATIB6) is the worst (81.04%).
Mid-sizetagsets are in the high 82%, with the notable ex-ception of KULICK, which does better than ERTS,in spite of having 1/4 the tagset size; moreover, it isthe best performer in unlabeled attachment accuracy(85.98%), in spite of being less than tenth the size ofBW.
Our extended mid-size tagset, CATIBEX, was amid-level performer as expected.In order to control the level of morphological andlexical information in the POS tagset, we used theabove-mentioned additional tagsets: CORE44 (40tags), and CORE12 (12 tags).
Both were alsomid-size mid-level performers (in spite of contain-ing no morphological extension), with CORE12 do-ing slightly better.
See Table 1 columns 2-4.5.4 Predicted POS tagsSo far we discussed optimal (gold) conditions.
Butin practice, POS tags are annotated by automatic tag-gers, so parsers get predicted POS tags as input, asopposed to gold (human-annotated) tags.
The moreinformative the tagset, the less accurate the tag pre-diction might be, so the effect on overall parsingquality is unclear.
Therefore, we repeated the exper-iments above with POS tags predicted by the Mor-phological Analysis and Disambiguation for Arabic(MADA) toolkit (Habash and Rambow, 2005).
SeeTable 1, columns 5-7.
It turned out that BW, thebest gold performer, with lowest POS prediction ac-curacy (81.8%), suffered the biggest drop (11.38%)and was the worst performer with predicted tags.The simplest tagset, CATIB6, and its extension, CAT-IBEX, benefited from the highest POS prediction ac-curacy (97.7%), and their performance suffered theleast.
CATIBEX was the best performer with pre-dicted POS tags.
Performance drop and POS pre-diction accuracy are given in columns 8 and 9, re-spectively.
Next, we augmented the parsing modelswith inflectional and lexical morphological features.5.5 Inflectional featuresExperimenting with inflectional morphological fea-tures is especially important in Arabic parsing, sinceArabic is morphologically rich.
In order to furtherexplore the contribution of inflectional and lexicalmorphological information in a controlled manner,we focused on the best performing core POS tagset,CORE12 as baseline; using three different setups, weadded nine morphological features, extracted fromMADA: DET, PERSON, ASPECT, VOICE, MOOD,GENDER, NUMBER, STATE, and CASE.
In setupAll , we augmented the baseline model with all nineMADA features (as nine additional MaltParser at-tributes); in setup Sep , we augmented the baselinemodel with each of the MADA features, one at atime, separately; and in setup Greedy , we com-bined them in a greedy heuristic (since the entirefeature space is too vast to exhaust): starting withthe most gainful feature from Sep, adding the nextmost gainful feature, keeping it as additional Malt-Parser attribute if it helped, or discarding it other-wise, and repeating this heuristics through the leastgainful feature.
We also augmented the same base-line CORE12 model with a manually constructed listof surface affixes (e.g., Al+, +wn, ~) as additionalMaltParser attributes (LINGNGRAMS).
This list wasalso in the base of the CATIBEX extension; it is lin-guistically informed, yet represents a simple (albeitshallow) alternative to morphological analysis.
Re-sults are given in Table 2.Somewhat surprisingly, setup All hurts perfor-mance on the predicted input.
This can be explainedif one examines the prediction accuracy of each fea-ture (Table 3).
Features which are not predictedwith very high accuracy, such as CASE (86.3%),can dominate the negative contribution, even thoughthey are principle top contributors in optimal (gold)conditions (see discussion below).
The determinerfeature (DET), followed by the STATE (constructstate, idafa) feature, were top individual contribu-tors in setup Sep.
Adding DET and all the so-calledphi-features (PERSON, NUMBER, GENDER) in theGreedy setup, yielded 1.43% gain over the CORE12baseline.
Adding LINGNGRAMS yielded a 1.19%gain over the CORE12 baseline.We repeated the same setups (All, Sep, andGreedy) with gold POS tags, to examine the contri-bution of the morphological features in optimal con-ditions.
Here CASE, followed by STATE and DET,were the top contributors.
Performance of CASE isthe notable difference from the predicted conditionsabove.
Surprisingly, only CASE and STATE helped inthe Greedy setup, although one might expect that thephi features help too.
(See lower half of Table 2).17Table 1: Parsing performance with each POS tagset, on gold and predicted input.
labeled = labeled attachment accuracy (depen-dency + relation).
unlabeled = unlabeled attachment accuracy (dependency only).
label acc = relation label prediction accuracy.labeled diff = difference between labeled attachment accuracy on gold and predicted input.
POS acc = POS tag prediction accuracy.tagset gold predicted gold-pred.
POS tagsetlabeled unlabled label acc.
labeled unlabled label acc.
labeled diff.
acc.
sizeCATIB6 81.04 83.66 92.59 78.31 82.03 90.55 -2.73 97.7 6CATIBEX 82.52 84.97 93.40 79.74 83.30 91.44 -2.78 97.7 44CORE12 82.92 85.40 93.52 78.68 82.48 90.63 -4.24 96.3 12CORE44 82.71 85.17 93.28 78.39 82.16 90.36 -4.32 96.1 40ERTS 82.97 85.23 93.76 78.93 82.56 90.96 -4.04 95.5 134KULICK 83.60 85.98 94.01 79.39 83.15 91.14 -4.21 95.7 32BW 84.02 85.77 94.83 72.64 77.91 86.46 -11.38 81.8 430Table 2: CORE12 POS tagset with morphological features.
Left half: Using predicted POS tags.
In it: Top part: Adding allnine features to CORE12.
Second part: Adding each feature separately, comparing difference from CORE12+madafeats, predicted(second part).
Third part: Greedily adding best features from third part, predicted; difference from previous successful greedy step.Bottom part: Surface affixes (leading and trailing character n-grams).
Right half: Left half repeated with gold tags.set predicted POS and features: gold POS and features:-up CORE12+.
.
.
labeled diff.
unlabeled CORE12+.
.
.
labeled diff.
unlabeledAll (baseline repeated) 78.68 ?
82.48 (baseline repeated) 82.92 ?
85.40+madafeats 77.91 -0.77 82.14 +madafeats 85.15 2.23 86.61Sep+DET 79.82 1.14 83.18 +CASE 84.61 1.69 86.30+STATE 79.34 0.66 82.85 +STATE 84.15 1.23 86.38+GENDER 78.75 0.07 82.35 +DET 83.96 1.04 86.21+PERSON 78.74 0.06 82.45 +NUMBER 83.08 0.16 85.50+NUMBER 78.66 -0.02 82.39 +PERSON 83.07 0.15 85.41+VOICE 78.64 -0.04 82.41 +VOICE 83.05 0.13 85.42+ASPECT 78.60 -0.08 82.39 +MOOD 83.05 0.13 85.47+MOOD 78.54 -0.14 82.35 +ASPECT 83.01 0.09 85.43+CASE 75.81 -2.87 80.24 +GENDER 82.96 0.04 85.24Greedy+DET+STATE 79.42 -0.40 82.84 +CASE+STATE 85.37 0.76 86.88+DET+GENDER 79.90 0.08 83.20 +CASE+STATE+DET 85.18 -0.19 86.66+DET+GENDER+PERSON 79.94 0.04 83.21 +CASE+STATE+NUMBER 85.36 -0.01 86.87+DET+PHI 80.11 0.17 83.29 +CASE+STATE+PERSON 85.27 -0.10 86.76+DET+PHI+VOICE 79.96 -0.15 83.18 +CASE+STATE+VOICE 85.25 -0.12 86.76+DET+PHI+ASPECT 80.01 -0.10 83.20 +CASE+STATE+MOOD 85.23 -0.14 86.72+DET+PHI+MOOD 80.03 -0.08 83.21 +CASE+STATE+ASPECT 85.23 -0.14 86.78?
+CASE+STATE+GENDER 85.26 -0.11 86.75+NGRAMSLING 79.87 1.19 83.21 +NGRAMSLING 84.02 1.10 86.165.6 Lexical featuresNext, we experimented with adding morpholog-ical features involving semantic abstraction tosome degree: the diacritized LEMMA (abstractingaway from inflectional information, and indicat-ing active/passive voice due to diacritization in-formation), the undiacritized lemma (LMM), theROOT (further abstraction indicating ?core?
pred-icate or action), and the PATTERN (a generallycomplementary abstraction, often indicating cau-sation and reflexiveness).
We experimented withthe same setups as above: All, Sep, and Greedy.Adding all four features yielded a minor gain insetup All.
LMM was the best single contributor(1.05%), closely followed by ROOT (1.03%) in Sep.CORE12+LMM+ROOT+LEMMA was the best greedycombination (79.05%) in setup Greedy.
See Table 4.5.7 Putting it all togetherWe further explored whether morphological datashould be added to an Arabic parsing model asstand-alone machine learning features, or shouldthey be used to enhance and extend a POS tagset.We created a new POS tagset, CORE12EX, size81(see bottom of Table 3), by extending the CORE12tagset with the features that most improved the18CORE12 baseline: DET and the phi features.
ButCORE12EX did worse than its non-extended (butfeature-enhanced) counterpart, CORE12+DET+PHI.Another variant, CORE12EX+DET+PHI, whichused both the extended tagset and the additionalDET and phi features, did not improve overCORE12+DET+PHI either.Following the results in Table 2, we addedthe affix features NGRAMSLING (which provedto help the CORE12 baseline) to the best aug-mented CORE12+DET+PHI model, dubbing the newmodel CORE12+DET+PHI+NGRAMSLING, but per-formance dropped here too.
We greedily augmentedCORE12+DET+PHI with lexical features, and foundthat the undiacritzed lemma (LMM) improved per-formance on predicted input (80.23%).
In order totest whether these findings hold with other tagsets,we added the winning features (DET+PHI, with andwithout LMM) to the best POS tagset in predictedconditions, CATIBEX.
Both variants yielded gains,with CATIBEX+DET+PHI+LMM achieving 80.45%accuracy, the best result on predicted input.5.8 Validating Results on Unseen Test SetOnce experiments on the development set (PATB3-DEV) were done, we ran the best performing mod-els on a previously unseen test set ?
the test split ofpart 3 of the PATB (PATB3-TEST).
Table 6 showsthat the same trends held on this set too, with evengreater relative gains, up to 1.77% absolute gains.Table 3: Feature prediction accuracy and set sizes.
* = The setincludes a "N/A" value.feature acc set sizenormalized word form (A,Y) 99.3 29737non-normalized word form 98.9 29980NGRAMSLING preffix 100.0 8NGRAMSLING suffix 100.0 20DET 99.6 3*PERSON 99.1 4*ASPECT 99.1 5*VOICE 98.9 4*MOOD 98.6 5*GENDER 99.3 3*NUMBER 99.5 4*STATE 95.6 4*CASE 86.3 5*ROOT 98.4 9646PATTERN 97.0 338LEMMA (diacritized) 96.7 16837LMM (undiacritized lemma) 98.3 15305CORE12EX 96.0 81Table 4: Lexical morpho-semantic features.
Top part: Addingeach feature separately; difference from CORE12, predicted.Bottom part: Greedily adding best features from previous part,predicted; difference from previous successful greedy step.POS tagset labeled diff.
unlab.
labelAllCORE12 (repeated) 78.68 ?
82.48 90.63CORE12+LMM+ROOT+LEMMA+PATTERN78.85 0.17 82.46 90.82SepCORE12+lmm 78.96 1.05 82.54 90.80CORE12+ROOT 78.94 1.03 82.64 90.72CORE12+LEMMA 78.80 0.89 82.42 90.71CORE12+PATTERN 78.59 0.68 82.39 90.60GreedyCORE12+LMM+ROOT 79.04 0.08 82.63 90.86CORE12+LMM+ROOT+LEMMA79.05 0.01 82.63 90.87CORE12+LMM+ROOT+PATTERN78.93 -0.11 82.58 90.82Table 6: Results on PATB3-TEST for models which performedbest on PATB3-DEV ?
predicted input.POS tagset labeled diff.
unlab.
labelCORE12 77.29 ?
81.04 90.05CORE12+DET+PHI 78.57 1.28 81.66 91.09CORE12+DET+PHI+LMM 79.06 1.77 82.07 91.376 Error AnalysisFor selected feature sets, we look at the overall er-ror reduction with respect to the CORE12 baseline,and see what dependency relations particularly profitfrom that feature combination: What dependenciesachieve error reductions greater than the average er-ror reduction for that feature set over the whole cor-pus.
We investigate dependencies by labels, and forMOD we also investigate by the POS label of the de-pendent node (so MOD-P means a preposition nodeattached to a governing node using a MOD arc).DET: As expected, it particularly helps IDF andMOD-N.
The error reduction for IDF is 19.3%!STATE: Contrary to na?ve expectations, STATEdoes not help IDF, but instead increases error by9.4%.
This is presumably because the feature doesnot actually predict construct state except when con-struct state is marked explicitly, but this is rare.DET+PHI: The phi features are the only subject-verb agreement features, and they are additionalagreement features (in addition to definiteness) fornoun-noun modification.
Indeed, relative to justadding DET, we see the strongest increases in thesetwo dependencies, with an additional average in-19Table 5: Putting it all togetherPOS tagset inp.qual.
labeled diff.
unlabeled label Acc.CORE12+DET+PHI (repeated) predicted 80.11 0.17 83.29 91.82CORE12+DET+PHI gold 84.20 -0.95 86.23 94.49CORE12EX predicted 78.89 -1.22 82.38 91.17CORE12EX gold 83.06 0.14 85.26 93.80CORE12EX+DET+PHI predicted 79.19 -0.92 82.52 91.39CORE12+DET+PHI+NGRAMSLING predicted 79.77 -0.34 83.03 91.66CORE12+DET+PHI+LMM predicted 80.23 0.12 83.34 91.94CORE12+DET+PHI+LMM+ROOT predicted 80.10 -0.13 83.25 91.84CORE12+DET+PHI+LMM+PATTERN predicted 80.03 -0.20 83.15 91.77CATIBEX+DET+PHI predicted 80.00 0.26 83.29 91.81CATIBEX+DET+PHI+LMM predicted 80.45 0.71 83.65 92.03crease for IDF (presumably because certain N-Nmodifications are rejected in favor of IDFs).
Allother dependencies remain at the same level as withonly DET.LMM, ROOT, LEMMA: These features abstractover the word form and thus allow generalizations inbilexical dependecies, which in parsing stand in forsemantic modeling.
The strongest boost from thesefeatures comes from MOD-N and MOD-P, whichis as expected since these dependencies are highlyambiguous, and MOD-P is never helped by the mor-phological features.DET+PHI+LMM: This feature combination yieldsgains on all main dependency types (SBJ, OBJ,IDF, MOD-N, MOD-P, MOD-V).
But the contri-bution from the inflectional and lexical features areunfortunately not additive.
We also compare the im-provement contributed just by LMM as compared toDET and PHI.
This improvement is quite small, butwe see that MOD-N does not improve (in fact, itgets worse ?
presumably because there are too manyfeatures), while MOD-P (which is not helped by themorphological features) does improve.
Oddly, OBJalso improves, for which we have no explanation.When we turn to our best-performing configura-tion, CATIBEX with the added DET, phi features(PERSON, NUMBER, GENDER), and LMM, we seethat this configuration improves over CORE12 withthe same features for two dependency types only:SBJ and MOD-N These are exactly the two typesfor which agreement features are useful, and boththe features DET+PHI and the CATIBEX POS tagsetrepresent information for agreement.
The questionarises why this information is not redundant.
Wespeculate that the fact that we are learning differ-ent classifiers for different POS tags helps Malt-Parser learn attachment decisions which are specificto types of dependent node morphology.In summary, our best performing configurationyields an error reduction of 8.3% over the core POStag (CORE12).
SBJ errors are reduced by 13.3%,IDF errors by 17.7%, and MOD-N errors by 14.9%.Error reduction for OBJ, MOD-P, and MOD-V areall less than 4%.
We note that the remaining MOD-P errors make up 6.2% of all dependency relations,roughly one third of remaining errors.7 Conclusions and Future WorkWe explored the contribution of different inflec-tional and lexical features to dependency parsing ofArabic, under gold and predicted POS conditions.While more informative features (e.g., richer POStags) yield better parsing quality in gold conditions,they are hard to predict, and as such they might notcontribute to ?
and even hurt ?
the parsing qualityunder predicted conditions.
We find that definiteness(DET), phi-features (PERSON, NUMBER, GENDER),and undiacritzed lemma (LMM) are most helpful forArabic parsing on predicted input, while CASE andSTATE are most helpful on gold.In the future we plan to improve CASE predictionaccuracy; produce high accuracy supertag features,modeling active and passive valency; and use otherparsers (e.g., McDonald and Pereira, 2006).AcknowledgmentsThis work was supported by the DARPAGALE program,contract HR0011-08-C-0110.
We thank Joakim Nivrefor his useful remarks, and Ryan Roth for his help withCATiB conversion and MADA.20ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of Computational Natural LanguageLearning (CoNLL), pages 149?164.Timothy A. Buckwalter.
2004.
Buckwalter Arabic Mor-phological Analyzer Version 2.0.
Linguistic DataConsortium, University of Pennsylvania, 2002.
LDCCat alog No.
: LDC2004L02, ISBN 1-58563-324-0.Michael Collins, Jan Hajic, Lance Ramshaw, andChristoph Tillmann.
1999.
A statistical parser forczech.
In Proceedings of the 37th Annual Meetingof the the Association for Computational Linguistics(ACL), College Park, Maryland, USA, June.Brooke Cowan and Michael Collins.
2005.
Morphologyand reranking for the statistical parsing of spanish.
InProceedings of Human Language Technology (HLT)and the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 795?802.Mona Diab and Yassine BenAjiba.
2010.
From raw textto base phrase chunks: The new generation of AMIRATools for the processing of Modern Standard Arabic.In (to appear).
Spring LNCS, Special Jubilee edition.Mona Diab.
2007.
Towards an optimal pos tag set formodern standard arabic processing.
In Proceedingsof Recent Advances in Natural Language Processing(RANLP), Borovets, Bulgaria.G?lsen Eryigit, Joakim Nivre, and Kemal Oflazer.
2008.Dependency parsing of turkish.
Computational Lin-guistics, 34(3):357?389.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedingsof the 43rd Annual Meeting of the the Association forComputational Linguistics (ACL), Ann Arbor, Michi-gan, June.Nizar Habash and Ryan Roth.
2009.
Catib: Thecolumbia arabic treebank.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van den Boschand A. Soudi, editors, Arabic Computational Mor-phology: Knowledge-based and Empirical Methods.Springer.Jan Hajic?
and Barbora Vidov?-Hladk?.
1998.
Tag-ging Inflective Languages: Prediction of Morpholog-ical Categories for a Rich, Structured Tagset.
In Pro-ceedings of the International Conference on Com-putational Linguistics (COLING)- the Association forComputational Linguistics (ACL), pages 483?490.Sandra K?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Synthesis Lectures onHuman Language Technologies.
Morgan and ClaypoolPublishers.Seth Kulick, Ryan Gabbard, and Mitch Marcus.
2006.Parsing the Arabic Treebank: Analysis and improve-ments.
In Proceedings of the Treebanks and Linguis-tic Theories Conference, pages 31?42, Prague, CzechRepublic.Mohamed Maamouri, Ann Bies, Timothy A. Buckwalter,andWigdanMekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.
InProceedings of the NEMLAR Conference on ArabicLanguage Resources and Tools, pages 102?109, Cairo,Egypt.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of the 11th Conference ofthe the European Chapter of the Association for Com-putational Linguistics (EACL).Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gulsen Eryigit, Sandra Kubler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Joakim Nivre, Igor M. Boguslavsky, and Leonid K.Iomdin.
2008.
Parsing the SynTagRus Treebank ofRussian.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (COLING),pages 641?648.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Conference on Parsing Technologies(IWPT), pages 149?160, Nancy, France.Joakim Nivre.
2008.
Algorithms for Deterministic Incre-mental Dependency Parsing.
Computational Linguis-tics, 34(4).Joakim Nivre.
2009.
Parsing Indian languages withMaltParser.
In Proceedings of the ICON09 NLP ToolsContest: Indian Language Dependency Parsing, pages12?18.Otakar Smr?.
2007.
Functional Arabic Morphology.
For-mal System and Implementation.
Ph.D. thesis, CharlesUniversity, Prague.Reut Tsarfaty and Khalil Sima?an.
2007.
Three-dimensional parametrization for parsing morphologi-cally rich languages.
In Proceedings of the 10th Inter-national Conference on Parsing Technologies (IWPT),pages 156?167, Morristown, NJ, USA.Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.2006.
Maximum Entropy Based Restoration of Ara-bic Diacritics.
In Proceedings of the 21st InternationalConference on Computational Linguistics (COLING)and the 44th Annual Meeting of the the Associationfor Computational Linguistics (ACL), pages 577?584,Sydney, Australia.21
