Acquisition of English-Chinese Transliterated Word Pairs from Parallel-Aligned Texts using a Statistical Machine Transliteration ModelChun-Jen Lee1, 21 Telecommunication Labs.Chunghwa Telecom Co., Ltd.Chungli, Taiwan, R.O.C.cjlee@cht.com.twJason S. Chang22 Department of Computer ScienceNational Tsing Hua UniversityHsinchu, Taiwan, R.O.C.jschang@cs.nthu.edu.twAbstractThis paper presents a framework for extract-ing English and Chinese transliterated wordpairs from parallel texts.
The approach isbased on the statistical machine transliterationmodel to exploit the phonetic similarities be-tween English words and corresponding Chi-nese transliterations.
For a given proper nounin English, the proposed method extracts thecorresponding transliterated word from thealigned text in Chinese.
Under the proposedapproach, the parameters of the model areautomatically learned from a bilingual propername list.
Experimental results show that theaverage rates of word and character precisionare 86.0% and 94.4%, respectively.
The ratescan be further improved with the addition ofsimple linguistic processing.1 IntroductionAutomatic bilingual lexicon construction based on bi-lingual corpora has become an important first step formany studies and applications of natural language proc-essing (NLP), such as machine translation (MT), cross-language information retrieval (CLIR), and bilingualtext alignment.
As noted in Tsuji (2002), many previousmethods (Dagan et al, 1993; Kupiec, 1993; Wu and Xia,1994; Melamed, 1996; Smadja et al, 1996) deal withthis problem based on frequency of words appearing inthe corpora, which can not be effectively applied to low-frequency words, such as transliterated words.
Thesetransliterated words are often domain-specific and cre-ated frequently.
Many of them are not found in existingbilingual dictionaries.
Thus, it is difficult to handletransliteration only via simple dictionary lookup.
ForCLIR, the accuracy of transliteration highly affects theperformance of retrieval.In this paper, we present a framework of acquisitionfor English and Chinese transliterated word pairs basedon the proposed statistical machine transliteration model.Recently, much research has been done on machinetransliteration for many language pairs, such as Eng-lish/Arabic (Al-Onaizan and Knight, 2002), Eng-lish/Chinese (Chen et al, 1998; Lin and Chen, 2002;Wan and Verspoor, 1998), English/Japanese (Knightand Graehl, 1998), and English/Korean (Lee and Choi,1997; Oh and Choi, 2002).
Most previous approaches tomachine transliteration have focused on the use of apronunciation dictionary for converting source wordsinto phonetic symbols, a manually assigned scoringmatrix for measuring phonetic similarities betweensource and target words, or a method based on heuristicrules for source-to-target word transliteration.
However,words with unknown pronunciations may cause prob-lems for transliteration.
In addition, using either a lan-guage-dependent penalty function to measure thesimilarity between bilingual word pairs, or handcraftedheuristic mapping rules for transliteration may lead toproblems when porting to other language pairs.The proposed method in this paper requires no con-version of source words into phonetic symbols.
Themodel is trained automatically on a bilingual propername list via unsupervised learning.The remainder of the paper is organized as follows:Section 2 gives an overview of machine transliterationand describes the proposed model.
Section 3 describeshow to apply the model for extraction of transliteratedtarget words from parallel texts.
Experimental setup andquantitative assessment of performance are presented inSection 4.
Concluding remarks are made in Section 5.2 Statistical Machine TransliterationModel2.1 Overview of the Noisy Channel ModelMachine transliteration can be regarded as a noisychannel, as illustrated in Figure 1.
Briefly, the languagemodel generates a source word E and the transliterationmodel converts the word E to a target transliteration C.Then, the channel decoder is used to find the word ?that is the most likely to the word E that gives rise to thetransliteration C.LanguageModelP(E)Transli-TerationModelP(C|E)ChannelDecoderargmaxEE CP(E|C)E?Figure 1.
The noisy channel model in ma-chine transliteration.Under the noisy channel model, the back-transliteration problem is to find out the most probableword E, given transliteration C. Letting P(E) be theprobability of a word E, then for a given transliterationC, the back-transliteration probability of a word E canbe written as P(E|C).
By Bayes?
rule, the transliterationproblem can be written as follows:.
)()|()(maxarg)|(maxarg?CPECPEPCEPEEE==    (1)Since P(C) is constant for the given C, we can rewriteEq.
(1) as follows:),|()(maxarg?
ECPEPEE=(2)The first term, P(E), in Eq.
(2) is the language model,the probability of E. The second term, P(C|E), in Eq.
(2)is the transliteration model, the probability of the trans-literation C conditioned on E.Below, we assume that E is written in English, whileC is written in Chinese.
Since Chinese and English arenot in the same language family, there is no simple ordirect way of mapping and comparison.
One feasiblesolution is to adopt a Chinese romanization system1 torepresent the pronunciation of each Chinese character.Among the many romanization systems for Chinese,Wade-Giles and Pinyin are the most widely used.
TheWade-Giles system is commonly used in Taiwan todayand has traditionally been popular among Westernscholars.
For this reason, we use the Wade-Giles systemto romanize Chinese characters.
However, the proposedapproach is equally applicable to other romanizationsystems.The language model gives the prior probability P(E)which can be modeled using maximum likelihood esti-mation.
As for the transliteration model P(C|E), we canapproximate it using the transliteration unit (TU), whichis a decomposition of E and C. TU is defined as a se-1  Ref.
sites: ?http://www.romanization.com/index.html?
and?http://www.edepot.com/taoroman.html?.quence of characters transliterated as a base unit.
ForEnglish, a TU can be a monograph, a digraph, or a tri-graph (Wells, 2001).
For Chinese, a TU can be a sylla-ble initial, a syllable final, or a syllable (Chao, 1968)represented by corresponding romanized characters.
Toillustrate how this approach works, take the example ofan English name, ?Smith?, which can be segmented intofour TUs and aligned with the romanized transliteration.Assuming that the word is segmented into ?S-m-i-th?,then a possible alignment with the Chinese translitera-tion ????
(Shihmissu)?
is depicted in Figure 2.S             m            i       thShih      m          i            ssu?
?
?Figure 2.
TU alignment between English andChinese romanized character sequences.2.2 Formal Description: Statistical Translitera-tion Model (STM)A word E with l characters and a romanized word Cwith n characters are denoted by le1  andnc1 , respec-tively.
Assume that the number of aligned TUs for (E,C) is N, and let },...,,{ 21 NmmmM =  be an alignmentcandidate, where mj is the match type of the j-th TU.The match type is defined as a pair of TU lengths for thetwo languages.
For instance, in the case of (Smith,Shihmissu), N is 4, and M is {1-4, 1-1, 1-1, 2-3}.
Wewrite E and C as follows:,,...,,,...,,21112111?????
======NNnNNlvvvvcCuuuueE(3)where ui and vj are the i-th TU of E and the j-th TU ofC, respectively.Then the probability of C given E, P(C|E), is formulatedas follows:).|(),|()|,()|( EMPEMCPEMCPECPMM??
==  (4)To reduce computational complexity, one alternativeapproach is to modify the summation criterion in Eq.
(4)into maximization.
Therefore, we can approximateP(C|E) as follows:)|(),|(max)|( EMPEMCPECPM?).
(),|(max MPEMCPM?
(5)We approximate )(),|( MPEMCP  as follows:),...,,()|()(),|( 2111 NNN mmmPuvPMPEMCP =).()|(1iiiNimPuvP=??
(6)Therefore, we have( ).
)(log)|(logmax)|(log1?=+?NiiiiMmPuvPECP(7)Let ),( jiS  be the maximum accumulated log prob-ability between the first i characters of E and the first jcharacters of C. Then, ),()|(log nlSECP = , the maxi-mum accumulated log probability among all possiblealignment paths of E with length l and C with length n,can be computed using a dynamic programming (DP)strategy, as shown in the following:Step 1 (Initialization):0)0,0( =S(8)Step 2 (Recursion):.0   ,0),(log)|(log),(max),(,njlikhPecPkjhiSjiSihijkjkh????++??=??
(9)Step 3 (Termination):),(log)|(log),(max),(,khPecPknhlSnlSlhlnknkh++??=??
(10)where ),( khP  is defined as the probability of the matchtype ?h-k?.2.3 Estimation of Model ParametersTo describe the iterative procedure for re-estimation ofprobabilities of )|( ij uvP  and )( imP , we first definethe following functions:),( ji vucount  = the number of occurrences ofaligned pair ui and vi in the trainingset.
)( iucount  = the number of occurrences of ui inthe training set.
),( khcount  = the total number of occurrencesof ui with length h aligned with vjwith length k in the training set.Therefore, the translation probability )|( ij uvP  can beapproximated as follows:.
)(),()|(ijiij ucountvucountuvP =(11)The probability of the match type, ),( khP , can be es-timated as follows:.
),(),(),( ?
?=i jjicountkhcountkhP(12)For the reason that ),( ji vucount  is unknown in thebeginning , a reasonable initial estimate of the parame-ters of the translation model is to constrain the TUalignments of a word pair (E, C) within a position dis-tance ?
(Lee and Choi, 1997).
Assume that 1?+= hppi euand 1?+= kqqj cv , and ),( ji vud?
is the allowable posi-tion distance within ?
for the aligned pair (ui, vi).
),( ji vud?
is defined as follows:,)1()1(,),(???????<??+??+<??=??
?nlkqhpandnlqpvud ji(13)where l and n are the length of the source word E andthe target word C, respectively.To accelerate the convergence of EM training andreduce the noisy TU aligned pairs (ui, vj), we restrict thecombination of TU pairs to limited patterns.
ConsonantTU pairs only with same or similar phonemes are al-lowed to be matched together.
An English consonant isalso allowed to matching with a Chinese syllable begin-ning with same or similar phonemes.
An English semi-vowel TU can either be matched with a Chineseconsonant or a vowel with same or similar phonemes, orbe matched with a Chinese syllable beginning withsame or similar phonemes.As for the probability of match type, ),( khP , it isset to uniform distribution in the initialization phase,shown as follows:,1),(TkhP =      (14)where T is the total number of match types allowed.Based on the Expectation Maximization (EM) algo-rithm (Dempster et al, 1977) with Viterbi decoding(Forney, 1973), the iterative parameter estimation pro-cedure is described as follows:Step 1 (Initialization):Use Eq.
(13) to generate likely TU alignmentpairs.
Calculate the initial model parameters,)|( ij uvP  and ),( khP , using Eq.
(11) and Eq.
(12).Step 2 (Expection):Based on current model parameters, find thebest Viterbi path for each E and C word pair inthe training set.Step 3 (Maximization):Based on all the TU alignment pairs obtainedfrom Step 2, calculate the new model parame-ters using Eqs.
(11) and (12).
Replace themodel parameters with the new model parame-ters.
If it reaches a stopping criterion or a pre-defined iteration numbers, then stop thetraining procedure.
Otherwise, go back to Step2.3 Extraction of Transliteration from Par-allel TextThe task of machine transliteration is useful for manyNLP applications, and one interesting related problem ishow to find the corresponding transliteration for a givensource word in a parallel corpus.
We will describe howto apply the proposed model for such a task.For that purpose, a sentence alignment procedure isapplied first to align parallel texts at the sentence level.Then, we use a tagger to identify proper nouns in thesource text.
After that, the model is applied to isolate thetransliteration in the target text.
In general, the pro-posed transliteration model could be further augmentedwith linguistic processing, which will be described inmore details in the next subsection.
The overall processis summarized in Figure 3.Bilingual corpusSentence alignmentSourcesentenceTargetsentenceProper names:Word extractionSource wordProper names:Source & Target wordsLinguisticprocessingTransli-teratorPrepro-cessingFigure 3.
The overall process for the extrac-tion of transliteration from parallel text.An excerpt from the magazine Scientific American(Cibelli et al, 2002) is illustrated as follows:Source sentence:?Rudolf Jaenisch, a cloning expert at theWhitehead Institute for Biomedical Re-search at the Massachusetts Institute ofTechnology, concurred:?Target sentence:???????????????????????????
?In the above excerpt, three English proper nouns ?Jae-nisch?, ?Whitehead?, and ?Massachusetts?
are identi-fied by a tagger.
Utilizing Eqs.
(7) and the DP approachformulated by Eqs.
(8)-(10), we found the target word?huaihaite (???
)?
most likely corresponding to?Whitehead?.
In order to retrieve the transliteration for agiven proper noun, we need to keep track of the optimalTU decoding sequence associated with the given Chi-nese term for each word pair under the proposedmethod.
The aligned TUs can be easily obtained viabacktracking the best Viterbi path (Manning andSchutze, 1999).
For the example mentioned above, thealignments of the TU matching pairs via the Viterbibacktracking path are illustrated in Figure 4.Match Type            TU Pair:0 - 1 ,    -- y0 - 1 ,    -- u0 - 1 ,    -- a0 - 1 ,    -- n2 - 2 , Wh -- hu1 - 1 , i     -- a1 - 0 , t     --1 - 1 , e    -- i1 - 1 , h    -- h0 - 1 , -- a2 - 1 , ea  -- i1 - 2 , d   -- te0 - 1 , -- s0 - 1 , -- h0 - 1 , -- e0 - 1 , -- n0 - 1 , -- g:????
?Figure 4.
The alignments of the TU matching pairsvia the Viterbi backtracking path.3.1 Linguistic ProcessingSome language-dependent knowledge can be integratedto further improve the performance, especially when wefocus on specific language pairs.Linguistic Processing Rule 1 (R1):Some source words have both transliteration and trans-lation, which are equally acceptable and can be usedinterchangeably.
For example, the source word ?Eng-land?
is translated into ???
(Yingkou)?
and transliter-ated into ????
(Yingkolan)?, respectively, as shownin Figure 5.
Since the proposed model is designed spe-cifically for transliteration, such cases may cause prob-lems.
One way to overcome this limitation is to handlethose cases by using a list of commonly used propernames and translations.England vs.
?
?The Spanish Armada sailed to England in1588.?????????????????
?England vs.??
?England is the only country coterminouswith Wales.???????????????
?Figure 5.
Examples of mixed usages oftranslation and transliteration.Linguistic Processing Rule 2 (R2):From error analysis of the aligned results of the trainingset, the proposed approach suffers from the fluid TUs,such as ?t?, ?d?, ?tt?, ?dd?, ?te?, and ?de?.
Sometimesthey are omitted in transliteration, and sometimes theyare transliterated as a Chinese character.
For instance,?d?
is usually transliterated into ??
?, ??
?, or ??
?corresponding to Chinese TU of ?te?.
The English TU?d?
is transliterated as ???
in (Clifford, ????
), butleft out in (Radford, ???).
In the example shown inFigure 6, ?David (??)?
is mistakenly matched up with?????.
(A boy by the name of David.)??
?
?
?
???????
Ta Wei Te ??????
David .Figure 6.
Example of the transliteratedword extraction for ?David?.However, that problem caused by fluid TUscan be partly overcome by adding more linguisticconstraints in the post-processing phase.
We calcu-late the Chinese character distributions of propernouns from a bilingual proper name list.
A smallset of Chinese characters is often used for translit-eration.
Therefore, it is possible to improve theperformance by pruning extra tailing characters,which do not belong to the transliterated characterset, from the transliteration candidates.
For in-stance, the probability of ?
?, ?, ?, ?, ??
beingused in transliteration is very low.
So correct trans-literation ???
?
for the source word ?David?could be extracted by removing the character ??
?.3.2 Working Flow by Integrating Linguistic andStatistical InformationCombining the linguistic processing and transliterationmodel, we present the algorithm for transliteration ex-traction as follows:Step 1: Look up the translation list as stated inR1.
If the translation of a source wordappears in both the entry of the transla-tion list and the aligned target sentence(or paragraph), then pick the translationas the target word.
Otherwise, go to Step2.Step 2: Pass the source word and its aligned tar-get sentence (or paragraph) through theproposed model to extract the targetword.Step 3: Apply linguistic processing R2 to re-move superfluous tailing characters inthe target word.After the above processing, the performance ofsource-target word extraction is significantly improvedover the previous experiment.4 ExperimentsIn this section, we focus on the setup of experimentsand performance evaluation for the proposed model.4.1 Experimental SetupThe corpus T0 for training consists of 2,430 pairs ofEnglish names together with their Chinese translitera-tions.
Two experiments are conducted.
In the first ex-periment, we analyze the convergence characteristics ofthis model training based on a similarity-based frame-work (Chen et al, 1998; Lin and Chen, 2002).
A valida-tion set T1, consisting of 150 unseen person name pairs,was collected from Sinorama Magazine (Sinorama,2002).
For each transliterated word in T1, a set of 1,557proper names is used as potential answers.
In the secondexperiment, a parallel corpus T2 was prepared to evalu-ate the performance of proposed methods.
T2 consists of500 bilingual examples from the English-Chinese ver-sion of the Longman Dictionary of Contempory English(LDOCE) (Proctor, 1988).4.2 Evaluation MetricIn the first experiment, a set of source words was com-pared with a given target word, and then was ranked bysimilarity scores.
The source word with the highestsimilarity score is chosen as the answer to the back-transliteration problem.
The performance is evaluatedby rates of the Average Rank (AR) and the AverageReciprocal Rank (ARR) following Voorhees and Tice(2000).
?==NiiRNAR1)(1(15)?==NiiRNARR1)(11(16)where N is the number of testing data, and R(i) is therank of the i-th testing data.
Higher values of ARR indi-cate better performance.00.511.522.533.51 2 3 4 5Iteration numberRateofAR0.720.740.760.780.80.820.84RateofARRARARRFigure 7.
Performance at each iteration onthe validation set T1.In Figure 7, we show the rates of AR and ARR for thevalidation set T1 by varying the number of iterations ofthe EM training algorithm from 1 to 6.
We note that therates become saturated at the 2nd iteration, which indi-cates the efficiency of the proposed training approach.As for the second experiment, performance on theextraction of transliterations is evaluated based on pre-cision and recall rates on the word and character level.Since we consider exact one proper name in the sourcelanguage and one transliteration in the target language ata time.
The word recall rates are same as word precisionrates:=)(WP Precision Word.wordscorrect of numberwordsextractedcorrectly  of number  (17)The character level recall and precision are as follows:=)( CPprecision Character,characters correct of numbercharacters extractedcorrectly  of number  (18)=)(CR Recall Character.characters correct of numbercharacters extractedcorrectly  of number  (19)For the purpose of easier evaluation, T2 was de-signed to contain exact one proper name in the sourcelanguage and one transliteration in the target languagefor each bilingual example.
Therefore, if more than oneproper name occurs in a bilingual example, we separatethem into several testing examples.
We also separate acompound proper name in one example into individualnames to form multiple examples.
For example, in thefirst case, two proper names ?Tchaikovsky?
and ?Stra-vinsky?
were found in the testing sample ?Tchaikovskyand Stravinsky each wrote several famous ballets?.
Inthe second case, a compound proper name ?CyrilTourneur?
was found in ?No one knows who wrote thatplay, but it is usually ascribed to Cyril Tourneur?.
How-ever, in the third case, ?New York?
is transliterated as awhole Chinese word ???
?, so it can not be separatedinto two words.
Therefore, the testing data for the aboveexamples will be semi-automatically constructed.
Forsimplicity, we considered each proper name in thesource sentence in turn and determined its correspond-ing transliteration independently.
Table 1 shows someexamples of the testing set T2.Table 1.
Part of bilingual examples of the test-ing set T2.In the experiment of transliterated word extraction,the proposed method achieves on average 86.0% wordaccuracy rate, 94.4% character precision rate, and96.3% character recall rate, as shown in row 1 of Table2.
The performance can be further improved with a sim-ple statistical and linguistic processing, as shown inTable 2.Methods WP CP CRBaseline 86.0% 94.4% 96.3%Baseline+R1 88.6% 95.4% 97.7%Baseline+R2 90.8% 97.4% 95.9%Baseline+R1+R2 94.2% 98.3% 97.7%Table 2.
The experimental results of transliter-ated word extraction for T2.In the baseline model, we find that there are someerrors caused by translations which are not strictly trans-literated; and there are some source words transferredinto target words by means of transliteration and transla-tion mutually.
Therefore, R1 can be viewed as the pre-processing to extract transliterated words.
Some errorsare further eliminated by R2 which considers the usageof the transliterated characters in the target language.
Inthis experiment, we use a transliterated character set of735 Chinese characters.5 ConclusionIn this paper, we describe a framework to deal with theproblem of acquiring English-Chinese bilingual translit-erated word pairs from parallel-aligned texts.
An unsu-pervised learning approach to the proposed machinetransliteration model is also presented.
The proposedapproach automatically learned the parameters of themodel from a bilingual proper name list.
It is not re-stricted to the availability of pronunciation dictionary inthe source language.
From the experimental results, itindicates that our methods achieve excellent perform-ance.
With the statistical-based characteristic of the pro-posed model, we plan to extend the experiments to bi-directional transliteration and other different corpora.ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Translatingnamed entities using monolingual and bilingual re-sources.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics(ACL), pages 400-408.Hsin-Hsi Chen, Sheng-Jie Huang, Yung-Wei Ding, andShih-Chung Tsai.
1998.
Proper name translation incross-language information retrieval.
In Proceedingsof 17th COLING and 36th ACL, pages 232-236.Yuen Ren Chao.
1968.
A Grammar of spoken Chinese.Berkeley, University of California Press.Dagan, I., Church, K. W., and Gale, W. A.
1993.
Robustbilingual word alignment for machine aided transla-tion.
In Proceedings of the Workshop on Very LargeCorpora: Academic and Industrial Perspectives,pages 1-8, Columbus Ohio.Jose B. Cibelli, Robert P. Lanza, Michael D. West, andCarol Ezzell.
2002.
What Clones?
Scientific Ameri-can, Inc., New York, January.http://www.sciam.com.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical Soci-ety, 39(1):1-38.G.
D. Forney.
1973.
The Viterbi algorithm.
Proceedingsof IEEE, 61:268-278, March.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics,24(4):599-612.Julian Kupiec.
1993.
An algorithm for finding nounphrase correspondences in bilingual corpora.
In Pro-ceedings of the 40th Annual Conference of theHe is a (second) Caesar in speech and leader-ship.?????????????????
?.Can you adduce any reason at all for hisstrange behaviour, Holmes????
?, ?????????????????
?They appointed him to catch all the rats inHamelin.??????????????
?.Drink Rossignol, the aristocrat of table wines!??????!
????????
!Cleopatra was bitten by an asp.???????????????
?.Schoenberg used atonality in the music of hismiddle period.?????????????
?.Now that this painting has been authenticatedas a Rembrandt, it's worth 10 times as muchas I paid for it!?????????????
?, ???????????????
!Association for Computational Linguistics (ACL),pages 17-22, Columbus, Ohio.Jae Sung Lee and Key-Sun Choi.
1997.
A statisticalmethod to generate various foreign word translitera-tions in multilingual information retrieval system.
InProceedings of the 2nd International Workshop onInformation Retrieval with Asian Languages(IRAL'97), pages 123-128, Tsukuba, Japan.Wei-Hao Lin and Hsin-Hsi Chen.
2002.
Backwardtransliteration by learning phonetic similarity.
InCoNLL-2002, Sixth Conference on Natural Lan-guage Learning, Taipei, Taiwan.Christopher D. Manning and Hinrich Schutze.
1999.Foundations of Statistical Natural Language Proc-essing, MIT Press; 1st edition.I Dan Melamed.
1996.
Automatic construction of cleanbroad coverage translation lexicons.
In Proceedingsof the 2nd Conference of the Association for Ma-chine Translation in the Americas (AMTA'96),Montreal, Canada.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciationand contextual rules.
In Proceedings of the 19th In-ternational Conference on Computational Linguis-tics (COLING), Taipei, Taiwan.P.
Proctor, 1988.
Longman English-Chinese Dictionaryof Contemporary English, Longman Group (FarEast) Ltd., Hong Kong.Sinorama.
2002.
Sinorama Magazine.http://www.greatman.com.tw/sinorama.htm.Bonnie Glover Stalls and Kevin Knight.
1998.
Translat-ing names and technical terms in Arabic text.
InProceedings of the COLING/ACL Workshop onComputational Approaches to Semitic Languages.Frank Z. Smadja, Kathleen McKeown, and VasileiosHatzivassiloglou.
1996.
Translating collocations forbilingual lexicons: a statistical approach.
Computa-tional Linguistics, 22(1):1-38.Keita Tsuji.
2002.
Automatic extraction of translationalJapanese-KATAKANA and English word pairsfrom bilingual corpora.
International Journal ofComputer Processing of Oriental Languages,15(3):261-279.Ellen M. Voorhees and Dawn M. Tice.
2000.
The trec-8question answering track report.
In English Text Re-trieval Conference (TREC-8).Stephen Wan and Cornelia Maria Verspoor.
1998.Automatic English-Chinese name transliteration fordevelopment of multilingual resources.
In Proceed-ings of 17th COLING and 36th ACL, pages 1352-1356.J.
C. Wells.
2001.
Longman Pronunciation Dictionary(New Edition), Addison Wesley Longman, Inc.Dekai Wu and Xuanyin Xia.
1994.
Learning an English-Chinese lexicon from a parallel corpus.
In Proceed-ings of the First Conference of the Association forMachine Translation in the Americas, pages 206?213.
