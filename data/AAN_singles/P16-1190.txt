Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2019?2028,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsJointly Learning to Embed and Predict with Multiple LanguagesDaniel C. Ferreira?Andr?
F. T.
Martins??
]Mariana S. C.
Almeida?
]?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Unbabel Lda, Rua Visconde de Santar?m, 67-B, 1000-286 Lisboa, Portugal]Instituto de Telecomunica?
?es, Instituto Superior T?cnico, 1049-001 Lisboa, Portugal{dcf,mla}@priberam.pt, {andre.martins}@unbabel.comAbstractWe propose a joint formulation for learn-ing task-specific cross-lingual word em-beddings, along with classifiers for thattask.
Unlike prior work, which firstlearns the embeddings from parallel dataand then plugs them in a supervisedlearning problem, our approach is one-shot: a single optimization problem com-bines a co-regularizer for the multilin-gual embeddings with a task-specific loss.We present theoretical results showingthe limitation of Euclidean co-regularizersto increase the embedding dimension,a limitation which does not exist forother co-regularizers (such as the `1-distance).
Despite its simplicity, ourmethod achieves state-of-the-art accura-cies on the RCV1/RCV2 dataset whentransferring from English to German, withtraining times below 1 minute.
On theTED Corpus, we obtain the highest re-ported scores on 10 out of 11 languages.1 IntroductionDistributed representations of text (embeddings)have been the target of much research in naturallanguage processing (Collobert and Weston, 2008;Mikolov et al, 2013; Pennington et al, 2014; Levyet al, 2015).
Word embeddings partially capturesemantic and syntactic properties of text in theform of dense real vectors, making them apt fora wide variety of tasks, such as language model-ing (Bengio et al, 2003), sentence tagging (Turianet al, 2010; Collobert et al, 2011), sentiment anal-ysis (Socher et al, 2011), parsing (Chen and Man-ning, 2014), and machine translation (Zou et al,2013).At the same time, there has been a consis-tent progress in devising ?universal?
multilin-gual models via cross-lingual transfer techniquesof various kinds (Hwa et al, 2005; Zeman andResnik, 2008; McDonald et al, 2011; Ganchevand Das, 2013; Martins, 2015).
This line of re-search seeks ways of using data from resource-rich languages to solve tasks in resource-poorlanguages.
Given the difficulty of handcraftinglanguage-independent features, it is highly appeal-ing to obtain rich, delexicalized, multilingual rep-resentations embedded in a shared space.A string of work started with Klementiev et al(2012) on learning bilingual embeddings for textclassification.
Hermann and Blunsom (2014) pro-posed a noise-contrastive objective to push theembeddings of parallel sentences to be close inspace.
A bilingual auto-encoder was proposed byChandar et al (2014), while Faruqui and Dyer(2014) applied canonical correlation analysis toparallel data to improve monolingual embeddings.Other works optimize a sum of monolingual andcross-lingual terms (Gouws et al, 2015; Soyeret al, 2015), or introduce bilingual variants ofskip-gram (Luong et al, 2015; Coulmance et al,2015).
Recently, Pham et al (2015) extended thenon-compositional paragraph vectors of Le andMikolov (2014) to a bilingual setting, achievinga new state of the art at the cost of more expensive(and non-deterministic) prediction.In this paper, we propose an alternative jointformulation that learns embeddings suited to a par-ticular task, together with the corresponding clas-sifier for that task.
We do this by minimizing acombination of a supervised loss function and amultilingual regularization term.
Our approachleads to a convex optimization problem and makesa bridge between classical co-regularization ap-proaches for semi-supervised learning (Sindhwaniet al, 2005; Altun et al, 2005; Ganchev et al,20192008) and modern representation learning.
Inaddition, we show that Euclidean co-regularizershave serious limitations to learn rich embeddings,when the number of task labels is small.
We es-tablish this by proving that the resulting embed-ding matrices have their rank upper bounded bythe number of labels.
This limitation does not ex-ist for other regularizers (convex or not), such asthe `1-distance and noise-contrastive distances.Our experiments in the RCV1/RCV2 datasetyield state-of-the-art accuracy (92.7%) with thissimple convex formulation, when transferringfrom English to German, without the need of neg-ative sampling, extra monolingual data, or non-additive representations.
For the reverse direction,our best number (79.3%), while far behind the re-cent para_doc approach (Pham et al, 2015), ison par with current compositional methods.On the TED corpus, we obtained general pur-pose multilingual embeddings for 11 target lan-guages, by considering the (auxiliary) task ofreconstructing pre-trained English word vectors.The resulting embeddings led to cross-lingualmulti-label classifiers that achieved the highest re-ported scores on 10 out of these 11 languages.12 Cross-Lingual Text ClassificationWe consider a cross-lingual classification frame-work, where a classifier is trained on a datasetfrom a source language (such as English) and ap-plied to a target language (such as German).
Later,we generalize this setting to multiple target lan-guages and to other tasks besides classification.The following data are assumed available:1.
A labeled dataset Dl:= {?x(m), y(m))}Mm=1,consisting of text documentsx in the source lan-guage categorized with a label y ?
{1, .
.
.
, L}.2.
An unlabeled parallel corpus Du:={(s(n), t(n))}Nn=1, containing sentences s in thesource language paired with their translations tin the target language (but no information abouttheir categories).Let VSand VTbe the vocabulary size of the sourceand target languages, respectively.
Throughout,we represent sentences s ?
RVSand t ?
RVTas vectors of word counts, and documents x asan average of sentence vectors.
We assume that1We provide the trained embeddings at http://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html.the unlabeled sentences largely outnumber the la-beled documents, N  M , and that the numberof labels L is relatively small.
The goal is to usethe data above to learn a classifier h : RVT?
{1, .
.
.
, L} for the target language.This problem is usually tackled with a two-stageapproach: in the first step, bilingual word embed-dings P ?
RVS?Kand Q ?
RVT?Kare learnedfrom Du, where each row of these matrices con-tains a Kth dimensional word representation in ashared vector space.
In the second step, a standardclassifier is trained onDl, using the source embed-dings P ?
RVS?K.
Since the embeddings are ina shared space, the trained model can be applieddirectly to classify documents in the target lan-guage.
We describe next these two steps in moredetail.
We assume throughout an additive repre-sentation for sentences and documents (denotedADD by Hermann and Blunsom (2014)).
Theserepresentations can be expressed algebraically asP>x,P>s,Q>t ?
RK, respectively.Step 1: Learning the Embeddings.
The cross-lingual embeddings P and Q are trained so thatthe representations of paired sentences (s, t) ?Duhave a small (squared) Euclidean distanced`2(s, t) =12?P>s?Q>t?2.
(1)Since a direct minimization of Eq.
1 leads to a de-generate solution (P = 0,Q = 0), Hermann andBlunsom (2014) use instead a noise-contrastivelarge-margin distance obtained via negative sam-pling,dns(s, t,n) = [m+ d`2(s, t)?
d`2(s,n)]+, (2)wheren is a random (unpaired) target sentence,mis a ?margin?
parameter, and [x]+:= max{0, x}.Letting J be the number of negative examples ineach sample, they arrive at the following objectivefunction to be minimized:Rns(P ,Q) :=1NN?n=1J?j=1dns(s(n), t(n),n(n,j)).
(3)This minimization can be carried out efficientlywith gradient-based methods, such as stochasticgradient descent or AdaGrad (Duchi et al, 2011).Note however that the objective function in Eq.
3is not convex.
Therefore, one may land at differentlocal minima, depending on the initialization.2020Step 2: Training the Classifier.
Once we havethe bilingual embeddings P and Q, we can com-pute the representation P>x ?
RKof each docu-ment x in the labeled dataset Dl.
Let V ?
RK?Lbe a matrix of parameters (weights), with one col-umn vyper label.
A linear model is used to makepredictions, according toy?
= argmaxy?
{1,...,L}v>yP>x= argmaxy?
{1,...,L}w>yx, (4)wherewyis a column of the matrixW := PV ?RVS?L.
In prior work, the perceptron algorithmwas used to learn the weights V from the labeledexamples inDl(Klementiev et al, 2012; Hermannand Blunsom, 2014).
Note that, at test time, itis not necessary to store the full embeddings: ifL  K, we may simply precompute W := PV(one weight per word and label) if the input is inthe source language?orQV , if the input is in thetarget language?and treat this as a regular bag-of-words linear model.3 Jointly Learning to Embed and ClassifyInstead of a two-stage approach, we propose tolearn the bilingual embeddings and the classifierjointly on Dl?
Du, as described next.Our formulation optimizes a combination of aco-regularization function R, whose goal is topush the embeddings of paired sentences in Dutostay close, and a loss function L, which fits themodel to the labeled data in Dl.The simplest choice forR is a simple Euclideanco-regularization function:R`2(P ,Q) =1NN?n=1d`2(s(n), t(n)) (5)=12NN?n=1?P>s(n)?Q>t(n)?2.An alternative is the `1-distance:R`1(P ,Q) =1NN?n=1?P>s(n)?Q>t(n)?1.
(6)One possible advantage of R`1(P ,Q) overR`2(P ,Q) is that the `1-distance is more robustto outliers, hence it is less sensitive to differencesin the parallel sentences.
Note that both functionsin Eqs.
5?6 are jointly convex on P and Q, un-like the one in Eq.
3.
They are also simpler and donot require negative sampling.
While these func-tions have a degenerate behavior in isolation (sincethey are both minimized by P = 0 and Q = 0),we will see that they become useful when pluggedinto a joint optimization framework.The next step is to define the loss function L toleverage the labeled data inDl.
We consider a log-linear model P (y |x;W ) ?
exp(w>yx), whichleads to the following logistic loss function:LLL(W ) = ?1MM?m=1logP (y(m)|x(m);W ).
(7)We impose thatW is of the formW = PV for afixedV ?
RK?L, whose choice we discuss below.Putting the pieces together and adding some ex-tra regularization terms, we formulate our joint ob-jective function as follows:F(P ,Q) = ?R(P ,Q) + L(PV )+?S2?P ?2F+?T2?Q?2F,(8)where ?, ?S, ?T?
0 are regularization constants.By minimizing a combination of L(PV ) andR(P ,Q), we expect to obtain embeddings Q?that lead to an accurate classifier h for the targetlanguage.
Note that P = 0 and Q = 0 is nolonger a solution, due to the presence of the lossterm L(PV ) in the objective.Choice of V .
In Eq.
8, we chose to keep V fixedrather than optimize it.
The rationale is that thereare many more degrees of freedom in the embed-ding matrices P and Q than in V (concretely,O(K(VS+ VT)) versus O(KL), where we are as-suming a small number of labels, L  VS+ VT).Our assumption is that we have enough degrees offreedom to obtain an accurate model, regardless ofthe choice of V .
These claims will be backed in?4 by a more rigorous theoretical result.
KeepingV fixed has another important advantage: it al-lows to minimize F with respect to P andQ only,which makes it a convex optimization problem ifwe choose R and L to be both convex?e.g., set-tingR ?
{R`2,R`1} and L := LLL.Relation to Multi-View Learning.
An interest-ing particular case of this formulation arises ifK = L and V = IL(the identity matrix).
Inthat case, we have W = P and the embeddingmatrices P and Q are in fact weights for everypair of word and label, as in standard bag-of-word2021models.
In this case, we may interpret the co-regularizerR(P ,Q) in Eq.
8 as a term that pushesthe label scores of paired sentences P>s(n)andQ>t(n)to be similar, while the source-based log-linear model is fit via L(W ).
The same idea un-derlies various semi-supervised co-regularizationmethods that seek agreement between multipleviews (Sindhwani et al, 2005; Altun et al, 2005;Ganchev et al, 2008).
In fact, we may regardthe joint optimization in Eq.
8 as a generalizationof those methods, making a bridge between thosemethods and representation learning.Multilingual Embeddings.
It is straightforwardto extend the framework herein presented to thecase where there are multiple target languages (sayR of them), and we want to learn one embeddingmatrix for each, {Q1, .
.
.
,QR}.
The simplest wayis to consider a sum of pairwise co-regularizers,R?
(P , {Q1, .
.
.
,QR}) :=R?r=1R(P ,Qr).
(9)If R is additive over the parallel sentences (whichis the case for R`2, R`1and Rns), then this pro-cedure is equivalent to concatenating all the par-allel sentences (regardless of the target language)and adding a language suffix to the words to dis-tinguish them.
This reduces directly to a problemin the same form as Eq.
8.Pre-Trained Source Embeddings.
In practice,it is often the case that pre-trained embeddings forthe source language are already available (let?Pbe the available embedding matrix).
It would befoolish not to exploit those resources.
In this sce-nario, the goal is to use?P and the dataset Duto obtain ?good?
embeddings for the target lan-guages (possibly tweaking the source embeddingstoo, P ?
?P ).
Our joint formulation in Eq.
8 canalso be used to address this problem.
It suffices toset K = L and V = IL(as in the multi-viewlearning case discussed above) and to define anauxiliary task that pushes P and?P to be similar.The simplest way is to use a reconstruction loss:L`2(P ,?P ) :=12?P ?
?P ?2F.
(10)The resulting optimization problem has resem-blances with the retrofitting approach of Faruquiet al (2015), except that the goal here is to ex-tend the embeddings to other languages, insteadof pushing monolingual embeddings to agree witha semantic lexicon.
We will present some experi-ments in ?5.2 using this framework.4 Limitations of the EuclideanCo-RegularizerOne may wonder how much the embedding di-mension K influences the learned classifier.
Thenext proposition shows the (surprising) result that,with the formulation in Eq.
8 with R = R`2, itmakes absolutely no difference to increase K pastthe number of labels L. Below, T ?
RVT?Nde-notes the matrix with columns t(1), .
.
.
, t(N).Proposition 1.
Let R = R`2and assume T hasfull row rank.2Then, for any choice of V ?RK?L, possibly with K > L, the following holds:1.
There is an alternative, low-dimensional, V??RK?
?Lwith K??
L such that the classifier ob-tained (for both languages) by optimizing Eq.
8using V?is the same as if using V .32.
This classifier depends on V only via the L-by-L matrix V>V .3.
IfP?,Q?are the optimal embeddings obtainedwith V , then we always have rank(P?)
?
Land rank(Q?)
?
L regardless of K.Proof.
See App.
A.1 in the supplemental material.Let us reflect for a moment on the practical im-pact of Prop.
1.
This result shows the limitationof the Euclidean co-regularizerR`2in a very con-crete manner: when R = R`2, we only need toconsider representations of dimension K ?
L.Note also that a corollary of Prop.
1 arises whenV>V = IL, i.e., when V is chosen to haveorthonormal columns (a sensible choice, since itcorresponds to seeking embeddings that leave thelabel weights ?uncorrelated?).
Then, the secondstatement of Prop.
1 tells us that the resulting clas-sifier will be the same as if we had simply setV = IL(the particular case discussed in ?3).
Wewill see in ?5.1 that, despite this limitation, thisclassifier is actually a very strong baseline.
Ofcourse, if the number of labels L is large enough,2This assumption is not too restrictive: it holds if N ?
VTand if no target sentence can be written as a linear combina-tion of the others (this can be accomplished if we removeredundant parallel sentences).3Let P ?,Q?
and P ??,Q??
be the optimal embeddingsobtained with V and V ?, respectively.
Since we are workingwith linear classifiers, the two classifiers are the same in thesense that P ?V = P ?
?V ?
and Q?V = Q?
?V ?.2022this limitation might not be a reason for concern.4An instance will be presented in ?5.2, where wewill see that the Euclidean co-regularizer excels.Finally, one might wonder whether Prop.
1 ap-plies only to the (Euclidean) `2norm or if it holdsfor arbitrary regularizers.
In fact, we show inApp.
A.2 that this limitation applies more gener-ally to Mahalanobis-Frobenius norms, which areessentially Euclidean norms after a linear trans-formation of the vector space.
However, it turnsout that for general norms such limitation does notexist, as shown below.Proposition 2.
IfR = R`1in Eq.
8, then the anal-ogous to Proposition 1 does not hold.
It also doesnot hold for the `?-norm and the `0-?norm.?Proof.
See App.
A.3 in the supplemental material.This result suggests that, for other regulariz-ers R 6= R`2, we may eventually obtain bet-ter classifiers by increasing K past L. As such,in the next section, we experiment with R ?
{R`2,R`1,Rns}, where Rnsis the (non-convex)noise-contrastive regularizer of Eq.
3.5 ExperimentsWe report results on two experiments: oneon cross-lingual classification on the ReutersRCV1/RCV2 dataset, and another on multi-labelclassification with multilingual embeddings on theTED Corpus.55.1 Reuters RCV1/RCV2We evaluate our framework on the cross-lingualdocument classification task introduced by Kle-mentiev et al (2012).
Following prior work,our dataset Duconsists of 500,000 parallel sen-tences from the Europarl v7 English-German cor-pus (Koehn, 2005); and our labeled dataset Dlconsists of English and German documents fromthe RCV1/RCV2 corpora (Lewis et al, 2004),each categorized with one out of L = 4 labels.
Weused the same split as Klementiev et al (2012):1,000 documents for training, of which 200 areheld out as validation data, and 5,000 for testing.4For regression tasks (such as the one presented in the lastparagraph of 3), instead of the ?number of labels,?
L shouldbe regarded as the number of output variables to regress.5Our code is available at https://github.com/dcferreira/multilingual-joint-embeddings.Note that, in this dataset, we are classifyingdocuments based on their bag-of-word representa-tions, and learning word embeddings by bringingthe bag-of-word representations of parallel sen-tences to be close together.
In this sense, we arebringing together these multiple levels of repre-sentations (document, sentence and word).We experimented with the joint formulation inEq.
8, with L := LLLand R ?
{R`2,R`1,Rns}.We optimized with AdaGrad (Duchi et al, 2011)with a stepsize of 1.0, using mini-batches of 100Reuters RCV1/RCV2 documents and 50,000 Eu-roparl v7 parallel sentences.
We found no need torun more than 100 iterations, with most of our runsconverging under 50.
Our vocabulary has 69,714and 175,650 words for English and German, re-spectively, when training on the English portionof the Reuters RCV1/RCV2 corpus, and 61,120and 183,888 words for English and German, whentraining in the German portion of the corpus.
Thisdifference is due to the inclusion of words in thetraining data into the vocabulary.
We do not re-move any words from the vocabulary, for simplic-ity.
We used the validation set to tune the hyper-parameters {?, ?S, ?T} and to choose the iterationnumber.
When using K = L, we chose V = IL;otherwise, we chose V randomly, sampling its en-tries from a Gaussian N (0, 0.1).Table 1 shows the results.
We include for com-parison the most competitive systems publishedto date.
The first thing to note is that our jointsystem with Euclidean co-regularization performsvery well for this task, despite the theoretical lim-itations shown in ?4.
Although its embedding sizeis only K = 4 (one dimension per label), it out-performed all the two-stage systems trained on thesame data, in both directions.For the EN?DE direction, our joint systemwith `1co-regularization achieved state-of-the-artresults (92.7%), matching two-stage systems thatuse extra monolingual data, negative sampling, ornon-additive document representations.
It is con-ceivable that the better results of R`1over R`2come from its higher robustness to differences inthe parallel sentences.For the DE?EN direction, our best result(79%) was obtained with the noise-contrastive co-regularizer, which outperformed all systems ex-cept para_doc (Pham et al, 2015).
Whilethe accuracy of para_doc is quite impressive,note that it requires 500-dimensional embeddings2023K EN?DE DE?ENI-Matrix [KTB12] 40 77.6 71.1ADD [HB14] 40 83.7 71.4ADD [HB14] 128 86.4 74.7BI [HB14] 40 83.4 69.2BI [HB14] 128 86.1 79.0BilBOWA [GBC15] 40 86.5 75.0Binclusion [SSA15] 40 86.8 76.7Bincl.+RCV [SSA15] (?)
40 92.7 84.4CLC-WA [SLLS15] (?)
40 91.3 77.2para_sum [PLM15] (?)
100 90.6 78.8para_doc [PLM15] (?)
500 92.7 91.5Joint,R`24 91.2 78.2Joint,R`14 92.7 76.0Joint,R`140 92.7 76.2Joint,Rns4 91.2 76.8Joint,Rns40 91.4 79.3Table 1: Accuracies in the RCV1/RCV2 dataset.
Shown for comparison are Klementiev et al (2012)[KTB12], Hermann and Blunsom (2014) [HB14], Gouws et al (2015) [GBC15], Soyer et al (2015)[SSA15], Shi et al (2015) [SLLS15], and Pham et al (2015) [PLM15].
Systems marked with (?)
usedthe full 1.8M parallel sentences in Europarl.
The one with (?)
used additional target monolingual datafrom RCV1/RCV2.
The bottom rows refer to our joint method, with Euclidean (`2), `1, and noise-contrastive co-regularization.
(hence many more parameters), was trained onmore parallel sentences, and requires more expen-sive (and non-deterministic) computation at testtime to compute a document?s embedding.
Ourmethod has the advantage of being simple andvery fast to train: it took less than 1 minute totrain the joint-R`1system for EN?DE, using asingle core on an Intel Xeon @2.5 GHz.
This canbe compared with Klementiev et al (2012), whotook 10 days on a single core, or Coulmance et al(2015), who took 10 minutes with 6 cores.6Although our theoretical results suggest that in-creasing K when using the `1norm may increasethe expressiveness of our embeddings, our resultsdo not support this claim (the improvements inDE?EN from K = 4 to K = 40 were tiny).However, it led to a gain of 2.5 points when us-ing negative sampling.
For K = 40, this system ismuch more accurate than Hermann and Blunsom(2014), which confirms that learning the embed-dings together with the task is highly beneficial.6Coulmance et al (2015) reports accuracies of 87.8%(EN?DE) and 78.7% (DE?EN), when using 10,000 train-ing documents from the RCV1/RCV2 corpora.5.2 TED CorpusTo assess the ability of our framework to han-dle multiple target languages, we ran a secondset of experiments on the TED corpus (Cettoloet al, 2012), using the training and test parti-tions created by Hermann and Blunsom (2014),downloaded from http://www.clg.ox.ac.uk/tedcorpus.
The corpus contains Englishtranscriptions and multilingual, sentence-alignedtranslations of talks from the TED conference in12 different languages, with 12,078 parallel docu-ments in the training partition (totalling 1,641,985parallel sentences).
Following their prior work, weused this corpus both as parallel data (Du) and asthe task dataset (Dl).
There are L = 15 labels anddocuments can have multiple labels.We experimented with two different strategies:?
A one-stage system (Joint), which jointly trainsthe multilingual embeddings and the multi-labelclassifier (similarly as in ?5.1).
To cope withmultiple target languages, we used a sum ofpairwise co-regularizers as described in Eq.
9.For classification, we use multinomial logisticregression, where we select those labels with aposterior probability above 0.18 (tuned on vali-2024dation data).?
A two-stage approach (Joint w/ Aux), where wefirst obtain multilingual embeddings by apply-ing our framework with an auxiliary task withpre-trained English embeddings (as described inEq.
10 and in the last paragraph of ?3), and thenuse the resulting multilingual representations totrain the multi-label classifier.
We address thismulti-label classification problem with indepen-dent binary logistic regressors (one per label),trained by running 100 iterations of L-BFGS(Liu and Nocedal, 1989).
At test time, we se-lect those labels whose posterior probability areabove 0.5.For the Joint w/ Aux strategy, we used the300-dimensional GloVe-840B vectors (Penning-ton et al, 2014), downloaded from http://nlp.stanford.edu/projects/glove/.Table 2 shows the results for cross-lingual clas-sification, where we use English as source andeach of the other 11 languages as target.
Wecompare our two strategies above with the strongMachine Translation (MT) baseline used by Her-mann and Blunsom (2014) (which translates theinput documents to English with a state-of-the-art MT system) and with their two strongest sys-tems, which build document-level representationsfrom embeddings trained bilingually or multi-lingually (called DOC/ADD single and DOC/ADDjoint, respectively).7Overall, our Joint systemwith `2regularization outperforms both Hermannand Blunsom (2014)?s systems (but not the MTbaseline) for 8 out of 11 languages, performinggenerally better than our `1-regularized system.However, the clear winner is our `2-regularizedJoint w/ Aux system, which wins over all systems(including the MT baseline) by a substantial mar-gin, for all languages.
This shows that pre-trainedsource embeddings can be extremely helpful inbootstrapping multilingual ones.8On the otherhand, the performance of the Joint w/ Aux sys-tem with `1regularization is rather disappointing.Note that the limitations of R`2shown in ?4 arenot a concern here, since the auxiliary task has7Note that, despite the name, the Hermann and Blunsom(2014)?s joint systems are not doing joint training as we are.8Note however that, overall, our Joint w/ Aux systemshave access to more data than our Joint systems and alsothan Hermann and Blunsom (2014)?s systems, since the pre-trained embeddings were trained on a large amount of En-glish monolingual data.
Yet, the amount of target languagedata is the same.L = 300 dimensions (the dimension of the pre-trained embeddings).
A small sample of the mul-tilingual embeddings produced by the winner sys-tem is shown in Table 4.Finally, we did a last experiment in which weuse our multilingual embeddings obtained withJoint w/ Aux to train monolingual systems for eachlanguage.
This time, we compare with a bag-of-words na?ve Bayes system (reported by Hermannand Blunsom (2014)), a system trained on thePolyglot embeddings from Al-Rfou et al (2013)(which are multilingual, but not in a shared rep-resentation space), and the two systems developedby Hermann and Blunsom (2014).
The results areshown in Table 3.
We observe that, with the excep-tion of Turkish, our systems consistently outper-form all the competitors.
Comparing the bottomtwo rows of Tables 2 and 3 we also observe that,for the `2-regularized system, there is not muchdegradation caused by cross-lingual training ver-sus training on the target language directly (in fact,for Spanish, Polish, and Brazilian Portuguese, theformer scores are even higher).
This suggests thatthe multilingual embeddings have high quality.6 ConclusionsWe proposed a new formulation which jointlyminimizes a combination of a supervised lossfunction with a multilingual co-regularizationterm using unlabeled parallel data.
This allowslearning task-specific multilingual embeddings to-gether with a classifier for the task.
Our methodachieved state-of-the-art accuracy on the ReutersRCV1/RCV2 cross-lingual classification task inthe English to German direction, while being ex-tremely simple and computationally efficient.
Ourresults in the Reuters RCV1/RCV2 task, obtainedusing Europarl v7 as parallel data, show that ourmethod has no trouble handling different levelsof representations simutaneously (document, sen-tence and word).
On the TED Corpus, we obtainedthe highest reported scores for 10 out of 11 lan-guages, using an auxiliary task with pre-trainedEnglish embeddings.AcknowledgmentsWe would like to thank the three anonymousreviewers.
This work was partially sup-ported by the European Union under H2020project SUMMA, grant 688139, and by Fun-da?
?o para a Ci?ncia e Tecnologia (FCT),2025Ara.
Ger.
Spa.
Fre.
Ita.
Dut.
Pol.
Br.
Pt.
Rom.
Rus.
Tur.MT Baseline [HB14] 42.9 46.5 51.8 52.6 51.4 50.5 44.5 47.0 49.3 43.2 40.9DOC/ADD single [HB14] 41.0 42.4 38.3 47.6 48.5 26.4 40.2 35.4 41.8 44.8 45.2DOC/ADD joint [HB14] 39.2 40.5 44.3 44.7 47.5 45.3 39.4 40.9 44.6 47.6 41.7Joint,R`2, K = 15 41.8 46.6 46.6 46.0 48.7 52.5 39.5 40.8 47.6 44.9 47.2Joint,R`1, K = 15 44.0 44.7 49.4 40.1 46.1 49.4 35.7 43.5 40.5 42.2 43.4Joint w/ Aux,R`2, K = 300 46.9 52.0 59.4 54.6 56.0 53.6 51.0 51.7 53.9 52.3 49.5Joint w/ Aux,R`1, K = 300 44.0 40.4 40.4 39.5 38.6 38.1 43.2 36.6 35.1 44.3 44.4Table 2: Cross-lingual experiments on the TED Corpus using English as a source language.
Reportedare the micro-averaged F1scores for a machine translation baseline and the two strongest systems ofHermann and Blunsom (2014), our one-stage joint system (Joint), and our two-stage system that trainsthe multilingual embeddings jointly with the auxiliary task of fitting pre-trained English embeddings(Joint w/ Aux), with both `1and `2regularization.
Bold indicates the best result for each target language.Ara.
Ger.
Spa.
Fre.
Ita.
Dut.
Pol.
Br.
Pt.
Rom.
Rus.
Tur.BOW baseline [HB14] 46.9 47.1 52.6 53.2 52.4 52.2 41.5 46.5 50.9 46.5 51.3Polyglot [HB14] 41.6 27.0 41.8 36.1 33.2 22.8 32.3 19.4 30.0 40.2 29.5DOC/ADD Single [HB14] 42.2 42.9 39.4 48.1 45.8 25.2 38.5 36.3 43.1 47.1 43.5DOC/ADD Joint [HB14] 37.1 38.6 47.2 45.1 39.8 43.9 30.4 39.4 45.3 40.2 44.1Joint w/ Aux,R`2, K = 300 48.6 54.4 57.5 55.8 56.9 54.5 46.1 51.3 56.5 53.0 49.5Joint w/ Aux,R`1, K = 300 52.4 47.8 57.8 50.0 53.3 52.3 47.6 49.0 49.2 51.4 50.9Table 3: Monolingual experiments on the TED Corpus.
Shown are the micro-averaged F1scores for abag-of-words baseline, a system trained on Polyglot embeddings, the two strongest systems of Hermannand Blunsom (2014), and our Joint w/ Aux system with `1and `2regularization.january_en science_en oil_en road_en speak_enjanuari_nl  ?`lw?_ar ?leo_pb route_fr spreken_nls?ubat_tr  ?`l?_ar olie_nl strada_it fala_pbgennaio_it ci?ncia_pb petrolio_it weg_nl  ?k?
?_arfebruarie_ro science_fr ?l_de drum_ro gesproken_nl?br ?r_ar s?tiint?a_ro p?trole_fr  ?syr_ar habla_esianuarie_ro wetenschap_nl petrol_tr estrada_pb konus?ma_trfebrero_es scienza_it petr?leo_es drogi_pl ???????
?_rujanvier_fr ciencia_es  ?nfX_ar lopen_nl horen_nl?nA?r_ar wissenschaft_de petr?leo_pb strade_it mowy_pljaneiro_pb cient?fica_pb petrol_ro drodze_pl vorbeasc?a_roenero_es nauka_pl aceite_es wegen_nl spreekt_nlseptember_nl bilim_tr rope?_pl yol_tr  ?d?_arsettembre_it s,tiint,a_ro ????
?_ru camino_es sprechen_deseptiembre_es s,tiint,?a_ro petrolul_ro conduce_ro ii_roseptember_de nauki_pl ????
?_ru andar_pb discours_frekim_tr ????
?_ru ?fX_ar ???
?_ru sentire_itFbtmbr_ar ?l?_ar ropy_pl syr_ar contar_pbfebbraio_it ?lw?_ar E?_ar ?????
?_ru ???
?_ruseptembrie_ro scientifica_it ulei_ro yolculuk_tr JP_arsetembro_pb scienze_it  ?z?yola_tr poser_frTable 4: Examples of nearest neighbor words for the multilingual embeddings trained with our Jointw/ Aux system with `2regularization.
Shown for each English word are the 20 closest target words inEuclidean distance, regardless of language.2026through contracts UID/EEA/50008/2013,through the LearnBig project (PTDC/EEI-SII/7092/2014), and the GoLocal project (grantCMUPERI/TIC/0046/2014).ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word represen-tations for multilingual NLP.
arXiv preprintarXiv:1307.1662 .Yasemin Altun, Mikhail Belkin, and David A.McAllester.
2005.
Maximum Margin Semi-Supervised Learning for Structured Variables.In Advances in Neural Information ProcessingSystems 18. pages 33?40.Yoshua Bengio, R?jean Ducharme, Pascal Vin-cent, and Christian Janvin.
2003.
A neural prob-abilistic language model.
Journal of MachineLearning Research 3:1137?1155.Mauro Cettolo, Christian Girardi, and MarcelloFederico.
2012.
Wit3: Web inventory of tran-scribed and translated talks.
In Proc.
of the16th Conference of the European Associationfor Machine Translation.
pages 261?268.Sarath Chandar, Stanislas Lauly, Hugo Larochelle,Mitesh M. Khapra, Balaraman Ravindran,Vikas Raykar, and Amrita Saha.
2014.
AnAutoencoder Approach to Learning BilingualWord Representations.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neu-ral networks.
In Proc.
of Empirical Methods forNatural Language Processing.
pages 740?750.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language process-ing: Deep neural networks with multitask learn-ing.
In Proc.
of the International Conference onMachine Learning.
ACM, pages 160?167.Ronan Collobert, Jason Weston, L?on Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural language processing (al-most) from scratch.
Journal of Machine Learn-ing Research 12:2493?2537.Jocelyn Coulmance, Jean-Marc Marty, GuillaumeWenzek, and Amine Benhalloum.
2015.
Trans-gram, Fast Cross-lingual Word-embeddings.Proceedings of the 2015 Conference on Empir-ical Methods in Natural Language Processing(EMNLP) pages 1109?1113.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for OnlineLearning and Stochastic Optimization.
Journalof Machine Learning Research 12:2121?2159.Manaal Faruqui, Jesse Dodge, Sujay K Jauhar,Chris Dyer, Eduard Hovy, and Noah A Smith.2015.
Retrofitting word vectors to semantic lex-icons.
In Proc.
of Annual Meeting of the North-American Chapter of the Association for Com-putational Linguistics.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multi-lingual correlation.
In Proc.
of Annual Meet-ing of the European Chapter of the Associationfor Computational Linguistics.
Association forComputational Linguistics.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-lingual discriminative learning of sequencemodels with posterior regularization.
In Proc.of Empirical Methods in Natural Language Pro-cessing.Kuzman Ganchev, Joao Graca, John Blitzer, andBen Taskar.
2008.
Multi-view learning overstructured and non-identical outputs.
In Proc.of Conference on Uncertainty in Artificial Intel-ligence.Stephan Gouws, Yoshua Bengio, and Greg Cor-rado.
2015.
BilBOWA: Fast Bilingual Dis-tributed Representations without Word Align-ments.
Proceedings of the 32nd InternationalConference on Machine Learning (2015) pages748?756.Karl Moritz Hermann and Phil Blunsom.
2014.Multilingual Models for Compositional Dis-tributed Semantics.
Proceedings of ACL pages58?68.Rebecca Hwa, Philip Resnik, Amy Weinberg,Clara Cabezas, and Okan Kolak.
2005.
Boot-strapping parsers via syntactic projection acrossparallel texts.
Natural language engineering11(3):311?325.Alexandre Klementiev, Ivan Titov, and BinodBhattarai.
2012.
Inducing crosslingual dis-tributed representations of words.
24th Inter-national Conference on Computational Linguis-tics - Proceedings of COLING 2012: TechnicalPapers (2012) pages 1459?1474.Philipp Koehn.
2005.
Europarl: A parallel corpus2027for statistical machine translation.
MT summit11.Quoc Le and Tomas Mikolov.
2014.
DistributedRepresentations of Sentences and Documents.International Conference on Machine Learning- ICML 2014 32:1188?1196.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.Improving distributional similarity with lessonslearned from word embeddings.
Transactions ofthe Association for Computational Linguistics3:211?225.David D. Lewis, Yiming Yang, Tony G. Rose, andFan Li.
2004.
RCV1: A New Benchmark Col-lection for Text Categorization Research.
Jour-nal of Machine Learning Research 5:361?397.D.
C. Liu and J. Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimiza-tion.
Mathematical Programming 45:503?528.Minh-Thang Luong, Hieu Pham, and Christo-pher D. Manning.
2015.
Bilingual Word Repre-sentations with Monolingual Quality in Mind.Workshop on Vector Modeling for NLP pages151?159.Andr?
F. T. Martins.
2015.
Transferring corefer-ence resolvers with posterior regularization.
InACL.Ryan McDonald, Slav Petrov, and Keith Hall.2011.
Multi-source transfer of delexicalized de-pendency parsers.
In Proc.
of Empirical Meth-ods in Natural Language Processing.Tomas Mikolov, Greg Corrado, Kai Chen, and Jef-frey Dean.
2013.
Efficient Estimation of WordRepresentations in Vector Space.
Proceedingsof the International Conference on LearningRepresentations (ICLR 2013) pages 1?12.Jeffrey Pennington, Richard Socher, and Christo-pher D Manning.
2014.
GloVe: Global Vectorsfor Word Representation.
In Proceedings of the2014 Conference on Empirical Methods in Nat-ural Language Processing.
pages 1532?1543.Kaare Brandt Petersen and Michael Syskind Ped-ersen.
2012.
The Matrix Cookbook.Hieu Pham, Minh-Thang Luong, and Christo-pher D. Manning.
2015.
Learning Dis-tributed Representations for Multilingual TextSequences.
Workshop on Vector Modeling forNLP pages 88?94.Tianze Shi, Zhiyuan Liu, Yang Liu, and MaosongSun.
2015.
Learning Cross-lingual Word Em-beddings via Matrix Co-factorization.
AnnualMeeting of the Association for ComputationalLinguistics pages 567?572.Vikas Sindhwani, Partha Niyogi, and MikhailBelkin.
2005.
A co-regularization approach tosemi-supervised learning with multiple views.In Proceedings of ICML workshop on learningwith multiple views.
Citeseer, pages 74?79.Richard Socher, Jeffrey Pennington, andEh Huang.
2011.
Semi-supervised recur-sive autoencoders for predicting sentimentdistributions.
In Conference on EmpiricalMethods in Natural Language Processing.pages 151?161.Hubert Soyer, Pontus Stenetorp, and AkikoAizawa.
2015.
Leveraging Monolingual Datafor Crosslingual Compositional Word Repre-sentations.
Proceedings of the 2015 Interna-tional Conference on Learning Representations(ICLR) .Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word representations: a simple and gen-eral method for semi-supervised learning.
InProc.
of the Annual Meeting of the Associationfor Computational Linguistics.Daniel Zeman and Philip Resnik.
2008.
Cross-language parser adaptation between related lan-guages.
In IJCNLP.
pages 35?42.Will Y Zou, Richard Socher, Daniel M Cer, andChristopher D Manning.
2013.
Bilingual wordembeddings for phrase-based machine transla-tion.
In Proc.
of Empirical Methods for NaturalLanguage Processing.
pages 1393?1398.2028
