Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 826?835,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsAnalyzing Linguistic Knowledge in Sequential Model of SentencePeng Qian Xipeng Qiu?
Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pqian11, xpqiu, xjhuang}@fudan.edu.cnAbstractSentence modelling is a fundamental top-ic in computational linguistics.
Recently,deep learning-based sequential models of sen-tence, such as recurrent neural network, haveproved to be effective in dealing with thenon-sequential properties of human language.However, little is known about how a recurrentneural network captures linguistic knowledge.Here we propose to correlate the neuronactivation pattern of a LSTM language modelwith rich language features at sequential,lexical and compositional level.
Qualitativevisualization as well as quantitative analy-sis under multilingual perspective reveals theeffectiveness of gate neurons and indicatesthat LSTM learns to allow different neuronsselectively respond to linguistic knowledgeat different levels.
Cross-language evidenceshows that the model captures different as-pects of linguistic properties for differentlanguages due to the variance of syntacticcomplexity.
Additionally, we analyze theinfluence of modelling strategy on linguisticknowledge encoded implicitly in differentsequential models.1 IntroductionSentence modelling is a central and fundamentaltopic in the study of language generation andcomprehension.
With the application of populardeep learning methods, researchers have found thatrecurrent neural network can successfully model thenon-sequential linguistic properties with sequential?Corresponding author.data input (Vinyals et al, 2015; Zhou and Xu,2015; Rockta?schel et al, 2015).
However, dueto the complexity of the neural networks and thelack of effective analytic methodology, little isknown about how a sequential model of sentence,such as recurrent neural network, captures linguisticknowledge.
This makes it hard to understandthe underlying mechanism as well as the model?sstrength and weakness.
Previous work (Li et al,2016) has attempted to visualize neural models inNLP, but only focus on analyzing the hidden layerand sentiment representation rather than grammarknowledge.Currently, there have been a few attempts (Yo-gatama et al, 2014; Ko?hn, 2015; Faruqui and Dyer,2015) at understanding what is embedded in theword vectors or building linguistically interpretableembeddings.
Few works focus on investigatingthe linguistic knowledge encoded in a sequentialneural network model of a sentence, not to mentionthe comparison of model behaviours from a cross-language perspective.
Our work, therefore, aims toshedding new insights into the following topics:a) How well does a sequential neural model (e.g.language model) encodes linguistic knowledgeof different levels?b) How does modelling strategy (e.g.
the optimiza-tion objective) influence the neuron?s ability ofcapturing linguistic knowledge?c) Does the sequential model behave similarly to-wards typologically diverse languages?To tackle the questions above, we propose tovisualize and analyze the neuron activation pattern826d?tem (chidren.Female.Plural.Dative)en (in.ADP.case)1868 (1868.NUM.nmod), (,.PUNCT.punct)le (the.DET.det)journal (journal.NOUN.nsubjpass)est (is.AUX.auxpass)transform?
(transform.VERB.root)en1868,lejournalesttransform?Model????
?
?ActivationInput Sequence(journal.NOUN.nsubjpass)?
?
?EEGSignalProcessing Timeen1868,lejournalesttransform?Model????
?
?ActivationInput Sequence(journal.NOUN.nsubjpass)?
?
?EEGSignalProcessing Time??
?BrainIs the mappingpossible?en1868,lejournalestModel????
?
?ActivationInput Sequence(NOUN.nsubjpass)Is the mapping possible?Figure 1: Experiment paradigm: correlating thedynamic activation pattern of the model neuronswith linguistic features .ID (Non-)Linguistic Knowledge LevelI Sequence Length SequentialII Gender / Definiteness LexicalPart-of-SpeechIII Case / VerbForm / Mood CompositionalSyntactic RoleTable 1: List of the linguistic features to becorrelated with model neuron behaviours.so as to understand how a sequential neural modelof sentence encodes linguistic properties of differentlevel.
By training vanilla LSTM language modelswith multilingual data and correlating the modelneuron?s activation with various linguistic features,we not only qualitatively show the activation patternof a certain model neuron, but also quantify theselectivity of the neuron towards input language dataor certain linguistic properties.2 Methodology2.1 A ?Brain?
Metaphor of Artificial ModelMitchell et al (2008) correlates brain activities withlinguistic stimuli under a popular brain-mappingparadigm.
Since brain is a ?black box?, researcherswant to decode what is represented in a certainneuronal cluster of the brain at a certain time step.Here we propose that this paradigm can be appliedto similar ?black-box?
model, such as the neuralnetwork.
This is what we call a ?brain?
metaphorof the artificial model, as is visualized in Figure 1.We treat the neural network as a simplified ?brain?.We correlate the neuron behaviours with the inputstimuli and design experiments to map the neuronactivation to an explicit linguistic feature.A sentence is, of course, a linear sequentialarrangement of a cluster of words, but more than justa simple addition of words, as there exist compli-cated non-sequential syntactic relations.
Thus, weconsider three levels of features in the analysis ofmodel behaviours, a) Sequential feature, a kind ofsuperficial feature shared by any sequence data, b)Lexical feature, which is stable and almost indepen-dent of the sentence context, and c) Compositionalfeature, which is required for building the meaningof a sentence.
Table 1 lists the details of the featuresinvolved in this paper.2.2 Model DescriptionSince the goal is to understand the internal neurons?behaviour and how the behaviour patterns can beinterpreted as a way to encode dynamic linguisticknowledge, we choose the most fundamental se-quential sentence models as the research objects.
Wedo not consider tree-structured model, as it explicitlyinvolves linguistic structure in model architecture.We focus on word-based language model and com-pare it to two other counterparts in this paper.Word-based Language Model Word-based lan-guage model (Mikolov et al, 2010) predicts theincoming word given the history context.Character-based Language Model Instead ofpredicting the next word, character-based languagemodel (Hermans and Schrauwen, 2013) predictsthe incoming character given the history charactersequence.Task-specific Model A common task-specificmodel takes word sequence as input, but onlypredicts the category (e.g.
sentiment) of the sentenceafter all the words are processed.
In this paper, weconsider a sequential model utilized for sentimentanalysis task.All the three sequential models are built on recur-rent neural network with LSTM unit (Hochreiter andSchmidhuber, 1997).
LSTM unit has a memory cellc and three gates: input gate i, output gate o andforget gate f , in addition to the hidden layer h ofa vanilla RNN.
The states of LSTM are updated asfollows:it = ?
(Wxixt +Whiht?1 +Wcict?1 + bi), (1)ft = ?
(Wxfxt +Whfht?1 +Wcfct?1 + bf ), (2)ct = ft  ct?1+ it  tanh(Wxcxt +Whcht?1 + bc), (3)ot = ?
(Wxoxt +Whoht?1 +Wcoct?1 + bo), (4)827ht = ot  tanh(ct), (5)where xt is the input vector at the current timestep, ?
denotes the logistic sigmoid function anddenotes elementwise multiplication.The dimension of the embeddings and the LSTMunit is 64.
All three models use pretrained wordembedding from Polyglot multilingual embeddings(Al-Rfou et al, 2013) trained with C&W (Collobertet al, 2011) model on Wikipedia.
We train a lotof word-based and character-based LSTM languagemodels with multilingual data from the UniversalTreebank 1.2 (Joakim Nivre and Zhu, 2015), aswell as a task-specific sentiment model on StanfordSentiment Treebank (Socher et al, 2013b).
Weseparate the training and testing data according to90%/10% principle.
We stop training when the lossof the test data does not decrease.Regarding the analysis of the model behaviours,we collect the internal neuron activation of thehidden layer, three gates, and memory cell for allthe data in the treebank/sentiment corpus1.
For thesake of notation, we refer the hidden layer, inputgate, output gate, forget gate and memory cell ash, i, f , o, c for three models, word-based languagemodel (WL), character-based language model (CL)and task-specific model for sentiment analysis (SA).We mark the index of the neuron in the superscriptand the meta information about the model in thesubscript.3 Qualitative Analysis3.1 Sequential FeatureKarpathy et al (2015) finds that some memoryneurons of the character language model are se-lective towards the length of the input sequence.Similar patterns are also observed among the mem-ory neuron activation pattern of the word-levellanguage model as is shown in Figure 2, where deeppurple color indicate strong negative activation anddeep green color indicate strong positive activation.Moreover, we compute the correlation between theinput sequence length and the activation pattern of1The analyses cover languages such as English (en), German(de), Latin (la), Ancient Greek (grc), Bulgarian (bg), Spanish(es), Portuguese (pt), Italian (it), French (fr), Dutch (nl),Norwegian (no), Hindi (hi), Slovenian (sl), Hungarian (hu),Indonesian (id) and Chinese (zh).Baghdadis do n'tventuremuch out of theirneighbourhoods any more , you neverknowwhere you might get stuck .Figure 2: Memory neuron c21en,WL that are sensitiveto the length of the input word sequence.en fr la grc es pt it nl no bg cs hi zh0.20.40.60.81MaxCorrelationrh if ocFigure 3: Comparison of neurons on correlatingwith the length of input sequence.
Only the bestcorrelation results are reported.iftheseweaponshavegonemissing it 's aterrifyingprospect .
"(a) h30en,WL + raw sentweaponshave if itprospect athese "missing.
'sterrifying gone(b) h30en,WL + shuffled sentFigure 4: Visualising the activation of a neurontowards raw English sentences and sentences withshuffled word order.every single neuron of h, i, f , o, c. Quantitativeresults in Figure 3 reveal that none of the hiddenlayer or gate neurons are strongly correlated withthis sequential feature.3.2 Lexical FeatureFor a inner neuron of the model, we can get theactivation of a certain neuron in a certain modelcomponent towards a certain input word.
A modelneuron may be most active towards the words ofsome category instead of other words.We notice that some neurons (e.g.
Neuronh30en,WL) strongly activate towards functional wordssuch as the determiners ?a?, as is visualized in Figure4.
This activation can be observed even when wefeed the model with a abnormal English sentencewith shuffled word order.Since it is not easy to go through all the neuronactivation pattern, we design a visualization methodto vividly show how a neuron selectively respond to828(a) h22en,WL (b) h30en,WL (c) h35en,WL (d) h23en,WL(e) i30en,WL (f) f18fr,WL (g) i54pt,WL (h) i28de,WLFigure 5: Visualising the lexical space responded by a certain neuron of the word-level language modeltrained on different languages.certain words, inspired by the work in Cukur et al(2013) and Huth et al (2016).Suppose the vocabulary of a language spansa default lexical space.
An internal neuron ofthe artificial model modulates this linguistic spacevia showing selective activation pattern towardscertain words.
we carry out PCA on all the wordembedding in the language model and extract themost prominent 3 principal components as the bases.Then we project the word vectors onto these threebasis to get a new representation of the words in alow dimensional space.
We draw all the words on aplane, where the location of each word is determinedby the first two components and the text coloris determined by three main components as RGBvalue.
To visualize how a target neuron x respond tothis lexical space, we modify the appearance of theword by scaling the size of the word text against theproduct of the log-transformed word frequency andthe absolute value of the mean activation, and settingthe degree of transparency of the text against therelative positive/negative activation strength amongall the existed activation value of a target neuronx.
In this way, large font size and low degree oftransparency of a word w indicate that the targetneuron x frequently activates towards the wordw.
This means that we can interpret a neuron?sselectivity towards the lexical space just by lookingfor large and explicit words on the visualized two-dimensional space.Figure 5 visualizes the lexical space responded byfour hidden layer neuron of the English languagemodel, as well as four gate neurons of differentlanguages respectively.
We can see that wordswith the similar grammatical functions are locatednear each other.
Besides, it is interesting to seethat some hidden layer neurons activate selectivelytowards determiner words, pronoun, preposition orauxiliary verbs.
This phenomena have also beenobserved on gate neurons.
For example, forget gateneuron f18fr,WL activates towards the determiners inFrench.
Input gate neuron i54pt,WL activates towardsthe determiners in Portuguese.
Notice that not allof the inner neurons show interpretable activationpattern towards the lexical space.829Sharon 'shard line hasworked intandem withHamas 'sterrorism toratchet uptensionsfurtherandfurther ,which spillover into theMuslim world and serve as arecruitingtool for al -Qaeda in itssearch foragentswilling to hit theUnitedStates .Sharon 'shard line hasworked intandem withHamas 'sterrorism toratchet uptensionsfurtherandfurther ,which spillover into theMuslim world and serve as arecruitingtool for al -Qaeda in itssearch foragentswilling to hit theUnitedStates .Figure 6: Visualising the Neuron h35en,WL neuronactivation towards verb-preposition composition.3.3 Compositional FeatureTo validate whether the internal neuron of the modelcan discriminate the local composition and long-distance composition, we choose the preposition asthe object for observation.In English, preposition can be combined with theprevious verb to form a compound verbal phrase,such as ?check it in?,?give him up?, ?find out whatit will take?.
This function of the preposition isannotated as the compound particle in the UniversalDependency Treebank.
Another function of thepreposition is to serve as the case marker, such as thepreposition in the phrase ?lives in the central area?,?Performances will be performed on a project basis?.Given that these two functions of the preposition arenot explicitly discriminated in the word form, thelanguage model should tell the difference betweenthe prepositions served as the compound particleand the prepositions served as the case marker ifit indeed has the ability to handle word meaningcomposition.For the hidden layer, we notice that hidden layerneuron h35en,WL is sensitive to the function of thepreposition.
It only activates when the possiblepreposition does not form a composition with theformer verb, as is vividly shown in Figure 6.
Theprepositions marked by dashed box serve as casemarker while those in solid box form a phrase withprevious verb.
The activation pattern are obviouslydifferent.
Similar pattern is also found in the gateneurons.4 Quantitative Analysis4.1 Decoding Lexical/Compositional FeatureVisualization only provides us with an intuitiveidea of what a single neuron is encoding whenprocessing language data.
In this section, weemploy a mapping paradigm to quantitatively rev athe linguistic knowledg distributed in the modelcomponents.Instead of looking at one single neuron, herewe use the whole 64 neurons of each modelcomponent as a 64-dimensional vector h, i, f , o,c respectively.
The basic method is to decodeinterpretable linguistic features from target neuronclusters, which has been used in (Ko?hn, 2015; Qianet al, 2016).
We hypothesize that there exists amap between a neuron cluster activation vector xand a high-level sparse linguistic feature vector yif the neuron cluster?s activation pattern implicitlyencode sufficient information about a certain lexicalor compositional feature.Hence we design a series of experiments to mapthe hidden layer, three gates, and memory cellvector activated by a target input word w in asentence to the corresponding linguistic features ofthe word w, which are annotated in the UniversalDependency Treebank.
Our experiments coverPOS TAG, SYNTACTIC ROLE, GENDER, CASE,DEFINITENESS, VERB FORM and MOOD.
Theselinguistic features are all represented as a one-hotvector.
The mapping model is a simple softmaxlayer, with the activation vector as the input andthe sparse vector as the output.
For each linguisticfeature of each language, a mapping model is trainedon the randomly-selected 90% of all the word tokensand evaluated over the remaining 10%.
Notice thatGENDER, CASE, DEFINITENESS, VERB FORM,and MOOD only apply to certain word categories.We give a default ?N/A?
tag to the words withoutthese annotations so that all the word can be usedfor training.
The evaluation result is only computedfrom the words with the features.
This requires themapping model to not only recognize the differencesbetween the sub-categories of a linguistic feature(e.g.
CASE), but also discriminate the words thatwe are interested in from other unrelated words (e.g.words without CASE annotations).
Accuracies foreach model component h, i, f , o, c are reported inFigure 7 and 8.Comparing different model components, we no-tice that gate neurons except output gate are general-ly better than hidden layer and memory cell neuronson decoding linguistic knowledge.
Input gate and830en bg sl fr de la nl no pt es it hu id hi zh grc0.40.60.8Accuracyh if oc(a) POS TAGen bg sl fr de la nl no pt es it hu id hi zh grc0.40.50.60.7Accuracyh if oc(b) SYNTACTIC ROLEbg es pt sl nl la no0.40.60.8Accuracy(c) CASEbg es pt it no0.20.40.60.8(d) GENDERFigure 7: Comparison of neurons on decoding POSAG, SYNTACTIC ROLE, CASE, and GENDER.forget gate are the best, while memory cell is theworst.
It shows that the gates of a recurrent languagemodel are more sensitive to the grammar knowledgeof the input words.Comparing decoding results on different lan-guages, we find that it is generally easier to de-code POS TAG than SYNTACTIC ROLE for allthe languages.
One interesting thing is that themapping model works better with Bulgarian, a slaviclanguage, but worse on Norwegian on decodingCASE while the situation is opposite on decodingGENDER.
It might be because that gender is aweakened grammatical feature in Bulgarian.
There-fore, knowledge about GENDER may not be soimportant in building the grammatical structure ofthe Bulgarian language data.es pt it no nl0.20.40.6Accuracyh if oc(a) VERB FORMes pt it no0.20.40.60.8Accuracy(b) TENSEbg es pt it no0.40.60.81h if oc(c) DEFINITENESSFigure 8: Comparison of LSTM neurons on decod-ing VERB FORM, TENSE, and DEFINITENESS.4.2 The Dynamics of Neuron BehaviourSince sentence meaning is dynamically constructedby processing the input sequence in a word-by-word way, it is reasonable to hypothesize that thelinguistic feature of an input word w won?t sharplydecay in the process.
Naturally, we would like toask whether it is possible to decode, or at leastpartially infer, a word?s property from the neuronbehaviours of its context words.
Specifically, ifthe model process a verbal phrase ?spill over?
or?in the garden?, will the property of the word?spill?, ?in?
be combined with the following wordand decodable from the model neuron activationbehaviours towards the following word, or will theproperty of the word ?over?, ?the garden?
be primedby the previous word and decodable from the modelneuron behaviours towards the previous word?To quantitatively explore this question, we carryout a mapping experiment similar to the previousone.
The difference is that here we map thehidden layer, three gates, and memory cell vectoractivated by a target input word w in a sentenceto the corresponding linguistic features of the pre-vious/following word w?2/?1/w+1/+2 in a 5-wordwindow context.
Results in Figure 9 shows that thelinguistic feature POS TAG is partially primed or831-2 -1 0 1 2 hi foc0.5Word Position Model NeuronAccuracyFigure 9: Neuron dynamics on decoding POS.ISO Language f i o hen English 0.316 -0.022 0.107 0.156la Latin 0.152 0.131 0.158 0.085grc Ancient Greek 0.293 0.248 0.166 0.274pt Portuguese 0.301 0.313 0.161 0.209nl Dutch 0.196 0.096 0.205 -0.134no Norwegian 0.335 0.057 0.269 0.033bg Bulgarian 0.324 0.280 0.071 -0.082Table 2: Comparison of model components?
corre-lation with tree structure stastistics.kept in the context words in English.
The longerdistance, the less probability to decode it fromthe neuron activations.
Still, the nearest contextwords w?1 and w+1 prime/keep the most relevantinformation of the target word w. Similar patternsare also found for other linguistic feature in otherlanguages.4.3 Correlation with Dependency TreeSince the sequential model can modelling non-sequential input, we naturally want to know whetherany component of the model is dynamically corre-lated with the statistics of tree structure.
Inspiredby the case study in Zhou and Xu (2015), we countthe syntactic depth of each word in a sentence andcompute the correlation between the depth sequenceand the dynamics of the average activation of themodel neurons in Table 2.
We did not find strongcorrelation between the mean neuron activationdynamics with the syntactic tree depth.
One possibleexplanation is that the language model only use thehistory information, while the depth of a word iscomputed in a relative global context.5 Model ComparisonIn this section, we would like to investigate whetherdifferent sentence modelling strategy and optimiza-tion objective affect the neuron?s implicit encodingof linguistic knowledge, especially the grammaticalproperties.5.1 Word vs. CharacterIt is obvious that word-based language model andcharacter-based language model intend to model thelanguage data at different granularity.
Although bothof them are effective, the latter is often criticized foran unreasonable modelling strategy.In addition to the findings in Karpathy et al(2015), we see that some of the hidden layer neuronsof the character-based language model seems to besensitive to specific characters and character cluster-s, as is indicated from the visualization of the neuronactivation pattern in Figure 10.
We are surprised tofind that some neuron of the hidden layer activatesselectively towards white space character.
This isinteresting as it means that the model learns todetect word boundary, which is exactly an importantlinguistic feature.Besides, some neuron activates selectively to-wards vowel/consonant characters in a phonograph-ic language, such as English.
This interestingphenomenon also indicates that the model implic-itly captures the phonology system, since it candiscriminate the vowel character clusters from theconsonant character clusters.
We also find these twodetectors in other languages, such as Indonesian andCzech in Figure 10.5.2 Word Prediction vs. Task-specific ModelWe compare a word-based LSTM language modeland a word-based LSTM sentiment model.
Here,for a fair comparison, all the models are trained onlyon the Stanford Sentiment Treebank Dataset (Socheret al, 2013a).
The results show that the neuronsin these two models displays similar behaviourstowards superficial sequential features, but totallydifferent behaviours towards high-level linguisticfeatures, such as semantic and syntactic knowledge.Both some of the internal neurons of the mem-ory cell in the language model and the sentimentmodel emerge to be sensitive to the length of the832l i k e h a v i n g j .
e d g a r h o o v e r u n w i t t i n g l yl i k e h a v i n g j .
e d g a r h o o v e r u n w i t t i n g l yj a r a k d e k a t , d u a k a p a l p e r u s a k , s t e r ej a r a k d e k a t , d u a k a p a l p e r u s a k , s t e r el n ?
h o t e x t u n a s t r ?
n c e a 4 , p i ?
d k o v ?
(a) h19en,CL, h33id,CL, h9cs,CL detect white space.n o s i n f o r m a c e f a x e m z h r u b a t i k r ?
t r y c h l1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0(b) h16en,CL, h7id,CL, h15cs,CL detect phonology.Figure 10: Visualising the activation of hidden neurons of English, Indonesian and Czech language model.h i f o c0.40.60.8Model NeuronsAccuracy WordLMCharLMSAFigure 11: Comparison between the internal neuronsof English word-based language model, character-based language model and sentiment model ondecoding POS TAG.input sequence, as we expected, since the sentencelength is a non-linguistic features shared by all thesequential input.
However, different optimizationobjectives force the models to represent and capturethe linguistic properties of different aspects.
Thelanguage model focus more on the syntactic aspect,as is visualized and quantified in previous sections.Neurons of the sentiment model tends to be sensitiveonly towards the sentiment aspect of the words,although the sentiment model use the similar LSTMunit, dimensionality and pretrained embedding.
Weapply the same visualization method in Section 3.2to the 64 hidden layer neurons of the sentiment mod-el and manually interpret the visualization resultsone by one.
We did not see any strong activationpattern towards the functional words like thosefound in language model hidden layer neurons.To quantify the differences of the linguisticknowledge encoded in different sentential model,we again use the previous feature-decoding exper-iment method.
We compare the performance of thecomponents in three models on decoding POS TAGfrom English data.
Notice that we use StanfordPOS Tagger (Kristina Toutanova and Singer, 2003)to automatically tag the sentences in the sentimentdata.
For the character-based language model, weuse the neuron activation towards the end characterof each words in the decoding experiment.Results in Figure 11 shows that even a character-based language model can achieve pretty well ondecoding the most important lexical features fromthe activation pattern of the internal neurons.
Thisis a strong evidence that word-level feature detectorcan emerge from a pure character-based model.Sentiment model, on the contrary, fails to capture thegrammatical knowledge, although we might thinkthat a successful sentiment analysis model should beable to combines the grammar property of the wordswith the sentiment information.
Current resultsindicate that for pure sequential model with vanillaLSTM units, the objective of the sentence modellingtasks will largely affect how the model acquires andencodes linguistic knowledge.6 Related WorksKarpathy et al (2015) explores the memory cell incharacter-based language model.
Their visualizationresults show some interesting properties of thememory neurons in LSTM unit.
However, their ex-ploration on character-based model does not intendto correlate high-level linguistic knowledge, whichare intuitively required for sequential modelling of asentence.Li et al (2016) propose a method for visualizingRNN-based sentiment analysis models and word-based LSTM auto-encoder in NLP tasks.
Li et al(2015) investigates the necessity of tree structurefor the modelling non-sequential properties of lan-guages.
Bowman et al (2015) studies the LSTM?sability of capturing non-sequential tree structure.Despite the useful findings, these works make noattempts to investigate the internal states of theneurons for a better understanding of the model?spower or weakness.Our work not only provides qualitative visual-ization of model neurons?
behaviours and detailed833quantitative investigation with multilingual evidence(16 for POS decoding experiment), but also revealthe influence of language syntactic complexity andmodelling strategy on how well the internal neuronscapture linguistic knowledge, which have beenoverlooked by previous work on interpreting neuralnetwork models.7 ConclusionIn this work, we analyze the linguistic knowledgeimplicitly encoded in the sequential model of sen-tence.
Through the visualization and quantificationof the correlation between the neuron activationbehaviour of different model components and lin-guistic features, we summarize that:?
Model neurons encode linguistic features atdifferent level.
Gate neurons encode more lin-guistic knowledge than memory cell neurons.?
Low-level sequential features are shared acrossmodels while high-level linguistic knowledge(lexical/compositional feature) are better cap-tured by language model instead of task-specified model on sentiment analysis.?
Multilingual evidence indicates that the modelare sensitive to the syntactic complexity ofthe language.
It would also be a promisingdirection to incorporate the factor of languagetypological diversity when designing advancedgeneral sequential model for languages otherthan English.?
Word-level feature detector can emerge from apure character-based model, due to the utilityof character composition.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir valuable comments.
This work was partiallyfunded by National Natural Science Foundation ofChina (No.
61532011 and 61672162), the NationalHigh Technology Research and Development Pro-gram of China (No.
2015AA015408).ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed word representations for multi-lingual nlp.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning,pages 183?192, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Samuel R. Bowman, Christopher D. Manning, andChristopher Potts.
2015.
Tree-structured com-position in neural networks without tree-structuredarchitectures.
Proceedings of the NIPS Workshopon Cognitive Computation: Integrating Neural andSymbolic Approaches.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Tolga Cukur, Shinji Nishimoto, Alexander G Huth, andJack L Gallant.
2013.
Attention during natural visionwarps semantic representation across the human brain.Nature Neuroscience, 16(6):763?70.Manaal Faruqui and Chris Dyer.
2015.
Non-distributional word vector representations.
arXivpreprint arXiv:1506.05230.M.
Hermans and B. Schrauwen.
2013.
Training andanalysing deep recurrent neural networks.
Advancesin Neural Information Processing Systems, pages 190?198.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Alexander G. Huth, Wendy A.
De Heer, Thomas L.Griffiths, Frdric E. Theunissen, and Jack L. Gallant.2016.
Natural speech reveals the semantic maps thattile human cerebral cortex.
Nature.Maria Jesus Aranzabe Masayuki Asahara Aitziber Atutx-a Miguel Ballesteros John Bauer Kepa Bengoetx-ea Riyaz Ahmad Bhat Cristina Bosco Sam Bow-man Giuseppe G. A. Celano Miriam Connor Marie-Catherine de Marneffe Arantza Diaz de Ilarraza KajaDobrovoljc Timothy Dozat Tomaz?
Erjavec Richa?rdFarkas Jennifer Foster Daniel Galbraith Filip GinterIakes Goenaga Koldo Gojenola Yoav Goldberg BertaGonzales Bruno Guillaume Jan Hajic?
Dag Haug RaduIon Elena Irimia Anders Johannsen Hiroshi KanayamaJenna Kanerva Simon Krek Veronika Laippala A-lessandro Lenci Nikola Ljubes?ic?
Teresa Lynn Christo-pher Manning Ctlina Mrnduc David Marec?ek He?ctorMart?
?nez Alonso Jan Mas?ek Yuji Matsumoto RyanMcDonald Anna Missila?
Verginica Mititelu YusukeMiyao Simonetta Montemagni Shunsuke Mori HannaNurmi Petya Osenova Lilja ?vrelid Elena PascualMarco Passarotti Cenel-Augusto Perez Slav PetrovJussi Piitulainen Barbara Plank Martin Popel ProkopisProkopidis Sampo Pyysalo Loganathan RamasamyRudolf Rosa Shadi Saleh Sebastian Schuster WolfgangSeeker Mojgan Seraji Natalia Silveira Maria SimiRadu Simionescu Katalin Simko?
Kiril Simov Aaron834Smith Jan S?te?pa?nek Alane Suhr Zsolt Sza?nto?
TakaakiTanaka Reut Tsarfaty Sumire Uematsu Larraitz UriaViktor Varga Veronika Vincze Zdene?k Z?abokrtsky?Daniel Zeman Joakim Nivre, Z?eljko Agic?
and HanzhiZhu.
2015.
Universal dependencies 1.2.
In LIN-DAT/CLARIN digital library at Institute of Formal andApplied Linguistics, Charles University in Prague.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.arXiv preprint arXiv:1506.02078.Arne Ko?hn.
2015.
Whats in an embedding?
analyzingword embeddings through multilingual evaluation.Christopher Manning Kristina Toutanova, Dan Klein andYoram Singer.
2003.
Part-of-speech tagging with acyclic dependency network.
In Proceedings of HLT-NAACL.Jiwei Li, Minh Thang Luong, Jurafsky Dan, and EudardHovy.
2015.
When are tree structures necessary fordeep learning of representations?
Proceedings ofEMNLP.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2016.
Visualizing and understanding neural models innlp.
In Proceedings of NAACL.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky`, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In INTER-SPEECH, pages 1045?1048.Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-son, Kai-Min Chang, Vicente L Malave, Robert AMason, and Marcel Adam Just.
2008.
Predictinghuman brain activity associated with the meanings ofnouns.
Science, 320(5880):1191?1195.Peng Qian, Xipeng Qiu, and Xuanjing Huang.
2016.
In-vestigating language universal and specific propertiesin word embeddings.
In Proceedings of ACL.Tim Rockta?schel, Edward Grefenstette, Karl MoritzHermann, Toma?s?
Koc?isky`, and Phil Blunsom.
2015.Reasoning about entailment with neural attention.arXiv preprint arXiv:1509.06664.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,Christopher Manning, and Andrew Ng andmChristo-pher Potts.
2013a.
Parsing with compositional vectorgrammars.
In EMNLP.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng, andChristopher Potts.
2013b.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the EMNLP.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Advances in NeuralInformation Processing Systems, pages 2755?2763.Dani Yogatama, Manaal Faruqui, Chris Dyer, andNoah A Smith.
2014.
Learning word representationswith hierarchical sparse coding.
arXiv preprintarXiv:1406.2035.Jie Zhou and Wei Xu.
2015.
End-to-end learning ofsemantic role labeling using recurrent neural networks.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.835
