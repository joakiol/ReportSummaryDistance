Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 932?940,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPConfidence Measure for Word AlignmentFei HuangIBM T.J.Watson Research CenterYorktown Heights, NY 10598, USAhuangfe@us.ibm.comAbstractIn this paper we present a confidence mea-sure for word alignment based on theposterior probability of alignment links.We introduce sentence alignment confi-dence measure and alignment link con-fidence measure.
Based on these mea-sures, we improve the alignment qual-ity by selecting high confidence sentencealignments and alignment links from mul-tiple word alignments of the same sen-tence pair.
Additionally, we removelow confidence alignment links from theword alignment of a bilingual trainingcorpus, which increases the alignmentF-score, improves Chinese-English andArabic-English translation quality and sig-nificantly reduces the phrase translationtable size.1 IntroductionData-driven approaches have been quite active inrecent machine translation (MT) research.
ManyMT systems, such as statistical phrase-based andsyntax-based systems, learn phrase translationpairs or translation rules from large amount ofbilingual data with word alignment.
The qual-ity of the parallel data and the word alignmenthave significant impacts on the learned transla-tion models and ultimately the quality of transla-tion output.
Due to the high cost of commissionedtranslation, many parallel sentences are automat-ically extracted from comparable corpora, whichinevitably introduce many ?noises?, i.e., inaccu-rate or non-literal translations.
Given the hugeamount of bilingual training data, word alignmentsare automatically generated using various algo-rithms ((Brown et al, 1994), (Vogel et al, 1996)Figure 1: An example of inaccurate translationand word alignment.and (Ittycheriah and Roukos, 2005)), which alsointroduce many word alignment errors.The example in Figure 1 shows the word align-ment of the given Chinese and English sentencepair, where the English words following each Chi-nese word is its literal translation.
We find untrans-lated Chinese and English words (marked withunderlines).
These spurious words cause signifi-cant word alignment errors (as shown with dashlines), which in turn directly affect the quality ofphrase translation tables or translation rules thatare learned based on word alignment.In this paper we introduce a confidence mea-sure for word alignment, which is robust to extraor missing words in the bilingual sentence pairs,as well as word alignment errors.
We proposea sentence alignment confidence measure basedon the alignment?s posterior probability, and ex-tend it to the alignment link confidence measure.We illustrate the correlation between the align-ment confidence measure and the alignment qual-ity on the sentence level, and present several ap-proaches to improve alignment accuracy based onthe proposed confidence measure: sentence align-ment selection, alignment link combination andalignment link filtering.
Finally we demonstrate932the improved alignments also lead to better MTquality.The paper is organized as follows: In section2 we introduce the sentence and alignment linkconfidence measures.
In section 3 we demon-strate two approaches to improve alignment accu-racy through alignment combination.
In section 4we show how to improve a MaxEnt word align-ment quality by removing low confidence align-ment links, which also leads to improved transla-tion quality as shown in section 5.2 Sentence Alignment ConfidenceMeasure2.1 DefinitionGiven a bilingual sentence pair (S,T ) whereS={s1,.
.
.
, sI} is the source sentence and T={t1,.
.
.
,tJ} is the target sentence.
Let A = {aij} bethe alignment between S and T .
The alignmentconfidence measure C(A|S, T ) is defined as thegeometric mean of the alignment posterior proba-bilities calculated in both directions:C(A|S, T ) =?Ps2t(A|S, T )Pt2s(A|T, S), (1)wherePs2t(A|S, T ) =P (A, T |S)?A?
P (A?, T |S).
(2)When computing the source-to-target algnmentposterior probability, the numerator is the sentencetranslation probability calculated according to thegiven alignment A:P (A, T |S) =J?j=1p(tj |si, aij ?
A).
(3)It is the product of lexical translation probabili-ties for the aligned word pairs.
For unaligned tar-get word tj , consider si = NULL.
The source-to-target lexical translation model p(t|s) and target-to-source model p(s|t) can be obtained throughIBM Model-1 or HMM training.
The denomina-tor is the sentence translation probability summingover all possible alignments, which can be calcu-lated similar to IBM Model 1 in (Brown et al,1994):?A?P (A?, T |S) =J?j=1I?i=1p(tj |si).
(4)Aligner F-score Cor.
Coeff.HMM 54.72 -0.710BM 62.53 -0.699MaxEnt 69.26 -0.699Table 1: Correlation coefficients of multiple align-ments.Note that here only the word-based lexiconmodel is used to compute the confidence measure.More complex models such as alignment models,fertility models and distortion models as describedin (Brown et al, 1994) could estimate the proba-bility of a given alignment more accurately.
How-ever the summation over all possible alignments isvery complicated, even intractable, with the richermodels.
For the efficient computation of the de-nominator, we use the lexical translation model.Similarly,Pt2s(A|T, S) =P (A,S|T )?A?
P (A?, S|T ), (5)andP (A,S|T ) =I?i=1p(si|tj , aij ?
A).
(6)?A?P (A?, S|T ) =I?i=1J?j=1p(si|tj).
(7)We randomly selected 512 Chinese-English (C-E) sentence pairs and generated word alignmentusing the MaxEnt aligner (Ittycheriah and Roukos,2005).
We evaluate per sentence alignment F-scores by comparing the system output with areference alignment.
For each sentence pair, wealso calculate the sentence alignment confidencescore ?
logC(A|S, T ).
We compute the corre-lation coefficients between the alignment confi-dence measure and the alignment F-scores.
Theresults in Figure 2 shows strong correlation be-tween the confidence measure and the alignmentF-score, with the correlation coefficients equals to-0.69.
Such strong correlation is also observed onan HMM alignment (Ge, 2004) and a Block Model(BM) alignment (Zhao et al, 2005) with varyingalignment accuracies, as seen in Table1.2.2 Sentence Alignment Selection Based onConfidence MeasureThe strong correlation between the sentence align-ment confidence measure and the alignment F-933Figure 2: Correlation between sentence alignmentconfidence measure and F-score.measure suggests the possibility of selecting thealignment with the highest confidence score to ob-tain better alignments.
For each sentence pair inthe C-E test set, we calculate the confidence scoresof the HMM alignment, the Block Model align-ment and the MaxEnt alignment, then select thealignment with the highest confidence score.
As aresult, 82% of selected alignments have higher F-scores, and the F-measure of the combined align-ments is increased over the best aligner (the Max-Ent aligner) by 0.8.
This relatively small improve-ment is mainly due to the selection of the wholesentence alignment: for many sentences the bestalignment still contains alignment errors, some ofwhich could be fixed by other aligners.
Therefore,it is desirable to combine alignment links from dif-ferent alignments.3 Alignment Link Confidence Measure3.1 DefinitionSimilar to the sentence alignment confidence mea-sure, the confidence of an alignment link aij in thesentence pair (S, T ) is defined asc(aij |S, T ) =?qs2t(aij |S, T )qt2s(aij |T, S)(8)where the source-to-target link posterior probabil-ityqs2t(aij |S, T ) =p(tj |si)?Jj?=1 p(tj?
|si), (9)which is defined as the word translation probabil-ity of the aligned word pair divided by the sumof the translation probabilities over all the targetwords in the sentence.
The higher p(tj |si) is,the higher confidence the link has.
Similarly, thetarget-to-source link posterior probability is de-fined as:qt2s(aij |T, S) =p(si|tj)?Ii?=1 p(si?
|tj).
(10)Intuitively, the above link confidence definitioncompares the lexical translation probability of thealigned word pair with the translation probabilitiesof all the target words given the source word.
If aword t occurs N times in the target sentence, forany i ?
{1, ..., I},J?j?=1p(tj?
|si) ?
Np(t|si),thus for any tj = t,qs2t(aij) ?1N.This indicates that the confidence score of anylink connecting tj to any source word is at most1/N .
On the one hand this is expected becausemultiple occurrences of the same word does in-crease the confusion for word alignment and re-duce the link confidence.
On the other hand, ad-ditional information (such as the distance of theword pair, the alignment of neighbor words) couldindicate higher likelihood for the alignment link.We will introduce a context-dependent link confi-dence measure in section 4.3.2 Alignment Link SelectionFrom multiple alignments of the same sentencepair, we select high confidence links from differentalignments based on their link confidence scoresand alignment agreement ratio.Typically, links appearing in multiple align-ments are more likely correct alignments.
Thealignment agreement ratio measures the popular-ity of a link.
Suppose the sentence pair (S, T ) havealignments A1,.
.
.
, AD, the agreement ratio of alink aij is defined asr(aij |S, T ) =?dC(Ad|S, T : aij ?
Ad)?d?
C(Ad?
|S, T ), (11)where C(A) is the confidence score of the align-ment A as defined in formula 1.
This formulacomputes the sum of the alignment confidencescores for the alignments containing aij , which is934Figure 3: Example of alignment link selection by combining MaxEnt, HMM and BM alignments.normalized by the sum of all alignments?
confi-dence scores.We collect all the links from all the alignments.For each link we calculate the link confidencescore c(aij) and the alignment agreement ratior(aij).
We link the word pair (si, tj) if eitherc(aij) > h1 or r(aij) > r1, where h1 and r1 areempirically chosen thresholds.We combine the HMM alignment, the BMalignment and the MaxEnt alignment (ME) us-ing the above link selection algorithm.
Figure3 shows such an example, where alignment er-rors in the MaxEnt alignment are shown with dot-ted lines.
As some of the links are correctlyaligned in the HMM and BM alignments (shownwith solid lines), the combined alignment correctssome alignment errors while still contains com-mon incorrect alignment links.Table 2 shows the precision, recall and F-scoreof individual alignments and the combined align-ment.
F-content and F-function are the F-scoresfor content words and function words, respec-tively.
The link selection algorithm improvesthe recall over the best aligner (the ME align-ment) by 7 points (from 65.4 to 72.5) while de-creasing the precision by 4.4 points (from 73.6to 69.2).
Overall it improves the F-score by 1.5points (from 69.3 to 70.8), 1.8 point improvementfor content words and 1.0 point for function words.It also significantly outperforms the traditionallyused heuristics, ?intersection-union-refine?
(Ochand Ney, 2003) by 6 points.4 Improved MaxEnt Aligner withConfidence-based Link FilteringIn addition to the alignment combination, we alsoimprove the performance of the MaxEnt alignerthrough confidence-based alignment link filtering.Here we select the MaxEnt aligner because it has935Precision Recall F-score F-content F-functionHMM 62.65 48.57 54.72 62.10 34.39BM 72.76 54.82 62.53 68.64 43.93ME 72.66 66.17 69.26 72.52 61.41Link-Select 69.19 72.49 70.81 74.31 60.26Intersection-Union-Refine 63.34 66.07 64.68 70.15 49.72Table 2: Link Selection and Combination Resultsthe highest F-measure among the three aligners,although the algorithm described below can be ap-plied to any aligner.It is often observed that words within a con-stituent (such as NP, PP) are typically translatedtogether, and their alignments are close.
As a re-sult the confidence measure of an alignment linkaij can be boosted given the alignment of its con-text words.
From the initial sentence alignmentwe first identify an anchor link amn, the high con-fidence alignment link closest to aij .
The an-chor link is considered as the most reliable con-nection between the source and target context.The context is then defined as a window center-ing at amn with window width proportional tothe distance between aij and amn.
When com-puting the context-dependent link confidence, weonly consider words within the context window.The context-dependent alignment link confidenceis calculated in the following steps:1.
Calculate the context-independent link con-fidence measure c(aij) according to formula(8).2.
Sort all links based on their link confidencemeasures in decreasing order.3.
Select links whose confidence scores arehigher than an empirically chosen thresholdH as anchor links 1.4.
Walking along the remaining sorted links.For each link {aij : c(aij) < H},(a) Find the closest anchor link amn2,(b) Define the context window width w =|m?
i|+ |n?
j|.1H is selected to maximize the F-score on an alignmentdevset.2When two equally close alignment links have the sameconfidence score), we randomly select one of the tied links asthe anchor link.
(c) Compute the link posterior probabilitieswithin the context window:qs2t(aij |amn) =p(tj |si)?j+wj?=j?w p(tj?
|si),qt2s(aij |amn) =p(si|tj)?i+wi?=i?w p(si?
|tj).
(d) Compute the context-dependent linkconfidence score c(aij |amn) =?qs2t(aij |amn)qt2s(aij |amn).If c(aij |amn) > H , add aij into the setof anchor links.5.
Only keep anchor links and remove all the re-maining links with low confidence scores.The above link filtering algorithm is designed toremove incorrect links.
Furthermore, it is possibleto create new links by relinking unaligned sourceand target word pairs within the context window iftheir context-dependent link posterior probabilityis high.Figure 4 shows context-independent link con-fidence scores for the given sentence alignment.The subscript following each word indicates theword?s position.
Incorrect alignment links areshown with dashed lines, which have low confi-dence scores (a5,7, a7,3, a8,2, a11,9) and will beremoved through filtering.
When the anchor linka4,11 is selected, the context-dependent link confi-dence of a6,12 is increased from 0.12 to 0.51.
Alsonote that a new link a7,12 (shown as a dotted line)is created because within the context window, thelink confidence score is as high as 0.96.
This ex-ample shows that the context-dependent link filter-ing not only removes incorrect links, but also cre-ate new links based on updated confidence scores.We applied the confidence-based link filter-ing on Chinese-English and Arabic-English wordalignment.
The C-E alignment test set is the same936Figure 4: Alignment link filtering based on context-independent link confidence.Precision Recall F-scoreBaseline 72.66 66.17 69.26+ALF 78.14 64.36 70.59Table 3: Confidence-based Alignment Link Filter-ing on C-E AlignmentPrecision Recall F-scoreBaseline 84.43 83.64 84.04+ALF 88.29 83.14 85.64Table 4: Confidence-based Alignment Link Filter-ing on A-E Alignment512 sentence pairs, and the A-E alignment testset is the 200 Arabic-English sentence pairs fromNIST MT03 test set.Tables 3 and 4 show the improvement ofC-E and A-E alignment F-measures with theconfidence-based alignment link filtering (ALF).For C-E alignment, removing low confidencealignment links increased alignment precision by5.5 point, while decreased recall by 1.8 point, andthe overall alignment F-measure is increased by1.3 point.
When looking into the alignment linkswhich are removed during the alignment link fil-tering process, we found that 80% of the removedlinks (1320 out of 1661 links) are incorrect align-ments, For A-E alignment, it increased the pre-cision by 3 points while reducing recall by 0.5points, and the alignment F-measure is increasedby about 1.5 points absolute, a 10% relative align-ment error rate reduction.
Similarly, 90% of theremoved links are incorrect alignments.5 TranslationWe evaluate the improved alignment on sev-eral Chinese-English and Arabic-English machinetranslation tasks.
The documents to be trans-lated are from difference genres: newswire (NW)and web-blog (WB).
The MT system is a phrase-based SMT system as described in (Al-Onaizanand Papineni, 2006).
The training data are bilin-gual sentence pairs with word alignment, fromwhich we obtained phrase translation pairs.
Weextract phrase translation tables from the baselineMaxEnt word alignment as well as the alignmentwith confidence-based link filtering, then trans-late the test set with each phrase translation ta-ble.
We measure the translation quality with au-tomatic metrics including BLEU (Papineni et al,2001) and TER (Snover et al, 2006).
The higherthe BLEU score is, or the lower the TER scoreis, the better the translation quality is.
We com-bine the two metrics into (TER-BLEU)/2 and tryto minimize it.
In addition to the whole test set?sscores, we also measure the scores of the ?tail?documents, whose (TER-BLEU)/2 scores are atthe bottom 10 percentile (for A-E translation) and20 percentile (for C-E translation) and are consid-ered the most difficult documents to translate.In the Chinese-English MT experiment, we se-lected 40 NW documents, 41 WB documents asthe test set, which includes 623 sentences with16667 words.
The training data includes 333 thou-sand C-E sentence pairs subsampled from 10 mil-lion sentence pairs according to the test data.
Ta-bles 5 and 6 show the newswire and web-blogtranslation scores as well as the number of phrasetranslation pairs obtained from each alignment.Because the alignment link filtering removes manyincorrect alignment links, the number of phrasetranslation pairs is reduced by 15%.
For newswire,the translation quality is improved by 0.44 on thewhole test set and 1.1 on the tail documents, asmeasured by (TER-BLEU)/2.
For web-blog, weobserved 0.2 improvement on the whole test setand 0.5 on the tail documents.
The tail documentstypically have lower phrase coverage, thus incor-rect phrase translation pairs derived from incorrect937# phrase pairs Average TailTER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering# phrase pairs Average TailTER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filteringalignment links are more likely to be selected.
Theremoval of incorrect alignment links and cleanerphrase translation pairs brought more gains on thetail documents.In the Arabic-English MT, we selected 80 NWdocuments and 55 WB documents.
The NW train-ing data includes 319 thousand A-E sentence pairssubsampled from 7.2 million sentence pairs withword alignments.
The WB training data includes240 thousand subsampled sentence pairs.
Tables 7and 8 show the corresponding translation results.Similarly, the phrase table size is significantly re-duced by 35%, while the gains on the tail docu-ments range from 0.6 to 1.4.
On the whole testset the difference is smaller, 0.07 for the newswiretranslation and 0.58 for the web-blog translation.6 Related WorkIn the machine translation area, most research onconfidence measure focus on the confidence ofMT output: how accurate a translated sentence is.
(Gandrabur and Foster, 2003) used neural-net toimprove the confidence estimate for text predic-tions in a machine-assisted translation tool.
(Ueff-ing et al, 2003) presented several word-level con-fidence measures for machine translation based onword posterior probabilities.
(Blatz et al, 2004)conducted extensive study incorporating varioussentence-level and word-level features thru multi-layer perceptron and naive Bayes algorithms forsentence and word confidence estimation.
(Quirk,2004) trained a sentence level confidence mea-sure using a human annotated corpus.
(Bach etal., 2008) used the sentence-pair confidence scoresestimated with source and target language mod-els to weight phrase translation pairs.
However,there has been little research focusing on confi-dence measure for word alignment.
This workis the first attempt to address the alignment con-fidence problem.Regarding word alignment combination, in ad-dition to the commonly used ?intersection-union-refine?
approach (Och and Ney, 2003), (Ayanand Dorr, 2006b) and (Ayan et al, 2005) com-bined alignment links from multiple word align-ment based on a set of linguistic and alignmentfeatures within the MaxEnt framework or a neuralnet model.
While in this paper, the alignment linksare combined based on their confidence scores andalignment agreement ratios.
(Fraser and Marcu, 2007) discussed the impactof word alignment?s precision and recall on MTquality.
Here removing low confidence links re-sults in higher precision and slightly lower recallfor the alignment.
In our phrase extraction, weallow extracting phrase translation pairs with un-aligned functional words at the boundary.
This issimilar to the ?loose phrases?
described in (Ayanand Dorr, 2006a), which increased the number ofcorrect phrase translations and improved the trans-lation quality.
On the other hand, removing incor-rect content word links produced cleaner phrasetranslation tables.
When translating documentswith lower phrase coverage (typically the ?tail?documents), high quality phrase translations areparticularly important because a bad phrase trans-lation can be picked up more easily due to limitedphrase translation pairs available.7 ConclusionIn this paper we presented two alignment confi-dence measures for word alignment.
The first isthe sentence alignment confidence measure, basedon which the best whole sentence alignment is se-938# phrase pairs Average TailTER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering# phrase pairs Average TailTER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filteringlected among multiple alignments and it obtained0.8 F-measure improvement over the single bestChinese-English aligner.
The second is the align-ment link confidence measure, which selects themost reliable links from multiple alignments andobtained 1.5 F-measure improvement.
When weremoved low confidence links from the MaxEntaligner, we reduced the Chinese-English align-ment error by 5% and the Arabic-English align-ment error by 10%.
The cleaned alignment sig-nificantly reduced the size of phrase translation ta-bles by 15-35%.
It furthermore led to better trans-lation scores for Chinese and Arabic documentswith different genres.
In particular, it improved thetranslation scores of the tail documents by 0.5-1.4points measured by the combined metric of (TER-BLEU)/2.For future work we would like to explore richermodels to estimate alignment posterior probabil-ity.
In most cases, exact calculation by summingover all possible alignments is impossible, and ap-proximation using N-best alignments is needed.AcknowledgmentsWe are grateful to Abraham Ittycheriah, Yaser Al-Onaizan, Niyu Ge and Salim Roukos and anony-mous reviewers for their constructive comments.This work was supported in part by the DARPAGALE project, contract No.
HR0011-08-C-0110.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion Models for Statistical Machine Translation.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 529?536, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Necip Fazil Ayan and Bonnie J. Dorr.
2006a.
Goingbeyond aer: An extensive analysis of word align-ments and their impact on mt.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 9?16,Sydney, Australia, July.
Association for Computa-tional Linguistics.Necip Fazil Ayan and Bonnie J. Dorr.
2006b.
A max-imum entropy approach to combining word align-ments.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Con-ference, pages 96?103, New York City, USA, June.Association for Computational Linguistics.Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.2005.
Neuralign: Combining word alignments us-ing neural networks.
In Proceedings of HumanLanguage Technology Conference and Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 65?72, Vancouver, British Columbia,Canada, October.
Association for ComputationalLinguistics.Nguyen Bach, Qin Gao, and Stephan Vogel.
2008.
Im-proving word alignment with language model basedconfidence scores.
In Proceedings of the ThirdWorkshop on Statistical Machine Translation, pages151?154, Columbus, Ohio, June.
Association forComputational Linguistics.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2004.
Confidence es-timation for machine translation.
In COLING ?04:Proceedings of the 20th international conference onComputational Linguistics, page 315, Morristown,NJ, USA.
Association for Computational Linguis-tics.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1994.
The Mathe-matic of Statistical Machine Translation: ParameterEstimation.
Computational Linguistics, 19(2):263?311.939Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine trans-lation.
Comput.
Linguist., 33(3):293?303.Simona Gandrabur and George Foster.
2003.
Confi-dence estimation for translation prediction.
In Pro-ceedings of the seventh conference on Natural lan-guage learning at HLT-NAACL 2003, pages 95?102,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Niyu Ge.
2004.
Max-posterior hmm alignmentfor machine translation.
In Presentation given atDARPA/TIDES NIST MT Evaluation workshop.Abraham Ittycheriah and Salim Roukos.
2005.
Amaximum entropy word aligner for arabic-englishmachine translation.
In HLT ?05: Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 89?96, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In ACL ?02:Proceedings of the 40th Annual Meeting on Asso-ciation for Computational Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Chris Quirk.
2004.
Training a sentence-level machinetranslation confidence measure.
In In Proc.
LREC2004, pages 825?828, Lisbon, Portual.
Springer-Verlag.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Proceedings of Association for MachineTranslation in the Americas.Nicola Ueffing, Klaus Macherey, and Hermann Ney.2003.
Confidence measures for statistical machinetranslation.
In In Proc.
MT Summit IX, pages 394?401.
Springer-Verlag.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th conferenceon Computational linguistics, pages 836?841, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Bing Zhao, Niyu Ge, and Kishore Papineni.
2005.Inner-outer bracket models for word alignment us-ing hidden blocks.
In HLT ?05: Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 177?184, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.940
