Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 368?375,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsVERTa participation in the WMT14 Metrics TaskElisabet ComellesUniversitat de BarcelonaBarcelona, Spainelicomelles@ub.eduJordi AtseriasYahoo!
LabsBarcelona, Spainjordi@yahoo-inc.comAbstractIn this paper we present VERTa, a lin-guistically-motivated metric that com-bines linguistic features at different lev-els.
We provide the linguistic motivationon which the metric is based, as well asdescribe the different modules in VERTaand how they are combined.
Finally, wedescribe the two versions of VERTa,VERTa-EQ and VERTa-W, sent toWMT14 and report results obtained inthe experiments conducted with theWMT12 and WMT13 data into English.1 IntroductionIn the Machine Translation (MT) process, theevaluation of MT systems plays a key role bothin their development and improvement.
From theMT metrics that have been developed during thelast decades, BLEU (Papineni et al., 2002) is oneof the most well-known and widely used, since itis fast and easy to use.
Nonetheless, researcherssuch as (Callison-Burch et al., 2006) and (Lavieand Dekowski, 2009) have claimed its weak-nesses regarding translation quality and its ten-dency to favour statistically-based MT systems.As a consequence, other more complex metricsthat use linguistic information have been devel-oped.
Some use linguistic information at lexicallevel, such as METEOR (Denkowski and Lavie,2011); others rely on syntactic information, ei-ther using constituent (Liu and Hildea, 2005) ordependency analysis (Owczarzack et al., 2007aand 2007b; He et al., 2010); others use morecomplex information such as semantic roles(Gim?nez and M?rquez, 2007 and 2008a; Lo etal., 2012).
All these metrics focus on partial as-pects of language; however, other researchershave tried to combine information at differentlinguistic levels in order to follow a more holisticapproach.
Some of these metrics follow a ma-chine-learning approach (Leusch and Ney, 2009;Albrecht and Hwa, 2007a and 2007b), otherscombine a wide variety of metrics in a simpleand straightforward way (Gim?nez, 2008b;Gim?nez and M?rquez, 2010; Specia and Gim?-nez, 2010).
However, very little research hasbeen performed on the impact of the linguisticfeatures used and how to combine this informa-tion from a linguistic point of view.
Hence, ourproposal is a linguistically-based metric, VERTa(Comelles et al., 2012), which uses a wide vari-ety of linguistic features at different levels, andaims at combining them in order to provide awider and more accurate coverage than thosemetrics working at a specific linguistic level.
Inthis paper we provide a description of the lin-guistic information used in VERTa, the differentmodules that form VERTa and how they arecombined according to the language evaluatedand the type of evaluation performed.
Moreover,the two versions of VERTa participating inWMT14, VERTa-EQ and VERTa-W are de-scribed.
Finally, for the sake of comparison, weuse the data available in WMT12 and WMT13 tocompare both versions to the metrics participat-ing in those shared tasks.2 Linguistic MotivationBefore developing VERTa, we analysed thoselinguistic phenomena that an MT metric shouldcover.
From this analysis, we decided to organisethe information into the following groups:Lexical information.
The use of lexicalsemantics plays a key role when compar-ing a hypothesis and reference segment,since it allows for identifying relations ofsynonymy, hypernymy and hyponymy.Morphological information.
This type ofinformation is crucial when dealing withlanguages with a rich inflectional mor-phology, such as Spanish, French or Cata-368lan because it helps in covering phenom-ena related to tense, mood, gender, num-ber, aspect or case.
In addition, morphol-ogy in combination with syntax (morpho-syntax) is also important to identifyagreement (i.e.
subject-verb agreement).This type of information should be takeninto account when evaluating the fluencyof a segment.Syntactic information.
This type of in-formation covers syntactic structure, syn-tactic relations and word order.Semantic information.
Named Entities(NEs), sentence polarity and time expres-sions are included here.All this information described above should betaken into account when developing a metric thataims at covering linguistic phenomena at differ-ent levels and evaluate both adequacy and flu-ency.3 Metric DescriptionIn order to cover the above linguistic features,VERTa is organised into different modules:Lexical similarity module, Morphological simi-larity module, Dependency similarity module andSemantic similarity module.
Likewise, an Ngramsimilarity module has also been added in order toaccount for similarity between chunks in the hy-pothesis and reference segments.
Each metricworks first individually and the final score is theFmean of the weighted combination of the Preci-sion and Recall of each metric in order to get theresults which best correlate with human assess-ment.
This way, the different modules can beweighted depending on their importance regard-ing the type of evaluation (fluency or adequacy)and language evaluated.
In addition, the modulardesign of this metric makes it suitable for all lan-guages.
Even those languages that do not have awide range of NLP tools available could beevaluated, since each module can be used iso-lated or in combination.All metrics use a weighted precision and recallover the number of matches of the particularelement of each level (words, dependency triples,ngrams, etc) as shown below.
)())((hhnmatchWP DRW nmatchD( (r))(r)Where r is the reference, h is the hypothesisand ?
is a function that given a segment willreturn the elements of each level (e.g.
words atlexical level and triples at dependency level).
Dis the set of different functions to project thelevel element into the features associated to eachlevel, such as word-form, lemma or partial-lemma at lexical level.
nmatch () is a functionthat returns the number of matches according tothe feature ?
(i.e.
the number of lexical matchesat the lexical level or the number of dependencytriples that match at the dependency level).
Fi-nally, W is the set of weights ]0 1] associated toeach of the different features in a particular levelin order to combine the different kinds ofmatches considered in that level.All modules forming VERTa and the linguis-tic features used are described in detail in thefollowing subsections.3.1 Lexical moduleInspired by METEOR, the lexical modulematches lexical items in the hypothesis segmentto those in the reference segment taking into ac-count several linguistic features.
However, whileMETEOR uses word-form, synonymy, stemmingand paraphrasing, VERTa relies on word-form,synonymy1, lemma, partial lemma2, hypernymsand hyponyms.
In addition, a set of weights isassigned to each type of match depending ontheir importance as regards semantics (see Table1).WMatch ExamplesHYP REF1 1 Word-form east east2 1 Synonym believed considered3 1 Hypernym barrel keg4 1 Hyponym keg barrel5 .8 Lemma is_BE are_BE6 .6 Part-lemma danger dangerousTable 1.
Lexical matches and examples.3.2 Morphological similarity moduleThe morphological similarity module is based onthe matches established in the lexical module(except for the partial-lemma match) in combina-tion with Part-of-Speech (PoS) tags from the an-notated corpus3.
The aim of this module is to1 Information on synonyms, lemmas, hypernyms andhyponyms is obtained from WordNet 3.0.2 Lemmas that share the first four letters.3 The corpus has been PoS tagged using the StanfordParser (de Marneffe et al.
2006).369compensate the broader coverage of the lexicalmodule, preventing matches such as invites andinvite, which although similar in terms of mean-ing, do not coincide as for their morphologicalinformation.
Therefore, this module turns moreappropriate to assess the fluency of a segmentrather than its adequacy.
In addition, this modulewill be particularly useful when evaluating lan-guages with a richer inflectional morphology (i.e.Romance languages).In line with the lexical similarity metric, themorphological similarity metric establishesmatches between items in the hypothesis and thereference sentence and a set of weights (W) isapplied.
However, instead of comparing singlelexical items as in the previous module, in thismodule we compare pairs of features in the orderestablished in Table 2.W Match ExamplesHYP REF1 (Word-form, PoS)(he, PRP) (he, PRP)1 (Synonym,PoS)(VIEW,NNS)(OPINON,NNS)1 (Hypern.,PoS)(PUBLICA-TION, NN)(MAGA-ZINE, NN)1 (Hypon.,PoS)(MAGA-ZINE, NN)(PUBLI-CATION,NN).8 (LEMMA,PoS)can_(CAN,MD)Could_(CAN, MD)Table 2.
Morphological module matches.3.3 Dependency similarity moduleThe dependency similarity metric helps in cap-turing similarities between semantically compa-rable expressions that show a different syntacticstructure (see Example 1), as well as changes inword order (see Example 2).Example 1:HYP: ...the interior minister...REF: ...the minister of interior...In example 1 both hypothesis and referencechunks convey the same meaning but their syn-tactic constructions are different.Example 2:HYP: After a meeting Monday night with thehead of Egyptian intelligence chief OmarSuleiman Haniya said....REF: Haniya said, after a meeting on Mondayevening with the head of Egyptian IntelligenceGeneral Omar Suleiman...In example 2, the adjunct realised by the PPAfter a meeting Monday night with the head ofEgyptian intelligence chief Omar Suleiman oc-cupies different positions in the hypothesis andreference strings.
In the hypothesis it is located atthe beginning of the sentence, preceding the sub-ject Haniya, whereas in the reference, it is placedafter the verb.
By means of dependencies, we canstate that although located differently inside thesentence, both subject and adjunct depend on theverb.This module works at sentence level and fol-lows the approach used by (Owczarzack et al.,2007a and 2007b) and (He et al., 2010) withsome linguistic additions in order to adapt it toour metric combination.
Similar to the morpho-logical module, the dependency similarity metricalso relies first on those matches established atlexical level ?
word-form, synonymy, hy-pernymy, hyponymy and lemma ?
in order tocapture lexical variation across dependencies andavoid relying only on surface word-form.
Then,by means of flat triples with the form La-bel(Head, Mod) obtained from the parser4, fourdifferent types of dependency matches have beendesigned (see Table 3) and weights have beenassigned to each type of match.W Match Type Match Descr.1 Complete Label1=Label2Head1=Head2Mod1=Mod21 Partial_no_label Label1?Label2Head1=Head2Mod1=Mod2.9 Partial_no_mod Label1=Label2Head1=Head2Mod1?Mod2.7 Partial_no_head Label1=Label2Head1?Head2Mod1=Mod2Table 3.
Dependency matches.In addition, dependency categories also re-ceive a different weight depending on how in-formative they are: dep, det and _5 which receive0.5, whereas the rest of categories are assignedthe maximum weight (1).Finally, a set of language-dependent rules hasbeen added with two goals: 1) capturing similari-ties between different syntactic structures con-4 Both hypothesis and reference strings are annotatedwith dependency relations by means of the Stanfordparser (de Marneffe et al.
2006).5 _ stands for no_dep_label370veying the same meaning; and 2) restricting cer-tain dependency relations (i.e.
subject word orderwhen translating from Arabic to English).3.4 Ngram similarity moduleThe ngram similarity metric matches chunks inthe hypothesis and reference segments and relieson the matches set by the lexical similarity met-ric, which allows us to work not only with word-forms but also with synonyms, lemmas, partial-lemmas, hypernyms and hyponyms as shown inExample 3, where the chunks [the situation inthe area] and [the situation in the region] domatch, even though area and region do not sharethe same word-form but a relation of synonymy.Example 3:HYP: ?
the situation in the area?REF: ?
the situation in the region?3.5 Semantics similarity moduleAs confirmed by the lexical module, semanticsplays an important role in the evaluation of ade-quacy.
This has also been claimed by (Lo andWu, 2010) who report that their metric based onsemantic roles outperforms other well-knownmetrics when adequacy is assessed.
With thisaim in mind the semantic similarity module usesother semantic features at sentence level: NEs,time expressions and polarity.Regarding NEs, we use Named-Entity recog-nition (NER) and Named-Entity linking (NEL).Following previous NE-based metrics (Reeder etal., 2011 and Gim?nez, 2008) the NER metric6aims at capturing similarities between NEs in thehypothesis and reference segments.
On the otherhand NEL7 focuses only on those NEs that ap-pear on Wikipedia, which allows for linking NEsregardless of their external form.
Thus, EU andEuropean Union will be captured as the sameNE, since both of them are considered as thesame organisation in Wikipedia.As regards time expressions, the TIMEX met-ric matches temporal expressions in the hypothe-sis and reference segments regardless of theirform.
The tool used is the Stanford TemporalTagger (Chang and Manning, 2012) which rec-ognizes not only points in time but also duration.By means of this metric, different syntactic struc-tures conveying the same time expression can be6 In order to identify NEs we use the Supersense Tag-ger (Ciaramita and Altun, 2006).7 The NEL module uses a graph-based NEL tool(Hachey, Radford and Curran, 2010) which links NEsin a text with those in Wikipedia pages.matched, such as on February 3rd and on thethird of February.Finally, it has been reported that negationmight pose a problem to SMT systems (Wetzeland Bond, 2012).
In order to answer such need, amodule that checks the polarity of the sentencehas been added using the dictionary strategy de-scribed (Atserias et al., 2012):Adding 0.5 for each weak positive word.Adding 1.0 for each strong positive word.Subtracting 0.5 for each weak negativeword.Subtracting 1.0 for each strong negativeword.For each query term score, the value is propa-gated to the query term positions by reducing itsstrength in a factor of 1/n, where n is the distancebetween the query term and the polar term.According to the experiments performed, thismodule shows a low correlation with humanjudgements on adequacy, since only partial as-pects of translation are considered, whereas hu-man judges assess whole segments.
However,regardless of how well/bad the module correlateswith human judgements, it proves useful tocheck partial aspects of the segments translated,such as the correct translation of NEs or the cor-rect translation of negation.3.6 Metrics combinationThe modular design of VERTa allows for pro-viding different weights to each module depend-ing on the type of evaluation and the languageevaluated.
Thus following linguistic criteriawhen evaluating adequacy, those modules whichmust play a key role are the lexical and depend-ency module, since they are more related to se-mantics; whereas, when evaluating fluency thoserelated to morphology, morphosyntax and con-stituent word order will be the most important.Moreover, metrics can also be combined depend-ing on the type of language evaluated.
If a lan-guage with a rich inflectional morphology is as-sessed, the morphology module should be givena higher weight; whereas if the language evalu-ated does not show such a rich inflectional mor-phology, the weight of the morphology moduleshould be lower.4 Experiments and resultsExperiments were carried out on WMT data,specifically on WMT12 and WMT13 data, alllanguages into English.
Languages ?all?
includeFrench, German, Spanish and Czech for WMT12371and French, German, Spanish, Czech and Rus-sian for WMT13.
Both segment and system levelevaluations were performed.
Evaluation sets pro-vided by WMT organizers were used to calculateboth segment and system level correlations.Since VERTa has been mainly designed to as-sess either adequacy or fluency separately, ourgoal for WMT14 was to find the best combina-tion in order to evaluate whole translation qual-ity.
Firstly we decided to explore the influence ofeach module separately.
To this aim, all modulesdescribed above, except for the semantics onewere used and tested separately.
Secondly, allmodules were assigned the same weight andtested in combination (VERTa-EQ).
The reasonwhy the semantics module was disregarded isthat it does not usually correlate well with humanjudgements, as stated above.
Each module wasset as follows:Lexical module.
As described above, ex-cept for the use of hypernyms/hyponymsmatches that were disregarded.Morphological module.
As describedabove, except for the lemma-PoS matchand the hypernyms/hyponyms-PoS match.Dependency module.
As described above.Ngram module.
As described above, usinga 2-gram length.Finally, we used the module combinationaimed at evaluating adequacy, which is mainlybased on the dependency and lexical modules,but with a stronger influence of the ngram mod-ule in order to control word order (VERTa-W).Weights were manually assigned, based on re-sults obtained in previous experiments conductedfor adequacy and fluency (Comelles et al., 2012),as follows:Lexical module:  0.41Morphological module: 0Dependency module: 0.40Ngram module: 0.19Experiments aimed at evaluating the influenceof each module (see Table 4 and Table 5) showthat the dependency module, in the case ofWMT12 data, and the lexical module in the caseof WMT13 data, are the most effective ones.However, the influence of the ngram module andthe morphological module varies depending onthe source language.
The fact that the depend-ency module correlates better with humanjudgements than others might be due to its flexi-bility to capture different syntactic constructionsthat convey the same meaning.
In addition, thegood performance of the lexical module is due tothe use of lexical semantic relations.
On the otherhand, in general the morphological moduleshows a better performance than the ngram one,which might be due to the type of source lan-guages and the possible translation mistakes.
Allsource languages are highly-inflected languagesand this might cause problems when translatinginto English, since its inflectional morphology isnot as rich as theirs.
As for the low performanceof the ngram module in the cs-en (especially, inWMT12 data), it might be due to the fact thatCzech word order is unrestricted, whereas Eng-lish shows a stricter word order and this mightcause translation issues.
A longer ngram distancemight have been more appropriate to controlword order in this case.Module fr-en de-en es-en cs-enLexical .16 .20 .18 .14Morph.
.17 .19 .18 .12Depend.
.18 .24 .20 .17Ngram .16 .17 .15 .08Table 4.
Segment-level Kendall?s tau correla-tion per module with WMT12 data.Module fr-ende-enes-encs-enru-enLexical .239 .254 .294 .227 .220Morph.
.236 .243 .295 .214 .191Depend.
.232 .247 .275 .220 .199Ngram .237 .245 .283 .213 .189Table 5.
Segment-level Kendall?s tau correla-tion per module with WMT13 data.Finally, two versions of VERTa were com-pared: the unweighted combination (VERTa-EQ)and the weighted one (VERTa-W).
These twoversions were also compared to some of the bestperforming metrics in WMT12 (see Table 6 andTable 7) and WMT13 (see Table 8 and Table 9):Spede07-pP, METEOR, SEMPOR and AMBER(Callison-Burch et al., 2012); SIMPBLEU-RECALL, METEOR and DEPREF-ALIGN 8 ).As regards WMT12 data at segment level, theunweighted version achieves similar results tothose obtained by the best performing metrics.On the other hand, VERTa-W?s results areslightly worse, especially for fr-en and es-enpairs, which is due to the fact that the morpho-logical module has been disregarded in this ver-8 http://www.statmt.org/wmt13/papers.html372sion.
Regarding system level correlation, neitherVERTa-EQ nor VERTa-W achieves a high cor-relation with human judgements.Metric fr-en de-en es-en cs-enSpede07-pP .26 .28 .26 .21METEOR .25 .27 .24 .21VERTa-EQ .26 .28 .26 .20VERTa-W .24 .28 .25 .20Table 6.
Segment-level Kendall?s tau correla-tion WMT12.Metric fr-en de-en es-en cs-enSEMPOR .80 .92 .94 .94AMBER .85 .79 .97 .83VERTa-EQ .83 .71 .89 .66VERTa-W .79 .73 .91 .66Table 7.
System-level Spearman?s rho correla-tion WMT12.As for segment level WMT13 results (see Ta-ble 8), although both VERTa-EQ and VERTa-W?s performance is worse than that of the twobest-performing metrics, both versions achieve athird and fourth position for all language pairs,except for fr-en.
As regards system level correla-tions (see Table 9), both versions of VERTashow the best performance for de-en and ru-enpairs, as well as for the average score.5 Conclusions and Future WorkIn this paper we have presented VERTa, a lin-guistically-based MT metric.
VERTa allows formodular combination depending on the languageand type of evaluation conducted.
AlthoughVERTa has been designed to evaluate adequacyand fluency separately, in order to evaluatewhole MT quality, a couple of versions havebeen used: VERTa-EQ, an unweighted versionthat uses all modules, and VERTa-W a weightedversion that uses the lexical, dependency andngram modules.Experiments have shown that the modules thatbest correlate with human judgements are thedependency and lexical modules.
In addition,both VERTa-EQ and VERTa-W have been com-pared to the best performing metrics in WMT12and WMT13 shared tasks.
VERTa-EQ hasproved to be in line with results obtained bySpede07-pP and METEOR in WMT12 at seg-ment level, while in WMT13, both VERTa andVERTa-W occupy the third and fourth positionafter METEOR and DEPREF-ALIGN as regardssegment level and the first position at systemlevel.In the future, we plan to continue working onthe improvement of VERTa and use automatictuning of module?s weight in order to achieve thefinal version that best correlates with humanjudgements on ranking.
Likewise, we would liketo explore the use of VERTa to evaluate otherlanguages but English and how NLP tool errorsmay influence the performance of the metric.6 AcknowledgementsWe would like to acknowledge Victoria Arranzand Irene Castell?n for their valuable commentsand sharing their knowledge.This work has been partially funded by the Span-ish Government (projects SKATeR, TIN2012-38584-C06-06 and Holopedia, TIN2010-21128-C02-02).Metric fr-en de-en es-en cs-en ru-en AverageSIMPBLEU-RECALL .303 .318 .388 .260 .234 .301METEOR .264 .293 .324 .265 .239 .277VERTa-EQ .252 .280 .318 .239 .215 .261VERTa-W .253 .278 .314 .238 .222 .261DEPREF-ALIGN .257 .267 .312 .228 .200 .253Table 8.
Segment-level Kendall?s tau correlation WMT13.Metric fr-en de-en es-en cs-en ru-en AverageMETEOR .984 .961 .979 .964 .789 .935DEPREF-ALIGN .995 .966 .965 .964 .768 .931VERTa-EQ .989 .970 .972 .936 .814 .936VERTa-W .989 .980 .972 .945 .868 .951Table 9.
System-level Spearman?s rho correlation WMT13.373ReferenceJ.
S. Albrecth and R. Hwa.
2007.
A Re-examinationof Machine Learning Approaches for Sentence-Level MT Evaluation.
In The Proceedings of the45th Annual Meeting of the ACL, Prague, CzechRepublic.J.
S. Albrecth and R. Hwa.
2007.
Regression for Sen-tence-Level MT Evaluation with Pseudo Refer-ences.
In The Proceedings of the 45th AnnualMeeting of the ACL, Prague, Czech Republic.J.
Atserias, R. Blanco, J. M. Chenlo and C.Rodriquez.
2012.
FBM-Yahoo at RepLab 2012,CLEF (Online Working Notes/Labs/Workshop)2012, September 20, 2012.C.
Callison-Burch, M. Osborne and P. Koehn.
2006.Re-evaluating the role of BLEU in machine trans-lation research.
In Proceedings of the EACL 2006.C.
Callison-Burch, P. Kohen, Ch.
Monz, M. Post, R.Soricut and L. Specia.
2012.
Findings of the 2012Workshop on Statistical Machine Translation.
InProceedings of the 7th Workshop on StatisticalMachine Translation.
Montr?al.
Canada.A.
X. Chang and Ch.
D. Manning.
2012.
SUTIME: ALibrary for Recognizing and Normalizing TimeExpressions.
8th International Conference on Lan-guage Resources and Evaluation (LREC 2012).M.
Ciaramita and Y. Altun.
2006.
Broad-coveragesense disambiguation and information extractionwith a supersense sequence tagger.
EmpiricalMethods in Natural Language Processing(EMNLP).E.
Comelles, J. Atserias, V. Arranz and I. Castell?n.2012.
VERTa: Linguistic features in MT evalua-tion.
Proceedings of the Eighth International Con-ference on Language Resources and Evaluation(LREC'12), Istanbul, Turkey.M.C.
de Marneffe, B. MacCartney and Ch.
D. Man-ning.
2006.
Generating Typed Dependency Parsesfrom Phrase Structure Parses in Proceedings of the5th Edition of the International Conference onLanguage Resources and Evaluation (LREC-2006).
Genoa, Italy.M.
J. Denkowski and A. Lavie.
2011.
METEOR 1.3:Automatic Metric for Reliable Optimization andEvaluation of Machine Translation Systems inProceedings of the 6th Workshop on StatisticalMachine Translation (ACL-2011).
Edinburgh,Scotland, UK.J.
Gim?nez and Ll.
M?rquez.
2007.
Linguistic fea-tures for automatic evaluation of heterogeneousMT systems in Proceedings of the 2nd Workshopon Statistical Machine Translation (ACL), Prague,Czech Repubilc.J.
Gim?nez and Ll.
M?rquez.
2008.
A smorgasbord offeatures for automatic MT evaluation in Proceed-ings of the 3rd Workshop on Statistical MachineTranslation (ACL).
Columbus.
OH.J.
Gimenez.
2008.
Empirical Machine Translationand its Evaluation.
Doctoral Dissertation.
UPC.J.
Gim?nez and Ll.
M?rquez.
2010.
Linguistic Meas-ures for Automatic Machine Translation Evalua-tion.
Machine Translation, 24(3?4),77?86.Springer.B.
Hachey, W. Radford and J. R. Curran.
2011.Graph-based named entity linking with Wikipediain Proceedings of the 12th International confer-ence on Web information system engineering,pages 213-226, Springer-Verlag, Berlin, Heidel-berg.Y.
He, J.
Du, A.
Way and J. van Genabith.
2010.
TheDCU Dependency-based Metric in WMT-MetricsMATR 2010.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetrics MATR (WMT 2010),  Uppsala, Sweden.A.
Lavie and M. J. Denkowski.
2009.
The METEORMetric for Automatic Evaluation of MachineTranslation.
Machine Translation, 23.G.
Leusch and H. Ney.
2008.
BLEUSP, INVWER,CDER: Three improved MT evaluation measures.In  NIST Metrics for Machine Translation 2008Evaluation (MericsMATR08), Waikiki, Honolulu,Hawaii,  October 2008.D.
Liu and D. Hildea.
2005.
Syntactic Features forEvaluation of Machine Translation in Proceedingsof the ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for Machine Translationand/or Summarization, Ann ArborCh.Lo and D. Wu.
2010.
Semantic vs. Syntactic vs.Ngram Structure for Machine Translation Evalua-tion.
In Proceedings of the 4th Workshop on SyntaxSemantics and Structure in Statistical Translation.Beijing.
China.Ch.
Lo, A. K. Tumurulu and D. Wu.
2012.
FullyAutomatic Semantic MT Evaluation.
Proceedingsof the 7th Wrokshop on Statistical Machine Trans-lation,  Montr?al, Canada, June 7-8.K.
Owczarzak,  J. van Genabith  and A.
Way.
2007.Dependency-Based Automatic Evaluation for Ma-chine Translation in Proceedings of SSST, NAACL-HLT/AMTA Workshop on Syntax and Structure IStatistical Translation, Rochester, New York.K.
Owczarzak,  J. van Genabith  and A.
Way.
2007.Labelled Dependencies in Machine TranslationEvaluation in Proceedings of the ACL Workshopon Statistical Machine Translation, Prague, CzechRepublic.374K.
Papineni, S. Roukos, T. Ward and W. Zhu.
2002.BLEU: A Method for Automatic Evaluation ofMachine Translation.
In Proceedings of the 40thAnnual Meeting of the Association for Computa-tional Linguistics (ACL02).
Philadelphia.
PA.F.
Reeder, K. Miller, J. Doyon and J.
White.
2001.The Naming of Things and the Confusion ofTongues: an MT Metric.
Proceedings of the Work-shop on MT Evaluation ?Who did what to whom?
?at Machine Translation Summit VIII.L.
Specia and J. Gim?nez.
2010.
Combining Confi-dence Estimation and Reference-based Metrics forSegment-level MT Evaluation.
The Ninth Confer-ence of the Association for Machine Translation inthe Americas (AMTA 2010), Denver, Colorado.D.
Wetzel and F. Bond.
2012.
Enriching Parallel Cor-pora for Statistical Machine Translation with Se-mantic Negation Rephrasing.
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Struc-ture in Statistical Translation, Jeju, Republic ofKorea.375
