Using Semantically Motivated Estimates to HelpSubcategorization AcquisitionAnna KorhonenComputer Laboratory, University of CambridgePembroke Street, Cambridge CB2 3QG, UKalk23@cl, cam.
ac.
ukAbst ractResearch into the automatic acquisition ofsubcategorization frames from corpora isstarting to produce large-scale computationallexicons which include valuable frequency in-formation.
However, the accuracy of theresulting lexicons shows room for improve-ment.
One source of error lies in the lackof accurate back-off estimates for subcatego-rization frames, delimiting the performanceof statistical techniques frequently employedin verbal acquisition.
In this paper, wepropose a method of obtaining more accu-rate, semantically motivated back-off esti-mates, demonstrate how these estimates canbe used to improve the learning of subcatego-rization frames, and discuss using the methodto benefit large-scale l xical acquisition.1 In t roduct ionManual development of large subcategorisedlexicons has proved difficult because pred-icates change behaviour between sublan-guages, domains and over time.
Yet parsersdepend crucially on such information, andprobabilistic parsers would greatly benefitfrom accurate information concerning the rel-ative frequency of different subcategorizationframes (SCFs) for a given predicate.Over the past years acquiring subcatego-rization dictionaries from textual corpora hasbecome increasingly popular (e.g.
Brent,1991, 1993; Ushioda et al, 1993; Briscoe andCarroll, 1997; Manning, 1993; Carroll andRooth 1998; Gahl, 1998; Lapata, 1999, Sarkarand Zeman, 2000).
The different approachesvary according to the methods used and thenumber of SCFs being extracted.
Regardlessof this, there is a ceiling on the performanceof these systems at around 80% token recall*.
*Token recall is the percentage of SCF tokens in asample of manually analysed text that were correctlyacquired by the system.One significant source of error lies in thestatistical filtering methods frequently usedto remove noise from automatically acquiredSCFs.
These methods are reported to be par-ticularly unreliable for low frequency scFs(Brent, 1991, 1993; Briscoe and Carroll, 1997;Manning, 1993; Manning and Schiitze, 1999;Korhonen, Gorrell and McCarthy, 2000), re-sulting in a poor overall performance.According to Korhonen, Gorrell and Mc-Carthy (2000), the poor performance o f sta-tistical filtering can be largely explained bythe zipfian nature of the data, coupled withthe fact that many statistical tests are basedon the assumption of two zipfian distributionscorrelating: the conditional SCF distributionof an individual verb (p(scfilverbj)) and theunconditional SCF distribution of all verbs ingeneral (p(scfl)).
Contrary to this assump-tion, however, there is no significant correla-tion between the two distributions.Korhonen, Gorrell and McCarthy (2000)have showed that a simple method of filteringSCFs on the basis of their relative frequencyperforms more accurately than statistical fil-tering.
This method sensitive to the sparsedata problem is best integrated with smooth-ing.
Yet the performance of the sophisticatedsmoothing techniques which back-off to an un-conditional distribution also suffer from thelack of correlation between p(scfi\[verbj) andp(scf0.In this paper, we propose a method for ob-taining more accurate back-off estimates forSCF acquisition.
Taking Levin's verb classifi-cation (Levin, 1993) as a starting point, weshow that in terms of SCF distributions, in-dividual verbs correlate better with other se-mantically similar verbs than with all verbsin general.
On the basis of this observation,we propose classifying verbs according to theirsemantic lass and using the conditional SCFdistributions of a few other members in the216same class as back-off estimates of the class(p( sc fi lsernantic class j)).Adopting the SCF acquisition system ofBriscoe and Carroll (1997) we report an ex-periment which demonstrates how these esti-mates can be used in filtering.
This is doneby acquiring the conditional SCF distributionsfor selected test verbs, smoothing these dis-tributions with the unconditional distributionof the respective verb class, and applying asimple method for filtering the resulting setof SCFs.
Our results show that the proposedmethod improves the acquisition of SCFs sig-nificantly.
We discuss how this method can beused to benefit large-scale SCF acquisition.We begin by reporting our findings that theSCF distributions of semantically similar verbscorrelate well (section 2).
We then introducethe method we adopted for constructing theback-~off estimates for the data used in our ex-periment (section 3.1), summarise the mainfeatures of the SCF acquisition approach (sec-tion 3.2), and describe the smoothing tech-niques adopted (section 3.3).
Finally, we re-view the empirical evaluation (section 4) anddiscuss directions for future work (section 5).2 Examin ing  SCF  Cor re la t ionbetween Semant ica l ly  S imi la rVerbsTo examine the degree of SCF correlationbetween semantically similar verbs, we tookLevin's verb classification (1993) as a start-ing point.
Levin verb classes are based onthe ability of a verb to occur in specificdiathesis alternations, i.e.
specific pairs ofsyntactic frames which are assumed to bemeaning preserving.
The classification pro-vides semantically-motivated setsof syntacticframes associated with individual classes.While Levin's shows that there is corre-lation between the SCFs related to the verbsense, our aim is to examine whether there isalso correlation between the SCFs specific tothe verb form.
Unlike Levin, we are concernedwith polysemic scF distributions involving allsenses of verbs.
In addition, we are not onlyinterested in the degree of correlation betweensets of SCFs, but also in comparing the rank-ing of SCFs between distributions.
Neverthe-less, Levin classes provide us a useful startingpoint.Focusing on five broad Levin classeschange of possession, assessment, killing, mo-tion, and destroy verbs - we chose four testverbs from each class and examined the de-gree with which the SCF distribution for theseverbs correlates with the SCF distributions fortwo other verbs from the same Levin class.The latter verbs were chosen so that one ofthe verbs is a synonym, and the other a hyper-nym, of a test verb.
We used WordNet (Milleret al, 1990) for defining and recognising thesesemantic relations.
We defined a hypernymas a test verb's hypernym in WordNet, and asynonym as a verb which, in WordNet, sharesthis same hypernym with a test verb.
We alsoexamined how well the SCF distribution forthe different test verbs correlates with the SCFdistribution of all English verbs in general andwith that of a semantically different verb (i.e.a verb belonging to a different Levin class).We used two methods for obtaining thescF distributions.
The first was to acquirean unfiltered subcategorization lexicon for 20million words of the British National Corpus(BN?)
(Leech, 1992) data using Briscoe andCarroll's (1997) system (introduced in sec-tion 3.2).
This gives us the observed istribu-tion of SCFs for individual verbs and that forall verbs in the BNC data.
The second methodwas to manually analyse around 300 occur-rences of each test verb in the BNC data.
Thisgives us an estimate of the correct SCF distri-butions for the individual verbs.
The estimatefor the correct distribution of SCFs over allEnglish verbs was obtained by extracting thenumber of verbs which are members of eachSCF class in the ANLT dictionary (Boguraev etal., 1987).The degree of correlation was examined bycalculating the Kullback-Leibler distance (KL)(Cover and Thomas, 1991) and the Spearmanrank correlation coefficient (Re) (Spearman,1904) between the different distributions 2.The results given in tables 1 and 2 were ob-tained by correlating the observed SCF distri-butions from the BNC data.
Table 1 showsan example of correlating the SCF distribu-tion of the motion verb .fly against hat of (i)its hypernym move, (ii) synonym sail, (iii) allverbs in general, and (iv) agree, which is notrelated semantically.
The results show thatthe SCF distribution for .fly clearly correlatesbetter with the SCF distribution for move andsail than that for all verbs and agree.
The av-2Note that Io., >_ 0, with IO., near to 0 denotingstrong association, and -1  _< RC < 1, with RC near to0 denoting a-low degree of association and ttc near to-1 and 1 denoting strong association.2171 I I KL I clfly move 0.25 0.83.fly sail 0.62 0.61.fly all verbs 2.13 0.51.fly agree 2.27 0.12Table 1: Correlating the SCF distribution of.fly against other SCF distributions\[-' KL I RChype~nym 0.65 0.71synonym 0.71 0.66all verbs 1.59 0.41semantically different verb 1.74 0.38Table 2: Overallerage results for all test verbs given in table 2indicate that the degree of SCF correlation isthe best with semantically similar verbs.
Hy-pernym and synonym relations are nearly asgood, the majority of verbs showing slightlybetter SCF correlation with hypernyms.
TheSCF correlation between individual verbs, andverbs in general, is poor, but still better thanwith semantically unrelated verbs.These findings with the observed SCF dis-tributions hold as well with the correct SCFdistributions, as seen in table 3.
The resultsshow that in terms of SCF distributions, verbsin all classes examined correlate better withtheir hypernym verbs than with all verbs ingeneral.As one might expect, the polysemy of theindividual verbs affects the degree of SCF cor-relation between semantically similar verbs.The degree of SCF correlation is higher withthose verbs whose predominant 3 sense is in-volved with the Levin class examined.
Forexample, the SCF distribution for the killingverb murder correlates better with that forthe verb kill than that for the verb execute,whose predominant sense is not involved withkilling verbs.These results how that the verb sense spe-cific SCF correlation observed by Levin ex-tends to the verb form specific SCF correlationand applies to the ranking of SCFs as well.This suggests that we can obtain more accu-rate back-off estimates for verbal acquisitionby basing them on a semantic verb type.
Tofind out whether such semantically motiwtedSPredomlnant sense refers here to the most frequentsense of verbs in WordNet.correlation resultsestimates can be used to improve SCF acqui-sition, we performed an experiment which wedescribe below.3 Exper iment3.1 Back-off Es t imates  for the  DataThe test data consisted of a total of 60 verbsfrom 12 broad Levin classes, listed in table 4.Two of the examined Levin classes were col-lapsed together with another similar Levinclass, making the total number of test classes10.
The verbs were chosen at random, sub-ject to the constraint that they occurred fre-quently enough in corpus data 4and when ap-plicable, represented different sub-classes ofeach examined Levin class.
To reduce theproblem of polysemy, we required that thepredominant sense of each verb corresponds tothe Levin class in question.
This was ensuredby manually verifying that the most frequentsense of a verb in WordNet corresponds to thesense involved in the particular Levin class.To obtain the back-off estimates, we chose4-5 representative rbs from each verb classand obtained correct SCF distributions forthese verbs by manually analysing around 300occurrences of each verb in the ant  data.
Wemerged the resulting set of SCF distributionsto construct he unconditional SCF distribu-tion for the verb class.
This approach wastaken to minimise the sparse data problemand cover SCF variations within verb classesand due to polysemy.
The bazk-off estimates4We required at least 300 occurrences foreach verb.This was merely to guarantee accurate enough testing,as we evaluated our results against manual analysis ofcorpus data (see section 4).218Verb classchange of possessionassessmentkillingdestroymotionAVERAGEHypernymKL RC0.61 0.640.28 0.710.70 0.630.30 0.600.29 0.730.44 0.66All VerbsKL RC1.16 0.380.73 0.481.14 0.371.19 0.291.72 0.421.19 0.39Table 3: Correlation results for five verb classesVerb classputtingsending and carrying,exerting forcechange of possessionassessment,searchingsocial interactionkillingdestroyappearance, disappearanceand occurrencemotionaspectuMTest verbsplace, lay, drop, pour, load, fillsend, ship, carry, bring, transportpull, pushgive, lend, contribute, donate, offerprovide, supply, acquire, buyanalysefish, explore, investigateagree, communicate, struggle, marry, meet, visitkill, murder, slaughter, strangledemolish, destroy, ruin, devastatearise, emerge, disappear, vanisharrive, depart, march, move, slide, swingtravel, walk, fly, sail, dancebegin, end, start, terminate, completeTable 4: Test datafor motion verbs, for example, were obtainedby merging the SCF distributions of the verbsmarch, move, fly, slide and sail.
Each verbused in obtaining the estimates was excludedwhen testing the verb itself.
For example,when acquiring subcategorization f rthe verbfly, estimates were obtained only using verbsmarch, move, slide and sail.3.2 F ramework  for SCF Acquisit ionBriscoe and Carroll's (1997) verbal acquisitionsystem distinguishes 163 SCFs and returns rel-ative frequencies for each SCF found for a givenpredicate.
The SCFs are a superset of classesfound in the ANLT and COMLEX (Grishmanet al, 1994) dictionaries.
They incorporateinformation about control of predicative ar-guments, as well as alternations such as ex-traposition and particle movement.
The sys-tem employs a shallow parser to obtain thesubcategorization information.
Potential SCFentries are filtered before the final SCF lexi-con is produced.
While Briscoe and Carroll(1997) used a statistical filter based on bi-nomial hypothesis test, we adopted anothermethod, where the conditional SCF distribu-tion from the system is smoothed before fil-tering the SCFS, using the different techniquesintroduced in section 3.3.
After smoothing,filtering is performed by applying a thresholdto the resulting set of probability estimates.We used training data to find an optimal av-erage threshold for each verb class examined.This filtering method allows us to examinethe benefits of smoothing without introducingproblems based on the statistical filter.3.3 Smoothing3.3.1 Add One SmoothingAdd one smoothing s has the effect of givingsome of the probability space to the SCFs un-seen in the conditional distribution.
As it as-sumes a uniform prior on events, it provides abaseline smoothing method against which the5See (Manning and Schiltze, 1999) for detailed in-formation about the smoothing techniques discussedhere.219more sophisticated methods can be compared.Let c(x=) be the frequency of a SCF given averb, N the total number of SCF tokens forthis verb in the conditional distribution, andC the total number of SCF types.
The esti-mated probability of the SCF is:P(xn) - c(xn) + 1N + C (1)3.3.2 Katz  Backing-offIn Katz backing-off (Katz, 1987), some of theprobability space is given to the SCFs unseenor of low frequency in the conditional distri-bution.
This is done by backing-off to an un-conditional distribution.
Let p(xn) be a prob-ability of a SCF in the conditional distribution,and p(xnv) its probability in the unconditionaldistribution, obtained by maximum likelihoodestimation.
The estimated probability of thescF is calculated as follows:P(xn)= { (1 -d )  xp(xn) ifc(x=) > clc~ x p(xnp) otherwise (2)The cut off frequency ci is an empiri-cally defined threshold etermining whetherto back-off or not.
When counts are lowerthan cl they are held too low to give an accu-rate estimate, and we back-off to an uncondi-tional distribution.
In this case, we discountp(x~) a certain amount o reserve some of theprobablity space for unseen and very low fre-quency scFs.
The discount (d) is defined em-pirically, and a is a normalization constantwhich ensures that the probabilities of the re-sulting distribution sum to 1.3.3.3 L inear Interpo lat ionWhile Katz backing-off consults different es-timates depending on their specificity, linearinterpolation makes a linear combination ofthem.
Linear interpolation is used here for thesimple task of combining a conditional distri-bution with an unconditional one.
The esti-mated probability of the SCF is given byP(xn) = Al(p(z,~)) + )~2(p(xnp)) (3)where the Ai denotes weights for differ-ent context sizes (obtained by optimising thesmoothing performance on the training datafor all zn) and sum to 1.4 Eva luat ion4.1 MethodTo evaluate the approach, we took a sample of20 million words of the BNC and extracted allsentences containing an occurrence of one ofthe 60 test verbs on average of 3000 citationsof each.
The sentences containing these verbswere processed by the SCF acquisition system,and the smoothing methods were applied be-fore filtering.
We also obtained results for abaseline without any smoothing.The results were evaluated against a man-ual analysis of the corpus data.
This wasobtained by analysing up to a maximum of300 occurrences for each test verb in BN?or LOB (Garside et al, 1987), Susanne andSEC (Taylor and Knowles, 1988) corpora.
Wecalculated type precision (percentage of SCFsacquired which were also exemplified in themanual analysis) and recall (percentage of theSCFs exemplified in the manual analysis whichwere also acquired automatically), and com-bined them into a single measure of overallperformance using the F measure (Manningand Schiitze, 1999).F = 2. precision, recall (4)precision -4- recallWe estimated accuracy with which the sys-tem ranks true positive classes against he cor-rect ranking.
This was computed by calculat-ing the percentage of pairs of SCFs at posi-tions (n, m) such that n < m in the systemranking that occur in the same order in theranking from the manual analysis.
This givesus an estimate of the accuracy of the relativefrequencies of SCFs output by the system.
Inaddition to the system results, we also calcu-lated KL and Rc between the acquired unfil-tered SCF distributions and the distributionsobtained from the manual analysis.4.2 ResultsTable 5 gives average results for the 60 testverbs using each method.
The results indi-cate that both add one smoothing and Katzbacking-off improve the baseline performanceonly slightly.
Linear interpolation outper-forms these methods, achieving better esultson all measures.
The improved KL indicatesthat the method improves the overall accu-racy of SCF distributions.
The  results withrtc and system accuracy show that it helpsto correct the ranking of SCFs.
The fact220Method KL RC accuracyBaseline 0.63 0.72 79.2Add one 0.64 0.74 79.0Katz backing-off 0.61 0.75 79.0Linear interpolation 0.51 0.82 84.4System results (%)\]precision I recall F measure78.5 63.3 70.185.3 59.7 70.276.4 67.6 71.787.8 68.7 77.1Table 5: Average results with different methods using semantically motivated back-off estimatesfor smoothingMethod KL  ac accuracyBaseline 0.63 0.72 79.2Katz backing-off 0.68 0.69 77.2Linear interpolation 0.79 0.64 76.7System resu l t s  (%)I precision I recall I F  measure78.5 63.3 70.175.2 61.7 67.871.4 64.1 67.6Table 6: Average results using the SCF distribution of all verbs as back-off estimates for smooth-ingthat both precision and recall show clear im-provement over the baseline results demon-strates that linear interpolation can be suc-cessfully combined with the filtering methodemployed.
These results eem to suggest thata smoothing method which affects both thehighly ranked SCFs and SCFs of low frequencyis profitable for this task.In this experiment, he semantically moti-vated back-off estimates helped to reduce thesparse data problem significantly.
While a to-tal of 151 correct SCFs were missing in the testdata, only three were missing after smoothingwith Katz backing-off or linear interpolation.For comparison, we re-run these experi-ments using the general SCF distribution ofall verbs as back-off estimates for smooth-ing 6.
The average results for the 60 test verbsgiven in table 6 show that when using theseestimates, we obtain worse results than withthe baseline method.
This demonstrates thatwhile such estimates provide an easy solutionto the sparse data problem, they can actuallydegrade the accuracy of verbal acquisition.Table 7 displays individual results for thedifferent verb classes.
It lists the results ob-tained with KL and Rc using the baselinemethod and linear interpolation with semanti-cally motivated estimates.
Examining the re-sults obtained with linear interpolation allowsus to consider the accuracy of the back-off es-6These estimates were obtained by extracting thenumber of verbs which are members of each SCF classin the ANLT dictionary.
See section 2 for details.timates for each verb class.
Out of ten verbclasses, eight show improvement with linearinterpolation, with both KL and Rc.
However,two verb classes - aspectual verbs, and verbsof appearance, disappearance and occurrence- show worse results when linear interpolationis used.According to Levin (1993), these two verbclasses need further classification before a fullsemantic account can be made.
The prob-lem with aspectual verbs is that the classcontains verbs taking sentential complements.As Levin does not classify verbs on basisof their sentential complement-taking proper-ties, more classification work is required be-fore we can obtain accurate SCF estimates forthis type of verb.The problem with verbs of appearance ismore specific to the verb class.
Levin remarksthat the definition of appearance verbs maybe too loose.
In addition, there are signifi-cant syntactic differences between the verbsbelonging to the different sub-classes.This suggests that we should examine thedegree of SCF correlation between verbs fromdifferent sub-classes before deciding on the fi-nal (sub-)class for which we obtain the es-timates.
As the results with the combinedLevin classes how, estimates can also be suc-cessfully built using verbs fromdifferent Levinclasses, provided that the classes are similarenough.221RCVerb class baseline linear i.putting 0.68 0.70sending and carrying, 0.72 0.96exerting forcechange of possession 0.61 0.75assessment, 0.61 0.70searchingsocial interaction 0.72 0.80killing 0.91 0.95destroy 0.70 0.97appearance, disappearanceand occurrence0.91aspectualKLbaseline linear i.0.70 0.660.64 0.500.61 0.600.81 0.620.65 0.580.69 0.670.95 0.2O0.14 0.17 0.83motion 0.66 0.58 0.56 0.660.48 0.54 0.86 0.89Table 7: Baseline and linear interpolation results for the verb classes5 Conc lus ionIn this paper, we have shown that the verbform specific SCF distributions of semanticallysimilar verbs correlate well.
On the basisof this observation, we have proposed usingverb class specific back-off estimates in SCFacquisition.
Employing the SCF acquisitionframework of Briscoe and Carroll (1997), wehave demonstrated that these estimates canbe used to improve SCF acquisition signifi-cantly, when combined with smoothing anda simple filtering method.We have not yet explored the possibilityof using the semantically motivated estimateswith statistical filtering.
In principle, thisshould help to improve the performance of thestatistical methods which make use of back-offestimates.
If filtering based on relative fre-quencies till achieves better results, it wouldbe worth investigating ways of handling thelow frequency data for integration with thismethod.
As Korhonen, Gorrell and McCarthy(2000) discuss, any statistical filtering methodwould work better at low frequences than theone applied, since this simply disregards alllow frequency SCFS.In addition to refining the filtering method,our future work will focus on integratingthis approach with large-scale scF acquisition.This will involve (i) defining the set of seman-tic verb classes across the lexicon, (ii) obtain-ing back-off estimates for each verb class, and(iii) implementing a method capable of auto-matically classifying verbs to semantic lasses.The latter can be done by linking the Word-Net synonym sets with semantic lasses, usinga similar method to that employed by Dorr(1997).
With the research reported, verbswere classified to semantic lasses accordingto their most frequent sense.
While this ap-proach proved satisfactory, our future workwill include investigating ways of addressingthe problem of polysemy better.The manual effort needed for obtaining theback-off estimates was quite high for this pre-liminary experiment.
However, our recent in-vestigation shows that the total number of se-mantic classes across the whole lexicon is un-likely to exceed 50.
This is because many ofthe Levin classes have proved similar enoughin terms of SCF distributions that they can becombined together.
Therefore the additionaleffort required to carry out the proposed workseems justified, given the accuracy enhance-ment reported.6 AcknowledgementsI thank Ted Briscoe and Diana McCarthy foruseful comments on this paper.Re ferencesBoguraev, B., Briscoe, E., Carroll, J., Carter,D.
and Grover, C. 1987.
The derivation of agrammatically-indexed lexicon from the Long-man Dictionary of Contemporary English.
InProceedings of the 25th Annual Meeting ofthe Association .for Computational Linguis-tics, Stanford, CA.
193-200.Brent, M. 1991.
Automatic acquisition ofsubcategorization frames from untagged text.222In Proceedings of the 29th Annual Meetingof the Association for Computational Linguis-tics, Berkeley, CA.
209-214.Brent, M. 1993.
From grammar to lexicon:unsupervised learning of lexical syntax.
Com-putational Linguistics 19.3: 243-262.Briscoe, E. and Carroll, J.
1993.
Gener-alised probabilistic Lt~ parsing for unification-based grammars.
Computational Linguistics19.1: 25-60.Briscoe, E.J.
and J. Carroll 1997.
Automaticextraction of subcategorization from corpora.In Proceedings of the 5th ACL Conf.
on Ap-plied Natural Language Processing, Washing-ton, DC.
356-363.Briscoe, T., Carroll, J. and Korhonen, A.1997.
Automatic extraction of subcategoriza-tion frames from corpora - a framework and3 experiments.
Sparkle WP5 Deliverable.Available in http://www.ilc.pi.cnr.it/.Carroll, G. and Rooth, M. 1998.
Valenceinduction with a head-lexicMized PCFG.
InProceedings of the 3rd Conference on Empir-ical Methods in Natural Language Processing,Granada, Spain.Cover, Thomas, M. and Thomas, J.A.
1991.Elements of Information Theory.
Wiley-Interscience, New York.Dorr, B.
1997.
Large-scale dictionary con-struction for foreign language tutoring andinterlingual machine translation.
MachineTranslation 12.4: 271-325.Gahl, S. 1998.
Automatic extraction of sub-corpora based on subcategorization framesfrom a part-of-speech tagged corpus.
In Pro-ceedings of the COLING-ACL'98, Montreal,Canada.Grishman, R., Macleod, C. and Meyers, A.1994.
Comlex syntax: building a computa-tional lexicon.
In Proceedings of the Interna-tional Conference on Computational Linguis-tics, COLING-94, Kyoto, Japan.
268-272.Garside, R., Leech, G. and Sampson, G. 1987.The computational nalysis of English: Acorpus-based approach.
Longman, London.Katz, S. M. 1987.
Estimation of probabili-ties from sparse data for the language modelcomponent of speech recogniser.
IEEE Trans-actions on Acoustics, Speech, and Signal Pro-cessing 35.3: 400-401.Korhonen, A., Gorrell, G. and McCarthy,D.
2000.
Statistical filtering and subcatego-rization frame acquisition.
In Proceedings ofthe Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large Corpora, Hong Kong.Leech, G. 1992.
100 million words of English:the British NationM Corpus.
Language Re-search 28(1): 1-13.Levin, B.
1993.
English Verb Classes andAlternations.
Chicago University Press,Chicago.Manning, C. 1993.
Automatic acquisition ofa large subcategorization dictionary from cor-pora.
In Proceedings of the 31st Annual Meet-ing of the Association for Computational Lin-guistics, Columbus, Ohio.
235-242.Manning, C. and Schiitze, H. 1999.
Founda-tions of Statistical Natural Language Process-ing.
New York University, Ms.Miller, G., Beckwith, R., Felbaum, C., Gross,D.
and Miller, K. 1993.
Introduction toWordNet: An On-Line Lexical Database.ftp//clarity.princeton.edu/pub/WordNet/5papers.ps.Sampson, G. 1995.
English for the computer.Oxford, UK: Oxford University Press.Sarkar, A. and Zeman, D. 2000.
Auto-matic Extraction of Subcategorization Framesfor Czech.
In Proceedings of the Inter-national Conference on Computational Lin-guistics, COLING-O0, Saarbrucken, Germany.691-697.Spearman, C. 1904.
The proof and mea-surement of association between two things.American Journal of Psychology 15: 72-101.Taylor, L. and Knowles, G. 1988.
Manualof information to accompany the SEC cor-pus: the machine-readable corpus of spokenEnglish.
University of Lancaster, UK, Ms.Ushioda, A., Ewns, D., Gibson, T. andWaibel, A.
1993.
The automatic acquisition offrequencies of verb subcategorization framesfrom tagged corpora.
In Boguraev, B. andPustejovsky, J. eds.
SIGLEX A CL Workshopon the Acquisition of Lexical Knowledge fromText.
Columbus, Ohio: 95-106.223
