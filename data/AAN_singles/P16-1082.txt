Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 866?875,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsModeling Concept Dependencies in a Scientific CorpusJonathan Gordon, Linhong Zhu, Aram Galstyan, Prem Natarajan, and Gully BurnsUSC Information Sciences InstituteMarina del Rey, CA, USA{jgordon, linhong, galstyan, pnataraj, burns}@isi.eduAbstractOur goal is to generate reading lists for stu-dents that help them optimally learn techni-cal material.
Existing retrieval algorithmsreturn items directly relevant to a querybut do not return results to help users readabout the concepts supporting their query.This is because the dependency structureof concepts that must be understood beforereading material pertaining to a given queryis never considered.
Here we formulate aninformation-theoretic view of concept de-pendency and present methods to constructa ?concept graph?
automatically from a textcorpus.
We perform the first human evalu-ation of concept dependency edges (to bepublished as open data), and the results ver-ify the feasibility of automatic approachesfor inferring concepts and their dependencyrelations.
This result can support search ca-pabilities that may be tuned to help userslearn a subject rather than retrieve docu-ments based on a single query.1 IntroductionCorpora of technical documents, such as the ACLAnthology, are valuable for learners, but it can bedifficult to find the most appropriate documents toread in order to learn about a concept.
This problemis made more complicated by the need to tracethe ideas back to those that need to be learnedfirst (e.g., before you can learn about Markov logicnetworks, you should understand first-order logicand probability).
That is, a crucial question whenlearning a new subject is ?What do I need to knowbefore I start reading about this?
?To answer this question, learners typically relyon the guidance of domain experts, who can devisepedagogically valuable reading lists that order doc-Automatic Speech Recognition (ASR) with HMMsNoisy Channel Model Viterbi Decoding for ASRTraining ASR ParametersViterbi AlgorithmDynamic ProgrammingDecoding/Search ProblemHMMsMarkov ChainsHMM Pronunciation LexiconIterative Parameter Estimation with EMGaussian Acoustic ModelDiscrete Fourier TransformsGaussian Mixture Models PhonemesN-gram Language ModelFigure 1: A human-authored concept graph excerpt,showing possible concepts related to automaticspeech recognition and their concept dependencies.uments to progress from prerequisite to target con-cepts.
Thus, it is desirable to have a model whereeach concept is linked to the prerequisite conceptsit depends upon ?
a concept graph.
A manuallyconstructed concept graph excerpt related to au-tomatic speech recognition is shown in Figure 1.The dependency relation between two concepts isinterpreted as whether understanding one conceptwould help a learner understand the other.Representing a scientific corpus in this waycan improve tasks such as curriculum plan-ning (Yang et al, 2015), automatic reading listgeneration (Jardine, 2014), and improving educa-tion quality (Rouly et al, 2015).
Motivated by theimportance of representing the content of a scien-tific corpus as a concept graph, the challenge weaddress in this work is to automatically infer theconcepts and their dependency relations.Towards this end, we first instantiate each con-cept as a topic from statistical topic modeling (Bleiet al, 2003).
To link concepts with directed depen-866dency edges, we propose the use of information-theoretic measures, which we compare againstbaseline methods of computing word similarity,hierarchical clustering, and citation prediction.
Wethen gather human annotations of concept graphnodes and edges learned from the ACL Anthology,which we use to evaluate these methods.The main contributions of this paper are:1 We introduce the concept graph representationfor modeling the technical concepts in a corpusand their relations.2 We present information-theoretic approaches toinfer concept dependence relations.3 We perform the first human annotation of con-cept dependence for a technical corpus.4 We release the human annotation data for use infuture research.In the following section, we contrast this prob-lem with previous work.
We then describe the con-cept graph framework (Section 3) and present au-tomatic approaches for inferring concept graphs(Section 4).
The details of human evaluation arepresented in Section 5.
We discuss some interest-ing open questions related to this work in Section 6before concluding this work.2 Related WorkThere is a long history of work on identifying struc-ture in the contents of a text corpus.
Our approachis to link documents to concepts and to model rela-tions among these concepts rather than to identifythe specific claims (Sch?afer et al, 2011) or empiri-cal results (Choi et al, 2016) in each document.
Inthis section, we first provide an overview of differ-ent relations between concepts, followed by discus-sion of some representative methods for inferringthem.
We briefly discuss the differences betweenthese relations and the concept dependency relationwe are interested in.Similarity Concepts are similar to the extent thatthey share content.
Grefenstette (1994) applied theJaccard similarity measure to relate concepts toeach other.
White and Jose (2004) empirically stud-ied 10 similarity metrics on a small sample of10 pairs of topics, and the results suggested thatcorrelation-based measures best match general sub-ject perceptions of search topic similarity.Hierarchy Previous work on linking conceptshas usually been concerned with forming subsump-tion hierarchies from text (Woods, 1997; Sander-son and Croft, 1999; Cimiano et al, 2005) ?
e.g.,Machine translation is part of Natural languageprocessing ?
and more recent work does so for sta-tistical topic models.
Jonyer et al (2002) appliedgraph-based hierarchical clustering to learn hierar-chies from both structured and unstructured data.Ho et al (2012) learn a topic taxonomy from theACL Anthology and from Wikipedia with a methodthat scales linearly with the number of topics andthe tree depth.Other relations Every pair of concepts is statis-tically correlated with each other based on wordco-occurrence (Blei and Lafferty, 2006) providing asimple baseline metric for comparison.
For a topicmodeling approach performed over document cita-tion links rather than over words or n-grams, Wanget al (2013) gave a topic A?s dependence on an-other topic B as the probability of a document in Aciting a document in B.Our approach to studying concept dependence dif-fers from the relations derived from similarity, hi-erarchy, correlation and citation mentioned above,but intuitively they are related.
We thus adapt onerepresentative method for the similarity (Grefen-stette, 1994), hierarchy (Jonyer et al, 2002), andcitation likelihood (Wang et al, 2013) relations asbaselines for computing concept dependency rela-tions in Section 4.2.3.Concept dependence is also related to curricu-lum planning.
Yang et al (2015) and Talukdar andCohen (2012) studied prerequisite relationships be-tween course material documents based on externalinformation from Wikipedia.
They assumed thathyperlinks between Wikipedia pages and coursematerial indicate a prerequisite relationship.
Withthis assumption, Talukdar and Cohen (2012) usecrowdsourcing approaches to obtain a subset of theprerequisite structure and train a maximum entropy?based classifier to identify the prerequisite structure.Yang et al (2015) applied both classification andlearning to rank approaches in order to classify orrank prerequisite structure.3 Concept Graph Representation of aText CorpusWe represent the scientific literature as a labeledgraph, where nodes represent both documents andconcepts ?
and, optionally, metadata (such as au-thor, title, conference, year) and features (such as867			Figure 2: The Concept Graph Data Schema.
Eachnode is a class and edges are named relations be-tween classes (with associated attributes).words, or n-grams) ?
and labeled edges representthe relations between nodes.
Figure 2 shows an ex-ample schema for a concept graph representationfor a scientific corpus.Concepts are abstract and require a concrete rep-resentation.
In this work, we use statistical topicmodeling, where each topic ?
a multinomial distri-bution over a vocabulary of words ?
is taken as asingle concept.
Documents are linked to conceptsby weighted edges, which can be derived from thetopic model?s document?topic composition distri-butions.
Other approaches to identifying conceptsare considered in Section 6.Concepts exhibit various relations to other con-cepts, such as hierarchy, connecting more generaland more specific concepts; similarity; and cor-relation.
We model each concept as a node andconcept-to-concept relations as directed, weighted,labeled edges.
The label of an edge denotes thetype of relation, such as ?is similar to?, ?dependson?, and ?relates to?, and the weights represent thestrength of different relations.In this work, we focus on concept dependency,which is the least studied of these relations and,intuitively, the most important for learners.
We con-sider there to be a dependency relation between twoconcepts if understanding one concept would helpyou to understand the other.
This notion forms thecore of our human-annotated data set which demon-strates that this idea is meaningful and robust forexpert annotators when asked to judge if there ex-ists a dependency relation between two conceptsdefined by LDA topics (see Section 5.2).4 Learning the Concept Graph4.1 Identifying ConceptsThe representation of concepts using topics is verygeneral, and any effective topic modeling approachcan be applied.
These include probabilistic latentsemantic indexing (PLSI) (Hofmann, 1999), latentDirichlet alocation (LDA) (Blei et al, 2003), andnon-negative matrix factorization (NMF) (Aroraet al, 2012).
In our experiments, we use the open-source tool Mallet (McCallum, 2002), which pro-vides a highly scalable implementation of LDA;see Section 5.1 for more details.4.2 Discovering Concept DependencyRelationsIdentifying concept dependency relations betweentopics is the key step for building a useful con-cept graph.
These relations add semantic structureto the contents of the text corpus, and they facili-tate search and ordering in information retrieval.
Inthis section, as a proof-of-concept, we propose twoinformation-theoretic approaches to learn conceptdependency relations: an approach based on crossentropy and another based on information flow.4.2.1 Cross-entropy ApproachThe intuition of the cross-entropy approach is sim-ple: Given concepts ciand cj, if most of the in-stances of cican be explained by the occurrencesof cj, but not vice versa, it is likely that cidependson cj.
For example, if ciis Markov logic networks(MLNs) and cjis Probability, we might say that ob-serving MLNs depends on seeing Probability sincemost of the times that we see MLNs, we also seeProbability, but the opposite does not hold.Given concepts ciand cj, the cross-entropy ap-proach predicts that cidepends on cjif they satisfythese conditions:1 The distribution of ciis better approximated bythat of cjthan the distribution of cjis approxi-mated by that of ci.2 The co-occurrence frequency of instances of ciand cjis relatively higher than that of a non-dependency pair.Therefore, to predict the concept dependency re-lation, we need to examine whether the distributionof cicould well approximate the distribution of cjand the joint distribution of ciand cj.
For this, weuse cross entropy and joint entropy:Cross entropy measures the difference betweentwo distributions.
Specifically, the cross entropyfor the distributions X and Y over a given set isdefined as:H(X ;Y ) = H(X)+DKL(X ||Y ) (1)868where H(X) is the entropy of X , and DKL(X ||Y ) isthe Kullback?Leibler divergence of an estimateddistribution Y from true distribution X .
Therefore,H(X ;Y ) examines how well the distribution of Yapproximates that of X .Joint entropy measures the information we ob-tained when we observe both X and Y .
The jointShannon entropy of two variables X and Y is de-fined as:H(X ,Y ) =?X?YP(X ,Y ) log2P(X ,Y ) (2)where P(X ,Y ) is the joint probability of these val-ues occurring together.Based on the conditions listed above and these def-initions, we say that cidepends on cjif and only ifthey satisfy the following constraints:H(ci;cj) > H(cj;ci)H(ci,cj)?
?
(3)with ?
as a threshold value, which can be inter-preted as ?the average joint entropy of any non-dependence concepts?.
The weight of the depen-dency is defined as:DCE(ci,cj) = H(ci;cj)The cross-entropy method is general and canbe applied to different distributions used to modelconcepts, such as distributions of relevant words,of relevant documents, or of the documents that arecited by relevant documents.4.2.2 Information-flow ApproachNow we consider predicting concept dependencyrelations from the perspective of navigating infor-mation.
Imagine that we already have a perfectconcept dependency graph.
When we are at a con-cept node (e.g., reading a document about it), thenavigation is more likely to continue to a conceptit depends on than to other concepts that it doesn?tdepend on.
To give a concrete example, if we arenavigating from the concept Page rank, it is morelikely for us to jump to Eigenvalue than to Lan-guage model.
Therefore, if concept cidepends onconcept cj, then cjgenerally receives more naviga-tion hits than ciand has higher ?information flow?.Based on this intuition, we can predict con-cept dependency relations using information flow:Given concepts ciand cj, cidepends on cjif theysatisfy these conditions:ParallelcorporaformachinetranslationJapanese0.31Comparablecorpora0.01Collocation0.02Datastructures0.06Idiomaticexpressions0.08Beam search&other search algorithmsObjectivefunctions0.04Machinetranslationmodels0.030.15Machinetranslationsystems0.290.31Computationallinguistics(discipline)0.25ParaphrasegenerationTextualentailment0.04Machinetranslationevaluation0.09Humanassessment0.11Figure 3: A concept graph excerpt related to ma-chine translation, where concepts are linked basedon cross entropy.
Concepts are represented by man-ually chosen names, and links to documents areomitted.1 The concept cireceives relatively lower naviga-tion hits than cj.2 The number of navigation traces from conceptcito cjis much stronger than that to anothernon-dependent concept ck.While we do not have data for human navigationbetween concepts, a natural way to simulate this isthrough information flow.
As proposed by Rosvalland Bergstrom (2008), we use the probability flowof random walks on a network as a proxy for infor-mation flow in the real system.
Given any observedgraph G, the information score I(v) of a node v,is defined as its steady state visit frequency.
Theinformation flow I(u,v) from node u to node v, isconsequently defined as the transition probability(or ?exit probability?)
from u to v.To this end, we construct a graph connectingconcepts by their co-occurrences in documents,and we can use either Map Equation (Rosvall and869Bergstrom, 2008) or Content Map Equation (Smithet al, 2014) to compute the information flow net-work and the information score for each conceptnode.
The details are outlined as follows:1 Construct a concept graph Gcobased on co-occurrence observations.
We define weighted,undirected edges within the concept graph basedon the number of documents in which the con-cepts co-occur.
Formally, given concepts ciandcjand a threshold 0?
?
?
1, the weighted edgeis calculated as:wco(ci,cj) ={?dp(ci|d)p(cj|d) if p(c|d) > ?0 otherwise(4)2 Given the graph Gco, we compute the informa-tion score I(c) for each concept node c and infor-mation flow I(ci,cj) between a pair of nodesciand cj.
For the details of calculating I(c)and I(ci,cj), refer to Map Equation (Rosvalland Bergstrom, 2008) and Content Map Equa-tion (Smith et al, 2014).3 Given two concepts ciand cj, we link cito cjwith a directed edge if I(ci) > I(cj) with weight:DIF(ci,cj) = I(ci,cj)The information flow approach for inferring de-pendency can be further improved with a few truehuman navigation traces.
As introduced earlier, theconcept graph representation facilitates applica-tions such as reading list generation, and documentretrieval.
Those applications enable the collectionof human navigation traces, which can provide abetter approximation of dependency relation.4.2.3 Baseline ApproachesSimilarity Relations Intuitively, concepts thatare more similar (e.g., Machine translation andMachine translation evaluation) are more likelyto be connected by concept dependency relationsthan less similar concepts are.
As a baseline, wecompute the Jaccard similarity coefficient based onthe top 20 words or n-grams in the concept?s topicword distributions.Hierarchical Relations Previous work haslooked at learning hierarchies that connect broadertopics (acting as equivalent proxies for concepts inour work) to more specific subtopics (Cimiano etal., 2005; Sanderson and Croft, 1999).
We compareagainst a method for doing so to see how close iden-tifying hierarchical relations comes to our goal ofidentifying concept dependency relations.
Specifi-cally, we perform agglomerative clustering over thetopic?topic co-occurrence graph Gcowith weightsdefined in Eq.
4, in order to obtain the hierarchicalrepresentation for concepts.Citation-based Given concepts ciand cj, if thedocuments that are highly related to cjare cited bymost of the instances of ci, cimay depend on cj.Wang et al (2013) used this approach in the contextof CitationLDA topic modeling, where topics arelearned from citation links rather than text.
Weadapt this for regular LDA so that the concept cidepends on cjwith weightDCite(ci,cj) =?d1?D?d2?Cd1T1,iT2, j(5)where D is the set of all documents, Cdare thedocuments cited by d, and Tx,yis the distributionof documents dxcomposed of concepts cy.
For thismethod, we return a score of 0 if the concepts donot co-occur in at least three documents.5 Evaluation of Concept GraphsThere are two main approaches to evaluating a con-cept graph: We can directly evaluate the graph,using human judgments to measure the quality ofthe concepts and the reliability of the links betweenthem.
Alternatively, we can evaluate the applica-tion of a concept graph to a task, such as orderingdocuments for a reading list or recommending doc-uments to cite when writing a paper.Our motivation to build a concept graph froma technical corpus is to improve performance atthe task of reading list generation.
However, anapplied evaluation makes it harder to judge thequality of the concept graph itself.
Each documentcontains a combination of concepts, which havedifferent ordering restrictions, and other factorsalso affect the quality of a reading list, such asthe classification of document difficulty and type(e.g., survey, tutorial, or experimental results).
Assuch, we focus on a direct human evaluation of ourproposed methods for building a concept graph andleave the measure of applied performance to futurework.5.1 Corpus and its Evaluation ConceptGraphsFor this evaluation, the scientific corpus we useis the ACL Anthology.
This consists of articlespublished in a variety of journals, conferences,870and workshops related to computational linguis-tics.
Specifically, we use a modified copy of theplain text distributed for the ACL Anthology Net-work (AAN), release 2013 (Radev et al, 2013),which includes 23,261 documents from 1965 to2013.
The AAN includes plain text for documents,with OCR performed using PDFBox.
We manuallysubstituted OmniPage OCR output from the ACLAnthology Reference Corpus, version 1 (Bird et al,2008) for documents where it was observed to beof higher quality.
The text was processed to joinwords that were split across lines with hyphens.
Wemanually removed documents that were not writtenin English or where text extraction failed, leaving20,264 documents, though this filtering was notexhaustive.The topic model we used was built using theMallet (McCallum, 2002) implementation of LDA.It is composed of bigrams, filtered of typical En-glish stop words before the generation of bigrams,so that, e.g., ?word to word?
yields the bigram?word word?.
We generated topic models consist-ing of between 20 and 400 topics and selected a300-topic model based on manual inspection.
Doc-uments were linked to concepts based on the docu-ment?s LDA topic composition.
The concept nodesfor each topic were linked in concept dependencyrelations using each of the methods described inSection 4, producing five concept graphs to evalu-ate.
We applied the general cross-entropy methodto the distribution of top-k bigrams for each con-cept.
For all methods, the results we report are fork = 20.
Changing this value shifts the precision?recall trade-off, but in our experiments, the relativeperformance of the methods are generally consis-tent for different values of k.Since it is impractical to manually annotate all pairsof concept nodes from a 300-node graph, we se-lected a subset of edges for evaluation.
Intuitively,the evaluation set should satisfy the following sam-pling criteria: (1) The evaluation set should coverthe top weighted edges for a precision evaluation.
(2) The evaluation set should cover the bottom-weighted edges for a recall evaluation.
(3) Theevaluation set should provide low-biased sampling.With respect to these requirements, we generatedan evaluation edge set as the union of the followingthree sets:1 Top-20 edges for each approach (including base-line approaches)2 A random shuffle selection from the union ofJudges All Coherent Related DependentNon-NLP 0.407 0.446 0.305 0.329NLP 0.526 0.610 0.448 0.395All 0.467 0.529 0.354 0.357Table 1: Inter-annotator agreement measured asPearson correlation.Relevant phrases:machine?translation, translation?system, mt?system, transfer?rules, mt?systems, lexical?transfer, analysis?transfer, translation?process, transfer?generation, transfer?component, analysis?synthesis, transfer?phase, analysis?generation, structural?transfer, transfer?approach, human?translation, transfer?grammar, analysis?phase, translation?systems, transfer?processRelevant documents:?
Slocum: Machine Translation: Its History, Current Status, and Future Prospects (89%)  ?
Slocum: A Survey of Machine Translation: Its History, Current Status, and Future Prospects (89%)  ?
Wilks, Carbonnell, Farwell, Hovy, Nirenburg: Machine Translation Again?
(56%)  ?
Slocum: An Experiment in Machine Translation (55%)  ?
Krauwer, Des Tombe: Transfer in a Multilingual MT System (54%)Figure 4: An example of the presentation of a topicfor human evaluation.the top-50 and bottom-50 edges in terms of thebaseline word similarity.13 A random shuffle section from the union of top-100 edges in terms of the proposed approaches.5.2 Human AnnotationFor annotation, we present pairs of topics followedby questions.
Each topic is presented to a judge asa list of the most relevant bigrams in descending or-der of their topic-specific ?collapsed?
probabilities.These are presented in greyscale so that the mostrelevant items appear black, fading through greyto white as the strength of that item?s associationwith the topic decreases.
The evaluation interfacealso lists the documents that are most relevant tothe topic, linked to the original PDFs.
These doc-uments can be used to clarify the occurrence ofunfamiliar terms, such as author names or commonexamples that may show up in the topic representa-tion.
An example topic is shown in Figure 4.For each topic, judges were asked:1 How clear and coherent is Topic 1?2 How clear and coherent is Topic 2?1We observe that usually if the edge strength in terms ofone of the information-theoretic methods is zero, the wordsimilarity is zero as well, but if the word similarity is zero,the edge strength in terms of the proposed methods may benon-zero.871Edges Top 20 Top 150 All scores > 0Prec.
Prec.
Rec.
f1Prec.
Rec.
f1Cross entropy (DCE) 0.851 0.765 0.358 0.487 0.693 0.670 0.681Information flow (DIF) 0.793 0.696 0.311 0.429 0.693 0.323 0.441Word similarity (DSim) 0.808 0.768 0.382 0.511 0.768 0.382 0.511Hierarchy (DHier) 0.680 0.692 0.297 0.416 0.686 0.638 0.661Cite (DCite) 0.693 0.718 0.343 0.465 0.693 0.670 0.681Random 0.659 0.661 0.580 0.500 0.658 1.000 0.794Table 2: Precision, recall, and f-scores (with different thresholds for which edges are included) for themethods of predicting dependency relations between concepts described in Section 4.2.If both topics are at least somewhat clear:3 How related are these topics?4 Would understanding Topic 1 help you to under-stand Topic 2?5 Would understanding Topic 2 help you to under-stand Topic 1?For each question, they could answer ?I don?tknow?
or select from an ordinal scale:1 Not at all2 Somewhat3 Very muchThe evaluation was completed by eight judgeswith varying levels of familiarity with the technicaldomain.
Four judges are NLP researchers: ThreePhD students working in the area and one of the au-thors.
Four judges are familiar with NLP but haveless experience with NLP research: two MS stu-dents, an AI PhD student, and one of the authors.The full evaluation was divided into 10 sets takinga total of around 6?8 hours per person to anno-tate.
Their overall inter-annotator agreement andthe agreement for each question type is given in Ta-ble 1.
Agreement is higher when we consider onlyjudgments from NLP researchers, but in all casesis moderate, indicating the difficulty of interpret-ing statistical topics as concepts and judging thestrength (if any) of the concept dependency relationbetween them.The topic coherence judgments that were col-lected served to make each human judge considerhow well she understood each topic before judgingtheir dependence.
The topic relatedness questionsprovided an opportunity to indicate that if the an-notator recognized a relation between the topicswithout needing to say that their was a dependence.5.3 Evaluation of Automatic MethodsTo measure the quality of the concept dependencyedges in our graphs, we compute the average preci-sion for the strongest edges in each concept graph,up to three thresholds: the top 20 edges, the top 150,and all edges with strength > 0.
These precisionscores are in Table 2 as well as the correspondingrecall, and f1scores for the larger thresholds.
De-spite the difference in inter-annotator agreementreported in Table 1, the ordering of methods byprecision is the same whether we consider onlythe judgments of NLP experts, non-NLP judges, oreveryone, so we only report the average across allannotators.When we examine the results of precision at 20 ?the strongest edges predicted by each method ?
wesee that the cross-entropy method performs best.For comparison, we report the accuracy of a base-line of random numbers between 0 and 1.
Whileall methods have better than chance precision, therandom baseline has higher recall since it predictsa dependency relation of non-zero strength for allpairs.
As we consider edges predicted with lowerconfidence, the word similarity approach shows thehighest precision.
A limitation of the word similar-ity baseline is that it is symmetric while conceptdependence relations can be asymmetric.Annotators marked many pairs of concepts asbeing at least somewhat co-dependent.
E.g., un-derstanding Speech recognition strongly helps youunderstand Natural language processing, but be-ing familiar with this broader topic also somewhathelps you understand the narrower one.
The preci-sion scores we report count both annotations of con-cept dependence (?Somewhat?
and ?Very much?
)as positive predictions, but other evaluation met-rics might show a greater benefit for methods likeDCEthat can predict dependency with asymmetricstrengths.6 DiscussionAnother natural evaluation of an automatically gen-erated concept graph would be to compare it to a872MachinetransliterationParallelcorporaformachinetranslation2.672.67Wordalignment2.33Machinetranslationmodels2.33Machinetranslationevaluation2.33Machinetranslationsystems2.33Sentencealignment2.67 Part-of-speechtagging2.33Comparablecorpora2.672.673.002.67Reorderingmodel3.00Beamsearch&othersearchalgorithms3.00Phrase-basedmachinetranslation3.00Languagemodel3.002.672.33Humanassessment2.333.003.00HiddenMarkovmodels3.00Codingscheme2.50Annotation3.002.502.67Datamodelsforlinguisticannotation2.332.672.672.672.50Figure 5: A concept graph excerpt related to machine translation, where concepts are joined based onthe judgments of human annotators.
Concepts are represented by manually chosen names, and links todocuments are omitted.human-generated gold standard, where an experthas created concept nodes at the optimal level ofgenerality and linked these by her understanding ofthe conceptual dependencies among concepts in thedomain.
However, there are several difficulties withthis approach: (1) It is quite labor-intensive to man-ually generate a concept graph; (2) we expect onlymoderate agreement between graphs produced bydifferent experts, who have different ideas of whatconcepts are important and distinct and which con-cepts are important to understanding others; and(3) the concept graphs we learn from a collectionof documents will differ significantly from thosewe imagine, without these differences necessarilybeing better or worse.In this work, we assume that a topic model pro-vides a reasonable proxy for the concepts a personmight identify in a technical corpus.
However, topicmodeling approaches are better at finding generalareas of research than at identifying fine-grainedconcepts like those shown in Figure 1.
The conceptgraph formalism can be extended with the use ofdiscrete entities, identified by a small set of names,e.g., (First-order logic, FOL).
We have performedinitial work on two approaches to extract entities:1 We can use an external reference, Wikipedia, tohelp entity extraction.
We count the occurrencesof each article title in the scientific corpus, andwe keep the high-frequency titles as entities.
Forexample, in the ACL Anthology corpus, we ob-tain 56 thousand entities (page titles) that oc-curred at least once and 1,123 entities that occurat least 100 times.2 We cannot assume that the important entitiesin every scientific or technical corpus will bewell-represented on Wikipedia.
In the absenceof a suitable external reference source, we canuse the open-source tool SKIMMR (Nov?a?cekand Burns, 2014) or the method proposed byJardine (2014) to extract important noun phrasesto use as entities.
The importance of a potentialentity can be computed based on the occurrencefrequency and the sentence-level co-occurrencefrequency with other phrases.Another limitation of using a topic model likeLDA as a proxy for concepts is that the topics arestatic, while a corpus may span decades of research.Studying how latent models might evolve or ?drift?over time within a textual corpus describing a tech-nical discipline is an important research question,and our approach could be extended to add or re-move topics in a central model over time.Despite its limitations, a topic model is usefulfor automatically discovering concepts in a corpuseven if the concept is not explicitly mentioned ina document (e.g., the words ?axiom?
or ?predi-873cate?
might indicate discussion of logic) or has nocanonical name.
The concept graph representationallows for the introduction of additional or alter-native features for concepts, making it suitable fornew methods of identifying and linking concepts.7 ConclusionsProblems such as reading list generation requirea representation of the structure of the content ofa scientific corpus.
We have proposed the conceptgraph framework, which gives weighted links fromdocuments to the concepts they discuss and linksconcepts to one another.
The most important linkin the graph is the concept dependency relation,which indicates that one concept helps a learnerto understand another, e.g., Markov logic networksdepends on Probability.We have presented four approaches to predictingthese relations.
We propose information-theoreticmeasures based on cross entropy and on informa-tion flow.
We also present baselines that computethe similarity of the word distributions associatedwith each concept, the likelihood of a citation con-necting the concepts, and a hierarchical clusteringapproach.
While word similarity proves a strongbaseline, the strongest edges predicted by the cross-entropy approach are more precise.
We are releas-ing human annotations of concept nodes and pos-sible dependency edges learned from the ACL An-thology as well as implementations of the methodsdescribed in this paper to enable future research onmodeling scientific corpora.2AcknowledgmentsThe authors thank Yigal Arens, Emily Sheng, andJon May for their valuable feedback on this work.This work was supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) viathe Air Force Research Laboratory.
The U.S. Gov-ernment is authorized to reproduce and distributereprints for Governmental purposes notwithstand-ing any copyright annotation thereon.
Disclaimer:The views and conclusions contained herein arethose of the authors and should not be interpreted asnecessarily representing the official policies or en-dorsements, either expressed or implied, of IARPA,AFRL, or the U.S. Government.2The code and data associated with this work are availableat http://techknacq.isi.eduReferencesSanjeev Arora, Rong Ge, and Ankur Moitra.
2012.Learning topic models ?
going beyond SVD.
In Pro-ceedings of the 53rd Annual Symposium on Founda-tions of Computer Science, pages 1?10.
IEEE.Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,Mark Joseph, Min-Yen Kan, Dongwon Lee, BrettPowley, Dragomir Radev, and Yee Fan Tan.
2008.The ACL Anthology Reference Corpus: A referencedataset for bibliographic research in computationallinguistics.
In Proceedings of the Sixth InternationalConference on Language Resources and Evaluation,Marrakech, Morocco, May.
European Language Re-sources Association.David Blei and John Lafferty.
2006.
Correlated topicmodels.
In Advances in Neural Information Process-ing Systems.David M. Blei, Andew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Eunsol Choi, Matic Horvat, Jon May, Kevin Knight,and Daniel Marcu.
2016.
Extracting structuredscholarly information from the machine translationliterature.
In Proceedings of the 10th InternationalConference on Language Resources and Evaluation.European Language Resources Association.Philipp Cimiano, Andreas Hotho, and Steffen Staab.2005.
Learning concept hierarchies from text cor-pora using formal concept analysis.
Journal of Arti-ficial Intelligence Research, 24(1):305?39, August.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers, Norwell, MA, USA.Qirong Ho, Jacob Eisenstein, and Eric P. Xing.
2012.Document hierarchies from text and links.
In Pro-ceedings of the International World Wide Web Con-ference, April.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the 22nd Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 50?7.ACM.James G. Jardine.
2014.
Automatically generat-ing reading lists.
Technical Report UCAM-CL-TR-848, University of Cambridge Computer Laboratory,February.Istvan Jonyer, Diane J. Cook, and Lawrence B. Holder.2002.
Graph-based hierarchical conceptual cluster-ing.
Journal of Machine Learning Research, 2:19?43, March.Andrew McCallum.
2002.
MALLET: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.874V?
?t Nov?a?cek and Gully APC Burns.
2014.
SKIMMR:Facilitating knowledge discovery in life sciences bymachine-aided skim reading.
PeerJ, 2:e483.Dragomir R. Radev, Pradeep Muthukrishnan, VahedQazvinian, and Amjad Abu-Jbara.
2013.
The ACLAnthology Network Corpus.
Language Resourcesand Evaluation, pages 1?26.Martin Rosvall and Carl T. Bergstrom.
2008.
Maps ofrandom walks on complex networks reveal commu-nity structure.
Proceedings of the National Academyof Sciences, 105(4):1118?23.Jean Michel Rouly, Huzefa Rangwala, and AdityaJohri.
2015.
What are we teaching?
: Automatedevaluation of CS curricula content using topic mod-eling.
In Proceedings of the Eleventh Annual In-ternational Conference on International ComputingEducation Research, pages 189?197.Mark Sanderson and Bruce Croft.
1999.
Deriving con-cept hierarchies from text.
In Proceedings of the22nd Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, pages 206?13, New York, NY, USA.
ACM.Ulrich Sch?afer, Bernd Kiefer, Christian Spurk, J?orgSteffen, and Rui Wang.
2011.
The ACL AnthologySearchbench.
In Proceedings of the ACL-HLT 2011System Demonstrations, pages 7?13.Laura M. Smith, Linhong Zhu, Kristina Lerman, andAllon G. Percus.
2014.
Partitioning networks withnode attributes by compressing information flow.arXiv preprint arXiv:1405.4332.Partha Pratim Talukdar and William W. Cohen.
2012.Crowdsourced comprehension: Predicting prerequi-site structure in Wikipedia.
In Proceedings of theSeventh Workshop on Building Educational Appli-cations Using NLP, pages 307?15.
Association forComputational Linguistics.Xiaolong Wang, Chengxiang Zhai, and Dan Roth.2013.
Understanding evolution of research themes:A probabilistic generative model for citations.
InProceedings of the 19th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 1115?23, New York, NY, USA.ACM.Ryen W. White and Joemon M. Jose.
2004.
A studyof topic similarity measures.
In Proceedings of the27th Annual International ACM SIGIR Conferenceon Research and development in Information Re-trieval, pages 520?1.
ACM.William A.
Woods.
1997.
Conceptual indexing: A bet-ter way to organize knowledge.
Technical report,Sun Microsystems, Inc., Mountain View, CA, USA.Yiming Yang, Hanxiao Liu, Jaime Carbonell, andWanli Ma.
2015.
Concept graph learning from ed-ucational data.
In Proceedings of the Eighth ACMInternational Conference on Web Search and DataMining, pages 159?68.
ACM.875
