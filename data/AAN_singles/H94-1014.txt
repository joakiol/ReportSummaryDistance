Language Modeling with Sentence-Level MixturesRukmini lyer t Mari Ostendorflf J. Robin Rohlicek~Boston Univers i ty  ~ BBN Inc.Boston,  MA 02215 Cambr idge,  MA 02138ABSTRACTThis paperintroduces a simple mixtare language model that attemptsto capture long distance conslraints ina sentence or paragraph.
Themodel is an m-component mixture of Irigram models.
The modelswere constructed using a 5K vocabulary and trained using a 76 mil-lion word Wail Street Journal text corpus.
Using the BU recognitionsystem, experiments show a 7% improvement i  recognition accu-racy with the mixture trigram models as compared tousing a Irigrammodel.1.
INTRODUCTIONThe overall performance of a large vocabulary continuousspeech recognizer is greatly impacted by the constraints im-posed by a language model, or the effective constraints of astochastic language model that provides the a priori proba-bility estimates of the word sequence P (wz , .
.
.
,wr ) .
Themost commonly used statistical language model assumes thatthe word sequence can be described as a high order Markerprocess, typically referred to as an n-gram model, where theprobability of a word sequence is given by:TP(w , .
.
.
.
= O)i=IThe standard n-gram models that are commonly used are thebigram (n = 2) and the trigram (n = 3) models, where n islimited primarily because of insufficient training data.
How-ever, with such low order dependencies, these models fail totake advantage of 'long distance constraints' over the sen-tence or paragraph.
Such long distance dependencies maybe grammatical, sin verb tense agreement orsingnlar/pinralquantifier-noun agreement.
Or, they may also be a conse-quence of the inhomogeneous nature of language; differentwords or word sequences are more likely for particular broadtopics or tasks.
Consider, for example, the following re-spouses made by the combined BU-BBN recognition systemon the 1993 Wall Street Journal (WSJ) benchmark H1-C1(20K) test:REF: the first recipient joseph webster junior ** ******a PHI BETA KAPPA chemistry GRAD who plans to takecourses this fall in ART RELIGION **** music and po-litical scienceHYP: the first recipient joseph webster junior HE FRI-DAY a CAP OF ***** chemislzy GRANT who plans totake comes this fall in AREN'T REALLY CHIN musicand pofitical scienceREF: *** COCAINE doesn't require a SYRINGE THEsymbol of drug abuse and CURRENT aids risk YET canbe just as ADDICTIVE and deadly as HEROINHYP: THE KING doesn't require a STRANGE A sym-bel of drug abuse and TRADE aids risk IT can be just asADDICTED and deadly as CHAIRMANIn the first example, "art" and "refigion" make more sense inthe context of "courses" than "aren't really chin", and simi-larly "heroin" should be more likely than "chairman" in thecontext of "drug abuse".The problem of representing long-distance d pendencies hasbeen explored in other stochastic language models, thoughthey tend to address only one or the other of the two is-sues raised here, i.e.
either sentence-level or task-level depen-dence.
Language model adaptation (e.g.
\[1, 2, 3\]) addressesthe problem of inhomogeneity of n-gram statistics, but mainlyrepresents ask level dependencies and does little to accountfor dependencies within a sentence.
A context-free grammarcould account for sentenco level dependencies, but it is costlyto build task-specific grammars as well as costly to implementin recognition.
A few automatic learning techniques, whichare straight-forward to apply to new tasks, have been inves-tigated for designing static models of long term dependence.For example, Bald et al \[4\] used decision tree clustering toreduce the number of n-grams while keeping n large.
Otherefforts include models that integrate n-grams with context-free grammars (e.g., \[5, 6, 7\]).Our approach to representing long term dependence attemptsto address both issues, while still using a very simple model.We propose to use a mixture of n-gram language models,but unlike previous work in mixture language modeling ourmixture components are combined at the sentence l vel.
Thecomponent n-grams enable us to capture topic dependence,while using mixtures at the sentence l vel captures the notionthat opics do not change mid-sentence.
Like the model pro-posed by Kneser and Stcinbiss \[8\], our language model usesm component language models, each of which can be identi-fied with the n-gram statistics of a speeific topic or broad class82of sentences.
However, unlike \[8\], which uses mixtures at then-gram level with dynamically adapted mixture coefficients,we use sentence-level mixtures to capture within-sentcnce de-pendencies.
Thus, the probability of a word sequence is theweighted combination:m T-- Ak nk=l  i=1  (2)Our approach as the advantage that it can be used eitheras a static or a dynamic model, and can easily leverage thetechniques that have been developed for e~ptive languagemodeling, particularly cache \[1, 9\] and trigger \[2, 3\] models.One might raise the issue of recognition search cost for amodel of mixtures at the sentence level, but in the N-bestrescoring framework \[10\] the additional cost of the mixturelanguage model is minimal.The general framework and mechanism for designing the mix-ture language model will be described in the next section,including descriptions of automatic topic clustering and ro-bust.estimation techniques.
Following this discussion, wewill present some experimental results on mixture languagemodeling obtained using the BU recognition system.
Finally,the paper will conclude with a discussion of the possible x-tensions of mixture language models, to dynamic languagemodeling and to applications other than speech transcription.2.
M IXTURE LANGUAGE MODEL2.1.
Genera l  F rameworkThe sentence-level mixture language model was originallymotivated by an observation that news stories (and certainlyother domains as well) often reflect he characteristics of dif-ferent opics or sub-domains, such as sports, finance, nationalnews and local news.
The likelihood of different words orn-grams could be very different in different sub-domains,and it is unlikely that one would switch sub-domains mid-sentence.
A model with sentence-level mixtures of topic-dependent component models would address this problem,but the model would be more general ff it also allowed forn-gram level mixtures within the components (e.g.
for robustestimation).
Thus, we propose amodel using mixtures at twolevels: the sentence and the n-gram level.
Using trigramcomponents, this model is described bym Tk=l  i---1+(1 - Ok)P1(w, lwi-1, w,-2)\], (3)where k is an index to the particular topic describedby the component language model Pk('\]'), PI(-\]') is atopic-independent model that is interpolated with the topic-dependent model for purposes of robust estimation or dynamiclanguage model adaptation, and At and 0k are the sentence-level and n-gram level mixture weights, respectively.
(Notethat he component-dependent term Pk (toi Item_ 1, w~-2) coulditself be a mixtme.
)Two important aspects of the model are the definition of "'top-ics" and robust parameter estimation.
The m componentdistributions of the language model correspond to different"topics", where topic can mean any broad class of sentences,such as subject area (as in the examples given above) or verbtense.
Topics can be specified by hand, according to text la-bels ff they are available, or by heuristic rules associated withknown characteristics of a task domain.
Topics can also bedetermined automatically, which is the approach taken here,using any of a variety of clustering methods to initialiTc thecomponent models.
Robust parameter stimation is anotherimportant issue in mixture language modeling, because theprocess of partitioning the d,tA into topic-dependent subsetsreduces the amount of training available to estimate each com-ponent language model.
These two issues, automatic clnster-ing for topic initialization and robust parameter estimation,are described further in the next two subsections.2.2.
C luster ing  A lgor i thmSince the standard WSJ language model training d~t~ doesnot have topic labels associated with the text, it was neces-sary to use automatic clustering to identify natural groupingsof the d~t~ into "topics".
Because of its conceptual simplic-ity, agglomerative clustering is used to partition the trainingdAt.
intO the desired number of clusters.
The clustering isat the paragraph level, relying on the assumption that an en-tire paragraph comes from a single topic.
Each paragraphbegins as a singleton cluster.
Paragraph pairs are then pro-gressively grouped into clusters by computing the similaritybetween clusters and grouping the two most similar clusters.The basic clustering algorithm is as follows:1.
Let the desired number of clusters be C* and the initialnumber of clusters C be the number of singleton da!~samples, or paragraphs.2.
Find the best matched clusters, say Ai and Aj, to mini-mize the similarity criterion S~.3.
Merge Ai and Aj and decrement C.4.
If current number of clusters C = C*, then stop; other-wise go to Step 2.At the end of this stage, we have the desired number of par-titions of the training datz: To save computation, we runagglomerative clustering first on subsets of the dnt~; and thencontinue by agglomerating resulting clusters into a final set ofm clusters.83A variety of similarity measures can be envisioned.
We usea normalized measure of the number of content words incommon between the two clusters.
(Paragraphs compriseboth function words (e.g.
is, that, bu0 and content words (e.gstocks, theater, trading), but the function words do not con-tribute towards the identification ofa paragraph as belongingto a particular topic so they are ignored in the similarity crite-rion.)
Letting Ai be the set of unique content words in clusteri, lAd the number of elements in Ai, and Ni the number ofparagraplas incluster i, then the specific measure of similarityof two clusters i and j is&~ = Ni~ I& n At\[I& u Ail ' (4)where/ N, += (5)is a normalization factor used to avoid the tendency for smallclusters to group with one large cluster rather than other smallclusters.At this point, we have only experimented with a small numberof clusters, so it is difficult to see coherent topics in them.However, it appears that the current models are putting newsrelated to foreign affairs (politics, as well as travel) into onecluster and news relating to finance (stocks, prices, loans) inanother.2.3.
Parameter EstimationEach component model is a conventional n-gram model.
Ini-tial n-gram estimates for the component models are basedon the partitions of the training data, obtained by using theabove clustering algorithm.
The initial component modelsare estimated separately for each cluster, where the Witten-Bell back-off \[11\] is used to compute the probabilities ofn-grams not observed in training, based on distributing a cer-tain amount of the total probability mass among unseen n-grams.
This method was chosen based on the results of \[12\]and our own comparative experiments with different back-offmethods for WSJ n-gram language models.
The parame-ters of the component models can be re-estimated using theExpectation-Maximization (EM) algorithm \[13\].
However,since the EM algorithm was computationally intensive, an it-erative re-labeling re-estimation technique was used.
At eachiteration, the training data is re-partitioned, by re-labelingeach utterance according to which component model maxi-mizes the likelihood of that utterance.
Then, the componentn-gram statistics are re-computed using the new subsets of thetraining data, again using the Witten-Bell back-off technique.The iterations continue until a steady state size for the clustersis reached.Since the component models are built on partitioned trainingdata, there is a danger of them being undertrained.
There aretwo main mechanisms we have explored for robust parameterestimation, inaddition to using standard back-off techniques.One approach is to include ageneral model PG trained on allthe data as one of the mixture components.
This approach asthe advantage that he general model will be more appropriatefor recognizing sentences that do not fall clearly into any of thetopic-dependent components, but the possible disadvantagethat he component models may be underutilized because theyare relatively undertrained.
An alternative is to interpolatethe general model with each component model at the n-gramlevel, but this may force the component models to be toogeneral in order to allow for unforeseen topics.
Given thesetrade-offs, we chose to implement a compromise between thetwo approaches, i.e.
to include a general model as one of thecomponents, aswell as some component level smoothing viainterpolation with a general model.
Specifically, the model isgiven byTP(w,,...,wT)---- ~ ~k I~(okPk(wilwi-,,wl-2)k=I,...,C,G i=1+(1 - Ok)Pa,(wilwi-l,wi-2)) (6)where Pa, is a general model (which may or may not be thesame as Pa), {Ak} provide weights for the different topics,and {Ok } serve to smooth the component models.Both sets of mixture weights are estimated on a separate dataset, using a maximum likelihood criterion and initializing withuniform weights.
To simplify the initial implementation, wedid not estimate the two sets of weights { Ak } and {0k } jointly.Rather, we first labeled the sentences in the mixture weightestimation data set according to their most likely componentmodels, and then separately estimated the weight 0k tO max-imize the likelihood of the data assigned to its cluster.
Fora single set of data, the mixture weight estimation algorithminvolves iteratively updating1 N o~ldpk(wl,.., Wn,)O~e'?
= N ~ ~;  0~'aP/(----'--~17.-'7:, w -~ ) (7)where n~ is the number of words in sentence i and N is the totalnumber of sentences incluster k. After the component modelshave been estimated, the sentence-level mixture weights { Ak }are estimated using an analogous algorithm.3.
EXPERIMENTSThe corpus used for training the different component modelscomprised the 38 million WSJ0 data, as well as the 38 mil-lion word augmented LM data obtained from BBN Inc. Thevocabulary is the standard 5K non-verbalized pronunciation(NVP) data augmented with the verbalized punctuation wordsand a few additional words.
In order to compute the mixtureweights, both at the trigram-level as well as the sentence-level, the WSJ1 speaker-independent transcriptions serve as84the "held out" da!a set.
Because we felt that he training datamay not accurately represent the optional verbalized punctu-ation frequency in the WS.I1 data, we chose to train modelson two dat~ sets.
The general model Pa and the componentmodels Pt were trained on the WSJ0 NVP data augnlentedby the BBN data.
The general model Pa, was trained onthe WSJ0 verbalized pronunciation data, so that using Po,in smoothing the component models also provides a simplemeans of allowing for verbalized pronunciation.The experiments compare a single trigram language modelto a five-component mixture of trigram models.
To explorethe trade-offs of using different numbers of clusters, we alsoconsider an eight-component trigram mixture.
Perplexity andrecognition results are reported on the Nov. '93 ARPA devel-opment and evaluation 5k vocabulary WSJ test sets.3.1.
Recognition ParadigmThe BU Stochastic Segment Model recognition system iscombined with the BBN BYBLOS system and uses the N-bestresc0ring formalism \[10\].
In this formalism, the BYBLOSsystem, a speaker-independent Hidden Markov Model Sys-tem \[14\] 1, is used to compute the top N sentence hypothesesof which the top 100 are subsequently rescored by the SSM.A five-pass earch strategy isused to generate the N-best hy-potheses, and these are re.scored with thirteen state HMMs.A weighted combination ofscores from different knowledgesources is used to re-rank the hypotheses and the top rankinghypothesis is used as the recognized output.
The weights forrecombination are estimated on one test set (in this case the93 H2 development test data) and held fixed for all other testsets.No.
of % Wordcomponents error Perplexity1 7.3 1185 7.1 1168 7.2 114Table 1: Dependence on number of components: evaluationon the '93 ARPA 5k WSJ development test set.The next series of experiments, summarized in Table 22 ,com-pared recognition performance for the BBN trigram languagemodel \[15\], the BU 5-component mixture model, and thecase where both language model scores are used in the N-bestreranking.
All language models were estimated from the sametraining a,!8: The results how a 7% reduction i  error rateon the evaluation test set, comparing the combined languagemodels to the BBN trigram.
It is interesting that he combi-nation of the trigram and the mixture model yielded a smallimprovement i  performance (not significant, but consistentacross lest sets), since the trigram is a component ofthe mix-ture model.
The difference between the mixture model and thetwo combined models corresponds to a linear vs. non-linearcombination fcomponent probabilities, respectively.For reference, we also include the best case system perfor-mance, which corresponds the the case where all acoustic andlanguage model scores.
Even with all the acoustic modelscores, adding the mixture language model improves perfor-mance, giving a best case result of 5.3% word error on the '935k WSJ evalnnt_ion lest set.We conducted a series of experiments in the rescoringparadigm to assess the usefulness of the mixture model.
Un-less otherwise noted, the only acoustic model score used wasbased on the stochastic segment model.
The language modelscores used varied with the experiments.
For the best-casesystem, we used all scores, which included the SSM and theBBN Byblos HMM and SNN acoustic scores, and both theBBN trigram and BU mixture language model scores.4.
DISCUSSIONIn summary, this paper presents anew approach to languagemodeling, which offers the potential for capturing both topic-dependent effects and long-range sentence l vel effects in2The performance figul~ quoted here are better than throe repoaed inthe official November 1993 WSJ benchmark esults, because more languagemodel training data was available in the experimant repoNe.d here.3.2.
ResultsThe results reported in Table 1 compare three different lan-guage models in terms of perplexity and recognition perfor-mance: a simple trigram, and five- and eight-component mix-tures.
The mixture models reduce the perplexity only by asmall amount, but there is a reduction i  word-error with thefive-component mixture model.
We hypothesize that thereis not enough training data to effectively use more mixturecomponents.1For an indication fthe performance of this system, see the benchmarksummary in \[17\].KSs used % Word ErrorAM LM Dev Evali trigram 7.4 6.1SSM mixture 7.1 5.8: both 7.0 5.7all both 6.3 5.3Table 2: Summary of results on'93 ARPA 5k WSJ test setsfor different acoustic model (AM) and language model (LM)knowledge sources (KSs).85a conceptually simple variation of statistical n-gram mod-els.
The model is actually a two-level mixture model, withseparate mixture weights at the n-gram and sentence l vels.Training involves either automatic clustering or heuristic rule, sto determine the initial topic-dependent models, and an itera-tive algorithm for estimating mixture-weights at the differentlevels.
Recognition experiments on the WSJ task showeda significant improvement in the accuracy for the BU-SSMrecognition system.This work can be extended in several ways.
First, time lim-itations ,did not permit us to explore the use of the completeEM algorithm for estimating mixture components and weightsjointly, ~xid we hope to investigate hat approach in the future.In addition, it may be useful to consider other metrics forautomatic topic clustering, such as a word count weighted byinverse document frequencies or a multinomial distributionassumption with a likelihood clustering criterion.
Of course,it would also be interesting tosee ff further performance gainscould be achieved with more clusters.
Much more could alsobe done in the area of robust parameter stimation.
For exam-ple, one could use an n-gram part-of-speech sequence modelas the base for all component models and topic-dependentword likelihoods given the part-of-speech label, a natural ex-tension of \[16\].Dynamic language model adaptation, which makes use ofthe previous document history to tune the language modelto that particular topic, can easily fit into the mixture modelframework in two ways.
First, the sentence-level mixtureweights can be adapted according to the likelihood of therespective mixture components in the previous utterance, asin \[8\] for n-gram level mixture weights.
Second, the dynamicn-gram cache model \[I, 9\] can easily be incorporated intothe mixture language model.
However, in the mixture model,it is possible to have component-dependent cache models,where each component cache would be updated after eachsentence according to the likelihood of that component giventhe recognized word string.
Trigger models \[2, 3\] could alsobe component dependenLThe simple static mixture language model can also be use-ful in applications other than continuous speech transcription.For example, topic -dependent models could be used for topicspotting.
In addition, as mentioned earlier, the notion of topicneed not be related to subject area, it can be related to speak-ing style or speaker goal.
In the ATIS task, for example,the goal of the speaker (e.g.
flight information request, re-spouse clarification, error correction) is likely to be reflectedin the language of the utterance.
Representing this structureexplicitly has the double benefit of improving recognitionperformance and providing information for a dialog model.From a cursory look at our recognition errors from the recentWSJ  benchmark tests, it is clear that topic-dependent modelswill not be enough to dramatically reduce word error rate.Out-of-vocabulary words and function words also representa major source of errors.
However, an important advantageof this framework is that it is a simple extension of existinglanguage modeling techniques that can easily be integratedwith other language modeling advances.AcknowledgmentsThis work was supported jointly by ARPA and ONR on grantONR ONR-N00014-92-J-1778.
We gratefully acknowledgethe cooperation of several researchers atBBN, who providedthe N-best hypotheses used in our recognition experiments,as well as additional language model training data.ReferencesI.
F. Jelinek, B. Merialdo, S. Roukos andM.
Strauss, "A DynamicLM for Speech Recognition," Proc.
DARPA Workshop onSpeech and Natural Language, pp.
293-295, 1991.2.
R. Lau, R. Rosenfeld and S. Roukos,'IYigger-Based LanguageModels: a Maximum Entropy Approach," Proc.
Int'l.
Conf.on Acoust., Speech and Signal Proc., VoL H, pp.
45-48, 1993.3.
R. Rosenfeld,"A Hybrid Approach to Adaptive Statistical Lan-guage Modeling," this proceedings.4.
L. R. Bah1, E E Brown, P. V. deSouza nd R. L. Mercer, "ATree-Based Statistical Language Model for Natural LanguageSpeech Recognition," IEEE Trans.
on Acouat., Speech, andSi&nalProc., Vol.
37, No.
7, pp.
1001-1008, 1989.5.
J. H. Wright, G. J. F. Jones and H. Lloyd-Thomas, "A Con-solldated Language Model For Speech Recognition," Proc.EuroSpeech, Vol.
2, pp.
977-980, 1993.6.
M. Mercer and J. R. Rohlicek, "Statistical Language ModelingCombining n-gram and Context Free Grammars," Proc.
lnt'l.Conf.
on Acouat., Speech and Signal Proc., Vol.
2, pp.
37-40,1993.7.
J. Lafferty, "Integrating Probabilistic Finite-State and Context-Free Models of Language," presentation at the IEEE ASRWorkshop, December 1993.8.
R. Kneser and V. Steinbiss, "On the Dynamic Adaptation OfStochastic LM," Proc.
Int'l.
Conf.
on Acoust., Speech andSignalProc., Vol.
2, pp.
586-589, 1993.9.
R. Kulm and R. de Mori, "A Cache Based Natural LanguageModel for Speech Recognition," IEEE Trans.
PAMI, VoL 14,pp.
570-583, 1992.10.
M. Ostendorf, A. Karman, S. Austin, O. Kimball, R. Schwartzand J. R. Rohlicek, "Integration of Diverse RecognitionMethodologies Through Reevaluation fN-Best Sentence Hy-potheses," Proc.
DARPA Workshop on Speech and NaturalLanguage, pp.
83-87, February 1991.11.
H.Witten and T. C. Bell, Whe Zero Frequency Estimation ofProbabilities of Novel Events in Adaptive Text Compression,"IEEE Trans.lnformation Theory, VoL 1T-37, No.
4, pp.
1085-1094, 1991.12.
R Placeway and R. Schwartz, "Estimation Of Powerful LMfrom Small and Large Corpora," Proc.
lnt'l.
Conf.
onAcoust.,Speech andSignaIProc., Vol.
2, pp.
33-36, 1993.13.
A. P. Dempster, N. M. Laird and D. B. Rubin, "MaximumLikelihood Estimation from Incomplete Data," Journal of theRoyal Statiatical Society (B), VoL 39, No.
1, pp.
1-38, 1977.8614.
BBN Byblos November 1993 WSJ Benchmark system.15.
R. Schwartz et al, "On Using Written Language'l~raining Datafor Spoken Language Modeling," this proceedings.16.
M. Elbeze andA.-M. Derouault, "A Morphological Model forLarge Vocabulary Speech Recognition," Proc.
Ira'l.
Conf.
onAcouat., SpeechandSisnalProc., VoL 1, pp.
577-580, 1990.17.
D. Pallett, J. Fiscus, W. Fisher, J. Garofolo, B. Lund and M.Pryzbocki, "1993 Benchmark Tests for the ARPA spoken Lan-guage Program," this p~gs .87
