Recurrent  Neura l -Network  .Learningof  Phono log ica l  Regular it ies m TurkishJennifer RoddCentre for Cognitive ScienceUniversity of Edinburgh2 Buccleugh PlaceEdinburgh, EH8 9LW, UKj enni~cogsci, ed.
ac.
ukAbstractSimple recurrent networks were trainedwith sequences of phonemes from a cor-pus of Turkish words.
The network's taskwas to predict he next phoneme.
The aimof the study was to look at the represen-tations developed within the hidden layerof the network in order to investigate theextent to which such networks can learnphonological regularities from such input.It was found that in the different networks,hidden units came to correspond to detec-tors for natural phonological classes uch asvowels, consonants, voiced stops, and frontand back vowels.
The initial state of thenetworks contained no information of thistype, nor were these classes explicit in theinput.
The networks were also able to en-code information about the temporal dis-tribution of these classes.1 Network ArchitectureThe network used is a simple recurrent network ofthe type first investigated by Elman (Elman, 1990).It consists of a feedforward network, supplementedwith recurrent connections from the hidden layer.It was trained by the back-propagation learning al-gorithm (Rumelhart, Hinton and Williams, 1986).The ability of such networks to extract phonologicalstructure is well established.
For example, Gasser(Gasser, 1992) showed that a similar network couldlearn distributed representations for syllables whentrained on words of an artificial anguage.
Figure 1shows the architecture of the network.
Within thisnetwork architecture, four different network config-urations were investigated.
These all had 28 units inboth the input and output layers; they varied onlyin the number of units in the hidden layer, rangingfrom two to five.Output UnitslFigure 1: Network ArchitectureAll connections in the network have an inherenttime delay of one time step.
This has the result thatthe recurrent connections between units in the hid-den layer give the network access to a copy of itshidden-layer activations at the previous time step.The delay also has the effect that it takes two timesteps for any information to propagate from the in-put layer through to the output layer.
The networkis fully connectedThe input to the network is a series of sequentiallypresented phonemes from a corpus of 602 Turkishwords.
Each phoneme is represented by a 28-bitvector in which each of the 28 Turkish phonemespresent in the corpus is represented by a differentbit.
Whenever a particular phoneme is present, thecorresponding bit is flipped on.
This sparse ncodingscheme was taken from Elman (Elman, 1990), andensures that each vector is orthogonal to every othervector.
Thus the network is given no informationabout the similarity between different phonemes.Each word is presented to the network as a seriesof such phonemes, with each phoneme presented ina successive time step.
Each word constitutes a dis-crete training item, i.e.
the network is not requiredto segment words.
During training, the weights areupdated after each training item, i.e.
after everyword.The task of the network for each input phonemeRodd 97 Recurrent NN LearningJennifer Rodd (1997) Recurrent Neural-Network Learning of Phonological Regularities in Turkish.T.M.
Ellison (ed.)
CoNLL97: Computational Natural Language Learning, ACL pp 97-106.
(~) 1997 Association for Computational LinguisticsInis to predict the following phoneme.
Due to the twotime delays present in the structure of the network,this prediction task is constructed by requiring theunits in the output layer to show the pattern of ac-tivation present at the input layer on the preced-ing time step.
The network's tructure nsures thatnone of this information from the preceding timestep could have propagated through to the outputunits by this time, and so the task is a genuine pre-diction task.The networks are trained using the Xerion simula-tor, using the back-propagation learning algorithm.A momentum term is used to reduce the trainingtime.2 Vowel Harmony in TurkishThe networks are trained on Turkish words.
Turkishwas chosen for its well-documented and interestingphonological structure; in particular its vowel har-mony (Lewis, 1967).
It has already been shown byHare (Hare, 1990) that vowel harmony can be mod-elled successfully by using connectionist models.
Shehas used recurrent networks of the type developedby Jordan (Jordan, 1986) to model vowel harmonyin Hungarian.
This model successfully accounts formany of the complexities of Hungarian vowel har-mony, and predicts the behaviour of both harmonicand transparent vowels.Unlike Hare, however, I am not concerned withthe modelling of a particular phonological process.
Iam interested in investigating what information sucha network can learn about the structure of a lan-guage, given minimal information.
Hare's networksare given a featural description of the phonemes in-volved, and so have an inherent measure of theirsimilarity, and therefore of the phonological classes.Further, Hare's networks are given only sequencesof vowels.
My networks are given both the vowelsand the intervening consonants, and therefore havethe possibility to simultaneously learn a wide rangeof phonological regularities.
Despite the presence ofintervening consonants, it was expected that suchnetworks could learn the basics of vowel harmony.Therefore, before discussing the networks in detail,let me first outline vowel harmony in Turkish.Clements and Sezer (Clements and Sezer, 1982)describe vowel harmony as a "system of phonolog-ical organization according to which all vowels aredrawn from one or the other of two (possibly over-lapping) sets within harmonic spans in the word".Turkish is an example of what Clements and Sezercall a "symmetrical vowel harmony system".
Wordsconsist of a stem and a sequence of suffixes.
Thevowels in the stem do not alternate, while the vow-els in the suffixes alternate such that they agree withthe nearest non-alternating vowel.
Specifically, inTurkish, each word will typically only contain vow-els with the same value for the feature \[::t=front\].
Thefronting of the stem vowels determines the frontingof the suffix vowel(s).
The fronting of the vowelswithin the stem itself is usually uniform.
There isalso vowel harmony for the feature \[:kround\].
Anyhigh vowel in the second syllable of the word (orlater) has the same value for the feature \[::l:round\]as the vowel in the preceding syllable.
Low vowelsafter the first syllable are all \[-round\].Table 1: Phonological features for Turkish vowelsI I'l?lilulil \[::l=front\] - + - + -- -- \[:kround\] - - + +Turkish also displays consonant harmony.
Theconsonants/k,  g, l /each have two phonetic shapes,which differ in the value of the feature \[:t:front\].
Thevalue for this feature is determined by the fronting ofthe vowels in the word.
However, this phenomenonof consonant harmony can clearly not be consideredin this study, as the two allophones for these conso-nants are represented by the same phoneme in theinput data.Clements and Sezer (Clements and Sezer, 1982)describe in detail a number of exceptions to thesebasic harmony rules, and provide an account forthese irregularities in terms of the presence of opaquevowels and consonants in the underlying represen-tation of the segments.
Exceptional cases includethe existence of some disharmonic polysyllabic roots.Disharmonic suffixes also exist, in which at least onevowel fails to alternate under any circumstances.The corpus used for this study contained 601Turkish words.
91% of these words showed harmonyfor the feature \[::t=front\].
The other 9% containedboth front and back vowels.3 The NetworksI will now discuss the results of four simulations us-ing networks of the type described above.
The onlydifference between the architectures of the networksis the number of units in the hidden layer; the train-ing data remained the same for all simulations.
Iwill discuss in detail results from individual train-ing runs.
It was found that different runs startingwith different initial randomized weights producedresults that were remarkably consistent.
Therefore,for clarity, I will discuss only one set of results foreach simulation.
The networks differed in the num-Rodd 98 Recurrent NN Learningber of times the corpus was seen in training; thisranged from 77 for Network 4, to 91 for Network1.
These numbers were determined by the pointat which the network reached a particular tolerancelevel for its error score.
For each network, a valuefor the tolerance in the error was chosen that consis-tently enabled the network to settle on the solutionsdescribed.
Lowering this tolerance resulted in a fail-ure of the learning algorithm to converge, while in-creasing the tolerance resulted in the network learn-ing very specific regularities about the training set.In such cases the regularities learnt depended on theinitial weights.
The tolerance levels were thereforechosen to produce networks that consistently learnedgeneral solutions to the prediction task.Network  1For this network, the hidden layer has only twounits.
The results of training such a network onthe corpus are extremely clear and consistent.
Thetraining algorithm for the particular network I shallconsider here converged after 54585 training exam-ples (i.e., each training example was seen approxi-mately 91 times).
The restriction to only two hiddenunits allows the network to encode a single regularityin the structure of the input.
The strongest phono-logical regularity present in Turkish, as with mostlanguages, is the alternation of vowels with conso-nants.
The corpus contains eight vowel types, butvery few vowel clusters.
Thus, when the networkhas seen a vowel, it can be almost certain that thefollowing phoneme will be a consonant.
Similarly,although consonant clusters are present in the cor-pus, single consonants are more frequent.
Thereforedirectly after a consonant, a vowel is the best pre-diction.Consistent with this hypothesis, analysis of thenetwork after training shows that indeed one of thehidden units has learned to respond most stronglyto vowels, and the other to consonants.
This can beseen by looking at the hidden-unit activation levelsone time step after presentation of a single phoneme.The activation of units in the network was alwayspositive, with an activation level of I correspondingto the maximum activation of a unit.
These acti-vation levels are shown in Figures 2 and 3, whichclearly demonstrate that the two hidden units havebeen used by the network to classify each of the in-put patterns as either a vowel or a consonant.Also of interest are the weights of the connectionsbetween the two hidden units and the output layer.Intuitively, we would expect that Hidden Unit 0,which responds most strongly to consonants, wouldhave the strongest connection to those output units1.00.80 0.6~ 0.40.20.0\[ ?
Consonants o Vowels \[eoe  ?
?
e m ?Y ?
C~d f oh 1 " ?b ,~ , n reoo?
Z g k s?
tVPo 5 a e \ ] i  ?
o ufi0 o o o ooPhonemesFigure 2: Activation of Hidden Unit 0, Network 1 inresponse to single phonemes at the input layer0.81.0oao7 0.640 .40.20.0e Consonants o Vowels Ju 0 ooo l i  o u f ie 0 ?0?
poeee  ?
??
ge  k ?
s~t  :~"  ?
~" e .
n r V .zb d f gh 1 m yPhonemesFigure 3: Activation of Hidden Unit 1, Network 1 inresponse to single phonemes at the input layerthat represent vowels.
This would encode the factthat when the network has just seen a consonant,it should predict a vowel.
Conversely, we would ex-pect Hidden Unit 1 to be most strongly connectedto consonants.
Indeed, this general pattern of con-nectivity is found.
However, the large variance inthe frequencies of the various phonemes makes ithard to make direct comparisons between the val-ues of the connection weights.
In other words, Hid-den Unit 1 may be more strongly connected to somehigh-frequency vowels than to some of the conso-nants such as /h / ,  which has only 23 tokens in thecorpus of 4198 phoneme tokens.A clearer pattern emerges by looking instead atthe activation levels of the output units, two t imesteps after the network was presented with partic-ular phonemes.
This two-time-step delay is simplyRodd 99 Recurrent NN Learningto account for the two time steps necessary for theinformation to propagate through the network.
Thisis equivalent to asking the network what phoneme itexpects to follow the single phoneme that has beenpresented.These activation levels were frequency adjustedby dividing the activation levels for the units rep-resenting each phoneme by the frequency of thatphoneme in the corpus.
This adjustment compen-sates for the networks' tendency to predict more fre-quent phonemes, and allows us to observe any othertrends superimposed on this frequency effect.
Infact, rather than absolute frequency, a proportionalfrequency measure is used.Figures 4 and 5 show the frequency adjusted acti-vation of the various units in the output layer af-ter the network has been presented with /1/ and/d /  (the frequency adjustment makes the units ofactivation for these graphs arbitrary).
This vowel-consonant pair was chosen because they have similarfrequencies in the corpus (176 and 177 out of a totalof 4198 respectively).
The output-layer activationlevels for the other 25 phonemes how the same pat-tern, with consonants activating units representingvowels, and vice versa.O ?
~ 321 ooooabC;de0 f~ o\[ ?
Consonants o Vowels Inr ?
oZ?
k~ ?
P ?
?
Yf ?
s ??
h i  i m t " 'VoO Oo uH Q oOPhonemesFigure 4: Frequency adjusted activation of units atthe output layer in response to "/1/" in the inputlayerThus we have seen that, given only two hid-den units, the recurrent network learns the differ-ence between the distributional properties of vowelsand consonants.
It has divided the group of inputphonemes into two natural classes, and it uses theserepresentations to predict the appropriate phonemein the output layer.
It is of interest that the conso-nant that is closest o the vowels in terms of activa-tion level is not one of the liquids, such as /y /o r / l / ,03>?9l ?
Consonants o Vowelsoeo'a  O0 0 O0I i oo  uf i/5b ?
?
?
kl  t ?
;~  PrSs .
yz ?
?
?d  f h oo ?n  oeo~ Voo  c~ msPhonemesFigure 5: Frequen.cy adjusted activation of units atthe output layer m response to " /d / "  in the inputlayerwhich are featurally most similar to vowels.
It is thestop consonant /p / .
This underlines the fact thatthe network is making the division on purely distri-butional grounds.
The fact that /p / i s  treated as themost "vowel-like" of the consonants tems from thefact that it is the consonant that occurs as the firstconsonant of a consonant cluster in the highest pro-portion of its instances.
This can be seen in Table2, which gives the total frequencies for the differ-ent consonants, as well as the number of times theyparticipate in the first and second positions withina consonant cluster; the total count includes conso-nants which participated in clusters, and consonantswhich appeared on their own between vowels.
/p /occurs only 55 times in the corpus, and in 32 of theseit is followed by another consonant ( / l / ,  / r /  or / t / ) .In this respect, its distribution is more vowel-likethen any other consonant.Table 2: Total frequencies for consonants in the cor-pus, and the number of times they appear in conso-nant cluster initial (CI) and consonant cluster final(CF) positions' l o ta l  ~1 ~P ' I  ' l b ta l  ~1 ~P"b 94  0 3 I m 167 28  72c ~J t lb : n 3tJ , 5z 105 O 5 p Ob ~ ~Z td 177 2 77 r 298 6b 27I 18 3 3 s 8~ lO lObb U 0 ?
9U 41 259 9 U t 167 26 49h 22 5 O v 23 12 3k 223 47 25 y l l2  l?
~l 311 52 146 z 46 I I  i 0Rodd 100 Recurrent NN LearningNetwork  2This network differs from Network 1 only in thatit has three hidden units.
This network convergedafter training on 47963 examples.
It was expectedthat not only would it learn the vowel-consonant dis-tinction, but it should be able to use the additionalhidden unit to encode another phonological regular-ity found in the corpus?
It was thought that theextra hidden unit might enable the network to learnbasic vowel harmony, but this is not the case.As anticipated, the network learns the vowel-consonant distinction in an identical way to Network1.
Hidden Unit 0 and Hidden Unit 2 in this simu-lation behave almost identically to the two hiddenunits in Network 1, Hidden Unit 0 responds maxi-mally to vowels, while Hidden Unit 2 responds maxi-mally to consonants.
The graphs of their activationsin response to single-letter inputs are extremely sim-ilar to Figures 2 and 3.This leaves the question of what Hidden Unit 1is being used for.
Figure 6 shows the activation ofHidden Unit 1 in response to the presentation ofsin-gle phonemes to the input layer.
This shows that itis clearly not involved in the consonant-vowel differ-ence; for vowels it is difficult to see any pattern inwhat it is learning, and it is certainly not learningvowel harmony?
I have already suggested that thereare differences between the consonants in terms oftheir participation in consonant clusters, and it isthese differences that this unit seems to be captur-ing.1.00.80"~ 0.60.40.21" Consonants o Vowels\]o ob d e i"0  ?
g 0 O0a C l Oo ?
u i i?
~ , o5  s "z?
ee  ?
?f k mn "r ?
t  .Y9 ?
P ?
vh ?l0.0 +PhonemesFigure 6: Activation of Hidden Unit 1, Network 2 inresponse to single phonemes at the input layerIn Turkish, voiced stop consonants are rarely fol-lowed by consonants.
In the corpus, /b /  and /g /are never followed by another consonant, wh i le /d /is only twice followed by a consonant.
The conso-nant /c /  also only occurred once in such a cluster,although it is worth noting that its overall frequencyin the corpus (33 out of a total of 4198 tokens in thecorpus) is lower than those of the three voiced stopconsonants (see Table 2).
Thus, when the networksees one of these consonants, is can be confident inits prediction of a vowel as the following phoneme.Indeed if we look at Figure 6, we can see that for theconsonants there is a cluster of high activation forthe voiced stop consonants , /b / , /d /and/g / ,  while/ c /has  a slightly lower activation?This suggests that Hidden Unit 1 is involved in en-coding the fact that some consonants are more likelyto be followed by consonants than others, i.e.
it islearning sonority.
"Sonority" is the characteristicthat is involved in determining what segments maylegitimately appear adjacent in clusters?
If the roleof this unit is to make predictions about consonantclusters, we would expect its activity to have the ef-fect of turning off the output units corresponding toconsonants?
This is indeed the case.
The connectionweights between Hidden Unit 1 and the output unitsare all nearly all strongly negative.
The exceptionsto this are the output units representing the conso-nants /1 / , /m/and/n / ,  which have small negative,or in the case o f /n /pos i t ive ,  connections.
The ac-tivation o f /1 /  and /m/  reflects the fact that theseconsonants are likely to occur in the final position ofa consonant cluster, wh i le /n / ' s  activation is proba-bly simply due to its high frequency?
It is the mostfrequent consonant in the corpus, with 313 tokens(out of a total of 4198 tokens).Also of interest are the weights of the recurrentconnections within the hidden layer?
Hidden Unit 1receives inhibition from Hidden Unit 2 via a strongnegative connection.
The connection from HiddenUnit 0 is small but positive?
This means that HiddenUnit 1 will be maximally active when the previousphoneme to have an influence in the hidden layerwas a vowel.
This is consistent with the idea thatthe unit is responding to the presence of a consonantthat was preceded by a vowel, i.e.
a consonant thatmay be the start of a consonant cluster.This means that the fact that this unit is alsostrongly activated for the vowels is not a problem?Vowels are almost always preceded by a consonant.Therefore, Hidden Unit 1 will be inhibited by theactivation of Hidden Unit 2.
Thus, activation ofHidden Unit 1 by a vowel in the input layer willbe insufficient to cause it to inhibit the prediction ofa consonant as the following phoneme?To summarize, Hidden Unit 1 is allowing for thefact that in some instances a consonant can followanother consonant.
In general, it acts to reduce theRodd 101 Recurrent NN Learningactivation of post-consonant consonants, but this in-hibition is less in the cases where the initial conso-nant is not a voiced stop.
There is also less inhibitionof those consonants that are more frequently foundat the ends of consonant clusters, than of any of theother consonants.Network  3This network was produced by adding a further unitto the hidden layer.
Training of this network con-verged after 46759 training examples.
Let us nowlook at the behaviour of these four hidden units inturn.The behaviour of Hidden Unit 2 is probably thesimplest o explain.
It is simply a consonant detec-tor such as those we have seen before.
Accordingly,it inhibits the activation of the units representingconsonants in the output layer, while strongly ac-tivating those units representing the more frequentvowels.
Aga in , /p / i s  treated as the most vowel-likeof the consonants.Figure 7 shows that Hidden Unit 3 has dividedthe input space into three categories.
It is moststrongly activated for the vowels /a/ ,  / l / ,  /o /and/u / ,  namely the \[-front\] vowels.
It also responds tothe \[+front\] vowels, but the level of response is lowerfor these vowels.
Lower again is the unit's responseto the consonants.
The activation of this unit hastwo main effects.
Firstly, the unit has a strong neg-ative connection to Hidden Unit 1.
We will returnto the effect of this later.1.0 ua0.8O?
~ 0.60.40.20.0e Consonants o Vowelslo oo1 UOo ooe iofibc~d ege  k l?
~n?
P ?ee ?
?
?
vY z?oO?
f ~h n rs?
t  ?PhonemesFigure 7: Activation of Hidden Unit 3, Network 3 inresponse to single phonemes at the input layerThe more immediate ffect of this unit on the out-put layer is similar to that of the vowel detectorsalready seen.
It acts to reduce the activation of theoutput units corresponding to vowels.
This preventsthe network from predicting a vowel immediately af-ter another vowel.
The unit's connections to conso-nant units in the output layer are less strongly neg-ative, or in the case o f /k / ,  / l /  and /p / ,  positive.These consonants do appear to follow back vowelsin a disproportionate number of cases.Now we come to the third hidden unit, HiddenUnit 1.
This is possibly the most interesting.
Itsresponse to input, shown in Figure 8, shows no clearpattern.
Note also that no phoneme raises its acti-vation above 0.6.
It responds more strongly to theconsonants, except fo r /h / , /p / , /v /and/z / .
Whatmakes these consonants different is that they aredisproportionately likely to begin consonant clusters(see Table 2).
Thus, this unit is active for consonantsthat are most likely to be followed by a vowel.1.00.8O?
~ 0.60.40.20.0I ?
Consonants o Vowels I0 ?
?
?
tC~do o~ ?
?
l on ee?e g ?
rs?
mab ?
Ok o\ ] i  ?
S t  o f ho  o?
pofivY?U?
?
ZPhonemesFigure 8: Activation of Hidden Unit 1, Network 3 inresponse to single phonemes at the input layerEarlier, I mentioned that Hidden Unit 3 has an ef-fect within the hidden layer.
The recurrent connec-tion within the hidden layer with the second largestweight is the connection from Hidden Unit 3 to Hid-den Unit 1, which is large and negative.
Thus Hid-den Unit 1 is turned off when the preceding inputwas a \[-front\] vowel.
Hidden Unit 1 is also turnedoff by Hidden Unit 2, which, as we saw earlier, isactivated by consonants.
So we have a unit whoseresponse is greatest for a \[+front\] vowel on the pre-ceding time step, followed by a consonant hat isunlikely to be starting a consonant cluster.
HiddenUnit 1 also has a positive self-recurrent connection,so that once it has been activated it will remain ac-tive unless inhibited.The rules of Turkish vowel harmony suggest hatthe phoneme most likely to follow a sequence of a\[+front\] vowel and a consonant, is another \[+front\]vowel.
Therefore, Hidden Unit 1 should activateRodd 102 Recurrent NN Learning\[+front\] vowels in the output layer.
The weightsfrom Hidden Unit 1 to the output units represent-ing vowels are given in Table 3.
This suggests thatrather than activating \[+front\] vowels, its actionis instead to reduce the activation of the \[-front\]vowel, in par t i cu la r , /a / , /1 /and/u / ,  which are themost frequent of the \[-front\] vowels.Table 3: Weights to output layer units representingdifferent vowels from Hidden Unit 1L::t:ti-ont -- + -- + -- + ~-I Weight -7.1 +1.2 -7.2 -0.4 +1.3 -0.6 -3.9 -0.6This asymmetry between the network's treatmentof front and back vowels has implications for itsperformance.
One measure of the network's perfor-mance is to input a single vowel and to look at itspredictions in the output layer.
Rather than lookingat the output in the time step when the network ispredicting the phoneme to follow the vowel in ques-tion, I have looked at the output in the followingstep.
This is the time step when the network is re-quired to predict the phoneme two time steps onfrom the vowel, which is more likely to be a vowel.The vowels predicted most strongly should agree infronting with the input vowel.Looking at such output units shows that, althoughthe network shows a preference for vowels of thesame \[~front\] value, there is an asymmetry in per-formance.
The difference between the output in theunits representing front and back vowels is approxi-mately twice as large when the input is a back vowel.In other words, the fact that a single unit is used toencode whether an input is \[+front\] or \[-front\] hasmeant that the network has in effect learnt frontingharmony better for back vowels than for front vow-els.
Looking at the training corpus reveals that ofthe 544 harmonic words in the corpus, 50.6% containonly back vowels, while the remaining 49.4%containonly front vowels.
Of the 57 disharmonic words inthe corpus, 53% had a front vowel as the first vowelin the word.
These small differences provide a possi-ble explanation for the fact that it is the back vowelsfor which vowel harmony is better learned, but it isinsufficient o explain the large asymmetry in thenetwork's performance.
This difference must there-fore be seen as a result of the limitations of the net-work architecture, and not a direct result of the datait was trained on.However, despite this, Hidden Units 1 and 3 to-gether have enabled the network to learn frontingharmony.This leaves us with just one hidden unit to ac-count for, Hidden Unit 0.
Its pattern of activationin response to inputs of individual etters shows noobvious categorization.
It responds highly to vow-els, as well as to most of the consonants, especially/h / ,  /m/ ,  / l / ,  / v /  and /y / .
It is difficult to see thatthis unit is contributing anything of importance tothe straightforward mapping from input to output.Thus, the key to its behaviour must lie within thehidden layer.Firstly it has a strong, negative self-recurrent link.Thus, once the unit is activated, it will, if left to it-self, continually turn itself on and off at successivetime steps.
The strongest connection within the hid-den layer is the positively weighted connection fromHidden Unit 2 to Hidden Unit 0.
Thus, this unitis on when the preceding input phoneme is a conso~nant.
It also has a positive link forward to HiddenUnit 2.
This will result in Hidden Unit 2 being ac-tivated by a consonant, and then being reactivatedtwo time steps later.
Thus, Hidden unit 2 appearsto oscillate in opposition to Hidden Unit 0.
Thebehaviour of this unit is clearly complex.
It is en-coding something about the temporal structure ofthe input, rather that making direct predictions onthe basis of the last input phoneme.
The exact de-tails of this behaviour are beyond the scope of thispaper.
However, one result of its behaviour is worthmentioning.Consider the activation of Hidden Unit 0 in re-sponse to the input of a single vowel phoneme atthe input layer.
In the first time step it respondswith activations ranging from 0.45 for /5 /  to 0.92fo r /e / .
Its activation then drops on the followingtime step in proportion to its initial activation, i.e.the negative self-recurrent link acts to reduce its ac-tivation most in those cases where it is most active.Then, on the following time step, its activation in-creases again for all the vowels.
Activation levelsrange from 0.80 fo r /5 / to  0.96 fo r /e / .
It is prob-able that Hidden Unit O's oscillatory behaviour isallowing the network to capture useful informationabout the vowel-consonant alternation over time.To summarize, this network has only four hiddenunits, and yet it shows complex behaviour.
It has en-coded much information about vowel-consonant al-ternation and vowel harmony.
Looking at the out-put layer also shows that it has some knowledgeabout consonant clusters.
For example, comparingthe consonants /d /and/n / , /n / i s  a more frequentphoneme, with a total of 313 tokens in the corpus to/d/ 's  177.
However, /d /  appears second in a con-sonant cluster 77 times, while /n /  appears in thisposition only once.
If we look at the activation ofthe output units representing these phonemes, weRodd 103 Recurrent NN Learningsee that, after consonants frequently in the clusterinitial position, such as /n /  itself o r / r / ,  /d / 's  ac-tivation is over 20 times greater than that o f /n / .Clearly, this network has also learned about whichconsonants are likely to fall in particular positionsin consonant clusters.What is also clear, however, is that as the net-works become more complicated they become in-creasingly harder to analyse.
No longer do wehave only simple detectors for phonological naturalclasses uch as consonant and vowels; i.e the networkis able to use the recurrent links to encode complextemporal properties of the input.
We also see thatthe network shows behaviours that are difficult toattribute to individual hidden units.Network  4This network has 5 hidden units, and saw 46157training examples.
The hidden units show manyof the characteristics already discussed, in terms oflearning about the properties of consonant clusters.Most of the network's behaviours are extremely com-plex, and not sufficiently different from patterns al-ready seen to make them of significant interest.
Ofmore interest is the ability of this network to capturevowel harmony, and it is to the units responsible forthis that I will limit my discussion.Hidden Unit 4 is used as a straightforward voweldetector such as we have seen before.
It is activatedmost strongly by the input units representing the 8vowels.
Its connections to the output units repre-senting vowels have high negative weights, to pre-vent the prediction of a vowel after the network hasseen a vowel.
Its self-recurrent connection also hasa large negative weight; vowel sequences were veryrare in the corpus.Hidden Units 0 and 2 are involved in the network'slearning of vowel harmony.
They both respond toconsonants as well as vowels, but for the moment letus consider just their responses to the activation ofthe input units representing vowels.
Hidden Unit 2responds trongly to the \[-front\] vowe ls /a / , /1 / , /o /and/u / ,  but shows negligible activation in responseto the \[+ front\] vowe ls /e / , / i / , /5 /and/ i i / .
HiddenUnit 0 shows the reverse pattern, except that itsresponse to the \[+front\] vowe l /5 / i s  not as large asthat to the other \[+front\] vowels.
The most likelyexplanation for this is that it is due simply to thelow frequency of this vowel in the corpus.
It is thelowest-frequency vowel, with only 44 tokens.
Thesepatterns are shown in Figures 9 and 10.Let us now look at the weights of the connec-tions from these two units to the output layer.
Toshowvowel harmony, we would expect o see the two1.00.8o?
~ 0.60.40.20.0"oaI o Vowels Io05 i 0oPhonemesFigure 9: Hidden Unit 2, Network 4 activation inresponse to single phonemes at the input layer1.00.800.6~ 0.40.2\] o Vowels \[o oe io OO o U5oiiPhonemesFigure 10: Hidden Unit 0, Network 4 activation inresponse to single phonemes at the input layerhidden units activating the output units that repre-sent vowels of the same value of \[::Lfront\] as thatto which they themselves are responding.
The onlyoutput units to which Hidden Unit 2 has a posi-tively weighted connection, are those representingthe phonemes /a /  and /1/, i.e.
the most frequent\[-front\] vowels.
The connections to the other twoless frequent \[-front\] vowels are small and negative,and are smaller than the negative weights for theconnection to the output units representing \[+front\]vowels.
Thus the unit is using the fact that it hasrecently seen one of the \[-front\] vowels to predictthe presence of another \[-front\] vowel, in particu-la r /a /and/1 / .
Hidden Unit 0 does not show sucha distinct pattern; rather it acts to inhibit HiddenUnit 2, and so prevents the prediction of a \[+front\]vowel.Rodd 104 Recurrent NN LearningIAlso of interest is the activation of the units inthe output layer when the network is presented witha single phoneme.
When this phoneme is a vowel,there is an interesting change in the prediction pat-tern with time.
Immediately after the vowel, theactivation for all vowels is low.
Thus the network,as before, knows that a consonant almost always fol-lows a vowel, and this general inhibition of the acti-vation of output units representing vowels overpow-ers any effects of the vowel harmony units.
How-ever on the following time step, the network showsa strong preference for vowels with the value for\[~front\], consistent with the previous vowel.
For ex-ample, the two most frequent vowels in the corpusare /a / ,  a \[-front\] vowel, and /e / ,  a \[+front\] vowel.If we input one of these vowels and then look at theresponse of the output units corresponding to thenext two most frequent vowels ( / i /a  \[+front\] voweland/ l /a  \[-front\] vowel), the pattern shown in Ta-ble 4 emerges.
The activation of the output unit isclearly higher where it agrees in fronting with the in-put vowel.
For the lower-frequency vowels, th e pat-tern is less strong, but still shows the vowel harmonyeffects.
This and the earlier asymmetries betweenthe learning of vowel harmony for the vowels of dif-ferent frequencies, cannot be explained in terms of ainteraction with harmony for the feature \[:?round\];such harmony was not observed to be significantlylearned by any of ~he networks in this study.
Pre-sumably additional hidden-layer resources are nec-essary for the learning of such detailed regularitiesin the corpus.Thus, not only are the units in the hidden layersuccessfully encoding the front-back distinction forvowels, but this is being translated at the appropri-ate time into the activation of output units consis-tent with vowel harmony of the feature \[=t:front\].Table 4: Frequency adjusted output unit activationof vowels /1/and / i /as  predictions two time stepsa f te r /a /and/e /Input OutputVowel \[:Lfront\] Vowel \[: front 1/a/ l - - f ront |  /~/ \[--frontJ/e/ L+front\]` /,/ \[-frontl/a/ I--frontl /i/ \[+frontl/e/ \[+front\] : /i/ \[+front\]OutputActivation6.730.070.013.29This persistence of the knowledge of the frontingof the vowels in the current word is most easily ex-plained by the fact that Hidden Unit 2 has a verystrong positive self-recurrent connection; this en-ables it to retain its high activation across the in-tervening consonants.
As previously discussed, Hid-den Unit 0 affects vowel prediction via Hidden Unit2, and so knowledge about the presence of \[+front\]vowels also persists over time.Therefore, unlike Network 4, Network 5 has de-voted two hidden units to learning the regularitiesinvolved in vowel harmony.
These two units are act-ing as detectors for the phonological natural classesof front and back vowels.4 Conc lus ionsThe four networks demonstrate he ability of simplerecurrent networks to capture the temporal struc-ture in phonological input.
With an appropriatenumber of hidden units, these hidden units can be-come detectors for phonological natural classes uchas vowels, consonants, voiced stop consonants, andfront and back vowels.
The prediction of the nextphoneme at the output layer is based on the presenceor absence of such classes of phonemes.
However,unlike standard phonological theories, the classesare graded.
For example, although the networksclearly treated consonants differently from vowels,some consonants are treated as more "vowel-like"than others.It is worth remembering that these categories arederived purely on distributional grounds.
The net-work has no knowledge of the articulatory or acous-tic features of the phonemes.
This perhaps explainswhy phonemes uch as / j /  and /w/  are tradition-ally classed as consonants, despite the fact that theyshare many acoustic or articulatory features withvowels.
On distributional grounds, their classifica-tion as consonants i undisputed.Another observation worth noting is the changein the functional roles of the hidden units as theirnumber increases.
For example in the case of conso-nant clusters, in Network 2, one of the three hid-den units is devoted to capturing the regularitiesin these clusters.
In the larger networks, however,while their performance clearly indicates they havelearned these regularities, it is less clear which unitsare implicated, and it appears that the function hasbecome distributed across the hidden units.To conclude, this study demonstrates that simplerecurrent networks can extract phonological regu-larities purely on the grounds of the distributionaldifferences between different phonemes.
The result-ing representations i  the hidden layer correspond togroups that are treated as natural classes in phono-logical theories.
While I am not suggesting thathumans perform anything like this prediction task,what is clear is that the extraction of some of thegeneralizations important for the learning of phono-logical rules can be achieved on purely distributionalRodd 105 Recurrent NN Learninggrounds.
In other words, the process of learningmore complex phonological rules may be facilitatedby the extraction of basic phonological c asses priorto the learning of these rules.The paper also demonstrates that while connec-tionist models containing many hidden units can besuccessfully used to model certain phonological pro-cesses in detail, restricting the number of hiddenunits allows us to investigate how representationsfor some of the basic phonological categories can belearned.Re ferencesGeorge N. Clements and Engin Sezer.
1982.Vowel and consonant disharmony in Turk-ish.
In H. van der Hulst and N. Smith, ed-itors, The Structure of Phonological Repre-sentations, Part II, pages 213-255, Dordrecht:Foris.Jeffrey L. Elman.
1990.
Finding structure in time.Cognitive Science, 14:179-211.Michael Gasser.
1992.
Learning distributed repre-sentations for syllables.
In Proceedings of thefourteenth Annual Conference of the CognitiveScience Society, pages 396-401.Mary Hare.
1990.
The role of similarity in Hungar-ian vowel harmony: a connectionist account.Connection Science, 2:123-149.Michael I. Jordan.
1986.
Serial order: a parallel dis-tributed processing approach.
In ICS ReportNo.
8604, UC San Diego.Geoffrey L. Lewis.
1967.
Turkish Grammar.
Oxford:Clarendon Press.David E. Rumelhart, Geoffrey E Hinton, andRonMd J. Williams.
1990.
Learning internalrepresentations by error propagation.
In D.E.Rumelhart and J.L.
McClelland, editors, Par-allel Distributed Processing, Volume I, pages318-364, MIT Press, Cambridge, MA.Rodd 106 Recurrent NN Learning
