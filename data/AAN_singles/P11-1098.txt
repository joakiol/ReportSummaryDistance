Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 976?986,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTemplate-Based Information Extraction without the TemplatesNathanael Chambers and Dan JurafskyDepartment of Computer ScienceStanford University{natec,jurafsky}@stanford.eduAbstractStandard algorithms for template-based in-formation extraction (IE) require predefinedtemplate schemas, and often labeled data,to learn to extract their slot fillers (e.g., anembassy is the Target of a Bombing tem-plate).
This paper describes an approach totemplate-based IE that removes this require-ment and performs extraction without know-ing the template structure in advance.
Our al-gorithm instead learns the template structureautomatically from raw text, inducing tem-plate schemas as sets of linked events (e.g.,bombings include detonate, set off, and de-stroy events) associated with semantic roles.We also solve the standard IE task, using theinduced syntactic patterns to extract role fillersfrom specific documents.
We evaluate on theMUC-4 terrorism dataset and show that we in-duce template structure very similar to hand-created gold structure, and we extract rolefillers with an F1 score of .40, approachingthe performance of algorithms that require fullknowledge of the templates.1 IntroductionA template defines a specific type of event (e.g.,a bombing) with a set of semantic roles (or slots)for the typical entities involved in such an event(e.g., perpetrator, target, instrument).
In contrast towork in relation discovery that focuses on learningatomic facts (Banko et al, 2007a; Carlson et al,2010), templates can extract a richer representationof a particular domain.
However, unlike relation dis-covery, most template-based IE approaches assumeforeknowledge of the domain?s templates.
Very littlework addresses how to learn the template structureitself.
Our goal in this paper is to perform the stan-dard template filling task, but to first automaticallyinduce the templates from an unlabeled corpus.There are many ways to represent events, rang-ing from role-based representations such as frames(Baker et al, 1998) to sequential events in scripts(Schank and Abelson, 1977) and narrative schemas(Chambers and Jurafsky, 2009; Kasch and Oates,2010).
Our approach learns narrative-like knowl-edge in the form of IE templates; we learn sets ofrelated events and semantic roles, as shown in thissample output from our system:Bombing Template{detonate, blow up, plant, explode, defuse, destroy}Perpetrator: Person who detonates, plants, blows upInstrument: Object that is planted, detonated, defusedTarget: Object that is destroyed, is blown upA semantic role, such as target, is a cluster of syn-tactic functions of the template?s event words (e.g.,the objects of detonate and explode).
Our goal isto characterize a domain by learning this templatestructure completely automatically.
We learn tem-plates by first clustering event words based on theirproximity in a training corpus.
We then use a novelapproach to role induction that clusters the syntacticfunctions of these events based on selectional prefer-ences and coreferring arguments.
The induced rolesare template-specific (e.g., perpetrator), not univer-sal (e.g., agent or patient) or verb-specific.After learning a domain?s template schemas, weperform the standard IE task of role filling from in-dividual documents, for example:Perpetrator: guerrillasInstrument: dynamiteTarget: embassy976This extraction stage identifies entities using thelearned syntactic functions of our roles.
We evalu-ate on the MUC-4 terrorism corpus with results ap-proaching those of supervised systems.The core of this paper focuses on how to char-acterize a domain-specific corpus by learning richtemplate structure.
We describe how to first expandthe small corpus?
size, how to cluster its events, andfinally how to induce semantic roles.
Section 5 thendescribes the extraction algorithm, followed by eval-uations against previous work in section 6 and 7.2 Previous WorkMany template extraction algorithms require fullknowledge of the templates and labeled corpora,such as in rule-based systems (Chinchor et al, 1993;Rau et al, 1992) and modern supervised classi-fiers (Freitag, 1998; Chieu et al, 2003; Bunescuand Mooney, 2004; Patwardhan and Riloff, 2009).Classifiers rely on the labeled examples?
surround-ing context for features such as nearby tokens, doc-ument position, syntax, named entities, semanticclasses, and discourse relations (Maslennikov andChua, 2007).
Ji and Grishman (2008) also supple-mented labeled with unlabeled data.Weakly supervised approaches remove some ofthe need for fully labeled data.
Most still require thetemplates and their slots.
One common approach isto begin with unlabeled, but clustered event-specificdocuments, and extract common word patterns asextractors (Riloff and Schmelzenbach, 1998; Sudoet al, 2003; Riloff et al, 2005; Patwardhan andRiloff, 2007).
Filatova et al (2006) integrate namedentities into pattern learning (PERSON won) to ap-proximate unknown semantic roles.
Bootstrappingwith seed examples of known slot fillers has beenshown to be effective (Surdeanu et al, 2006; Yan-garber et al, 2000).
In contrast, this paper removesthese data assumptions, learning instead from a cor-pus of unknown events and unclustered documents,without seed examples.Shinyama and Sekine (2006) describe an ap-proach to template learning without labeled data.They present unrestricted relation discovery as ameans of discovering relations in unlabeled docu-ments, and extract their fillers.
Central to the al-gorithm is collecting multiple documents describ-ing the same exact event (e.g.
Hurricane Ivan), andobserving repeated word patterns across documentsconnecting the same proper nouns.
Learned patternsrepresent binary relations, and they show how toconstruct tables of extracted entities for these rela-tions.
Our approach draws on this idea of using un-labeled documents to discover relations in text, andof defining semantic roles by sets of entities.
How-ever, the limitations to their approach are that (1)redundant documents about specific events are re-quired, (2) relations are binary, and (3) only slotswith named entities are learned.
We will extendtheir work by showing how to learn without theseassumptions, obviating the need for redundant doc-uments, and learning templates with any type andany number of slots.Large-scale learning of scripts and narrativeschemas also captures template-like knowledgefrom unlabeled text (Chambers and Jurafsky, 2008;Kasch and Oates, 2010).
Scripts are sets of re-lated event words and semantic roles learned bylinking syntactic functions with coreferring argu-ments.
While they learn interesting event structure,the structures are limited to frequent topics in a largecorpus.
We borrow ideas from this work as well, butour goal is to instead characterize a specific domainwith limited data.
Further, we are the first to applythis knowledge to the IE task of filling in templatementions in documents.In summary, our work extends previous work onunsupervised IE in a number of ways.
We are thefirst to learn MUC-4 templates, and we are the firstto extract entities without knowing how many tem-plates exist, without examples of slot fillers, andwithout event-clustered documents.3 The Domain and its TemplatesOur goal is to learn the general event structure ofa domain, and then extract the instances of eachlearned event.
In order to measure performancein both tasks (learning structure and extracting in-stances), we use the terrorism corpus of MUC-4(Sundheim, 1991) as our target domain.
This cor-pus was chosen because it is annotated with tem-plates that describe all of the entities involved ineach event.
An example snippet from a bombingdocument is given here:977The terrorists used explosives against thetown hall.
El Comercio reported that allegedShining Path members also attacked public fa-cilities in huarpacha, Ambo, tomayquichua,and kichki.
Municipal official Sergio Hornawas seriously wounded in an explosion inAmbo.The entities from this document fill the followingslots in a MUC-4 bombing template.Perp: Shining Path members Victim: Sergio HornaTarget: public facilities Instrument: explosivesWe focus on these four string-based slots1 fromthe MUC-4 corpus, as is standard in this task.
Thecorpus consists of 1300 documents, 733 of whichare labeled with at least one template.
There are sixtypes of templates, but only four are modestly fre-quent: bombing (208 docs), kidnap (83 docs), attack(479 docs), and arson (40 docs).
567 documents donot have any templates.
Our learning algorithm doesnot know which documents contain (or do not con-tain) which templates.
After learning event wordsthat represent templates, we induce their slots, notknowing a priori how many there are, and then fillthem in by extracting entities as in the standard task.In our example above, the three bold verbs (use, at-tack, wound) indicate the Bombing template, andtheir syntactic arguments fill its slots.4 Learning Templates from Raw TextOur goal is to learn templates that characterize adomain as described in unclustered, unlabeled doc-uments.
This presents a two-fold problem to thelearner: it does not know how many events exist, andit does not know which documents describe whichevent (some may describe multiple events).
We ap-proach this problem with a three step process: (1)cluster the domain?s event patterns to approximatethe template topics, (2) build a new corpus specific toeach cluster by retrieving documents from a largerunrelated corpus, (3) induce each template?s slotsusing its new (larger) corpus of documents.4.1 Clustering Events to Learn TemplatesWe cluster event patterns to create templates.
Anevent pattern is either (1) a verb, (2) a noun in Word-1There are two Perpetrator slots in MUC-4: Organizationand Individual.
We consider their union as a single slot.Net under the Event synset, or (3) a verb and thehead word of its syntactic object.
Examples of eachinclude (1) ?explode?, (2) ?explosion?, and (3) ?ex-plode:bomb?.
We also tag the corpus with an NERsystem and allow patterns to include named entitytypes, e.g., ?kidnap:PERSON?.
These patterns arecrucially needed later to learn a template?s slots.However, we first need an algorithm to cluster thesepatterns to learn the domain?s core events.
We con-sider two unsupervised algorithms: Latent DirichletAllocation (LDA) (Blei et al, 2003), and agglomer-ative clustering based on word distance.4.1.1 LDA for Unknown DataLDA is a probabilistic model that treats documentsas mixtures of topics.
It learns topics as discretedistributions (multinomials) over the event patterns,and thus meets our needs as it clusters patterns basedon co-occurrence in documents.
The algorithm re-quires the number of topics to be known ahead oftime, but in practice this number is set relatively highand the resulting topics are still useful.
Our best per-forming LDA model used 200 topics.
We had mixedsuccess with LDA though, and ultimately found ournext approach performed slightly better on the doc-ument classification evaluation.4.1.2 Clustering on Event DistanceAgglomerative clustering does not require fore-knowledge of the templates, but its success relies onhow event pattern similarity is determined.Ideally, we want to learn that detonate and destroybelong in the same cluster representing a bombing.Vector-based approaches are often adopted to rep-resent words as feature vectors and compute theirdistance with cosine similarity.
Unfortunately, theseapproaches typically learn clusters of synonymouswords that can miss detonate and destroy.
Ourgoal is to instead capture world knowledge of co-occuring events.
We thus adopt an assumption thatcloseness in the world is reflected by closeness in atext?s discourse.
We hypothesize that two patternsare related if they occur near each other in a docu-ment more often than chance.Let g(wi, wj) be the distance between two events(1 if in the same sentence, 2 in neighboring, etc).
LetCdist(wi, wj) be the distance-weighted frequency of978kidnap: kidnap, kidnap:PER, abduct, release, kidnap-ping, ransom, robbery, registrationbombing: explode, blow up, locate, place:bomb, det-onate, damage, explosion, cause, damage, ...attack: kill, shoot down, down, kill:civilian, kill:PER,kill:soldier, kill:member, killing, shoot:PER, wave, ...arson: burn, search, burning, clip, collaborate, ...Figure 1: The 4 clusters mapped to MUC-4 templates.two events occurring together:Cdist(wi, wj) =?d?D?wi,wj?d1?
log4(g(wi, wj)) (1)where d is a document in the set of all documentsD.
The base 4 logarithm discounts neighboring sen-tences by 0.5 and within the same sentence scores 1.Using this definition of distance, pointwise mutualinformation measures our similarity of two events:pmi(wi, wj) = Pdist(wi, wj)/(P (wi)P (wj)) (2)P (wi) =C(wi)?j C(wj)(3)Pdist(wi, wj) =Cdist(wi, wj)?k?l Cdist(wk, wl)(4)We run agglomerative clustering with pmi overall event patterns.
Merging decisions use the averagelink score between all new links across two clusters.As with all clustering algorithms, a stopping crite-rion is needed.
We continue merging clusters un-til any single cluster grows beyond m patterns.
Webriefly inspected the clustering process and chosem = 40 to prevent learned scenarios from intuitivelygrowing too large and ambiguous.
Post-evaluationanalysis shows that this value has wide flexibility.For example, the Kidnap and Arson clusters are un-changed in 30 < m < 80, and Bombing unchangedin 30 < m < 50.
Figure 1 shows 3 clusters (of 77learned) that characterize the main template types.4.2 Information Retrieval for TemplatesLearning a domain often suffers from a lack of train-ing data.
The previous section clustered events fromthe MUC-4 corpus, but its 1300 documents do notprovide enough examples of verbs and argumentcounts to further learn the semantic roles in eachcluster.
Our solution is to assemble a larger IR-corpus of documents for each cluster.
For exam-ple, MUC-4 labels 83 documents with Kidnap, butour learned cluster (kidnap, abduct, release, ...) re-trieved 3954 documents from a general corpus.We use the Associated Press and New York Timessections of the Gigaword Corpus (Graff, 2002) asour general corpus.
These sections include approxi-mately 3.5 million news articles spanning 12 years.Our retrieval algorithm retrieves documents thatscore highly with a cluster?s tokens.
The docu-ment score is defined by two common metrics: wordmatch, and word coverage.
A document?s matchscore is defined as the average number of times thewords in cluster c appear in document d:avgm(d, c) =?w?c?t?d 1{w = t}|c|(5)We define word coverage as the number of seencluster words.
Coverage penalizes documents thatscore highly by repeating a single cluster word a lot.We only score a document if its coverage, cvg(d, c),is at least 3 words (or less for tiny clusters):ir(d, c) ={avgm(d, c) if cvg(d, c) > min(3, |c|/4)0 otherwiseA document d is retrieved for a cluster c ifir(d, c) > 0.4.
Finally, we emphasize precisionby pruning away 50% of a cluster?s retrieved doc-uments that are farthest in distance from the meandocument of the retrieved set.
Distance is the co-sine similarity between bag-of-words vector repre-sentations.
The confidence value of 0.4 was chosenfrom a manual inspection among a single cluster?sretrieved documents.
Pruning 50% was arbitrarilychosen to improve precision, and we did not exper-iment with other quantities.
A search for optimumparameter values may lead to better results.4.3 Inducing Semantic Roles (Slots)Having successfully clustered event words and re-trieved an IR-corpus for each cluster, we now ad-dress the problem of inducing semantic roles.
Ourlearned roles will then extract entities in the next sec-tion and we will evaluate their per-role accuracy.Most work on unsupervised role induction fo-cuses on learning verb-specific roles, starting withseed examples (Swier and Stevenson, 2004; He and979Gildea, 2006) and/or knowing the number of roles(Grenager and Manning, 2006; Lang and Lapata,2010).
Our previous work (Chambers and Juraf-sky, 2009) learned situation-specific roles over nar-rative schemas, similar to frame roles in FrameNet(Baker et al, 1998).
Schemas link the syntactic rela-tions of verbs by clustering them based on observingcoreferring arguments in those positions.
This paperextends this intuition by introducing a new vector-based approach to coreference similarity.4.3.1 Syntactic Relations as RolesWe learn the roles of cluster C by clustering the syn-tactic relations RC of its words.
Consider the fol-lowing example:C = {go off, explode, set off, damage, destroy}RC = {go off:s, go off:p in, explode:s, set off:s}where verb:s is the verb?s subject, :o the object, andp in a preposition.
We ideally want to cluster RC as:bomb = {go off:s, explode:s, set off:o, destroy:s}suspect = {set off:s}target = {go off:p in, destroy:o}We want to cluster all subjects, objects, andprepositions.
Passive voice is normalized to active2.We adopt two views of relation similarity:coreferring arguments and selectional preferences.Chambers and Jurafsky (2008) observed that core-ferring arguments suggest a semantic relation be-tween two predicates.
In the sentence, he ran andthen he fell, the subjects of run and fall corefer, andso they likely belong to the same scenario-specificsemantic role.
We applied this idea to a new vec-tor similarity framework.
We represent a relationas a vector of all relations with which their argu-ments coreferred.
For instance, arguments of therelation go off:s were seen coreferring with men-tions in plant:o, set off:o and injure:s. We representgo off:s as a vector of these relation counts, callingthis its coref vector representation.Selectional preferences (SPs) are also useful inmeasuring similarity (Erk and Pado, 2008).
A re-lation can be represented as a vector of its observedarguments during training.
The SPs for go off:s inour data include {bomb, device, charge, explosion}.We measure similarity using cosine similarity be-tween the vectors in both approaches.
However,2We use the Stanford Parser at nlp.stanford.edu/softwarecoreference and SPs measure different types of sim-ilarity.
Coreference is a looser narrative similarity(bombings cause injuries), while SPs capture syn-onymy (plant and place have similar arguments).
Weobserved that many narrative relations are not syn-onymous, and vice versa.
We thus take the max-imum of either cosine score as our final similaritymetric between two relations.
We then back off tothe average of the two cosine scores if the max is notconfident (less than 0.7); the average penalizes thepair.
We chose the value of 0.7 from a grid search tooptimize extraction results on the training set.4.3.2 Clustering Syntactic FunctionsWe use agglomerative clustering with the abovepairwise similarity metric.
Cluster similarity is theaverage link score over all new links crossing twoclusters.
We include the following sparsity penaltyr(ca, cb) if there are too few links between clustersca and cb.score(ca, cb) =?wi?ca?wj?cbsim(wi, wj)?r(ca, cb) (6)r(ca, cb) =?wi?ca?wj?cb1{sim(wi, wj) > 0}?wi?ca?wj?cb1(7)This penalizes clusters from merging when theyshare only a few high scoring edges.
Clusteringstops when the merged cluster scores drop belowa threshold optimized to extraction performance onthe training data.We also begin with two assumptions about syntac-tic functions and semantic roles.
The first assumesthat the subject and object of a verb carry differentsemantic roles.
For instance, the subject of sell fillsa different role (Seller) than the object (Good).
Thesecond assumption is that each semantic role has ahigh-level entity type.
For instance, the subject ofsell is a Person or Organization, and the object is aPhysical Object.We implement the first assumption as a constraintin the clustering algorithm, preventing two clustersfrom merging if their union contains the same verb?ssubject and object.We implement the second assumption by auto-matically labeling each syntactic function with a roletype based on its observed arguments.
The role typesare broad general classes: Person/Org, Physical Ob-ject, or Other.
A syntactic function is labeled as a980Bombing Template (MUC-4)Perpetrator Person/Org who detonates, blows up, plants,hurls, stages, is detained, is suspected, is blamed on,launchesInstrument A physical object that is exploded, explodes, ishurled, causes, goes off, is planted, damages, is set off, isdefusedTarget A physical object that is damaged, is destroyed, isexploded at, is damaged, is thrown at, is hit, is struckPolice Person/Org who raids, questions, discovers, investi-gates, defuses, arrestsN/A A physical object that is blown up, destroysAttack/Shooting Template (MUC-4)Perpetrator Person/Org who assassinates, patrols, am-bushes, raids, shoots, is linked toVictim Person/Org who is assassinated, is toppled, is gunneddown, is executed, is evacuatedTarget Person/Org who is hit, is struck, is downed, is set fireto, is blown up, surroundedInstrument A physical object that is fired, injures, downs, isset off, is explodedKidnap Template (MUC-4)Perpetrator Person/Org who releases, abducts, kidnaps,ambushes, holds, forces, captures, is imprisoned, freesTarget Person/Org who is kidnapped, is released, is freed,escapes, disappears, travels, is harmed, is threatenedPolice Person/Org who rules out, negotiates, condemns, ispressured, finds, arrests, combsWeapons Smuggling Template (NEW)Perpetrator Person/Org who smuggles, is seized from, iscaptured, is detainedPolice Person/Org who raids, seizes, captures, confiscates,detains, investigatesInstrument A physical object that is smuggled, is seized, isconfiscated, is transportedElection Template (NEW)Voter Person/Org who chooses, is intimidated, favors, is ap-pealed to, turns outGovernment Person/Org who authorizes, is chosen, blames,authorizes, deniesCandidate Person/Org who resigns, unites, advocates, ma-nipulates, pledges, is blamedFigure 2: Five learned example templates.
All knowledge except the template/role names (e.g., ?Victim?)
is learned.class if 20% of its arguments appear under the cor-responding WordNet synset3, or if the NER systemlabels them as such.
Once labeled by type, we sep-arately cluster the syntactic functions for each roletype.
For instance, Person functions are clusteredseparate from Physical Object functions.
Figure 2shows some of the resulting roles.Finally, since agglomerative clustering makeshard decisions, related events to a template may havebeen excluded in the initial event clustering stage.To address this problem, we identify the 200 nearbyevents to each event cluster.
These are simply thetop scoring event patterns with the cluster?s originalevents.
We add their syntactic functions to their bestmatching roles.
This expands the coverage of eachlearned role.
Varying the 200 amount does not leadto wide variation in extraction performance.
Onceinduced, the roles are evaluated by their entity ex-traction performance in Section 5.4.4 Template EvaluationWe now compare our learned templates to thosehand-created by human annotators for the MUC-4terrorism corpus.
The corpus contains 6 template3Physical objects are defined as non-person physical objectsBombing Kidnap Attack ArsonPerpetrator x x x xVictim x x x xTarget x x xInstrument x xFigure 3: Slots in the hand-crafted MUC-4 templates.types, but two of them occur in only 4 and 14 of the1300 training documents.
We thus only evaluate the4 main templates (bombing, kidnapping, attack, andarson).
The gold slots are shown in figure 3.We evaluate the four learned templates that scorehighest in the document classification evaluation(to be described in section 5.1), aligned with theirMUC-4 types.
Figure 2 shows three of our four tem-plates, and two brand new ones that our algorithmlearned.
Of the four templates, we learned 12 of the13 semantic roles as created for MUC.
In addition,we learned a new role not in MUC for bombings,kidnappings, and arson: the Police or Authoritiesrole.
The annotators chose not to include this in theirlabeling, but this knowledge is clearly relevant whenunderstanding such events, so we consider it correct.There is one additional Bombing and one Arson rolethat does not align with MUC-4, marked incorrect.981We thus report 92% slot recall, and precision as 14of 16 (88%) learned slots.We only measure agreement with the MUC tem-plate schemas, but our system learns other events aswell.
We show two such examples in figure 2: theWeapons Smuggling and Election Templates.5 Information Extraction: Slot FillingWe now present how to apply our learned templatesto information extraction.
This section will describehow to extract slot fillers using our templates, butwithout knowing which templates are correct.We could simply use a standard IE approach, forexample, creating seed words for our new learnedtemplates.
But instead, we propose a new methodthat obviates the need for even a limited human la-beling of seed sets.
We consider each learned se-mantic role as a potential slot, and we extract slotfillers using the syntactic functions that were previ-ously learned.
Thus, the learned syntactic patterns(e.g., the subject of release) serve the dual purposeof both inducing the template slots, and extractingappropriate slot fillers from text.5.1 Document ClassificationA document is labeled for a template if two differentconditions are met: (1) it contains at least one trig-ger phrase, and (2) its average per-token conditionalprobability meets a strict threshold.Both conditions require a definition of the condi-tional probability of a template given a token.
Theconditional is defined as the token?s importance rel-ative to its uniqueness across all templates.
Thisis not the usual conditional probability definition asIR-corpora are different sizes.P (t|w) =PIRt(w)?s?T PIRs(w)(8)where PIRt(w) is the probability of pattern w in theIR-corpus of template t.PIRt(w) =Ct(w)?v Ct(v)(9)where Ct(w) is the number of times word w appearsin the IR-corpus of template t. A template?s triggerwords are defined as words satisfying P (t|w) > 0.2.Kidnap Bomb Attack ArsonPrecision .64 .83 .66 .30Recall .54 .63 .35 1.0F1 .58 .72 .46 .46Figure 4: Document classification results on test.Trigger phrases are thus template-specific patternsthat are highly indicative of that template.After identifying triggers, we use the above defi-nition to score a document with a template.
A doc-ument is labeled with a template if it contains atleast one trigger, and its average word probabilityis greater than a parameter optimized on the trainingset.
A document can be (and often is) labeled withmultiple templates.Finally, we label the sentences that contain trig-gers and use them for extraction in section 5.2.5.1.1 Experiment: Document ClassificationThe MUC-4 corpus links templates to documents,allowing us to evaluate our document labels.
Wetreat each link as a gold label (kidnap, bomb, orattack) for that document, and documents can havemultiple labels.
Our learned clusters naturally do nothave MUC labels, so we report results on the fourclusters that score highest with each label.Figure 4 shows the document classificationscores.
The bombing template performs best withan F1 score of .72.
Arson occurs very few times,and Attack is lower because it is essentially an ag-glomeration of diverse events (discussed later).5.2 Entity ExtractionOnce documents are labeled with templates, we nextextract entities into the template slots.
Extraction oc-curs in the trigger sentences from the previous sec-tion.
The extraction process is two-fold:1.
Extract all NPs that are arguments of patterns in thetemplate?s induced roles.2.
Extract NPs whose heads are observed frequentlywith one of the roles (e.g., ?bomb?
is seen with In-strument relations in figure 2).Take the following MUC-4 sentence as an example:The two bombs were planted with the exclusivepurpose of intimidating the owners of...982The verb plant is in our learned bombing cluster, sostep (1) will extract its passive subject bombs andmap it to the correct instrument role (see figure 2).The human target, owners, is missed because intim-idate was not learned.
However, if owner is in theselectional preferences of the learned ?human target?role, step (2) correctly extracts it into that role.These are two different, but complementary,views of semantic roles.
The first is that a role is de-fined by the set of syntactic relations that describe it.Thus, we find all role relations and save their argu-ments (pattern extraction).
The second view is thata role is defined by the arguments that fill it.
Thus,we extract all arguments that filled a role in training,regardless of their current syntactic environment.Finally, we filter extractions whose WordNet ornamed entity label does not match the learned slot?stype (e.g., a Location does not match a Person).6 Standard EvaluationWe trained on the 1300 documents in the MUC-4corpus and tested on the 200 document TST3 andTST4 test set.
We evaluate the four string-basedslots: perpetrator, physical target, human target, andinstrument.
We merge MUC?s two perpetrator slots(individuals and orgs) into one gold Perpetrator slot.As in Patwardhan and Riloff (2007; 2009), we ig-nore missed optional slots in computing recall.
Weinduced clusters in training, performed IR, and in-duced the slots.
We then extracted entities from thetest documents as described in section 5.2.The standard evaluation for this corpus is to reportthe F1 score for slot type accuracy, ignoring the tem-plate type.
For instance, a perpetrator of a bombingand a perpetrator of an attack are treated the same.This allows supervised classifiers to train on all per-petrators at once, rather than template-specific learn-ers.
Although not ideal for our learning goals, wereport it for comparison against previous work.Several supervised approaches have presented re-sults on MUC-4, but unfortunately we cannot com-pare against them.
Maslennikov and Chua (2006;2007) evaluated a random subset of test (they report.60 and .63 F1), and Xiao et al (2004) did not eval-uate all slot types (they report .57 F1).Figure 5 thus shows our results with previouswork that is comparable: the fully supervised andP R F1Patwardhan & Riloff-09 : Supervised 48 59 53Patwardhan & Riloff-07 : Weak-Sup 42 48 44Our Results (1 attack) 48 25 33Our Results (5 attack) 44 36 40Figure 5: MUC-4 extraction, ignoring template type.F1 Score Kidnap Bomb Arson AttackResults .53 .43 .42 .16 / .25Figure 6: Performance of individual templates.
Attackcompares our 1 vs 5 best templates.weakly supervised approaches of Patwardhan andRiloff (2009; 2007).
We give two numbers for oursystem: mapping one learned template to Attack,and mapping five.
Our learned templates for Attackhave a different granularity than MUC-4.
Ratherthan one broad Attack type, we learn several: Shoot-ing, Murder, Coup, General Injury, and Pipeline At-tack.
We see these subtypes as strengths of our al-gorithm, but it misses the MUC-4 granularity of At-tack.
We thus show results when we apply the bestfive learned templates to Attack, rather than just one.The final F1 with these Attack subtypes is .40.Our precision is as good as (and our F1 score near)two algorithms that require knowledge of the tem-plates and/or labeled data.
Our algorithm insteadlearned this knowledge without such supervision.7 Specific EvaluationIn order to more precisely evaluate each learnedtemplate, we also evaluated per-template perfor-mance.
Instead of merging all slots across all tem-plate types, we score the slots within each templatetype.
This is a stricter evaluation than Section 6; forexample, bombing victims assigned to attacks werepreviously deemed correct4.Figure 6 gives our results.
Three of the four tem-plates score at or above .42 F1, showing that ourlower score from the previous section is mainly dueto the Attack template.
Arson also unexpectedly4We do not address the task of template instance identifica-tion (e.g., splitting two bombings into separate instances).
Thisrequires deeper discourse analysis not addressed by this paper.983Precision Recall F1Kidnap .82 .47 .60 (+.07)Bomb .60 .36 .45 (+.02)Arson 1.0 .29 .44 (+.02)Attack .36 .09 .15 (0.0)Figure 7: Performance of each template type, but onlyevaluated on documents labeled with each type.
All oth-ers are removed from test.
The parentheses indicate F1gain over evaluating on all test documents (figure 6).scored well.
It only occurs in 40 documents overall,suggesting our algorithm works with little evidence.Per-template performace is good, and our .40overall score from the previous section illustratesthat we perform quite well in comparison to the .44-.53 range of weakly and fully supervised results.These evaluations use the standard TST3 andTST4 test sets, including the documents that are notlabeled with any templates.
74 of the 200 test doc-uments are unlabeled.
In order to determine wherethe system?s false positives originate, we also mea-sure performance only on the 126 test documentsthat have at least one template.
Figure 7 presents theresults on this subset.
Kidnap improves most signifi-cantly in F1 score (7 F1 points absolute), but the oth-ers only change slightly.
Most of the false positivesin the system thus do not originate from the unla-beled documents (the 74 unlabeled), but rather fromextracting incorrect entities from correctly identifieddocuments (the 126 labeled).8 DiscussionTemplate-based IE systems typically assume knowl-edge of the domain and its templates.
We beganby showing that domain knowledge isn?t necessar-ily required; we learned the MUC-4 template struc-ture with surprising accuracy, learning new seman-tic roles and several new template structures.
Weare the first to our knowledge to automatically in-duce MUC-4 templates.
It is possible to take theselearned slots and use a previous approach to IE (suchas seed-based bootstrapping), but we presented analgorithm that instead uses our learned syntactic pat-terns.
We achieved results with comparable preci-sion, and an F1 score of .40 that approaches prioralgorithms that rely on hand-crafted knowledge.The extraction results are encouraging, but thetemplate induction itself is a central contribution ofthis work.
Knowledge induction plays an importantrole in moving to new domains and assisting userswho may not know what a corpus contains.
Re-cent work in Open IE learns atomic relations (Bankoet al, 2007b), but little work focuses on structuredscenarios.
We learned more templates than just themain MUC-4 templates.
A user who seeks to knowwhat information is in a body of text would instantlyrecognize these as key templates, and could then ex-tract the central entities.We hope to address in the future how the al-gorithm?s unsupervised nature hurts recall.
With-out labeled or seed examples, it does not learn asmany patterns or robust classifiers as supervised ap-proaches.
We will investigate new text sources andalgorithms to try and capture more knowledge.
Thefinal experiment in figure 7 shows that perhaps newwork should first focus on pattern learning and entityextraction, rather than document identification.Finally, while our pipelined approach (templateinduction with an IR stage followed by entity ex-traction) has the advantages of flexibility in devel-opment and efficiency, it does involve a numberof parameters.
We believe the IR parameters arequite robust, and did not heavily focus on improvingthis stage, but the two clustering steps during tem-plate induction require parameters to control stop-ping conditions and word filtering.
While all learn-ing algorithms require parameters, we think it is im-portant for future work to focus on removing someof these to help the algorithm be even more robust tonew domains and genres.AcknowledgmentsThis work was supported by the National ScienceFoundation IIS-0811974, and this material is alsobased upon work supported by the Air Force Re-search Laboratory (AFRL) under prime contract no.FA8750-09-C-0181.
Any opinions, findings, andconclusion or recommendations expressed in thismaterial are those of the authors and do not necessar-ily reflect the view of the Air Force Research Labo-ratory (AFRL).
Thanks to the Stanford NLP Groupand reviewers for helpful suggestions.984ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In ChristianBoitet and Pete Whitelock, editors, ACL-98, pages 86?90, San Francisco, California.
Morgan Kaufmann Pub-lishers.Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007a.
Learningrelations from the web.
In Proceedings of the Interna-tional Joint Conferences on Artificial Intelligence (IJ-CAI).Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007b.
Open in-formation extraction from the web.
In Proceedings ofthe International Joint Conferences on Artificial Intel-ligence (IJCAI).David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent dirichlet alocation.
Journal of Machine LearningResearch.Razvan Bunescu and Raymond Mooney.
2004.
Collec-tive information extraction with relational markov net-works.
In Proceedings of the Association of Computa-tional Linguistics (ACL), pages 438?445.Andrew Carlson, J. Betteridge, R.C.
Wang, E.R.
Hr-uschka Jr., and T.M.
Mitchell.
2010.
Coupled semi-supervised learning for information extraction.
In Pro-ceedings of the ACM International Conference on WebSearch and Data Mining (WSDM).Nathanael Chambers and Dan Jurafsky.
2008.
Unsuper-vised learning of narrative event chains.
In Proceed-ings of the Association of Computational Linguistics(ACL), Hawaii, USA.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised learning of narrative schemas and their partici-pants.
In Proceedings of the Association of Computa-tional Linguistics (ACL), Columbus, Ohio.Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.2003.
Closing the gap: Learning-based informationextraction rivaling knowledge-engineering methods.In Proceedings of the Association of ComputationalLinguistics (ACL).Nancy Chinchor, David Lewis, and Lynette Hirschman.1993.
Evaluating message understanding systems: ananalysis of the third message understanding confer-ence.
Computational Linguistics, 19:3:409?449.Katrin Erk and Sebastian Pado.
2008.
A structured vec-tor space model for word meaning in context.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods on Natural Language Processing (EMNLP).Elena Filatova, Vasileios Hatzivassiloglou, and KathleenMcKeown.
2006.
Automatic creation of domain tem-plates.
In Proceedings of the Association of Computa-tional Linguistics (ACL).Dayne Freitag.
1998.
Toward general-purpose learningfor information extraction.
In Proceedings of the As-sociation of Computational Linguistics (ACL), pages404?408.David Graff.
2002.
English gigaword.
Linguistic DataConsortium.Trond Grenager and Christopher D. Manning.
2006.
Un-supervised discovery of a statistical verb lexicon.
InProceedings of the the 2006 Conference on EmpiricalMethods on Natural Language Processing (EMNLP).Shan He and Daniel Gildea.
2006.
Self-training andco-training for semantic role labeling: Primary report.Technical Report 891, University of Rochester.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through unsupervised cross-document infer-ence.
In Proceedings of the Association of Compu-tational Linguistics (ACL).Niels Kasch and Tim Oates.
2010.
Mining script-likestructures from the web.
In Proceedings of NAACLHLT, pages 34?42.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In Proceedings of the NorthAmerican Association of Computational Linguistics.Mstislav Maslennikov and Tat-Seng Chua.
2007.
Auto-matic acquisition of domain knowledge for informa-tion extraction.
In Proceedings of the Association ofComputational Linguistics (ACL).Siddharth Patwardhan and Ellen Riloff.
2007.
Effectiveie with semantic affinity patterns and relevant regions.In Proceedings of the 2007 Conference on EmpiricalMethods on Natural Language Processing (EMNLP).Siddharth Patwardhan and Ellen Riloff.
2009.
A unifiedmodel of phrasal and sentential evidence for informa-tion extraction.
In Proceedings of the 2009 Conferenceon Empirical Methods on Natural Language Process-ing (EMNLP).Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, andLois Childs.
1992.
Ge nltoolset: Muc-4 test resultsand analysis.
In Proceedings of the Message Under-standing Conference (MUC-4), pages 94?99.Ellen Riloff and Mark Schmelzenbach.
1998.
An em-pirical approach to conceptual case frame acquisition.In Proceedings of the Sixth Workshop on Very LargeCorpora.Ellen Riloff, Janyce Wiebe, and William Phillips.
2005.Exploiting subjectivity classification to improve infor-mation extraction.
In Proceedings of AAAI-05.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,plans, goals and understanding.
Lawrence Erlbaum.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemptiveie using unrestricted relation discovery.
In Proceed-ings of NAACL.985Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic ie pattern acquisition.
In Pro-ceedings of the Association of Computational Linguis-tics (ACL), pages 224?231.Beth M. Sundheim.
1991.
Third message understand-ing evaluation and conference (muc-3): Phase 1 statusreport.
In Proceedings of the Message UnderstandingConference.Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006.A hybrid approach for the acquisition of informationextraction patterns.
In Proceedings of the EACL Work-shop on Adaptive Text Extraction and Mining.Robert S. Swier and Suzanne Stevenson.
2004.
Unsu-pervised semantic role labelling.
In Proceedings ofthe 2004 Conference on Empirical Methods on Nat-ural Language Processing (EMNLP).Jing Xiao, Tat-Seng Chua, and Hang Cui.
2004.
Cas-cading use of soft and hard matching pattern rulesfor weakly supervised information extraction.
InProceedings of the 20th International Conference onComputational Linguistics (COLING).Roman Yangarber, Ralph Grishman, Pasi Tapanainen,and Silja Huttunen.
2000.
Automatic acquisitionof domain knowledge for information extraction.
InCOLING, pages 940?946.986
