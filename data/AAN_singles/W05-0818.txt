Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 111?114,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005LIHLA: Shared task system descriptionHelena M. Caseli, Maria G. V. NunesNILC ?
ICMC ?
Univ.
Sa?o PauloCP 668P, 13560-970 Sa?o Carlos?SP, Brazil{helename,gracan}@icmc.usp.brMikel L. ForcadaTransducens ?
DLSI ?
Univ.
d?AlacantE-03071 Alacant, Spainmlf@dlsi.ua.esAbstractIn this paper we describe LIHLA, a lexicalaligner which uses bilingual probabilis-tic lexicons generated by a freely availa-ble set of tools (NATools) and language-independent heuristics to find links be-tween single words and multiword unitsin sentence-aligned parallel texts.
Themethod has achieved an alignment errorrate of 22.72% and 44.49% on English?Inuktitut and Romanian?English parallelsentences, respectively.1 IntroductionAlignment of words and multiword units plays animportant role in many natural language processing(NLP) applications, such as example-based machinetranslation (EBMT) (Somers, 1999) and statisticalmachine translation (SMT) (Ayan et al, 2004; Ochand Ney, 2000), transfer rule learning (Carl, 2001;Menezes and Richardson, 2001), bilingual lexi-cography (Go?mez Guinovart and Sacau Fontenla,2004), and word sense disambiguation (Gale et al,1992), among others.Aligning two (or more) texts means findingcorrespondences (translation equivalences) betweensegments (paragraphs, sentences, words, etc.)
of thesource text and segments of its translation (the tar-get text).
Following the same idea of many recentlyproposed approaches on lexical alignment (e.g., Wuand Wang (2004) and Ayan et al (2004)), themethod described in this paper, LIHLA (Language-Independent Heuristics Lexical Aligner) starts fromstatistical alignments between single words (de-fined in bilingual lexicons) and applies language-independent heuristics to them, aiming at finding thebest alignments between words or multiword units.Although the most frequent alignment category is1 : 1 (in which one source word is translated exactlyas one target word), other categories such as omis-sions (1 : 0 or 0 : 1) or those involving multiwordunits (n : m, with n and/or m ?
1) are also possible.This paper is organized as follows: section 2 ex-plains how LIHLA works; section 3 describes someexperiments carried out with LIHLA together withtheir results and, in section 4, some concluding re-marks are presented.2 How LIHLA worksAs the first step, LIHLA uses alignments betweensingle words defined in two bilingual lexicons(source?target and target?source) generated fromsentence-aligned parallel texts using NATools.1Given two sentence-aligned corpus files, the NA-Tools word aligner ?based on the Twenty-One sys-tem (Hiemstra, 1998)?
counts the co-occurrencesof words in all aligned sentence pairs and builds asparse matrix of word-to-word probabilities (ModelA) using an iterative expectation-maximization al-gorithm (5 iterations by default).
Finally, the ele-ments with higher values in the matrix are cho-sen to compose two probabilistic bilingual lexi-cons (source?target and target?source) (Simo?es andAlmeida, 2003).
For each word in the corpus, each1NATools is a set of tools developed to work with parallelcorpora, which is freely available in http://natura.di.uminho.pt/natura/natura/.111bilingual lexicon gives: the number of occurrencesof that word in the corpus (its absolute frequency)and its most likely translations together with theirprobabilities.The construction of the bilingual lexicons is anindependent prior step for the alignment performedby LIHLA and the same bilingual lexicons can beused several times to align parallel sentences.So, using the two bilingual lexicons generatedby NATools and some language-independent heuris-tics, LIHLA tries to find the best alignment betweensource and target tokens (words, numbers, specialcharacters, etc.)
in a pair of parallel sentences.
Foreach source token sj in source sentence S, LIHLAwill look for the best token ti in the target parallelsentence T applying these heuristics in sequence:1.
Exact matchLIHLA creates a 1 : 1 alignment between sjand ti if they are identical.
This heuristic staysfor exact matches, for instance, between propernames and numbers.2.
Best candidate according to the bilinguallexiconLIHLA looks for possible translations of sj inthe source?target bilingual lexicon (BS) andmakes an intersection between them and thewords in T .
In this intersection, if no candi-date word identical to those in BS is found,then LIHLA tries to look for cognates forthose words using the longest common subse-quence ratio (LCSR).2 By doing this, LIHLAcan deal with small changes in possible trans-lations such as different forms of the same verb,changes in gender and/or number of nouns,adjectives, and so on.Then, LIHLA selects the best target candidateword ti for sj ?the best candidate word accor-ding to BS among those in a position whichis favorably situated in relation to sj?
andlooks for multiword units involving sj and ti?those words that occur immediately beforeand/or after sj (for source multiword units) or2The LCSR of two words is computed by dividing the lengthof their longest common subsequence by the length of thelonger word.
For example, the LCSR of Portuguese word alin-hamento and Spanish word alineamiento is 1012 ' 0.83 as theirlongest common subsequence is a-l-i-n-a-m-e-n-t-o.ti (for target multiword units) and are not pos-sible translations for other words in T and S,respectively.
According to the multiword unitsthat have (or not) been found, a 1 : 1, 1 : n,m : 1 or m : n alignment is established.
Anomission alignment for sj (1 : 0) can also beestablished if no target candidate word ti thatsatisfies this heuristic is available.3.
CognatesIf no possible translation for sj is found in thebilingual lexicon and the target sentence (T ) atthe same time, LIHLA uses the LCSR to lookfor cognates for sj in T and sets a 1 : 1 align-ment between sj and its best cognate or a 1 : 0alignment if there is no cognate available.These heuristics are applied while alignments canstill be produced and a maximum number of itera-tions is not reached (see section 3 for the numberof iterations performed in the experiments describedin this paper).
Furthermore, at the first iteration,all words with a frequency higher than a set thres-hold are ignored to avoid erroneous alignments sinceall subsequent alignments are based on the previousones.In its last step (which is optional and has notbeen performed in the experiments described inthis paper), LIHLA aligns the remaining unalignedsource and target tokens between two pairs of al-ready aligned tokens establishing several 1 : 1 align-ments when there are the same number of sourceand target tokens, or just one alignment involvingall source and target tokens if they exist in differentquantities.
The decision of creating n 1 : 1 align-ments in spite of just one n : n alignment when thereis the same number of source and target tokens is dueto the fact that a 1 : 1 alignment is more likely to befound than a n : n one.3 ExperimentsIn this section we present the experiments carriedout with LIHLA for the ?Shared task on word align-ment?
in the Workshop on Building and Using Pa-rallel Texts during ACL2005.
Systems participa-ting in this shared task were provided with trainingdata (consisting of sentence-aligned parallel texts)for three pairs of languages: English?Inuktitut,112Romanian?English and English?Hindi.
Further-more, the systems would choose to participate in oneor both subtasks of ?limited resources?
(where sys-tems were allowed to use only the resources pro-vided) and ?unlimited resources?
(where systemswere allowed to use any resources in addition tothose provided).
The system described in this pa-per, LIHLA, participated in the subtask of limited re-sources aligning English?Inuktitut and Romanian?English test sets.The training sets ?composed of 338,343English?Inuktitut aligned sentences (omission caseswere excluded from the whole set of 340,526 pairs)and 48,478 Romanian?English aligned ones?
wereused to build the bilingual lexicons.
Then,without changing any default parameter (thresholdfor LCSR, maximum number of iterations, etc.
),LIHLA aligned the 75 English?Inuktitut and the 203Romanian?English parallel sentences on test sets.The whole alignment process (bilingual lexicon ge-neration and alignment itself) did not take more than17 minutes for English?Inuktitut (3 iterations persentence, on average) and 7 minutes for Romanian?English (4 iterations per sentence, on average).The evaluation was run with respect to precision,recall, F -measure, and alignment error rate (AER)considering sure and probable alignments but notNULL ones (Mihalcea and Pedersen, 2003).
Tables1 and 2 present metric values for English?Inuktitutand Romanian?English alignments, respectively, asprovided by the organization of the shared task.Metric Sure ProbablePrecision 46.55% 79.53%Recall 73.72% 18.71%F -measure 57.07% 30.30%AER 22.72%Table 1: LIHLA results for English?InuktitutMetric Sure ProbablePrecision 57.68% 57.68%Recall 53.51% 53.51%F -measure 55.51% 55.51%AER 44.49%Table 2: LIHLA results for Romanian?EnglishThe results obtained in these experiments werenot so good as those achieved by LIHLA on thelanguage pairs for which it was developed, thatis, 92.48% of precision and 88.32% of recall onPortuguese?Spanish parallel texts and 84.35% ofprecision and 76.39% of recall on Portuguese?English ones.3The poor performance in the English?Inuktikuttask may be partly due to the fact that Inuktikut isa polysynthetic language, that is, one in which, un-like in English, words are formed by long strings ofconcatenated morphemes.
This makes it difficult forNATools to build reasonable dictionaries and leadto a predominance of n : 1 alignments, which areharder to determine ?this fact can be confirmed bythe better precision of LIHLA when probable align-ments were considered (see table 1).
The perfor-mance in the English?Romanian task, not very farfrom the English?Portuguese task used to tune upthe parameters of the algorithm, is harder to explainwithout further analysis.The difference in precision and recall betweenthe two language pairs is due to the fact that onthe English?Inuktitut reference corpus in addition tosure alignments the probable ones were also anno-tated while in Romanian?English only sure align-ments are found.
This indicates that evaluatingalignment systems is not a simple task since theirperformance depends not only on the language pairsand the quality of parallel corpora (constant criteriain this shared task) but also the way the referencecorpus is built.So, at this moment, it would be unfair to blamethe worse performance of LIHLA on its alignmentmethodology since it has been applied to the newlanguage pairs without changing any of its defaultparameters.
Maybe a simple optimization of para-meters for each pair of languages could bring betterresults and also the impact of size and quality oftraining and reference corpora used in these experi-ments should be investigated.
Then, the only conclu-sion that can be taken at this moment is that LIHLA,with its heuristics and/or default parameters, can notbe indistinctly applied to any pair of languages.Despite of its performance, LIHLA has some3For more details of these experiments see (Caseli et al, ac-cepted paper).113advantages when compared to other lexical align-ment methods found in the literature, such as: itdoes not need to be trained for a new pair of lan-guages (as in Och and Ney (2000)) and neither doesit require pre-processing steps to handle texts (asin Go?mez Guinovart and Sacau Fontenla (2004)).Furthermore, the whole alignment process (bilinguallexical generation and alignment itself) has provedto be very fast as mentioned previously.4 Concluding remarksThis paper has presented a lexical alignmentmethod, LIHLA, which aligns words and multi-word units based on initial statistical word-to-wordcorrespondences and language-independent heuris-tics.In the experiments carried out at the ?Sharedtask on word alignment?
which took place at theWorkshop on Building and Using Parallel Textsduring ACL2005, LIHLA has been evaluated onEnglish?Inuktitut and Romanian?English paralleltexts achieving an AER of 22.72% and 44.49%,respectively.As future work, we aim at investigating the impactof using additional linguistic information (such aspart-of-speech tags) on LIHLA?s performance.
Also,as a long-term goal, LIHLA will be part of a systemimplemented to learn transfer rules from sequencesof aligned words.AcknowledgmentsWe thank FAPESP, CAPES, CNPq and theSpanish Ministry of Science & Technology (ProjectTIC2003-08681-C02-01) for financial support.ReferencesNecip F. Ayan, Bonnie J. Dorr, and Nizar Habash.
2004.Multi-Align: Combining linguistic and statistical tech-niques to improve alignments for adaptable MT.
InR.
E. Frederking and K. B. Taylor, editors, Proceed-ings of the 6th Conference of the AMTA (AMTA-2004),number 3265 in Lecture Notes in Artificial Inteligence(LNAI), pages 17?26.
Springer-Verlag Berlin Heidel-berg.Michael Carl.
2001.
Inducing probabilistic invertibletranslation grammars from aligned texts.
In Pro-ceedings of CoNLL-2001, pages 145?151, Toulouse,France.Helena M. Caseli, Maria das Grac?as V. Nunes, andMikel L. Forcada.
(accepted paper).
LIHLA: Alexical aligner based on language-independent heuris-tics.
In Proceedings of the V Encontro Nacional deIntelige?ncia Artificial (ENIA05), Sa?o Leopoldo, RS,Brazil.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
Using bilingual materials to developword sense disambiguation methods.
In Proceedingsof the 4th International Conference on Theoretical andMethodological Issues in Machine Translation (TMI1992), pages 101?112, Montreal, Canada, June.Xavier Go?mez Guinovart and Elena Sacau Fontenla.2004.
Me?todos de optimizacio?n de la extraccio?n dele?xico bilingu?e a partir de corpus paralelos.
Proce-samiento del Lenguaje Natural, 33:133?140.Djoerd Hiemstra.
1998.
Multilingual domain modelingin Twenty-One: automatic creation of a bi-directionaltranslation lexicon from a parallel corpus.
In Pe-ter Arno Coppen, Hans van Halteren, and Lisanne Te-unissen, editors, Proceedings of the 8th CLIN meeting,pages 41?58.Arul Menezes and Stephen D. Richardson.
2001.
A best-first alignment algorithm for automatic extraction oftransfer mappings from bilingual corpora.
In Proceed-ings of the Workshop on Data-driven Machine Trans-lation at 39th Annual Meeting of the ACL (ACL-2001),pages 39?46, Toulouse, France.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In HLT-NAACL 2003Workshop: Building and Using Parallel Texts DataDriven Machine Translation and Beyond, pages 1?10,Edmonton, May?June.Franz J. Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the ACL (ACL-2000), pages 440?447, Hong Kong, China, October.Alberto M. Simo?es and Jose?
J. Almeida.
2003.
NA-Tools ?
A statistical word aligner workbench.
Pro-cessamiento del Lenguaje Natural, 31:217?224.Harold Somers.
1999. Review article: Example-basedmachine translation.
Machine Translation, 14(2):113?157.Hua Wu and Haifeng Wang.
2004.
Improving domain-specific word alignment with a general bilingual cor-pus.
In R. E. Frederking and K. B. Taylor, editors, Pro-ceedings of the 6th Conference of the AMTA (AMTA-2004), number 3265 in Lecture Notes in ArtificialInteligence (LNAI), pages 262?271.
Springer-VerlagBerlin Heidelberg.114
