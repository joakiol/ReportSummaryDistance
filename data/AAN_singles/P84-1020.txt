LIMITED DOMAIN SYSTEMS FOR LANGUAGE TEACHINGS G Pulman,Linguistics, EASUniversity of East Anglia,Norwich NR4 7Tj, UK.This abstract describes a natural language systemwhich deals usefully with ungrammatical input anddescribes some actual and potential applicationsof it in computer aided second language learning.However, this is not the only area in which theprinciples of the system might be used, and theaim in building it was simply to demonstrate theworkability of the general mechanism, and providea framework for assessing developments of it.BACKGROUNDThe really hard problem in natural languageprocessing, for any purpose, is the role ofnon-linguistic knowledge in the understandingprocess.
The correct treatment of even thesimplest type of non-syntactic phenomena seems todemand a formidable amount of encyclopedicknowledge, and complex inferences therefrom.
Todate, the only systems which have simulatedunderstanding to any convincing degree have doneso by sidestepping this problem and restrictingthe factual domain in which they operate veryseverely.
In such limited domains semantic orpragmatic processing to the necessary depth canbe achieved by brute force, as a last resort.However, such systems are typically difficult totransport from one domain to another.In many contexts this state of affairs isunsatisfactory - something more than fragile,toy, domain dependent systems is required.
Butthere are also situations in which the use oflanguage within a limited factual domain mightwell be all that was required.
Second languagelearning, especially during the early stages, isone, where quite a lot of the time what isimportant is practice and training in correctusage of basic grammatical forms, not theconveying of facts about the world.
If someonecan be taught to use the comparative constructionwhen talking about, say, lions and tigers, he isnot likely to encounter much difficulty of alinguistic nature when switching to talk aboutcars and buses, overdrafts and bank loans, etc.,even thought the system he was using might.Several existing limited domain systems mightlend themselves to exploitation for thesepurposes: one example might be the programdescribed by Isard (1974) which plays a game ofnoughts and crosses with the user and thenengages in a dialogue about the game.
Althoughthe domain is tiny the program can deal with muchof the modal and tense system of English, as wellas some conditionals.
Also dealing with noughtsand crosses is the program described by Davey(1978), which is capable of (and thereforecapable of detecting ) correct uses ofconjunctions like 'but' and 'although'.
Otherexamples of systems geared to a particular domainand often to a particular syntactic constructionwill spring readily to mind.
Embedded ineducationally motivated settings, such systemsmight well form the basis for programs givinginstruction and practice in some of thesetraditionally tricky parts of English grammar.Such, at any rate, is the philosophy behind thepresent work.
The idea is that there is scope forusing limited systems in an area where theirlimitations do not matter.ERROR DETECTION AND REPORTINGOf course, such an application carries its ownspecial requirements.
By definition, a languagelearner interacting with such a system is likelyto be giving it input which is ill-formed in someway quite often.
It is not a feature of most NLsystems that they respond usefully in thissituation: in a language tuition context, anefficient method for detecting and diagnosingerrors is essential.The problem has of course not gone unnoticed.Hayes and Mouradian (1981), Kwasny and Sondheimer(1981) - among others - have presented techniquesfor allowing a parser to succeed even withill-formed or partial input.
The ATN basedframework of the latter also generatesdescriptions of the linguistic requirements whichhave had to be violated in order for the parse tosucceed.
Such descriptions might well form thebasis for a useful interaction between system andlearner.
However, the work most directly relatedto that reported here, and an influence on it, isthat by Weischedel et al(1978) and Weischedeiand Black (1980), (see also Hendr~x (1977).
Theyalso describe ATN based systems, this time84specifically intended for use in languagetutoring programs.
The earlier paper describestwo techniques \[or handling errors: encodinglikely errors directly into the network, so thatthe ungrammatical sentences are treated likegrammatical ones, except that error messages areprinted; and using 'failable' predicates on arcsfor such things as errors of agreement.
Thedisadvantages of such a system are obvious: thegrammar writer has to predict in advance likelymistakes and a/low for them in designing the ATN.Unpredicted errors cannot be handled.The later paper describes a generalisation ofthese techniques, with two new features:condition-action pairs on selected states of theATN for generating reports (1980:100) and the useof a 'longest path' heuristic (lOI) for decidingbetween alternative failed parsings.
Althoughimpressive in its coverage, We~schedel and Blackreport two major problems with the system: thedifficulty of locating precisely where in asentence the parser failed, and the difficulty ofgenerating appropriate responses for the user.Those derived from relaxed predicates for themeanings of states were often fairly technical:some helpful examples of usage were given in somecases, but these had to be prestored and indexedby particular lexical items (103).The problem of accurately locatingungrammaticality is one that is extremelydifficult, but arguably made more difficult thanit need be by adopting the ATN framework forgrammatical description.
The ATN formalism issimply too rich: a successful parse in generaldepends not only on having traversed the networkand consumed all the inpul but on having variousregisters appropriately filled.
Since theregisters may be inspected at different pointsthis makes it difficult to provide an algorithmicmethod of locating ungrammaticality.The problem of generating error reports andhelpful responses for the learner is also mademore difficult than it need be if this isconceived of as something extra which needs to beadded to a system already capable of dealing withwe/ l - fo rmed input .
Th is  i s  because there  i s  aperfectly straightforward sense in which thisproblem has already been solved if the systemcontains an adequate grammar.
Such a grammar, byexplicitly c~aracterising well-formedness,automatically provides an implicitcharacterisation of how far actual inputs deviatefrom expected inputs.
It also contains all thegrammatical information necessary for providingthe user with examples of correct usage.
Thesetwo types of information ought to be sufficientto generate appropriate reports.THE SYSTEMThe syntactic theory underlying the presentsystem is that of Generalised Phrase StructureGrammar, of the vintage described in Gazdar(1982).
This is a more constrained grammaticalformalism than that of an ATN, and hence it waspossible to develop a relatively simple procedurefor almost always accurately locatingungrammaticality, and also for automaticallygenerating error reports of varying degrees ofcomplexity, as well as examples of correct usage.All this is done using no information over andabove what is already encoded in the grammar:nothing need be anticipated or pre-stored.Briefly, on the GPSG theory, the syntacticdescription of a language consists of two parts:a basic context-free grammar generating simplecanonical structures, and a set of metarules,which generate rules for more complex structuresfrom the basic rules.
The result of applying themetarules to the basic rules is a large CFG.The system contains a suite of pre-compilationprograms which manipulate a GPSG into the formused by the parser.
First, the metarules areapplied, producing a large, simple, CFG.
Themetarule expansion routine is in fact only fullydefined for a subset of the metarules permittedby the theory.
Roughly speaking, only metaruleswhich do not contain variables which could beinstantiated more than one way on any given ruleapplication will be accepted.
This is not atheoretically motivated restriction but simply ashort cut to enable a straighforward patternmatching production system already available inPop-ll to be transferred wholesale.
A set offilters can be specified for the output by thesame means if required.Next, the resulting CFG is compiled into anequivalent RTN, and finally this RTN is optimisedand reduced, using a variant of a standardalgorithm for ordinary transition networks (Ahoand Uilman (1977:101).
The intention behind thisextensive preprocessing, apart from increasedefficiency, is that the eventual system could betailored by teachers for their own purposes.
Allthat would be needed is the ability to write GPSgrammars, or simple CF grammars, with noknowledge needed of the internal workings of thesystem.To give an example of the effect of thispre-processing, the grammar used by the system inthe interchanges below contained about 8 rulesand 4 metarules.
These expand to a simple CFG ofabout 60 rules; this compiles to an RTN of over200 states, and the final optimised RTN containsabout 40 states.The parser is a standard RTN parser operatingbreadth first.
The error detection routine ispart of the main loop of the parser and works asfollows: when no transition can be taken from aparticular state in the network, a record istaken of the overall state of the machine.
Thiscontains information about how much of thesentence has been successfully parsed, the treebuilt, a list of states to POP to etc.
If thisrecord represents a more successful parse thanany record so far it is preserved.
This meansthat at the end of an unsuccessful parse thesystem has a record of the most successful pathpursued, and this record is passed to the errorreporting routine.If desired, all such records could bepreserved during a parse and some procedure forchoosing between then defined.
This would meanthat ambiguous parses can be treatedindependently, whereas at present only one recordrepresenting the most successful path through theinput on any reading is retained.85The error reporting routine is based around anRTN generator, which simply picks up from thepoint in the network indicated by the recordhanded to it, using the information in thatrecord, as well as the RTN, and a specialsub-lexicon described below.
It is capable ofgenerating error reports of several differenttypes:(i) it can say what constituent(s) it was tryingto complete(ii) it can say what type of item it wasexpecting to find at the point of failure -either using the terminology of the grammar, orby example(iii) it can say what would be necessary tocontinue the sentence correctly, by generatingexample continuations.Here are some transcriptions of typicalexchanges with the system using the small grammarmentioned above::go();** ready?
william put the book on the shelfok?
did william put the book on the shelfyes?
was the book put on the shelfyes?
who put the book on the shelfwilliam?
what did william put on the shelfthe book?
what was put on the shelfthe book?
who was the book put on the shelf bywilliam?
what did william putsentence ok up to here:what did william put ...expecting toofindone of the followingpreposition (in, on, etc)examples of grammatical continuationswhat did william put ...with something?
what did the readsentence ok up to here:what did the ...expecting to findone of the followingadjective (nice, big etc.
)/ noun (boy, girl etc.
)examples ot grammatical continuationswhat did the ...whatdoyoucallit hit?
william hit jumble with a stick bigsentence ok up to here:william hit jumble with a stick ...expecting to findend of sentence(NB this response is not as helpful as it couldbe, since the system does not look at the inputafter the point of failure).v who did was hitsentence ok up to here:who did ...expecting to findone of the followingnoun phraseexamples of grammatical continuationswho did ...something's thing hit?
who william did hitsentence ok up to here:who ...expecting to findone of the followingverbl (did, was, etc.
)/ verb2 (hit, read, etc.
)examples of grammatical continuationswho ...read somethingput something with somethingAn attraction of this mechanism, apart fromits simplicity, is that it is defined for thewhole class of CFGs; this class of grammars iscurrently believed to be more or less adequatefor English and for most of most other languages(Gazdar 1982).
The two problems faced by thesystem of Weischedel and Black seem to have beenovercome in a reasonably satisfying way: sinceafter optimisation, the only non-determinism inthe RTN is due to genuine ambiguity, we can besure that the system will, given the way itoperates, almost always locate accurately thepoint of failure in all non-ambiguous cases.
Andof course, when working with such limited domainswe can control for ambiguity to a large extent,and deal with it by brute force if necessary.However, no such procedure can be whollylearner-proof, (as one of our referees haspointed out).
A user might, for example, misspellhis intended word and accidentally produceanother legitimate word which could fitsyntactically.
Under these circumstances theparser would proceed unknowingly past the realpoint of error.The error reports delivered by the system canbe as technical or informal as the grammar writerwants, or simply be prompts and examples ofcorrect usage.
In practice, simple one wordprompts seem to be as useful as any moreelaborated response.
As will be clear from theexamples, both for prompts and continuations, thesystem uses a restricted sub-lexicon to m~nimisethe likelihood of generating grammaticalnonsense.
This sub-lexicon contains vague andgeneral purpose words like 'thing' and 'whatsit'.This apart, no extra work has to be done once thegrammar has been written: the system uses onlyits knowledge of what is grammatical to diagnoseand report on what is ungrammatical.DEVELOPMENTSThe mechanism is currently embedded within twosmall domains.
The one illustrated here is 'told'a simple 'story' and then asks or answersquestions about that.
The sample grammar wasintended to demonstrate the interaction of whquestions with passives, among other things.Although we are not here concerned with thesemantics of these domains, they are fairlysimple, and several different types of semanticcomponents are used depending on the nature ofthe domain.
For some domains a proceduralsemantics is appropriate, manipulating objects on86a screen or asking and answering questions aboutthem.
In the 'William' program here a productionsystem again based on the Pop-ll matchingprocedures is used, currently being coupled to asimple backwards chaining inference mechanism.Neither the grammatical routines nor anyembodiment of them constitute a complete tuitionsystem, or anything approaching that: they aremerely frameworks for experimentation.
But thesyntactic error detection routines could be usedin many other environments where useful feedbackof this type was required, say in databaseinterrogation or machine translation.
Within alanguage tuition context the mechanism could beused to advantage without an associatedsemantics, in some of the more traditional typesof computer aided EFL teaching programs: forexample, gap-filling, drill and practice,sentence completion, or grammatical paraphrasetasks.
Only trivial adjustments would be neededto the overall mechanism for this to become apowerful and sophisticated framework within whichto elaborate such programs.However, there are several ways in which thegeneral mechanism might be improved upon, mostimmediately, the following:(i) if a parse fails early in the sentence, theuser only gets a report based on that part of thesentence, when there may be more serious errorslater one (or some praiseworthy use of thelanguage).
In these cases a secondary parselooking for well-formed sub-constituents, insomething like the way a chart parser might do,would provide useful information.
(I am gratefulto Steve Isard and Henry Thompson for thissuggestion).
(ii) the quality of the example continuationscould be improved.
Eventually it would bedesirable to have the generator semanticallyguided, but this is by no means trivial, even ina limited domain.
There are several heuristicswhich can produce a better type of continuation,however: using a temporary lexicon containingwords from the unparsed portion of the sentence,or from the most recently parsed sentences, orcombinations of these with the restrictedsub-lexicon.
In the best cases this type ofheuristic can be spectacularly successful,producing a grammatical version of what the userwas trying to say.
However, they can also flopbadly: more testing on real students would be oneway of disceovering which of these alternativesis best.
(iii) as suggested in Weischedel and Black, itmight be profitable to explore the use ofsemantic grammars - grammars using semanticallyrather than syntactically motivated categories -in the system.
Although of dubious theoreticalstatus, they are a useful engineering tool: thenon-terminals can be labelled in adomain-specific way that is transparent for theuser, and, being semantically motivated, thesystem could appear as if it were doing semanticdiagnosis of a limited type as well as syntacticdiagnosis.
For example, instead of being promptedfor an adjective, the user might be prompted for'a word describing the appearance of a car', orsomething equally specific.
Furthermore, theavailability of the pre-compilation programsmeans that it should be possible to use themetarule formalism for these grammars also: thisshould go some way towards minimising theirlinguistic disadvantages, namely, a tendency torepetition and redundancy in expressing factsabout the languages they generate.The system is written in Pop-ll (a Lisp-likelanguage) within the POPLOG programmingenvironment developed by the University ofSussex.
At UEA POPLOG runs on a VAX 11/780 underVMS.REFERENCESAho, A. and Ullman, J.
(1977) Principles ofCompiler Design,London, Addison Wesley Publishing Co.Davey, A.
(1978) Discourse ProductionEdinburgh University PressGazdar, G. (1982) Phrase Structure Grammarin P. Jacobson and G.K. Pullum (eds) The Natureof Syntactic Representation, Dordrecht: D.ReidelPublishing.Hayes, P.J., and Mouradian, G.V.
(1981) FlexibleParsingAJCL 7, 232-242Hendrix, G. (1977) Human Engineering for AppliedNL ProcessingIJCAI 5, Cambridge MA.Isard, S.D.
(1974) What would you have doneif...?Theoretical Linguistics 1, No 3.Kwasny, S. and Sondheimer, N. (1981) RelaxationTechniques for Parsing Ill-formed InputAJCL 7, 99-108Weischedel, R. et al (1978) An ArtificialIntelligence Approach to Language InstructionArtificial Intelligence 10,3Weischedel R. and Black, J.
(1980) RespondingIntelligently to Unparsable InputsAJCL 6, 97-10987
