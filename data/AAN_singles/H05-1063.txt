Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 499?506, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMining Context Specific Similarity Relationships Using The World WideWebDmitri Roussinov Leon J. Zhao Weiguo FanDepartment of Information SystemsW.P.
Carey School of BusinessDepartment of ManagementInformation SystemsDepartment of InformationSystemsArizona State University University of Arizona Virginia TechTempe, AZ, 85287 Tucson, AZ 85721 Blacksburg, VA 24061dmitri.roussinov@asu.edu lzhao@bpa.arizona.edu wfan@vt.eduAbstractWe have studied how context specific webcorpus can be automatically created andmined for discovering semantic similarityrelationships between terms (words orphrases) from a given collection ofdocuments (target collection).
Theserelationships between terms can be used toadjust the standard vectors spacerepresentation so as to improve theaccuracy of similarity computation betweentext documents in the target collection.
Ourexperiments with a standard test collection(Reuters) have revealed the reduction ofsimilarity errors by up to 50%, twice asmuch as the improvement by using otherknown techniques.1 IntroductionMany modern information management tasks such asdocument retrieval, clustering, filtering andsummarization rely on algorithms that computesimilarity between text documents.
For example,clustering algorithms, by definition, place documentssimilar to each other into the same cluster.
Topicdetection algorithms attempt to detect documents orpassages similar to those already presented to theusers.
?Query by example?
retrieval is based onsimilarity between a document selected as exampleand the other ones in the collection.
Even a classicalretrieval task can be formulated as rank orderingaccording to the similarity between the document(typically very short) representing user?s query andall the documents in the collection.For similarity computation, text documents arerepresented by terms (words or phrases) that theyhave, and encoded by vectors according to apredominantly used vector space model (Salton &McGill, 1983).
Each coordinate corresponds to aterm (word or phrase) possibly present within adocument.
Within that model, a high similaritybetween a pair of documents can be only indicatedby sharing same terms.
This approach has apparentlimitations due to the notorious vocabulary problem(Furnas et al, 1997): people very often use differentwords to describe semantically similar objects.
Forexample, within a classical vector space model, thesimilarity algorithm would treat words car andautomobile as entirely different, ignoring semanticsimilarity relationship between them.It has been known for a long time that semanticsimilarity relationships between terms can bediscovered by their co-occurrence in the samedocuments or in the vicinity of each other withindocuments (von Rijsbergen, 1977).
Until the 1990s,the studies exploring co-occurrence information forbuilding a thesaurus and using it in automated queryexpansion (adding similar words to the user query)resulted in mixed results (Minker et al, 1972; Peat &Willett, 1991).
The earlier difficulties may haveresulted from the following reasons:1) The test collections were small, sometimes onlyfew dozens of documents.
Thus, there was only asmall amount of data available for statistical co-occurrence analysis (mining), not enough to establishreliable associations.2) The evaluation experiments were based onretrieval tasks,  short, manually composed queries.The queries were at times ambiguous and, as a result,wrong terms were frequently added to the query.
E.g.initial query ?jaguar?
may be expanded with thewords ?auto?, ?power?, ?engine?
since they co-occurwith ?jaguar?
in auto related documents.
But, if theuser was actually referring to an animal then theretrieval accuracy would degrade after the expansion.3) The expansion models were overly simplistic, e.g.by merely adding more keywords to Boolean queries(e.g.
?jaguar OR auto OR power OR car?
).Although more recent works removed some of thelimitations and produced more encouraging results(Grefenstette, 1994; Church et al, 1991; Hearst etal., 1992;  Schutze and Pedersen, 1997; Voorhees,1994) there are still a number of questions thatremain open:1) What is the range for the magnitude of theimprovement.
Can the effect be of practicalimportance?2) What are the best mining algorithms andformulas?
How crucial is the right choice of them?3) What is the best way to select a corpus formining?
Specifically, is it enough to mine onlywithin the same collection that is involved inretrieval, clustering or other processing (targetcollection), or constructing and mining a larger499external corpus (like a subset of World Wide Web)would be of much greater help?4) Even if the techniques studied earlier are effective(or not) for query expansion within the documentretrieval paradigm, are they also effective for a moregeneral task of document similarity computation?Similarity computation stays behind almost allinformation retrieval tasks including text documentretrieval, summarization, clustering, categorization,query by example etc.
Since documents are typicallylonger than user composed queries, their vectorspace representations are much richer and thusexpanding them may be more reliable due to implicitdisambiguation.Answering these questions constitutes the novelty ofour work.
We have developed a Context SpecificSimilarity Expansion (CSSE) technique based onword co-occurrence analysis within pagesautomatically harvested from the WWW (Webcorpus) and performed extensive testing with a wellknown Reuters collection (Lewis, 1997).
To test thesimilarity computation accuracy, we designed asimple combinatorial metric which reflects howaccurately (as compared to human judgments) thealgorithm, given a document in the collection, ordersall the other documents in the collection by theperceived (computed) similarity.
We believe thatusing this metric is more objective and reliable thantrying to include all the traditional metrics specific toeach application (e.g.
recall/precision for documentretrieval, type I/II errors for categorization,clustering accuracy etc.)
since the latter may dependon the other algorithmic and implementation detailsin the system.
For example, most clusteringalgorithms rely on the notion of similarity betweentext documents, but each algorithm (k-means,minimum variance, single link, etc.)
follows its ownstrategy to maximize similarity within a cluster.We have found out that our CSSE technique havereduced similarity errors by up to 50%, twice asmuch as the improvement due to using other knowntechniques such as Latent Semantic Indexing (LSI)and Pseudo Relevance Feedback (PRF) within thesame experimental framework.
In addition to thisdramatic improvement, we have established theimportance of the following for the success of theexpansion: 1) using external corpus (a constructedsubset of WWW) in addition to the target collection2) taking the context of the target collection intoconsideration 3) using the appropriate miningformulas.
We suggest that these three crucialcomponents within our technique make itsignificantly distinct from those explored early andalso explain more encouraging results.The paper is structured as follows.
Section 2discusses previous research  results that are closelyrelated to our investigation.
Section 3 presentsalgorithms implemented in our experiments.
Section4 describes our  experiments including errorreduction, sensitivity analysis, and  comparison withother techniques.
Finally, Section 5 concludes thepaper by explaining our key contributions andoutlining our future research.2 Related WorkMost of the prior works performed only miningwithin the target collection itself and revealed resultsranging from small improvements to negative effects(degrading performance).
Throughout our paper, werefer to them as self -mining to distinguish frommining external corpus, which we believe is morepromising for similarity computation betweendocuments due to the following intuitiveconsideration.
Within self-mining paradigm, terms t1and t2 have to frequently co-occur in the collectionin order to be detected as associated (synonymic).
Inthat case, expanding document D representation witha term t2 when the document already has term t1 isnot statistically likely to enrich its representationsince t2 is likely to be in document D anyway.
Webelieve mining external larger and contextuallyrelated corpus has the potential to discover moreinteresting associations with much higher reliabilitythan just from the target collection.
That is why, thispaper focuses on constructing and mining theexternal corpus.There are very few studies that used external corpusand standard evaluation collections.
Grefenstette(1994) automatically built a thesaurus and applied itfor query expansion, producing better results thanusing the original queries.
Gauch et al (1998) usedone standard collection for mining (TREC4) andanother (TREC5) for testing and achieved 7.6%improvement.
They also achieved 28.5%improvement on the narrow-domain Cystic Fibrosiscollection.
Kwok (1998) also reported similar resultswith TREC non Web collections.
Ballesteros andCroft (1998) used unlinked corpora to reduce theambiguity associated with phrasal and termtranslation in Cross-Language Retrieval.There are even fewer studies involving semanticmining on the Web and its methodologicalevaluation.
G?ry and Haddad G?ry (1999) usedabout 60,000 documents from one specific domainfor mining similarity among French terms and testedthe results using 4 ad hoc queries.
Sugiura andEtzioni (2000) developed a tool called Q-Pilot thatmined the web pages retrieved by commercial searchengines and expanded the user query by addingsimilar terms.
They reported preliminary yetencouraging results but tested only the overallsystem, which includes the other, not directly relatedto mining features, such as clustering, pseudo-relevance feedback, and selecting the appropriateexternal search engine.
Furthermore, they only usedthe correctness of the engine selection as theevaluation metric .
There are some other well knowntechniques that do not perform mining for athesaurus explicitly but still capture and utilizesemantic similarity between the terms in an implicitway, namely Latent Semantic Indexing (LSI) andPseudo Relevance Feedback (PRF).
Latent SemanticIndexing (Analysis) (Deerwester et al, 1998) atechnique based on Singular Value Decomposition,was studied in a number of works .
It reduces thenumber of dimensions in the document space thusreducing the noise (linguistic variations) andbringing semantically similar terms together, thus it500takes into consideration the correlation between theterms.
The reported improvements so far howeverhave not exceeded 10-15% in standard collections)and sensitive to the choice of the semantic axis(reduced dimensions).
The general idea behind thePseudo Relevance Feedback (PRF) (Croft &Harper, 1979) or its more recent variation calledLocal Context Analysis (Xu & Croft, 2000) is toassume that the top rank retrieved documents arerelevant and use certain terms from them for thequery expansion.
A simple approach has been foundto increase performance over 23% on the TREC3and TREC4 collections and became internal part ofmodern IR systems.
Although this idea has been onlyapplied so far to users?
queries, we extended it in thisstudy to similarity computation between documentsin order to compare with our approach.
Although webelieve this extension is novel, it is not the focus ofthis study.
It is also worth mentioning that both LSIand PRF fall into ?self-mining?
category since theydo not require external corpus.A manually built and maintained ontology (athesaurus), such as WorldNet, may serve as a sourceof similarity between terms and has been shown tobe useful for retrieval tasks (Voorhees, 1994).However, one major drawback of manual approachis high cost of creating and maintaining.
Besides, thesimilarity between terms is context specific.
Forexample, for a campus computer support center thewords student, faculty, user are almost synonyms,but for designers of educational software (e.g.Blackboard), the words student and faculty wouldrepresent entirely different roles.Although the terms ?mining?, ?web mining?
and?knowledge discovery?
have been used by otherresearchers in various contexts (Cooley, 1997), webelieve it is legitimate to use them to describe ourwork for two major reasons: 1) We use algorithmsand formulas coming from the data mining field,specifically signal to noise ratio association metric(Church, 1989; Church, 1991) 2) Our approachinteracts with commercial search engines andharvests web pages contextually close to the targetcollection, and there is mining of resources (thesearch engine database) and discovery of content(web pages) involved.
We admit that the term?mining?
may be also used for a more sophisticatedor different kind of processing than our approachhere.3 Algorithms And ImplementationsThe target collection (Reuters in our experiment) isindexed and its most representative terms are used toconstruct a corpus from an external source (e. g.World Wide Web).
The term-to-term similaritymatrix is created by co-occurrence analysis withinthe corpus and subsequently used to expanddocument vectors in order to improve the accuracy(correctness) of similarity computation between thedocuments in the target collection.
Although in thiswork we do not study the effects on the individualapplications of the similarity computation, it iscrucial for such tasks as retrieval, clustering,categorization or topic detection.3.1 Building a Web CorpusWe designed and implemented a heuristic algorithmthat takes advantage of the capabilities provided bycommercial web search engines.
In our study, weused AltaVista (www.altavista.com), but most othersearch engines would also qualify for the task.Ideally, we would like to obtain web pages that-40-2002040600 0.2 0.4 0.6 0.8 1 1.2 1.4Average vector changeError Reduction(%)Thresh = .2Thresh = .3Thresh = .1Thresh = .4Thresh = .5 Thresh = .6Figure 1.
The average error reduction (%) as a function of average document vector change Ca for variousthreshold parameters Thresh.501contain the terms from the target collection in thesimilar context.
While constructing Web corpus, ourspider automatically sends a set of queries toAltaVista and obtains the resulting URLs.
The spidercreates one query for each term ti out of 1000 mostfrequent terms in the target collection (stop wordsexcluded) according to the following formula:qi = ?+?
+ ti + ?
?
+ context_hint, where + means string concatenation, quotes areused to represent text strings literally andcontext_hint is composed of the top most frequentterms in the target collection (stop words excluded)separated by empty space.
Although this way ofdefining context may seem a bit simplistic , it stillworked surprisingly well for our purpose.According to AltaVista, a word or phrase precededby '+' sign has to be present in the search results.
Thepresence of the other words and phrases (context hintstring in our case) is only desirable but not required.The total number of the context hint terms (108 inthis study) is limited by the maximum length of thequery string that the search engine can accept.We chose to use only top 1000 terms for constructingcorpus to keep the downloading time manageable.We believe using a larger corpus would demonstrateeven larger improvement.
Approximately 10% ofthose terms were phrases.
We only used the top 200hits from each query and only first 20Kbytes ofHTML source from each page to convert it into plaintext.
After removing duplicate URLs and emptypages, we had 19,198 pages in the Web corpus tomine.Downloading took approximately 6 hours and wasperformed in parallel, spawning up to 20 javaprocesses at a time, but it still remained the largestscalability bottleneck.3.2 Semantic Similarity DiscoveryCSSE performs co-occurrence analysis at thedocument level and computes the following values:df(t1, t2) is the joint document frequency, i.e., thenumber of web pages where both terms t1 and t2occur.
df(t) is the document frequency of the term t,i.e., the number of web pages in which the term toccurs.
Then, CSSE applies a well known signal tonoise ratio formula coming from data mining(Church, 1991) to establish similarity between termst1 and t2:sim(t1, t2)=)2()1()2,1(logtdftdfttdfN?
?/ Nlog ,         (1)where N is the total number of documents in themining collection (corpus),log N is the normalizing factor, so the sim valuewould not exceed 1 and be comparable acrosscollections of different size.Based on the suggestions from the other studiesusing formula (1), before running our tests, wedecided to discard as spurious all the co-occurrencesthat happened only within one or two pages and allthe similarities that are less than the specifiedthreshold (Thresh).3.3 Vector ExpansionSince we were modifying document vectors (moregeneral case), but not queries as in the majority ofprior studies, we refer to the process as vectorexpansion.
As we wrote in literature review, thereare many possible heuristic ways to perform vectorexpansion.
After preliminary tests, we settled on thesimple linear modification with post re-normalizationas presented below.
The context of the targetcollection is represented by the similarity matrixsim(t1, t2) mined as described in the precedingsection.
Our vector expansion algorithm adds all therelated terms to the vector representation of thedocument D with the weights proportional to thedegree of the relationships and the global inversedocument frequency (IDF) weighting of the addedterms:w(t, D)?
= w(t, D) +??
dt tdfNttsimDtwa1 )(log),'(),'( , where- 8 0- 6 0- 4 0- 2 002 00 0 .
5 1 1 .
5A v e r a g e  v e c t o r  c h a n g eError Reduction(%)Thresh = .6Thresh = .4Thresh = .3Thresh = .5Thresh = .2Thresh = .1Figure 2.
The average error reduction (%) as a function of average document vector change Ca for variousthreshold parameters Tresh without ?context hint?
terms.502w(t, D) is the initial, not expanded, weight of theterm t in the document D (assigned according to TF-IDF weighting scheme in our case); w?
(t, D) is themodified weight of the term t in the document D; t?iterates through all (possibly repeating) terms in thedocument D ; a is the adjustment factor (a parametercontrolled in the expansion process).4 Experiments4.1 Similarity Error ReductionSince in this study we were primarily concerned withimproving similarity computation but not retrievalper se, we chose a widely used for text categorizationReuters collection (Lewis, 1997) over TREC orsimilar collections with relevance judgments.
Weused a modified version of Lewis?
(1992) suggestionto derive our evaluation metric, which is similar tothe metric derived from Kruskal-Goodman statisticsused in Haveliwala et al (2002) for a study withYahoo web directory (www.yahoo.com).
Intuitively,the metric reflects the probability of algorithmguessing the correct order (called ground truth),imposed by a manually created hierarchy (simplifiedto a partition in Reuters case).
Ideally, for eachdocument D, the similarity computation algorithmshould indicate documents sharing one or moreReuters categories with document D to be moresimilar to the document D than the documents notsharing any categories with D. We formalized thisintuitive requirement into a metric by the followingway.
Let?s define a test set Sa to be the set of all thedocument triples (D, D1, D2) such that D?D1,D?D2, D1?D2, and furthermore D shares at least onecommon category with D1 but no commoncategories with D2.
We defined total error count(Ec) as the number of triples in the test set Sa suchthat sim(D, D1) < sim(D, D2) since it should be theother way around.
Our accuracy metric reportedbelow is the total error count normalized by the sizeof the test set Sa: similarity error = Ec / #Sa,computed for each Reuters topics and averagedacross all of them.
The metric ranges from 0 (idealcase) to .5 (random ordering).
It also needed anadjustment to provide the necessary continuity asjustified in the following.
Since the documents arerepresented by very sparse vectors, very often (about5% of all triples) documents D, D1, D2 do not haveany terms in common and as a result similaritycomputation results in a tie: sim(D,D1) = sim (D,D2).
A tie can not be considered an error because inthat case one can suggest a trivial improvement tothe similarity algorithm by simply breaking the tiesat random in any direction with an equal chance, andthus reducing errors in 50% of all ties.
This is whythe metric counts half of all the ties as errors, whichcompletely removes this discontinuity.We used all the Reuters 78 topics from the?commodity code?
group since they are the most?semantic ?, not trying the others (EconomicIndicator Codes, Currency Codes, Corporate Codes).We discarded the topics that had only 1 documentand used only the documents that had at least one ofthe topics.
This reduced our test collection to 1841documents, still statistically powerful andcomputationally demanding since millions of tripleshad to be considered (even after somestraightforward algorithmic optimizations).
Afterindexing and stemming (Porter, 1980) the totalnumber of unique stems used for the vectorrepresentation was 11461.WeightingSchemebooleanvectorsTF only  IDF only  TF-IDFSimilarity Error 0.1750 0.1609 0.1278 0.1041Table 2.
Comparison of different weighting schemeswith the original (not expanded) documents.Table 2 lists the similarity error averaged by topicsfor the different weighting schemes we tried first inour experiment.
Since TF-IDF weighting was by farthe best in this evaluation set up, we limited ourexpansion experiments to TF-IDF scheme only.
Forsimilarity measure between document vectors, weused the most common negative Euclidian distanceafter normalizing the vectors to unit length.
It can beshown, that cosine metric (dot product), the otherpopular metric, results in the same order and, thussame similarity error as well.
Without normalizationor stemming the errors were almost twice as muchlarger.Although we varied the adjustment parameter a inour experiment, for better interpretation, we plottedour primary metric (average error reduction) as afunction of Ca, the average Euclidian distancebetween the original and the modified documentvectors when both vectors are normalized to unitlength.
Ca serves as a convenient parametercontrolling the degree of change in the documentvectors, better than a, because same values of a mayresult in different changes depending on the term-to-term similarity matrix sim(t1, t2).
In theory, Cavaries from 0 (no change) to 2 , the case ofmaximum possible change (no common termsbetween initial and expanded representation).
Byvarying adjustment factor a from 0 to 10 and higherwe observed almost the entire theoretical range ofCa: starting from negligible change and going all theway to 2 , where the added terms entirelydominated the original ones.
The average number ofterms in the document representation was in 60-70range before expansion and in 200-300 range afterthe expansion.
This of course increasedcomputational burden.
Nevertheless, even after theexpansion, the vector representations still remainedsparse and we were able to design and implementsome straightforward algorithmic improvementstaking advantage of this sparsity to keep processingtime manageable.
The expansion for entire Reuterscollection was taking less than one minute on aworkstation with Pentium III 697 MHz processor,256 MB of RAM, with all the sparse representationsof the documents and similarity matrix stored inprimary memory.
This renders the expansion suitablefor online processing.To evaluate the performance of each technique, weused the error reduction (%) relatively to the baselineshown in Table 1 (TF-IDF column) averaged acrossall the topics, which corresponds to the lowest503original non-expanded similarity error.
Figure 1shows the error reduction as a function of Ca forvarious values of Thresh.
We stopped increasing Caonce the improvement dropped below -10% to savetesting time.
Several facts can be observed from theresults:1) The error reduction for Thresh in the mid range ofCa [.2-.4] is very stable, achieves 50% , which isvery large compared with the other knowntechniques we used for comparison as discussedbelow.
The effect is also comparable with thedifference between various weighting functions(Table 2), which we believe renders theimprovement practically significant.2) For small thresholds (Thresh < .1), the effect isnot that stable, possibly since many non-reliableassociations are involved in the expansion.3) Larger thresholds (Thresh > .4) are also not veryreliable since they result in a small number ofassociations created, and thus require large values ofadjustment parameter a in order to producesubstantial average changes in the document vectors(Ca), which results in too drastic change in somedocument vectors.4) The error reduction curve is unimodal: it startsfrom 0 for small Ca, since document vectors almostdo not change, and grows to achieve maximum forCa somewhere in relatively wide .1 - .5 range.
Then,it decreases, because document vectors may bedrifting too far from the original ones, falling below0 for some large values of Ca.5) For thresholds (Thresh) .2 and .3, the effect stayspositive even for large values of Ca, which is aninteresting phenomenon because document vectorsare getting almost entirely replaced by theirexpanded representations.Some sensitivity of the results with respect to theparameters Thresh, Ca is a limitation as occurssimilarly to virtually all modern IR improvementtechniques.
Indeed, Latent Semantic Indexing (LSI)needs to have number of semantic axis to becorrectly set, otherwise the performance maydegrade.
Pseudo Relevance Feedback (PRF) dependson several parameters such as number of documentsto use for feedback, adjustment factor, etc.
Allpreviously studied expansion techniques depend onthe adjustment factor as  well.
The specific choice ofthe parameters for real life applications is typicallyperformed manually based on trial and error or byfollowing a machine learning approach: splitting datainto training and testing sets.
Based on the aboveresults, the similarity threshold (Thresh) in .2-.4 andCa in .1-.5 range seem to be a safe combination, notdegrading and likely to significantly (20-50%)improve performance.
The performance curve beingunimodal with respect to both Ca and Thresh alsomakes it easier to tune by looking for maxima.Although we have involved only one test collectionin this study, this collection (Reuters) varies greatlyin the content and the size of the documents, so wehope our results will generalize to other collections.We also verified that the effect typically diminisheswhen the size of the mining collection (corpus) isreduced by random sub-sampling.
Those results werealso similar to those obtained 4 months earlier,although only 80% of the pages in the mining corpusremained.-30-20-100100 500 1000Number of Semantic AxisError Reduction(%)All topics Tough Topics OnlyFigure 4.
Comparing to LSI.-20020400 0.2 0.4 0.6 0.8 1 1.2Average vector changeError Reduction(%)Thresh = .3Thresh = .4Thresh = .2Thresh = .5 Thresh = .6 Thresh = .1Figure 3.
The average error reduction (%) as a function of average document vector change Ca forvarious threshold parameters Tresh without using external mining collection.504-20-1001020300 0.2 0.4 0.6 0.8 1Average vector changeError Reduction(%)Nc=5 Nc=10 Nc=20Figure 5.
The error reduction as the function of theaverage vector change due to Pseudo RelevanceFeedback for several cut-off numbers Nc.4.2 Sensitivity AnalysisTo test the importance of the context, we removedthe ?context hint?
terms from the queries used by ouragent, and created another (less context specific)corpus for mining.
We obtained 175,336 uniqueURLs, much more than with using ?context hint?terms since the overlap between different queryresults was much smaller.
We randomly selected25,000 URLs of them and downloaded the referredpages.
Then, to make the comparison more objective,we randomly selected  19,198 pages (same numberas with using context hint) of the non-emptydownloaded pages.
We mined the similarityrelationships from the selected documents in thesame way as described above.
The resultingimprovement (shown in the Figure 2) was indeedmuch smaller (13% and less) than with using?context hint?
terms.
It also degrades much quickerfor larger Ca and more sensitive to the choice ofThresh.
This may explain why mixed results werereported in the literate when the similarity thesauruswas constructed in a very general setting, but notspecifically for the target collection in mind.
It isalso interesting to note a similar behavior of errorreduction as the function of Ca and Thresh: it isunimodal with maximum in approximately samerange of arguments.
This may also serve as indirectevidence of stability of the effect (even if smaller inthat case) with respect to the parameters involved.To verify the importance of using external corpus vs.self-mining, we mined the similarity relationshipsfrom the same collection (Reuters) that we used forthe tests (target collection) using the same miningalgorithms.
Figure 3 shows that the effect of such?self-mining?
is relatively modest (up to 20%),confirming that using the external corpus (the Webin our approach) was crucial.
Again, the behavior ofthe error reduction (even smaller in that case) withrespect to Ca and Thresh is similar to the contextspecific web corpus mining.4.3 Comparison with Other TechniquesFigure 4 shows the similarity error reduction as afunction of the number of semantic axis when LSI isapplied.
The effect with the entire collection (secondcolumn) is always negative.
So, the Reuterscollection in our experiment set up was found to benot a good application of LSI technique, possiblybecause many of the topics have already small errorseven before applying LSI.
To verify ourimplementation and the applicability of LSI to thesimilarity computation, we applied it only to the?tougher?
26 topics, those in the upper half ifordered by the original similarity error.
As Figure 4reveals, LSI is effective in that case for numbers ofsemantic axis comparable with number of topics inthe target collection.
Our findings are well in linewith reported in prior research.We adapted the classic Pseudo Relevance Feedbackalgorithm (Qiu, 1993), which has been so far appliedonly to document retrieval tasks, to similaritycomputation in a straightforward way and also triedseveral variations of if (not described here due tolack of space).
Figure 5 shows the effect as afunction of adjustment factor a for various cut-offparameters Nc (the number of top ranked documentsused for feedback).
The effect achieves themaximum of around 21%, consistent with the resultsreported in prior research.
The improvement is closein magnitude to the one due to ?self-mining?described above.
We do not claim that our approachis better than PRF since it is not entirely meaningfulto make this comparison due to the number ofparameters and implementation details involved inboth.
Also, more important, the techniques rely ondifferent source of data: PRF is a ?self-mining?approach while CSSE builds and mines externalcorpus.
Thus, CSSE can be used in addition to PRF.5 ConclusionsIn this paper, we proposed and empirically studied anapproach to improve similarity computation betweentext documents by creating a context specific Webcorpus and performing similarity mining within it.The results demonstrated that the similarity errorscan be reduced by additional 50% after all thestandard procedures such as stemming, termweighting, and vector normalization.
We alsoestablished the crucial importance of the followingthree factors, which we believe make our techniquedistinct from those already explored early andexplain more encouraging results that we obtained:1) Using external corpus.
2) Taking the context ofthe target collection into consideration.
3) Using theappropriate mining formula.
Another importantdistinction and possible explanation of a moredramatic effect is our focus on similaritycomputation between text documents, rather than ondocument retrieval tasks, which have been moreextensively studied in the past.
Similaritycomputation is a more general procedure, which inturns defines the quality of virtually all other specifictasks such as document retrieval, summarization,clustering, categorization, topic detection, query byexample, etc.
Our future plans are to overcome someof the limitations in this study, specifically usingmore than a single (although standard and verydiverse) collection and study other experimentalsetups, such as document retrieval, textcategorization, or topic detection and tracking.5056 AcknowledgementWeiguo Fan's work is supported by NSF under thegrant number ITR0325579.ReferencesChurch, K.W., Gale, W., Hanks, P., Hindle, D.(1991).
Using Statistics in Lexical Analysis.
In:Uri Zernik (ed.
), Lexical Acquisition: ExploitingOn-Line Resources to Build a Lexicon.
NewJersey: Lawrence Erlbaum, 1991, pp.
115-164.Church, K.W., Hanks, P. (1989).
Word AssociationNorms, Mutual Information and Lexicography.
InProceedings of the 27th Annual Conference of theAssociation of Computational Linguistics, 1989,pp.
76-83.Cooley, R., Mobasher, B. and Srivastava, J.
(1997).Web Mining: Information and Pattern Discoveryon the World Wide Web (with R. Cooley and J.Srivastava), in Proceedings of the 9th IEEEInternational Conference on Tools with ArtificialIntelligence (ICTAI'97), November 1997.Croft, W.B., and Harper, D.J.
(1979).
Usingprobabilistic models of document retrieval withoutrelevance information.
Journal of Documentation,35, pp.
285-295.Deerwester S., Dumais S., Furnas G., Landauer T.K.,and Harshman R., Indexing by Latent SemanticAnalysis.
Journal of the American Society forInformation Science 41 (1990), 391-407.Furnas, G. W., Landauer, T. K., Gomez, L. M., &Dumais, S. T. (1987).
The Vocabulary Problem inHuman-System Communication.
Communicationsof the ACM, 30(11), pp.
964-971.G?ry, M., Haddad, M. H. (1999).
KnowledgeDiscovery for Automatic Query Expansion on theWorld Wide Web.
International Workshop on theWorld-Wide Web and Conceptual Modeling(WWWCM'99), in conjunction with the 18thInternational Conference on Conceptual Modeling(ER'99), Paris, France, November 15-18, 1999,pp.
334-347.Grefenstette, G. (1994).
Explorations in AutomaticThesaurus Discovery.
Kluwer AcademicPublishers, Moston, MA.Haveliwala, T.H, Gionis, A., Klein, D., Indyk, P.(2002).
Evaluating Strategies for Similarity Searchon the Web.
WWW2002, May 7-11, 2002,Honolulu, Hawaii, USA.Hearst, M. (1992).
Automatic Acquisition ofHyponyms from Large Text Corpora, Proceedingsof the Fourteenth International Conference onComputational Linguistics, Nantes, France, July1992.Kwok, K.L.
(1998).
Improving two-stage ad-hocretrieval for short queries.
Twenty-First AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pp.
250-256, New York, August 1998.Lewis, D.D.
(1992).
Representation and Learning inInformation Retrieval.
Doctoral Dissertation.University of Massachusetts at Amherst.Lewis, D.D.
(1997).
Reuters-21578 textcategorization test collection, Distribution 1.0,Sept 26, 1997.Minker, J., Wilson, G. A.
& Zimmerman, B. H.(1972).
An evaluation of query expansion by theaddition of clustered terms for a documentretrieval system.
Information Storage andRetrieval, pp.
329-348.Peat, H. J.
& Willett, P. (1991).
The limitations ofterm co-occurrence data for query expansion indocument retrieval systems.
Journal of theAmerican Society for Information Science, 42(5),pp.
378-383.Porter, M.F.
(1980).
An algorithm for suffixstripping.
Program, 14, pp.
130--137, 1980.Qiu, Y.
(1993).
Concept Based Query Expansion.Proceedings of SIGIR-93, 16th ACM InternationalConference on Research and Development inInformation Retrieval.Salton, G. and McGill, M.J. (1983).
Introduction toModern Information Retrieval.
New York.McGraw-Hill.Schutze, H. and Pedersen, J.O.
(1997).
A co-occurrence-based thesaurus and two applicationsto information retrieval.
Information Processingand Management.
33(3), pp.
307-318.Sugiura, A., and Etzioni, O.
(2000).
Query Routingfor Web Search Engines: Architecture andExperiments.
9th International World Wide WebConference, Amsterdam, May 15-19, 2000.van Rijsbergen, C.J.. (1977).
A theoretical basis forthe use of co-occurrence data in informationretrieval.
Journal of Documentation, 33(2):106--119, 1977.Voorhees, E. M. (1994).
Query expansion usinglexical-semantic relations.
In Proceedings of the17th Annual International ACM/SIGIRConference, pp.
61-69, Dublin, Ireland.Xu, J. and Croft, W.B.
(2000).
Improving theeffectiveness of information retrieval with localcontext analysis.
ACM Transactions onInformation Systems (TOIS), 18(1):79--112,2000.Ballesteros, L., Croft, W.B.
(1998).
ResolvingAmbiguity for Cross-Language Retrieval.
InProceedings of the 21th Annual InternationalACM/SIGIR Conference, pp.
64-71.506
