Proceedings of ACL-08: HLT, pages 443?451,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCollecting a Why-question corpus for development and evaluation of anautomatic QA-systemJoanna Mrozinski Edward WhittakerDepartment of Computer ScienceTokyo Institute of Technology2-12-1-W8-77 Ookayama, Meguro-kuTokyo 152-8552 Japan{mrozinsk,edw,furui}@furui.cs.titech.ac.jpSadaoki FuruiAbstractQuestion answering research has only recentlystarted to spread from short factoid questionsto more complex ones.
One significant chal-lenge is the evaluation: manual evaluation is adifficult, time-consuming process and not ap-plicable within efficient development of sys-tems.
Automatic evaluation requires a cor-pus of questions and answers, a definition ofwhat is a correct answer, and a way to com-pare the correct answers to automatic answersproduced by a system.
For this purpose wepresent a Wikipedia-based corpus of Why-questions and corresponding answers and arti-cles.
The corpus was built by a novel method:paid participants were contacted through aWeb-interface, a procedure which allowed dy-namic, fast and inexpensive development ofdata collection methods.
Each question in thecorpus has several corresponding, partly over-lapping answers, which is an asset when es-timating the correctness of answers.
In ad-dition, the corpus contains information re-lated to the corpus collection process.
We be-lieve this additional information can be used topost-process the data, and to develop an auto-matic approval system for further data collec-tion projects conducted in a similar manner.1 IntroductionAutomatic question answering (QA) is an alternativeto traditional word-based search engines.
Instead ofreturning a long list of documents more or less re-lated to the query parameters, the aim of a QA sys-tem is to isolate the exact answer as accurately aspossible, and to provide the user only a short textclip containing the required information.One of the major development challenges is eval-uation.
The conferences such as TREC1, CLEF2and NTCIR3 have provided valuable QA evaluationmethods, and in addition produced and distributedcorpora of questions, answers and correspondingdocuments.
However, these conferences have fo-cused mainly on fact-based questions with short an-swers, so called factoid questions.
Recently morecomplex tasks such as list, definition and discourse-based questions have also been included in TREC ina limited fashion (Dang et al, 2007).
More complexhow- and why-questions (for Asian languages) werealso included in the NTCIR07, but the provided datacomprised only 100 questions, of which some werealso factoids (Fukumoto et al, 2007).
Not only isthe available non-factoid data quite limited in size,it is also questionable whether the data sets are us-able in development outside the conferences.
Linand Katz (2006) suggest that training data has to bemore precise, and, that it should be collected, or atleast cleaned, manually.Some corpora of why-questions have been col-lected manually: corpora described in (Verberne etal., 2006) and (Verberne et al, 2007) both com-prise fewer than 400 questions and correspondinganswers (one or two per question) formulated by na-tive speakers.
However, we believe one answer perquestion is not enough.
Even with factoid questionsit is sometimes difficult to define what is a correct1http://trec.nist.gov/2http://www.clef-campaign.org/3http://research.nii.ac.jp/ntcir/443answer, and complex questions result in a whole newlevel of ambiguity.
Correctness depends greatly onthe background knowledge and expectations of theperson asking the question.
For example, a correctanswer to the question ?Why did Mr. X take Ms. Yto a coffee shop??
could be very different dependingon whether we knew that Mr. X does not drink cof-fee or that he normally drinks it alone, or that Mr. Xand Ms. Y are known enemies.The problem of several possible answers and, inconsequence, automatic evaluation has been tackledfor years within another field of study: automaticsummarisation (Hori et al, 2003; Lin and Hovy,2003).
We believe that the best method of provid-ing ?correct?
answers is to do what has been done inthat field: combine a multitude of answers to ensureboth diversity and consensus among the answers.Correctness of an answer is also closely related tothe required level of detail.
The Internet FAQ pageswere successfully used to develop QA-systems (Jijk-oun and de Rijke, 2005; Soricut and Brill, 2006), ashave the human-powered question sites such as An-swers.com, Yahoo Answers and Google Answers,where individuals can post questions and receive an-swers from peers (Mizuno et al, 2007).
Both re-sources can be assumed to contain adequately error-free information.
FAQ pages are created so as toanswer typical questions well enough that the ques-tions do not need to be repeated.
Question sites typ-ically rank the answers and offer bonuses for peo-ple providing good ones.
However, both sites sufferfrom excess of information.
FAQ-pages tend to alsoanswer questions which are not asked, and also con-tain practical examples.
Human-powered answersoften contain unrelated information and discourse-like elements.
Additionally, the answers do not al-ways have a connection to the source material fromwhich they could be extracted.One purpose of our project was to take part inthe development of QA systems by providing thecommunity with a new type of corpus.
The cor-pus includes not only the questions with multipleanswers and corresponding articles, but also certainadditional information that we believe is essential toenhance the usability of the data.In addition to providing a new QA corpus, wehope our description of the data collection processwill provide insight, resources and motivation forfurther research and projects using similar collectionmethods.
We collected our corpus through AmazonMechanical Turk service 4 (MTurk).
The MTurkinfrastructure allowed us to distribute our tasks toa multitude of workers around the world, withoutthe burden of advertising.
The system also allowedus to test the workers suitability, and to reward thework without the bureaucracy of employment.
Toour knowledge, this is the first time that the MTurkservice has been used in equivalent purpose.We conducted the data collection in three steps:generation, answering and rephrasing of questions.The workers were provided with a set of Wikipediaarticles, based on which the questions were createdand the answers determined by sentence selection.The WhyQA-corpus consists of three parts: originalquestions along with their rephrased versions, 8-10partly overlapping answers for each question, andthe Wikipedia articles including the ones corre-sponding to the questions.
The WhyQA-corpus isin XML-format and can be downloaded and usedunder the GNU Free Documentation License fromwww.furui.cs.titech.ac.jp/ .2 SetupQuestion-answer pairs have previously been gen-erated for example by asking workers to both aska question and then answer it based on a giventext (Verberne et al, 2006; Verberne et al, 2007).We decided on a different approach for two reasons.Firstly, based on our experience such an approach isnot optimal in the MTurk framework.
The tasks thatwere welcomed by workers required a short atten-tion span, and reading long texts was negatively re-ceived with many complaints, sloppy work and slowresponse times.
Secondly, we believe that the afore-mentioned approach can produce unnatural ques-tions that are not actually based on the informationneed of the workers.We divided the QA-generation task into twophases: question-generation (QGenHIT) and an-swering (QAHIT).
We also trimmed the amount ofthe text that the workers were required to read to cre-ate the questions.
These measures were taken bothin order to lessen the cognitive burden of the task4http://www.mturk.com444and to produce more natural questions.In the first phase the workers generated the ques-tions based on a part of Wikipedia article.
The re-sulting questions were then uploaded to the systemas new HITs with the corresponding articles, andanswered by available (different) workers.
Our hy-pothesis is that the questions are more natural if theiranswer is not known at the time of the creation.Finally, in an additional third phase, 5 rephrasedversions of each question were created in order togain variation (QRepHIT).
The data quality was en-sured by requiring the workers to achieve a certainresult from a test (or a Qualification) before theycould work on the aforementioned tasks.Below we explain the MTurk system, and then ourcollection process in detail.2.1 Mechanical TurkMechanical Turk is a Web-based service, offered byAmazon.com, Inc.
It provides an API through whichemployers can obtain a connection to people to per-form a variety of simple tasks.
With tools providedby Amazon.com, the employer creates tasks, and up-loads them to the MTurk Web-site.
Workers can thenbrowse the tasks and, if they find them profitableand/or interesting enough, work on them.
When thetasks are completed, the employer can download theresults, and accept or reject them.
Some key con-cepts of the system are listed below, with short de-scriptions of the functionality.?
HIT Human Intelligence Task, the unit of apayable chore in MTurk.?
Requester An ?employer?, creates and uploadsnew HITs and rewards the workers.
Requesterscan upload simple HITs through the MTurk Re-quester web site, and more complicated onesthrough the MTurk Web Service APIs.?
Worker An ?employee?, works on the hitsthrough the MTurk Workers?
web site.?
Assignment.
One HIT consists of one or moreassignments.
One worker can complete a sin-gle HIT only once, so if the requester needsmultiple results per HIT, he needs to set theassignment-count to the desired figure.
A HITis considered completed when all the assign-ments have been completed.?
Rewards At upload time, each HIT has to beassigned a fixed reward, that cannot be changedlater.
Minimum reward is $0.01.
Amazon.comcollects a 10% (or a minimum of $0.05) servicefee per each paid reward.?
Qualifications To improve the data quality,a HIT can also be attached to certain tests,?qualifications?
that are either system-providedor created by the requester.
An example ofa system-provided qualification is the averageapproval ratio of the worker.Even if it is possible to create tests that workershave to pass before being allowed to work on a HITso as to ensure the worker?s ability, it is impossibleto test the motivation (for instance, they cannot beinterviewed).
Also, as they are working through theWeb, their working conditions cannot be controlled.2.2 Collection processThe document collection used in our research wasderived from the Wikipedia XML Corpus by De-noyer and Gallinari (2006).
We selected a total of84 articles, based on their length and contents.
Acertain length was required so that we could expectthe article to contain enough interesting material toproduce a wide selection of natural questions.
Thearticles varied in topic, degree of formality and theamount of details; from ?Horror film?
and ?Christ-mas worldwide?
to ?G-Man (Half-Life)?
and ?His-tory of London?.
Articles consisting of bulleted listswere removed, but filtering based on the topic of thearticle was not performed.
Essentially, the articleswere selected randomly.2.2.1 QGenHITThe first phase of the question-answer generationwas to generate the questions.
In QGenHIT we pre-sented the worker with only part of a Wikipedia ar-ticle, and instructed them to think of a why-questionthat they felt could be answered based on the origi-nal, whole article which they were not shown.
Thisapproach was expected to lead to natural curiosityand questions.
Offering too little information wouldhave lead to many questions that would finally beleft unanswered, and it also did not give the workersenough to work on.
Giving too much information445Qualification The workers were required to pass a test before working on the HITs.QGenHIT Questions were generated based on partial Wikipedia articles.
These questions werethen used to create the QAHITs.QAHIT Workers were presented with a question and a corresponding article.
The task was toanswer the questions (if possible) through sentence selection.QRepHIT To ensure variation in the questions, each question was rephrased by 5 different workers.Table 1: Main components of the corpus collection process.Article topic: Fermi paradoxOriginal question Why is the moon crucial to the rare earth hypothesis?Rephrased Q 1 How does the rare earth theory depend upon the moon?Rephrased Q 2 What makes the moon so important to rare earth theory?Rephrased Q 3 What is the crucial regard for the moon in the rare earth hypothesis?Rephrased Q 4 Why is the moon so important in the rare earth hypothesis?Rephrased Q 5 What makes the moon necessary, in regards to the rare earth hypothesis?Answer 1.
Sentence ids: 20,21.
Duplicates: 4.
The moon is important because its gravitational pullcreates tides that stabilize Earth?s axis.
Without this stability, its variation, known as precession ofthe equinoxes, could cause weather to vary so dramatically that it could potentially suppress the morecomplex forms of life.Answer 2.
Sentence ids: 18,19,20.
Duplicates: 2.
The popular Giant impact theory asserts that itwas formed by a rare collision between the young Earth and a Mars-sized body, usually referred to asOrpheus or Theia, approximately 4.45 billion years ago.
The collision had to occur at a precise angle,as a direct hit would have destroyed the Earth, and a shallow hit would have deflected the Mars-sizedbody.
The moon is important because its gravitational pull creates tides that stabilize Earth?s axis.Answer 3.
Sentence ids: 20,21,22.
Duplicates: 2.
The moon is important because its gravitationalpull creates tides that stabilize Earth?s axis.
Without this stability, its variation, known as precessionof the equinoxes, could cause weather to vary so dramatically that it could potentially suppress themore complex forms of life.
The heat generated by the Earth/Theia impact, as well as subsequentLunar tides, may have also significantly contributed to the total heat budget of the Earth?s interior,thereby both strengthening and prolonging the life of the dynamos that generate Earth?s magnetic fieldDynamo 1.Answer 4.
Sentence ids: 18,20,21.
No duplicates.
The popular Giant impact theory asserts thatit was formed by a rare collision between the young Earth and a Mars-sized body, usually referredto as Orpheus or Theia, approximately 4.45 billion years ago.
The moon is important because itsgravitational pull creates tides that stabilize Earth?s axis.
Without this stability, its variation, knownas precession of the equinoxes, could cause weather to vary so dramatically that it could potentiallysuppress the more complex forms of life.Answer 5.
Sentence ids: 18,21.
No duplicates.
The popular Giant impact theory asserts that itwas formed by a rare collision between the young Earth and a Mars-sized body, usually referred to asOrpheus or Theia, approximately 4.45 billion years ago.
Without this stability, its variation, knownas precession of the equinoxes, could cause weather to vary so dramatically that it could potentiallysuppress the more complex forms of life.Table 2: Data example: Question with rephrased versions and answers.446(long excerpts from the articles) was severely dis-liked among the workers simply because it took along time to read.We finally settled on a solution where the partialcontent consisted of the title and headers of the arti-cle, along with the first sentences of each paragraph.The instructions to the questions demanded rigidlythat the question starts with the word ?Why?, as itwas surprisingly difficult to explain what we meantby why-questions if the question word was not fixed.The reward per HIT was $0.04, and 10 questionswere collected for each article.
We did not force thequestions to be different, and thus in the later phasesome of the questions were removed manually asthey were deemed to mean exactly the same thing.However, there were less than 30 of these duplicatequestions in the whole data set.2.2.2 QAHITAfter generating the questions based on partial ar-ticles, the resulting questions were uploaded to thesystem as HITs.
Each of these QAHITs presented asingle question with the corresponding original arti-cle.
The worker?s task was to select either 1-3 sen-tences from the text, or a No-answer-option (NoA).Sentence selection was conducted with Javascriptfunctionality, so the workers had no chance to in-clude freely typed information within the answer (al-though a comment field was provided).
The rewardper HIT was $0.06.
At the beginning, we collected10 answers per question, but we cut that down to 8because the HITs were not completed fast enough.The workers for QAHITs were drawn from thesame pool as the workers for QGenHIT, and it waspossible for the workers to answer the questions theyhad generated themselves.2.2.3 QRepHITAs the final step 5 rephrased versions of eachquestion were generated.
This was done to com-pensate the rigid instructions of the QGenHIT andto ensure variation in the questions.
We have not yetmeasured how well the rephrased questions matchthe answers of their original versions.
In the finalQRepHIT questions were grouped into groups of 5.Each HIT consisted of 5 assignments, and a $0.05reward was offered for each HIT.QRepHIT required the least amount of design andtrials, and workers were delighted with the task.
TheHITs were completed fast and well even in the casewhen we accidentally uploaded a set of HITs withno reward.As with QAHIT, the worker pool for creating andrephrasing questions was the same.
The questionswere rephrased by their creator in 4 cases.2.3 QualificationsTo improve the data quality, we used the qualifi-cations to test the workers.
For the QGenHITs weonly used the system-provided ?HIT approval rate?-qualification.
Only workers whose previous workhad been approved in 80% of the cases were able towork on our HITs.In addition to the system-provided qualification,we created a why-question-specific qualification.The workers were presented with 3 questions, andthey were to answer each by either selecting 1-3 most relevant sentences from a list of about 10sentences, or by deciding that there is no answerpresent.
The possible answer-sentences were di-vided into groups of essential, OK and wrong, andone of the questions did quite clearly have no an-swer.
The scoring was such that it was impossibleto get approved results if not enough essential sen-tences were included.
Selecting sentences from theOK-group only was not sufficient, and selecting sen-tences from the wrong-group was penalized.
A min-imum score per question was required, but also thetotal score was relevant ?
component scores couldcompensate each other up to a point.
However, ifthe question with no answer was answered, the scorecould not be of an approvable level.
This qualifica-tion was, in addition to the minimum HIT approvalrate of 80%, a prerequisite for both the QRepHITsand the QAHITs.A total of 2355 workers took the test, and 1571(67%) of them passed it, thus becoming our avail-able worker pool.
However, in the end the actualnumber of different workers was only 173.Examples of each HIT, their instructions and theQualification form are included in the final corpus.The collection process is summarised in Table 1.4473 Corpus descriptionThe final corpus consists of questions with theirrephrased versions and answers.
There are total of695 questions, of which 159 were considered unan-swerable based on the articles, and 536 that have 8-10 answers each.
The total cost of producing thecorpus was about $350, consisting of $310 paid inworkers rewards and $40 in Mechanical Turk fees,including all the trials conducted during the devel-opment of the final system.Also included is a set of Wikipedia documents(WikiXML, about 660 000 articles or 670MB in com-pressed format), including the ones corresponding tothe questions (84 documents).
The source of Wik-iXML is the English part of the Wikipedia XMLCorpus by Denoyer and Gallinari (2006).
In theoriginal data some of the HTML-structures like listsand tables occurred within sentences.
Our sentence-selection approach to QA required a more fine-grained segmentation and for our purpose, muchof the HTML-information was redundant anyway.Consequently we removed most of the HTML-structures, and the table-cells, list-items and othersimilar elements were converted into sentences.Apart from sentence-information, only the section-title information was maintained.
Example data isshown in Table 2.3.1 Task-related informationDespite the Qualifications and other measures takenin the collection phase of the corpus, we believe thequality of the data remains open to question.
How-ever, the Mechanical Turk framework provided addi-tional information for each assignment, for examplethe time workers spent on the task.
We believe thisinformation can be used to analyse and use our databetter, and have included it in the corpus to be usedin further experiments.?
Worker Id Within the MTurk framework, eachworker is assigned a unique id.
Worker id canbe used to assign a reliability-value to the work-ers, based on the quality of their previous work.It was also used to examine whether the sameworkers worked on the same data in differentphases: Of the original questions, only 7 wereanswered and 4 other rephrased by the sameworker they were created by.
However, it hasto be acknowledged that it is also possible forone worker to have had several accounts in thesystem, and thus be working under several dif-ferent worker ids.?
Time On Task The MTurk framework alsoprovides the requester the time it took for theworker to complete the assignment after ac-cepting it.
This information is also included inthe corpus, although it is impossible to knowprecisely how much time the workers actuallyspent on each task.
For instance, it is possiblethat one worker had several assignments openat the same time, or that they were not concen-trating fully on working on the task.
A highvalue of Time On Task thus does not necessar-ily mean that the worker actually spent a longtime on it.
However, a low value indicates thathe/she did only spend a short time on it.?
Reward Over the period spent collecting thedata, we changed the reward a couple of timesto speed up the process.
The reward is reportedper HIT.?
Approval Status Within the collection pro-cess we encountered some clearly unacceptablework, and rejected it.
The rejected work is alsoincluded in the corpus, but marked as rejected.The screening process was by no means per-fect, and it is probable that some of the ap-proved work should have been rejected.?
HIT id, Assignment id, Upload Time HIT andassignment ids and original upload times of theHITs are provided to make it possible to retracethe collection steps if needed.?
Completion Time Completion time is thetimestamp of the moment when the task wascompleted by a worker and returned to the sys-tem.
The time between the completion timeand the upload time is presumably highly de-pendent on the reward, and on the appeal of thetask in question.3.2 Quality experimentsAs an example of the post-processing of the data,we conducted some preliminary experiments on theanswer agreement between workers.448Out of the 695 questions, 159 were filtered out inthe first part of QAHIT.
We first uploaded only 3 as-signments, and the questions that 2 out of 3 work-ers deemed unanswerable were filtered out.
Thisleft 536 questions which were considered answered,each one having 8-10 answers from different work-ers.
Even though in the majority of cases (83% of thequestions) one of the workers replied with the NoA,the ones that answered did agree up to a point: ofall the answers, 72% were such that all of their sen-tences were selected by at least two different work-ers.
On top of this, an additional 17% of answersshared at least one sentence that was selected bymore than one worker.To understand the agreement better, we also cal-culated the average agreement of selected sentencesbased on sentence ids and N-gram overlaps betweenthe answers.
In both of these experiments, onlythose 536 questions that were considered answer-able were included.3.2.1 Answer agreement on sentence idsAs the questions were answered by means of sen-tence selection, the simplest method to check theagreement between the workers was to comparethe ids of the selected sentences.
The agreementwas calculated as follows: each answer was com-pared to all the other answers for the same ques-tion.
For each case, the agreement was defined asAgreement = CommonIdsAllIds , where CommonIdsis the number of sentence ids that existed in bothanswers, and AllIds is the number of different idsin both answers.
We calculated the overall averageagreement ratio (Total Avg) and the average of thebest matches between two assignments within oneHIT (Best Match).
We ran the test for two data sets:The most typical case of the workers cheating wasto mark the question unaswerable.
Because of thisthe first data set included only the real answers, andthe NoAs were removed (NoA not included, 3872answers).
If an answer was compared with a NoA,the agreement was 0, and if two NoAs were com-pared, the agreement was 1.
We did, however, alsoinclude the figures for the whole data set (NoA in-cluded, 4638 answers).
The results are shown in Ta-ble 3.The Best Match -results were quite high com-pared to the Total Avg.
From this we can concludeTotal Avg Best MatchNoA not included 0.39 0.68NoA included 0.34 0.68Table 3: Answer agreement based on sentence ids.that in the majority of cases, there was at least onequite similar answer among those for that HIT.
How-ever, comparing the sentence ids is only an indica-tive measure, and it does not tell the whole storyabout agreement.
For each document there may ex-ist several separate sentences that contain the samekind of information, and so two answers can be alikeeven though the sentence ids do not match.3.2.2 Answer agreement based on ROUGEDefining the agreement over several passages oftexts has for a long time been a research prob-lem within the field of automatic summarisation.For each document it is possible to create severalsummarisations that can each be considered cor-rect.
The problem has been approached by usingthe ROUGE-metric: calculating the N-gram over-lap between manual, ?correct?
summaries, and theautomatic summaries.
ROUGE has been proven tocorrelate well with human evaluation (Lin and Hovy,2003).Overlaps of higher order N-grams are more usablewithin speech summarisation as they take the gram-matical structure and fluency of the summary intoaccount.
When selecting sentences, this is not an is-sue, so we decided to use only unigram and bigramcounts (Table 4: R-1, R2), as well as the skip-bigramvalues (R-SU) and the longest common N-gram met-ric R-L. We calculated the figures for two data setsin the same way as in the case of sentence id agree-ment.
Finally, we set a lower bound for the resultsby comparing the answers to each other randomly(the NoAs were also included).The final F-measures of the ROUGE results arepresented in Table 4.
The figures vary from 0.37 to0.56 for the first data set, and from 0.28 to 0.42 tothe second.
It is debatable how the results shouldbe interpreted, as we have not defined a theoreticalupper bound to the values, but the difference to therandomised results is substantial.
In the field of au-tomatic summarisation, the overlap of the automatic449results and corresponding manual summarisations isgenerally much lower than the overlap between ouranswers (Chali and Kolla, 2004).
However, it is dif-ficult to draw detailed conclusions based on compar-ison between these two very different tasks.R-1 R-2 R-SU R-LNoA not included 0.56 0.46 0.37 0.52NoA included 0.42 0.35 0.28 0.39Random Answers 0.13 0.01 0.02 0.09Table 4: Answer agreement: ROUGE-1, -2, -SU and -L.The sentence agreement and ROUGE-figures donot tell us much by themselves.
However, they arean example of a procedure that can be used to post-process the data and in further projects of similarnature.
For example, the ROUGE similarity couldbe used in the data collection phase as a tool of au-tomatic approval and rejection of workers?
assign-ments.4 Discussion and future workDuring the initial trials of data collection we encoun-tered some unexpected phenomena.
For example,increasing the reward did have a positive effect inreducing the time it took for HITs to be completed,however it did not correlate in desirable way withdata quality.
Indeed the quality actually decreasedwith increasing reward.
We believe that this unex-pected result is due to the distributed nature of theworker pool in Mechanical Turk.
Clearly the moti-vation of some workers is other than monetary re-ward.
Especially if the HIT is interesting and canbe completed in a short period of time, it seems thatthere are people willing to work on them even forfree.MTurk requesters cannot however rely on thisvoluntary workforce.
From MTurk Forums it is clearthat some of the workers rely on the money theyget from completing the HITs.
There seems to be acritical reward-threshold after which the ?real work-force?, i.e.
workers who are mainly interested in per-forming the HITs as fast as possible, starts to partic-ipate.
When the motivation changes from voluntaryparticipation to maximising the monetary gain, thequality of the obtained results often understandablysuffers.It would be ideal if a requester could rely on thevoluntary workforce alone for results, but in manycases this may result either in too few workers and/ortoo slow a rate of data acquisition.
Therefore it is of-ten necessary to raise the reward and rely on efficientautomatic validation of the data.We have looked into the answer agreement ofthe workers as an experimental post-processing step.We believe that further work in this area will providethe tools required for automatic data quality control.5 ConclusionsIn this paper we have described a dynamic and inex-pensive method of collecting a corpus of questionsand answers using the Amazon Mechanical Turkframework.
We have provided to the communitya corpus of questions, answers and correspondingdocuments, that we believe can be used in the de-velopment of QA-systems for why-questions.
Wepropose that combining several answers from dif-ferent people is an important factor in defining the?correct?
answer to a why-question, and to that goalhave included several answers for each question inthe corpus.We have also included data that we believe isvaluable in post-processing the data: the work his-tory of a single worker, the time spent on tasks, andthe agreement on a single HIT between a set of dif-ferent workers.
We believe that this information, es-pecially the answer agreement of workers, can besuccessfully used in post-processing and analysingthe data, as well as automatically accepting and re-jecting workers?
submissions in similar future datacollection exercises.AcknowledgmentsThis study was funded by the Monbusho Scholar-ship of Japanese Government and the 21st CenturyCOE Program ?Framework for Systematization andApplication of Large-scale Knowledge Resources(COE-LKR)?ReferencesYllias Chali and Maheedhar Kolla.
2004.
Summariza-tion Techniques at DUC 2004.
In DUC2004.Hoa Trang Dang, Diane Kelly, and Jimmy Lin.
2007.Overview of the TREC 2007 Question Answering450Track.
In E. Voorhees and L. P. Buckland, editors, Six-teenth Text REtrieval Conference (TREC), Gaithers-burg, Maryland, November.Ludovic Denoyer and Patrick Gallinari.
2006.
TheWikipedia XML Corpus.
SIGIR Forum.Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, andTsunenori Mori.
2007.
An Overview of the 4th Ques-tion Answering Challenge (QAC-4) at NTCIR work-shop 6.
In Proceedings of the Sixth NTCIR WorkshopMeeting, pages 433?440.Chiori Hori, Takaaki Hori, and Sadaoki Furui.
2003.Evaluation Methods for Automatic Speech Summa-rization.
In In Proc.
EUROSPEECH, volume 4, pages2825?2828, Geneva, Switzerland.Valentin Jijkoun and Maarten de Rijke.
2005.
RetrievingAnswers from Frequently Asked Questions Pages onthe Web.
In CIKM ?05: Proceedings of the 14th ACMinternational conference on Information and knowl-edge management, pages 76?83, New York, NY, USA.ACM Press.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic Eval-uation of Summaries Using N-gram Co-occurrenceStatistics.
In Human Technology Conference (HLT-NAACL), Edmonton, Canada.Jimmy Lin and Boris Katz.
2006.
Building a ReusableTest Collection for Question Answering.
J.
Am.
Soc.Inf.
Sci.
Technol., 57(7):851?861.Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, andKatunobu Itou.
2007.
Non-factoid Question Answer-ing Experiments at NTCIR-6: Towards Answer TypeDetection for Realworld Questions.
In Proceedings ofthe 6th NTCIR Workshop Meeting on Evaluation of In-formation Access Technologies, pages 487?492.Radu Soricut and Eric Brill.
2006.
Automatic QuestionAnswering Using the Web: Beyond the Factoid.
Inf.Retr., 9(2):191?206.Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2006.
Data for Question Answering:the Case of Why.
In LREC.Susan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen.
2007.
Discourse-based Answer-ing of Why-questions.
Traitement Automatique desLangues, 47(2: Discours et document: traitementsautomatiques):21?41.451
