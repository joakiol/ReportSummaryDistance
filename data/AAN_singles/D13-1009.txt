Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85?90,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsQuestion Difficulty Estimation in Community Question Answering Services?Jing Liu?
Quan Wang?
Chin-Yew Lin?
Hsiao-Wuen Hon?
?Harbin Institute of Technology, Harbin 150001, P.R.China?Peking University, Beijing 100871, P.R.China?Microsoft Research Asia, Beijing 100080, P.R.Chinajliu@ir.hit.edu.cn quanwang1012@gmail.com {cyl,hon}@microsoft.comAbstractIn this paper, we address the problem ofestimating question difficulty in communityquestion answering services.
We propose acompetition-based model for estimating ques-tion difficulty by leveraging pairwise compar-isons between questions and users.
Our ex-perimental results show that our model sig-nificantly outperforms a PageRank-based ap-proach.
Most importantly, our analysis showsthat the text of question descriptions reflectsthe question difficulty.
This implies the pos-sibility of predicting question difficulty fromthe text of question descriptions.1 IntroductionIn recent years, community question answering (C-QA) services such as Stackoverflow1 and Yahoo!Answers2 have seen rapid growth.
A great dealof research effort has been conducted on CQA, in-cluding: (1) question search (Xue et al 2008; Du-an et al 2008; Suryanto et al 2009; Zhou et al2011; Cao et al 2010; Zhang et al 2012; Ji etal., 2012); (2) answer quality estimation (Jeon et al2006; Agichtein et al 2008; Bian et al 2009; Liuet al 2008); (3) user expertise estimation (Jurczykand Agichtein, 2007; Zhang et al 2007; Bouguessaet al 2008; Pal and Konstan, 2010; Liu et al 2011);and (4) question routing (Zhou et al 2009; Li andKing, 2010; Li et al 2011).
?This work was done when Jing Liu and Quan Wang werevisiting students at Microsoft Research Asia.
Quan Wang iscurrently affiliated with Institute of Information Engineering,Chinese Academy of Sciences.1http://stackoverflow.com2http://answers.yahoo.comHowever, less attention has been paid to questiondifficulty estimation in CQA.
Question difficulty es-timation can benefit many applications: (1) Expertsare usually under time constraints.
We do not wantto bore experts by routing every question (includingboth easy and hard ones) to them.
Assigning ques-tions to experts by matching question difficulty withexpertise level, not just question topic, will makebetter use of the experts?
time and expertise (Ack-erman and McDonald, 1996).
(2) Nam et al(2009)found that winning the point awards offered by thereputation system is a driving factor in user partici-pation in CQA.
Question difficulty estimation wouldbe helpful in designing a better incentive mechanis-m by assigning higher point awards to more diffi-cult questions.
(3) Question difficulty estimation canhelp analyze user behavior in CQA, since users maymake strategic choices when encountering questionsof different difficulty levels.To the best of our knowledge, not much researchhas been conducted on the problem of estimatingquestion difficulty in CQA.
The most relevant workis a PageRank-based approach proposed by Yang etal.
(2008) to estimate task difficulty in crowdsourc-ing contest services.
Their key idea is to constructa graph of tasks: creating an edge from a task t1 toa task t2 when a user u wins task t1 but loses taskt2, implying that task t2 is likely to be more diffi-cult than task t1.
Then the standard PageRank al-gorithm is employed on the task graph to estimatePageRank score (i.e., difficulty score) of each task.This approach implicitly assumes that task difficultyis the only factor affecting the outcomes of competi-tions (i.e.
the best answer).
However, the outcomesof competitions depend on both the difficulty levelsof tasks and the expertise levels of competitors (i.e.85other answerers).Inspired by Liu et al(2011), we propose acompetition-based approach which jointly modelsquestion difficulty and user expertise level.
Our ap-proach is based on two intuitive assumptions: (1)given a question answering thread, the difficulty s-core of the question is higher than the expertise scoreof the asker, but lower than that of the best answerer;(2) the expertise score of the best answerer is higherthan that of the asker as well as all other answer-ers.
Given the two assumptions, we can determinethe question difficulty score and user expertise scorethrough pairwise comparisons between (1) a ques-tion and an asker, (2) a question and a best answerer,(3) a best answerer and an asker, and (4) a best an-swerer and all other non-best answerers.The main contributions of this paper are:?We propose a competition-based approach to es-timate question difficulty (Sec.
2).
Our model signif-icantly outperforms the PageRank-based approach(Yang et al 2008) for estimating question difficultyon the data of Stack Overflow (Sec.
3.2).
?Additionally, we calibrate question difficulty s-cores across two CQA services to verify the effec-tiveness of our model (Sec.
3.3).
?Most importantly, we demonstrate that differentwords or tags in the question descriptions indicatequestion difficulty levels.
This implies the possibil-ity of predicting question difficulty purely from thetext of question descriptions (Sec.
3.4).2 Competition based Question DifficultyEstimationCQA is a virtual community where people can askquestions and seek opinions from others.
Formally,when an asker ua posts a question q, there will beseveral answerers to answer her question.
One an-swer among the received ones will be selected as thebest answer by the asker ua or voted by the com-munity.
The user who provides the best answer iscalled the best answerer ub, and we denote the set ofall non-best answerers as S = {uo1 , ?
?
?
, uoM}.
As-suming that question difficulty scores and user ex-pertise scores are expressed on the same scale, wemake the following two assumptions:?The difficulty score of question q is higher thanthe expertise score of asker ua, but lower than thatof the best answerer ub.
This is intuitive since thebest answer ub correctly responds to question q thatasker ua does not know.
?The expertise score of the best answerer ub ishigher than that of asker ua and all answerers in S.This is straightforward since the best answerer ubsolves question q better than asker ua and all non-best answerers in S.Let?s view question q as a pseudo user uq.
Tak-ing a competitive viewpoint, each pairwise compar-ison can be viewed as a two-player competition withone winner and one loser, including (1) one compe-tition between pseudo user uq and asker ua, (2) onecompetition between pseudo user uq and the bestanswerer ub, (3) one competition between the bestanswerer ub and asker ua, and (4) |S| competitionsbetween the best answerer ub and all non-best an-swers in S. Additionally, pseudo user uq wins thefirst competition and the best answerer ub wins allremaining (|S| + 2) competitions.Hence, the problem of estimating the question d-ifficulty score (and the user expertise score) is castas a problem of learning the relative skills of play-ers from the win-loss results of the generated two-player competitions.
Formally, let Q denote the setof all questions in one category (or topic), andRq de-note the set of all two-player competitions generatedfrom question q ?
Q, i.e., Rq = {(ua ?
uq), (uq ?ub), (ua ?
ub), (uo1 ?
ub), ?
?
?
, (uo|S| ?
ub)},where j ?
i means that user i beats user j in thecompetition.
DefineR =?q?QRq (1)as the set of all two-player competitions.
Our prob-lem is then to learn the relative skills of players fromR.
The learned skills of the pseudo question usersare question difficulty scores, and the learned skillsof all other users are their expertise scores.TrueSkill In this paper, we follow (Liu et al2011) and apply TrueSkill to learn the relative skill-s of players from the set of generated competitionsR (Equ.
1).
TrueSkill (Herbrich et al 2007) is aBayesian skill rating model that is developed for es-timating the relative skill levels of players in games.In this paper, we present a two-player version ofTrueSkill with no-draw.TrueSkill assumes that the practical performanceof each player in a game follows a normal distribu-86tion N(?, ?2), where ?
means the skill level of theplayer and ?
means the uncertainty of the estimatedskill level.
Basically, TrueSkill learns the skill lev-els of players by leveraging Bayes?
theorem.
Giv-en the current estimated skill levels of two players(priori probability) and the outcome of a new gamebetween them (likelihood), TrueSkill model updatesits estimation of player skill levels (posterior prob-ability).
TrueSkill updates the skill level ?
and theuncertainty ?
intuitively: (a) if the outcome of a newcompetition is expected, i.e.
the player with higherskill level wins the game, it will cause small updatesin skill level ?
and uncertainty ?
; (b) if the outcomeof a new competition is unexpected, i.e.
the playerwith lower skill level wins the game, it will causelarge updates in skill level ?
and uncertainty ?.
Ac-cording to these intuitions, the equations to updatethe skill level ?
and uncertainty ?
are as follows:?winner = ?winner +?2winnerc ?
v(tc ,?c), (2)?loser = ?loser ??2loserc?
v(tc, ?c), (3)?2winner = ?2winner ?
[1 ?
?2winnerc2?
w(tc, ?c)],(4)?2loser = ?2loser ?
[1 ??2loserc2?
w(tc, ?c)], (5)where t = ?winner ?
?loser and c2 = 2?2 +?2winner+?2loser.
Here, ?
is a parameter representingthe probability of a draw in one game, and v(t, ?
)and w(t, ?)
are weighting factors for skill level ?and standard deviation ?
respectively.
Please referto (Herbrich et al 2007) for more details.
In thispaper, we set the initial values of the skill level ?and the standard deviation ?
of each player the sameas the default values used in (Herbrich et al 2007).3 Experiments3.1 Data SetIn this paper, we use Stack Overflow (SO) for ourexperiments.
We obtained a publicly available da-ta set3 of SO between July 31, 2008 and August 1,2012.
SO contains questions with various topics,such as programming, mathematics, and English.
Inthis paper, we use SO C++ programming (SO/CPP)3http://blog.stackoverflow.com/category/cc-wiki-dump/and mathematics4 (SO/Math) questions for our mainexperiments.
Additionally, we use the data of MathOverflow5 (MO) for calibrating question difficultyscores across communities (Sec.
3.3).
The statisticsof these data sets are shown in Table 1.SO/CPP SO/Math MO# of questions 122, 012 51, 174 27, 333# of answers 357, 632 94, 488 65, 966# of users 67, 819 16, 961 12, 064Table 1: The statistics of the data sets.To evaluate the effectiveness of our proposedmodel for estimating question difficulty scores, werandomly sampled 300 question pairs from bothSO/CPP and SO/Math, and we asked experts tocompare the difficulty of every pair.
We had twograduate students majoring in computer science an-notate the SO/CPP question pairs, and two gradu-ate students majoring in mathematics annotate theSO/Math question pairs.
When annotating eachquestion pair, only the titles, descriptions, and tagsof the questions were shown, and other information(e.g.
users, answers, etc.)
was excluded.
Given eachpair of questions (q1 and q2), the annotators wereasked to give one of four labels: (1) q1 ?
q2, whichmeans that the difficulty of q1 was higher than q2;(2) q1 ?
q2, which means that the difficulty of q1was lower than q2; (3) q1 = q2, which means thatthe difficulty of q1 was equal to q2; (4) Unknown,which means that the annotator could not make adecision.
The agreements between annotators onboth SO/CPP (kappa value = 0.741) and SO/Math(kappa value = 0.873) were substantial.
When eval-uating models, we only kept the pairs that annotatorshad given the same labels.
There were 260 SO/CPPquestion pairs and 280 SO/Math question pairs re-maining.3.2 Accuracy of Question Difficulty EstimationWe employ a standard evaluation metric for infor-mation retrieval: accuracy (Acc), defined as follows:Acc = the number of correct pairwise comparisonsthe total number of pairwise comparisons.We use the PageRank-based approach proposedby Yang et al(2008) as a baseline.
As described in4http://math.stackexchange.com5http://mathoverflow.net87 	     	     	 	 !
"!# Figure 1: The distributions of calibrated question d-ifficulty scores of MO and SO/Math.Sec.
1, this is the most relevant method for our prob-lem.
Table 2 gives the accuracy of the baseline andour Competition-based approach on SO/CPP andSO/Math.
From the results, we can see that (1) theproposed Competition-based approach significant-ly outperformed the PageRank-based approach onboth data sets; (2) PageRank-based approach onlyachieved a similar performance as randomly guess-ing.
This is because the PageRank-based approachonly models the outcomes of competitions affectedby question difficulty.
However, the outcomes ofcompetitions depend on both the question difficultylevels and the expertise levels of competitors.
OurCompetition-based approach considers both thesefactors for modeling the competitions.
The exper-imental results demonstrate the advantage of our ap-proach.Acc@SO/CPP Acc@SO/MathPageRank 50.38% 48.93%Competition 66.54% 71.79%Table 2: Accuracy on SO/CPP and SO/Math.3.3 Calibrating Question Difficulty acrossCQA ServicesBoth MO and SO/Math are CQA services for askingmathematics questions.
However, these two servicesare designed for different audiences, and they havedifferent types of questions.
MO?s primary goalis asking and answering research level mathemat-ics questions6.
In contrast, SO/Math is for peoplestudying mathematics at any level in related field-s7.
Usually, the community members in MO arenot interested in basic mathematics questions.
If6http://mathoverflow.net/faq7http://area51.stackexchange.com/proposals/3355/mathematicsa posted question is too elementary, someone willsuggest moving it to SO/Math.
Similarly, if a post-ed question is advanced, the community members inSO/Math will recommend moving it to MO.
Hence,it is expected that the ratio of difficult questions inMO is higher than SO/Math.
In this section, we ex-amine whether our competition-based model can i-dentify such differences.We first calibrate the estimated question difficul-ty scores across these two services on a same scale.The key idea is to link the users who participate inboth services.
In both MO and SO/Math, users canspecify their home pages.
We assume that if a us-er u1 on MO and a user u2 on SO/Math have thesame home page URL, they should be linked as onenatural person in the real world.
We successfullylinked 633 users.
They provided 18, 196 answers inSO/Math among which 10, 993 (60.41%) were se-lected as the best answers.
In contrast, they provided8, 044 answers inMO among which 3, 215 (39.97%)were selected as the best answers.
This shows thatthese users reflect more competitive contests in MO.After the common users are linked, we have a jointdata set of MO and SO/Math.
Then, we can calibratethe estimated question difficulty scores across thetwo services by performing the competition-basedmodel on the joint data set.
Figure 1 shows the dis-tributions of the calibrated question difficulty scoresof MO and SO/Math on the same scale.
As expect-ed, we observed that the ratio of difficult question-s in MO was higher than SO/Math.
Additionally,these two distributions were significantly differen-t (Kolmogorov-Smirnov Test, p-value < 0.05).
Thisdemonstrates that our competition-based model suc-cessfully identified the difference between questionson two CQA services.3.4 Analysis on the Question DescriptionsIn this section, we analyze the text of question de-scriptions on the scale of question difficulty scoresestimated by the competition model.Micro Level We first examine the frequency dis-tributions of individual words over the question d-ifficulty scores.
Figure 3 shows the examples offour words in SO/CPP.
We observe that the words?list?
and ?array?
have the lowest mean of difficul-ty scores, compared to the words ?virtual?
and ?gcc?.This is reasonable, since ?list?
and ?array?
are related88(a) Easy questions (b) Normal questions (c) Hard questionsFigure 2: Tag clouds on SO/Math questions with different difficulty levels       !"#"#$%""Figure 3: The frequency distributions of words onthe scale of question difficulty scores (SO/CPP).to basic concepts in programming language, while?virtual?
and ?gcc?
are related to more advanced top-ics.
It can be observed that the order of the means ofthe difficulty scores of these words are well alignedto our learning process.Macro Level We evenly split the range of ques-tion difficulty scores into n buckets, and we groupedthe questions into the n buckets according to whichbucket their difficulty scores were in.
Then, we hadn question buckets and each bucket corresponded toa word distribution of questions.
Let variable X de-note the distance between the difficulty scores in twoquestion buckets (which is the difference betweenthe average difficulty scores of questions in the twobuckets), and variable Y denote the Jensen-Shannondistance between word distributions in two questionbuckets.
We examined the correlation between vari-able X and variable Y .
The experimental resultsshowed that the correlation between these two vari-ables were strongly positive.
Specifically, the cor-relation coefficient on SO/CPP was 0.8129 and onSO/Math was 0.7412.
In other words, when the dis-tance between the difficulty scores of two bucketsbecome larger, the two word distributions in the twobuckets become less similar, and vice versa.We further visualized the word distribution ineach question bucket.
We set n as 3, and we hadthree question buckets: (1) easy questions; (2) nor-mal questions; and (3) hard questions.
Figure 3.4plots the tag clouds of SO/Math questions in thethree buckets.
The size of tags is proportional tothe frequency of tags in each bucket.
We observedthat (1) the tag ?homework?
and ?calculus?
become s-maller from easy questions to hard questions; (2) thetag ?set-theory?
becomes larger.
These observationsalso reflect our learning process.The above experimental results show that differ-ent words or tags of question descriptions reflect thequestion difficulty levels.
This implies the possibil-ity of predicting question difficulty purely from thetext of question descriptions.4 Conclusion and Future WorkIn this paper, we address the problem of estimatingquestion difficulty in CQA services.
Our proposedcompetition-based model for estimating questiondifficulty significantly outperforms the PageRank-based approach.
Most importantly, our analysisshows that the text of question descriptions reflect-s the question difficulty.
In the future, we wouldlike to explore predicting question difficulty fromthe text of question descriptions.
We also will inves-tigate non-technical areas, where there might be nostrongly distinct notion of experts and non-experts.AcknowledgmentsWe would like to thank Yunbo Cao and Jie Cai fortheir valuable suggestions for this paper, and theanonymous reviewers for their helpful comments.89ReferencesM.S.
Ackerman and D.W. McDonald.
1996.
Answergarden 2: merging organizational memory with col-laborative help.
In Proceedings of CSCW.E.
Agichtein, C. Castillo, D. Donato, A. Gionis, andG.
Mishne.
2008.
Finding high-quality content in so-cial media.
In Proceedings of WSDM.J.
Bian, Y. Liu, D. Zhou, E. Agichtein, and H. Zha.
2009.Learning to recognize reliable users and content in so-cial media with coupled mutual reinforcement.
In Pro-ceedings of WWW.M.
Bouguessa, B. Dumoulin, and S. Wang.
2008.
Iden-tifying authoritative actors in question-answering fo-rums: the case of yahoo!
answers.
In Proceeding ofSIGKDD.Xin Cao, Gao Cong, Bin Cui, and Christian S Jensen.2010.
A generalized framework of exploring categoryinformation for question retrieval in community ques-tion answer archives.
In Proceedings of WWW.H.
Duan, Y. Cao, C.Y.
Lin, and Y. Yu.
2008.
Searchingquestions by identifying question topic and questionfocus.
In Proceedings of ACL.R.
Herbrich, T. Minka, and T. Graepel.
2007.
Trueskil-l: A bayesian skill rating system.
In Proceedings ofNIPS.J.
Jeon, W.B.
Croft, J.H.
Lee, and S. Park.
2006.
Aframework to predict the quality of answers with non-textual features.
In Proceedings of SIGIR.Zongcheng Ji, Fei Xu, Bin Wang, and Ben He.
2012.Question-answer topic model for question retrieval incommunity question answering.
In Proceedings ofCIKM.P.
Jurczyk and E. Agichtein.
2007.
Discovering au-thorities in question answer communities by using linkanalysis.
In Proceedings of CIKM.B.
Li and I.
King.
2010.
Routing questions to appropriateanswerers in community question answering services.In Proceedings of CIKM.B.
Li, I.
King, and M.R.
Lyu.
2011.
Question routing incommunity question answering: putting category in itsplace.
In Proceedings of CIKM.Y.
Liu, J. Bian, and E. Agichtein.
2008.
Predicting in-formation seeker satisfaction in community questionanswering.
In Proceedings of SIGIR.J.
Liu, Y.I.
Song, and C.Y.
Lin.
2011.
Competition-baseduser expertise score estimation.
In Proceedings of SI-GIR.K.K.
Nam, M.S.
Ackerman, and L.A. Adamic.
2009.Questions in, knowledge in?
: a study of naver?s ques-tion answering community.
In Proceedings of CHI.A.
Pal and J.A.
Konstan.
2010.
Expert identification incommunity question answering: exploring question s-election bias.
In Proceedings of CIKM.M.A.
Suryanto, E.P.
Lim, A.
Sun, and R.H.L.
Chiang.2009.
Quality-aware collaborative question answer-ing: methods and evaluation.
In Proceedings of WSD-M.Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft.
2008.Retrieval models for question and answer archives.
InProceedings of SIGIR.Jiang Yang, Lada Adamic, and Mark Ackerman.
2008.Competing to share expertise: the taskcn knowledgesharing community.
In Proceedings of ICWSM.J.
Zhang, M.S.
Ackerman, and L. Adamic.
2007.
Ex-pertise networks in online communities: structure andalgorithms.
In Proceedings of WWW.Weinan Zhang, Zhaoyan Ming, Yu Zhang, Liqiang Nie,Ting Liu, and Tat-Seng Chua.
2012.
The use of depen-dency relation graph to enhance the term weighting inquestion retrieval.
In Proceedings of COLING.Y.
Zhou, G. Cong, B. Cui, C.S.
Jensen, and J. Yao.
2009.Routing questions to the right users in online commu-nities.
In Proceedings of ICDE.Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.
2011.Phrase-based translation model for question retrievalin community question answer archives.
In Proceed-ings of ACL.90
