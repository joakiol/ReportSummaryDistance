Comput ing  Phrasa l - s igns  in HPSG pr ior  to Pars ingKentaro  Tor i sawa and  aun ' i ch i  Tsu j i il )c i )ar tmenl ;  of In format ,  ion S(:icncc, Un ivers i ty  of Tokyo ,l tongo  7-3-1, Bunkyo-ku ,  Tokyo ,  113, Japan{tor i sawa, tsu j i i}0 is .o .u - ' cokyo .ac .
jpAbstractThis t)ai)er deseril)es techniques to com-pile lexical entries in I IPSG (Pollard andSag, 1.987; Poll;ml and Sag, 1993)-stylegrammar into a set of finite state au-tomata.
The states in automat~L arepossible signs derived fl'om h',xical en-tries and contaili nformation raised fl'omthe lexical entries.
The automatt~ areaugmented with feature structures use(lby a partial unification routine and de-layed/frozen definite el;rose programs.1 introductionOur aim is to 1)uild an e, fli(:ient and robust \]tl)SG -based parser.
I IPSG has 1)(;eu re, gar(led as a so-phisticated but fl:;tgile and inettieient ff~mwwork.However, its principle-based al'(:hitecture nablesa parser to handle real world texts only by giv-ing concise core grammar, including principles andtemplates for lexicM entries, defau l t  lexical en-tries(Horiguchi et al, 1995).
The architecture isdifferent fl'om those of eonvelltional unification-l)t~sed ff)rmMisms which require hundreds of CFGskelet;ons to t)arse real world texl;s.However, tiles(', design prin(:il)les of l l I 'SG havedraw-backs in parsing cost.
That is, signs/featurestructures corresponding ~o non-termillal symbolsill CFG become vi,sible, only after applying t)l'inci-pies a l ld  ~t t)&rs0r has  to  Cl'e~4te ~(;&tlIl;C st l ' I lCt l l leS()lie by one using unification.
\]in addition, identitychecking of non-terlninM symbols used to elimi-nate spurious signs must be replaced with sub-sumption checking, which flHther detcrior~tes ef-fi('ien(:y.Our grammar eompih',r COlnputes keleta.l 1)~rtof possible phrasal-signs froln individual h!xicalenl;l'ies prior to parsing, and generates a set offinite state automata from h'~xical entries to ;tvoidthe above draw-I)acks.
We call this operation Oil-l ine ra is ing and an automaton thus generated iscalled a Lexieal  Ent ry  Automaton  (LA).
itsstates corresponds to 1)art of sigl,s and each tran-sition between stales (;orrespon(ls to at)plication ofa rule schema,  which is a nonqexical comt)onentof grammar.Our parsing algorithm adopts a two-i)hasedl/arsing method.Phase  1 \]iottom-up (:hart-like 1)~rsing with LAs.Ilewriting lt,ule:MOTIIEI{(\[1\]), IIEAI)-I)TR (\[81) NON-IIEAt)-I)TI~(\[S\])FS:sign \ [ '  ...... 't tSy l l  b;I I } ) C ~)a,content\[4\]scm iIKlices \[3\], ...... ~ .
\]1 ...... l-dr,, \[s\] '~Y" \[ .~,l,,:~,: ( ,~ rl>S(!
l l l  i nd icesindices \] Jgoals arg2 2arg2fl '(!t!z(~Figur(' 1 : AlL e.xalnph; of a rule s(:hema.Phase  2 Computing part of feature, structureswhich cannot l)e (:omputcd at (;oml)ile-tinm.We call tile tbature structures that are repre-sented as states in automa,t;~ mtd are COml)utedat conlpih>time Core -s t ruc tures ,  and the fca-tllre strllCtlll'es whi(;h are t;o l)e (:Olnl)ut, ed in Phase2, Sub-s t ruc tures .
In l)h~sc 1 parsing, t~ (:ore-si, ructme.
(:orr(,spond to a state in an \] ,A.
The costof comt)uting sub-structures at Phase 2 is inin-imized by Dependency  Ana lys i s  mM Par t ia lUni f icat ion.Tile next section describes rule schcmtm~, cen-tral eompouents of the.
formalism, and gives ~ def-inition of Definite Clause Programs.
Section 3 de-scribes how to obtain LAs h'om lexical entries andhow to perform the Phase i p;~rsing.
Section 4 ex-plMns the Phase 2 Parsing algorithm.
A parsingexmnple is ln'es(;iLted in Section 5.
The effective-ness of our method is exeinplified with a series (117eXl)eriments in Section 6.2 Rule Sch(:nmta and DefiniteClaus( ;  l ) rogramsOur fonmdism has only one type of compolmntg-tS llOll-10xic&l (;ollll)Ollellt,q of  ~ra l l l l nar ,  i.e., ruleschemata .
I An example is showll in Figure 1.
Aruh' s(:henl;~ consists of the following two items.Ih l  ()Ill' cilrr(!ilt, sysLeil/~ rill(', s(;h(2mltt~ l'('.
goltc.r-i~t('.
(1 froul principh,.s and rewriting rules ~m(:ording (;c);L SlmCifical, ion given by ~ progr~mmter.949ru le (R)  a rewriting rule without specific syntac-tic categories;fs(R)  a feature structure.A characteristic of HPSG is in the flexibilityof principles which demands complex operations,such as append or subtraction of list-value featurestructures.
In our formalism, those operations aretreated by a Definite Clause Program.
A DCP canbe seen as a logic program language whose argu-ments are feature structures.
An auxiliary term, aquery to a DCP augmenting a rule schema, is em-bedded in a feature structure of a rule schema asthe value of goals .
The rule schema in the exam-pie has an auxiliary term, append(\[1\], [2\], \[3\] ).The bottom-up application of the rule schemaR is carried out as follows.
First, two daugh-ter signs are substituted to the HEAD-DTP~ positionand NOR-HEAD-DTI~ position of the rewriting rulerule(R).
Then, the signs are unified with thehead-dt r  value and the non-head-dt r  value ofthe feature structure of the schema, fs(R) .
Fi-nally, the auxiliary term for DCPs given in theschema is evaluated?Our definition of a DCP has a more operationalflavor than that given by Carpenter(Carpenter,1992)?
The definition is crucial to capture the cor-rectness of our method.
2Def in i t ion 1 (DCP)  A definite clause program(DCP) is a finite set of feature structures, each ofwhich has the following form.goals (HI 1-13) Jnext -s teps  \[ goaSs (1~o, B1 , ' ' ' ,  ~ ,  \[1\] ) \]a where 0 <_ n and H, Bo," ?
?, B,~ are feature struc-tures.A feature structure of the above form corre-sponds to a clause in Prolog.
H, B0 , .
.
.
,B~ cor-responds to literals in Prolog.
H is the head andBo," ' ,  B,~ are literals in the body of a clause.Def in i t ion 2 (Execut ion  of DCP)  Executionof a DCP P for the query,C2~e~y = \[ go~Ss (qo, q~,'" qz) \]is a sequence of unification,Query U rl U r2 U ... Urnwhere ri = \[ (next -s teps)  i-1 Ci \], C, ?
P orCi = \[ goa ls  0 \].
/f  the execution is terminated,C~ must be unifiable with\[ goa ls  () \].
In thiscase, we call the sequence (r l , ' " , r ,~} a reso lu -t ion  sequence .2Though, through the rest of the paper, we treatthe definition as if it were used in an actual implemen-tation, the actual implementation uses a more efficientmethod whose output is equivalent with the result ob-tained by the defiifition.a (H0,.- ' ,  H~, \[1\]) is an abbreviation ofres t  " ' res t  \[1\]E21\[ ?
N ~,bcat 0My colleague\[ sig,!
~d \]S 2 maJ subcat ('ql ms.) Vsubcat (\[2\]NP)ln~i sutJcat (~m a j  Vsubcat (\[lJNI',\[2\]NP)wrote a good paperFigure 3: A parsing example(next -s teps)  i-1 \ [goals of QueryHrl Hr2 H. ?
?
Hri represents the goals which are to be solved inthe steps following the i-th step.
The goals areinstantiated by the steps fi'om the first one to i-thone, through structure sharings.
The result of ex-ecution in a Prolog-like sense appears in the query,Figure 2 is an example of execution for the queryappend( \ [a \ ] ,  \[b\] ,X) , whose definition is basedon a standard definition of append in Prolog.Given this definition of DCPs, an applicationof a rule schema to two (laughter signs D1 andD2 can be expressed in the following form, where@1, r2 , . "
,  r,~} is a resolution sequence:M= \[ head-dtrnon_head_dtr D~D2 \]Ufs(R)U','tUr.2U...Ur,~3 Lex ica l  Ent ry  AutomataThis section presents a Lexical  Ent ry  Automa-ton  (LA).
The ineifieiency of parsing in HPSGis due to the fact that what kind of constituentsphrasal-signs would become is invisible until thewhole sequence of applications of rule schematais completed.
Consider the parse tree in Figure3.
The phrasal-signs $1 and $2 are invisible untila parser creates the feature structures describingthem, using expensive unification.Our parsing method avoids this on-line con-struction of phrasal-signs by computing skeletalpart of parse trees prior to parsing.
\[n Figure3, our compiler generates $1 and $2 only fromthe lexical entry "wrote," without specifying thenon-head daughters indicated by the triangles inFigure 3.
Since the non-head aughters are token-identical with subcat  values of the lexical entryfor "wrote", the obtained skeletal parse tree con-tains the information that St takes a noun phraseas object and $2 selects another noun-phrase.Then unifying those non-head daughters with ac-tual signs constructed from input, parsing can bedone.
An LA expresses a set of such skeletal parsetrees.
A state in an LA corresponds to a phrasal-sign suc h as Sj and $2.
They are called core-s t ruc tures .
A transition arc is a domination linkbetween a phrasal-sign and its head daughter, andits condition for transition on input is a non-head950Qt le ry  :P l"Ogt'iUll:I,'~xecut io t :Q10 (J2 =:C; I =: goalsllext-stepsgoals1 ...../ i~l'g, 1 (}/ lll'g~ \[2\] I\[\]\] ,L ,,,.ga \[21 m,,,l.~ \[J\]\]next -s teps.
.
.
.
.
..-,,.~a \[ (!~'q\]\[l\[~i) \]}argl 4 \[ e-list \]arg'3 6&rgl  ( : goals arg2(21 I I ( ;2 U \[ nca:t-.sL,ep.s C I \] =:( goals( '2  =next-stepsl\[7\]\[ (,-list 1}\[ ;~, \[\[{!\[)e list \])\[ (2 31\[8\] \](\[q\[ ,, }))a,'g2 r , (f:~t\[ \ 4 \ ] ) \ [ \ [7 \ ]gl 'ga ( \ [3 J l \ [~})goals arg2 4 i\[r\]re'g3I\[r)\[ e-list \ ] )next -s teps/ ,u~J 4 \[ ,<i.~t \] goals / arg2 5 b.... xt-steps goals <) \]I\[r\]\[ ,!-li,~t \ ] )  \]Figure 2: An examl)le of DCP 's  executiondaughter, such as signs tagged \[1\] and \[2\] in Fig-ure 3.
Kasper c.t al.
1)resented an idea similar tothis @l ine  raising in their work on HPSG-TAGcompiler(Kasper et al, 1995).
The difference, isthat our algorithm is based ou substitution, notadjoining, Furthermore, it is not clear in theirwork how offl ine raising is used to improve ef\[i-cicncy of parsing.Before giving the definition of LAs, we detinethe notion of a quasi-sign, which is part of a signand constitutes l~As.Def in i t ion  3 (quas i - s ign(n) )  For a given inte-ger n, a fcatu,e structure S is a q'aasi-sign(n)if it has some of tile following four attributes:syn, sem, head-dt r ,non-head-dt r  and doesnot /Lave values for the paths (head-dt r  +non-head-dtr)"".A qua,.si-sign('n) cannot rel)resent a parse treewhose height is inore than n, while a sign canexpress a parse tree with any height.
Tlm)ugh therest of this 1)aper, we often extract a quasi-sig"n.(n)S from a sign or a quasi-sig',,(,n/) S' where '., <n'.
This operation is denote(l by S' = c'x(S',,n).This means that 5' is equivMent to S' except ff)rthe attr ibutes head-dt r  mM non-head-dt r  whoseroot is the (head-dtr + non-head-dtr)  '~ value inS'.
Note that S and S' are completely differententities.
In other words, S and S' pose differentscopes on structure sharing tags, in addition, wealso extract a feature structure F reached by apath or an attr ibute 1) in a feature structure IP'.We denote this by F = va l (F ' ,p )  and regard Fand F'  as different entities.Def in i t ion 4 (Lex ica l  Ent ry  Auton,  a ton(LA) )A Lezical Entry Automaton is a tuplc (Q,A,qo}whel'e~Q:  a set of states, where a .state is aquasi-sign(O).A : a ,set of transition arcs between states, wherea transition arc is a tuple (qd, q .... N ,D ,R)where qd, q,.
6 Q, N is a quasi-sign(O), D isa quasi -s ign( I )  and R is a rule schema.qo : tile initial state, which corresponds to a lezi-cal erLtry.In a transition :-tt'(; < qd, q ..... N, D, 1~} , q,~ denotesthe destination of the transition arc, and qd is theroot of the arc.
The N is a non-head daughterof a l)hrasal-sign, i.e., the destination state of thetransition, and expresses the input condition forthe transition.
The D is used to represe, nt: the de-pendency 1)etween the nn)ther sign and the daugh-ters through structure sharings.
This is called aDependency  Feature  S t rueture(DFS)  of thetransition arc, the role of which will be discussedin Section 4.
1~, is the rule schema used to createthis arc.An LA is generated fl'om a lexieal entry l by thefollowing recursive pro(:edure:1.
Let; ,~; 1)e {/}, A be an eml)ty set and sd = /2.
For ea(:h rule, schema 1~, and for each of itsea(:h resolution sequence (rl , .
.
.
, 'r ,~} obtain,1) - \[ head-dt r  ,Sd \]uf.s(l~) u r, u .-.
o r,~and if l) is afeature structure, obtain s,, = ex(D,O) andN = ex(w~l(D, non-head-dt r ) ,  0).a.
If D is a t~ature structure,?
If the, re is a state s~,~ 6 S such that s',~,s .... 4 let s,~ be s~,~.
Otherwise, add s,,~to 5".
* If there is no T'r = \'/~"d, '~,,~"', N" ,  D",  1~)A such that .%~ ~ s{',~, s,z ~ sSl, N4For ~my feature structures f ~md f' ,  f ~ f '  ifff E f '~md f '  E f951Phase2-proc-dcp(e : dge);assume  = (1, r, S, Dep)return S U sub-structure(e)sub-structure(e : edge);assmne e = (l, r, S, Dep)If Dep = 4)then return sub(S),elsefor each (D, eh, e~, R) C Dep,assume that el~ = (lh, rh, Sh, Deph)and e~ : (In, r~, Sn, Dep,~)Sh := sub-structure(eh),S,~ := Sn U sub-strueture(e~)If neither of Sh and Sn is n i l ,s%bo :~fv(dep(D) u sub(fs(R)),\[ head-dt r  Sh \]non-head-dtr S n 'rs)............................... (A)for each resolution sequence,rd,sub := .sub0 LIT 1 I~ ?
?
?
U Ti............................... (B)If sub is not a feature structure oreither of Sh or S~ is n i l ,then return n i lelse return subFigure 4: A recursive procedure for tile Phase 2N"  and D ~ D", then, add the tuple(s,t, s,,~, N, D, R) to A.4.
If the new quasi-sign(O) (s,~) was added toS in the previous tep, let sd be s,~ and go toStep 2.When this terminates, (S, A, l) is the LA for 1.The major difference of Step 2 and thenormal application of a rule schema is thatnon-head-dt r  values are not specified in Step 2.In spite of this underspecification, certain partsof the non-head-dt r  are instantiated becausethey are token-identicM with certain values of thehead-d%r domain.
By unifying non-head-dt rvalues with actual signs to be constructed fl'om in-put sentences, a parser can obtain parsing results.For more intuitive explanation, see (Torisawa andTsujii, 1996).However, this simple LA generation algorithmhas a termination problem.
There are two poten-tial causes of non-termination.
The first is thegenerative capacity of a feature structure of a ruleschema, i.e., a rule schema can generate infinitevariety of signs.
The second is non-termination ofthe execution of DCP in Step 2 because of lack ofconcrete non-head daughters.For the first case, consider a rule schema withthe following feature structure.head-dt r  syn  \[ counter  \[1\] \] \]Then, this can generate an infinite se-quence of signs, each of which contains a part,\[ counter  <bar, ba, r , .
.
.
,bar)  l and is not equiv-alent to any previously generated sign.
In orderto resolve this difficulty, we apply tim res t r i c t ion(Shieber, 1985) to a rule schemata nd a lexicalentry, and split the feature structure F = fs (R)of a rule schema R or a lexical entry F = l,into two, namely, core(F) and sub(F) such thatF = core(F) U sub(F).
The definition of the re-striction here is given as follows.Definition 5 (paths)  For arty node n in a fea-ture structure F, paths(n,F)  is a set of all thepaths that reaches n from the root of F.Definition 6 (Restriction Schema) Arestriction schema rs is a set of paths.Definition 7 (Res)  F '  = Res(F, rs) is a ma.~;i-real feature structure such that each node n in F ~satisfies the following conditions.?
The~ is a node no in f: such thatpaths(no,F) = path.s(n,F') and type('n) =t?tpe(no).?
For any p C paths('n,F'), there is no pathp,, 6 rs which prefixes p.Res eliminates the feature structure nodeswhich is specified by a restriction schema.
For acertMn given restriction schema rs, eore(fs(l~,)) -=Res( fs (R) , rs )  and sub(fs(R)) is a mini-mM feature structure such that core( fs (R) )Usub(fs(R)) = fs(R) .
Tile nodes eliminated byRes must appear in sub(fs(R)).
In tile example,if we add (syn, counter} to a restriction schemaand replace f s (R)  with eorc(fs(.R)) in the Mgo-rithm for generating LAs, the termination prob-lenl does not occur because LAs can contain a loopand equivMent signs are reduced to one state inLAs.
The sub(fs(R)) contains the syn lcounter ,and the value is treated at Phase 2.The other problem, i.e., termination of DCPs,often occurs because of underspecification f thenork-head-dtr wines.
Consider the rule schemain F igure 1.
The append does not terminate atPhase 2 because the ind ices  value of non-head(laughters is \[ ?
\].
(Consider the case of execut-ing append(X, (b),Y) in Prolog.)
We introducethe .freeze Nnctor in Prolog which delays theevaluation of the second argument of the func-tors if the first arguruent is not instantiated.
Forinstance, f reeze  (X, append(X, \ [b\] ,  Z) ) means todelay the ewfluation of append until X is instan-tinted.
We introduce the functor in the followingforln.goals arg2 (flarg3 \ [~freeze \]This means the resolution of this query is notperformed if \[1\] is \[?\].
The delayed evaluationis considered later when tile non-head-dt r  val-ues are instantiated by an actual sign.
Note thatthis change does not affect the discussion on thecorrectness of our parsing method, because thedifference can be seen as only changes of order ofunification.Now, tile two phases of our parsing algorithmcan be described in more detail.Phase  1 : Enumerate possible parses or edges ina chart only with unifiability checking in abottom-up chart-parsing like manner.952Phase  2 : For comt)leted parse trees, computesub-structures by DFSs, ,sub(fs(R))  for eachschema R and frozen 1)C1 ) programs.Note that, in \['has(; 1, unification is replacedwith nnifiability checking, which is more efficientthan unification in terlns of space an(l time.
Theintended side effect by unification, such as build-ing up logical forms in sere va lues ,  is COmlnttedat Phase 2 only for the parse trees covering thewhole input.a.1 Phase  1 Pars ingThe Phase~ 1parsing algorithm is quite similar to abottom-up chart parsing for CFG.
The Mgorithmhas a chart and edges.Def in i t ion 8 (edge)  An edge is a tupla(1, r ,  S, l)ep) where,?
1 and r arc.
vertexes in the chart.?
S is a slate of an LA.?
.l)ep i.s a .set of tuples in the form of(D, eh, c,,, ll} wh, e, rc.
eh a7%d Cn aTY; (:dges, \])i.s a quasi-.sign(I) and R is a rule .schema.The intuition behind this definition is,?
?'
l)lays the role of a non-/termimd in CFG,though it is actually a quasi-sign(O).?
ch and e,~ denote a head daughter edge and anon-head daughter edge, respectively.?
Dep represents the dependency of anedge and its daughter edges.
Where(D, eh,c,~,l~} E Dcp, D is a DIeS of a tran-sition arc.
Basi(:ally, Phase 1 parsing createsthese tuples, and \])hase 2 parsing uses them.The Phase 1 parsing (:onsists of the folh)wingsteps.
Assume that a word in i nput  \]n~s a lexicalentry L~ and that an LA (Q,;,A,,q~) generatedfi'om Li is attached to the word:1.
Create an edge li -= (j.i,ji + 1,q~,()) in thechart for each Li, for at)propriate .ji.2.
For an edge e. 1 whose state is q~ in the chart,pick u t) an edge e2 which is adjacent to eland whose state is q~.3.
For a transition arc (ql, q, N, D, ll), check ifN is unifiable with q2.4.
If the unifiability check is successful, find anedge (l = ('m,d,'n,d,q, Depd) strictly coveringel and e2.5.
if there is, replace d with a new edge(m,,,'na,q, Dep,z U {(D,c, ,eu,B)})  it) the.chart.6.
Otherwise, createa new edge (Tn, n, q, {(D, el, e2, R)}) strictlycovering el and e2.7.
Go to steI) 2.4 Phase  2 Pars ingThe algorithnl of Phase 2 parsing is given inFigure 4.
The procedure sub-.structure is a re-cursive 1)rocedure which takes an edge as in-put and builds Ul) sub-structures, which is dif-fer'ential feature structures representing modifica-tions to core-structures, in a bottoln-U 1) nlanner.The obtained sub-structures are unified with core-structures when 1) the input edge covers a wholeinput or 2) the edge is a non-head daughter edgeof sonm other edge.
Note that the .~ub-struet'aretreats sub( fs (R) ) ,  a feature structure eliminatedl)y the restriction in the generation of LAs, (the(A) 1)art in Figure 4) and frozen goals of DCPs,by additional ewduation of DCPs.
(the (B) part)Here, we use two techniques: ()tie is dependencyanalysis which is eml)odied by the function dep inFigure 4.
The other is a partiM unification routineexpressed by p_nn i fy  in the figure.The del)endency analysis is represented withthe function, dep(F,'rs), where F is a DFS andrs is a restriction schema used in generation ofLAs:Def in i t ion 9 (dep)  For a feature structure \["'and the.
restriction schema r.s, F = dep(l  c~,r,s)is a maximal fc.atu're~ structure such O~,at any 'node'n in F sati,~fies the conjunction of th, e. followingtwo conditions:t. There is a node n' in f i'' ,such, thatv(tm.+, .
,  P)  - ~), ,m.,+, , ' ,F ' )  a,.Z t:,mc.
(7,0 :=typc(n').2.
Where A) ha.
= 'n or B) n,t is a descendan?of n, pa, ths(n,z,F)  contains a path.
prefixedby one of (head-dtr), (non-head-dtr) and<goa:ts>.3.
The diajunetion of the following three condi-tions is satisfied where A)  n,t = n or B) 'n(tis a descendant of n.?
For .some p G pa, th, s(7t~l,F), there  i.s apath, p,,.
E 'rs wh, ieh prefixes p.?
Some p ~ p.,th,@n,t,F) is prefixed by(~m.,ls).?
7'here is no node 'n.
in F .~'uch th, ati) there is paths Pi,7)'2 ~ paths('n<,., f;')such that Pi is prefixed by (syn) 07'(sere) aTtd P2 is 'p'r'efi;Le.d by (head-dtr)Or (non-head-dtr>, and i/) for a~ty p Gpaths(rid, F)  there is p,~ E path..s(n,~, F)which prefixes p.Roughly, dep eliminates 1) the descendantnodes of the node which apl)ears both in syn/semdomains and head-dt r /non-head-dt r  domainsand 2) the nodes at)peering only in syn/sem do-mains, excet)t for the node which el)pears ins'ab(fs(\]?))
or goa ls  domains.
In other words,it removes the feature structures that have I)eenalready raised to core-structures or other DFSs,ex(:ept for the structure sharings, and leaves thosewhich will be required by DCPs or xub( fs (R) ) .p_uni f y( Fl , F.2 , r s ) is a partial unification rou-tine where Fl and F2 are feature structures, andrs is a restriction schema used in generation ofLAs.
l{oughly, it performs unification of F, andl'12 only for common part of Ft, F.2, and it pro-duces unified results only for the node 'n in Fl ifs'nj is ~t descendant of 'n2 in a feaiure structur{~ l,'i l l  'nt # n2, and the.r('.
~u:e paths  Pl 6 path, s(~,,l, \ [" )  ~HldI)'2 E pa, th, s(n.2, l"),  nnd p2 l)r('.fixes p l .953phon "wrote"syn , o rv , \ ]  .... .... subcat <NI'\[1\],NP\[2\])rein wrote sere content agentobjectindices 0Figure 5: A lexical entry for "wrote"$2 ?
A StateP b  T :N  T2:N S1 A Transition ArcT I :NP  (N denotesL a non-head-dtr.
)Figure 6: The LA derived from "wrote"n has a counter part in F~.
More precisely, it pro-duces the unification results for a nod(; n in Fjsuch that?
there is a path p ~ paths(n, I~) such that thenode reached by 1) is also defined in F2, or?
there is a path p ~ paths(n, F1) prefixed bysome p,, C rs or (goals).Note that a node is unified if its structure-shared part has a counter-I)art in F2.
Intuitively,the routing produces unified results for the part ofFi instantiated by /7'2.
The other part, that is notproduced by p_unify, is not required at Phase 2because it is already computed in a state or DFSsin LAs when the LAs are generated.
Then, a signcan be obtained by unifying a sub-structure andthe corresponding core-structure.5 ExampleThis section describes the parsing process of thesentence "My colleague wrote a good paper."
TheLA generated fronl the lexical entry for "wrote"in Figure 5 is given in Figure 6.
The transition arcT1 between the states L and S1 is generated bythe rule schema in Figure 1.
Note thai; the queryto DCP, freeze(\[1\], append(Ill, \[2\], \[3\])), is used toobtain union of indices values of daughters and theresult is written to the indices values of the mothersign.
During the generation of the transition arc,since the first argument of the query is \[ ?
\], it isfrozen.
The core-structures arid the dependency-analyzed DFSs that augment he LA are shownin Figure 7.
We assume that we do not use anyrestriction, i.e., for any lexical entry l and ruleschenaata 2~, s,bb(1) ~-\[?1 and sub(fs(I{)) = \[?1.Note that, in the DFSs, the already raised fea-ture structures are eliminated and, that the DFSof the transition arc T contains the frozen queryas the goals .Assmne that the noun phrases "My colleague"and '% good paper" are already recognized by aparser.
At phase 1, they are checked if they areunifiable to the condition of transition arcs T1 andT2, i.e., the NPs which are non-head daughters$2synsenl5'1synhead \[ ..... ior V \] \]subcat 0 \]rehl wrote content agent ?object ?indices ?head \[ major V \] \]subcat (NP\[2\])rein wrote senl content agent \]objectindices A_The dependency-anMyzed DFS of T2syn \[~ ..... l \[s\] \]selnindices \[a\]- synSign 1 \] .... \]h~ad-dtr \[a\] s~beat .\[qN;'\[r\])Z\ [}  t f l l la jor  ..... syn subcttt ) non-head-dtr \[5\]sere indicesgoals arglarg2arg2The dependency-analyzed DFS of T1head 8\]syn subcat {/10\]?\[9\]}content \[6\] agent sere objectindices \[3\]?
signsy1 ..... I-dtr \[3\] 9\])..... \[ ic'::~itceet: t \[2 t (, \]syn \[ t subcat ) non-head-dtr \[5\]sere indices 1 ..... \]> freeze 1goals argla,-~2 ()arg2Nll\]N,I\]Figure 7: States and DFSs in tim LA in Figure 6The sub-structure for $2content | agen~ 1\]my_colleaque |..... \[ obje,:t \[2\]good_p.vJ4' Jindices {\[llmy_collea.o .
.. \[2\]good_paper)The sub-structure for S1sere \[ object \[1\]good_paper \]indices (\[1\]good_paper)The goals,head-dtr,non-head- dtr vMues are omitted.Figure 8: The sub-structures obtained in the pars-ing954__Parsing ~ i thn lPhase 1 onlyPheuse 1 & Phase 2Phase 1 & Phase 2~ f - ~naive application of rule sche, matanaive application of rule s<:hentata~\['ylTe.
-6f sent;ences(# of sentences)glT 70  - - - -<ufly successful~7~)enD.
s ~ s ~Ass)onTy ~fuT- -  -_ (~)_  .
.
.
.
.
.
.19.219.218.817.133:\[.4Av 3n< e&1.25 ~L121_3.00 ~1.65)85.091093.22 ~2.1~A bracketed t ime indicates non-(~(\] execution tim(< '\]'he Xl)erimeuts was l)(nformed on SparcStat ion 20 with 128 MI) I IAMFigure 9: Ext>eriments on a Japanese newsl)aper(Asahi Shinl)un)<)f $1 and $2.
Since all l;he u,dfial)ility <:lwx:k-ings ,'/.1"o successful, Phase 1 parsing produ(:es theparse tree whose form is presented in Figure 3.The Phase 2 1)arsing produces the sub-structuresin Figure 8.
Note that the frozen goals are eval-uated and the ind ices  wdues have al)prot)riatevalues.
A l)arsing result is obtaine{l by unifyingthe sub-structure for 5"2 with tim correspon<lingcore -s t ruc t l l re .The amount of the feature stru<:ture nodes gen-erate(1 during t)arsing are r(~<lu(:e(1 :<m~t>are(l tothe case of the naive at)l)lication of rule schematapresented in Section 2.
The important point isthat they contMn only either the part iu theDFSs that was instantiated by head daughters'sub-structures, and non-head daughters' core-structures and sub-structures, or the part thatcontributes to the DCP's exaluation.
The featurestructure that does not al)pear i ,  a sub-structureappears in the corresponding core-structure.
Se, eFigure 7.
Because of these 1)rot>erties, the correct-ness of our parsing nmthod is guaranteed.
('lbri-sawa and Tsujii, 1996).7 Conc lus ionWe have lu'esented a two-phased t)arsing nlethodtor HPSG.
In the first l)hase,, our 1)arser pro-duces parse trees using Lexical Entry Automntacompilcxl from lexical entries, in the secondphase, only the feature structures whi<:h luust \])e(:ompute(\[ dynamically are (:omputed.
As a re-suit, amount of the fl;ature structures unifie<l at1)arsing-time is reduce.d.
We also showed the el'-feet of our optinfization te(:hniques by a series ofexl)erinwats <m a real world text.\]t can l)e noticed that ea<:h transition arc of timcOral)ileal l,As can be seen as a rewriting rule inCFG (or a dott;ed notation in a chart parser.)
Webelie.ve this can Ol)en the way to integrate severaJn,et;hods deveh)l>ed for CI,'G, including the inside-outside algorithm tot grmmnar learning or disambiguation, into an HPSC, framework.
We also 1)e-lieve that, by pursuing this direction for optimiz-ing ttl)SG parsers, we can reach the point whe.regrammar learning from corl)ora can be done withconcise, and linguistically well-defined (:ore grant-Itt;tr.6 Ex I )e r imentsWe have implenmnted our parsing metho<l inCommon Lisp Ol)je<:t Systen~.
hnprovenmnt byour method has /)een measured on 70 ra.ndonflyselected Japanese sentences from a newsl)at)er(Asahi Shinbun).
The used grammar (',onsists ofjust 5 rule schemata, which are generated fl'omprinciples and rewriting rules, aim 55 default lex-ical entries given for each part of speech, with 44manually tailored lexical entries.
The total num-ber of states in the LAs compiled fl'oln them was1490.
The grammar does not have a semanticpart.
The results arc.
l)resented in Figure 9.
Ourgrammar produ<:ed l>ossil)le parse trees for 43 sen-ten<'.es (61.4%).
We compared the.
execution timeof our I)arsing method and a more naive algorithm,which l)erforms Phase 1 parsing with LAs and al)-plys rule s(:hemata to (:olnph'.ted pars<; trees in thenaive way described in Se<:tion 2.
As the.
naive al-gorithm caused thrashing for storage in GC, it ispointless to compare those tigures simply.
How-ever, it is obvious that our method is much fi~sterthan the naive one.
We could not measure the ex-ecution time for a totally naive algorithm whicht)uilds parse trees without LAs because of Uwash-ing.ReferencesBob Carl>enter.
1992.
The Looi<: of "/\]qped F.a,t..,.+'.Str'ucturcs.
Cambridge University Press.Keiko Horiguchi, Kentaro Torisawa, and Jun'ichiTsujii.
1995.
Automatic acquisition of cont(;ntwords using an IIPSG.-based parser.
In NL-1"1~S'95.Robert Kasper, Bernd Kiefer, Klaus Netter, andK.
Vijay-Shanker.
1995.
Compilation of I IPSGto TAG.
In ACL 95.Carl Pollard and Ivan A.
Sag.
1987. h~,fovmatio,,-Based Syntaz and Semau, ties Vol.
1.
CSLI lec-ture notes  11o.1 3.
(,'arl Pollard and Ivan A.
Sag.
1993. lh..a.d-Driv<'.n Phrase Structure Grammar.
Universityof Chicago Press an(l CSLI l)ul)li<:ations.Stuart C. Shieber.
1985.
Using restri<:tion toextend I)arsing algorithms for conq)lex featurebased formalisms.
In A CL85.Kentaro Torisawa and Jun'ichi Tsujii.
1996.
()if-line raising, dei)endency analysis an{l l>artialunifieat;ion.
In Third Iu, ternational Conferenceon HPSG.
In the pr<)ceedings of TALN '96.955
