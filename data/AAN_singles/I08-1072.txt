Context Feature Selection for Distributional SimilarityMasato Hagiwara, Yasuhiro Ogawa, and Katsuhiko ToyamaGraduate School of Information Science,Nagoya UniversityFuro-cho, Chikusa-ku, Nagoya, JAPAN 464-8603{hagiwara, yasuhiro, toyama}@kl.i.is.nagoya-u.ac.jpAbstractDistributional similarity is a widely usedconcept to capture the semantic relatednessof words in various NLP tasks.
However, ac-curate similarity calculation requires a largenumber of contexts, which leads to imprac-tically high computational complexity.
Toalleviate the problem, we have investigatedthe effectiveness of automatic context selec-tion by applying feature selection methodsexplored mainly for text categorization.
Ourexperiments on synonym acquisition haveshown that while keeping or sometimes in-creasing the performance, we can drasticallyreduce the unique contexts up to 10% of theoriginal size.
We have also extended themeasures so that they cover context cate-gories.
The result shows a considerable cor-relation between the measures and the per-formance, enabling the automatic selectionof effective context categories for distribu-tional similarity.1 IntroductionSemantic similarity of words is one of the most im-portant lexical knowledge for NLP tasks includingword sense disambiguation and synonym acquisi-tion.
To measure the semantic relatedness of words,a concept called distributional similarity has beenwidely used.
Distributional similarity represents therelatedness of two words by the commonality ofcontexts the words share, based on the distributionalhypothesis (Harris, 1985), which states that seman-tically similar words share similar contexts.A wide range of contextual information, suchas surrounding words (Lowe and McDonald, 2000;Curran and Moens, 2002a), dependency or casestructure (Hindle, 1990; Ruge, 1997; Lin, 1998),and dependency path (Lin and Pantel, 2001; Padoand Lapata, 2007), has been utilized for similar-ity calculation, and achieved considerable success.However, a major problem which arises when adopt-ing distributional similarity is that it easily yields ahuge amount of unique contexts.
This can lead tohigh dimensionality of context space, often up to theorder of tens or hundreds of thousands, which makesthe calculation computationally impractical.
Be-cause not all of the contexts are useful, it is stronglyrequired for the efficiency to eliminate the unwantedcontexts to ease the expensive cost.To tackle this issue, Curran and Moens (2002b)suggest assigning an index vector of canonical at-tributes, i.e., a small number of representative el-ements extracted from the original vector, to eachword.
When the comparison is performed, canonicalattributes of two target words are firstly consulted,and the original vectors are referred to only if theattributes have a match between them.
However, itis not clear whether the condition for canonical at-tributes they adopted, i.e., that the attributes must bethe most weighted subject, direct object, or indirectobject, is optimal in terms of the performance.There are also some existing studies which paidattention to the comparison of context categoriesfor synonym acquisition (Curran and Moens, 2002a;Hagiwara et al, 2006).
However, they have con-ducted only a posteriori comparison based on perfor-mance evaluation, and we are afraid that these find-553ings are somewhat limited to their own experimentalsettings which may not be applicable to completelynew settings, e.g., one with a new set of contextsextracted from different sources.
Therefore, generalquantitative measures which can be used for reduc-tion and selection of any kind of contexts and con-text categories are strongly required.Shifting our attention from word similarity toother areas, a great deal of studies on feature selec-tion has been conducted in the literature, especiallyfor text categorization (Yang and Pedersen, 1997)and gene expression classification (Ding and Peng,2003).
Whereas these methods have been successfulin reducing feature size while keeping classificationperformance, the problem of distributional similar-ity is radically different from that of classification,and whether the same methods are applicable andeffective for automatic context selection in the simi-larity problem is yet to be investigated.In this paper, we firstly introduce existing quan-titative methods for feature selection, namely, DF,TS, MI, IG, CHI2, and show how to apply them tothe distributional similarity problem to measure thecontext importance.
We then extracted dependencyrelations as context from the corpus, and conductedautomatic synonym acquisition experiments to eval-uate the context selection performance, reducing theunimportant contexts based on the feature selectionmethods.
Finally we extend the context importanceto cover context categories (RASP2 grammatical re-lations), and show that the above methods are alsoeffective in selecting categories.This paper is organized as follows: in Section2, five existing context selection methods are in-troduced, and how to apply classification-based se-lection methods to distributional similarity is de-scribed.
In Section 3 and 4, the synonym acquisitionmethod and evaluation measures, AP and CC, em-ployed in the evaluation experiments are detailed.Section 5 includes two main experiments and theirresults: context reduction and context category se-lection, along with experimental settings and discus-sions.
Section 6 concludes this paper.2 Context Selection MethodsIn this section, context selection methods proposedfor text categorization or information retrieval areintroduced.
In the following, n and m representthe number of unique words and unique contexts,respectively, and N(w, c) denotes the number of co-occurrence of word w and context c.2.1 Document Frequency (DF)Document frequency (DF), commonly used forweighting in information retrieval, is the number ofdocuments a term co-occur with.
However, in thedistributional similarity settings, DF corresponds toword frequency, i.e., the number of unique words thecontext co-occurs with:df(c) = |{w|N(w, c) > 0}|.The motivation of adopting DF as a context selectioncriterion is the assumption that the contexts sharedby many words should be informative.
It is to note,however, that the contexts with too high DF are notalways useful, since there are some exceptions in-cluding so-called stopwords.2.2 Term Strength (TS)Term strength (TS), proposed by Wilbur andSirotkin (1992) and applied to text categorizationby Yang and Wilbur (1996), measures how likely aterm is to appear in ?similar documents,?
and it isshown to achieve a successful outcome in reducingthe amount of vocabulary for text retrieval.
For dis-tributional similarity, TS is defined as:s(c) = P (c ?
C(w2)|c ?
C(w1)),where (w1, w2) is a related word pair and C(w) isa set of contexts co-occurring with the word w, i.e.,C(w) = {c|N(w, c) > 0}.
s(c) is calculated, let-ting PH be a set of related word pairs, ass(c) = |{(w1, w2) ?
PH |c ?
C(w1) ?
C(w2)}||{(w1, w2) ?
PH |c ?
C(w1)}|.What makes TS different from DF is that it re-quires a training set PH consisting of related wordpairs.
We used the test set for class s = 1 as PHdescribed in the next section.2.3 Formalization of Distributional SimilarityThe following methods, MI, IG, and CHI2, are rad-ically different from the above ones, in that they are554designed essentially for ?class classification?
prob-lems.
Thus we formalize distributional similarity asa classification problem as described below.First of all, we deal with word pairs, instead ofwords, as the targets of classification, and define fea-tures f1, ..., fm corresponding to contexts c1, ..., cm,for each pair.
The feature fj = 1 if the two words ofthe pair has the context cj in common, and fj = 0otherwise.
Then, we define target class s, so thats = 1 when the pair is semantically related, ands = 0 if not.
These defined, distributional similar-ity is formalized as a binary classification problemwhich assigns the word pairs to the class s ?
{0, 1}based on the features c1, ..., cm.
Finally, to calcu-late the specific values of the following feature im-portance measures, we prepare two test sets of re-lated word pairs for class s = 1 and unrelated onesfor class s = 0.
This enables us to apply existingfeature selection methods designed for classificationproblems to the automatic context selection.The two test sets, related and unrelated one, areprepared using the reference sets described in Sec-tion 4.
More specifically, we created 5,000 relatedword pairs by extracting from synonym pairs in thereference set, and 5,000 unrelated ones by firstly cre-ating random pairs of LDV, whose detail is describedlater, and then manually making sure that no relatedpairs are included in these random pairs.2.4 Mutual Information (MI)Mutual information (MI), commonly used for wordassociation and co-occurrence weighing in statisti-cal NLP, is the measure of the degree of dependencebetween two events.
The pointwise MI value of fea-ture f and class s is calculated as:I(f, s) = log P (f, s)P (f)P (s).To obtain the final context importance, we combinethe MI value over both of the classes as Imax(cj) =maxs?
{0,1} I(fj , s).
Note that, here we employedthe maximum value of pointwise MI values sinceit is claimed to be the best in (Yang and Peder-sen, 1997), although there can be other combinationways such as weighted average.2.5 Information Gain (IG)Information gain (IG), often employed in the ma-chine learning field as a criterion for feature impor-tance, is the amount of gained information of anevent by knowing the outcome of the other event,and is calculated as the weighted sum of the point-wise MI values over all the event combinations:G(cj) =?fj?{0,1}?s?
{0,1}P (fj , s) logP (fj , s)P (fj)P (s).2.6 ?2 Statistic (CHI2)?2 statistic (CHI2) estimates the lack of indepen-dence between classes and features, which is equalto the summed difference of observed and expectedfrequency over the contingency table cells.
Morespecifically, letting F jnm(n,m ?
{0, 1}) be the num-ber of word pairs with fj = n and s = m, and thenumber of all pairs be N , ?2 statistic is defined as:?2(cj)= N(F11F00 ?
F01F10)(F11 + F01)(F10 + F00)(F11 + F10)(F01 + F00).3 Synonym Acquisition MethodThis section describes the synonym acquisitionmethod, a major and important application of distri-butional similarity, which we employed for the eval-uation of automatic context selection.
Here we men-tion how to extract the original contexts from cor-pora in detail, as well as the calculation of weightand similarity between words.3.1 Context ExtractionWe adopted dependency structure as the context ofwords since it is the most widely used and well-performing contextual information in the past stud-ies (Ruge, 1997; Lin, 1998).
As the extraction of ac-curate and comprehensive dependency structure is initself a difficult task, the sophisticated parser RASPToolkit 2 (Briscoe et al, 2006) was utilized to ex-tract this kind of word relations.
Take the followingsentence for example:Shipments have been relatively level since January,the Commerce Department noted.555RASP outputs the extracted dependency structureas n-ary relations as follows, which are called gram-matical relations.
Annotations regarding suffix, partof speech tags, offsets for individual words are omit-ted for simplicity.
(ncsubj be Shipment _)(aux be have)(xcomp _ be level)(ncmod _ be relatively)(ccomp _ level note)(ncmod _ note since)(ncsubj note Department _)(det Department the)(ncmod _ Department Commerce)(dobj since January)While the RASP outputs are n-ary relations ingeneral, what we need here is co-occurrences ofwords and contexts, so we extract the set of co-occurrences of stemmed words and contexts by tak-ing out the target word from the relation and replac-ing the slot by an asterisk ?*?
:(words) - (contexts)Shipment - ncsubj:be:*_have - aux:be:*be - ncsubj:*:Shipment:_be - aux:*:havebe - xcomp:_:*:levelbe - ncmod:_:*:relativelyrelatively - ncmod:_:be:*level - xcomp:_:be:*level - ccomp:_:*:note...Summing all these up produces the raw co-occurrence count N(w, c) of word w and context c.3.2 Similarity CalculationAlthough it is possible to use the raw count acquiredabove for the similarity calculation, directly usingthe raw count may cause performance degradation,thus we need an appropriate weighting measure.
Inresponse to the preliminary experiment results, weemployed pointwise mutual information as weight:wgt(w, c) = log P (w, c)P (w)P (c)Here we made a small modification to bind theweight to non-negative such that wgt(w, c) ?
0,because negative weight values sometimes worsenthe performance (Curran and Moens, 2002b).
Theweighting by PMI is applied after the pre-processingincluding frequency cutoff and context selection.As for the similarity measure, we used Jaccard co-efficient, which is widely adopted to capture overlapproportion of two sets:?c?C(w1)?C(w2) min(wgt(w1, c),wgt(w2, c))?c?C(w1)?C(w2) max(wgt(w1, c),wgt(w2, c)).4 Evaluation MeasuresThis section describes the two evaluation methodswe employed ?
average precision (AP) and corre-lation coefficient (CC).4.1 Average Precision (AP)The first evaluation measure, average precision(AP), is a common evaluation scheme for informa-tion retrieval, which evaluates how accurately themethods are able to extract synonyms.
We first pre-pare a set of query words, for which synonyms areobtained to evaluate the precision.
We adopted theLongman Defining Vocabulary (LDV) 1 as the can-didate set of query words.
For each word in LDV,three existing thesauri are consulted: Roget?s The-saurus (Roget, 1995), Collins COBUILD Thesaurus(Collins, 2002), and WordNet (Fellbaum, 1998).The union of synonyms obtained when the LDVword is looked up as a noun is used as the refer-ence set, except for words marked as ?idiom,?
?in-formal,?
?slang?
and phrases comprised of two ormore words.
The LDV words for which no nounsynonyms are found in any of the reference thesauriare omitted.
From the remaining 771 LDV words,100 query words are randomly extracted, and foreach of them the eleven precision values at 0%, 10%,..., and 100% recall levels are averaged to calculatethe final AP value.4.2 Correlation Coefficient (CC)The second evaluation measure is correlation coef-ficient (CC) between the target similarity and thereference similarity, i.e., the answer value of sim-ilarity for word pairs.
The reference similarity iscalculated based on the closeness of two words inthe tree structure of WordNet.
More specifically, thesimilarity between word w with senses w1, ..., wm1and word v with senses v1, ..., vm2 is obtained as fol-lows.
Let the depth of node wi and vj be di and dj ,1http://www.cs.utexas.edu/users/kbarker/working notes/ldoce-vocab.html556and the depth of the deepest common ancestors ofboth nodes be ddca.
The similarity is thensim(w, v) = maxi,jsim(wi, vj) = maxi,j2 ?
ddcadi + dj,which takes the value between 0.0 and 1.0.
Then,the value of CC is calculated as the correlation co-efficient of reference similarities r = (r1, r2, ..., rn)and target similarities s = (s1, s2, ..., sn) over theword pairs in sample set Ps, which is created bychoosing the most similar 2,000 word pairs from4,000 randomly created pairs from LDV.
To avoidtest-set dependency, all the CC values presented inthis paper are the average values of three trials usingdifferent test sets.5 ExperimentsNow we describe the experimental settings and theevaluation results of context selection methods.5.1 Experimental SettingsAs for the corpus, New York Times section of En-glish Gigaword 2, consisting of around 914 millionwords and 1.3 million documents was analyzed toobtain word-context co-occurrences.
Frequency cut-off was applied as a pre-processing in order to filterout any words and contexts with low frequency andto reduce computational cost.
More specifically, anywords w such that?c tf(w, c) < ?f and any con-texts c such that?w tf(w, c) < ?f , with ?f = 40,were removed from the co-occurrence data.Since we set our purpose here to the automaticacquisition of synonymous nouns, only the nounsexcept for proper nouns were selected.
To distin-guish nouns, using POS tags annotated by RASP2,any words with POS tags APP, ND, NN, NP, PN, PPwere labeled as nouns.
This left a total of 40,461unique words and 139,618 unique context, whichcorresponds to the number of vectors and the dimen-sionality of semantic space, respectively.5.2 Context ReductionIn the first experiment, we show the effectiveness ofthe five contextual selection methods introduced inSection 2 for context reduction problem.
The five2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2003T05measures were calculated for each context, and con-texts were sorted by their importance.
The change ofperformance, AP and CC, was calculated on elimi-nating the low-ranked contexts and varying the pro-portion of remaining ones, until only 0.2% (279 innumber) of the unique contexts are left.The result is displayed in Figure 1.
The overallobservation is that the performance not only kept theoriginal level but also slightly improved even duringthe ?aggressive?
reduction when more than 80% ofthe original contexts were eliminated and less than20,000 contexts were left.
It was not until 90% (ap-prox.
10,000 remaining) elimination that the APvalues began to fall.
The tendency of performancechange was almost the same for AP and CC, butwe observe a slight difference regarding which ofthe five measures were effective.
More specifically,TS, IG and CHI2 worked well for AP, and DF, TS,while CHI2 did for CC.
On the whole, TS and CHI2were performing the best, whereas the performanceof MI quickly worsened.
Although the task is dif-ferent, this experiment showed a very consistent re-sult compared with the one of Yang and Pedersen?s(1997).
This means that feature selection methodsare also effective for context selection in distribu-tional similarity, and our formalization of the prob-lem described in Section 2 turned out to be appro-priate for the purpose.5.3 Context Category SelectionWe are then naturally interested in what kinds ofcontexts are included in these top-ranked effectiveones and how much they affect the overall perfor-mance.
To investigate this, we firstly built a set ofelite contexts, by gathering each top 10% (13,961in number) contexts chosen by DF, TS, IG, andCHI2, and obtaining the intersection of these fourtop-ranked contexts.
It was found that these four hada great deal of overlap among them, the number ofwhich turned out to be 6,440.Secondly, to measure the degree of effect a con-text category has, we defined category importanceas the sum of all IG values of the contexts whichbelong to the category.
The reason is that, (a) IGwas one of the best-performing criteria as the previ-ous experiment showed, and (b) IG value for a set ofcontexts can be calculated as the sum of IG values ofindividual elements, assuming that all the contexts5570.100.150.200.25020000400006000080000100000120000Number of Unique ContextCorrelationCoefficient (CC)DFTSMIIGCHI2`(c)6.0%8.0%10.0%12.0%14.0%020000400006000080000100000120000Number of Unique ContextAveragePrecision (AP)DFTSMIIGCHI2 (a)0.100.150.200.2505000100001500020000Number of Unique ContextCorrelationCoefficient (CC)DFTSIGCHI2`(d)6.0%8.0%10.0%12.0%14.0%05000100001500020000Number of Unique ContextAveragePrecision (AP)DFTSIGCHI2 (b)Figure 1: Performance of synonym acquisition on automatic context reduction(a) The overall view and (b) the close-up of 0 to 20,000 unique contexts for AP,and (c) the overall view and (b) the close-up for CCare mutually independent, which is a naive but prac-tical assumption because of the high independenceof acquired contexts from corpora.For the categories: ncsubj, dobj, obj, obj2,ncmod, xmod, cmod, ccomp, det, ta, based on theRASP2 grammatical relations which occur fre-quently (more than 1.0%) in the corpus, their cat-egory importance within the elite context set wascomputed and showed in Figure 2.
The graph alsoshows the performance of individual context cat-egories, calculated when each category was sepa-rately extracted from the entire corpus.
The re-sult indicates that there is a considerable correlation(r = 0.760) between category importance and per-formance, which means it is possible to predict thefinal performance of any context categories by cal-culating their category importance values in the lim-ited size of selected context set.As for the qualitative difference of category types,the result also shows the effectiveness of modifica-tion (ncmod) category, which is consistent with theresult (Hagiwara et al, 2006) that mod is more con-tributing than subj and obj, which have been ex-tensively used in the past.
However, it can be seenthat the reason why the ncmod performs well may beonly because it is the largest category in size (2,5155580% 2% 4% 6% 8% 10% 12% 14%ncsubjdobjobjobj2ncmodxmodcmodccompdettaAverage Precision (AP)0 2 4 6 8 10Category Importance (CI)APCIFigure 2: Performance of synonym acquisition vscontext category importancein the elite contexts).
The investigation of the rela-tions between context size and performance shouldbe conducted in the future.6 ConclusionIn this study, we firstly introduced feature selec-tion methods, previously proposed for text catego-rization, and showed how to apply them for auto-matic context selection for distributional similarityby formalizing the similarity problem as classifica-tion.
We then extracted dependency-based contextfrom the corpus, and conducted evaluation experi-ments on automatic synonym acquisition.The experimental results showed that while keep-ing or even improving the original performance, itis possible to eliminate a large proportion of con-texts (almost up to 90%).
We also extended the con-text importance to cover context categories based onRASP2 grammatical relations, and showed a consid-erable correlation between the importance and theactual performance, suggesting the possibility of au-tomatic context category selection.As the future works, we should further discussother kinds of formalization of distributional simi-larity and their impact, because we introduced andonly briefly described a quite simple formalizationmodel in Section 2.3.
More detailed investigationson the contributions of sub-categories of contexts,and other contexts than dependency structure, suchas surrounding words and dependency path, is alsothe future work.ReferencesTed Briscoe, John Carroll and Rebecca Watson.
2006.The Second Release of the RASP System.
Proc.
of theCOLING/ACL 2006 Interactive Presentation Sessions,77?80.Collins.
2002.
Collins Cobuild Major New Edition CD-ROM.
HarperCollins Publishers.James R. Curran and Marc Moens.
2002.
Scaling Con-text Space.
Proc.
of ACL 2002, 231?238.James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Workshopon Unsupervised Lexical Acquisition.
Proc.
of ACLSIGLEX, 231?238.Chris Ding and Hanchuan Peng.
2003.
Minimum Re-dundancy Feature Selection from Microarray GeneExpression Data.
Proc.
of the IEEE Computer Soci-ety Conference on Bioinformatics, 523?528.Editors of the American Heritage Dictionary.
1995.
Ro-get?s II: The New Thesaurus, 3rd ed.
Houghton Mif-flin.Christiane Fellbaum.
1998.
WordNet: an electronic lexi-cal database, MIT Press.Masato Hagiwara, Yasuhiro Ogawa, Katsuhiko Toyama.2006.
Selection of Effective Contextual Informationfor Automatic Synonym Acquisition.
Proc.
of COL-ING/ACL 2006, 353?360.Zellig Harris.
1985.
Distributional Structure.
Jerrold J.Katz (ed.)
The Philosophy of Linguistics.
Oxford Uni-versity Press.
26?47Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
Proc.
of the 28th An-nual Meeting of the ACL, 268?275.Will Lowe and Scott McDonald.
2000.
The direct route:Mediated priming in semantic space.
Proc.
of the 22ndAnnual Conference of the Cognitive Science Society,675?680.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
Proc.
of COLING/ACL 1998, 786?774.559Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question answering.
Natural LanguageEngineering, Volume 7, Issue 4, 343?360.Seastian Pado and Mirella Lapata.
2007.
Dependency-Based Construction of Semantic Space Models Com-putational Linguistics, Volume 33, Issue 2, 161?199.Gerda Ruge.
1997.
Automatic detection of thesaurus re-lations for information retrieval applications.
Founda-tions of Computer Science: Potential - Theory - Cogni-tion, LNCS, Volume 1337, 499?506, Springer Verlag,Berlin, Germany.Yiming Yang and John Wilbur.
1996.
Using corpusstatistics to remove redundant words in text categoriza-tion.
Journal of the American Society for InformationScience, Volume 47, Issue 5, 357?369.Yiming Yang and Jan O. Pedersen.
1997.
A Compara-tive Study on Feature Selection in Text Categorization.Proc.
of ICML 97, 412?420.John Wilbur and Karl Sirotkin.
1992.
The automaticidentification of stop words.
Journal of InformationScience, 45?55.560
