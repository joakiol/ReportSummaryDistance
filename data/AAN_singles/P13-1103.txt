Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1042?1051,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIntegrating Multiple Dependency Corporafor Inducing Wide-coverage Japanese CCG ResourcesSumire Uematsu?uematsu@cks.u-tokyo.ac.jpTakuya Matsuzaki?takuya-matsuzaki@nii.ac.jpHiroki Hanaoka?hanaoka@nii.ac.jpYusuke Miyao?yusuke@nii.ac.jpHideki Mima?mima@t-adm.t.u-tokyo.ac.jp?The University of TokyoHongo 7-3-1, Bunkyo, Tokyo, Japan?National Institute of InfomaticsHitotsubashi 2-1-2, Chiyoda, Tokyo, JapanAbstractThis paper describes a method of in-ducing wide-coverage CCG resources forJapanese.
While deep parsers with corpus-induced grammars have been emergingfor some languages, those for Japanesehave not been widely studied, mainly be-cause most Japanese syntactic resourcesare dependency-based.
Our method firstintegrates multiple dependency-based cor-pora into phrase structure trees and thenconverts the trees into CCG derivations.The method is empirically evaluated interms of the coverage of the obtained lexi-con and the accuracy of parsing.1 IntroductionSyntactic parsing for Japanese has been domi-nated by a dependency-based pipeline in whichchunk-based dependency parsing is applied andthen semantic role labeling is performed on the de-pendencies (Sasano and Kurohashi, 2011; Kawa-hara and Kurohashi, 2011; Kudo and Matsumoto,2002; Iida and Poesio, 2011; Hayashibe et al,2011).
This dominance is mainly because chunk-based dependency analysis looks most appropriatefor Japanese syntax due to its morphosyntactic ty-pology, which includes agglutination and scram-bling (Bekki, 2010).
However, it is also true thatthis type of analysis has prevented us from deepersyntactic analysis such as deep parsing (Clark andCurran, 2007) and logical inference (Bos et al,2004; Bos, 2007), both of which have been sur-passing shallow parsing-based approaches in lan-guages like English.In this paper, we present our work on induc-ing wide-coverage Japanese resources based oncombinatory categorial grammar (CCG) (Steed-man, 2001).
Our work is basically an extension ofa seminal work on CCGbank (Hockenmaier andSteedman, 2007), in which the phrase structuretrees of the Penn Treebank (PTB) (Marcus et al,1993) are converted into CCG derivations and awide-coverage CCG lexicon is then extracted fromthese derivations.
As CCGbank has enabled a va-riety of outstanding works on wide-coverage deepparsing for English, our resources are expected tosignificantly contribute to Japanese deep parsing.The application of the CCGbank method toJapanese is not trivial, as resources like PTB arenot available in Japanese.
The widely used re-sources for parsing research are the Kyoto corpus(Kawahara et al, 2002) and the NAIST text corpus(Iida et al, 2007), both of which are based on thedependency structures of chunks.
Moreover, therelation between chunk-based dependency struc-tures and CCG derivations is not obvious.In this work, we propose a method to integratemultiple dependency-based corpora into phrasestructure trees augmented with predicate argumentrelations.
We can then convert the phrase structuretrees into CCG derivations.
In the following, wedescribe the details of the integration method aswell as Japanese-specific issues in the conversioninto CCG derivations.
The method is empiricallyevaluated in terms of the quality of the corpus con-version, the coverage of the obtained lexicon, andthe accuracy of parsing with the obtained gram-mar.
Additionally, we discuss problems that re-main in Japanese resources from the viewpoint ofdeveloping CCG derivations.There are three primary contributions of this pa-per: 1) we show the first comprehensive results forJapanese CCG parsing, 2) we present a methodol-ogy for integrating multiple dependency-based re-1042INP: I?NPy: I?NP: I?NP: I?giveS\NP/NP/NP :?x?y?z.give?yxzthemNP :them?NP :ythem?
>S\NP/NP :?y?z.give?y them?zmoneyNP :money?NP :money?NP :money?
>S\NP :?z.give?money?them?z <S :give?money?them?I ??
?ambassadorNPnc?NOMNPga\NPnc <NPgaNPgaNPga?
?negotiationNPnc?DATNPni\NPnc <NPniNPni?
?participationSstem\NPga\NPni?do-CONTScont\Sstem <BScont\NPga\NPni?PAST-BASESbase\ScontSbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase?
?governmentNPnc?NOMNPga\NPnc <NPgaNPgaNPga?
?ambassadorNPnc?ACCNPwo\NPnc <NPwoNPwo?
?negotiationNPnc?DATNPni\NPnc <NPni??
?participationSvo s\NPga\NPni?CAUSEScont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont?
?governmentNP?NOMNPga\NP <NPgaNPgaNPga???ambassador-ACCNPwoNPgaNPga???negotiation-DATNPniNPga??
?joinSvo s\NPga\NPni?causeScont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont?
?negotiationNPnc?DATNPni\NPnc <NPniNPgaNPga?
?participationSstem\NPni?doSvo s\Sstem <BSvo s\NPni?CAUSEScont\Svo sScont\Svo s <BScont\NPni?PASTSbase\ScontScont\Svo sScont <BSbase\NPni <Sbase1Figure 1: A CCG derivation.X/Y : f Y : a ?
X : fa (>)Y : a X\Y : a ?
X : fa (<)X/Y : f Y/Z : g ?
X/Z : ?x.f(gx) (> B)Y\Z : g X\Y : f ?
X\Z : ?x.f(gx) (< B)Figure 2: Combinatory rules (used in the currentimplementation).sources to induce CCG derivations, and 3) we in-vestigate the possibility of further improving CCGanalysis by additional resources.2 Background2.1 Combinatory Categorial GrammarCCG is a syntactic theory widely accepted in theNLP field.
A grammar based on CCG theory con-sists of categories, which represent syntactic cat-egories of words and phrases, and combinatoryrules, which are rules to combine the categories.Categories are either ground categories like S andNP or complex categories in the form of X/Y orX\Y , where X and Y are the categories.
Cate-gory X/Y intuitively means that it becomes cat-egory X when it is combined with another cat-egory Y to its right, and X\Y means it takes acategory Y to its left.
Categories are combinedby applying combinatory rules (Fig.
2) to formcategories for larger phrases.
Figure 1 shows aCCG analysis of a simple English sentence, whichis called a derivation.
The verb give is assignedcategory S\NP/NP/NP , which indicates that ittakes two NPs to its right, one NP to its left, and fi-nally becomes S. Starting from lexical categoriesassigned to words, we can obtain categories forphrases by applying the rules recursively.An important property of CCG is a clear inter-face between syntax and semantics.
As shown inFig.
1, each category is associated with a lambdaterm of semantic representations, and each com-binatory rule is associated with rules for semanticcomposition.
Since these rules are universal, wecan obtain different semantic representations byswitching the semantic representations of lexicalcategories.
This means that we can plug in a vari-Sentence S Verb S\$ (e.g.
S\NPga)Noun phrase NP Post particle NPga|o|ni|to\NPAuxiliary verb S\STable 1: Typical categories for Japanese syntax.Cat.
Feature Value InterpretationNP case ga nominalo accusativeni dativeto comitative, complementizer, etc.nc noneS form stem stembase baseneg imperfect or negativecont continuativevo s causativeTable 2: Features for Japanese syntax (those usedin the examples in this paper).ety of semantic theories with CCG-based syntacticparsing (Bos et al, 2004).2.2 CCG-based syntactic theory for JapaneseBekki (2010) proposed a comprehensive theoryfor Japanese syntax based on CCG.
While the the-ory is based on Steedman (2001), it provides con-crete explanations for a variety of constructions ofJapanese, such as agglutination, scrambling, long-distance dependencies, etc.
(Fig.
3).The ground categories in his theory are S, NP,and CONJ (for conjunctions).
Table 1 presentstypical lexical categories.
While most of themare obvious from the theory of CCG, categoriesfor auxiliary verbs require an explanation.
InJapanese, auxiliary verbs are extensively used toexpress various semantic information, such astense and modality.
They agglutinate to the mainverb in a sequential order.
This is explained inBekki?s theory by the category S\S combined witha main verb via the function composition rule(<B).
Syntactic features are assigned to categoriesNP and S (Table 2).
The feature case represents asyntactic case of a noun phrase.
The feature formdenotes an inflection form, and is necessary forconstraining the grammaticality of agglutination.Our implementation of the grammar basicallyfollows Bekki (2010)?s theory.
However, as a firststep in implementing a wide-coverage Japaneseparser, we focused on the frequent syntactic con-structions that are necessary for computing pred-icate argument relations, including agglutination,inflection, scrambling, case alternation, etc.
Otherdetails of the theory are largely simplified (Fig.
3),1043INP: I?NPy: I?NP: I?NP: I?giveS\NP/NP/NP :?x?y?z.give?yxzthemNP :them?NP :ythem?
>S\NP/NP :?y?z.give?y them?zmoneyNP :money?NP :money?NP :money?
>S\NP :?z.give?money?them?z <S :give?money?them?I ??
?ambassadorNPnc?NOMNPga\NPnc <NPgaNPgaNPga?
?negotiationNPnc?DATNPni\NPnc <NPniNPni?
?participationSstem\NPga\NPni?do-CONTScont\Sstem <BScont\NPga\NPni?PAST-BASESbase\ScontSbase\Scont <BSbase\NPga\NPni <Sbase\NPga <Sbase?
?governmentNPnc?NOMNPga\NPnc <NPgaNPgaNPga?
?ambassadorNPnc?ACCNPwo\NPnc <NPwoNPwo?
?negotiationNPnc?DATNPni\NPnc <NPni??
?participationSvo s\NPga\NPni?CAUSEScont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont?
?governmentNP?NOMNPga\NP <NPgaNPgaNPga???ambassador-ACCNPwoNPgaNPga???negotiation-DATNPniNPga??
?joinSvo s\NPga\NPni?causeScont\NPga\NPwo\(Svo s\NPga) <Scont\NPga\NPwo\NPni <Scont\NPga\NPwo <Scont\NPga <Scont?
?negotiationNPnc?DATNPni\NPnc <NPniNPgaNPga?
?participationSstem\NPni?doSvo s\Sstem <BSvo s\NPni?CAUSEScont\Svo sScont\Svo s <BScont\NPni?PASTSbase\ScontScont\Svo sScont <BSbase\NPni <Sbase1Figure 3: A simplified CCG analysis of the sentence ?The ambassador participated in the negotiation.
?.S ?
NP/NP (RelExt)S\NP1 ?
NP1/ P1 (RelIn)S ?
S1/S1 (Con)S\$1\NP1 ?
(S1\$1\NP1)/(S1\$1\NP1) (ConCoord)Figure 4: Type changing rules.
The upper two arefor relative clauses and the others for continuousclauses.coordination and semantic representation in par-ticular.
The current implementation recognizescoordinated verbs in continuous clauses (e.g., ????????????
?/he played the pia o andsang?
), but the treatment of other types of coor-dination is largely simplified.
For semantic repre-sentation, we define predicate argument structures(PASs) rather than the theory?s formal representa-tion based on dynamic logic.
Sophisticating oursemantic representation is left for future work.For parsing efficiency, we modified the treat-ment of some constructions so that empty el-ements are excluded from the implementation.First, we define type changing rules to producerelative and continuous clauses (shown in Fig.
4).The rules produce almost the same results as thetheory?s treatment, but without using empty ele-ments (pro, etc.).
We also used lexical rules totreat pro-drop and scrambling.
For the sentence inFig.
3, the deletion of the nominal phrase (???
), the dative phrase (???
), or both results invalid sentences, and shuffling the two phrases doesso as well.
Lexical entries with the scrambled ordropped arguments are produced by lexical rulesin our implementation.2.3 Linguistic resources for Japanese parsingAs described in Sec.
1, dependency-based analysishas been accepted for Japanese syntax.
Researchon Japanese parsing also relies on dependency-based corpora.
Among them, we used the follow-ing resources in this work.Kyoto corpus A news text corpus annotatedwith morphological information, chunk bound-Kyoto CorpusChunk??
?
government NOM ??
?
ambassador ACC ??
?
negotiation DAT ??
?
?
?
participation do cause PASTNAIST CorpusDep.Causer ARG-ga ARG-niFigure 5: The Kyoto and NAIST annotations for?The government had the ambassador participatein the negotiation.?.
Accusatives are labeled asARG-ga in causative (see Sec.
3.2).aries, and dependency relations among chunks(Fig.
5).
The dependencies are classified into fourtypes: Para (coordination), A (apposition), I (ar-gument cluster), and Dep (default).
Most of thedependencies are annotated as Dep.NAIST text corpus A corpus annotated withanaphora and coreference relations.
The same setas the Kyoto corpus is annotated.1 The corpusonly focuses on three cases: ?ga?
(subject), ?o?
(direct object), and ?ni?
(indirect object) (Fig.
5).Japanese particle corpus (JP) (Hanaoka et al,2010) A corpus annotated with distinct gram-matical functions of the Japanese particle (postpo-sition) ?to?.
In Japanese, ?to?
has many functions,including a complementizer (similar to ?that?
), asubordinate conjunction (similar to ?then?
), a co-ordination conjunction (similar to ?and?
), and acase marker (similar to ?with?
).2.4 Related workResearch on Japanese deep parsing is fairly lim-ited.
Formal theories of Japanese syntax werepresented by Gunji (1987) based on Head-drivenPhrase Structure Grammar (HPSG) (Sag et al,2003) and by Komagata (1999) based on CCG, al-though their implementations in real-world pars-ing have not been very successful.
JACY (Siegel1In fact, the NAIST text corpus includes additional texts,but in this work we only use the news text section.1044and Bender, 2002) is a large-scale Japanese gram-mar based on HPSG, but its semantics is tightlyembedded in the grammar and it is not as easyto systematically switch them as it is in CCG.Yoshida (2005) proposed methods for extractinga wide-coverage lexicon based on HPSG from aphrase structure treebank of Japanese.
We largelyextended their work by exploiting the standardchunk-based Japanese corpora and demonstratedthe first results for Japanese deep parsing withgrammar induced from large corpora.Corpus-based acquisition of wide-coverageCCG resources has enjoyed great success for En-glish (Hockenmaier and Steedman, 2007).
Inthat method, PTB was converted into CCG-basedderivations from which a wide-coverage CCG lex-icon was extracted.
CCGbank has been used forthe development of wide-coverage CCG parsers(Clark and Curran, 2007).
The same methodologyhas been applied to German (Hockenmaier, 2006),Italian (Bos et al, 2009), and Turkish (C?ak?c?,2005).
Their treebanks are annotated with depen-dencies of words, the conversion of which intophrase structures is not a big concern.
A notablecontribution of the present work is a method for in-ducing CCG grammars from chunk-based depen-dency structures, which is not obvious, as we dis-cuss later in this paper.CCG parsing provides not only predicate argu-ment relations but also CCG derivations, whichcan be used for various semantic processing tasks(Bos et al, 2004; Bos, 2007).
Our work consti-tutes a starting point for such deep linguistic pro-cessing for languages like Japanese.3 Corpus integration and conversionFor wide-coverage CCG parsing, we need a)a wide-coverage CCG lexicon, b) combinatoryrules, c) training data for parse disambiguation,and d) a parser (e.g., a CKY parser).
Since d) isgrammar- and language-independent, all we haveto develop for a new language is a)?c).As we have adopted the method of CCGbank,which relies on a source treebank to be convertedinto CCG derivations, a critical issue to address isthe absence of a Japanese counterpart to PTB.
Weonly have chunk-based dependency corpora, andtheir relationship to CCG analysis is not clear.Our solution is to first integrate multipledependency-based resources and convert theminto a phrase structure treebank that is independentProperNoun?????
YeltsinNPProperNoun???
RussiaNoun???presidentPostP?
DATPPNPAux???
notVPVerb??
forgiveVerbSuffix?
PASSIVEVP Aux?
PASTVP?to Russian president Yeltsin?
?
(one) was not forgiven?Figure 6: Internal structures of a nominal chunk(left) and a verbal chunk (right).of CCG analysis (Step 1).
Next, we translate thetreebank into CCG derivations (Step 2).
The ideaof Step 2 is similar to what has been done withthe English CCGbank, but obviously we have toaddress language-specific issues.3.1 Dependencies to phrase structure treesWe first integrate and convert available Japanesecorpora?namely, the Kyoto corpus, the NAISTtext corpus, and the JP corpus ?into a phrasestructure treebank, which is similar in spirit toPTB.
Our approach is to convert the depen-dency structures of the Kyoto corpus into phrasestructures and then augment them with syntac-tic/semantic roles from the other two corpora.The conversion involves two steps: 1) recogniz-ing the chunk-internal structures, and (2) convert-ing inter-chunk dependencies into phrase struc-tures.
For 1), we don?t have any explicit infor-mation in the Kyoto corpus although, in princi-ple, each chunk has internal structures (Vadas andCurran, 2007; Yamada et al, 2010).
The lack ofa chunk-internal structure makes the dependency-to-constituency conversion more complex than asimilar procedure by Bos et al (2009) that con-verts an Italian dependency treebank into con-stituency trees since their dependency trees are an-notated down to the level of each word.
For thecurrent implementation, we abandon the idea ofidentifying exact structures and instead basicallyrely on the following generic rules (Fig.
6):Nominal chunks Compound nouns are firstformed as a right-branching phrase andpost-positions are then attached to it.Verbal chunks Verbal chunks are analyzed asleft-branching structures.The rules amount to assume that all but the lastword in a compound noun modify the head noun(i.e., the last word) and that a verbal chunk is typ-ically in a form V A1 .
.
.
An, where V is a verb1045PPNoun??
birthPostPcm??fromPPNoun?
deathPostPcm??
toPostPadnom?
adnominalPPPPNoun??
processPostPcm?
ACC  PPNPPPNoun??
birthPostPcm??fromPPNoun?
deathPostPcm??
toPostPadnom?
adnominalPP PPNoun??
processPostPcm?
ACCPara Dep?$ proFess froP EirtK to deatK?Figure 7: From inter-chunk dependencies to a tree.
(or other predicative word) and Ais are auxiliaries(see Fig.
6).
We chose the left-branching structureas default for verbal chunks because the semanticscopes of the auxiliaries are generally in that or-der (i.e., A1 has the narrowest scope).
For bothcases, phrase symbols are percolated upward fromthe right-most daughters of the branches (exceptfor a few cases like punctuation) because in almostall cases the syntactic head of a Japanese phrase isthe right-most element.In practice, we have found several patterns ofexceptions for the above rules.
We implementedexceptional patterns as a small CFG and deter-mined the chunk-internal structures by determin-istic parsing with the generic rules and the CFG.For example, two of the rules we came up with arerule A: Number ?
PrefixOfNumber Numberrule B: ClassifierPhrase ?
Number Classifierin the precedence: rule A > B > generic rules.Using the above, we bracket a compound noun?
?
?
?
?approximately thousand people deathPrefixOfNumber Number Classifier CommonNoun?death of approximately one thousand people?as in(((?
?)
?)
??
)(((approximately thousand) people) death)We can improve chunk-internal structures to someextent by refining the CFG rules.
A complete solu-tion like the manual annotation by Vadas and Cur-ran (2007) is left for future work.The conversion of inter-chunk dependenciesinto phrase structures may sound trivial, but it isnot necessarily easy when combined with chunk-internal structures.
The problem is to which nodein the internal structure of the head the dependentdep modifier-type precedencePara ?
?/PostPcm ?
?/PostPcm, */(Verb|Aux), ...Dep */PostPcm */(Verb|Aux), */Noun, ...Dep */PostPadnom */Noun, */(Verb|Aux), ...Table 3: Rules to determine adjoin position.PPNoun?
dogPostP?
DATVPNPAdj??whiteNPVPNoun?catVerb??
sayAux?
PASTVP PPVerb??
go!PostP?CMPARG - toARG - niARG - gaARG - gaARG - gaARG - ga ARG - niARG - CLSNAISTJPFigure 8: Overlay of pred-arg structure annotation(?The white cat who said ?Go!?
to the dog.?
).tree is adjoined (Fig.
7).
In the case shown in thefigure, three chunks are in the dependency relationindicated by arrows on the top.
The dotted arrowsshow the nodes to which the subtrees are adjoined.Without any human-created resources, we can-not always determine the adjoin positions cor-rectly.
Therefore, as a compromise, we wound upimplementing approximate heuristic rules to deter-mine the adjoin positions.
Table 3 shows examplesof such rules.
A rule specifies a precedence of thepossible adjoin nodes as an ordered list of patternson the lexical head of the subtree under an ad-join position.
The precedence is defined for eachcombination of the type of the dependent phrase,which is determined by its lexical head, and thedependency type in the Kyoto corpus.To select the adjoin position for the left-mostsubtree in Fig.
7, for instance, we look up therule table using the dependency type, ?Para?, andthe lexical head of the modifier subtree, ?
?
?/PostPcm?, as the key, and find the precedence ??
?/PostPcm, */(Verb|Aux), ...?.
We thus select thePP-node on the middle subtree indicated by thedotted arrow because its lexical head (the right-most word), ?
?
?/PostPcm?, matches the firstpattern in the precedence list.
In general, we seekfor an adjoin node for each pattern p in the prece-dence list, until we find a first match.The semantic annotation given in the NAISTcorpus and the JP corpus is overlaid on the phrasestructure trees with slight modifications (Fig.
8).1046PPNoun??
negotiationPostPcm?
DAT VP Noun??
participationVerb?
doVerbSuffix?
CAUSEAux?
PASTVPVPSNPniNP??
negotiationT1?
DAT T4T5??
participationS?S?
doS?S?
CAUSES?S?
PASTT3T2S ???
or   ?B?
or   ?B?
or   ?BNPniNPnc??
negotiationNPni?NPnc?
DATSvo_s?NPniSvo_s?NPni??
participationSvo_s?Svo_s?
doScont?Svo_s?
CAUSESbase?Scont?
PASTScont?NPniSbase?NPniSbaseStep 2 - 1Step 2 - 2, 2 - 3Figure 9: A phrase structure into a CCG deriva-tion.In the figure, the annotation given in the two cor-pora is shown inside the dotted box at the bottom.We converted the predicate-argument annotationsgiven as labeled word-to-word dependencies intothe relations between the predicate words and theirargument phrases.
The results are thus similar tothe annotation style of PropBank (Palmer et al,2005).
In the NAIST corpus, each pred-arg re-lation is labeled with the argument-type (ga/o/ni)and a flag indicating that the relation is medi-ated by either a syntactic dependency or a zeroanaphora.
For a relation of a predicate wp and itsargument wa in the NAIST corpus, the boundaryof the argument phrase is determined as follows:1.
If wa precedes wp and the relation is medi-ated by a syntactic dep., select the maximumPP that is formed by attaching one or morepostpositions to the NP headed by wa.2.
If wp precedes wa or the relation is mediatedby a zero anaphora, select the maximum NPheaded by wa that does not include wp.In the figure, ??/dog?/DAT?
is marked as the ni-argument of the predicate ???/say?
(Case 1), and??
?/white ?/cat?
is marked as its ga-argument(Case 2).
Case 1 is for the most basic construction,where an argument PP precedes its predicate.
CaseVP??
?friend- DATPP VP?
?meet - BASENPni ?VP10 ?
?10 o?clock - TIMEPP VP?
?meet - BASET/T ?XS??
?friend- DATNPni S?NPni?
?meet - BASES10 ?
?10 o?clock - TIMES?S S?
?meet - BASE?
(to) Peet at ten??
(to) Peet a friend?Figure 10: An argument post particle phrase (PP)(upper) and an adjunct PP (lower).2 covers the relative clause construction, where arelative clause precedes the head NP, the modifi-cation of a noun by an adjective, and the relationsmediated by zero anaphora.The JP corpus provides only the function labelto each particle ?to?
in the text.
We determinedthe argument phrases marked by the ?to?
particleslabeled as (nominal or clausal) argument-markersin a similar way to Case 1 above and identified thepredicate words as the lexical heads of the phrasesto which the PPto phrases attach.3.2 Phrase structures to CCG derivationsThis step consists of three procedures (Fig.
9):1.
Add constraints on categories and featuresto tree nodes as far as possible and assign acombinatory rule to each branching.2.
Apply combinatory rules to all branching andobtain CCG derivations.3.
Add feature constraints to terminal nodes.3.2.1 Local constraint on derivationsAccording to the phrase structures, the first proce-dure in Step 2 imposes restrictions on the resultingCCG derivations.
To describe the restrictions, wefocus on some of the notable constructions and il-lustrate the restrictions for each of them.Phrases headed by case marker particles Aphrase of this type must be either an argument(Fig.
10, upper) or a modifier (Fig.
10, lower) of apredicative.
Distinction between the two is madebased on the pred-arg annotation of the predica-tive.
If a phrase is found to be an argument, 1) cat-egory NP is assigned to the corresponding node,2) the case feature of the category is given accord-ing to the particle (in the case of Fig.
10 (upper),1047VPVerb?
?Speak - NEGAux??
?not- CONTAux?PAST- BASEVPS cont ?SS ba s e ?S?did not speak??
or ?B?
or ?B  S cont ?NPg aSneg ?NPg a?
?Speak - NEGS cont ?Sneg??
?not- CONTS b ase ?S cont?PAST- BASES b ase ?NPg aFigure 11: An auxiliary verb and its conversion.VPVerb??
inquire - NEGVerbSuffix???
cause - BASE??
?
her - DATPPVPARG - ga?
(to) Kave Ker inTuire?
?S?NPni[1 ]S?S???
cause - BASE??
?
her - DATNPni[1 ]SS?NPni[1 ]??
inquire - NEGga         [1]NPni[1 ]ga: [1 ]Figure 12: A causative construction.ni for dative), and 3) the combinatory rule thatcombines the particle phrase and the predicativephrase is assigned backward function applicationrule (<).
Otherwise, a category T/T is assigned tothe corresponding modifier node and the rule willbe forward function application (>).Auxiliary verbs As described in Sec.
2.2, anauxiliary verb is always given the category S\Sand is combined with a verbal phrase via < or <B(Fig.
11).
Furthermore, we assign the form featurevalue of the returning category S according to theinflection form of the auxiliary.
In the case shownin the figure, Sbase\S is assigned for ??/PAST-BASE?
and Scont\S for ????/not-CONT?.
Asa result of this restriction, we can obtain condi-tions for every auxiliary agglutination because thetwo form values in S\S are both restricted afterapplying combinatory rules (Sec.
3.2.2).Case alternations In addition to the argu-ment/adjunct distinction illustrated above, a pro-cess is needed for argument phrases of predicatesinvolving case alternation.
Such predicates areeither causative (see Fig.
12) or passive verbsand can be detected by voice annotation from theNAIST corpus.
For an argument of that type ofverb, its deep case (ga for Fig.
12) must be usedto construct the semantic representation, namelythe PAS.
As well as assigning the shallow casevalue (ni in Fig.
12) to the argument?s categoryNP, as usual, we assign a restriction to the PASS?NPo[1]S?NPo??buy-CONTS?S?PAST-ATTRNP?bookNPNP[1]?NP[1]VPVerb??buy-CONTAux?PAST-ATTRNoun?bookNPS?NP[1]NP[1]?NP[1]Noun?storeNP?
?book-ACCPP VPVerb?
?buy-CONTAux?PAST-ATTRVP XSNP?NPNP?storeNPNP?NP?
?book-ACCNPoSS?NPoS?NPo?
?buy-CONTS?S?PAST-ATTR?a store wKere (,) EougKt tKe EooN?
?a EooN wKiFK (,) EougKt?Figure 13: A relative clause with/without argu-ment extraction (upper/lower, respectively).of the verb so that the semantic argument corre-sponding to the deep case is co-indexed with theargument NP.
These restrictions are then utilizedfor PAS construction in Sec.
3.2.3.Relative clauses A relative clause can be de-tected as a subtree that has a VP as its left childand an NP as its right child, as shown in Fig.
13.The conversion of the subtree consists of 1) in-serting a node on the top of the left VP (see theright-hand side of Fig.
13), and 2) assigning theappropriate unary rule to make the new node.
Thedifference between candidate rules RelExt and Re-lIn (see Fig.
4) is whether the right-hand NP isan obligatory argument of the VP or not, whichcan be determined by the pred-arg annotation onthe predicate in the VP.
In the upper example inFig.
13, RelIn is assigned because the right NP?book?
is annotated as an accusative argument ofthe predicate ?buy?.
In contrast, RelExt is as-signed in the lower side in the figure because theright NP ?store?
is not annotated as an argument.Continuous clauses A continuous clause can bedetected as a subtree with a VP of continuous formas its left child and a VP as its right child.
Itsconversion is similar to that of a relative clause,and only differs in that the candidate rules are Conand ConCoord.
ConCoord generates a continu-ous clause that shares arguments with the mainclause while Con produces one without shared ar-guments.
Rule assignment is done by comparingthe pred-arg annotations of the two phrases.1048Training Develop.
Test#Sentences 24,283 4,833 9,284#Chunks 234,685 47,571 89,874#Words 664,898 136,585 255,624Table 4: Statistics of input linguistic resources.3.2.2 Inverse application of rulesThe second procedure in Step 2 begins with as-signing a category S to the root node.
A combi-natory rule assigned to each branching is then ?in-versely?
applied so that the constraint assigned tothe parent transfers to the children.3.2.3 Constraints on terminal nodesThe final process consists of a) imposing restric-tions on the terminal category in order to instan-tiate all the feature values, and b) constructing aPAS for each verbal terminal.
An example of pro-cess a) includes setting the form features in theverb category, such as S\NPni, according to theinflection form of the verb.
As for b), argumentsin a PAS are given according to the category andthe partial restriction.
For instance, if a categoryS\NPni is obtained for ???/inquire?
(Fig.
12),the PAS for ?inquire?
is unary because the cate-gory has one argument category (NPni), and thecategory is co-indexed with the semantic argumentga in the PAS due to the partial restriction depictedin Sec.
3.2.1.
As a result, a lexical entry is ob-tained as??
` S\NPni[1]: inquire([1]).3.3 Lexical entriesFinally, lexical rules are applied to each of the ob-tained lexical entries in order to reduce them tothe canonical form.
Since words in the corpus (es-pecially verbs) often involve pro-drop and scram-bling, there are a lot of obtained entries that haveslightly varied categories yet share a PAS.
We as-sume that an obtained entry is a variation of thecanonical one and register the canonical entries inthe lexicon.
We treat only subject deletion for pro-drop because there is not sufficient information tojudge the deletion of other arguments.
Scramblingis simply treated as permutation of arguments.4 EvaluationWe used the following for the implementation ofour resources: Kyoto corpus ver.
4.02, NAIST text2http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto\%20University\%20Text\%20CorpusTraining Develop.
TestSt.1 St.2 St.1 St.2 St.1 St.2Sent.
24,283 24,116 4,833 4,803 9,284 9,245Converted 24,116 22,820 4,803 4,559 9,245 8,769Con.
rate 99.3 94.6 99.4 94.9 99.6 94.9Table 5: Statistics of corpus conversion.Sentential CoverageCovered Uncovered Cov.
(%)Devel.
3,920 639 85.99Test 7,610 1,159 86.78Lexical CoverageWord Known Unknowncombi.
cat.
wordDevel.
127,144 126,383 682 79 0Test 238,083 236,651 1,242 145 0Table 6: Sentential and lexical coverage.corpus ver.
1.53, and JP corpus ver.
1.04.
Theintegrated corpus is divided into training, devel-opment, and final test sets following the standarddata split in previous works on Japanese depen-dency parsing (Kudo and Matsumoto, 2002).
Thedetails of these resources are shown in Table 4.4.1 Corpus conversion and lexicon extractionTable 5 shows the number of successful conver-sions performed by our method.
In total, we ob-tained 22,820 CCG derivations from 24,283 sen-tences (in the training set), resulting in the to-tal conversion rate of 93.98%.
The table showswe lost more sentences in Step 2 than in Step 1.This is natural because Step 2 imposed more re-strictions on resulting structures and therefore de-tected more discrepancies including compoundingerrors.
Our conversion rate is about 5.5 pointslower than the English counterpart (Hockenmaierand Steedman, 2007).
Manual investigation of thesampled derivations would be beneficial for theconversion improvement.For the lexicon extraction from the CCGbank,we obtained 699 types of lexical categories from616,305 word tokens.
After lexical reduction, thenumber of categories decreased to 454, which inturn may produce 5,342 categories by lexical ex-pansion.
The average number of categories for aword type was 11.68 as a result.4.2 Evaluation of coverageFollowing the evaluation criteria in (Hockenmaierand Steedman, 2007), we measured the coverage3http://cl.naist.jp/nldata/corpus/4https://alaginrc.nict.go.jp/resources/tocorpus/tocorpusabstract.html1049of the grammar on unseen texts.
First, we obtainedCCG derivations for evaluation sets by applyingour conversion method and then used these deriva-tions as gold standard.
Lexical coverage indicatesthe number of words to which the grammar assignsa gold standard category.
Sentential coverage indi-cates the number of sentences in which all wordsare assigned gold standard categories 5.Table 6 shows the evaluation results.
Lexicalcoverage was 99.40% with rare word treatment,which is in the same level as the case of the En-glish CCG parser C&C (Clark and Curran, 2007).We also measured coverage in a ?weak?
sense,which means the number of sentences that aregiven at least one analysis (not necessarily cor-rect) by the obtained grammar.
This number was99.12 % and 99.06 % for the development and thetest set, respectively, which is sufficiently high forwide-coverage parsing of real-world texts.4.3 Evaluation of parsing accuracyFinally, we evaluated the parsing accuracy.
Weemployed the parser and the supertagger of(Miyao and Tsujii, 2008), specifically, its gen-eralized modules for lexicalized grammars.
Wetrained log-linear models in the same way as(Clark and Curran, 2007) using the training set astraining data.
Feature sets were simply borrowedfrom an English parser; no tuning was performed.Following conventions in research on Japanese de-pendency parsing, gold morphological analysis re-sults were input to a parser.
Following C&C, theevaluation measure was precision and recall overdependencies, where a dependency is defined as a4-tuple: a head of a functor, a functor category, anargument slot, and a head of an argument.Table 7 shows the parsing accuracy on the de-velopment and the test sets.
The supertagging ac-curacy is presented in the upper table.
While ourcoverage was almost the same as C&C, the perfor-mance of our supertagger and parser was lower.To improve the performance, tuning disambigua-tion models for Japanese is a possible approach.Comparing the parser?s performance with previ-ous works on Japanese dependency parsing is dif-ficult as our figures are not directly comparableto theirs.
Sassano and Kurohashi (2009) reportedthe accuracy of their parser as 88.48 and 95.095Since a gold derivation can logically be obtained if goldcategories are assigned to all words in a sentence, sententialcoverage means that the obtained lexicon has the ability toproduce exactly correct derivations for those sentences.Supertagging accuracyLex.
Cov.
Cat.
Acc.Devel.
99.40 90.86Test 99.40 90.69C&C 99.63 94.32Overall performanceLP LR LF UP UR UFDevel.
82.55 82.73 82.64 90.02 90.22 90.12Test 82.40 82.59 82.50 89.95 90.15 90.05C&C 88.34 86.96 87.64 93.74 92.28 93.00Table 7: Parsing accuracy.
LP, LR and LF refer tolabeled precision, recall, and F-score respectively.UP, UR, and UF are for unlabeled.in unlabeled chunk-based and word-based F1 re-spectively.
While our score of 90.05 in unlabeledcategory dependency seems to be lower than theirword-based score, this is reasonable because ourcategory dependency includes more difficult prob-lems, such as whether a subject PP is shared bycoordinated verbs.
Thus, our parser is expected tobe capable of real-world Japanese text analysis aswell as dependency parsers.5 ConclusionIn this paper, we proposed a method to inducewide-coverage Japanese resources based on CCGthat will lead to deeper syntactic analysis forJapanese and presented empirical evaluation interms of the quality of the obtained lexicon andthe parsing accuracy.
Although our work is basi-cally in line with CCGbank, the application of themethod to Japanese is not trivial due to the fact thatthe relationship between chunk-based dependencystructures and CCG derivations is not obvious.Our method integrates multiple dependency-based resources to convert them into an integratedphrase structure treebank.
The obtained treebankis then transformed into CCG derivations.
Theempirical evaluation in Sec.
4 shows that our cor-pus conversion successfully converts 94 % of thecorpus sentences and the coverage of the lexiconis 99.4 %, which is sufficiently high for analyz-ing real-world texts.
A comparison of the parsingaccuracy with previous works on Japanese depen-dency parsing and English CCG parsing indicatesthat our parser can analyze real-world Japanesetexts fairly well and that there is room for improve-ment in disambiguation models.1050ReferencesDaisuke Bekki.
2010.
Formal Theory of Japanese Syn-tax.
Kuroshio Shuppan.
(In Japanese).Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of COLING 2004, pages1240?1246.Johan Bos, Cristina Bosco, and Alessandro Mazzei.2009.
Converting a dependency treebank to a cate-gorial grammar treebank for Italian.
In Proceedingsof the Eighth International Workshop on Treebanksand Linguistic Theories (TLT8), pages 27?38.Johan Bos.
2007.
Recognising textual entailment andcomputational semantics.
In Proceedings of SeventhInternational Workshop on Computational Seman-tics IWCS-7, page 1.Ruken C?ak?c?.
2005.
Automatic induction of a CCGgrammar for Turkish.
In Proceedings of ACL Stu-dent Research Workshop, pages 73?78.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4).Takao Gunji.
1987.
Japanese Phrase Structure Gram-mar: A Unification-based Approach.
D. Reidel.Hiroki Hanaoka, Hideki Mima, and Jun?ichi Tsujii.2010.
A Japanese particle corpus built by example-based annotation.
In Proceedings of LREC 2010.Yuta Hayashibe, Mamoru Komachi, and Yuji Mat-sumoto.
2011.
Japanese predicate argument struc-ture analysis exploiting argument position and type.In Proceedings of IJCNLP 2011, pages 201?209.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Julia Hockenmaier.
2006.
Creating a CCGbank anda wide-coverage CCG lexicon for German.
In Pro-ceedings of the Joint Conference of COLING/ACL2006.Ryu Iida and Massimo Poesio.
2011.
A cross-lingualILP solution to zero anaphora resolution.
In Pro-ceedings of ACL-HLT 2011, pages 804?813.Ryu Iida, Mamoru Komachi, Kentaro Inui, and YujiMatsumoto.
2007.
Annotating a Japanese textcorpus with predicate-argument and coreference re-lations.
In Proceedings of Linguistic AnnotationWorkshop, pages 132?139.Daisuke Kawahara and Sadao Kurohashi.
2011.
Gen-erative modeling of coordination by factoring paral-lelism and selectional preferences.
In Proceedingsof IJCNLP 2011.Daisuke Kawahara, Sadao Kurohashi, and KoitiHasida.
2002.
Construction of a Japaneserelevance-tagged corpus.
In Proceedings of the 8thAnnual Meeting of the Association for Natural Lan-guage Processing, pages 495?498.
(In Japanese).Nobo Komagata.
1999.
Information Structure in Texts:A Computational Analysis of Contextual Appropri-ateness in English and Japanese.
Ph.D. thesis, Uni-versity of Pennsylvania.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analyisis using cascaded chunking.
InProceedings of CoNLL 2002.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Ivan A.
Sag, Thomas Wasow, and Emily M. Bender.2003.
Syntactic Theory: A Formal Introduction, 2ndEdition.
CSLI Publications.Ryohei Sasano and Sadao Kurohashi.
2011.
A dis-criminative approach to Japanese zero anaphora res-olution with large-scale lexicalized case frames.
InProceedings of IJCNLP 2011.Manabu Sassano and Sadao Kurohashi.
2009.
A uni-fied single scan algorithm for Japanese base phrasechunking and dependency parsing.
In Proceedingsof ACL-IJCNLP 2009.Melanie Siegel and Emily M. Bender.
2002.
Efficientdeep processing of Japanese.
In Proceedings of the3rd Workshop on Asian Language Resources and In-ternational Standardization.Mark Steedman.
2001.
The Syntactic Process.
MITPress.David Vadas and James Curran.
2007.
Adding nounphrase structure to the Penn Treebank.
In Proceed-ings of ACL 2007, pages 240?247.Emiko Yamada, Eiji Aramaki, Takeshi Imai, andKazuhiko Ohe.
2010.
Internal structure of a diseasename and its application for ICD coding.
Studiesin health technology and informatics, 160(2):1010?1014.Kazuhiro Yoshida.
2005.
Corpus-oriented develop-ment of Japanese HPSG parsers.
In Proceedings ofthe ACL Student Research Workshop.1051
