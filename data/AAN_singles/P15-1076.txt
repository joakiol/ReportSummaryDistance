Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 785?794,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsTrans-dimensional Random Fields for Language ModelingBin Wang1, Zhijian Ou1, Zhiqiang Tan21Department of Electronic Engineering, Tsinghua University, Beijing 100084, China2Department of Statistics, Rutgers University, Piscataway, NJ 08854, USAwangbin12@mails.tsinghua.edu.cn, ozj@tsinghua.edu.cn,ztan@stat.rutgers.eduAbstractLanguage modeling (LM) involvesdetermining the joint probability ofwords in a sentence.
The conditionalapproach is dominant, representing thejoint probability in terms of conditionals.Examples include n-gram LMs and neuralnetwork LMs.
An alternative approach,called the random field (RF) approach, isused in whole-sentence maximum entropy(WSME) LMs.
Although the RF approachhas potential benefits, the empiricalresults of previous WSME models arenot satisfactory.
In this paper, we revisitthe RF approach for language modeling,with a number of innovations.
Wepropose a trans-dimensional RF (TDRF)model and develop a training algorithmusing joint stochastic approximation andtrans-dimensional mixture sampling.
Weperform speech recognition experimentson Wall Street Journal data, and find thatour TDRF models lead to performances asgood as the recurrent neural network LMsbut are computationally more efficient incomputing sentence probability.1 IntroductionLanguage modeling is crucial for a varietyof computational linguistic applications, suchas speech recognition, machine translation,handwriting recognition, information retrieval andso on.
It involves determining the joint probabilityp(x) of a sentence x, which can be denoted asa pair x = (l, xl), where l is the length andxl= (x1, .
.
.
, xl) is a sequence of l words.Currently, the dominant approach is conditionalmodeling, which decomposes the joint probabilityof xlinto a product of conditional probabilities11And the joint probability of x is modeled as p(x) =by using the chain rule,p(x1, .
.
.
, xl) =l?i=1p(xi|x1, .
.
.
, xi?1).
(1)To avoid degenerate representation of the con-ditionals, the history of xi, denoted as hi=(x1, ?
?
?
, xi?1), is reduced to equivalence classesthrough a mapping ?
(hi) with the assumptionp(xi|hi) ?
p(xi|?(hi)).
(2)Language modeling in this conditionalapproach consists of finding suitable mappings?
(hi) and effective methods to estimatep(xi|?(hi)).
A classic example is the traditionaln-gram LMs with ?
(hi) = (xi?n+1, .
.
.
, xi?1).Various smoothing techniques are used forparameter estimation (Chen and Goodman, 1999).Recently, neural network LMs, which have begunto surpass the traditional n-gram LMs, also followthe conditional modeling approach, with ?
(hi)determined by a neural network (NN), which canbe either a feedforward NN (Schwenk, 2007) or arecurrent NN (Mikolov et al, 2011).Remarkably, an alternative approach is used inwhole-sentence maximum entropy (WSME) lan-guage modeling (Rosenfeld et al, 2001).
Specifi-cally, a WSME model has the form:p(x;?)
=1Zexp{?Tf(x)} (3)Here f(x) is a vector of features, which can bearbitrary computable functions of x, ?
is the cor-responding parameter vector, and Z is the globalnormalization constant.
Although WSME mod-els have the potential benefits of being able tonaturally express sentence-level phenomena andintegrate features from a variety of knowledgep(xl)p(?EOS?|xl), where ?EOS?
is a special token placedat the end of every sentence.
Thus the distribution of thesentence length is implicitly modeled.785sources, their performance results ever reportedare not satisfactory (Rosenfeld et al, 2001; Amayaand Bened?
?, 2001; Ruokolainen et al, 2010).The WSME model defined in (3) is basically aMarkov random field (MRF).
A substantial chal-lenge in fitting MRFs is that evaluating the gradi-ent of the log likelihood requires high-dimensionalintegration and hence is difficult even for mod-erately sized models (Younes, 1989), let alnethe language model (3).
The sampling methodspreviously tried for approximating the gradient arethe Gibbs sampling, the Independence Metropolis-Hasting sampling and the importance sampling(Rosenfeld et al, 2001).
Simple applications ofthese methods are hardly able to work efficient-ly for the complex, high-dimensional distributionsuch as (3), and hence the WSME models are infact poorly fitted to the data.
This is one of thereasons for the unsatisfactory results of previousWSME models.In this paper, we propose a new languagemodel, called the trans-dimensional randomfield (TDRF) model, by explicitly takingaccount of the empirical distributions of lengths.This formulation subsequently enables us todevelop a powerful Markov chain Monte Carlo(MCMC) technique, called trans-dimensionalmixture sampling and then propose an effectivetraining algorithm in the framework of stochasticapproximation (SA) (Benveniste et al, 1990;Chen, 2002).
The SA algorithm involves jointlyupdating the model parameters and normalizationconstants, in conjunction with trans-dimensionalMCMC sampling.
Section 2 and 3 present themodel definition and estimation respectively.Furthermore, we make several additional in-novations, as detailed in Section 4, to enablesuccessful training of TDRF models.
First, thediagonal elements of hessian matrix are estimat-ed during SA iterations to rescale the gradient,which significantly improves the convergence ofthe SA algorithm.
Second, word classing is in-troduced to accelerate the sampling operation andalso improve the smoothing behavior of the mod-els through sharing statistical strength betweensimilar words.
Finally, multiple CPUs are used toparallelize the training of our RF models.In Section 5, speech recognition experimentsare conducted to evaluate our TDRF LMs, com-pared with the traditional 4-gram LMs and the re-current neural network LMs (RNNLMs) (Mikolovet al, 2011) which have emerged as a new state-of-art of language modeling.
We explore the useof a variety of features based on word and classinformation in TDRF LMs.
In terms of word errorrates (WERs) for speech recognition, our TDRFLMs alone can outperform the KN-smoothing 4-gram LM with 9.1% relative reduction, and per-form comparably to the RNNLM with a slight0.5% relative reduction.
To our knowledge, thisresult represents the first strong empirical evidencesupporting the power of using the whole-sentencelanguage modeling approach.
Our open-sourceTDRF toolkit is released publicly2.2 Model DefinitionThroughout, we denote3by xl= (x1, .
.
.
, xl) asentence (i.e., word sequence) of length l rangingfrom 1 to m. Each element of xlcorresponds toa single word.
For l = 1, .
.
.
,m, we assumethat sentences of length l are distributed from anexponential family model:pl(xl;?)
=1Zl(?
)e?Tf(xl), (4)where f(xl) = (f1(xl), f2(xl), .
.
.
, fd(xl))Tisthe feature vector and ?
= (?1, ?2, .
.
.
, ?d)Tisthe corresponding parameter vector, and Zl(?)
isthe normalization constant:Zl(?)
=?xle?Tf(xl)(5)Moreover, we assume that length l is associatedwith probability pilfor l = 1, .
.
.
,m. Therefore,the pair (l, xl) is jointly distributed asp(l, xl;?)
= pilpl(xl;?).
(6)We provide several comments on the abovemodel definition.
First, by making explicit therole of lengths in model definition, it is clear thatthe model in (6) is a mixture of random fieldson sentences of different lengths (namely on sub-spaces of different dimensions), and hence will becalled a trans-dimensional random field (TDRF).Different from the WSME model (3), a crucialaspect of the TDRF model (6) is that the mixtureweights pilcan be set to the empirical lengthprobabilities in the training data.
The WSME2http://oa.ee.tsinghua.edu.cn/?ouzhijian/software.htm3We add sup or subscript l, e.g.
in xl, pl(), to make clearthat the variables and distributions depend on length l.786model (3) is essentially also a mixture of RFs, butthe mixture weights implied are proportional to thenormalizing constants Zl(?
):p(l, xl;?)
=Zl(?)Z(?)1Zl(?
)e?Tf(xl), (7)where Z(?)
=?ml=1Zl(?
).A motivation for proposing (6) is that it isvery difficult to sample from (3), namely (7),as a mixture distribution with unknown weightswhich typically differ from each other by orders ofmagnitudes, e.g.
1040or more in our experiments.Setting mixture weights to the known, empiricallength probabilities enables us to develop a veryeffective learning algorithm, as introduced in Sec-tion 3.
Basically, the empirical weights serve as acontrol device to improve sampling from multipledistributions (Liang et al, 2007; Tan, 2015) .Second, it can be shown that if we incorporatethe length features4in the vector of features f(x)in (3), then the distribution p(x;?)
in (3) underthe maximum entropy (ME) principle will take theform of (6) and the probabilities (pi1, .
.
.
, pim) in(6) implied by the parameters for the length fea-tures are exactly the empirical length probabilities.Third, a feature fi(xl), 1 ?
i ?
d, can be anycomputable function of the sentence xl, such asn-grams.
In our current experiments, the featuresfi(xl) and their corresponding parameters ?iaredefined to be position-independent and length-independent.
For example, fi(xl) =?kfi(xl, k),where fi(xl, k) is a binary function of xlevaluatedat position k. As a result, the feature fi(xl) takesvalues in the non-negative integers.3 Model EstimationWe develop a stochastic approximation algorith-m using Markov chain Monte Carlo to estimatethe parameters ?
and the normalization constantsZ1(?
), ..., Zm(?)
(Benveniste et al, 1990; Chen,2002).
The core algorithms newly designed inthis paper are the joint SA for simultaneouslyestimating parameters and normalizing constants(Section 3.2) and trans-dimensional mixture sam-pling (Section 3.3) which is used as Step I of thejoint SA.
The most relevant previous works thatwe borrowed from are (Gu and Zhu, 2001) on SAfor fitting a single RF, (Tan, 2015) on sampling and4The length feature corresponding to length l is a binaryfeature that takes one if the sentence x is of length l, andotherwise takes zero.estimating normalizing constants from multipleRFs of the same dimension, and (Green, 1995) ontrans-dimensional MCMC.3.1 Maximum likelihood estimationSuppose that the training dataset consists of nlsentences of length l for l = 1, .
.
.
,m. First,the maximum likelihood estimate of the lengthprobability pilis easily shown to be nl/n, wheren =?ml=1nl.
By abuse of notation, we setpil= nl/n hereafter.
Next, the log-likelihood of?
given the empirical length probabilities isL(?)
=1nm?l=1?xl?Dllog pl(xl;?
), (8)where Dlis the collection of sentences of length lin the training set.
By setting to 0 the derivative of(8) with respect to ?, we obtain that the maximumlikelihood estimate of ?
is determined by thefollowing equation:?L(?)?
?= p?
[f ]?
p?
[f ] = 0, (9)where p?
[f ] is the expectation of the feature vectorf with respect to the empirical distribution:p?
[f ] =1nm?l=1?xl?Dlf(xl), (10)and p?
[f ] is the expectation of f with respect tothe joint distribution (6) with pil= nl/n:p?
[f ] =m?l=1nlnp?,l[f ], (11)and p?,l[f ] =?xlf(xl)pl(xl;?).
Eq.
(9) hasthe form of equating empirical expectations p?
[f ]with theoretical expectations p?
[f ], as similarlyfound in maximum likelihood estimation of singlerandom field models.3.2 Joint stochastic approximationTraining random field models is challenging dueto numerical intractability of the normalizing con-stants Zl(?)
and expectations p?,l[f ].
We proposea novel SA algorithm for estimating the parame-ters ?
by (9) and, simultaneously, estimating thelog ratios of normalization constants:??l(?)
= logZl(?)Z1(?
), l = 1, .
.
.
,m (12)787Algorithm 1 Joint stochastic approximationInput: training set1: set initial values ?
(0)= (0, .
.
.
, 0)Tand?
(0)= ??(?(0))?
??1(?
(0))2: for t = 1, 2, .
.
.
, tmaxdo3: set B(t)= ?4: set (L(t,0), X(t,0)) = (L(t?1,K), X(t?1,K))Step I: MCMC sampling5: for k = 1?
K do6: sampling (See Algorithm 3)(L(t,k), X(t,k)) = SAMPLE(L(t,k?1), X(t,k?1))7: set B(t)= B(t)?
{(L(t,k), X(t,k))}8: end forStep II: SA updating9: Compute ?
(t)based on (14)10: Compute ?
(t)based on (15) and (16)11: end forwhere Z1(?)
is chosen as the reference value andcan be calculated exactly.
The algorithm can beobtained by combining the standard SA algorithmfor training single random fields (Gu and Zhu,2001) and a trans-dimensional extension of theself-adjusted mixture sampling algorithm (Tan,2015).Specifically, consider the following joint distri-bution of the pair (l, xl):p(l, xl;?, ?)
?pile?le?Tf(xl), (13)where pilis set to nl/n for l = 1, .
.
.
,m, but?
= (?1, .
.
.
, ?m)Twith ?1= 0 are hypothesizedvalues of the truth ??(?)
= (??1(?
), .
.
.
, ??m(?
))Twith ??1(?)
= 0.
The distribution p(l, xl;?, ?
)reduces to p(l, xl;?)
in (6) if ?
were identicalto ??(?).
In general, p(l, xl;?, ?)
differs fromp(l, xl;?)
in that the marginal probability oflength l is not necessarily pil.The joint SA algorithm, whose pseudo-code isshown in Algorithm 1, consists of two steps ateach time t as follows.Step I: MCMC sampling.
Generate a sampleset B(t)with p(l, xl;?
(t?1), ?
(t?1)) as the station-ary distribution (see Section 3.3).Step II: SA updating.
Compute?
(t)= ?
(t?1)+ ??{p?
[f ]??
(l,xl)?B(t)f(xl)K}(14)where ?
?is a learning rate of ?
; compute?
(t?12)= ?
(t)+ ??
{?1(B(t))pi1, .
.
.
,?m(B(t))pim}(15)?
(t)= ?(t?12)?
?
(t?12)1(16)where ?
?is a learning rate of ?, and ?l(B(t)) is therelative frequency of length l appearing in B(t):?l(B(t)) =?
(j,xj)?B(t)1(j = l)K. (17)The rationale in (15) is to adjust ?
based onhow the relative frequencies of lengths ?l(B(t))are compared with the desired length probabili-ties pil.
Intuitively, if the relative frequency ofsome length l in the sample set B(t)is greater(or respectively smaller) than the desired lengthprobability pil, then the hypothesized value ?
(t?1)lis an underestimate (or overestimate) of ??l(?
(t?1))and hence should be increased (or decreased).Following Gu & Zhu (2001) and Tan (2015), weset the learning rates in two stages:??={t??
?if t ?
t01t?t0+t?
?0if t > t0(18)??={(0.1t)??
?if t ?
t010.1(t?t0)+(0.1t0)?
?if t > t0(19)where 0.5 < ?
?, ?
?< 1.
In the first stage (t ?
t0),a slow-decaying rate of t?
?is used to introducelarge adjustments.
This forces the estimates ?
(t)and ?
(t)to fall reasonably fast into the true values.In the second stage (t > t0), a fast-decayingrate of t?1is used.
The iteration number t ismultiplied by 0.1 in (19), to make the the learningrate of ?
decay more slowly than ?.
Commonly,t0is selected to ensure there is no more significantadjustment observed in the first stage.3.3 Trans-dimensional mixture samplingWe describe a trans-dimensional mixture sam-pling algorithm to simulate from the joint distri-bution p(l, xl;?, ?
), which is used with (?, ?)
=(?
(t?1), ?
(t?1)) at time t for MCMC sampling inthe joint SA algorithm.
The name ?mixture sam-pling?
reflects the fact that p(l, xl;?, ?)
representsa labeled mixture, because l is a label indicatingthat xlis associated with the distribution pl(xl; ?
).With fixed (?, ?
), this sampling algorithm canbe seen as formally equivalent to reversible jumpMCMC (Green, 1995), which was originally pro-posed for Bayes model determination.The trans-dimensional mixture sampling algo-rithm consists of two steps at each time t: localjump between lengths and Markov move of sen-tences for a given length.
In the following, we de-note byL(t?1)andX(t?1)the length and sequence788before sampling, but use the short notation (?, ?
)for (?
(t?1), ?
(t?1)).Step I: Local jump.
The Metropolis-Hastingsmethod is used in this step to sample the length.Assuming L(t?1)= k, first we draw a new lengthj ?
?
(k, ?).
The jump distribution ?
(k, l) isdefined to be uniform at the neighborhood of k :?
(k, l) =??????
?13, if k ?
[2,m?
1], l ?
[k ?
1, k + 1]12, if k = 1, l ?
[1, 2] or k = m, l ?
[m?
1,m]0, otherwise(20)where m is the maximum length.
Eq.
(20) restrictsthe difference between j and k to be no more thanone.
If j = k, we retain the sequence and performthe next step directly, i.e.
set L(t)= k and X(t)=X(t?1).
If j = k + 1 or j = k ?
1, the two casesare processed differently.If j = k + 1, we first draw an element(i.e., word) Y from a proposal distribution:Y ?
gk+1(y|X(t?1)).
Then we setL(t)= j (= k + 1) and X(t)= {X(t?1), Y } withprobabilitymin{1,?
(j, k)?
(k, j)p(j, {X(t?1), Y };?, ?
)p(k,X(t?1);?, ?
)gk+1(Y |X(t?1))}(21)where {X(t?1), Y } denotes a sequence of lengthk + 1 whose first k elements are X(t?1)and thelast element is Y .If j = k ?
1, we set L(t)= j (= k ?
1) andX(t)= X(t?1)1:jwith probabilitymin{1,?
(j, k)?
(k, j)p(j,X(t?1)1:j;?, ?
)gk(X(t?1)k|X(t?1)1:j)p(k,X(t?1);?, ?
)}(22)where X(t?1)1:jis the first j elements of X(t?1)andX(t?1)kis the kth element of X(t?1).In (21) and (22), gk+1(y|xk) can be flexiblyspecified as a proper density function in y.
In ourapplication, we find the following choice worksreasonably well:gk+1(y|xk) =p(k + 1, {xk, y};?, ?
)?wp(k + 1, {xk, w};?, ?).
(23)Step II: Markov move.
After the step of localjump, we obtainX(t)=????
?X(t?1)if L(t)= k{X(t?1), Y } if L(t)= k + 1X(t?1)1:k?1if L(t)= k ?
1(24)Then we perform Gibbs sampling on X(t), fromthe first element to the last element (Algorithm 2)Algorithm 2 Markov Move1: for i = 1?
L(t)do2: draw W ?
p(L(t), {X(t)1:i?1, w,X(t)i+1:L(t)};?, ?
)3: set X(t)i= W4: end for4 Algorithm Optimization andAccelerationThe joint SA algorithm may still suffer fromslow convergence, especially when ?
is high-dimensional.
We introduce several techniques forimproving the convergence of the algorithm andreducing computational cost.4.1 Improving SA recursionWe propose two techniques to effectively improvethe convergence of SA recursion.The first technique is to incorporate Hessianinformation, similarly as in related works on s-tochastic approximation (Gu and Zhu, 2001) andstochastic gradient descent algorithms (Byrd et al,2014).
But we only use the diagonal elements ofthe Hessian matrix to re-scale the gradient, due tohigh-dimensionality of ?.Taking the second derivatives of L(?)
yieldsHi= ?d2L(?
)d?2i= p[f2i]?m?l=1pil(pl[fi])2(25)where Hidenotes the ith diagonal element ofHessian matrix.
At time t, before updating theparameter ?
(Step II in Section 3.2), we computeH(t?12)i=1K?
(l,xl)?B(t)fi(xl)2?m?l=1pil(p?l[fi])2,(26)H(t)i= H(t?1)i+ ?H(H(t?12)i?H(t?1)i), (27)where p?l[fi] = |B(t)l|?1?
(l,xl)?B(t)lfi(xl), andB(t)lis the subset, of size |B(t)l|, containing allsentences of length l in B(t).The second technique is to introduce the ?mini-batch?
on the training set.
At each iteration, asubset D(t)of K sentences are randomly selectedfrom the training set.
Then the gradient is approx-imated with the overall empirical expectation p?
[f ]being replaced by the empirical expectation overthe subset D(t).
This technique is reminiscent ofstochastic gradient descent using a random sub-sample of training data to achieve fast convergence7890 20 40 60 80 100120140160180200t/10?log?likelihoodwithout hessianwith hessian(a)0 500 1000 1500 200050100150200t/10negative log?likelihoodHessian+mini?batchHessian(b)Figure 1: Examples of convergence curves ontraining set after introducing hessian and trainingset mini-batching.of optimization algorithms (Bousquet and Bottou,2008).By combining the two techniques, we revise theupdating equation (14) of ?
to?
(t)i= ?(t?1)i+?
?max(H(t)i, h)?{?(l,xl)?D(t)fi(xl)K??
(l,xl)?B(t)fi(xl)K}(28)where 0 < h < 1 is a threshold to avoid H(t)ibeing too small or even zero.
Moreover, a constanttcis added to the denominator of (18), to avoid toolarge adjustment of ?, i.e.??={1tc+t?
?if t ?
t0,1tc+t?t0+t?
?0if t > t0.
(29)Fig.1(a) shows the result after introducing hessianestimation, and Fig.1(b) shows the effect of train-ing set mini-batching.4.2 Sampling accelerationFor MCMC sampling in Section 3.3, the Gibbssampling operation of drawing X(t)i(Step 2 in Al-gorithms 2) involves calculating the probabilitiesof all the possible elements in position i. Thisis computationally costly, because the vocabularysize |V| is usually 10 thousands or more in prac-tice.
As a result, the Gibbs sampling operationpresents a bottleneck limiting the efficiency ofsampling algorithms.We propose a novel method of using class in-formation to effectively reduce the computationalcost of Gibbs sampling.
Suppose that each wordin vocabulary V is assigned to a single class.If the total class number is |C|, then there are,on average, |V|/|C| words in each class.
Withthe class information, we can first draw the classof X(t)i, denoted by c(t)i, and then draw a wordAlgorithm 3 Class-based MCMC sampling1: function SAMPLE((L(t?1), X(t?1)))2: set k = L(t?1)3: init (L(t), X(t)) = (k,X(t?1))Step I: Local jump4: generate j ?
?
(k, ?)
(Eq.
(20))5: if j = k + 1 then6: generate C ?
Qk+1(c)7: generate Y ?
g?k+1(y|X(t?1), C) (Eq.31)8: set L(t)= j and X(t)= {X(t?1), Y } withprobability (Eq.21) and (Eq.32)9: end if10: if j = k ?
1 then11: set L(t)= j and X(t)= X(t?1)1:k?1with probabil-ity Eq.
(22) and (Eq.32)12: end ifStep II: Markov move13: for i = 1?
L(t)do14: draw C ?
Qi(c)15: set c(t)i= C with probability (Eq.30)16: draw W ?
Vc(t)i17: set X(t)i= W18: end for19: return (L(t), X(t))20: end functionbelonging to class c(t)i.
The computational cost isreduced from |V| to |C|+ |V|/|C| on average.The idea of using class information to accel-erate training has been proposed in various con-texts of language modeling, such as maximumentropy models (Goodman, 2001b) and RNN LMs(Mikolov et al, 2011).
However, the realization ofthis idea is different for training our models.The pseudo-code of the new sampling method isshown in Algorithm 3.
Denote by Vcthe subset ofV containing all the words belonging to class c. Inthe Markov move step (Step 13 to 18 in Algorithm3), at each position i, we first generate a class Cfrom a proposal distributionQi(c) and then acceptC as the new c(t)iwith probabilitymin{1,Qi(c(t)i)Qi(C)pi(C)pi(c(t)i)}(30)wherepi(c) =?w?Vcp(L(t), {X(t)1:i?1, w,X(t)i+1:L(t)};?, ?
).The probabilities Qi(c) and pi(c) depend on{X(t)1:i?1, X(t)i+1:L(t)}, but this is suppressed in thenotation.
Then we normalize the probabilities ofwords belonging to class c(t)iand draw a word asthe new X(t)ifrom the class c(t)i.Similarly, in the local jump step with k =L(t?1), if the proposal j = k + 1 (Step 5 to 9790in Algorithm 3), we first generate C ?
Qk+1(c)and then generate Y from class C byg?k+1(y|xk, C) =p(k + 1, {xk, y};?, ?
)?w?VCp(k + 1, {xk, w};?, ?
)(31)with xk= X(t?1).
Then we set L(t)= j andX(t)= {X(t?1), Y } with probability as definedin (21), by settinggk+1(y|xk) = Qk+1(C)g?k+1(y|xk, C).
(32)If the proposal j = k ?
1, similarly we useacceptance probability (22) with (32).In our application, we construct Qi(c) dynami-cally as follows.
Write xlfor {X(t?1), Y } in Step8 or for X(t)in Step 11 of Algorithm 3.
First,we construct a reduced model pcl(xl), by includingonly the features that depend on xlithrough itsclass and retaining the corresponding parametersin pl(xl;?, ?).
Then we define the distributionQi(c) = pcl({xl1:i?1, c, xli+1:l}),which can be directly calculated without knowingthe value of xli.4.3 Parallelization of samplingThe sampling operation can be easily parallelizedin SA Algorithm 1.
At each time t, both theparameters ?
and log normalization constants ?are fixed at ?
(t?1)and ?(t?1).
Instead of simu-lating one Markov Chain, we simulate J MarkovChains on J CPU cores separately.
As a result, togenerate a sample set B(t)of size K, only K/Jsampling steps need to be performed on each CPUcore.
By parallelization, the sampling operation iscompleted J times faster than before.5 Experiments5.1 PTB perplexity resultsIn this section, we evaluate the performance ofLMs by perplexity (PPL).
We use the Wall StreetJournal (WSJ) portion of Penn Treebank (PTB).Sections 0-20 are used as the training data (about930K words), sections 21-22 as the developmentdata (74K) and section 23-24 as the test data(82K).
The vocabulary is limited to 10K words,with one special token ?UNK?
denoting wordsnot in the vocabulary.
This setting is the same asthat used in other studies (Mikolov et al, 2011).The baseline is a 4-gram LM with modifiedKneser-Ney smoothing (Chen and Goodman,Type Featuresw (w?3w?2w?1w0)(w?2w?1w0)(w?1w0)(w0)c (c?3c?2c?1c0)(c?2c?1c0)(c?1c0)(c0)ws (w?3w0)(w?3w?2w0)(w?3w?1w0)(w?2w0)cs (c?3c0)(c?3c?2c0)(c?3c?1c0)(c?2c0)wsh (w?4w0) (w?5w0)csh (c?4c0) (c?5c0)cpw (c?3c?2c?1w0) (c?2c?1w0)(c?1w0)Table 1: Feature definition in TDRF LMs1999), denoted by KN4.
We use the RNNLMtoolkit5to train a RNNLM (Mikolov et al, 2011).The number of hidden units is 250 and otherconfigurations are set by default6.Word classing has been shown to be useful inconditional ME models (Chen, 2009).
For ourTDRF models, we consider a variety of featuresas shown in Table 1, mainly based on word andclass information.
Each word is deterministicallyassigned to a single class, by running the automat-ic clustering algorithm proposed in (Martin et al,1998) on the training data.In Table 1, wi, ci, i = 0,?1, .
.
.
,?5 denote theword and its class at different position offset i,e.g.
w0, c0denotes the current word and its class.We first introduce the classic word/class n-gramfeatures (denoted by ?w?/?c?)
and the word/classskipping n-gram features (denoted by ?ws?/?cs?
)(Goodman, 2001a).
Second, to demonstrate thatlong-span features can be naturally integrated inTDRFs, we introduce higher-order features ?w-sh?/?csh?, by considering two words/classes sep-arated with longer distance.
Third, as an exampleof supporting heterogenous features that combinedifferent information, the crossing features ?cp-w?
(meaning class-predict-word) are introduced.Note that for all the feature types in Table 1, onlythe features observed in the training data are used.The joint SA (Algorithm 1) is used to train theTDRF models, with all the acceleration methodsdescribed in Section 4 applied.
The minibatchsize K = 300.
The learning rates ?
?and ?
?are configured as (29) and (19) respectively with?
?= ?
?= 0.6 and tc= 3000.
For t0, it is firstinitialized to be 104.
During iterations, we monitorthe smoothed log-likelihood (moving average of1000 iterations) on the PTB development data.5http://rnnlm.org/6Minibatch size=10, learning rate=0.1, BPTT steps=5.
17sweeps are performed before stopping, which takes about 25hours.
No word classing is used, since classing in RNNLMsreduces computation but at cost of accuracy.
RNNLMs wereexperimented with varying numbers of hidden units (100-500).
The best result from using 250 hidden units is reported.791models PPL (?
std.
dev.
)KN4 142.72RNN 128.81TDRF w+c 130.69?1.64Table 2: The PPLs on the PTB test data.
The classnumber is 200.We set t0to the current iteration number once therising percentage of the smoothed log-likelihoodswithin 100 iterations is below 20%, and thencontinue 5000 further iterations before stopping.The configuration of hessian estimation (Section4.1) is ?H= ?
?and h = 10?4.
L2regularizationwith constant 10?5is used to avoid over-fitting.
8CPU cores are used to parallelize the algorithm, asdescribed in Section 4.3, and the training of eachTDRF model takes less than 20 hours.The perplexity results on the PTB test data aregiven in Table 2.
As the normalization constantsof TDRF models are estimated stochastically, wereport the Monte Carlo mean and standard devi-ation from the last 1000 iterations for each PPL.The TDRF model using the basic ?w+c?
featuresperforms close to the RNNLM in perplexity.
To becompact, results with more features are presentedin the following WSJ experiment.5.2 WSJ speech recognition resultsIn this section, we continue to use the LMs ob-tained above (using PTB training and develop-ment data), and evaluate their performance mea-sured by WERs in speech recognition, by re-scoring 1000-best lists from WSJ?92 test data (330sentences).
The oracle WER of the 1000-best listsis 3.4%, which are generated from using the Kalditoolkit7with a DNN-based acoustic model.TDRF LMs using a variety of features anddifferent number of classes are tested.
The resultsare shown in Table 3.
Different types of features,like the skipping features, the higher-order fea-tures and the crossing features can all be easilysupported in TDRF LMs, and the performanceis improved to varying degrees.
Particularly, theTDRF using the ?w+c+ws+cs+cpw?
features withclass number 200 performs comparable to theRNNLM in both perplexity and WER.
Numerical-ly, the relative reduction is 9.1% compared withthe KN4 LMs, and 0.5% compared with the RNNLM.7http://kaldi.sourceforge.net/model WER PPL (?
std.
dev.)
#featKN4 8.71 295.41 1.6MRNN 7.96 256.15 5.1MWSMEs (200c)w+c+ws+cs 8.87 ?
2.8?
10125.2Mw+c+ws+cs+cpw 8.82 ?
6.7?
10126.4MTDRFs (100c)w+c 8.56 268.25?3.52 2.2Mw+c+ws+cs 8.16 265.81?4.30 4.5Mw+c+ws+cs+cpw 8.05 265.63?7.93 5.6Mw+c+ws+cs+wsh+csh 8.03 276.90?5.00 5.2MTDRFs (200c)w+c 8.46 257.78?3.13 2.5Mw+c+ws+cs 8.05 257.80?4.29 5.2Mw+c+ws+cs+cpw 7.92 264.86?8.55 6.4Mw+c+ws+cs+wsh+csh 7.94 266.42?7.48 5.9MTDRFs (500c)w+c 8.72 261.02?2.94 2.8Mw+c+ws+cs 8.29 266.34?6.13 5.9MTable 3: The WERs and PPLs on the WSJ?92 testdata.
?#feat?
denotes the feature number.
Differ-ent TDRF models with class number 100/200/500are reported (denoted by ?100c?/?200c?/?500c?
)5.3 Comparison and discussionTDRF vs WSME.
For comparison, Table 3 alsopresents the results from our implementation ofthe WSME model (3), using the same features asin Table 1.
This WSME model is the same as in(Rosenfeld, 1997), but different from (Rosenfeldet al, 2001), which uses the traditional n-gramLM as the priori distribution p0.For the WSME model (3), we can still use aSA training algorithm, similar to that developed inSection 3.2, to estimate the parameters ?.
But inthis case, there is no need to introduce ?l, becausethe normalizing constants Zl(?)
are canceled outas seen from (7).
Specifically, the learning rate ?
?and the L2regularization are configured the sameas in TDRF training.
A fixed number of iterationswith t0= 5000 is performed.
The total iterationnumber is 10000, which is similar to the iterationnumber used in TDRF training.In order to calculate perplexity, we need toestimate the global normalizing constant Z(?)
=?ml=1Zl(?)
for the WSME model.
Similarlyas in (Tan, 2015), we apply the SA algorithmin Section 3.2 to estimate the log normalizingconstants ?, while fixing the parameters ?
to bethose already estimated from the WSME modeland using uniform probabilities pil?
m?1.The resulting PPLs of these WSME models areextremely poor.
The average test log-likelihoodsper sentence for these two WSME models are792?494 and ?509 respectively.
However, the W-ERs from using the trained WSME models inhypothesis re-ranking are not as poor as would beexpected from their PPLs.
This appears to indicatethat the estimated WSME parameters are not sobad for relative ranking.
Moreover, when theestimated ?
and ?
are substituted into our TDRFmodel (6) with the empirical length probabilitiespil, the ?corrected?
average test log-likelihoodsper sentence for these two sets of parameters areimproved to be ?152 and ?119 respectively.
Theaverage test log-likelihoods are both ?96 for thetwo corresponding TDRF models in Table 3.
Thisis some evidence for the model deficiency of theWSME distribution as defined in (3), and intro-ducing the empirical length probabilities gives amore reasonable model assumption.TDRF vs conditional ME.
After training, TDRFmodels are computationally more efficient in com-puting sentence probability, simply summing upweights for the activated features in the sentence.The conditional ME models (Khudanpur and Wu,2000; Roark et al, 2004) suffer from the expen-sive computation of local normalization factors.This computational bottleneck hinders their usein practice (Goodman, 2001b; Rosenfeld et al,2001).
Partly for this reason, although buildingconditional ME models with sophisticated featuresas in Table 1 is theoretically possible, such workhas not been pursued so far.TDRF vs RNN.
The RNN models suffer fromthe expensive softmax computation in the outputlayer8.
Empirically in our experiments, the aver-age time costs for re-ranking of the 1000-best listfor a sentence are 0.16 sec vs 40 sec, based onTDRF and RNN respectively (no GPU used).6 Related WorkWhile there has been extensive research on con-ditional LMs, there has been little work on thewhole-sentence LMs, mainly in (Rosenfeld et al,2001; Amaya and Bened?
?, 2001; Ruokolainen etal., 2010).
Although the whole-sentence approachhas potential benefits, the empirical results of pre-vious WSME models are not satisfactory, almostthe same as traditional n-gram models.
Afterincorporating lexical and syntactic information,a mere relative improvement of 1% and 0.4%8This deficiency could be partly alleviated withsome speed-up methods, e.g.
using word clustering(Mikolov, 2012) or noise contrastive estimation (Mnih andKavukcuoglu, 2013).respectively in perplexity and in WER is reportedfor the resulting WSEM (Rosenfeld et al, 2001).Subsequent studies of using WSEMs with gram-matical features, as in (Amaya and Bened?
?, 2001)and (Ruokolainen et al, 2010), report perplexityimprovement above 10% but no WER improve-ment when using WSEMs alone.Most RF modeling has been restricted to fixed-dimensional spaces9.
Despite recent progress,fitting RFs of moderate or large dimensions re-mains to be challenging (Koller and Friedman,2009; Mizrahi et al, 2013).
In particular, thework of (Pietra et al, 1997) is inspiring to us,but the improved iterative scaling (IIS) methodfor parameter estimation and the Gibbs samplerare not suitable for even moderately sized models.Our TDRF model, together with the joint SA al-gorithm and trans-dimensional mixture sampling,are brand new and lead to encouraging results forlanguage modeling.7 ConclusionIn summary, we have made the following contri-butions, which enable us to successfully train T-DRF models and obtain encouraging performanceimprovement.?
The new TDRF model and the joint SA train-ing algorithm, which simultaneously updatesthe model parameters and normalizing con-stants while using trans-dimensional mixturesampling.?
Several additional innovations including ac-celerating SA iterations by using Hessianinformation, introducing word classing to ac-celerate the sampling operation and improvethe smoothing behavior of the models, andparallelization of sampling.In this work, we mainly explore the use of fea-tures based on word and class information.
Futurework with other knowledge sources and larger-scale experiments is needed to fully exploit theadvantage of TDRFs to integrate richer features.8 AcknowledgmentsThis work is supported by Toshiba Corporation,National Natural Science Foundation of China(NSFC) via grant 61473168, and Tsinghua Ini-tiative.
We thank the anonymous reviewers forhelpful comments on this paper.9Using local fixed-dimensional RFs in sequential modelswas once explored, e.g.
temporal restricted Boltzmannmachine (TRBM) (Sutskever and Hinton, 2007).793ReferencesFredy Amaya and Jos?e Miguel Bened??.
2001.
Im-provement of a whole sentence maximum entropylanguage model using grammatical features.
InAssociation for Computational Linguistics (ACL).Albert Benveniste, Michel M?etivier, and PierrePriouret.
1990.
Adaptive algorithms and stochasticapproximations.
New York: Springer.Olivier Bousquet and Leon Bottou.
2008.
Thetradeoffs of large scale learning.
In NIPS, pages161?168.Richard H Byrd, SL Hansen, Jorge Nocedal, andYoram Singer.
2014.
A stochastic quasi-newtonmethod for large-scale optimization.
arXiv preprintarXiv:1401.7020.Stanley F. Chen and Joshua Goodman.
1999.
An em-pirical study of smoothing techniques for languagemodeling.
Computer Speech & Language, 13:359?394.Hanfu Chen.
2002.
Stochastic approximation and itsapplications.
Springer Science & Business Media.Stanley F. Chen.
2009.
Shrinking exponential lan-guage models.
In Proc.
of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Joshua Goodman.
2001a.
A bit of progress in languagemodeling.
Computer Speech & Language, 15:403?434.Joshua Goodman.
2001b.
Classes for fast maximumentropy training.
In Proc.
of International Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP).Peter J.
Green.
1995.
Reversible jump markovchain monte carlo computation and bayesian modeldetermination.
Biometrika, 82:711?732.Ming Gao Gu and Hong-Tu Zhu.
2001.
Maxi-mum likelihood estimation for spatial models bymarkov chain monte carlo stochastic approximation.Journal of the Royal Statistical Society: Series B(Statistical Methodology), 63:339?355.Sanjeev Khudanpur and Jun Wu.
2000.
Maximum en-tropy techniques for exploiting syntactic, semanticand collocational dependencies in language model-ing.
Computer Speech & Language, 14:355?372.Daphne Koller and Nir Friedman.
2009.
Probabilisticgraphical models: principles and techniques.
MITpress.Faming Liang, Chuanhai Liu, and Raymond J Carroll.2007.
Stochastic approximation in monte carlocomputation.
Journal of the American StatisticalAssociation, 102(477):305?320.Sven Martin, J?org Liermann, and Hermann Ney.
1998.Algorithms for bigram and trigram word clustering.Speech Communication, 24:19?37.Tomas Mikolov, Stefan Kombrink, Lukas Burget,Jan H Cernocky, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Proc.
of International Conference onAcoustics, Speech and Signal Processing (ICASSP).Tom?a?s Mikolov.
2012.
Statistical language modelsbased on neural networks.
Ph.D. thesis, BrnoUniversity of Technology.Yariv Dror Mizrahi, Misha Denil, and Nando de Fre-itas.
2013.
Linear and parallel learning of markovrandom fields.
arXiv preprint arXiv:1308.6342.Andriy Mnih and Koray Kavukcuoglu.
2013.
Learningword embeddings efficiently with noise-contrastiveestimation.
In Neural Information Processing Sys-tems (NIPS).Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1997.
Inducing features of random fields.IEEE Transactions on Pattern Analysis and MachineIntelligence, 19:380?393.Brian Roark, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative languagemodeling with conditional random fields and theperceptron algorithm.
In Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics (ACL), page 47.Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language mod-els: a vehicle for linguistic-statistical integration.Computer Speech & Language, 15:55?73.Ronald Rosenfeld.
1997.
A whole sentence maximumentropy language model.
In Proc.
of AutomaticSpeech Recognition and Understanding (ASRU).Teemu Ruokolainen, Tanel Alum?ae, and Marcus Do-brinkat.
2010.
Using dependency grammar featuresin whole sentence maximum entropy language mod-el for speech recognition.
In Baltic HLT.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech & Language, 21:492?518.Ilya Sutskever and Geoffrey E Hinton.
2007.
Learn-ing multilevel distributed representations for high-dimensional sequences.
In International Confer-ence on Artificial Intelligence and Statistics (AIS-TATS).Zhiqiang Tan.
2015.
Optimally adjusted mixture sam-pling and locally weighted histogram.
In TechnicalReport, Department of Statistics, Rutgers University.Laurent Younes.
1989.
Parametric inference forimperfectly observed gibbsian fields.
Probabilitytheory and related fields, 82:625?645.794
