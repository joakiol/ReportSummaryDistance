Assigning Function Tags to Parsed Text*Don B laheta  and  Eugene Charn iak{dpb, ec}@cs, brown, eduDepartment of Computer ScienceBox 1910 / 115 Waterman St .
- -4th  floorBrown UniversityProvidence, RI 02912Abst rac tIt is generally recognized that the common on-terminal abels for syntactic constituents (NP,VP, etc.)
do not exhaust he syntactic and se-mantic information one would like about partsof a syntactic tree.
For example, the Penn Tree-bank gives each constituent zero or more 'func-tion tags' indicating semantic roles and otherrelated information ot easily encapsulated inthe simple constituent labels.
We present a sta-tistical algorithm for assigning these functiontags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, whichrises to 99% when considering 'no tag' as a validchoice.1 In t roduct ionParsing sentences using statistical informationgathered from a treebank was first examined adecade ago in (Chitrao and Grishman, 1990)and is by now a fairly well-studied problem((Charniak, 1997), (Collins, 1997), (Ratna-parkhi, 1997)).
But to date, the end product ofthe parsing process has for the most part beena bracketing with simple constituent labels likeNP, VP, or SBAR.
The Penn treebank contains agreat deal of additional syntactic and seman-tic information from which to gather statistics;reproducing more of this information automat-ically is a goal which has so far been mostlyignored.
This paper details a process by whichsome of this information--the function tags--may be recovered automatically.In the Penn treebank, there are 20 tags (fig-ure 1) that can be appended to constituent la-bels in order to indicate additional informationabout the syntactic or semantic role of the con-* This research was funded in part by NSF grants LIS-SBR-9720368 and IGERT-9870676.stituent.
We have divided them into four cate-gories (given in figure 2) based on those in thebracketing uidelines (Bies et al, 1995).
A con-stituent can be tagged with multiple tags, butnever with two tags from the same category.
1In actuality, the case where a constituent hastags from all four categories never happens, butconstituents with three tags do occur (rarely).At a high level, we can simply say that hav-ing the function tag information for a given textis useful just because any further informationwould help.
But specifically, there are distinctadvantages for each of the various categories.Grammatical tags are useful for any applicationtrying to follow the thread of the text--they findthe 'who does what' of each clause, which canbe useful to gain information about the situa-tion or to learn more about the behaviour ofthe words in the sentence.
The form/functiontags help to find those constituents behaving inways not conforming to their labelled type, aswell as further clarifying the behaviour of ad-verbial phrases.
Information retrieval applica-tions specialising in describing events, as with anumber of the MUC applications, could greatlybenefit from some of these in determining thewhere-when-why of things.
Noting a topicalisedconstituent could also prove useful to these ap-plications, and it might also help in discourseanalysis, or pronoun resolution.
Finally, the'miscellaneous' tags are convenient at varioustimes; particularly the CLI~ 'closely related' tag,which among other things marks phrasal verbsand prepositional ditransitives.To our knowledge, there has been no attemptso far to recover the function tags in pars-ing treebank text.
In fact, we know of only1There is a single exception i the corpus: one con-stituent is tagged with -LOC-I~R.
This appears to be anerror.234ADV Non-specific adverbialBNF BenefemtiveCLF It-cleftCLR 'Closely related'DIR DirectionDTV DativeEXT ExtentHLN HeadlineLGS Logical subjectL0C LocationMNI~ MannerN0M NominalPRD PredicatePRP PurposePUT Locative complement of 'put'SBJ SubjectTMP TemporalTPC TopicTTL TitleV0C VocativeGrammaticalDTV 0.48%LGS 3.0%PRD 18.%PUT 0.26%SBJ 78.%v0c 0.025%Figure 1: Penn treebank function tags53.% Form/Function 37.% Topicalisation 2.2%0.25% NOM 6.8% 2.5% TPC 100% 2.2%1.5% ADV 11.% 4.2%9.3% BN'F 0.072% 0.026%0.13% DIR 8.3% 3.0%41.% EXT 3.2% 1.2%0.013% LOC 25.% 9.2%MNR 6.2% 2.3%PI~ 5.2% 1.9%33.% 12.%Miscellaneous 9.5%CLR 94.% 8.8%CLF 0 .34% 0.03%HLN 2.6% 0.25%TTL 3.1% 0.29%Figure 2: Categories of function tags and their relative frequenciesone project that used them at all: (Collins,1997) defines certain constituents as comple-ments based on a combination of label and func-tion tag information.
This boolean condition isthen used to train an improved parser.2 FeaturesWe have found it useful to define our statisti-cal model in terms of features.
A 'feature', inthis context, is a boolean-valued function, gen-erally over parse tree nodes and either node la-bels or lexical items.
Features can be fairly sim-ple and easily read off the tree (e.g.
'this node'slabel is X', 'this node's parent's label is Y'), orslightly more complex ('this node's head's part-of-speech is Z').
This is concordant with the us-age in the maximum entropy literature (Bergeret al, 1996).When using a number of known features toguess an unknown one, the usual procedure isto calculate the value of each feature, and thenessentially look up the empirically most proba-ble value for the feature to be guessed based onthose known values.
Due to sparse data, someof the features later in the list may need to beignored; thus the probability of an unknown fea-ture value would be estimated asP(flYl, ?
?, Y,)P ( f l f l ,  f2 , .
.
.
, f j ) ,  j < n ,  (1)where/3 refers to an empirically observed prob-ability.
Of course, if features 1 through i onlyco-occur a few times in the training, this valuemay not be reliable, so the empirical probabilityis usually smoothed:P(flf l ,  Ii)AiP(flfl, fa , .
.
.
,  fi)+ (2)The values for )~i can then be determined ac-cording to the number of occurrences of features1 through i together in the training.One way to think about equation 1 (andspecifically, the notion that j will depend onthe values of f l .
.
.
fn) is as follows: We beginwith the prior probability of f .
If we have dataindicating P(flfl), we multiply in that likeli-hood, while dividing out the original prior.
Ifwe have data for /3(f l f l ,  f2), we multiply thatin while dividing out the P(flfl) term.
This isrepeated for each piece of feature data we have;at each point, we are adjusting the probability235P(flfl,f2,... ,fn) p(/) P(SlA) P(SlSl, S:)P(f) P(f lf l)P(flfl,..., Yi-1, A)-,_-o " p- ff,P(flft, $2,..., f~)P(flA, A,... ,f?-x)j<n(3)we already have estimated.
If knowledge aboutfeature fi makes S more likely than with justf l .
.
.
fi-1, the term where fi is added will begreater than one and the running probabilitywill be adjusted upward.
This gives us the newprobability shown in equation 3, which is ex-actly equivalent to equation 1 since everythingexcept the last numerator cancels out of theequation.
The value of j is chosen such thatfeatures f l .
.
- f j  are sufficiently represented inthe training data; sometimes all n features areused, but often that would cause sparse dataproblems.
Smoothing isperformed on this equa-tion exactly as before: each term is interpolatedbetween the empirical value and the prior esti-mated probability, according to a value of Aithat estimates confidence.
But aside from per-haps providing a new way to think about theproblem, equation 3 is not particularly usefulas it is--it is exactly the same as what we hadbefore.
Its real usefulness comes, as shown in(Charniak, 1999), when we move from the no-tion of a feature chain to a feature tree.These feature chains don't capture verythingwe'd like them to.
If there are two independentfeatures that are each relatively sparse but occa-sionally carry a lot of information, then puttingone before the other in a chain will effectivelyblock the second from having any effect, sinceits information is (uselessly) conditioned on thefirst one, whose sparseness will completely di-lute any gain.
What we'd really like is to be ableto have a feature tree, whereby we can conditionthose two sparse features independently on onecommon predecessor feature.
As we said be-fore, equation 3 represents, for each feature fi,the probability of f based on fi and all its pre-decessors, divided by the probability of f basedonly on the predecessors.
In the chain case, thismeans that the denominator is conditioned onevery feature from 1 to i - 1, but if we use afeature tree, it is conditioned only on those fea-tures along the path to the root of the tree.A notable issue with feature trees as opposedto feature chains is that the terms do not allcancel out.
Every leaf on the tree will be repre-target ~featureFigure 3: A small example feature treesented in the numerator, and every fork in thetree (from which multiple nodes depend) willbe represented at least once in the denomina-tor.
For example: in figure 3 we have a smallfeature tree that has one target feature and fourconditioning features.
Features b and d are in-dependent ofeach other, but each depends on a;c depends directly only on b.
The unsmoothedversion of the corresponding equation would beP(fla, b, c, d) ,~p ,~ P(fla) ~)(f\]a, b) P(f\[a, b, c) P(fla, d)which, after cancelling of terms and smoothing,results inP(fla, b, c, d) P(fla, b, c)P(fla, d)P(fla) (4)Note that strictly speaking the result is not aprobability distribution.
It could be made intoone with an appropriate normalisation--theso-called partition function in the maximum-entropy literature.
However, if the indepen-dence assumptions made in the derivation ofequation 4 are good ones, the partition func-tion will be close to 1.0.
We assume this to bethe case for our feature trees.Now we return the discussion to function tag-ging.
There are a number of features that seem236functiontag labelsucceeding preceding, , .
/ -d~e l  laf)elpare_p~gra-'n~arent's parent'slabel head's POSgrandparent'sh ~ P O SheadS~ parent'sP ~ e a dheadalt-head'sPOs alt-~eadFigure 4: The feature tree used to guess function tagsto condition strongly for one function tag or an-other; we have assembled them into the featuretree shown in figure 4.
2 This figure should berelatively self-explanatory, except for the notionof an 'alternate head'; currently, an alternatehead is only defined for prepositional phrases,and is the head of the object of the preposi-tional phrase.
This data is very important indistinguishing, for example, 'by John' (whereJohn might be a logical subject) from 'by nextyear' (a temporal modifier) and 'by selling it'(an adverbial indicating manner).3 Exper imentIn the training phase of our experiment, wegathered statistics on the occurrence of func-tion tags in sections 2-21 of the Penn treebank.Specifically, for every constituent in the tree-bank, we recorded the presence of its functiontags (or lack thereof) along with its condition-ing information.
From this we calculated theempirical probabilities of each function tag ref-erenced in section 2 of this paper.
Values of )~were determined using EM on the developmentcorpus (treebank section 24).To test, then, we simply took the output ofour parser on the test corpus (treebank section23), and applied a postprocessing step to addfunction tags.
For each constituent in the tree,we calculated the likelihood of each function tagaccording to the feature tree in figure 4, andfor each category (see figure 2) we assigned themost likely function tag (which might be thenull tag).2The reader will note that  the ' features'  l isted in thetree are in fact not  boolean-valued; each node in thegiven tree can be assumed to s tand for a chain of booleanfeatures, one per potent ia l  value at  that  node, exact lyone of which will be true.4 Eva luat ionTo evaluate our results, we first need to deter-mine what is 'correct'.
The definition we choseis to call a constituent correct if there exists inthe correct parse a constituent with the samestart and end points, label, and function tag(or lack thereof).
Since we treated each of thefour function tag categories as a separate fea-ture for the purpose of tagging, evaluation wasalso done on a per-category basis.The denominator of the accuracy measureshould be the maximum possible number wecould get correct.
In this case, that meansexcluding those constituents hat were alreadywrong in the parser output; the parser we usedattains 89% labelled precision-recall, so roughly11% of the constituents are excluded from thefunction tag accuracy evaluation.
(For refer-ence, we have also included the performance ofour function tagger directly on treebank parses;the slight gain that resulted is discussed below.
)Another consideration is whether to countnon-tagged constituents in our evaluation.
Onthe one hand, we could count as correct anyconstituent with the correct tag as well as anycorrectly non-tagged constituent, and use asour denominator the number of all correctly-labelled constituents.
(We will henceforth referto this as the 'with-null' measure.)
On the otherhand, we could just count constituents with thecorrect tag, and use as our denominators thetotal number of tagged, correctly-labelled con-stituents.
We believe the latter number ('no-null') to be a better performance metric, as itis not overwhelmed by the large number of un-tagged constituents.
Both are reported below.237CategoryGrammaticalForm/FunctionTopicalisationMiscellaneousOverallTable 1: Baseline performanceBaseline 1(never tag) Tag Precision86.935% SBJ 10.534%91.786% THP 3.105%99.406% TPC 0.594%98.436% CLR 1.317%94.141% - -  3.887%Baseline 2 (always choose most likely tag)Recall F-measure80.626% 18.633%37.795% 5.738%100.00% 1.181%84.211% 2.594%66.345% 7.344%Table 2: Performance within each categoryWith-null - - -No-nu l l - -Category Accuracy Precision Recall F-measureGrammatical 98.909% 95.472% 95.837% 95.654%Form/Function 97.104% 80.415% 77.595% 78.980%Topicalisation 99.915% 92.195% 93.564% 92.875%Miscellaneous 98.645% 55.644% 65.789% 60.293%5 Resu l ts5.1 Base l inesThere are, it seems, two reasonable baselinesfor this and future work.
First of all, most con-stituents in the corpus have no tags at all, soobviously one baseline is to simply guess no tagfor any constituent.
Even for the most com-mon type of function tag (grammatical), thismethod performs with 87% accuracy.
Thus thewith-null accuracy of a function tagger needs tobe very high to be significant here.The second baseline might be useful in ex-amining the no-null accuracy values (particu-larly the recall): always guess the most commontag in a category.
This means that every con-stituent gets labelled with '-SBJ-THP-TPC-CLR'(meaning that it is a topicalised temporal sub-ject that is 'closely related' to its verb).
Thiscombination of tags is in fact entirely illegalby the treebank guidelines, but performs ad-equately for a baseline.
The precision is, ofcourse, abysmal, for the same reasons the firstbaseline did so well; but the recall is (as onemight expect) substantial.
The performancesof the two baseline measures are given in Table1.5.2 Per fo rmance  in ind iv idua lcategor iesIn table 2, we give the results for each category.The first column is the with-null accuracy, andthe precision and recall values given are the no-null accuracy, as noted in section 4.Grammatical tagging performs the best of thefour categories.
Even using the more difficultno-null accuracy measure, it has a 96% accu-racy.
This seems to reflect the fact that gram-matical relations can often be guessed based onconstituent labels, parts of speech, and high-frequency lexical items, largely avoiding sparse-data problems.
Topicalisation can similarly beguessed largely on high-frequency information,and performed almost as well (93%).On the other hand, we have theform/function tags and the 'miscellaneous'tags.
These are characterised by much moresemantic information, and the relationshipsbetween lexical items are very important,making sparse data a real problem.
All thesame, it should be noted that the performanceis still far better than the baselines.5.3 Per fo rmance  w i th  o ther  featuretreesThe feature tree given in figure 4 is by no meansthe only feature tree we could have used.
In-238Table 3: Overall performance on different inputsWith-null - -No-nu l l -Category Accuracy Precision Recall F-measureParsed 98.643% 87.173% 87.381% 87.277%Treebank 98.805% 88.450% 88.493% 88.472%deed, we tried a number of different rees on thedevelopment corpus; this tree gave among thebest overall results, with no category perform-ing too badly.
However, there is no reason touse only one feature tree for all four categories;the best results can be got by using a separatetree for each one.
One can thus achieve slight(one to three point) gains in each category.5.4 Overal l  per fo rmanceThe overall performance, given in table 3, ap-pears promising.
With a tagging accuracy ofabout 87%, various information retrieval andknowledge base applications can reasonably ex-pect to extract useful information.The performance given in the first row is (likeall previously given performance values) thefunction-tagger's performance on the correctly-labelled constituents output by our parser.
Forcomparison, we also give its performance whenrun directly on the original treebank parse; sincethe parser's accuracy is about 89%, working di-rectly with the treebank means our statisticsare over roughly 12% more constituents.
Thissecond version does slightly better.The main reason that tagging does worse onthe parsed version is that although the con-stituent itself may be correctly bracketed and la-belled, its exterior conditioning information canstill be incorrect.
An example of this that ac-tually occurred in the development corpus (sec-tion 24 of the treebank) is the 'that' clause inthe phrase 'can swallow the premise that the re-wards for such ineptitude are six-figure salaries',correctly diagrammed in figure 5.
The functiontagger gave this SBAR an ADV tag, indicating anunspecified adverbial function.
This seems ex-tremely odd, given that its conditioning infor-mation (nodes circled in the figure) clearly showthat it is part of an NP, and hence probably mod-ifies the preceding NN.
Indeed, the statistics givethe probability of an ADV tag in this condition-ing environment as vanishingly small.vPthe ( premise ) ~Figure 5: SBAR and conditioning infothe premise ~ ...Figure 6: SBAR and conditioning info, as parsedHowever, this was not the conditioning infor-mation that the tagger received.
The parserhad instead decided on the (incorrect) parse infigure 6.
As such, the tagger's decision makesmuch more sense, since an SBAR under two VPswhose heads are VB and MD is rather likely to bean ADV.
(For instance, the 'although' clause ofthe sentence 'he can help, although he doesn'twant to.'
has exactly the conditioning environ-ment given in figure 6, except that its prede-cessor is a comma; and this SBAR would be cor-rectly tagged ADV.)
The SBAR itself is correctlybracketed and labelled, so it still gets countedin the statistics.
Happily, this sort of case seemsto be relatively rare.239Another thing that lowers the overall perfor-mance somewhat is the existence of error and in-consistency in the treebank tagging.
Some tagsseem to have been relatively easy for the humantreebank taggers, and have few errors.
Othertags have explicit caveats that, however well-justified, proved difficult to remember for thetaggers--for instance, there are 37 instances ofa PP being tagged with LGS (logical subject) inspite of the guidelines specifically saying, '\[LGS\]attaches to the NP object of by and not to thePP node itself.'
(Bies et al, 1995) Each mistag-ging in the test corpus can cause up to two spu-rious errors, one in precision and one in recall.Still another source of difficulty comes when theguidelines are vague or silent on a specific issue.To return to logical subjects, it is clear that 'theloss' is a logical subject in 'The company washurt by the loss', but what about in 'The com-pany was unperturbed by the loss' ?
In addition,a number of the function tags are authorised for'metaphorical use', but what exactly constitutessuch a use is somewhat inconsistently marked.It is as yet unclear just to what degree thesetagging errors in the corpus are affecting ourresults.6 Conc lus ionThis work presents a method for assigning func-tion tags to text that has been parsed to thesimple label level.
Because of the lack of priorresearch on this task, we are unable to com-pare our results to those of other researchers;but the results do seem promising.
However, agreat deal of future work immediately suggestsitself:?
Although we tested twenty or so featuretrees besides the one given in figure 4, thespace of possible trees is still rather un-explored.
A more systematic investiga-tion into the advantages ofdifferent featuretrees would be useful.?
We could add to the feature tree the val-ues of other categories of function tag, orthe function tags of various tree-relatives(parent, sibling).?
One of the weaknesses of the lexical fea-tures is sparse data; whereas the part ofspeech is too coarse to distinguish 'by John'(LGS) from 'by Monday' (TMP), the lexi-cal information may be too sparse.
Thiscould be assisted by clustering the lexicalitems into useful categories (names, dates,etc.
), and adding those categories as an ad-ditional feature type.?
There is no reason to think that this workcould not be integrated irectly into theparsing process, particularly if one's parseris already geared partially or entirely to-wards feature-based statistics; the func-tion tag information could prove quite use-ful within the parse itself, to rank severalparses to find the most plausible.Re ferencesAdam L. Berger, Stephen A. Della Pietra,and Vincent J. Della Pietra.
1996.
Amaximum entropy approach to natural lan-guage processing.
Computational Linguistics,22(1):39-71.Ann Bies, Mark Ferguson, Karen Katz, andRobert MacIntyre, 1995.
Bracketing Guide-lines for Treebank H Style Penn TreebankProject, January.Eugene Charniak.
1997.
Statistical pars-ing with a context-free grammar and wordstatistics.
In Proceedings of the FourteenthNational Conference on Artificial Intelli-gence, pages 598-603, Menlo Park.
AAAIPress/MIT Press.Eugene Charniak.
1999.
A maximum-entropy-inspired parser.
Technical Report CS-99-12,Brown University, August.Mahesh V. Chitrao and Ralph Grishman.
1990.Statistical parsing of messages.
In DARPASpeech and Language Workshop, pages 263-266.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics,pages 16-23.Adwait Ratnaparkhi.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
In Proceedings of the SecondAnnual Conference on Empirical Methods inNatural Language Processing, pages 1-10.240
