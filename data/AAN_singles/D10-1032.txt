Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 325?334,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsTense Sense Disambiguation: a New Syntactic Polysemy TaskRoi ReichartICNCHebrew University of Jerusalemroiri@cs.huji.ac.ilAri RappoportInstitute of Computer ScienceHebrew University of Jerusalemarir@cs.huji.ac.ilAbstractPolysemy is a major characteristic of natu-ral languages.
Like words, syntactic formscan have several meanings.
Understanding thecorrect meaning of a syntactic form is of greatimportance to many NLP applications.
In thispaper we address an important type of syn-tactic polysemy ?
the multiple possible sensesof tense syntactic forms.
We make our dis-cussion concrete by introducing the task ofTense Sense Disambiguation (TSD): given aconcrete tense syntactic form present in a sen-tence, select its appropriate sense among aset of possible senses.
Using English gram-mar textbooks, we compiled a syntactic sensedictionary comprising common tense syntac-tic forms and semantic senses for each.
We an-notated thousands of BNC sentences using thedefined senses.
We describe a supervised TSDalgorithm trained on these annotations, whichoutperforms a strong baseline for the task.1 IntroductionThe function of syntax is to combine words to ex-press meanings, using syntactic devices such asword order, auxiliary words, and morphology (Gold-berg, 1995).
Virtually all natural language devicesused for expressing meanings (e.g., words) exhibitpolysemy.
Like words, concrete syntactic forms (thesentence words generated by specific syntactic de-vices) can have several meanings.
Consider the fol-lowing sentences:(a) They are playing chess in the park.
(b) They are playing chess next Tuesday.Both contain the concrete syntactic form ?are play-ing?, generated by the abstract syntactic form usu-ally known as ?present progressive?
(am/is/are + V-ing).
In (a), the meaning is ?something happeningnow?, while in (b) it is ?a plan to do something in thefuture?.
Note that the polysemy is of the syntacticform as a unit, not of individual words.
In particu-lar, the verb ?play?
is used in the same sense in bothcases.In this paper we address a prominent type of syn-tactic form polysemy: the multiple possible sensesthat tense syntactic forms can have.
Disambiguat-ing the polysemy of tense forms is of theoreticaland practical importance (Section 2).
To make ourdiscussion concrete, we introduce the task of TenseSense Disambiguation (TSD): given a concrete tensesyntactic form in a sentence, select its correct senseamong a given set of possible senses (Section 3).The disambiguation of polysemy is a fundamentalproblem in NLP.
For example, Word Sense Disam-biguation (WSD) continues to attract a large numberof researchers (Agirre and Edmonds, 2006).
TSDhas the same structure as WSD, with different dis-ambiguated entities.For experimenting with the TSD task, we com-piled an English syntactic sense dictionary basedon a thorough study of three major English gram-mar projects (Section 4).
We selected 3000 sen-tences from the British National Corpus containing4702 concrete syntactic forms, and annotated eachof these by its sense (Section 5).We developed a su-pervised learning TSD algorithm that uses variousfeature types and takes advantage of the task struc-ture (Section 6).
Our algorithm substantially outper-325forms the ?most frequent sense?
baseline (Section 7).TSD is fundamental to sentence understandingand thus to NLP applications such as textual infer-ence, question answering and information retrieval.To the best of our knowledge, this is the first paper toaddress this task.
In Section 8 we discuss researchdirections relevant to TSD placing the new task inthe context of the previous research of syntactic am-biguity resolution.2 TSD MotivationIn this work we follow linguistics theories that positthat tense does not directly reflect conceptual time asone might think.
Dinsmore (1991) and Cutrer (1994)explain that the same tense may end up indicatingvery different objective time relations relative to thesentence production time.Fauconnier (2007) exemplifies such phenomena.In the following sentences, the present tense corre-sponds to the future time: (1) The boat leaves nextweek.
(2) When he comes tomorrow, I will tell himabout the party.
(3) If I see him next week, I will askhim to call you.In contrast, the following present tense sentencestalk about events that happened in the past: (1) I amwalking down the street one day when suddenly thisguy walks up to me.
(2) He catches the ball.
Heruns.
He makes a touchdown.
(morning-after sportsreport).Another set of examples is related to the pasttense.
In the following sentences it corresponds toa present time: (1) Do you have a minute?
I wantedto ask you a question.
(2) I wish I lived closer to myfamily now.
In contrast, in the following two sen-tences, it corresponds to a future time: (1) If I hadthe time next week, I would go to your party.
(2) Icannot go to the concert tonight.
You will have totell me how it was.Fauconnier explains these phenomena by a modelfor the grammar of tense.
According to this model,the grammar specifies partial constraints on time andfact/prediction status that hold locally between men-tal spaces within a discourse configuration.
We mayobtain actual information about time by combiningthis with other available pragmatic information.
Ac-cordingly, the same tense may end up indicatingvery different objective time relations relative to thespeech event.TSD fits well with modern linguistics theories.For example, in the construction grammar frame-work (Goldberg, 1995), the ?construction?
is the ba-sic unit, comprised of a form and a meaning.
Words,multiword expressions, and syntactic forms are allvalid constructions.
It is thus very natural to addressthe sense disambiguation problem for all of these.
Inthis paper we focus on tense constructions.For many NLP applications, it is very importantto disambiguate the tense forms of the sentence.Among these applications are: (1) machine transla-tion, as the actual time described by one tense formin the source language may be described by a dif-ferent tense form in the target language; (2) under-standing the order of events in a text; (3) textual en-tailment, when the optional entailed sentences referto the time and/or order of events of the source sen-tence.
Many more examples also exist.3 The TSD TaskIn this section we formally define the TSD task, dis-cuss its nature vs. WSD, and describe various con-crete task variants.Task definition.
First, some essential terminol-ogy.
The function of syntax is to combine lexi-cal items (words, multiword expressions) to expressmeanings.
This function is achieved through syntac-tic devices.
The most common devices in Englishare word order, morphology, and the usage of auxil-iary words.
An Abstract Syntactic Form (ASF) is aparticular set of devices that can be used to express aset of meanings.
A Concrete Syntactic Form (CSF)is a concrete set of words generated by an ASF forexpressing a certain meaning in an utterance1.
ACSF is ambiguous if its generating ASF has morethan one meaning, which is the usual case.
In thiscase we also say that the ASF is ambiguous.Here are a few examples.
The ?present progres-sive?
ASF has the form ?am/is/are V-ing?2, whichemploys all three main devices.
It is ambiguous,1In some linguistic theories, the central notion is the con-struction, which combines an ASF (referred to as the form ofthe construction) with a single meaning (Goldberg, 1995).2Note that strictly speaking, these are three different ASFs.We refer to this ASF family by a single name because they havethe same set of meanings and because it is standard to treat themas a single ASF.326as shown in Section 1.
The ?present simple?
ASFhas the form ?V(+s)?3, and is ambiguous as well: inthe sentence ?My Brother arrives this evening?, theCSF ?arrives?
conveys the meaning of ?a future eventarranged for a definite time?, while in the sentence?The sun rises in the East?
the meaning is that of arepeated event.TSD vs. WSD.
The TSD task is to disambiguatethe semantic sense of a tense syntactic form.
TSDis clearly different from WSD.
This is obvious whenthe CSF comprises two words that are not a multi-word expression, and is usually also the case when itcomprises a single word.
Consider the ?My Brotherarrives this evening?
example above.
While the verb?arrive?
has two main senses: ?reach a place?, and?begin?, as in ?Summer has arrived?, in that examplewe focused on the disambiguation of the tense senseof the ?arrives?
construction.Concrete task variants.
Unlike with words, thepresence of a particular CSF in a sentence is nottrivially recognizable.
Consequently, there are threeversions of the TSD task: (1) we are given the sen-tence, a marked subset of its words comprising aCSF, and the ASF that has generated these words;(2) we are given the sentence and a marked subsetof its words comprising a CSF, without knowing thegenerating ASF; (3) we are given only the sentenceand we need to find the contained CSFs and theirASFs.
In all cases, we need to disambiguate thesense of the ASFs.
We feel that the natural granu-larity of the task is captured by version (2).
How-ever, since the ASF can usually be identified usingrelatively simple features, we also report results forversion (1).
The main difficulty in all versions isidentifying the appropriate sense, as is the case withWSD.4 The Syntactic Sense DictionaryA prerequisite to any concrete experimentation withthe TSD task is a syntactic sense dictionary.
Basedon a thorough examination of three major Englishgrammar projects, we compiled a set of 18 com-mon English tense ASFs and their possible senses.The projects are (1) the Cambridge University Press3Again, these are two ASFs, one adding an ?s?
and one usingthe verb as is.English Grammar In Use series, comprising threebooks (essential, intermediate and advanced) (Mur-phy, 2007; Murphy, 1994; Hewings, 2005); (2)the English grammar texts resulting from the sem-inal corpus-based Cobuild project (elementary, ad-vanced) (Willis and Wright, 2003; Willis, 2004); (3)the Longman Grammar of Spoken and Written En-glish (Biber et al, 1999).As in any sense dictionary, in many cases it is hardto draw the line between senses.
In order to be ableto explore the computational limits of the task, wehave adopted a policy of fine sense granularity.
Forexample, senses 1 and 3 of the ?present simple?
ASFin Table 1 can be argued to be quite similar to eachother, having a very fine semantic distinction.
A spe-cific application may choose to collapse some sensesinto one.We used the conventional ASF names, whichshould not be confused with their meanings (e.g., the?present simple?
ASF can be used to refer to future,not present, events, as in Table 1, sense 4).The ASF set thus obtained is: real conditionals,hypothetical conditionals, wishes, reported speech,present simple, present progressive, present perfect,present perfect progressive, past simple, past pro-gressive, past perfect, past perfect progressive, ?be+ going + to + infinitive?, future progressive, futureperfect, future perfect progressive, ?would?
tenseforms, and ?be + to + infinitive?.
Note that the firstfour ASFs are not direct tense forms; we includethem because they involve tensed sub-sentenceswhose disambiguation is necessary for disambigua-tion of the whole ASF.
The total number of possiblesenses for these 18 ASFs is 103.Table 1 shows the complete senses set for the?present simple?
and ?be + to + infinitive?
ASFs, plusan example sentence for each sense.
Space limita-tions prevent us from listing all form senses here;we will make the listing available online.5 Corpus Creation and AnnotationWe selected 3000 sentences from the British Na-tional Corpus (BNC) (Burnard, 2000), containing4702 CSFs (1.56 per sentence).
These sentenceswith their CSFs were sense annotated.
To selectthe 3000 sentences, we randomly sampled sentencesfrom the various written and spoken sections of the327Present Simple1 Things that are always trueIt gets cold in the winter.2 Regular and repeated actions and habitsMy parents often eat meat.3 General factsMr.
Brown is a teacher.4 A future event arranged for a definite timeThe next train arrives at 11:30.5 Plans, expectations and hopesWe hope to see you soon.6 Ordering someone to do somethingTake your hands out of your pockets!7 Something happening now, with verbs that arenot used in the present progressive in this senseI do not deny the allegation.8 Events happening now (informal;common in books, scripts, radio etc.
)She goes up to this man and looks into his eyes.9 Past actionsI was sitting in the park reading a newspaperwhen all of a sudden this dog jumps at me.10 Newspaper headlines, for recent eventsQuake hits central Iran.11 When describing the content of a bookThompson gives an exhaustive list in chapter six.
?be + to + infinitive?1 Events that are likely to happen in the near futurePolice officers are to visit every home in the area.2 Official arrangements, formal instructions & or-dersYou are not to leave without my permission.3 In an if-clause to say that something musthappen before something else can happenIf the human race is to survive, we must look atenvironmental problems now.Table 1: The full set of senses of the ?present simple?and ?be + to + infinitive?
abstract syntactic forms (ASFs),with an example for each.corpus, giving each section an equal weight.
Toguarantee ample representation of ASFs, we man-ually defined auxiliary words typical of each ASF(e.g., ?does?, ?been?
etc), and sampled hundreds ofsentences for each set of these auxiliary words.
Tomake sure that our definition of auxiliary words doesnot skew the sampling process, and to obtain ASFsthat do not have clear auxiliary words, we have alsoadded 1000 random sentences.
The number of CSFinstances obtained for each ASF ranges from 100(future perfect) to over 850 (present simple).
Allsenses are represented; the number of senses repre-sented by at least 15 CSFs is 77 (out of 103, averagenumber of CSFs per sense is 45.65).We implemented an interactive application thatdisplays a sentence and asks an annotator to (1) markwords that participate in the CSFs contained in thesentence; (2) specify the ASF(s) of these CSFs; and(3) select the appropriate ASF sense from the setof possible senses.
Annotators could also indicate?none of these senses?, which they did for 2.6% (122out of 4702) of the CSFs.Annotation was done by two annotators (univer-sity students).
To evaluate inter-annotator agree-ment, a set of 210 sentences (7% of the corpus),containing at least 10 examples of each ASF, wastagged by both annotators.
The CSF+ASF identifi-cation inter-annotator agreement was 98.7%, and theinter-annotator agreement for the senses was 84.2%.We will make the annotated corpus and annotationguidelines available online.6 Learning AlgorithmIn this section we describe our learning model forthe TSD task.
First, note that the syntactic sense isnot easy to deduce from readily computable anno-tations such as the sentence?s POS tagging, depen-dency structure, or parse tree (see Section 8).
Hence,a learning algorithm is definitely needed.As common in supervised learning, we encode theCSFs into feature vectors and then apply a learningalgorithm to induce a classifier.
We first discuss thefeature set and then the algorithm.Features.
We utilize three sets of features: basicfeatures, lexical features, and a set of features basedon part-of-speech (POS) tags (Table 2).
The ?aux-iliary words?
referred to in the table are the manu-ally specified words for each ASF that have assistedus in sampling the corpus (see Section 5).
?Contentwords?
are the non-auxiliary words appearing in theCSF4.
Content words are usually verbs, since we fo-cus here on tense-related ASFs.
The position anddistance of a form are based on its leftmost word(auxiliary or content).The personal pronouns used in the position fea-tures are: I, you, he, she, it, they, and we.
For4Usually, there is a single content word.
However, there maybe more than one, e.g.
for phrasal verbs.328simplicity, we considered every word starting witha capital letter that is not the first word in the sen-tence to be a name.Each ?Conditional?
CSF contains two tense CSFs.The one that is not the CSF currently encoded by thefeatures is referred to as its ?mate?.For the time lexical features we used 16 words(e.g., recently, often, now).
For the reported speechlexical features we used 14 words (e.g., said, replied,wrote5).
The words were obtained from the gram-mar texts and our corpus development set.The POS tagset used by the POS-based features isthat of the WSJ PennTreebank (see Section 7).
Thepossible verb tags in this tagset are: VB for the baseform, VBD for past tense, VBN for past participle,VBG for a present participle or gerund (-ing), VBPfor present tense that is not 3rd person singular, andVBZ for present simple 3rd person singular.Conjunctions and prepositions are addressedthrough the POS tags CC and IN.
Using the PRPtag to detect pronouns or lexical lists for conjunc-tions and prepositions yielded no significant changein the results.In Section 7 we explore the impact each of thefeature sets has on the performance of the algorithm.Our results indicate that the basic features have thestrongest impact, the POS-based features enhancethe performance in specific cases and the lexical fea-tures only marginally affect the final results.Algorithm.
Denote by xi the feature vector of aCSF instance i, by Ci the set of possible labels forxi, and by ci ?
Ci the correct label.
The trainingset is {(xj , Cj , cj)}nj=1.
Let (xn+1, Cn+1) be a testCSF.
As noted in Section 3, there are two versionsof the task, one in which Ci includes the totality ofsense labels, and one in which it includes only the la-bels associated with a particular ASF.
In both cases,the task is to select which of the labels in Cn+1 is itscorrect label cn+1.Owing to the task structure, it is preferable touse an algorithm that allows us to restrict the pos-sible labels of each CSF.
For both task versions, thiswould help in computing better probabilities duringthe training stage, since we know the ASF type oftraining CSFs.
For the task version in which the ASF5These are all in a past form due to the semantics of thereported speech form.Basic FeaturesForm words.
Auxiliary and content words of the CSF.Form type.
The type, if it is known during test time.Other forms.
The auxiliary and content words (andtype, if known) of the other CSFs present in the sen-tence.Position.
The position of the CSF in the sentence, itsdistance from the end of the sentence, whether it is inthe first (last) three words in the sentence, its distancefrom the closest personal pronoun or name.Wish.
Is there a CSF of type ?wish?
before the en-coded form, the number of CSFs between that ?wish?form and the encoded CSF (if there are several such?wish?
forms, we take the closest one to the encodedform).Conditional.
Does the word ?if?
appear before the en-coded form, is the ?if?
the first word in the sentence,the number of CSFs between the ?if?
and the encodedform, the auxiliary and content words (and type, ifknown) of the mate form, is there a comma betweenthe encoded form and its mate form, does the word?then?
appear between the encoded form and its mateform.Punctuation.
The type of end of sentence marker, dis-tance of the encoded form from the closest predeces-sor (successor) comma.Lexical FeaturesTime.
Time words appearing in the sentence, if any.Reported speech.
Reported speech words appearingin the sentence, if any.Be.
Does the encoded form contain the verb ?be?.Features Based on POS TagsForm.The POS of the verb in the encoded form.Other forms.
The POS of the verb in the other CSFsin the sentence.POS tags.
The POS tags of the two words to the left(right) of the encoded form.Conjunction POS.
Is there a Conjunction (CC) be-tween the encoded form and its closest predecessor(successor) form, the distance from that conjunction.Preposition POS.
Is there a Preposition (IN) betweenthe encoded form and its closest predecessor (succes-sor) form, the distance from that preposition.Table 2: Basic features (top), lexical features (middle)and POS tags-based features (bottom) used by the TSDclassifier.type is known at test time, this would also help dur-ing the test stage.For the version in which ASF type is known at testtime, we experimented in two scenarios.
In the first,329we take the ASF type at test time from the manualannotation and provide it to the algorithm.
In thesecond, instead of the manual annotation, we imple-mented a simple rule-based classifier for selectingASF types.
The classifier decides what is the type ofan ASF according to the POS tag of its verb and toits auxiliary words (given in the annotation).
For ex-ample, if we see the auxiliary phrase ?had been?
andthe verb POS is not VBG, then the ASF is ?past per-fect simple?.
This classifier?s accuracy on our devel-opment (test) data is 94.1 (91.6)%.
In this scenario,when given a test CSF, Xn+1, its set of possible la-bels Cn+1 is defined by the classifier output.
In thefeatures in which ASF type is used (see table 2), it istaken from the classifier output in this case.The sequential model algorithm presented byEven-Zohar and Roth (2001) directly supports thislabel restriction requirement 6.
We use the SNOWlearning architecture for multi-class classification(Roth, 1998), which contains an implementation ofthat algorithm.
The SNOW system allows us notto define restrictions if so desired.
It also lets uschoose the learning algorithm used when it buildsits classifier network.
The algorithm can be Percep-tron (MacKay, 2002), Winnow (Littlestone, 1988)or Naive Bayes (MacKay, 2002)7.
In Section 7 weanalyze the effect that these decisions have on ourresults.Classifier Selection.
Investigating the best config-uration of the SNOW system with development data,we found that Naive Bayes gave the best or closeto best result in all experimental conditions.
Wetherefore report our results when this algorithm isused.
Naive Bayes is particularly useful when rela-tively small amounts of training CSF instances areavailable (Zhang, 2004), and achieves good resultswhen compared to other classifiers for the WSD task(Mooney, 1996), which might explain our results.Fine tuning of Winnow parameters also leads to highperformance (sometimes the best), but most otherparameter configurations lead to disappointing re-6Note that the name of the learning algorithm is derivedfrom the fact that it utilizes classifiers to sequentially restrictthe number of competing classes while maintaining with highprobability the presence of the true outcome.
The classificationtask it performs is not sequential in nature.7Or a combination of these algorithms, which we did notexplore in this paper.sults.
For the Perceptron, most parameter config-urations lead to good results (much better than thebaseline), but these were a few percent worse thanthe best Winnow or Naive Bayes results.7 Experimental ResultsExperimental setup.
We divided the 3000 anno-tated sentences (containing 4702 CSFs) to threedatasets: training data (2100 sentences, 3183forms), development data (300 sentences, 498forms) and test data (600 sentences, 1021 forms).We used the development data to design the featuresfor our learning model and to tune the parametersof the SNOW sequential model.
In addition we usedthis data to design the rules of the ASF type classifier(which is not statistical and does not have a trainingphase).For the POS features, we induced POS tags usingthe MXPOST POS tagger (Ratnaparkhi, 1996).
Thetagger was trained on sections 2-21 of the WSJ Pen-nTreebank (Marcus et al, 1993) annotated with goldstandard POS tags.
We used a publicly available im-plementation of the sequential SNOW model8.We experimented in three conditions.
In the first(TypeUnknown), the ASF type is not known at testtime.
In the last two, it is known at test time.These two conditions differ in whether the type istaken from the gold standard annotation of the testsentences (TypeKnown), or from the output of thesimple rule-based classifier (TypeClassifier, see Sec-tion 6).
For both conditions, the results reported be-low are when both ASF type features and possiblelabels sets are provided during training by the man-ual annotation.
This is true also for the training ofthe MFS baseline (see below)9.We report an algorithm?s quality using accuracy,that is, the number of test CSFs that were correctlyresolved by the algorithm divided by the total num-ber of test CSFs.Baseline.
We compared the performance of our al-gorithm to the ?most frequent sense?
(MFS) base-8http://l2r.cs.uiuc.edu/?cogcomp/asoftware.php?skey=SNOW9For the TypeClassifier condition, we also experimented us-ing an ML technique that sometimes reduces noise, where train-ing is done using the classifier types.
We obtained very similarresults to those reported.330TypeUnknown TypeClassifier TypeKnownOur algorithm 49.7% 58.8% 62%MFS baseline 13.5% 42.9% 46.7%Table 3: Performance of our algorithm and of the MFSbaseline where at test time ASF type is known (right),unknown (left) or given by a simple rule-based classifier(middle).
Our algorithm is superior in all three condi-tions.Constrained Model Unconstrained ClassifierAll Base+Lexical All Base+Lexicalfeatures features features featuresType 57.9% 57.7% 53% 50.1%featuresNo type 57.2% 55.4% 48% 42.6%featuresTable 4: Impact of POS features.
When the constrainedmodel is used (left section), POS features have no effecton the results when ASF type information is encoded.When an unconstrained classifier is used, POS featuresaffect the results both when ASF type features are usedand when they are not (see discussion in the text).line.
This baseline is common in semantic disam-biguation tasks and is known to be quite strong.
Inthe condition where the ASF type is not known attest time, MFS gives each form in the test set thesense that was the overall most frequent in the train-ing set.
That is, in this case the baseline gives alltest set CSFs the same sense.
When the ASF typeis known at test time, MFS gives each test CSF themost frequent sense of that ASF type in the trainingset.
That is, in this case all CSFs having the sameASF type get the same sense, and forms of differenttypes are guaranteed to get different senses.Recall that the condition where ASF type isknown at test time is further divided to two condi-tions.
In the TypeKnown condition, MFS selects themost frequent sense of the manually created ASFtype, while in the TypeClassifier condition it selectsthe most frequent sense of the type decided by therule-based classifier.
In this condition, if the classi-fier makes a mistake, MFS will necessarily make amistake as well.Note that a random baseline which selects a sensefor every test CSF from a uniform distribution overthe possible senses (103 in our case) would scorevery poorly.Results.
Table 3 shows our results.
Results areshown where ASF type is not known at test time(left), when it is decided at test time by a rule-basedclassifier (middle) and when it is known at test time(right).
Our algorithm outperforms the MFS base-line in all three conditions.
As expected, both our al-gorithm and the MFS baseline perform better whenASF type information is available at test time (Type-Classifier and TypeKnown conditions), and improveas this data becomes more accurate (the TypeKnowncondition)10.Analyzing the per-type performance of our algo-rithm reveals that it outperforms the MFS baselinefor each and every ASF type.
For example, in theTypeKnown condition, the accuracy gain of our al-gorithm over the baseline11 varies from 4% for the?present perfect?
to 30.6% and 29.1% for the ?pastperfect?
and ?present simple?
ASFs.Below we analyze the roles of the different com-ponents of our learning algorithm in performing theTSD task.
Since this is the first exploration of thetask, it is important to understand what propertiesare essential for achieving good performance.
Theanalysis is done by experimenting with developmentdata, and focuses on the TypeKnown and TypeUn-known conditions.
Patterns for the TypeClassifiercondition are very similar to the patterns for theTypeKnown condition.The Possible Senses Constraint.
We use thelearning model of Even-Zohar and Roth (2001),which allows us to constrain the possible sensesan input vector can get to the senses of its ASFtype.
We ran our model without this constraint dur-ing both training and test time (recall that for theabove results, this constraint was always active dur-ing training).
In this case, the only difference be-tween the TypeKnown and the TypeUnknown con-ditions is whether ASF type features are encoded attest time.
In the TypeKnown condition, the accu-racy of the algorithm drops from 57.9% (when us-ing training and test time constraints and ASF typefeatures) to 53% (when using only ASF type fea-tures but no constraints).
In the TypeUnknown con-dition, accuracy drops from 57.24% (when usingtraining time constraints) to 48.03% (when neitherconstraints nor ASF type features are used).
Note10Recall that the performance of the rule-based ASF typeclassifier on test data is not 100% but 91.6% (Section 6).11accuracy(algorithm)?
accuracy(MFS).331that the difference between the constrained modeland the unconstrained model is quite large.The MFS baseline achieves on development data42.9% and 13.2% in the TypeKnown and TypeUn-known conditions respectively12.
Thus, the algo-rithm outperforms the baseline both when the con-strained model is used and when an unconstrainedmulti-class classifier is used.Note also that when constraints on the possiblelabels are available at training time, test time con-straints and ASF type features (whose inclusion isthe difference between the TypeKnown and Type-Unknown) have a minor effect on the results (57.9%for TypeKnown compared to 57.24% for TypeUn-known).
However, when training time constraintson the possible labels are not available at trainingtime, ASF type features alone do have a significanteffect on the result (53% for TypeKnown comparedto 48.03% for TypeUnknown).POS Features.
We next explore the impact of thePOS features on the results.
These features encodethe inflection of the verbs in the CSF, as well as thePOS tags of the two words to the left and right of theCSF.Verb forms provide some partial information cor-responding to the ASF type features encoded at theTypeKnown scenario.
Table 4 shows that when bothlabel constraints and ASF type features are used,POS features have almost no impact on the final re-sults.
When the constrained model is used but ASFtype features are not encoded, POS features have aneffect on the results.
We conclude that when usingthe constrained model, POS features are importantmainly for ASF type information.
When the uncon-strained classifier is used, POS features have an ef-fect on performance whether ASF type features areencoded or not.
In the last case the impact of POSfeatures is larger.
In other words, when using an un-constrained classifier, POS features give more thanASF type information to to the model.Lexical Features.
To explore the impact of thelexical features, we removed the following features:time words, reported speech words and ?be?
indi-cation features.
We saw no impact on model per-formance when using the constrained model, and a12Note that these numbers are for development data only.0.5% decrease when using the unconstrained classi-fier.
That is, our model does not require these lexicalfeatures, which is somewhat counter-intuitive.
Lex-ical statistics may turn out to be helpful when usinga much larger training set.Conditional and Wish Features.
The condition-als and ?wish?
features have a more substantial im-pact on the results, as they have a role in defining theoverall syntactic structure of the sentence.
Discard-ing these features leads to 4% and 1.4% degradationin model accuracy when using the constrained andunconstrained models respectively.8 Relevant Previous WorkAs far as we know, this is the first paper to addressthe TSD task.
In this section we describe relatedresearch directions and compare them with TSD.A relevant task to TSD is WSD (Section 1 andSection 3).
Many algorithmic approaches and tech-niques have been applied to supervised WSD (forreviews see (Agirre and Edmonds, 2006; Mihalceaand Pedersen, 2005; Navigli, 2009)).
Among theseare various classifiers, ensemble methods combin-ing several supervised classifiers, bootstrapping andsemi-supervised learning methods, using the Webas a corpus and knowledge-based methods relyingmainly on machine readable dictionaries.
Specif-ically related to this paper are works that exploitsyntax (Martinez et al, 2002; Tanaka et al, 2007)and ensemble methods (e.g.
(Brody et al, 2006))to WSD.
The references above also describe someunsupervised word sense induction algorithms.Our TSD algorithm uses the SNOW algorithm,which is a sparse network of classifiers (Section 6).Thus, it most resembles the ensemble approach toWSD.
That approach has achieved very good resultsin several WSD shared tasks (Pedersen, 2000; Flo-rian and Yarowsky, 2002).Since temporal reasoning is a direct applica-tion of TSD, research on this direction is relevant.Such research goes back to (Passonneau, 1988),which introduced the PUNDIT temporal reasoningsystem.
For each tensed clause, PUNDIT first de-cides whether it refers to an actual time (as in ?Weflew TWA to Boston?)
or not (as in ?Tourists flewTWA to Boston?, or ?John always flew his own planeto Boston?).
The temporal structure of actual time332clauses is then further analyzed.
PUNDIT?s classi-fication is much simpler than in the TSD task, ad-dressing only actual vs. non-actual time.
PUNDIT?salgorithmic approach is that of a Prolog rule basedsystem, compared to our statistical learning corpus-based approach.
We are not aware of further re-search that followed their sense disambiguation di-rection.Current temporal reasoning research focuses ontemporal ordering of events (e.g., (Lapata, 2006;Chambers and Jurafsky, 2008)), for which an ac-cepted atomic task is the identification of the tem-poral relation between two expressions (see e.g., theTempEval task in SemEval ?07 (Verhagen et al,2007)).
This direction is very different from TSD,which deals with the semantics of individual con-crete tense syntactic forms.
In this sense, TSD is aneven more atomic task for temporal reasoning.A potential application of TSD is machine trans-lation where it can assist in translating tense and as-pect.
Indeed several papers have explored tense andaspect in the MT context.
Dorr (1992) explored theintegration of tense and aspect information with lex-ical semantics for machine translation.
Schiehlen(2000) analyzed the effect tense understanding hason MT.
Ye and Zhang (2005) explored tense taggingin a cross-lingual context.
Ye et al, (2006) extractedfeatures for tense translation between Chinese andEnglish.
Murata et al, (2007) compared the perfor-mance of several MT systems in translating tenseand aspect and found that various ML techniquesperform better on the task.Another related field is ?deep?
parsing, where asentence is annotated with a structure containing in-formation that might be relevant for semantic inter-pretation (e.g.
(Hajic, 1998; Baldwin et al, 2007)).TSD senses, however, are not explicitly representedin these grammatical structures, and we are notaware of any work that utilized them to do some-thing close to TSD.
This is a good subject for futureresearch.9 Conclusion and Future WorkIn this paper we introduced the Tense Sense Disam-biguation (TSD) task, defined as selecting the cor-rect sense of a concrete tense syntactic form in a sen-tence among the senses of abstract syntactic formsin a syntactic sense dictionary.
Unlike in other se-mantic disambiguation tasks, the sense to be disam-biguated is not lexical but of a syntactic structure.We prepared a syntactic sense dictionary, annotateda corpus by it, and developed a supervised classifierfor sense disambiguation that outperformed a strongbaseline.An obvious direction for future work is to expandthe annotated corpus and improve the algorithm byexperimenting with additional features.
For exam-ple, we saw that seeing the full paragraph containinga sentence helps human annotators decide on the ap-propriate sense which implies that using larger con-texts may improve the algorithm.TSD can be a very useful operation for varioushigh-level applications, for example textual infer-ence, question answering, and information retrieval,in the same way that textual entailment (Dagan etal., 2006) was designed to be.
In fact, TSD can assisttextual entailment as well, since the sense of a tenseform may provide substantial information about therelations entailed from the sentence.
Using TSDin such applications is a major direction for futurework.ReferencesEneko Agirre and Philip Edmonds (Eds).
2006.
WordSense Disambiguation: Algorithms and Applications.Springer Verlag.Timothy Baldwin, Mark Dras, Julia Hockenmaier, TracyHolloway King, and Gertjan van Noord.
2007.
TheImpact of Deep Linguistic Processing on ParsingTechnology.
IWPT ?07.Douglas Biber, Stig Johansson, Geoffrey Leech, SusanConard, Edward Finegan.
1999.
Longman Grammarof Spoken and Written English.
Longman.Samuel Brody, Roberto Navigli and Mirella Lapata.2006.
Ensemble Methods for Unsupervised WSD.ACL-COLING ?06.Lou Burnard.
2000.
The British National Corpus UserReference Guide.
Technical Report, Oxford Univer-sity.Nathanael Chambers and Dan Jurafsky.
2008.
JointlyCombining Implicit Constraints Improves TemporalOrdering.
EMNLP ?08.Michelle Cutrer.
1994.
Time and Tense in Narratives andin Everyday Language.
PhD dissertation, Universityof California at San Diego.333Ido Dagan, Oren Glickman and Bernardo Magnini.
2006.The PASCAL Recognising Textual Entailment Chal-lenge.
Lecture Notes in Computer Science 2006,3944:177-190.John Dinsmore.
1991.
Partitioned representations.
Dor-drecht, Netherlands: Kluwer.Bonnie Dorr.
1992.
A Two-Level Knowledge Repre-sentation for Machine Translation: Lexical Semanticsand Tense/Aspect.
In James Pustejovsky and SabineBergler, editors, Lexical Semantics and KnowledgeRepresentation.Yair Even-Zohar and Dan Roth.
2001.
A SequentialModel for Multi-Class Classification.
EMNLP ?01.Gilles Fauconnier.
2007.
Mental Spaces.
in Dirk Geer-aerts and Hubert Cuyckens, editors, The Oxford Hand-book of Cognitive Linguistics.Radu Florian and David Yarowsky.
2002.
ModelingConsensus: Classifier Combination for Word SenseDisambiguation.
EMNLP ?02.Adele E. Goldberg.
1995.
Constructions: A Construc-tion Grammar Approach to Argument Structure.
Uni-versity of Chicago Press.Jan Hajic.
1998.
Building a Syntactically AnnotatedCorpus: The Prague Dependency Treebank.
Issues ofValency and Meaning, 106?132.Martin Hewings.
2005.
Advanced Grammar in Use, Sec-ond Edition.
Cambridge University University.Mirella Lapata and Alex Lascarides.
2006.
LearningSentence-internal Temporal Relations.
Journal of Ar-tificial Intelligence Research, 27:85?117.Nick Littlestone.
1988.
Learning Quickly When Irrele-vant Attributes Abound: A New Linear-threshold Al-gorithm.
Machine Learning, 285?318.David MacKay.
2002.
Information Theory, Infer-ence and Learning Algorithms.
Cambridge UniversityPress.Mitchell P. Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.David Martinez, Eneko Agirre, Lluis Marquez.
2002.Syntactic Features for High Precision Word Sense Dis-ambiguation.
COLING ?02.Rada Mihalcea and Ted Pedersen.
2005.
Advances inWord Sense Disambiguation.
Tutorial in ACL ?05.Raymond J. Mooney.
1996.
Comparative Experimentson Disambiguating Word Senses: An Illustration ofthe Role of Bias in Machine Learning.
EMNLP ?96.Masaki Murata, Qing Ma, Kiyotaka Uchimoto, ToshiyukiKanamaru and Hitoshi Isahara.
2007.
Japanese-to-English translations of Tense, Aspect, and ModalityUsing Machine-Learning Methods and Comparisonwith Cachine-Translation Systems on Market.
LREC?07.Raymond Murphy.
1994.
English Grammar In Use, Sec-ond Edition.
Cambridge University Press.Raymond Murphy.
2007.
Essential Grammar In Use,Third Edition.
Cambridge University Press.Roberto Navigli.
2009.
Word Sense Disambiguation: aSurvey.
ACM Computing Surveys, 41(2) 1?69.Rebecca J. Passonneau.
1988.
A Computational Modelof Semantics of Tenses and Aspect.
ComputationalLinguistics, 14(2):44?60.Ted Pedersen.
2000.
A Simple Approach to Building En-sembles of Naive Bayesian Classifiers for Word SenseDisambiguation.
NAACL ?00.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
EMNLP ?06.Dan Roth.
1998.
Learning to Resolve Natural LanguageAmbiguities: A Unified Approach.
AAAI ?98.Michael Schiehlen.
2000.
Granularity Effects in TenseTranslation.
COLING ?00.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
SemEval-2007 Task 15: TempEval TemporalRelation Identification.
ACL ?07.Takaaki Tanaka, Francis Bond, Timothy Baldwin, SanaeFujita and Chikara Hashimoto.
2007.
Word SenseDisambiguation Incorporating Lexical and StructuralSemantic Information.
EMNLP-CoNLL ?07.Dave Willis and Jon Wright.
2003.
Collins Cobuild El-ementary English Grammar, Second Edition.
Harper-Collins Publishers.Dave Willis.
2004.
Collins Cobuild Intermediate EnglishGrammar, Second Edition.
HarperCollins Publishers.Yang Ye, Victoria Li Fossum and Steven Abney.
2006.Latent Features in Automatic Tense Translation be-tween Chinese and English.
SIGHAN ?06.Yang Ye and Zhu Zhang.
2005.
Tense Tagging for Verbsin Cross-Lingual Context: A Case Study.
IJCNLP ?05.Harry Zhang.
2004.
The Optimality of Naive Bayes.FLAIRS ?04.334
