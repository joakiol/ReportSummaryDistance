Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 843?853, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsInducing a Discriminative Parser to Optimize MachineTranslation ReorderingGraham Neubig1,2, Taro Watanabe2, Shinsuke Mori11Graduate School of Informatics, Kyoto UniversityYoshida Honmachi, Sakyo-ku, Kyoto, Japan2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, JapanAbstractThis paper proposes a method for learninga discriminative parser for machine trans-lation reordering using only aligned par-allel text.
This is done by treating theparser?s derivation tree as a latent variablein a model that is trained to maximize re-ordering accuracy.
We demonstrate thatefficient large-margin training is possibleby showing that two measures of reorder-ing accuracy can be factored over the parsetree.
Using this model in the pre-orderingframework results in significant gains intranslation accuracy over standard phrase-based SMT and previously proposed unsu-pervised syntax induction methods.1 IntroductionFinding the appropriate word ordering in thetarget language is one of the most difficult prob-lems for statistical machine translation (SMT),particularly for language pairs with widely di-vergent syntax.
As a result, there is a largeamount of previous research that handles theproblem of reordering through the use of im-proved reordering models for phrase-based SMT(Koehn et al2005), hierarchical phrase-basedtranslation (Chiang, 2007), syntax-based trans-lation (Yamada and Knight, 2001), or pre-ordering (Xia and McCord, 2004).In particular, systems that use source-language syntax allow for the handling of long-distance reordering without large increases inThe first author is now affiliated with the Nara Instituteof Science and Technology.decoding time.
However, these require a goodsyntactic parser, which is not available for manylanguages.
In recent work, DeNero and Uszko-reit (2011) suggest that unsupervised grammarinduction can be used to create source-sentenceparse structure for use in translation as a partof a pre-ordering based translation system.In this work, we present a method for inducinga parser for SMT by training a discriminativemodel to maximize reordering accuracy whiletreating the parse tree as a latent variable.
As alearning framework, we use online large-marginmethods to train the model to directly minimizetwo measures of reordering accuracy.
We pro-pose a variety of features, and demonstrate thatlearning can succeed when no linguistic informa-tion (POS tags or parse structure) is available inthe source language, but also show that this lin-guistic information can be simply incorporatedwhen it is available.
Experiments find that theproposed model improves both reordering andtranslation accuracy, leading to average gainsof 1.2 BLEU points on English-Japanese andJapanese-English translation without linguisticanalysis tools, or up to 1.5 BLEU points whenthese tools are incorporated.
In addition, weshow that our model is able to effectively max-imize various measures of reordering accuracy,and that the reordering measure that we choosehas a direct effect on translation results.2 Preordering for SMTMachine translation is defined as transforma-tion of source sentence F = f1 .
.
.
fJ to targetsentence E = e1 .
.
.
eI .
In this paper, we take843Figure 1: An example with a source sentence F re-ordered into target order F ?, and its correspondingtarget sentence E. D is one of the BTG derivationsthat can produce this ordering.the pre-ordering approach to machine transla-tion (Xia and McCord, 2004), which performstranslation as a two step process of reorderingand translation (Figure 1).
Reordering first de-terministically transforms F into F ?, which con-tains the same words as F but is in the order ofE.
Translation then transforms F ?
into E usinga method such as phrase-based SMT (Koehn etal., 2003), which can produce accurate transla-tions when only local reordering is required.This general framework has been widely stud-ied, with the majority of works relying on asyntactic parser being available in the sourcelanguage.
Reordering rules are defined overthis parse either through machine learning tech-niques (Xia and McCord, 2004; Zhang et al2007; Li et al2007; Genzel, 2010; Dyer andResnik, 2010; Khalilov and Sima?an, 2011) orlinguistically motivated manual rules (Collins etal., 2005; Xu et al2009; Carpuat et al2010;Isozaki et al2010b).
However, as building aparser for each source language is a resource-intensive undertaking, there has also been someinterest in developing reordering rules withoutthe use of a parser (Rottmann and Vogel, 2007;Tromble and Eisner, 2009; DeNero and Uszko-reit, 2011; Visweswariah et al2011), and wewill follow this thread of research in this paper.In particular, two methods deserve mentionfor being similar to our approach.
First, DeNeroand Uszkoreit (2011) learn a reordering modelthrough a three-step process of bilingual gram-mar induction, training a monolingual parserto reproduce the induced trees, and traininga reordering model that selects a reorderingbased on this parse structure.
In contrast, ourmethod trains the model in a single step, treat-ing the parse structure as a latent variable ina discriminative reordering model.
In additionTromble and Eisner (2009) and Visweswariah etal.
(2011) present models that use binary clas-sification to decide whether each pair of wordsshould be placed in forward or reverse order.
Incontrast, our method uses traditional context-free-grammar models, which allows for simpleparsing and flexible parameterization, includingfeatures such as those that utilize the existenceof a span in the phrase table.
Our work is alsounique in that we show that it is possible to di-rectly optimize several measures of reorderingaccuracy, which proves important for achievinggood translations.13 Training a Reordering Model withLatent DerivationsIn this section, we provide a basic overview ofthe proposed method for learning a reorderingmodel with latent derivations using online dis-criminative learning.3.1 Space of ReorderingsThe model we present here is based on thebracketing transduction grammar (BTG, Wu(1997)) framework.
BTGs represent a binarytree derivation D over the source sentence Fas shown in Figure 1.
Each non-terminal nodecan either be a straight (str) or inverted (inv)production, and terminals (term) span a non-empty substring f .2The ordering of the sentence is determined bythe tree structure and the non-terminal labelsstr and inv, and can be built bottom-up.
Eachsubtree represents a source substring f and itsreordered counterpart f ?.
For each terminalnode, no reordering occurs and f is equal to f ?.1The semi-supervised method of Katz-Brown et al(2011) also optimizes reordering accuracy, but requiresmanually annotated parses as seed data.2In the original BTG framework used in translation,terminals produce a bilingual substring pair f/e, but aswe are only interested in reordering the source F , wesimplify the model by removing the target substring e.844For each non-terminal node spanning f with itsleft child spanning f1 and its right child span-ning f2, if the non-terminal symbol is str, thereordered strings will be concatenated in orderas f ?
= f ?1f ?2, and if the non-terminal symbol isinv, the reordered strings will be concatenatedin inverted order as f ?
= f ?2f ?1.We define the space of all reorderings that canbe produced by the BTG as F ?, and attempt tofind the best reordering F?
?
within this space.33.2 Reorderings with LatentDerivationsIn order to find the best reordering F?
?
given onlythe information in the source side sentence F , wedefine a scoring function S(F ?|F ), and choosethe ordering of maximal score:F?
?
= arg maxF ?S(F ?|F ).As our model is based on reorderings licensedby BTG derivations, we also assume that thereis an underlying derivation D that produced F ?.As we can uniquely determine F ?
given F andD, we can define a scoring function S(D|F ) overderivations, find the derivation of maximal scoreD?
= arg maxDS(D|F )and use D?
to transform F into F ?.Furthermore, we assume that the scoreS(D|F ) is the weighted sum of a number of fea-ture functions defined over D and FS(D|F,w) =?iwi?i(D,F )where ?i is the ith feature function, and wi isits corresponding weight in weight vector w.Given this model, we must next consider howto learn the weights w. As the final goal of ourmodel is to produce good reorderings F ?, it isnatural to attempt to learn weights that will al-low us to produce these high-quality reorderings.3BTGs cannot reproduce all possible reorderings, butcan handle most reorderings occurring in natural trans-lated text (Haghighi et al2009).Figure 2: An example of (a) the ranking functionr(fj), (b) loss according to Kendall?s ?
, (c) loss ac-cording to chunk fragmentation.4 Evaluating ReorderingsBefore we explain the learning algorithm, wemust know how to distinguish whether the F ?produced by the model is good or bad.
Thissection explains how to calculate oracle reorder-ings, and assign each F ?
a loss and an accuracyaccording to how well it reproduces the oracle.4.1 Calculating Oracle OrderingsIn order to calculate reordering quality, we firstdefine a ranking function r(fj |F,A), which indi-cates the relative position of source word fj inthe proper target order (Figure 2 (a)).
In or-der to calculate this ranking function, we defineA = a1, .
.
.
,aJ , where each aj is a set of the in-dices of the words in E to which fj is aligned.4Given these alignments, we define an orderingfunction aj1 < aj2 that indicates that the in-dices in aj1 come before the indices in aj2 .
For-mally, we define this function as ?the first indexin aj1 is at most the first index in aj2 , similarlyfor the last index, and either the first or lastindex in aj1 is less than that of aj2 .
?Given this ordering, we can sort every align-ment aj , and use its relative position in the sen-tence to assign a rank to its word r(fj).
In4Null alignments require special treatment.
To do so,we can place unaligned brackets and quotes directly be-fore and after the spans they surround, and attach allother unaligned words to the word directly to the rightfor head-initial languages (e.g.
English), or left for head-final languages (e.g.
Japanese).845the case of ties, where neither aj1 < aj2 noraj2 < aj1 , both fj1 and fj2 are assigned thesame rank.
We can now define measures of re-ordering accuracy for F ?
by how well it arrangesthe words in order of ascending rank.
It shouldbe noted that as we allow ties in rank, thereare multiple possible F ?
where all words are instrictly ascending order, which we will call ora-cle orderings.4.2 Kendall?s ?The first measure of reordering accuracy thatwe will consider is Kendall?s ?
(Kendall, 1938),a measure of pairwise rank correlation whichhas been proposed for evaluating translation re-ordering accuracy (Isozaki et al2010a; Birchet al2010) and pre-ordering accuracy (Talbotet al2011).
The fundamental idea behind themeasure lies in comparisons between each pair ofelements f ?j1 and f ?j2 of the reordered sentence,where j1 < j2.
Because j1 < j2, f ?j1 comes beforef ?j2 in the reordered sentence, the ranks shouldbe r(f ?j1) ?
r(f ?j2) in order to produce the cor-rect ordering.Based on this criterion, we first define a lossLt(F ?)
that will be higher for orderings that arefurther from the oracle.
Specifically, we take thesum of all pairwise orderings that do not followthe expected orderLt(F ?)
=J?1?j1=1J?j2=j1+1?
(r(f ?j1) > r(f?j2))where ?(?)
is an indicator function that is 1 whenits condition is true, and 0 otherwise.
An exam-ple of this is given in Figure 2 (b).To calculate an accuracy measure for orderingF ?, we first calculate the maximum loss for thesentence, which is equal to the total number ofnon-equal rank comparisons in the sentence5maxF ?Lt(F ?)
=J?1?j1=1J?j2=j1+1?
(r(f ?j1) 6= r(f?j2)).
(1)5The traditional formulation of Kendall?s ?
assumesno ties in rank, and thus the maximum loss can be cal-culated as J(J ?
1)/2.Finally, we use this maximum loss to normalizethe actual loss to get an accuracyAt(F ?)
= 1?Lt(F ?)maxF?
?Lt(F?
?
),which will take a value between 0 (when F ?
hasmaximal loss), and 1 (when F ?
matches one ofthe oracle orderings).
In Figure 2 (b), Lt(F ?)
=2 and maxF?
?Lt(F?
?)
= 8, so At(F ?)
= 0.75.4.3 Chunk FragmentationAnother measure that has been used in eval-uation of translation accuracy (Banerjee andLavie, 2005) and pre-ordering accuracy (Talbotet al2011) is chunk fragmentation.
This mea-sure is based on the number of chunks that thesentence needs to be broken into to reproducethe correct ordering, with a motivation that thenumber of continuous chunks is equal to thenumber of times the reader will have to jump toa different position in the reordered sentence toread it in the target order.
One way to measurethe number of continuous chunks is consideringwhether each word pair f ?j and f ?j+1 is discon-tinuous (the rank of f ?j+1 is not equal to or onegreater than f ?j)discont(f ?j , f ?j+1) =?
(r(f ?j) 6= r(f ?j+1) ?
r(f ?j) + 1 6= r(f ?j+1))and sum over all word pairs in the sentence tocreate a sentence-based lossLc(F ?)
=J?1?j=1discont(f ?j , f ?j+1) (2)While this is the formulation taken by previ-ous work, we found that this under-penalizesbad reorderings of the first and last words ofthe sentence, which can contribute to the lossonly once, as opposed to other words which cancontribute to the loss twice.
To account forthis, when calculating the chunk fragmentationscore, we additionally add two sentence bound-ary words f0 and fJ+1 with ranks r(f0) = 0 andr(fJ+1) = 1 + maxf ?j?F ?r(f ?j) and redefine the sum-mation in Equation (2) to consider these words(e.g.
Figure 2 (c)).846procedure WeightUpdate(F , A, w)D ?
parse(F,w) .
Create parse forestD?
?
argmaxD?DS(D|F,w) + L(D|F,A).
Find the model parseD?
?
argminD?DL(D|F,A)?
?S(D|F,w).
Find the oracle parseif L(D?|F,A) 6= L(D?|F,A) thenw ?
?
(w + ?(?
(D?, F )?
?
(D?, F ))).
Perform weight updateend ifend procedureFigure 3: An online update for sentence F , alignmentA, and weight vector w. ?
is a very small constant,and ?
and ?
are defined by the update strategy.Similarly to Kendall?s ?
, we can also definean accuracy measure between 0 and 1 using themaximum loss, which will be at most J + 1,which corresponds to the total number of com-parisons made in calculating the loss6Ac(F ?)
= 1?Lc(F ?
)J + 1.In Figure 2 (c), Lc(F ?)
= 3 and J + 1 = 6, soAc(F ?)
= 0.5.5 Learning a BTG Parser forReorderingNow that we have a definition of loss over re-orderings produced by the model, we have aclear learning objective: we would like to findreorderings F ?
with low loss.
The learning algo-rithm we use to achieve this goal is motivatedby discriminative training for machine transla-tion systems (Liang et al2006), and extendedto use large-margin training in an online frame-work (Watanabe et al2007).5.1 Learning AlgorithmLearning uses the general framework of large-margin online structured prediction (Crammeret al2006), which makes several passes throughthe data, finding a derivation with high modelscore (the model parse) and a derivation with6It should be noted that for sentences of length one orsentences with tied ranks, the maximum loss may be lessthan J +1, but for simplicity we use this approximation.minimal loss (the oracle parse), and updating wif these two parses diverge (Figure 3).In order to create both of these parses effi-ciently, we first create a parse forest encoding alarge number of derivations Di according to themodel scores.
Next, we find the model parse D?i,which is the parse in the forest Di that maxi-mizes the sum of the model score and the lossS(Dk|Fk,w)+L(Dk|Fk, Ak).
It should be notedthat here we are considering not only the modelscore, but also the derivation?s loss.
This isnecessary for loss-driven large-margin training(Crammer et al2006), and follows the basicintuition that during training, we would like tomake it easier to select negative examples withlarge loss, causing these examples to be penal-ized more often and more heavily.We also find an oracle parse D?i, which is se-lected solely to minimize the loss L(Dk|Fk, Ak).One important difference between the model wedescribe here and traditional parsing models isthat the target derivation D?k is a latent variable.Because many Dk achieve a particular reorder-ing F ?, many reorderings F ?
are able to mini-mize the loss L(F ?k|Fk, Ak).
Thus it is necessaryto choose a single oracle derivation to treat asthe target out of many equally good reorderings.DeNero and Uszkoreit (2011) resolve this ambi-guity with four features with empirically tunedscores before training a monolingual parser andreordering model.
In contrast, we follow previ-ous work on discriminative learning with latentvariables (Yu and Joachims, 2009), and breakties within the pool of oracle derivations by se-lecting the derivation with the largest modelscore.
From an implementation point of view,this can be done by finding the derivation thatminimizes L(Dk|Fk, Ak)?
?S(Dk|Fk,w), where?
is a constant small enough to ensure that theeffect of the loss will always be greater than theeffect of the score.Finally, if the model parse D?k has a loss thatis greater than that of the oracle parse D?k, weupdate the weights to increase the score of theoracle parse and decrease the score of the modelparse.
Any criterion for weight updates may beused, such as the averaged perceptron (Collins,2002) and MIRA (Crammer et al2006), but847we opted to use Pegasos (Shalev-Shwartz et al2007) as it allows for the introduction of regu-larization and relatively stable learning.To perform this full process, given a sourcesentence Fk, alignment Ak, and model weightsw we need to be able to efficiently calculatescores, calculate losses, and create parse forestsfor derivations Dk, the details of which will beexplained in the following sections.5.2 Scoring Derivation TreesFirst, we must consider how to efficiently assignscores S(D|F,w) to a derivation or forest duringparsing.
The most standard and efficient way todo so is to create local features that can be cal-culated based only on the information includedin a single node d in the derivation tree.
Thescore of the whole tree can then be expressed asthe sum of the scores from each node:S(D|F,w) =?d?DS(d|F,w)=?d?D?iwi?i(d, F ).Based on this restriction, we define a number offeatures that can be used to score the parse tree.To ease explanation, we represent each node inthe derivation as d = ?s, l, c, c + 1, r?, where sis the node?s symbol (str, inv, or term), whilel and r are the leftmost and rightmost indicesof the span that d covers.
c and c + 1 are therightmost index of the left child and leftmostindex of the right child for non-terminal nodes.All features are intersected with the node la-bel s, so each feature described below corre-sponds to three different features (or two forfeatures applicable to only non-terminal nodes).?
?lex: Identities of words in positions fl, fr,fc, fc+1, fl?1, fr+1, flfr, and fcfc+1.?
?class: Same as ?lex, but with words ab-stracted to classes.
We use the 50 classesautomatically generated by Och (1999)?smethod that are calculated during align-ment in standard SMT systems.?
?balance: For non-terminals, features indi-cating whether the length of the left span(c?
l+1) is lesser than, equal to, or greaterthan the length of the right span (r ?
c).?
?table: Features, bucketed by length, thatindicate whether ?fl .
.
.
fr?
appears as acontiguous phrase in the SMT trainingdata, as well as the log frequency of thenumber of times the phrase appears totaland the number of times it appears as acontiguous phrase (DeNero and Uszkoreit,2011).
Phrase length is limited to 8, andphrases of frequency one are removed.?
?pos: Same as ?lex, but with words ab-stracted to language-dependent POS tags.?
?cfg: Features indicating the label of thespans fl .
.
.
fr, fl .
.
.
fc, and fc+1 .
.
.
fr in asupervised parse tree, and the intersectionof the three labels.
When spans do not cor-respond to a span in the supervised parsetree, we indicate ?no span?
with the label?X?
(Zollmann and Venugopal, 2006).Most of these features can be calculated fromonly a parallel corpus, but ?pos requires a POStagger and ?cfg requires a full syntactic parserin the source language.
As it is preferable tohave a method that is applicable in languageswhere these tools are not available, we performexperiments both with and without the featuresthat require linguistic analysis tools.5.3 Finding Losses for Derivation TreesThe above features ?
and their correspondingweights w are all that are needed to calculatescores of derivation trees at test time.
However,during training, it is also necessary to find modelparses according to the loss-augmented scoringfunction S(D|F,w)+L(D|F,A) or oracle parsesaccording to the loss L(D|F,A).
As noted byTaskar et al2003), this is possible if our lossescan be factored in the same way as the featurespace.
In this section, we demonstrate that theloss L(d|F,A) for the evaluation measures wedefined in Section 4 can (mostly) be factoredover nodes in a fashion similar to features.8485.3.1 Factoring Kendall?s ?For Kendall?s ?
, in the case of terminal nodes,Lt(d = ?term, l, r?|F,A) can be calculated byperforming the summation in Equation (1).
Wecan further define this sum recursively and usememoization for improved efficiencyLt(d|F,A) =Lt(?term, l, r ?
1?|F,A)+r?1?j=l?
(r(fj) > r(fr)).
(3)For non-terminal nodes, we first focus onstraight non-terminals with parent node d =?str, l, c, c+1, r?, and left and right child nodesdl = ?sl, l, lc, lc+1, c?
and dr = ?sr, c+1, rc, rc+1, r?.
First, we note that the loss for the subtreerooted at d can be expressed asLt(d|F,A) =Lt(dl|F,A) + Lt(dr|F,A)+c?j1=lr?j2=c+1?
(r(fj1) > r(fj2)).In other words, the subtree?s total loss can befactored into the loss of its left subtree, theloss of its right subtree, and the additional losscontributed by comparisons between the wordsspanning both subtrees.
In the case of invertedterminals, we must simply reverse the compari-son in the final sum to be ?
(r(fj1) < r(fj2)).5.3.2 Factoring Chunk FragmentationChunk fragmentation loss can be factored in asimilar fashion.
First, it is clear that the loss forthe terminal nodes can be calculated efficientlyin a fashion similar to Equation (3).
In order tocalculate the loss for non-terminals d, we notethat the summation in Equation (2) can be di-vided into the sum over the internal bi-gramsin the left and right subtrees, and the bi-gramspanning the reordered treesLc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)+ discont(f ?c, f ?c+1).However, unlike Kendall?s ?
, this equation re-lies not on the ranks of fc and fc+1 in the origi-nal sentence, but on the ranks of f ?c and f ?c+1 inthe reordered sentence.
In order to keep trackof these values, it is necessary to augment eachnode in the tree to be d = ?s, l, c, c + 1, r, tl, tr?with two additional values tl and tr that indi-cate the position of the leftmost and rightmostwords after reordering.
Thus, a straight non-terminal parent d with children dl = ?sl, l, lc, lc+1, c, tl, tlr?
and dr = ?sr, c+1, rc, rc+1, r, trl, tr?will have loss as followsLc(d|F,A) =Lc(dl|F,A) + Lc(dr|F,A)+ discont(ftlr, ftrl)with a similar calculation being possible for in-verted non-terminals.5.4 Parsing Derivation TreesFinally, we must be able to create a parse forestfrom which we select model and oracle parses.As all feature functions factor over single nodes,it is possible to find the parse tree with the high-est score in O(J3) time using the CKY algo-rithm.
However, when keeping track of targetpositions for calculation of chunk fragmentationloss, there are a total of O(J5) nodes, an unrea-sonable burden in terms of time and memory.To overcome this problem, we note that this set-ting is nearly identical to translation using syn-chronous CFGs with an integrated bigram LM,and thus we can employ cube-pruning to reduceour search space (Chiang, 2007).6 ExperimentsOur experiments test the reordering and trans-lation accuracy of translation systems using theproposed method.
As reordering metrics, we useKendall?s ?
and chunk fragmentation (Talbot etal., 2011) comparing the system F ?
and oracleF ?
calculated with manually created alignments.As translation metrics, we use BLEU (Papineniet al2002), as well as RIBES (Isozaki et al2010a), which is similar to Kendall?s ?
, but eval-uated on the target sentence E instead of the re-ordered sentence F ?.
All scores are the averageof three training runs to control for randomnessin training (Clark et al2011).For translation, we use Moses (Koehn et al2007) with lexicalized reordering (Koehn et al2005) in all experiments.
We test three types849en-ja ja-enChunk ?
BLEU RIBES Chunk ?
BLEU RIBESorig 61.22 73.46 21.87 68.25 66.42 72.99 18.34 65.363-step 63.51 72.55 21.45 67.66 67.17 73.01 17.78 64.423-step+?pos 64.28 72.11 21.45 67.44 67.56 74.21 18.18 64.653-step+?cfg 65.76 75.32 21.67 68.47 67.23 74.06 18.18 64.93lader 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93lader+?pos 73.97 79.24 23.32 69.78 75.49 78.79 19.89 67.24lader+?cfg 75.06 80.53 23.36 70.89 75.14 77.80 19.35 66.12Table 2: Reordering (chunk, ?)
and translation (BLEU, RIBES) results for each system.
Bold numbersindicate no significant difference from the best system (bootstrap resampling with p > 0.05) (Koehn, 2004).sent.
word (ja) word (en)RM-train 602 14.5k 14.3kRM-test 555 11.2k 10.4kTM/LM 329k 6.08M 5.91MTune 1166 26.8k 24.3kTest 1160 28.5k 26.7kTable 1: The number of sentences and words fortraining and testing the reordering model (RM),translation model (TM), and language model (LM).of pre-ordering: original order with F ?
?
F(orig), pre-orderings learned using the 3-stepprocess of DeNero and Uszkoreit (2011) (3-step), and the proposed model with latentderivations (lader).7 Except when stated oth-erwise, lader was trained to minimize chunkfragmentation loss with a cube pruning stackpop limit of 50, and the regularization constantof 10?3 (chosen through cross-validation).We test our systems on Japanese-English andEnglish-Japanese translation using data fromthe Kyoto Free Translation Task (Neubig, 2011).We use the training set for training translationand language models, the development set forweight tuning, and the test set for testing (Table1).
We use the designated development and testsets of manually created alignments as trainingdata for the reordering models, removing sen-tences of more than 60 words.As default features for lader and the mono-lingual parsing and reordering models in 3-step,we use all the features described in Section 5.27Available open-source: http://phontron.com/laderexcept ?pos and ?cfg.
In addition, we test sys-tems with ?pos and ?cfg added.
For English,we use the Stanford parser (Klein and Manning,2003) for both POS tagging and CFG parsing.For Japanese, we use the KyTea tagger (Neu-big et al2011) for POS tagging,8 and the EDAword-based dependency parser (Flannery et al2011) with simple manual head-rules to converta dependency parse to a CFG parse.6.1 Effect of Pre-orderingTable 2 shows reordering and translation resultsfor orig, 3-step, and lader.
It can be seenthat the proposed lader outperforms the base-lines in both reordering and translation.9 Thereare a number of reasons why lader outper-forms 3-step.
First, the pipeline of 3-stepsuffers from error propogation, with errors inmonolingual parsing and reordering resultingin low overall accuracy.10 Second, as Section5.1 describes, lader breaks ties between ora-cle parses based on model score, allowing easy-to-reproduce model parses to be chosen dur-ing training.
In fact, lader generally foundtrees that followed from syntactic constituency,while 3-step more often used terminal nodes8In addition, following the example of Sudoh et al(2011a)?s reordering rules, we lexicalize all particles.9It should be noted that our results for 3-step aresignificantly worse than those of DeNero and Uszkoreit(2011).
Likely reasons include a 20x difference in trainingdata size, the fact that we are using naturally translatedtext as opposed to text translated specifically to createword alignments, or differences in implementation.10When using oracle parses, chunk accuracy was up to81%, showing that parsing errors are highly detrimental.850en-ja ja-enChunk ?
BLEU RIBES Chunk ?
BLEU RIBESLc 73.19 78.44 23.11 69.86 75.14 79.14 19.54 66.93Lt 70.37 79.57 22.57 69.47 72.51 78.93 18.52 66.26Lc + Lt 72.55 80.58 22.89 70.34 74.44 79.82 19.21 66.48Table 3: Results for systems trained to optimize chunk fragmentation (Lc) or Kendall?s ?
(Lt).that spanned constituent boundaries (as long asthe phrase frequency was high).
Finally, as Sec-tion 6.2 shows in detail, the ability of lader tomaximize reordering accuracy directly allows forimproved reordering and translation results.It can also be seen that incorporating POStags or parse trees improves accuracy of bothlader and 3-step, particularly for English-Japanese, where syntax has proven useful forpre-ordering, and less so for Japanese-English,where syntactic pre-ordering has been less suc-cessful (Sudoh et al2011b).We also tested Moses?s implementation of hi-erarchical phrase-based SMT (Chiang, 2007),which achieved BLEU scores of 23.21 and 19.30for English-Japanese and Japanese-English re-spectively, approximately matching lader inaccuracy, but with a significant decrease in de-coding speed.
Further, when pre-ordering withlader and hierarchical phrase-based SMT werecombined, BLEU scores rose to 23.29 and 19.69,indicating that the two techniques can be com-bined for further accuracy improvements.6.2 Effect of Training LossTable 3 shows results when one of three losses isoptimized during training: chunk fragmentation(Lc), Kendall?s ?
(Lt), or the linear interpola-tion of the two with weights chosen so that bothlosses contribute equally (Lt + Lc).
In general,training successfully maximizes the criterion it istrained on, and Lt +Lc achieves good results onboth measures.
We also find that Lc and Lc+Ltachieve the best translation results, which isin concert with Talbot et al2011), who findchunk fragmentation is better correlated withtranslation accuracy than Kendall?s ?
.
This isan important result, as methods such as thatof Tromble and Eisner (2009) optimize pairwiseen-ja ja-enBLEU/RIBES BLEU/RIBESorig 21.87 68.25 18.34 65.36man-602 23.11 69.86 19.54 66.93auto-602 22.39 69.19 18.58 66.07auto-10k 22.53 69.68 18.79 66.89Table 4: Results based on data size, and whethermanual or automatic alignments are used in training.word comparisons equivalent to Lt, which maynot be optimal for translation.6.3 Effect of Automatic AlignmentsTable 4 shows the difference between using man-ual and automatic alignments in the training oflader.
lader is able to improve over the origbaseline in all cases, but when equal numbersof manual and automatic alignments are used,the reorderer trained on manual alignments issignificantly better.
However, as the number ofautomatic alignments is increased, accuracy im-proves, approaching that of the system trainedon a smaller number of manual alignments.7 ConclusionWe presented a method for learning a discrim-inative parser to maximize reordering accuracyfor machine translation.
Future work includesapplication to other language pairs, develop-ment of more sophisticated features, investiga-tion of probabilistic approaches to inference, andincorporation of the learned trees directly intree-to-string translation.AcknowledgmentsWe thank Isao Goto, Tetsuo Kiso, and anony-mous reviewers for their helpful comments, andDaniel Flannery for helping to run his parser.851ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
ME-TEOR: An automatic metric for MT evaluationwith improved correlation with human judgments.In Proc.
ACL Workshop.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating re-ordering.
Machine Translation, 24(1):15?26.Marine Carpuat, Yuval Marton, and Nizar Habash.2010.
Improving arabic-to-english statistical ma-chine translation by reordering post-verbal sub-jects for alignment.
In Proc.
ACL.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2).Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis test-ing for statistical machine translation: Control-ling for optimizer instability.
In Proc.
ACL, pages176?181.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
ACL.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and ex-periments with perceptron algorithms.
In Proc.EMNLP, pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.John DeNero and Jakob Uszkoreit.
2011.
Induc-ing sentence structure from parallel corpora forreordering.
In Proc.
EMNLP.Chris Dyer and Philip Resnik.
2010.
Context-freereordering, finite-state translation.
In Proc.
HLT-NAACL.Daniel Flannery, Yusuke Miyao, Graham Neubig,and Shinsuke Mori.
2011.
Training dependencyparsers from partially annotated corpora.
In Proc.IJCNLP, pages 776?784, Chiang Mai, Thailand,November.Dmitriy Genzel.
2010.
Automatically learningsource-side reordering rules for large scale machinetranslation.
In Proc.
COLING.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with super-vised ITG models.
In Proc.
ACL.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-suhito Sudoh, and Hajime Tsukada.
2010a.
Auto-matic evaluation of translation quality for distantlanguage pairs.
In Proc.
EMNLP, pages 944?952.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada,and Kevin Duh.
2010b.
Head finalization: Asimple reordering rule for sov languages.
In Proc.WMT and MetricsMATR.Jason Katz-Brown, Slav Petrov, Ryan McDon-ald, Franz Och, David Talbot, Hiroshi Ichikawa,Masakazu Seno, and Hideto Kazawa.
2011.
Train-ing a parser for machine translation reordering.
InProc.
EMNLP, pages 183?192.Maurice G. Kendall.
1938.
A new measure of rankcorrelation.
Biometrika, 30(1/2):81?93.Maxim Khalilov and Khalil Sima?an.
2011.
Context-sensitive syntactic source-reordering by statisticaltransduction.
In Proc.
IJCNLP.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proc.
ACL, pages423?430.Phillip Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProc.
HLT, pages 48?54.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descrip-tion for the 2005 IWSLT speech translation eval-uation.
In Proc.
IWSLT.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
ACL, pages 177?180.Philipp Koehn.
2004.
Statistical significance testsfor machine translation evaluation.
In Proc.EMNLP.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A probabilisticapproach to syntax-based reordering for statisticalmachine translation.
In Proc.
ACL.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimi-native approach to machine translation.
In Proc.ACL, pages 761?768.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proc.
ACL,pages 529?533, Portland, USA, June.Graham Neubig.
2011.
The Kyoto free translationtask.
http://www.phontron.com/kftt.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proc.
EACL.852Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Proc.COLING, pages 311?318.Kay Rottmann and Stephan Vogel.
2007.
Word re-ordering in statistical machine translation with apos-based distortion model.
In Proc.
of TMI-2007.Shai Shalev-Shwartz, Yoram Singer, and NathanSrebro.
2007.
Pegasos: Primal estimated sub-gradient solver for SVM.
In Proc.
ICML, pages807?814.Katsuhito Sudoh, Kevin Duh, Hajime Tsukada,Masaaki Nagata, Xianchao Wu, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2011a.
NTT-UT statistical machine translation in NTCIR-9PatentMT.
In Proc.
NTCIR.Katsuhito Sudoh, Xianchao Wu, Kevin Duh, Ha-jime Tsukada, and Masaaki Nagata.
2011b.
Post-ordering in statistical machine translation.
InProc.
MT Summit.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-son Katz-Brown, Masakazu Seno, and Franz Och.2011.
A lightweight evaluation framework for ma-chine translation reordering.
In Proc.
WMT.Ben Taskar, Carlos Guestrin, and Daphne Koller.2003.
Max-margin Markov networks.
Proc.
NIPS,16.Roy Tromble and Jason Eisner.
2009.
Learning lin-ear ordering problems for better translation.
InProc.
EMNLP.Karthik Visweswariah, Rajakrishnan Rajkumar,Ankur Gandhe, Ananthakrishnan Ramanathan,and Jiri Navratil.
2011.
A word reorderingmodel for improved machine translation.
In Proc.EMNLP.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin train-ing for statistical machine translation.
In Proc.EMNLP, pages 764?773.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel cor-pora.
Computational Linguistics, 23(3).Fei Xia and Michael McCord.
2004.
Improving astatistical MT system with automatically learnedrewrite patterns.
In Proc.
COLING.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In Proc.NAACL.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
ACL.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.In Proc.
ICML, pages 1169?1176.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Chunk-level reordering of source language sen-tences with automatically learned rules for statis-tical machine translation.
In Proc.
SSST.Andreas Zollmann and Ashish Venugopal.
2006.Syntax augmented machine translation via chartparsing.
In Proc.
WMT, pages 138?141.853
