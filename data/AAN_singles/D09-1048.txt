Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 459?467,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPProjecting Parameters for Multilingual Word Sense DisambiguationMitesh M. Khapra Sapan Shah Piyush Kedia Pushpak BhattacharyyaDepartment of Computer Science and EngineeringIndian Institute of Technology, BombayPowai, Mumbai ?
400076,Maharashtra, India.
{miteshk,sapan,charasi,pb}@cse.iitb.ac.inAbstractWe report in this paper a way of doing WordSense Disambiguation (WSD) that has its ori-gin in multilingual MT and that is cognizantof the fact that parallel corpora, wordnets andsense annotated corpora are scarce re-sources.
With respect to these resources, lan-guages show different levels of readiness;however a more resource fortunate languagecan help a less resource fortunate language.Our WSD method can be applied to a lan-guage even when no sense tagged corpora forthat language is available.
This is achieved byprojecting wordnet and corpus parametersfrom another language to the language inquestion.
The approach is centered around anovel synset based multilingual dictionary andthe empirical observation that within a domainthe distribution of senses remains more or lessinvariant across languages.
The effectivenessof our approach is verified by doing parameterprojection and then running two differentWSD algorithms.
The accuracy values of ap-proximately 75% (F1-score) for three lan-guages in two different domains establish thefact that within a domain it is possible to cir-cumvent the problem of scarcity of resourcesby projecting parameters like sense distribu-tions, corpus-co-occurrences, conceptual dis-tance, etc.
from one language to another.1 IntroductionCurrently efforts are on in India to build large scaleMachine Translation and Cross Lingual Searchsystems in consortia mode.
These efforts are large,in the sense that 10-11 institutes and 6-7 languagesspanning the length and breadth of the country areinvolved.
The approach taken for translation istransfer based which needs to tackle the problem ofword sense disambiguation (WSD) (Sergei et.
al.,2003).
Since 90s machine learning based ap-proaches to WSD using sense marked corpora havegained ground (Eneko Agirre & Philip Edmonds,2007).
However, the creation of sense marked cor-pora has always remained a costly proposition.Statistical MT has obviated the need for elaborateresources for WSD, because WSD in SMT hap-pens implicitly through parallel corpora (Brown et.al., 1993).
But parallel corpora too are a very cost-ly resource.The above situation brings out the challengesinvolved in Indian language MT and CLIR.
Lackof resources coupled with the multiplicity of Indianlanguages severely affects the performance of sev-eral NLP tasks.
In the light of this, we focus on theproblem of developing methodologies that reuseresources.
The idea is to do the annotation workfor one language and find ways of using them foranother language.Our work on WSD takes place in a multilingualsetting involving Hindi (national language of India;500 million speaker base), Marathi (20 millionspeaker base), Bengali (185 million speaker base)and Tamil (74 million speaker base).
The wordnetof Hindi and sense marked corpora of Hindi areused for all these languages.
Our methodologyrests on a novel multilingual dictionary organiza-tion and on the idea of ?parameter projection?
fromHindi to the other languages.
Also the domains ofinterest are tourism and health.The roadmap of the paper is as follows.
Section2 describes related work.
In section 3 we introducethe parameters essential for domain-specific WSD.Section 4 builds the case for parameter projection.Section 5 introduces the Multilingual DictionaryFramework which plays a key role in parameterprojection.
Section 6 is the core of the work, wherewe present parameter projection from one languageto another.
Section 7 describes two WSD algo-rithms which combine various parameters for do-459main-specific WSD.
Experiments and results arepresented in sections 8 and 9.
Section 10 concludesthe paper.2 Related workKnowledge based approaches to WSD such asLesk?s algorithm (Michael Lesk, 1986), Walker?salgorithm (Walker D. & Amsler R., 1986), concep-tual density (Agirre Eneko & German Rigau, 1996)and random walk algorithm (Mihalcea Rada, 2005)essentially do Machine Readable Dictionary loo-kup.
However, these are fundamentally overlapbased algorithms which suffer from overlap sparsi-ty, dictionary definitions being generally small inlength.Supervised learning algorithms for WSD aremostly word specific classifiers, e.g., WSD usingSVM (Lee et.
al., 2004), Exemplar based WSD(Ng Hwee T. & Hian B. Lee, 1996) and decisionlist based algorithm (Yarowsky, 1994).
The re-quirement of a large training corpus renders thesealgorithms unsuitable for resource scarce languag-es.Semi-supervised and unsupervised algorithmsdo not need large amount of annotated corpora, butare again word specific classifiers, e.g., semi-supervised decision list algorithm (Yarowsky,1995) and Hyperlex (V?ronis Jean, 2004)).
Hybridapproaches like WSD using Structural SemanticInterconnections (Roberto Navigli & Paolo Velar-di, 2005) use combinations of more than oneknowledge sources (wordnet as well as a smallamount of tagged corpora).
This allows them tocapture important information encoded in wordnet(Fellbaum, 1998) as well as draw syntactic genera-lizations from minimally tagged corpora.At this point we state that no single existing so-lution to WSD completely meets our requirementsof multilinguality, high domain accuracy andgood performance in the face of not-so-largeannotated corpora.3 Parameters for WSDWe discuss a number of parameters that play acrucial role in WSD.
To appreciate this, considerthe following example:The river flows through this region to meet the sea.The word sea is ambiguous and has three senses asgiven in the Princeton Wordnet (PWN):S1: (n) sea (a division of an ocean or a large bodyof salt water partially enclosed by land)S2: (n) ocean, sea (anything apparently limitless inquantity or volume)S3: (n) sea (turbulent water with swells of consi-derable size) "heavy seas"Our first parameter is obtained from Domainspecific sense distributions.
In the above example,the first sense is more frequent in the tourism do-main (verified from manually sense marked tour-ism corpora).
Domain specific sense distributioninformation should be harnessed in the WSD task.The second parameter arises from the domin-ance of senses in the domain.
Senses are ex-pressed by synsets, and we define a dominantsense as follows:A few dominant senses in the Tourism domain are{place, country, city, area}, {body of water}, {flo-ra, fauna}, {mode of transport} and {fine arts}.
Indisambiguating a word, that sense which belongsto the sub-tree of a domain-specific dominantsense should be given a higher score than othersenses.
The value of this parameter (?)
is decidedas follows:?
= 1; if the candidate synset is a dominant synset?
= 0.5; if the candidate synset belongs to the sub-tree of a dominant synset?
= 0.001; if the candidate synset is neither a do-minant synset nor belongs to the sub-tree of a do-minant synset.Our third parameter comes from Corpus co-occurrence.
Co-occurring monosemous words aswell as already disambiguated words in the con-text help in disambiguation.
For example, the wordriver appearing in the context of sea is a mono-semous word.
The frequency of co-occurrence ofriver with the ?water body?
sense of sea is high inthe tourism domain.
Corpus co-occurrence is cal-A synset node in the wordnet hypernymyhierarchy is called Dominant if the syn-sets in the sub-tree below the synset arefrequently occurring in the domain cor-pora.460culated by considering the senses which occur in awindow of 10 words around a sense.Our fourth parameter is based on the semanticdistance between any pair of synsets in terms ofthe shortest path length between two synsets in thewordnet graph.
An edge in the shortest path can beany semantic relation from the wordnet relationrepository (e.g., hypernymy, hyponymy, meronymy,holonymy, troponymy etc.
).For nouns we do something additional over andabove the semantic distance.
We take advantage ofthe deeper hierarchy of noun senses in the wordnetstructure.
This gives rise to our fifth and final pa-rameter which arises out of the conceptual dis-tance between a pair of senses.
Conceptualdistance between two synsets S1 and S2 is calcu-lated using Equation (1), motivated by Agirre Ene-ko & German Rigau (1996).Concep-tualDistance(S1, S2)=Length of the path between (S1,S2) in terms of hypernymy hie-rarchyHeight of the lowest commonancestor of S1 and S2 in the word-net hierarchy(1)The conceptual distance is proportional to thepath length between the synsets, as it should be.The distance is also inversely proportional to theheight of the common ancestor of two sense nodes,because as the common ancestor becomes moreand more general the conceptual relatedness tendsto get vacuous (e.g., two nodes being relatedthrough entity which is the common ancestor ofEVERYTHING, does not really say anythingabout the relatedness).To summarize, our various parameters used fordomain-specific WSD are:Wordnet-dependent parameters?
belongingness-to-dominant-concept?
conceptual-distance?
semantic-distanceCorpus-dependent parameters?
sense distributions?
corpus co-occurrence.In section 7 we show how these parameters areused to come up with a scoring function for WSD.4 Building a case for Parameter Projec-tionWordnet-dependent parameters depend on thegraph based structure of Wordnet whereas theCorpus-dependent parameters depend on variousstatistics learnt from a sense marked corpora.
Boththe tasks of (a) constructing a wordnet from scratchand (b) collecting sense marked corpora for mul-tiple languages are tedious and expensive.
An im-portant question being addressed in this paper is:whether the effort required in constructing seman-tic graphs for multiple wordnets and collectingsense marked corpora can be avoided?
Our find-ings seem to suggest that by projecting relationsfrom the wordnet of a language and by projectingcorpus statistics from the sense marked corpora ofthe language we can achieve this end.
Before weproceed to discuss the way to realize parameterprojection, we present a novel dictionary whichfacilitates this task.5 Synset based multilingual dictionaryParameter projection as described in section 4 restson a novel and effective method of storage and useof dictionary in a multilingual setting proposed byMohanty et.
al.
(2008).
For the purpose of currentdiscussion, we will call this multilingual dictionaryframework MultiDict.
One important departurefrom traditional dictionary is that synsets arelinked, and after that the words inside the syn-sets are linked.
The basic mapping is thus be-tween synsets and thereafter between the words.Concepts L1(Eng-lish)L2 (Hindi) L3 (Mara-thi)04321: ayouthfulmale per-son{malechild,boy}{?????
ladkaa,????
baalak,?????bachchaa}{?????
mulgaa ,?????
porgaa ,???
por }Table 1: Multilingual Dictionary FrameworkTable 1 shows the structure of MultiDict, with oneexample row standing for the concept of boy.
Thefirst column is the pivot describing a concept witha unique ID.
The subsequent columns show thewords expressing the concept in respective lan-guages (in the example table above, English, Hindiand Marathi).
Thus to express the concept ?04321:a youthful male person?, there are two lexical ele-ments in English, which constitute a synset.
Cor-respondingly, the Hindi and Marathi synsetscontain 3 words each.461It may be noted that the central language whosesynsets the synsets of other languages link to isHindi.
This way of linking synsets- more popularlyknown as the expansion approach- has several ad-vantages as discussed in (Mohanty et.
al., 2008).One advantage germane to the point of this paperis that the synsets in a particular column automati-cally inherit the various semantic relations of theHindi wordnet (Dipak Narayan et.
al., 2000),which saves the effort involved in reconstructingthese relations for multiple languages.After the synsets are linked, cross linkages areset up manually from the words of a synset to thewords of a linked synset of the central language.The average number of such links per synset perlanguage pair is approximately 3.
These cross-linkages actually solve the problem of lexicalchoice in translating from text of one language toanother.Thus for the Marathi word ?????
{mulagaa} de-noting ?a youthful male person?, the correct lexi-cal substitute from the corresponding Hindi synsetis ?????
{ladakaa} (Figure 1).
One might argue thatany word within the synset could serve the purposeof translation.
However, the exact lexical substitu-tion has to respect native speaker acceptability.Figure 1: Cross linked synset members for theconcept: a youthful male personWe put these cross linkages to another use, asdescribed later.Since it is the MultiDict which is at the heart ofparameter projection, we would like to summarizethe main points of this section.
(1) By linking withthe synsets of Hindi, the cost of building wordnetsof other languages is partly reduced (semantic rela-tions are inherited).
The wordnet parameters ofHindi wordnet now become projectable to otherlanguages.
(2) By using the cross linked words inthe synsets, corpus parameters become projectable(vide next section).6 Parameter projection using MultDict6.1 P(Sense|Word) parameterSuppose a word (say, W) in language L1 (say, Ma-rathi) has k senses.
For each of these k senses weare interested in finding the parameter P(Si|W)-which is the probability of sense Si given the wordW expressed as:?
??
?)
=#(??
,?)#(??
,?
)?where ?#?
indicates ?count-of?.
Consider the exam-ple of two senses of the Marathi word ????
{saagar}, viz., sea and abundance and the corres-ponding cross-linked words in Hindi (Figure 2 be-low):Marathi            HindiFigure 2: Two senses of the Marathi word ????
(saagar), viz., {water body} and {abundance}, andthe corresponding cross-linked words in Hindi1.The probability P({water body}|saagar) for Mara-thi is#({?????
????
}, ??????)#({?????
????
}, ??????)
+ #({?????????
}, ??????
)We propose that this can be approximated by thecounts from Hindi sense marked corpora by replac-ing saagar with the cross linked Hindi words sa-mudra and saagar, as per Figure 2:#({water body}, samudra)#({water body}, samudra) + #({abundance}, saagar)1 Sense_8231 shows the same word saagar for both Marathiand Hindi.
This is not uncommon, since Marathi and Hindi aresister languages.?????/MW1mulagaa,?????/MW2poragaa,???
/MW3pora?????/HW1ladakaa,????/HW2baalak,?????
/HW3bachcha,????
/HW4choraamale-child/HW1,boy/HW2Marathi Synset Hindi Synset   English SynsetSense_2650Sense_8231saagar (sea){water body}saagar (sea){abundance}samudra (sea){water body}saagar (sea){abundance}462Thus, the following formula is used for calculat-ing the sense distributions of Marathi words usingthe sense marked Hindi corpus from the same do-main:?
??
?)
=#(??
, ?????_??????_?????_????)#(??
, ?????_??????_?????_????)?
(2)Note that we are not interested in the exact sensedistribution of the words, but only in the relativesense distribution.To prove that the projected relative distributionis faithful to the actual relative distribution ofsenses, we obtained the sense distribution statisticsof a set of Marathi words from a sense tagged Ma-rathi corpus (we call the sense marked corpora of alanguage its self corpora).
These sense distribu-tion statistics were compared with the statistics forthese same words obtained by projecting from asense tagged Hindi corpus using Equation (2).
Theresults are summarized in Table 2.Sr.NoMarathiWordSynset P(S|word)as learntfromsensetaggedMarathicorpusP(S|word) asprojectedfrom sensetaggedHindi cor-pus1 ????
??
(kimat){ worth } 0.684 0.714{ price }  0.315 0.2852 ?????
(rasta){ roadway } 0.164 0.209{road,route}0.835 0.7703 ?????
(thikan){ land site,place}0.962 0.878{ home } 0.037 0.124 ????
(saagar){waterbody}1.00 1.00{abun-dance}0 0Table 2: Comparison of the sense distributions ofsome Marathi words learnt from Marathi sensetagged corpus with those projected from Hindisense tagged corpus.The fourth row of Table 2 shows that whenever????
(saagar) (sea) appears in the Marathi tourismcorpus there is a 100% chance that it will appear inthe ?water body?
sense and 0% chance that it willappear in the sense of ?abundance?.
Column 5shows that the same probability values are ob-tained using projections from Hindi tourism cor-pus.
Taking another example, the third row showsthat whenever ?????
(thikaan) (place, home) ap-pears in the Marathi tourism corpus there is a muchhigher chance of it appearing in the sense of?place?
(96.2%) then in the sense of ?home?(3.7%).
Column 5 shows that the relative proba-bilities of the two senses remain the same evenwhen using projections from Hindi tourism corpus(i.e.
by using the corresponding cross-linked wordsin Hindi).
To quantify these observations, we cal-culated the average KL divergence and Spearman?scorrelation co-efficient between the two distribu-tions.
The KL divergence is 0.766 and Spearman?scorrelation co-efficient is 0.299.
Both these valuesindicate that there is a high degree of similaritybetween the distributions learnt using projectionand those learnt from the self corpus.6.2 Co-occurrence parameterSimilarly, within a domain, the statistics of co-occurrence of senses remain the same across lan-guages.
For example, the co-occurrence of the Ma-rathi synsets {????
(akash) (sky), ?????
(ambar)(sky)} and {???
(megh) (cloud), ????
(abhra)(cloud)} in the Marathi corpus remains more orless same as (or proportional to) the co-occurrencebetween the corresponding Hindi synsets in theHindi corpus.Sr.
No Synset Co-occurringSynsetP(co-occurrence)as learntfrom sensetaggedMarathicorpusP(co-occurrence)as learntfrom sensetaggedHindicorpus1 {??
?, ?????
}{small bush}{??
?, ?????,????
?, ?????,??
?, ????
}{tree}0.125 0.1252 {??
?, ????}{cloud}{????,????,?????
}{sky}0.167 0.1543 {????
?, ?^???,?^???,??????}{geographicalarea}{????,???
}{travel}0.0019 0.0017Table 3: Comparison of the corpus co-occurrencestatistics learnt from Marathi and Hindi Tourismcorpus.463Table 3 shows a few examples depicting similaritybetween co-occurrence statistics learnt from Mara-thi tourism corpus and Hindi tourism corpus.
Notethat we are talking about co-occurrence of synsetsand not words.
For example, the second row showsthat the probability of co-occurrence of the synsets{cloud} and {sky} is almost same in the Marathiand Hindi corpus.7 Our algorithms for WSDWe describe two algorithms to establish the use-fulness of the idea of parameter projection.
Thefirst algorithm- called iterative WSD (IWSD-) isgreedy, and the second based on PageRank algo-rithm is exhaustive.
Both use scoring functions thatmake use of the parameters detailed in the previoussections.7.1 Iterative WSD (IWSD)We have been motivated by the Energy expressionin Hopfield network (Hopfield, 1982) in formulat-ing a scoring function for ranking the senses.
Hop-field Network is a fully connected bidirectionalsymmetric network of bi-polar (0/1 or +1/-1) neu-rons.
We consider the asynchronous HopfieldNetwork.
At any instant, a randomly chosen neu-ron (a) examines the weighted sum of the input, (b)compares this value with a threshold and (c) gets tothe state of 1 or 0, depending on whether the inputis greater than or less than or equal to the thre-shold.
The assembly of 0/1 states of individualneurons defines a state of the whole network.
Eachstate has associated with it an energy, E, given bythe following expression?
= ?????
+  ?????>???=1????
(3)where, N is the total number of neurons in the net-work, ??
and ??
are the activations of neurons i andj respectively and ???
is the weight of the connec-tion between neurons i and j.
Energy is a funda-mental property of Hopfield networks, providingthe necessary machinery for discussing conver-gence, stability and such other considerations.The energy expression as given above cleanlyseparates the influence of self-activations of neu-rons and that of interactions amongst neurons tothe global macroscopic property of energy of thenetwork.
This fact has been the primary insight forequation (4) which was proposed to score the mostappropriate synset in the given context.
The cor-respondences are as follows:Neuron ?
SynsetSelf-activation ?
Corpus Sense Distribu-tionWeight of connec-tion between twoneurons?Weight as a function ofcorpus co-occurrenceand Wordnet distancemeasures between syn-sets??
= argmax???
?
??
+  ???
?
??
?
???
?
J4????
?,J = ???
??
?????????????
???????
= ??????????????????????????????
(??)??
= ?
??
| ???????
=  ??????????????????
??
, ???
1 ????????????????????(??
, ??
)?
1 ???????????????????????(??
, ??
)The component ??
?
??
of the energy due to the selfactivation of a neuron can be compared to the cor-pus specific sense of a word in a domain.
The othercomponent ???
?
??
?
??
coming from the interactionof activations can be compared to the score of asense due to its interaction in the form of corpusco-occurrence, conceptual distance, and wordnet-based semantic distance with the senses of otherwords in the sentence.
The first component thuscaptures the rather static corpus sense, whereas thesecond expression brings in the sentential context.Algorithm 1: performIterativeWSD(sentence)1.
Tag all monosemous words in the sentence.2.
Iteratively disambiguate the remaining words in thesentence in increasing order of their degree of polyse-my.3.
At each stage select that sense for a word which max-imizes the score given by Equation (4)Algorithm1: Iterative WSDIWSD is clearly a greedy algorithm.
It bases itsdecisions on already disambiguated words, andignores words with higher degree of polysemy.
Forexample, while disambiguating bisemous words,the algorithm uses only the monosemous words.4647.2 Modified PageRank algorithmRada Mihalcea (2005) proposed the idea of usingPageRank algorithm to find the best combinationof senses in a sense graph.
The nodes in a sensegraph correspond to the senses of all the words in asentence and the edges depict the strength of inte-raction between senses.
The score of each node inthe graph is then calculated using the followingrecursive formula:?????
??
=1?
d + d ?WijWjkSk?Out  Si?
Score SjS j?In S iInstead of calculating Wij  based on the overlapbetween the definition of senses Si and S  as pro-posed by Rada Mihalcea (2005), we calculate theedge weights using the following formula:???
=  ??????????????????
??
, ???
1 ????????????????????
??
, ???
1 ???????????????????????
??
, ???
?
??
| ??????
?
??
| ??????
= ???????
??????
?????????
0.85This formula helps capture the edge weights interms of the corpus bias as well as the interactionbetween the senses in the corpus and wordnet.
Itshould be noted that this algorithm is not greedy.Unlike IWSD, this algorithm allows all the sensesof all words to play a role in the disambiguationprocess.8 Experimental Setup:We tested our algorithm on tourism corpora in 3languages (viz., Marathi, Bengali and Tamil) andhealth corpora in 1 language (Marathi) using pro-jections from Hindi.
The corpora for both the do-mains were manually sense tagged.
A 4-fold crossvalidation was done for all the languages in boththe domains.
The size of the corpus for each lan-guage is described in Table 4.Language # of polysemous words(tokens)TourismDomainHealthDomainHindi 50890 29631Marathi 32694 8540Bengali 9435  -Tamil 17868 -Table 4: Size of manually sense tagged corpora fordifferent languages.Table 5 shows the number of synsets in MultiDictfor each language.Language # of synsets inMultiDictHindi 29833Marathi 16600Bengali 10732Tamil 5727Table 5: Number of synsets for each languageAlgorithm LanguageMarathi BengaliP  % R % F % P  % R % F %IWSD (training on self corpora; no parameter pro-jection) 81.29 80.42 80.85 81.62 78.75 79.94IWSD (training on Hindi and reusing parametersfor another language) 73.45 70.33 71.86 79.83 79.65 79.79PageRank (training on self corpora; no parameterprojection) 79.61 79.61 79.61 76.41 76.41 76.41PageRank (training on Hindi and reusing parame-ters  for another language) 71.11 71.11 71.11 75.05 75.05 75.05Wordnet Baseline 58.07 58.07 58.07 52.25 52.25 52.25Table 6: Precision, Recall and F-scores of IWSD, PageRank and Wordnet Baseline.
Values are re-ported with and without parameter projection.4659 Results and DiscussionsTable 6 shows the results of disambiguation (preci-sion, recall and F-score).
We give values for twoalgorithms in the tourism domain: IWSD and Pa-geRank.
In each case figures are given for bothwith and without parameter projection.
The word-net baseline figures too are presented for the sakeof grounding the results.Note the lines of numbers in bold, and comparethem with the numbers in the preceding line.
Thisshows the fall in accuracy value when one tries theparameter projection approach in place of self cor-pora.
For example, consider the F-score as givenby IWSD for Marathi.
It degrades from about 81%to 72% in using parameter projection in place ofself corpora.
Still, the value is much more than thebaseline, viz., the wordnet first sense (a typicallyreported baseline).Coming to PageRank for Marathi, the fall in ac-curacy is about 8%.
Appendix A shows the corres-ponding figure for Tamil with IWSD as 10%.Appendix B reports the fall to be 11% for a differ-ent domain- Health- for Marathi (using IWSD).In all these cases, even after degradation the per-formance is far above the wordnet baseline.
Thisshows that one could trade accuracy with the costof creating sense annotated corpora.10 Conclusion and Future Work:Based on our study for 3 languages and 2 domains,we conclude the following:(i) Domain specific sense distributions- ifobtainable- can be exploited to advantage.
(ii) Since sense distributions remain same acrosslanguages, it is possible to create a disambiguationengine that will work even in the absence of sensetagged corpus for some resource deprivedlanguage, provided (a) there are aligned and crosslinked sense dictionaries for the language inquestion and another resource rich language, (b)the domain in which disambiguation needs to beperformed for the resource deprived language isthe same as the domain for which sense taggedcorpora is available for the resource rich language.
(iii) Provided the accuracy reduction is not drastic,it may make sense to trade high accuracy for theeffort in collecting sense marked corpora.It would be interesting to test our algorithm onother domains and other languages to conclusivelyestablish the effectiveness of parameter projectionfor multilingual WSD.It would also be interesting to analyze the con-tribution of corpus and wordnet parameters inde-pendently.ReferencesAgirre Eneko & German Rigau.
1996.
Word sense dis-ambiguation using conceptual density.
In Proceed-ings of the 16th International Conference onComputational Linguistics (COLING), Copenhagen,Denmark.Dipak Narayan, Debasri Chakrabarti, Prabhakar Pandeand P. Bhattacharyya.
2002.
An Experience in Build-ing the Indo WordNet - a WordNet for Hindi.
FirstInternational Conference on Global WordNet, My-sore, India.Eneko Agirre & Philip Edmonds.
2007.
Word SenseDisambiguation Algorithms and Applications.
Sprin-ger Publications.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
The MIT Press.Hindi Wordnet.http://www.cfilt.iitb.ac.in/wordnet/webhwn/J.
J. Hopfield.
April 1982.
"Neural networks and physi-cal systems with emergent collective computationalabilities", Proceedings of the National Academy ofSciences of the USA, vol.
79 no.
8 pp.
2554-2558.Lee Yoong K., Hwee T. Ng & Tee K. Chia.
2004.
Su-pervised word sense disambiguation with supportvector machines and multiple knowledge sources.Proceedings of Senseval-3: Third InternationalWorkshop on the Evaluation of Systems for the Se-mantic Analysis of Text, Barcelona, Spain, 137-140.Lin Dekang.
1997.
Using syntactic dependency as localcontext to resolve word sense ambiguity.
In Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics (ACL), Madrid, 64-71.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedings ofthe 5th annual international conference on Systemsdocumentation, Toronto, Ontario, Canada.Mihalcea Rada.
2005.
Large vocabulary unsupervisedword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In Proceedings ofthe Joint Human Language Technology and Empiri-466cal Methods in Natural Language Processing Confe-rence (HLT/EMNLP), Vancouver, Canada, 411-418.Ng Hwee T. & Hian B. Lee.
1996.
Integrating multipleknowledge sources to disambiguate word senses: Anexemplar-based approach.
In Proceedings of the 34thAnnual Meeting of the Association for Computation-al Linguistics (ACL), Santa Cruz, U.S.A., 40-47.Peter F. Brown and Vincent J.Della Pietra and StephenA.
Della Pietra and Robert.
L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics Vol19, 263-311.Rajat Mohanty, Pushpak Bhattacharyya, PrabhakarPande, Shraddha Kalele, Mitesh Khapra and AdityaSharma.
2008.
Synset Based Multilingual Dictionary:Insights, Applications and Challenges.
Global Word-net Conference, Szeged, Hungary, January 22-25.Resnik Philip.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of ACL Workshopon Tagging Text with Lexical Semantics, Why, Whatand How?
Washington, U.S.A., 52-57.Roberto Navigli, Paolo Velardi.
2005.
Structural Se-mantic Interconnections: A Knowledge-Based Ap-proach to Word Sense Disambiguation.
IEEETransactions On Pattern Analysis and Machine Intel-ligence.Sergei Nirenburg, Harold Somers, and Yorick Wilks.2003.
Readings in Machine Translation.
Cambridge,MA: MIT Press.V?ronis Jean.
2004.
HyperLex: Lexical cartography forinformation retrieval.
Computer Speech & Language,18(3):223-252.Walker D. and Amsler R. 1986.
The Use of MachineReadable Dictionaries in Sublanguage Analysis.
InAnalyzing Language in Restricted Domains, Grish-man and Kittredge (eds), LEA Press, pp.
69-83.Yarowsky David.
1994.
Decision lists for lexical ambi-guity resolution: Application to accent restoration inSpanish and French.
In Proceedings of the 32nd An-nual Meeting of the association for ComputationalLinguistics (ACL), Las Cruces, U.S.A., 88-95.Yarowsky David.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics (ACL),Cambridge, MA, 189-196.Appendix A: Results for Tamil (TourismDomain)Algorithm P  % R  % F %IWSD (training onTamil) 89.50 88.18 88.83IWSD (training onHindi and reusing  forTamil) 84.60 73.79 78.82Wordnet Baseline 65.62 65.62 65.62Table 7: Tamil Tourism corpus using parametersprojected from HindiAppendix B: Results for Marathi (HealthDomain)AlgorithmWordsP  % R  % F %IWSD (training on Mara-thi) 84.28 81.25 82.74IWSD (training on Hindiand reusing  for Marathi) 75.96 67.75 71.62Wordnet Baseline 60.32 60.32 60.32Table 8: Marathi Health corpus parameters pro-jected from Hindi467
