2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 793?802,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsTextual Predictors of Bill Survival in Congressional CommitteesTae Yano Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{taey,nasmith}@cs.cmu.eduJohn D. WilkersonDepartment of Political ScienceUniversity of WashingtonSeattle, WA 98195, USAjwilker@u.washington.eduAbstractA U.S. Congressional bill is a textual artifactthat must pass through a series of hurdles tobecome a law.
In this paper, we focus on oneof the most precarious and least understoodstages in a bill?s life: its consideration, behindclosed doors, by a Congressional committee.We construct predictive models of whether abill will survive committee, starting with astrong, novel baseline that uses features of thebill?s sponsor and the committee it is referredto.
We augment the model with informationfrom the contents of bills, comparing differenthypotheses about how a committee decides abill?s fate.
These models give significant re-ductions in prediction error and highlight theimportance of bill substance in explanations ofpolicy-making and agenda-setting.1 IntroductionIn representative governments, laws result from acomplex social process.
Central to that process islanguage.
Text data emerging from the process in-clude debates among legislators (Laver et al, 2003;Quinn et al, 2010; Beigman Klebanov et al, 2008),press releases (Grimmer, 2010), accounts of thesedebates in the press, policy proposals, and laws.In the work reported here, we seek to exploit textdata?specifically, the text of Congressional bills?to understand the lawmaking process.
We consideran especially murky part of that process that is dif-ficult to study because it happens largely behindclosed doors: the handling of bills by Congressionalcommittees.
This early stage of a bill?s life is precar-ious: roughly 85% of bills do not survive commit-tee.
By contrast, nearly 90% of bills that are recom-mended by a committee (i.e., survive the committeeand are introduced for debate on the floor) will sur-vive a roll call vote by the legislature.
Because fil-tering by these powerful Congressional committeesis both more opaque and more selective than the ac-tions of the legislature as a whole, we believe thattext-based models can play a central role in under-standing this stage of lawmaking.This paper?s contributions are: (i) We formu-late computationally the prediction of which billswill a survive Congressional committee, presentinga (baseline) model based on observable features as-sociated with a bill, the committee(s) it is assignedto, members of that committee, the Congress as awhole, and expert combinations of those features.The task formulation and baseline model are novel.
(ii) We propose several extensions of that strongbaseline with information derived from the text ofa bill.
(iii) We validate our models on a hard predic-tive task: predicting which bills will survive com-mittee.
Text is shown to be highly beneficial.
(iv)We present a discussion of the predictive features se-lected by our model and what they suggest about theunderlying political process.
(v) We release our cor-pus of over 50,000 bills and associated metadata tothe research community for further study.1We give brief background on how bills becomeU.S.
laws in ?2.
We describe our data in ?3.
Themodeling framework and baseline are then intro-duced (?4), followed by our text-based models withexperiments (?5), then further discussion (?6).1http://www.ark.cs.cmu.edu/bills7932 How Bills Become LawsIn the U.S., federal laws are passed by theU.S.
Congress, which consists of two ?chambers,?the House of Representatives (commonly called the?House?)
and the Senate.
To become law, a bill (i.e.,a proposed law) must pass a vote in both chambersand then be signed by the U.S. President.
If the Pres-ident refuses to sign a bill (called a ?veto?
), it maystill become law if both chambers of Congress over-rides the veto through a two-thirds majority.Much less discussed is the process by which billscome into existence.
A bill is formally proposedby a member of Congress, known as its sponsor.Once proposed, it is routed to one or more (usu-ally just one) of about twenty subject-specializingcommittees in each chamber.
Unlike floor proceed-ings, transcripts of the proceedings of Congressionalcommittees are published at the discretion of thecommittee and are usually publicly unavailable.Each committee has a chairman (a member of themajority party in the chamber) and is further dividedinto subcommittees.
Collectively a few thousandbills per year are referred to Congress?
committeesfor consideration.
Committees then recommend (re-port) only about 15% for consideration and votingby the full chamber.The U.S. House is larger (435 voting memberscompared to 100 in the Senate) and, in recent his-tory, understood to be more polarized than the Sen-ate (McCarty et al, 2006).
All of its seats are upfor election every two years.
A ?Congress?
oftenrefers to a two-year instantiation of the body with aparticular set of legislators (e.g., the 112th Congressconvened on January 3, 2011 and adjourns on Jan-uary 3, 2013).
In this paper, we limit our attentionto bills referred to committees in the House.3 DataWe have collected the text of all bills introduced inthe U.S. House of Representatives from the 103rdto the 111th Congresses (1/3/1993?1/3/2011).
Herewe consider only the version of the bill as originallyintroduced.
After introduction, a bill?s title and con-tents can change significantly, which we ignore here.These bills were downloaded directly from theLibrary of Congress?s Thomas website.2 Informa-2http://thomas.loc.gov/home/thomas.phpCong.
Maj.Total Survival Rate (%)Introduced Total Rep. Dem.103 Dem.
5,311 11.7 3.4 16.2104 Rep. 4,345 13.7 19.7 6.1105 Rep. 4,875 13.2 19.0 5.4106 Rep. 5,682 15.1 20.9 7.0107 Rep. 5,768 12.1 17.5 5.8108 Rep. 5,432 14.0 21.0 5.9109 Rep. 6,437 11.8 16.9 5.1110 Dem.
7,341 14.5 8.5 18.0111 Dem.
6,571 12.6 8.1 14.5Total 51,762 13.2 15.9 10.7Table 1: Count of introduced bills per Congress, alongwith survival rate, and breakdown by the bill sponsor?sparty affiliation.
Note that the probability of survival in-creases by a factor of 2?5 when the sponsor is in the ma-jority party.
Horizontal lines delineate presidential ad-ministrations (Clinton, Bush, and Obama).tion about the makeup of House committees wasobtained from Charles Stewart?s resources at MIT,3while additional sponsor and bill information (e.g.,sponsor party affiliation and bill topic) was obtainedfrom E. Scott Adler and John Wilkerson?s Congres-sional Bills Project at the University of Washing-ton.4In our corpus, each bill is associated with its title,text, committee referral(s), and a binary value indi-cating whether or not the committee reported the billto the chamber.
We also extracted metadata, such assponsor?s name, from each bill?s summary page pro-vided by the Library of Congress.There were a total of 51,762 bills in the Houseduring this seventeen-year period, of which 6,828survived committee and progressed further.
See Ta-ble 1 for the breakdown by Congress and party.In this paper, we will consider a primary train-testsplit of the bills by Congress, with the 103rd?110thCongresses serving as the training dataset and the111th as the test dataset.
This allows us to simulatethe task of ?forecasting?
which bills will survive in afuture Congress.
In ?5.5, we will show that a similarresult is obtained on different data splits.These data are, in principle, ?freely available?to the public, but they are not accessible in a uni-3http://web.mit.edu/17.251/www/data_page.html4http://congressionalbills.org794fied, structured form.
Considerable effort mustbe expended to align databases from a variety ofsources, and significant domain knowledge aboutthe structure of Congress and its operation is re-quired to disambiguate the data.
Further explorationof the deeper relationships among the legislators,their roles in past Congresses, their standing withtheir constituencies, their political campaigns, andso on, will require ongoing effort in joining datafrom disparate sources.When we consider a larger goal of understandinglegislative behavior across many legislative bodies(e.g., states in the U.S., other nations, or interna-tional bodies), the challenge of creating and main-taining such reliable, clean, and complete databasesseems insurmountable.We view text content?noisy and complex as itis?as an attractive alternative, or at least a comple-mentary information source.
Though unstructured,text is made up of features that are relatively easyfor humans to interpret, offering a way to not onlypredict, but also explain legislative outcomes.4 A Predictive ModelWe next consider a modeling framework for predict-ing bill survival or death in committee.
We brieflyreview logistic regression models (section 4.1), thenturn to the non-textual features that form a baselineand a starting point for the use of text (section 4.2).4.1 Modeling FrameworkOur approach to predicting a bill?s survival is logis-tic regression.
Specifically, let X be a random vari-able associated with a bill, and let f be a feature vec-tor function that encodes observable features of thebill.
Let Y be a binary random variable correspond-ing to bill survival (Y = 1) or death (Y = 0).
Let:pw(Y = 1 | X = x) =expw>f(x)1 + expw>f(x)(1)where w are ?weight?
parameters associating eachfeature in the feature vector f(x) with each outcome.This leads to the predictive rule:y?
(x) ={1 if w>f(x) > 00 otherwise(2)We train the model by maximizing log-likelihoodplus a a sparsity-inducing log-prior that encouragesmany weights to go to zero:maxw?i log pw(yi | xi)?
?
?w?1 (3)where i indexes training examples (specifically, eachtraining instance is a bill referred to a single com-mittee).
The second term is an `1 norm, equivalentto a Laplacian prior on the weights.
The value of?, which controls sparsity, is chosen on a held-outsubset of the training data.Linear models like this one, commonly called?exponential?
or ?max ent?
models, are attractivebecause they are intelligible.
The magnitude of aweight indicates a feature?s importance in the pre-diction, and its sign indicates the direction of the ef-fect.We note that the `1 regularizer is not ideal foridentifying predictive features.
When two featuresare strongly correlated, it tends to choose one ofthem to include in the model and eliminate the other,despite the fact that they are both predictive.
It istherefore important to remember that a weight ofzero does not imply that the corresponding featureis unimportant.
We chose to cope with this poten-tial elimination of good features so that our modelswould be compact and easily interpretable.4.2 FeaturesIn American politics, the survival or death of manybills can be explained in terms of expertise, en-trepreneurship, and procedural control, which aremanifest in committee membership, sponsor at-tributes, and majority party affiliation.
We there-fore begin with a strong baseline that includes fea-tures encoding many expected effects on bill suc-cess.
These include basic structural features andsome interactions.The basic features are all binary.
The value ofthe random variable X includes information aboutthe bill, its sponsor, and the committee to which thebill is referred.
In addition to a bias feature (alwaysequal to 1), we include the following features:1.
For each party p, is the bill?s sponsor affiliated withp?2.
Is the bill?s sponsor in the same party as the com-mittee chair?
Equivalently, is the bill?s sponsor inthe majority party of the House?3.
Is the bill?s sponsor a member of the committee?7954.
Is the bill?s sponsor a majority member of the com-mittee?
(This feature conjoins 2 and 3.)5.
Is the bill?s sponsor the chairman of the committee?6.
For each House member j, did j sponsor the bill?7.
For each House member j, is the bill sponsored by jand referred to a committee he chairs?
(This featureconjoins 5 and 6.)8.
For each House member j, is the bill sponsored byj and is j in the same party as the committee chair?
(This feature conjoins 2 and 6.)9.
For each state s, is the bill?s sponsor from s?10.
For each month m, is the bill introduced during m?11.
For v ?
{1, 2}, is the bill introduced during the vthyear of the (two-year) Congress?The features above were engineered in prelimi-nary model development, before text was incorpo-rated.54.3 ExperimentPerformance.
Considering the 111th Congress as atest set (6,571 instances), a most-frequent-class pre-dictor (i.e., a constant prediction that no bill willsurvive committee) achieves an error rate of 12.6%(more details in Table 3).
A model trained onthe 103rd?110th Congresses (45,191 bills) contains3,731 instantiated features above achieved 11.8% er-ror (again, see Table 3).Discussion.
When inspecting linear models, consid-ering feature weights can be misleading, since (evenwith regularization) large weights often correspondto small effects in the training data.
Our method-ology for inspecting models is therefore as follows:we calculate the impact of each feature on the finaldecision for class y, defined for feature j aswjN?Ni=1 fj(xi) (4)where i indexes test examples (of which there areN ).
Impact is the average effect of a feature on themodel?s score for class y.
Note that it is not affected5One surprisingly detrimental feature, omitted here, wasthe identity of the committee.
Bill success rates vary greatlyacross committees (e.g., Appropriations recommends about halfof bills, while Ways and Means only 7%).
We suspect thatthis feature simply has poor generalization ability across Con-gresses.
(In ?5.2 we will consider preferences of individuals oncommittees, based on text, which appears to benefit predictiveperformance.
)Bill Survivalsponsor is in the majority party (2) 0.525sponsor is in the majority party and on thecommittee (4)0.233sponsor is a Democrat (1) 0.135sponsor is on the committee (3) 0.108bill introduced in year 1 (11) 0.098sponsor is the referred committee?s chair (5) 0.073sponsor is a Republican (1) 0.069Bill Deathbill?s sponsor is from NY (9) -0.036sponsor is Ron Paul (Rep., TX) (6) -0.023bill introduced in December (10) -0.018sponsor is Bob Filner (Dem., CA) (6) -0.013Table 2: Baseline model: high-impact features associatedwith each outcome and their impact scores (eq.
4).by the true label for an example.
Impact is addi-tive, which allows us to measure and compare theinfluence of sets of features within a model on modelpredictions.
Impact is not, however, directly compa-rable across models.The highest impact features are shown in Table 2.Unsurprisingly, the model?s predictions are stronglyinfluenced (toward survival) when a bill is sponsoredby someone who is on the committee and/or in themajority party.
Feature 2, the sponsor being on thecommittee, accounted for nearly 27% of all (abso-lute) impact, followed by the member-specific fea-tures (6?8, 19%), the sponsor being in the majorityand on the committee (4, 12%), and the party of thesponsor (1, 10%).We note that impact as a tool for interpreting mod-els has some drawbacks.
If a large portion of billsin the test set happen to have a particular feature,that feature may have a high impact score for thedominant class (death).
This probably explains thehigh impact of ?sponsor is a Democrat?
(Table 2);Democrats led the 111th Congress, and introducedmore bills, most of which died.5 Adding TextWe turn next to the use of text data to augment thepredictive power of our baseline model.
We willpropose three ways of using the title and/or text ofa bill to create features.
From a computational per-spective, each approach merely augments the base-line model with features that may reduce predictive796errors?our measure of the success of the hypothe-sis.
From a political science perspective, each pro-posal corresponds to a different explanation of howcommittees come to decisions.5.1 Functional Bill CategoriesAn important insight from political science is thatbills can be categorized in general ways that are re-lated to their likelihood of success.
In their study onlegislative success, Adler and Wilkerson (2005) dis-tinguish Congressional bills into several categoriesthat capture bills that are on the extremes in termsof the importance and/or urgency of the issue ad-dressed.
We expect to find that distinguishing billsby their substance will reduce prediction errors.?
bills addressing trivial issues, such as those nam-ing a federal building or facility or coining com-memorative medals;?
bills that make technical changes to existing laws,usually at the request of the executive agency re-sponsible for its implementation;?
bills addressing recurring issues, such as annualappropriations or more sporadic reauthorizationsof expiring federal programs or laws; and?
bills addressing important, urgent issues, such asbills introduced in response to the 9/11 terroristattacks or a sharp spike in oil prices.Adler and Wilkerson (2005) annotated House billsfor the 101st?105th Congresses using the above cat-egories (all other bills were deemed to be ?discre-tionary?).
Out of this set we use the portion thatoverlaps with our bill collection (103rd?105th).
Of14,528 bills, 1,580 were labeled as trivial, 119 astechnical, 972 as recurring, and 1,508 as important.Our hypothesis is that these categories can help ex-plain which bills survive committees.To categorize the bills in the other Congressesof our dataset, we trained binary logistic regressionmodels to label bills with each of the three most fre-quent bill types above (trivial, recurring, and impor-tant) based on unigram features of the body of billtext.
(There is some overlap among categories in theannotated data, so we opted for three binary clas-sifiers rather than multi-class.)
In a ten-fold cross-validated experiment, this model averaged 83% ac-curacy across the prediction tasks.
We used the man-ually annotated labels for the bills in the 103rd?105th Congresses; for other bills, we calculated eachmodel?s probability that the bill belonged to the tar-get category.6 These values were used to define bi-nary indicators for each classifier?s probability re-gions: [0, 0.3); [0.3, 0.4); [0.4, 0.5); [0.5, 1.0].
Foreach of the three labels, we included two classifierstrained with different hyperparameter settings, giv-ing a total of 24 additional features.
All baselinefeatures were retained.Performance.
Including functional category fea-tures reduces the prediction error slightly but signif-icantly relative to the baseline (just over 1% relativeerror reduction)?see Table 3.7Discussion.
Considering the model?s weights, thelog-odds are most strongly influenced toward billsuccess by bills that seem ?important?
according tothe classifiers.
55% of this model?s features had non-zero impact on test-set predictions; compare this toonly 36% of the baseline model?s features.8 Further,the category features accounted for 66% of the total(absolute) impact of all features.
Taken altogether,these observations suggest that bill category featuresare a more compact substitute for many of the base-line features,9 but that they do not offer much ad-ditional predictive information beyond the baseline(error is only slightly reduced).
It is also possi-ble that our categories do not perfectly capture theperceptions of committees making decisions aboutbills.
Refinement of the categories within the pre-6In preliminary experiments, we used the 103rd?105th datato measure the effect of automatic vs. manual categories.Though the particulars of the earlier model and the smallerdataset size make controlled comparison impossible, we notethat gold-standard annotations achieved 1?2% lower absoluteerror across cross-validation folds.7We note that preliminary investigations conjoining the billcategory features with baseline features did not show any gains.Prior work by Adler and Wilkerson (2012) suggests that bill cat-egory interacts with the sponsor?s identity, but does not considerbill success prediction; we leave a more careful exploration ofthis interaction in our framework to future work.8Note that `1-regularized models make global decisionsabout which features to include, so the new features influencewhich baseline features get non-zero weights.
Comparing theabsolute number of features in the final selected models is notmeaningful, since it depends on the hyperparameter ?, which istuned separately for each model.9This substitutability is unsurprising in some scenarios; e.g.,successful reauthorization bills are often sponsored by commit-tee leadership.797Model Error (%) False + False ?
True + # Feats.
Size Effectivemost frequent class 12.6 0 828 0 ?
?
?
?4.2 baseline (no text) 11.8 69 709 119 3,731 1,284 460?5.1 bill categories 11.7 52 716 112 3,755 274 152?5.2proxy vote, chair only 10.8 111 596 232 3,780 1,111 425proxy vote, majority 11.3 134 606 222 3,777 526 254proxy vote, whole committee 10.9 123 596 232 3,777 1,131 433proxy vote, all three 10.9 110 606 222 3,872 305 178?5.3 unigram & bigram 9.8 106 541 287 28,246 199 194?5.4 full model (all of the above) 9.6 120 514 314 28,411 1,096 1,069Table 3: Key experimental results; models were trained on the 103rd?110th Congresses and tested on the 111th.Baseline features are included in each model listed below the baseline.
?# Feats.?
is the total number of featuresavailable to the model; ?Size?
is the number of features with non-zero weights in the final selected sparse model;?Effective?
is the number of features with non-zero impact (eq.
4) on test data.
Each model?s improvement over thebaseline is significant (McNemar?s test, p < 0.0001 except bill categories, for which p < 0.065).dictive framework we have laid out here is left tofuture research.5.2 Textual Proxy VotesWe next consider a different view of text: as a meansof profiling the preferences and agendas of legisla-tors.
Our hypothesis here is that committees oper-ate similarly to the legislature as a whole: when abill comes to a committee for consideration, mem-bers of the committee vote on whether it will sur-vive.
Of course, deliberation and compromise maytake place before such a vote; our simple model doesnot attempt to account for such complex processes,instead merely positing a hidden roll call vote.Although the actions of legislators on commit-tees are hidden, their voting behavior on the flooris observed.
Roll call data is frequently used in po-litical science to estimate spatial models of legis-lators and legislation (Poole and Rosenthal, 1985;Poole and Rosenthal, 1991; Jackman, 2001; Clintonet al, 2004).
These models help visualize politics interms of intuitive, low-dimensional spaces which of-ten correspond closely to our intuitions about ?left?and ?right?
in American politics.
Recently, Gerrishand Blei (2011) showed how such models could nat-urally be augmented with models of text.
Such mod-els are based on observed voting; it is left to futurework to reduce the dimensionality of hidden voteswithin the survival prediction model here.Our approach is to construct a proxy vote; an es-timate of a roll call vote by members of the com-mittee on the bill.
We consider three variants, eachbased on the same estimate of the individual com-mittee members?
votes:?
Only the committee chairman?s vote matters.?
Only majority-party committee members vote.?
All committee members vote.We will compare these three versions of the proxyvote feature experimentally, but abstractly they canall be defined the same way.
Let C denote the set ofcommittee members who can vote on a bill x. Thenthe proxy vote equals:1|C|?j?C E[Vj,x] (5)(If x is referred to more than one committee, we av-erage the above feature across committees.)
We treatthe vote by representative j on bill x as a binary ran-dom variable Vj,x corresponding to a vote for (1) oragainst (0) the bill.
We do not observe Vj,x; insteadwe estimate its expected value, which will be be-tween 0 and 1.
Note that, by linearity of expecta-tion, the sum in equation 5 is the expected value ofthe number of committee members who ?voted?
forthe bill; dividing by |C| gives a value that, if our esti-mates are correct, should be close to 1 when the billis likely to be favored by the committee and 0 whenit is likely to be disfavored.To estimate E[Vj,x], we use a simple probabilis-tic model of Vj,x given the bill x and the past vot-ing record of representative j.10 Let Rj be a set of10We note that the observable roll call votes on the floor of798bills that representative j has publicly voted on, onthe floor of the House, in the past.11 For x ?
Rj ,let Vj,x be 1 if j voted for the bill and 0 if j votedagainst it.
Further, define a similarity measure be-tween bills; here we use cosine similarity of twobills?
tfidf vectors.12 We denote by sim(x, x?)
thesimilarity of bills x and x?.The probabilistic model is as follows.
First, therepresentative selects a bill he has voted on previ-ously; he is likely to choose a bill that is similar tox.
More formally, given representative j and bill x,randomly choose a bill X ?
from Rj according to:p(X ?
= x?
| j, x) = exp sim(x,x?)Px??
?Rjexp sim(x,x??)
(6)An attractive property of this distribution is that ithas no parameters to estimate; it is defined entirelyby the text of bills in Rj .
Second, the representa-tive votes on x identically to how he voted on X ?.Formally, let Vj,x = Vj,x?
, which is observed.The above model gives a closed form for the ex-pectation of Vj,x:E[Vj,x] =?x?
?Rj p(X?
= x?
| j, x) ?
Vj,x?
(7)In addition to the proxy vote score in eq.
5, we cal-culate a similar expected vote based on ?nay?
votes,and consider a second score that is the ratio of the?yea?
proxy vote to the ?nay?
proxy vote.
Bothof these scores are continuous values; we quantizethem into bins, giving 141 features.13Performance.
Models built using the baseline fea-tures plus, in turn, each of the three variations of theproxy vote feature (C defined to include the chairthe U.S. House consist of a very different sample of bills thanthose we consider in this study; indeed, votes on the floor cor-respond to bills that survived committee.
We leave attempts tocharacterize and control for this bias to future work.11To simplify matters, we use all bills from the training pe-riod that j has voted on.
For future predictions (on the test set),these are all in the past, but in the training set they may includebills that come later than a given training example.12We first eliminated punctutation and numbers from thetexts, then removed unigrams which occured in more than 75%or less than 0.05% of the training documents.
Tfidf scores werecalculated based on the result.13We discretized the continuous values by 0.01 increment forproxy vote score, and 0.1 increment for proxy vote rate scores.We further combined outlier bins (one for exremely large val-ues, one for extremely small values).only, majority party members, or the full commit-tee), and all three sets of proxy vote features, werecompared?see Table 3.
All three models showedimprovement over the baseline.
Using the chairman-only committee (followed closely by whole commit-tee and all three) turned out to be the best performingamong them, with a 8% relative error reduction.Discussion.
Nearly 58% of the features in the com-bined model had non-zero impact at test time, and38% of total absolute impact was due to these fea-tures.
Comparing the performance of these fourmodels suggests that, as is widely believed in polit-ical science, the preferences of the committee chairare a major factor in which bills survive.5.3 Direct Use of Content: Bag of WordsOur third hypothesis is that committees make collec-tive decisions by considering the contents of bills di-rectly.
A sensible starting point is to treat our modelas a document classifier and incorporate standardfeatures of the text directly into the model, ratherthan deriving functional categories or proxy votesfrom the text.14 Perhaps unsurprisingly, this ap-proach will perform better than the previous two.Following Pang and Lee (2004), who used wordand bigram features to model an author?s sentiment,and Kogan et al (2009), who used word and bigramfeatures to directly predict a future outcome, we in-corporate binary features for the presence or absenceof terms in the body and (separately) in the title ofthe bill.
We include unigram features for the bodyand unigram and bigram features for the title.15 Theresult is 28,246 features, of which 24,515 are lexical.Performance.
Combined with baseline features,word and bigram features led to nearly 18% relativeerror reduction compared to the baseline and 9% rel-ative to the best model above (Table 3).
The modelis very small (under 200 features), and 98% of thefeatures in the model impacted test-time predictions.The model?s gain over the baseline is not sensitive tothe score threshold; see Figure 1.A key finding is that the bag of words model out-14The models from ?5.1 and ?5.2 can be understood froma machine learning perspective as task-specific dimensionalityreduction methods on the words.15Punctuation marks are removed from the text, and numbersare collapsed into single indicator.
We filtered terms appearingin fewer than 0.5% and more than 30% of training documents.799Bill Survival Bill DeathContents Title Contents Titleresources 0.112 title as 0.052 percent -0.074 internal -0.058ms 0.056 other purposes 0.041 revenue -0.061 the internal 0.024authorization 0.053 for other 0.028 speaker -0.050 revenue -0.022information 0.049 amended by 0.017 security -0.037 prohibit -0.020authorize 0.030 of the 0.017 energy -0.037 internal revenue -0.019march 0.029 for the 0.014 make -0.030 the social -0.018amounts 0.027 public 0.012 require -0.029 amend title -0.016its 0.026 extend 0.011 human -0.029 to provide -0.015administration 0.026 designate the 0.010 concerned -0.029 establish -0.015texas 0.024 as amended 0.009 department -0.027 SYMBOL to -0.014interior 0.023 located 0.009 receive -0.025 duty on -0.013judiciary 0.021 relief 0.009 armed -0.024 revenue code -0.013Table 4: Full model:text terms withhighest impact(eq.
4).
Impactscores are notcomparable acrossmodels, so for com-parison, the impactsfor the features fromTable 2 here are,respectively: 0.534,0.181, 10?4, 0.196,0.123, 0.063, 0.053;-0.011, 0, 0.003, 0.0.0 0.2 0.4 0.6 0.80.20.40.60.81.0RecallPrecisionBag of WordsBaselineFigure 1: Precision-recall curve (survival is the targetclass) comparing the bag of words model to the baseline.performs the bill categories and proxy vote models.This suggests that there is more information in thetext contents than either the functional categories orsimilarity to past bills.165.4 Full ModelFinally, we considered a model using all three kindsof text features.
Shown in Table 3, this reduces erroronly 2% relative to the bag of words model.
Thisleads us to believe that direct use of text capturesmost of what functional bill category and proxy votefeatures capture about bill success.16We also experimented with dimensionality reduction withlatent Dirichlet alocation (Blei et al, 2003).
We used the topicposteriors as features in lieu of words during training and test-ing.
The symmetric Dirichlet hyperparameter was fixed at 0.1,and we explored 10?200 topics.
Although this offered speedupsin training time, the performance was consistently worse thanthe bag of words model, for each number of topics.Table 4 shows the terms with greatest impact.When predicting bills to survive, the model seemsto focus on explanations for minor legislation.
Forexample, interior and resources may indicate non-controversial local land transfer bills.
In titles, des-ignate and located have to do with naming federalbuildings (e.g., post offices).As for bills that die, the model appears to havecaptured two related facts about proposed legisla-tion.
One is that legislators often sponsor bills toexpress support or concern about an issue with littleexpectation that the bill will become a law.
If such?position-taking?
accounts for many of the bills pro-posed, then we would expect features with high im-pact toward failure predictions to relate to such is-sues.
This would explain the terms energy, security,and human (if used in the context of human rights orhuman cloning).
The second fact is that some billsdie because committees ultimately bundle their con-tents into bigger bills.
There are many such bills re-lating to tax policy (leading to the terms contained inthe trigram Internal Revenue Service, the Americantax collection agency) and Social Security policy (acollection of social welfare and social insurance pro-grams), for example.1717The term speaker likely refers to the first ten bill numbers,which are ?reserved for the speaker,?
which actually implies thatno bill was introduced.
Our process for marking bills that sur-vive (based on committee recommendation data) leaves theseunmarked, hence they ?died?
in our gold-standard data.
Theexperiments revealed this uninteresting anomaly.800ModelError (%)109th 110thmost frequent class 11.8 14.5?4.2 baseline (no text) 11.1 13.9?5.1 bill categories 10.9 13.6?5.2 proxy vote, all three 9.9 12.7?5.3 unigram & bigram 8.9 10.6?5.4 full model 8.9 10.9Table 5: Replicated results on two different data splits.Columns are marked by the test-set Congress.
See ?5.5.5.5 ReplicationTo avoid drawing conclusions based on a single,possibly idiosyncratic Congress, we repeated the ex-periment using the 109th and 110th Congresses astest datasets, training only on bills prior to the testset.
The error patterns are similar to the primarysplit; see Table 5.6 DiscussionFrom a political science perspective, our experimen-tal results using text underscore the importance ofconsidering the substance of policy proposals (here,bills) when attempting to explain their progress.
Animportant research direction in political science, onein which NLP must play a role, is how differenttypes of issues are managed in legislatures.
Our re-sults also suggest that political considerations mayinduce lawmakers to sponsor certain types of billswith no real expectation of seeing them enacted intolaw.Considerable recent work has modeled text along-side data about social behavior.
This includes pre-dictive settings (Kogan et al, 2009; Lerman etal., 2008), various kinds of sentiment and opin-ion analysis (Thomas et al, 2006; Monroe et al,2008; O?Connor et al, 2010; Das et al, 2009), andexploratory models (Steyvers and Griffiths, 2007).In political science specifically, the ?text as data?movement (Grimmer and Stewart, 2012; O?Connoret al, 2011) has leveraged tools from NLP in quan-titative research.
For example, Grimmer (2010) andQuinn et al (2006) used topic models to study, re-spectively, Supreme Court proceedings and Senatespeeches.
Closest to this work, Gerrish and Blei(2011) combined topic models with spatial roll callmodels to predict votes in the legislature from textalone.
Their best results, however, came from atext regression model quite similar to our direct textmodel.7 ConclusionsWe presented a novel task: predicting whether aCongressional bill will be recommended by a com-mittee.
We introduced a strong, expert-informedbaseline that uses basic social features, then demon-strated substantial improvents on the task using textin a variety of ways.
Comparison leads to insightsabout American lawmaking.
The data are availableto the research community.AcknowledgmentsWe thank the anonymous reviewers, David Bamman,Justin Grimmer, Michael Heilman, Brendan O?Connor,Dani Yogatama, and other members of the ARK researchgroup for helpful feedback.
This research was supportedby DARPA grant N10AP20042.ReferencesE.
Scott Adler and John Wilkerson.
2005.
The scopeand urgency of legislation: Reconsidering bill successin the house of representatives.
Paper presented at theannual meeting of the American Political Science As-sociation.E.
Scott Adler and John Wilkerson.
2012.
Congress andthe Politics of Problem Solving.
Cambridge UniversityPress, London.Beata Beigman Klebanov, Daniel Diermeier, and EyalBeigman.
2008.
Lexical cohesion analysis of politi-cal speech.
Political Analysis, 16(4):447?463.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Joshua Clinton, Simon Jackman, and Doug Rivers.
2004.The statistical analysis of roll-call data.
American Po-litical Science Review, 98(2):355?370.Pradipto Das, Rohini Srihari, and Smruthi Mukund.2009.
Discovering voter preferences in blogs usingmixtures of topic models.
In Proceedings of the ThirdWorkshop on Analytics for Noisy Unstructured TextData.Sean Gerrish and David Blei.
2011.
Predicting legisla-tive roll calls from text.
In Proc.
of ICML.Justin Grimmer and Brandon Stewart.
2012.
Text asdata: The promise and pitfalls of automatic contentanalysis methods for political documents.
http://www.stanford.edu/?jgrimmer/tad2.pdf.801Justin Grimmer.
2010.
A Bayesian hierarchical topicmodel for political texts: Measuring expressed agen-das in Senate press releases.
Political Analysis,18(1):1?35.Simon Jackman.
2001.
Multidimensional analysis ofroll call data via Bayesian simulation: Identification,estimation, inference, and model checking.
PoliticalAnalysis, 9(3):227?241.Shimon Kogan, Dimitry Levin, Bryan R. Routledge, Ja-cob S. Sagi, and Noah A. Smith.
2009.
Predictingrisk from financial reports with regression.
In Proc.
ofNAACL.Michael Laver, Kenneth Benoit, and John Garry.
2003.Extracting policy positions from political texts usingwords as data.
American Political Science Review,97(2):311?331.Kevin Lerman, Ari Gilder, Mark Dredze, and FernandoPereira.
2008.
Reading the markets: Forecasting pub-lic opinion of political candidates by news analysis.
InProc.
of COLING.Nolan McCarty, Howard Rosenthal, and Keith T. Poole.2006.
Polarized America: The Dance of Ideology andUnequal Riches.
MIT Press.Burt Monroe, Michael Colaresi, and Kevin M. Quinn.2008.
Fightin?
words: Lexical feature selection andevaluation for identifying the content of political con-flict.
Political Analysis, 16(4):372?403.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From tweets to polls: Linking text sentiment to publicopinion time series.
In Proc.
of ICWSM.Brendan O?Connor, David Bamman, and Noah A. Smith.2011.
Computational text analysis for social science:Model complexity and assumptions.
In Proc.
of theNIPS Workshop on Comptuational Social Science andthe Wisdom of Crowds.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
of ACL.Keith T. Poole and Howard Rosenthal.
1985.
A spatialmodel for legislative roll call analysis.
American Jour-nal of Political Science, 29(2):357?384.Keith T. Poole and Howard Rosenthal.
1991.
Patterns ofcongressional voting.
American Journal of PoliticalScience, 35(1):228?278.Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,Michael H. Crespin, and Dragomir R. Radev.
2006.An automated method of topic-coding legislativespeech over time with application to the 105th?108thU.S.
Senate.
Paper presented at the meeting of theMidwest Political Science Association.Kevin M. Quinn, Burt L Monroe, Michael Colaresi,Michael H. Crespin, and Dragomir R. Radev.
2010.How to analyze political attention with minimal as-sumptions and costs.
American Journal of PoliticalScience, 54(1):209?228.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
In T. Landauer, D. McNamara, S. Den-nis, and W. Kintsch, editors, Handbook of Latent Se-mantic Analysis.
Lawrence Erlbaum.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proc.
ofEMNLP.802
