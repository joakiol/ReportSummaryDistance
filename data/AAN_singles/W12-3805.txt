Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 37?46, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsImproving Speculative Language Detection using Linguistic KnowledgeGuillermo MoncecchiFacultad de Ingenier?
?aUniversidad de la Repu?blicaMontevideo, UruguayJean-Luc MinelLaboratoire MoDyCoUniversite?
Paris OuestNanterre La De?fense, FranceDina WonseverFacultad de Ingenier?
?aUniversidad de la Repu?blicaMontevideo, UruguayAbstractIn this paper we present an iterative method-ology to improve classifier performance by in-corporating linguistic knowledge, and proposea way to incorporate domain rules into thelearning process.
We applied the methodol-ogy to the tasks of hedge cue recognition andscope detection and obtained competitive re-sults on a publicly available corpus.1 IntroductionA common task in Natural Language Processing(NLP) is to extract or infer factual information fromtextual data.
In the field of natural sciences this taskturns out to be of particular importance, becausescience aims to discover or describe facts from theworld around us.
Extracting these facts from thehuge and constantly growing body of research ar-ticles in areas such as, for example, molecular biol-ogy, becomes increasingly necessary, and has beenthe subject of intense research in the last decade(Ananiadou et al, 2006).
The fields of informationextraction and text mining have paid particular atten-tion to this issue, seeking to automatically populatestructured databases with data extracted or inferredfrom text.
In both cases, the problem of speculativelanguage detection is a challenging one, because itmay correspond to a subjective attitude of the writertowards the truth value of certain facts, and that in-formation should not be lost when the fact is ex-tracted or inferred.When researchers express facts and relations intheir research articles, they often use speculative lan-guage to convey their attitude to the truth of whatis said.
Hedging, a term first introduced by Lakoff(1973) to describe ?words whose job is to makethings fuzzier or less fuzzy?
is ?the expression of ten-tativeness and possibility in language use?
(Hyland,1995), and is extensively used in scientific writing.Hyland (1996a) reports one hedge in every 50 wordsof a corpus of research articles; Light et al (2004)mention that 11% of the sentences in MEDLINEcontain speculative language.
Vincze et al (2008)report that 18% of the sentences in the scientific ab-stracts section of the Bioscope corpus correspond tospeculations.Early work on speculative language detectiontried to classify a sentence either as speculativeor non-speculative (see, for example, Medlock andBriscoe (2007)).
This approach does not take intoaccount the fact that hedging usually affects propo-sitions or claims (Hyland, 1995) and that sentencesoften include more than one of them.
When the Bio-scope corpus (Vincze et al, 2008) was developedthe notions of hedge cue (corresponding to what waspreviously called just ?hedges?
in the literature) andscope (the propositions affected by the hedge cues)were introduced.
In this context, speculative lan-guage recognition can be seen as a two-phase pro-cess: first, the existence of a hedge cue in a sentenceis detected, and second, the scope of the inducedhedge is determined.
This approach was first usedby Morante et al (2008) and subsequently in manyof the studies presented in the CoNLL-2010 Confer-ence Shared Task (Farkas et al, 2010a), and is theone used in this paper.For example, the sentence(1) This finding {suggests suggests that the BZLF137promoter {may may be regulated by the degreeof squamous differentiation}may}suggests.contains the word ?may?
that acts as a hedge cue(i.e.
attenuating the affirmation); this hedge onlyaffects the propositions included in the subordinateclause that contains it.Each of these phases can be modelled (albeit withsome differences, described in the following sec-tions) as a sequential classification task, using a sim-ilar approach to that commonly used for named en-tity recognition or semantic labelling: every wordin the sentence is assigned a class, identifying spansof text (as, for example, scopes) with, for example, aspecial class for the first and last element of the span.Correctly learning these classes is the computationaltask to be solved.In this paper we present a methodology and ma-chine learning system implementing it that, basedon previous work on speculation detection, studieshow to improve recognition by analysing learningerrors and incorporating advice from domain expertsin order to solve the errors without hurting overallperformance.
The methodology proposes the use ofdomain knowledge rules that suggest a class for aninstance, and shows how to incorporate them intothe learning process.
In our particular task domainknowledge is linguistic knowledge, as hedging andscopes issues are general linguistic devices.
In thispaper we are going both terms interchangeably.The paper is organized as follows.
In Section 2we review previous theoretical work on speculativelanguage and the main computational approaches tothe task of detecting speculative sentences.
Section3 briefly describes the corpus used for training andevaluation.
In Section 4 we present the specific com-putational task to which our methodology was ap-plied.
In Section 5 we present the learning method-ology we propose to use, and describe the systemwe implemented, including lexical, syntactic and se-mantic attributes we experimented with.
We presentand discuss the results obtained in Section 6.
Finally,in Section 7 we analyse the approach presented hereand discuss its advantages and problems, suggestingfuture lines of research.2 Related workThe grammatical phenomenon of modality, definedas ?a category of linguistic meaning having to dowith the expression of possibility and necessity?
(von Fintel, 2006) has been extensively studied inthe linguistic literature.
Modality can be expressedusing different linguistic devices: in English, for ex-ample, modal auxiliaries (such as ?could?
or ?must?
),adverbs (?perhaps?
), adjectives (?possible?
), or otherlexical verbs (?suggest?,?indicate?
), are used to ex-press the different ways of modality.
Other lan-guages express modality in different forms, for ex-ample using the subjunctive mood.
Palmer (2001)considers modality as the grammaticalization ofspeakers?
attitudes and opinions, and epistemicmodality, in particular, applies to ?any modal sys-tem that indicates the degree of commitment by thespeaker to what he says?.Although hedging is a concept that is closelyrelated to epistemic modality, they are different:modality is a grammatical category, whereas hedg-ing is a pragmatic position (Morante and Sporleder,2012).
This phenomenon has been theoreticallystudied in different domains and particularly in sci-entific writing (Hyland, 1995; Hyland, 1996b; Hy-land, 1996a).From a computational point of view, speculativelanguage detection is an emerging area of research,and it is only in the last five years that a relativelylarge body of work has been produced.
In the re-mainder of this section, we survey the main ap-proaches to hedge recognition, particularly in En-glish and in research discourse.Medlock and Briscoe (2007) applied a weakly su-pervised learning algorithm to classify sentences asspeculative or non-speculative, using a corpus theybuilt and made publicly available.
Morante andDaelemans (2009) not only tried to detect hedgecues but also to identify their scope, using a met-alearning approach based on three supervised learn-ing methods.
They achieved an F1 of 84.77 forhedge identification, and 78.54 for scope detection(using gold-standard hedge signals) in the Abstractssections of the Bioscope corpus.Task 2 of the CoNLL-2010 Conference SharedTask (Farkas et al, 2010b) proposed solving theproblem of in-sentence hedge cue phrase identi-38fication and scope detection in two different do-mains (biological publications and Wikipedia arti-cles), based on manually annotated corpora.
Theevaluation criterion was in terms of precision, recalland F-measure, accepting a scope as correctly clas-sified if the hedge cue and scope boundaries wereboth correctly identified.The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a su-pervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing informa-tion, also including certain linguistic rules.
Forscope detection, Morante et al (2010) obtained anF-score of 57.3, using also a sequence classificationapproach for detecting boundaries (tagged in FOLformat, where the first token of the span is markedwith an F, while the last one is marked with anL).
The attributes used included lexical information,dependency parsing information, and some featuresbased on the information in the parse tree.The approximation of Velldal et al (2010) forscope detection was somewhat different: they de-veloped a set of handcrafted rules, based on depen-dency parsing and lexical features.
With this ap-proach, they achieved an F-score of 55.3, the thirdbest for the task.
Similarly, Kilicoglu and Bergler(2010) used a pure rule-based approach based onconstituent parse trees in addition to syntactic de-pendency relations, and achieved the fourth best F-score for scope detection, and the highest precisionof the whole task (62.5).
In a recent paper, Vell-dal et al (2012) reported a better F-score of 59.4 onthe same corpus for scope detection using a hybridapproach that combined a set of rules on syntacticfeatures and n-gram features of surface forms andlexical information and a machine learning systemthat selected subtrees in constituent structures.3 CorpusThe system presented in this paper uses the Bio-scope corpus (Vincze et al, 2008) as a learningsource and for evaluation purposes.
The Bioscopecorpus is a freely available corpus of medical freetexts, biological full papers and biological abstracts,annotated at a token level with negative and specu-lative keywords, and at sentence level with their lin-guistic scope.Clinical Full Abstract#Documents 954 9 1273#Sentences 6383 2670 11871%Hedge Sentences 13.4 19.4 17.7#Hedge cues 1189 714 2769Table 1: Bioscope corpus statistics about hedgingTable 1, extracted from Vincze et al (2008), givessome statistics related to hedge cues and sentencesfor the three sub corpora included in Bioscope.For the present study, we usee only the Abstractsub corpus for training and evaluation.
We randomlyseparated 20% of the corpus, leaving it for evalu-ation purposes.
We further sub-divided the remain-ing training corpus, separating another 20% that wasused as a held out corpus.
All the models presentedhere were trained on the resulting training corpusand their performance evaluated on the held out cor-pus.
The final results were computed on the previ-ously unseen evaluation corpus.4 Task descriptionFrom a computational point of view, both hedgecue identification and scope detection can be seenas a sequence classification problem: given a sen-tence, classify each token as part of a hedge cue (orscope) or not.
In almost every classification prob-lem, two main approaches can be taken (althoughmany variations and combinations exist in the lit-erature): build the classifier as a set of handcraftedrules, which, from certain attributes of the instances,decide which category it belongs to, or learn theclassifier from previously annotated examples, in asupervised learning approach.The rules approach is particularly suitable whendomain experts are available to write the rules, andwhen features directly represent linguistic informa-tion (for example, POS-tags) or other types of do-main information.
It is usually a time-consumingtask, but it probably grasps the subtleties of the lin-guistic phenomena studied better, making it possibleto take them into account when building the classi-fier.
The supervised learning approach needs taggeddata; in recent years the availability of tagged text39has grown, and this type of method has become thestate-of-the-art solution for many NLP problems.In our particular problem, we have both taggeddata and expert knowledge (represented by the bodyof work on modality and hedging), so it seems rea-sonable to see how we can combine the two methodsto achieve better classification performance.4.1 Identifying hedge cuesThe best results so far for this task used a tokenclassification approach or sequential labelling tech-niques, as Farkas et al (2010b) note.
In both cases,every token in the sentence is assigned a class la-bel indicating whether or not that word is acting as ahedge cue.
To allow for multiple-token hedge cues,we identify the first token of the span with the classB and every other token in the span with I, keepingthe O class for every token not included in the span,as the following example shows:(2) The/O findings/O indicate/B that/I MNDA/Oexpression/O is/O .
.
.
[ 401.8]After token labelling, hedge cue identification canbe seen as the problem of assigning the correct classto each token of an unlabelled sentence.
Hedge cueidentification is a sequential classification task: wewant to assign classes to an entire ordered sequenceof tokens and try to maximize the probability of as-signing the correct classes to every token in the se-quence, considering the sequence as a whole, notjust as a set of isolated tokens.4.2 Determining the scope of hedge cuesThe second sub-task involves marking the part of thesentence affected by the previously identified hedgecue.
Scopes are also spans of text (typically longerthan multi-word hedge cues), so we could use thesame reduction to a token classification task.
Be-ing longer, FOL classes are usually used for clas-sification, identifying the first token of the scopeas F, the last token as L and any other token inthe sentence as O.
Scope detection poses an addi-tional problem: hedge cues cannot be nested, butscopes (as we have already seen) usually are.
Inexample 1, the scope of ?may?
is nested within thescope of ?suggests?.
To overcome this, Moranteand Daelemans (2009) propose to generate a dif-ferent learning example for each cue in the sen-tence.
In this setting, each example becomes a pair?labelled sentence, hedge cue position?.
So, for ex-ample 1, the scope learning instances would be:(3) ?This/O finding/O suggests/F that/O the/OBZLF1/O promoter/O may/O be/Oregulated/O by/O the/O degree/O of/Osquamous/O differentiation/L./O, 3?
(4) ?This/O finding/O suggests/O that/O the/FBZLF1/O promoter/O may/O be/Oregulated/O by/O the/O degree/O of/Osquamous/O differentiation/L./O, 8?Learning on these instances, and using a similarapproach to the one used in the previous task, weshould be able to identify scopes for previously un-seen examples.
Of course, the two tasks are not in-dependent: the success of the second one dependson the success of the first.
Accordingly, evaluationof the second task can be done using gold standardhedge cues or with the hedge cues learned in the firsttask.5 Methodology and System DescriptionTo approach both sequential learning tasks, we fol-low a learning methodology (depicted in Figure 1),that starts with an initial guess of attributes for su-pervised learning and a learning method, and triesto improve its performance by incorporating domainknowledge.
We consider that expressing this knowl-edge through rules (instead of learning features) is abetter way for a domain expert to suggest new use-ful information or to generalize certain relations be-tween attributes and classification results when thelearning method cannot achieve this because of in-sufficient training data.
These rules, of course, haveto be converted to attributes to incorporate them intothe learning process.
These attributes are what wecall knowledge rules and their generation will be de-scribed in the Analysis section.5.1 PreprocessingBefore learning, we propose to add every possibleitem of external information to the corpus so as tointegrate different sources of knowledge (either theresult of external analysis or in the form of seman-tic resources).
After this step, all the informationis consolidated into a single structure, facilitating40subsequent analysis.
In our case, we incorporatePOS-tagging information, resulting from the appli-cation of the GENIA tagger (Tsuruoka et al, 2005),and deep syntax information obtained with the ap-plication of the Stanford Parser (Klein and Manning,2003), leading to a syntax-oriented representation ofthe training data.
For a detailed description of theenriching process, the reader is referred to Moncec-chi et al (2010).5.2 Initial ClassifierThe first step for improving performance is, ofcourse, to select an initial set of learning features,and learn from training data to obtain the first clas-sifier, in a traditional supervised learning scenario.The sequential classification method will depend onthe addressed task.
After learning, the classifier isapplied on the held out corpus to evaluate its per-formance (usually in terms of Precision, Recall andF1-measure), yielding performance results and a listof errors for analysis.
This information is the sourcefor subsequent linguistic analysis.
As such, it seemsimportant to provide ways to easily analyse instanceattributes and learning errors.
For our tasks, wehave developed visualization tools to inspect the treerepresentation of the corpus data, the learning at-tributes, and the original and predicted classes.5.3 AnalysisFrom the classifier results on the held-out corpus,an analysis phase starts, which tries to incorporatelinguistic knowledge to improve performance.One typical form of introducing new informationis through learning features: for example, we canadd a new attribute indicating if the current instance(in our case, a sentence token) belongs to a list ofcommon hedge cues.However, linguistic or domain knowledge canalso naturally be stated as rules that suggest the classor list of classes that should be assigned to instances,based on certain conditions on features, linguisticknowledge or data observation.
For example, basedon corpus annotation guidelines, a rule could statethat the scope of a verb hedge cue should be the verbphrase that includes the cue, as in the expression(5) This finding {suggests suggests that the BZLF1promoter may be regulated by the degree ofsquamous differentiation}suggests.We assume that these rules take the form ?if a con-dition C holds then classify instance X with class Y?.In the previous example, assuming a FOL formatfor scope identification, the token ?suggest?
shouldbe assigned class F and the token ?differentiation?should be assigned class L, assigning class O to ev-ery other token in the sentence.The general problem with these rules is that aswe do not know in fact if they always apply, we donot want to directly modify the classification results,but to incorporate them as attributes for the learningtask.
To do this, we propose to use a similar ap-proach to the one used by Rosa?
(2011), i.e.
to incor-porate these rules as a new attribute, valued with theclass predictions of the rule, trying to ?help?
the clas-sifier to detect those cases where the rule should fire,without ignoring the remaining attributes.
In the pre-vious example, this attribute would be (when the rulecondition holds) valued F or L if the token corre-sponds to the first or last word of the enclosing verbphrase, respectively.
We have called these attributesknowledge rules to reflect the fact that they suggesta classification result based on domain knowledge.This configuration allows us to incorporateheuristic rules without caring too much about theirpotential precision or recall ability: we expect theclassification method to do this for us, detecting cor-relations between the rule result (and the rest of theattributes) and the predicted class.There are some cases where we do actually wantto overwrite classifier results: this is the case whenwe know the classifier has made an error, becausethe results are not well-formed.
For example, wehave included a rule that modifies the assignedclasses when the classifier has not exactly found oneF token and one L token, as we know for sure thatsomething has gone wrong.
In this case, we decidedto assign the scope based on a series of postprocess-ing rules: for example, assign the scope of the en-closing clause in the syntax tree as hedge scope, inthe case of verb hedge cues.For sequential classification tasks, there is an ad-ditional issue: sometimes the knowledge rule indi-cates the beginning of the sequence, and its end canbe determined using the remaining attributes.
Forexample, suppose the classifier suggests the class41Hedge PPOS GPPOS Lemma PScope GPScope ScopeO VP S This O O OO VP S finding O O OO VP S suggest O O OO VP S that O O OO VP S the O F FO VP S BZLF1 O O OO VP S promoter O O OB VP S may F O OO VP S be O O OO VP S regulate O O OO VP S by O O OO VP S the O O OO VP S degree O O OO VP S of O O OO VP S squamous O O OO VP S differentiation L L OO VP S .
O O OTable 2: Evaluation instance where the scope endingcould not be identifiedscope in the learning instance shown in table 2 (us-ing as attributes the scopes of the parent and grand-parent constituents for the hedge cue in the syntaxtree).
If we could associate the F class suggestedby the classifier with the grand parent scope rule,we would not be concerned about the prediction forthe last token, because we would knew it would al-ways correspond to the last token of the grand par-ent clause.
To achieve this, we modified the class wewant to learn, introducing a new class, say X, insteadof F, to indicate that, in those cases, the L token mustnot be learned, but calculated in the postprocessingstep, in terms of other attributes?
values (in this ex-ample, using the hedge cue grandparent constituentlimits).
This change also affects the classes of train-ing data instances (in the example, every traininginstance where the scope coincides with the grandparent scope attribute will have its F-classified to-ken class changed to X).In the previous example, if the classifier assignsclass X to the ?the?
token, the postprocessing stepwill change the class assigned to the ?differentiation?token to L, no matter which class the classifier hadpredicted, changing also the X class to the originalF, yielding a correctly identified scope.After adding the new attributes and changing therelevant class values in the training set, the processstarts over again.
If performance on the held out cor-pus improves, these attributes are added to the bestconfiguration so far, and used as the starting pointfor a new analysis.
When no further improvementcan be achieved, the process ends, yielding the bestFigure 1: Methodology overviewclassifier as a result.We applied the proposed methodology to the tasksof hedge cue detection and scope resolution.
Wewere mainly interested in evaluating whether sys-tematically applying the methodology would indeedimprove classifier performance.
The following sec-tions show how we tackled each task, and how wemanaged to incorporate expert knowledge and im-prove classification.5.4 Hedge Cue IdentificationTo identify hedge cues we started with a sequen-tial classifier based on Conditional Random Fields(Lafferty et al, 2001), the state-of-the-art classifi-cation method used for sequence supervised learn-ing in many NLP tasks.
The baseline configurationwe started with included a size-2 window of surfaceforms to the left and right of the current token, pairsand triples of previous/current surface forms.
Thisled to a highly precise classifier (an F-measure of95.5 on the held out corpus).
After a grid searchon different configurations of surface forms, lemmasand POS tags, we found (somewhat surprisingly)that the best precision/recall tradeoff was obtainedjust using a window of size 2 of unigrams of sur-face forms, lemmas and tokens with a slightly worseprecision than the baseline classifier, but compen-42Configuration P R F1Baseline 95.5 74.0 83.4Conf1 94.7 80.3 86.9Conf2 91.3 84.0 87.5Table 3: Classification performance on the held out cor-pus for hedge cue detection.
Conf1 corresponds to win-dows of Word, Lemma and POS attributes and Conf2 in-corporates hedge cue candidates and cooccuring wordssated by an improvement of about six points in re-call, achieving an F-score of 86.9.In the analysis step of the methodology we foundthat most errors came from False Negatives, i.e.words incorrectly not marked as hedges.
We alsofound that those words actually occurred in the train-ing corpus as hedge cues, so we decided to add newrule attributes indicating membership to certain se-mantic classes.
After checking the literature, weadded three attributes:?
Hyland words membership: this feature was setto Y if the word was part of the list of wordsidentified by Hyland (2005)?
Hedge cue candidates: this feature was set toY if the word appeared as a hedge cue in thetraining corpus?
Words co-occurring with hedge cue candidates:this feature was set to Y if the word cooccuredwith a hedge cue candidate in the training cor-pus.
This feature is based on the observationthat 43% of the hedges in a corpus of scientificarticles occur in the same sentence as at leastanother device (Hyland, 1995).After adding these attributes and tuning the win-dow sizes, performance improved to an F-score of87.5 in the held-out corpus5.5 Scope identificationTo learn scope boundaries, we started with a similarconfiguration of a CRF classifier, using a window ofsize 2 of surface forms, lemmas and POS-tags, andthe hedge cue identification attribute (either obtainedfrom the training corpus when using gold standardhedge cues or learned in the previous step), achiev-ing a performance of 63.7 in terms of F-measure.When we incorporated information in the form of aknowledge rule that suggested the scope of the con-stituent of the parsing tree headed by the parent nodeof the first word of the hedge cue, and an attributecontaining the parent POS-tag, performance rapidlyimproved about two points measured in terms of F-score.After several iterations, and analyzing classifica-tion errors, we included several knowledge rules, at-tributes and postprocessing rules that dramaticallyimproved performance on the held-out corpus:?
We included attributes for the scope of the nextthree ancestors of the first word of the hedgecue in the parsing tree, and their respectivePOS-tags, in a similar way as with the parent.We also included a trigram with the ancestorsPOS from the word upward in the tree.?
For parent and grandparent scopes, we incor-porated X and Y classes instead of F, and mod-ified postprocessing to use the last token of thecorresponding scope when one of these classeswas learned.?
We modified the ancestors scopes to reflectsome corpus annotation guidelines or other cri-teria induced after data examination.
For ex-ample, we decided not to include adverbialphrases or prepositional phrases at the begin-ning of scopes, when they corresponded to aclause, as in(6) In addition,{unwanted and potentiallyhazardous specificities may beelicited.
.
.}?
We added postprocessing rules to cope withcases where (probably due to insufficient train-ing data), the classifier missclasified certain in-stances.
For example, we forced classificationto use the next enclosing clause (instead of verbphrase), when the hedge cue was a verb conju-gated in passive voice, as in(7) {GATA3 , a member of the GATA familythat is abundantly expressed in theT-lymphocyte lineage , is thought toparticipate in ...}.43Configuration Gold-P P R F1Baseline 66.4 68.6 59.6 63.8Conf1 68.7 71.3 61.8 66.2Conf2 73.3 75.6 65.4 70.1Conf3 80.9 82.1 71.3 76.3Conf4 88.2 82.0 76.3 79.1Table 4: Classification performance on the held out cor-pus.
The baseline used a window of Word, Lemma,POS attributes and hedge cue tag; Conf1 included parentscopes, Conf2 added grandparents information; Conf3added postprocessing rules.
Finally, Conf4 used adjustedscopes and incorporated new postprocessing rules?
We excluded references at the end of sentencesfrom all the calculated scopes.?
We forced classification to the next S,VP or NPancestor constituent in the syntax tree (depend-ing on the hedge cue POS), when full scopescould not be determined by the statistical clas-sifier (missing either L or F, or learning morethan one of them in the same sentence).Table 4 summarizes the results of scope identifi-cation in the held out corpus.
The first results wereobtained using gold-standard hedge cues, while thesecond ones used the hedge cues learned in the pre-vious step (for hedge cue identification, we used thebest configuration we found).
In the gold-standardresults, Precision, Recall and the F-measure arethe same because every False Positive (incorrectlymarked scope) implied a False Negative (the missedright scope).6 EvaluationTo determine classifier performance, we evaluatedthe classifiers found after improvement on the eval-uation corpus.
We also evaluated the less efficientclassifiers to see whether applying the iterative im-provement had overfitted the classifier to the corpus.To evaluate scope detection, we used the best con-figuration found in the evaluation corpus for hedgecue identification.
Tables 5 and 6 show the resultsfor the hedge cue recognition and scope resolution,respectively.
In both tasks, classifier performanceConfiguration P R F1Baseline 97.9 78.0 86.8Conf1 95.9 84.9 90.1Conf2 94.1 88.6 91.3Table 5: Classification performance on the evaluationcorpus for hedge cue detectionConfiguration Gold-P P R F1Baseline 74.0 71.9 68.1 70.0Conf1 76.5 74.4 70.2 72.3Conf2 80.0 77.2 72.9 75.0Conf3 83.1 80.0 75.2 77.3Conf4 84.7 80.1 75.8 77.9Table 6: Classification performance on the evaluationcorpus for scope detectionimproved in a similar way to the results obtained onthe held out corpus.Finally, to compare our results with state-of-the-art methods (even though that was not the mainobjective of the study), we used the corpus of deCoNLL 2010 Shared Task to train and evaluate ourclassifiers, using the best configurations found in theevaluation corpus, and obtained competitive resultsin both subtasks of Task 2.
Our classifier for hedgecue detection achieved an F-measure of 79.9, bet-ter than the third position in the Shared Task forhedge identification.
Scope detection results (us-ing learned hedge cues) achieved an F-measure of54.7, performing better than the fifth result in thecorresponding task, and five points below the bestresults obtained so far in the corpus (Velldal et al,Hedge cue iden-tificationScope detectionBest results 81.7/81.0/81.3 59.6/55.2/57.3Our results 83.2/76.8/79.9 56.7/52.8/54.7Table 7: Classification performance compared withbest results in CoNLL Shared Task.
Figures representPrecision/Recall/F1-measure442012).
Table 7 summarizes these results in terms ofPrecision/Recall/F1-measure.7 Conclusions and Future ResearchIn this paper we have presented an iterative method-ology to improve classifier performance by incor-porating linguistic knowledge, and proposed a wayto incorporate domain rules to the learning process.We applied the methodology to the task of hedgecue recognition and scope finding, improving per-formance by incorporating information of trainingcorpus occurrences and co-occurrences for the firsttask, and syntax constituents information for the sec-ond.
In both tasks, results were competitive withthe best results obtained so far on a publicly avail-able corpus.
This methodology could be easily usedfor other sequential (or even traditional) classifica-tion tasks.Two directions are planned for future research:first, to improve the classifier results by incorporat-ing more knowledge rules such as those described byVelldal et al (2012) or semantic resources, speciallyfor the scope detection task.
Second, to improve themethodology, for example by adding some way toselect the most common errors in the held out cor-pus and write rules based on their examination.ReferencesS.
Ananiadou, D. Kell, and J. Tsuj.
2006.
Text min-ing and its potential applications in systems biology.Trends in Biotechnology, 24(12):571?579, December.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010a.
The CoNLL-2010 shared task: Learning to detect hedges and theirscope in natural language text.
In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 1?12, Uppsala, Sweden,July.
Association for Computational Linguistics.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,Gyo?rgy Mo?ra, and Ja?nos Csirik, editors.
2010b.
Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning.
Association forComputational Linguistics, Uppsala, Sweden, July.Ken Hyland.
1995.
The author in the text: Hedging sci-entific writing.
Hongkong Papers in Linguistics andLanguage Teaching, 18:33?42.Ken Hyland.
1996a.
Talking to the academy: Forms ofhedging in science research articles.
Written Commu-nication, 13(2):251?281.Ken Hyland.
1996b.
Writing without conviction?
Hedg-ing in science research articles.
Applied Linguistics,17(4):433?454, December.Ken Hyland.
2005.
Metadiscourse: Exploring Interac-tion in Writing.
Continuum Discourse.
Continuum.Halil Kilicoglu and Sabine Bergler.
2010.
A high-precision approach to detecting hedges and theirscopes.
In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning, pages70?77, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In ACL ?03: Proceedingsof the 41st Annual Meeting on Association for Compu-tational Linguistics, pages 423?430, Morristown, NJ,USA.
Association for Computational Linguistics.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of ICML-01, pages 282?289.George Lakoff.
1973.
Hedges: A study in meaning crite-ria and the logic of fuzzy concepts.
Journal of Philo-sophical Logic, 2(4):458?508, October.Marc Light, Xin Y. Qiu, and Padmini Srinivasan.
2004.The language of bioscience: Facts, speculations, andstatements in between.
In Lynette Hirschman andJames Pustejovsky, editors, HLT-NAACL 2004 Work-shop: BioLINK 2004, Linking Biological Literature,Ontologies and Databases, pages 17?24, Boston,Massachusetts, USA, May.
Association for Computa-tional Linguistics.Ben Medlock and Ted Briscoe.
2007.
Weakly supervisedlearning for hedge classification in scientific literature.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics.Guillermo Moncecchi, Jean-Luc Minel, and Dina Won-sever.
2010.
Enriching the bioscope corpus with lex-ical and syntactic information.
In Workshop in Natu-ral Language Processing and Web-based Tecnhologies2010, pages 137?146, November.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the BioNLP 2009 Workshop, pages 28?36, Boulder, Colorado, June.
Association for Compu-tational Linguistics.Roser Morante and Caroline Sporleder.
2012.
Modal-ity and negation: An introduction to the special issue.Computational Linguistics, pages 1?72, February.Roser Morante, Anthony Liekens, and Walter Daele-mans.
2008.
Learning the scope of negation inbiomedical texts.
In EMNLP ?08: Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 715?724, Morristown, NJ,USA.
Association for Computational Linguistics.45Roser Morante, Vincent Van Asch, and Walter Daele-mans.
2010.
Memory-based resolution of in-sentencescopes of hedge cues.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 40?47, Uppsala, Sweden, July.Association for Computational Linguistics.R.
F. Palmer.
2001.
Mood and Modality.
CambridgeTextbooks in Linguistics.
Cambridge University Press,New York.Aiala Rosa?.
2011.
Identificacio?n de opiniones de difer-entes fuentes en textos en espan?ol.
Ph.D. thesis, Uni-versidad de la Repu?blica (Uruguay), Universite?
ParisOuest (France), September.Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,and Shixi Fan.
2010.
A cascade method for detect-ing hedges and their scope in natural language text.In Proceedings of the Fourteenth Conference on Com-putational Natural Language Learning, pages 13?17,Uppsala, Sweden, July.
Association for ComputationalLinguistics.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust Part-of-Speech tagger for biomedical text.
In PanayiotisBozanis and Elias N. Houstis, editors, Advances in In-formatics, volume 3746, chapter 36, pages 382?392.Springer Berlin Heidelberg, Berlin, Heidelberg.Erik Velldal, Lilja ?vrelid, and Stephan Oepen.
2010.Resolving speculation: Maxent cue classification anddependency-based scope rules.
In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 48?55, Uppsala, Sweden,July.
Association for Computational Linguistics.Erik Velldal, Lilja ?vrelid, Jonathon Read, and StephanOepen.
2012.
Speculation and negation: Rules,rankers, and the role of syntax.
Computational Lin-guistics, pages 1?64, February.Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-orgy Mora, and Janos Csirik.
2008.
The bioscope cor-pus: biomedical texts annotated for uncertainty, nega-tion and their scopes.
BMC Bioinformatics, 9(Suppl11):S9+.Kail von Fintel, 2006.
Modality and Language.
MacMil-lan Reference USA.46
