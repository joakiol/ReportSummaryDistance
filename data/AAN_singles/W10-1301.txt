Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing NLG and Sensors to Support Personal Narrative forChildren with Complex Communication NeedsRolf Black Joe Reddington, Ehud Reiter, Nava Tintarev Annalu WallerSchool of Computing Department of Computing Science School of ComputingUniversity of Dundee            University of Aberdeen University of Dundeerolfblack@computing.dundee.ac.uk{j.reddington, e.reiter  n.tintarev}@abdn.ac.uk awaller@computing.dundee.ac.ukAbstractWe are building a tool that helps children withComplex Communication Needs1 (CCN) tocreate stories about their day at school.
Thetool uses Natural Language Generation (NLG)technology to create a draft story based onsensor data of the child?s activities, which thechild can edit.
This work is still in its earlystages, but we believe it has great potential tosupport interactive personal narrative which isnot well supported by current Augmentativeand Alternative Communication (AAC) tools.1 IntroductionMany tools have been developed to help children andadults who cannot speak (or who have limited speech)communicate better.
However, most of these tools havefocused on supporting communication for practicalgoals, such as ?I am thirsty.?
But human communica-tion is also used for social goals; we develop friendshipsand other inter-personal relationships via social interac-tion and communication.
The bulk of conversation ischaracterized by free narrative (Cheepen 1988).
One ofthe most important types of conversational narrative ispersonal narrative: someone telling a story about whathappened to him or her.People with limited or no functional speech do tellstories, but these tend to be in monologue form, or in asequence of pre-stored utterances on voice output com-munication aids (Waller 2006).
Individuals who use1The term Complex Communication Needs (CCN) describesindividuals who, due to motor, language, cognitive, and/orsensory perceptual impairments (e.g., as a result of cerebralpalsy), do not develop speech and language skills as expected.This heterogeneous group typically experiences restrictedaccess to the environment, limited interactions with theircommunication partners, and few opportunities for communi-cation (Light and Drager 2007).Augmentative and Alternative Communication (AAC)tools tend to be passive, responding to questions withsingle words or short sentences (e.g.
Soto, Hartmann etal.
2006) and if able to initiate and maintain extendedconversations tend to relate experience word for wordeach time they tell a story, even though much of conver-sation is reused (Clarke and Clarke 1977).
This is timeconsuming and physically exhausting ?
typical ratesrange from 8 to 10 words per minute up to 12 to 15 perminute when techniques such as word prediction areused (Higginbotham, Shane et al 2007), with the resultthat people seldom engage in storytelling.
Despite theimportance of narrative, little work has been done onspecific tools to help language-impaired individualsengage in personal storytelling.
In this paper, we de-scribe our work in progress on building a tool that usesNatural Language Generation (NLG) technology to helpchildren tell stories about their day at school, describingboth the work we have done to date, and the challengesthat we face in further developing this concept.2 Background2.1 AACTechnology underpins much of Augmentative and Al-ternative Communication (AAC), a field that attempts toaugment natural speech and provides alternative ways tocommunicate for people with limited or no speech.
Atthe simplest level, people with Complex Communica-tion Needs (CCN) can cause a pre-stored message to bespoken by activating a single switch.
At the most so-phisticated level, literate users can generate novel textusing input methods ranging from a single switch to afull keyboard.Despite advances in AAC, there are still many indi-viduals for whom communication remains problematic.Although some individuals with CCN become effectivecommunicators, most do not ?
they tend to be passivecommunicators, responding mainly to questions orprompts at a one or two word level.
Conversational1skills such as initiation, elaboration and storytelling areseldom observed (Waller 2006).One reason for the reduced levels of communicativeability is the cognitive demands of AAC interfaces.
Cur-rent AAC technology provides the user with a purelyphysical link to speech output.
The user is required tohave sufficient cognitive abilities and physical staminato translate what they want to say into the sequence ofoperations needed to produce the desired output.
Mne-monic codes and dynamic displays (Beukelman andMirenda 2005) provide some help in the retrievalprocess, but users still have to master complex retrievaland production strategies.A second reason for the impoverished quality ofconversation is the focus of AAC devices on transac-tional communication; conversation which expressesneeds wants and information transfer, for example, ?Iam thirsty?, ?I use a straw for drinking?.
Instead, inter-active conversation is characterized by free narrativeand phatic conversation, for example, ?Guess whathappened this morning?
?, ?Hello?, and ?How areyou??
Without easy access to extended interactivecommunication, it is difficult to develop the skillsneeded to initiate new topics and engage in storytelling.2.2 Importance of NarrativeConversational narratives (oral stories told duringinteractive conversations) are crucial to social engage-ment.
Narratives provide a means for people to relateand share experiences, develop organizational skills,work through problems, develop self image, expresspersonality, give form and meaning to life, and allowpeople to be interesting entertainers (Waller 2006).Narrative skills develop experientially with childrenbeing able to engage in storytelling even before they areverbal (Bruner 1975).
Early personal experience storiesconsist of a high point, for example, ?Mummy fall!
?with adults scaffolding the full story, eliciting the ?who?,?what?, ?when?
and ?where?.
However, not all expe-riences make good stories.
An experience becomes astory if the storyteller has an emotional connection tothe event (Labov, 1972), or if the event is unusual (Qua-sthoff & Nikolaus, 1982).Parents of typically developing children encouragedevelopment of narrative skills by eliciting stories fromtheir children (Peterson and McCabe 1983), but the de-velopment of narrative skills is problematic for peoplewith CCN.
We recall a study where disabled childrenwere told different stories more often than typicallydeveloping peers who were read the same story nightafter night (Light, Binger et al 1994).
In doing so, thedisabled children did not have the chance to learn thesequence of stories, or the structure commonly used innarrative such as beginning, middle and end.
As such,initially children should use the same story templateconsistently until they are ready to progress to anotherone.It is difficult to provide access to event informationwhich may become a story, and few AAC systems pro-vide support for interactive story narration.
HoweverNLG gives us a possibility to change the underlyingparadigm of AAC.
Instead of placing the entire cogni-tive load on the user, AAC devices can be designed tosupport the retrieval of story events and the scaffoldingof story narration for individuals with CCN.2.3 NLG, Data-to-textNLG systems generate texts in English (or other humanlanguages) from non-linguistic data (Reiter and Dale2000).
Our vision is to use an NLG system to generatea draft story, which the child can edit.
The non-linguistic input to our story-generator is sensor dataabout the child?s activities, including location data(where the child was) and interaction data (what peopleand objects the child interacted with).
We also want toallow teachers and school staff to enter informationabout the child?s activities (such as voice messages).A number of data-to-text systems (Reiter 2007) havebeen developed in recent years, which generate Englishsummaries of sensor and other numerical data.
Themost popular application area has been weather fore-casting (generating textual weather forecasts from theresults of a numerical atmosphere simulation model),and indeed several weather forecast generators havebeen fielded and used operationally (Goldberg, Driedgeret al 1994; Reiter, Sripada et al 2005).
A number ofdata-to-text systems have also been developed in themedical community, such as BabyTalk (Gatt, Portet etal.
2009), which generates summaries of clinical datafrom a neonatal intensive care unit, and the commercialNarrative Engine (Harris 2008) which summarizes dataacquired during a doctor/patient encounter.Most previous research in data-to-text has focusedon summarizing technical data for expert users, with thegoal of effectively communicating key information.
Inour work, in contrast, the focus is on summarizing dataabout everyday events, with the goal of having some-thing interesting to talk about.
There has been consider-able work in the computational creativity community ongenerating interesting stories (P?r?z and Sharples 2004),but it has focused on fictional written stories, where thecomputer system can say whatever it wishes, withoutthe constraint of describing real events.Most previous work in NLG has focused on com-puter systems which generate texts without human in-put.
However, in our case we want children to be ableto annotate (evaluate) and edit stories, as far as theirabilities permit.
There has been some research on hu-man post-editing of NLG texts (Sripada, Reiter et al2005), but this has focused on editing at the text level.2Since editing at the text level is very laborious for AACusers, we need a higher-level interface that lets childrenedit content and structure without needing to typewords.
We also want children to be able to control howa story is narrated, perhaps in response to a listener?squestions or body language.
For example children maywish to add comments such as ?It was awesome!
?, ortell events out of sequence.In short, we need to develop interfaces and interac-tion techniques that allow our users to control the NLGsystem.
Unfortunately there has been very little pre-vious work on this topic, indeed almost nothing isknown about Human-Computer Interaction aspects ofNLG systems.
Developing a better understanding ofthese aspects is one of the main research challenges weface from an NLG perspective.3 Current and ongoing work3.1 ?How was School today??
?We developed an initial version of ?How was Schooltoday???
in 2009; see Reiter et al(2009) for more de-tails about this system.Fig.
1: Participating pupil with support worker:The prototype system is mounted on the wheelchair, andthe pupil has access to the system via head switch con-trolled row/column scanning.This system used Radio Frequency Identification(RFID), an emerging application in AAC to identify orgive access to relevant vocabulary (Bart, Riny et al2008; DeRuyter and Fried-Oken 2010).
Sensors wereused to track both location (by putting tags on doors,which were automatically sensed by a long-range RFIDreader) and interaction (by asking staff to manuallyswipe RFID cards in a short-range reader when the childinteracted with a person or object).
Staff could alsorecord spoken messages about interesting events duringthe day (see Fig.
1).The software analyzed this data to remove sensor noise,and then compared it to a timetable which specifiedwhere children were supposed to be, what they weresupposed to be doing and which teacher was supposedto be taking the class throughout the day.
This allowedthe software to both fill in missing information, and toidentify divergences from the schedule.
The result ofthis process was a series of events (which correspondedto classes, for example, maths class), each of which hada set of associated messages (interactions during theevent, divergence from schedule, etc.
).After the data analysis was completed, an NLG sys-tem identified the events most interesting (to the child),using a heuristic that took into consideration both howinherent interesting an event was (for example, lunchwas regarded as an inherently interesting event thatchildren were likely to want to talk about) and alsowhether an event was unusual or not.
The latter is basedon the observation that most personal narratives focuson unexpected or unusual events.
Unusual events wereidentified by the presence of recorded voice messagesand by divergence from the timetable, e.g.
a differentteacher present or a different location.
The system se-lected the five most ?interesting?
events and displayedthem to the child in a simple visual editing interface.
Inthis interface the child could delete events he/she didnot wish to talk about, and also annotate events withsimple opinions (evaluations), such as I liked it, usingthe evaluation buttons on the interface, generating ap-propriate utterances according to the last narrated eventor message.
(see Fig.
2).When editing was finished, the NLG system generat-ed texts describing the events and messages, which thechild communicated using a simple narration interface(which was similar to the editing interface).
Emphasiswas placed on providing quick access to messages tominimize the length of pauses between utterances due tothe physical accessing difficulties of the users.
Thenarrative model is based on the Labov social narrativemodel (Labov 1972) which emphasizes the highpointand evaluation.
The dialogue model from beginningthrough to highpoint to the end with the user being ableto add evaluations at any point of the narration.
Storiesare initially chronological order but interactively underthe control of the user.
This control of narration differssignificantly from current AAC interventions wherenarrative tends to be output in a monologue format.From an NLG perspective, the system was fairlystraightforward.
The most challenging microplanningtasks were choosing connectives, time phrases, lexicalvariety in embellishments, and pronouns based on dis-course context.
Connectives and time phrases were ne-cessary since children could narrate events in differentorders (for example, ?I went to maths.
Then I went tolunch?
versus ?I went to lunch.
Before lunch, I went tomaths?).
Document structuring  was simple because weShort range RFID readerand microphone for voicemessage recordingVisual interfaceAccess switch in headrestLong range RFID reader3assumed that the children would choose their own orderin which to narrate events.
In fact some children are notable to do this; such children would need to be sup-ported by a more sophisticated document planner thathad a model of appropriate text structures in this do-main.We asked two children to use the system for oneweek for a qualitative formative evaluation.
Research-ers supplied ongoing support during this, primarily trialobserving how the children used the system, and dis-cussing it with teachers, therapists and parents.
Gener-ally it worked well for one child, Julie2, who had severemotor impairment (no independent means of mobilityand interacted with a computer using a head switch withrow/column scanning, see Fig.
1).
Her expressive abili-ties were limited but her comprehension skills werecomparable to her non-disabled peers with some deve-lopmental delay.
The other child, Jessica, had morecognitive impairment, and found the interface too diffi-cult.Fig.
2: Example screenshot from interface1: Navigation: Day and date of story, maximum of fivestory events, exit; 2: Event messages, numbers vary foreach event.
Here: 2 computer-generated messages, 3recorded messages, 1 user added evaluation;3: Sequential message navigation: previous, repeat,next; 4: Evaluation: delete event, negative evaluation,positive evaluation;In a second evaluation, a third child, Eric, joined andall three children used the system over two weeks each.In this evaluation, we asked teachers and other staff touse the system without on-site support from the re-searchers.
This highlighted many practical usabilityissues, such as delays caused by starting the system inthe morning, and problems caused by limited batterylife.
We eliminated the long-range RFID sensor becauseof its difficult setup; instead we asked staff to swipe2The names of the children mentioned in this paper arechanged to ensure anonymity.door cards when children entered rooms.
However, thisstrategy was not successful, as it was difficult for staffto remember to swipe both interactions and locationchanges.The participants took the system home for use withtheir parents who gave positive feedback but also re-ported issues with system usability (e.g.
lack of accessto stories from previous days) or suitability (too compli-cated interface for Jessica).Eric?s timetable was different from Julie and Jessi-ca?s, because he visited college one morning a day, andwe could not collect data during this period.
Since someof the most exciting events in a school day happen out-side the school building (sports and school trips as wellas college), in the long term we do need to see if we cancollect data outside as well as inside the school.3.2 HWST exampleJulie used the system on her DynaVox?
Vmax?Voice Output Communication Aid (VOCA) via headswitch using row/column scanning.
The above transcriptshows an extract of a conversation Julie had with herSpeech and Language Therapist (SLT) on day threeabout her experiences during day two.
The researcher(RA) had been present all day for technical support.
Theconversation extract starts with Julie reporting about hermorning break.In this example Julie is able to quickly reply to con-text related questions from her communication partnerusing the evaluations (?So what happened??
?
?It wasfun!?).
Compared to conversations usually observedbetween aided and unaided partners Julie is able to con-trol the conversation when starting a new topic aftertalking about the morning break, inviting her communi-cation partner to prompt for more detailed information.Julie provides this with her next generated phrase.When she is asked about the event she replies with anevaluation the system has generated in relation to itspreviously generated message ?A visitor was there.
?.We note that the system is able to refer to the correctgender of the visitor.1 Julie {next} [I had break.
]2 Julie {next} [Lesley was there.
]3 SLT Lesley was there?4 Julie ((Opens mouth in agreement, then turns backto screen))5 SLT Ok mhmh.
So what happened?6 Julie {positive evaluation} [It was fun.
]7 SLT Oh good!
((laughs)) I?m glad to hear it!8 RA We like Lesley.9 SLT ((nods in direction of RA))10 Julie ((smiles))11 Julie:  {next} [Then I went to Junior Primary in-stead of Reading Class.
]1234412 SLT: Right, you went to Junior Primary?
I wonderwhy that was?13 Julie: {next} [A visitor was there.
]14 SLT:  Oh, a visitor, right.
Wonder what the visitorwas doing?15 Julie: {next} [?The dental hygienist came to give atalk.?
]16 SLT: Oh, dental hygienist.17 Julie:  {previous} [A visitor was there.
]18 SLT: That was the visitor, okay.
That?s why youwent to junior primary, uhm, what did youthink of the talk?19 Julie:  {positive evaluation} [She was nice.
]20 SLT: She was nice, that was good!
((laughs))Notation:- Switch selected button by Julie: {curly brackets}- Natural speech: standard text.- Computer generated language accessed using onebutton: [standard text in square brackets].- Recorded messages accessed using one button:[?quoted standard text in square brackets?
].- Paralinguistic behaviors:((standard text in double brackets)).3.3  ?How was School Today?
?
in the WildWe have now started a new project to further developour work, called ?How was School today???
?
in theWild (?in the wild?
indicates that the focus is on how thetechnology works in a real school environment).
Thebasic goal is to improve the system sufficiently so that itcan be tried out over a period of several months, withchildren with varying levels and types of impairments;we will also work with several schools in the initialphase, although for practical reasons the evaluationsmay be at just one school.During this project we will do some work on theissues described in Section 4; in particular we will try tomake the system usable by children with different im-pairments and ability levels (Section 4.1).
This meanshaving a very simple interface for children with consi-derable cognitive impairments (such as Jessica); butalso giving children with more cognitive abilities theopportunity to exert more control over the story (duringboth editing and narration), for example by supporting aricher range of annotations, and by making it easier todescribe events and messages in any order.Another intermediate goal is to improve the integra-tion of voice messages entered by staff with the com-puter-generated messages.
This could be done by somecombination of training staff to enter messages in a spe-cific way (referring to the child in the first person); ask-ing staff to annotate the messages so the computerknows something about their content; and/or usingspeech recognition to analyze the voice messages.
Ingeneral there is a lot of interesting information that canonly come from staff, and we need to think about thebest way to help staff enter information in a way that iseasy for them and useful for our system.Now that a complete system is built, we are also ableto thoroughly and formally evaluate the system.
Mul-tiple baseline single case study methodology will beused (Schlosser 2003) to evaluate the use and impact ofour system.
We intend to have up to four children (withvarying ability levels) use the system for a period of 3months.
This will give us a chance to observe the im-pact of the system on the users and their environmentsuch as the children?s interaction with the system andhow staff at the school envisage using this new tool.The observations will be supported by semi-structuredinterviews with the children, their classroom teacher,their speech and language therapist and a parent.We will look at the children?s conversations (withand without using our system) about interesting, stagedevents with different partners, analyzing them conversa-tional characteristics such as narrative initiation, struc-ture, length and evaluation.
Analysis methods willinclude the Revised Edinburgh Functional Communica-tion Profile (REFCP) (Wirz, Skinner et al 1990).However, much of our focus will be on addressingthe practical issues that make it difficult to use our cur-rent research prototype over a period of months.
Wehave identified many such issues, both from our pre-vious evaluations (Section 3.1) and also from a ques-tionnaire that was distributed to school staff during anin-service day.Location tracking ?
There are problems with bothof the techniques we have tried to date (automaticallyreading RFID tags on doors, and asking staff to swipelocation information).
In this project we intend to trytracking the location of a child using Wi-Fi locationtracking, which seems to be rapidly gaining popularityin the commercial world (Liu, Sen et al 2008).Data entry, 2D bar codes ?
We need to allow staffto easily enter and update information about the children(for example, their timetables) and sensor tags (e.g., if anew tag is given to a visitor).
For the latter, we want toinvestigate 2D bar codes, which could allow encodingof alphanumeric input data without reference to a cen-tral database.Portability, battery life ?
The current system runs ona tablet PC (8?-12?
touch screen, generic or VOCAhardware).
During the evaluation, late powering up, run-down batteries or simply forgetting a component causedsignificant data loss and usability issues.
A future proto-type should favor an ?always-on?
system, such as a mo-bile phone, allowing for easy portability and extendedbattery life.Story generation ?
The prototype system was onlyable to create a story towards the end of a day and gave5only access to stories generated on that day.
However,often the user desired to tell stories that had occurred onprevious days, or to, say, tell a story at lunch that oc-curred in the morning.
When data was insufficient forthe system to create a story, the only output was an errormessage ?Can?t generate story right now.?
This fru-strated users, so future systems should be able to delivera story with incomplete data.Voice messages ?
as mentioned above, we want tohandle these in a more sophisticated way.
From a morepractical perspective, we also want to make it easier forstaff to listen to and change previously recorded mes-sages.
We also want to allow parents to record messag-es about events at home.4 Long-term vision and issues4.1 Supporting children with different levelsand types of impairmentA key issue in AAC is of course the diversity of AACusers.
Children with CCN differ enormously in terms ofcognitive ability, motor ability, and social ability.
Thiswas clear even in our initial evaluation where weworked just with two children, and discovered that ourinterface worked well for Julie but not Jessica.Julie has little functional speech and severe physicalimpairments, and accesses her VOCA using a headswitch through the slow process of scanning the inter-face.
Her VOCA interface consists of a grid of 15 to 30buttons per page, with more than 20 pages of vocabu-lary.
However, her cognitive skills were sufficient forher to master the interface on the second day.
She usedthe system quite successfully, as shown in the examplein Section 3.2.Jessica also has severe physical impairments butdoes not use technology to support her communication(she has functional speech).
She has cognitive impair-ments, which (amongst other things) affect her ability toremember and place events correctly in time.
She hadmore difficulty mastering the interface than Julie.
Wesimplified the interface for her (no editing, minimalcontrol of narration), and then she displayed pragmaticsknown from typical language development in children,by telling her story with no room for interaction of hercommunication partner.We also need to keep in mind that abilities are notstatic, but are likely to progress with age (see also Sec-tion 4.2) and (hopefully) with the assistance of commu-nication aids.
For example, the WriteTalk projectshowed how pupils were both able to initiate and con-trol communication more effectively with Talk:About?and how their formal writing skills improved over time(Waller et al, 1999).In summary, some children may need a very simpleinterface because of cognitive impairments, but thisshould grow with them.
For example, the best narrationtool for Jessica at her current stage of development isprobably a single button that advances sequentiallythrough the computer-generated story.
The challenge isto provide an interface that Jessica can initially use viarepeatedly pressing an ?Advance?
button, but whichgives her the possibility of exerting more control as herskills and abilities develop.Other children (such as Julie) may have motor diffi-culties that restrict the way in which they can interactwith computer systems, and thus may require simplecontrols although they have reasonable cognitive skills.Restricted motor skills make certain tasks, such as en-tering an arbitrary word, quite difficult and time con-suming; hence the interface must avoid such tasks, andinstead endeavor to give the child as much control aspossible with a minimum amount of data entry.
Oncethese users master a basic story telling structure, it mayhelp them develop their conversation skills if they use awide variety of conversation patterns.
For this purpose,it may be worthwhile for the system to randomly varythe structure and language used in the narratives.Still other children, for example on the autistic spec-trum, may have good cognitive and motor abilities, butnot have the experience of expressive communicationnecessary to develop interactive skills.
These childrenare more likely to benefit from a system that supportsthe pragmatics of language in general and personal narr-ative in particular.
For example, children on the highfunctioning end of autism may be comfortable with ra-ther advanced software, which can help them adapt theirstorytelling according to the intended listener.
Indeed,giving these children more complex controls, if donecorrectly, can make the software fun and challenging ina positive way.In the long term, as we broaden the range of childrenwe work with, there may be overlaps between our workand research on tools to help typically developing child-ren create stories, such as Robertson and Good (2005),and also between our work and research on tools to helpadults with CCN tell personal narratives, such as Demp-ster (2008).
Ideally it would be very nice to combinethese efforts and create a story telling tool that could beused across the age and impairment spectrum.4.2 Narrative across the lifespanWe would like our tool to be able to support childrenover time, as their abilities grow and as their expe-riences accumulate.
From the perspective of changingabilities, the challenge is to offer children an interfacewhich is not only appropriate for their current stage ofdevelopment (Sect 4.1), but also allows and indeed en-6courages them to exert more control over story content,language, and narration as their abilities grow.We would also like our tool to become a repositoryof a child?s personal stories.
The ability to relate rele-vant stories can influence the quality of life, as well associal development and successful transitions.
The lifestories of people who use AAC are often held by parentsand siblings (e.g., stories relating to health care(Hemsley, Balandin et al 2007)), and there is the inevit-able concern that these stories and others are lost asparents age and siblings move away.Technology has the potential both to support theacquisition of conversational skills for people who useAAC and to provide a repository for life stories.
In thecontext of our work, it is essential that we provide waysof enabling children to develop their narrative skills sothat they are more able to manage their own story repo-sitory.
In terms of development, young children willnarrate recent stories regardless of conversational con-text.
By enabling the child to develop story structure byscaffolding interaction and enabling children to easilyannotate stories, the child will begin to anticipate andcontrol conversation.Conversational narratives have traditionally not beensupported by AAC tools partly due to the fact that theyare so nebulous; they emerge during interactive conver-sation (to date, events have to be manually input into asystem and it is difficult to predict what events will be-come a story); ?new?
stories are repeated often (to date itis difficult to save conversation online); as stories agethey are repeated in context (retrieval is often contextuale.g.
topic based) and they grow longer having more em-bellishments added to them.
The technology we are de-veloping provides an opportunity for children to accessinformation about personal events over time, which theycan communicate and narrate during a conversation.They can also evaluate (annotate) their stories, therebyembellishing and lengthening the stories.
However thiswill only be possible if the children can easily accesspreviously experienced, generated and saved stories.We can provide fast access to recent stories whileanticipating the use of older stories such as for examplethose which closely match the current conversation top-ic.
In a research prototype called PROSE (Waller andNewell 1997), stories had to be physically tagged; thereis now the potential to automate topic matching by re-cognizing topic words spoken by a listener and parsingstored information for appropriate stories.
Over a life-time, some stories may fall into disuse, while others willbe weighted more strongly depending on frequency ofuse and relevance.4.3 True dialogue in narrationThe ultimate goal of our research is to enable children totell stories in the context of a social dialogue; for exam-ple, we want children to be able to chat to their parentsand other interested parties about what they did duringthe day.Our current system incorporates a simple model of aconversation, where children are restricted at any pointto choosing from a small number of options.
The childchooses an event to talk about, and then goes throughthe sequence of messages associated with that event.The child has the freedom to switch to a different event,hence controlling the conversation, and to add annota-tions/evaluations (for example ?it was fun!?
).This is adequate in many cases, but in the long termwe would like to support more complex conversations;for example interrupting a discussion about today?sevents to talk about what happened yesterday, or to dis-cuss a particular teacher instead of an event.
We wouldalso like children to easily be able to add conversationalphrases, such as ?Guess what happened today atschool?.Because our children have motor and cognitive im-pairments, we cannot present them with a large numberof options for conversational moves.
Ideally, the systemwould detect what the conversational partner wishes totalk about, and from this present the child with a smallnumber of appropriate choices.
For example, if the con-versational partner asks the child what happened overthe past week, our system would detect this and thengive the child the option of talking about any individualweekday or the week in general.One way of detecting what the conversational part-ner intends is to use speech recognition and NaturalLanguage Processing (NLP) technology to analyze whathe or she says.
Speech and NLP technology tend towork best when it is possible to train the system to theuser?s voice, and also (in essence) train the user to un-derstand what the speech/NLP system can and cannotdo.
This should be possible in our context, at least forpeople (such as parents) with whom the child regularlyinteracts.Another possibility is to create a graphical user in-terface for the conversational partner, perhaps on thesame device that the child uses, which the partner coulduse to indicate what he/she wants to talk about.
This isprobably technically easier, but does move away fromthe goal of having as natural a dialogue as possible.4.4 Pragmatics of interacting with othersCurrently, ?How was School today???
supports story-telling between language-impaired children and adultswho are the children?s parents, carers, teachers, andtherapists.
But of course for normally developing child-ren, many of their most important social interactions arewith other children.An interesting example here is the STANDUP sys-tem, which was developed to help children who use7AAC create and tell novel punning riddles.
The studyresults suggested that children saved the jokes so thatthey could retell them to friends and family (Waller,Black et al 2009).
Whilst the evidence is anecdotal,there did also appear to be a marked increase in joketelling by participants, both amongst their peers andwith adults in the home environment.
Hence STANDUPsucceeded in supporting interaction with other childrenas well as with adults.One of the key challenges in interacting with otherchildren, and indeed with adults who are not formallyinvolved in the care or teaching of the child, is to adaptthe story to the interests of the recipients.
In otherwords, a child?s parents and teachers will not insist onstories that are interesting to them, but other conversa-tional partners will.
These conversational partners mayalso need additional information.
For example ?Janecame to take me to the OT room?
makes more sense ifthe recipient knows that Jane is the occupational therap-ist; parents and teachers already know this, but otherpeople may need to be told this.
Also if the conversa-tional partner was present at an event, this should beacknowledged and indeed used in the story.
For exam-ple, "Did you really enjoy maths?
I thought it was bor-ing!
?In short, telling stories to peers and adults who donot know the child well requires adapting the story tothe interests, knowledge, and involvement of the part-ner; this is part of learning pragmatics.
This is not some-thing we are looking at currently, but it is somethingthat we hope to look at in the future.4.5 Security and privacy issuesWe need to ensure that data about the children is privateand secure.
Taken to its logical conclusion, our projectwould result in an intimate record of the child's life atschool, home and beyond.
It is important that both theraw data and the generated content are under the controlof the child and his/her guardians, with the child exer-cising as much control as possible.
This is especiallyimportant since children with learning difficulties arevery vulnerable; there is potential for great harm if dataabout a child?s activities got into the hands of a mali-cious outsider.In a study on the software tool TalksBac, whichsupports personal narrative (Waller, Dennis et al 1997),privacy issues were coded along with stories.
This al-lowed the NLG process to decide the appropriateness oftelling a story to a specific communication partner.Children in general do not care who they tell their sto-ries to.
Only when older children learn to distinguishwhich story is appropriate for a conversation partner.This process could be embedded into the predictionalgorithm that presents stories for narration.
Currentlyprediction on AAC devices only support character, wordor phrase selection.Another concern is information that is embarrassingor otherwise puts the child in a negative light; for exam-ple, imagine a staff member entered the voice message"I refused to eat my lunch today".
We believe that thechild should be free to delete such messages; she shouldnever be forced to include material in a story that shedoes not want to include.A related issue is whether we should allow storiesgenerated for one child to use information acquiredabout another child.
In principle this is very valuable,for example it allows messages such as ?Jane didn?t eather lunch today?.
But is this acceptable from the pers-pective of ensuring the privacy of data about Jane?s ac-tivities?
On the other hand, this is exactly the sort ofthing that a normally developed child would say about aclassmate.5 ConclusionIn addition to having difficulty in communicating de-sires and needs, language-impaired children also find ithard to participate in social linguistic interaction thatwould help create and build up friendships and otherinterpersonal relationships.
We believe that we can helpthese children participate in such interactions by givingthem a tool that helps them tell a story about their day atschool, by using an NLG system that has access to sen-sor and other data about the child?s activities.
We arestill at an early stage in this work, but our initial proto-type system has shown great potential to improve thequality of life of children with limited speech.
Our cur-rent work plans to explore this potential further whileevaluating the efficacy of the system for four childrenwith varying ability levels.AcknowledgementsWe would like to express our thanks to the children,their parents and staff and the special school where thisproject had its base.
Without their valuable contribu-tions and feedback this research would not have beenpossible.
We would also like to thank DynaVox Sys-tems Ltd for supplying the communication devices torun our system on.This research was supported by the UK Engineeringand Physical Sciences Research Council under grantsEP/F067151/1, EP/F066880/1, EP/E011764/1,EP/H022376/1, and EP/H022570/1.8ReferencesAgrawal, R. and Ramakrishnan, S. (2000) Privacy-preserving data mining.
ACM InternationalConference on Management of Data, pp.
439--450,Bart, H., V. Riny, et al (2008).
LinguaBytes.Proceedings of the 7th international conference onInteraction design and children.
Chicago, Illinois,ACM: 17-20.Beukelman, D. R. and P. Mirenda (2005).
Augmentativeand Alternative Communication: Management ofSevere Communication Disorders in Children andAdults.
Baltimore, Paul H. Brookes Publishing Co.Bruner, J.
(1975).
"From communication to language: Apsychological perspective."
Cognition 3: 255-289.Cheepen, C. (1988).
The predictability of  informalconversation.
Oxford, Printer Publishers Ltd.Clarke, H. H. and E. V. Clarke (1977).
Psychology andLanguage.
New York, Harcourt Brace Jovanovich.Dempster, M. (2008).
Using natural languagegeneration to encourage effective communication innonspeaking people.
Proceedings of YoungResearchers Consortium, ICCHP'08.DeRuyter and Fried-Oken.
(2010).
"Context-sensitivemessaging with RFID technology."
Retrieved 2010,April 10, from http://aac-rerc.psu.edu/index.php/projecttypes/listGatt, A., F. Portet, et al (2009).
"From Data to Text inthe Neonatal Intensive Care Unit: Using NLGTechnology for Decision Support and InformationManagement."
AI Communications 22: 153-186.Goldberg, E., N. Driedger, et al (1994).
"Using natural-language processing to produce weather forecasts.
"IEEE Expert 9(2): 45-53.Harris, M. (2008).
Building a Large-Scale Commer-cialNLG System for an EMR.
Proc of INLG-2008.Hemsley, B., S. Balandin, et al (2007).
"Familycaregivers discuss roles and  needs in supportingadults with cerebral palsy and complexcommunication needs in the hospital setting.
"Journal of Developmental and Physical Disabilities19(2): 115-124.Higginbotham, D. J., H. Shane, et al (2007).
"Access toAAC: Present, past, and future."
Augmentative &Alternative Communication 23(3): 243-257.Labov, W. (1972).
Language in the inner city: Studies inthe Black English Vernacular.
Philadelphia,University of Pennsylvania Press.Light, J., C. Binger, et al (1994).
"Story Readinginteractions between preschoolers who use AAC andtheir mothers."
Augmentative and AlternativeCommunication 10: 255-268.Light, J. and K. Drager (2007).
"AAC Technologies forYoung Children with Complex CommunicationNeeds: State of the Science and Future ResearchDirections."
Augmentative and AlternativeCommunication 23(3): pp.
204 ?
216.Liu, X., A. Sen, et al (2008).
A Software Client for Wi-Fi Based Real-Time Location Tracking of Patients.Medical Imaging and Informatics.Berlin/Heidelberg, Springer.
4987/2008: 141-150.P?r?z, R. P. y. and M. Sharples (2004).
"ThreeComputer-Based Models of StoryTelling: BRUTUS,MINSTREL, and MEXICA."
Knowledge-BasedSystems 17: 15-29.Peterson, C. and A. McCabe (1983).
Developmentalpsycholinguistics: Three ways of looking at a child?snarrative.
New York, Plenum.Reiter, E. (2007).
An Architecture for Data-to-TextSystems.
ENLG-2007.Reiter, E. and R. Dale (2000).
Building Natural-Language Generation Systems., CambridgeUniversity Press.Reiter, E., S. Sripada, et al (2005).
"Choosing Words inComputer-Generated Weather Forecasts."
ArtificialIntelligence 167: 137-169.Reiter, E., R. Turner, et al (2009).
Using NLG to HelpLanguage-Impaired Users Tell Stories andParticipate in Social Dialogues.
ENLG2009.
Athens,Greece, Association for Computational Linguistics.Robertson, J. and J.
Good (2005).
"Story creation invirtual game worlds."
Communications of the ACM48: 61-65.Schlosser, R. W. (2003).
The Efficacy of Augmentativeand Alternative Communication.
San Diego,Elsevier Science.Soto, G., E. Hartmann, et al (2006).
"Exploring theElements of Narrative that Emerge in theInteractions between an 8-Year-Old Child who usesan AAC Device and her Teacher."
Augmentativeand Alternative Communication 22(4): pp.
231 -241.Sripada, S., E. Reiter, et al (2005).
Evaluating an NLGSystem using Post-Edit Data: Lessons Learned.Proceedings of ENLG-2005, 10th EuropeanWorkshop on Natural Language Generation,Aberdeen, Scotland.Waller, A.
(2006).
"Communication Access toConversational Narrative."
Topics in LanguageDisorders 26(3): 221-239.Waller, A., R. Black, et al (2009).
"Evaluating theSTANDUP Pun Generating Software with Childrenwith Cerebral Palsy."
ACM Trans.
Access.
Comput.1(3): 27.Waller, A., F. Dennis, et al (1997).
"Evaluating the useof TalksBac, a predictive communication device fornon-fluent aphasic adults."
International Journal ofLanguage and Communication Disorders 33: 45-70.Waller, A. and A. F. Newell (1997).
"Towards anarrative based communication system."
EuropeanJournal of Disorders of Communication 32: 289-306.9
