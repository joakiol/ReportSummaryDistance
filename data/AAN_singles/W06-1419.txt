Proceedings of the Fourth International Natural Language Generation Conference, pages 127?129,Sydney, July 2006. c?2006 Association for Computational LinguisticsEvaluations of NLG Systems: common corpus and tasks or commondimensions and metrics?C?cile Paris, Nathalie Colineau and Ross WilkinsonCSIRO ICT CentreLocked Bag 17, North RydeNSW 1670, Australia{Cecile.Paris, Nathalie.Colineau, Ross.Wilkinson}@csiro.auAbstractIn this position paper, we argue that acommon task and corpus are not the onlyways to evaluate Natural Language Gen-eration (NLG) systems.
It might be, infact, too narrow a view on evaluation andthus not be the best way to evaluate thesesystems.
The aim of a common task andcorpus is to allow for a comparativeevaluation of systems, looking at the sys-tems?
performances.
It is thus a ?system-oriented?
view of evaluation.
We arguehere that, if we are to take a system ori-ented view of evaluation, the communitymight be better served by enlarging theview of evaluation, defining commondimensions and metrics to evaluate sys-tems and approaches.
We also argue thatend-user (or usability) evaluations formanother important aspect of a system?sevaluation and should not be forgotten.1 IntroductionFor this special session, a specific question wasasked: what would a shared task and shared cor-pus be that would enable us to perform compara-tive evaluations of alternative techniques in natu-ral language generation (NLG)?
In this positionpaper, we question the appropriateness of thisspecific question and suggest that the communitymight be better served by (1) looking at a differ-ent question: what are the dimensions and met-rics that would allow us to compare varioustechniques and systems and (2) not forgetting butencouraging usability evaluations of specific ap-plications.The purpose of defining a shared task and ashared corpus is to compare the performance ofvarious systems.
It is thus a system-oriented viewof evaluation, as opposed to an end-user oriented(or usability) view of evaluation.
It is, however,potentially a narrow view of a system-orientedevaluation, as it looks at the performance of anNLG system within a very specific context ?
thusessentially looking at the performance of a spe-cific application.
We argue here that (1), even ifwe take a system-oriented view of evaluation, theevaluation of NLG systems should not be limitedto their performance in a specific context butshould take other system?s characteristics intoaccount, and that (2) end-user evaluations arecrucial.2 Enlarging the view of system-orientedevaluationsThe comparison of NLG systems should notbe limited to a particular task in a specific con-text.
Most systems are designed for specific ap-plications in specific domains and tend to betuned for these applications.
Evaluating them ina context of a specific common evaluation taskmight de-contextualise them and might encour-age fine-tuning for this task, which might not beuseful in general.
Furthermore, the evaluation ofa system should not be limited to its performancein a specific context but should address charac-teristics such as:?
Cost of building (time and effort);?
Ease of extension, maintainability and cus-tomisability to handle new requirements(time, effort and expertise required);?
Cost of porting to a new domain or applica-tion (time, effort and expertise required);?
Cost of data capture if required (how expen-sive, expertise required);?
Coverage issues (users, tasks, dimensions ofcontext; and?
Ease of integration with other software.These dimensions are important if we want thetechnology to be adopted and if we want poten-127tial users of the technology to be able to make aninformed choice as to what approach to choosewhen.Most NLG systems are built around a specificapplication.
Using them in the context of a dif-ferent application or domain might be difficult.While one can argue that basic techniques do notdiffer from one application to another, the cost ofthe modifications required and the expertise andskills needed may not be worth the trouble.
Itmay be simply cheaper and more convenient torebuild everything.
However, firstly, this mightnot be an option, and, secondly, it may increasethe cost of using an NLG approach to such anextent as to make it unaffordable.
In addition,applications evolve over time and often require aquick deployment.
It is thus increasingly desir-able to be able to change (update) an application,enabling it to respond appropriately to the newsituations which it must now handle: this mayrequire the ability to handle new situations (e.g.,generate new texts) or the ability to respond dif-ferently than originally envisaged to knownsituations.
This is important for at least two rea-sons:(1) We are designers not domain experts.Although we usually carry out a do-main/corpus/task analysis beforehand to acquirethe domain knowledge and understand the users?needs in terms of the text to be generated, it isalmost impossible to become a domain expertand know what is the most appropriate in eachsituation.
Thus, the design of a specific applica-tion should allow the experts to take on controland ensure the application is configured appro-priately.
This imposes the additional constraintthat an application should be maintainable di-rectly by a requirement specialist, an author, ex-pert or potentially the reader/listener;(2) Situations are dynamic ?
what is satis-factory today may be unsatisfactory tomorrow.We must be prepared to take on board new re-quirements as they come in.These requirements, of course, come at a cost.With this in mind, then, we believe that there isanother side to system-oriented evaluation whichwe, as designers of NLG systems, need to con-sider: the ease or cost of developing flexible ap-plications that can be easily configured andmaintained to meet changing requirements.
As astart towards this goal, we attempted to lookmore precisely at one of the characteristics men-tioned above, the cost of maintaining and extend-ing an application, attempting to understand whatwe should take into account to evaluate a systemon that dimension.
We believe asking the follow-ing questions might be useful.
When there arenew requirements:(1) What changes are needed and do themodifications require the development of newresources, the implementation of additional func-tionality to the underlying architecture, or both?
(2) Who can do it and what is the expertiserequired?
?
NLG systems are now quite complexand require a lot of expertise that may be sharedamong several individuals (e.g., software engi-neering, computational linguistics, domain ex-pertise, etc.).
(3) How hard it is?
?
How much effort andtime would be required to modify/update the sys-tem to the new requirements?In asking these questions, we believe it is alsouseful to decouple a specific system and its un-derlying architecture, and ask the appropriatequestions to both.3 Usability Evaluations of NLG SystemsWhen talking about evaluation of NLG systems,we should also remember that usabilityevaluations are crucial, as they can confirm theusefulness of a system for its purpose and look atthe impact of the generated text on its intendedaudience.
There has been an increasing numberof such evaluations ?
e.g., (Reiter et al, 2001,Paris et al, 2001, Colineau et al, 2002,Kushniruk et al, 2002, Elhadad et al, 2005) ?and we should continue to encourage them aswell as develop and share methodologies (andpitfalls) for performing these evaluations.
It isinteresting, in fact, to note that communities thathave emphasized common task and corpusevaluations, such as the IR community, are nowturning their attention to stakeholder-basedevaluations such as task-based evaluations.
Inlooking at ways to evaluate NLG systems, wemight again enlarge our view beyondreader/listener-oriented usability evaluations, asreaders are not the only persons potentiallyaffected by our technology.
When doing ourevaluations, then, we must also consider otherparties.
Considering NLG systems as informationsystems, we might consider the followingstakeholders beyond the reader:?
The creators of the information: for someapplications, this may refer to the personcreating the resources or the information re-quired for the NLG system.
This might be,for example, the people writing the frag-ments of text that will be later assembled128automatically.
Or it might include the personwho will author the discourse rules or thetemplates required.
With respect to thesepeople, we might ask questions such as:?Does employing this NLG system/approachsave them time?
?, ?Is it easy for them to up-date the information??1?
The ?owners?
of the information.
We referhere to the organisation choosing to employan NLG system.
Possible questions heremight be: ?Does the automatically generatedtext achieve its purpose with respect to theorganisation?
?, ?Can the organisation conveysimilar messages with the automated system?
(e.g., branding issues).4 DiscussionIn this short position paper, we have argued thatwe need to enlarge our view of evaluation to en-compass both usability evaluation (and includeusers beyond readers/listeners) and system-oriented evaluations.
While we recognise that itis crucial to have ways to compare systems andapproaches (the main advantage of having acommon corpus and task), we suggest that weshould look for ways to enable these compari-sons without narrowing our view on evaluationand de-contextualising the systems under consid-eration.
We have presented some possible di-mensions on which approaches and systemscould be evaluated.
While we understand how toperform usability evaluations, we believe that animportant question is whether it is possible toagree on dimensions for system-oriented evalua-tions and on ?metrics?
for these dimensions, toallow us to evaluate the different applicationsand approaches, and allow potential users of thetechnology to choose the appropriate one fortheir needs.
In our own work, we exploit an NLGarchitecture to develop adaptive hypermedia ap-plications (Paris et al, 2004), and some of ourgoals (Colineau et al, 2006) are to:?
Articulate a comprehensive framework forthe evaluation of approaches to buildingtailored information delivery systems andspecific applications built using these ap-proaches.?
Identify how an application or an ap-proach measures along some dimensions1 We realise that, for some NLG applications, theremight be no authors if all the data exploited by thesystem comes from underlying existing sources, e.g.,weather or stock data or existing textual resources.
(in particular for system-oriented evalua-tion).We believe these are equally important for theevaluation of NLG systems.AcknowledgementsWe would like to thank the reviewers of thepaper for their useful comments.ReferencesColineau, N., Paris, C. & Vander Linden, K. 2002.
AnEvaluation of Procedural Instructional Text.
In theProceedings of the International Natural LanguageGeneration Conference (INLG) 2002, NY.Colineau, N., Paris, C. & Wilkinson, R. 2006.
To-wards Measuring the Cost of Changing AdaptiveHypermedia Systems.
In Proceedings of the In-ternational Conference on Adaptive Hypermediaand Adaptive Web-based Systems (AH2006).
259-263, Dublin, Ireland.
LNCS 4018.Elhadad, N. McKeown, K. Kaufman, D. & Jordan, D.2005.
Facilitating physicians' access to informationvia tailored text summarization.
In AMIA AnnualSymposium, 2005, Washington DC.Kushniruk, A., Kan, MY, McKeown, K., Klavans, J.,Jordan, D., LaFlamme, M. & Patel, V. 2002.
Us-ability evaluation of an experimental text summari-zation system and three search engines: Implica-tions for the reengineering of health care interfaces.In Proceedings of the American Medical Informat-ics Association Annual Symposium (AMIA 2002).Paris, C., Wan, S., Wilkinson, R. & Wu, M. 2001.Generating Personalised Travel Guides?
And whowants them?
In Proceedings of the 2001 Interna-tional Conference on User Modelling (UM?01),Sondhofen, Germany.Paris, C., Wu, M., Vander Linden, K., Post, M. & Lu,S.
2004.
Myriad: An Architecture for Contextual-ised Information Retrieval and Delivery.
In Pro-ceedings of the International Conference on Adap-tive Hypermedia and Adaptive Web-based Systems(AH2004).
205-214, The Netherlands.Reiter, E., Robertson, R., Lennox A. S. & Osman, L.(2001).
Using a randomised controlled clinical trialto evaluate an NLG system.
In Proceedings ofACL'01, Toulouse, France, 434-441.129
