Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 53?61,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsVowel and Diacritic Restoration for Social Media TextsK?ubra Adal?Dep.
of Computer Eng.Istanbul Technical UniversityIstanbul, Turkeykubraadali@itu.edu.trG?uls?en Eryi?gitDep.
of Computer Eng.Istanbul Technical UniversityIstanbul, Turkeygulsen.cebiroglu@itu.edu.trAbstractIn this paper, we focus on two importantproblems of social media text normaliza-tion, namely: vowel and diacritic restora-tion.
For these two problems, we pro-pose a hybrid model consisting both a dis-criminative sequence classifier and a lan-guage validator in order to select one ofthe morphologically valid outputs of thefirst stage.
The proposed model is lan-guage independent and has no need formanual annotation of the training data.
Wemeasured the performance both on syn-thetic data specifically produced for thesetwo problems and on real social mediadata.
Our model (with 97.06% on syn-thetic data) improves the state of the artresults for diacritization of Turkish by 3.65percentage points on ambiguous cases andfor the vowel restoration by 45.77 percent-age points over a rule based baseline with62.66% accuracy.
The results on real dataare 95.43% and 69.56% accordingly.1 IntroductionIn recent years, with the high usage of computersand social networks like Facebook and Twitter, theanalysis of the social media language has becomea very popular and crucial form of business intelli-gence.
But unfortunately, this language is very dif-ferent from the well edited written texts and muchmore similar to the spoken language, so that, theavailable NLP tools do not perform well on thisnew platform.As we all know, Twitter announced (at April1st, 2013)1that it is shifting to a two-tiered ser-vice where the basic free service ?Twttr?
will onlyallow to use consonants in the tweets.
Although,1https://blog.twitter.com/2013/annncng-twttrthis is a very funny joke, people nowadays are al-ready very used to use this style of writing withoutvowels in order to fit their messages into 140 char-acters Twitter or 160 characters SMS messages.As a result, the vowelization problem (Twttr ?Twitter) is no more limited with some specific lan-guage families (e.g.semitic languages) (Gal, 2002;Zitouni et al., 2006) but it became a problem of so-cial media text normalization in general.Diacritics are some marks (e.g.
accents, dots,curves) added to the characters and have a wideusage in many languages.
The absence of thesemarks in Web2.0 language is very common andposses a big problem in the automatic process-ing of this data by NLP tools.
Although, in theliterature, the term ?diacritization?
is used bothfor vowel and diacritic restoration for semitic lan-guages, in this paper, we use this term only forthe task of converting an ASCII text to its properform (with accents and special characters).
ATurkish example is the word ?dondu?
(it is frozen)which may be the ascii form of both ?dondu?
(it isfrozen) or ?d?ond?u?
(it returned) where the ambigu-ity should be resolved according to the context.
Insome studies, this task is also referred as ?unicodi-fication?
(Scannell, 2011) or ?deasciification?
(T?ur,2000).In this paper, we focus on these two importantproblems of social text normalization, namely: di-acritization and vowelization.
These two problemscompose almost the quarter (26.5%) of the nor-malization errors within a 25K Turkish TweeterData Set.
We propose a two stage hybrid model:firstly a discriminative model as a sequence classi-fication task by using CRFs (Conditional RandomFields) and secondly a language validator over thefirst stage?s results.
Although in this paper, we pre-sented our results on Turkish (which is a highly ag-glutinative language with very long words full ofun-ascii characters), the proposed model is totallylanguage independent and has no need for manual53annotation of the training data.
For morpholog-ically simpler languages, it would be enough touse a lexicon lookup for the language validationstage (whereas we used a morphological analyzerfor the case of Turkish).
With our proposed model,we obtained the highest results in the literature forTurkish diacritization and vowelization.The remaining of the paper is structured as fol-lows: Section 2 discusses the related work, Sec-tion 3 tries to show the complexity of diacritiza-tion and the vowelization tasks by giving exam-ples from an agglutinative language; Turkish.
Sec-tion 4 introduces our proposed model and Section5 presents our experiments and results.
The con-clusion is given in Section 6.2 Related WorkThe vowelization problem is mostly studied forsemitic languages and many different methods areapplied to this problem.
The problem is generallyreferred as diacritization for these languages, sincediacritics are placed on consonants for the purposeof vowel restoration.
For example, the short vow-els in Arabic are only pronounced by the use ofdiacritics put on other consonants.
Some of thesestudies are as follows: Gal (2002) reports the re-sults on Hebrew by using HMMs and Zitouni etal.
(2006) on Arabic by using maximum entropybased models.
Al-Shareef and Hain (Al-Shareefand Hain, 2012) deals with the vowelization ofcolloquial Arabic for automatic speech recogni-tion task by using CRFs on speaker and contextualinformation.
Haertel et al.
(2010) uses conditionalmarkov models for the vowel restoration problemof Syriac.
Nelken and Shieber (2005) uses a finitestate transducer approach for Arabic as well.
Tothe best of our knowledge, the vowelization workon Turkish is the first study on a language whichdo not possess the vowelization problem by its na-ture.
We believe that on that sense, our hybridmodel will be a good reference for future studiesin social media text normalization where the prob-lem is disregarded in recent studies.The diacritization task on the other hand isnot addressed as frequently as the vowelizationproblem2.
Some studies are as follows: Scan-nell (2011) uses a Naive Bayes classifier for bothword-level and character-level modeling.
Eachambiguous character in the input is regarded as2Here, we exclude all the works done for semitic lan-guages.
The reason is explained on the former paragraph.an independent classification problem.
They areusing lexicon lookup which is not feasible for ev-ery possible word surface form in agglutinative orhighly inflected languages.
They refer to a lan-guage model in ambiguous cases.
They testedtheir system for 115 languages as well as for Turk-ish (92.8% on a much easier data set than ours (re-fer to Section 5.1) .
Simard and Deslauriers (2001)tries to recover the missing accents in French.They are using a generative statistical model forthis purpose.
De Pauw et al.
(2007) also test theirMBL (memory based learning) model on differ-ent languages.
Although they do not test for Turk-ish, the most attractive part of theirs results is thatthe performances for highly inflectional languagesdiffer sharply from the others towards the negativeside.
Nguyen and Ock (2010) deals with the dia-critization of Vietnamese by using Adaboost andC4.5.The work done so far for the diacritizationof Turkish are from T?ur (2000) (character-basedHMM model), Zemberek (2007), Y?uret and de laMaza (Y?uret and de la Maza, 2006) (GPA: a kindof decision list algorithms).
We give the compar-ison of the two later systems on our data set andpropose a discriminative equivalent of the HMMapproach used in T?ur (2000) (see Section 5 for fur-ther discussions).
For the vowelization, the onlystudy that we could find is from T?ur (2000) whichuses again the same character-level HMM modelinto this problem (with an equivalent discrimina-tive model given at Table 8 ?3ch model).3 The complexityThis section tries to draw light upon the complex-ity of diacritization and the vowelization tasks bygiving examples from an agglutinative language;Turkish.3.1 TurkishTurkish is an agglutinative language which meansthat words have theoretically an infinite numberof possible surface forms due to the iterative con-catenation of inflectional and derivational suffixes.As for other similar languages, this property of thelanguage makes impractical for Turkish words tobe validated by using a lexicon.
And also, the in-creasing length3of the words creates a big searchspace especially for the vowelization task.3The average word length is calculated as 6.1 for Turkishnouns and 7.6 for verbs in a 5 million word corpus(Ak?n andAk?n, 2007).54Turkish alphabet has 7 non-ascii characters thatdon?t exist in the Latin alphabet (c?, ?g, ?,?I, ?o, s?,?u) and the ascii counterparts of these letters (c, g,i, I, o, s, u) are also valid letters in the alphabetwhich causes an important disambiguation prob-lem at both word and sentence level.
The alphabetcontains 8 vowels (a(A), e(E), ?
(I), i(?I), o(O), ?o(?O),u(U), ?u(?U)) in total.3.2 DiacritizationThe following real example sentence taken fromsocial media ?Ruyamda evde oldugunu gordum.
?,written by using only the ascii alphabet, has twopossible valid diacritized versions:1.
?R?uyamda evde oldu?gunu g?ord?um.?
(I had a dream that you were at home.)2.
?R?uyamda evde ?old?u?g?un?u g?ord?um.?
(I had a dream that you died at home.
)As can be observed from this sentence some ofthe asciified words (e.g.
?oldugunu?)
has morethan one possible valid counterparts which causesthe meaning change of the whole sentence.The problem is the decision of the appropri-ate forms for the critical letters (C, G, I, O, S,U)4.
Although the problem seems like a multi-class classification problem, it is in essence abinary-classification task for each critical letterand can be viewed as a binary sequence classifi-cation task for the whole word so that the orig-inal word will be chosen from (2n) possibilitieswhere n is the number of occurrence of criticalletters (C, G, I, O, S, U) in the ascii version.For example the word ?OldUGUnU?
has (25=32)possible transformations whereas only 2 of them(?oldu?gunu?
and ??old?u?g?un?u?)
are valid Turkishwords.
Figure 1 gives a second example and showsall the possible (22=4) diacritized versions of theword ?aCI?
where again only two of them arevalid words (emphasized with a bold backgroundcolour): ?ac???
(angle) and ?ac??
(pain).3.3 VowelizationVowelization on the other hand causes much morecomplexity when compared to diacritization.
Eachposition5between consequent consonants, at the4From this point on, we will show the ascii versions ofthese letters as capital letters meaning that they may appearin the diacritized version of the word either in their ascii formor in their diacritized form.
Ex: the capital C will becomeeither c or c?
after the processing.5For the sake of simplicity, we just assumed that only zeroor one vowel may appear between two consonants whereasthere exist some words with consecutive vowels (such asFigure 1: Possible Diacritized Versions of ?aCI?beginning or ending of the word may take onevowel or not resulting a selection from 9 classlabels (the 8 vowel letters + the null charac-ter).
For example, the vowelization of the word?slm?(?hi?
written without vowels, with n=4 po-sitions s l m ) will produce 94= 6561 possibili-ties where 39 of them are valid Turkish words (e.g.?salam?
(salami), ?sulama?
(watering), ?sal?m?
(myraft), ?selam?
(hi), ?s?lam?
(my furlough) etc...).Figure 2: Proposed Model4 Proposed ModelMost of the previous work in the literature (Sec-tion 2) uses either some (generative of discrimina-tive) machine learning models or some nlp tools(e.g.
morphological analyzers, pos taggers, lin-guistic rules) in order to solve the vowelization?saat?
(clock)) although very rarely55problem.
As it is shown in the previous section,for languages with higher number of vowels andword length due to their rich inflectional morphol-ogy, the search space gets very high very rapidly.Since the problem is mostly similar to generation,in order to increase the likelihood of the generatedoutput word, most of the approaches include char-acter level probabilities or relationships.
In thiscase, it is unfair to expect from a machine learn-ing system to generate morphologically valid out-puts (especially for highly inflectional languages)while trying to maximize the overall character se-quence probability.We propose a two stage model (Figure 2) whichhas basically two components.1.
a discriminative sequence classifier2.
a language validator4.1 Discriminative Sequence ClassifierIn the first stage, we use CRFs6(Lafferty et al.,2001) in order to produce the most probable out-put words.
This stage treats the diacritization andvowelization as character level sequence labelingtasks, but since it is a discriminative model, it isalso possible to provide neighboring words as fea-tures into the system.
During training, each in-stance within a sequence has basically the follow-ing main parts:1. features related to the current and neighbor-ing tokens (namely surface form or lemma)2. features related to the current and neighbor-ing characters83.
class labelThe test data is also prepared similarly exceptthe gold standard class labels.Table 1 and Table 2 show instance samples forthe sample words (?OldUGUnU?
and ?
s l m ?
)given in Section 3.
As can be observed from thetables, we have 7 different class labels in diacriticrestoration and 9 different class labels in vowelrestoration (one can refer to Section 3 for the de-tails).
The sequences represent words in focus andeach instance line within a sequence stands for thecharacter position in focus.
The sample for dia-critization has 5 character features and 2 word fea-tures where the current character feature limits the6In this work, we used CRF++7which is an open sourceimplementation of CRFs.8The feature related to the current character is only avail-able in diacritization modelnumber of the class labels to be assigned to thatposition by 2.
The sample for vowelization has 1word feature and 6 character features.Curr.
Neig.
Curr.
Neig.
Neig.
Neig.
Neig.
ClassLetter Word(+1) Word Ch(-2) Ch(-1) Ch(+1) Ch(+2) LabelO GOrdUm OldUGUnU l d ?oU GOrdUm OldUGUnU l d G U ?uG GOrdUm OldUGUnU d U U n ?gU GOrdUm OldUGUnU U G n U ?uU GOrdUm OldUGUnU U n ?uTable 1: Diacritization: Instance Representationfor the word ?oldugunu??OldUGUnU?
5 critical positionsCurr.
Neig.
Neig.
Neig.
Neig.
Neig.
Neig.
ClassWord Ch(-3) Ch(-2) Ch(-1) Ch(+1) Ch(+2) Ch(+3) Labelslm s l mslm s l m eslm s l m aslm s l mTable 2: Vowelization: Instance Representationfor the word ?slm??
s l m ?
4 possible vowel positionsCRFs are log-linear models and in order toget advantage of the useful feature combinations,one needs to provide these as new features to theCRFs.
In order to adopt a systematic way, we tookthe features?
combinations for character featuresand word features separately.
For character fea-tures we took the combinations up to 6-grams for?3ch and for the neighboring word features up to4 grams.
The number of features affects directlythe maximum amount of training data that couldbe used during the experiments.
The total numberof feature templates after the addition of featurecombinations ranges between 7 for the simplestcase and 30 for our model with maximum num-ber of features.
Three sample feature templates aregiven below for the sample sequence of Table 1.The templates are given in [pos,col] format, wherepos stands for the relative position of the token infocus and col stands for the feature column num-ber in the input file.
U06 is the template for us-ing the sixth9feature in Table 1 (Neigh.
Ch(+1)).U13 is a bigram feature combination of 2nd and3th features (the current token and the next token).U11 is a fourgram feature combination of 4th, 5th,6th and 7th features of our feature set that refersto the group of the previous two characters and thenext two characters.9in CRF++ feature templates the features indexes startfrom 0.56U06 : %x[0, 5]U13 : %x[0, 1]/%x[0, 2]U11 : %x[0, 3]/%x[0, 4]/%x[0, 5]/%x[0, 6]4.2 Language ValidatorThe n best sequences of the discriminative classi-fier is then transferred to the language validator.We use a two-level morphological analyzer (S?ahinet al., 2013) for the Turkish case since in this ag-glutinative language it is impractical to validate aword by making a lexicon lookup.
But this sec-ond part may be replaced by any language valida-tor (for other languages) which will filter only thevalid outputs from the n best results of the discrim-inative classifier.
Figure 2 shows an example caseof the process for vowelization.
The system takesthe consonant sequence ?kd?
and the 5 best outputof the first stage is produced as ?kidi, kedi, kad?,kado, kada?.
The language validator then choosesthe most probable valid word ?kedi?
(cat) as itsoutput.
One should notice that if none of the nmost probable results is a valid word, then the sys-tem won?t produce any suggestion at all.
We showexperimental results on the effect of the selectionof n in the next section.5 Experimental Setup And ResultsIn this section, we first present our datasets andevaluation strategy.
We then continue with the di-acritization experiments and finally we share theresults of our vowelization experiments.5.1 Datasets and Evaluation MethodologyFor both of the diacritization and vowelizationtasks, creating the labeled data is a straightforwardtask since the reverse operations for these (con-verting from formally written text to their Asciiform or to a form without vowels) can be accom-plished automatically for most of the languages(except semitic languages where the vowels do notappear in the formal form).
To give an examplefrom Turkish, the word ?oldu?gunu?
may be auto-matically converted to the form ?OldUGUnU?
fordiacritization and ?
l d g n ?
for vowelization ex-periments.
We used data from three different cor-pora: METU Corpus (Say et al., 2002) and twoweb corpora from Y?ld?z and Tantu?g (2012) andSak et al.
(2011).In order to judge different approaches fairly,we aimed to create a decently though test set.Since the vowelization task already comprises avery high ambiguity, we focused to the ambigu-ous diacritization samples.
With this purpose, wefirst took the Turkish dictionary and converted allthe lemmas within the dictionary into their Asciiforms.
We then created the possible diacritizedforms (Figure 1) and created a list of all ambigu-ous lemmas (1221 lemmas in total) by finding allthe lemmas which could be produced as the out-put of diacritization.
For example ?ac???
and ?ac?
?are put into this list after this operation.
Althoughthis ambiguous lemmas list may be extended byalso considering interfusing surface forms, for thesake of simplicity we just considered to take theambiguous lemmas from the dictionary.
We thensearched our three corpora (and the WEB wherenot available in these) for the words with an am-biguous lemma and created our test set so that foreach ambiguous lemma there is exactly one sen-tence consisting of it.
As a result, we collected atest set of 1157 sentences (17923 tokens) consist-ing of 1871 ambiguous words10in total.
The re-maining sentences from the corpora are used dur-ing training.
Since the feature set size directly af-fects the amount of usable training data, for differ-ent experiment sets we used different size of train-ing data each time trying to use the data from thethree corpora in equal amounts.After evaluating with synthetically producedtraining and test sets, we also tested our best per-formed models on real data collected from socialmedia (25K tweets with at least one erroneoustoken) and normalized manually (Eryi?git et al.,2013).
This data consists 58836 tokens that havetext normalization problems where 3.75% is dueto missing vowels and 22.8% is due to misuse ofTurkish characters.
In order to separate these spe-cific error sets, we first automatically aligned theoriginal and normalized tweets and then appliedsome filters over the aligned data: e.g.
Deasci-ification errors are selected so that the characterlength of the original word and its normalizedform should be the same and the differing lettersshould only be the versions of the same Turkishcharacters.For the evaluation of diacritization, we providetwo accuracy scores: Accuracy over the entirewords (AccOverallEquation 1) and accuracy overthe ambiguous words alone (AccAmbEquation 210One should note that each sentence in the test set con-tains at least one or more ambiguous surface forms.
The testdata will be available to the researches via http://...57over 1871 ambiguous words in the test set).
Sincethe vowelization problem is almost entirely am-biguous, the two scores are almost the same for theentire tests (# of words ?
# of amb.
words).That is why, for the vowelization task we provideonly AccOverall.AccOverall=# of corr.
diacritized words# of words(1)AccAmb=# of corr.
diacritized amb.
words# of amb.
words(2)In the work of T?ur (2000), the accuracy scoreis provided as the correctly determined characterswhich we do not find useful for the given tasks:AccAmb=# of corr.
diacritized amb.
chars# of amb.
chars.
Thisscore gives credit to the systems although the pro-duced output word is not a valid Turkish word.
Forexample, if a vowelization system produces an in-valid output as ?oldgn?
for the input ?
l d g n ?, itwill have a 1/5 (one correct character over 5 possi-ble positions) score whereas in our evaluation thisoutput will be totally penalized.5.2 Diacritization ExperimentsFor diacritization, we designed four sets of ex-periments.
The first set of experiments (Table 3)presents the results of our baseline systems.
Weprovide four baseline systems.
The first one is arule based diacritics restorer which creates all thepossible diacritics for a given input and outputs thefirst morphologically valid one.
As the proposedmodel does, the rule based system also validatesits outputs by using the morphological analyzerintroduced in Section 4.2.
One can see from thetable that the accuracy on the ambiguous words ofthis system is nearly 70%.
Our second baselineuses a unigram language model in order to selectthe most probable valid output of the morpholog-ical analyzer.
Our third baseline is a baseline forour discriminative classifier (with only ?2 neigh-boring characters) without the language validatorcomponent.
In this model, the top most output ofthe CRF is accepted as the correct output word.One can observe that this baseline although it per-forms better than the rule based system, it is worsethan the second baseline with a language modelcomponent.
Our last baseline is the baseline forthe proposed system in this paper with a discrimi-native classifier (using only ?2 neighboring char-acters) and a language validator which chooses thefirst valid output within the top 5 results of theclassifier.
It outperforms all the previous base-lines.Acc AccSystem Overall AmbRule based 90.38 69.17Rule based + Unigram LM 91.94 83.54CRF ?2ch 87.93 77.24CRF ?2ch + Lang.Valid.
94.88 88.51Table 3: Diacritization Baseline ResultsThe second set of experiments given in Table 4is for the feature selection of the proposed model.We test with the neighboring characters up to ?3and together with the surface form of the cur-rent token sformcurrand/or the first n charac-ters of the current token firstnchcurras lemmafeature.
For both of the first two sets of exper-iments (Table 3 and Table 4) we used a train-ing data of size 4591K (the max.
possible sizefor the most complex feature set in these experi-ments; (last line of Table 4).
It can be observedfrom Table 4 that although ?3ch (2nd line) per-forms better than ?2ch (1st line), when we usethese together with sformcurrwe obtain betterresults with ?2ch (3rd line).
Since ?3ch (7 char-acters in total) will be very close to the wholenumber of characters within the surface form, thenew feature?s help is more modest in?3ch model.In these experiments we try to optimize on theoverall accuracies.
Our highest score in this ta-ble is with the?2ch+sformcurr+first5chcurr(last line) but since the difference between this and?2ch + sformcurris not statistically significant(with McNemar?s test p<0.01) and the size of themaximum possible training data could still be im-proved for the latter model, we decided to continuewith ?2ch+ sformcurr.In the third set of diacritization experiments(Table 5) we investigated the effect of using theneighboring tokens as features.
In this experi-ment set, the training data size is decreased to amuch lower size, only 971K in order to be ableto train with ?2 neighboring tokens.
Each lineof the table is the addition of the surface formsfor the precised positions to the model of the firstline ?2ch + sformcurr.
We tested with all thecombinations in the ?2 window size.
For exam-58Acc AccFeature Combinations Overall Amb?2ch 94.88 88.51?3ch 95.76 91.05?2ch+ sformcurr96.26 91.60?3ch+ sformcurr96.20 91.71?2ch+ first3chcurr95.29 90.17?2ch+ first4chcurr95.60 89.06?2ch+ first5chcurr95.95 90.72?2ch+ sformcurr+ first3chcurr96.23 91.82?2ch+ sformcurr+ first4chcurr96.26 91.82?2ch+ sformcurr+ first5chcurr96.28 91.60Table 4: Diacritization Feature Selection IAcc AccFeatures Overall Amb?2ch+ sformcurr95.29 90.61+sform001095.49 90.72+sform001195.39 90.39+sform010093.77 83.32+sform011095.39 90.28+sform011195.26 89.95+sform110095.24 89.83+sform111095.21 89.50+sform111195.11 89.17Table 5: Diacritization Feature Selection IIple sform0010means that the surface form of thetoken at position +1 is added to the features.
Thisfeature set outperformed all the other ones.Acc AccSystem Overall AmbY?uret (2006) 95.93 91.05Zemberek (2007) 87.71 82.55?2ch+ sformcurr96.15 92.04?2ch+ sformcurr+ sform001097.06 94.70Table 6: Diacritization Results Comparison withPrevious WorkFinally, in Table 6, we give the comparison re-sults of our proposed model with the availableTurkish deasciifiers (the tools?
original name givenby the authors) (Y?uret and de la Maza, 2006; Ak?nand Ak?n, 2007).
We both tested by ?2ch +sformcurrand?2ch+sformcurr+sform0010.Both of the models are tested with maximum pos-sible size of training data: 10379K and 5764K suc-cessively.
Our proposed model for diacritizationoutperformed all of the other methods with a suc-cess rate of 97.06%.
It outperformed the state ofthe art by 1.13 percentage points in overall accu-racy and by 3.65 percentage points in ambiguouscases (both results statistically significant).5.3 Vowelization ExperimentsFor the vowelization, we designed similar set ofexperiments.
In Table 7, we provide the resultsfor a rulebased baseline and our proposed modelwith ?2ch.
It is certainly a very time consumingprocess to produce all the possible forms for thevowelization task (see Section 3.3).
Thus, for therule based baseline we stopped the generation pro-cess once we find a valid output.
The baseline ofthe proposed model provides a 28.44 percentagepoints improvements over the rule based system.We did not try to compare our results with thework of T?ur (2000) (an HMM model on charac-ter level) firstly because the developed model wasnot available for testing, secondly because the pro-vided evaluation (see Section 5.1) was useless forour purposes and finally because our ?3 charac-ter model provided in the second line of Table 8 isa discriminative counterpart of his 6-gram genera-tive model.AccSystem OverallRule based 16.89CRF ?2ch+Lang.Valid.
45.33Table 7: Vowelization Baseline ResultsTable 8 gives the feature selection tests?
resultssimilarly to the previous section.
This time we ob-tained the highest score with ?3ch + sformcurr59.17%.
In this set of experiments, we used4445K of training data.In order to investigate the impact of neighbor-ing tokens, in the experiments given in Table 9,we had to continue with ?2ch + sformcurrwithAccFeature Combinations Overall?2ch 45.33?3ch 57.20?2ch+ sformcurr57.22?3ch+ sformcurr59.17?2ch+ first3chcurr40.44?2ch+ first4chcurr40.48?2ch+ first5chcurr44.22?2ch+ sformcurr+ first3chcurr45.89?2ch+ sformcurr+ first4chcurr45.89?2ch+ sformcurr+ first5chcurr49.58Table 8: Vowelization Feature Selection I59AccFeatures Overall?2ch+ sformcurr54.07+sform001050.89+sform001149.60+sform010031.84+sform011049.41+sform011147.78+sform110048.98+sform111047.88+sform111147.21Table 9: Vowelization Feature Selection II971K of training data.11We could not obtain anyimprovement with the neighboring tokens.
We re-late these results to the fact that the neighboringtokens are also in vowel-less form in the trainingdata so that this information do not help the dis-ambiguation of the current token.
Since we couldnot add the word based features to this task by thismodel, for future work we are planning to applya word based language model over the proposedmodel?s possible output sequences.In the final experiment set given in Table 10,we trained our best performing model ?3ch +sformcurrwith the maximum possible trainingdata (6653K).
We also tested with different N val-ues of CRF output.
Although there is a slight in-crease on the overall accuracy by passing fromN=5 to N=10, the increase is much higher whenwe evaluate with AcctopN.
Equation 3 gives thecalculation of this score which basically calculatesthe highest score that could be obtained after per-fect reranking of the top N results.
In this score thesystem is credited if the correct vowelized answeris within the top N results of the system.
We seefrom the table that there is still a margin for theimprovement in top 10 results (up to 85.09% forthe best model).
This strengthens our believe forthe need of a word based language model over theproposed model outputs.
Our vowelization modelin its current state achieves an accuracy score of62.66% with a 45.77 percentage points improve-ments over the rule based baseline.11If we select the larger model, it is going to be impossi-ble to feed enough training data to the system.
Since in thisset of experiments (Table 9) we only investigate the impactof neighboring tokens, we had/preferred to select the smallercharacter model.AcctopN=?1 if result exists within top N# of words(3)Acc AccSystem Overall top N?3ch+ sformcurrWith Top 5 Poss.
from CRF 62.05 80.21?3ch+ sformcurrWith Top 7 Poss.
from CRF 62.36 82.53?3ch+ sformcurrWith Top 10 Poss.
from CRF 62.66 85.09Table 10: Vowelization Top N ResultsFinally we test our best models on voweliza-tion and diacritization errors from our Tweeterdata set and obtained 95.43% for diacritization and69.56% for vowelization.6 Conclusion And Future WorkIn this paper, we proposed a hybrid model for thediacritization and vowelization tasks which is anemerging problem of social media text normaliza-tion.
Although the tasks are previously investi-gated for different purposes especially for semiticlanguages, to the best of our knowledge, this isthe first time that they are evaluated together forthe social media data on a language which do notpossess these problems in its formal form but onlyin social media platform.
We obtained the high-est scores for the diacritization (97.06%) and vow-elization (62.66%) of Turkish.We have two future plans for the vowelizationpart of the proposed model.
The first one, as de-tailed in previous section, is the application of aword based language model over the valid CRFoutputs.
The second one is the extension for par-tial vowelization.
Although in this work, we de-signed the vowelization task as the overall genera-tion of the entire vowels within a vowel-less word,we observe from the social web data that peoplealso tend to write with partially missing vowels.As an example, they are writing ?sevyrm?
insteadof the word ?seviyorum?
(I love).
In this case, theposition between the consonants ?s?
and ?v?
is con-strained to the letter ?e?
and it is meaningless togenerate the other possibilities for the remaining 7vowels.
For this task, we are planning to focus onconstrained Viterbi algorithms during the decod-ing stage.60The tool?s web api and the produced data setswill be available to the researchers from the fol-lowing address http://tools.nlp.itu.edu.tr/(Eryi?git,2014)AcknowledgmentThis work is part of a research project supportedby TUBITAK 1001(Grant number: 112E276) asan ICT cost action (IC1207) project.ReferencesAhmet Afsin Ak?n and Mehmet D?undar Ak?n.
2007.Zemberek, an open source nlp framework for turkiclanguages.
Structure.Sarah Al-Shareef and Thomas Hain.
2012.
Crf-baseddiacritisation of colloquial Arabic for automaticspeech recognition.
In INTERSPEECH.
ISCA.Guy De Pauw, Peter W. Wagacha, and Gilles-Mauricede Schryver.
2007.
Automatic diacritic restorationfor resource-scarce languages.
In Proceedings of the10th international conference on Text, speech anddialogue, TSD?07, pages 170?179, Berlin, Heidel-berg.
Springer-Verlag.G?uls?en Eryi?git, Fatih Samet C?etin, Meltem Yan?k,Tanel Temel, and?Iyas C?ic?ekli.
2013.
Turksent: Asentiment annotation tool for social media.
In Pro-ceedings of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 131?134,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.G?uls?en Eryi?git.
2014.
ITU Turkish NLP web service.In Proceedings of the Demonstrations at the 14thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL), Gothen-burg, Sweden, April.
Association for ComputationalLinguistics.Ya?akov Gal.
2002.
An hmm approach to vowelrestoration in Arabic and Hebrew.
In Proceed-ings of the ACL-02 workshop on Computational ap-proaches to semitic languages, SEMITIC ?02, pages1?7, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Robbie A. Haertel, Peter McClanahan, and Eric K.Ringger.
2010.
Automatic diacritization for low-resource languages using a hybrid word and con-sonant cmm.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 519?527, Stroudsburg, PA,USA.
Association for Computational Linguistics.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In ICML, pages 282?289.Rani Nelken and Stuart M. Shieber.
2005.
Arabic di-acritization using weighted finite-state transducers.In Proceedings of the ACL Workshop on Compu-tational Approaches to Semitic Languages, Semitic?05, pages 79?86, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Kiem-Hieu Nguyen and Cheol-Young Ock.
2010.
Di-acritics restoration in Vietnamese: letter based vs.syllable based model.
In Proceedings of the 11thPacific Rim international conference on Trends inartificial intelligence, PRICAI?10, pages 631?636,Berlin, Heidelberg.
Springer-Verlag.Muhammet S?ahin, Umut Sulubacak, and G?uls?enEryi?git.
2013.
Redefinition of Turkish morphol-ogy using flag diacritics.
In Proceedings of TheTenth Symposium on Natural Language Processing(SNLP-2013), Phuket, Thailand, October.Has?im Sak, Tunga G?ung?or, and Murat Sarac?lar.
2011.Resources for Turkish morphological processing.Lang.
Resour.
Eval., 45(2):249?261, May.Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut?Ozge.
2002.
Development of a corpus and a tree-bank for present-day written Turkish.
In Proceed-ings of the Eleventh International Conference ofTurkish Linguistics, Famaguste, Cyprus, August.Kevin P. Scannell.
2011.
Statistical unicodification ofAfrican languages.
Lang.
Resour.
Eval., 45(3):375?386, September.Michel Simard and Alexandre Deslauriers.
2001.Real-time automatic insertion of accents in Frenchtext.
Nat.
Lang.
Eng., 7(2):143?165, June.G?okhan T?ur.
2000.
A statistical information extrac-tion system for Turkish.
Ph.D. thesis, Department ofComputer Engineering and the Institute of Engineer-ing and Science of Bilkent University, Ankara.Eray Y?ld?z and C?uneyd Tantu?g.
2012.
Evaluationof sentence alignment methods for English-Turkishparallel texts.
In Proceedings of the First Workshopon Language Resources and Technologies for TurkicLanguages (LREC), Istanbul, Turkey, 23-25 May.Deniz Y?uret and Michael de la Maza.
2006.
Thegreedy prepend algorithm for decision list induction.In Proceedings of the 21st international conferenceon Computer and Information Sciences, ISCIS?06,pages 37?46, Berlin, Heidelberg.
Springer-Verlag.Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.2006.
Maximum entropy based restoration of Ara-bic diacritics.
In Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Associationfor Computational Linguistics, ACL-44, pages 577?584, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.61
