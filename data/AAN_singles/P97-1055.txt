Paradigmatic Cascades: a Linguistically Sound Model ofPronunciation by AnalogyFranco is  YvonENST and CNRS,  URA 820Computer  Science Depar tment46 rue Bar rau l t  - F 75 013 Par isyvon~?nf ,  enst .
f rAbst rac tWe present and experimentally evaluate anew model of pronunciation by analogy:the paradigmatic cascades model.
Given apronunciation lexicon, this algorithm firstextracts the most productive paradigmaticmappings in the graphemic domain, andpairs them statistically with their corre-late(s) in the phonemic domain.
Thesemappings are used to search and retrievein the lexical database the most promisinganalog of unseen words.
We finally applyto the analogs pronunciation the correlatedseries of mappings in the phonemic domainto get the desired pronunciation.1 Mot ivat ionPsychological models of reading aloud traditionallyassume the existence of two separate routes for con-verting print to sound: a direct lexical route, whichis used to read familiar words, and a dual route rely-ing upon abstract letter-to-sound rules to pronouncepreviously unseen words (Coltheart, 1978; Coltheartet al, 1993).
This view has been challenged bya number of authors (e.g.
(Glushsko, 1981)), whoclaim that the pronunciation process of every word,familiar or unknown, could be accounted for in aunified framework.
These single-route models cru-cially suggest that the pronunciation of unknownwords results from the parallel activation of similarlexical items (the lexical neighbours).
This idea hasbeen tentatively implemented both into various ym-bolic analogy-based algorithms (e.g.
(Dedina andNusbaum, 1991; Sullivan and Damper, 1992)) andinto connectionist pronunciation devices (e.g.
(Sei-denberg and McClelland, 1989)).The basic idea of these analogy-based models isto pronounce an unknown word x by recombin-ing pronunciations of lexical items sharing commonsubparts with x.
To illustrate this strategy, Ded-ina and Nussbaum show how the pronunciation ofthe sequence lop in the pseudo-word blope is analo-gized with the pronunciation of the same sequencein sloping.
As there exists more than one way to re-combine segments of lexical items, Dedina and Nuss-baum's algorithm favors recombinations includinglarge substrings of existing words.
In this model,the similarity between two words is thus implicitelydefined as a function of the length of their commonsubparts: the longer the common part, the betterthe analogy.This conception of analogical processes has an im-portant consequence: it offers, as Damper and East-mona ((Damper and Eastmond, 1996)) state it, "noprincipled way of deciding the orthographic neigh-bouts of a novel word which are deemed to influ-ence its pronunciation (...)".
For example, in themodel proposed by Dedina and Nusbaum, any wordhaving a common orthographic substring with theunknown word is likely to contribute to its pronun-ciation, which increases the number of lexical neigh-bouts far beyond acceptable limits (in the case ofblope, this neighbourhood would contain every En-glish word starting in bl, or ending in ope, etc).From a computational standpoint, implement-ing the recombination strategy requires a one-to-one alignment between the lexical graphemic andphonemic representations, where each grapheme ismatched with the corresponding phoneme (a nullsymbol is used to account for the cases where thelengths of these representations differ).
This align-ment makes it possible to retrieve, for any graphemicsubstring of a given lexical item, the correspondingphonemic string, at the cost however of an unmoti-vated complexification f lexical representations.In comparison, the paradigmati c cascades model(PCP for short) promotes an alternative view ofanalogical processes, which relies upon a linguisti-cally motivated similarity measure between words.428The basic idea of our model is to take advantageof the internal structure of "natural" lexicons.
Infact, a lexicon is a very complex object, whose ele-ments are intimately tied together by a number offine-grained relationships (typically induced by mor-phological processes), and whose content is severelyrestricted, on a language-dependant basis, by a com-plex of graphotactic, phonotactic and morphotac-tic constraints.
Following e.g.
(Pirrelli and Fed-erici, 1994), we assume that these constraints ur-face simultaneously in the orthographical and inthe phonological domain in the recurring patternof paradigmatically alterning pairs of lexical items.Extending the idea originally proposed in (Federici,Pirrelli, and Yvon, 1995), we show that it is possibleto extract these alternation patterns, to associatealternations in one domain with the related alterna-tion in the other domain, and to construct, using thispairing, a fairly reliable pronunciation procedure.2 The  Parad igmat ic  Cascades Mode lIn this section, we introduce the paradigmatic cas-cades model.
We first formalize the concept of aparadigmatic relationship.
We then go through thedetails of the learning procedure, which essentiallyconsists in an extensive search for such relationships.We finally explain how these patterns are used in thepronunciation procedure.2.1 Paradigmatic Relationships andAlternationsThe paradigmatic ascades model crucially reliesupon the existence of numerous paradigmatic rela-tionships in lexical databases.
A paradigmatic re-lationship involves four lexical entries a, b, c, d, andexpresses that these forms are involved in an ana-logical (in the Saussurian (de Saussure, 1916) sense)proportion: a is to b as e is to d (further along ab-breviated as a : b = c : d, see also (Lepage andShin-Ichi, 1996) for another utilization of this kindof proportions).
Morphologically related pairs pro-vide us with numerous examples of orthographicalproportions, as in:reactor : reaction = fac tor  : fact ion  (1)Considering these proportions in terms of ortho-graphical alternations, that is in terms of partialfnnctions in the graphemic domain, we can see thateach proportion involves two alternations.
The firstone transforms reactor into reaction (and fac torinto fact ion) ,  and consists in exchanging the suffixesor and ion.
The second one transforms reactor intofac tor  (and react ion into fact ion) ,  and consists inexchanging the prefixes re and f.  These alternationsare represented on figure 1.freactor ?
reactionfactor ?
factionfFigure 1: An Analogical ProportionFormally, we define the notion of a paradigmaticrelationship as follows.
Given E, a finite alphabet,and/:,  a finite subset of E*, we say that (a, b) E / :  x/:is paradigmatically related to (c, d) E / :  x / :  iff thereexits two partial functions f and g from E* to E*,where f exchanges prefixes and g exchanges suffixes,and:f (a )  = c and f(b) = d (2)g(a) = b and g(c) = d (3)f and g are termed the paradigmatic alternationsassociated with the relationship a : b =:,9 c : d.The domain of an alternation f will be denoted bydora(f).2.2 The  Learn ing  ProcedureThe main purpose of the learning procedure is toextract from a pronunciation lexicon, presumablystructured by multiple paradigmatic relationships,the most productive paradigmatic alternations.Let us start with some notations: Given G agraphemic alphabet and P a phonetic alphabet, apronunciation lexicon ?
is a subset of G* ?
P*.
Therestriction of/:  on G* (respectively P*) will be noted/:a (resp./:p).
Given two strings x and y, p re f (x ,  y)(resp.
su f f (x ,  y)) denotes their longest common pre-fix (resp.
suffix).
For two strings x and y having anon-empty common prefix (resp.
suff?x) u, f~y (resp,g~y) denotes the function which transforms x into y:as x = uv, and as y = ut, f~y substitutes a final vwith a final t. ~ denotes the empty string.Given /:, the learning procedure searches /:G forany for every 4-uples (a, b, c, d) of graphemic stringssuch that a : b =:,g c : d. Each match incrementsthe productivity of the related alternations f andg.
This search is performed using using a slightlymodified version of the algorithm presented in (Fed-erici, Pirrelli, and Yvon, 1995), which applies to ev-ery word x in / : c  the procedure detailled in table 1.In fact, the properties of paradigmatic relation-ships, notably their symetry, allow to reduce dra-matically the cost of this procedure, since not all429GETALTERNATIONS (X)1 z)(x) ~- {y e 12a/(t = pref(x, y)) # ~}2 for yeD(x)3 do4 P(x,y)  ~- {(z,t) c 12~ ?
12~1z = f;,~(t)}5 if P(x,y) ?
O6 then7 IT~crementCount ( fSy)8 IncrementCount (f:Pt)Table 1: The Learning Procedure4-uple of strings in ?c, need to be examined uringthat stage.For each graphemic alternation, we also recordtheir correlated alternation(s) in the phonologicaldomain, and accordingly increment their productiv-ity.
For instance, assuming that factor and reactorrespectively receive the pronunciations/faekt0r/and/rii~ektor/, the discovery of the relationship ex-pressed in (1) will lead our algorithm to record thatthe graphemic alternation f -+ re correlates in thephonemic domain with the alternation / f / -+  /ri:/.Note that the discovery of phonemic orrelates doesnot require any sort of alignment between the or-thographic and the phonemic representations: theprocedure simply records the changes in the phone-mic domain when the Mternation applies in thegraphemic domain.At the end of the learning stage, we have in handa set A = {Ai} of functions exchanging suffixes orprefixes in the graphemic domain, and for each Aiin A:(i) a statistical measure Pi of its productivity, de-fined as the likelihood that the transform of alexical item be another lexieal item:Pi = I {x e dom(di) and Ai(x) E 12}1i dom(&) l  (4)(ii) a set {Bi, j}, j  G {1. .
.h i} of correlated func-tions in the phonemic domain, and a statisticalmeasure Pi,j of their conditional productivity,i.e.
of the likelihood that the phonetic alterna-tion Bi , j  correlates with Ai .Table 2 gives the list of the phonological correlatesof the alternation which consists in adding the suffixly, corresponding to a productive rule for derivingadverbs from adjectives in English.
If the first linesof table 2 are indeed <'true" phonemic orrelates ofthe derivation, corresponding to various classes ofadjectives, a careful examination of the last lines re-veals that the extraction procedure is easily fooledalternationxx-It~x-lol lx-I~l/ -~x -+x-/iin/x-I1dlx-~iv~x-~o~x -+z-/ Ir/  -+x-/3n/Examplex-/l i ' /  goodx-/adli'/ markedx-/oli:/ equalx-/l i ' /  capablex-~i:~ coolx-/enli'/ cleanx-/aldli'/ idx-/aIli:/ livex-/51i'/ loathx-/laI/ impx-/3:li'/ earx-/onlil/ onTable 2: Phonemic orrelates of x --+ x - lyby accidental pairs like imp-imply, on-only or ear-early.
A simple pruning rule was used to get rid ofthese alternations on the basis of their productivity,and only alternations which were observed at leasttwice were retained.It is important o realize that A allows to specifiylexical neighbourhoods in 12a: given a lexical entryx, its nearest neighbour is simply f(x),  where f isthe most productive alternation applying to x. Lex-ical neighbourhoods in the paradigmatic ascadesmodel are thus defined with respect to the locallymost productive alternations.
As a consequence,the definition of neighbourhoods implicitely incorpo-rates a great deal of linguistic knowledge xtractedfl'om the lexicon, especially regarding morphologicalprocesses and phonotactic onstraints, which makesit much for relevant for grounding the notion of anal-ogy between lexical items than, say, any neighbour-hood based on the string edition metric.2.3 The Pronunc iat ion  of  Unknown WordsSupose now that we wish to infer the pronunciationof a word x, which does not appear in the lexicon.This goal is achieved by exploring the neighbour-hood of x defined by A, in order to find one or severalanalogous lexica.1 entry(ies) y.
The second stage ofthe pronunciation procedure is to adapt the knownpronunciation of y, and derive a suitable pronuncia-tion for x: the idea here is to mirror in the phonemicdomain the series of alternations which transform xinto y in the graphemic domain, using the statisticalpairing between alternations that is extracted ur-ing the learning stage.
The complete pronunciationprocedure is represented on figure 2.Let us examine carefully how these two aspects ofthe pronunciation procedure are implemented.
Thefirst stage is I;o find a lexical entry in the neighbour-430Graphcmic domain Phonemic domainFigure 2: The pronunciation of an unknown wordhood of x defined by L:.The basic idea is to generate A(x), definedas {Ai(x), forAi E ,4, x E domain(Ai)}, which con-tains all the words that can be derived from x us-ing a function in ,4.
This set, better viewed as astack, is ordered according to the productivity of theAi: the topmost element in the stack is the nearestneighbour of x, etc.
The first lexical item found infl, (x) is the analog of x.
If A (x) does not containany known word, we iterate the procedure, usingx I, the top-ranked element of .4 (x), instead of x.This expands the set of possible analogs, which isaccordingly reordered, etc.
This basic search strat-egy, which amounts to the exploration of a deriva-tion tree, is extremely ressource consuming (everyexpension stage typically adds about a hundred ofnew virtual analogs), and is, in theory, not guar-anted to terminate.
In fact, the search problem isequivalent to the problem of parsing with an unre-stricted Phrase Structure Grammar, which is knownto be undecidable.We have evaluated two different search strategies,which implement various ways to alternate betweenexpansion stages (the stack is expanded by gener-ating the derivatives of the topmost element) andmatching stages (elements in the stack are lookedfor in the lexicon).
The first strategy implements adepth-first search of the analog set: each time thetopmost element of the stack is searched, but notfound, in the lexicon, its derivatives are immediatelygenerated, and added to the stack.
In this approach,the position of an analog in the stack is assessed a.s afunction of the "distance" between the original wordx and the analog y = A~ (A~_, (... A~ (x))), accord-ing to:l=kd(x, y) = 1-I/----1The search procedure is stopped as soon an ana-log is found in L:a, or else, when the distance be-tween x and the topmost element of the stack, whichmonotonously decreases (Vi, pi < 1), falls below apre-defined theshold.The second strategy implements a kind of com-promise between depth-first and breadth-first explo-ration of the derivation tree, and is best understoodif we first look at a concrete xample.
Most alter-nations substituting one initial consonant are veryproductive, in English like in many other languages.Therefore, aword starting with say, a p, is very likelyto have a very close derivative where the initial phas been replaced by say, a r. Now suppose thatthis word starts with pl: the alternation will de-rive an analog starting with rl, and will assess itwith a very high score.
This analog will, in turn,derive many more virtual analogs tarting with rl,once its suffixes will have been substituted uringanother expansion phase.
This should be avoided,since there are in fact very few words starting withthe prefix rl: we would therefore like these words tobe very poorly ranked.
The second search strategyhas been devised precisely to cope with this problem.The idea is to rank the stack of analogs accordingto the expectation of the number of lexical deriva-tives a given analog may have.
This expectation iscomputed by summing up the productivities of allthe alternations that can be applied to an analog yaccording to:p, (61i/yEdom(Ai)This ranking will necessarily assess any analog start-ing in rl with a low score, as very few alternationswill substitute its prefix.
However, the computationof (6) is much more complex than (5), since it re-quires to examine a given derivative before it can bepositioned in the stack.
This led us to bring for-ward the lexical matching stage: during the expan-sion of the topmost stack element, all its derivativesare looked for in the lexicon.
If several derivativesare simultaneously found, the search procedure haltsand returns more than one analog.The expectation (6) does not decrease as morederivatives are added to the stack; consequently,it cannot be used to define a stopping criterion.The search procedure is therefore stopped whenal} derivatives up to a given depth (2 in our ex-periments) have been generated, and unsuccessfullylooked for in the lexicon.
This termination criterionis very restrictive, in comparison to the one imple-mented in the depth-first strategy, since it makes itimpossible to pronounce very long derivatives, forwhich a significant number of alternations need to431be applied before an analog is found.
An example isthe word synergistically, for which the "breadth-first" search terminates uncessfully, whereas thedepth-first search manages to retrieve the "analog"energy.
Nonetheless, the results reported hereafterhave been obtained using this "breadth-first" strat-egy, mainly because this search was associated witha more efficient procedure for reconstructing pronun-ciations (see below).Various pruning procedures have also been imple-mented in order to control the exponential growth ofthe stack.
For example, one pruning procedure de-tects the most obvious derivation cycles, which gen-erate in loops the same derivatives; another prun-ing procedure tries to detect commutating alterna-tions: substituting the prefix p, and then the suffixs often produces the same analog than when alter-nations apply in the reverse order, etc.
More de-tails regarding implementational aspects are givenin (Yvon, 1996b).If the search procedure returns an analog y =Aik(A ik_~( .
.
.A i l (x ) ) )  in ?, we can build a pronun-ciation for x, using the known pronunciation ?
(y)of y.
'For this purpose, we will use our knowledgeof the Bi,j,  for i E {i l .
.
.
ik}, and generate ev-ery possible transforms of q;(y) in the phonologicaldomain: -1 -1 {Bik,jk(Bik_~,jk_~ (.
.. (q~(y))))), with jk in{ 1 ... nik }, and order this set using some function ofthe Pi,j.
The top-ranked element in this set is thepronunciation of x.
Of course, when the search fails,this procedure fails to propose any pronunciation.In fact, the results reported hereafter use a slightlyextended version of this procedure, where the pro-nunciations of more than one a.nMog are used forgenerating and selecting the pronunciation ofthe un-known word.
The reason for using multiple analogsis twofold: first, it obviates the risk of being wronglyinfluenced by one very exceptional analog; second,it enables us to model conspiracy effects more accu-rately.
Psychological models of reading aloud indeedassume that the pronunciation of an unknown wordis not influenced by just one analog, but rather byits entire lexical neighbourhood.3 Experimental Results3.1 Exper imenta l  DesignWe have evaluated this algorithm on two differentpronunciation tasks.
The first experiment consistsin infering the pronunciation of the 70 pseudo-wordsoriginally used in Glushko's experiments, which havebeen used as a test-bed for various other pronun-ciation algorithms, and allow for a fair head-to-head comparison between the paradigmatic cascadesmodel and other analogy-based procedures.
Forthis experiment, we have used the entire nettalk(Sejnowski and Rosenberg, 1987) database (about20 000 words) as the learning set.The second series of experiments i intended toprovide a more realistic evaluation of our model illthe task of pronouncing unknown words.
We haveused the following experimental design: 10 pairs ofdisjoint (learning set, test set) are randomly selectedfrom the nettalk database and evaluated.
In eachexperiment, the test set contains abou~ the tenthof the available data.
A transcription is judged tobe correct when it matches exactly the pronuncia--tion listed in the database at the segmental level.The number of correct phonemes in a transcriptionis computed on the basis of the string-to-string editdistance with the target pronunciation.
For eachexperiment, we measure the percentage of phonemeand words that are correctly predicted (referred toas correctness), and two additional figures, which areusually not significant in context of the evaluationof transcription systems.
Recall that our algorithm,unlike many other pronunciation algorithms, is likelyto remain silent.
In order to take this aspect into ac-count, we measure in each experiment the numberof words that can not be pronounced at all (the si-lence), and the percentage of phonemes and wordsthat are correctly transcribed amongst hose wordsthat have been pronounced at all (the precision).
Theaverage values for these measures are reported here-after.3.2 Pseudo-wordsAll but one pseudo-words of Glushko's test set couldbe pronounced by the paradigmatic cascades algo-rithm, and amongst the 69 pronunciation suggestedby our program, only 9 were uncorrect (that is, werenot proposed by human subjects in Glushko's ex-periments), yielding an overall correctness of 85.7%,and a precision of 87.3%.An important property of our algortihm is that itallows to precisely identify, for each pseudo-word,the lexical entries that have been analogized, i.e.whose pronunciation was used in the inferential pro-cess.
Looking at these analogs, it appears that threeof our errors are grounded on very sensible analo-gies, and provide us with pronunciations that seemat least plausible, even if they were not suggested inGlushko's experiments.
These were pild and bild,analogized with wild, and pornb, analogized withtomb.These results compare favorably well with the per-formances reported for other pronunciation by anal-ogy algorithms ((Damper and Eastmond, 1996) re-432ports very similai" correctness figures), especially ifone remembers that our results have been obtained,wilhout resorting to any kind of pre-alignment be-tween the graphemic and phonemic strin9s in thelea'icons.3.3 Lexical EntriesThis second series of experiment is intended toprovide us with more realistic evaluations of theparadigmatic ascade rnodeh Glushko's pseudo-words have been built by substituting the initialconsonant or existing monosyllabic words, and con-sl.itute theretore an over-simplistic test-bed.
Thenettalk dataset contains plurisyllabic words, com-plex derivatives, loan words, etc, and allows to testthe ability of our model to learn complex morpho-phonological phenomenas, notably vocalic alterna-tions and other kinds of phonologically conditionedroot a.llomorphy, that are very difficult to learn.With this new test set, the overall performancesof our algorithm averages at about 54.5% of en-tirely correct words, corresponding to a 76% perphoneme correctness.
If we keep the words thatcould not be pronounced at all (about 15% of thetest set) apart fi'oln the evaluation, the per word andper phoneme precision improve considerably, reach-ing respectively 65% and 93%.
Again, these pre-cision results compare relatively well with the re-suits achieved on the same corpus using other self-learning algorithms for grapheme-to-phoneme trma-scription (e.g.
(van den Bosch and Daelemans, 1993;Yvon, 1996a)), which, unlike ours, benefit fromthe knowledge of tile alignment between graphemicand phonemic strings.
Table 3 suimnaries the per-forma.uce (in terms of per word correctness, si-lence, and precision) of various other pronunciationsystems, namely PRONOUNCE (Dedina and Nus-baum, 1991), DEC (Torkolla, 1993), SMPA (Yvon,1!)96a).
All these models have been tested nsing ex-a.c(.ly the sanle evMual.ion procedure and data.
(see(Yvon, 1996b), which also contains an evalution per-formed with a French database suggesting that thish'arning strategy effectively applies to other lan-guages).System corr.
prec.
silenceDE(/', 56.67 56.67 0SMPA 63.96 64.24 0.42PRONOUNC.F, 56.56 56.75 0.32I)CP 54A9 63.95 14.80Table 3: A Comparatiw.
l~;valuation'\[a/)le 3 pinpoints the main weakness of our model,that is, its significant silence rate.
The careful ex-alnination of the words that cannot be pronouncedreveals that they are either loan words, which arevery isolated in an English lexicon, and .for whichno analog can be found; or complex morphologicalderivatives for which the search procedure is stoppedbefore the existing analog(s) can be reached.
Typicalexamples are: synergistically, timpani, hangdog,oasis, pemmican, to list just a few.
This suggeststhat the words which were not pronounced are notrandomly distributed.
Instead, they mostly belongto a linguistically homogeneous group, the group offoreign words, which, for lack of better evidence,should better be left silent, or processed by anotherpronnnciation procedure (for example a rule-basedsystem (Coker, Church, and Liberman, 1990)), thanuncorrectly analogized.Some complementary esults finally need to bementioned here, in relation to the size of lexicalneighbourhoods.
In fact, one of our main goal wasto define in a sensible way the concept of a lexicalneighbourhood: it is therefore important o checkthat our model manages to keep this neighbourhoodrelatively small.
Indeed, if this neighbourhood canbe quite large (typically 50 analogs) for short words,the number of analogs used in a pronunciation aver-ages at about 9.5, which proves that our definitionof a lexical ncighbourhood is sufficiently restrictive.4 D iscuss ion  and Perspect ives4.1 Re la ted  worksA large number of procedures aiming at the auto-matic discovery of pronunciation "rules" have beenproposed over the past few years: connectionistmodels (e.g.
(Sejnowski and Rosenberg, 1987)), tra-ditional symbolic machine learning techniques (in-duction of decision trees, k-nearest neighbours) e.g.
(Torkolla, 1993; van den Bosch and Daelemans,1993), as well as various recombination techniques(Yvon, 1996a).
In these models, orthographical cor-respondances are primarily viewed as resulting froma strict underlying phonographical system, whereeach grapheme ncodes exactly one phoneme.
Thisassumption is reflected by the possibility of align-ing on a one-to-one basis graphemic and phonemicstrings, and these models indeed use this kind ofalignment .o initiate learning.
Under this view, tileorthographical representation of individual words isstrongly subject to their phonological forms on anword per word basis.
The main task of a machine-learning algorithm is thus mainly to retrieve, ona statistical basis, these grapheme-phoneme corre-spondances, which are, in languages like French or433English, accidentally obscured by a multitude of ex-ceptional and idiosyncratic orrespondances.
Thereexists undoubtly strong historical evidences support-ing the view that the orthographical system of mosteuropean languages developped from a such phono-graphical system, and languages like Spanish or Ital-ian still offer examples of that kind of very regularorganization.Our model, which extends the proposals of (Coker,Church, and Liberman, 1990), and more recently,of (Federici, Pirrelli, and Yvon, 1995), entertains adifferent view of orthographical systems.
Even weif acknowledge the mostly phonographical organiza-tion of say, French orthography, we believe that thenmltiple deviations from a strict grapheme-phonemecorrespondance are best captured in a model whichweakens somehow the assumption of a strong de-pendancy between orthographical nd phonologicalrepresentations.
In our model, each domain has itsown organization, which is represented in the formof systematic (paradigmatic) set of oppositions andalternations.
In both domain however, this orga-nization is subject to the same paradigmatic prin-ciple, which makes it possible to represent he re-lationships between orthographical and phonologi-cal representations in the form of a statistical pair-ing between alternations.
Using this model, it be-comes possible to predict correctly the outcome inthe phonological domain of a given derivation in theorthographic domain, including patterns of vocalicalternations, which are notoriously difficult to modelusing a "rule-based" approach.4.2 Ach ievementsThe paradigmatic ascades model offers an origi-nal and new framework for extracting informationfrom large corpora.
In the particular context ofgrapheme-to-phoneme transcription, it provides uswith a more satisfying model of pronunciation byanalogy, which:?
gives a principled way to automatically learnlocal similarities that implicitely incorporate asubstantial knowledge of the morphological pro-cesses and of the phonotactic onstraints, bothin the graphemic and the phonemic domain.This has allowed us to precisely define and iden-tify the content of lexical neighbourhoods;?
achieves a very high precision without resortingto pre-aligned ata, and detects automaticMlythose words that are potentially the most dif-ficult to pronounce (especially foreign words).Interestingly, the ability of our model to pro-cess data which are not aligned makes it directlyapplicable to the reverse problem, i.e.
phoneme-to-grapheme conversion.is computationally tractable, even if extremelyressource-consuming i  the current version ofour algorithm.
The main trouble here comesfrom isolated words: for these words, the searchprocedure wastes a lot of time examining a verylarge number of very unlikely analogs, before re-alizing that there is no acceptable l xical neigh-bout.
This aspect definitely needs to be im-proved.
We intend to explore several directionsto improve this search: one possibility is to usea graphotactieal model (e.g.
a rt-gram model) inorder to make the pruning of the derivation treemore effective.
We expect such a model to biasthe search in favor of short words, which aremore represented than very long derivatives.Another possibility is to tag, during the learningstage, alternations with one or several morpho-syntactic labels expressing morphotactical re-strictions: this would restrict the domain of analternation to a certain class of words, and ac-cordingly reduce the expansion of the analogset.4.3 Perspect ivesThe paradigmatic cascades model achieves quite sat-isfactory generalization performances when evalu-ated in the task of pronouncing unknown words.Moreover, this model provides us with an effectiveway to define the lexical neighbourhood of a givenword, on the basis of "surface" (orthographical) localsimilarities.
It remains however to be seen how thismodel can be extended to take into account otherfactors which have been proven to influence analogi-cal processes.
For instance, frequency effects, whichtend to favor the more frequent lexical neighbours,need to be properly model, if we wish to make amore realistic account of the human performance inthe pronunciation task.In a more general perspective, tile notion of simi-larity between linguistic objects plays a central rolein many corpus-based natural language processingapplications.
This is especially obvious in the con-text of example-based learning techniques, where theinference of some unknown linguistics property of anew object is performed on the basis of the mostsimilar available xample(s).
The use of some kindof similarity measure has also demonstrated its effec-tiveness to circumvent the problem of data sparse-ness in the context of statistical language modeling.In this context, we believe that our model, whichis precisely capable of detecting local similarities in434lexicons, and to 16erform, on the basis of these sinai-larities~ a global inferential transfer of knowledge, isespecially well suited for a large range of NLP tasks.Encouraging results on the task of learning the En-glish past-tense forms have already l~een reported in(Yvon, 1996b), and we intend to continue to test thismodel on various other potentially relevant applica-tions, such as morpho-syntactical "guessing", part-of-speech tagging, etc.ReferencesCoker, Cecil H., Kenneth W. Church, and Mark Y.Liberman.
1990.
Morphology and rhyming: twopowerful alternatives to letter-to-sound rules.
InProceedings of the ESCA Conference on SpeechSynthesis, Autrans, France.Coltheart, Max.
1978.
Lexical access in simple read-ing tasks.
In G. Underwood, editor, Strategiesof information processing.
Academic Press, NewYork, pages 151-216.Coltheart, Max, Brent Curtis, Paul Atkins, andMichael Haller.
1993.
Models of reading aloud:dual route and parallel distributed processing ap-proaches.
Psychological Review, 100:589-608.Damper, Robert I. and John F. G. Eastmond.
1996.Pronuncing text by analogy.
In Proceedings ofthe seventeenth International Conference on Com-putational Linguistics (COLING'96), pages 268-273, Copenhagen, Denmark.de Saussure, Ferdinand.
1916.
Cours de Linguis-tique Ggn@rale.
Payot, Paris.Dedina, Michael J. and Howard C. Nusbaum.
1991.PRONOUNCE: a program for pronunciation byanalogy.
Computer Speech and Langage, 5:55-64.Federici, Stefano, Vito Pirrelli, and Franqois Yvon.1995.
Advances in analogy-based learning: falsefriends and exceptional items in pronunciation byparadigm-driven analogy.
In Proceedings of I J-CA I'95 workshop on 'New Approaches to Learningfor Natural Language Processing', pages 158-163,Montreal.Glushsko, J, R. 1981.
Principles for pronouncingprint: the psychology of phonography.
In A. M.Lesgold and C. A. Perfetti, editors, InteractiveProcesses in Reading, pages 61-84, Hillsdale, NewJersey.
Erlbaum.Lepage, Yves and Ando Shin-Ichi.
1996.
Saussuriananalogy : A theoretical account and its applica-tion.
In Proceedings of the seventeenth Interna-tional Conference on Computational Linguistics(COLING'96)~ pages 717-722, Copenhagen, Den-1Tlarl(.Pirrelli, Vito and Stefano Federici.
1994.
"Deriva-tional" paradigms in morphonology.
In Proceed-ings of the sixteenth International Conference onComputational Linguistics (COLING'94), Kyoto,Japan.Seidenberg, M. S. and James.
L. McClelland.
1989.A distributed, developnaental model of wordrecognition and naming.
Psychological review,96:523-568.Sejnowski, Terrence J. and Charles R. Rosenberg.1987.
Parrallel network that learn to pronounceEnglish text.
Complex Systems, 1:145-168.Sullivan, K.P.H and Robert I. Damper.
1992.
Novel-word pronunciation within a text-to-speech sys-tem.
In G~rard Bailly a.nd Christian Benoit, edi-tors, Talking Machines, pages 183-195.
North Hol-land.Torkolla, Karl.
1993.
An efficient way to learnEnglish grapheme-to-phoneme rules automati-cally.
In PTvceedings of the International Confer-ence on Acoustics, Speech and Signal Processing(ICASSP), volume 2, pages 199-202, Minneapo-lis, Apr.van den Bosch, Antal and Walter Daelemans.
1993.Data-oriented methods for grapheme-to-phonemeconversion.
In Proceedings of the European Chap-ter of the Association for Computational Linguis-tics (EACL), pages 45-53, Utrecht.Yvon, Francois.
1996a.
Grapheme-to-phonemeconversion using multiple unbounded overlappingchunks.
In Proceedings of the conference on NewMethods in Natural Language Processing (NeM-LaP II), pages 218-228, Ankara, Turkey.Yvon, Francois.
1996b.
Prononcer par analogie :motivations, formalisations et dvaluations.
Ph.D.thesis, Ecole Nationale Sup6.rieure des T@l~com-munications, Paris.435
