Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 114?119,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Lucene and Maximum Entropy Model Based Hedge Detection SystemLin ChenUniversity of Illinois at ChicagoChicago, IL, USAlin@chenlin.netBarbara Di EugenioUniversity of Illinois at ChicagoChicago, IL, USAbdieugen@uic.eduAbstractThis paper describes the approach tohedge detection we developed, in order toparticipate in the shared task at CoNLL-2010.
A supervised learning approach isemployed in our implementation.
Hedgecue annotations in the training data areused as the seed to build a reliable hedgecue set.
Maximum Entropy (MaxEnt)model is used as the learning technique todetermine uncertainty.
By making use ofApache Lucene, we are able to do fuzzystring match to extract hedge cues, andto incorporate part-of-speech (POS) tagsin hedge cues.
Not only can our systemdetermine the certainty of the sentence,but is also able to find all the containedhedges.
Our system was ranked third onthe Wikipedia dataset.
In later experi-ments with different parameters, we fur-ther improved our results, with a 0.612F-score on the Wikipedia dataset, and a0.802 F-score on the biological dataset.1 IntroductionA hedge is a mitigating device used to lessen theimpact of an utterance1.
As a very important wayto precisely express the degree of accuracy andtruth assessment in human communication, hedg-ing is widely used in both spoken and written lan-guages.
Detecting hedges in natural language textcan be very useful for areas like text mining andinformation extraction.
For example, in opinionmining, hedges can be used to assess the degreeof sentiment, and refine sentiment classes from{positive, negative, objective} to {positive, some-how positive, objective, somehow objective, nega-tive, somehow negative}.1http://en.wikipedia.org/wiki/Hedge(linguistics)Hedge detection related work has been con-ducted by several people.
Light et al (2004)started to do annotations on biomedicine articleabstracts, and conducted the preliminary work ofautomatic classification for uncertainty.
Medlockand Briscoe (2007) devised detailed guidelinesfor hedge annotations, and used a probabilisticweakly supervised learning approach to classifyhedges.
Ganter and Strube (2009) took Wikipediaarticles as training corpus, used weasel words?
fre-quency and syntactic patterns as features to clas-sify uncertainty.The rest of the paper is organized as follows.Section 2 shows the architecture of our system.Section 3 explains how we make use of ApacheLucene to do fuzzy string match and incorporatePOS tag in hedge cues and our method to gener-ate hedge cue candidates.
Section 4 describes thedetails of using MaxEnt model to classify uncer-tainty.
We present and discuss experiments andresults in section 5, and conclude in section 6.2 System ArchitectureOur system is divided into training and testingmodules.
The architecture of our system is shownin Figure 1.In the training module, we use the training cor-pus to learn a reliable hedge cue set with bal-anced support and confidence, then train a Max-Ent model for each hedge cue to classify the un-certainty for sentences matched by that hedge cue.In the testing module, the learned hedge cuesare used to match the sentences to classify, theneach matched sentence is classified using the cor-responding MaxEnt model.
A sentence will beclassified as uncertain if the MaxEnt model deter-mines it is.
Because of this design, our system isnot only able to check if a sentence is uncertain,but also can detect the contained hedges.114Testing DataLucene IndexerIndex with POSHedge CuesMore Cue?Get A Hedge CueGet Matched SentencesMore Sentence?MarkedUncertain?Get the MaxEnt ModelUncertain?Mark Sentece UncertaintyyesnonoyesGet A SentenceyesyesOutputMarkedUncertaintynoLucene IndexerTraining DataIndex with POSHedge CuesAnnotation ExtenderToken PrunerPOS Tag ReplacerConfidence?Support?MaxEnt ModelsMaxEnt TrainerHedge Cue Candidate GeneratorHedge CueCandidatesyesTrainging TestingFigure 1: System Architecture3 Learn Hedge CuesThe training data provided by CoNLL-2010shared task contain ?<ccue></ccue>?
annota-tions for uncertain sentences.
Most of the annota-tions are either too strict, which makes them hardto use to match other sentences, or too general,which means that most of the matched sentencesare not uncertain.Similar to how Liu (2007) measures the useful-ness of association rules, we use support and con-fidence to measure the usefulness of a hedge cue.Support is the ratio of sentences containing ahedge cue to all sentences.
Because in a train-ing dataset, the number of all the sentences is afixed constant, we only use the number of sen-tences containing the hedge cue as support, seeformula 1.
In the other part of this paper, sentencesmatched by hedge cues means sentences containshedge cues.
We use support to measure the degreeof generality of a hedge cue.sup = count of matched sentences (1)Confidence is the ratio of sentences which con-tain a hedge cue and are uncertain to all the sen-tences containing the hedge cue, as formula 2.We use confidence to measure the reliability fora word or phrase to be a hedge cue.conf = count of matched and uncertaincount of matched sentences (2)3.1 Usage of Apache LuceneApache Lucene2 is a full text indexing Java libraryprovided as an open source project of ApacheFoundation.
It provides flexible indexing andsearch capability for text documents, and it hasvery high performance.
To explain the integra-tion of Lucene into our implementation, we needto introduce several terms, some of which comefrom McCandless et al (2010).?
Analyzer: Raw texts are preprocessed beforebeing added to the index: text preprocessingcomponents such as tokenization, stop wordsremoval, and stemming are parts of an ana-lyzer.?
Document: A document represents a collec-tion of fields, it could be a web page, a textfile, or only a paragraph of an article.?
Field: A field represents a document or themeta-data associated with that document, likethe author, type, URL.
A field has a name anda value, and a bunch of options to control howLucene will index its value.?
Term: The very basic unit of a search.
It con-tains a field name and a value to search.2http://lucene.apache.org115?
Query: The root class used to do search uponan index.In our implementation, Lucene is used for thefollowing 3 purposes:?
Enable quick counting for combinations ofwords and POS tags.?
Store the training and testing corpus for fastcounting and retrieval.?
Allow gap between words or POS tags inhedge cues to match sentences.Lucene provides the capability to build cus-tomized analyzers for complex linguistics analy-sis.
Our customized Lucene analyzer employs to-kenizer and POS tagger from OpenNLP tools3 todo tokenization and POS tagging.
For every wordin the sentence, we put two Lucene tokens in thesame position, by setting up the second token?s Po-sitionIncremental attribute to be 0.For example, for sentence it is believed to bevery good, our analyzer will make Lucene store itas Figure 2 in its index.It to beis believed very goodPRP TO VBVBZ VBN RB VBN60 1 2 3 4 5Figure 2: Customized Tokenizer ExampleIndexing text in that way, we are able to matchsentences cross words and POS tags.
For example,the phrase it is believed will be matched by it is be-lieved, it is VBN, it VBZ believed.
This techniqueenables us to generalize a hedge cue.In our implementation, all the data for trainingand testing are indexed.
The indexing schema is: asentence is treated as a Lucene document; the con-tent of the sentence is analyzed by our customizedanalyzer; other information like sentence id, sen-tence position, uncertainty is stored as fields of thedocument.
In this way, we can query all thosefields, and when we find a match, we can easilyget al the information out just from the index.Lucene provides various types of queries tosearch the indexed content.
We use SpanNear-Query and BooleanQuery to search the matchedsentences for hedge cues.
We rely on SpanNear-Query?s feature of allowing positional restriction3http://opennlp.sourceforge.netwhen matching sentences.
When building a Span-NearQuery, we can specify the position gap al-lowed among the terms in the query.
We build aSpanNearQuery from a hedge cue, put each tokenas a term of the query, and set the position gap tobe 2.
Take Figure 3 as an example, because thegap between token is and said is 1, is less than thespecified gap setting 2, so It is widely said to begood will count as a match with hedge cue is said.Figure 3: SpanNearQuery Matching ExampleWe use BooleanQuery with nested SpanNear-Query and TermQuery to count uncertain sen-tences, then to calculate the confidence of a hedgecue.3.2 Hedge Cue Candidate GenerationWe firstly tried to use the token as the basic unit forhedge cues.
However, several pieces of evidencesuggest it is not appropriate.?
Low Coverage.
We only get 42 tokens inWikipedia training data, using 20, 0.4 as thethresholds for support and confidence.?
Irrelevant words or stop words with lowerthresholds.
When we use 5, 0.3 as the thresh-olds for coverage and confidence, we get 279tokens, however, words like is, his, musical,voters, makers appear in the list.We noticed that many phrases with similarstructures or fixed collocations appear very oftenin the annotations, like it is believed, it is thought,many of them, many of these and etc.
Based on thisobservation, we calculated the support and confi-dence for some examples, see table 1.Hedge Cue Sup.
Conf.it is believed 14 .93by some 30 .87many of 135 .65Table 1: Hedge Cue ExamplesWe decided to use the phrase or collocation asthe basic unit for hedge cues.
There are two prob-lems in using the original annotations as hedgecues:116?
High confidence but low coverage: annota-tions that contain proper nouns always havevery high confidence, usually 100%, how-ever, they have very low support.?
High coverage but low confidence: annota-tions with only one token are very frequent,but only a few of them result in enough con-fidence.To balance confidence and support, we built ourhedge cue candidate generator.
Its architecture ispresented in Figure 4.Cue AnnotationsCue CandidatesAnnotation ExtenderTokens > 1NOToken PrunerPOS Tag ReplacerYESFigure 4: Hedge Cue Candidate GeneratorThe three main components of the hedge cuecandidate generator are described below.Annotation Extender: When the input hedgecue annotation contains only 1 token, this compo-nent will be used.
It will generate 3 more hedgecue candidates by adding the surrounding tokens.We expect to discover candidates with higher con-fidence.Token Pruner: According to our observations,proper nouns rarely contribute to the uncertaintyof a sentence, and our Lucene based string match-ing method ensures that the matched sentences re-main matched after we remove tokens from theoriginal cue annotation.
So we remove propernouns in the original cue annotation to generatehedge cue candidates.
By using this component,we expect to extract hedge cues with higher sup-port.POS Tag Replacer: This component is used togeneralize similar phrases, by using POS tags toreplace the concrete words.
For example, we usethe POS tag VBN to replace believed in it is be-lieved to generate it is VBN.
Hence, when a sen-tence contains it is thought in the testing dataset,even if it is thought never appeared in the train-ing data set, we will still be able to match it andclassify it against the trained MaxEnt model.
Weexpect that this component will be able to increasesupport.
Due to the O(2n) time complexity, we didnot try the brute force approach to replace everyword, only the words with the POS tags in Table 2are replaced in the process.POS Description ExampleVBN past participle verb it is believedNNS plural common noun some countriesDT determiner some of thoseCD numeral, cardinal one of the bestTable 2: POS Tag Replacer ExamplesAfter hedge cue candidates are generated, weconvert them to Lucene queries to calculate theirconfidence and support.
We prune those that fallbelow the predefined confidence and support set-tings.4 Learn UncertaintyNot all the learned hedge cues have 100% uncer-tainty confidence, given a hedge cue, we need tolearn how to classify whether a matched sentenceis uncertain or not.
The classification model is,given a tuple of (Sentence, Hedge Cue), in whichthe sentence contains the hedge cue, we classify itto the outcome set {Certain, Uncertain}.MaxEnt is a general purpose machine learn-ing technique, it makes no assumptions in addi-tion to what we know from the data.
MaxEnt hasbeen widely used in Natural Language Processing(NLP) tasks like POS tagging, word sense disam-biguation, and proved its efficiency.
Due to Max-Ent?s capability to combine multiple and depen-dent knowledge sources, we employed MaxEnt asour machine learning model.
Features we used totrain the model include meta information featuresand collocation features.Meta Information Features include three fea-tures:?
Sentence Location: The location of the sen-tence in the article, whether in the title or inthe content.
We observed sentences in the ti-tle are rarely uncertain.?
Number of Tokens: The number of tokensin the sentence.
Title of article is usuallyshorter, and more likely to be certain.117?
Hedge Cue Location: The location ofmatched tokens in a sentence.
We considerthem to be in the beginning, if the first tokenof the matched part is the first token in thesentence; to be at the end, if the last token ofthe matched part is the last token of the sen-tence; otherwise, they are in the middle.
Wewere trying to use this feature as a simplifiedversion to model the syntactic role of hedgecues in sentences.Collocation Features include the word and POStag collocation features:?
Word Collocation: Using a window size of 5,extract all the word within that window, ex-cluding punctuation.?
POS Tag Collocation: Using a window sizeof 5, extract all the POS tags of tokens withinthat window, excluding punctuation.We use the OpenNLP MaxEnt4 Java library asthe MaxEnt trainer and classifier.
For each hedgecue, the training is iterated 100 times, with no cutoff threshold for events.5 Experiments and DiscussionWe first ran experiments to evaluate the perfor-mance of the entire system.
We used officialdataset as training and testing, with different con-fidence and support thresholds.
The result on offi-cial Wikipedia dataset is presented in Table 3.
Re-sult on the biological dataset is listed in Table 4.In the result tables, the first 2 columns are the con-fidence and support threshold; ?Cues?
is the num-ber of generated hedge cues; the last 3 columns arestandard classifier evaluation measures.Our submitted result used 0.35, 5 as the thresh-olds for confidence and support.
We officiallyplaced third on the Wikipedia dataset, with a0.5741 F-score, and third from last on the biolog-ical dataset, with a 0.7692 F-score.
In later ex-periments, we used different parameters, which re-sulted in a 0.03 F-score improvement.
We believethe big difference of ranking on different datasetscomes from the incomplete training.
Due to incor-rect estimation of running time, we only used thesmaller training file in our submitted biological re-sult.From Table 3 and 4, we can see that a higherconfidence threshold gives higher precision, and4http://maxent.sourceforge.netConf.
Sup.
Cues Prec.
Recall F0.410 360 0.658 0.561 0.60615 254 0.672 0.534 0.59520 186 0.682 0.508 0.5820.4510 293 0.7 0.534 0.60615 190 0.717 0.503 0.59120 137 0.732 0.476 0.5770.55 480 0.712 0.536 0.61210 222 0.736 0.492 0.59015 149 0.746 0.468 0.57520 112 0.758 0.443 0.559Table 3: Evaluation Result on Wikipedia DatasetConf.
Sup.
Cues Prec.
Recall F0.410 330 0.68 0.884 0.76915 229 0.681 0.861 0.7620 187 0.679 0.842 0.7520.4510 317 0.689 0.878 0.77215 220 0.69 0.857 0.76420 179 0.688 0.838 0.7560.55 586 0.724 0.899 0.80210 297 0.742 0.841 0.78815 206 0.742 0.819 0.77920 169 0.74 0.8 0.769Table 4: Evaluation Result on Biological Dataseta lower support threshold leads to higher recall.Since the lower support threshold could generatemore hedge cues, it will generate less training in-stances for hedge cues with both low confidenceand support, which affects the performance of theMaxEnt classifier.
In both datasets, it appears that0.5 and 5 are the best thresholds for confidenceand support, respectively.Beyond the performance of the entire system,our hedge cue generator yields very promising re-sults.
Using the best parameters we just notedabove, our hedge cue generator generates 52 hedgecues with confidence 100% on the Wikipediadataset, and 332 hedge cues in the biologicaldataset.
Some hedge cue examples are shown inTable 5.We also ran experiments to verify the perfor-mance of our MaxEnt classifier.
We used the samesetting of datasets as for the system performanceevaluation.
Given a hedge cue, we extracted allthe matched sentences from the training set to traina MaxEnt classifier, and used it to classify thematched sentences by the hedge cue in testing set.118Hedge Cue Sup.
Conf.
TestSize Prec.
Recall Findicated that 63 0.984 6 1.0 1.0 1.0by some 30 0.867 29 0.966 1.000 0.983are considered 29 0.724 10 0.750 0.857 0.800some of NNS 62 0.613 27 1.000 0.778 0.875the most JJ 213 0.432 129 0.873 0.475 0.615Table 6: MaxEnt Classifier PerformanceHedge Cue Conf.
Sup.probably VBN 1.0 21DT probably 1.0 15many NNS believe 1.0 10NNS suggested DT 1.0 248results suggest 1.0 122has VBN widely VBN 1.0 10Table 5: Generated Hedge Cue ExamplesTable 6 shows the results, the hedge cues weremanually chosen with relative higher support.We can see that the performance of the MaxEntclassifier correlates tightly with confidence andsupport.
Higher confidence means a more accu-rate detection for a phrase to be hedge cue, whilehigher support means more training instances forthe classifier: the best strategy would be to findhedge cues with both high confidence and support.While experimenting with the system, we foundseveral potential improvements.?
Normalize words.
Take the word suggest asan example.
In the generated hedge cues,we found that its other forms are everywhere,like it suggested, NNS suggests a, and DTsuggesting that.
As we put POS tags intoLucene index, we can normalize words totheir base forms using a morphology parser,and put base forms into index.
After that, thequery with suggest will match all the forms.?
Use more sophisticated features to train theMaxEnt classifier.
Currently we only useshallow linguistics information as features,however we noticed that the role of the phrasecould be very important to decide whether itindicates uncertainty.
We can deep parse sen-tences, extract the role information, and addit to the feature list of classifier.6 ConclusionIn this paper, we described the hedge detectionsystem we developed to participate in the sharedtask of CoNLL-2010.
Our system uses a heuristiclearner to learn hedge cues, and uses MaxEnt asits machine learning model to classify uncertaintyfor sentences matched by hedge cues.
Hedge cuesin our system include both words and POS tags,which make themmore general.
Apache Lucene isintegrated into our system to efficiently run com-plex linguistic queries on the corpus.AcknowledgmentsThis work is supported by award IIS-0905593from the National Science Foundation.ReferencesViola Ganter and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 173?176, Suntec, Singapore,August.
Association for Computational Linguistics.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The language of bioscience: Facts, specula-tions, and statements in between.
In Proceedings ofBioLink 2004 Workshop on Linking Biological Lit-erature, Ontologies and Databases: Tools for Users,pages 17?24, Boston, Mass, May.Bing Liu.
2007.
Web data mining: exploring hyper-links, contents, and usage data.
Springer.Michael McCandless, Erik Hatcher, and Otis Gospod-neti.
2010.
Lucene in action.
Manning PublicationsCo, 2nd edition.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 992?999.
Association for Computational Lin-guistics, June.119
