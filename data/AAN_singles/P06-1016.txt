Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 121?128,Sydney, July 2006. c?2006 Association for Computational LinguisticsModeling Commonality among Related Classes in Relation ExtractionZhou GuoDong      Su Jian      Zhang MinInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613Email: {zhougd, sujian, mzhang}@i2r.a-star.edu.sgAbstractThis paper proposes a novel hierarchical learn-ing strategy to deal with the data sparsenessproblem in relation extraction by modeling thecommonality among related classes.
For eachclass in the hierarchy either manually prede-fined or automatically clustered, a linear dis-criminative function is determined in a top-down way using a perceptron algorithm withthe lower-level weight vector derived from theupper-level weight vector.
As the upper-levelclass normally has much more positive train-ing examples than the lower-level class, thecorresponding linear discriminative functioncan be determined more reliably.
The upper-level discriminative function then can effec-tively guide the discriminative function learn-ing in the lower-level, which otherwise mightsuffer from limited training data.
Evaluationon the ACE RDC 2003 corpus shows that thehierarchical strategy much improves the per-formance by 5.6 and 5.1 in F-measure onleast- and medium- frequent relations respec-tively.
It also shows that our system outper-forms the previous best-reported system by 2.7in F-measure on the 24 subtypes using thesame feature set.1 IntroductionWith the dramatic increase in the amount of tex-tual information available in digital archives andthe WWW, there has been growing interest intechniques for automatically extracting informa-tion from text.
Information Extraction (IE) issuch a technology that IE systems are expectedto identify relevant information (usually of pre-defined types) from text documents in a certaindomain and put them in a structured format.According to the scope of the ACE program(ACE 2000-2005), current research in IE hasthree main objectives: Entity Detection andTracking (EDT), Relation Detection andCharacterization (RDC), and Event Detectionand Characterization (EDC).
This paper willfocus on the ACE RDC task, which detects andclassifies various semantic relations between twoentities.
For example, we want to determinewhether a person is at a location, based on theevidence in the context.
Extraction of semanticrelationships between entities can be very usefulfor applications such as question answering, e.g.to answer the query ?Who is the president of theUnited States?
?.One major challenge in relation extraction isdue to the data sparseness problem (Zhou et al2005).
As the largest annotated corpus in relationextraction, the ACE RDC 2003 corpus showsthat different subtypes/types of relations aremuch unevenly distributed and a few relationsubtypes, such as the subtype ?Founder?
underthe type ?ROLE?, suffers from a small amount ofannotated data.
Further experimentation in thispaper (please see Figure 2) shows that most rela-tion subtypes suffer from the lack of the trainingdata and fail to achieve steady performance giventhe current corpus size.
Given the relative largesize of this corpus, it will be time-consuming andvery expensive to further expand the corpus witha reasonable gain in performance.
Even if we cansomehow expend the corpus and achieve steadyperformance on major relation subtypes, it willbe still far beyond practice for those minor sub-types given the much unevenly distributionamong different relation subtypes.
While variousmachine learning approaches, such as generativemodeling (Miller et al2000), maximum entropy(Kambhatla 2004) and support vector machines(Zhao and Grisman 2005; Zhou et al2005), havebeen applied in the relation extraction task, noexplicit learning strategy is proposed to deal withthe inherent data sparseness problem caused bythe much uneven distribution among differentrelations.This paper proposes a novel hierarchicallearning strategy to deal with the data sparsenessproblem by modeling the commonality amongrelated classes.
Through organizing variousclasses hierarchically, a linear discriminativefunction is determined for each class in a top-down way using a perceptron algorithm with thelower-level weight vector derived from the up-per-level weight vector.
Evaluation on the ACERDC 2003 corpus shows that the hierarchical121strategy achieves much better performance thanthe flat strategy on least- and medium-frequentrelations.
It also shows that our system based onthe hierarchical strategy outperforms the previ-ous best-reported system.The rest of this paper is organized as follows.Section 2 presents related work.
Section 3describes the hierarchical learning strategy usingthe perceptron algorithm.
Finally, we presentexperimentation in Section 4 and conclude thispaper in Section 5.2 Related WorkThe relation extraction task was formulated atMUC-7(1998).
With the increasing popularity ofACE, this task is starting to attract more andmore researchers within the natural languageprocessing and machine learning communities.Typical works include Miller et al(2000), Ze-lenko et al(2003), Culotta and Sorensen (2004),Bunescu and Mooney (2005a), Bunescu andMooney  (2005b), Zhang et al(2005), Roth andYih (2002), Kambhatla (2004), Zhao and Grisman(2005) and Zhou et al(2005).Miller et al(2000) augmented syntactic fullparse trees with semantic information of entitiesand relations, and built generative models to in-tegrate various tasks such as POS tagging, namedentity recognition, template element extractionand relation extraction.
The problem is that suchintegration may impose big challenges, e.g.
theneed of a large annotated corpus.
To overcomethe data sparseness problem, generative modelstypically applied some smoothing techniques tointegrate different scales of contexts in parameterestimation, e.g.
the back-off approach in Milleret al(2000).Zelenko et al(2003) proposed extracting re-lations by computing kernel functions betweenparse trees.
Culotta and Sorensen (2004) extendedthis work to estimate kernel functions betweenaugmented dependency trees and achieved F-measure of 45.8 on the 5 relation types in theACE RDC 2003 corpus1.
Bunescu and Mooney(2005a) proposed a shortest path dependencykernel.
They argued that the information tomodel a relationship between two entities can betypically captured by the shortest path betweenthem in the dependency graph.
It achieved the F-measure of 52.5 on the 5 relation types in theACE RDC 2003 corpus.
Bunescu and Mooney(2005b) proposed a subsequence kernel and ap-1 The ACE RDC 2003 corpus defines 5/24 relationtypes/subtypes between 4 entity types.plied it in protein interaction and ACE relationextraction tasks.
Zhang et al(2005) adopted clus-tering algorithms in unsupervised relation extrac-tion using tree kernels.
To overcome the datasparseness problem, various scales of sub-treesare applied in the tree kernel computation.
Al-though tree kernel-based approaches are able toexplore the huge implicit feature space withoutmuch feature engineering, further research workis necessary to make them effective and efficient.Comparably, feature-based approachesachieved much success recently.
Roth and Yih(2002) used the SNoW classifier to incorporatevarious features such as word, part-of-speech andsemantic information from WordNet, and pro-posed a probabilistic reasoning approach to inte-grate named entity recognition and relationextraction.
Kambhatla (2004) employed maxi-mum entropy models with features derived fromword, entity type, mention level, overlap, de-pendency tree, parse tree and achieved F-measure of 52.8 on the 24 relation subtypes inthe ACE RDC 2003 corpus.
Zhao and Grisman(2005) 2  combined various kinds of knowledgefrom tokenization, sentence parsing and deepdependency analysis through support vector ma-chines and achieved F-measure of 70.1 on the 7relation types of the ACE RDC 2004 corpus3.Zhou et al(2005) further systematically exploreddiverse lexical, syntactic and semantic featuresthrough support vector machines and achieved F-measure of 68.1 and 55.5 on the 5 relation typesand the 24 relation subtypes in the ACE RDC2003 corpus respectively.
To overcome the datasparseness problem, feature-based approachesnormally incorporate various scales of contextsinto the feature vector extensively.
These ap-proaches then depend on adopted learning algo-rithms to weight and combine each featureeffectively.
For example, an exponential modeland a linear model are applied in the maximumentropy models and support vector machines re-spectively to combine each feature via thelearned weight vector.In summary, although various approacheshave been employed in relation extraction, theyimplicitly attack the data sparseness problem byusing features of different contexts in feature-based approaches or including different sub-2 Here, we classify this paper into feature-based ap-proaches since the feature space in the kernels ofZhao and Grisman (2005) can be easily representedby an explicit feature vector.3 The ACE RDC 2004 corpus defines 7/27 relationtypes/subtypes between 7 entity types.122structures in kernel-based approaches.
Until now,there are no explicit ways to capture the hierar-chical topology in relation extraction.
Currently,all the current approaches apply the flat learningstrategy which equally treats training examplesin different relations independently and ignorethe commonality among different relations.
Thispaper proposes a novel hierarchical learningstrategy to resolve this problem by consideringthe relatedness among different relations andcapturing the commonality among related rela-tions.
By doing so, the data sparseness problemcan be well dealt with and much better perform-ance can be achieved, especially for those rela-tions with small amounts of annotated examples.3 Hierarchical Learning StrategyTraditional classifier learning approaches applythe flat learning strategy.
That is, they equallytreat training examples in different classesindependently and ignore the commonalityamong related classes.
The flat strategy will notcause any problem when there are a large amountof training examples for each class, since, in thiscase, a classifier learning approach can alwayslearn a nearly optimal discriminative function foreach class against the remaining classes.
How-ever, such flat strategy may cause big problemswhen there is only a small amount of trainingexamples for some of the classes.
In this case, aclassifier learning approach may fail to learn areliable (or nearly optimal) discriminative func-tion for a class with a small amount of trainingexamples, and, as a result, may significantly af-fect the performance of the class or even theoverall performance.To overcome the inherent problems in theflat strategy, this paper proposes a hierarchicallearning strategy which explores the inherentcommonality among related classes through aclass hierarchy.
In this way, the training exam-ples of related classes can help in learning a reli-able discriminative function for a class with onlya small amount of training examples.
To reducecomputation time and memory requirements, wewill only consider linear classifiers and apply thesimple and widely-used perceptron algorithm forthis purpose with more options open for futureresearch.
In the following, we will first introducethe perceptron algorithm in linear classifierlearning, followed by the hierarchical learningstrategy using the perceptron algorithm.
Finally,we will consider several ways in building theclass hierarchy.3.1 Perceptron Algorithm_______________________________________Input:  the initial weight vector w , the trainingexample sequenceTtYXyx tt ...,2,1,),( =??
and the number ofthe maximal iterations N (e.g.
10 in thispaper) of the training sequence4Output: the weight vector w  for the lineardiscriminative function  xwf ?=BEGINww =1REPEAT for t=1,2,?,T*N1.
Receive the instance nt Rx ?2.
Compute the output ttt xwo ?=3.
Give the prediction )( tt osigny =?4.
Receive the desired label }1,1{ +??ty5.
Update the hypothesis according tottttt xyww ?+=+1            (1)where 0=t?
if the margin of tw  at thegiven example ),( tt yx  0>?
ttt xwyand 1=t?
otherwiseEND REPEATReturn 5/41*?
?=+=NNiiTwwEND BEGIN_______________________________________Figure 1: the perceptron algorithmThis section first deals with binary classificationusing linear classifiers.
Assume an instance spacenRX =  and a binary label space }1,1{ +?=Y .With any weight vector nRw?
and a giveninstance nRx?
, we associate a linear classifierwh  with a linear discriminative function5xwxf ?=)(  by )()( xwsignxhw ?=  , where1)( ?=?
xwsign  if 0<?
xw  and 1)( +=?
xwsignotherwise.
Here, the margin of w  at ),( tt yx  isdefined as tt xwy ?
.
Then if the margin is positive,we have a correct prediction with tw yxh =)( , andif the margin is negative, we have an error withtw yxh ?
)( .
Therefore, given a sequence oftraining examples TtYXyx tt ...,2,1,),( =??
,linear classifier learning attemps to find a weightvector w  that achieves a positive margin on asmany examples as possible.4 The training example sequence is feed N times forbetter performance.
Moreover, this number can con-trol the maximal affect a training example can pose.This is similar to the regulation parameter C inSVM, which affects the trade-off between complex-ity and proportion of non-separable examples.
As aresult, it can be used to control over-fitting androbustness.5 )( xw ?
denotes the dot product of the weight vectornRw?
and a given instance nRx?
.123The well-known perceptron algorithm, asshown in Figure 1, belongs to online learning oflinear classifiers, where the learning algorithmrepresents its t -th hyposthesis by a weight vectornt Rw ?
.
At trial t , an online algorithm receivesan instance nt Rx ?
, makes its prediction)( ttt xwsigny ?=?and receives the desired label}1,1{ +?
?ty .
What distinguishes different onlinealgorithms is how they update tw  into 1+tw  basedon the example ),( tt yx  received at trial t .
Inparticular, the perceptron algorithm updates thehypothesis by adding a scalar multiple of theinstance, as shown in Equation 1 of Figure 1,when there is an error.
Normally, the tradictionalperceptron algorithm initializes the hypothesis asthe zero vector 01 =w .
This is usually the mostnatural choice, lacking any other preference.SmoothingIn order to further improve the performance, weiteratively feed the training examples for a possi-ble better discriminative function.
In this paper,we have set the maximal iteration number to 10for both efficiency and stable performance andthe final weight vector in the discriminative func-tion is averaged over those of the discriminativefunctions in the last few iterations (e.g.
5 in thispaper).BaggingOne more problem with any online classifierlearning algorithm, including the perceptron al-gorithm, is that the learned discriminative func-tion somewhat depends on the feeding order ofthe training examples.
In order to eliminate suchdependence and further improve the perform-ance, an ensemble technique, called bagging(Breiman 1996), is applied in this paper.
In bag-ging, the bootstrap technique is first used to buildM (e.g.
10 in this paper) replicate sample sets byrandomly re-sampling with replacement from thegiven training set repeatedly.
Then, each trainingsample set is used to train a certain discrimina-tive function.
Finally, the final weight vector inthe discriminative function is averaged overthose of the M discriminative functions in theensemble.Multi-Class ClassificationBasically, the perceptron algorithm is only forbinary classification.
Therefore, we must extendthe perceptron algorithms to multi-classclassification, such as the ACE RDC task.
Forefficiency, we apply the one vs. others strategy,which builds K classifiers so as to separate oneclass from all others.
However, the outputs forthe perceptron algorithms of different classesmay be not directly comparable since anypositive scalar multiple of the weight vector willnot affect the actual prediction of a perceptronalgorithm.
For comparability, we map theperceptron algorithm output into the probabilityby using an additional sigmoid model:)exp(11)|1(BAffyp ++==          (2)where xwf ?=  is the output of a perceptronalgorithm and the coefficients A & B are to betrained using the model trust alorithm asdescribed in Platt (1999).
The final decision of aninstance in multi-class classification isdetermined by the class which has the maximalprobability from the corresponding perceptronalgorithm.3.2 Hierarchical Learning Strategy using thePerceptron AlgorithmAssume we have a class hierarchy for a task, e.g.the one in the ACE RDC 2003 corpus as shownin Table 1 of Section 4.1.
The hierarchical learn-ing strategy explores the inherent commonalityamong related classes in a top-down way.
Foreach class in the hierarchy, a linear discrimina-tive function is determined in a top-down waywith the lower-level weight vector derived fromthe upper-level weight vector iteratively.
This isdone by initializing the weight vector in trainingthe linear discriminative function for the lower-level class as that of the upper-level class.
Thatis, the lower-level discriminative function has thepreference toward the discriminative function ofits upper-level class.
For an example, let?s lookat the training of the ?Located?
relation subtypein the class hierarchy as shown in Table 1:1) Train the weight vector of the lineardiscriminative function for the ?YES?relation vs. the ?NON?
relation with theweight vector initialized as the zero vector.2) Train the weight vector of the lineardiscriminative function for the ?AT?
relationtype vs. all the remaining relation types(including the ?NON?
relation) with theweight vector initialized as the weight vectorof the linear discriminative function for the?YES?
relation vs. the ?NON?
relation.3) Train the weight vector of the lineardiscriminative function for the ?Located?relation subtype vs. all the remaining relationsubtypes under all the relation types(including the ?NON?
relation) with the124weight vector initialized as the weight vectorof the linear discriminative function for the?AT?
relation type vs. all the remainingrelation types.4) Return the above trained weight vector as thediscriminatie function for the ?Located?relation subtype.In this way, the training examples in differ-ent classes are not treated independently anymore, and the commonality among relatedclasses can be captured via the hierarchical learn-ing strategy.
The intuition behind this strategy isthat the upper-level class normally has morepositive training examples than the lower-levelclass so that the corresponding linear discrimina-tive function can be determined more reliably.
Inthis way, the training examples of related classescan help in learning a reliable discriminativefunction for a class with only a small amount oftraining examples in a top-down way and thusalleviate its data sparseness problem.3.3 Building the Class HierarchyWe have just described the hierarchical learningstrategy using a given class hierarchy.
Normally,a rough class hierarchy can be given manuallyaccording to human intuition, such as the one inthe ACE RDC 2003 corpus.
In order to exploremore commonality among sibling classes, wemake use of binary hierarchical clustering forsibling classes at both lowest and all levels.
Thiscan be done by first using the flat learning strat-egy to learn the discriminative functions for indi-vidual classes and then iteratively combining thetwo most related classes using the cosine similar-ity function between their weight vectors in abottom-up way.
The intuition is that relatedclasses should have similar hyper-planes to sepa-rate from other classes and thus have similarweight vectors.?
Lowest-level hybrid: Binary hierarchicalclustering is only done at the lowest levelwhile keeping the upper-level class hierar-chy.
That is, only sibling classes at the low-est level are hierarchically clustered.?
All-level hybrid: Binary hierarchical cluster-ing is done at all levels in a bottom-up way.That is, sibling classes at the lowest level arehierarchically clustered first and then siblingclasses at the upper-level.
In this way, the bi-nary class hierarchy can be built iterativelyin a bottom-up way.4 ExperimentationThis paper uses the ACE RDC 2003 corpus pro-vided by LDC to train and evaluate the hierarchi-cal learning strategy.
Same as Zhou et al(2005),we only model explicit relations and explicitlymodel the argument order of the two mentionsinvolved.4.1 Experimental SettingType Subtype Freq Bin TypeAT Based-In 347 MediumLocated 2126 LargeResidence 308 MediumNEAR Relative-Location 201 MediumPART Part-Of 947 LargeSubsidiary 355 MediumOther 6 SmallROLE Affiliate-Partner 204 MediumCitizen-Of 328 MediumClient 144 SmallFounder 26 SmallGeneral-Staff 1331 LargeManagement 1242 LargeMember 1091 LargeOwner 232 MediumOther 158 SmallSOCIAL Associate 91 SmallGrandparent 12 SmallOther-Personal 85 SmallOther-Professional 339 MediumOther-Relative 78 SmallParent 127 SmallSibling 18 SmallSpouse 77 SmallTable 1: Statistics of relation types and subtypesin the training data of the ACE RDC 2003 corpus(Note: According to frequency, all the subtypesare divided into three bins: large/ middle/ small,with 400 as the lower threshold for the large binand 200 as the upper threshold for the small bin).The training data consists of 674 documents(~300k words) with 9683 relation exampleswhile the held-out testing data consists of 97documents (~50k words) with 1386 relation ex-amples.
All the experiments are done five timeson the 24 relation subtypes in the ACE corpus,except otherwise specified, with the final per-formance averaged using the same re-samplingwith replacement strategy as the one in the bag-ging technique.
Table 1 lists various types andsubtypes of relations for the ACE RDC 2003corpus, along with their occurrence frequency inthe training data.
It shows that this corpus suffersfrom a small amount of annotated data for a fewsubtypes such as the subtype ?Founder?
underthe type ?ROLE?.For comparison, we also adopt the same fea-ture set as Zhou et al(2005): word, entity type,125mention level, overlap, base phrase chunking,dependency tree, parse tree and semantic infor-mation.4.2 Experimental ResultsTable 2 shows the performance of the hierarchi-cal learning strategy using the existing class hier-archy in the given ACE corpus and itscomparison with the flat learning strategy, usingthe perceptron algorithm.
It shows that the purehierarchical strategy outperforms the pure flatstrategy by 1.5 (56.9 vs. 55.4) in F-measure.
Italso shows that further smoothing and baggingimprove the performance of the hierarchical andflat strategies by 0.6 and 0.9 in F-measure re-spectively.
As a result, the final hierarchicalstrategy achieves F-measure of 57.8 and outper-forms the final flat strategy by 1.8 in F-measure.Strategies  P R FFlat 58.2 52.8 55.4Flat+Smoothing 58.9 53.1 55.9Flat+Bagging 59.0 53.1 55.9Flat+Both 59.1 53.2 56.0Hierarchical 61.9 52.6 56.9Hierarchical+Smoothing 62.7 53.1 57.5Hierarchical+Bagging 62.9 53.1 57.6Hierarchical+Both 63.0 53.4 57.8Table 2: Performance of the hierarchical learningstrategy using the existing class hierarchy and itscomparison with the flat learning strategyClass Hierarchies P R FExisting 63.0 53.4 57.8Entirely Automatic 63.4 53.1 57.8Lowest-level Hybrid 63.6 53.5 58.1All-level Hybrid 63.6 53.6 58.2Table 3: Performance of the hierarchical learningstrategy using different class hierarchiesTable 3 compares the performance of the hi-erarchical learning strategy using different classhierarchies.
It shows that, the lowest-level hybridapproach, which only automatically updates theexisting class hierarchy at the lowest level, im-proves the performance by 0.3 in F-measurewhile further updating the class hierarchy at up-per levels in the all-level hybrid approach onlyhas very slight effect.
This is largely due to thefact that the major data sparseness problem oc-curs at the lowest level, i.e.
the relation subtypelevel in the ACE corpus.
As a result, the finalhierarchical learning strategy using the class hi-erarchy built with the all-level hybrid approachachieves F-measure of 58.2 in F-measure, whichoutperforms the final flat strategy by 2.2 in F-measure.
In order to justify the usefulness of ourhierarchical learning strategy when a rough classhierarchy is not available and difficult to deter-mine manually, we also experiment using en-tirely automatically built class hierarchy (usingthe traditional binary hierarchical clustering algo-rithm and the cosine similarity measurement)without considering the existing class hierarchy.Table 3 shows that using automatically builtclass hierarchy performs comparably with usingonly the existing one.With the major goal of resolving the datasparseness problem for the classes with a smallamount of training examples, Table 4 comparesthe best-performed hierarchical and flat learningstrategies on the relation subtypes of differenttraining data sizes.
Here, we divide various rela-tion subtypes into three bins: large/middle/small,according to their available training data sizes.For the ACE RDC 2003 corpus, we use 400 asthe lower threshold for the large bin6 and 200 asthe upper threshold for the small bin7.
As a re-sult, the large/medium/small bin includes 5/8/11relation subtypes, respectively.
Please see Table1 for details.
Table 4 shows that the hierarchicalstrategy outperforms the flat strategy by1.0/5.1/5.6 in F-measure on thelarge/middle/small bin respectively.
This indi-cates that the hierarchical strategy performsmuch better than the flat strategy for thoseclasses with a small or medium amount of anno-tated examples although the hierarchical strategyonly performs slightly better by 1.0 and 2.2 in F-measure than the flat strategy on those classeswith a large size of annotated corpus and on allclasses as a whole respectively.
This suggeststhat the proposed hierarchical strategy can welldeal with the data sparseness problem in theACE RDC 2003 corpus.An interesting question is about the similar-ity between the linear discriminative functionslearned using the hierarchical and flat learningstrategies.
Table 4 compares the cosine similari-ties between the weight vectors of the linear dis-criminative functions using the two strategies fordifferent bins, weighted by the training data sizes6 The reason to choose this threshold is that no rela-tion subtype in the ACE RC 2003 corpus has train-ing examples in between 400 and 900.7 A few minor relation subtypes only have very fewexamples in the testing set.
The reason to choosethis threshold is to guarantee a reasonable number oftesting examples in the small bin.
For the ACE RC2003 corpus, using 200 as the upper threshold willfill the small bin with about 100 testing exampleswhile using 100 will include too few testing exam-ples for reasonable performance evaluation.126of different relation subtypes.
It shows that thelinear discriminative functions learned using thetwo strategies are very similar (with the cosinesimilarity 0.98) for the relation subtypes belong-ing to the large bin while the linear discrimina-tive functions learned using the two strategies arenot for the relation subtypes belonging to themedium/small bin with the cosine similarity0.92/0.81 respectively.
This means that the use ofthe hierarchical strategy over the flat strategyonly has very slight change on the linear dis-criminative functions for those classes with alarge amount of annotated examples while itseffect on those with a small amount of annotatedexamples is obvious.
This contributes to and ex-plains (the degree of) the performance differencebetween the two strategies on the different train-ing data sizes as shown in Table 4.Due to the difficulty of building a large an-notated corpus, another interesting question isabout the learning curve of the hierarchical learn-ing strategy and its comparison with the flatlearning strategy.
Figure 2 shows the effect ofdifferent training data sizes for some major rela-tion subtypes while keeping all the training ex-amples of remaining relation subtypes.
It showsthat the hierarchical strategy performs much bet-ter than the flat strategy when only a smallamount of training examples is available.
It alsoshows that the hierarchical strategy can achievestable performance much faster than the flatstrategy.
Finally, it shows that the ACE RDC2003 task suffers from the lack of training exam-ples.
Among the three major relation subtypes,only the subtype ?Located?
achieves steady per-formance.Finally, we also compare our system with theprevious best-reported systems, such as Kamb-hatla  (2004) and Zhou et al(2005).
Table 5shows that our system outperforms the previousbest-reported system by 2.7 (58.2 vs. 55.5) in F-measure, largely due to the gain in recall.
It indi-cates that, although support vector machines andmaximum entropy models always perform betterthan the simple perceptron algorithm in most (ifnot all) applications, the hierarchical learningstrategy using the perceptron algorithm can eas-ily overcome the difference and outperforms theflat learning strategy using the overwhelmingsupport vector machines and maximum entropymodels in relation extraction, at least on the ACERDC 2003 corpus.Large Bin (0.98) Middle Bin (0.92) Small Bin (0.81) Bin Type(cosine similarity)P R F P R F P R FFlat Strategy 62.3 61.9 62.1 60.8 38.7 47.3 33.0 21.7 26.2Hierarchical Strategy 66.4 60.2 63.1 67.6 42.7 52.4 40.2 26.3 31.8Table 4: Comparison of the hierarchical and flat learning strategies on the relation subtypes of differ-ent training data sizes.
Notes: the figures in the parentheses indicate the cosine similarities betweenthe weight vectors of the linear discriminative functions learned using the two strategies.10203040506070200400600800100012001400160018002000Training Data SizeF-measureHS: General-StaffFS: General-StaffHS: Part-OfFS: Part-OfHS: LocatedFS: LocatedFigure 2: Learning curve of the hierarchical strategy and its comparison with the flat strategy for somemajor relation subtypes (Note: FS for the flat strategy and HS for the hierarchical strategy)Performance SystemP R FOur: Perceptron Algorithm + Hierarchical Strategy 63.6 53.6 58.2Zhou et al(2005): SVM + Flat Strategy 63.1 49.5 55.5Kambhatla (2004): Maximum Entropy + Flat Strategy 63.5 45.2 52.8Table 5: Comparison of our system with other best-reported systems1275 ConclusionThis paper proposes a novel hierarchical learningstrategy to deal with the data sparseness problemin relation extraction by modeling the common-ality among related classes.
For each class in aclass hierarchy, a linear discriminative functionis determined in a top-down way using the per-ceptron algorithm with the lower-level weightvector derived from the upper-level weight vec-tor.
In this way, the upper-level discriminativefunction can effectively guide the lower-leveldiscriminative function learning.
Evaluation onthe ACE RDC 2003 corpus shows that the hier-archical strategy performs much better than theflat strategy in resolving the critical data sparse-ness problem in relation extraction.In the future work, we will explore the hier-archical learning strategy using other machinelearning approaches besides online classifierlearning approaches such as the simple percep-tron algorithm applied in this paper.
Moreover,just as indicated in Figure 2, most relation sub-types in the ACE RDC 2003 corpus (arguablythe largest annotated corpus in relation extrac-tion) suffer from the lack of training examples.Therefore, a critical research in relation extrac-tion is how to rely on semi-supervised learningapproaches (e.g.
bootstrap) to alleviate its de-pendency on a large amount of annotated trainingexamples and achieve better and steadier per-formance.
Finally, our current work is done whenNER has been perfectly done.
Therefore, itwould be interesting to see how imperfect NERaffects the performance in relation extraction.This will be done by integrating the relation ex-traction system with our previously developedNER system as described in Zhou and Su (2002).ReferencesACE.
(2000-2005).
Automatic Content Extraction.http://www.ldc.upenn.edu/Projects/ACE/Bunescu R. & Mooney R.J. (2005a).
A shortestpath dependency kernel for relation extraction.HLT/EMNLP?2005: 724-731.
6-8 Oct 2005.Vancover, B.C.Bunescu R. & Mooney R.J. (2005b).
SubsequenceKernels for Relation Extraction  NIPS?2005.Vancouver, BC, December 2005Breiman L. (1996) Bagging Predictors.
MachineLearning, 24(2): 123-140.Collins M. (1999).
Head-driven statistical modelsfor natural language parsing.
Ph.D. Dissertation,University of Pennsylvania.Culotta A. and Sorensen J.
(2004).
Dependencytree kernels for relation extraction.
ACL?2004.423-429.
21-26 July 2004.
Barcelona, Spain.Kambhatla N. (2004).
Combining lexical, syntacticand semantic features with Maximum Entropymodels for extracting relations.ACL?2004(Poster).
178-181.
21-26 July 2004.Barcelona, Spain.Miller G.A.
(1990).
WordNet: An online lexicaldatabase.
International Journal of Lexicography.3(4):235-312.Miller S., Fox H., Ramshaw L. and Weischedel R.(2000).
A novel use of statistical parsing to ex-tract information from text.
ANLP?2000.
226-233.
29 April  - 4 May 2000, Seattle, USAMUC-7.
(1998).
Proceedings of the 7th MessageUnderstanding Conference (MUC-7).
MorganKaufmann, San Mateo, CA.Platt J.
1999.
Probabilistic Outputs for SupportVector Machines and Comparisions to regular-ized Likelihood Methods.
In Advances in LargeMargin Classifiers.
Edited by Smola .J., BartlettP., Scholkopf B. and Schuurmans D. MIT Press.Roth D. and Yih W.T.
(2002).
Probabilistic reason-ing for entities and relation recognition.
CoL-ING?2002.
835-841.26-30 Aug 2002.
Taiwan.Zelenko D., Aone C. and Richardella.
(2003).
Ker-nel methods for relation extraction.
Journal ofMachine Learning Research.
3(Feb):1083-1106.Zhang M., Su J., Wang D.M., Zhou G.D. and TanC.L.
(2005).
Discovering Relations from a LargeRaw Corpus Using Tree Similarity-based Clus-tering, IJCNLP?2005, Lecture Notes inComputer Science (LNCS 3651).
378-389.
11-16Oct 2005.
Jeju Island, South Korea.Zhao S.B.
and Grisman R. 2005.
Extracting rela-tions with integrated information using kernelmethods.
ACL?2005: 419-426.
Univ of Michi-gan-Ann Arbor?
USA?
25-30 June 2005.Zhou G.D. and Su Jian.
Named Entity Recogni-tion Using a HMM-based Chunk Tagger,ACL?2002.
pp473-480.
Philadelphia.
July2002.Zhou G.D., Su J. Zhang J. and Zhang M. (2005).Exploring various knowledge in relation extrac-tion.
ACL?2005.
427-434.
25-30 June, Ann Ar-bor, Michgan, USA.128
