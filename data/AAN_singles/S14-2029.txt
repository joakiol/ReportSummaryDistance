Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 186?191,Dublin, Ireland, August 23-24, 2014.CMUQ@Qatar:Using Rich Lexical Features forSentiment Analysis on TwitterSabih Bin Wasi, Rukhsar Neyaz, Houda Bouamor, Behrang MohitCarnegie Mellon University in Qatar{sabih, rukhsar, hbouamor, behrang}@cmu.eduAbstractIn this paper, we describe our system forthe Sentiment Analysis of Twitter sharedtask in SemEval 2014.
Our system usesan SVM classifier along with rich set oflexical features to detect the sentiment ofa phrase within a tweet (Task-A) and alsothe sentiment of the whole tweet (Task-B).
We start from the lexical features thatwere used in the 2013 shared tasks, we en-hance the underlying lexicon and also in-troduce new features.
We focus our fea-ture engineering effort mainly on Task-A.
Moreover, we adapt our initial frame-work and introduce new features for Task-B.
Our system reaches weighted score of87.11% in Task-A and 64.52% in Task-B.This places us in the 4th rank in the Task-A and 15th in the Task-B.1 IntroductionWith more than 500 million tweets sent per day,containing opinions and messages, Twitter1hasbecome a gold-mine for organizations to monitortheir brand reputation.
As more and more userspost about products and services they use, Twit-ter becomes a valuable source of people?s opin-ions and sentiments: what people can think abouta product or a service, how positive they can beabout it or what would people prefer the product tobe like.
Such data can be efficiently used for mar-keting.
However, with the increasing amount oftweets posted on a daily basis, it is challenging andexpensive to manually analyze them and locate themeaningful ones.
There has been a body of re-cent work to automatically learn the public sen-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/1http://twitter.comtiments from tweets using natural language pro-cessing techniques (Pang and Lee, 2008; Jansenet al., 2009; Pak and Paroubek, 2010; Tang et al.,2014).
However, the task of sentiment analysis oftweets in their free format is harder than that of anywell-structured document.
Tweet messages usu-ally contain different kinds of orthographic errorssuch as the use of special and decorative charac-ters, letter or word duplication, extra punctuation,as well as the use of special abbreviations.In this paper, we present our machine learn-ing based system for sentiment analysis of Twittershared task in SemEval 2014.
Our system takesas input an arbitrary tweet and assigns it to oneof the following classes that best reflects its sen-timent: positive, negative or neutral.
While pos-itive and negative tweets are subjective, neutralclass encompasses not only objective tweets butalso subjective tweets that does not contain any?polar?
emotion.
Our classifier was developed asan undergrad course project but later pursued asa research topic.
Our training, development andtesting experiments were performed on data setspublished in SemEval 2013 (Nakov et al., 2013).Motivated with its performance, we participatedin SemEval 2014 Task 9 (Rosenthal et al., 2014).Our approach includes an extensive usage of off-the-shelf resources that have been developed forconducting NLP on social media text.
Our orig-inal aim was enhancement of the task-A.
More-over, we adapted our framework and introducednew features for task-B and participated in bothshared tasks.
We reached an F-score of 83.3% inTask-A and an F-score of 65.57% in Task-B.
Thatplaced us in the 4th rank in the task-A and 15thrank in the task-B.Our approach includes an extensive usage ofoff-the-shelf resources that have been developedfor conducting NLP on social media text.
Thatincludes the Twitter Tokenizer and also the Twit-ter POS tagger, several sentiment analysis lexica186and finally our own enhanced resources for spe-cial handling of Twitter-specific text.
Our origi-nal aim in introducing and evaluating many of thefeatures was enhancement of the task-A.
More-over, we adapted our framework and introducednew features for task-B and participated in bothshared tasks.
We reached an F-score of 83.3% inTask-A and an F-score of 65.57% in Task-B.
Thatplaced us in the 4th rank in the task-A and 15thrank in the task-B.2 System OverviewWe participate in tasks A and B.
We use three-way classification framework in which we designand use a rich feature representation of the Twittertext.
In order to process the tweets, we start witha pre-processing step, followed by feature extrac-tion and classifier training.2.1 Data Pre-processingBefore the tweet is fed to the system, it goesthrough pre-processing phase that breaks tweetstring into words (tokenization), attaches more in-formation to each word (POS tagging), and othertreatments.Tokenization: We use CMU ARK Tok-enizer (Owoputi et al., 2013) to tokenize eachtweet.
This tokenizer is developed to tokenizenot only space-separated text but also tokens thatneed to be analyzed separately.POS tagging: We use CMU ARK POS Tag-ger (Owoputi et al., 2013) to assign POS tags tothe different tokens.
In addition to the grammat-ical tags, this tagger assigns also twitter-specifictags like @ mentions, hash tags, etc.
This infor-mation is used later for feature extraction.Other processing: In order to normalize the dif-ferent tokens and convert them into a correct En-glish, we find acronyms in the text and add theirexpanded forms at the end of the list.
We decideto keep both the acronym and the new word to en-sure that if the token without its expansion wasthe word the user meant, then we are not losingany information by getting its acronym.
We ex-tend the NetLingo2top 50 Internet acronym list toadd some missing acronyms.
In order to reduce in-flectional forms of a word to a common base formwe use WordNetlemmatizer in NLTK (Bird et al.,2http://www.netlingo.com/top50/popular-text-terms.phpTweet ?This is so awesome@henry:D!
#excited?Bag of Words ?This?
:1, ?is?
:1, ?so?:1,?awesome?
:1, ?@henry?:1,?:D?
:1, ?!,?
:1, #excited?
:1POS features numHashTags:1, numAd-verb:1, numAdjective:1Polarity features positiveWords:1, negWords:0,avgScore: -0.113Task-B specificfeaturesnumCapsWords:0, numEmo-ticons:1, numUrls:0Table 1: Set of Features demonstrated on a sampletweet for Task-B.2009)3.
This could be useful for the feature extrac-tion, to get as much matches as possible betweenthe train and test data (e.g., for bag-of-words fea-ture).2.2 Feature ExtractionAssigning a sentiment to a single word, phrase ora full tweet message requires a rich set of fea-tures.
For this, we adopt a forward selection ap-proach (Ladha and Deepa, 2011) to select the fea-tures that characterize to the best the different sen-timents and help distinguishing them.
In this ap-proach, we incrementally add the features one byone and test whether this boosts the developmentresults.
We heavily rely on a binary feature rep-resentation (Heinly et al., 2012) to ensure the ef-ficiency and robustness of our classifier.
The dif-ferent features used are illustrated in the examplegiven in Table 1.Bag-of-words feature: indicates whether agiven token is present in the phrase.Morpho-syntactic feature: we use the POS andtwitter-specific tags extracted for each token.
Wecount the number of adjectives, adverbs and hash-tags present in the focused part of the tweet mes-sage (entire tweet or phrase).
We tried addingother POS based features (e.g., number of posses-sive pronouns, etc.
), but only the aforementionedtags increased the result figures for both tasks.Polarity-based features: we use freely avail-able sentiment resources to explicitly define thepolarity at a token-level.
We define three featurecategories, based on the lexicon used:3http://www.nltk.org/api/nltk.stem.html187Task-A Task-BDev Train Test Dev Train TestPositive 57.09 % 62.06% 59.49% 34.76% 37.59% 39.01%Negative 37.89% 33.01% 35.31% 20.56% 15.06% 17.15%Neutral 5.02% 4.93% 5.21% 44.68% 47.36% 43.84%All 1,135 9,451 10,681 1,654 9,684 8,987Table 2: Class size distribution for all the three sets for both Task-A and Task-B.?
Subjectivity: number of words mapped to?positive?
from the MPQA Subjectivity lexi-con (Wilson et al., 2005).?
Hybrid Lexicon: We combine the Senti-ment140 lexicon (Mohammad et al., 2013)with the Bing Liu?s bag of positive and neg-ative words (Hu and Liu, 2004) to create adictionary in which each token is assigned asentiment.?
Token weight: we use the SentiWordNetlexicon (Baccianella et al., 2010) to definethis feature.
SentiWordNet contains positive,negative and objective scores between 0 and1 for all senses in WordNet.
Based on thissense level annotation, we first map each to-ken to its weight in this lexicon and then thesum of all these weights was used as the tweetweight.Furthermore, in order to take into account thepresence of negative words, which modify the po-larity of the context within which they are invoked,we reverse the polarity score of adjectives or ad-verbs that come within 1-2 token distance after anegative word.Task specific features: In addition to the fea-tures described above, we also define some task-specific ones.
For example, we indicate the num-ber of capital letters in the phrase as a feature inTask-A.
This could help in this task, since we arefocusing on short text.
For Task-B we indicateinstead the number of capital words.
This relieson the intuition that polarized tweets would carrymore (sometimes all) capital words than the neu-tral or objective ones.
We also added the numberof emoticons and number of URL links as fea-tures for Task-B.
Here, the goal is to segregatefact-containing objective tweets from emotion-containing subjective tweets.2.3 ClassifierWe use a Support Vector Machine (SVM) classi-fier (Chang and Lin, 2011) to which we providethe rich set of features described in the previoussection.
We use a linear kernel and tune its param-eter C separately for the two tasks.
Task-A sys-tem was bound tight to the development set withC=0.18 whereas in Task-B the system was givenfreedom by setting C=0.55.
These values wereoptimized during the development using a brute-force mechanism.Task-A Task-BLiveJournal 2014 83.89 65.63SMS 2013 88.08 62.95Twitter 2013 89.85 65.11Twitter 2014 83.45 65.53Sarcasm 78.07 40.52Weighted average 87.11 64.52Table 3: F1 measures and final results of the sys-tem for Task-A and Task-B for all the data setsincluding the weighted average of the sets.3 Experiments and ResultsIn this section, we explain details of the data andthe general settings for the different experimentswe conducted.
We train and evaluate our classifierfor both tasks with the training, development andtesting datasets provided for the SemEval 2014shared task.
The size of the three datasets weuse as well as their class distributions are illus-trated in Table 2 .
It is important to note thatthe total dataset size for training and developmentset (10,586) is about the same as test set mak-ing the learning considerably challenging for cor-rect predictions.
Positive instances covered morethan half of each dataset for Task-A while Neutralwere the most popular class for Task-B.
The classdistribution of training set is the same as the testset.188Task-A Task-Ball features 87.11 64.52all-preprocessing 80.79(-6.32) 59.20(-5.32)all-ARK tokenization 83.69(-3.42) 60.61(-3.91)all-other treatments 85.06(-2.05) 62.19(-2.33)only BOW 81.69(-5.42) 57.85(-6.67)all-bow 82.05(-5.06) 52.04(-12.48)all-pos 86.92(-0.19) 64.31(-0.21)all-polarity based features 81.80(-5.31) 57.95(-6.57)all-SVM tuning 80.82(-6.29) 21.41(-43.11)all-SVM c=0.01 84.20(-2.91) 59.87(-4.65)all-SVM c=selected 87.11(0.00) 64.52(0.00)all-SVM c=1 86.39(-0.72) 62.51(-2.01)Table 4: F-scores obtained on the test sets with the specific feature removed.The test dataset is composed of five differ-ent sets: Twitter2013 a set of tweets collectedfor the SemEval2013 test set, Twitter2014, tweetscollected for this years version, LiveJournal2014consisting of formal tweets, SMS2013, a collec-tion of sms messages,TwitterSarcasm, a collectionof sarcastic tweets.
The results of our system areshown in Table 3.
The top five rows shows theresults by the SemEval scorer for all the data setsused by them.
This scorer took the average of F1-score of only positive and negative classes.
Thelast row shows the weighted average score of allthe scores for Task A and B from the different datasets.Our scores for Task-A and Task-B were 83.45and 65.53 respectively for Twitter 2014.Our system performed better on Twitter andSMS test sets from 2013.
This was reasonablesince we tuned our system on these datasets.
Onthe other hand, the system performed worst on sar-casm test set.
This drop is extremely evident inTask-B where the results were dropped by 25%.To analyze the effects of each step of our sys-tem, we experimented with our system using dif-ferent configurations.
The results are shown in Ta-ble 4 and our analysis is described in the followingsubsections.
The results were scored by SemEval2014 scorer and we took the weighted average ofall data sets to accurately reflect the performanceof our system.We show the polarities values assigned to eachtoken of a tweet by our classifier, in Table 5.Tokens POS Tags Sentiments PolarityThis O Neutral -0.194Is V Neutral -0.115So R Neutral -0.253Awesome A Positive 2.351@Henry @ - -#excited # Positive 1.84Table 5: Polarity assigned using our classifier toeach word of a Tweet message.3.1 Preprocessing EffectsWe compared the effects of basic tokenization(based on white space) against the richer ARKTwitter tokenizer.
The scores dropped by 3.42%and 3.91% for Task-A and Task-B, respectively.Other preprocessing enhancements like lemmati-zation and acronym additions also gave our sys-tem performance a boost.
Again, the effects weremore visible for Task-B than for Task-A.
Over-all, the system performance was boosted by 6.32%for Task-A and 5.32% for Task-B.
Consideringthe overall score for Task-B, this is a significantchange.3.2 Feature Engineering EffectsTo analyze the effect of feature extraction pro-cess, we ran our system with different kind offeatures disabled - one at a time.
For Task-A,unigram model and polarity based features wereequally important.
For Task-B, bag of words fea-ture easily outperformed the effects of any otherfeature.
However, polarity based features weresecond important class of features for our system.These suggest that if more accurate, exhaustive189and social media representative lexicons are made,it would help both tasks significantly.
POS basedfeatures were not directly influential in our system.However, these tags helped us find better matchesin lexicons where words are further identified withtheir POS tag.3.3 Classifier TuningWe also analyzed the significance of SVM tuningto our system.
Without setting any parameter toSVMutil library (Chang and Lin, 2011), we no-ticed a drop of 6.29% to scores of Task-A and asignificant drop of 43.11% to scores of Task-B.Since the library use poly kernel by default, theresults were drastically worse for Task-B due tolarge feature set.
We also compared the perfor-mance with SVM kernel set to C=1.
In this re-stricted setting, the results were slightly lower thanthe result obtained for our final system.4 DiscussionDuring this work, we found that two improve-ments to our system would have yielded betterscores.
The first would be lexicons: Since thelexicons like Sentiment140 Lexicon are automati-cally generated, we found that they contain somenoise.
As we noticed a drop of that our resultswere critically dependent on these lexicons, thisnoise would have resulted in incorrect predictions.Hence, more accurate and larger lexicons are re-quired for better classification, especially for thetweet-level task.
Unlike SentiWordNet these lexi-cons should contain more informal words that arecommon in social media.
Additionally, as we cansee our system was not able to confidently predictsarcasm tweets on both expression and messagelevel, special attention is required to analyze thenature of sarcasm on Twitter and build a featureset that can capture the true sentiment of the tweet.5 ConclusionWe demonstrated our classification system thatcould predict sentiment of an input tweet.
Oursystem performed more accurately in expression-level prediction than on entire tweet-level predic-tion.
Our system relied heavily on bag-of-wordsfeature and polarity based features which in turnrelied on correct part-of-speech tagging and third-party lexicons.
With this system, we ranked 4thin SemEval 2014 expression-level prediction taskand 15th in tweet-level prediction task.AcknowledgmentWe would like to thank Kemal Oflazer and theshared task organizers for their support through-out this work.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh conferenceon International Language Resources and Evalua-tion (LREC?10), pages 2200?2204, Valletta, Malta.Steven Bird, Ewan Klein, and Edward Loper.2009.
Natural Language Processing with Python.O?Reilly Media, Inc.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A Library for Support Vector Machines.ACM Transactions on Intelligent Systems and Tech-nology, 2:27:1?27:27.Jared Heinly, Enrique Dunn, and Jan-Michael Frahm.2012.
Comparative Evaluation of Binary Features.In Proceedings of the 12th European Conference onComputer Vision, pages 759?773, Firenze, Italy.Minqing Hu and Bing Liu.
2004.
Mining and Sum-marizing Customer Reviews.
In Proceedings of theTenth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 168?177, Seattle, WA, USA.Bernard J Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter Power: Tweets asElectronic Word of Mouth.
Journal of the Ameri-can society for information science and technology,60(11):2169?2188.L.
Ladha and T. Deepa.
2011.
Feature Selection Meth-ods and Algorithms.
International Journal on Com-puter Science and Engineering (IJCSE), 3:1787?1797.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
In SecondJoint Conference on Lexical and Computational Se-mantics (*SEM), Volume 2: Proceedings of the Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 321?327, Atlanta, Geor-gia, USA.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 Task 2: Sentiment Analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA.190Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah A.Smith.
2013.
Improved Part-of-Speech Tagging forOnline Conversational Text with Word Clusters.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 380?390, Atlanta, Georgia.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa Corpus for Sentiment Analysis and Opinion Min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), pages 1320?1326, Valletta, Malta.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis, volume 2.
Now Publishers Inc.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 Task 9:Sentiment Analysis in Twitter.
In Proceedings of theEighth International Workshop on Semantic Evalu-ation (SemEval?14), Dublin, Ireland.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014.
Learning Sentiment-Specific Word Embedding for Twitter SentimentClassification.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics, pages 1555?1565, Baltimore, Maryland.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.
In Proceedings of the con-ference on human language technology and empiri-cal methods in natural language processing, pages347?354.191
