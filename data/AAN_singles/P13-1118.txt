Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201?1211,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDERIVBASE: Inducing and Evaluating aDerivational Morphology Resource for GermanBritta Zeller?
Jan ?najder?
Sebastian Pad??
?Heidelberg University, Institut f?r Computerlinguistik69120 Heidelberg, Germany?University of Zagreb, Faculty of Electrical Engineering and ComputingUnska 3, 10000 Zagreb, Croatia{zeller, pado}@cl.uni-heidelberg.de jan.snajder@fer.hrAbstractDerivational models are still an under-researched area in computational morphol-ogy.
Even for German, a rather resource-rich language, there is a lack of large-coverage derivational knowledge.
This pa-per describes a rule-based framework forinducing derivational families (i.e., clus-ters of lemmas in derivational relation-ships) and its application to create a high-coverage German resource, DERIVBASE,mapping over 280k lemmas into more than17k non-singleton clusters.
We focus on therule component and a qualitative and quan-titative evaluation.
Our approach achievesup to 93% precision and 71% recall.
Weattribute the high precision to the fact thatour rules are based on information fromgrammar books.1 IntroductionMorphological processing is generally recognizedas an important step for many NLP tasks.
Morpho-logical analyzers such as lemmatizers and part ofspeech (POS) taggers are commonly the first NLPtools developed for any language (Koskenniemi,1983; Brill, 1992).
They are also applied in NLPapplications where little other linguistic analysis isperformed, such as linguistic annotation of corporaor terminology acquisition; see Daille et al (2002)for an informative summary.Most work on computational morphology hasfocused on inflectional morphology, that is, thehandling of grammatically determined variation ofform (Bickel and Nichols, 2001), which can beunderstood, overimplifying somewhat, as a normal-ization step.
Derivational morphology, which isconcerned with the formation of new words fromexisting ones, has received less attention.
Exam-ples are nominalization (to understand?
the un-derstanding), verbalization (the shelf ?
to shelve),and adjectivization (the size ?
sizable).
Part ofthe reason for the relative lack of attention lies inthe morphological properties of English, such asthe presence of many zero derivations (the fish?to fish), the dominance of suffixation, and the rel-ative absence of stem changes in derivation.
Forthese reasons, simple stemming algorithms (Porter,1980) provide a cheap and accurate approximationto English derivation.Two major NLP resources deal with derivation.WordNet lists so-called ?morphosemantic?
rela-tions (Fellbaum et al, 2009) for English, and anumber of proposals exist for extending WordNetsin other languages with derivational relations (Bil-gin et al, 2004; Pala and Hlav?c?kov?, 2007).
Cat-Var, the ?Categorial Variation Database of English?
(Habash and Dorr, 2003), is a lexicon aimed specif-ically at derivation.
It groups English nouns, verbs,adjectives, and adverbs into derivational equiva-lence classes or derivational families such asaskV askerN askingN askingADerivational families are commonly understood asgroups of derivationally related lemmas (Daille etal., 2002; Milin et al, 2009).
The lemmas in CatVarcome from various open word classes, and multiplewords may be listed for the same POS.
The abovefamily lists two nouns: an event noun (asking) andan agentive noun (asker).
However, CatVar doesnot consider prefixation, which is why, e.g., theadjective unasked is missing.CatVar has found application in different areasof English NLP.
Examples are the acquisition ofparaphrases that cut across POS lines, applied, forexample, in textual entailment (Szpektor and Da-gan, 2008; Berant et al, 2012).
Then there is theinduction and extension of semantic roles resourcesfor predicates of various parts of speech (Meyers etal., 2004; Green et al, 2004).
Finally, CatVar has1201been used as a lexical resource to generate sentenceintersections (Thadani and McKeown, 2011).In this paper, we describe the project of obtain-ing derivational knowledge for German to enablesimilar applications.
Even though there are twoderivational resources for this language, IMSLEX(Fitschen, 2004) and CELEX (Baayen et al, 1996),both have shortcomings.
The former does not ap-pear to be publicly available, and the latter has alimited coverage (50k lemmas) and does not ex-plicitly represent derivational relationships withinfamilies, which are necessary for fine-grained op-timization of families.
For this reason, we lookinto building a novel derivational resource for Ger-man.
Unfortuantely, the approach used to buildCatVar cannot be adopted: it builds on a collectionof high-quality lexical-semantic resources such asNOMLEX (Macleod et al, 1998), which are notavailable for German.Instead, we employ a rule-based framework todefine derivation rules that cover both suffixationand prefixation and describes stem changes.
Fol-lowing the work of ?najder and Dalbelo Ba?ic?
(2010), we define the derivational processes usingderivational rules and higher-order string transfor-mation functions.
The derivational rules inducea partition of the language?s lemmas into deriva-tional families.
Our method is applicable to manylanguages if the following are available: (1) a com-prehensive set of lemmas (optionally including gen-der information); (2) knowledge about admissiblederivational patterns, which can be gathered, forexample, from linguistics textbooks.The result is a freely available high-precisionhigh-coverage resource for German derivationalmorphology that has a structure parallel to Cat-Var, but was obtained without using manually con-structed lexical-semantic resources.
We conducta thorough evaluation of the induced derivationalfamilies both regarding precision and recall.Plan of the paper.
Section 2 discusses priorwork.
Section 3 defines our derivation model thatis applied to German in Section 4.
Sections 5 and6 present our evaluation setup and results.
Section7 concludes the paper and outlines future work.2 Related WorkComputational models of morphology have a longtradition.
Koskenniemi (1983) was the first whoanalyzed and generated morphological phenomenacomputationally.
His two-level theory has beenapplied in finite state transducers (FST) for severallanguages (Karttunen and Beesley, 2005).Many recent approaches automatically inducemorphological information from corpora.
Theyare either based solely on corpus statistics (D?jean,1998), measure semantic similarity between inputand output lemma (Schone and Jurafsky, 2000),or bootstrap derivation rules starting from seed ex-amples (Piasecki et al, 2012).
Hammarstr?m andBorin (2011) give an extensive overview of state-of-the-art unsupervised learning of morphology.Unsupervised approaches operate at the level ofword-forms and have complementary strengths andweaknesses to rule-based approaches.
On the up-side, they do not require linguistic knowledge; onthe downside, they have a harder time distinguish-ing between derivation and inflection, which mayresult in lower precision, and are not guaranteedto yield analyses that correspond to linguistic intu-ition.
An exception is the work by Gaussier (1999),who applies an unsupervised model to constructderivational families for French.For German, several morphological tools exist.Morphix is a classification-based analyzer and gen-erator of German words on the inflectional level(Finkler and Neumann, 1988).
SMOR (Schmidet al, 2004) employs a finite-state transducer toanalyze German words at the inflectional, deriva-tional, and compositional level, and has been usedin other morphological analyzers, e.g., Morphisto(Zielinski and Simon, 2008).
The site canoonet1 of-fers broad-coverage information about the Germanlanguage including derivational word formation.3 FrameworkIn this section, we describe our rule-based model ofderivation, its operation to define derivational fam-ilies, and the application of the model to German.We note that the model is purely surface-based,i.e., it does not model any semantic regularities be-yond those implicit in string transformations.
Webegin by outlining the characteristics of Germanderivational morphology.3.1 German Derivational MorphologyAs German is a morphologically complex language,we analyzed its derivation processes before imple-menting our rule-based model.
We relied on tradi-tional grammar books and lexicons, e.g., Hoeppner(1980) and Augst (1975), in order to linguistically1http://canoo.net1202justify our assumptions as well as to achieve thebest possible precision and coverage.We concentrate on German derivational pro-cesses that involve nouns, verbs, and adjectives.2Nouns are simple to recognize due to capitaliza-tion: stauenV ?
StauN (to jam ?
jam), essenV ?EssenN (to eat ?
food).
Verbs bear three typicalsuffixes (-en, -eln, -ern).
An example of a derivedverb is festA ?
festigenV (tight ?
to tighten), where-ig is the derivational suffix.
Adjectivization workssimilarlty: TagN ?
t?glichA (day ?
daily).This example shows that derivation can also in-volve stem changes in the form of umlaut (e.g.,a ?
?)
and ablaut shift, e.g., siedenV ?
SudN(to boil ?
infusion).
Other frequent processesin German derivation are circumfixation (HaftN?
inhaftierenV (arrest ?
to arrest)) and prefixation(hebenV ?
behebenV (to raise ?
to remedy)).
Pre-fixation often indicates a semantic shift, either interms of the general meaning (as above) or in termsof the polarity ( klarA ?
unklarA (clear ?
unclear)).Also note that affixes can be either Germanic, e.g.,?len ?
?lung (to oil ?
oiling), or Latin/Greek, e.g.,generieren ?
Generator (to generate ?
generator).As this analysis shows, derivation in Germaninvolves transformation as well as affixation pro-cesses, which has to be taken into account whenmodeling a derivational resource.3.2 A Rule-based Derivation ModelThe purpose of a derivational model is to definea set of transformations that correspond to validderivational word formation rules.
Rule-basedframeworks offer convenient representations forderivational morphology because they can take ad-vantage of linguistic knowledge about derivation,have interpretable representations, and can be fine-tuned for high precision.
The choice of the frame-work is in principle arbitrary, as long as it can con-veniently express the derivational phenomena ofa language.
Typically used for this purpose aretwo-level formalism rules (Karttunen and Beesley,1992) or XFST replace rules (Beesley and Kart-tunen, 2003).In this paper, we adopt the modeling frameworkproposed by ?najder and Dalbelo Ba?ic?
(2010).The framework corresponds closely to simple,human-readable descriptions in traditional gram-2We ignore adverb derivation; the German language dis-tinguishes between adverbial adjectives and adverbs, the latterbeing a rather unproductive class and thus of no interest forderivation (Schiller et al, 1999).mar books.
The expressiveness of the formalismis equivalent to the replacement rules commonlyused in finite state frameworks, thus the rules canbe compiled into FSTs for efficient processing.The framework makes a clear distinction be-tween inflectional and derivational morphology andprovides separate modeling components for thesetwo; we only make use of the derivation modelingcomponent.
We use an implementation of the mod-eling framework in Haskell.
For details, see thestudies by ?najder and Dalbelo Ba?ic?
(2008) and?najder and Dalbelo Ba?ic?
(2010).The building blocks of the derivational compo-nent are derivational rules (patterns) and transfor-mation functions.
A derivational rule describes thederivation of a derived word from a basis word.
Aderivational rule d is defined as a triple:d = (t,P1,P2) (1)where t is the transformation function that mapsthe word?s stem (or lemma) into the derived word?sstem (or lemma), while P1 and P2 are the sets ofinflectional paradigms of the basis word and thederived word, respectively, which specify the mor-phological properties of the rule?s input and output.For German, our study assumes that inflectionalparadigms are combinations of part-of-speech andgender information (for nouns).A transformation function t : S ?
?
(S) mapsstrings to a set of strings, representing possibletransformations.
At the lowest level, t is definedin terms of atomic string replacement operations(replacement of prefixes, suffixes, and infixes).
Theframework then uses the notion of higher-orderfunctions ?
functions that take other transforma-tions as arguments and return new transformationsas results ?
to succinctly define common deriva-tional processes such as prefixation, suffixation,and stem change.
More complex word-formationrules, such as those combining prefixation and suf-fixation, can be obtained straightforwardly by func-tional composition.Table 1 summarizes the syntax we use for trans-formation functions and shows two example deriva-tional rules.
Rule 1 defines an English adjectiviza-tion rule.
It uses the conditional try operator toapply to nouns with and without the -ion suffix(action ?
active, instinct ?
instinctive).
Infix re-placement is used to model stem alternation, asshown in rule 2 for German nominalization, e.g.,vermachtA ?
Verm?chtnisN (bequethed ?
bequest).1203Function Descriptionsfx (s) concatenate the suffix sdsfx (s) delete the suffix saifx (s1, s2) alternate the infix s1 to s2try(t) perform transformation t, if possibleopt(t) optionally perform transformation tuml alternate infixes for an umlaut shift:uml = aifx ({(a, ?
), (o, ?
), (u, ?
)})Examples1 (EN) (sfx (ive) ?
try(dsfx (ion)),N ,A)?derive -ive adjectives from nouns poten-tially ending in -ion?2 (DE) (sfx (nis) ?
try(uml),A,N )?derive -nis nouns from adjectives withoptional umlaut creation?Table 1: Transformation functions and exemplaryderivational rules in the framework by ?najder andDalbelo Ba?ic?
(2010)N and A denote the paradigms for nouns (withoutgender restriction) and adjectives, respectively.3.3 Induction of Derivational FamiliesRecall that our goal is to induce derivational fami-lies, that is, classes of derivationally related words.We define derivational families on the basis ofderivational rules as follows.Given a lemma-paradigm pair (l, p) as input,a single derivational rule d = (t,P1,P2) gen-erates a set of possible derivations Ld(l, p) ={(l1, p1), .
.
.
, (ln, pn)}, where p ?
P1 and pi ?
P2for all i.
Given a set of derivational rules D, we de-fine a binary derivation relation?D between twolemma-paradigm pairs that holds if the second paircan be derived from the first one as:(l1, p1)?D (l2, p2) (2)iff ?d ?
D. (l2, p2) ?
Ld(l1, p1)Let L denote the set of lemma-paradigm pairs.
Theset of derivational families defined by D on L isgiven by the equivalence classes of the transitive,symmetric, and reflexive closure of?D over L.Note that in addition to the quality of the rules,the properties ofL plays a central role in the qualityof the induced families.
High coverage of L is im-portant because the transitivity of?D ranges onlyover lemmas in L, so low coverage of L may resultin fragmented derivational families.
However, Lshould also not contain erroneous lemma-paradigmpairs.
The reason is that the derivational rules onlydefine admissible derivations, which need not bemorphologically valid, and therefore routinely over-generate; L plays an important role in filtering outderivations that are not attested in the data.4 Building the Resource4.1 Derivational RulesWe implemented the derivational rules from Hoepp-ner (1980) for verbs, nouns, and adjectives, cov-ering all processes described in Section 3.1 (zeroderivation, prefixation, suffixation, circumfixation,and stem changes).
We found many derivationalpatterns in German to be conceptually simple (e.g.,verb-noun zero derivation) so that substantial cov-erage can already be achieved with very simpletransformation functions.
However, there are manymore complex patterns (e.g., suffixation combinedwith optional stem changes) that in sum also af-fect a considerable number of lemmas, which re-quired us to either implement low-coverage rulesor generalize existing rules.
In order to preserveprecision as much as possible, we restricted ruleapplication by using try instead of opt, and by usinggender information from the noun paradigms (forexample, some rules only apply to masculine nounsand produce female nouns).
As a result, we endup with high-coverage rules, such as derivationsof person-denoting nouns (SchuleN ?
Sch?lerN(school ?
pupil)) as well as high-accuracy rulessuch as negation prefixes (PolN ?
GegenpolN (pole?
antipole)).Even though we did not focus on the explana-tory relevance of rules, we found that the under-lying modeling formalism, and the methodologyused to develop the model, offer substantial lin-guistic plausibility in practice.
We had to resort toheuristics mostly for words with derivational trans-formations that are motivated by Latin or Greekmorphology and do not occur regularly in German,e.g., selegierenV ?
SelektionN (select ?
selection).In the initial development phase, we imple-mented 154 rules, which took about 22 person-hours.
We then revised the rules with the aim ofincreasing both precision and recall.
To this end,we constructed a development set comprised of asample of 1,000 derivational families induced us-ing our rules.
On this set, we inspected the deriva-tional families for false positives, identified theproblematic rules, and identified unused and redun-dant rules.
In order to identify the false negatives,we additionally sampled a list of 1,000 lemmas andused string distance measures (cf.
Section 5.1) to re-trieve the 10 most similar words for each lemma not1204Process N-N N-A N-V A-A A-V V-VZero derivation ?
1 5 ?
?
?Prefixation 10 ?
5 5 2 9+ Stem change ?
?
3 ?
1 ?Suffixation 15 35 20 1 14 ?+ Stem change 2 8 7 ?
3 1Circumfixation ?
?
1 ?
?
?+ Stem change ?
?
1 ?
?
?Stem change ?
?
7 ?
?
2Total 27 44 49 6 20 12Table 2: Breakdown of derivation rules by categoryof the basis and the derived wordalready covered by the derivational families.
Therefinement process took another 8 person-hours.
Itrevealed three redundant rules and seven missingrules, leading us to a total of 158 rules.Table 2 shows the distribution of rules with re-spect to the derivational processes they implementand the part of speech combinations for the ba-sis and the derived words.
All affixations occurboth with and without stem changes, mostly um-laut shifts.
Suffixation is by far the most frequentlyused derivation process, and noun-verb derivationis most diverse in terms of derivational processes.We also estimated the reliability of derivationalrules by analyzing the accuracy of each rule onthe development set.
We assigned each rule a con-fidence rating on a three-level scale: L3 ?
veryreliable (high-accuracy rules), L2 ?
generally reli-able, and L1 ?
less reliable (low-accuracy rules).We manually analyzed the correctness of rule ap-plications for 100 derivational families of differentsize (counting 2 up to 114 lemmas), and assigned55, 79, and 24 rules to L3, L2 and L1, respectively.4.2 Data and PreprocessingFor an accurate application of nominal derivationrules, we need a lemma list with POS and genderinformation.
We POS-tag and lemmatize SDEWAC,a large German-language web corpus from whichboilerplate paragraphs, ungrammatical sentences,and duplicate pages were removed (Faa?
et al,2010).
For POS tagging and lemmatization, we useTreeTagger (Schmid, 1994) and determine gram-matical gender with the morphological layer ofthe MATE Tools (Bohnet, 2010).
We treat propernouns like common nouns.We apply three language-specific filtering stepsbased on observations in Section 3.1.
First, we dis-card non-capitalized nominal lemmas.
Second, wedeleted verbal lemmas not ending in verb suffixes.Third, we removed frequently occurring erroneouscomparative forms of adjectives (usually formedby adding -er, like neuer / newer) by checking forthe presence of lemmas without -er (neu / new).An additional complication in German concernsprefix verbs, because prefix is separated in tensedinstances.
For example, the 3rd person male singu-lar of aufh?ren (to stop) is er h?rt auf (he stops).Since most prefixes double as prepositions, the cor-rect lemmas can only be reconstructed by parsing.We parse the corpus using the MST parser (Mc-Donald et al, 2006) and recover prefix verbs bysearching for instances of the dependency relationlabeled PTKVZ.Since SDEWAC, as a web corpus, still containserrors, we only take into account lemmas that occurthree times or more in the corpus.
Considering thesize of SDEWAC, we consider this as a conservativefiltering step that preserves high recall and providesa comprehensive basis for evaluation.
After prepro-cessing and filtering, we run the induction of thederivational families as explained in Section 3 toobtain the DERIVBASE resource.4.3 Statistics on DERIVBASEThe preparation of the SDEWAC corpus as ex-plained in Section 4.2 yields 280,336 lemmas,which we cover with our resource.
We induceda total of 239,680 derivational families from thisdata, with 17,799 non-singletons and 221,881 sin-gletons (most of them due to compound nouns).11,039 of the families consist of two lemmas, whilethe biggest contains 116 lemmas (an overgeneratedfamily).
The biggest family with perfect precision(i.e., it contains only morphologically related lem-mas) contains 40 lemmas, e.g., haltenV , erhaltenV ,Verh?ltnisN (to hold, to uphold, relation), etc.
Forcomparison, CatVar v2.1 contains only 82,676 lem-mas in 13,368 non-singleton clusters and 38,604singletons.The following sample family has seven membersacross all three POSes and includes prefixation,suffixation, and infix umlaut shifts:taubA (numbA), TaubheitNf (numbnessN ),bet?ubenV (to anesthetizeV ), Bet?ubungNf(anesthesiaN ), bet?ubtA (anesthetizedA),bet?ubendA (anestheticA), Bet?ubenNn(act of anesthetizingN )12055 Evaluation5.1 BaselinesWe use two baselines against which we comparethe induced derivational families: (1) clusters ob-tained with the German version of Porter?s stem-mer (Porter, 1980)3 and (2) clusters obtained us-ing string distance-based clustering.
We have con-sidered a number of string distance measures andtested them on the development set (cf.
Section4.1).
The measure proposed by Majumder et al(2007) turned out to be the most effective in cap-turing suffixal variation.
For words X and Y , it isdefined asD4(X,Y ) =n?m+ 1n+ 1n?i=m12i?m (3)where m is the position of left-most character mis-match, and n + 1 is the length of the longer ofthe two strings.
To capture prefixal variation andstem changes, we use the n-gram based measureproposed by Adamson and Boreham (1974):Dicen(X,Y ) = 1?2cx+ y (4)where x and y are the total number of distinct n-grams inX and Y , respectively, and c is the numberof distinct n-grams shared by both words.
In ourexperiments, the best performance was achievedwith n = 3.We used hierarchical agglomerative clusteringwith average linkage.
To reduce the computationalcomplexity, we performed a preclustering step byrecursively partitioning the set of lemmas sharingthe same prefix into partitions of manageable size(1000 lemmas).
Initially, we set the number of clus-ters to be roughly equal to the number of inducedderivational families.
For the final evaluation, weoptimized the number of clusters based on F1 scoreon calibration and validation sets (cf.
Section 5.3).5.2 Evaluation MethodologyThe induction of derivational families could be eval-uated globally as a clustering problem.
Unfortu-nately, cluster evaluation is a non-trivial task forwhich there is no consensus on the best approach(Amig?
et al, 2009).
We decided to perform ourevaluation at the level of pairs: we manually judgefor a set of pairs whether they are derivationallyrelated or not.3http://snowball.tartarus.orgWe obtain the gold standard for this evaluationby sampling lemmas from the lemma list.
With ran-dom sampling, the evaluation would be unrealisticbecause a vast majority of pairs would be deriva-tionally unrelated and count as true negatives in ouranalysis.
Moreover, in order to reliably estimate theoverall precision of the obtained derivational fam-ilies, we need to evaluate on pairs sampled fromthese families.
On the other hand, in order to assessrecall, we need to sample from pairs that are notincluded in our derivational families.To obtain reliable estimates of both precisionand recall, we decided to draw two different sam-ples: (1) a sample of lemma pairs sampled fromthe induced derivational families, on which weestimate precision (P-sample) and (2) a sampleof lemma pairs sampled from the set of possiblyderivationally related lemma pairs, on which weestimate recall (R-sample).
In both cases, pairs(l1, l2) are sampled in two steps: first a lemma l1is drawn from a non-singleton family, then the sec-ond lemma l2 is drawn from the derivational familyof l1 (P-sample) or the set of lemmas possibly re-lated to l1 (R-sample).
The set of possibly relatedlemmas is a union of the derivational family of l1,the clusters of l1 obtained with the baseline meth-ods, and k lemmas most similar to l1 according tothe two string distance measures.
We use k = 7in our experiments.
This is based on preliminaryexperiments on the development set (cf.
Section4.1), which showed that k = 7 retrieves about 92%of the related lemmas retrieved for k = 20 witha much smaller number of true negatives.
Thus,the evaluation on the R-sample might overestimatethe recall, but only slightly so, while the P-sampleyields a reliable estimate of precision by reducingthe number of true negatives in the sample.Both samples contain 2400 lemma pairs each.Lemmas included in the development set (Sec-tion 4.1) were excluded from sampling.5.3 Gold Standard AnnotationTwo German native speakers annotated the pairsfrom the P-sample and R-samples.
We defined fivecategories into which all lemma pairs are classifiedas shown in Table 3.
We count R and M as positivesand N, C, L as negatives (cf.
Section 3).4 Notethat this binary distinction would be sufficient tocompute recall and precision.
However, the more4Ambiguous lemmas are categorized as positive (R or M)if there is a matching sense.1206Label Description ExampleR l1 and l2 are morphologi-cally and semantically re-latedkratzigA ?
verkratztA(scratchy ?
scuffed)M l1 and l2 are morphologi-cally but not semanticallyrelatedbombenV ?
bombigA(to bomb ?
smashing)N no morphological relation belebtA ?
lobenV(lively ?
to praise)C no derivational relation,but the pair is composi-tionally relatedFilmendeN ?
filmenV(end of film ?
to film)L not a valid lemma (mis-lemmatization, wronggender, foreign words)HaufeN ?
H?ufungN(N/A ?
accumulation)Table 3: Categories for lemma pair classificationAgreement Cohen?s ?R-sample 0.85 0.79P-sample 0.86 0.70Table 4: Inter-annotator agreement on validationsamplefine-grained five-class annotation scheme providesa more detailed picture.
The separation between Rand M gives a deeper insight into the semantics ofthe derivational families.
Distinguishing betweenC and N, in turn, allows us to identify the pairs thatare derivationally unrelated, but compositionallyrelated, e.g., EhemannN ?
EhefrauN (husband ?wife).We first carried out a calibration phase in whichthe annotators double-annotated 200 pairs fromeach of the two samples and refined the annotationguidelines.
In a subsequent validation phase, wecomputed inter-annotator agreements on the anno-tations of another 200 pairs each from the P- andthe R-samples.
Table 4 shows the proportion ofidentical annotations by both annotators as well asCohen?s ?
score (Cohen, 1968).
We achieve sub-stantial agreement for ?
(Carletta, 1996).
On theP-sample, ?
is a little lower because the distribu-tion of the categories is skewed towards R, whichmakes an agreement by chance more probable.In our opinion, the IAA results were sufficientlyhigh to switch to single annotation for the produc-tion phase.
Here, each annotator annotated another1000 pairs from the P-sample and R-sample sothat the final test set consists of 2000 pairs fromeach sample.
The P-sample contains 1663 positive(R+M) and 337 negative (N+C+L) pairs, respec-tively, the R-sample contains 575 positive and 1425negative pairs.
As expected, there are more positivePrecision RecallMethod P-sample R-sampleDERIVBASE (initial) 0.83 0.58DERIVBASE-L123 0.83 0.71DERIVBASE-L23 0.88 0.61DERIVBASE-L3 0.93 0.35R-sampleStemming 0.66 0.07String distance D4 0.36 0.20String distance Dice3 0.23 0.23Table 5: Precision and recall on test samplespairs in the P-sample and more negative pairs inthe R-sample.6 Results6.1 Quantitative EvaluationTable 5 presents the overall results.
We eval-uate four variants of the induced derivationalfamilies: those obtained before rule refinement(DERIVBASE initial), and three variants after rulerefinement: using all rules (DERIVBASE-L123),excluding the least reliable rules (DERIVBASE-L23), and using only highly reliable rules(DERIVBASE-L3).We measure the precision of our method on theP-sample and recall on the R-sample.
For the base-lines, precision was also computed on the R-sample(computing it on P-sample, which is obtained fromthe induced derivational families, would severelyunderestimate the number of false positives).
Weomit the F1 score because its use for precision andrecall estimates from different samples is unclear.DERIVBASE reaches 83% precision when us-ing all rules and 93% precision when using onlyhighly reliable rules.
DERIVBASE-L123 achievesthe highest recall, outperforming other methodsand variants by a large margin.
Refinement of theinitial model has produced a significant improve-ment in recall without losses in precision.
The base-lines perform worse than our method: the stemmerwe use is rather conservative, which fragments thefamilies and leads to a very low recall.
The stringdistance-based approaches achieve more balancedprecision and recall scores.
Note that for thesemethods, precision and recall can be traded offagainst each other by varying the number of clus-ters; we chose the number of clusters by optimizingthe F1 score on the calibration and validaton sets.All subsequent analyses refer to DERIVBASE-1207AccuracyCoverage High Low TotalHigh 18 ?
18Low 53 21 74Total 71 21 92Table 6: Proportions of accuracy and coverage fordirect derivations (measured on P-sample)P R P RN-N 0.78 0.68 N-A 0.89 0.83A-A 0.87 0.70 N-V 0.79 0.68V-V 0.55 0.24 A-V 0.88 0.73Table 7: Precision and recall across different partof speech (first POS: basis; second POS: derivedword)L123, which is the model with the highest recall.If optimal precision is required, DERIVBASE-L3should however be preferred.Analysis by frequency.
We cross-classified ourrules according to high/low accuracy and high/lowcoverage based on the pairs in the P-sample.We only considered directly derivationally related(?D) pairs and defined ?high accuracy?
and ?highcoverage?
as all rules above the 25th percentile interms of accuracy and coverage, respectively.
Theresults are shown in Table 6: all high-coveragerules are also highly accurate.
Most rules are ac-curate but infrequent.
Only 21 rules have a lowaccuracy, but all of them apply infrequently.Analysis by parts of speech.
Table 7 shows pre-cision and recall values for different part of speechcombinations for the basis and derived words.
Highprecision and recall are achieved for N-A deriva-tions.
The recall is lowest for V-V derivations,suggesting that the derivational phenomena for thisPOS combination are not yet covered satisfactorily.6.2 Error analysisTable 8 shows the frequencies of true positives andfalse positives on the P-sample and false negativeson the R-sample for each annotated category.
Truenegatives are not reported, since their analysis givesno deeper insight.True positives.
In our analysis we treated both Rand M pairs as related, but it is interesting to seehow many of the true positives are in fact semanti-cally unrelated.
Out of 1,663 pairs, 90% are seman-tically as well as morphologically related (R), e.g.,TPs FPs FNsLabel P-sample P-sample R-sampleR 1,492 ?
107M 171 ?
60N ?
216 ?C ?
7 ?L ?
114 ?Total 1,663 337 167Table 8: Predictions over annotated categoriesalkoholisierenV ?
antialkoholischA (to alcoholize?
nonalcoholic), BeschuldigungN ?
unschuldigA(accusation ?
innocent).
Most R pairs result fromhigh-accuracy rules, i.e., zero derivation, negationprefixation and simple suffixation.
The remaining10% are only morphologically related (M), e.g.,beschwingtA ?
schwingenV (cheerful ?
to swing),StolzierenN ?
stolzA (strut ?
proud).
In both pairs,the two lemmas share a common semantic concept?
i.e., being in motion or being proud ?
but nowa-day?s meanings have grown apart from each other.Among the M true positives, we observe prefixa-tion derivations in 66% of the cases, often involv-ing prefixation at both lemmas, e.g., ErdenklicheN?
bedenklichA (imaginable ?
questionable).False positives.
We observe many errors in pairsinvolving short lemmas, e.g., GenN ?
genierenV(gene ?
to be embarrassed), where orthographiccontext is unsufficient to reject the derivation.About 64% of the 337 incorrect pairs are of classN (unrelated lemmas).
For example, the rule forderiving nouns denoting a male person incorrectlylinks MorseN ?
M?rserN (Morse ?
mortar).
Tran-sitively applied rules often produce incorrect pairs;e.g., SpeicheN ?
speicherbarA (spoke ?
storable)results from the rule chain SpeicheN ?
SpeicherN?
speichernV ?
speicherbarA (spoke?
storage?
to store?
storable).
Chains that involve ablautshifts (cf.
Section 3.1) can lead to surprising re-sults, e.g., ErringungN ?
rangiertA (achievement ?shunted).
Meanwhile, some pairs judged as un-related by the annotators might conceivably beweakly related, such as schl?rfenV and schlurfenV(to sip ?
to shuffle), both of which refer to specificlong drawn out sounds.
About 20% out of these un-related lemma pairs is due to derivations betweenproper nouns (PNs) and common nouns.
This hap-pens especially for short PNs (cf.
the above exam-ple of Morse).
However, since PNs also participatein valid derivations (e.g., Chaplin ?
chaplinesque),1208one could investigate their impact on derivationsrather than omitting them.Errors of the category L ?
34% of the false posi-tives ?
are caused during preprocessing by the lem-matizer.
They cannot be blamed on our derivationalmodel, but of course form part of the output.False negatives.
Errors of this type are due tomissing derivation rules, erroneous rules that leavesome lemmas undiscovered, or the absence of lem-mas in the corpus required for transitive closure.About 64% of the 167 missed pairs are of categoryR.
About half of these pairs result from a lack ofprefixation rules ?
mainly affecting verbs ?
with awide variety of prefixes (zu-, um-, etc.
), includingprepositional prefixes like herum- (around) or ?ber-(over).
We intentionally ignored these derivations,since they frequently lead to semantically unrelatedpairs.
In fact, merely five of the remaining 36%false negative pairs (M) do not involve prefixation.However, this analysis as well as the rather low cov-erage for verb-involved rules (cf.
Table 7) showsthat DERIVBASE might benefit from more prefixrules.
Apart from the lack of prefixation coverageand a few other, rather infrequent rules, we did notfind any substantial deficits.
Most of the remainingerrors are due to German idiosyncrasies and excep-tional derivations, e.g., fahrenV ?
FahrtN (drive ?trip), where the regular zero derivation would resultin Fahr.7 Conclusion and Future WorkIn this paper, we present DERIVBASE, a deriva-tional resource for German based on a rule-basedframework.
A few work days were enough to buildthe underlying rules with the aid of grammar text-books.
We collected derivational families for over280,000 lemmas with high accuracy as well as solidcoverage.
The resource is freely available.5Our approach for compiling a derivational re-source is not restricted to German.
In additionto the typologically most similar Germanic andRomance languages, it is also applicable to agglu-tinative languages like Finnish, or other fusionallanguages like Russian.
Its main requirements area large list of lemmas for the language (optionallywith further morphological features) and linguisticliterature on morphological patterns.We have employed an evaluation method thatuses two separate samples to assess precision and5http://goo.gl/7KG2U; license cc-by-sa 3.0recall to deal with the high number of false neg-atives.
Our analyses indicate two interesting di-rections for future work: (a) specific handling ofproper nouns, which partake in specific derivations;and (b) the use of graph clustering instead of thetransitive closure to avoid errors resulting fromlong transitive chains.Finally, we plan to employ distributional seman-tics methods (Turney and Pantel, 2010) to help re-move semantically unrelated pairs as well as distin-guish automatically between only morphologically(M) or both morphologically and semantically (R)related pairs.
Last, but not least, this allows us togroup derivation rules according to their semanticproperties.
For example, nouns with -er suffixesoften denote persons and are agentivizations of abasis word (Bilgin et al, 2004).AcknowledgmentsThe first and third authors were supported bythe EC project EXCITEMENT (FP7 ICT-287923).The second author was supported by the CroatianScience Foundation (project 02.03/162: ?Deriva-tional Semantic Models for Information Retrieval?
).We thank the reviewers for their constructive com-ments.ReferencesGeorge W. Adamson and Jillian Boreham.
1974.
Theuse of an association measure based on characterstructure to identify semantically related pairs ofwords and document titles.
Information Processingand Management, 10(7/8):253?260.Enrique Amig?, Julio Gonzalo, Javier Artiles, and Fe-lisa Verdejo.
2009.
A comparison of extrinsicclustering evaluation metrics based on formal con-straints.
Information Retrieval, 12(4):461?486.Gerhard Augst.
1975.
Lexikon zur Wortbil-dung.
Forschungsberichte des Instituts f?r DeutscheSprache.
Narr, T?bingen.Harald R. Baayen, Richard Piepenbrock, and Leon Gu-likers.
1996.
The CELEX Lexical Database.
Re-lease 2.
LDC96L14.
Linguistic Data Consortium,University of Pennsylvania, Philadelphia, PA.Kenneth R Beesley and Lauri Karttunen.
2003.
Finitestate morphology, volume 18.
CSLI publicationsStanford.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2012.
Learning entailment relations by global graphstructure optimization.
Computational Linguistics,38(1):73?111.1209Balthazar Bickel and Johanna Nichols.
2001.
Inflec-tional morphology.
In Timothy Shopen, editor, Lan-guage Typology and Syntactic Description, VolumeIII: Grammatical categories and the lexicon, pages169?240.
CUP, Cambridge.Orhan Bilgin, ?zlem ?etinog?lu, and Kemal Oflazer.2004.
Morphosemantic relations in and acrossWordnets.
In Proceedings of the Global WordNetConference, pages 60?66, Brno, Czech Republic.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 89?97, Beijing, China.Eric Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Workshop on Speechand Natural Language, pages 112?116, Harriman,New York.Jean C. Carletta.
1996.
Assessing agreement on clas-sification tasks: the kappa statistic.
ComputationalLinguistics, 22(2):249?254.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement with provision for scaled disagreement orpartial credit.
Psychological Bulletin, 70:213?220.B?atrice Daille, C?cile Fabre, and Pascale S?billot.2002.
Applications of computational morphology.In Paul Boucher, editor, Many Morphologies, pages210?234.
Cascadilla Press.Herv?
D?jean.
1998.
Morphemes as necessary conceptfor structures discovery from untagged corpora.
InProceedings of the Joint Conferences on New Meth-ods in Language Processing and Computational Nat-ural Language Learning, pages 295?298, Sydney,Australia.Gertrud Faa?, Ulrich Heid, and Helmut Schmid.
2010.Design and application of a gold standard for mor-phological analysis: SMOR in validation.
In Pro-ceedings of the Seventh International Conferenceon Language Resources and Evaluation, pages 803?810.Christiane Fellbaum, Anne Osherson, and Peter Clark.2009.
Putting semantics into WordNet?s "morphose-mantic" links.
In Proceedings of the Third Languageand Technology Conference, pages 350?358, Poz-nan?, Poland.Wolfgang Finkler and G?nter Neumann.
1988.
Mor-phix - a fast realization of a classification-based ap-proach to morphology.
In Proceedings of 4th Aus-trian Conference of Artificial Intelligence, pages 11?19, Vienna, Austria.Arne Fitschen.
2004.
Ein computerlinguistischesLexikon als komplexes System.
Ph.D. thesis, IMS,Universit?t Stuttgart.
?ric Gaussier.
1999.
Unsupervised learning of deriva-tional morphology from inflectional lexicons.
InACL?99 Workshop Proceedings on UnsupervisedLearning in Natural Language Processing, pages24?30, College Park, Maryland, USA.Rebecca Green, Bonnie J. Dorr, and Philip Resnik.2004.
Inducing frame semantic verb classes fromwordnet and ldoce.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, pages 375?382, Barcelona, Spain.Nizar Habash and Bonnie Dorr.
2003.
A categorialvariation database for English.
In Proceedings ofthe Anuual Meeting of the North American Associ-ation for Computational Linguistics, pages 96?102,Edmonton, Canada.Harald Hammarstr?m and Lars Borin.
2011.
Unsuper-vised learning of morphology.
Computational Lin-guistics, 37(2):309?350.Wolfgang Hoeppner.
1980.
Derivative Wortbildungder deutschen Gegenwartssprache und ihre algorith-mische Analyse.
Narr, T?bingen.Lauri Karttunen and Kenneth R Beesley.
1992.
Two-level rule compiler.
Xerox Corporation.
Palo AltoResearch Center.Lauri Karttunen and Kenneth R. Beesley.
2005.Twenty-five years of finite-state morphology.
InAntti Arppe, Lauri Carlson, Krister Lind?n, Jussi Pi-itulainen, Mickael Suominen, Martti Vainio, HannaWesterlund, and Anssi Yli-Jyr, editors, Inquiriesinto Words, Constraints and Contexts.
Festschrift forKimmo Koskenniemi on his 60th Birthday, pages 71?83.
CSLI Publications, Stanford, California.Kimmo Koskenniemi.
1983.
Two-level Morphology:A General Computational Model for Word-FormRecognition and Production.
Ph.D. thesis, Univer-sity of Helsinki.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
NOMLEX:A lexicon of nominalizations.
In In Proceedings ofEuralex98, pages 187?193.Prasenjit Majumder, Mandar Mitra, Swapan K. Parui,Gobinda Kole, Pabitra Mitra, and KalyankumarDatta.
2007.
YASS: Yet another suffix strip-per.
ACM Transactions on Information Systems,25(4):18:1?18:20.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with atwo-stage discriminative parser.
In In Proceedingsof the Conference on Computational Natural Lan-guage Learning, pages 216?220, New York, NY.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating noun ar-gument structure for NomBank.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation, Lisbon, Portugal.1210Petar Milin, Victor Kuperman, Aleksandar Kostic, andR Harald Baayen.
2009.
Paradigms bit by bit: Aninformation theoretic approach to the processing ofparadigmatic structure in inflection and derivation.Analogy in grammar: Form and acquisition, pages214?252.Karel Pala and Dana Hlav?c?kov?.
2007.
Derivationalrelations in Czech WordNet.
In Proceedings of theACL Workshop on Balto-Slavonic Natural LanguageProcessing: Information Extraction and EnablingTechnologies, pages 75?81.Maciej Piasecki, Radoslaw Ramocki, and MarekMaziarz.
2012.
Recognition of Polish derivationalrelations based on supervised learning scheme.
InProceedings of the Eighth International Conferenceon Language Resources and Evaluation, pages 916?922, Istanbul, Turkey.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Anne Schiller, Simone Teufel, Christine St?ckert, andChristine Thielen.
1999.
Guidelines f?r das Tag-ging deutscher Textcorpora mit STTS.
Technicalreport, Institut fur maschinelle Sprachverarbeitung,Stuttgart.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.Smor: A German computational morphology cover-ing derivation, composition and inflection.
In Pro-ceedings of the Fourth International Conference onLanguage Resources and Evaluation, Lisbon, Portu-gal.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, UK.Patrick Schone and Daniel Jurafsky.
2000.Knowledge-free induction of morphology us-ing latent semantic analysis.
In Proceedings of theConference on Natural Language Learning, pages67?72, Lisbon, Portugal.Jan ?najder and Bojana Dalbelo Ba?ic?.
2008.
Higher-order functional representation of Croatian inflec-tional morphology.
In Proceedings of the 6th In-ternational Conference on Formal Approaches toSouth Slavic and Balkan Languages, pages 121?130,Dubrovnik, Croatia.Jan ?najder and Bojana Dalbelo Ba?ic?.
2010.
Acomputational model of Croatian derivational mor-phology.
In Proceedings of the 7th InternationalConference on Formal Approaches to South Slavicand Balkan Languages, pages 109?118, Dubrovnik,Croatia.Idan Szpektor and Ido Dagan.
2008.
Learning en-tailment rules for unary templates.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics, pages 849?856, Manchester, UK.Kapil Thadani and Kathleen McKeown.
2011.
To-wards strict sentence intersection: Decoding andevaluation strategies.
In Proceedings of the ACLWorkshop on Monolingual Text-To-Text Generation,pages 43?53, Portland, Oregon.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Andrea Zielinski and Christian Simon.
2008.
Mor-phisto - an open source morphological analyzer forGerman.
In Proceedings of the 7th InternationalWorkshop on Finite-State Methods and Natural Lan-guage Processing, pages 224?231, Ispra, Italy.1211
