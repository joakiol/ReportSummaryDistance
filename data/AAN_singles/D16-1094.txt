Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 944?949,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsRicher Interpolative Smoothing Based on Modified Kneser-NeyLanguage ModelingEhsan Shareghi,?
Trevor Cohn?
and Gholamreza Haffari??
Faculty of Information Technology, Monash University?
Computing and Information Systems, The University of Melbournefirst.last@{monash.edu,unimelb.edu.au}AbstractIn this work we present a generalisation of theModified Kneser-Ney interpolative smoothingfor richer smoothing via additional discountparameters.
We provide mathematical under-pinning for the estimator of the new discountparameters, and showcase the utility of ourrich MKN language models on several Euro-pean languages.
We further explore the in-terdependency among the training data size,language model order, and number of dis-count parameters.
Our empirical results illus-trate that larger number of discount parame-ters, i) allows for better allocation of mass inthe smoothing process, particularly on smalldata regime where statistical sparsity is se-vere, and ii) leads to significant reduction inperplexity, particularly for out-of-domain testsets which introduce higher ratio of out-of-vocabulary words.11 IntroductionProbabilistic language models (LMs) are the coreof many natural language processing tasks, such asmachine translation and automatic speech recogni-tion.
m-gram models, the corner stone of languagemodeling, decompose the probability of an utter-ance into conditional probabilities of words given afixed-length context.
Due to sparsity of the eventsin natural language, smoothing techniques are criti-cal for generalisation beyond the training text whenestimating the parameters of m-gram LMs.
Thisis particularly important when the training text is1For the implementation see: https://github.com/eehsan/cstlmsmall, e.g.
building language models for translationor speech recognition in low-resource languages.A widely used and successful smoothing methodis interpolated Modified Kneser-Ney (MKN) (Chenand Goodman, 1999).
This method uses a linear in-terpolation of higher and lower order m-gram prob-abilities by preserving probability mass via absolutediscounting.
In this paper, we extend MKN by in-troducing additional discount parameters, leading toa richer smoothing scheme.
This is particularly im-portant when statistical sparsity is more severe, i.e.,in building high-order LMs on small data, or whenout-of-domain test sets are used.Previous research in MKN language modeling,and more generally m-gram models, has mainlydedicated efforts to make them faster and more com-pact (Stolcke et al, 2011; Heafield, 2011; Shareghiet al, 2015) using advanced data structures such assuccinct suffix trees.
An exception is HierarchicalPitman-Yor Process LMs (Teh, 2006a; Teh, 2006b)providing a rich Bayesian smoothing scheme, forwhich Kneser-Ney smoothing corresponds to an ap-proximate inference method.
Inspired by this work,we directly enrich MKN smoothing realising someof the reductions while remaining more efficient inlearning and inference.We provide estimators for our additional discountparameters by extending the discount bounds inMKN.
We empirically analyze our enriched MKNLMs on several European languages in in- and out-of-domain settings.
The results show that our dis-counting mechanism significantly improves the per-plexity compared to MKN and offers a more elegant944way of dealing with out-of-vocabulary (OOV) wordsand domain mismatch.2 Enriched Modified Kneser-NeyInterpolative Modified Kneser-Ney (MKN) (Chenand Goodman, 1999) smoothing is widely acceptedas a state-of-the-art technique and is implemented inleading LM toolkits, e.g., SRILM (Stolcke, 2002)and KenLM (Heafield, 2011).MKN uses lower order k-gram probabilities tosmooth higher order probabilities.
P (w|u) is de-fined as,c(uw)?
Dm(c(uw))c(u) +?
(u)c(u) ?
P?
(w|pi(u))where c(u) is the frequency of the pattern u, ?(.)
isa constant ensuring the distribution sums to one, andP?
(w|pi(u)) is the smoothed probability computedrecursively based on a similar formula2 conditionedon the suffix of the pattern u denoted by pi(u).
Ofparticular interest are the discount parameters Dm(.
)which remove some probability mass from the max-imum likelihood estimate for each event which isredistributed over the smoothing distribution.
Thediscounts are estimated asDm(i) =??????
?0, if i = 01?
2n2[m]n1[m]n1[m]n1[m]+2n2[m] , if i = 12?
3n3[m]n2[m]n1[m]n1[m]+2n2[m] , if i = 23?
4n4[m]n3[m] .n1[m]n1[m]+2n2[m] , if i ?
3where ni(m) is the number of unique m-grams3 offrequency i.
This effectively leads to three discountparameters {Dm(1),Dm(2),Dm(3+)} for the distri-butions on a particular context length, m.2.1 Generalised MKNNey et al (1994) characterized the data sparsity us-ing the following empirical inequalities,3n3[m] < 2n2[m] < n1[m] for m ?
3It can be shown (see Appendix A) that these em-pirical inequalities can be extended to higher fre-2Note that in all but the top layer of the hierarchy, con-tinuation counts, which count the number of unique contexts,are used in place of the frequency counts (Chen and Goodman,1999).3Continuation counts are used for the lower layers.quencies and larger contexts m > 3,(N ?m)nN?m[m] < ... < 2n2[m]< n1[m] <?i>0ni[m] n0[m] < ?mwhere ?m is the possible number of m-grams overa vocabulary of size ?, n0[m] is the number of m-grams that never occurred, and ?i>0 ni[m] is thenumber of m-grams observed in the training data.We use these inequalities to extend the discountdepth of MKN, resulting in new discount parame-ters.
The additional discount parameters increase theflexibility of the model in altering a wider range ofraw counts, resulting in a more elegant way of as-signing the mass in the smoothing process.
In ourexperiments, we set the number of discounts to 10for all the levels of the hierarchy, (compare this tothese in MKN).4 This results in the following esti-mators for the discounts,Dm(i) =??
?0, if i = 0i?
(i + 1)ni+1[m]ni[m]n1[m]n1[m]+2n2[m] , if i < 1010?
11n11[m]n10[m] .n1[m]n1[m]+2n2[m] , if i ?
10It can be shown that the above estimators for our dis-count parameters are derived by maximizing a lowerbound on the leave-one-out likelihood of the trainingset, following (Ney et al, 1994; Chen and Goodman,1999) (see Appendix B for the proof sketch).3 ExperimentsWe compare the effect of using different numbers ofdiscount parameters on perplexity using the Finnish(FI), Spanish (ES), German (DE), English (EN) por-tions of the Europarl v7 (Koehn, 2005) corpus.
Foreach language we excluded the first 10K sentencesand used it as the in-domain test set (denoted as EU),skipped the second 10K sentences, and used the restas the training set.
The data was tokenized, sentencesplit, and the XML markup discarded.
We testedthe effect of domain mismatch, under two settingsfor out-of-domain test sets: i) mild using the Span-ish section of news-test 2013, the German, Englishsections of news-test 2014, and the Finnish section4We have selected the value of 10 arbitrarily; however ourapproach can be used with larger number of discount parame-ters, with the caveat that we would need to handle sparse countsin the higher orders.945Perplexitysize (M) size (K) MKN (D1...3) MKN (D[1...4]) MKN (D[1...10])Training tokens sents Test tokens sents OOV% m = 2 m = 5 m = 10 m = 2 m = 5 m = 10 m = 2 m = 5 m = 10NT 19.8 3 9.2 6536.6 5900.3 5897.3 6451.3 5827.6 5824.6 6154.4 5575.0 5572.5FI 46.5 2.2 EU 197.3 10 6.1 390.7 287.4 286.8 390.7 287.3 286.6 390.4 287.3 286.8TW 10.9 1.3 52.1 57 825.1 51 744.1 51 740.1 55 550.2 49 884.2 49 881.3 47 696.2 43 277.3 43 275.5NT 70.7 3 9.1 565.6 431.5 429.4 560.0 425.5 423.5 541.5 409.0 407.3ES 68.0 2.2 EU 281.5 10 2.4 92.7 51.5 51.1 92.8 51.5 51.1 92.8 51.4 51.0TW 3141.3 293 78.5 17 804.2 14 062.7 14 027.1 17 121.4 13 487.4 13 454.1 14 915.7 11 832.1 11 807.2NT 64.5 3 18.7 2190.7 1784.6 1781.8 2158.9 1755.8 1753.2 2065.3 1680.6 1678.3DE 61.2 2.3 EU 244.0 10 4.6 156.9 91.7 91.2 156.9 91.6 91.2 156.4 91.7 91.2MED 317.7 10 59.8 5135.7 4232.4 4226.7 5007.5 4123.0 4117.5 4636.0 3831.2 3826.6NT 69.5 3 5.5 1089.2 875.0 872.2 1071.1 857.2 854.4 1011.5 806.7 804.4EN 67.5 2.2 EU 274.9 10 1.7 90.1 48.4 48.1 90.1 48.3 48.0 90.5 48.3 48.0MED 405.9 10 44.1 2319.7 1947.9 1942.5 2261.6 1893.3 1888.2 2071.9 1734.9 1730.8Table 1: Perplexity for various m-gram orders m ?
2, 3, 10 and training languages from Europarl, using differentnumbers of discount parameters for MKN.
MKN (D[1...3]), MKN (D[1...4]), MKN (D[1...10]) represent vanilla MKN,MKN with 1 more discounts, and MKN with 7 more discount parameters, respectively.
Test sets sources EU, NT,TW, MED are Europarl, news-test, Twitter, and medical patent descriptions, respectively.
OOV is reported as the ratio|{OOV ?test-set}||{w?test-set}| .0124814CZ FI ES DE EN FR CZ FI ES DE EN FR[Corpus][%Perplexity Reduction]pplD[1...10]pplD[1...4]Figure 1: Percentage of perplexity reduction forpplxD[1...4] and pplxD[1...10] compared with pplxD[1..3] ondifferent training corpora (Europarl CZ, FI, ES, DE, EN,FR) and on news-test sets (NT) for m = 2 (left), andm = 10 (right).of news-test 2015 (all denoted as NT)5, and ii) ex-treme using a 24 hour period of streamed Finnish,and Spanish tweets6 (denoted as TW), and the Ger-man and English sections of the patent descriptionof medical translation task7 (denoted as MED).
SeeTable 1 for statistics of the training and test sets.3.1 PerplexityTable 1 shows substantial reduction in perplexity onall languages for out-of-domain test sets when ex-panding the number of discount parameters from 3in vanilla MKN to 4 and 10.
Consider the English5http://www.statmt.org/{wmt13,14,15}/test.tgz6Streamed via Twitter API on 17/05/2016.7http://www.statmt.org/wmt14/medical-task/news-test (NT), in which even for a 2-gram languagemodel a single extra discount parameter (m = 2,D[1...4]) improves the perplexity by 18 points andthis improvement quadruples to 77 points when us-ing 10 discounts (m = 2, D[1...10]).
This effectis consistent across the Europarl corpora, and forall LM orders.
We observe a substantial improve-ments even for m = 10-gram models (see Figure 1).On the medical test set which has 9 times higherOOV ratio, the perplexity reduction shows a simi-lar trend.
However, these reductions vanish when anin-domain test set is used.
Note that we use the sametreatment of OOV words for computing the perplex-ities which is used in KenLM (Heafield, 2013).3.2 AnalysisOut-of-domain and Out-of-vocabulary We se-lected the Finnish language for which the numberand ratio of OOVs are close on its out-of-domainand in-domain test sets (NT and EU), while show-ing substantial reduction in perplexity on out-of-domain test set, see FI bars on Figure 1.
Figure 2(left), shows the full perplexity results for Finnishfor vanilla MKN, and our extensions when tested onin-domain (EU) and out-of-domain (NT) test sets.The discount plot, Figure 2 (middle) illustrates thebehaviour of the various discount parameters.
Wealso measured the average hit length for queries byvaryingm on in-domain and out-of-domain test sets.As illustrated in Figure 2 (right) the in-domain testset alows for longer matches to the training data as9462 3 4 5 6 7 8 9 10 ?2853905500580059006550mPerplexityll l l l l l l l lll l l l l l l l ll D[1...3]D[1...4]D[1...10] NTEU1 2 3 4 5 6 7 8 9 1011.52.53.54.5iDiscountlllll lll llll l l ll l l llll l l lllllll l llllllll lllllllllll1?gram2?gram3?gram4?gram5?gram6?gram7?gram8?gram9?gram10?gram2 3 4 5 6 7 8 9 1011.522.53mAverage hitlengthNTEUFigure 2: Statistics for the Finnish section of Europarl.
The left plot illustrates the perplexity when tested on an out-of-domain (NT) and in-domain (EU) test sets varying LM order, m. The middle plot shows the discount parametersDi?
[1...10] for different m-gram orders.
The right plot correspond to average hit length on EU and NT test sets.m DiscountPerplexity2 3 4 5 6 7 8 9 10 341016782190m DiscountPerplexity2 3 4 5 6 7 8 9 10 3410180426Figure 3: Perplexity (z-axis) vs. m ?
[2...10] (x-axis) vs.number of discounts Di?3,4,10 (y-axis) for German lan-guage trained on Europarl (left), and CommonCrawl2014(right) and tested on news-test.
Arrows show the direc-tion of the increase.m grows.
This indicates that having more discountparameters is not only useful for test sets with ex-tremely high number of OOV, but also allows for amore elegant way of assigning mass in the smooth-ing process when there is a domain mismatch.Interdependency of m, data size, and discountsTo explore the correlation between these factors weselected the German and investigated this correla-tion on two different training data sizes: Europarl(61M words), and CommonCrawl 2014 (984Mwords).
Figure 3 illustrates the correlation betweenthese factors using the same test set but with smalland large training sets.
Considering the slopes of thesurfaces indicates that the small training data regime(left) which has higher sparsity, and more OOV inthe test time benefits substantially from the more ac-curate discounting compared to the large training set(right) in which the gain from discounting is slight.88Nonetheless, the improvement in perplexity consistentlygrows with introducing more discount parameters even under4 ConclusionsIn this work we proposed a generalisation of Modi-fied Kneser-Ney interpolative language modeling byintroducing new discount parameters.
We providethe mathematical proof for the discount bounds usedin Modified Kneser-Ney and extend it further and il-lustrate the impact of our extension empirically ondifferent Europarl languages using in-domain andout-of-domain test sets.The empirical results on various training and testsets show that our proposed approach allows for amore elegant way of treating OOVs and mass assign-ments in interpolative smoothing.
In future work,we will integrate our language model into the Mosesmachine translation pipeline to intrinsically measureits impact on translation qualities, which is of partic-ular use for out-of-domain scenario.AcknowledgementsThis research was supported by the National ICTAustralia (NICTA) and Australian Research CouncilFuture Fellowship (project number FT130101105).This work was done when Ehsan Shareghi was anintern at IBM Research Australia.A.
InequalitiesWe prove that these inequalities hold in expectationby making the reasonable assumption that events inthe large training data regime, which suggests that more dis-count parameters, e.g., up to D30, may be required for largertraining corpus to reflect the fact that even an event with fre-quency of 30 might be considered rare in a corpus of nearly 1billion words.947the natural language follow the power law (Clausetet al, 2009), p(C(u) = f) ?
f?1?
1sm , where smis the parameter of the distribution, and C(u) is therandom variable denoting the frequency of the m-grams pattern u.
We now compute the expectednumber of unique patterns having a specific fre-quency E[ni[m]].
Corresponding to each m-gramspattern u, let us define a random variable Xu whichis 1 if the frequency of u is i and zero otherwise.
Itis not hard to see that ni[m] = ?uXu, andE[ni[m]]= E[?uXu]=?uE[Xu] = ?mE[Xu]= ?m(p(C(u) = i)?
1 + p(C(u) 6= i)?
0)?
?mi?1?
1sm .We can verify that(i+ 1)E[ni+1[m]]< iE[ni[m]]?
(i+ 1)?m(i+ 1)?1?
1sm < i?mi?1?
1sm ?i 1sm < (i+ 1) 1sm .which completes the proof of the inequalities.B.
Discount bounds proof sketchThe leave-one-out (leaving those m-grams whichoccurred only once) log-likelihood function of theinterpolative smoothing is lower bounded by back-off model?s (Ney et al, 1994), hence the estimateddiscounts for later can be considered as an approx-imation for the discounts of the former.
Consider abackoff model with absolute discounting parameterD, were P (wi|wi?1i?m+1) is defined as:????????
?c(wii?m+1)?Dc(wi?1i?m+1)if c(wii?m+1) > 0Dn1+(wi?1i?m+1 ?)c(wi?1i?m+1)P?
(wi|wi?1i?m+2) if c(wii?m+1) = 0where n1+(wi?1i?m+1 ?)
is the number of uniqueright contexts for the wi?1i?m+1 pattern.
Assume thatfor any choice of 0 < D < 1 we can define P?
suchthat P (wi|wi?1im+1) sums to 1.
For readability weuse the ?
(wi?1i?m+1) =n1+(wi?1i?m+1 ?
)c(wi?1i?m+1)?1replacement.Following (Chen and Goodman, 1999), rewritingthe leave-one-out log-likelihood for KN (Ney et al,1994) to include more discounts (in this proof up toD4), results in:?wii?m+1c(wii?m+1)>4c(wii?m+1) logc(wii?m+1)?
1?D4c(wi?1i?m+1)?
1+4?j=2(?wii?m+1c(wii?m+1)=jc(wii?m+1) logc(wii?m+1)?
1?Dj?1c(wi?1i?m+1)?
1)+?wii?m+1c(wii?m+1)=1(c(wii?m+1) log(4?j=1nj [m]Dj)?(wi?1i?m+1)P?
)which can be simplified to,?wii?m+1c(wii?m+1)>4c(wii?m+1) log(c(wii?m+1)?
1?D4)+4?j=2(jnj [m] log(j ?
1?Dj?1))+n1[m] log(4?j=1nj [m]Dj) + constTo find the optimal D1, D2, D3, D4 we set the par-tial derivatives to zero.
For D3,?
?D3= n1[m]n3[m]?4j=1 nj [m]Dj?
4n4[m]3?D3= 0?n1[m]n3[m](3?D3) = 4n4[m]4?j=1nj [m]Dj ?3n1[m]n3[m]?D3n1[m]n3[m]?
4n4[m]n1[m]D1 > 0?
3?
4n4[m]n3[m]D1 > D3 And after taking c(wii?m+1) = 5 out of the summa-tion, for D4:??D4=?c(wii?m+1)>5?c(wii?m+1)c(wii?m+1)?
1?D?
5n5[m]4?D4+ n1[m]n4[m]?4j=1 nj [m]Dj= 0?
?5n5[m]4?D4+ n1[m]n4[m]?4j=1 nj [m]Dj> 0?
n1[m]n4[m](4?D4)> 5n5[m]4?j=1nj [m]Dj ?
4?
5n5[m]n4[m]D1 > D4 948ReferencesStanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech & Language, 13(4):359?393.Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJNewman.
2009.
Power-law distributions in empiricaldata.
SIAM review, 51(4):661?703.Kenneth Heafield.
2011.
KenLM: Faster and smaller lan-guage model queries.
In Proceedings of the Workshopon Statistical Machine Translation.Kenneth Heafield.
2013.
Efficient Language ModelingAlgorithms with Applications to Statistical MachineTranslation.
Ph.D. thesis, Carnegie Mellon University.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of theMachine Translation summit.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochasticlanguage modelling.
Computer Speech & Language,8(1):1?38.Ehsan Shareghi, Matthias Petri, Gholamreza Haffari, andTrevor Cohn.
2015.
Compact, efficient and unlimitedcapacity: Language modeling with compressed suffixtrees.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.Andreas Stolcke, Jing Zheng, Wen Wang, and VictorAbrash.
2011.
SRILM at sixteen: Update andoutlook.
In Proceedings of IEEE Automatic SpeechRecognition and Understanding Workshop.Andreas Stolcke.
2002.
SRILM?an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference of Spoken Language Processing.Yee Whye Teh.
2006a.
A Bayesian interpretation of in-terpolated Kneser-Ney.
Technical report, NUS Schoolof Computing.Yee Whye Teh.
2006b.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics.949
