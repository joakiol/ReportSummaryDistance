Domain Portability in Speech-to-Speech TranslationAlon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin HanAlicia Tribble, Donna Gates, Dorcas Wallace and Kay PetersonLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, USAalavie@cs.cmu.edu1.
INTRODUCTIONSpeech-to-speech translation has made significant advances overthe past decade, with several high-visibility projects (C-STAR, Verb-mobil, the Spoken Language Translator, and others) significantlyadvancing the state-of-the-art.
While speech recognition can cur-rently effectively deal with very large vocabularies and is fairlyspeaker independent, speech translation is currently still effectiveonly in limited, albeit large, domains.
The issue of domain porta-bility is thus of significant importance, with several current researchefforts designed to develop speech-translation systems that can beported to new domains with significantly less time and effort thanis currently possible.This paper reports on three experiments on portability of a speech-to-speech translation system between semantic domains.1 The ex-periments were conducted with the JANUS system [5, 8, 12], ini-tially developed for a narrow travel planning domain, and portedto the doctor-patient domain and an extended tourism domain.
Theexperiments cover both rule-based and statistical methods, and hand-written as well as automatically learned rules.
For rule-based sys-tems, we have investigated the re-usability of rules and other knowl-edge sources from other domains.
For statistical methods, we haveinvestigated how much additional training data is needed for eachnew domain.
We are also experimenting with combinations ofhand-written and automatically learned components.
For speechrecognition, we have conducted studies of what parameters changewhen a recognizer is ported from one domain to another, and howthese changes affect recognition performance.2.
DESCRIPTION OF THE INTERLINGUAThe first two experiments concern the analysis component of ourinterlingua-based MT system.
The analysis component takes a sen-tence as input and produces an interlingua representation as output.We use a task-oriented interlingua [4, 3] based on domain actions.Examples of domain actions are giving information about the on-set of a symptom (e.g., I have a headache) or asking a patient1We have also worked on the issue of portability across languagesvia our interlingua approach to translation [3] and on portability ofspeech recognition across languages [10]..to perform some action (e.g., wiggle your fingers).
The interlin-gua, shown in the example below, has five main components: (1) aspeaker tag such as a: for doctor (agent) and c: for a patient (cus-tomer), (2) a speech act, in this case, give-information (3)some concepts (+body-state and+existence), and (4) somearguments (body-state-spec= andbody-location=), and(5) some sub-arguments (identifiability=no andinside=head).I have a pain in my head.c:give-information+existence+body-state(body-state-spec=(pain,identifiability=no),body-location=(inside=head))3.
EXPERIMENT 1:EXTENSION OF SEMANTIC GRAMMARRULES BY HAND AND BY AUTOMATICLEARNINGExperiment 1 concerns extension of the coverage of semanticgrammars in the medical domain.
Semantic grammars are basedon semantic constituents such as request information phrases (e.g.,I was wondering : : : ) and location phrases (e.g., in my right arm)rather than syntactic constituents such as noun phrases and verbphrases.
In other papers [12, 5], we have described how our mod-ular grammar design enhances portability across domains.
Theportable grammar modules are the cross-domain module, contain-ing rules for things like greetings, and the shared module, contain-ing rules for things like times, dates, and locations.
Figure 1 showsa parse tree for the sentence How long have you had this pain?
XDMindicates nodes that were produced by cross-domain rules.
MED in-dicates nodes that were produced by rules from the new medicaldomain grammar.The preliminary doctor-patient grammar focuses on three med-ical situations: give-information+existence ?
givinginformation about the existence of a symptom (I have been get-ting headaches); give-information+onset ?
giving infor-mation about the onset of a symptom (The headaches started threemonths ago); and give-information+occurrence ?
giv-ing information about the onset of an instance of the symptoms(The headaches start behind my ears).
Symptoms are expressedas body-state (e.g., pain), body-object (e.g., rash), andbody-event (e.g., bleeding).Our experiment on extendibility was based on a hand writtenseed grammar that was extended by hand and by automatic learn-ing.
The seed grammar covered the domain actions mentionedabove, but did not cover very many ways to phrase each domainaction.
For example, it might have covered The headaches started[request-information+existence+body-state]::MED( WH-PHRASES::XDM( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )HAVE-GET-FEEL::MED ( GET ( have ) ) youHAVE-GET-FEEL::MED ( HAS ( had ) )[super_body-state-spec=]::MED( [body-state-spec=]::MED( ID-WHOSE::MED( [identifiability=]( [id:non-distant] ( this ) ) )BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )Figure 1: Parser output with nodes produced by medical and cross-domain grammars.Seed Extended LearnedIF 37.2 37.2 31.3Domain Action 37.2 37.2 31.3Speech ActRecall 43.3 48.2 49.3Precision 71.0 75.0 45.8Concept ListRecall 2.2 10.1 32.5Precision 12.5 42.2 25.1Top-Level ArgumentsRecall 0.0 7.2 29.6Precision 0.0 42.2 34.4Top-Level ValuesRecall 0.0 8.3 29.8Precision 0.0 50.0 39.2Sub-Level ArgumentsRecall 0.0 28.3 14.1Precision 0.0 48.2 12.6Sub-level ValuesRecall 1.2 28.3 14.1Precision 6.2 48.2 12.9Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen datathree months ago but not I started getting the headaches three monthsago.
The seed grammar was extended by hand and by automaticlearning to cover a development set of 133 utterances.
The re-sult was two new grammars, a human-extended grammar and amachine-learned grammar, referred to as the extended and learnedgrammars in Table 1.
The two new grammars were then tested on132 unseen sentences in order to compare generality of the rules.Results are reported only for 83 of the 132 sentences which werecovered by the current interlingua design.
The remaining 49 sen-tences were not covered by the current interlingua design and werenot scored.
Results are shown in Table 1.The parsed test sentences were scored in comparison to a hand-coded correct interlingua representation.
Table 1 separates resultsfor six components of the interlingua: speech act, concepts, top-level arguments, top-level values, sub-level arguments, and sub-level values, in addition to the total interlingua, and the domainaction (speech act and concepts combined).
The components of theinterlingua were described in Section 2.The scores for the total interlingua and domain action are re-ported as percent correct.
The scores for the six components of theinterlingua are reported as average percent precision and recall.
Forexample, if the correct interlingua for a sentence has two concepts,and the parser produces three, two of which are correct and one ofwhich is incorrect, the precision is 66% and the recall is 100%.Several trends are reflected in the results.
Both the human-ex-tended grammar and the machine-learned grammar show improvedperformance over the seed grammar.
However, the human extendedgrammar tended to outperform the automatically learned grammarin precision, whereas the automatically learned grammar tended tooutperform the human extended grammar in recall.
This result is tobe expected: humans are capable of formulating correct rules, butmay not have time to analyze the amount of data that a machine cananalyze.
(The time spent on the human extended grammar after theseed grammar was complete was only five days.
)Grammar Induction: Our work on automatic grammar induc-tion for Experiment 1 is still in preliminary stages.
At this point,we have experimented with completely automatic induction (no in-teraction with a user)2 of new grammar rules starting from a coregrammar and using a development set of sentences that are notparsable according to the core grammar.
The development sen-tences are tagged with the correct interlingua, and they do not strayfrom the concepts covered by the core grammar ?
they only cor-respond to alternative (previously unseen) ways of expressing thesame set of covered concepts.
The automatic induction is basedon performing tree matching between a skeletal tree representationobtained from the interlingua, and a collection of parse fragments2Previous work on our project [2] investigated learning of grammarrules with user interaction.
[give-information+onset+symptom][manner=][sudden]suddenly[symptom-location=]DETPDETPOSSmyBODYLOCATIONBODYFLUID[urine]urinebecame [adj:symptom-name=]ADJ-SYMPTOMFUNCTION-ADJ-VALS [attribute=][color_attribute]colored[abnormal]disParse chunk #1 Parse chunk #2 Parse chunk #3Original interlingua:give-information+onset+symptom(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,manner=sudden)Learned Grammar Rule:s[give-information+onset+symptom]( [manner=] [symptom-location=] *+became [adj:symptom-name=] )Figure 2: A reconstructed parse tree from the Interlinguathat is derived from parsing the new sentence with the core gram-mar.
Extensions to the existing rules are hypothesized in a way thatwould produce the correct interlingua representation for the inpututterance.Figure 2 shows a tree corresponding to an automatically learnedrule.
The input to the learning algorithm is the interlingua (shownin bold boxes in the figure) and three parse chunks (circled in thefigure).
The dashed edges are augmented by the learning algorithm.4.
EXPERIMENT 2:PORTING TO A NEW DOMAINUSING A HYBRID RULE-BASED ANDSTATISTICAL ANALYSIS APPROACHWe are in the process of developing a new alternative analysisapproach for our interlingua-based speech-translation systems thatcombines rule-based and statistical methods and we believe inher-ently supports faster porting into new domains.
The main aspectsof the approach are the following.
Rather than developing com-plete semantic grammars for analyzing utterances into our interlin-gua (either completely manually, or using grammar induction tech-niques), we separate the task into two main levels.
We continue todevelop and maintain rule-based grammars for phrases that corre-spond to argument-level concepts of our interlingua representation(e.g., time expressions, locations, symptom-names, etc.).
However,instead of developing grammar rules for assembling the argument-level phrases into appropriate domain actions, we apply machinelearning and classification techniques [1] to learn these mappingsfrom a corpus of interlingua tagged utterances.
(Earlier work onthis task is reported in [6].
)We believe this approach should prove to be more suitable forfast porting into new domains for the following reasons.
Many ofthe required argument-level phrase grammars for a new domain arelikely to be covered by already existing grammar modules, as canbe seen by examining the XDM (cross-domain) nodes in Figure 1.The remaining new phrase grammars are fairly fast and straightfor-ward to develop.
The central questions, however, are whether thestatistical methods used for classifying strings of arguments intodomain actions are accurate enough, and what amounts of taggeddata are required to obtain reasonable levels of performance.
Toassess this last question, we tested the performance of the currentspeech-act and concept classifiers for the expanded travel-domainwhen trained with increasing amounts of training data.
The resultsof these experiments are shown in Figure 3.
We also report theperformance of the domain-action classification derived from thecombined speech-act and concepts.
As can be seen, performancereaches a relative plateau at around 4000-5000 utterances.
We seethese results as indicative that this approach should indeed prove tobe significantly easier to port to new domains.
Creating a taggeddatabase of this order of magnitude can be done in a few weeks,rather than the months required for complete manual grammar de-velopment time.5.
EXPERIMENT 3:PORTING THE SPEECH RECOGNIZERTO NEW DOMAINSWhen the speech recognition components (acoustic models, pro-nunciation dictionary, vocabulary, and language model) are portedacross domains and languages mainly three types of mismatchesSpeech Act Classification Accuracy for 16-foldCross-Validation00.10.20.30.40.50.60.70.8500 1000 2000 3000 4000 5000 6009Training Set SizeMeanAccuracyConcept Sequence Classification Accuracy for 16-fold Cross-Validation00.10.20.30.40.50.60.7500 1000 2000 3000 4000 5000 6009Training Set SizeMeanAccuracyDialog Act Classification Accuracy for 16-foldCross-Validation00.050.10.150.20.250.30.350.40.450.5500 1000 2000 3000 4000 5000 6009Training Set SizeMeanAccuracyFigure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training DataBaseline Systems WER on Different Tasks [%]BN (Broadcast News) h4e98 1, all F-conditions 18.5ESST (scheduling and travel planning domain) 24.3BN+ESST 18.4C-STAR (travel planning domain) 20.2Adaptation!Meeting RecognitionESST on meeting data 54.1BN on meeting data 44.2+ acoustic MAP Adaptation (10h meeting data) 40.4+ language model interpolation (16 meetings) 38.7BN+ESST on meeting data 42.2+ language model interpolation (16 meetings) 39.0Adaptation!
Doctor-Patient DomainC-STAR on doctor-patient data 34.1+ language model interpolation ( 34 dialogs) 25.1Table 2: Recognition Resultsoccur: (1) mismatches in recording condition; (2) speaking stylemismatches; as well as (3) vocabulary and language model mis-matches.
In the past these problems have mostly been solved bycollecting large amounts of acoustic data for training the acousticmodels and development of the pronunciation dictionary, as wellas large text data for vocabulary coverage and language model cal-culation.
However, especially for highly specialized domains andconversational speaking styles, large databases cannot always beprovided.
Therefore, our research has focused on the problem ofhow to build LVCSR systems for new tasks and languages [7, 9]using only a limited amount of data.
In this third experiment weinvestigate the results of porting the speech recognition componentof our MT system to different new domains.
The experiments andimprovements were conducted with the Janus Speech RecognitionToolkit JRTk [13].Table 2 shows the results of porting four baseline speech recog-nition systems to the doctor-patient domain, and to the meeting do-main.
The four baseline systems are trained on Broadcast News(BN), English SpontaneousScheduling Task (ESST), combined BNand ESST, and the travel planning domain of the C-STAR consor-tium (http://www.c-star.org).
The given tasks illustratea variety of domain size, speaking styles and recording conditionsranging from clean spontaneous speech in a very limited domain(ESST, C-STAR) to highly conversational multi-party speech in anextremely broad domain (Meeting).
As a consequence the errorrates on the meeting data are quite high but using MAP (MaximumA Posteriori) acoustic model adaptation and language model adap-tation the error rate can be reduced by about 10.2% relative over theBN baseline system.
With the doctor-patient data the drop in errorrate was less severe which can be explained by the similar speakingstyle and recording conditions for C-STAR and doctor-patient data.Details about the applied recognition engine can be found in [10]for ESST and [11] for the BN system.6.
ACKNOWLEDGMENTSThe research work reported here was funded in part by the DARPATIDES Program and supported in part by the National ScienceFoundation under Grant number 9982227.
Any opinions, findingsand conclusions or recomendations expressed in this material arethose of the author(s) and do not necessarily reflect the views of theNational Science Foundation (NSF) or DARPA.7.
REFERENCES[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van denBosch.
TiMBL: Tilburg Memory Based Learner, version 3.0Reference Guide.
Technical Report Technical Report 00-01,ILK, 2000.
Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`.
Epiphenomenal Grammar Acquisition withGSG.
In Proceedings of the Workshop on ConversationalSystems of the 6th Conference on Applied Natural LanguageProcessing and the 1st Conference of the North AmericanChapter of the Association for Computational Linguistics(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,T.
Watanabe, and M. Woszczyna.
Evaluation of a PracticalInterlingua for Task-Oriented Dialogue.
In Workshop onApplied Interlinguas: Practical Applications of InterlingualApproaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel.
An InterlinguaBased on Domain Actions for Machine Translation ofTask-Oriented Dialogues.
In Proceedings of the InternationalConference on Spoken Language Processing (ICSLP?98),pages Vol.
4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,D.
Koll, and A. Waibel.
The Janus-III Translation System.Machine Translation.
To appear.
[6] M. Munk.
Shallow statistical parsing for machine translation.Master?s thesis, University of Karlsruhe, Karlsruhe,Germany, 1999.
http://www.is.cs.cmu.edu/papers/speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel.
Polyphone Decision TreeSpecialization for Language Adaptation.
In Proceedings ofthe ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel.
Interactive Translation of Conversational Speech.Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,and M. Woszczyna.
Multilinguality in Speech and SpokenLanguage Systems.
Proceedings of the IEEE, Special Issueon Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.Multilingual Speech Recognition, chapter From Speech Inputto Augmented Word Lattices, pages 33?45.
Springer Verlag,Berlin, Heidelberg, New York, artificial Intelligence edition,2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,F.
Metze, and M. Bett.
Advances in Meeting Recognition.Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,A.
Lavie, L. Levin, and A. Waibel.
A Modular Approach toSpoken Language Translation for Large Domains.
InProceedings of Conference of the Association for MachineTranslation in the Americas (AMTA?98), Langhorn, PA,October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.Recognition of Conversational Telephone Speech using theJanus Speech Engine.
In Proceedings of the ICASSP?97,Mu?nchen, Germany, 1997.
