Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 193?202, Dublin, Ireland, August 23-29 2014.A Study of using Syntactic and Semantic Structuresfor Concept Segmentation and LabelingIman Saleh?, Shafiq Joty, Llu?
?s M`arquez,Alessandro Moschitti, Preslav NakovALT Research GroupQatar Computing Research Institute{sjoty,lmarquez,amoschitti,pnakov}@qf.org.qaScott Cyphers, Jim GlassMIT CSAILCambridge, Massachusetts 02139USA{cyphers,glass}@mit.eduAbstractThis paper presents an empirical study on using syntactic and semantic information for ConceptSegmentation and Labeling (CSL), a well-known component in spoken language understand-ing.
Our approach is based on reranking N -best outputs from a state-of-the-art CSL parser.
Weperform extensive experimentation by comparing different tree-based kernels with a variety ofrepresentations of the available linguistic information, including semantic concepts, words, POStags, shallow and full syntax, and discourse trees.
The results show that the structured representa-tion with the semantic concepts yields significant improvement over the base CSL parser, muchlarger compared to learning with an explicit feature vector representation.
We also show thatshallow syntax helps improve the results and that discourse relations can be partially beneficial.1 IntroductionSpoken Language Understanding aims to interpret user utterances and to convert them to logical forms,or, equivalently, database queries, which can then be used to satisfy the user?s information needs.
Thisprocess is known as Concept Segmentation and Labeling (CSL): it maps utterances into meaning repre-sentations based on semantic constituents.
The latter are basically sequences of semantic entities, oftenreferred to as concepts, attributes or semantic tags.
Traditionally, grammar-based methods have beenused for CSL, but more recently machine learning approaches to semantic structure computation havebeen shown to yield higher accuracy.
However, most previous work did not exploit syntactic/semanticstructures of the utterances, and the state-of-the-art is represented by conditional models for sequence la-beling, such as Conditional Random Fields (Lafferty et al., 2001) trained with simple morphological andlexical features.
In our study, we measure the impact of syntactic and discourse structures by also com-bining them with innovative features.
In the following subsections, we present the application contextfor our CSL task and then we outline the challenges and the findings of our research.1.1 Semantic parsing for the ?restaurant?
domainWe experiment with the dataset of McGraw et al.
(2012), containing spoken and typed questions aboutrestaurants, which are to be answered using a database of free text such as reviews, categorical data suchas names and locations, and semi-categorical data such as user-reported cuisines and amenities.Semantic parsing, in the form of sequential segmentation and labeling, makes it easy to convert spokenand typed questions such as ?cheap lebanese restaurants in doha with take out?
into database queries.First, a language-specific semantic parser tokenizes, segments and labels the question:[Pricecheap] [Cuisinelebanese] [Otherrestaurants in] [Citydoha] [Otherwith] [Amenitytake out]Then, label-specific normalizers are applied to the segments, with the option to possibly relabel mis-labeled segments; at this point, discourse history may be incorporated as well.
[Pricelow] [Cuisinelebanese] [Citydoha] [Amenitycarry out]?Iman Saleh (iman.saleh@fci-cu.edu.eg) is affiliated to Faculty of Computers and Information, Cairo University.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/193Finally, a database query is formed from the list of labels and values, and is then executed against thedatabase, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed.
{$and [{cuisine:"lebanese"}, {city:"doha"}, {price:"low"}, {amenity:"carry out"}]}1.2 Related work on CSLPieraccini et al.
(1991) used Hidden Markov Models (HMMs) for CSL, where the observations wereword sequences and the hidden states were meaning units, i.e, concepts.
In subsequent work (Rubinsteinand Hastie, 1997; Santaf?e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other genera-tive models were applied, which model the joint probability of a word sequence and a concept sequence,as well as discriminative models, which directly model a conditional probability over the concepts in theinput text.Seneff (1989) and Miller et al.
(1994) used stochastic grammars for CSL.
In particular, they appliedstochastic Finite State Transducers (FST) for recognizing constituent annotations.
FSTs describe localsyntactic structures with a sequence of words, e.g., noun phrases or even constituents.
Papineni et al.
(1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Laffertyet al., 2001) are considered to be the state-of-the-art.
More recently, Wang et al.
(2009) illustrated anapproach for CSL that is specific to query understanding for web applications.
A general survey of CSLapproaches can be found in (De Mori et al., 2008).
CSL is also connected to a large body of work onshallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview.Another relevant line of research with a considerable body of work is reranking in NLP.
Tree kernelsfor reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002).
Some variants usedexplicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins,2005).
Other reranking work using tree kernels regards predicate argument structures (Moschitti et al.,2006) and named entities (Nguyen and Moschitti, 2012).
In (Dinarelli et al., 2011), we rerank CSLhypotheses using structures built on top of concepts, words and features that are simpler than thosestudied in this paper.
The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similarto ours, as it models the extraction of semantics as a reranking task using string kernels.1.3 Syntactic and semantic structures for CSLThe related work has highlighted that automatic CSL is mostly based on powerful machine learning al-gorithms and simple feature representations based on word and tag n-grams.
In this paper, we study theimpact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing anddiscourse structure.
We use a reranking approach to select the best hypothesis annotated with conceptsderived by a local model, where the hypotheses are represented as trees enriched with semantic con-cepts similarly to (Dinarelli et al., 2011).
These tree-based structures can capture dependencies betweensentence constituents and concepts.
However, extracting features from them is rather difficult as theirnumber is exponentially large.
Thus, we rely on structural kernels (e.g., see (Moschitti, 2006)) for au-tomatically encoding tree fragments, which represent syntactic and semantic dependencies from wordsand concepts, and we train the reranking functions with Support Vector Machines (e.g., see (Joachims,1999)).
Additionally, we experiment with several types of kernels and newly designed feature vectors.We test our models on the above-mentioned Restaurant domain.
The results show that (i) the basicCRF model, in fact semi-CRF (see below), is very accurate, achieving more than 83% in F1-score, whichindicates that improving over the semi-CRF approach is very hard; (ii) the upper-bound performanceof the reranking approach is very high as well, i.e., the correct annotation is generated in the first 100hypotheses in 98.72% of the cases; (iii) our feature vectors show improvement only when all featuregroups are used together; otherwise, we only observe marginal improvement; (iv) structural kernels yielda 10% relative error reduction from the semi-CRF baseline, which is more than double the feature vectorresult; (v) syntactic information significantly improves on the best model, but only when using shallowsyntax; and finally, (vi) although, discourse structures provide good improvement over the semi-CRFmodel, they perform lower than shallow syntax (thus, a valuable use of discourse features is still an openproblem that we plan to pursue in future work).1942 CSL rerankingReranking is based on a list of N annotation hypotheses, which are generated and sorted by probabilityusing local classifiers.
Then a reranker, typically a meta-classifier, tries to select the best hypothesis fromthe list.
The reranker can exploit global information, and, specifically, the dependencies between thedifferent concepts that are made available by the local model.
We use semi-CRF as our local model sinceit yields the highest accuracy in CSL (when using a single model), and preference reranking with kernelmachines to rerank the N hypotheses generated by the semi-CRF.2.1 Basic parser using semi-CRFWe use a semi-Markov CRF (Sarawagi and Cohen, 2004), or semi-CRF, a variation of a linear-chainCRF (Lafferty et al., 2001), to produce the N -best list of labeled segment hypotheses that serve as theinput to reranking.
In a linear-chain CRF, with a sequence of tokens x and labels y, we approximatep(y|x) as a product of factors of the form p(yi|yi?1, x), which corresponds to features of the formfj(yi?1, yi, i, x), where i iterates over the token/label positions.
This supports a Viterbi search for theapproximateN best values of y. WithM label values, if for each label ymwe know the bestN sequencesof labels y1, y2, .
.
.
, yi?1= ym, then we can use p(yi|yi?1, x) to get the probability for extending eachpath by each possible label yi= y?m.
Then for each label y?m, we will have MN paths and scores, onefrom each of the paths of length i?
1 ending with ym.
For each y?m, we pick the N best extended paths.With semi-CRF, we want a labeled segmentation s rather than a sequence of labels.
Each segmentsi= (yi, ti, ui) has a label yias well as a starting and ending token position for the segment, tianduirespectively, where ui+ 1 = ti+1.
We approximate p(s|x), with factors of the form p(si|si?1, x),which we simplify to p(yi, ui|yi?1, ti), so features take the form fj(yi?1, yi, ti, ui), i.e., they can use theprevious segment?s label and the current segment?s label and endpoints.
The Viterbi search is extendedto search for a pair of label and segment end.
Whereas for M labels we kept track of MN paths, wemust keep track of MLN paths, where L is the maximum segment length.We use token n-gram features relative to the segment boundaries, n-grams within the segment, tokenregular expression and lexicon features within a segment.
Each of these features also includes the labelsof the previous and current segment, and the segment length.2.2 Preference reranking with kernel machinesPreference reranking (PR) uses a classifier C of pairs of hypotheses ?Hi, Hj?, which decides if Hiisbetter thanHj.
Given each training question Q, positive and negative examples are generated for trainingthe classifier.
We adopt the following approach for example generation: the pairs ?H1, Hi?
constitutepositive examples, where H1has the lowest error rate with respect to the gold standard among thehypotheses for Q, and vice versa, ?Hi, H1?
are considered as negative examples.
At testing time, givena new question Q?, C classifies all pairs ?Hi, Hj?
generated from the annotation hypotheses of Q?
: apositive classification is a vote for Hi, otherwise the vote is for Hj.
Also, the classifier score can be usedas a weighted vote.
Hkare then ranked according to the number (sum) of the (weighted) votes they get.We build our reranker with kernel machines.
The latter, e.g., SVMs, classify an input object o usingthe following function: C(o) =?i?iyiK(o, oi), where ?iare model parameters estimated from thetraining data, oiare support objects and yiare the labels of the support objects.
K(?, ?)
is a kernelfunction, which computes the scalar product between the two objects in an implicit vector space.
In thecase of the reranker, the objects o are ?Hi, Hj?, and the kernel is defined as follow:K(?H1, H2?, ?H?1, H?2?)
= S(H1, H?1) + S(H2, H?2)?
S(H1, H?2)?
S(H2, H?1).Our reranker also includes traditional feature vectors in addition to the trees.
Therefore, we define eachhypothesis H as a tuple ?T,~v?
composed of a tree T and a feature vector ~v.
We then define a structuralkernel (similarity) between two hypotheses H and H?as follows: S(H,H?)
= STK(T, T?)
+ Sv(~v,~v?
),where STKis one of the tree kernel functions defined in Section 3.1, and Svis a kernel over featurevectors (see Section 3.3), e.g., linear, polynomial, gaussian, etc.195(a) Basic Tree (BT).
(b) Discourse Tree (DT).
(c) Shallow Syntactic Tree (ShT).
(d) Syntactic Tree (ST).
(e) BT with POS (BTP).Figure 1: Syntactic/semantic trees.
The numeric semantic tagset is defined in Table 7.3 Structural kernels for semantic parsingIn this section, we briefly describe the kernels we use in S(H,H?)
for preference reranking.
We engineerthem by combining three aspects: (i) different types of existing tree kernels, (ii) new syntactic/semanticstructures for representing CSL, and (iii) new feature vectors.3.1 Tree kernelsStructural kernels, e.g., tree and sequence kernels, measure the similarity between two structures in termsof their shared substructures.
One interesting aspect is that these kernels correspond to a scalar productin the fragment space, where each substructure is a feature.
Therefore, they can be used in the trainingand testing algorithms of kernel machines (see Section 2.2).
Below, we briefly describe different types ofkernels we tested in our study, which are made available in the SVM-Light-TK toolkit (Moschitti, 2006).Subtree Kernel (K0) is one of the simplest tree kernels, as it only generates complete subtrees, i.e., treefragments that, given any arbitrary starting node, necessarily include all its descendants.Syntactic Tree Kernel (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob-jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannotbe separated from their parents.
In other words, substructures are composed of atomic building blockscorresponding to nodes, along with all of their direct children.
In the case of a syntactic parse tree, theseare complete production rules for the associated parser grammar.Syntactic Tree Kernel + BOW (K2) extends ST by allowing leaf nodes to be part of the feature space.The leaves of the trees correspond to words, i.e., we allow bag-of-words (BOW).Partial Tree Kernel (K3) can be effectively applied to both constituency and dependency parse trees.It generates all possible connected tree fragments, e.g., sibling nodes can be also separated and be partof different tree fragments.
In other words, a fragment is any possible tree path from whose nodes othertree paths can depart.
Thus, it can generate a very rich feature space.Sequence Kernel (K4) is the traditional string kernel applied to the words of a sentence.
In our case, weapply it to the sequence of concepts.3.2 Semantic/syntactic structuresAs mentioned before, tree kernels allow us to compute structural similarities between two trees withoutexplicitly representing them as feature vectors.
For the CSL task, we experimented with a number of treerepresentations that incorporate different levels of syntactic and semantic information.To capture the structural dependencies between the semantic tags, we use a basic tree (Figure 1a)where the words of a sentence are tagged with their semantic tags.
More specifically, the words in thesentence constitute the leaves of the tree, which are in turn connected to the pre-terminals containing thesemantic tags in BIO notation (?B?=begin, ?I?=inside, ?O?=outside).
The BIO tags are then generalizedin the upper level, and so on.
The basic tree does not include any syntactic information.196However, part-of-speech (POS) and phrasal information could be informative for both segmentationand labeling in semantic parsing.
To incorporate this information, we use two extensions of the basictree: one that includes the POS tags of the words (Figure 1e), and another one that includes both POStags and syntactic chunks (Figure 1c).
The POS tags are children of the semantic tags, whereas thechunks (i.e., phrasal information) are included as parents of the semantic tags.We also experiment with full syntactic trees (Figure 1d) to see the impact of deep syntactic informa-tion.
The semantic tags are attached to the pre-terminals (i.e., POS tags) in the syntactic tree.
We use theStanford POS tagger and syntactic parser and the Twitter NLP tool1for the shallow trees.A sentence containing multiple clauses exhibits a coherence structure.
For instance, in our example,the first clause ?along my route tell me the next steak house?
is elaborated by the second clause ?that iswithin a mile?.
The relations by which clauses in a text are linked are called coherence relations (e.g.,Elaboration, Contrast).
Discourse structures capture this coherence structure of text and provide addi-tional semantic information that could be useful for the CSL task (Stede, 2011).
To build the discoursestructure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generatesdiscourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson,1988), as exemplified in Figure 1b.
Notice that a text span linked by a coherence relation can be either anucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is.3.3 New featuresIn order to compare to the structured representation, we also devoted significant effort towards engineer-ing a set of features to be used in a flat feature-vector representation; they can be used in isolation or incombination with the kernel-based approach (as a composite kernel using a linear combination):CRF-based: these include the basic features used to train the initial semi-CRF model (cf.
Section 2.1).n-gram based: we collected 3- and 4-grams of the output label sequence at the level of concepts, withartificial tags inserted to identify the start (?S?)
and end (?E?)
of the sequence.2Probability-based: two features computing the probability of the label sequence as an average of theprobabilities at the word level p(li|wi) (i.e., assuming independence between words).
The unigram prob-abilities are estimated by frequency counts using maximum likelihood in two ways: (i) from the complete100-best list of hypotheses; (ii) from the training set (according to the gold standard annotation).DB-based: a single feature encoding the number of results returned from the database when constructinga query using the conjunction of all semantic segments in the hypothesis.
Three possible values areconsidered by using a threshold t: 0 (if the query result is void), 1 (if the number of results is in [1, t]),and 2 (if the number of results is greater than t).
In our case, t is empirically set to 10,000.4 ExperimentsThe experiments aim at investigating which structures, and thus which linguistic models and combinationwith other models, are the most appropriate for our reranker.
We first calculate the oracle accuracy inorder to compute an upper bound of the reranker.
Then we present experiments with the feature vectors,tree kernels, and representations of linguistic information introduced in the previous sections.4.1 Experimental setupIn our experiments, we use questions annotated with semantic tags in the restaurant domain,3which werecollected by McGraw et al.
(2012) through crowdsourcing on Amazon Mechanical Turk.4We split thedataset into training, development and test sets.
Table 1 shows statistics about the dataset and about thesize of the parts we used for training, development and testing (see the semi-CRF line).We subsequently split the training data randomly into ten folds.
We generated the N -best lists onthe training set in a cross-validation fashion, i.e., iteratively training on nine folds and annotating theremaining fold.
We computed the 100-best hypotheses for each example.1Available from http://nlp.stanford.edu/software/index.shtml and https://github.com/aritter/twitter nlp, respectively.2For instance, if the output sequence is Other-Rating-Other-Amenity the 3-gram patterns would be: S-Other-Rating, Other-Rating-Other, Rating-Other-Amenity, and Other-Amenity-E.3http://www.sls.csail.mit.edu/downloads/restaurant4We could not use the datasets used by Dinarelli et al.
(2011), because they use French and Italian corpora for which thereare no reliable syntactic and discourse parsers.197Train Devel.
Test Totalsemi-CRF 6,922 739 1,521 9,182Reranker 28,482 3,695 7,605 39,782Table 1: Number of instances and pairs used totrain the semi-CRF and rerankers, respectively.N 1 2 5 10 100F183.03 87.76 92.63 95.23 98.72Table 2: Oracle F1-score for N -best listsof different lengths.We used the development set to experiment and tune the hyper-parameters of the reranking model.
Theresults on the development set presented in Section 4.2 were obtained by semi-CRF and reranking modelslearned on the training set.
The results on the test set were obtained by models trained on the trainingplus development sets.
Similarly, the N -best lists for the development and test sets were generated usinga single semi-CRF model trained on the training set and the training+development sets, respectively.Each generated hypothesis is represented using a semantic tree and a feature vector (explained inSection 3) and two extra features accounting for (i) the semi-CRF probability of the hypothesis, and(ii) the hypothesis reciprocal rank in the N -best list.
SVM-Light-TK5is used to train the reranker witha combination of tree kernels and feature vectors (Moschitti, 2006; Joachims, 1999).
Although wetried several parameters on the validation set, we observed that the default values yielded the highestresults.
Thus, we used the default c (trade-off) and tree kernel parameters and a linear kernel for thefeature vectors.
Table 1 shows the sizes of the train, the development and the test sets used for thesemi-CRF as well as the number of pairs generated for the reranker.
As a baseline, we picked the best-scored hypothesis in the list, according to the semi-CRF tagger.
The evaluation measure used in allthe experiments is the harmonic mean of precision and recall, i.e., the F1-score (van Rijsbergen, 1979),computed at the token level and micro-averaged over the different semantic types.6We used paired t-testto measure the statistical significance of the improvements: we split the test set into 31 equally-sizedsamples and performed t-tests based on the F1-scores of different models on the resulting samples.4.2 ResultsOracle accuracy.
Table 2 shows the oracle F1-score for N -best lists of different lengths, i.e., whichcan be achieved by picking the best candidate of the N -best list for various values of N .
We can see thatgoing to 5-best increases the oracle F1-score by almost ten points absolute.
Going down to 10-best onlyadds 2.5 extra F1points absolute, and a 100-best list adds 3.5 F1points more to yield a respectable F1-score of 98.72.
This high result can be explained considering that the size of the complete hypothesis setis smaller than 100 for most questions.
Thus, we can conclude that theN -best lists do include many goodoptions and do offer quite a large space for potential improvement.
We can further observe that going to5-best lists offers a good balance between the length of the list and the possibility to improve F1-score:generally, we do not want too long N -best lists since they slow down computation and also introducemore opportunities to make the wrong choice for a reranker (since there are just more candidates tochoose from).
In our experiments with larger N , we observed improvements only for 10 and only on thedevelopment set; thus, we will focus on 5-best lists in our experiments below.K0 K1 K2 K3 K4Dev 84.21 82.92 83.07 85.07 83.78Test 84.08 83.19 83.20 84.61 82.93Table 3: Results for using different tree kernels on the basic tree (BT) representation.Choosing the best tree kernel.
We first select the most appropriate tree kernel to limit the numberof experiment variables.
Table 3 shows the results of different tree kernels using the basic tree (BT)representation (see Figure 1a).
We can observe that for both the development set and the test set, kernelK3 (see Section 3.1) yields the highest F1-score.Impact of feature vectors.
Table 4 presents the results for the feature vector experiments in termsof F1-scores and relative error reductions (row RER).
The first column shows the baseline, when noreranking is used; the following four columns contain the results when using vectors including different5http://disi.unitn.it/moschitti/Tree-Kernel.htm6?Other?
is not considered a semantic type, thus ?Other?
tokens are not included in the F1calculation.198Baseline n-grams CRF features Count DB ProbBased AllFeatDev 83.86 83.79 83.96 83.80 83.86 83.87 84.49RER -0.4 0.6 -0.4 0.0 0.0 3.9Test 83.03 82.90 83.44 82.90 83.01 83.09 83.86RER -0.7 2.4 -0.7 -0.1 0.3 4.8Table 4: Feature vector experiments: F1score and relative error reduction (in %).Combining AllFeat andBaseline BT BTP ShT ST AllFeat +BT +ShT +ShT +BTDev 83.86 85.07 85.41 85.06 84.30 84.49 85.57 85.58 85.33RER 7.5 9.6 7.4 2.8 3.9 10.6 10.7 9.1Test 83.03 84.61 84.63 84.07 83.81 83.86 84.67 84.79 84.76RER 9.3 9.4 6.1 4.5 4.8 9.6 10.2 10.2p.v.
0.00049 0.0002 0.012 0.032 0.00018 0.00028 0.00004 0.000023Table 5: Tree kernel experiments: F1-score, relative error reduction (in %), and p-values.kinds of features: (i) n-gram features, (ii) all features used by the semi-CRF, (iii) count features, and(iv) database (DB) features.
In each case, we include two additional features: the semi-CRF score(i.e., the probability) and the reciprocal rank of the hypothesis in the N -best list.
Among (i)?
(iv), onlythe semi-CRF features seem to help; the rest either show no improvements or degrade the performance.However, putting all these features together (AllFeat) yields sizable gains in terms of F1-score and arelative error reduction of 4-5%; the improvement is statistically significant, and it is slightly larger onthe test dataset compared to the development dataset.Impact of structural kernels and combinations.
Table 5 shows the results when experimenting withvarious tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented withpart-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST).
Wecan see that the basic tree works rather well, yielding +1.6 F1-score on the test dataset, but adding POSinformation can help a bit more, especially for the tuning dataset.
Interestingly, the syntactic tree kernels,ShT and ST, perform worse than BT and BTP, especially on the test dataset.
The last three columns in thetable show the results when we combine the AllFeat feature vector (see Table 4) with BT and ShT.
We cansee that combining AllFeat with ShT works better, on both development and test sets, than combining itwith BT or with both ShT and BT.
Also note the big jump in performance from AllFeat to AllFeat+ShT.Overall, we can conclude that shallow syntax has a lot to offer over AllFeat, and it is preferable over BTin the combination with AllFeat.
The improvements reported in Tables 5 and 6 are statistically significantwhen compared to the semi-CRF baseline as shown by the p.v.
(value) row.
Moreover, the improvementof AllFeat + ShT over BT is also statistically significant (p.v.<0.05).Combining AllFeat andBaseline DS +DS +DS +BT +DS +ShTDev 83.86 84.61 85.14 85.43 85.46RER 4.7 7.9 9.7 9.9Test 83.03 84.38 84.55 84.63 84.67RER 7.9 8.9 9.4 9.6p.v.
0.0005 0.0001 0.00066 0.00015Table 6: Experiments with discourse kernels: F1score, relative error reduction (in %), and p-values.Discourse structure.
Finally, Table 6 shows the results for the discourse tree kernel (DS), which wedesigned and experimented with for the first time in this paper.
We see that DS yields sizable improve-ments over the baseline.
We also see that further gains can be achieved by combining DS with AllFeat,and also with BT and ShT, the best combination being AllFeat+DS+ShT (see last column).
However,comparing to Table 5, we see that it is better to use just AllFeat+ShT and leave DS out.
We would liketo note though that the discourse parser produced non-trivial trees for only 30% of the hypotheses (dueto the short, simple nature of the questions); in the remaining cases, it probably hurt rather than helped.We conclude that discourse structure has clear potential, but how to make best use of it, especially in thecase of short simple questions, remains an open question that deserves further investigation.199Tag ID Other Rating Restaurant Amenity Cuisine Dish Hours Location Price0 Other 8260 35 43 110 15 19 55 113 91 Rating 29 266 0 14 3 6 0 0 82 Restaurant 72 6 657 20 19 15 0 5 03 Amenity 117 9 10 841 27 27 7 12 74 Cuisine 36 2 12 26 543 44 3 1 05 Dish 23 0 4 20 33 324 1 4 06 Hours 61 0 1 2 6 1 426 9 17 Location 104 1 14 20 2 1 1 1457 08 Price 22 1 0 7 0 2 0 1 204Table 7: Confusion matrix for the output of the best performing system.4.3 Error analysis and discussionTable 7 shows the confusion matrix for our best-performing model AllFeat+ShT (rows = gold standardtags; columns = system predicted tags).
Given the good results of the semantic parser, the numbers in thediagonal are clearly dominating the weight of the matrix.
The largest errors correspond to missed (firstcolumn) and over-generated (first row) entity tokens.
Among the proper confusions between semantictypes, Dish and Cuisine tend to mislead each other most.
This is due to the fact that these two tagsare semantically similar, thus making them hard to distinguish.
We can also notice that it is difficult toidentify Amenity correctly, and the model mistakenly tags many other tags as Amenity.
We looked intosome examples to further investigate the errors.
Our findings are as follow:Inaccuracies and inconsistencies in human annotations.
Since the annotations were done in Me-chanical Turk, they have many inaccuracies and inconsistencies.
For example, the word good withexactly the same sense was tagged as both Other and Rating by the Turkers in the following examples:Gold: [Otherany good] [Pricecheap] [Cuisinegerman] [Otherrestaurants] [Locationnearby]Model: [Otherany] [Ratinggood] [Pricecheap] [Cuisinegerman] [Otherrestaurants] [Locationnearby]Gold: [Otherany place] [Locationalong the road] [Otherhas a] [Ratinggood] [Dishbeer] [Otherselection that also serves] ...Requires lexical semantics and more coverage.
In some cases our model fails to generalize well.
Forinstance, it fails to correctly tag establishments and tameles for the following examples.
This suggeststhat we need to consider other forms of semantic information, e.g., distributional and compositionalsemantics computed from large corpora and/or using Web resources such as Wikipedia.Gold: [Otherany] [Locationdancing establishments] [Otherwith] [Pricereasonable] [Otherpricing]Model: [Otherany] [Amenitydancing] [Otherestablishments] [Otherwith] [Pricereasonable] [Otherpricing]Gold: [Otherany] [Cuisinemexican] [Otherplaces have a] [Dishtameles] [Amenityspecial today]Model: [Otherany] [Cuisinemexican] [Otherplaces have a] [Amenitytameles] [Otherspecial] [Hourstoday]5 ConclusionsWe have presented a study on the usage of syntactic and semantic structured information for improvedConcept Segmentation and Labeling (CSL).
Our approach is based on reranking a set of N -best se-quences generated by a state-of-the-art semi-CRF model for CSL.
The syntactic and semantic informa-tion was encoded in tree-based structures, which we used to train a reranker with kernel-based SupportVector Machines.
We empirically compared several variants of syntactic/semantic structured representa-tions and kernels, including also a vector of manually engineered features.The first and foremost conclusion from our study is that structural kernels yield significant improve-ment over the strong baseline system, with a relative error reduction of ?10%.
This more than doublesthe improvement when using the explicit feature vector.
Second, we observed that shallow syntacticinformation also improves results significantly over the best model.
Unfortunately, the results obtainedusing full syntax and discourse trees are not so clear.
This is probably explained by the fact that userqueries are rather short and linguistically not very complex.
We also observed that the upper bound per-formance for the reranker still leaves large room for improvement.
Thus, it remains to be seen whethersome alternative kernel representations can be devised to make better use of discourse and other syntac-tic/semantic information.
Also, we think that some innovative features based on analyzing the resultsobtained from our database (or the Web) when querying with the segments represented in each hypothe-ses have the potential to improve the results.
All these concerns will be addressed in future work.200AcknowledgmentsThis research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Re-search Institute (QCRI) within the Qatar Foundation in collaboration with MIT.
It is part of the InteractivesYstems for Answer Search (Iyas) project.ReferencesMichael Collins and Nigel Duffy.
2002.
New ranking algorithms for parsing and tagging: Kernels over discretestructures, and the voted perceptron.
In Proceedings of the 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 263?270, Philadelphia, PA, USA.Renato De Mori, Dilek Hakkani-T?ur, Michael McTear, Giuseppe Riccardi, and Gokhan Tur.
2008.
Spokenlanguage understanding: a survey.
IEEE Signal Processing Magazine, 25:50?58.Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi.
2011.
Discriminative reranking for spoken lan-guage understanding.
IEEE Transactions on Audio, Speech and Language Processing, 20(2):526?539.Ruifang Ge and Raymond Mooney.
2006.
Discriminative reranking for semantic parsing.
In Proceedings of the21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Associationfor Computational Linguistics, COLING-ACL?06, pages 263?270, Sydney, Australia.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic labeling of semantic roles.
Computational Linguistics,28(3):245?288.Thorsten Joachims.
1999.
Advances in kernel methods.
In Bernhard Sch?olkopf, Christopher J. C. Burges, andAlexander J. Smola, editors, Making Large-scale Support Vector Machine Learning Practical, pages 169?184,Cambridge, MA, USA.
MIT Press.Shafiq Joty, Giuseppe Carenini, and Raymond Ng.
2012.
A novel discriminative framework for sentence-level dis-course analysis.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processingand Computational Natural Language Learning, EMNLP-CoNLL ?12, pages 904?915, Jeju Island, Korea.Rohit Kate and Raymond Mooney.
2006.
Using string-kernels for learning semantic parsers.
In Proceedings ofthe 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Associationfor Computational Linguistics, COLING-ACL ?06, pages 913?920, Sydney, Australia.Terry Koo and Michael Collins.
2005.
Hidden-variable models for discriminative reranking.
In Proceedingsof the Conference on Human Language Technology and Empirical Methods in Natural Language Processing,HLT-EMNLP ?05, pages 507?514, Vancouver, British Columbia, Canada.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.
Boosting-based parse reranking with subtree features.
InProceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages 189?196, Ann Arbor, MI, USA.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proceedings of the 18th International Conference on MachineLearning, ICML ?01, pages 282?289, Williamstown, MA, USA.William Mann and Sandra Thompson.
1988.
Rhetorical structure theory: Toward a functional theory of textorganization.
Text, 8(3):243?281.Llu?
?s M`arquez, Xavier Carreras, Kenneth Litkowski, and Suzanne Stevenson.
2008.
Semantic Role Labeling: AnIntroduction to the Special Issue.
Computational Linguistics, 34(2):145?159.Ian McGraw, Scott Cyphers, Panupong Pasupat, Jingjing Liu, and Jim Glass.
2012.
Automating crowd-supervisedlearning for spoken language systems.
In Proceedings of 13th Annual Conference of the International SpeechCommunication Association, INTERSPEECH ?12, Portland, OR, USA.Scott Miller, Richard Schwartz, Robert Bobrow, and Robert Ingria.
1994.
Statistical language processing usinghidden understanding models.
In Proceedings of the workshop on Human Language Technology, HLT ?94,pages 278?282, Morristown, NJ, USA.Alessandro Moschitti, Daniele Pighin, and Roberto Basili.
2006.
Semantic role labeling via tree kernel jointinference.
In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),pages 61?68, New York City, June.201Alessandro Moschitti.
2006.
Efficient convolution kernels for dependency and constituent syntactic trees.
InProceedings of the 17th European Conference on Machine Learning, ECML?06, pages 318?329, Berlin, Hei-delberg.
Springer-Verlag.Truc-Vien T. Nguyen and Alessandro Moschitti.
2012.
Structural reranking models for named entity recognition.Intelligenza Artificiale, 6(2):177?190.Kishore Papineni, Salim Roukos, and Todd Ward.
1998.
Maximum likelihood and discriminative training ofdirect translation models.
In Proceedings of the IEEE International Conference on Acoustics, Speech andSignal Processing, volume 1, pages 189?192, Seattle, WA, USA.Roberto Pieraccini, Esther Levin, and Chin-Hui Lee.
1991.
Stochastic representation of conceptual structure inthe ATIS task.
In Proceedings of the Workshop on Speech and Natural Language, HLT ?91, pages 121?124,Pacific Grove, CA, USA.Christian Raymond and Giuseppe Riccardi.
2007.
Generative and discriminative algorithms for spoken languageunderstanding.
In Proceedings of 8th Annual Conference of the International Speech Communication Associa-tion, INTERSPEECH ?07, pages 1605?1608, Antwerp, Belgium.Yigal Dan Rubinstein and Trevor Hastie.
1997.
Discriminative vs informative learning.
In Proceedings of theThird International Conference on Knowledge Discovery and Data Mining, KDD ?97, pages 49?53, NewportBeach, CA, USA.Guzm?an Santaf?e, Jose Lozano, and Pedro Larra?naga.
2007.
Discriminative vs. generative learning of Bayesiannetwork classifiers.
Lecture Notes in Computer Science, 4724:453?464.Sunita Sarawagi and William Cohen.
2004.
Semi-Markov conditional random fields for information extraction.In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, NIPS ?04, pages1185?1192, Vancouver, British Columbia, Canada.Stephanie Seneff.
1989.
TINA: A probabilistic syntactic parser for speech understanding systems.
In Proceedingsof the Workshop on Speech and Natural Language, HLT ?89, pages 168?178, Philadelphia, PA, USA.Manfred Stede.
2011.
Discourse Processing.
Synthesis Lectures on Human Language Technologies.
Morgan andClaypool Publishers.Cornelis Joost van Rijsbergen.
1979.
Information Retrieval.
Butterworth.Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Jakub Szymanski.
2009.
Semi-supervised learning of semanticclasses for query understanding: from the web and for the web.
In Proceedings of the 18th ACM Conference onInformation and Knowledge Management, CIKM ?09, pages 37?46, New York, NY, USA.202
