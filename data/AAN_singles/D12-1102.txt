Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1114?1124, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsOn Amortizing Inference Cost for Structured PredictionVivek Srikumar?
and Gourab Kundu?
and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL.
61801{vsrikum2, kundu2, danr}@illinois.eduAbstractThis paper deals with the problem of predict-ing structures in the context of NLP.
Typically,in structured prediction, an inference proce-dure is applied to each example independentlyof the others.
In this paper, we seek to op-timize the time complexity of inference overentire datasets, rather than individual exam-ples.
By considering the general inferencerepresentation provided by integer linear pro-grams, we propose three exact inference the-orems which allow us to re-use earlier solu-tions for certain instances, thereby completelyavoiding possibly expensive calls to the infer-ence procedure.
We also identify several ap-proximation schemes which can provide fur-ther speedup.
We instantiate these ideas to thestructured prediction task of semantic role la-beling and show that we can achieve a speedupof over 2.5 using our approach while retain-ing the guarantees of exactness and a furtherspeedup of over 3 using approximations thatdo not degrade performance.1 IntroductionTypically, in structured prediction applications, ev-ery example is treated independently and an infer-ence algorithm is applied to each one of them.
Forexample, consider a dependency parser that uses themaximum spanning tree algorithm (McDonald et al2005) or its integer linear program variants (Riedeland Clarke, 2006; Martins et al 2009) to make pre-dictions.
Given a trained model, the parser addresses* These authors contributed equally to this work.each sentence separately and runs the inference al-gorithm to predict the parse tree.
Thus, the timecomplexity of inference over the test set is linear inthe size of the corpus.In this paper, we ask the following question: Fora given task, since the inference procedure predictsstructures from the same family of structures (depen-dency trees, semantic role structures, etc.
), can thefact that we are running inference for a large num-ber of examples help us improve the time complexityof inference?
In the dependency parsing example,this question translates to asking whether, havingparsed many sentences, we can decrease the parsingtime for the next sentence.Since any combinatorial optimization problemcan be phrased as an integer linear program (ILP),we frame inference problems as ILPs for the purposeof analysis.
By analyzing the objective functionsof integer linear programs, we identify conditionswhen two ILPs have the same solution.
This allowsus to reuse solutions of previously solved problemsand theoretically guarantee the optimality of the so-lution.
Furthermore, in some cases, even when theconditions are not satisfied, we can reuse previoussolutions with high probability of being correct.Given the extensive use of integer linear programsfor structured prediction in Natural Language Pro-cessing over the last few years, these ideas can be ap-plied broadly to NLP problems.
We instantiate ourimproved inference approaches in the structured pre-diction task of semantic role labeling, where we usean existing implementation and a previous trainedmodel that is based on the approach of (Punyakanoket al 2008).
We merely modify the inference pro-1114cess to show that we can realize the theoretical gainsby making fewer calls to the underlying ILP solver.Algorithm SpeedupTheorem 1 2.44Theorem 2 2.18Theorem 3 2.50Table 1: The speedup for semantic role labeling cor-responding to the three theorems described in thispaper.
These theorems guarantee the optimality ofthe solution, thus ensuring that the speedup is notaccompanied by any loss in performance.Table 1 presents a preview of our results, whichare discussed in Section 4.
All three approaches inthis table improve running time, while guaranteeingoptimum solutions.
Allowing small violations to theconditions of the theorems provide an even higherimprovement in speedup (over 3), without loss ofperformance.The primary contributions of this paper are:1.
We pose the problem of optimizing inferencecosts over entire datasets rather than individ-ual examples.
Our approach is agnostic to theunderlying models and allows us to use pre-trained scoring functions.2.
We identify equivalence classes of ILP prob-lems and use this notion to prove exact con-ditions under which no inference is required.These conditions lead to algorithms that canspeed up inference problem without losing theexactness guarantees.
We also use these con-ditions to develop approximate inference algo-rithms that can provide a further speedup.3.
We apply our approach to the structured pre-diction task of semantic role labeling.
By nothaving to perform inference on some of the in-stances, those that are equivalent to previouslyseen instances, we show significant speed up interms of the number of times inference needs tobe performed.
These gains are also realized interms of wall-clock times.The rest of this paper is organized as follows: Insection 2, we formulate the problem of amortizedinference and provide motivation for why amortizedgains can be possible.
This leads to the theoreticaldiscussion in section 3, where we present the meta-algorithm for amortized inference along with sev-eral exact and approximate inference schemes.
Weinstantiate these schemes for the task of semanticrole labeling (Section 4).
Section 5 discusses relatedwork and future research directions.2 MotivationMany NLP tasks can be phrased as structured pre-diction problems, where the goal is to jointly assignvalues to many inference variables while account-ing for possible dependencies among them.
This de-cision task is a combinatorial optimization problemand can be solved using a dynamic programming ap-proach if the structure permits.
In general, the infer-ence problem can be formulated and solved as inte-ger linear programs (ILPs).Following (Roth and Yih, 2004) Integer linearprograms have been used broadly in NLP.
For exam-ple, (Riedel and Clarke, 2006) and (Martins et al2009) addressed the problem of dependency pars-ing and (Punyakanok et al 2005; Punyakanok etal., 2008) dealt with semantic role labeling with thistechnique.In this section, we will use the ILP formulationof dependency parsing to introduce notation.
Thestandard approach to framing dependency parsing asan integer linear program was introduced by (Riedeland Clarke, 2006), who converted the MST parserof (McDonald et al 2005) to use ILP for inference.The key idea is to build a complete graph consist-ing of tokens of the sentence where each edge isweighted by a learned scoring function.
The goalof inference is to select the maximum spanning treeof this weighted graph.2.1 Problem FormulationIn this work, we consider the general inference prob-lem of solving a 0-1 integer linear program.
To per-form inference, we assume that we have a model thatassigns scores to the ILP decision variables.
Thus,our work is applicable not only in cases where in-ference is done after a separate learning phase, as in(Roth and Yih, 2004; Clarke and Lapata, 2006; Rothand Yih, 2007) and others, but also when inferenceis done during the training phase, for algorithms like1115the structured perceptron of (Collins, 2002), struc-tured SVM (Tsochantaridis et al 2005) or the con-straints driven learning approach of (Chang et al2007).Since structured prediction assigns values to acollection of inter-related binary decisions, we de-note the ith binary decision by yi ?
{0, 1} and theentire structure as y, the vector composed of all thebinary decisions.
In our running example, each edgein the weighted graph generates a single decisionvariable (for unlabeled dependency parsing).
Foreach yi, let ci ?
< denote the weight associated withit.
We denote the entire collection of weights by thevector c, forming the objective for this ILP.Not all assignments to these variables are valid.Without loss of generality, these constraints can beexpressed using linear inequalities over the infer-ence variables, which we write as MTy ?
b fora real valued matrix M and a vector b.
In depen-dency parsing, for example, these constraints ensurethat the final output is a spanning tree.Now, the overall goal of inference is to find thehighest scoring structure.
Thus, we can frame infer-ence as an optimization problem p with n inferencevariables as follows:arg maxy?
{0,1}ncTy (1)subject to MTy ?
b.
(2)For brevity, we denote the space of feasible solutionsthat satisfy the constraints for the ILP problem p asKp = {y ?
{0, 1}n|MTy ?
b}.
Thus, the goal ofinference is to findarg maxy?KpcTy.We refer to Kp as the feasible set for the inferenceproblem p and yp as its solution.In the worst case, integer linear programs areknown to be NP-hard.
Hence, solving large prob-lems, (that is, problems with a large number of con-straints and/or variables) can be infeasible.For structured prediction problems seen in NLP,we typically solve many instances of inference prob-lems.
In this paper, we investigate whether an infer-ence algorithm can use previous predictions to speedup inference time, thus giving us an amortized gainin inference time over the lifetime of the program.We refer to inference algorithms that have this capa-bility as amortized inference algorithms.In our running example, each sentence corre-sponds to a separate ILP.
Over the lifetime of thedependency parser, we create one inference instance(that is, one ILP) per sentence and solve it.
An amor-tized inference algorithm becomes faster at parsingas it parses more and more sentences.2.2 Why can inference costs be amortized overdatasets?In the rest of this section, we will argue that the timecost of inference can be amortized because of thenature of inference in NLP tasks.
Our argument isbased on two observations, which are summarized inFigure (1): (1) Though the space of possible struc-tures may be large, only a very small fraction ofthese occur.
(2) The distribution of observed struc-tures is heavily skewed towards a small number ofthem.x?s p?s y?sILPformulationInferenceExamplesILPsSolutionsFigure 1: For a structured prediction task, the infer-ence problem p for an example x needs to be for-mulated before solving it to get the structure y. Instructured prediction problems seen in NLP, whilean exponential number of structures is possible for agiven instance, in practice, only a small fraction ofthese ever occur.
This figure illustrates the empiricalobservation that there are fewer inference problemsp?s than the number of examples and the number ofobserved structures y?s is even lesser.As an illustration, consider the problem of part-of-speech tagging.
With the standard Penn Treebanktag set, each token can be assigned one of 45 labels.Thus, for a sentence of size n, we could have 45nstructures out of which the inference process needsto choose one.
However, a majority of these struc-tures never occur.
For example, we cannot have a111601000002000003000004000005000000  10  20  30  40  50Number of tokensPart-of-speech statistics, using tagged Gigaword textNumber of examples of size Number of unique POS tag sequences(a) Part-of-speech tagging01000002000003000004000005000000  10  20  30  40  50Size of sentenceUnlabeled dependency parsing statistics, using tagged Gigaword textNumber of examples of sizeNumber of unique dependency trees(b) Unlabeled dependency parsing0200004000060000800001000001200001400001600000  1  2  3  4  5  6  7  8Size of the input (number of argument candidates)SRL statistics , using tagged Gigaword textNumber of examples of sizeNumber of unique SRL structures(c) Semantic role labelingFigure 2: Number of inference instances for different input sizes (red solid lines) and the number of uniquestructures for each size (blue dotted lines).
The x-axis indicates the size of the input (number of tokensfor part of speech and dependency, and number of argument candidates for SRL.)
Note that the number ofinstances is not the number of unique examples of a given length, but the number of times an inferenceprocedure is called for an input of a given size.0246810120  5000  10000  15000  20000Solution IdLog frequency of solutions for sentences with 5 tokens(a) Sentence length = 5-10123456780  50000  100000  150000  200000  250000Solution IdLog frequency of solutions for sentences with 10 tokens(b) Sentence length = 10-10123456780  50000  100000  150000  200000  250000  300000  350000Solution IdLog frequency of solutions for sentences with 15 tokens(c) Sentence length = 15Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentenceswith five, ten and fifteen tokens.
The x-axes list different unique part-of-speech tag sequences for the entiresentence.
These plots show that for sentences of a given length, most structures (solutions) that are possiblenever occur, or occur very infrequently; only a few of the possible structures (solutions) actually occurfrequently.sentence where all the tokens are determiners.Furthermore, many sentences of the same sizeshare the same part-of-speech tag sequence.
Toquantify the redundancy of structures, we part-of-speech tagged the English Gigaword corpus (Graffand Cieri, 2003).
Figure (2a) shows the numberof sentences in the corpus for different sentencelengths.
In addition, it also shows the number ofunique part-of-speech tag sequences (over the en-tire sentence) for each size.
We see that the numberof structures is much fewer than the number of in-stances for any sentence size.
Note that 45n quicklyoutgrows the number of sentences as n increases.The figures (2b) and (2c) show similar statistics forunlabeled dependency parsing and semantic role la-beling.
In the former case, the size of the instance isthe number of tokens in a sentence, while in the lat-ter, the size is the number of argument candidatesthat need to be labeled for a given predicate.
Inboth cases, we see that the number of empiricallyobserved structures is far fewer than the number ofinstances to be labeled.Thus, for any given input size, the number of in-stances of that size (over the lifetime of the program)far exceeds the number of observed structures forthat size.
Moreover, the number of observed struc-tures is significantly smaller than the number of the-oretically possible structures.
Thus, we have a smallnumber of structures that form optimum structuresfor many inference instances of the same size.Our second observation deals with the distribu-tion of structures for a given input size.
Figure (3)1117shows the log frequencies of part-of-speech taggingsequences for sentences of lengths five, ten and fif-teen.
In all cases, we see that a few structures aremost frequent.
We observed similar distributions ofstructures for all input sizes for dependency parsingand semantic role labeling as well.Since the number of structures for a given exam-ple size is small, many examples x?s, and hencemany inference problems p?s, are associated withthe same structure y.
These observations suggest thepossibility of getting an amortized gain in inferencetime by characterizing the set of inference problemsthat produce the same structure.
Then, for a new in-ference problem, if we can identify that it belongs toa known set, that is, will yield a solution that we havealready seen, we do not have to run inference at all.The second observation also suggests that this char-acterization of sets of problems that have the samesolution can be done in a data-driven way becausecharacterizing a small number of structures can giveus high coverage.3 Amortizing inference costsIn this section, we present different schemes foramortized inference leading up to an inference meta-algorithm.
The meta-algorithm is both agnostic tothe underlying inference algorithm that is used bythe problem and maintains the exactness propertiesof the underlying inference scheme.
That is, if wehave an exact/approximate inference algorithm witha certain guarantees, the meta-algorithm will havethe same guarantees, but with a speedup.3.1 NotationFor an integer linear program p with np variables,we denote its objective coefficients by cp and its fea-sible set byKp.
We denote its solution as as yp.
Werepresent vectors by boldfaced symbols and their ithcomponent using subscripts.We consider many instantiations of the inferenceproblem and use superscripts to denote each indi-vidual instance.
Thus, we have a large collection ofinference instances P = {p1,p2, ?
?
? }
along withtheir respective solutions {y1p,y2p, ?
?
?
}.Definition 1 (Equivalence classes of ILPs).
Two in-teger linear programs are said to be in the sameequivalence class if they have the same number ofinference variables and the same feasible set.We square brackets to denote equivalence classes.If [P ] is an equivalence class of ILPs, we use thenotation K[P ] to denote its feasible set and n[P ] todenote the number of variables.
Also, for a programp, we use the notation p ?
[P ] to indicate that itbelongs to the equivalence class [P ].3.2 Exact theoremsOur goal is to characterize the set of objective func-tions which will have the same solution for a givenequivalence class of problems.Suppose we have solved an ILP p to get a solutionyp.
For every inference variable that is active in thesolution (i.e., whose value is 1), increasing the corre-sponding objective value will not change the optimalassignment to the variables.
Similarly, for all othervariables (whose value in the solution is 0), decreas-ing the objective value will not change the optimalsolution.
This intuition gives us our first theorem forchecking whether two ILPs have the same solutionby looking at the difference between their objectivecoefficients.Theorem 1.
Let p denote an inference problemposed as an integer linear program belonging to anequivalence class [P ].
Let q ?
[P ] be another infer-ence instance in the same equivalence class.
Define?c = cq ?
cp to be the difference of the objectivecoefficients of the ILPs.
Then, yp is the solution ofthe problem q if for each i ?
{1, ?
?
?
, np}, we have(2yp,i ?
1)?ci ?
0 (3)The condition in the theorem, that is, inequal-ity (3), requires that the objective coefficients corre-sponding to values yp,i that are set to 1 in p increase,and those that correspond to values of yp,i set to 0,decrease.
Under these conditions, if yp is the max-imizer of the original objective, then it maximizesthe new objective too.Theorem 1 identifies perturbations of an ILP?s ob-jective coefficients that will not change the optimalassignment.
Next, we will characterize the sets ofobjective values that will have the same solution us-ing a criterion that is independent of the actual so-lution.
Suppose we have two ILPs p and q in anequivalence class [P ] whose objective values are cpand cq respectively.
Suppose y?
is the solution to1118both these programs.
That is, for every y ?
K[P ],we have cTpy ?
cTpy?
and cTqy ?
cTqy?.
Multiply-ing these inequalities by any two positive real num-bers x1 and x2 and adding them shows us that y?is also the solution for the ILP in [P ] which has theobjective coefficients x1cp + x2cq.
Extending thisto an arbitrary number of inference problems givesus our next theorem.Theorem 2.
Let P denote a collection{p1,p2, ?
?
?
,pm} of m inference problems inthe same equivalence class [P ] and suppose thatall the problems have the same solution, yp.
Letq ?
[P ] be a new inference program whose optimalsolution is y.
Then y = yp if there is some x ?
<msuch that x ?
0 andcq =?jxjcjp.
(4)From the geometric perspective, the pre-conditionof this theorem implies that if the new coefficientslie in the cone formed by the coefficients of the pro-grams that have the same solution, then the new pro-gram shares the solution.Theorems 1 and 2 suggest two different ap-proaches for identifying whether a new ILP canuse the solution of previously solved inference in-stances.
These theorems can be combined to get asingle criterion that uses the objective coefficients ofpreviously solved inference problems and their com-mon solution to determine whether a new inferenceproblem will have the same solution.
Given a collec-tion of solved ILPs that have the same solution, fromtheorem 2, we know that an ILP with the objectivecoefficients c =?j xjcjp will share the solution.Considering an ILP whose objective vector is c andapplying theorem 1 to it gives us the next theorem.Theorem 3.
Let P denote a collection{p1,p2, ?
?
?
,pm} of m inference problemsbelonging to the same equivalence class [P ].Furthermore, suppose all the programs have thesame solution yp.
Let q ?
[P ] be a new inferenceprogram in the equivalence class.
For any x ?
<m,define ?c(x) = cq ?
?j xjcjp.
The assignmentyp is the optimal solution of the problem q if thereis some x ?
<m such that x ?
0 and for eachi ?
{1, np}, we have(2yp,i ?
1)?ci ?
0 (5)Theorem ConditionTheorem 1 ?i ?
{1, ?
?
?
, np},(2yp,i ?
1)?ci ?
0; ?i.Theorem 2 ?
x ?
<m, such thatx ?
0 and cq =?j xjcjpTheorem 3 ?
x ?
<m, such thatx ?
0 and (2yp,i ?
1)?ci ?
0; ?i.Table 2: Conditions for checking whether yp is thesolution for an inference problem q ?
[P ] accordingto theorems 1, 2 and 3.
Please refer to the statementsof the theorems for details about the notation.3.3 ImplementationTheorems 1, 2 and 3 each specify a condition thatchecks whether a pre-existing solution is the opti-mal assignment for a new inference problem.
Theseconditions are summarized in Table 2.
In all cases,if the condition matches, the theorems guarantee thatthe two solutions will be the same.
That is, applyingthe theorems will not change the performance of theunderlying inference procedure.
Only the number ofinference calls will be decreased.In our implementation of the conditions, we useda database1 to cache ILPs and implemented theretrieval of equivalence classes and solutions asqueries to the database.
To implement theorem 1,we iterate over all ILPs in the equivalence class andcheck if the condition is satisfied for one of them.The conditions of theorems 2 and 3 check whether acollection of linear (in)equalities has a feasible solu-tion using a linear program solver.We optimize the wall-clock time of theorems 2and 3 by making two observations.
First, we do notneed to solve linear programs for all possible ob-served structures.
Given an objective vector, we onlyneed consider the highest scoring structures withinan equivalence class.
(All other structures cannotbe the solution to the ILP.)
Second, since theorem2 checks whether an ILP lies within a cone, we canoptimize the cache for theorem 2 by only storing theILPs that form on the boundary of the cone.
A sim-ilar optimization can be performed for theorem 3 aswell.
Our implementation uses the following weakerversion of this optimization: while caching ILPs, we1We used the H2 database engine, which can be downloadedfrom http://h2database.com, for all caching.1119do not add an instance to the cache if it already satis-fies the theorem.
This optimization reduces the sizeof the linear programs used to check feasibility.3.4 Approximation schemesSo far, in the above three theorems, we retain theguarantees (in terms of exactness and performance)of the underlying inference procedure.
Now, we willlook at schemes for approximate inference.
Unlikethe three theorems listed above, with the followingamortized inference schemes, we are not guaranteedan optimal solution.3.4.1 Most frequent solutionThe first scheme for approximation uses the ob-servation that the most frequent solution occurs anoverwhelmingly large number of times, compared tothe others.
(See the discussion in section 2.2 and fig-ures 3a, 3b and 3c for part-of-speech tagging.)
Un-der this approximation scheme, given an ILP prob-lem, we simply pick the most frequent solution forthat equivalence class as the solution, provided thissolution has been seen a sufficient number of times.If the support available in the cache is insufficient,we call the underlying inference procedure.3.4.2 Top-K approximationThe previous scheme for approximate amortizedinference is agnostic to the objective coefficients ofinteger linear program to be solved and uses onlyits equivalence class to find a candidate structure.The top-K approach extends this by scoring the Kmost frequent solutions using the objective coeffi-cients and selecting the highest scoring one as thesolution to the ILP problem.
As with the previousscheme, we only consider solutions that have suffi-cient support.3.4.3 Approximations to theorems 1 and 3The next approximate inference schemes relaxesthe conditions in theorems 1 and 3 by allowing theinequalities to be violated by .
That is, the inequal-ity (3) from Theorem 1 now becomes(2yp,i ?
1)?ci +  ?
0.
(6)The inequality (5) from Theorem 3 is similarly re-laxed as follows:(2yp,i ?
1)?ci +  ?
0 (7)3.5 Amortized inference algorithmEach exact and approximate inference approach de-scribed above specifies a condition to check whetheran inference procedure should be called for anew problem.
This gives us the following meta-algorithm for amortized inference, parameterized bythe actual scheme used: If the given input instance psatisfies the condition specified by the scheme, thenuse the cached solution.
Otherwise, call the infer-ence procedure and cache the solution for future use.4 ExperimentsIn this section, we apply the theory from Section 3 tothe structure prediction problem of semantic role la-beling.
Since the inference schemes presented aboveare independent of the learning aspects, we use anoff-the-shelf implementation and merely modify theinference as discussed in Section 3.5.The goal of the experiments is to show that us-ing an amortized inference algorithm, we can makefewer calls to the underlying inference procedure.For the exact inference algorithms, doing so will notchange the performance as compared to the under-lying system.
For the approximations, we can makea trade-off between the inference time and perfor-mance.4.1 Experimental setupOur goal is to simulate a long-running NLP processthat can use a cache of already solved problems toimprove inference time.
Given a new input problem,our theorems require us to find all elements in theequivalence class of that problem along with theirsolutions.
Intuitively, we expect a higher probabilityof finding members of an arbitrary equivalence classif the size of the cache is large.
Hence, we processedsentences from the Gigaword corpus and cached theinference problems for our task.The wall-clock time is strongly dependent on suchspecific implementation of the components, whichare independent of the main contributions of thiswork.
Also, in most interesting applications, thecomputation time for each step will be typicallydominated by the number of inference steps, espe-cially with efficient implementations of caching andretrieval.
Hence, the number of calls to the underly-ing procedure is the appropriate complexity param-1120eter.
Let NBase be the number of times we wouldneed to call the underlying inference procedure hadwe not used an amortized algorithm.
(This is thesame as the number of inference problems.)
Let NAbe the number of times the underlying inference pro-cedure is actually called using an amortized algo-rithm A.
We define the speedup of A asSpeedup(A) =NBaseNA.
(8)We also report the clock speedup of our implemen-tation for all algorithms, which is the ratio of thewall-clock time taken by the baseline algorithm tothat of the amortized algorithm.
For measuring time,we only measure the time for inference as the otheraspects (feature extraction, scoring, etc.)
are notchanged.4.2 Semantic Role LabelingThe goal of Semantic Role Labeling (SRL) (Palmeret al 2010) is to identify and assign semantic rolesto arguments of verb predicates in a sentence.
Forexample, consider the the sentence John gave theball to Mary.
The verb give takes three arguments,John, the ball and to Mary, which are labeled A0,A1 and A2 respectively.We used the system of (Punyakanok et al 2008)as our base SRL system.
It consists of two classi-fiers trained on the Propbank corpus.
The first one,called the argument identifier, filters argument can-didates which are generated using a syntactic parse-based heuristic.
The second model scores each can-didate that has not been filtered for all possible argu-ment labels.
The scores for all candidates of a pred-icate are combined via inference.
As in the systemof (Punyakanok et al 2008), the softmax functionis applied to the raw classifier scores to ensure thatthey are in the same numeric range.Inference mandates that certain structural andlinguistic constraints hold over the full predicate-argument structure for a verb.
(Punyakanok et al2008) modeled inference via an integer linear pro-gram instance, where each assignment of labelsto candidates corresponds to one decision variable.Given a set of argument candidates, the feasible setof decisions is dependent of the number of argumentcandidates and the verb predicate.
Thus, in termsof the notation used in this paper, the equivalenceclasses are defined by the pair (predicate, number ofargument candidates).We ran the semantic role labeler on 225,000 verbpredicates from the Gigaword corpus and cachedthe equivalence classes, objective coefficients andsolutions generated by the SRL system.
We re-port speedup for the various amortized inferenceschemes on the standard Penn Treebank test set.
Onthis data, the unaltered baseline system, processes5127 integer linear programs and achieves an F1 of75.85%.Table 3 shows the speedup and performance forthe various inference schemes.
The most frequentand top-K systems are both naive solutions that takeadvantage of the cache of stored problems.
In spiteof their simplicity, they attain F1 scores of 62%and 70.06% because few structures occur most fre-quently, as described in section 2.2.
We see that allthe exact theorems attain a speedup higher than twowithout losing performance.
(The variation in F1 be-tween them is because of the existence of differentequivalent solutions in terms of the objective value.
)This shows us that we can achieve an amortized gainin inference.
Note that a speedup of 2.5 indicatesthat the solver is called only for 40% of the exam-ples.
The approximate versions of theorems 1 and 3(with  = 0.3 in both cases, which was not tuned)attain an even higher gain in speedup over the base-line than the base versions of the theorems.
Interest-ingly, the SRL performance in both cases does notdecline much even though the conditions of the the-orems may be violated.5 Related work and Future directionsIn recent years, we have seen several approaches tospeeding up inference using ideas like using the cut-ting plane approach (Riedel, 2009), dual decompo-sition and Lagrangian relaxation (Rush et al 2010;Chang and Collins, 2011).
The key difference be-tween these and the work in this paper is that allthese approaches solve one instance at a time.
Sincewe can use any inference procedure as a underlyingsystem, the speedup reported in this paper is appli-cable to all these algorithms.Decomposed amortized inference In this paper,we have taken advantage of redundancy of struc-tures that can lead to the re-use of solutions.
In the1121Type Algorithm # instances # solver Speedup Clock F1calls speedupExact Baseline 5127 5217 1.0 1.0 75.85Exact Theorem 1 5127 2134 2.44 1.54 75.90Exact Theorem 2 5127 2390 2.18 1.14 75.79Exact Theorem 3 5127 2089 2.50 1.36 75.77Approx.
Most frequent (Support = 50) 5127 2812 1.86 1.57 62.00Approx.
Top-10 solutions (Support = 50) 5127 2812 1.86 1.58 70.06Approx.
Theorem 1 (approx,  = 0.3) 5127 1634 3.19 1.81 75.76Approx.
Theorem 3 (approx,  = 0.3) 5127 1607 3.25 1.50 75.46Table 3: Speedup and performance for various inference methods for the task of Semantic Role Labeling.All the exact inference algorithms get a speedup higher than two.
The speedup of the approximate versionof the theorems is even higher without loss of performance.
The clock speedup is defined as the ratio of theinference times of the baseline and the given algorithm.
All numbers are averaged over ten trials.part of speech example, we showed redundancy ofstructures at the sentence level (Figure 2a).
How-ever, for part-of-speech tagging, the decisions arerarely, if at all, dependent on a very large context.One direction of future work is to take advantage ofthe fact that the inference problem can be split intosmaller sub-problems.
To support this hypothesis,we counted the number of occurrences of ngramsof tokens (including overlapping and repeated men-tions) for n <= 10 and compared this to the numberof unique part-of-speech ngrams of this length.
Fig-ure 4 shows these two counts.
Following the argu-ment in Section 2.2, this promises a large amortizedgain in inference time.
We believe that such decom-position can also be applied to other, more complexstructured prediction tasks.The value of approximate inference From theexperiments, we see that the first two approximateinference schemes (most frequent solution and thetop-K scheme) can speed up inference with theonly computational cost being the check for pre-conditions of the exact theorems.
Effectively, thesealgorithms have parameters (i.e., the support param-eter) that allow us to choose between the inferencetime and performance.
Figure 5 shows the perfor-mance of the most frequent and top-K baselines fordifferent values of the support parameter, which in-dicates how often a structure must occur for it to beconsidered.
We see that for lower values of support,we can get a very high speedup but pay with poorerperformance.01e+062e+063e+064e+065e+066e+067e+060  2  4  6  8  10Number of tokensPart-of-speech ngram statistics, using tagged Gigaword textNumber of instances for size Number of unique structures for given lengthFigure 4: The red line shows the number of ngramsof tokens (including overlapping and repeated oc-currences) in the Gigaword corpus and the blue lineshows the number of unique POS tag sequences.However, the prediction of the approximate al-gorithms can be used to warm-start any solver thatcan accept an external initialization.
Warm-startinga solver can give a way to get the exact solution andyet take advantage of the frequency of structures thathave been observed.Lifted inference The idea of amortizing inferencetime over the dataset is conceptually related to theidea of lifted inference (de Salvo Braz et al 2005).We abstract many instances into equivalence classesand deal with the inference problem with respect tothe equivalence classes in the same way as done inlifted inference algorithms.112211.522.533.50  200  400  600  800  1000 50556065707580SupportPerformance of the most frequent and top-K schemes for different values of supportSpeedupPerformance of most frequent solution (F1)Performance of top-K solution (F1)Figure 5: Most frequent solutions and top-K:Speedup and SRL performance (F1) for differentvalues of the support parameter, using the most-frequent solutions (dashed blue line) and the top-K scheme (thick gray line).
Support indicates howmany times a structure should be seen for it to beconsidered.
Note that the speedup values for bothschemes are identical (red line).6 ConclusionIn this paper, we addressed structured prediction inthe context of NLP and proposed an approach to im-prove inference costs over an entire dataset, ratherthan individual instances.
By treating inferenceproblems as instances of integer linear programs, weproposed three exact theorems which identify exam-ples for which the inference procedure need not becalled at all and previous solutions can be re-usedwith the guarantee of optimality.
In addition, wealso proposed several approximate algorithms.
Weapplied our algorithms, which are agnostic to theactual tasks, to the problem semantic role labeling,showing significant decrease in the number of infer-ence calls without any loss in performance.
Whilethe approach suggested in this paper is evaluated insemantic role labeling, it is generally applicable toany NLP task that deals with structured prediction.AcknowledgementsThe authors wish to thank Sariel Har-Peled and the membersof the Cognitive Computation Group at the University of Illi-nois for insightful discussions and the anonymous reviewers fortheir valuable feedback.
This research is sponsored by the ArmyResearch Laboratory (ARL) under agreement W911NF-09-2-0053.
The authors also gratefully acknowledge the supportof the Defense Advanced Research Projects Agency (DARPA)Machine Reading Program under Air Force Research Labo-ratory (AFRL) prime contract no.
FA8750-09-C-0181.
Thiswork is also supported by the Intelligence Advanced ResearchProjects Activity (IARPA) Foresight and Understanding fromScientific Exposition (FUSE) Program via Department of In-terior National Business Center contract number D11PC2015.Any opinions, findings, and conclusions or recommendationsexpressed in this material are those of the author(s) and do notnecessarily reflect the view of ARL, DARPA, AFRL, IARPA,or the US government.ReferencesY-W. Chang and M. Collins.
2011.
Exact decodingof phrase-based translation models through lagrangianrelaxation.
EMNLP.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In ACL.J.
Clarke and M. Lapata.
2006.
Constraint-basedsentence compression: An integer programming ap-proach.
In ACL.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In EMNLP.R.
de Salvo Braz, E. Amir, and D. Roth.
2005.
Liftedfirst-order probabilistic inference.
In IJCAI.D Graff and C. Cieri.
2003.
English gigaword.A.
Martins, N. A. Smith, and E. Xing.
2009.
Conciseinteger linear programming formulations for depen-dency parsing.
In ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005.Non-projective dependency parsing using spanningtree algorithms.
In EMNLP, pages 523?530, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.M.
Palmer, D. Gildea, and N. Xue.
2010.
Semantic RoleLabeling, volume 3.
Morgan & Claypool Publishers.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The neces-sity of syntactic parsing for semantic role labeling.
InIJCAI.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In EMNLP.S.
Riedel.
2009.
Cutting Plane MAP Inference forMarkov Logic.
Machine Learning.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InHwee Tou Ng and Ellen Riloff, editors, CoNLL.1123D.
Roth and W. Yih.
2007.
Global inference for entityand relation identification via a linear programmingformulation.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.A.
M. Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On dual decomposition and linear program-ming relaxations for natural language processing.
InEMNLP.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research.1124
