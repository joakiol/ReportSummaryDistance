Proceedings of NAACL-HLT 2013, pages 22?31,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsImproved Reordering for Phrase-Based Translation using Sparse FeaturesColin CherryNational Research Council CanadaColin.Cherry@nrc-cnrc.gc.caAbstractThere have been many recent investigationsinto methods to tune SMT systems using largenumbers of sparse features.
However, therehave not been nearly so many examples ofhelpful sparse features, especially for phrase-based systems.
We use sparse features to ad-dress reordering, which is often considered aweak point of phrase-based translation.
Usinga hierarchical reordering model as our base-line, we show that simple features couplingphrase orientation to frequent words or word-clusters can improve translation quality, withboosts of up to 1.2 BLEU points in Chinese-English and 1.8 in Arabic-English.
We com-pare this solution to a more traditional max-imum entropy approach, where a probabilitymodel with similar features is trained on word-aligned bitext.
We show that sparse decoderfeatures outperform maximum entropy hand-ily, indicating that there are major advantagesto optimizing reordering features directly forBLEU with the decoder in the loop.1 IntroductionWith the growing adoption of tuning algorithms thatcan handle thousands of features (Chiang et al2008; Hopkins and May, 2011), SMT system de-signers now face a choice when incorporating newideas into their translation models.
Maximum like-lihood models can be estimated from large word-aligned bitexts, creating a small number of highlyinformative decoder features; or the same ideas canbe incorporated into the decoder?s linear model di-rectly.
There are trade-offs to each approach.
Max-imum likelihood models can be estimated from mil-lions of sentences of bitext, but optimize a mis-matched objective, predicting events observed inword aligned bitext instead of optimizing translationquality.
Sparse decoder features have the oppositeproblem; with the decoder in the loop, we can onlytune on small development sets,1 but a translationerror metric directly informs training.We investigate this trade-off in the context of re-ordering models for phrase-based decoding.
Start-ing with the intuition that most lexicalized reorder-ing models do not smooth their orientation distri-butions intelligently for low-frequency phrase-pairs,we design features that track the first and last words(or clusters) of the phrases in a pair.
These featuresare incorporated into a maximum entropy reorder-ing model, as well as sparse decoder features, to seewhich approach best complements the now-standardrelative-frequency lexicalized reordering model.We also view our work as an example of strongsparse features for phrase-based translation.
Fea-tures from hierarchical and syntax-based transla-tion (Chiang et al 2009) do not easily transferto the phrase-based paradigm, and most work thathas looked at large feature counts in the context ofphrase-based translation has focused on the learn-ing method, and not the features themselves (Hop-kins and May, 2011; Cherry and Foster, 2012; Gim-pel and Smith, 2012).
We show that by targetingreordering, large gains can be made with relativelysimple features.2 BackgroundPhrase-based machine translation constructs its tar-get sentence from left-to-right, with each translationoperation selecting a source phrase and appendingits translation to the growing target sentence, until1Some systems tune for BLEU on much larger sets (Simi-aner et al 2012; He and Deng, 2012), but these require excep-tional commitments of resources and time.22all source words have been covered exactly once.The first phrase-based translation systems appliedonly a distortion penalty to model reordering (Koehnet al 2003; Och and Ney, 2004).
Any devia-tion from monotone translation is penalized, witha single linear weight determining how quickly thepenalty grows.2.1 Lexicalized ReorderingImplemented in a number of phrase-based decoders,the lexicalized reordering model (RM) uses word-aligned data to determine how each phrase-pairtends to be reordered during translation (Tillmann,2004; Koehn et al 2005; Koehn et al 2007).The core idea in this RM is to divide reorderingevents into three orientations that can be easily deter-mined both during decoding and from word-aligneddata.
The orientations can be described in terms ofthe previously translated source phrase (prev) andthe next source phrase to be translated (next):?
Monotone (M): next immediately follows prev.?
Swap (S): prev immediately follows next.?
Discontinuous (D): next and prev are not adja-cent in the source.Note that prev and next can be defined for construct-ing a translation from left-to-right or from right-to-left.
Most decoders incorporate RMs for both direc-tions; our discussion will generally only cover left-to-right, with the right-to-left case being implicit andsymmetrical.As the decoder extends its hypothesis by trans-lating a source phrase, we can assess the impliedorientations to determine if the resulting reorderingmakes sense.
This is done using the probability ofan orientation given the phrase pair pp = [src, tgt ]extending the hypothesis:2P (o|pp) ?cnt(o, pp)?o cnt(o, pp)(1)where o ?
{M,S,D}, cnt uses simple heuristics onword-alignments to count phrase pairs and their ori-entations, and the ?
symbol allows for smoothing.The log of this probability is easily folded into thelinear models that guide modern decoders.
Better2pp corresponds to the phrase pair translating next for theleft-to-right model, and prev for right-to-left.performance is achieved by giving each orientationits own log-linear weight (Koehn et al 2005).2.2 Hierarchical ReorderingIntroduced by Galley and Manning (2008), the hier-archical reordering model (HRM) also tracks statis-tics over orientations, but attempts to increase theconsistency of orientation assignments.
To do so,they remove the emphasis on the previously trans-lated phrase (prev ), and instead determine orienta-tion using a compact representation of the full trans-lation history, as represented by a shift-reduce stack.Each source span is shifted onto the stack as it istranslated; if the new top is adjacent to the span be-low it, then a reduction merges the two.Orientations are determined in terms of the topof this stack,3 rather than the previously translatedphrase prev.
The resulting orientations are moreconsistent across different phrasal decompositionsof the same translation, and more consistent with thestatistics extracted from word aligned data.
This re-sults in a general improvement in performance.
Weassume the HRM as our baseline reordering model.It is important to note that although our maximumentropy and sparse reordering solutions build on theHRM, the features in this paper can still be appliedwithout a shift-reduce stack, by using the previouslytranslated phrase where we use the top of the stack.2.3 Maximum Entropy ReorderingOne frequent observation regarding both the RM andthe HRM is that the statistics used to grade orien-tations are very sparse.
Each orientation predic-tion P (o|pp) is conditioned on an entire phrase pair.Koehn et al(2005) experiment with alternatives,such as conditioning on only the source or the tar-get, but using the entire pair generally performs best.The vast majority of phrase pairs found in bitext withstandard extraction heuristics are singletons (morethan 92% in our Arabic-English bitext), and the cor-responding P (o|pp) estimates are based on a singleobservation.
Because of these heavy tails, there havebeen several attempts to use maximum entropy tocreate more flexible distributions.One straight-forward way to do so is to continuepredicting orientations on phrases, but to use maxi-3In the case of the right-to-left model, an approximation ofthe top of the stack is used instead.23mum entropy to consider features of the phrase pair.This is the approach taken by Xiong et al(2006);their maximum entropy model chooses between Mand S orientations, which are the only two optionsavailable in their chart-based ITG decoder.
Nguyenet al(2009) build a similar model for a phrase-basedHRM, using syntactic heads and constituent labelsto create a rich feature set.
They show gains over anHRM baseline, albeit on a small training set.A related approach is to build a reordering modelover words, which is evaluated at phrase bound-aries at decoding time.
Zens and Ney (2006) pro-pose one such model, with jumps between wordsbinned very coarsely according to their directionand distance, testing models that differentiate onlyleft jumps from right, as well as the cross-productof {left, right} ?
{adjacent, discontinuous}.
Theirfeatures consider word identity and automatically-induced clusters.
Green et al(2010) present a sim-ilar approach, with finer-grained distance bins, us-ing word-identity and part-of-speech for features.Yahyaei and Monz (2010) also predict distance bins,but use much more context, opting to look at bothsides of a reordering jump; they also experimentwith hard constraints based on their model.Tracking word-level reordering simplifies the ex-traction of complex models from word alignments;however, it is not clear if it is possible to enhancea word reordering model with the stack-based his-tories used by HRMs.
In this work, we construct aphrase orientation maximum entropy model.3 MethodsOur primary contribution is a comparison betweenthe standard HRM and two feature-based alterna-tives.
Since a major motivating concern is smooth-ing, we begin with a detailed description of ourHRM baseline, followed by our maximum entropyHRM and our novel sparse reordering features.3.1 Relative Frequency ModelThe standard HRM uses relative frequencies to buildsmoothed maximum likelihood estimates of orien-tation probabilities.
Orientation counts for phrasepairs are collected from bitext, using the method de-scribed by Galley and Manning (2008).
The proba-bility model P (o|pp = [src, tgt ]) is estimated usingrecursive MAP smoothing:P (o|pp) =cnt(o, pp) + ?sPs(o|src) + ?tPt(o|tgt)?o cnt(o, pp) + ?s + ?tPs(o|src) =?tgt cnt(o, src, tgt) + ?gPg(o)?o,tgt cnt(o, src, tgt) + ?gPt(o|tgt) =?src cnt(o, src, tgt) + ?gPg(o)?o,src cnt(o, src, tgt) + ?gPg(o) =?pp cnt(o, pp) + ?u/3?o,pp cnt(o, pp) + ?u(2)where the various ?
parameters can be tuned em-pirically.
In practice, the model is not particularlysensitive to these parameters.43.2 Maximum Entropy ModelNext, we describe our implementation of a maxi-mum entropy HRM.
Our goal with this system isto benefit from modeling features of a phrase pair,while keeping the system architecture as simple andreplicable as possible.
To simplify training, we learnour model from the same orientation counts thatpower the relative-frequency HRM.
To simplify de-coder integration, we limit our feature space to in-formation from a single phrase pair.In a maximum entropy model, the probability ofan orientation o given a phrase pair pp is given by alog-linear model:P (o|pp) =exp(w ?
f(o, pp))?o?
exp(w ?
f(o?, pp))(3)where f(o, pp) returns features of a phrase-pair, andw is the learned weight vector.
We build two models,one for left-to-right translation, and one for right-to-left.
As with the relative frequency model, welimit our discussion to the left-to-right model, withthe other direction being symmetrical.We construct a training example for each uniquephrase-pair type (as opposed to token) found in ourbitext.
We use the orientation counts observed fora phrase pair ppi to construct its example weight:ci =?o cnt(o, ppi).
The same counts are used toconstruct a target distribution P?
(o|ppi), using the4We use a historically good setting of ??
= 10 throughout.24Base:bias; src ?
tgt ; src; tgtsrc.first ; src.last ; tgt .first ; tgt .lastclust50(src.first); clust50(src.last)clust50(tgt .first); clust50(tgt .last)?
Orientation {M,S,D}Table 1: Features for the Maximum Entropy HRM.unsmoothed relative frequency estimate in Equa-tion 1.
We then train our weight vector to minimize:12||w||2+C?ici[log?o exp (w ?
f(o, ppi))?
?o P?
(o|ppi) (w ?
f(o, ppi))](4)where C is a hyper-parameter that controls theamount of emphasis placed on minimizing loss ver-sus regularizing w.5 Note that this objective is a de-parture from previous work, which tends to create anexample for each phrase-pair token, effectively as-signing P?
(o|pp) = 1 to a single gold-standard ori-entation.
Instead, our model attempts to reproducethe target distribution P?
for the entire type, wherethe penalty ci for missing this target is determinedby the frequency of the phrase pair.
The resultingmodel will tend to match unsmoothed relative fre-quency estimates for very frequent phrase pairs, andwill smooth intelligently using features for less fre-quent phrase pairs.All of the features returned by f(o|pp) are derivedfrom the phrase pair pp = [src, tgt ], with the goalof describing the phrase pair at a variety of granu-larities.
Our features are described in Table 1, usingthe following notation: the operators first and lastreturn the first and last words of phrases,6 while theoperator clust50 maps a word onto its correspondingcluster from an automatically-induced, determinis-tic 50-word clustering provided by mkcls (Och,1999).
Our use of words at the corners of phrases(as opposed to the syntactic head, or the last alignedword) follows Xiong et al(2006), while our use ofword clusters follows Zens and Ney (2006).
Eachfeature has the orientation o appended onto it.To help scale and to encourage smoothing, weonly allow features that occur in at least 5 phrase pair5Preliminary experiments indicated that the model is robustto the choice of C; we use C = 0.1 throughout.6first = last for a single-word phraseBase:src.first ; src.last ; tgt .first ; tgt .lasttop.src.first ; top.src.last ; top.tgt .lastbetween words?
Representation{80-words, 50-clusters, 20-clusters}?
Orientation{M,S,D}Table 2: Features for the Sparse Feature HRM.tokens.
Furthermore, to deal with the huge numberof extracted phrase pairs (our Arabic system extractsroughly 88M distinct phrase pair types), we subsam-ple pairs that have been observed only once, keepingonly 10% of them.
This reduces the number of train-ing examples from 88M to 19M.3.3 Sparse Reordering FeaturesThe maximum entropy approach uses features tomodel the distribution of orientations found in wordalignments.
Alternatively, a number of recent tun-ing methods, such as MIRA (Chiang et al 2008)or PRO (Hopkins and May, 2011), can handle thou-sands of features.
These could be used to tune simi-lar features to maximize BLEU directly.Given the appropriate tuning architecture, thesparse feature approach is actually simpler in manyways than the maximum entropy approach.
Thereis no need to scale to millions of training exam-ples, and there is no question of how to integrate thetrained model into the decoder.
Instead, one simplyimplements the desired features in the decoder?s fea-ture API and then tunes as normal.
The challenge isto design features so that the model can be learnedfrom small tuning sets.The standard approach for sparse feature designin SMT is to lexicalize only on extremely fre-quent words, such as the top-80 words from eachlanguage (Chiang et al 2009; Hopkins and May,2011).
We take that approach here, but we alsouse deterministic clusters to represent words fromboth languages, as provided by mkcls.
These clus-ters mirror parts-of-speech quite effectively (Blun-som and Cohn, 2011), without requiring linguisticresources.
They should provide useful generaliza-tion for reordering decisions.
Inspired by recent suc-cesses in semi-supervised learning (Koo et al 2008;25corpus sentences words (ar) words (en)train 1,490,514 46,403,734 47,109,486dev 1,663 45,243 50,550mt08 1,360 45,002 51,341mt09 1,313 40,684 46,813Table 3: Arabic-English Corpus.
For English dev and testsets, word counts are averaged across 4 references.Lin and Wu, 2009), we cluster at two granularities(20 clusters and 50 clusters), and allow the discrim-inative tuner to determine how to best employ thevarious representations.We add the sparse features in Table 2 to ourdecoder to help assess reordering decisions.
Aswith the maximum entropy model, orientation is ap-pended to each feature.
Furthermore, each featurehas a different version for each of our three wordrepresentations.
Like the maximum entropy model,we describe the phrase pair being added to the hy-pothesis in terms of the first and last words of itsphrases.
Unlike the maximum entropy model, wemake no attempt to use entire phrases or phrase-pairs as features, as they would be far too sparse forour small tuning sets.
However, due to the sparsefeatures?
direct decoder integration, we have accessto a fair amount of extra context.
We represent thecurrent top of the stack (top) using its first and lastsource words (accessible from the HRM stack), andits last target word (accessible using language modelcontext).
Furthermore, for discontinuous (D) orien-tations, we can include an indicator for each sourceword between the current top of the stack and thephrase being added.Because the sparse feature HRM has no accessto phrase-pair or monolingual phrase features, andbecause it completely ignores our large supply ofword-aligned training data, we view it as compli-mentary to the relative frequency HRM.
We alwaysinclude both when tuning and decoding.
Further-more, we only include sparse features in the left-to-right translation direction, as the features alreadyconsider context (top) as well as the next phrase.4 Experimental DesignWe test our reordering models in Arabic to Englishand Chinese to English translation tasks.
Both sys-tems are trained on data from the NIST 2012 MTcorpus sentences words (ch) words (en)train 3,505,529 65,917,610 69,453,695dev 1,894 48,384 53,584mt06 1,664 39,694 47,143mt08 1,357 33,701 40,893Table 4: Chinese-English Corpus.
For English dev andtest sets, word counts are averaged across 4 references.evaluation; the Arabic system is summarized in Ta-ble 3 and the Chinese in Table 4.
The Arabic sys-tem?s development set is the NIST mt06 test set, andits test sets are mt08 and mt09.
The Chinese sys-tem?s development set is taken from the NIST mt05evaluation set, augmented with some material re-served from our NIST training corpora in order tobetter cover newsgroup and weblog domains.
Its testsets are mt06 and mt08.4.1 Baseline SystemFor both language pairs, word alignment is per-formed by GIZA++ (Och and Ney, 2003), with5 iterations of Model 1, HMM, Model 3 andModel 4.
Phrases are extracted with a length limitof 7 from alignments symmetrized using grow-diag-final-and (Koehn et al 2003).
Conditionalphrase probabilities in both directions are estimatedfrom relative frequencies, and from lexical probabil-ities (Zens and Ney, 2004).
4-gram language mod-els are estimated from the target side of the bitextwith Kneser-Ney smoothing.
Relative frequencyand maximum entropy RMs are represented with sixfeatures, with separate weights for M, S and D inboth directions (Koehn et al 2007).
HRM orien-tations are determined using an unrestricted shift-reduce parser (Cherry et al 2012).
We also em-ploy a standard distortion penalty incorporating theminimum completion cost described by Moore andQuirk (2007).
Our multi-stack phrase-based decoderis quite similar to Moses (Koehn et al 2007).For all systems, parameters are tuned with abatch-lattice variant of hope-fear MIRA (Chiang etal., 2008; Cherry and Foster, 2012).
Preliminary ex-periments suggested that the sparse reordering fea-tures have a larger impact when tuned with latticesas opposed to n-best lists.264.2 EvaluationWe report lower-cased BLEU (Papineni et al 2002),evaluated using the same English tokenization usedin training.
For our primary results, we perform ran-dom replications of parameter tuning, as suggestedby Clark et al(2011).
Each replication uses a dif-ferent random seed to determine the order in whichMIRA visits tuning sentences.
We test for signifi-cance using Clark et als MultEval tool, which usesa stratified approximate randomization test to ac-count for multiple replications.5 ResultsWe begin with a comparison of the reordering mod-els described in this paper: the hierarchical reorder-ing model (HRM), the maximum entropy HRM(Maxent HRM) and our sparse reordering features(Sparse HRM).
Results are shown in Table 5.Our three primary points of comparison have beentested with 5 replications.
We report BLEU scoresaveraged across replications as well as standard de-viations, which indicate optimizer stability.
We alsoprovide unreplicated results for two systems, one us-ing only the distortion penalty (No RM), and oneusing a non-hierarchical reordering model (RM).These demonstrate that our baseline already hasquite mature reordering capabilities.The Maxent HRM has very little effect on trans-lation performance.
We found this surprising; weexpected large gains from improving the reorder-ing distributions of low-frequency phrase-pairs.
See?5.1 for further exploration of this result.The Sparse HRM, on the other hand, performsvery well.
It produces significant BLEU score im-provements on all test sets, with improvements rang-ing between 1 and 1.8 BLEU points.
Even withmillions of training sentences for our HRM, thereis a large benefit in building HRM-like features thatare tuned to optimize the decoder?s BLEU score onsmall tuning sets.
We examine the impact of subsetsof these features in ?5.2.The test sets?
standard deviations increase from0.1 under the baseline to 0.3 under the Sparse HRMfor Chinese-English, indicating a decrease in opti-mizer stability.
With so many features trained onso few sentences, this is not necessarily surprising.Fortunately, looking at the actual replications (notBase:src.first ; src.last ; tgt .first ; tgt .last?
Representation{80-words, 50-clusters}?
Orientation{M,S,D}Table 6: Intersection of Maxent & Sparse HRM features.shown), we confirmed that if a replication producedlow scores in one test, it also produced low scoresin the other.
This means that one should be able tooutperform the average case by using a dev-test setto select among replications.5.1 Maximum Entropy AnalysisThe next two sections examine our two solutionsin detail, starting with the Maxent HRM.
To avoidexcessive demands on our computing resources, allexperiments report tuning with a single replicationwith the same seed.
We select Arabic-English forour analysis, as this pair has high optimizer stabilityand fast decoding speeds.Why does the Maxent HRM help so little?
Webegin by investigating some design decisions.
Onepossibility is that our subsampling of frequency-1training pairs (see ?3.2) harmed performance.
Totest the impact of this decision, we train a Max-ent HRM without subsampling, taking substantiallylonger.
The resulting BLEU scores (not shown) arewell within the projected standard deviations for op-timizer instability (0.1 BLEU from Table 5).
Thisindicates that subsampling is not the problem.
Toconfirm our choice of hyperparameters, we conducta grid search over the Maxent HRM?s regulariza-tion parameter C (see Equation 4), covering the set{1, 0.1, 0.01, 0.001}, where C = 0.1 is the valueused throughout this paper.
Again, the resultingBLEU scores (not shown) are all within 0.1 of themeans reported in Table 5.Another possibility is that the Maxent HRM hasan inferior feature set.
We selected features for ourMaxent and Sparse HRMs to be similar, but also toplay to the strengths of each method.
To level theplaying field, we train and test both systems with thefeature set shown in Table 6, which is the intersec-tion of the features from Tables 1 and 2.
The result-ing average BLEU scores are shown in Table 7.
With27Chinese-English Arabic-EnglishMethod n tune std mt06 std mt08 std tune std mt08 std mt09 stdNo RM 1 24.3 ?
32.0 ?
26.4 ?
41.7 ?
41.4 ?
44.1 ?RM 1 25.2 ?
33.3 ?
27.4 ?
42.4 ?
42.6 ?
45.2 ?HRM (baseline) 5 25.6 0.0 34.2 0.1 28.0 0.1 42.9 0.0 42.9 0.1 45.5 0.0HRM + Maxent HRM 5 25.6 0.0 34.3 0.1 28.1 0.1 43.0 0.0 42.9 0.0 45.6 0.1HRM + Sparse HRM 5 28.0 0.1 35.4 0.3 29.0 0.3 47.0 0.1 44.6 0.1 47.3 0.1Table 5: Comparing reordering methods according to BLEU score.
n indicates the number of tuning replications,while standard deviations (std) indicate optimizer stability.
Test scores that are significantly higher (p < 0.01) thanthe HRM baseline are highlighted in bold.Method ?HRM +HRMHRM (baseline) ?
44.2OriginalMaxent HRM 44.2 44.2Sparse HRM 45.4 46.0IntersectionMaxent HRM 43.8 44.2Sparse HRM 45.2 46.0Table 7: Arabic-English BLEU scores with each system?soriginal feature set versus the intersection of the two fea-ture sets, with and without the relative frequency HRM.BLEU is averaged across mt08 and mt09.the baseline HRM included, performance does notchange for either system with the intersected featureset.
Sparse features continue to help, while the max-imum entropy model does not.
Without the HRM,both systems degrade under the intersection, thoughthe Sparse HRM still improves over the baseline.Finally, we examine Maxent HRM performanceas a function of the amount of word-aligned train-ing data.
To do so, we hold all aspects of our sys-tem constant, except for the amount of bitext used totrain either the baseline HRM or the Maxent HRM.Importantly, the phrase table always uses the com-plete bitext.
For our reordering training set, we holdout the final two thousand sentences of bitext to cal-culate perplexity.
This measures the model?s sur-prise at reordering events drawn from previously un-seen alignments; lower values are better.
We pro-ceed to subsample sentence pairs from the remain-ing bitext, in order to produce a series of trainingbitexts of increasing size.
We subsample with theprobability of accepting a sentence pair, Pa, set to{0.001, 0.01, 0.1, 1}.
It is important to not confusethis subsampling of sentence pairs with the sub-sampling of low-frequency phrase pairs (see ?3.2),which is still carried out by the Maxent HRM foreach training scenario.Figure 1 shows how BLEU (averaged across bothtest sets) and perplexity vary as training data in-creases from 1.5K sentences to the full 1.5M.
AtPa < 0.1, corresponding to less than 150K sen-tences, the maximum entropy model actually makesa substantial difference in terms of BLEU.
However,these deltas narrow to nothing as we reach millionsof training sentences.
This is consistent with the re-sults of Nguyen et al(2009), who report that maxi-mum entropy reordering outperforms a similar base-line, but using only 50K sentence pairs.A related observation is that held-out perplexitydoes not seem to predict BLEU in any useful way.In particular, perplexity does not predict that the twosystems will become similar as data grows, nor doesit predict that maxent?s performance will level off.Predicting the orientations of unseen alignments isnot the same task as predicting the orientation for aphrase during translation.
We suspect that perplexityplaces too much emphasis on rare or previously un-seen phrase pairs, due to phrase extraction?s heavytails.
Preliminary attempts to correct for this us-ing absolute discounting on the test counts did notresolve these issues.
Unfortunately, in maximizing(regularized or smoothed) likelihood, both maxentand relative frequency HRMs are chasing the per-plexity objective, not the BLEU objective.5.2 Sparse Feature AnalysisThe results in Table 7 from ?5.1 already provideus with a number of insights regarding the SparseHRM.
First, note that the intersected feature set usesonly information found within a single phrase.
Thefact that the Sparse HRM performs so well with281.55?1.6?1.65?1.7?1.75?1.8?1.85?1.9?1.95?2?0.001?
0.01?
0.1?
1?Perplexity?as?Data?Grows?MaxEnt?RelFreq?41.5?42?42.5?43?43.5?44?44.5?0?
0.001?
0.01?
0.1?
1?BLEU?as?Data?Grows?Figure 1: Learning curves for Relative Frequency and Maximum Entropy reordering models on Arabic-English.Feature Group Count BLEUNo Sparse HRM 0 44.2Between 312 44.4Stack 1404 45.2Phrase 1872 45.920 Clusters 506 45.450 Clusters 1196 45.880 Words 1886 45.8Full Sparse HRM 3588 46.0Table 8: Versions of the Sparse HRM built using or-ganized subsets of the complete feature set for Arabic-English.
Count is the number of distinct features, whileBLEU is averaged over mt08 and mt09.intersected features indicates that modeling contextoutside a phrase is not essential for strong perfor-mance.
Furthermore, the ?HRM portion of the ta-ble indicates that the sparse HRM does not requirethe baseline HRM to be present in order to outper-form it.
This is remarkable when one considers thatthe Sparse HRM uses less than 4k features to modelphrase orientations, compared to the 530M proba-bilities7 maintained by the baseline HRM?s relativefrequency model.To determine which feature groups are most im-portant, we tested the Sparse HRM on Arabic-English with a number of feature subsets.
We reportBLEU scores averaged over both test sets in Table 8.First, we break our features into three groups accord-ing to what part of the hypothesis is used to assessorientation.
For each of these location groups, allforms of word representation (clusters or frequentwords) are employed.
The groups consist of Be-788.4M phrase pairs ?
3 orientations (M, S and D) ?
2translation directions (left-to-right and right-to-left).tween: the words between the top of the stack andthe phrase to be added; Stack: words describingthe current top of the stack; and Phrase: words de-scribing the phrase pair being added to the hypothe-sis.
Each group was tested alone to measure its use-fulness.
This results in a clear hierarchy, with thephrase features being the most useful (nearly as use-ful as the complete system), and the between fea-tures being the least.
Second, we break our featuresinto three groups according to how words are rep-resented.
For each of these representation groups,all location groups (Between, Stack and Phrase) areemployed.
The groups are quite intuitive: 20 Clus-ters, 50 Clusters or 80 Words.
The differences be-tween representations are much less dramatic thanthe location groups.
All representations performwell on their own, with the finer-grained ones per-forming better.
Including multiple representationsprovides a slight boost, but these experiments sug-gest that a leaner model could certainly drop one ortwo representations with little impact.In its current implementation, the Sparse HRM isroughly 4 times slower than the baseline decoder.Our sparse feature infrastructure is designed for flex-ibility, not speed.
To affect reordering, each sparsefeature template is re-applied with each hypothesisextension.
However, the intersected feature set from?5.1 is only 2 times slower, and could be made fasterstill.
That feature set uses only within-phrase fea-tures to asses orientations; therefore, the total weightfor each orientation for each phrase-pair could bepre-calculated, making its cost comparable to thebaseline.29Chinese-English tune mt06 mt08Base 27.7 39.9 33.7+Sparse HRM 29.2 41.0 34.1Arabic-English tune mt08 mt09Base 49.6 49.1 51.6+Sparse HRM 51.7 49.9 52.2Table 9: The effect of Sparse HRMs on complex systems.5.3 Impact on Competition-Grade SMTThus far, we have employed a baseline that has beendesigned for both translation quality and replicabil-ity.
We now investigate the impact of our SparseHRM on a far more complex baseline: our internalsystem used for MT competitions such as NIST.The Arabic system uses roughly the same bilin-gual data as our original baseline, but also includesa 5-gram language model learned from the EnglishGigaword.
The Chinese system adds the UN bitextas well as the English Gigaword.
Both systems makeheavy use of linear mixtures to create refined transla-tion and language models, mixing across sources ofcorpora, genre and translation direction (Foster andKuhn, 2007; Goutte et al 2009).
They also mixmany different sources of word alignments, withthe system adapting across alignment sources us-ing either binary indicators or linear mixtures.
Im-portantly, these systems already incorporate thou-sands of sparse features as described by Hopkins andMay (2011).
These provide additional informationfor each phrase pair through frequency bins, phrase-length bins, and indicators for frequent alignmentpairs.
Both systems include a standard HRM.The result of adding the Sparse HRM to these sys-tems is shown in Table 9.
Improvements range from0.4 to 1.1 BLEU, but importantly, all four test setsimprove.
The impact of these reordering features isreduced slightly in the presence of more carefullytuned translation and language models, but they re-main a strong contributor to translation quality.6 ConclusionWe have shown that sparse reordering features canimprove the quality of phrase-based translations,even in the presence of lexicalized reordering mod-els that track the same orientations.
We have com-pared this solution to a maximum entropy model,which does not improve our HRM baseline.
Ouranalysis of the maximum entropy solution indicatesthat smoothing the orientation estimates is not a ma-jor concern in the presence of millions of sentencesof bitext.
This implies that our sparse features areachieving their improvement because they optimizeBLEU with the decoder in the loop, side-steppingthe objective mismatch that can occur when train-ing on word-aligned data.
The fact that this is possi-ble with such small tuning corpora is both surprisingand encouraging.In the future, we would like to investigate howto incorporate useful future cost estimates for oursparse reordering features.
Previous work has shownfuture distortion penalty estimates to be importantfor both translation speed and quality (Moore andQuirk, 2007; Green et al 2010), but we have ig-nored future costs in this work.
We would also liketo investigate features inspired by transition-basedparsing, such as features that look further down thereordering stack.
Finally, as there is evidence thatideas from lexicalized reordering can help hierarchi-cal phrase-based SMT (Huck et al 2012), it wouldbe interesting to explore the use of sparse RMs inthat setting.AcknowledgmentsThanks to George Foster, Roland Kuhn and theanonymous reviewers for their valuable commentson an earlier draft.ReferencesPhil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised part ofspeech induction.
In ACL, pages 865?874, Portland,Oregon, USA, June.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In HLT-NAACL, pages 427?436, Montre?al, Canada, June.Colin Cherry, Robert C. Moore, and Chris Quirk.
2012.On hierarchical re-ordering and permutation parsingfor phrase-based decoding.
In Proceedings of theWorkshop on Statistical Machine Translation, pages200?209, Montre?al, Canada, June.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In EMNLP, pages 224?233.30David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In HLT-NAACL, pages 218?226.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In ACL, pages 176?181.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the Workshopon Statistical Machine Translation, pages 128?135.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP, pages 848?856, Honolulu, Hawaii.Kevin Gimpel and Noah A. Smith.
2012.
Structuredramp loss minimization for machine translation.
InHLT-NAACL, Montreal, Canada, June.Cyril Goutte, David Kurokawa, and Pierre Isabelle.2009.
Improving SMT by learning the translation di-rection.
In EAMT Workshop on Statistical Multilin-gual Analysis for Retrieval and Translation.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion cost forstatistical machine translation.
In HLT-NAACL, pages867?875, Los Angeles, California, June.Xiaodong He and Li Deng.
2012.
Maximum expectedbleu training of phrase and lexicon translation mod-els.
In Proceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume 1:Long Papers), pages 292?301, Jeju Island, Korea, July.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In EMNLP, pages 1352?1362.Matthias Huck, Stephan Peitz, Markus Freitag, and Her-mann Ney.
2012.
Discriminative reordering exten-sions for hierarchical phrase-based machine transla-tion.
In Proceedings of the 16th Annual Conferenceof the European Association for Machine Translation(EAMT), pages 313?320, Trento, Italy, May.Philipp Koehn, Franz Joesef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL, pages 127?133.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProceedings to the International Workshop on SpokenLanguage Translation (IWSLT).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,pages 177?180, Prague, Czech Republic, June.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In ACL,pages 595?603, Columbus, Ohio, June.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of the JointConference of the ACL and the AFNLP, pages 1030?1038, Singapore, August.Robert C. Moore and Chris Quirk.
2007.
Faster beam-search decoding for phrasal statistical machine trans-lation.
In MT Summit XI, September.Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,and Thai Phuong Nguyen.
2009.
Improving a lexi-calized hierarchical reordering model using maximumentropy.
In MT Summit XII, Ottawa, Canada, August.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1), March.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4), December.Franz Josef Och.
1999.
An efficient method for deter-mining bilingual word classes.
In EACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In ACL, pages 311?318.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in smt.
InACL, pages 11?21, Jeju Island, Korea, July.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In HLT-NAACL,pages 101?104, Boston, USA, May.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In COLING-ACL, pages521?528, Sydney, Australia, July.Sirvan Yahyaei and Christof Monz.
2010.
Dynamic dis-tortion in a discriminative reordering model for statis-tical machine translation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation(IWSLT), pages 353?360.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical machine translation.
In HLT-NAACL, pages 257?264, Boston, USA, May.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 55?63, New York City, June.31
