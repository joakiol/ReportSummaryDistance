Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147?1157,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSyntax-Based Grammaticality Improvement using CCG and Guided SearchYue ZhangUniversity of CambridgeComputer Laboratoryyue.zhang@cl.cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorystephen.clark@cl.cam.ac.ukAbstractMachine-produced text often lacks grammat-icality and fluency.
This paper studies gram-maticality improvement using a syntax-basedalgorithm based on CCG.
The goal of thesearch problem is to find an optimal parse treeamong all that can be constructed through se-lection and ordering of the input words.
Thesearch problem, which is significantly harderthan parsing, is solved by guided learning forbest-first search.
In a standard word order-ing task, our system gives a BLEU score of40.1, higher than the previous result of 33.7achieved by a dependency-based system.1 IntroductionMachine-produced text, such as SMT output, oftenlacks grammaticality and fluency, especially whenusing n-gram language modelling (Knight, 2007).Recent efforts have been made to improve grammat-icality using local language models (Blackwood etal., 2010) and global dependency structures (Wan etal., 2009).
We study grammaticality improvementusing a syntax-based system.The task is effectively a text-to-text generationproblem where the goal is to produce a grammati-cal sentence from an ungrammatical and fragmen-tary input.
The input can range from a bag-of-words (Wan et al, 2009) to a fully-ordered sentence(Blackwood et al, 2010).
A general form of theproblem is to construct a grammatical sentence froma set of un-ordered input words.
However, in caseswhere the base system produces fluent subsequenceswithin the sentence, constraints on the choice andorder of certain words can be fed to the grammati-cality improvement system.
The input may also in-clude words beyond the output of the base system,e.g.
extra words from the SMT lattice, so that con-tent word insertion and deletion can be performedimplicity via word selection.We study the above task using CCG (Steedman,2000).
The main challenge is the search problem,which is to find an optimal parse tree among all thatcan be constructed with any word choice and orderfrom the set of input words.
We use an approximatebest-first algorithm, guided by learning, to tacklethe more-than-factorial complexity.
Beam-search isused to control the volume of accepted hypotheses,so that only a very small portion of the whole searchspace is explored.
The search algorithm is guided byperceptron training, which ensures that the exploredpath in the search space consists of highly proba-ble hypotheses.
This framework of best-first searchguided by learning is a general contribution of thepaper, which could be applied to problems outsidegrammaticality improvement.We evaluate our system using the generation taskof word-order recovery, which is to recover the orig-inal word order of a fully scrambled input sentence(Bangalore et al, 2000; Wan et al, 2009).
Thisproblem is an instance of our general task formu-lation, but without any input constraints, or con-tent word selection (since all input words are used).It is straightforward to use this task to evaluateour system and compare with existing approaches.Our system gave 40.1 BLEU score, higher than thedependency-based system of Wan et al (2009), forwhich a BLEU score of 33.7 was reported.11472 The GrammarCombinatory Categorial Grammar (CCG; Steedman(2000)) is a lexicalized grammar formalism, whichassociates words with lexical categories.
Lexicalcategories are detailed grammatical labels, typicallyexpressing subcategorisation information.
CCG, andparsing with CCG, has been described in detailelsewhere (Clark and Curran, 2007; Hockenmaier,2003); here we provide only a short description.During CCG parsing, adjacent categories are com-bined using CCG?s combinatory rules.
For example,a verb phrase in English (S\NP ) can combine withan NP to its left:NP S\NP ?
SIn addition to binary rule instances, such as theone above, there are also unary rules which operateon a single category in order to change its type.
Forexample, forward type-raising can change a subjectNP into a complex category looking to the right fora verb phrase:NP ?
S/(S\NP)Following Hockenmaier (2003), we extract thegrammar by reading rule instances directly from thederivations in CCGbank (Hockenmaier and Steed-man, 2007), rather than defining the combinatoryrule schema manually as in Clark and Curran (2007).3 The Search AlgorithmThe input to the search algorithm is a set of words,each word having a count that specifies the maxi-mum number of times it can appear in the output.Typically, most input words can occur only once inthe output.
However, punctuation marks and func-tion words can be given a higher count.
Dependingon the fluency of the base output (e.g.
the outputof the base SMT system), some constraints can begiven to specific input words, limiting their order oridentifying them as an atomic phrase, for example.The goal of the search algorithm is to find an op-timal parse tree (including the surface string) amongall that can be constructed via selecting and orderinga subset of words from the input multiset.
The com-plexity of this problem is much higher than a typicalparsing problem, since there is an exponential num-ber of word choices for the output sentence, eachwith a factorial number of orderings.
Moreover, dy-namic programming packing for parsers, such as aCKY chart, is not applicable, because of the lack ofa fixed word order.We perform approximate search using a best-first algorithm.
Starting from single words, candi-date parses are constructed bottom-up.
Similar to abest-first parser (Caraballo and Charniak, 1998), thehighest scored hypothesis is expanded first.
A hy-pothesis is expanded by applying CCG unary rulesto the hypothesis, or by combining the hypothesiswith existing hypotheses using CCG binary rules.We use beam search to control the number of ac-cepted hypotheses, so that the computational com-plexity of expanding each hypothesis is linear in thesize of the beam.
Since there is no guarantee that agoal hypothesis will be found in polynomial time,we apply a robustness mechanism (Riezler et al,2002; White, 2004), and construct a default outputwhen no goal hypothesis is found within a time limit.3.1 Data StructuresEdges are the basic structures that represent hy-potheses.
Each edge is a CCG constituent, spanninga sequence of words.
Similar to partial parses in atypical chart parser, edges have recursive structures.Depending on the number of subedges, edges canbe classified into leaf edges, unary edges and binaryedges.
Leaf edges, which represent input words,are constructed first in the search process.
Existingedges are expanded to generate new edges via unaryand binary CCG rules.
An edge that meets the outputcriteria is called a goal edge.
In the experiments ofthis paper, we define a goal edge as one that includesall input words the correct number of times.The signature of an edge consists of the cate-gory label, surface string and head word of the con-stituent.
Two edges are equivalent if they sharethe same signature.
Given our feature definitions,a lower scoring edge with the same signature as ahigher scoring edge cannot be part of the highestscoring derivation.The number of words in the surface string of anedge is called the size of the edge.
Other importantsubstructures of an edge include a bitvector and anarray, which stores the indices of the input wordsthat the edge contains.
Before two edges are com-bined using a binary CCG rule, an input check is per-1148formed to make sure that the total count for a wordfrom the two edges does not exceed the count forthat word in the input.
Intuitively, an edge can recordthe count of each unique input word it contains,and perform the input check in linear time.
How-ever, since most input words typically occur once,they can be indexed and represented by a bitvector,which allows a constant time input check.
The fewmultiple-occurrence words are stored in a count ar-ray.In the best-first process, edges to be expanded areordered by their scores, and stored in an agenda.Edges that have been expanded are stored in a chart.There are many ways in which edges could be or-dered and compared.
Here the chart is organised asa set of beams, each containing a fixed number ofedges with a particular size.
This is similar to typicaldecoding algorithms for phrase-based SMT (Koehn,2010).
In each beam, edges are ordered by theirscores, and low score edges are pruned.
In additionto pruning by the beam, only the highest scored edgeis kept among all that share the same signature.3.2 The Search ProcessFigure 1 shows pseudocode for the search algorithm.During initialization, the agenda (a) and chart (c)are cleared.
All candidate lexical categories are as-signed to each input word, and the resulting leafedges are put onto the agenda.In the main loop, the best edge (e) is popped fromthe agenda.
If e is a goal hypothesis, it is appendedto a list of goals (goal), and the loop is continuedwithout e being expanded.
If e or any equivalentedge e?
of e is already in the chart, the loop continueswithout expanding e. It can be proved that any edgein the chart must have been combined with e?, andtherefore the expansion of e is unnecessary.Edge e is first expanded by applying unary rules,and any new edges are put into a list (new).
Next, eis matched against each existing edge e?
in the chart.e and e?
can be combined if they pass the input check,and there is a binary rule in which the constituentsare combined.
e and e?
are combined in both possibleorders, and any resulting edge is added to new.At the end of each loop, edges from new are addedto the agenda, and new is cleared.
The loop contin-ues until a stopping criterion is met.
A typical stop-ping condition is that goal contains N goal edges.a?
INITAGENDA(input)c?
INITCHART()new?
[]goal?
[]while not STOP(goal, time):e?
POPBEST(a)if GOALTEST(e)APPEND(goal, e)continuefor e?
in c:if EQUIV(e?, e):continuefor e?
in UNARY(e, grammar):APPEND(new, e?
)for e?
in c:if CANCOMBINE(e, e?):e?
?
BINARY(e, e?, grammar)APPEND(new, e?
)if CANCOMBINE(e?, e):e?
?
BINARY(e?, e, grammar)APPEND(new, e?
)for e?
in new:ADD(a, e?
)ADD(c, e)new?
[]Figure 1: The search algorithm.We set N to 1 in our experiments.
For practicalreasons we also include a timeout stopping condi-tion.
If no goal edges are found before the timeoutis reached, a default output is constructed by the fol-lowing procedure.
First, if any two edges in the chartpass the input check, and the words they containconstitute the full input set, they are concatenated toform an output string.
Second, when no two edges inthe chart meet the above condition, the largest edgee?
in the chart is chosen.
Then edges in the chart areiterated over in the larger first order, with any edgethat passes the input check with e?
concatenated withe?
and e?
updated.
The final e?, which can be shorterthan the input, is taken as the default output.4 Model and FeaturesWe use a discriminative linear model to score edges,where the score of an edge e is calculated using theglobal feature vector ?
(e) and the parameter vector1149~w of the model.SCORE(e) = ?
(e) ?
~w?
(e) represents the counts of individual featuresof e. It is computed incrementally as the edge isbuilt.
At each constituent level, the incremental fea-ture vector is extracted according to the feature tem-plates from Table 1, and we use the term constituentlevel vector ?
to refer to it.
So for any edge e, ?
(e)consists of features from the top rule of the hierar-chical structure of e.
?
(e) can be written as the sumof ?(e?)
of all recursive subedges e?
of e, includinge itself:?
(e) =?e??e?(e?
)The parameter update in Section 5 is in terms of con-stituent level vectors.The features in Table 1 represent patterns in-cluding the constituent label; the head word of theconstituent; the size of the constituent; word, POSand lexical category N-grams resulting from a bi-nary combination; and the unary and binary rulesby which the constituent is constructed.
They canbe classified roughly into ?parsing?
features (thoseabout the parse structure, such as the binary rule)and ?generation features?
(those about the surfacestring, such as word bigrams), although some fea-tures, such as ?rule + head word + non-head word?,contain both types of information.5 The Learning AlgorithmThe statistical model plays two important roles inour system.
First, as in typical statistical systems, itis expected to give a higher score to a more correcthypothesis.
Second, it is also crucial to the speed ofthe search algorithm, since the best-first mechanismrelies on a model to find goal hypotheses efficiently.As an indication of the impact of the model on effi-ciency, if the model parameters are set to all zeros,the search algorithm cannot find a result for the firstsentence in the development data within two hours.We perform training on a corpus of CCG deriva-tions, where constituents in a gold-standard deriva-tion serve as gold edges.
The training algorithmruns the decoder on each training example, updat-ing the model when necessary, until the gold goalcondition featureconstituent + sizeall edges constituent + head wordconstituent + size + head wordconstituent + head POSconstituent + leftmost wordconstituent + rightmost wordsize > 1 consti.
+ leftmost POS bigramconsti.
+ rightmost POS bigramconsti.
+ lmost POS + rmost POSthe binary rulethe binary rule + head wordrule + head word + non-head wordbigrams resulting from combinationbinary POS bigrams resulting from combi.edges word trigrams resulting from combi.POS trigrams resulting from combi.resulting lexical categary trigramsresulting word + POS bigramsresulting POS + word bigramsresulting POS + word + POS trigramsunary unary ruleedges unary rule + headwTable 1: Feature template definitions.edge is recovered.
We use the perceptron (Rosen-blatt, 1958) to perform parameter updates.
The tra-ditional perceptron has been adapted to structuralprediction (Collins, 2002) and search optimizationproblems (Daume?
III and Marcu, 2005; Shen et al,2007).
Our training algorithm can be viewed as anadaptation of the perceptron to our best-first frame-work for search efficiency and accuracy.We choose to update parameters as soon as thebest edge from the agenda is not a gold-standardedge.
The intuition is that all gold edges are forcedto be above all non-gold edges on the agenda.
Thisis a strong precondition for parameter updates.
Analternative is to update when a gold-standard edgefalls off the chart, which corresponds to the pre-condition for parameter updates of Daume?
III andMarcu (2005).
However, due to the complexity ofour search task, we found that reasonable trainingefficiency cannot be achieved by the weaker alterna-tives.
Our updates lead both to correctness (edges inthe chart are correct) and efficiency (correct edgesare found at the first possible opportunity).1150During a perceptron update, an incorrect predic-tion, corresponding to the current best edge in theagenda, is penalized, and the corresponding goldedge is rewarded.
However, in our scenario it is notobvious what the corresponding gold edge shouldbe, and there are many ways in which the goldedge could be defined.
We investigated a numberof alternatives, for example trying to find the ?bestmatch?
for the incorrect prediction.
In practice wefound that the simple strategy of selecting the lowestscored gold-standard edge in the agenda was effec-tive, and the results presented in this paper are basedon this method.After an update, there are at least two alterna-tive methods to continue.
The first is to reinitial-ize the agenda and chart using the new model, andcontinue until the current training example is cor-rectly predicted.
This method is called aggressivetraining (Shen et al, 2007).
In order to achievereasonable efficiency, we adopt a second approach,which is to continue training without reinitializingthe agenda and chart.
Instead, only edges from thetop of the agenda down to the lowest-scoring gold-standard edge are given new scores according to thenew parameters.Figure 2 shows pseudocode for the learning al-gorithm applied to one training example.
The ini-tialization is identical to the test search, except thatthe list of goal edges is not maintained.
In the mainloop, the best edge e is popped off the agenda.
If itis the gold goal edge, the training for this sentencefinishes.
If e is not a gold edge, parameter updatesare performed and the loop is continued with e be-ing discarded.
Only gold edges are pushed onto thechart throughout the training process.When updating parameters, the current non-goldedge (e) is used as the negative example, and thesmallest gold edge in the agenda (minGold) is usedas the corresponding positive example.
The modelparameters are updated by adding the constituentlevel feature vector (see Section 4) of minGold, andsubtracting the constituent level feature vector of e.Note that we do not use the global feature vector inthe update, since only the constituent level param-eter vectors are compatible for edges with differentsizes.
After a parameter update, edges are rescoredfrom the top of the agenda down to minGold.The training algorithm iterates through all train-a?
INITAGENDA(input)c?
INITCHART()new?
[]while true:e?
POPBEST(a)if GOLD(e) and GOALTEST(e):returnif not GOLD(e):popped?
[]n?
0while n < GOLDCOUNT(a):e??
POPBEST(a)APPEND(popped, e?
)if GOLD(e?):minGold?
e?n?
n+ 1~w?
~w ?
?
(e) + ?
(minGold)for e?
in popped:RECOMPUTESCORE(e?
)ADD(a, e?
)for e?
in c:RECOMPUTESCORE(e?
)continuefor e?
in UNARY(e, grammar):APPEND(new, e?
)for e?
in c:if CANCOMBINE(e, e?):e?
?
BINARY(e, e?, grammar)APPEND(new, e?
)if CANCOMBINE(e?, e):e?
?
BINARY(e?, e, grammar)APPEND(new, e?
)for e?
in new:ADD(a, e?
)ADD(c, e)new?
[]Figure 2: The learning algorithm.ing examples N times, and the final parameter vec-tor is used as the model.
In our experiments, N ischosen according to results on development data.6 ExperimentsWe use CCGBank (Hockenmaier and Steedman,2007) for experimental data.
CCGbank is the CCGversion of the Penn Treebank.
Sections 02?21 are1151used for training, section 00 is used for developmentand section 23 for the final test.Original sentences from CCGBank are trans-formed into bags of words, with sequence informa-tion removed, and passed to our system as inputdata.
The system outputs are compared to the orig-inal sentences for evaluation.
Following Wan et al(2009), we use the BLEU metric (Papineni et al,2002) for string comparison.
Whilst BLEU is notan ideal measure of fluency or grammaticality, be-ing based on n-gram precision, it is currently widelyused for automatic evaluation and allows us to com-pare directly with existing work (Wan et al, 2009).In addition to the surface string, our system alsoproduces the CCG parse given an input bag of words.The quality of the parse tree can reflect both thegrammaticality of the surface string and the qualityof the trained grammar model.
However, there isno direct way to automatically evaluate parse treessince output word choice and order can be differ-ent from the gold-standard.
Instead, we indirectlymeasure parse quality by calculating the precision ofCCG lexical categories.
Since CCG lexical categoriescontain so much syntactic information, they providea useful measure of parse quality.
Again because theword order can be different, we turn both the outputand the gold-standard into a bag of word/categorypairs, and calculate the percentage of matched pairsas the lexical category precision.For fair comparison with Wan et al (2009), wekeep base NPs as atomic units when preparing theinput.
Wan et al (2009) used base NPs from PennTreebank annotation, while we extract base NPsfrom the CCGBbank by taking as base NPs the NPsthat do not recursively contain other NPs.
Thesebase NPs mostly correspond to the base NPs fromthe Penn Treebank.
In the training data, there are242,813 Penn Treebank base NPs with an averagesize of 1.09, and 216,670 CCGBank base NPs withan average size of 1.19.6.1 Development TestsTable 2 shows a set of development experiment re-sults after one training iteration.
Three differentmethods of assigning lexical categories are used.The first (?dictionary?)
is to assign all possible lex-ical categories to each input word from the dictio-nary.
The lexical category dictionary is built usingLength TimeoutMethod Timeout BLEU ratio ratio0.5s 34.98 84.02 62.261s 35.40 85.66 57.87dictionary 5s 36.27 89.05 45.7910s 36.45 89.13 42,1350s 37.07 92.52 32.410.5s 36.54 84.26 66.071s 37.50 86.69 58.22?
= 0.0001 5s 38.75 90.15 43.2310s 39.14 91.35 38.3650s 39.58 93.09 30.530.5s 40.87 85.66 61.271s 42.04 87.99 53.11?
= 0.075 5s 43.99 91.20 40.3010s 44.23 92.14 35.7050s 45.08 93.70 29.43Table 2: Development tests using various levels of lexicalcategories and timeouts, after one training iteration.the training sections of CCGBank.
For each wordoccurring more than 20 times in the corpus, the dic-tionary has an entry with all lexical categories theword has been seen with.
For the rest of the words,the dictionary maintains an entry for each POS whichcontains all lexical categories it has been seen with.There are on average 26.8 different categories foreach input word by this method.In practice, it is often unnecessary to leave lexi-cal category disambiguation completely to the gram-maticality improvement system.
When it is reason-able to assume that the input sentence for the gram-maticality improvement system is sufficiently fluent,a list of candidate lexical categories can be assignedautomatically to each word via supertagging (Clarkand Curran, 2007) on the input sequence.
We usethe C&C supertagger1 to assign a set of probablelexical categories to each input word using the gold-standard order.
When the input is noisy, the accuracyof a supertagger tends to be lower than when the in-put is grammatical.
One way to address this problemis to allow the supertagger to produce a larger listof possible supertags for each input word, and leavethe ambiguity to the grammatical improvement sys-tem.
We simulate the noisy input situation by using1http://svn.ask.it.usyd.edu.au/trac/candc/wiki/Download.1152Precisiondictionary 58.5%?
= 0.0001 59.7%?
= 0.075% 77.0%Table 3: Lexical category accuracies.
Timeout = 5s.
1training iteration.a small probability cutoff (?)
value in the supertag-ger, and supertag correctly ordered input sentencesbefore breaking them into bags of words.
With a ?value of 0.0001, there are 5.4 lexical categories foreach input word in the development test (which issmaller than the dictionary case).The average number of lexical categories perword drops to 1.3 when ?
equals 0.075, which is thevalue used for parsing newspaper text in Clark andCurran (2007).
We include this ?
in our experimentsto compare the effect of different ?
levels.The table shows that the BLEU score of the gram-maticality improvement system is higher when a su-per tagger is used, and the higher the ?
value, thebetter the BLEU score.
In practice, the ?
valueshould be set in accordance with the lack of gram-maticality and fluency in the input.
The dictionarymethod can be used when the output is extremelyunreliable, while a small beta value can be used ifthe output is almost fluent.Due to the default output mechanism on timeout,the system can sometimes fail to produce sentencesthat cover all input words.
We choose five differenttimeout settings between 0.5s to 50s, and comparethe speed/quality tradeoff.
In addition to BLEU, wereport the percentage of timeouts and the ratio of thesum of all output sentence lengths to the sum of allinput sentence lengths.When the timeout value increases, the BLEUscore generally increases.
The main effect of a largertimeout is the increased possibility of a completesentence being found.
As the time increases from0.5s to 50s using the dictionary method, for exam-ple, the average output sentence length increasesfrom 84% of the input length to 93%.Table 3 shows the lexical category accuracies us-ing the dictionary, and supertagger with different ?levels.
The timeout limit is set to 5 seconds.
Asthe lexical category ambiguity decreases, the accu-Length dictionary ?
= 0.0001 ?
= 0.075?
5 75.65 89.42 92.64?
10 57.74 66.00 78.54?
20 42.44 48.89 58.23?
40 37.48 40.32 46.00?
80 36.50 39.01 44.26all 36.27 38.75 43.99Table 4: BLEU scores measured on different lengths ondevelopment data.
Timeout = 5s.
1 training iteration.racy increases.
The best lexical category accuracyof 77% is achieved when using a supertagger witha ?
level 0.075, the level for which the least lexicalcategory disambiguation is required.
However, com-pared to the 93% lexical category accuracy of a CCGparser (Clark and Curran, 2007), which also uses a ?level of 0.075 for the majority of sentences, the ac-curacy of our grammaticality improvement systemis much lower.
The lower score reflects the lowerquality of the parse trees produced by our system.Besides the difference in the algorithms themselves,one important reason is the much higher complexityof our search problem.Table 4 shows the BLEU scores measured by dif-ferent sizes of input.
We also give some exampleoutput sentences in Figure 3.
It can be seen fromthe table that the BLEU scores are higher when thesize of input is smaller.
For sentences shorter than20 words, our system generally produces reason-ably fluent and grammatical outputs.
For longer sen-tences, the grammaticality drops.
There are threepossible reasons.
First, larger constituents requiremore steps to construct.
The model and search algo-rithm face many more ambiguities, and error propa-gation is more severe.
Second, the search algorithmoften fails to find a goal hypothesis before timeout,and a default output that is less grammatical thana complete constituent is constructed.
Long sen-tences have comparatively more input words uncov-ered in the output.
Third, the upper bound is not 100,and presumably lower for longer sentences, becausethere are many ways to generate a grammatical sen-tence given a bag of words.
For example, the bag{ cats, chase, dogs } can produce two equally fluentand grammatical sentences.The relatively low score for long sentences is un-1153(dictionary) our products There is no asbestos in now .(?
= 0.0001) in our products now There is no asbestos .(?
= 0.075) There is no asbestos in our products now .
(dictionary) No price for the new shares has been set .
(both ?)
No price has been set for the new shares .
(all) Federal Data Corp. got a $ 29.4 million Air Force contract forintelligence data handling .
(dictionary) was a nonexecutive director of Rudolph Agnew and former chairmanof Consolidated Gold Fields PLC , this British industrialconglomerate , 55 years old .
named(?
= 0.0001) old Consolidated Gold Fields PLC , was named 55 years , formerchairman of Rudolph Agnew and a nonexecutive director of thisBritish industrial conglomerate .(?
= 0.075) Consolidated Gold Fields PLC , 55 years old , was named formerchairman of Rudolph Agnew and a nonexecutive director of thisBritish industrial conglomerate .
(dictionary) McDermott International Inc. said its Babcock & Wilcox unitcompleted the sale of its Bailey Controls Operations forFinmeccanica S.p .
A. to $ 295 million .(?
= 0.0001) $ 295 million McDermott International Inc. for the sale ofits Babcock & Wilcox unit said its Bailey Controls Operationscompleted to Finmeccanica S.p .
A.
.(?
= 0.075) McDermott International Inc. said its Bailey ControlsOperations completed the sale of Finmeccanica S.p .
A. for itsBabcock & Wilcox unit to $ 295 million .Figure 3: Example outputs on development data.likely to be such a problem in practice, becausethe base system (e.g.
an SMT system) is likely toproduce sentences with locally fluent subsequences.When fluent local phrases in the input are treated asatomic units, the effective sentence length is shorter.All the above development experiments were per-formed using only one training iteration.
Figure 4shows the effect of different numbers of training it-erations.
For the final test, based on the graphs inFigure 4, we chose the training iterations to be 8, 6and 4 for the dictionary, ?
= 0.0001 and ?
= 0.075methods, respectively.6.2 Final AccuraciesTable 5 shows the final results of our system, to-gether with the MST-based (?Wan 2009 CLE?
)and assignment-based (?Wan 2009 AB?)
systemsof Wan et al (2009).
Our system outperforms theBLEUWan 2009 CLE 26.8Wan 2009 AB 33.7This paper dictionary 40.1This paper ?
= 0.0001 43.2This paper ?
= 0.075 50.1Table 5: Final accuracies.dependency grammar-based systems, and using asupertagger with small ?
value produces the bestBLEU.
Note that through the use of a supertagger,we are no longer assuming that the input is a bag ofwords without any order, and therefore only the dic-tionary results are directly comparable with Wan etal.
(2009)2.2We also follow Wan et al (2009) by assuming each word isassociated with its POS tag.11540.340.360.380.40.420.440.460.481  2  3  4  5  6  7  8BLEUtraining iteration0.0750.0001dictionaryFigure 4: The effect of training iterations.7 Related WorkBoth Wan et al (2009) and our system use approx-imate search to solve the problem of input word or-dering.
There are three differences.
First, Wan etal.
use a dependency grammar to model grammati-cality, while we use CCG.
Compared to dependencytrees, CCG has stronger category constraints on theparse structure.
Moreover, CCG allows us to reducethe ambiguity level of the search algorithm throughthe assignment of possible lexical categories to inputwords, which is useful when the input has a basicdegree of fluency, as is often the case in a grammat-icality improvement task.Second, we use learning to optimise search in or-der to explore a large search space.
In contrast, Wanet al break the search problem into a sequence ofsub tasks and use greedy search to connect them.Finally, in addition to ordering, our algorithm fur-ther allows word selection.
This gives our systemthe flexibility to support word insertion and deletion.White (2004) describes a system that performsCCG realization using best-first search.
The searchprocess of our algorithm is similar to his work.The problem we solve is different from realization,which takes an input in logical form and producesa corresponding sentence.
Without constraints, theword order ambiguities can be much larger with abag of words, and we use learning to guide oursearch algorithm.
Espinosa et al (2008) apply hy-pertagging to logical forms to assign lexical cate-gories for realization.
White and Rajkumar (2009)further use perceptron reranking on N-best outputsto improve the quality.The use of perceptron learning to improve searchhas been proposed in guided learning for easy-firstsearch (Shen et al, 2007) and LaSO (Daume?
III andMarcu, 2005).
LaSO is a general framework forvarious search strategies.
Our learning algorithm issimilar to LaSO with best-first inference, but the pa-rameter updates are different.
In particular, LaSOupdates parameters when all correct hypotheses arelost, but our algorithm makes an update as soon asthe top item from the agenda is incorrect.
Our algo-rithm updates the parameters using a stronger pre-condition, because of the large search space.
Givenan incorrect hypothesis, LaSO finds the correspond-ing gold hypothesis for perceptron update by con-structing its correct sibling.
In contrast, our algo-rithm takes the lowest scored gold hypothesis cur-rently in the agenda to avoid updating parametersfor hypotheses that may have not been constructed.Our parameter update strategy is closer to theguided learning mechanism for the easy-first algo-rithm of Shen et al (2007), which maintains a queueof hypotheses during search, and performs learningto ensure that the highest scored hypothesis in thequeue is correct.
However, in easy-first search, hy-potheses from the queue are ranked by the score oftheir next action, rather than the hypothesis score.Moreover, Shen et al use aggressive learning andregenerate the queue after each update, but we per-form non-agressive learning, which is faster and ismore feasible for our complex search space.
Similarmethods to Shen et al (2007) have also been usedin Shen and Joshi (2008) and Goldberg and Elhadad(2010).8 ConclusionWe proposed a grammaticality improvement systemusing CCG, and evaluated it using a standard inputword ordering task.
Our system gave higher BLEUscores than the dependency-based system of Wan etal.
(2009).
We showed that the complex search prob-lem can be solved effectively using guided learningfor best-first search.Potential improvements to our system can bemade in several areas.
First, a large scale lan-guage model can be incorporated into our model inthe search algorithm, or through reranking.
Sec-ond, a heuristic future cost (e.g.
Varges and Mel-1155lish (2010)) can be considered for each hypothesisso that it also considers the words that have not beenused, leading to better search.
Future work also in-cludes integration with an SMT system, where con-tent word selection will be applicable.AcknowledgementsWe thank Graeme Blackwood, Bill Byrne, Adria` deGispert, Stephen Wan and the anonymous reviewersfor their discussions and suggestions.
Yue Zhangand Stephen Clark are supported by the EuropeanUnion Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement no.
247762.ReferencesSrinivas Bangalore, Owen Rambow, and Steve Whittaker.2000.
Evaluation metrics for generation.
In Proceed-ings of the First International Natural Language Gen-eration Conference (INLG2000), Mitzpe, pages 1?8.Graeme Blackwood, Adria` de Gispert, and WilliamByrne.
2010.
Fluency constraints for minimum bayes-risk decoding of statistical machine translation lattices.In Proceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010), pages71?79, Beijing, China, August.
Coling 2010 Organiz-ing Committee.Sharon A. Caraballo and Eugene Charniak.
1998.
Newfigures of merit for best-first probabilistic chart pars-ing.
Comput.
Linguist., 24:275?298, June.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, USA.Hal Daume?
III and Daniel Marcu.
2005.
Learning assearch optimization: approximate large margin meth-ods for structured prediction.
In ICML, pages 169?176.Dominic Espinosa, Michael White, and Dennis Mehay.2008.
Hypertagging: Supertagging for surface real-ization with CCG.
In Proceedings of ACL-08: HLT,pages 183?191, Columbus, Ohio, June.
Associationfor Computational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 742?750, Los Angeles, California, June.Association for Computational Linguistics.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Parsing with generative mod-els of predicate-argument structure.
In Proceedings ofthe 41st Meeting of the ACL, pages 359?366, Sapporo,Japan.Kevin Knight.
2007.
Automatic language translationgeneration help needs badly.
In MT Summit XI Work-shop on Using Corpora for NLG: Keynote Address.Phillip Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. III Maxwell, and Mark John-son.
2002.
Parsing the Wall Street Journal usinga lexical-functional grammar and discriminative esti-mation techniques.
In Proceedings of 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 271?278, Philadelphia, Pennsylvania,USA, July.
Association for Computational Linguistics.F.
Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psychological Review, 65:386?408.Libin Shen and Aravind Joshi.
2008.
LTAG dependencyparsing with bidirectional incremental construction.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 495?504, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classifica-tion.
In Proceedings of ACL, pages 760?767, Prague,Czech Republic, June.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, Mass.Sebastian Varges and Chris Mellish.
2010.
Instance-based natural language generation.
Natural LanguageEngineering, 16(3):309?346.Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.2009.
Improving grammaticality in statistical sen-tence generation: Introducing a dependency spanningtree algorithm with an argument satisfaction model.1156In Proceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 852?860, Athens, Greece, March.
Association for Compu-tational Linguistics.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore,August.
Association for Computational Linguistics.Michael White.
2004.
Reining in CCG chart realization.In Proc.
INLG-04, pages 182?191.1157
