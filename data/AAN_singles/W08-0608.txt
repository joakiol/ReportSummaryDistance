BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 54?62,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCascaded Classifiers for Confidence-Based Chemical Named EntityRecognitionPeter CorbettUnilever Centre ForMolecular Science InformaticsChemical Laboratory University Of CambridgeCB2 1EW, UKptc24@cam.ac.ukAnn CopestakeComputer LaboratoryUniversity Of CambridgeCB3 0FD, UKaac10@cl.cam.ac.ukAbstractChemical named entities represent an impor-tant facet of biomedical text.
We have de-veloped a system to use character-based n-grams, Maximum Entropy Markov Modelsand rescoring to recognise chemical namesand other such entities, and to make confi-dence estimates for the extracted entities.
Anadjustable threshold allows the system to betuned to high precision or high recall.
At athreshold set for balanced precision and recall,we were able to extract named entities at anF score of 80.7% from chemistry papers and83.2% from PubMed abstracts.
Furthermore,we were able to achieve 57.6% and 60.3% re-call at 95% precision, and 58.9% and 49.1%precision at 90% recall.
These results showthat chemical named entities can be extractedwith good performance, and that the proper-ties of the extraction can be tuned to suit thedemands of the task.1 IntroductionSystems for the recognition of biomedical namedentities have traditionally worked on a ?first-best?approach, where all of the entities recognised haveequal status, and precision and recall are givenroughly equal importance.
This does not reflect thatfact that precision is of greater importance for someapplications, and recall is the key for others.
Fur-thermore, knowing the confidence1 with which the1In this paper, we use ?confidence?
to refer to a system?sestimate of the probability that a potential named entity is a cor-rect named entity.system has assigned the named entities is likely tobe useful in a range of different applications.Named entities of relevance to biomedical sci-ence include not only genes and proteins but alsoother chemical substances which can be of inter-est as drugs, metabolites, nutrients, enzyme cofac-tors, experimental reagents and in many other roles.We have recently investigated the issue of chemicalnamed entities (Corbett et al, 2007), by compiling aset of manual annotation guidelines, demonstrating93% interannotator agreement and manually anno-tating a set of 42 chemistry papers.
In this paper wedemonstrate a named entity recogniser that assignsa confidence score to each named entity, allowing itto be tuned for high precision or recall.Our review of the methods of chemical namedentity recognition showed a consistent theme: theuse of character-based n-Grams to identify chemi-cal names via their constituent substrings (Wilbur etal., 1999; Vasserman, 2004; Townsend et al, 2005).This can be a powerful technique, due to systematicand semisystematic chemical names and additionalconventions in drug names.
However this techniquedoes not cover all aspects of chemical nomenclature.Much current named entity work uses approacheswhich combine the structured prediction abilitiesof HMMs and their derivatives with techniqueswhich enable the use of large, diverse feature setssuch as maximum entropy (also known as logis-tic regression).
Maximum Entropy Markov Mod-els, (MEMMs) (McCallum et al, 2000) provide arelatively simple framework for this.
MEMMs dohave a theoretical weakness, namely the ?label bias?problem (Lafferty et al, 2001), which has been ad-54dressed with the development of Conditional Ran-dom Fields (CRFs).
CRFs are now a mainstay ofthe field, being used in a high proportion of entriesin the latest BioCreative evaluation (Krallinger andHirschman, 2007).
However, despite the label biasproblem, MEMMs still attract interest due to practi-cal advantages such as shorter training cycles.The framework of HMMs and their successors of-fers three modes of operation; first-best, n-best andconfidence-based.
In first-best NER, the Viterbi al-gorithm is used to identify a single sequence of la-bels for the target sentence.
In n-best operation,the n best sequences for the sentence are identi-fied, along with their probabilities, for example bycoupling the Viterbi algorithm with A* search.
Inconfidence-based operation, potential entities (witha probability above a threshold) are identified di-rectly, without directly seeking a single optimal la-belling for the entire sentence.
This is done byexamining the probability of the label transitionswithin the entity, and the forward and backwardprobabilities at the start and end of the entity.
Thismode has been termed the Constrained Forward-Backward algorithm (Culotta andMcCallum, 2004).Where a single unambiguous non-overlapping la-belling is required, it can be obtained by identify-ing cases where the entities overlap, and discardingthose with lower probabilities.Confidence-based extraction has two main advan-tages.
First, it enables the balance between precisionand recall to be controlled by varying the probabilitythreshold.
Second, confidence-based NER avoidsover-commitment in systems where it is used as apreprocessor, since multiple overlapping options canbe used as input to later components.The optimum balance between recall and preci-sion depends on the application of the NER and onthe other components in the system.
High precisionis useful in search even when recall is low whenthere is a large degree of redundancy in the informa-tion in the original documents.
High precision NERmay also be useful in contexts such as the extractionof seed terms for clustering algorithms.
Balancedprecision/recall is often appropriate for search, al-though in principle it is desirable to be able to shiftthe balance if there are too many/too few results.Balanced precision/recall is also generally assumedfor use in strictly pipelined systems, when a singleset of consistent NER results is to be passed on tosubsequent processing.
Contexts where high recallis appropriate include those where a search is beingcarried out where there is little redundancy (cf Car-penter 2007) or where the NER system is being usedwith other components which can filter the results.One use of our NER system is within a languageprocessing architecture (Copestake et al, 2006) thatsystematically allows for ambiguity by treating theinput/output of each component as a lattice (repre-sented in terms of standoff annotation on an orig-inal XML document).
This system exploits rela-tively deep parsing, which is not fully robust to NERerrors but which can exploit complex syntactic in-formation to select between candidate NER results.NER preprocessing is especially important in thecontext of chemistry terms which utilise punctuationcharacters (e.g., ?2,4-dinitrotoluene?, ?2,4- and 2,6-dinitrotoluene?)
since failure to identify these willlead to tokenisation errors in the parser.
Such errorsfrequently cause complete parse failure, or highlyinaccurate analyses.
In our approach, the NER re-sults contribute edges to a lattice which can (option-ally) be treated as tokens by the parser.
The NERresults may compete with analyses provided by themain parser lexicon.
In this context, some NER er-rors are unimportant: e.g., the parser is not sensitiveto all the distinctions between types of named entity.In other cases, the parser will filter the NER results.Hence it makes sense to emphasise recall over pre-cision.
We also hypothesise that we will be able toincorporate the NER confidence scores as featuresin the parse ranking model.Another example of the use of high-recall NER inan integrated system is shown in the editing work-flows used by the Royal Society of Chemistry intheir Project Prospect system (Batchelor and Cor-bett, 2007), where chemical named entity recogni-tion is used to produce semantically-enriched jour-nal articles.
In this situation, high recall is desirable,as false positives can be removed in two ways; byremoving entities where a chemical structure cannotbe assigned, and by having them checked by a tech-nical editor.
False negatives are harder to correct.The use of confidence-based recognition has beendemonstrated with CRFs in the domain of contactdetails (Culotta and McCallum, 2004), and usingHMMs in the domain of gene annotation (Carpen-55ter, 2007).
In the latter case, the LingPipe toolkitwas used in the BioCreative 2 evaluation withoutsignificant adaptation.
Although only 54% preci-sion was achieved at 60% recall (the best systemswere achieving precision and recall scores in thehigh eighties), the system was capable of 99.99%recall with 7% precision, and 95% recall with 18%precision, indicating that very high recall could beobtained in this difficult domain.Another potential use of confidence-based NERis the potential to rescore named entities.
In thisapproach, the NER system is run, generating a setof named entities.
Information obtained about theseentities throughout the document (or corpus) thatthey occur in can then be used in further classi-fiers.
We are not aware of examples of rescoringbeing applied to confidence-based NER, but thereare precedents using other modes of operations.
Forexample, Krishnan and Manning (2006) describe asystem where a first-best CRF is used to analyse acorpus, the results of which are then used to gener-ate additional features to use in a second first-bestCRF.
Similarly, Yoshida and Tsujii (2007) use an n-best MEMM to generate multiple analyses for a sen-tence, and re-rank the analyses based on informationextracted from neighbouring sentences.Therefore, to explore the potential of these tech-niques, we have produced a chemical NER systemthat uses a MEMM for confidence-based extractionof named entities, with an emphasis on the use ofcharacter-level n-Grams, and a rescoring system.2 CorpusPreviously, we have produced a set of annotationguidelines for chemical named entities, and usedthem to annotate a set of 42 chemistry papers (Cor-bett et al, 2007).
Inter-annotator agreement wastested on 14 of these, and found to be 93%.
The an-notation guidelines specified five classes of namedentity, which are detailed in Table 1.
The annotationwas performed on untokenised text.To test the applicability of the method to adifferent corpus, we retrieved 500 PubMed ab-stracts and titles, and annotated them using thesame methods.
The abstracts were acquired us-ing the query metabolism[Mesh] AND drugAND hasabstract.
This produced a diverse setof abstracts spanning a wide range of subject ar-eas, but which contain a higher proportion of rele-vant terms than PubMed overall.
445 out of 500 ab-stracts contained at least one named entity, whereas249 contained at least ten.
Notably, the ASE classwas more common in the PubMed corpus than inthe chemistry papers, reflecting the important of en-zymes to biological and medical topics.In this study, we have left out the named entitytype CPR, as it is rare (<1%) and causes difficultieswith tokenisation.
This entity type covers cases suchas the ?1,3-?
in ?1,3-disubstituted?, and as such re-quires the ?1,3-?
to be a separate token or token se-quence.
However, we have found that recognitionof the other four classes is improved if words suchas ?1,3-disubstituted?
are kept together as single to-kens.
Therefore it makes sense to treat the recogni-tion of CPR as an essentially separate problem - aproblem that will not be addressed here.Type Description Example nCh nPMCM compound citric acid 6865 4494RN reaction methylation 288 401CJ adjective pyrazolic 60 87ASE enzyme demethylase 31 181CPR prefix 1,3- 53 21Table 1: Named Entity types.
nCh = number in Chem-istry corpus, nPM = number in PubMed corpus.3 MethodsOur system is quite complex, and as such we havemade the source code available (see below).
The fol-lowing gives an outline of the system:3.1 External ResourcesChemical names were extracted from the chem-ical ontology ChEBI (Degtyarenko et al, 2008),and a standard English word list was taken from/usr/share/dict/words on a Linux system2.A list of chemical element names and symbols wasalso compiled.
To overcome the shortage of enti-ties of type ASE, a list of words from enzyme names2This dictionary was chosen as it contains inflectional formsof English words.
Our system does not perform stemming,partly because suffixes are often good cues as to whether a wordis chemical or not.56ending in ?-ase?
was extracted from the Gene Ontol-ogy (GO), and hand sorted into words of type ASE,and words not of type ASE.3.2 Overview of operationThe text is tokenised before processing; this isdone using the tokeniser described in our previouswork (Corbett et al, 2007), which is adapted tochemical text.Our system uses three groups of classifiers torecognise chemical names.
The first classifier?the?preclassifier?
?uses character-level n-grams to esti-mate the probabilities of whether tokens are chemi-cal or not.
The output of this classification is com-bined with information from the suffix of the word,and is used to provide features for the MEMM.The second group of classifiers constitute theMEMM proper.
Named entities are represented us-ing an BIO-encoding, and methods analogous toother confidence-based taggers (Culotta and McCal-lum, 2004; Carpenter, 2007) are used to estimatethe conditional probability of tag sequences corre-sponding to named entities.
The result of this isa list of potential named entities, with start posi-tions, end positions, types and probabilities, whereall of the probabilities are above a threshold value.A small set of hand-written filtering rules is used toremove obvious absurdities, such as named entitiesending in the word ?the?, and simple violations ofthe annotation guidelines, such as named entities oftype ASE that contain whitespace.
These filteringrules make very little difference at recall values upto about 80%?however, we have found that they areuseful for improving precision at very high recall.The third group of classifiers?one per entitytype?implement a rescoring system.
After all ofthe potential entities from a document have beengenerated, a set of features is generated for each en-tity.
These features are derived from the probabili-ties of other entities that share the same text stringas the entity, from probabilities of potential syn-onyms found via acronym matching and other pro-cesses, and most importantly, from the pre-rescoringprobability of the entities themselves.
In essence,the rescoring process performs Bayesian reasoningby adjusting the raw probabilities from the previ-ous stage up or down based on nonlocal informationwithin the document.3.3 Overview of trainingA form of training conceptually similar to cross-validation is used to train the three layers of clas-sifiers.
To train the overall system, the set of docu-ments used for training is split into three.
Two thirdsare used to train a MEMM, which is then used togenerate training data for the rescorer using the held-out last third.
This process is repeated another twotimes, holding out a different third of the trainingdata each time.
Finally, the rescorer is trained usingall of the training data generated by this procedure,and the final version of the MEMM is generated us-ing all of the training data.
This procedure ensuresthat both the MEMM and the rescorer are able tomake use of all of the training data, and also thatthe rescorer is trained to work with the output of aMEMM that has not been trained on the documentsthat it is to rescore.A similar procedure is used when training theMEMM itself.
The available set of documents to useas training data is divided into half.
One half is usedto train the preclassifier and build its associated dic-tionaries, which are then used to generate featuresfor the MEMM on the other half of the data.
Theroles of each half are then reversed, and the sameprocess is applied.
Finally, the MEMM is trainedusing all of the generated features, and a new pre-classifier is trained using all of the available trainingdata.It should be noted that the dictionaries extractedduring the training of the preclassifier are also useddirectly in the MEMM.3.4 The character n-gram based preclassifierDuring the training of the preclassifier, sets of to-kens are extracted from the hand-annotated train-ing data.
A heuristic is used to classify theseinto ?word tokens?
?those that match the regex.*[a-z][a-z].
*, and ?nonword tokens?
?thosethat do not (this class includes many acronyms andchemical formulae).
The n-gram analysis is onlyperformed upon ?word tokens?.The token sets that are compiled are chemi-cal word tokens (those that only appear insidenamed entities), nonchemical word tokens (thosethat do not appear in entities), chemical nonwordtokens, nonchemical nonword tokens and ambigu-57ous tokens?those that occur both inside and out-side of named entities.
A few other minor sets arecollected to deal with tokens related to such propernoun-containing entities as ?Diels?Alder reaction?.Some of this data is combined with external dic-tionaries to train the preclassifier, which works us-ing 4-grams of characters and modified Kneser-Neysmoothing, as described by Townsend et al (2005).The set of ?chemical word tokens?
is used as a set ofpositive examples, along with tokens extracted fromChEBI, a list of element names and symbols, andthe ASE tokens extracted from the GO.
The negativeexamples used are the extracted ?nonchemical wordtokens?, the non-ASE tokens from the GO and to-kens taken from the English dictionary?except forthose that were listed as positive examples.
This getsaround the problem that the English dictionary con-tains the names of all of the elements and a numberof simple compounds such as ?ethanol?.During operation, n-gram analysis is used to cal-culate a score for each word token, of the form:ln(P (token|chem)) ?
ln(P (token|nonchem))If this score is above zero, the preclassifier clas-sifies the token as chemical and gives it a tentativetype, based on its suffix.
This can be considered tobe a ?first draft?
of its named entity type.
For exam-ple tokens ending in ?-ation?
are given the type RN,whereas those ending in ?-ene?
are given type CM.3.5 The MEMMThe MEMM is a first-order MEMM, in that it has aseparate maximum-entropy model for each possiblepreceeding tag.
No information about the tag se-quence was included directly in the feature set.
Weuse the OpenNLP MaxEnt classifier3 for maximum-entropy classification.The feature set for the MEMM is divided intothree types of features; type 1 (which apply to thetoken itself), type 2 (which can apply to the token it-self, the previous token and the next token) and type3 (which can act as type 2 features, and which canalso form bigrams with other type 3 features).An example type 1 feature would be 4G=ceti,indicating that the 4-gram ceti had been foundin the token.
An example type 2 feature would be3http://maxent.sourceforge.net/c-1:w=in, indicating that the previous token was?in?.
An example bigram constructed from type 3features would be bg:0:1:ct=CJ w=acid, in-dicating that the preclassifier had classified the tokenas being of type CJ, and having a score above zero,and that the next token was ?acid?.Type 1 features include 1, 2, 3 and 4-grams ofcharacters found within the token, whether the to-ken appeared in any of the word lists, and features torepresent the probability and type given by the pre-classifier for that token.
Type 2 features include thetoken itself with any terminal letter ?s?
removed, thetoken converted to lowercase (if it matched the regex.*[a-z][a-z].
*), and a three-character suffixtaken from the token.
The token itself was usuallyused as a type 2 feature, unless it unless it was short(less than four characters), or had been found to bean ambiguous token during preclassifier training, inwhich case it was type 3.
Other type 3 features in-clude a word shape feature, and tentative type of thetoken if the preclassifier had classed it as chemical.A few other features were used to cover a few spe-cial cases, and were found to yield a slight improve-ment during development.After generating the features, a feature selectionbased on log-likelihood ratios is used to remove theleast informative features, with a threshold set to re-move about half of them.
This was found duringdevelopment to have only a very small beneficial ef-fect on the performance of the classifier, but it didmake training faster and produced smaller models.This largely removed rare features which were onlyfound on a few non-chemical tokens.3.6 The rescorerThe rescoring system works by constructing fourmaximum entropy classifiers, one for each entitytype.
The output of these classifiers is a probabil-ity of whether or not a potential named entity reallyis a correct named entity.
The generation of featuresis done on a per-document basis.The key features in the rescorer represent theprobability of the potential entity as estimated bythe MEMM.
The raw probability p is converted tothe logit scorel = ln(p) ?
ln(1 ?
p)This mirrors the way probabilities are represented58within maximum entropy (aka logistic regression)classifiers.
If l is positive, int(min(15.0, l) ?
50)instances 4 of the feature conf+ are generated, anda corresponding technique is used if l is negative.Before generating further features, it is necessaryto find entities that are ?blocked?
?entities that over-lap with other entities of higher confidence.
For ex-ample, consider ?ethyl acetate?, which might giverise to the named entity ?ethyl acetate?
with 98%confidence, and also ?ethyl?
with 1% confidence and?acetate?
with 1% confidence.
In this case, ?ethyl?and ?acetate?
would be blocked by ?ethyl acetate?.Further features are generated by collecting to-gether all of the unblocked5 potential entities of atype that share the same string, calculating the max-imum and average probability, and calculating thedifference between the p and those quantities.Some acronym and abbreviation handling is alsoperformed.
The system looks for named entities thatare surrounded by brackets.
For each of these, a listof features is generated that is then given to everyother entity of the same string.
If there is a potentialentity to the left of the bracketed potential abbre-viation, then features are generated to represent theprobability of that potential entity, and how well thestring form of that entity matches the potential ab-breviation.
If no potential entity is found to matchwith, then features are generated to represent howwell the potential abbreviation matches the tokensto the left of it.
By this method, the rescorer cangather information about whether a potential abbre-viation stands for a named entity, something otherthan a named entity?or whether it is not an abbre-viation at all, and use that information to help scoreall occurrences of that abbreviation in the document.4 EvaluationThe systems were evaluated by 3-fold cross-validation methodology, whereby the data was splitinto three equal folds (in the case of the chemistry4We found that 15.0 was a good threshold by experimenta-tion on development data: papers annotated during trial runs ofthe annotation process.5Doing this without regards for blocking causes problems.In a document containing both ?ethyl acetate?
and ?ethylgroup?, it would be detrimental to allow the low confidencefor the ?ethyl?
in ?ethyl acetate?
to lower the confidence of the?ethyl?
in ?ethyl group?.papers, each fold consists of one paper per journal.For the PubMed abstracts, each fold consists of onethird of the total abstracts).
For each fold, the systemwas trained on the other two folds and then evaluatedon that fold, and the results were pooled.The direct output from the system is a list ofputative named entities with start positions, endpositions, types and confidence scores.
This listwas sorted in order of confidence?most confidentfirst?and each entity was classified as a true posi-tive or a false positive according to whether an ex-act match (start position, end position and type allmatched perfectly) could be found in the annotatedcorpus.
Also, the number of entities in the annotatedcorpus was recorded.Precision/recall curves were plotted from theselists by selecting the first n elements, and calculat-ing precision and recall taking all of the elements inthis sublist as true or false positives, and all the enti-ties in the corpus that were not in the sublist as falsenegatives.
The value of n was gradually increased,recording the scores at each point.
The area underthe curve (treating precision as zero at recall valueshigher than the highest reported) was used to calcu-late mean average precision (MAP).
Finally, F weregenerated by selecting all of the entities with a con-fidence score of 0.3 or higher.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0RecallPrecisionFull SystemNo RescorerNo PreclassifierNo n?GramsCustomised LingPipe HMMPure LingPipe HMMFigure 1: Evaluation on chemistry papers.The results of this evaluation on the corpus of59chemistry papers is show in Figure 1.
The full sys-tem achieves 57.6% recall at 95% precision, 58.9%precision at 90% recall, and 78.7% precision and82.9% recall (F = 80.7%) at a confidence thresholdof 0.3.
Also shown are the results of successivelyeliminating parts of the system.
?No Rescorer?
re-moves the rescorer.
In ?No Preclassifier?, the pre-classifier is disabled, and all of the dictionaries ex-tracted during the training of the preclassifier arealso disabled.
Finally, in ?No n-Grams?, the 1-, 2-, 3- and 4-grams used directly by the MEMM arealso disabled, showing the results of using a sys-tem where no character-level n-grams are used at all.These modifications apply successively?for exam-ple, in the ?No n-Grams?
case the rescorer and pre-classifier are also disabled.
These results validate thethe cascade of classifiers, and underline the impor-tance of character-level n-grams in chemical NER.We also show comparisons to an HMM-basedapproach, based on LingPipe 3.4.0.6 This is es-sentially the same system as described by Corbettet al (2007), but operating in a confidence-basedmode.
The HMMs used make use of character-leveln-Grams, but do not allow the use of the rich fea-ture set used by the MEMM.
The line ?CustomisedLingPipe HMM?
shows the system using the cus-tom tokenisation and ChEBI-derived dictionary usedin the MEMM system, whereas the ?Pure LingPipeHMM?
shows the system used with the default to-keniser and no external dictionaries.
In the regionwhere precision is roughly equal to recall (mimick-ing the operation of a first-best system), the fact thatthe MEMM-based system outperforms an HMM isno surprise.
However, it is gratifying that a clearadvantage can be seen throughout the whole recallrange studied (0-97%), indicating that the trainingprocesses for the MEMM are not excessively at-tuned to the first-best decision boundary.
This in-creased accuracy comes at a price in the speed ofdevelopment, training and execution.It is notable that we were not able to achieve ex-tremes of recall at tolerable levels of precision us-ing any of the systems, whereas it was possible forLingPipe to achieve 99.99% recall at 7% precision inthe BioCreative 2006 evaluation.
There are a num-ber of potential reasons for this.
The first is that the6http://alias-i.com/lingpipe/tokeniser used in all systems apart from the ?PureLingPipe HMM?
system tries in general to makeas few token boundaries as possible; this leads tosome cases where the boundaries of the entities tobe recognised in the test paper occur in the middleof tokens, thus making those entities unrecognisablewhatever the threshold.
However this does not ap-pear to be the whole problem.
Other factors that mayhave had an influence include the more generousmethod of evaluation at BioCreative 2006, (whereseveral allowable alternatives were given for diffi-cult named entities), and the greater quantity and di-versity (sentences selected from a large number ofdifferent texts, rather than a relatively small numberof whole full papers) of training data.
Finally, theremight be some important difference between chem-ical names and gene names.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0RecallPrecisionFull SystemNo RescorerNo PreclassifierNo n?GramsCustomised LingPipe HMMPure LingPipe HMMFigure 2: Evaluation on PubMed abstracts.Figure 2 shows the results of running the sys-tem on the set of annotated PubMed abstracts.
Thefull system achieves 60.3% recall at 95% precision,49.1% precision at 90% recall, and 85.0% preci-sion and 81.6% recall (F = 83.2%) at a confidencethreshold of 0.3.
In PubMed abstracts, it is commonto define ad-hoc abbreviations for chemicals withinan abstract (e.g., the abstract might say ?dexametha-sone (DEX)?, and then use ?DEX?
and not ?dexam-ethasone?
throughout the rest of the abstract).
Therescorer provides a good place to resolve these ab-60breviations, and thus has a much larger effect thanin the case of chemistry papers where these ad hocabbreviations are less common.
It is also notablethat the maximum recall is lower in this case.
Onesystem?the ?Pure LingPipe HMM?, which uses adifferent, more aggressive tokeniser from the othersystems?has a clear advantage in terms of maxi-mum recall, showing that overcautious tokenisationlimits the recall of the other systems.In some cases the systems studied behavestrangely, having ?spikes?
of lowered precision atvery low recall, indicating that the systems can occa-sionally be overconfident, and assign very high con-fidence scores to incorrect named entities.Corpus System MAP FChemistry Full 87.1% 80.8%Chemistry No Rescorer 86.8% 81.0%Chemistry No Preclassifier 82.7% 74.8%Chemistry No n-Grams 79.2% 72.2%Chemistry Custom LingPipe 75.9% 74.6%Chemistry Pure LingPipe 66.9% 63.2%Chemistry No Overlaps 82.9% 80.8%PubMed Full 86.1% 83.2%PubMed No Rescorer 83.3% 79.1%PubMed No Preclassifier 81.4% 73.4%PubMed No n-Grams 77.6% 70.6%PubMed Custom LingPipe 78.6% 75.6%PubMed Pure LingPipe 71.9% 66.1%Table 2: F scores (at confidence threshold of 0.3) andMean Average Precision (MAP) values for Figs.
1-3.Neither corpus contains enough data for the re-sults to reach a plateau?using additional trainingdata is likely to give improvements in performance.The ?No Overlaps?
line in Figure 3 shows the ef-fect of removing ?blocked?
named entities (as de-fined in section 3.6) prior to rescoring.
This sim-ulates a situation where an unambiguous inline an-notation is required?for example a situation wherea paper is displayed with the named entities beinghighlighted.
This condition makes little differenceat low to medium recall, but it sets an effective max-imum recall of 90%.
The remaining 10% of casespresumably consist of situations where the recog-niser is finding an entity in the right part of the text,but making boundary or type errors.0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0RecallPrecisionFull SystemNo OverlapsFigure 3: Evaluation on chemistry papers, showing ef-fects of disallowing overlapping entities.5 ConclusionWe have demonstrated that MEMMs can be adaptedto recognise chemical named entities, and that thebalance between precision and recall can be tunedeffectively, at least in the range of 0 - 95% recall.The MEMM system is available as part of the OS-CAR3 chemical named entity recognition system.
7AcknowledgementsPTC thanks Peter Murray-Rust for supervision.
Wethank the Royal Society of Chemistry for provid-ing the papers, and the EPSRC (EP/C010035/1) forfunding.
We thank the reviewers for their helpfulsuggestions and regret that we did not have the timeor space to address all of the issues raised.ReferencesColin Batchelor and Peter Corbett.
2007.
Semantic en-richment of journal articles using chemical named en-tity recognition Proceedings of the ACL 2007 Demoand Poster Sessions, pp 45-48.
Prague, Czech Repub-lic.Bob Carpenter.
2007.
LingPipe for 99.99% Recall ofGene Mentions Proceedings of the Second BioCre-ative Challenge Evaluation Workshop, 307-309.7https://sourceforge.net/projects/oscar3-chem61Ann Copestake, Peter Corbett, Peter Murray-Rust, C. J.Rupp, Advaith Siddharthan, Simone Teufel and BenWaldron.
2006.
An Architecture for Language Tech-nology for Processing Scientific Texts.
Proceedings ofthe 4th UK E-Science All Hands Meeting, Nottingham,UK.Peter Corbett, Colin Batchelor and Simone Teufel.
2007.Annotation of Chemical Named Entities BioNLP2007: Biological, translational, and clinical languageprocessing, pp 57-64.
Prague, Czech Republic.Aron Culotta and Andrew McCallum 2004.
ConfidenceEstimation for Information Extraction Proceedings ofHuman Language Technology Conference and NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL), pp 109-112.
Boston,MA.Kirill Degtyarenko, Paula de Matos, Marcus Ennis, JannaHastings, Martin Zbinden, AlanMcNaught, Rafael Al-cantara, Michael Darsow, Mickael Guedj and MichaelAshburner.
2008.
ChEBI: a database and ontology forchemical entities of biological interest.
Nucleic AcidsRes, Vol.
36, Database issue D344-D350.The Gene Ontology Consortium 2000.
Gene Ontology:tool for the unification of biology.
Nature Genetics,Vol.
25, 26-29.Martin Krallinger and Lynette Hirschman, editors.
2007.Proceedings of the Second BioCreative ChallengeEvaluation Workshop.Vijay Krishnan and Christopher D. Manning.
2006.
AnEffective Two-Stage Model for Exploiting Non-LocalDependencies in Named Entity Recognition.
Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, 1121-1128.Sindey, Australia.John Lafferty, Andrew McCallum and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
Pro-ceedings of the Eighteenth International Conferenceon Machine Learning, 282-289.Andrew McCallum, Dayne Freitag and Fernando Pereira.2000.
Maximum Entropy Markov Models for Infor-mation Extraction and Segmentation Proceedings ofthe Seventeenth International Conference on MachineLearning, 591-598.
San Fransisco, CA.Joe A. Townsend, Ann Copestake, Peter Murray-Rust, Si-mone H. Teufel and Christopher A. Waudby.
2005.Language Technology for Processing Chemistry Pub-lications Proceedings of the fourth UK e-Science AllHands Meeting, 247-253.
Nottingham, UK.Alexander Vasserman 2004 Identifying ChemicalNames in Biomedial Text: An Investigation of theSubstring Co-occurence Based Approaches Proceed-ings of the Student Research Workshop at HLT-NAACLW.
John Wilbur, George F. Hazard, Jr., Guy Divita,James G. Mork, Alan R. Aronson and Allen C.Browne.
1999 Analysis of Biomedical Text for Chem-ical Names: A Comparison of Three Methods Proc.AMIA Symp.
176-180.Kazuhiro Yoshida and Jun?ichi Tsujii.
2007.
Rerankingfor Biomedical Named-Entity Recognition BioNLP2007: Biological, translational, and clinical languageprocessing, pp 57-64.
Prague, Czech Republic.62
