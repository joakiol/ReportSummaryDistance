Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 306?314,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAutomatically Evaluating Content Selection in Summarization withoutHuman ModelsAnnie LouisUniversity of Pennsylvanialannie@seas.upenn.eduAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractWe present a fully automatic method forcontent selection evaluation in summariza-tion that does not require the creation ofhuman model summaries.
Our work capi-talizes on the assumption that the distribu-tion of words in the input and an informa-tive summary of that input should be sim-ilar to each other.
Results on a large scaleevaluation from the Text Analysis Con-ference show that input-summary compar-isons are very effective for the evaluationof content selection.
Our automatic meth-ods rank participating systems similarlyto manual model-based pyramid evalua-tion and to manual human judgments ofresponsiveness.
The best feature, Jensen-Shannon divergence, leads to a correlationas high as 0.88 with manual pyramid and0.73 with responsiveness evaluations.1 IntroductionThe most commonly used evaluation method forsummarization during system development andfor reporting results in publications is the auto-matic evaluation metric ROUGE (Lin, 2004; Linand Hovy, 2003).
ROUGE compares system sum-maries against one or more model summariesby computing n-gram word overlaps between thetwo.
The wide adoption of such automatic mea-sures is understandable because they are conve-nient and greatly reduce the complexity of eval-uations.
ROUGE scores also correlate well withmanual evaluations of content based on compar-ison with a single model summary, as used inthe early editions of the Document UnderstandingConferences (Over et al, 2007).In our work, we take the idea of automaticevaluation to an extreme and explore the feasi-bility of developing a fully automatic evaluationmethod for content selection that does not makeuse of human model summaries at all.
To this end,we show that evaluating summaries by comparingthem with the input obtains good correlations withmanual evaluations for both query focused and up-date summarization tasks.Our results have important implications for fu-ture development of summarization systems andtheir evaluation.High correlations between system ranking pro-duced with the fully automatic method andmanual evaluations show that the new eval-uation measures can be used during systemdevelopment when human model summariesare not available.Our results provide validation of several featuresthat can be optimized in the development ofnew summarization systems when the objec-tive is to improve content selection on aver-age, over a collection of test inputs.
However,none of the features is consistently predictiveof good summary content for individual in-puts.We find that content selection performance onstandard test collections can be approximatedwell by the proposed fully automatic method.This result greatly underlines the need to re-quire linguistic quality evaluations alongsidecontent selection ones in future evaluationsand research.2 Model-free methods for evaluationProposals for developing fully automatic methodsfor summary evaluation have been put forwardin the past.
Their attractiveness is obvious forlarge scale evaluations, or for evaluation on non-standard test sets for which human models are notavailable.306For example in Radev et al (2003), a largescale fully automatic evaluation of eight summa-rization systems on 18,000 documents was per-formed without any human effort.
A search enginewas used to rank documents according to their rel-evance to a given query.
The summaries for eachdocument were also ranked for relevance with re-spect to the same query.
For good summariza-tion systems, the relevance ranking of summariesis expected to be similar to that of the full docu-ments.
Based on this intuition, the correlation be-tween relevance rankings of summaries and orig-inal documents was used to compare the differentsystems.
The approach was motivated by the as-sumption that the distribution of terms in a goodsummary is similar to the distribution of terms inthe original document.Even earlier, Donaway et al (2000) suggestedthat there are considerable benefits to be had inadopting model-free methods of evaluation involv-ing direct comparisons between the original docu-ment and its summary.
The motivation for theirwork was the considerable variation in content se-lection choices in model summaries (Rath et al,1961).
The identity of the model writer signifi-cantly affects summary evaluations (also noted byMcKeown et al (2001), Jing et al (1998)) andevaluations of the same systems can be rather dif-ferent when different models are used.
In theirexperiments, Donaway et al (2000) demonstratedthat the correlations between manual evaluationusing a model summary anda) manual evaluation using a different modelsummaryb) automatic evaluation by directly comparinginput and summary1,are the same.
Their conclusion was that such au-tomatic methods should be seriously considered asan alternative to model based evaluation.In this paper, we present a comprehensive studyof fully automatic summary evaluation withoutany human models.
A summary?s content isjudged for quality by directly estimating its close-ness to the input.
We compare several probabilisticand information-theoretic approaches for charac-terizing the similarity and differences between in-put and summary content.
A simple information-theoretic measure, Jensen Shannon divergence be-tween input and summary, emerges as the best fea-1They used cosine similarity to perform the input-summary comparison.ture.
System rankings produced using this mea-sure lead to correlations as high as 0.88 with hu-man judgements.3 TAC summarization track3.1 Query-focused and Update SummariesTwo types of summaries, query-focused and up-date summaries, were evaluated in the summariza-tion track of the 2008 Text Analysis Conference(TAC)2.
Query-focused summaries were producedfrom input documents in response to a stated userinformation need.
The update summaries requiremore sophistication: two sets of articles on thesame topic are provided.
The first set of articlesrepresents the background of a story and users areassumed to be already familiar with the informa-tion contained in them.
The update task is to pro-duce a multi-document summary from the secondset of articles that can serve as an update to theuser.
This task is reminiscent of the novelty de-tection task explored at TREC (Soboroff and Har-man, 2005).3.2 DataThe test set for the TAC 2008 summarization taskcontains 48 inputs.
Each input consists of two setsof 10 documents each, called docsets A and B.Both A and B are on the same general topic butB contains documents published later than thosein A.
In addition, the user?s information need as-sociated with each input is given by a query state-ment consisting of a title and narrative.
An exam-ple query statement is shown below.Title: Airbus A380Narrative: Describe developments in the pro-duction and launch of the Airbus A380.A system must produce two summaries: (1) aquery-focused summary of docset A, (2) a compi-lation of updates from docset B, assuming that theuser has read all the documents in A.
The max-imum length for both types of summaries is 100words.There were 57 participating systems in TAC2008.
We use the summaries and evaluations ofthese systems for the experiments reported in thepaper.3.3 Evaluation metricsBoth manual and automatic evaluations were con-ducted at NIST to assess the quality of summaries2http://www.nist.gov/tac307manual score R-1 recall R-2 recallQuery Focused summariespyramid score 0.859 0.905responsiveness 0.806 0.873Update summariespyramid score 0.912 0.941responsiveness 0.865 0.884Table 1: Spearman correlation between manualscores and ROUGE-1 and ROUGE-2 recall.
Allcorrelations are highly significant with p-value <0.00001.produced by the systems.Pyramid evaluation: The pyramid evaluationmethod (Nenkova and Passonneau, 2004) has beendeveloped for reliable and diagnostic assessmentof content selection quality in summarization andhas been used in several large scale evaluations(Nenkova et al, 2007).
It uses multiple humanmodels from which annotators identify seman-tically defined Summary Content Units (SCU).Each SCU is assigned a weight equal to thenumber of human model summaries that expressthat SCU.
An ideal maximally informative sum-mary would express a subset of the most highlyweighted SCUs, with multiple maximally infor-mative summaries being possible.
The pyramidscore for a system summary is equal to the ratiobetween the sum of weights of SCUs expressedin a summary (again identified manually) and thesum of weights of an ideal summary with the samenumber of SCUs.Four human summaries provided by NIST foreach input and task were used for the pyramidevaluation at TAC.Responsiveness evaluation: Responsiveness of asummary is a measure of overall quality combin-ing both content selection and linguistic quality:summaries must present useful content in a struc-tured fashion in order to better satisfy the user?sneed.
Assessors directly assigned scores on ascale of 1 (poor summary) to 5 (very good sum-mary) to each summary.
These assessments aredone without reference to any model summaries.The (Spearman) correlation between the pyramidand responsiveness metrics is high but not perfect:0.88 and 0.92 respectively for query focused andupdate summarization.ROUGE evaluation: NIST also evaluated thesummaries automatically using ROUGE (Lin,2004; Lin and Hovy, 2003).
Comparison betweena summary and the set of four model summariesis computed using unigram (R1) and bigram over-laps (R2)3.
The correlations between ROUGE andmanual evaluations is shown in Table 1 and variesbetween 0.80 and 0.94.Linguistic quality evaluation: Assessors scoredsummaries on a scale from 1 (very poor) to 5 (verygood) for five factors of linguistic quality: gram-maticality, non-redundancy, referential clarity, fo-cus, structure and coherence.We do not make use of any of the linguisticquality evaluations.
Our work focuses on fully au-tomatic evaluation of content selection, so man-ual pyramid and responsiveness scores are usedfor comparison with our automatic method.
Thepyramid metric measures content selection exclu-sively, while responsiveness incorporates at leastsome aspects of linguistic quality.4 Features for content evaluationWe describe three classes of features to compareinput and summary content: distributional simi-larity, summary likelihood and use of topic signa-tures.
Both input and summary words were stop-word filtered and stemmed before computing thefeatures.4.1 Distributional SimilarityMeasures of similarity between two probabilitydistributions are a natural choice for the task athand.
One would expect good summaries to becharacterized by low divergence between proba-bility distributions of words in the input and sum-mary, and by high similarity with the input.We experimented with three common measures:KL and Jensen Shannon divergence and cosinesimilarity.
These three metrics have already beenapplied for summary evaluation, albeit in differ-ent contexts.
In Lin et al (2006), KL and JS di-vergences between human and machine summarydistributions were used to evaluate content selec-tion.
The study found that JS divergence alwaysoutperformed KL divergence.
Moreover, the per-formance of JS divergence was better than stan-dard ROUGE scores for multi-document summa-rization when multiple human models were usedfor the comparison.The use of cosine similarity in Donaway etal.
(2000) is more directly related to our work.They show that the difference between evaluations3The scores were computed after stemming but stopwords were retained in the summaries.308based on two different human models is about thesame as the difference between system rankingbased on one model summary and the ranking pro-duced using input-summary similarity.
Inputs andsummaries were compared using only one metric:cosine similarity.Kullback Leibler (KL) divergence: The KL di-vergence between two probability distributions Pand Q is given byD(P ||Q) =?wpP(w) log2pP(w)pQ(w)(1)It is defined as the average number of bits wastedby coding samples belonging to P using anotherdistribution Q, an approximate of P .
In our case,the two distributions are those for words in theinput and summary respectively.
Since KL di-vergence is not symmetric, both input-summaryand summary-input divergences are used as fea-tures.
In addition, the divergence is undefinedwhen pP(w) > 0 but pQ(w) = 0.
We performsimple smoothing to overcome the problem.p(w) =C + ?N + ?
?
B(2)Here C is the count of word w and N is thenumber of tokens; B = 1.5|V |, where V is theinput vocabulary and ?
was set to a small valueof 0.0005 to avoid shifting too much probabilitymass to unseen events.Jensen Shannon (JS) divergence: The JS diver-gence incorporates the idea that the distance be-tween two distributions cannot be very differentfrom the average of distances from their mean dis-tribution.
It is formally defined asJ(P ||Q) =12[D(P ||A) + D(Q||A)], (3)where A = P+Q2is the mean distribution of Pand Q.
In contrast to KL divergence, the JS dis-tance is symmetric and always defined.
We useboth smoothed and unsmoothed versions of the di-vergence as features.Similarity between input and summary: Thethird metric is cosine overlap between the tf ?
idfvector representations (with max-tf normalization)of input and summary contents.cos?
=vinp.vsumm||vinp||||vsumm||(4)We compute two variants:1.
Vectors contain all words from input andsummary2.
Vectors contain only topic signatures fromthe input and all words of the summaryTopic signatures are words highly descriptive ofthe input, as determined by the application of log-likelihood test (Lin and Hovy, 2000).
Using onlytopic signatures from the input to represent text isexpected to be more accurate because the reducedvector has fewer dimensions compared with usingall the words from the input.4.2 Summary likelihoodThe likelihood of a word appearing in the sum-mary is approximated as being equal to its proba-bility in the input.
We compute both a summary?sunigram probability as well as its probability un-der a multinomial model.Unigram summary probability:(pinpw1)n1(pinpw2)n2...(pinpwr)nr (5)where pinpwiis the probability in the input ofword wi, niis the number of times wiappearsin the summary, and w1...wrare all words in thesummary vocabulary.Multinomial summary probability:N !n1!n2!...nr!(pinpw1)n1(pinpw2)n2...
(pinpwr)nr (6)where N = n1+ n2+ ...+ nris the total numberof words in the summary.4.3 Use of topic words in the summarySummarization systems that directly optimize formore topic signatures during content selectionhave fared very well in evaluations (Conroy et al,2006).
Hence the number of topic signatures fromthe input present in a summary might be a goodindicator of summary content quality.
We experi-ment with two features that quantify the presenceof topic signatures in a summary:1.
Fraction of the summary composed of input?stopic signatures.2.
Percentage of topic signatures from the inputthat also appear in the summary.While both features will obtain higher valuesfor summaries containing many topic words, thefirst is guided simply by the presence of any topicword while the second measures the diversity oftopic words used in the summary.3094.4 Feature combination using linearregressionWe also evaluated the performance of a linear re-gression metric combining all of the above fea-tures.
The value of the regression-based score foreach summary was obtained using a leave-one-out approach.
For a particular input and system-summary combination, the training set consistedonly of examples which included neither the sameinput nor the same system.
Hence during training,no examples of either the test input or system wereseen.5 Correlations with manual evaluationsIn this section, we report the correlations betweensystem ranking using our automatic features andthe manual evaluations.
We studied the predictivepower of features in two scenarios.MACRO LEVEL; PER SYSTEM: The values of fea-tures were computed for each summary submittedfor evaluation.
For each system, the feature valueswere averaged across all inputs.
All participatingsystems were ranked based on the average value.Similarly, the average manual score, pyramid orresponsiveness, was also computed for each sys-tem.
The correlations between the two rankingsare shown in Tables 2 and 4.MICRO LEVEL; PER INPUT: The systems wereranked for each input separately, and correlationsbetween the summary rankings for each inputwere computed (Table 3).The two levels of analysis address differentquestions: Can we automatically identify sys-tem performance across all test inputs (macrolevel) and can we identify which summaries for agiven input were good and which were bad (mi-cro level)?
For the first task, the answer is a defi-nite ?yes?
while for the second task the results aremixed.In addition, we compare our results to model-based evaluations using ROUGE and analyze theeffects of stemming the input and summary vo-cabularies.
In order to allow for in-depth discus-sion, we will analyze our findings only for queryfocused summaries.
Similar results were obtainedfor the evaluation of update summaries and are de-scribed in Section 7.5.1 Performance at macro levelTable 2 shows the Spearman correlation betweenmanual and automatic scores averaged across theFeatures pyramid respons.JS div -0.880 -0.736JS div smoothed -0.874 -0.737% of input topic words 0.795 0.627KL div summ-inp -0.763 -0.694cosine overlap 0.712 0.647% of summ = topic wd 0.712 0.602topic overlap 0.699 0.629KL div inp-summ -0.688 -0.585mult.
summary prob.
0.222 0.235unigram summary prob.
-0.188 -0.101regression 0.867 0.705ROUGE-1 recall 0.859 0.806ROUGE-2 recall 0.905 0.873Table 2: Spearman correlation on macro level forthe query focused task.
All results are highly sig-nificant with p-values < 0.000001 except unigramand multinomial summary probability, which arenot significant even at the 0.05 level.48 inputs.
We find that both distributional simi-larity and the topic signature features produce sys-tem rankings very similar to those produced by hu-mans.
Summary probabilities, on the other hand,turn out to be unpredictive of content selectionperformance.
The linear regression combinationof features obtains high correlations with manualscores but does not lead to better results than thesingle best feature: JS divergence.JS divergence outperforms other features in-cluding the regression metric and obtains the bestcorrelations with both types of manual scores, 0.88with pyramid score and 0.74 with responsiveness.The regression metric performs comparably withcorrelations of 0.86 and 0.70.
The correlations ob-tained by both JS divergence and the regressionmetric with pyramid evaluations are in fact betterthan that obtained by ROUGE-1 recall (0.85).The best topic signature based feature?percentage of input?s topic signatures that arepresent in the summary?ranks next only to JS di-vergence and regression.
The correlation betweenthis feature and pyramid and responsiveness eval-uations is 0.79 and 0.62 respectively.
The propor-tion of summary content composed of topic wordsperforms worse as an evaluation metric with cor-relations 0.71 and 0.60.
This result indicates thatsummaries that cover more topics from the inputare judged to have better content than those inwhich fewer topics are mentioned.Cosine overlaps and KL divergences obtaingood correlations but still lower than JS diver-gence or percentage of input topic words.
Further,rankings based on unigram and multinomial sum-310mary probabilities do not correlate significantlywith manual scores.5.2 Performance on micro levelOn a per input basis, the proposed metrics are notthat effective in distinguishing which summarieshave better content.
The minimum and maximumcorrelations with manual evaluations across the 48inputs are given in Table 3.
The number and per-centage of inputs for which correlations were sig-nificant are also reported.Now, JS divergence obtains significant correla-tions with pyramid scores for 73% of the inputsbut for particular inputs, the correlation can beas low as 0.27.
The results are worse for otherfeatures and for comparison with responsivenessscores.At the micro level, combining features with re-gression gives the best result overall, in contrast tothe findings for the macro level setting.
This re-sult has implications for system development; nosingle feature can reliably predict good content fora particular input.
Even a regression combinationof all features is a significant predictor of contentselection quality in only 77% of the cases.We should note however, that our features arebased only on the distribution of terms in the in-put and therefore less likely to inform good con-tent for all input types.
For example, a set ofdocuments each describing different opinion on agiven issue will likely have less repetition on bothlexical and content unit level.
The predictivenessof features like ours will be limited for such in-puts4.
However, model summaries written for thespecific input would give better indication of whatinformation in the input was important and inter-esting.
This indeed is the case as we shall see inSection 6.Overall, the micro level results suggest that thefully automatic measures we examined will not beuseful for providing information about summaryquality for an individual input.
For averages overmany test sets, the fully automatic evaluations givemore reliable and useful results, highly correlatedwith rankings produced by manual evaluations.4In fact, it would be surprising to find an automaticallycomputable feature or feature combination which would beable to consistently predict good content for all individual in-puts.
If such features existed, an ideal summarization systemwould already exist.5.3 Effects of stemmingThe analysis presented so far is on features com-puted after stemming the input and summarywords.
We also computed the values of the samefeatures without stemming and found that diver-gence metrics benefit greatly when stemming isdone.
The biggest improvements in correlationsare for JS and KL divergences with respect to re-sponsiveness.
For JS divergence, the correlationincreases from 0.57 to 0.73 and for KL divergence(summary-input), from 0.52 to 0.69.Before stemming, the topic signature and bagof words overlap features are the best predictorsof responsiveness (correlations are 0.63 and 0.64respectively) but do not change much after stem-ming (topic overlap?0.62, bag of words?0.64).Divergences emerge as better metrics only afterstemming.Stemming also proves beneficial for the likeli-hood features.
Before stemming, their correlationsare directed in the wrong direction, but they im-prove after stemming to being either positive orcloser to zero.
However, even after stemming,summary probabilities are not good predictors ofcontent quality.5.4 Difference in correlations: pyramid andresponsiveness scoresOverall, we find that correlations with pyramidscores are higher than correlations with respon-siveness.
Clearly our features are designed tocompare input-summary content only.
Since re-sponsiveness judgements were based on both con-tent and linguistic quality of summaries, it is notsurprising that these rankings are harder to repli-cate using our content based features.
Neverthe-less, responsiveness scores are dominated by con-tent quality and the correlation between respon-siveness and JS divergence is high, 0.73.Clearly, metrics of linguistic quality should beintegrated with content evaluations to allow forbetter predictions of responsiveness.
To date, fewattempts have been made to automatically eval-uate linguistic quality in summarization.
Lapataand Barzilay (2005) proposed a method for co-herence evaluation which holds promise but hasnot been validated so far on large datasets suchas those used in TAC and DUC.
In a simpler ap-proach, Conroy and Dang (2008) use higher orderROUGE scores to approximate both content andlinguistic quality.311pyramid responsivenessfeatures max min no.
significant (%) max min no.
significant (%)JS div -0.714 -0.271 35 (72.9) -0.654 -0.262 35 (72.9)JS div smoothed -0.712 -0.269 35 (72.9) -0.649 -0.279 33 (68.8)KL div summ-inp -0.736 -0.276 35 (72.9) -0.628 -0.261 35 (72.9)% of input topic words 0.701 0.286 31 (64.6) 0.693 0.279 29 (60.4)cosine overlap 0.622 0.276 31 (64.6) 0.618 0.265 28 (58.3)KL div inp-summ -0.628 -0.262 28 (58.3) -0.577 -0.267 22 (45.8)topic overlap 0.597 0.265 30 (62.5) 0.689 0.277 26 (54.2)% summary = topic wd 0.607 0.269 23 (47.9) 0.534 0.272 23 (47.9)mult.
summary prob.
0.434 0.268 8 (16.7) 0.459 0.272 10 (20.8)unigram summary prob.
0.292 0.261 2 ( 4.2) 0.466 0.287 2 (4.2)regression 0.736 0.281 37 (77.1) 0.642 0.262 32 (66.7)ROUGE-1 recall 0.833 0.264 47 (97.9) 0.754 0.266 46 (95.8)ROUGE-2 recall 0.875 0.316 48 (100) 0.742 0.299 44 (91.7)Table 3: Spearman correlations at micro level (query focused task).
Only the minimum, maximumvalues of the significant correlations are reported together with the number and percentage of significantcorrelations.update input only avg.
update & backgroundfeatures pyramid respons.
pyramid respons.JS div -0.827 -0.764 -0.716 -0.669JS div smoothed -0.825 -0.764 -0.713 -0.670% of input topic words 0.770 0.709 0.677 0.616KL div summ-inp -0.749 -0.709 -0.651 -0.624KL div inp-summ -0.741 -0.717 -0.644 -0.638cosine overlap 0.727 0.691 0.649 0.631% of summary = topic wd 0.721 0.707 0.647 0.636topic overlap 0.707 0.674 0.645 0.619mult.
summmary prob.
0.284 0.355 0.152 0.224unigram summary prob.
-0.093 0.038 -0.151 -0.053regression 0.789 0.605 0.699 0.522ROUGE-1 recall 0.912 0.865 .
.ROUGE-2 recall 0.941 0.884 .
.regression combining features comparing with background and update inputs (without averaging)correlations = 0.8058 with pyramid, 0.6729 with responsivenessTable 4: Spearman correlations at macro level for update summarization.
Results are reported separatelyfor features comparing update summaries with the update input only or with both update and backgroundinputs and averaging the two.6 Comparison with ROUGEFor manual pyramid scores, the best correlation,0.88, we observed in our experiments was withJS divergence.
This result is unexpectedly highfor a fully automatic evaluation metric.
Note thatthe best correlation between pyramid scores andROUGE (for R2) is 0.90, practically identical withJS divergence.
For ROUGE-1, the correlation is0.85.In the case of manual responsiveness, whichcombines aspects of linguistic quality along withcontent selection evaluation, the correlation withJS divergence is 0.73.
For ROUGE, it is 0.80for R1 and 0.87 for R2.
Using higher order n-grams is obviously beneficial as observed from thedifferences between unigram and bigram ROUGEscores.
So a natural extension of our featureswould be to use distance between bigram distri-butions.
At the same time, for responsiveness,ROUGE-1 outperforms all the fully automatic fea-tures.
This is evidence that the model summariesprovide information that is unlikely to ever be ap-proximated by information from the input alone,regardless of feature sophistication.At the micro level, ROUGE does clearly betterthan all the automatic measures.
The results areshown in the last two rows of Table 3.
ROUGE-1recall obtains significant correlations for over 95%of inputs for responsiveness and 98% of inputs forpyramid evaluation compared to 73% (JS diver-gence) and 77% (regression).
Undoubtedly, at theinput level, comparison with model summaries issubstantially more informative.When reference summaries are available,ROUGE provides scores that agree best with hu-man judgements.
However, when model sum-312maries are not available, our features can providereliable estimates of system quality when averagedover a set of test inputs.
For predictions at the levelof individual inputs, our fully automatic featuresare less useful.7 Update SummarizationIn Table 4, we report the performance of our fea-tures for system evaluation on the update task.
Thecolumn, ?update input only?
summarizes the cor-relations obtained by features comparing the sum-maries with only the update inputs (set B).
Wealso compared the summaries individually to theupdate and background (set A) inputs.
The twosets of features were then combined by a) averag-ing (?avg.
update and background?)
and b) linearregression (last line of Table 4).As in the case of query focused summarization,JS divergence and percentage of input topic sig-natures in summary are the best features for theupdate task as well.
The overall best feature isJS divergence between the update input and thesummaries?correlations of 0.82 and 0.76 withpyramid and responsiveness.Interestingly, the features combining both up-date and background inputs do not lead to bettercorrelations than those obtained using the updateinput only.
The best performance from combinedfeatures is given by the linear regression metric.Although the correlation of this regression featurewith pyramid scores (0.80) is comparable to JS di-vergence with update inputs, its correlation withresponsiveness (0.67) is clearly lower.
These re-sults show that the term distributions in the updateinput are sufficiently good predictors of contentfor update summaries.
The role of the backgroundinput appears to be negligable.8 DiscussionWe have presented a successful framework formodel-free evaluations of content which uses theinput as reference.
The power of model-free eval-uations generalizes across at least two summariza-tion tasks: query focused and update summariza-tion.We have analyzed a variety of features for input-summary comparison and demonstrated that thestrength of different features varies considerably.Similar term distributions in the input and the sum-mary and diverse use of topic signatures in thesummary are highly indicative of good content.We also find that preprocessing like stemming im-proves the performance of KL and JS divergencefeatures.Very good results were obtained from a corre-lation analysis with human judgements, showingthat input can indeed substitute for model sum-maries and manual efforts in summary evaluation.The best correlations were obtained by a singlefeature, JS divergence (0.88 with pyramid scoresand 0.73 with responsiveness at system level).Our best features can therefore be used to eval-uate the content selection performance of systemsin a new domain where model summaries are un-available.
However, like all other content evalua-tion metrics, our features must be accompanied byjudgements of linguistic quality to obtain whole-some indicators of summary quality and systemperformance.
Evidence for this need is providedby the lower correlations with responsiveness thanthe content-only pyramid evaluations.The results of our analysis zero in on JS diver-gence and topic signature as desirable objectives tooptimize during content selection.
On the macrolevel, they are powerful predictors of content qual-ity.
These findings again emphasize the need foralways including linguistic quality as a componentof evaluation.Observations from our input-based evaluationalso have important implications for the design ofnovel summarization tasks.
We find that high cor-relations with manual evaluations are obtained bycomparing query-focused summaries with the en-tire input and making no use of the query at all.Similarly in the update summarization task, thebest predictions of content for update summarieswere obtained using only the update input.
Theuncertain role of background inputs and queriesexpose possible problems with the task designs.Under such conditions, it is not clear if query-focused content selection or ability to compile up-dates are appropriately captured by any evaluation.ReferencesJ.
Conroy and H. Dang.
2008.
Mind the gap: Dangersof divorcing evaluations of summary content fromlinguistic quality.
In Proceedings of the 22nd Inter-national Conference on Computational Linguistics(Coling 2008), pages 145?152.J.
Conroy, J. Schlesinger, and D. O?Leary.
2006.Topic-focused multi-document summarization usingan approximate oracle score.
In Proceedings ofACL, short paper.313R.
Donaway, K. Drummey, and L. Mather.
2000.
Acomparison of rankings produced by summarizationevaluation measures.
In NAACL-ANLP Workshopon Automatic Summarization.H.
Jing, R. Barzilay, K. Mckeown, and M. Elhadad.1998.
Summarization evaluation methods: Experi-ments and analysis.
In In AAAI Symposium on Intel-ligent Summarization, pages 60?68.M.
Lapata and R. Barzilay.
2005.
Automatic evalua-tion of text coherence: Models and representations.In IJCAI?05.C.
Lin and E. Hovy.
2000.
The automated acquisitionof topic signatures for text summarization.
In Pro-ceedings of the 18th conference on Computationallinguistics, pages 495?501.C.
Lin and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurance statistics.
InProceedings of HLT-NAACL 2003.C.
Lin, G. Cao, J. Gao, and J. Nie.
2006.
Aninformation-theoretic approach to automatic evalu-ation of summaries.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 463?470.C.
Lin.
2004.
ROUGE: a package for automatic eval-uation of summaries.
In ACL Text SummarizationWorkshop.K.
McKeown, R. Barzilay, D. Evans, V. Hatzivas-siloglou, B. Schiffman, and S. Teufel.
2001.Columbia multi-document summarization: Ap-proach and evaluation.
In DUC?01.A.
Nenkova and R. Passonneau.
2004.
Evaluatingcontent selection in summarization: The pyramidmethod.
In HLT/NAACL.A.
Nenkova, R. Passonneau, and K. McKeown.
2007.The pyramid method: Incorporating human con-tent selection variation in summarization evaluation.ACM Trans.
Speech Lang.
Process., 4(2):4.P.
Over, H. Dang, and D. Harman.
2007.
Duc in con-text.
Inf.
Process.
Manage., 43(6):1506?1520.D.
Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,H.
Qi, A.
C?elebi, D. Liu, and E. Drabek.
2003.Evaluation challenges in large-scale multi-documentsummarization: the mead project.
In Proceedings ofACL 2003, Sapporo, Japan.G.
J. Rath, A. Resnick, and R. Savage.
1961.
Theformation of abstracts by the selection of sentences:Part 1: sentence selection by man and machines.American Documentation, 2(12):139?208.I.
Soboroff and D. Harman.
2005.
Novelty detec-tion: the trec experience.
In HLT ?05: Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 105?112.314
