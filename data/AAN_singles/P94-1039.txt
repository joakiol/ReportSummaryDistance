Acquiring Receptive Morphology:A Connectionist ModelMichael GasserComputer Science and Linguistics DepartmentsIndiana UniversityAbst rac tThis paper describes a modular connectionist modelof the acquisition of receptive inflectional morphology.The model takes inputs in the form of phones oneat a time and outputs the associated roots and in-flections.
Simulations using artificial anguage stimulidemonstrate he capacity of the model to learn suffix-ation, prefixation, infixation, circumfixation, mutation,template, and deletion rules.
Separate network mod-ules responsible for syllables enable to the network tolearn simple reduplication rules as well.
The model alsoembodies constraints against association-line crossing.In t roduct ionFor many natural anguages, a major problem for alanguage learner, whether human or machine, is thesystem of bound morphology of the language, whichmay carry much of the functional load of the grammar.While the acquisition of morphology has sometimesbeen seen as the problem of learning how to transformone linguistic form into another form, e.g., by \[Plunkettand Marchman, 1991\] and \[Rumelhart and McClelland,1986\], from the learner's perspective, the problem isone of learning how forms map onto meanings.
Mostwork which has viewed the acquisition of morphology inthis way, e.g., \[Cottrell and Plunkett, 1991\], has takentile perspective of production.
But a human languagelearner almost certainly learns to understand polymor-phemic words before learning to produce them, and pro-duction may need to build on perception \[Gasser, 1993\].Thus it seems reasonable to begin with a model of theacquisition of receptive morphology.In this paper, I will deal with that component of re-ceptive morphology which takes sequences of phones,each expressed as a vector of phonetic features, andidentifies them as particular morphemes.
This processignores the segmentation f words into phone sequences,the morphological structure of words, and the the se-mantics of morphemes.
I will refer to this task as rootand inflection identification.
It is assumed that childrenlearn to identify roots and inflections through the pre-sentation of paired forms and sets of morpheme mean-ings.
They show evidence of generalization when theyare able to identify the root and inflection of a novelcombination of familiar morphemes.At a minimum, a model of the acquisition of this ca-pacity should succeed on the full range of morphologicalrule types attested in the world's languages, it shouldembody known constraints on what sorts of rules arepossible in human language, and it should bear a re-lationship to the production of morphologically com-plex words.
This paper describes a psychologicallymotivated connectionist model (Modular Connection-ist Network for the Acquisition of Morphology, MC-NAM) which shows evidence of acquiring all of the basicrule types and which also experiences relative difficultylearning rules which seem not to be possible.
In anotherpaper \[Gasser, 1992\], I show how the representationsthat develop during the learning of root and inflectionidentification can support word production.
Althoughstill tentative in several respects, MCNAM appears tobe the first computational model of the acquisition ofreceptive morphology to apply to this diversity of mor-phological rules.
In contrast to symbolic models of lan-guage acquisition, it succeeds without built-in symbolicdistinctions, for example, the distinction between stemand affix.The paper is organized as follows.
I first provide abrief overview of the categories of morphological rulesfound in the world's languages.
I then present hemodel and discuss imulations which demonstrate thatit generalizes for most kinds of morphological rules.Next, focusing on template morphology, I show how thenetwork implements he analogue of autosegments andhow the model embodies one constraint on the sorts ofrules that can be learned.
Finally, I discuss augmenta-tion of the model with a hierarchical structure reflect-ing the hierarchy of metrical phonology; this additionis necessary for the acquisition of the most challengingtype of morphological rule, reduplication.Categor ies  o f  Morpho log ica l  P rocessesFor the sake of convenience, I will be discussing mor-phology in terms of the conventional notions of roots,inflections, and rules.
However, a human languagelearner does not have direct access to the root for a279given form, so the problem of learning morphology can-not be one of discovering how to add to or modify aroot.
And it is not clear whether there is anything likea symbolic morphological rule in the brain of a languagelearner.The following kinds of inflectional or derivationalmorphological rules are attested in the world's lan-guages: aj~zation, by which a grammatical morphemeis added to a root (or stem), either before (prefixation),after (suJ~ation), both before and after (eircumfixa-tion), or within (infixation); mutation, by which oneor more root segments themselves are modified; tem-plate rules, by which a word can be described as acombination of a root and a template specifying howsegments are to be intercalated between the root seg-ments; deletion, by which one or more segments aredeleted; reduplication, by which a copy, or a systemat-ically and altered copy, of some portion of the root isadded to it.
Examples of each rule type are included inthe description of the stimuli used in the simulations.The  Mode lThe approach to language acquisition exemplified inthis paper differs from traditional symbolic approachesin that the focus is on specifying the sort of mechanismwhich has the capacity to learn some aspect of language,rather than the knowledge which this seems to require.Given the basic problem of what it means to learn re-ceptive morphology, the goal is to begin with a verysimple architecture and augment it as necessary.
Inthis paper, I first describe a version' of the model whichis modular with respect to the identification of root andinflections.
The advantages of this version over the sim-pler model in which these tasks are shared by the samehidden layer is described in a separate paper \[Gasser,1994\].
Later I discuss a version of the model which in-corporates modularity at the level of the syllable andmetrical foot; this is required to learn reduplication.The model described here is connectionist.
Thereare several reasons why one might want to investigatelanguage acquisition from the perspective of connec-tionism.
For the purposes of this paper, the most im-portant is the hope that a connectionist network, or adevice making use of a related statistical approach tolearning, may have the capacity to learn a task suchas word recognition without pre-wired symbolic knowl-edge.
That is, such a model would make do withoutpre-existing concepts uch as root  and affix or distinc-tions such as regular vs. irregular morphology.
If suc-cessful, this model would provide a simpler account ofthe acquisition of morphology than one which beginswith symbolic knowledge and constraints.Words takes place in time, and a psychologicallyplausible account of word recognition must take thisfact into account.
Words are often recognized long be-fore they finish; hearers eem to be continuously com-paring the contents of a linguistic short-term memorywith the phonologicM representations in their mentallexicons \[Marslen-Wilson and Tyler, 1980\].
Thus thetask at hand requires a short-term emory of some sort.Of the various ways of representing short-term emoryin connectionist networks \[Port, 1990\], the most flexibleapproach makes use of recurrent connections on hiddenunits.
This has the effect of turning the hidden layerinto a short-term memory which is not bounded by afixed limit on the length of the period it can store.
Themodel to be described here is one of the simpler possiblenetworks of this type, a version of the s imple  recur-rent  network  due to \[Elman, 1990\].The Version 1 network is shown in Figure 1 Each boxrepresents a layer of connectionist processing units andeach arrow a complete set of weighted connections be-tween two layers.
The network operates as follows.
Asequence of phones is presented to the input layer oneat a time.
That is, each tick of the network's clock rep-resents the presentation of a single phone.
Each phoneunit represents a phonetic feature, and each word con-sists of a sequence of phones preceded by a boundary"phone" consisting of 0.0 activations.Figure h MCNAM: Version 1An input phone pattern sends activation to the net-work's hidden layers.
Each hidden layer also receivesactivation from the pattern that appeared there on theprevious time step.
Thus each hidden unit is joined by atime-delay connection to each other hidden unit withinits layer.
It is the two previous hidden-layer patternswhich represent the system's hort-term emory of thephonological context.
At the beginning of each word se-quence, the hidden layers are reinitialized to a patternconsisting of 0.0 activations.Finally the output units are activated by the hiddenlayers.
There are at least three output layers.
Onerepresents simply a copy of the current input phone.Training the network to auto-associate its current in-put aids in learning the root and inflection identifica-tion task because it forces the network to learn to dis-tinguish the individual phones at the hidden layers, aprerequisite to using the short-term emory effectively.The second layer of output units represents the root"meaning".
For each root there is a single output unit.Thus while there is no real semantics, the association280between the input phone sequence and the "meaning"is an arbitrary one.
The remaining groups of outputunits represent the inflection "meaning"; one group isshown in the figure.
There is a layer of units for eachseparate inflectional category (e.g., tense and aspect)and a unit for each separate inflection within its layer.One of the hidden layers connects to the root outputlayer, the other to the inflection output layers.For each input phone, the network receives a tar-get consisting of the correct phone, root, and inflectionoutputs for the current word.
The phone target is iden-tical to the input phone.
The root and inflection tar-gets, which are constant throughout the presentation ofa word, are the patterns associated with the root andinflection for the input word.The network is trained using the backpropagationlearning algorithm \[Rumelhart et al, 1986\], which ad-justs the weights on the network's connections in such away as to minimize the error, that is, the difference be-tween the network's outputs and the targets.
For eachmorphological rule, a separate network is trained on asubset of the possible combinations of root and inflec-tion.
At various points during training, the networkis tested on unfamiliar words, that is, novel combina-tions of roots and inflections.
The performance of thenetwork is the percentage of the test roots and inflec-tions for which its output is correct at the end of eachword sequence.
An output is considered "correct" if itis closer to the correct root (or inflection) than to anyother.
The network is evaluated at the end of the wordbecause in general it may need to wait that long to haveenough information to identify both root and inflection.ExperimentsGenera l  Per fo rmance  o f  the  Mode lIn all of the experiments reported on here, the stim-uli presented to the network consisted of words in anartificial anguage.
The phoneme inventory of the lan-guage was made up 19 phones (24 for the mutationrule, which nasalizes vowels).
For each morphologicalrule, there were 30 roots, 15 each of CVC and CVCVCpatterns of phones.
Each word consisted of either twoor three morphemes, a root and one or two inflections(referred to as "tense" and "aspect" for convenience).Examples of each rule, using the root vibun: (1) suf-fix: present-vibuni, past-vibuna; (2) prefix: present-ivibun, past-avibun; (3) infix: present-vikbun, past-vinbun; (4) circumfix: present-ivibuni, past-avibuna;(5) mutation: present-vibun, past-viban; (6) deletion:present-vibun, past-vibu; (7) template: present-vaban,past-vbaan; (8) two-suffix: present perfect-vibunak,present progressive-vibunas, past perfect-vibunik, pastprogressive-vibunis; (9) two-prefix: present perfect-kavibun, present progressive-kivibun, past perfect-savibuu, past progressive-sivibun; (10) prefix-suffix:present perfect-avibune, present progressive-avibunu,past perfect-ovibune, past progressive-ovibunu.
No ir-regular forms were included.For each morphological rule there were either 60 (30roots x 2 tense inflections) or 120 (30 roots x 2 tenseinflections x 2 aspect inflections) different words.
Fromthese 2/3 were selected randomly as training words, andthe remaining 1/3 were set aside as test words.
For eachrule, ten separate networks with different random initialweights were trained and tested.
Training for the tense-only rules proceeded for 150 epochs (repetitions of alltraining patterns); training for the tense-aspect ruleslasted 100 epochs.
Following training the performanceof the network on the test patterns was assessed.Figure ??.
shows the mean performance of the net-work on the test patterns for each rule following train-ing.
Note that chance performance for the roots was.033 and for the inflections .5 since there were 30 rootsand 2 inflections in each category.
For all tasks, in-cluding both root and inflection identification the net-work performs well above chance.
Performance is farfrom perfect for some of the rule types, but further im-provement is possible with optimization of the learningparameters.Interestingly, template rules, which are problematicfor some symbolic approaches to morphology processingand acquisition, are among the easiest for the network.Thus it is informative to investigate further how thenetwork solved this task.
For the particular templaterule, the two forms of each root shared the same initialand final consonant.
This tended to make root identi-fication relatively easy.
With respect o inflections, thepattern is more like infixation than prefixation or suffix-ation because all of'the segments relevant o the tense,that is, the /a /s ,  are between the first and last segment.But inflection identifation for the template is consider-ably higher than for infixation, probably because of theredundancy: the present ense is characterized by an/a /  in second position and a consonant in third posi-tion, the past tense by a consonant in second positionand an /a / in  third position.To gain a better understanding of the way in whichthe network solves a template morphology task, a fur-ther experiment was conducted.
In this experiment,each root consisted of a sequence of three consonantsfrom the set /p, b, m, t, d, s, n, k, g/.
There werethree tense morphemes, each characterized by a partic-ular template.
The present emplate was ClaC2aCaa,the past template aCtC2aaC3, and the future templateaClaC2Caa.
Thus the three forms for the root pmnwere pamana, apmaan, and apamna.
The networklearns to recognize the tense templates very quickly;generalization is over 90% following only 25 epochs oftraining.
This task is relatively easy since the vowelsappear in the same sequential positions for each tense.More interesting is the performance of the root identi-fication part of the network, which must learn to rec-ognize the commonality among sequences of the sameconsonants even though, for any pair of forms for agiven root, only one of the three consonants appearsin the same position.
Performance reaches 72% on the2811ED.?
:: 0.75?
0.5 o t--c0o'~ 0.25Q.0Suf PreRoot identIn Circ Del Mut Tem 2-suf 2-pre P+sType of inflection- - Chancefor rootInfll ident ~ Infl2 ident .
.
.
.
.
.
.
.
.
.
.
.
Chance  fo r in f /Figure 2: Performance on Test Words Following Trainingtest words following 150 epochs.To better visualize the problem, it helps to exam-ine what happens in hidden-layer space for the rootlayer as a word is processed.
This 15-dimensional spaceis impossible to observe directly, but we can get anidea of the most significant movements through thisspace through the use of principal component analysis,a technique which is by now a familiar way of analyz-ing the behavior of recurrent networks \[Elman, 1991,Port, 1990\].
Given a set of data  vectors, principal com-ponent analysis yields a set of orthogonal vectors, orcomponents, which are ranked in terms of how much ofthe variance in the data they account for.Principal components for the root identification hid-den layer vectors were extracted for a single networkfollowing 150 repetit ions of the template training pat-terns.
The paths through the space defined by the firsttwo components of the root identification hidden layeras the three forms of the root pds are presented to thenetwork are shown in Figure 3.
Points marked in thesame way represent the same root consonant.
1 What  wesee is that,  as the root hidden layer processes the word,it passes through roughly similar regions in hidden-layerspace as it encounters the consonants of the root, inde-1Only two points appear for the first root consonant be-cause the first two segments of the past and future forms ofa given root are the same.pendent of their sequential position.
In a sense theseregions correspond to the autosegments of autosegmen-tal phonological and morphological nalyses.Const ra in ts  on  Morpho log ica l  P rocessesIn the previous sections, I have described how mod-ular simple recurrent networks have the capacity tolearn to recognize morphologically complex words re-sulting from a variety of morphological processes.
Butis this approach too powerful?
Can these networkslearn rules of types that people cannot?
While it isnot completely clear what rules people can and can-not learn, some evidence in this direction comes fromexamining large numbers of languages.
One possibleconstraint on morphological rules comes from autoseg-mental analyses: the association lines that join one tierto another should not cross.
Another way of statingthe constraint is to say that the relative position of twosegments within a morpheme remains the same in thedifferent forms of the word.Can a recognition etwork learn a rule which vio-lates this constraint as readily as a comparable onewhich does not?
To test this, separate networks weretrained to learn the following two template morphologyrules, involving three forms: (1) present: CzaC2aCaa,past: aCiC2aaC3, future: aClaC2C3a (2) present:ClaC2Caaa, past: aC1C2aCaa, future: aClaC3aC2.282PC 2, , , , i , , ,  .
.
.
.
.
.
2-0.4 -0.2- -  ?-0.2  0 .4  ?
PCpds  + futpds  +prespds  +pa s tcons l  ?cons2  ?cons3  \[\]Figure 3: Template Rule, Root Hidden Layer, Principal Components 1 and 2, padasa, apdaas, apadsaBoth rules produce the three forms of each root usingthe three root consonants and sequences of th reea 's .In each case each of the three consonants appears inthe same position in two of the three forms.
The sec-ond rule differs from the first in that the order of thethree consonants i not constant; the second and thirdconsonant of the present and past forms reverse theirrelative positions in the future form.
In the terms of alinguistic analysis, the root consonants would appear inone order in the underlying representation of the root(preserved in the present and past forms) but in thereverse order in the future form.
The underlying orderis preserved in all three forms for the first rule.
I willrefer to the first rule as the "favored" one, the secondas the "disfavored" one.In the experiments esting the ease with which thesetwo rules were learned, a set of thirty roots was againgenerated randomly.
Each root consisted of three con-sonants limited to the set: {p, b, m, t, d, n, k, g}.
Asbefore, the networks were trained on 2/3 of the possi-ble combinations of root and inflection (60 words in all)and tested on the remaining third (30 words).
Separatenetworks were trained on the two rules.
Mean resultsfor 10 different networks for each rule are shown in Fig-ure 4.
While the disfavored rule is learned to some ex-tent, there is a clear advantage for the favored over thedisfavored rule with respect o generalization for rootidentification.
Since the inflection is easily recognizedby the pattern of consonants and vowels, the order ofthe second and third root consonants i irrelevant to in-flection identification.
Root identification, on the otherhand, depends crucially on the sequence of consonants.With the first rule, in fact, it is possible to completelyignore the CV templates and pay attention only to theroot consonants in identifying the root.
With the sec-ond rule, however, the only way to be sure which rootis intended is to keep track of which sequences occurwith which templates.
With the two possible roots finand fnt, for example, there would be no way of knowingwhich root appeared in a form not encountered duringtraining unless the combination of sequence and tensehad somehow been attended to during training.
In thisease, the future of one root has the same sequence ofconsonants as the present and past of the other.
Thus,to the extent hat roots overlap with one another, rootidentification with the disfavored rule presents a hardertask to a network.
Given the relatively small set ofconsonants in these experiments, there is considerableoverlap among the roots, and this is reflected in thepoor generalization for the disfavored rule.
Thus forthis word recognition etwork, a rule which apparentlycould not occur in human language is somewhat moredifficult than a comparable one which could.2830.80.70.60.5o~ 0.42 o.3ft.0.20.1 /0 25 50 75 100 125 150Epochs of trainingDisfavoredFavored~ ChanceFigure 4: Template Rules, Favored and Disfavored, Root IdentificationRedupl icationWe have yet to deal with reduplication.
The parsing ofan unfamiliar word involving reduplication apparentlyrequires the ability to notice the similarity between therelevant portions of the word.
For the networks we haveconsidered so far, recognition of reduplication wouldseem to be a difficult, if not an impossible, task.
Con-sider the case in which a network has just heard thesequence tamkam.
At this point we would expect a hu-man listener to be aware that the two syllables rhymed,that is, that they had the same vowel and final conso-nant (rime).
But at the point following the second m,the network does not have direct access to representa-tions for the two subsequences to be compared.
If ithas been trained to identify sequences like tamkara, itwill at this point have a representation f the entire se-quence in its contextual short-term emory.
However,this representation will not distinguish the two sylla-bles, so it is hard to see how they might be compared.To test whether Version 1 of the model could handlereduplication, etworks were trained to perform inflec-tion identification only.
The stimuli consisted of two-syllable words, where the initial consonant (the onset)of each syllable came from the set /p ,  b, f, v, m, t, d, s,z, n, k, g, x, 7, xj/, the vowel from the set / i ,  e, u, o, a/,and the final consonant, when there was one, from theset /n ,  s/.
Separate networks were trained to turn ontheir single output unit when the onsets of the two syl-lables were the same and when the rimes were the same.The training set consisted of 200 words.
In each case,half of the sequences satisfied the reduplication crite-rion.
Results of the two experiments are shown in Fig-ure 5 by the lines marked "Seq".
Clearly these networksfailed to learn this relatively simple reduplication task.While these experiments do not prove conclusively thata recurrent network, presented with words one segmentat a time, cannot learning reduplication, it is obviousthat this is a difficult task for these networks.In a sequential network, input sequences are realizedas movements through state space.
It appears, how-ever, that recognition of reduplication requires the ex-plicit comparison of static representations of the sub-sequences in question, e.g., for syllables in the case ofsyllable reduplication.
If a simple recurrent network istrained to identify, that is, to distinguish, the syllablesin a language, then the pattern appearing on the hid-den layer following the presentation of a syllable mustencode all of the segments in the syllable.
It is, in effect,a summary of the sequence that is the syllable.It is a simple matter to train a network to distinguishall possible syllables in a language.
We treat the syl-lables as separate words in a network like the ones wehave been dealing with, but with no inflection module.A network of this type was trained to recognize all 165284t=o00 Q. oQ.0.80.70.60.50.4I .
~ .. ?
_7~ ............0 40 80 120 160Epochs of trainingFF Rime RedupFF Onset RedupSeq Onset RedupSeq Rime RedupI I ChanceFigure 5: Reduplication Rules, Sequential and Feedforward Networks Trained with Distributed Syllablespossible syllables in the same artificial language usedin the experiment with the sequential network.
Whenpresented to the network, each syllable sequence wasfollowed by a boundary segment.The hidden-layer pattern appearing at the end ofeach syllable-plus-boundary sequence was then treatedas a static representation f the syllable sequence for asecond task.
Previous work \[Gasser, 1992\] has shownthat these representations embody the structure of theinput sequences in ways which permit generalizations.In this case, the sort of generalization which interestsus concerns the recognition of similarities between syl-lables with th,e same onsets or rimes.
Pairs of thesesyllable representations, encoding the same syllables asthose used to train the sequential network in the pre-vious experiment, were used as inputs to two simplefeedforward networks, one trained to respond if its twoinput syllables had the same onset, the other trainedto respond if the two inputs had the same rime, thatis, the same rules trained in the previous experiment.Again the training set consisted of 200 pairs of syllables,the test set of 50 pairs in each case.
Results of theseexperiments are shown in Figure 5 by the lines labeled"FF".
Although performance is far from perfect, it isclear that these networks have made the appropriategeneralization.
This means that the syllable represen-tations encode the structure of the syllables in a formwhich enables the relevant comparisons to be made.What I have said so far about reduplication, how-ever, falls far short of an adequate account.
First, thereis the problem of how the network is to make use ofstatic syllable representations i  recognizing reduplica-tion.
That is, how is access to be maintained to therepresentation for the syllable which occurred two ormore time steps back?
For syllable representations tobe compared irectly, a portion of the network needs torun, in a sense, in syllable time.
That is, rather thanindividual segments, the inputs to the relevant portionof the network need to be entire syllable representa-tions.
Combining this with the segment-level inputsthat we have made use of in previous experiments givesa hierarchical architecture like that shown in Figure 6.In this network, word recognition, which takes placeat the output level, can take as its input both segmentand syllable sequences.
The segment portion of the net-work, appearing on the left in the figure, is identical towhat we have seen thus far.
(Hidden-layer modularityis omitted from the figure to simplify it.)
The syllableportion, on the right, runs on a different "clock" fromthe segment portion.
In the segment portion activationis passed forward and error backward each time a newsegment is presented to the network.
In the syllableportion this happens each time a new syllable appears.
(The different update clock is indicated by the dashedarrows in the figure.)
Just as the segment subnetworkbegins with context-free segment representations, thesyllable subnetwork takes as inputs context-free sylla-bles.
This is achieved by replacing the context (that is,the recurrent input to the SYLLABLE layer) by a bound-axy pattern at the beginning of each new syllable.There remains the question of how the network isto know when one syllable ends and another begins.Unfortunately this interesting topic is beyond the scopeof this project.285~11 r??'
2 ~ 1_~ I~\ [ "% I hidden2 I&_ k ,+I111 hidden1~,  i | _ _\ [ \ [~ segment I!IIFigure 6: MCNAM: Version 2Conc lus ionsCan connectionist networks which are more than unin-teresting implementations of symbolic models learn togeneralize about morphological rules of different ypes?Much remains to be done before this question can be an-swered, but, for receptive morphology at least, the ten-tative answer is yes.
In place of built-in knowledge, e.g,linguistic notions such as affix and tier and constraintssuch as the prohibition against association line crossing,we have processing and learning algorithms and partic-ular architectural features, e.g., recurrent connectionson the hidden layer and modular hidden layers.
Someof the linguistic notions may prove unnecessary alto-gether.
For example, there is no place or state in thecurrent model which corresponds to the notion affix.Others may be realized very differently from the wayin which they are envisioned in conventional models.An autosegment, for example, corresponds roughly to aregion in hidden-layer space in MCNAM.
But this is aregion which took on this significance only in responseto the set of phone sequences and morphological targetswhich the network was trained on.Language is a complex phenomenon.
Connectionistshave sometimes been guilty of imagining naively thatsimple, uniform networks would handle the whole spec-trum of linguistic phenomena.
The tack adopted in thisproject has been to start simple and augment the modelwhen this is called for.
MCNAM in its present form isalmost certain to fail as a general model of morphol-ogy acquisition and processing, but these early resultsindicate that it is on the right track.
In any case, themodel yields many detailed predictions concerning thedifficulty of particular morphological rules for partic-ular phonological systems, so an obvious next step ispsycholinguistic experiments o test the model.Re ferences\[Cottrell and Plunkett, 1991\] Garrison W. Cottrell andKim Plunkett.
Learning the past tense in a recurrentnetwork: Acquiring the mapping from meaning tosounds.
Annual Conference of the Cognitive ScienceSociety, 13:328-333, 1991.\[Elman, 1990\] Jeffrey Elman.
Finding structure intime.
Cognitive Science, 14:179-211, 1990.\[Elman, 1991\] Jeffrey L. Elman.
Distributed represen-tations, simple recurrent networks, and grammaticalstructure.
Machine Learning, 7:195-225, 1991.\[Gasser, 1992\] Michael Gasser.
Learning distributedsyllable representations.
Annual Conference of theCognitive Science Society, 14:396-401, 1992.\[Gasser, 1993\] Michael Gasser.
Learning words in time:Towards a modular connectionist account of the ac-quisition of receptive morphology.
Technical Report384, Indiana University, Computer Science Depart-ment, Bloomington, 1993.\[Gasser, 1994\] Michael Gasser.
Modularity in a connec-tionist model of morphology acquisition.
Proceedingsof the International Conference on ComputationalLinguistics, 15, 1994.\[Marslen-Wilson and Tyler, 1980\] William D. Marslen-Wilson and Lorraine K. Tyler.
The temporal struc-ture of spoken language understanding.
Cognition,8:1-71, 1980.\[Plunkett and Marchman, 1991\] Kim Plun-kett and Virginia Marchman.
U-shaped learning andfrequency effects in a multi-layered perceptron: Im-plications for child language acquisition.
Cognition,38:1-60, 1991.\[Port, 1990\] Robert Port.
Representation and recog-nition of temporal patterns.
Connection Science,2:151-176, 1990.\[Rumelhart and McClelland, 1986\] David E. Rumel-hart and James L. McClelland.
On learning thepast tense of English verbs.
In James L. McClel-land and David E. Rumelhart, editors, Parallel Dis-tributed Processing, Volume 2, pages 216-271.
MITPress, Cambridge, MA, 1986.\[Rumelhart e al., 1986\] David E. Rumelhart, GeoffreyHinton, and Ronald Williams.
Learning internalrepresentations by error propagation.
In David E.Rumelhart and Jay L. McClelland, editors, Paral-lel Distributed Processing, Volume 1, pages 318-364.MIT Press, Cambridge, MA, 1986.286
