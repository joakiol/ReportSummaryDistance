Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 47?55,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsA Boosted Semi-Markov PerceptronTomoya IwakuraFujitsu Laboratories Ltd.1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japaniwakura.tomoya@jp.fujitsu.comAbstractThis paper proposes a boosting algorithmthat uses a semi-Markov perceptron.
Thetraining algorithm repeats the training of asemi-Markov model and the update of theweights of training samples.
In the boost-ing, training samples that are incorrectlysegmented or labeled have large weights.Such training samples are aggressivelylearned in the training of the semi-Markovperceptron because the weights are usedas the learning ratios.
We evaluate ourtraining method with Noun Phrase Chunk-ing, Text Chunking and Extended NamedEntity Recognition.
The experimental re-sults show that our method achieves betteraccuracy than a semi-Markov perceptronand a semi-Markov Conditional RandomFields.1 IntroductionNatural Language Processing (NLP) basic tasks,such as Noun Phrase Chunking, Text Chunking,and Named Entity Recognition, are realized bysegmenting words and labeling to the segmentedwords.
To realize these tasks, supervised learn-ing algorithms have been applied successfully.
Inthe early stages, algorithms for training classifiers,including Maximum Entropy Models (Tsuruokaand Tsujii, 2005), AdaBoost-based learning algo-rithms (Carreras et al 2002), and Support VectorMachines (SVMs) (Kudo and Matsumoto, 2001)were widely used.
Recently, learning algorithmsfor structured prediction, such as linear-chainstructured predictions, and semi-Markov model-based ones, have been widely used.
The examplesof linear-chain structured predictions include Con-ditional Random Fields (CRFs) (Lafferty et al2001) and structured perceptron (Collins, 2002).The examples of semi-Markov model-based onesinclude semi-Markov model perceptron (Cohenand Sarawagi, 2004), and semi-Markov CRFs(Sarawagi and Cohen, 2005).
Among thesemethods, semi-Markov-based ones have showngood performance in terms of accuracy (Cohenand Sarawagi, 2004; Sarawagi and Cohen, 2005;Okanohara et al 2006; Iwakura et al 2011).One of the reasons is that a semi-Markov learnertrains models that assign labels to hypothesizedsegments (i.e., word chunks) instead of labelingto individual words.
This enables use of featuresthat cannot be easily used in word level processingsuch as the beginning word of a segment, the endword of a segment, and so on.To obtain higher accuracy, boosting methodshave been applied to learning methods for trainingclassifiers.
Boosting is a method to create a finalhypothesis by repeatedly generating a weak hy-pothesis and changing the weights of training sam-ples in each training iteration with a given weaklearner such as a decision stump learner (Schapireand Singer, 2000) and a decision tree learner (Car-reras et al 2002).
However, to the best of ourknowledge, there are no approaches that applyboosting to learning algorithms for structured pre-diction.
In other words, if we can successful applyboosting to learning algorithms for structured pre-diction, we expect to obtain higher accuracy.This paper proposes a boosting algorithm fora semi-Markov perceptron.
Our learning methoduses a semi-Markov perceptron as a weak learner,and AdaBoost is used as the boosting algorithm.To apply boosting to the semi-Markov perceptron,the following methods are proposed; 1) Use theweights of training samples decided by AdaBoostas the learning ratios of the semi-Markov percep-tron, and 2) Training on AdaBoost with the lossbetween the correct output of a training sampleand the incorrect output that has the highest score.By the first method, the semi-Markov perceptroncan aggressively learn training samples that are in-47correctly classified at previous iteration becausesuch training samples have large weights.
The sec-ond method is a technique to apply AdaBoost tolearning algorithms for structured prediction thatgenerate negative samples from N-best outputs(Cohen and Sarawagi, 2004), or consider all pos-sible candidates (Sarawagi and Cohen, 2005).
Wealso prove the convergence of our training method.This paper is organized as follows: In Section2, we describe AdaBoost and Semi-Markov per-ceptron is described in Section 3.
Our proposedmethod is described in Section 4, and the experi-mental setting, the experimetal results and relatedwork are described in Section 5, 6, and 7.2 AdaBoostLet X be a domain or sample space and Y bea set of labels {?1,+1}.
The goal is to in-duce a mapping F : X ?
Y.
Let S be{(x1, y1), ..., (xm, ym)}, which is a set of trainingsamples, where xi is a sample in X , and each yibelongs toY .
Each boosting learner learns T typesof weak hypothesis with a given weak learner toproduce a final hypothesis F :F (x) = sign(?Tt=1?tht(x)).where sign(x) is 1 if x is positive, otherwise, itreturns -1.The ht (1 ?
t ?
T ) is the t-th weak hypothe-sis learned by the weak learner.
ht(x) is the pre-diction to x ?
X with ht, and ?t is the confi-dence value of ht that is calculated by the boostinglearner.The given weak learner learns a weak hypoth-esis ht from training samples S = {(xi, yi)}mi=1and weights over samples {wt,1, ..., wt,m} atround t. wt,i is the weight of sample number iat round t for 1 ?
i ?
m. We set w1,i to 1/m.After obtaining t-th weak hypothesis ht, theboosting learner calculates the confidence-value?t for ht.
Then, the boosting learner updates theweight of each sample.
We use the AdaBoostframework (Freund and Schapire, 1997; Schapireand Singer, 1999).
The update of the sampleweights in AdaBoost is defined as follows:wt+1,i = wt,ie?yi ?tht(xi)Zt(?t),(1)where e is Napier?s constant andZt(?t) =m?i=1wt,ie?yi ?tht(xi) (2)# Training data: S = {(Xi,Yi)}mi=1# The learning rations of S: {?i}mi=1# The maximum iteration of perceptron: PSemiMarkovPerceptron(S, P, {?i}mi=1)w = ?0, ..., 0?
# Weight vectora = ?0, ..., 0?
# For averaged perceptronc = 1 # The total number of iterationFor p = 1...PFor i = 1...mY?i = argmaxY?Y(Xi)w ?
?
(Xi,Y)IfY?i ?= Yiw = w + ?i(?
(Xi,Yi) ?
?
(Xi,Y?i ))a = w + c?i(?
(Xi,Yi) ?
?
(Xi,Y?i ))endIfc++endForendForreturn (w - a / c)Figure 1: A pseudo code of a semi-Markov per-ceptron.is the normalization factor for?mi=1 wt+1,i = 1.Let pi be any predicate and [[pi]] be 1 if pi holdsand 0 otherwise.
The following upper bound holdsfor the training error of F consisting of T weakhypotheses (Schapire and Singer, 1999):1mm?i=1[[F (xi) ?= yi]] ?T?t=1Zt(?t).
(3)Eq.
(1) and Eq.
(3) suggest AdaBoost-based learn-ing algorithms will converge by repeatedly select-ing a confidence-value of ?t for ht at each round,that satisfies the following Eq.
(4) at each round:Zt(?t) < 1.
(4)3 Semi-Markov PerceptronIn a semi-Markov learner, instead of labeling indi-vidual words, hypothesized segments are labeled.For example, if a training with an input ?I win?and a label set {NP ,V P } is conducted, consid-ered segments with their labels are the follow-ing: ?
[I](NP ) [win](NP )?, ?
[I](NP ) [win](V P )?,?
[I](V P ) [win](NP )?, ?
[I](V P ) [win](V P )?, ?
[Iwin](NP )?, and ?
[I win](V P )?.Figure 1 shows a pseudo code of a semi-Markovperceptron (Semi-PER) (Cohen and Sarawagi,2004).
We used the averaged perceptron (Collins,482002) based on the efficient implementation de-scribed in (Daume?
III, 2006).
Let S ={(Xi,Yi)}mi=1 be a set of m training data, Xibe i-th training sample represented by a word se-quence, and Yi be the correct segments and thecorrect labeling of Xi.
Yi consists of |Yi| seg-ments.
Yi(j) means the j-th segment of Yi, andl(Yi(j)) means the label ofYi(j).?
(X,Y) is a mapping to a D-dimensional fea-ture vector defined as?
(X,Y) =D?d=1|Y|?j=1?d(X,Y(j)),where ?d is a feature represented by an indicatorfunction that maps an inputX and a segment withits label Y(j) to a D-dimensional vector.
For ex-ample, ?100(X,Y(j)) might be the 100-th dimen-sion?s value is 1 if the beginning word of Y(j) is?Mr.?
and the label l(Y(j)) is ?NP?.w is a weight vector trained with a semi-Markov perceptron.
w??
(X,Y) is the score givento segments with their labels Y of X, and Y(X)is the all possible segments with their labels forX.
The learning ratios of the training samples are{?i}mi=1, and the ratios are set to 1 in a usual semi-Markov perceptron training.In the training of the Semi-PER, for a givenXi,the learner finds Y?i with the Viterbi decoding asdescribed in (Cohen and Sarawagi, 2004):Y?i = argmaxY?Y(Xi)w ?
?
(X,Y).If Y?i is not equivalent to Yi (i.e.
Y?i ?= Yi), theweight w is updated as follows:w = w + ?i(?
(Xi,Yi) ?
?
(Xi,Y?i )).The algorithm takes P passes over the trainingsamples.4 A Boosted Semi-Markov PerceptronThis section describes how we apply AdaBoost toa semi-Markov perceptron training.4.1 Applying BoostingFigure 2 shows a pseudo code for our boosting-based Semi-PER.
To train the Semi-PER withinan AdaBoost framework, we used the weights ofsamples decided by AdaBoost as learning ratios.The initial weight value of i-th sample at boosting# Training data: S = {(Xi,Yi)}mi=1# A weight vector at boosting round t: Wt# The weights of S at round t: {wt,i}mi=1# The iteration of perceptron training: P# The iteration of boosting training: TSemiBoost(S, T , P )W0 = ?0, ..., 0?Set initial value: w1,i = 1/m (for 1 ?
i ?
m)While t ?
Twt=SemiMarkovPerceptron(S,P,{wt,i}mi=1)Find ?t that satisfies Z?t(?t) < 1.Update :Wt = Wt?1 + ?twtFor i = 1...mwt+1,i = wt,i ?
e?
?tdt(Xi)/Z?t(?t)t++endWhilereturnWTFigure 2: A pseudo code of a boosting for a semi-Markov perceptron.round 1 is w1,i = 1/m.
In the first iteration, Semi-PER is trained with the initial weights of samples.Then, we update the weights of training sam-ples.
Our boosting algorithm assigns largerweights to training samples incorrectly segmentedor labeled.
To realize this, we first define a loss forXi at boosting round t as follows:dt(Xi) = st(Xi,Yi) ?
st(Xi,Yti),where,Yti = argmaxY?Y(Xi)?Y ?=Yist(Xi,Y),andst(X,Y) = wt ?
?
(X,Y).st(X,Y) is a score of a word sequence X that issegmented and labeled as Y, and wt is a weightvector trained by Semi-PER at boosting round t.When a given input is correctly segmented andlabeled, the second best output is generated witha forward-DP backward-A* N-best search algo-rithm (Nagata, 1994).
Then we find a confidence-value ?t that satisfies Z?t(?t) < 1:Z?t(?t) =m?i=1wt,ie?
?tdt(Xi).
(5)After obtaining ?t, the weight of each sample isupdated as follows:wt+1,i = wt,i ?
e?
?tdt(Xi)/Z?t(?t).
(6)49If st(Xi,Yi) is greater than st(Xi,Yti) (i.e., 0 <dt(Xi)), the weight of Xi is decreased becauseXi is correctly segmented and labeled.
Otherwise(dt(Xi) < 0), Xi has a larger weight value.
Theupdated weights are used as the learning ratiosin the training of Semi-PER at the next boostinground.
Finally, we update the weight vector Wttrained with boosting as follows:Wt = Wt?1 + ?twtThis process is repeated T times, and a modelWT , which consists of T types of Semi-PER-based models, is obtained.In test phase, the segments and labels of a wordsequenceX is decided as follows:Y?
= argmaxY?Y(X)WT ?
?
(X,Y).4.2 Learning a Confidence ValueSince our algorithm handles real valued scoresof samples given by Semi-PER on the exponen-tial loss of AdaBoost, it?s difficult to analyti-cally determine a confidence-value ?t that satisfiesZ?t(?t) < 1 at boosting round t.Therefore, we use a bisection search to find aconfidence-value.
To detemin the range for the bi-section search, we use a range between 0 and theconfidence-value for a weak hypothesis ht that re-turns its prediction as one of {-1,+1}.
We defineht(Xi) as sign(dt(Xi)).
Schapire and Singer pro-posed an algorithm based on AdaBoost, called realAdaBoost (Schapire and Singer, 1999).
The realAdaBoost analytically calculates the confidence-value that minimizes Eq.
(2).
The derivation ofZt(?t) with ?t isZ ?t(?t) =m?i=1?ht(Xi)wt,ie?
?tht(Xi).By solving Z ?t(?t) = 0, we obtain?
?t =12 log(?mi=1 wt,i[[ht(Xi) = 1]]?mi=1 wt,i[[ht(Xi) = ?1]]).Finally, we select the value that minimizes Eq.
(5) from the range between 0 and 2 ?
?
?t with thebisection search as the confidence-value ?t.
Thisis because we expect to find a better confidence-value from a wider range.4.3 Convergence AnalysisIf we repeatedly find a confidence-value (0 < ?t)that satisfies Z?t(?t) < 1 at each boosting round,the training of the semi-Markov model will beconverged as in the classification case describedin Section 2.1 The following bound on the train-ing error can be proved:1mm?i=1[[Y?i ?= Yi]] ?T?t=1Z?t(?t)whereY?i = argmaxY?Y(Xi)WT ?
?
(Xi,Y).By unraveling Eq.
(6), we have thatwT+1,i = wT,i ?
e?
?tdt(Xi)/Z?t(?t)= e?
?Tt=1 ?tdt(Xi)m?Tt=1 Z?t(?t)= e?
?Tt=1 ?twt?(?(Xi,Yi)??
(Xi,Yti))m?Tt=1 Z?t(?t).Therefore, ifY?i ?= Yi,e?
?Tt=1 ?twt?(?(Xi,Yi)??
(Xi,Y?i ))m?Tt=1 Z?t(?t)?
wT+1,i,since, for 1 ?
t ?
T ,wt ?
?
(Xi,Y?i ) ?
wt ?
?
(Xi,Yti).Moreover, when Y?i ?= Yi, the following is satis-fied.1 ?
e?
?Tt=1 ?twt?(?(Xi,Yi)??
(Xi,Y?i ))?
e?
?Tt=1 ?twt?(?(Xi,Yi)??
(Xi,Yti))= e?
?Tt=1 ?tdt(Xi).Therefore,[[Y?i ?= Yi]] ?
e?
?Tt=1 ?tdt(Xi).These give the stated bound on training error;1mm?i=1[[Y?i ?= Yi]] ?
?mi=1 e?
?Tt=1 ?tdt(Xi)m=m?i=1(T?t=1Z?t(?t))wT+1,i=T?t=1Z?t(?t).10 < ?t means the weighted error of the current Semi-PER,?mi=1[[Yti ?= Yi]]wi,t, is less than 0.5 on the trainingdata.
Fortunately, this condition was always satisfied with thetraining of Semi-PER in our experiments.505 Experimental Settings5.1 Noun Phrase ChunkingThe Noun Phrase (NP) chunking task was cho-sen because it is a popular benchmark for test-ing a structured prediction.
In this task, nounphrases called base NPs are identified.
?
[He] (NP)reckons [the current account deficit] (NP)...?
isan example.
The training set consists of 8,936sentences, and the test set consists of 2,012 sen-tences.2 To tune parameters for each algorithm,we used the 90% of the train data for the trainingof parameter tuning, and the 10% of the trainingdata was used as a development data for measur-ing accuracy at parameter tuning.
A final modelwas trained from all the training data with the pa-rameters that showed the highest accuracy on thedevelopment data.5.2 Text ChunkingWe used a standard data set prepared for CoNLL-2000 shared task.3 This task aims to identify10 types of chunks, such as, NP, VP, PP, ADJP,ADVP, CONJP, INITJ, LST, PTR, and SBAR.?
[He] (NP) [reckons] (VP) [the current accountdeficit] (NP)...?
is an example of text chunk-ing.
The data consists of subsets of Penn WallStreet Journal treebank; training (sections 15-18)and test (section 20).
To tune parameters for eachalgorithm, we used the same approach of the NPchunking one.5.3 Japanese Extended NE RecognitionTo evaluate our algorithm on tasks that includelarge number of classes, we used an extended NErecognition (ENER) task (Sekine et al 2002).This Japanese corpus for ENER (Hashimoto et al2008) consists of about 8,500 articles from 2005Mainichi newspaper.
The corpus includes 240,337tags for 191 types of NEs.
To segment words fromJapanese sentences, we used ChaSen.4 Words mayinclude partial NEs because words segmented withChaSen do not always correspond with NE bound-aries.
If such problems occur when we segmentthe training data, we annotated a word chunk withthe type of the NE included in the word chunk.The evaluations are performed based on the gold2We used the data obtained from ftp://ftp.cis.upenn.edu/pub/chunker/ .3http://lcg-www.uia.ac.be/conll2000/chunking/4We used ChaSen-2.4.2 with Ipadic-2.7.0.
ChaSen?s webpage is http://chasen-legacy.sourceforge.jp/.Table 1: Features.
[tj , CLj ], [tj ,WBj ], [tj , PBj ],[tj , wbp], [tj , pbp],[tj , wep], [tj , pep], [tj , wip],[tj , pip] ,[tj , wbp, wep], [tj , pbp, pep],[tj , wbp, pep], [tj , pbp, wep],[tj , wbp?1], [tj , pbp?1], [tj , wbp?2], [tj , pbp?2],[tj , wep+1], [tj , pep+1], [tj , wep+2], [tj , pep+2],[tj , pbp?2, pbp?1], [tj , pep+1, pep+2],[tj , pbp?2, pbp?1, pbp], [tj , pep, pep+1, pep+2]% Features used for only Text Chunking and NP Chunking[tj , wbp, wip], [tj , wbp, pip],[tj , wbp, pip], [tj , pbp, pip],[tj , wep, wip], [tj , wep, pip],[tj , wep, pip], [tj , pep, pip],[tj , wbp, wep, wip], [tj , wbp, wep, pip],[tj , wbp, wep, pip], [tj , wbp, pep, pip]standard data for the test.
We created the follow-ing sets for this experiment.
Training data is newsarticles from January to October 2005 in the cor-pus, which includes 205,876 NEs.
Developmentdata is news articles in November 2005 in the cor-pus, which includes 15,405 NEs.
Test data is newsarticles in December 2005 in the corpus, which in-cludes 19,056 NEs.5.4 Evaluation MetricsOur evaluation metrics are recall (RE), precision(PR), and F-measure (FM ) defined as follows:RE = Cok/Call, PR = Cok/CrecandFM = 2 ?RE ?
PR/(RE + PR),where Cok is the number of correctly recognizedchunks with their correct labels,Call is the numberof all chunks in a gold standard data, and Crec isthe number of all recognized chunks.5.5 FeaturesTable 1 lists features used in our experiments.
ForNP Chunking and Text Chunking, we added fea-tures derived from segments in addition to ENERfeatures.5wk is the k-th word, and pk is the Part-Of-Speech (POS) tag of k-th word.
bp is the positionof the first word of the current segment in a given5We did not use the additional features for ENER becausethe features did not contribute to accuracy.51word sequence.
ep indicates the position of the lastword of the current segment.
ip is the position ofwords inside the current segment (bp < ip < ep).If the length of the current segment is 2, we usefeatures that indicate there is no inside word as thefeatures of ip-th words.
tj is the NE class labelof j-th segment.
CLj is the length of the currentsegment, whether it be 1, 2, 3, 4, or longer than 4.WBj indicates word bigrams, and PBj indicatesPOS bigrams inside the current segment.5.6 Algorithms to be ComparedThe following algorithms are compared with ourmethod.?
Semi-Markov perceptron (Semi-PER)(Cohen and Sarawagi, 2004): We used one-best output for training.
This Semi-PER isalso used as the weak learner of our boostingalgorithm.?
Semi-Markov CRF (Semi-CRF) (Sarawagiand Cohen, 2005): To train Semi-CRF, astochastic gradient descent (SGD) trainingfor L1-regularized with cumulative penalty(Tsuruoka et al 2009) was used.
The batchsize of SGD was set to 1.These algorithms are based on sequentiallyclassifying segments of several adjacent words,rather than single words.
Ideally, all the possi-ble word segments of each input should be con-sidered for this algorithm.
However, the trainingof these algorithms requires a great deal of mem-ory.
Therefore, we limit the maximum size of theword-segments.
We use word segments consistingof up to ten words due to the memory limitation.We set the maximum iteration for Semi-PERto 100, and the iteration number for Semi-CRFtrained with SGD to 100 ?
m, where m is thenumber of training samples.
The regularizationparameter C of Semi-CRF and the number of it-eration for Semi-PER are tuned on developmentdata.6 For our boosting algorithm, the number ofboosting iteration is tuned on development datawith the number of iteration for Semi-PER tunedon development data.
We set the maximum itera-tion number for boosting to 50.6For C of Semi-CRF,{1, 10?1, 10?2, 10?3, 10?4, 10?5, 10?6, 10?7, 10?8, 10?9}were examined.Table 2: Results of NP Chunking.Learner F-measure Recall PrecisionSemi-PER 94.32 94.53 94.11Semi-CRF 94.32 94.52 94.13Semi-Boost 94.60 94.85 94.35Table 3: Results of Text Chunking.Learner F-measure Recall PrecisionSemi-PER 94.10 94.15 94.05Semi-CRF 93.79 93.96 93.62Semi-Boost 94.15 94.27 94.036 Experimental ResultsWe used a machine with Intel(R) Xeon(R) CPUX5680@ 3.33GHz and 72 GBmemory.
In the fol-lowing, our proposed method is referred as Semi-Boost.6.1 NP ChunkingTable 2 shows the experimental results on NPChunking.
Semi-Boost showed the best accuracy.Semi-Boost showed 0.28 higher F-measure thanSemi-PER and Semi-CRF.
To compare the results,we employed a McNemar paired test on the label-ing disagreements as was done in (Sha and Pereira,2003).
All the results indicate that there is a sig-nificant difference (p < 0.01).
This result showsthat Semi-Boost showed high accuracy.6.2 Text ChunkingTable 3 shows the experimental results on TextChunking.
Semi-Boost showed 0.36 higher F-measure than Semi-CRF, and 0.05 higher F-measure than Semi-PER.
The result of McNemartest indicates that there is a significant difference(p < 0.01) between Semi-Boost and Semi-CRF.However, there is no significant difference be-tween Semi-Boost and Semi-PER.6.3 Extended Named Entity RecognitionTable 4 shows the experimental results on ENER.We could not train Semi-CRF because of the lackof memory for this task.
Semi-Boost showed 0.24higher F-measure than that of Semi-PER.
The re-sults indicate there is a significant difference (p <52Table 4: Experimental results for ENER.Learner F-measure Recall PrecisionSemi-PER 81.86 79.06 84.87Semi-CRF N/ASemi-Boost 82.10 79.36 85.03Table 5: Training time of each learner (second)for NP Chunking (NP), Text Chunking (TC) andENER.
The number of Semi-Boost iteration isonly one time.
The +20 cores means training ofSemi-Boost with 20 cores.Learner NP TC ENERSemi-PER 475 559 13,559Semi-CRF 2,120 8,228 N/ASemi-Boost 499 619 32,370+20 cores 487 650 19,5980.01).76.4 Training SpeedWe compared training speed under the followingcondition; The iteration for Semi-PER is 100, theiteration number for Semi-CRF trained with SGDis 100?m, wherem is the number of training sam-ples, and the one time iteration of boosting withthe perceptron iteration 100.
Therefore, all train-ing methods attempted 100?m times estimation.Table 5 shows the training time of each learner.In NP Chunking, the training time of Semi-PER,Semi-CRF, and Semi-Boost were 475 seconds,2,120 seconds, and 499 seconds.
In Text Chunk-ing, the training time of Semi-PER, Semi-CRF,and our method were 559 seconds, 8,228 seconds,and 619 seconds.
Semi-Boost shows competitivetraining speed with Semi-PER and 4 to 13 timesfaster training speed in terms of the total numberof parameter estimations The difference of timebetween Semi-PER and our method is the time forcalculating confidence-value of boosting.When Semi-Boost trained a model for ENER,the training speed was degraded.
The training timeof Semi-Boost was 32,370 and the training timeof Semi-PER was 13,559.
One of the reasons isthe generation of an incorrect output of each train-7The results on the test data were compared by characterunits as in Japanese morphological analysis (Iwakura et al2011).
This is because the ends or beginnings of JapaneseNEs do not always correspond with word boundaries.Table 6: The best results for NP Chunking (FM ).
(Kudo and Matsumoto, 2001) 94.22(Sun et al 2009) 94.37This paper 94.60ing sample.
In our observation, when the num-ber of classes is increased, the generation speed ofincorrect outputs with N-best search is degraded.To improve training speed, we used 20 cores forgenerating incorrect outputs.
When the trainingwith 20 cores was conducted, the training data wassplit to 20 portions, and each portion was pro-cessed with one of each core.
The training timewith the 20 cores was 19,598 for ENER.
However,the training time of NP Chunking was marginallyimproved and that of Text Chunking was slightlyincreased.
This result implies that multi-core pro-cessing is effective for the training of large classeslike ENER in Semi-Boost.In fact, since Semi-Boost requires additionalboosting iterations, the training time of Semi-Boost increases.
However, the training time in-creases linearly by the number of boosting itera-tion.
Therefore, Semi-Boost learned models fromthe large training data of ENER.6.5 Memory UsageSemi-Boost consumed more memory than Semi-PER.
This is because our learning method main-tains a weight vector for boosting in addition tothe weight vector of Semi-PER.
Compared withSemi-CRF, Semi-Boost showed lower memoryconsumption.
On the training data for Text Chunk-ing, the memory size of Semi-Boost, Semi-PER,and Semi-CRF are 4.4 GB, 4.1 GB, and 18.0 GB.When we trained models for ENER, Semi-PERconsumed 32 GB and Semi-Boost consumed 33GB.
However, Semi-CRF could not train mod-els because of the lack of memory.
This is be-cause Semi-CRF maintains a weight vector and aparameter vector for L1-norm regularization andSemi-CRF considers all possible patterns gener-ated from given sequences in training.
In contrast,Semi-PER and Semi-Boost only consider featuresthat appeared in correct ones and incorrectly rec-ognized ones.
These results indicate that Semi-Boost can learn models from large training data.53Table 7: The best results for Text Chunking (FM ).Semi-supervised learning(Ando and Zhang, 2005) 94.39(Iwakura and Okamoto, 2008) 94.32(Suzuki and Isozaki, 2008) 95.15With additional resources(Zhang et al 2001) 94.17(Daume?
III and Marcu, 2005) 94.4Without lexical resources(Kudo and Matsumoto, 2001) 93.91(Kudo et al 2005) 94.12(Tsuruoka and Tsujii, 2005) 93.70(Tsuruoka et al 2009) 93.68This paper 94.157 Related Work7.1 NP ChunkingTable 6 shows the previous best results for NPChunking.
The F-measure of Semi-Boost is 94.60that is 0.23 higher than that of (Sun et al 2009)and 0.38 higher than that of (Kudo and Mat-sumoto, 2001).7.2 Text ChunkingTable 7 shows the previous best results for TextChunking.
We see that our method attaineda higher accuracy than the previous best re-sults obtained without any additional lexical re-sources such as chunking methods based on SVM(Kudo and Matsumoto, 2001), CRF with rerank-ing (Kudo et al 2005), Maximum Entropy (Tsu-ruoka and Tsujii, 2005), and CRF (Tsuruoka et al2009).
This result indicates that our method per-forms well in terms of accuracy.The previous results with lexical resources orsemi-supervised ones showed higher accuracythan that of our method.
For example, lexical re-sources such as lists of names, locations, abbrevi-ations and stop words were used (Daume?
III andMarcu, 2005), and a full parser output was usedin (Zhang et al 2001).
Semi-supervised onesused a generative model trained from automati-cally labeled data (Suzuki and Isozaki, 2008), thecandidate tags of words collected from automati-cally labeled data (Iwakura and Okamoto, 2008),or automatically created classifiers by learningfrom thousands of automatically generated aux-iliary classification problems from unlabeled data(Ando and Zhang, 2005).
Our algorithm can alsoincorporate the lexical resources and the semi-supervised approaches.
Future work should evalu-ate the effectiveness of the incorporation of them.7.3 Extended Named Entity RecognitionFor ENER, the best result was the Semi-PER one(Iwakura et al 2011).
The F-measure of Semi-PER was 81.95, and the result was higher than NEchunker based on structured perceptron (Collins,2002), and NE chunkers based on shift-reduce-parsers (Iwakura et al 2011).
Our method showed0.15 higher F-measure than that of the Semi-PERone.
This result is also evidence that our methodperforms well in terms of accuracy.7.4 Training MethodsThere have been methods proposed to improve thetraining speed for semi-Markov-based learners.With regard to reducing the space of lattices builtinto the semi-Markov-based algorithms, a methodwas proposed to filter nodes in the lattices with anaive Bayes classifier (Okanohara et al 2006).
Toimprove training speed of Semi-CRF, a succinctrepresentation of potentials common across over-lapping segments of semi-Markov model was pro-posed (Sarawagi, 2006).
These methods can alsobe applied to Semi-PER.
Therefore, we can expectimproved training speed with these methods.Recent online learners update both parametersand the estimate of their confidence (Dredze andCrammer, 2008; Crammer et al 2009; Mejerand Crammer, 2010; Wang et al 2012).
Inthese algorithms, less confident parameters are up-dated more aggressively than more confident ones.These algorithms maintain the confidences of fea-tures.
In contrast, our boosting approach main-tains the weights of training samples.
In futurework, we?d like to consider the use of these algo-rithms in boosting of semi-Markov learners.8 ConclusionThis paper has proposed a boosting algorithm witha semi-Markov perceptron.
The experimental re-sults on Noun Phrase Chunking, Text Chunkingand Japanese Extended Named Entity Recognitionhave shown that our method achieved better accu-racy than a semi-Markov perceptron and a semi-Markov CRF.
In future work, we?d like to evaluatethe boosting algorithm with structured predictiontasks such as POS tagging and parsing.54ReferencesRie Ando and Tong Zhang.
2005.
A high-performancesemi-supervised learning method for text chunking.In Proc.
of ACL?05, pages 1?9.Xavier Carreras, Llu?
?s Ma`rques, and Llu?
?s Padro?.2002.
Named entity extraction using adaboost.
InProc.
of CoNLL?02, pages 167?170.William W. Cohen and Sunita Sarawagi.
2004.
Ex-ploiting dictionaries in named entity extraction:combining semi-markov extraction processes anddata integration methods.
In Proc.
of KDD?04,pages 89?98.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: theory and ex-periments with perceptron algorithms.
In Proc.
ofEMNLP?02, pages 1?8.Koby Crammer, Alex Kulesza, and Mark Dredze.2009.
Adaptive regularization of weight vectors.
InProc.
of NIPS?09, pages 414?422.Hal Daume?
III and Daniel Marcu.
2005.
Learn-ing as search optimization: Approximate large mar-gin methods for structured prediction.
In Proc.
ofICML?05, pages 169?176.Hal Daume?
III.
2006.
Practical Structured LearningTechniques for Natural Language Processing.
Ph.D.thesis, University of Southern California.Mark Dredze and Koby Crammer.
2008.
Online meth-ods for multi-domain learning and adaptation.
InProc.
of EMNLP?08, pages 689?697.Yoav Freund and Robert E. Schapire.
1997.
Adecision-theoretic generalization of on-line learningand an application to boosting.
J. Comput.
Syst.
Sci.,55(1):119?139.Taiichi Hashimoto, Takashi Inui, and Koji Murakami.2008.
Constructing extended named entity anno-tated corpora.
IPSJ SIG Notes, 2008(113):113?120.Tomoya Iwakura and Seishi Okamoto.
2008.
A fastboosting-based learner for feature-rich tagging andchunking.
In Proc.
of CoNLL?08, pages 17?24.Tomoya Iwakura, Hiroya Takamura, and Manabu Oku-mura.
2011.
A named entity recognition methodbased on decomposition and concatenation of wordchunks.
In Proc.
of IJCNLP?11, pages 828?836.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withSupport Vector Machines.
In Proc.
of NAACL?01,pages 192?199.Taku Kudo, Jun Suzuki, and Hideki Isozaki.
2005.Boosting-based parse reranking with subtree fea-tures.
In Proc.
of ACL?05, pages 189?196.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proc.
of ICML?01, pages 282?289.Avihai Mejer and Koby Crammer.
2010.
Confidencein structured-prediction using confidence-weightedmodels.
In Proc.
of EMNLP?10, pages 971?981.Masaaki Nagata.
1994.
A stochastic japanese mor-phological analyzer using a forward-dp backward-a* n-best search algorithm.
In Proc.
of COLING?94,pages 201?207.Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-ruoka, and Jun?ichi Tsujii.
2006.
Improvingthe scalability of semi-markov conditional randomfields for named entity recognition.
In Proc.
ofACL?06, pages 465?472.Sunita Sarawagi and William W. Cohen.
2005.
Semi-markov conditional random fields for informationextraction.
In Proc.
of NIPS?04, pages 1185?1192.Sunita Sarawagi.
2006.
Efficient inference on se-quence segmentation models.
In Proc.
of ICML?06,pages 793?800.Robert E. Schapire and Yoram Singer.
1999.
Improvedboosting algorithms using confidence-rated predic-tions.
Machine Learning, 37(3):297?336.Robert E. Schapire and Yoram Singer.
2000.
Boostex-ter: A boosting-based system for text categorization.Machine Learning, 39(2/3):135?168.Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.2002.
Extended named entity hierarchy.
In Proc.
ofLREC?02.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proc.
of NAACLHLT?03, pages 134?141.Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, andJun?ichi Tsujii.
2009.
Latent variable perceptronalgorithm for structured classification.
In Proc.
ofIJCAI?09, pages 1236?1242.Jun Suzuki and Hideki Isozaki.
2008.
Semi-supervisedsequential labeling and segmentation using giga-word scale unlabeled data.
In Proc.
of ACL-08:HLT, pages 665?673.Yoshimasa Tsuruoka and Junichi Tsujii.
2005.
Bidi-rectional inference with the easiest-first strategy fortagging sequence data.
In Proc.
of HLT/EMNLP,pages 467?474.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumulativepenalty.
In Proc.
of ACL/IJCNLP, pages 477?485.Jialei Wang, Peilin Zhao, and Steven C.H.
Hoi.
2012.Exact soft confidence-weighted learning.
In Proc.
ofICML?12, pages 121?128.Tong Zhang, Fred Damerau, and David Johnson.
2001.Text chunking using regularized winnow.
In Proc.
ofACL?01, pages 539?546.55
