Probing the lexicon in evaluating commercial MT systemsMart in  Vo lkUnivers i ty  of Zur ichDepar tment  of Computer  Science, Computat iona l  L inguist ics  GroupWinter thurers t r .
190, CH-8057 Zur ichvolk?ifi, unizh, chAbst rac tIn the past the evaluation of machine trans-lation systems has focused on single sys-tem evaluations because there were onlyfew systems available.
But now there areseveral commercial systems for the samelanguage pair.
This requires new methodsof comparative evaluation.
In the paper wepropose a black-box method for comparingthe lexical coverage of MT systems.
Themethod is based on lists of words from dif-ferent frequency classes.
It is shown howthese word lists can be compiled and usedfor testing.
We also present he results ofusing our method on 6 MT systems thattranslate between English and German.1 In t roduct ionThe evaluation of machine translation (MT) sys-tems has been a central research topic in recentyears (cp.
(Sparck-Jones and Galliers, 1995; King,1996)).
Many suggestions have focussed on measur-ing the translation quality (e.g.
error classificationin (Flanagan, 1994) or post editing time in (Minnis,1994)).
These measures are time-consuming and dif-ficult to apply.
But translation quality rests on thelinguistic competence of the MT system which againis based first and foremost on grammatical coverageand lexicon size.
Testing grammatical coverage canbe done by using a test suite (cp.
(Nerbonne t al.,1993; Volk, 1995)).
Here we will advocate a prob-ing method for determining the lexical coverage ofcommercial MT systems.We have evaluated 6 MT systems which translatebetween English and German and which are all po-sitioned in the low price market (under US$ 1500).?
German Assistant in Accent Duo V2.0 (de-veloper: MicroTac/Globalink; distributor: Ac-cent)* Langenscheidts T1 Standard V3.0 (developer:GMS; distributor: Langenscheidt)?
Personal Translator plus V2.0 (developer: IBM;distributor: von Rheinbaben & Busch)?
Power Translator Professional (developer/dis-tributor: Globalink) 1?
Systran Professional for Windows (developer:Systran S.A.; distributor: Mysoft)?
Telegraph V1.0 (developer/distributor: Glob-alink)The overall goal of our evaluation was a compar-ison of these systems resulting in recommendationson which system to apply for which purpose.
Theevaluation consisted of compiling a list of criteriafor self evaluation and three experiments with ex-ternal volunteers, mostly students from a local in-terpreter school.
These experiments were performedto judge the information content of the translations,the translation quality, and the user-friendliness.The list of criteria for self evaluation consisted oftechnical, linguistic and ergonomic issues.
As partof the linguistic evaluation we wanted to determinethe lexical coverage of the MT systems ince onlysome of the systems provide figures on lexicon sizein the documentation.Many MT system evaluations in the past havebeen white-box evaluations performed by a test-ing team in cooperation with the developers (see(Falkedal, 1991) for a survey).
But commercialMT systems can only be evaluated in a black-boxsetup since the developer typically will not makethe source code and even less likely the linguisticsource data (lexicon and grammar) available.
Mostof the evaluations described in the literature havecentered around one MT system.
But there are1Recently a newer version has been announced as"Power Translator Pro 6.2".112hardly any reports on comparative evaluations.
Anoted exception is (Rinsche, 1993), which comparesSYSTRAN 2, LOGOS and METAL for German - En-glish translation 3.
She uses a test suite with 5000words of authentic texts (from an introduction toComputer Science and from an official journal of theEuropean Commission).
The resulting translationsare qualitatively evaluated for lexicon, syntax andsemantics errors.
The advantage of this approach isthat words are evaluated in context.
But the resultsof this study cannot be used for comparing the sizesof lexicons ince the number of error tokens is givenrather than the number of error types.
Furthermoreit is questionable if a running text of 5000 words saysmuch about lexicon size, since most of this figure isusually taken up by frequent closed class words.If we are mainly interested in lexicon size thismethod has additional drawbacks.
First, it is time-consuming to find out if a word is translated cor-rectly within running text.
Second, it takes a lot ofredundant translating to find missing lexical items.So, if we want to compare the lexicon size of differ-ent MT systems, we have to find a way to determinethe lexical coverage by executing the system withselected lexical items.
We therefore propose to usea special word list with words in different frequencyranges to probe the lexicon efficiently.2 Our  method  o f  p rob ing  the  lex iconLexicon size is an important selling argument forprint dictionaries and for MT systems.
The countingmethods however are not standardized and thereforethe advertised numbers need to be taken with greatcare (for a discussion see (Landau, 1989)).
In a simi-lar manner the figures for lexicon size in MT systems("a lexicon of more than 100.000 words", "more than3.000 verbs").need to be critically examined.
Whilewe cannot determine the absolute lexicon size with ablack-box test we can determine the relative lexicalcoverage of systems dealing with the same languagepair.When selecting the word lists for our lexicon eval-uation we concentrated on adjectives, nouns, andverbs.
We assume that the relatively small num-ber of closed class words like determiners, pronouns,prepositions, conjunctions, and adverbs must be ex-haustively included in the lexicon.
For each of the:SYSTRAN is not to be confused with Systran Pro-fessional for Windows.
SYSTRAN is a system with adevelopment history dating back to the seventies.
It isweU known for its long-standing employment with theEuropean Commission.3Part of the study is also concerned with French -English translation.three word classes in question (Adj, N, V) we testedwords with high, medium, and low absolute fre-quency.
We expected that words with high fre-quency should all be included in the lexicon, whereaswords with medium and low frequency should giveus a comparative measure of lexicon size.
With theseword lists we computed:1.
What percentage of the test words is trans-lated?2.
What percentage of the test words is correctlytranslated?The difference between 1. and 2. stems mostlyfrom the fact that the MT systems regard unknownwords as compounds, split them up into knownunits, and translate these units.
Obviously this re-sults in sometimes bizarre word creations (see sec-tion 2.3).Our evaluation consisted of three steps.
First, weprepared the word lists.
Second, we ran the tests onall systems.
Finally, we evaluated the output.
Thesesteps had to be done for both translation directions(German to English and vice versa), but here weconcentrate on English to German.2.1 P reparat ion  of  the word listsWe extracted the words for our test from the CELEXdatabase.
CELEX (Baayen, Piepenbrock, and vanRijn, 1995) is a lexical database for English, Ger-man and Dutch.
It contains 51,728 stems for Ger-man (among them 9,855 adjectives; 30,715 nouns;9,400 verbs) and 52,447 stems for English (amongthem 9,214 adjectives; 29,494 nouns; 8,504 verbs).This database also contains frequency data whichfor German were derived from the Mannheim cor-pus of the "Institut fiir deutsche Sprache" and forEnglish were computed from the Cobuild corpus ofthe University of Birmingham.
Looking at the fre-quency figures we decided to take:?
The 100 most frequent adjectives, nouns, verbs.
* 100 adjectives, nouns, verbs with frequency 25or less.
Frequency 25 was chosen because it isa medium frequency for all three word classes.?
The first 100 adjectives, nouns, verbs with fre-quency 1.
44CELEX also contains entries with frequency 0, butwe wanted to assure a minimal degree of commonnessby selecting words with frequency 1.
Still, many wordswith frequency 1seem exotic or idiosyncratic uses.113Unfortunately the CELEX data contain somenoise especially for the German entries.
This meantthat the extracted word lists had to be manual lychecked.
One problem is that some stems occurtwice in the list.
This is the case if a verb is usedwith a prefix in both the separable and the fixedvariant (as e.g.
iibersetzen engl.
to translate vs. toferry across).
Since our test does not distinguishthese variants we took only one of these stems.
An-other problem is that  the frequency count is purelywordform-based.
That  means, if a word is frequentlyused as an adverb and seldom as a verb the count ofthe total  number of occurrences will be attr ibuted toboth the adverb and the verb stem.
Therefore, somewords appear at strange frequency positions.
Forexample the very unusual German verb heuen (engl.to make hay) is l isted among the 100 most frequentverbs.
This is due to the fact that its 3rd personpast tense form is a homograph of the frequent ad-verb heute (engl.
today).
Such obviously misplacedwords were el iminated from the list, which was re-filled with subsequent items in order to contain ex-actly 100 words in each frequency class of each word.The English data  in CELEX are more reliable.The frequency count has been disambiguated forpart  of speech by manual ly  checking 100 occurrencesof each word-form and thus est imating the total  dis-tr ibution.
In this way it has been determined thatbank is used as a noun in 97% of all occurrences(in 3% it is a verb).
This does not say anythingabout the distr ibut ion of the different noun readings(financial institution vs. a slope alongside a riveretc.
).If a word is the same in English and in German (ase.g.
international, Squaw) it must also be excludedfrom our test list.
This is because some systems in-sert the source word into the target sentence if thesource word (and its translat ion) is not in the lexi-con.
If source word and target word are identical wecannot determine if the word in the target sentencecomes from the lexicon or is simply inserted becauseit is unknown.After the word lists had been prepared, we con-structed a simple sentence with every word sincesome systems cannot translate lists with single wordunits.
Wi th  the sentence we were trying to get eachsystem to translate a given word in the intendedpart of speech.
For German we chose the sentencetemplates:(1) Es ist (adjective/.Ein (noun) ist gut.Wir  mtissen (verb/.Adjectives were tested in predicative use since thisis the only posit ion where they appear uninflected.Nouns were embedded within a simple copula sen-tence.
The indefinite article for a noun sentence wasmanual ly adjusted to 'eine' for female gender nouns.Nouns that occur only in a plural form also needspecial t reatment,  i.e.
a plural determiner and a plu-ral copula form.
Verbs come after the modal  verbmiissen because it requires an infinitive and it doesnot distinguish between separable prefix verbs andother verbs.
On similar reasons we took for English:(2) This is (adjective).The (noun) can be nice.We (verb).The modal  can was used in noun sentences toavoid number agreement problems for plural-onlywords like people.
Our sentence list for Englishnouns thus looked like:(3) 1.
The t ime can be nice.2.
The man can be nice.3.
The people can be nice.300.
The unlikel ihood can be nice.2.2 Runn ing  the  tes tsThe sentence lists for adjectives, nouns, and verbswere then loaded as source document in one MT sys-tem after the other.
Each system translated the sen-tence lists and the target document was saved.
Mostsystems allow to set a subject area parameter  (forsubjects such as finances, electrical engineering, oragriculture).
This option is meant to d isambiguatebetween different word senses.
The German nounBank is translated as English bank if the subject areais finances, otherwise it is translated as bench.
Nosubject area lexicon was act ivated in our test runs.We concentrated on checking the general vocabulary.In addit ion Systran allows for the selection of doc-ument types (such as prose, user manuals,  corre-spondence, or parts lists).
Unfortunately the doc-umentat ion does not tell us about the effects of sucha selection.
No document ype was selected for ourtests.Running the tests takes some t ime since 900 sen-tences need to be translated by 6 systems.
On our486-PC the systems differ greatly in speed.
Thefastest system processes at about 500 words perminute whereas the slowest system reaches only 50words per minute.2.3 Eva luat ing  the  tes tsAfter all the systems had processed the sentencelists, the resulting documents were merged for ease114of inspection.
Every source sentence was groupedtogether with all its translations.
Example 4 showsthe English adjective hard (frequency rank 41) withits translations.41.
This is hard.41.
G. Assistant Dieser ist hart.41.
Lang.
T1 Dies ist schwierig.
(4) 41.
Personal Tr.
dies ist schwer.41.
Power Tr.
Dieses ist hart.41.
Systran Dieses ist hart.41.
Telegraph Dies ist hart.Note that the 6 MT systems give three differenttranslations for hard all of which are correct given anappropriate context.
It is also interesting to see thatthe demonstrative pronoun this is translated into dif-ferent forms of its equivalent pronoun in German.These sentence groups must then be checked man-ually to determine whether the given translation iscorrect.
The translated sentences were annotatedwith one of the following tags:u (unknown word) The source word is unknownand is inserted into the translation.
Seldom:The source word is a compound, part of which isunknown and inserted into the translation (thewarm-heartedness : das warme heartedness).w (wrong t rans la t ion)The  source word is in-correctly translated either because of an in-correct segmentation of a compound (spot-on: erkennen-auf/Stelle-auf instead of haarge-nau/exakt) or (seldom) because of an incor-rect lexicon entry (would : wiirdelen instead ofwiirden).m (missing word) The source word is not trans-lated at all and is missing in the target sentence.wf (wrong form) The source word was found inthe lexicon, but it is translated in an inappro-priate form (e.g.
it was translated as a verb al-though it must be a noun) or at least in an un-expected form (e.g.
it appears with duplicatedparts (windscreen-wiper : Windschutzscheiben-scheibenwischer) ).s (sense preserv ing ly  segmented)  Thesource word was segmented and the units weretranslated.
The translation is not correct butthe meaning of the source word ~an be inferred(unreasonableness : Vernunfllos-heit instead ofVnvernunft).f (missing interfix (nouns only))The source word was segmented into units andcorrectly translated.
But the resulting Germancompound is missing an interfix (windscreen-wiper : Windschutzscheibe- Wischer).wd (wrong determiner (nouns only))The source word was correctly translated butcomes with an incorrect determiner (wristband: die Handgelenkband instead of das Handge-lenkband).c (correct)  The translation is correct.Out of these tags only u can be inserted auto-matically when the target sentence word is identicalwith the source word.
Some of the tested translationsystems even mark an unknown word in the targetsentence with special symbols.
All other tags hadto be manually inserted.
Some of the low frequencyitems required extensive dictionary look-up to verifythe decision.
After all translations had been tagged,the tags were checked for consistency and automat-ically summed up.3 Resu l t s  o f  our  eva luat ionThe MT systems under investigation translate be-tween English and German and we employed ourevaluation method for both translation directions.Here we will report on the results for translatingfrom English to German.
First, we will try to an-swer the question of what percentage of the testwords was t rans lated at all (correctly or incor-rectly).
This figure is obtained by taking the un-known words as negative counts and all others aspositive counts.
We thus obtained the triples in ta-ble 1.
The first number in a triple is the percentageof positive counts in the high frequency class, thesecond number is the percentage of positive countsin the medium frequency class, and the third num-ber is the percentage of positive counts in the lowfrequency class.In table 1 we see immediately that there were nounknown words in the high frequency class for anyof the systems.
The figures for the medium and lowfrequency classes require a closer look.
Let us ex-plain what these figures mean, taking the GermanAssistant as an example: 14 adjectives (14 nouns, 21verbs) of the medium frequency class were unknown,resulting in 86% adjectives (86% nouns, 79% verbs)getting a translation.
In the low frequency class 49adjectives, 53 nouns, and 61 verbs got a translation.The average is computed as the mean value overthe three word classes.
Comparing the systems'averages we can observe that Personal Translatorscores highest for all frequency classes.
Langenschei-dts T1 and Telegraph are second best with about the115G.
Assistant Lang.
T1 Personal Tr.
Power Tr.
Systran Telegraphadjectives 100/86/49 100/98/66 100/95/84 100/87/54 100/49/31 100/97/59nouns 100/86/53 100/91/62 100/97/78 100/83/53 100/59/32 100/94/63verbs 100/79/61 100/97/73 100/97/88 100/84/55 100/61/37 100/93/75average 100/84/54 100/95/67 100/96/83 100/85/54 100/56/33 100/95/66Table 1: Percentage of words translated correctly or incorrectlyG.
Assistant Lang.
T1 Personal Tr.
Power Tr.
Systran Telegraphadjectivesnounsverbsaverage100/79/2499/83/3897/78/5099/80/37100/92/36100/88/5099/93/59100/91/48100/94/77100/95/74100/97/86100/95/79100/86/49100/81/47100/84/50100/84/49100/47/23100/57/27100/61/33lOO/55/28100/96/53100/92/53100/93/73'\[!I!/mt~Table 2: Percentage of correctly translated wordssame scores.
German Assistant and Power Transla-tor rank third while Systran clearly has the lowestscores.
This picture becomes more detailed when welook at the second question.The second question is about the percentage ofthe test words that are cor rect ly  t rans la ted .
Forthis, we took unknown words, wrong translations,and missing words as negative counts and all othersas positive counts.
Note that our judgement doesnot say that a word is translated correctly in a givencontext.
It merely states that a word is translatedin a way that is understandable in some context.Table 2 gives additional evidence that PersonalTranslator has the most elaborate lexicon for Englishto German translation while German Assistant andSystran have the least elaborate.
Telegraph is onsecond position followed by Langenscheidts T1 andPower Translator.
We can also observe that thereare only small differences between the figures in ta-ble 1 and table 2 as far as the high and mediumfrequency classes are concerned.
But there are dif-ferences of up to 30% for the low frequency class.This means that we will get many wrong transla-tions if a word is not included in the lexicon and hasto be segmented for translation.While annotating sentences with the tags we ob-served that verbs obtained many 'wrong form' judge-ments (20% and more for the low frequency class).This is probably due to the fact that many Englishverbs in the low frequency class are rare uses of ho-mograph nouns (e.g.
to keyboard, to pitchfork, to sec-tion).
If we omit the 'wrong form' tags from the posi-tive count (i.e.
we accept only words that are correct,sense preservingly segmented, or close to correct be-cause of minor orthographical mistakes) we obtainthe figures in table 3.In this table we can see even clearer the wide cov-erage of the Personal Translator lexicon because thesystem correctly recognizes around 70% of all lowfrequency words while all the other systems figurearound 40% or less.
It is also noteworthy that theSystran results differ only slightly between table 2and table 3.
This is due to the fact that Systrandoes not give many wrong form (wf) translations.Systran does not offer a translation of a word if it isin the lexicon with an inappropriate part of speech.So, if we try to translate the sentence in example 5Systran will not offer a translation although keyboardas a noun is in the lexicon.
All the other systems givethe noun reading in such cases.
(5) We keyboard.So the difference between the figures in tables 2and 3 gives an indication of the precision that wecan expect when the translation system deals withinfrequent words.
The smaller the difference, themore often the system will provide the correct partof speech (if it translates at all).3.1 Some observat ionsNLP systems can widen the coverage of their lexiconconsiderably if they employ word-building processeslike composition and derivation.
Especially deriva-tion seems a useful module for MT systems ince themeaning shift in derivation is relatively predictableand therefore the derivation process can be recreatedin the target language in most cases.It is therefore surprising to note that all systemsin our test seem to lack an elaborate derivation mod-ule.
All of them know the noun weapon but none isable to translate weaponless, although the Englishderivation suffix -less has an equivalent in German116adjectivesnounsverbsG.
Assistant90/72/2198/80/3097/63/16Lang.
T197/74/28100/83/4497/85/26Personal Tr.99/92/69100/94/7399/91/67Power Tr.92/75/4398/77/44100/76/22Systran97/43/21100/55/24100/53/13Telegraph92/84/4499/90/4699/86/41average 95/72/22 98/81/33 99/92/70 97/76/36 99/50/19 97/87/44Table 3: Percentage of correctly translated words (without 'wrong forms')o Assistant I L ng Ti Personal I Power I Systr n I Telegraph Iwd-nouns 8 2 - 7 0 2Table 4: Number of incorrect gender assignments-los.
German Assistant treats this word as a com-pound and incorrectly translates it as Waffe-weniger(engl.
less weapon).
Due to the lack of derivationmodules, words like uneventful, unplayable, tearless,or thievish are either in the lexicon or they are nottranslated.
Traces of a derivational process based onprefixes have been found for Langenscheidts T1 andfor Personal Translator.
They use the derivationalprefix re- to translate English reorient as Germanorientieren wieder which is not correct but can beregarded as sense preserving.On the other hand all systems employ segmen-tation on unknown compounds.
Example 6 showsthe different ranslations for a compound noun.
Themarker 'M' in the Langenscheidts T1 translation in-dicates that the translation has been found via com-pound segmentation.
While Springpferd, Turnpferdor simply Pferd could count as correct ranslations ofvaulting-horse, Springen-Pferd can still be regardedas sense-preservingly segmented.English: vaulting-horse(6)G. Assistant Gewblbe-Pferd wLang.
T1 (M\[Springpferd\]) cPersonal Tr.
Wblbungspferd wPower Tr.
Springen - Pferd sSystran Vaultingpferd uTelegraph Gewblbe-Kavallerie wAn example of a verb compound that gets a trans-lation via segmentation is t0 tap-dance and an adjec-tive compound example is sweet-scented.
All of theseexamples are hyphenated compounds.
If we lookat compounds that form an orthographic unit likevestryman, waterbird we can only find evidence forsegmentations by Langenscheidts T1 and GermanAssistant.
These findings only relate to translatingfrom English to German.
Working in the oppositedirection all systems perform segmentatiqn of ortho-graphic unit compounds since this is a very commonfeature of German.As another side effect we used the lexicon evalua-tion to check for agreement within the noun phrase.Translating from English to German the MT systemhas to get the gender of the German noun from thelexicon since it cannot be derived from the Englishsource.
We can check if these nouns get the cor-rect gender assignment if we look at the form of thedeterminer.
Table 4 gives the number of incorrectdeterminer selections (over all frequency classes).Since gender assignment in choosing the deter-miner is such a basic operation all systems are able todo this in most cases.
But in particular if noun com-pounds are segmented and the translation is synthe-sized this operation sometimes fails.
Personal Trans-lator does not give a determiner form in these cases.It simply gives the letter 'd' as the beginning letterof all three different forms (der, die, das).3.2 Compar ing  t rans la t ion  d i rect ionsComparing the results for English to German trans-lation with German to English is difficult becauseof the different corpora used for the CELEX fre-quencies.
Especially it is not evident whether ourmedium frequency (25 occurrences) leads to wordsof similar prominence in both languages.
Neverthe-less our results indicate that some systems focus oneither of the two translation directions and there-fore have a more elaborate lexicon in one direction.This can be concluded since these systems how big-ger differences than the others.
For instance, Tele-graph, Systran and Langenscheidts T1 score muchbetter for German to English.
For Telegraph therate of unknown words dropped by 2% for mediumfrequency and by 12% for low frequency, tbr Systranthe same rate dropped by 36% for medium frequencyand by 33% for low frequency words, and for Lan-genscheidts T1 the rate dropped by 1% for mediumfrequency and by 16% for low frequency.
The latter117reflects the figures in the Langenscheidts T1 man-ual, where they report an inbalance in the lexiconof 230'000 entries for German to English and 90'000entries for the opposite direction.
Personal Transla-tor again ranks among the systems with the widestcoverage while German Assistant shows the smallestcoverage.4 Conc lus ionsAs more translation systems become available thereis an increasing demand for comparative evaluations.The method for checking lexical coverage as intro-duced in this paper is one step in this direction.
Tak-ing the most frequent adjectives, nouns, and verbs isnot very informative and mostly serves to anchor themethod.
But medium and low frequency words givea clear indication of the underlying relative lexiconsize.
Of course, the introduced method cannot claimthat the relative lexicon sizes correspond exactly tothe computed percentages.
For this the test sampleis too small.
The method provides a plausible hy-pothesis but it cannot prove in a strict sense thatone lexicon necessarily is bigger than another.
Aproof, however, cannot be expected from any black-box testing method.We mentioned above that some systems ubclas-sify their lexical entries according to subject areas.They do this to a different extent.Langenscheidts  T1 has a total of 55 subject ar-eas.
They are sorted in a hierarchy which isthree levels deep.
An example is Technologywith its subfields Space Technology, Food Tech-noloy, Technical Norms etc.
Multiple ~ subjectareas from different levels can be selected andprioritized.Personal  Trans lator  has 22 subject areas.
Theyare all on the same level.
Examples are: Biol-ogy, Computers, Law, Cooking.
Multiple selec-tions can be made, but they cannot be priori-tized.Power  Trans lator  and Telegraph do not comewith built-in subject dictionaries but these canbe purchased separately and added to the sys-tem.Syst ran  has 22 "Topical Glossaries", all on thesame level.
Examples are: Automotive, Avi-ation/Space, Chemistry.
Multiple subject areascan be selected and prioritized.Our tests were run without any selection of a sub-ject area.
We tried to check if a lexicon entry thatis marked with a subject area will still be found ifno subject area is selected.
This check can only beperformed reliably for Langenscheidt T1 since this isthe only system that makes the lexicon transparentto the user to the point that one can access the sub-ject area of every entry.
Personal Translator onlyallows to look at an entry and its translation op-tions, but not at its subject marker, and Systrandoes not allow any access to the built-in lexicon.For Langenscheidts T1 we tested the word compilerwhich is marked with data processing and computersoftware.
This lexical entry does not have any read-ing without a subject area marker, but the word isstill found at translation if no subject area is chosen.That means that a subject area, if chosen, is used asdisambiguator, but if translating without a subjectarea the system has access to the complete lexicon.In this respect our tests have put Power Translatorand Telegraph at a disadvantage since we did notextend their lexicons with any add-on lexicons.
Onlytheir built-in lexicons were evaluated here.Of course, lexical coverage by itself does not guar-antee a good translation.
It is a necessary but not asufficient condition.
It must be complemented withlexical depth and grammatical coverage.
Lexiealdepth can be evaluated in two dimensions.
The firstdimension describes the number of readings avail-able for an entry.
A look at some common nounsthat received ifferent ranslations from our test sys-tems reveals that there are big differences in this di-mension which are not reflected by our test results.Table 7 gives the number of readings for the wordorder ('N' standing for noun readings, 'V' for ver-bal, 'Prep' for prepositional, and 'Phr' for phrasalreadings).G.
Assistant 9 N 3 VLang.
T1 4 N 4 VPersonal Tr.
6 N 5 V(7) Power Tr.
1 N 1 VSystran n.a.Telegraph 10 N 4 V1 Prep1 Prep2 PhrThere is no information for Systran since the built-in lexicon cannot be accessed.
German Assistantcontains awide variety of readings although it scoredbadly in our tests.
Power Translator on the contrarygives only the most likely readings.
Still, there re-mains the question of whether a system is able topick the most appropriate reading in a given con-text, which brings us to the second dimension.The second dimension of lexical depth is aboutthe amount of syntactic and semantic knowledge at-tributed to every reading.
This also varies a greatdeal.
Telegraph offers 16 semantic features (ani-118mate, time, place etc.
), German Assistant 9 andLangenscheidts T1 5.
Power Translator offers fewsemantic features for verbs (movement, direction).The fact that these features are available does notentail that they are consistenly set at every appro-priate reading.
And even if they are set, it does notfollow that they are all optimally used during thetranslation process.To check these lexicon dimensions new tests needto be developped.
We think that it is especiallytricky to get to all the readings along the first di-mension.
One idea is to use the example sentenceslisted with the different readings in a comprehen-siveprint dictionary.
If these sentences are carefullydesigned they should guide an MT system to therespective translation alternatives.Our method for determining lexical coverage couldbe refined by looking at more frequency classes (e.g.an additional class between medium and low fre-quency).
But since the results of working with onemedium and one low frequency class show clear dis-tinctions between the systems, it is doubtful thatthe additional cost of taking more classes will pro-vide significantly better figures.The method as introduced in this paper requiresextensive manual abor in checking the translationresults.
Carefully going through 900 words each for6 systems including dictionary look-up for unclearcases takes about 2 days time.
This could be reducedby automatically accessing translation lists or reli-able bilingual dictionaries.
Judging sense-preservingsegmentations or other close to correct ranslationsmust be left over to the human expert.A special purpose translation list could be incre-mentally built up in the following manner.
For thefirst system all 900 words will be manually checked.All translations with their tags will be entered intothe translation list.
For the second system only thosewords will be checked where the translation differsfrom the translation saved in the translation list.Every new judgement will be added to the transla-tion list for comparison with the next system's trans-lations.5 AcknowledgementsI would like to thank Dominic A. Merz for his helpin performing the evaluation and for many helpfulsuggestions on earlier versions of the paper.Linguistic Data Consortium, University of Penn-sylvania.Falkedal, Kirsten.
1991.
Evaluation Methodsfor Machine Translation Systems.
An historicaloverview and a critical account.
ISSCO.
Univer-sity of Geneva.
Draft Report.Flanagan, Mary A.
1994.
Error classification forMT evaluation.
In Technology partnerships forcrossing the language barrier: Proceedings of the1st Conference of the Association for MachineTranslation in the Americas, pages 65-71, Wash-ington,DC.
Association for Machine Translationin the Americas.Landau, Sidney I.
1989.
Dictionaries.
The art andcraft of lexicography.
Cambridge University Press,Cambridge.
first published 1984.King, Margaret.
1996.
Evaluating natural anguageprocessing systems.
CACM, 39(1):73-79.Minnis, Stephen.
1994.
A simple and practicalmethod for evaluating machine translation qual-ity.
Machine Translation, 9(2):133-149.Rinsche, Adriane.
1993.
Evaluationsverfahren fiirmaschinelle ~)bersetzungssysteme - zur Methodikund experimentellen Praxis.
Kommission derEurop~ischen Gemeinschaften, GeneraldirektionXIII; Informationstechnologien, Informationsin-dustrie und Telekommunikation, Luxemburg.Nerbonne, J., K. Netter, A.K.
Diagne, L. Dickmann,and J. Klein.
1993.
A diagnostic tool for ger-man syntax.
Machine Translation (Special Issueon Evaluation of MT Systems), (also as DFKI Re-search Report RR-91-18), 8(1-2):85-108.Sparck-Jones, K. and J.R. Galliers.
1995.
Evalu-ating Natural Language Processing Systems.
AnAnalysis and Review.
Number 1083 in LectureNotes in Artificial Intelligence.
Springer Verlag,Berlin.Volk, Martin.
1995.
Einsatz einer Testsatzsamm-lung im Grammar Engineering, volume 30 ofSprache und Information.
Niemeyer Verlag,Tiibingen.ReferencesBaayen, R. H., R. Piepenbrock, and H. van Rijn.1995.
The CELEX lexical database (CD-ROM).119
