Proceedings of NAACL-HLT 2013, pages 1000?1009,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsCombining Heterogeneous Models for Measuring Relational SimilarityAlisa Zhila?Instituto Politecnico NacionalMexico City, Mexicoalisa.zhila@gmail.comWen-tau Yih Christopher MeekMicrosoft ResearchRedmond, WA 98052, USA{scottyih,meek}@microsoft.comGeoffrey ZweigMicrosoft ResearchRedmond, WA 98052, USAgzweig@microsoft.comTomas Mikolov?BRNO University of TechnologyBRNO, Czech Republictmikolov@gmail.comAbstractIn this work, we study the problem of mea-suring relational similarity between two wordpairs (e.g., silverware:fork and clothing:shirt).Due to the large number of possible relations,we argue that it is important to combine mul-tiple models based on heterogeneous informa-tion sources.
Our overall system consists oftwo novel general-purpose relational similar-ity models and three specific word relationmodels.
When evaluated in the setting of arecently proposed SemEval-2012 task, our ap-proach outperforms the previous best systemsubstantially, achieving a 54.1% relative in-crease in Spearman?s rank correlation.1 IntroductionThe problem of measuring relational similarity isto determine the degree of correspondence betweentwo word pairs.
For instance, the analogous wordpairs silverware:fork and clothing:shirt both exem-plify well a Class-Inclusion:Singular Collective re-lation and thus have high relational similarity.
Un-like the problem of attributional similarity, whichmeasures whether two words share similar attributesand is addressed in extensive research work (Bu-danitsky and Hirst, 2006; Reisinger and Mooney,2010; Radinsky et al 2011; Agirre et al 2009; Yihand Qazvinian, 2012), measuring relational similar-ity is a relatively new research direction pioneeredby Turney (2006), but with many potential appli-cations.
For instance, problems of identifying spe-cific relations between words, such as synonyms,?Work conducted while interning at Microsoft Research.antonyms or associations, can be reduced to mea-suring relational similarity compared to prototypicalword pairs with the desired relation (Turney, 2008).In scenarios like information extraction or questionanswering, where identifying the existence of cer-tain relations is often the core problem, measuringrelational similarity provides a more flexible solu-tion rather than creating relational classifiers for pre-defined or task-specific categories of relations (Tur-ney, 2006; Jurgens et al 2012).In order to promote this research direction, Ju-rgens et al(2012) proposed a new shared task ofmeasuring relational similarity in SemEval-2012 re-cently.
In this task, each submitted system is re-quired to judge the degree of a target word pairhaving a particular relation, measured by its re-lational similarity compared to a few prototypicalexample word pairs.
The system performance isevaluated by its correlation with the human judg-ments using two evaluation metrics, Spearman?srank correlation and MaxDiff accuracy (more de-tails of the task and evaluation metrics will be givenin Sec.
3).
Although participating systems incorpo-rated substantial amounts of information from lex-ical resources (e.g., WordNet) and contextual pat-terns from large corpora, only one system (Rink andHarabagiu, 2012) is able to outperform a simplebaseline that uses PMI (pointwise mutual informa-tion) scoring, which demonstrates the difficulty ofthis task.In this paper, we explore the problem of mea-suring relational similarity in the same task setting.We argue that due to the large number of possiblerelations, building an ensemble of relational simi-1000larity models based on heterogeneous informationsources is the key to advance the state-of-the-art onthis problem.
By combining two general-purpose re-lational similarity models with three specific word-relation models covering relations like IsA and syn-onymy/antonymy, we improve the previous state-of-the-art substantially ?
having a relative gain of54.1% in Spearman?s rank correlation and 14.7% inthe MaxDiff accuracy!Our main contributions are threefold.
First, wepropose a novel directional similarity method basedon the vector representation of words learned froma recurrent neural network language model.
The re-lation of two words is captured by their vector off-set in the latent semantic space.
Similarity of rela-tions can then be naturally measured by a distancefunction in the vector space.
This method alonealready performs better than all existing systems.Second, unlike the previous finding, where SVMslearn a much poorer model than naive Bayes (Rinkand Harabagiu, 2012), we show that using a highly-regularized log-linear model on simple contextualpattern features collected from a document collec-tion of 20GB, a discriminative approach can learn astrong model as well.
Third, we demonstrate that byaugmenting existing word-relation models, whichcover only a small number of relations, the overallsystem can be further improved.The rest of this paper is organized as follows.
Wefirst survey the related work in Sec.
2 and formallydefine the problem in Sec.
3.
We describe the indi-vidual models in detail in Sec.
4.
The combinationapproach is depicted in Sec.
5, along with experi-mental comparisons to individual models and exist-ing systems.
Finally, Sec.
6 concludes the paper.2 Related WorkBuilding a classifier to determine whether a relation-ship holds between a pair of words is a natural ap-proach to the task of measuring relational similarity.While early work was mostly based on hand-craftedrules (Finin, 1980; Vanderwende, 1994), Rosarioand Hearst (2001) introduced a machine learning ap-proach to classify word pairs.
They targeted clas-sifying noun modifier pairs from the medical do-main into 13 classes of semantic relations.
Fea-tures for each noun modifier pair were constructedusing large medical lexical resources and a multi-class classifier was trained using a feed-forward neu-ral network with one hidden layer.
This work waslater extended by Nastase and Szpakowicz (2003)to classify general domain noun-modifier pairs into30 semantic relations.
In addition to extracting fea-tures using WordNet and Roget?s Thesaurus, theyalso experimented with several different learners in-cluding decision trees, memory-based learning andinductive logic programming methods like RIPPERand FOIL.
Using the same dataset as in (Nastaseand Szpakowicz, 2003), Turney and Littman (2005)created a 128-dimentional feature vector for eachword pair based on statistics of their co-occurrencepatterns in Web documents and applied the k-NNmethod (k = 1 in their work).Measuring relational similarity, which determineswhether two word pairs share the same relation, canbe viewed as an extension of classifying relationsbetween two words.
Treating a relational similar-ity measure as a distance metric, a testing pair ofwords can be judged by whether they have a rela-tion that is similar to some prototypical word pairshaving a particular relation.
A multi-relation clas-sifier can thus be built easily in this framework asdemonstrated in (Turney, 2008), where the prob-lems of identifying synonyms, antonyms and asso-ciated words are all reduced to finding good anal-ogous word pairs.
Measuring relational similarityhas been advocated and pioneered by Turney (2006),who proposed a latent vector space model for an-swering SAT analogy questions (e.g., mason:stonevs.
carpenter:wood).
In contrast, we take a slightlydifferent view when building a relational similaritymeasure.
Existing classifiers for specific word re-lations (e.g., synonyms or Is-A) are combined withgeneral relational similarity measures.
Empirically,mixing heterogeneous models tends to make the fi-nal relational similarity measure more robust.Although datasets for semantic relation classifica-tion or SAT analogous questions can be used to eval-uate a relational similarity model, their labels are ei-ther binary or categorical, which makes the datasetssuboptimal for determining the quality of a modelwhen evaluated on instances of the same relationclass.
As a result, Jurgens et al(2012) proposed anew task of ?Measuring Degrees of Relational Simi-larity?
at SemEval-2012, which includes 79 relation1001categories exemplified by three or four prototypicalword pairs and a schematic description.
For exam-ple, for the Class-Inclusion:Taxonomic relation, theschematic description is ?Y is a kind/type/instanceof X?.
Using Amazon Mechanical Turk1, they col-lected word pairs for each relation, as well as theirdegrees of being a good representative of a partic-ular relation when compared with defining exam-ples.
Participants of this shared task proposed var-ious kinds of approaches that leverage both lexicalresources and general corpora.
For instance, theDuluth systems (Pedersen, 2012) created word vec-tors based on WordNet and estimated the degree ofa relation using cosine similarity.
The BUAP sys-tem (Tovar et al 2012) represented each word pairas a whole by a vector of 4 different types of fea-tures: context, WordNet, POS tags and the aver-age number of words separating the two words intext.
The degree of relation was then determinedby the cosine distance of the target pair from theprototypical examples of each relation.
Althoughtheir models incorporated a significant amount ofinformation of words or word pairs, unfortunately,the performance were not much better than a ran-dom baseline, which indicates the difficulty of thistask.
In comparison, a supervised learning approachseems more promising.
The UTD system (Rink andHarabagiu, 2012), which mined lexical patterns be-tween co-occurring words in the corpus and thenused them as features to train a Naive Bayes classi-fier, achieved the best results.
However, potentiallydue to the large feature space, this strategy did notwork as well when switching the learning algorithmto SVMs.3 Problem Definition & Task DescriptionFollowing the setting of SemEval-2012 Task 2 (Ju-rgens et al 2012), the problem of measur-ing the degree of relational similarity is to rateword pairs by the degree to which they areprototypical members of a given relation class.For instance, comparing to the prototypical wordpairs, {cutlery:spoon, clothing:shirt, vermin:rat} ofthe Class-Inclusion:Singular Collective relation, wewould like to know among the input word pairs{dish:bowl, book:novel, furniture:desk}, which one1http://www.mturk.combest demonstrates the relation.Because our approaches are evaluated using thedata provided in this SemEval-2012 task, we de-scribe briefly below how the data was collected, aswell as the metrics used to evaluate system perfor-mance.
The dataset consists of 79 relation classesthat are chosen according to (Bejar et al 1991)and broadly fall into 10 main categories, includ-ing Class-Inclusion, Part-Whole, Similar and more.With the help of Amazon Mechanical Turk, Jurgenset al(2012) used a two-phase approach to collectword pairs and their degrees.
In the first phase,a lexical schema, such as ?a Y is one item in acollection/group of X?
for the aforementioned rela-tion Class-Inclusion:Singular Collective, and a fewprototypical pairs for each class were given to theworkers, who were asked to provide approximatelya list of 40 word pairs representing the same rela-tion class.
Naturally, some of these pairs were bet-ter examples than the others.
Therefore, in the sec-ond phase, the goal was to measure the degree oftheir similarity to the corresponding relation.
Thiswas done using the MaxDiff technique (Louviereand Woodworth, 1991).
For each relation, about onehundred questions were first created.
Each questionconsists of four different word pairs randomly sam-pled from the list.
The worker was then asked tochoose the most and least representative word pairsfor the specific relation in each question.The set of 79 word relations were randomly splitinto training and testing sets.
The former contains10 relations and the latter has 69.
Word pairs in all79 relations were given to the task participants in ad-vance, but only the human judgments of the trainingset were available for system development.
In thiswork, we treat the training set as the validation set?
all the model exploration and refinement is doneusing this set of data, as well as the hyper-parametertuning when learning the final model combination.The quality of a relational similarity measure isestimated by its correlation to human judgments.This is evaluated using two metrics in the task: theMaxDiff accuracy and Spearman?s rank correlationcoefficient (?).
A system is first asked to pick themost and least representative word pairs of eachquestion in the MaxDiff setting.
The average accu-racy of the predictions compared to the human an-swers is then reported.
In contrast, Spearman?s ?1002measures the correlation between the total orderingsof all word pairs of a relation, where the total order-ing is derived from the MaxDiff answers (see (Jur-gens et al 2012) for the exact procedure).4 Models for Relational SimilarityWe investigate three types of models for relationalsimilarity.
Operating in a word vector space, the di-rectional similarity model compares the vector dif-ferences of target and prototypical word pairs to es-timate their relational similarity.
The lexical pat-tern method collects contextual information of pairsof words when they co-occur in large corpora, andlearns a highly regularized log-linear model.
Finally,the word relation models incorporate existing, spe-cific word relation measures for general relationalsimilarity.4.1 Directional Similarity ModelOur first model for relational similarity extends pre-vious work on semantic word vector representa-tions to a directional similarity model for pairs ofwords.
There are many different methods for cre-ating real-valued semantic word vectors, such asthe distributed representation derived from a wordco-occurrence matrix and a low-rank approxima-tion (Landauer et al 1998), word clustering (Brownet al 1992) and neural-network language model-ing (Bengio et al 2003; Mikolov et al 2010).
Eachelement in the vectors conceptually represents somelatent topicality information of the word.
The goalof these methods is that words with similar mean-ings will tend to be close to each other in the vectorspace.Although the vector representation of singlewords has been successfully applied to problemslike semantic word similarity and text classifica-tion (Turian et al 2010), the issue of how to repre-sent and compare pairs of words in a vector spaceremains unclear (Turney, 2012).
In a companionpaper (Mikolov et al 2013), we present a vectoroffset method which performs consistently well inidentifying both syntactic and semantic regularities.This method measures the degree of the analogy?a is to b as c is to d?
using the cosine score of(~vb?~va +~vc, ~vd), where a, b, c, d are the four givenwords and ~va, ~vb, ~vc, ~vd are the corresponding vec-qshirtclothingfurnituredeskv1v2'v2'Figure 1: Directional vectors ?1 and ?2 capture the rela-tions of clothing:shirt and furniture:desk respectively inthis semantic vector space.
The relational similarity ofthese two word pairs is estimated by the cosine of ?.tors.
In this paper, we propose a variant called thedirectional similarity model, which performs bet-ter for semantic relations.
Let ?i = (wi1 , wi2) and?j = (wj1 , wj2) be the two word pairs being com-pared.
Suppose (~vi1 , ~vi2) and (~vj1 , ~vj2) are the cor-responding vectors of these words.
The directionalvectors of ?i and ?j are defined as ~?i ?
~vi2 ?
~vi1and ~?j ?
~vj2 ?
~vj1 , respectively.
Relational simi-larity of these two word pairs can be measured bysome distance function of ?i and ?j , such as the co-sine function:~?i ?
~?j?~?i?
?~?j?The rationale behind this variant is as follows.
Be-cause the difference of two word vectors reveals thechange from one word to the other in terms of mul-tiple topicality dimensions in the vector space, twoword pairs having similar offsets (i.e., being rela-tively parallel) can be interpreted as they have simi-lar relations.
Fig.
1 further illustrates this method.Compared to the original method, this variantplaces less emphasis on the similarity betweenwords wj1 and wj2 .
That similarity is necessaryfor syntactic relations where the words are often re-lated by morphology, but not for semantic relations.On semantic relations studied in this paper, the di-rectional similarity model performs about 18% rela-tively better in Spearman?s ?
than the original one.The quality of the directional similarity methoddepends heavily on the underlying word vectorspace model.
We compared two choices with dif-1003Word Embedding Spearman?s ?
MaxDiff Acc.
(%)LSA-80 0.055 34.6LSA-320 0.066 34.4LSA-640 0.102 35.7RNNLM-80 0.168 37.5RNNLM-320 0.214 39.1RNNLM-640 0.221 39.2RNNLM-1600 0.234 41.2Table 1: Results of measuring relational similarity usingthe directional similarity method, evaluated on the train-ing set.
The 1600-dimensional RNNLM vector spaceachieves the highest Spearman?s ?
and MaxDiff accuracy.ferent dimensionality settings: the word embeddinglearned from the recurrent neural network languagemodel (RNNLM)2 and the LSA vectors, both weretrained using the same Broadcast News corpus of320M words as described in (Mikolov et al 2011).All the word vectors were first normalized to unitvectors before applying the directional similaritymethod.
Given a target word pair, we computedits relational similarity compared with the prototyp-ical word pairs of the same relation.
The averageof these measurements was taken as the final modelscore.
Table 1 summarizes the results when evalu-ated on the training set.
As shown in the table, theRNNLM vectors consistently outperform their LSAcounterparts with the same dimensionality.
In addi-tion, more dimensions seem to preserve more infor-mation and lead to better performance.
Therefore,we take the 1600-dimensional RNNLM vectors toconstruct our final directional similarity model.4.2 Lexical Pattern ModelOur second model for measuring relational similar-ity is built based on lexical patterns.
It is well-knownthat contexts in which two words co-occur often pro-vide useful cues for identifying the word relation.For example, having observed frequent text frag-ments like ?X such as Y?, it is likely that there is aClass-Inclusion:Taxonomic relation between X andY; namely, Y is a type of X.
Indeed, by mining lexicalpatterns from a large corpus, the UTD system (Rinkand Harabagiu, 2012) managed to outperform otherparticipants in the SemEval-2012 task of measuringrelational similarity.2http://www.fit.vutbr.cz/?imikolov/rnnlmIn order to find more co-occurrences of each pairof words, we used a large document set that con-sists of the Gigaword corpus (Parker et al 2009),Wikipedia and LA Times articles3, summing up tomore than 20 Gigabytes of texts.
For each wordpair (w1, w2) that co-occur in a sentence, we col-lected the words in between as its context (or so-called ?raw pattern?).
For instance, ?such as?
wouldbe the context extracted from ?X such as Y?
forthe word pair (X, Y).
To reduce noise, contexts withmore than 9 words were dropped and 914,295 pat-terns were collected in total.Treating each raw pattern as a feature where thevalue is the logarithm of the occurrence count, wethen built a probabilistic classifier to determine theassociation of the context and relation.
For each re-lation, we treated all its word pairs as positive ex-amples and all the word pairs in other relations asnegative examples4.
79 classifiers were trained intotal, where each one was trained using 3,218 ex-amples.
The degree of relational similarity of eachword pair can then be judged by the output of thecorresponding classifier5.
Although this seems like astandard supervised learning setting, the large num-ber of features poses a challenge here.
Using almost1M features and 3,218 examples, the model couldeasily overfit if not regularized properly, which mayexplain why learning SVMs on pattern features per-formed poorly (Rink and Harabagiu, 2012).
In-stead of employing explicit feature selection meth-ods, we used an efficient L1 regularized log-linearmodel learner (Andrew and Gao, 2007) and chosethe hyper-parameters based on model performanceon the training data.
The final models we chosewere trained with L1 = 3, where 28,065 featuresin average were selected automatically by the algo-3We used a Nov-2010 dump of English Wikipedia, whichcontains approximately 917M words after pre-processing.
TheLA Times corpus consists of articles from 1985 to 2002 and hasabout 1.1B words.4Given that not all word pairs belonging to the same relationcategory are equally good, removing those with low judgmentscores may help improve the quality of the labeled data.
Weleave this study to future work.5Training a separate classifier for each MaxDiff question us-ing all words pairs except the four target pairs appears to be abetter setting, as it would avoid including the target pairs in thetraining process.
We did not use this setting because it is morecomplicated and performed roughly the same empirically.1004rithm.
The performance on the training data is 0.322in Spearman?s ?
and 41.8% in MaxDiff accuracy.4.3 Word Relation ModelsThe directional similarity and lexical pattern mod-els can be viewed as general purpose methods forrelational similarity as they do not differentiate thespecific relation categories.
In contrast, for specificword relations, there exist several high-quality meth-ods.
Although they are designed for detecting spe-cific relations between words, incorporating themcould still improve the overall results.
Next, we ex-plore the use of some of these word relation mod-els, including information encoded in the knowledgebase and a lexical semantic model for synonymy andantonymy.4.3.1 Knowledge BasesPredetermined types of relations can often befound in existing lexical and knowledge databases,such as WordNet?s Is-A taxonomy and the exten-sive relations stored in the NELL (Carlson et al2010) knowledge base.
Although in theory, theseresources can be directly used to solve the problemof relational similarity, such direct approaches oftensuffer from two practical issues.
First, the word cov-erage of these databases is usually very limited andit is common that the relation of a given word pairis absent.
Second, the degree of relation is often notincluded, which makes the task of measuring the de-gree of relational similarity difficult.One counter example, however, is Probase (Wuet al 2012), which is a knowledge base that es-tablishes connections between more than 2.5 mil-lion concepts discovered automatically from theWeb.
For the Is-A and Attribute relations it en-codes, Probase also returns the probability that twoinput words share the relation, based on the co-occurrence frequency.
We used some relations inthe training set to evaluate the quality of Probase.For instance, its Is-A model performs exception-ally well on the relation Class-Inclusion:Taxonomic,reaching a high Spearman?s ?
= 0.642 and MaxD-iff accuracy 55.8%.
Similarly, its Attribute modelperforms better than our lexical pattern modelon Attribute:Agent Attribute-State with Spearman?s?
= 0.290 and MaxDiff accuracy 32.7%.4.3.2 Lexical Semantics MeasuresMost lexical semantics measures focus on the se-mantic similarity or relatedness of two words.
Sinceour task focuses on distinguishing the difference be-tween word pairs in the same relation category.
Thecrude relatedness model does not seem to help in ourpreliminary experimental study.
Instead, we lever-age the recently proposed polarity-inducing latentsemantic analysis (PILSA) model (Yih et al 2012),which specifically estimates the degree of synonymyand antonymy.
This method first forms a signed co-occurrence matrix using synonyms and antonyms ina thesaurus and then generalizes it using a low-rankapproximation derived by SVD.
Given two words,the cosine score of their PILSA vectors tend to benegative if they are antonymous and positive if syn-onymous.
When tested on the Similar:Synonymityrelation, it has a Spearman?s ?
= 0.242 and MaxD-iff accuracy 42.1%, both are better than those of ourdirectional similarity and lexical pattern models.5 Model CombinationIn order to fully leverage the diverse models pro-posed in Sec.
4, we experiment with a model combi-nation approach and conduct a model ablation study.Performance of the combined and individual modelsis evaluated using the test set and compared with ex-isting systems.We seek an optimal linear combination of all theindividual models by treating their output as fea-tures and use a logistic regression learner to learnthe weights6.
The training setting is essentially thesame as the one used to learn the lexical patternmodel (Sec.
4.2).
For each relation, we treat all theword pairs in this relation group as positive exam-ples and all other word pairs as negative ones.
Con-sequently, 79 sets of weights for model combinationare learned in total.
The average Spearman?s ?
of the10 training relations is used for selecting the valuesof the L1 and L2 regularizers7.
Evaluated on the re-maining 69 relations (i.e., the test set), the averageresults of each main relation group and the overall6Nonlinear methods, such as MART (Friedman, 2001), donot perform better in our experiments (not reported here).7We tested 15 combinations, where L1 ?
{0, 0.01, 0.1} andL2 ?
{0, 0.001, 0.01, 1, 10}.
The parameter setting that gavethe highest Spearman rank correlation coefficient score on thetraining set was selected.1005Relation Group Rand.
BUAP DuluthV 0 UTDNB DS Pat.
IsA Attr.
PILSA Com.Class-Inclusion 0.057 0.064 0.045 0.233 0.350 0.422 0.619 -0.137 0.029 0.519Part-Whole 0.012 0.066 -0.061 0.252 0.317 0.244 -0.014 0.026 -0.010 0.329Similar 0.026 -0.036 0.183 0.214 0.254 0.245 -0.020 0.133 0.058 0.303Contrast -0.049 0.000 0.142 0.206 0.063 0.298 -0.012 -0.032 -0.079 0.268Attribute 0.037 -0.095 0.044 0.158 0.431 0.198 -0.008 0.016 -0.052 0.406Non-Attribute -0.070 0.009 0.079 0.098 0.195 0.117 0.036 0.078 -0.093 0.296Case Relations 0.090 -0.037 -0.011 0.241 0.503 0.288 0.076 -0.075 0.059 0.473Cause-Purpose -0.011 0.114 0.021 0.183 0.362 0.234 0.044 -0.059 0.038 0.296Space-Time 0.013 0.035 0.055 0.375 0.439 0.248 0.064 -0.002 -0.018 0.443Reference 0.142 -0.001 0.028 0.346 0.301 0.119 0.033 -0.123 0.021 0.208Average 0.018 0.014 0.050 0.229 0.324?
0.235 0.058?
-0.010?
-0.009?
0.353?Relation Group Rand.
BUAP DuluthV 0 UTDNB DS Pat.
IsA Attr.
PILSA Com.Class-Inclusion 30.1 29.0 26.7 39.1 46.7 43.4 59.6 24.7 32.3 51.2Part-Whole 31.9 35.1 29.4 40.9 43.9 38.1 31.3 29.5 31.0 42.9Similar 31.5 29.1 37.1 39.8 38.5 38.4 30.8 36.3 34.2 43.3Contrast 30.4 32.4 38.3 40.9 33.6 42.2 32.3 31.8 30.1 42.8Attribute 30.2 29.2 31.9 36.5 47.9 38.3 30.7 31.0 28.8 48.3Non-Attribute 28.9 30.4 36.0 36.8 38.7 36.7 32.3 32.8 27.7 42.6Case Relations 32.8 29.5 28.2 40.6 54.3 42.2 32.8 25.7 31.0 50.6Cause-Purpose 30.8 35.4 29.5 36.3 45.3 38.0 30.3 28.1 32.0 41.7Space-Time 30.6 32.5 31.9 43.2 50.0 39.2 33.2 29.3 30.6 47.7Reference 35.1 30.0 31.9 41.2 45.7 36.9 30.4 27.2 30.2 42.5Average 31.2 31.7 32.4 39.4 44.5?
39.2 33.3?
29.8?
30.7?
45.2?Table 2: Average Spearman?s ?
(Top) and MaxDiff accuracy (%) (Bottom) of each major relation group and all 69testing relations.
The best result in each row is highlighted in boldface font.
Statistical significance tests are conductedby comparing each of our systems with the previous best performing system, UTDNB .
?
and ?
indicate the differencein the average results is statistically significant with 95% or 99% confidence level, respectively.results are presented in Table 2.
For comparison, wealso show the performance of a random baseline andthe best performing system of each participant in theSemEval-2012 task.We draw two conclusions from this table.
First,both of our general relational similarity models, thedirectional similarity (DS) and lexical pattern (Pat)models are fairly strong.
The former outperformsthe previous best system UTDNB in both Spear-man?s ?
and MaxDiff accuracy, where the differ-ences are statistically significant8; the latter hascomparable performance, where the differences arenot statistically significant.
In contrast, while theIsA relation from Probase is exceptionally goodin identifying Class-Inclusion relations, with highSpearman?s ?
= 0.619 and MaxDiff accuracy8We conducted a paired-t test on the results of each of the69 relation.
The difference is considered statistically significantif the p-value is less than 0.05.59.6%, it does not have high correlations with hu-man judgments in other relations.
Like in the case ofProbase Attribute and PILSA, specific word-relationmodels individually are not good measures for gen-eral relational similarity.
Second, as expected, com-bining multiple diverse models (Com) is a robuststrategy, which provides the best overall perfor-mance.
It achieves superior results in both evalua-tion metrics compared to UTDNB and only a lowerSpearman?s ?
value in one of the ten relation groups(namely, Reference).
The differences are statisti-cally significant with p-value less than 10?3.In order to understand the interaction among dif-ferent component models, we conducted an ablationstudy by iteratively removing one model from the fi-nal combination.
The weights are re-trained usingthe same procedure that finds the best regularizationparameters with the help of training data.
Table 3summarizes the results and compares them with the1006Spearman?s ?
MaxDiff Accuracy (%)Relation Group Com.
-Attr -IsA -PILSA -DS -Pat Com.
-Attr -IsA -PILSA -DS -PatClass-Inclusion 0.519 0.557 0.467 0.593 0.490 0.570 51.2 53.7 49.2 54.6 49.3 56.2Part-Whole 0.329 0.326 0.335 0.331 0.277 0.285 42.9 42.1 42.6 41.8 38.5 42.9Similar 0.303 0.269 0.302 0.281 0.256 0.144 43.3 41.2 42.7 40.5 40.2 38.9Contrast 0.268 0.234 0.267 0.289 0.260 0.156 42.8 42.0 42.4 41.5 42.7 38.1Attribute 0.406 0.409 0.405 0.433 0.164 0.447 48.3 47.8 48.2 49.1 36.9 49.0Non-Attribute 0.296 0.287 0.296 0.276 0.123 0.283 42.6 42.9 42.6 41.8 36.0 43.0Case Relations 0.473 0.497 0.470 0.484 0.309 0.498 50.6 52.5 50.2 50.9 42.9 53.2Cause-Purpose 0.296 0.282 0.299 0.301 0.205 0.296 41.7 41.6 41.6 41.2 36.6 44.1Space-Time 0.443 0.425 0.443 0.420 0.269 0.431 47.7 47.2 47.7 46.9 40.5 49.5Reference 0.208 0.238 0.205 0.168 0.102 0.210 42.5 42.3 42.6 41.8 36.1 41.4Average 0.353 0.348 0.350 0.354 0.238?
0.329 45.2 45.0 44.9?
44.7 39.6?
45.4Table 3: Average Spearman?s ?
and MaxDiff accuracy results of different model combinations.
Com indicates combin-ing all models, where other columns show the results when the specified model is removed.
The best result in each rowis highlighted in boldface font.
Statistical significance tests are conducted by comparing each ablation configurationwith Com.
?
indicates the difference in the average results is statistically significant with 99% confidence level.original combination model.Overall, it is clear that the directional similaritymethod based on RNNLM vectors is the most crit-ical component model.
Removing it from the fi-nal combination decreases both the Spearman?s ?and MaxDiff accuracy by a large margin; both dif-ferences (Com vs. -DS) are statistically significant.The Probase IsA model also has an important im-pact on the performance on the Class-Inclusion re-lation group.
Eliminating the IsA model makesthe overall MaxDiff accuracy statistically signifi-cantly lower (Com vs. -IsA).
Again, the benefitsof incorporating Probase Attribute and PILSA mod-els are not clear.
Removing them from the finalcombination lowers the MaxDiff accuracy, but nei-ther the difference in Spearman?s ?
nor MaxDiffaccuracy is statistically significant.
Compared tothe RNNLM directional similarity model, the lex-ical pattern model seems less critical.
Removingit lowers the Similar and Contrast relation groups,but improves some other relation groups like Class-Inclusion and Case Relations.
The final MaxDiff ac-curacy becomes slightly higher but the Spearman?s?
drops a little (Com vs. -Pat); neither is statisticallysignificant.Notice that the main purpose of the ablation studyis to verify the importance of an individual compo-nent model when a significant performance drop isobserved after removing it.
However, occasionallythe overall performance may go up slightly.
Typi-cally this is due to the fact that some models do notprovide useful signals to a particular relation, but in-stead introduce more noise.
Such effects can oftenbe alleviated when there are enough quality trainingdata, which is unfortunately not the case here.6 ConclusionsIn this paper, we presented a system that combinesheterogeneous models based on different informa-tion sources for measuring relational similarity.
Ourtwo individual general-purpose relational similaritymodels, directional similarity and lexical patternmethods, perform strongly when compared to ex-isting systems.
After incorporating specific word-relation models, the final system sets a new state-of-the-art on the SemEval-2012 task 2 test set, achiev-ing Spearman?s ?
= 0.353 and MaxDiff accuracy45.4% ?
resulting in 54.1% and 14.7% relative im-provement in these two metrics, respectively.Despite its simplicity, our directional similarityapproach provides a robust model for relational sim-ilarity and is a critical component in the final sys-tem.
When the lexical pattern model is included, ouroverall model combination method can be viewedas a two-stage learning system.
As demonstrated inour work, with an appropriate regularization strat-egy, high-quality models can be learned in bothstages.
Finally, as we observe from the positive ef-fect of adding the Probase IsA model, specific word-relation models can further help improve the system1007although they tend to cover only a small number ofrelations.
Incorporating more such models could bea steady path to enhance the final system.In the future, we plan to pursue several researchdirections.
First, as shown in our experimental re-sults, the model combination approach does not al-ways outperform individual models.
Investigatinghow to select models to combine for each specific re-lation or relation group individually will be our nextstep for improving this work.
Second, because thelabeling process of relational similarity comparisonsis inherently noisy, it is unrealistic to request a sys-tem to correlate human judgments perfectly.
Con-ducting some user study to estimate the performanceceiling in each relation category may help us focuson the weaknesses of the final system to enhanceit.
Third, it is intriguing to see that the directionalsimilarity model based on the RNNLM vectors per-forms strongly, even though the RNNLM trainingprocess is not related to the task of relational sim-ilarity.
Investigating the effects of different vectorspace models and proposing some theoretical jus-tifications are certainly interesting research topics.Finally, we would like to evaluate the utility our ap-proach in other applications, such as the SAT anal-ogy problems proposed by Turney (2006) and ques-tion answering.AcknowledgmentsWe thank Richard Socher for valuable discussions,Misha Bilenko for his technical advice and anony-mous reviewers for their comments.ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?caand A. Soroa.
2009.
A study on similarity and re-latedness using distributional and WordNet-based ap-proaches.
In NAACL ?09, pages 19?27.Galen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In ICML ?07.I.I.
Bejar, R. Chaffin, and S.E.
Embretson.
1991.
Cog-nitive and psychometric analysis of analogical prob-lem solving.
Recent research in psychology.
Springer-Verlag.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3:1137?1155.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.A.
Budanitsky and G. Hirst.
2006.
Evaluating WordNet-based measures of lexical semantic relatedness.
Com-putational Linguistics, 32:13?47, March.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Proceedings of theTwenty-Fourth Conference on Artificial Intelligence(AAAI 2010).Timothy W. Finin.
1980.
The Semantic Interpretationof Compound Nominals.
Ph.D. thesis, University ofIllinois at Urbana-Champaign.J.H.
Friedman.
2001.
Greedy function approximation: agradient boosting machine.
Ann.
Statist, 29(5):1189?1232.David Jurgens, Saif Mohammad, Peter Turney, and KeithHolyoak.
2012.
SemEval-2012 Task 2: Measuringdegrees of relational similarity.
In Proceedings of theSixth International Workshop on Semantic Evaluation(SemEval 2012), pages 356?364, Montre?al, Canada,7-8 June.
Association for Computational Linguistics.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes, 25, pages 259?284.Jordan J. Louviere and G. G. Woodworth.
1991.
Best-worst scaling: A model for the largest difference judg-ments.
Technical report, University of Alberta.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In INTER-SPEECH, pages 1045?1048.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernocky.
2011.
Strategies for train-ing large scale neural network language models.
InASRU.Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013.Linguistic regularities in continuous space word repre-sentations.
In Proceedings of NAACL-HLT.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Proceedings ofthe 5th International Workshop on Computational Se-mantics.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword fourth edi-tion.
Technical report, Linguistic Data Consortium,Philadelphia.Ted Pedersen.
2012.
Duluth: Measuring degrees of re-lational similarity with the gloss vector measure of se-mantic relatedness.
In Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation (SemEval10082012), pages 497?501, Montre?al, Canada, 7-8 June.Association for Computational Linguistics.K.
Radinsky, E. Agichtein, E. Gabrilovich, andS.
Markovitch.
2011.
A word at a time: computingword relatedness using temporal semantic analysis.
InWWW ?11, pages 337?346.J.
Reisinger and R. Mooney.
2010.
Multi-prototypevector-space models of word meaning.
In NAACL ?10.Bryan Rink and Sanda Harabagiu.
2012.
UTD: Deter-mining relational similarity using lexical patterns.
InProceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 413?418,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.Barbara Rosario and Marti Hearst.
2001.
Classify-ing the semantic relations in noun compounds via adomain-specific lexical hierarchy.
In Proceedings ofthe 2001 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-01, pages 82?90.Mireya Tovar, J. Alejandro Reyes, Azucena Montes,Darnes Vilarin?o, David Pinto, and Saul Leo?n.
2012.BUAP: A first approximation to relational similaritymeasuring.
In Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),pages 502?505, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of Association forComputational Linguistics (ACL 2010).Peter Turney and Michael Littman.
2005.
Corpus-basedlearning of analogies and semantic relations.
MachineLearning, 60 (1-3), pages 251?278.P.
D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Peter Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In In-ternational Conference on Computational Linguistics(COLING).Peter D. Turney.
2012.
Domain and function: Adual-space model of semantic relations and compo-sitions.
Journal of Artificial Intelligence Research(JAIR), 44:533?585.Lucy Vanderwende.
1994.
Algorithm for automaticinterpretation of noun sequences.
In Proceedings ofCOLING-94, pages 782?788.Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.Zhu.
2012.
Probase: a probabilistic taxonomy fortext understanding.
In Proceedings of the 2012 ACMSIGMOD International Conference on Management ofData, pages 481?492, May.Wen-tau Yih and Vahed Qazvinian.
2012.
Measur-ing word relatedness using heterogeneous vector spacemodels.
In Proceedings of NAACL-HLT, pages 616?620, Montre?al, Canada, June.Wen-tau Yih, Geoffrey Zweig, and John Platt.
2012.
Po-larity inducing latent semantic analysis.
In Proceed-ings of NAACL-HLT, pages 1212?1222, Jeju Island,Korea, July.1009
