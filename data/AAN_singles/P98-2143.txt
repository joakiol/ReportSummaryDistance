Robust pronoun resolution with limited knowledgeRus lan  Mi tkovSchool  o f  Languages  and European StudiesUnivers i ty  o f  Wo lverhamptonStaf ford StreetWolverhampton  WV 1 1SBUni ted K ingdomR.
M i tkov@wlv .
ac.
ukAbstractMost traditional approaches to anaphora resolutionrely heavily on linguistic and domain knowledge.One of the disadvantages of developing aknowledge-based system, however, is that it is a very labour-intensive and time-consuming task.
This paper pres-ents a robust, knowledge-poor approach to resolvingpronouns in technical manuals, which operates ontexts pre-processed by a part-of-speech tagger.
Inputis checked against agreement and for a number ofantecedent i dicators.
Candidates are assigned scoresby each indicator and the candidate with the highestscore is returned as the antecedent.
Evaluation reportsa success rate of 89.7% which is better than the suc-cess rates of the approaches selected for comparisonand tested on the same data.
In addition, preliminaryexperiments show that the approach can be success-fully adapted for other languages with minimummodifications.1.
IntroductionFor the most part, anaphora resolution has focusedon traditional linguistic methods (Carbonell &Brown 1988; Carter 1987; Hobbs 1978; Ingria &Stallard 1989; Lappin & McCord 1990; Lappin &Leass 1994; Mitkov 1994; Rich & LuperFoy 1988;Sidner 1979; Webber 1979).
However, to representand manipulate the various types of linguistic anddomain knowledge involved requires considerablehuman input and computational expense.While various alternatives have been proposed,making use of e.g.
neural networks, a situation se-mantics framework, or the principles of reasoningwith uncertainty (e.g.
Connoly et al 1994; Mitkov1995; Tin & Akman 1995), there is still a strongneed for the development of robust and effectivestrategies to meet the demands of practical NLPsystems, and to enhance further the automatic pro-cessing of growing language resources.Several proposals have already addressed theanaphora resolution problem by deliberately imitingthe extent o which they rely on domain and/or lin-guistic knowledge (Baldwin 1997; Dagan & Itai1990; Kennedy & Boguraev 1996; Mitkov 1998;Nasukawa 1994; Williams et al 1996).
Our work isa continuation of these latest rends in the search forinexpensive, fast and reliable procedures for anaph-ora resolution.
It is also an example of how anaphorsin a specific genre can be resolved quite successfullywithout any sophisticated linguistic knowledge oreven without parsing.
Finally, our evaluation showsthat the basic set of antecedent tracking indicatorscan work well not only for English, but also forother languages (in our case Polish and Arabic).2.
The approachWith a view to avoiding complex syntactic, seman-tic and discourse analysis (which is vital for real-world applications), we developed a robust, knowl-edge-poor approach to pronoun resolution whichdoes not parse and analyse the input in order toidentify antecedents of anaphors.
It makes use ofonly a part-of-speech tagger, plus simple nounphrase rules (sentence constituents are identified atthe level of noun phrase at most) and operates on thebasis of antecedent-tracking preferences (referred tohereafter as "antecedent indicators").
The approachworks as follows: it takes as an input the output of atext processed by a part-of-speech tagger, identifiesthe noun phrases which precede the anaphor within adistance of 2 sentences, checks them for gender andnumber agreement with the anaphor and then appliesthe genre-specific antecedent indicators to the re-maining candidates (see next section).
The nounphrase with the highest aggregate score is proposedas antecedent; in the rare event of a tie, priority isgiven to the candidate with the higher score for im-mediate reference.
If immediate reference has notbeen identified, then priority is given to the candi-869date with the best collocation pattern score.
If thisdoes not help, the candidate with the higher scorefor indicating verbs is preferred.
If still no choice ispossible, the most recent from the remaining candi-dates is selected as the antecedent.2.1 Antecedent indicatorsAntecedent indicators (preferences) play a decisiverole in tracking down the antecedent from a set ofpossible candidates.
Candidates are assigned a score(-1, 0, 1 or 2) for each indicator; the candidate withthe highest aggregate score is proposed as the ante-cedent.
The antecedent indicators have been identi-fied empirically and are related to salience(definiteness, givenness, indicating verbs, lexicalreiteration, section heading preference, "non-prepositional" noun phrases), to structural matches(collocation, immediate reference), to referentialdistance or to preference of terms.
Whilst some ofthe indicators are more genre-specific (term prefer-ence) and others are less genre-specific ("immediatereference"), the majority appear to be genre-independent.
In the following we shall outline somethe indicators used and shall illustrate them by ex-amples.Indicating verbsIf a verb is a member of the Verb_set = {discuss,present, illustrate, identify, summarise, examine,describe, define, show, check, develop, review, re-port, outline, consider, investigate, explore, assess,analyse, synthesise, study, survey, deal, cover}, weconsider the first NP following it as the preferred an-tecedent (scores 1 and 0).
Empirical evidence sug-gests that because of the salience of the nounphrases which follow them, the verbs listed aboveare particularly good indicators.Lexical reiterationLexically reiterated items are likely candidates forantecedent (a NP scores 2 if is repeated within thesame paragraph twice or more, 1 if repeated onceand 0 if not).
Lexically reiterated items include re-peated synonymous noun phrases which may oftenbe preceded by definite articles or demonstratives.Also, a sequence of noun phrases with the samehead counts as lexical reiteration (e.g.
"toner bottle","bottle of toner", "the bottle").Section heading preferenceDefinitenessDefinite noun phrases in previous sentences aremore likely antecedents of pronominal anaphorsthan indefinite ones (definite noun phrases score 0and indefinite ones are penalised by -1).
We regard anoun phrase as definite if the head noun is modifiedby a definite article, or by demonstrative or posses-sive pronouns.
This rule is ignored if there are nodefinite articles, possessive or demonstrative pro-nouns in the paragraph (this exception is taken intoaccount because some English user's guides tend toomit articles).GivennessNoun phrases in previous sentences representing the"given information" (theme) 1 are deemed goodcandidates for antecedents and score 1 (candidatesnot representing the theme score 0).
In a coherenttext (Firbas 1992), the given or known information,or theme, usually appears first, and thus forms a co-referential ink with the preceding text.
The newinformation, or rheme, provides some informationabout the theme.lWe use the simple heuristics that the given informationis the first noun phrase in a non-imperative s ntence.870If a noun phrase occurs in the heading of the section,part of which is the current sentence, then we con-sider it as the preferred candidate (1, 0).
"Non-prepositional" noun phrasesA "pure", "non-prepositional" noun phrase is given ahigher preference than a noun phrase which is partof a prepositional phrase (0, -1).
Example:Insert the cassette i into the VCR making sure it i issuitable for the length of recording.Here "the VCR" is penalised (-1) for being part ofthe prepositional phrase "into the VCR".This preference can be explained in terms of sali-ence from the point of view of the centering theory.The latter proposes the ranking "subject, direct ob-ject, indirect object" (Brennan et al 1987) and nounphrases which are parts of prepositional phrases areusually indirect objects.Collocation pattern preferenceThis preference is given to candidates which have anidentical collocation pattern with a pronoun (2,0).The collocation preference here is restricted to thepatterns "noun phrase (pronoun), verb" and "verb,noun phrase (pronoun)".
Owing to lack of syntacticinformation, this preference is somewhat weakerthan the collocation preference described in (Dagan& Itai 1990).
Example:Press the key i down and turn the volume up... Pressit i again.Immediate referenceIn technical manuals the "immediate reference" cluecan often be useful in identifying the antecedent.The heuristics used is that in constructions of theform "...(You) V l NP ... con (you) V 2 it (con (you)V 3 it)", where con ~ {and/or/before/after...}, thenoun phrase immediately after V l is a very likelycandidate for antecedent of the pronoun "it" imme-diately following V 2 and is therefore given prefer-ence (scores 2 and 0).This preference can be viewed as a modificationof the collocation preference.
It is also quite fre-quent with imperative constructions.
Example:To print the paper, you can stand the printer i up orlay it i flat.To turn on the printer, press the Power button i andhold it i down for a moment.Unwrap the paper i, form it i and align it i, then load it iinto the drawer.Referential distanceIn complex sentences, noun phrases in the previousclause 2 are the best candidate for the antecedent ofan anaphor in the subsequent clause, followed bynoun phrases in the previous entence, then by nounssituated 2 sentences further back and finally nouns 3sentences further back (2, 1, 0, -1).
For anaphors insimple sentences, noun phrases in the previous sen-tence are the best candidate for antecedent, followedby noun phrases situated 2 sentences further backand finally nouns 3 sentences further back (1, 0, -1).Term preferenceNPs representing terms in the field are more likelyto be the antecedent than NPs which are not terms(score 1 if the NP is a term and 0 if not).2Identification of clauses in complex sentences i  doneheuristically.871As already mentioned, each of the antecedent in-dicators assigns a score with a value {-1, 0, 1, 2}.These scores have been determined experimentallyon an empirical basis and are constantly being up-dated.
Top symptoms like "lexical reiteration" as-sign score "2" whereas "non-prepositional" nounphrases are given a negative score of "-1".
Weshould point out that the antecedent indicators arepreferences and not absolute factors.
There might becases where one or more of the antecedent indicatorsdo not "point" to the correct antecedent.
For in-stance, in the sentence "Insert the cassette into theVCR i making sure it i is turned on", the indicator"non-prepositional noun phrases" would penalise thecorrect antecedent.
When all preferences (antecedentindicators) are taken into account, however, the rightantecedent is still very likely to be tracked down - inthe above example, the "non-prepositional nounphrases" heuristics (penalty) would be overturned bythe "collocational preference" heuristics.2.2 Informal description of the algorithmThe algorithm for pronoun resolution can be de-scribed informally as follows:1.
Examine the current sentence and the two pre-ceding sentences (if available).
Look for nounphrases 3 only to the left of the anaphor 42.
Select from the noun phrases identified onlythose which agree in gender and number 5 withthe pronominal anaphor and group them as a setof potential candidates3.
Apply the antecedent indicators to each poten-tial candidate and assign scores; the candidatewith the highest aggregate score is proposed as3A sentence splitter would already have segmented thetext into sentences, a POS tagger would already havedetermined the parts of speech and a simple phrasalgrammar would already have detected the noun phrases4In this project we do not treat cataphora; non-anaphoric"it" occurring in constructions such as "It is important","It is necessary" is eliminated by a "referential filter"5Note that this restriction may not always apply in lan-guages other than English (e.g.
German); on the otherhand, there are certain collective nouns in English whichdo not agree in number with their antecedents (e.g.
"government", team", "parliament" etc.
can be referredto by "they"; equally some plural nouns (e.g.
"data") canbe referred to by "it") and are exempted from the agree-ment est.
For this purpose we have drawn up a compre-hensive list of all such cases; to our knowledge, no othercomputational treatment of pronominal anaphora resolu-tion has addressed the problem of "agreement excep-tions".antecedent.
If two candidates have an equalscore, the candidate with the higher score forimmediate reference is proposed as antecedent.I f  immediate reference does not hold, proposethe candidate with higher score for collocationalpattern.
If collocational pattern suggests a tie ordoes not hold, select the candidate with higherscore for indicating verbs.
If this indicator doesnot hold again, go for the most recent candidate.3.
EvaluationFor practical reasons, the approach presented oesnot incorporate syntactic and semantic information(other than a list of domain terms) and it is not real-istic to expect its performance to be as good as anapproach which makes use of syntactic and semanticknowledge in terms of constraints and preferences.The lack of syntactic information, for instance,means giving up c-command constraints and subjectpreference (or on other occasions object preference,see Mitkov 1995) which could be used in centertracking.
Syntactic parallelism, useful in discrimi-nating between identical pronouns on the basis oftheir syntactic function, also has to be forgone.
Lackof semantic knowledge rules out the use of verb se-mantics and semantic parallelism.
Our evaluation,however, suggests that much less is lost than mightbe feared.
In fact, our evaluation shows that the re-sults are comparable to syntax-based methods(Lappin & Leass 1994).
We believe that the goodsuccess rate is due to the fact that a number of ante-cedent indicators are taken into account and no fac-tor is given absolute preference.
In particular, thisstrategy can often override incorrect decisions linkedwith strong centering preference (Mitkov & Belguith1998) or syntactic and semantic parallelism prefer-ences (see below).3.1 Evaluation AOur first evaluation exercise (Mitkov & Stys 1997)was based on a random sample text from a technicalmanual in English (Minolta 1994).
There were 71pronouns in the 140 page technical manual; 7 of thepronouns were non-anaphoric and 16 exophoric.
Theresolution of anaphors was carried out with a suc-cess rate of 95.8%.
The approach being robust (anattempt is made to resolve each anaphor and a pro-posed antecedent is returned), this figure representsboth "precision" and "recall" if we use the MUCterminology.
To avoid any terminological confusion,we shall therefore use the more neutral term"success rate" while discussing the evaluation.872In order to evaluate the effectiveness of the ap-proach and to explore i f /how far it is superior overthe baseline models for anaphora resolution, we alsotested the sample text on (i) a Baseline Model whichchecks agreement in number and gender and, wheremore than one candidate remains, picks as antece-dent the most recent subject matching the genderand number of the anaphor (ii) a Baseline Modelwhich picks as antecedent the most recent nounphrase that matches the gender and number of theanaphor.The success rate of the "Baseline Subject" was29.2%, whereas the success rate of "Baseline MostRecent NP" was 62.5%.
Given that our knowledge-poor approach is basically an enhancement of abaseline model through a set of antecedent indica-tors, we see a dramatic improvement in performance(95.8%) when these preferences are called upon.Typically, our preference-based model provedsuperior to both baseline models when the antece-dent was neither the most recent subject nor themost recent noun phrase matching the anaphor ingender and number.
Example:Identify the drawer i by the lit paper port LED andadd paper to it i.The aggregate score for "the drawer" is 7(definiteness 1 + givenness 0 + term preference 1 +indicating verbs l + lexical reiteration 0 + sectionheading 0 + collocation 0 + referential distance 2 +non-prepositional noun phrase 0 + immediate refer-ence 2 = 7), whereas aggregate score for the mostrecent matching noun phrase ("the lit paper portLED") is 4 (definiteness 1 + givenness 0 + termpreference 1 + indicating verbs 0 + lexical reitera-tion 0 + section heading 0 + collocation 0 + referen-tial distance 2 + non-prepositional noun phrase 0 +immediate reference 0 = 4).From this example we can also see that ourknowledge-poor approach successfully tackles casesin which the anaphor and the antecedent have notonly different syntactic functions but also differentsemantic roles.
Usually knowledge-based ap-proaches have difficulties in such a situation becausethey use preferences such as "syntactic parallelism"or "semantic parallelism".
Our robust approach doesnot use these because it has no information about thesyntactic structure of the sentence or about the syn-tactic function/semantic role of each individualword.As far as the typical failure cases are concerned,we anticipate the knowledge-poor approach to havedifficulties with sentences which have a more com-plex syntactic structure.
This should not be surpris-ing, given that the approach does not rely on anysyntactic knowledge and in particular, it does notproduce any parse tree.
Indeed, the approach fails onthe sentence:The paper through key can be used to feed \[ablank sheet of paper\]i through the copier outinto the copy tray without making a copy oniti.where "blank sheet of paper" scores only 2 as op-posed to the "the paper through key" which scores 6.3.2 Evaluation BSimilarly to the first evaluation, we found that therobust approach was not very successful on sen-tences with too complicated syntax - a price we haveto pay for the "convenience" of developing a knowl-edge-poor system.The results from experiment 1 and experiment 2can be summarised in the following (statistically)slightly more representative figures.Success rate(=Precision/Recall)Robustapproach89.7%Baselinesubject31.55% /48.55%Baselinemost recent65.95%We carried out a second evaluation of the approachon a different set of sample texts from the genre oftechnical manuals (47-page Portable Style-WriterUser's Guide (Stylewriter 1994).
Out of 223 pro-nouns in the text, 167 were non-anaphoric (deicticand non-anaphoric "it").
The evaluation carried outwas manual to ensure that no added error was gen-erated (e.g.
due to possible wrong sentence/clausedetection or POS tagging).
Another reason for doingit by hand is to ensure a fair comparison with BreckBaldwin's method, which not being available to us,had to be hand-simulated (see 3.3).The evaluation indicated 83.6% success rate.
The"Baseline subject" model tested on the same datascored 33.9% recall and 67.9% precision, whereas"Baseline most recent" scored 66.7%.
Note that"Baseline subject" can be assessed both in terms ofrecall and precision because this "version" is notrobust: in the event of no subject being available, itis not able to propose an antecedent ( he manualguide used as evaluation text contained many im-perative zero-subject sentences).In the second experiment we evaluated the ap-proach from the point of view also of its "criticalsuccess rate".
This measure (Mitkov 1998b) appliesonly to anaphors "ambiguous" from the point ofview of number and gender (i.e.
to those "tough"anaphors which, after activating the gender andnumber filters, still have more than one candidatefor antecedent) and is indicative of the performanceof the antecedent indicators.
Our evaluation estab-lished the critical success rate as 82%.A case where the system failed was when theanaphor and the antecedent were in the same sen-tence and where preference was given to a candidatein the preceding sentence.
This case and other casessuggest that it might be worthwhile reconsider-ing/refining the weights for the indicator "referentialdistance".The lower figure in "Baseline subject" correspondsto "recall" and the higher figure - to "precision".If we regard as "discriminative power" of eachantecedent indicator the ratio "number of successfulantecedent identifications when this indicator wasapplied"/"number of applications of this indicator"(for the non-prepositional noun phrase and definite-ness being penalising indicators, this figure is calcu-lated as the ratio "number of unsuccessful antece-dent identifications"/"number of applications"), theimmediate reference merges as the most discrimi-native indicator (100%), followed by non-prepositional noun phrase (92.2%), collocation(90.9%), section heading (61.9%), lexical reiteration(58.5%), givenness (49.3%), term preference(35.7%) and referential distance (34.4%).
The rela-tively low figures for the majority of indicatorsshould not be regarded as a surprise: firstly, weshould bear in mind that in most cases a candidatewas picked (or rejected) as an antecedent on the ba-sis of applying a number of different indicators andsecondly, that most anaphors had a relatively highnumber of candidates for antecedent.In terms of frequency of use ("number of non-zeroapplications"/"number of anaphors"), the most fre-quently used indicator proved to be referential dis-tance used in 98.9% of the cases, followed by termpreference (97.8%), givenness (83.3%), lexical reit-eration (64.4%), definiteness (40%), section heading(37.8%), immediate reference (31.1%) and colloca-tion (11.1%).
As expected, the most frequent indica-tors were not the most discriminative ones.3.3 Comparison to similar approaches: compara-tive evaluation of Breck Baldwin's CogNIACWe felt appropriate to extend the evaluation of ourapproach by comparing it to Breck Baldwin's Cog-NIAC (Baldwin 1997) approach which features"high precision coreference with limited knowledge873and linguistics resources".
The reason is that bothour approach and Breck Baldwin's approach sharecommon principles (both are knowledge-poor anduse a POS tagger to provide the input) and thereforea comparison would be appropriate.Given that our approach is robust and returns an-tecedent for each pronoun, in order to make thecomparison as fair as possible, we used CogNIAC's"resolve all" version by simulating it manually onthe same training data used in evaluation B above.CogNIAC successfully resolved the pronouns in75% of the cases.
This result is comparable with theresults described in (Baldwin 1997).
For the trainingdata from the genre of technical manuals, it was rule5 (see Baldwin 1997) which was most frequentlyused (39% of the cases, 100% success), followed byrule 8 (33% of the cases, 33% success), rule 7 (11%,100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).It would be fair to say that even though the resultsshow superiority of our approach on the trainingdata used (the genre of technical manuals), theycannot be generalised automatically for other genresor unrestricted texts and for a more accurate picture,further extensive tests are necessary.4.
Adapting the robust approach for otherlanguagesAn attractive feature of any NLP approach would beits language "universality".
While we acknowledgethat most of the monolingual NLP approaches arenot automatically transferable (with the same degreeof efficiency) to other languages, it would be highlydesirable if this could be done with minimal adapta-tion.We used the robust approach as a basis for devel-oping a genre-specific reference resolution approachin Polish.
As expected, some of the preferences hadto be modified in order to fit with specific featuresof Polish (Mitkov & Stys 1997).
For the time being,we are using the same scores for Polish.The evaluation for Polish was based technicalmanuals available on the Internet (Internet Manual,1994; Java Manual 1998).
The sample texts con-tained 180 pronouns among which were 120 in-stances of exophoric reference (most being zero pro-nouns).
The robust approach adapted for Polishdemonstrated a high success rate of 93.3% in resolv-ing anaphors (with critical success rate of 86.2%).Similarly to the evaluation for English, we com-pared the approach for Polish with (i) a BaselineModel which discounts candidates on the basis ofagreement in number and gender and, if there werestill competing candidates, elects as the antecedentthe most recent subject matching the anaphor in874gender and number (ii) a Baseline Model whichchecks agreement in number and gender and, if therewere still more than one candidate left, picks up asthe antecedent the most recent noun phrase thatagrees with the anaphor.Our preference-based approach showed clear su-periority over both baseline models.
The first Base-line Model (Baseline Subject) was successful in only23.7% of the cases, whereas the second (BaselineMost Recent) had a success rate of 68.4%.
There-fore, the 93.3% success rate (see above) demon-strates a dramatic increase in precision, which is dueto the use of antecedent tracking preferences.We have recently adapted the approach for Ara-bic as well (Mitkov & Belguith 1998).
Our evalua-tion, based on 63 examples (anaphors) from a tech-nical manual (Sony 1992), indicates a success rate of95.2% (and critical success rate 89.3 %).5.
ConclusionWe have described a robust, knowledge-poor ap-proach to pronoun resolution which operates on textspre-processed by a part-of-speech tagger.
Evaluationshows a success rate of 89.7% for the genre of tech-nical manuals and at least in this genre, the approachappears to be more successful than other similarmethods.
We have also adapted and evaluated theapproach for Polish (93.3 % success rate) and forArabic (95.2% success rate).ReferencesBaldwin, Breck.
1997.
"CogNIAC: high precisioncoreference with limited knowledge and linguistic re-sources".
Proceedings of the ACL'97/EACL'97 workshopon Operational factors in practical, robust anaphoraresolution, 38-45, Madrid, Spain.Brennan, S., M. Fridman and C. Pollard.
1987.
A center-ing approach to pronouns.
Proceedings of the 25th An-nual Meeting of the ACL (ACL'87), 155-162, Stanford,CA, USA.Carbonell, James G. & Ralf D. Brown.
1988.
"Anaphoraresolution: a multi-strategy approach".
Proceedings ofthe 12. International Conference on Computational Lin-guistics (COLING'88), Vol.I.
96-101, Budapest, Hun-gary.Carter, David M. 1987.
Interpreting anaphora in naturallanguage texts.
Chichester: Ellis HorwoodConnoly, Dennis, John D. Burger & David S. Day.
1994.
"A Machine learning approach to anaphoric reference".Proceedings of the International Conference "NewMethods in Language Processing", 255-261, Manches-ter, United Kingdom.Dagan, ldo & Alon Itai.
1990.
"Automatic processing oflarge corpora for the resolution of anaphora references".Proceedings of the 13thelnternational Conference onComputational Linguistics (COLING'90), Vol.
III, 1-3,Helsinki, FinlandFirbas, Jan. 1992.
Functional sentence perspective inwritten and spoken communication.
Cambridge: Cam-bridge University PressHobbs, Jerry R. 1978 "Resolving pronoun references".Lingua, 44, 339-352.Ingria, Robert J.P. & David Stallard.
1989.
"A computa-tional mechanism for pronominal reference".
Proceed-ings of the 27th Annual Meeting of the ACL, 262-271,Vancouver, British Columbia.Internet Manual.
1994.
Translation of Internet Manuallnternet i okolice: Przewodnik po swiatowych sieciachkomputerowych.
Tracy LaQuey, Jeanne C. Ryer Trans-lated by Monika Zielinska, BIZNET Poland.Java Manual.
1998.
Jezyk Java.
Clico, Krakow.Kennedy, Christopher & Branimir Boguraev, 1996.
"Anaphora for everyone: pronominal anaphora resolu-tion without a parser".
Proceedings of the 16th Interna-tional Conference on Computational Linguistics(COLING'96), 113-118.
Copenhagen, DenmarkLappin, Shalom & Michael McCord.
1990.
"Anaphoraresolution in slot grammar".
Computational Linguistics,16:4, 197-212.Lappin, Shalom & Herbert Leass.
1994.
"An algorithmfor pronominal anaphora resolution".
ComputationalLinguistics, 20(4), 535-561.Minolta.
1994.
Minolta Operator's Manual for Photocop-ier EP5325.
Technical Manual Minolta Camera Co.,Ltd., Business Equipment Division 3-13, 2-Chome,Azuchi, -Machi, Chuo-Ku, Osaka 541, JapanMitkov, Ruslan.
1994.
"An integrated model for anaphoraresolution".
Proceedings of the 15th International Con-ference on Computational Linguistics (COLING'94),1170-1176, Kyoto, Japan.Mitkov, Ruslan.
1995.
"Un uncertainty reasoning ap-proach for anaphora resolution".
Proceedings of theNatural Language Processing Pacific Rim Symposium(NLPRS'95), 149-154, Seoul, Korea.Mitkov, Ruslan.
1998a.
"Pronoun resolution: the practicalalternative".
In T. McEnery, S. Botley(Eds) DiscourseAnaphora nd Anaphor Resolution.
John Benjamins.Mitkov, Ruslan.
1998b.
"Evaluating anaphora resolutionapproaches".
Proceedings of the Discourse Anaphoraand Anaphora Resolution Colloquium (DAARC'2),(forthcoming) Lancaster, UKMitkov, Ruslan & Malgorzata Stys.
1997.
"Robust refer-ence resolution with limited knowledge: high precisiongenre-specific approach for English and Polish".
Pro-ceedings of the International Conference "Recent Ad-vances in Natural Language Proceeding" (RANLP'97),74-8 l, Tzigov Chark, Bulgaria.Mitkov, Ruslan & Lamia Belguith.
1998.
"Pronoun reso-lution made simple: a robust, knowledge-poor approachin action".
Proceedings of the International Conference"Traduction Automatique et Langage Naturel"(TALN'98) (forthcoming).
Paris, France.Rich, Elaine & Susann LuperFoy.
1988.
"An architecturefor anaphora resolution".
Proceedings of the SecondConference on Applied Natural Language Processing(ANLP-2), 18-24, Texas, U.S.A.Sidner, Candy L. 1979.
Towards a computational theoryof definite anaphora comprehension in English dis-course.
Technical Report No.
537.
M.I.T., ArtificialIntelligence Laboratory.Sony.
1992.
Video cassette recorder.
Operating Instruc-tions.
Sony Corporation.Stylewriter 1994.
Portable StyleWriter.
User's guide.Apple Computers.Tin, Erkan & Varol, Akman.
1994.
"Situated processingof pronominal anaphora".
Proceedings of theKONVENS"94 Conference, 369-378, Webber, Bonnie L.1979.
A formal approach to discourse anaphora.
Lon-don: Garland Publishing.Williams, Sandra, Mark Harvey & Keith Preston.
1996.
"Rule-based reference resolution for unrestricted textusing part-of-speech tagging and noun phrase parsing".Proceedings of the International Colloquium on Dis-course Anaphora and Anaphora Resolution (DAARC),441-456.
Lancaster, UK.875
