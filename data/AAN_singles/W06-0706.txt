Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 40?47,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomating Help-desk Responses: A Comparative Study ofInformation-gathering ApproachesYuval Marom and Ingrid ZukermanFaculty of Information TechnologyMonash UniversityClayton, VICTORIA 3800, AUSTRALIA{yuvalm,ingrid}@csse.monash.edu.auAbstractWe present a comparative study of corpus-based methods for the automatic synthe-sis of email responses to help-desk re-quests.
Our methods were developed byconsidering two operational dimensions:(1) information-gathering technique, and(2) granularity of the information.
In par-ticular, we investigate two techniques ?
re-trieval and prediction ?
applied to infor-mation represented at two levels of granu-larity ?
sentence-level and document level.We also developed a hybrid method thatcombines prediction with retrieval.
Ourresults show that the different approachesare applicable in different situations, ad-dressing a combined 72% of the requestswith either complete or partial responses.1 IntroductionEmail inquiries sent to help desks often ?revolvearound a small set of common questions and is-sues?.1 This means that help-desk operators spendmost of their time dealing with problems thathave been previously addressed.
Further, a sig-nificant proportion of help-desk responses containa low level of technical content, corresponding,for example, to inquires addressed to the wronggroup, or insufficient detail provided by the cus-tomer about his or her problem.
Organizations andclients would benefit if the efforts of human oper-ators were focused on difficult, atypical problems,and an automated process was employed to dealwith the easier problems.1http://customercare.telephonyonline.com/ar/telecom_next_generation_customer.In this paper, we report on our experiments withcorpus-based approaches to the automation ofhelp-desk responses.
Our study is based on a cor-pus of 30,000 email dialogues between users andhelp-desk operators at Hewlett-Packard.
These di-alogues deal with a variety of user requests, whichinclude requests for technical assistance, inquiriesabout products, and queries about how to returnfaulty products or parts.In order to restrict the scope of our study, weconsidered two-turn short dialogues, comprising arequest followed by an answer, where the answerhas at most 15 lines.
This yields a sub-corpus of6659 dialogues.
As a first step, we have automat-ically clustered the corpus according to the sub-ject line of the first email.
This process yielded15 topic-based datasets that contain between 135and 1200 email dialogues.
Owing to time limita-tions, the procedures described in this paper wereapplied to 8 of the datasets, corresponding to ap-proximately 75% of the dialogues.Analysis of our corpus yields the following ob-servations.?
O1: Requests containing precise information,such as product names or part specifications,sometimes elicit helpful, precise answers re-ferring to this information, while other timesthey elicit answers that do not refer to the queryterms, but contain generic information (e.g.,referring customers to another help group orasking them to call a particular phone num-ber).
Request-answer pair RA1 in Figure 1 il-lustrates the first situation, while the pair RA2illustrates the second.22Our examples are reproduced verbatim from the corpus(except for URLs and phone numbers which have been dis-guised by us), and some have user or operator errors.40RA1:Do I need Compaq driver software for my armada 1500docking station?
This in order to be able to re-install win98?I would recommend to install the latest system rompaq,on the laptop and the docking station.
Just select themodel of computer and the operating system you have.http://www.thislink.com.RA2:Is there a way to disable the NAT rewall on the Com-paq CP-2W so I don?t get a private ip address throughthe wireless network?Unfortunately, you have reached the incorrect eResponsequeue for your unit.
Your device is supported at the fol-lowing link, or at 888-phone-number.
We apologizefor the inconvenience.Figure 1: Sample request-answer pairs.?
O2: Operators tend to re-use the same sen-tences in different responses.
This is partly aresult of companies having in-house manualsthat prescribe how to generate an answer.
Forinstance, answers A3 and A4 in Figure 2 sharethe sentence in italics.These observations prompt us to consider com-plementary approaches along two separate dimen-sions of our problem.
The first dimension pertainsto the technique applied to determine the informa-tion in an answer, and the second dimension per-tains to the granularity of the information.?
Observation O1 leads us to consider two tech-niques for obtaining information: retrieval andprediction.
Retrieval returns an informationitem by matching its terms to query terms(Salton and McGill, 1983).
Hence, it is likelyto obtain precise information if available.
Incontrast, prediction uses features of requestsand responses to select an information item.For example, the absence of a particular termin a request may be a good predictive feature(which cannot be considered in traditional re-trieval).
Thus, prediction could yield repliesthat do not match particular query terms.?
Observation O2 leads us to consider two levelsof granularity: document and sentence.
That is,we can obtain a document comprising a com-plete answer on the basis of a request (i.e., re-use an answer to a previous request), or we canobtain individual sentences and then combinethem to compose an answer, as is done in multi-document summarization (Filatova and Hatzi-vassiloglou, 2004).
The sentence-level granu-A3:If you are able to see the Internet then it sounds likeit is working, you may want to get in touch with your ITdepartment to see if you need to make any changes to yoursettings to get it to work.
Try performing a soft reset, bypressing the stylus pen in the small hole on the bottom lefthand side of the Ipaq and then release.A4:I would recommend doing a soft reset by pressing thestylus pen in the small hole on the left hand side of theIpaq and then release.
Then charge the unit overnightto make sure it has been long enough and then see whathappens.
If the battery is not charging then the unit willneed to be sent in for repair.Figure 2: Sample answers that share a sentence.larity enables the re-use of a sentence for dif-ferent responses, as well as the composition ofpartial responses.The methods developed on the basis of thesetwo dimensions are: Retrieve Answer, Predict An-swer, Predict Sentences, Retrieve Sentences andHybrid Predict-Retrieve Sentences.
The first fourmethods represent the possible combinations ofinformation-gathering technique and level of gran-ularity; the fifth method is a hybrid where thetwo information-gathering techniques are appliedat the sentence level.
The generation of re-sponses under these different methods combinesdifferent aspects of document retrieval, question-answering, and multi-document summarization.Our aim in this paper is to investigate when thedifferent methods are applicable, and whether in-dividual methods are uniquely successful in cer-tain situations.
For this purpose, we decided toassign a level of success not only to complete re-sponses, but also to partial ones (obtained with thesentence-based methods).
The rationale for thisis that we believe that a partial high-precision re-sponse is better than no response, and better thana complete response that contains incorrect infor-mation.
We plan to test these assumptions in fu-ture user studies.The rest of this paper is organized as follows.In the next section, we describe our five meth-ods, followed by the evaluation of their results.
InSection 4, we discuss related research, and thenpresent our conclusions and plans for future work.412 Information-gathering Methods2.1 Retrieve a Complete AnswerThis method retrieves a complete document (an-swer) on the basis of request lemmas.
We use co-sine similarity to determine a retrieval score, anduse a minimal retrieval threshold that must be sur-passed for a response to be accepted.We have considered three approaches to index-ing the answers in our corpus: according to thecontent lemmas in (1) requests, (2) answers, or(3) requests&answers.
The results in Section 3 arefor the third approach, which proved best.
To il-lustrate the difference between these approaches,consider request-answer pair RA2.
If we receiveda new request similar to that in RA2, the answerin RA2 would be retrieved if we had indexed ac-cording to requests or requests&answers.
How-ever, if we had indexed only on answers, then theresponse would not be retrieved.2.2 Predict a Complete AnswerThis prediction method first groups similar an-swers in the corpus into answer clusters.
For eachrequest, we then predict an answer cluster on thebasis of the request features, and select the answerthat is most representative of the cluster (closest tothe centroid).
This method would predict a groupof answers similar to the answer in RA2 from theinput lemmas ?compaq?
and ?cp-2w?.The clustering is performed in advance of theprediction process by the intrinsic classificationprogram Snob (Wallace and Boulton, 1968), us-ing the content lemmas (unigrams) in the answersas features.
The predictive model is a DecisionGraph (Oliver, 1993) trained on (1) input features:unigram and bigram lemmas in the request,3 and(2) target feature ?
the identifier of the answercluster that contains the actual answer for the re-quest.4 The model provides a prediction of whichresponse cluster is most suitable for a given re-quest, as well as a level of confidence in this pre-diction.
We do not attempt to produce an answerif the confidence is not sufficiently high.In principle, rather than clustering the answers,the predictive model could have been trained onindividual answers.
However, on one hand, the3Significant bigrams are obtained using the NSP package(http://www.d.umn.edu/?tpederse/nsp.html).4At present, the clustering features differ from the predic-tion features because these parts of the system were devel-oped at different times.
In the near future, we will align thesefeatures.dimensionality of this task is very high, and onthe other hand, answers that share significant fea-tures would be predicted together, effectively act-ing as a cluster.
By clustering answers in advance,we reduce the dimensionality of the problem, atthe expense of some loss of information (sincesomewhat dissimilar answers may be grouped to-gether).2.3 Predict SentencesThis method looks at each answer sentence asthough it were a separate document, and groupssimilar sentences into clusters in order to obtainmeaningful sentence abstractions and avoid redun-dancy.5 For instance, the last sentence in A3 andthe first sentence in A4 are assigned to the samesentence cluster.
As for Answer Prediction (Sec-tion 2.2), this clustering process also reduces thedimensionality of the problem.Each request is used to predict promising clus-ters of answer sentences, and an answer is com-posed by extracting a sentence from such clus-ters.
Because the sentences in each cluster orig-inate in different response documents, the pro-cess of selecting them for a new response corre-sponds to multi-document summarization.
In fact,our selection mechanism, described in more de-tail in (Marom and Zukerman, 2005), is based ona multi-document summarization formulation pro-posed by Filatova and Hatzivassiloglou (2004).In order to be able to generate appropriate an-swers in this manner, the sentence clusters shouldbe cohesive, and they should be predicted withhigh confidence.
A cluster is cohesive if the sen-tences in it are similar to each other.
This meansthat it is possible to obtain a sentence that repre-sents the cluster adequately (which is not the casefor an uncohesive cluster).
A high-confidence pre-diction indicates that the sentence is relevant tomany requests that share certain regularities.
Ow-ing to these requirements, the Sentence Predictionmethod will often produce partial answers (i.e., itwill have a high precision, but often a low recall).2.3.1 Sentence clusteringThe clustering is performed by applying Snobusing the following sentence-based and word-based features, all of which proved significant for5We did not cluster request sentences, as requests are of-ten ungrammatical, which makes it hard to segment them intosentences, and the language used in requests is more diversethan the corporate language used in responses.42at least some datasets.
The sentence-based fea-tures are:?
Number of syntactic phrases in the sentence(e.g., prepositional, subordinate) ?
gives anidea of sentence complexity.?
Grammatical mood of the main clause (5states: imperative, imperative-step, declara-tive, declarative-step, unknown) ?
indicates thefunction of the sentence in the answer, e.g., anisolated instruction, part of a sequence of steps,part of a list of options.?
Grammatical person in the subject of the mainclause (4 states: first, second, third, unknown)?
indicates the agent (e.g., organization orclient) or patient (e.g., product).The word-based features are binary:?
Significant lemma bigrams in the subject of themain clause and in the ?augmented?
object inthe main clause.
This is the syntactic object ifit exists or the subject of a prepositional phrasein an imperative sentence with no object, e.g.,?click on the following link.??
The verbs in the sentence and their polarity (as-serted or negated).?
All unigrams in the sentence, excluding verbs.2.3.2 Calculation of cluster cohesionTo measure the textual cohesion of a cluster, weinspect the centroid values corresponding to theword features.
Due to their binary representation,the centroid values correspond to probabilities ofthe words appearing in the cluster.
Our measure issimilar to entropy, in the sense that it yields non-zero values for extreme probabilities (Marom andZukerman, 2005).
It implements that idea that acohesive group of sentences should agree stronglyon both the words that appear in these sentencesand the words that are omitted.
Hence, it is pos-sible to obtain a sentence that adequately repre-sents a cohesive sentence cluster, while this is notthe case for a loose sentence cluster.
For exam-ple, the italicized sentences in A3 and A4 belongto a highly cohesive sentence cluster (0.93), whilethe opening answer sentence in RA1 belongs toa less cohesive cluster (0.7) that contains diversesentences about the Rompaq power management.2.3.3 Sentence-cluster predictionUnlike Answer Prediction, we use a SupportVector Machine (SVM) for predicting sentenceclusters.
A separate SVM is trained for each sen-tence cluster, with unigram and bigram lemmas ina request as input features, and a binary target fea-ture specifying whether the cluster contains a sen-tence from the response to this request.During the prediction stage, the SVMs predictzero or more clusters for each request.
One repre-sentative sentence (closest to the centroid) is thenextracted from each highly cohesive cluster pre-dicted with high confidence.
These sentences willappear in the answer (at present, these sentencesare treated as a set, and are not organized into acoherent reply).2.4 Retrieve SentencesAs for Sentence Prediction (Section 2.3), thismethod looks at each answer sentence as though itwere a separate document.
For each request sen-tence, we retrieve candidate answer sentences onthe basis of the match between the content lem-mas in the request sentence and the answer sen-tence.
For example, while the first answer sen-tence in RA1 might match the first request sen-tence in RA1, an answer sentence from a differentresponse (about re-installing Win98) might matchthe second request sentence.
The selection of in-dividual text units from documents implementsideas from question-answering approaches.We are mainly interested in answer sentencesthat ?cover?
request sentences, i.e., the terms inthe request should appear in the answer.
Hence,we use recall as the measure for the goodness of amatch, where recall is defined as follows.recall = TF.IDF of lemmas in request sent & answer sentTF.IDF of lemmas in request sentenceWe initially retain the answer sentences whose re-call exceeds a threshold.6Once we have the set of candidate answer sen-tences, we attempt to remove redundant sentences.This requires the identification of sentences thatare similar to each other ?
a task for which weuse the sentence clusters described in Section 2.3.Again, this redundancy-removal step essentiallycasts the task as multi-document summarization.Given a group of answer sentences that belong to6To assess the goodness of a sentence, we experimentedwith f-scores that had different weights for recall and preci-sion.
Our results were insensitive to these variations.43the same cohesive cluster, we retain the sentencewith the highest recall (in our current trials, a clus-ter is sufficiently cohesive for this purpose if itscohesion ?
0.7).
In addition, we retain all the an-swer sentences that do not belong to a cohesivecluster.
All the retained sentences will appear inthe answer.2.5 Hybrid Predict-Retrieve SentencesIt is possible that the Sentence Prediction methodpredicts a sentence cluster that is not sufficientlycohesive for a confident selection of a representa-tive sentence, but instead the ambiguity can be re-solved through cues in the request.
For example,selecting between a group of sentences concerningthe installation of different drivers might be possi-ble if the request mentions a specific driver.
Thusthe Sentence Prediction method is complementedwith the Sentence Retrieval method to form a hy-brid, as follows.?
For highly cohesive clusters predicted withhigh confidence, we select a representative sen-tence as before.?
For clusters with medium cohesion predictedwith high confidence, we attempt to match thesentences with the request sentences, using theSentence Retrieval method but with a lower re-call threshold.
This reduction takes place be-cause the high prediction confidence providesa guarantee that the sentences in the cluster aresuitable for the request, so there is no need fora convervative recall threshold.
The role of re-trieval is now to select the sentence whose con-tent lemmas best match the request.?
For uncohesive clusters or clusters predictedwith low confidence, we have to resort to wordmatches, which means reverting to the higher,more convervative recall threshold, because weno longer have the prediction confidence.3 EvaluationAs mentioned in Section 1, our corpus was dividedinto topic-based datasets.
We have observed thatthe different datasets lend themselves differentlyto the various information-gathering methods de-scribed in the previous section.
In this section, weexamine the overall performance of the five meth-ods across the corpus, as well as their performancefor different datasets.3.1 MeasuresWe are interested in two performance indicators:coverage and quality.Coverage is the proportion of requests forwhich a response can be generated.
The variousinformation gathering methods presented in theprevious section have acceptance criteria that indi-cate that there is some level of confidence in gen-erating a response.
A request for which a plannedresponse fails to meet these criteria is not covered,or addressed, by the system.
We are interested inseeing if the different methods are applicable indifferent situations, that is, how exclusively theyaddress different requests.
Note that the sentence-based methods generate partial responses, whichare considered acceptable so long as they containat least one sentence generated with high confi-dence.
In many cases these methods produce obvi-ous and non-informative sentences such as ?Thankyou for contacting HP?, which would be deemedan acceptable response.
We have manually ex-cluded such sentences from the calculation of cov-erage, in order to have a more informative compar-ison between the different methods.Ideally, the quality of the generated responsesshould be measured through a user study, wherepeople judge the correctness and appropriatenessof answers generated by the different methods.However, we intend to refine our methods fur-ther before we conduct such a study.
Hence, atpresent we rely on a text-based quantitative mea-sure.
Our experimental setup involves a standard10-fold validation procedure, where we repeatedlytrain on 90% of a dataset and test on the remaining10%.
We then evaluate the quality of the answersgenerated for the requests in each test split, bycomparing them with the actual responses givenby the help-desk operator for these requests.We are interested in two quality measures:(1) the precision of a generated response, and(2) its overall similarity to the actual response.
Thereason for this distinction is that the former doesnot penalize for a low recall ?
it simply mea-sures how correct the generated text is.
As statedin Section 1, a partial but correct response may bebetter than a complete response that contains in-correct units of information.
On the other hand,more complete responses are favoured over par-tial ones, and so we use the second measure to getan overall indication of how correct and completea response is.
We use the traditional Information44Table 1: Performance of the different methods, measured as coverage, precision and f-score.Method Coverage Precision Ave (stdev) F-score Ave (stdev)Answer Retrieval 43% 0.37 (0.34) 0.35 (0.33)Answer Prediction 29% 0.82 (0.21) 0.82 (0.24)Sentence Prediction 34% 0.94 (0.13) 0.78 (0.18)Sentence Retrieval 9% 0.19 (0.19) 0.12 (0.11)Sentence Hybrid 43% 0.81 (0.29) 0.66 (0.25)Combined 72% 0.80 (0.25) 0.50 (0.33)Retrieval precision and f-score measures (Saltonand McGill, 1983), employed on a word-by-wordbasis, to evaluate the quality of the generated re-sponses.73.2 ResultsTable 1 shows the overall results obtained usingthe different methods.
We see that combined thedifferent methods can address 72% of the requests.That is, at least one of these methods can producesome non-empty response to 72% of the requests.Looking at the individual coverages of the differ-ent methods we see that they must be applicable indifferent situations, because the highest individualcoverage is 43%.The Answer Retrieval method addresses 43% ofthe requests, and in fact, about half of these (22%)are uniquely addressed by this method.
However,in terms of the quality of the generated response,we see that the performance is very poor (both pre-cision and f-score have very low averages).
Nev-ertheless, there are some cases where this methoduniquely addresses requests quite well.
In three ofthe datasets, Answer Retrieval is the only methodthat produces good answers, successfully address-ing 15-20 requests (about 5% of the requests inthese datasets).
These requests include severalcases similar to RA2, where the request was sentto the wrong place.
We would expect Answer Pre-diction to be able to handle such cases as well.However, when there are not enough similar casesin the dataset (as is the case with the three datasetsreferred to above), Answer Prediction is not ableto generalize from them, and therefore we can onlyrely on a new request closely matching an old re-quest or an old answer.The Answer Prediction method can address29% of the requests.
Only about a tenth of these7We have also employed sequence-based measures usingthe ROUGE tool set (Lin and Hovy, 2003), with similar re-sults to those obtained with the word-by-word measure.are uniquely addressed by this method, but thegenerated responses are of a fairly high quality,with an average precision and f-score of 0.82.Notice the large standard deviation of theseaverages, suggesting a somewhat inconsistentbehaviour.
This is due to the fact that this methodgives good results only when complete templateresponses are found.
In this case, any re-usedresponse will have a high similarity to the actualresponse.
However, when this is not the case,the performance degrades substantially, resultingin inconsistent behaviour.
This behaviour is par-ticularly prevalent for the ?product replacement?dataset, which comprises 18% of the requests.The vast majority of the requests in this datasetask for a return shipping label to be mailed to thecustomer, so that he or she can return a faultyproduct.
Although these requests often containdetailed product descriptions, the responses rarelyrefer to the actual products, and often contain thefollowing generic answer.A5:Your request for a return airbill has been received and hasbeen sent for processing.
Your replacement airbill will besent to you via email within 24 hours.Answer Retrieval fails in such cases, because eachrequest has precise information about the actualproduct, so a new request can neither match anold request (about a different product) nor can itmatch the generic response.
In contrast, AnswerPrediction can ignore the precise informationin the request, and infer from the mention ofa shipping label that the generic response isappropriate.
When we exclude this dataset fromthe calculations, both the average precision andf-score for the Answer Prediction method fall be-low those of the Sentence Prediction and Hybridmethods.
This means that Answer Prediction issuitable when requests that share some regularityreceive a complete template answer.The Sentence Prediction method can find reg-45ularities at the sub-document level, and thereforedeal with cases when partial responses can be gen-erated.
It produces such responses for 34% ofthe requests, and does so with a consistently highprecision (average 0.94, standard deviation 0.13).Only an overall 1% of the requests are uniquelyaddressed by this method, however, for the casesthat are shared between this method and otherones, it is useful to compare the actual quality ofthe generated response.
In 5% of the cases, theSentence Prediction method either uniquely ad-dresses requests, or jointly addresses requests to-gether with other methods but has a higher f-score.This means that in some cases a partial responsehas a higher quality than a complete one.Like the document-level Answer Retrievalmethod, the Sentence Retrieval method performspoorly.
It is difficult to find an answer sentencethat closely matches a request sentence, and evenwhen this is possible, the selected sentences tendto be different to the ones used by the help-deskoperators, hence the low precision and f-score.This is discussed further below in the context ofthe Sentence Hybrid method.The Sentence Hybrid method extends the Sen-tence Prediction method by employing sentenceretrieval as well, and thus has a higher coverage(45%).
In fact, the retrieval component serves todisambiguate between groups of candidate sen-tences, thus enabling more sentences to be in-cluded in the generated response.
This, however,is at the expense of precision, as we also saw forthe pure Sentence Retrieval method.
Although re-trieval selects sentences that match closely a givenrequest, this selection can differ from the ?selec-tions?
made by the operator in the actual response.Precision (and hence f-score) penalizes such sen-tences, even when they are more appropriate thanthose in the model response.
For example, con-sider request-answer pair RA6.
The answer isquite generic, and is used almost identically forseveral other requests.
The Hybrid method al-most reproduces this answer, replacing the firstsentence with A7.
This sentence, which matchesmore request words than the first sentence in themodel answer, was selected from a sentence clus-ter that is not highly cohesive, and contains sen-tences that describe different reasons for settingup a repair (the matching word in A7 is ?screen?
).The Hybrid method outperforms the other meth-ods in about 10% of the cases, where it eitherRA6:My screen is coming up reversed (mirrored).
There mustbe something loose electronically because if I put thestylus in it?s hole and move it back and forth, I can get thescreen to display properly momentarily.
Please advisewhere to send for repairs.To get the iPAQ serviced, you can call1-800-phone-number, options 3, 1 (enter a 10digit phone number), 2.
Enter your phone number twiceand then wait for the routing center to put you throughto a technician with Technical Support.
They can get theunit picked up and brought to our service center.A7:To get the iPAQ repaired (battery, stylus lock andscreen), please call 1-800-phone-number, options3, 1 (enter a 10 digit phone number), 2.uniquely addresses requests, or addresses themjointly with other methods but produces responseswith a higher f-score.3.3 SummaryIn summary, our results show that each of the dif-ferent methods is applicable in different situations,all occurring significantly in the corpus, with theexception of the Sentence Retrieval method.
TheAnswer Retrieval method uniquely addresses alarge portion of the requests, but many of its at-tempts are spurious, thus lowering the combinedoverall quality shown at the bottom of Table 1 (av-erage f-score 0.50), calculated by using the bestperforming method for each request.
The AnswerPrediction method is good at addressing situationsthat warrant complete template responses.
How-ever, its confidence criteria might need refiningto lower the variability in quality.
The combinedcontribution of the sentence-based methods is sub-stantial (about 15%), suggesting that partial re-sponses of high precision may be better than com-plete responses with a lower precision.4 Related ResearchThere are very few reported attempts at corpus-based automation of help-desk responses.
The re-trieval system eResponder (Carmel et al, 2000)is similar to our Answer Retrieval method, wherethe system retrieves a list of request-response pairsand presents a ranked list of responses to theuser.
Our results show that due to the repeti-tions in the responses, multi-document summa-rization can be used to produce a single (possi-bly partial) representative response.
This is rec-ognized by Berger and Mittal (2000), who em-ploy query-relevant summarization to generate re-sponses.
However, their corpus consists of FAQ46request-response pairs ?
a significantly differentcorpus to ours in that it lacks repetition and redun-dancy, and where the responses are not personal-ized.
Lapalme and Kosseim (2003) propose a re-trieval approach similar to our Answer Retrievalmethod, and a question-answering approach, butapplied to a corpus of technical documents ratherthan request-response pairs.
The methods pre-sented in this paper combine different aspects ofdocument retrieval, question-answering and multi-document summarization, applied to a corpus ofrepetitive request-response pairs.5 Conclusion and Future WorkWe have presented four basic methods and onehybrid method for addressing help-desk requests.The basic methods represent the four ways of com-bining level of granularity (sentence and docu-ment) with information-gathering technique (pre-diction and retrieval).
The hybrid method appliesprediction possibly followed by retrieval to infor-mation at the sentence level.
The results show thatwith the exception of Sentence Retrieval, the dif-ferent methods can address a significant portion ofthe requests.
A future avenue of research is thusto characterize situations where different methodsare applicable, in order to derive decision proce-dures that determine the best method automati-cally.
We have also started to investigate an in-termediate level of granularity: paragraphs.Our results suggest that the automatic evalua-tion method requires further consideration.
Asseen in Section 3, our f-score penalizes the Sen-tence Prediction and Hybrid methods when theyproduce good answers that are more informativethan the model answer.
As mentioned previously,a user study would provide a more conclusiveevaluation of the system, and could be used to de-termine preferences regarding partial responses.Finally, we propose the following extensions toour current implementation.
First, we would liketo improve the representation used for clustering,prediction and retrieval by using features that in-corporate word-based similarity metrics (Pedersenet al, 2004).
Secondly, we intend to investigatea more focused sentence retrieval approach thatutilizes syntactic matching of sentences.
For ex-ample, if a sentence cluster is strongly predictedby a request, but the cluster is uncohesive becauseof a low verb agreement, then the retrieval shouldfavour the sentences whose verbs match those inthe request.AcknowledgmentsThis research was supported in part by grantLP0347470 from the Australian Research Coun-cil and by an endowment from Hewlett-Packard.The authors also thank Hewlett-Packard for theextensive help-desk data, and Tony Tony for as-sistance with the sentence-segmentation software,and Kerri Morgan and Michael Niemann for de-veloping the syntactic feature extraction code.ReferencesA.
Berger and V.O.
Mittal.
2000.
Query-relevant sum-marization using FAQs.
In ACL2000 ?
Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics, pages 294?301, HongKong.D.
Carmel, M. Shtalhaim, and A. Soffer.
2000. eRe-sponder: Electronic question responder.
In CoopIS?02: Proceedings of the 7th International Confer-ence on Cooperative Information Systems, pages150?161, Eilat, Israel.E.
Filatova and V. Hatzivassiloglou.
2004.
A formalmodel for information selection in multi-sentencetext extraction.
In COLING?04 ?
Proceedings ofthe 20th International Conference on ComputationalLinguistics, pages 397?403, Geneva, Switzerland.G.
Lapalme and L. Kosseim.
2003.
Mercure: Towardsan automatic e-mail follow-up system.
IEEE Com-putational Intelligence Bulletin, 2(1):14?18.C.Y.
Lin and E.H. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Language Tech-nology Conference (HLT-NAACL 2003), Edmonton,Canada.Y.
Marom and I. Zukerman.
2005.
Towards a frame-work for collating help-desk responses from mul-tiple documents.
In Proceedings of the IJCAI05Workshop on Knowledge and Reasoning for Answer-ing Questions, pages 32?39, Edinburgh, Scotland.J.J.
Oliver.
1993.
Decision graphs ?
an extension ofdecision trees.
In Proceedings of the Fourth Interna-tional Workshop on Artificial Intelligence and Statis-tics, pages 343?350, Fort Lauderdale, Florida.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity ?
measuring the relatednessof concepts.
In AAAI-04 ?
Proceedings of theNineteenth National Conference on Artificial Intel-ligence, pages 25?29, San Jose, California.G.
Salton and M.J. McGill.
1983.
An Introduction toModern Information Retrieval.
McGraw Hill.C.S.
Wallace and D.M.
Boulton.
1968.
An informationmeasure for classification.
The Computer Journal,11(2):185?194.47
