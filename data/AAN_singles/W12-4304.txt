Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 27?36,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsOpen-domain Anatomical Entity Mention DetectionTomoko Ohta 1 Sampo Pyysalo 1 Jun?ichi Tsujii 2 Sophia Ananiadou 11National Centre for Text Mining and University of Manchester,Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester, UK2Microsoft Research Asia, Beijing, Chinaokap.tiffany@gmail.com, sampo.pyysalo@gmail.comjtsujii@microsoft.com, sophia.ananiadou@manchester.ac.ukAbstractAnatomical entities such as kidney, muscleand blood are central to much of biomedicalscientific discourse, and the detection of men-tions of anatomical entities is thus necessaryfor the automatic analysis of the structure ofdomain texts.
Although a number of resourcesand methods addressing aspects of the taskhave been introduced, there have so far beenno annotated corpora for training and evaluat-ing systems for broad-coverage, open-domainanatomical entity mention detection.
We in-troduce the AnEM corpus, a domain- andspecies-independent resource manually anno-tated for anatomical entity mentions using afine-grained classification system.
The cor-pus texts are selected randomly from citationabstracts and full-text papers with the aim ofmaking the corpus representative of the en-tire available biomedical scientific literature.We demonstrate the use of the corpus throughan evaluation of the broad-coverage MetaMaptagger and a CRF-based system trained on thecorpus data, considering also a combinationof these two methods.
The combined sys-tem demonstrates a promising level of per-formance, approaching 80% F-score for men-tion detection for a relaxed matching criterion.The corpus and other introduced resources areavailable under open licences from http://www.nactem.ac.uk/anatomy/.1 IntroductionEntity mention detection is a prerequisite for mostefforts to systematically analyse and represent thestructure of scientific discourse.
In the life sciences,a comprehensive analysis must include entities atmultiple levels of biological organization, from themolecular to the organism level.
The detection ofreferences to anatomical entities such as ?kidney?and ?blood?
is thus required for the automatic struc-tured analysis of biomedical scientific text.Although a wealth of lexical and ontological re-sources covering anatomical entities are available(Rosse and Mejino, 2003; Smith et al, 2007; Boden-reider, 2004; Haendel et al, 2009), such resourcesdo not alone confer the ability to reliably detectmentions of anatomical entities in natural language(Gerner et al, 2010a; Travillian et al, 2011; Pyysaloet al, 2012b).
To support the development and eval-uation of reliable anatomical entity mention detec-tion methods, corpus resources annotated specifi-cally for the task are necessary.In this study, we aim to create a reference standardfor evaluating methods for anatomical entity men-tion detection and for training machine learning-based methods for the task.
We seek to selecta set of texts that are representative of the rele-vant scientific literature, i.e.
open-domain in thesense of avoiding bias toward, for example, specificspecies, levels of biological organization (e.g.
sub-cellular or gross anatomy), parts of documents (e.g.abstracts), or subdomains of life science.
In sup-port of our annotation, we draw on a granularity-based, species-independent upper-level ontology ofanatomy as well as relevant species-specific onto-logical resources.The overall aim of our efforts is to create methodsand resources for comprehensive event-based anal-ysis (Ananiadou et al, 2010) of biomedical scien-tific discourse involving anatomy-level entities andprocesses.
In aiming to establish a stable basisfor anatomical entity mention detection, the presentstudy is an important step toward this goal.27Label Ontology classes ExamplesAnatomicalentityAnatomicalstructure ORGANISM SUBDIVISION organism subdivision CARO head, limbANATOMICAL SYSTEM anatomical system CARO vascular systemORGAN compound organ CARO liver, heartMULTI-TISSUE STRUCTURE multi-tissue structure CARO arteryTISSUE portion of tissue CARO epitheliumCELL cell CARO epithelial cellDEVELOPING ANATOMICAL STRUCTURE developing anatomical structure UBERON embryoCELLULAR COMPONENT cellular component GO mitochondrionORGANISM SUBSTANCE portion of organism substance CARO bloodIMMATERIAL ANATOMICAL ENTITY immaterial anatomical entity CARO lumenPATHOLOGICAL FORMATION - carcinomaTable 1: Annotations targets with applied label, corresponding ontology classes, and common examples.2 Corpus Annotation2.1 Ontological BasisFollowing our previous efforts on anatomical en-tity classification (Pyysalo et al, 2012b), we baseour definition of annotated mention scope, the sub-division of anatomical entities into classes, andthe class labels applied in our annotation primar-ily on the Common Anatomy Reference Ontology(CARO) (Haendel et al, 2008).
CARO is a small,species-independent ontology of anatomical entitiesbased on the upper-level structure of the Founda-tional Model of Anatomy (FMA) ontology of hu-man anatomy (Rosse and Mejino, 2003; Rosse andMejino, 2008).
CARO has been proposed as a stan-dard for unifying the upper-level structure of thevarious existing species-specific ontologies and isadopted by many of the over 40 ontologies involv-ing the anatomy domain in the Open BiomedicalOntologies (OBO) foundry1 (Smith et al, 2007).CARO adheres to disjoint classes and single inher-itance, and divides anatomical structures primarilyby granularity (Kumar et al, 2004), a systematic no-tion familiar to those working in the life sciences.Although we draw primarily on CARO, we fol-low the well-established cellular component subon-tology of the Gene Ontology (GO) (Ashburner etal., 2000) in grouping sub-cellular structures undera single upper-level category.
For developing struc-tures that resist granularity-based categorization dueto occupying different levels at different stages ofdevelopment, we adopt a separate DEVELOPINGANATOMICAL STRUCTURE category, as done alsoin e.g.
Uberon (Haendel et al, 2009).1http://obofoundry.org/2.2 Annotation ScopeWe diverge from the scope of anatomy ontologies intwo important aspects in our annotation.First, ontologies of anatomy commonly incorpo-rate everything from molecules to whole organismswithin their scope.
However, in entity mention de-tection, many molecular level anatomical entitiesfall within the scope of the established gene/proteinmention detection tasks (e.g.
(Kim et al, 2004; Tan-abe et al, 2005)), and whole organism mentionssimilarly largely within what is covered by existingmethods and resources for organism mention detec-tion (Gerner et al, 2010b; Naderi et al, 2011).
Toavoid overlap with established tasks and to focus onthe novel aspects of anatomical entity mention de-tection, we exclude biological macromolecules andmentions of organism names from the scope of ourannotation, as argued in (Pyysalo et al, 2012b).Second, these ontologies typically representcanonical anatomy, an idealized state that is rarely(if ever) encountered in reality (Bada and Hunter,2011).
As our annotation is intended to cover ref-erences to real-world anatomy, we explicitly includein the scope of our annotation also healthy as well aspathological variants of canonical anatomy.
We in-clude also entities derived from these anatomical en-tities through (planned) processing such as surgicalor laboratory procedures, even when these processedentities are no longer properly part of the originalorganism.
Finally, we annotate pathological forma-tions such as scars and carcinomas that are part ofindividual organisms but have no correspondence incanonical anatomy (Smith et al, 2005).Table 1 presents the class labels applied in the an-notation with the corresponding ontology classes.28In contrast, the 3 cases of metastatic cancer of the GB had no blood flow signal in the wall of the GBPathological form Organ OSubst MTS OrganPart-ofPart-ofFigure 1: Example sentence with annotation.
OSUBST and MTS abbreviate for ORGANISM SUBSTANCE and MULTI-TISSUE STRUCTURE, respectively.2.3 RepresentationThe primary corpus annotation marks mentions ofanatomical entities as contiguous spans of charactersin text, each of which is assigned a type (Figure 1).As the CARO-based categorization has comprehen-sive coverage and disjoint classes, each annotationcan be assigned exactly one type (class label).In addition to identifying and typing anatomicalentity mentions, we further apply binary attributes(?flags?)
marking the following characteristics ofeach mention:DEVELOPING developing variant of anatomicalentity, e.g.
fetal liverPATHOLOGICAL pathological variant of anatomi-cal entity, e.g.
carcinoma cellPLANT anatomical entity that is part of a plant(member of the Viridiplantae kingdom), e.g.roots, leafPROCESSED variant of anatomical entity that hasundergone planned processing, e.g.
tissue spec-imenAny combination of attributes can apply to a singlemention.
These attributes allow the identification ofsubsets of annotations that may be out of scope forsome efforts (e.g.
pathological or processed entities)and facilitate the analysis of mention detection sys-tem performance by identifying particular problem-atic categories.2.4 Annotation CriteriaIn very brief summary, we annotate spans of text thatrefer to anatomical entities as defined above.
Men-tions that involve only metaphorical senses of suchentities (?on the other hand?)
or artificial analogues(?artificial heart?)
are not annotated.The primary targets of our annotation are anatom-ical entity names (e.g.
?lymphocyte?)
and nominalmentions of anatomical entities (e.g.
?muscle tis-sue?).
Both names and nominal mentions are anno-tated similarly, without distinction.
We exclude pro-nouns (it, that) from annotation even when they un-cytoplasm of phagocytic microgliaOrganism substance CellPart-ofthyroid and eye muscle membranesTissue TissueFragFigure 2: Part-of relation marking entity mention span-ning a prepositional phrase (above) and Frag relationmarking coordination with ellipsis (below).ambiguously refer to an anatomical entity; we con-sider the identification and resolution of such men-tions part of the distinct coreference resolution task(see e.g.
Pradhan et al (2011)).In addition to names and nominal mentions, wemark adjectives that have an unambiguous senseof relating to a specific anatomical entity.
Thus,for example, both ?kidney?
and ?renal?
(relating tothe kidneys) are annotated as ORGAN in expres-sions such as ?kidney failure?
and ?renal failure?.The choice to annotate adjectival references is mo-tivated by the expected needs of applications mak-ing use of automatically detected anatomical entitymentions.
For example, for semantic search target-ing documents relating to organ failure, a documentdiscussing ?renal failure?
is obviously relevant andshould be recovered.Syntactically, annotations mainly cover basenoun phrases without determiners, i.e.
nouns withpremodifiers relevant to identifying the specificanatomical entity referred to.
We exclude nounphrase postmodifiers such as prepositional phrasesfrom the span of single annotations, but apply aseparate level of annotation for part-of relationsthat allow such alternate spans to be recoveredwhen they identify an anatomical entity (Figure 2top).
Similarly, we decompose coordinated ref-erences to anatomical entities involving ellipsis tonon-overlapping spans, but mark the cases using afrag(ment) relation type (Figure 2 bottom).
(Due tospace considerations, we omit detailed discussion ofthese relation annotations.)
Together with the prop-erties described in Section 2.3, these constraints as-sure that any single token is assigned at most oneclass label and allow the annotation to be repre-29Matching criterionTask Strict Left boundary Right boundaryMention detection (single class) 89.2%/ 82.0%/ 85.4% 93.0%/ 85.5%/ 89.1% 94.6%/ 86.9%/ 90.6%Detection and classification (multi-class) 85.6%/ 78.7%/ 82.0% 87.0%/ 80.0%/ 83.3% 90.2%/ 82.9%/ 86.4%Table 2: Inter-annotator agreement results (precision / recall / F-score).sented in the standard BIO format and to be straight-forwardly applied with many existing entity mentiontaggers.By contrast to previously introduced domain re-sources for e.g.
molecular entity and organism men-tion detection (Tanabe et al, 2005; Gerner et al,2010b), we do not incorporate any specificity con-straints in our annotation criteria.
That is, non-specific expressions such as ?tissue?
and ?organ?
aremarked identically to specific ones such as ?epithe-lium?
and ?heart?.
This choice seeks to assure thegenerality of the task and methods for addressing it.2.5 Text SelectionTexts for the corpus were drawn from two sources:the PubMed2 database of publication abstracts, andthe PubMed Central3 (PMC) Open Access subsetof full-text publications.
PubMed, containing morethan 20 million citations, has a very broad coverageof domain scientific texts but is limited to publica-tion abstracts, while PMC has lower coverage butdoes provide over 400,000 full-text documents un-der open licenses.
By sampling both sources, weseek to assure the corpus is relevant to IE efforts re-gardless of their choice of texts.To avoid bias toward e.g.
subdomains of biol-ogy or specific species, we selected texts from bothsources by random sampling.
For PubMed, we sim-ply selected a random set of citations and extractedtheir abstract and title texts.
For PMC, we initiallyextracted all non-overlapping section texts (PMCXML <sec> elements) as well as caption texts(<caption> elements), and then selected a ran-dom set of extracts.
This selection seeks to maxi-mize the diversity of the texts in the full-text sec-tion of the corpus, and the selection of extracts largerthan isolated sentences aims to allow the corpus tobe used to study methods making use of broadercontext, e.g.
by incorporating constraints such asone sense per discourse (Gale et al, 1992).2http://pubmed.com3http://www.ncbi.nlm.nih.gov/pmc/We selected a total of 500 documents using thisprotocol, half from PubMed and half from PMCdocument extracts.
(Descriptive statistics of the ab-stracts and full-text extracts subcorpora are givenlater in Table 3.
)2.6 Annotation ProcessPrimary annotation was created by a PhD biologistwith extensive experience in domain information ex-traction and text annotation (TO).
The use of any rel-evant resources, such as the full article being anno-tated or species-specific anatomy ontologies in theOBO foundry, was encouraged for resolving unclearor ambiguous cases during annotation.
Initial anno-tation was produced entirely manually.
To furtherassure the quality of the annotation, a series of au-tomatic tests was performed and used as the basisof a further manual round of revision.4 Annotationguidelines were initially created based on those cre-ated by our previous domain-specific effort (Pyysaloet al, 2012a) and revised throughout the annotationeffort to document specific decisions made duringannotation.
The annotations were created using theBRAT annotation tool (Stenetorp et al, 2012).To evaluate the annotation consistency, we per-formed an inter-annotator agreement (IAA) exper-iment.
After brief training with annotation guide-lines provided by the primary annotator, a random10% of the corpus was independently annotated bya PhD computer scientist with experience in domaintext annotation and anatomy ontologies (SP).
IAAwas evaluated using the same criteria as applied inexperiments (see Section 3.4), holding the primaryannotation as gold.
The results are shown in Table 2.We find very good agreement both for mention de-tection (ignoring classification) as well as for the fulltask, indicating that the task is well defined and theannotation consistency high.4No automatically suggested annotations were incorporatedinto the corpus without manual verification.303 MethodsWe next present the methods applied in our anatomi-cal entity mention detection experiments.
We aim toevaluate the capacity of the newly annotated corpusto support reliable mention detection and to estab-lish initial baseline results for the newly introducedresource, and thus focus only on relatively straight-forward applications of existing methods.3.1 MetaMapMetaMap5 (Aronson, 2001) is a tool capable ofdetecting mentions of concepts from the exten-sive UMLS Metathesaurus (Bodenreider, 2004)in text.
The metathesaurus and MetaMap havebroad coverage of concepts relevant to biologyand medicine and provide a categorization ofconcepts into 133 semantic types, ranging fromAmino Acid to Health Care Activity toVertebrate, many directly relevant to anatomi-cal entities.
MetaMap is a key component of theprocess used by the National Library of Medicine(NLM) to index publications in the PubMeddatabase and has been applied in numerous other in-formation extraction and information retrieval tasks(Aronson and Lang, 2010).In initial experiments, we applied MetaMap totraining set documents to identify the subset of the133 semantic classes relevant to anatomy, select-ing 14 classes (including e.g.
Cell, Tissue andBody Substance) for final experiments.6 Dur-ing testing, we used command-line arguments to re-strict output to the selected semantic classes.
Thecore tagging functionality of MetaMap is rule-based,and it does not support training on tagged datafor concept mention detection.
With the exceptionof the semantic class selection, the evaluation ofMetaMap reflects an ?off-the-shelf?
application ofthe general-purpose tool.3.2 CRF taggingConditional Random Fields (CRF) (Lafferty et al,2001) are graphical models that are frequently ap-5http://metamap.nlm.nih.gov/6In brief, we tagged the training data with MetaMap, ex-tracted the subset of semantic classes giving more than 5%precision against the gold annotations, and manually analysedthese to select this subset.
The selected classes are detailed insupplementary material available on the project webpage.plied to sequence labeling tasks, and CRFs formthe basis of state-of-the-art methods for many en-tity mention tagging tasks.
We performed experi-ments using the NERsuite entity mention recogni-tion toolkit, based on the CRFsuite implementationof CRFs (Okazaki, 2007).
NERsuite provides anextensive set of features applied in entity mentiondetection, allowing the tool to achieve performancecompetitive with state-of-the-art methods for manybiomedical domain tasks through retraining with-out task-specific adaptation7.
Retraining the tool fornew tasks is also straightforward, allowing applica-tion to new tasks with modest effort.We set the L2 regularization parameter of thelearning method using held-out evaluation withtraining set data, picking out of a set of values 2n(n ?
Z) the one giving best performance.8 Otherlearning method parameters were left at default val-ues.3.3 System combinationAs a third system, we apply a straightforward com-bination of the MetaMap and CRF tagging systems,where we initially tag the data using MetaMap andthen incorporate the classes assigned by MetaMapas features for training and testing with NERsuite(stacking).
More specifically, we create a BIO-tagged version of MetaMap output segmented tomatch NERsuite tokenization, and assign each tokenthe BIO tag based on the MetaMap semantic typecode (e.g.
B-cell) as a feature.Excepting for the addition of these MetaMap-derived features, NERsuite is applied as describedabove (Section 3.2).3.4 Experimental settingWe split the corpus data into two primary parts: atraining set consisting of 60% of the documents anda test set of the remaining 40%.
The data splitswere performed independently for the two subcor-pora (abstracts and full-text extracts), using strati-fied sampling to assure broadly comparable statisti-cal properties between the sets.
The test set was heldout during development and only applied for the fi-nal experiments.7http://nersuite.nlplab.org/8Specifically, C2 = 2?5 was selected.31DatasetSource Item Train Test TotalAbst.Document 150 100 250Word 28,960 18,199 47,159Entity 1,182 764 1,946FTEDocument 150 100 250Word 26,306 17,955 44,261Entity 697 492 1,189TotalDocument 300 200 500Word 55,266 36,154 91,420Entity 1,879 1,256 3,135Table 3: Overall corpus statistics.
Statistics given sepa-rately for the abstracts (abst.)
and full-text extracts (FTE)subcorpora as well as for the total.We perform experiments in two settings: a single-class setting where the task is restricted to the detec-tion of anatomical entity mentions without classifi-cation, and a multi-class setting where the correctclass label must further be assigned to each detectedmention.
As MetaMap uses UMLS semantic classesthat do not fully align with the applied CARO-basedclasses, MetaMap is only applied in the single-classsetting.For evaluation, we adopted the protocol, crite-ria and metrics of the established BioNLP/JNLPBAshared task 2004 (Kim et al, 2004).
To assure com-patibility, we created our evaluation tool on the ba-sis of the shared task evaluation script.
The eval-uation is thus based on entity-wise (microaverage)precision/recall/F-score metrics, and tagging perfor-mance is separately evaluated under strict match, leftboundary match and right boundary match criteria.In the former setting, a predicted entity must exactlymatch the extent of a gold standard entity, while inthe latter two settings, it is enough that the left/rightboundary matches.3.5 FormatThe annotation is distributed in the standard column-based BIO format applied for e.g.
CoNLL 2003(Tjong Kim Sang and De Meulder, 2003) andJNLPBA (Kim et al, 2004) data, among other es-tablished datasets.4 Results4.1 Corpus statisticsTable 3 presents the overall corpus statistics.
Wenote that the abstracts and full-text extracts (FTE)Type CountCELL 776MULTI-TISSUE STRUCTURE 639ORGAN 381PATHOLOGICAL FORMATION 368ORGANISM SUBSTANCE 291CELLULAR COMPONENT 199TISSUE 169ORGANISM SUBDIVISION 162IMMATERIAL ANATOMICAL ENTITY 60ANATOMICAL SYSTEM 51DEVELOPING ANATOMICAL STRUCTURE 39Table 4: Annotation statistics by type.subcorpora are of comparable size in terms of theirword counts, but the number of annotations is 1.6times higher in the abstracts subcorpus (1.5 cor-recting for number of words).
This difference inanatomical entity mention density between abstractsand full texts parallels the findings of Cohen et al(2010) on the relative density of gene, drug and dis-ease mentions.
We further note that the estimateddensity of anatomical entity mentions in abstracts(approx.
41 per 1000 words) and full texts (27 per1000) are broadly comparable to the gene mentiondensity estimates of Cohen et al (61 and 47 for ab-stracts and full texts, respectively).Table 4 presents a breakdown by annotation type.There are large differences in the number of anno-tations by type, with the majority class CELL out-numbering the rarest type 20-fold.
While the totalnumber of annotated examples is likely to be suf-ficient for training machine learning-based taggersand most of the classes contain a respectable num-ber of examples, the statistics suggest that the leastfrequently annotated types may represent challengesfor learning.4.2 Entity Mention DetectionTable 5 presents the experimental results for anatom-ical entity mention detection (single-class).
In termsof F-score, we find the same ranking of the threemethods for all three criteria, with the CRF-basedtagger outperforming the rule-based MetaMap, andthe combination method outperforming its compo-nents.
Although it is not surprising that a dedicatedmachine learning-based system is capable of outper-forming a general-purpose, largely rule-based sys-tem, this result does reflect positively on both the32Matching criterionMethod Strict Left boundary Right boundaryMetaMap 50.78% / 64.49% / 56.82% 54.67% / 69.43% / 61.17% 58.18% / 73.89% / 65.10%NERsuite 77.98% / 52.15% / 62.50% 81.43% / 54.46% / 65.27% 90.00% / 60.19% / 72.14%MetaMap + NERsuite 82.09% / 62.42% / 70.92% 84.61% / 64.33% / 73.09% 90.68% / 68.95% / 78.34%Table 5: Overall single-class anatomical entity mention detection results (precision / recall / F-score).Matching criterionMethod Strict Left boundary Right boundaryNERsuite 72.07% / 42.12% / 53.17% 72.75% / 42.52% / 53.67% 85.69% / 50.08% / 63.22%MetaMap + NERsuite 75.41% / 51.75% / 61.38% 76.45% / 52.47% / 62.23% 83.99% / 57.64% / 68.37%Table 6: Overall anatomical entity mention detection and classification results (precision / recall / F-score).consistency of the annotation as well as the suffi-ciency of the size of the newly introduced corpus.In this application, we find that MetaMap tends tofavor recall over precision ?
perhaps reflecting itsfocus on IR applications (Aronson and Lang, 2010)?
while the trained machine learning-based modelsare clearly biased in favor of high precision.As expected on the basis of the results of previousevaluations using similar experimental setups (Kimet al, 2004), results are notably better under the re-laxed matching criteria.
In particular, requiring onlythe right boundaries of annotations to match yieldsF-scores nearly 10% points higher than under strictmatching.
Recalling that the annotations primar-ily mark base noun phrases, this suggests that thesystems comparatively frequently identify the headword of an anatomical entity mention correctly butdiffer from gold annotation regarding the choice ofpremodifiers included in the span of the annotation.As limited variation in premodifier selection is ar-guably acceptable for many applications and relaxedmatching criteria are frequently applied in domaintagging tasks (Kim et al, 2004; Wilbur et al, 2007),we propose to consider performance under the re-laxed right boundary match criterion as the primaryresult for evaluation using the new corpus.Table 6 presents the results for anatomical entitymention detection and classification using the 11-class categorization used in annotation.9 While per-formance in terms of F-score is approximately 10%points lower than for the single-class task, this dropis comparatively modest given the large number of9Note that evaluation using MetaMap only is not possible asits semantic classes differ from those used in the annotation.distinct classes, indicating that the number of an-notations of most individual classes is sufficient forlearning.While these initial results are not as high as forestablished entity mention detection tasks in the do-main (Wilbur et al, 2007; Rebholz-Schuhmann etal., 2011), we consider the level of performancequite good given the many new challenges relat-ing to the task.
Further, as the mention detectionmethods were also applied with only modest specificadaptation to the task, we believe there remain manyopportunities for further development of methodsfor the task.4.3 DiscussionMany commonly targeted mention types in boththe ?general?
and the biological domain are fre-quently characterized by obvious surface features:the names of people and locations are capitalized inmany languages, as are genera in scientific species?names, and many gene and chemical names havecomparable features distinguishing them from com-mon nouns (consider e.g.
p53, IgE, c-myc, Ca2+,H2SO4).
By contrast, many typical anatomical en-tity mentions are common noun compounds lackingobvious distinguishing surface features.
This factlikely contributes to the comparatively low perfor-mance of the CRF-based tagger when applied with-out support from lexical resources.A further challenge that arises comparatively fre-quently in anatomical entity mention detection isambiguity between entity mentions and other wordssharing the same surface form.
For example, whileBarack Obama, Sweden, p53 and H2SO4 can be33safely identified as mentions of a person, country,gene, and chemical without reference to context,face should not be marked as an anatomical entitymention in face the facts, nor should Airways inBritish Airways.
Thus, approaches relying on simplematching against lexical resources will not sufficefor accurate anatomical entity mention detection.Our evaluation results demonstrated a clear ad-vantage to combining detection based on lexical re-sources with machine learning-based tagging, an ap-proach we believe will be key to the further develop-ment of reliable anatomical entity mention taggingthat we will seek to explore in detail in future work.To facilitate analysis of the performance of the meth-ods, we provide the predictions of each method insupplementary data on the project homepage.5 Related workA number of domain corpora such as GENIA (Ohtaet al, 2002), BioInfer (Pyysalo et al, 2007), and therecently introduced CellFinder corpus (Neves et al,2012) include annotation for at least some classesof anatomical entities.
However, such corpora typ-ically cover only specific subdomains of the litera-ture, such as transcription factors in human bloodcells (GENIA), protein-protein interactions (BioIn-fer), or stem cells (CellFinder).
To the best of ourknowledge, this is the first effort introducing a cor-pus annotated for anatomical entity mentions thatspecifically aims to be representative of the entireavailable literature.
We note that there is a well-established precedent to this goal: sentences forthe de facto standard corpus for gene/protein namerecognition, GENETAG (Tanabe et al, 2005), weresimilarly selected from PubMed abstracts withoutdomain restrictions.The BioNLP/JNLPBA shared task 2004 (Kim etal., 2004) targeted the detection of mentions of fivetypes of biological entities, including two that wouldfall within in the scope of our CELL annotation(?Cell type?
and ?Cell line?).
Other than this com-paratively early shared task, collaborative domainefforts such as BioCreative (Krallinger et al, 2008)and CALBC (Rebholz-Schuhmann et al, 2011) havenot targeted anatomical entity mentions.Some recent studies have considered the use ofontological resources for the detection of anatomi-cal entity mentions in natural language expressions.In previous work (Pyysalo et al, 2012b), we studiedthe classification of isolated noun phrases extractedfrom PubMed to identify anatomy terms.
Travillianet al (2011) considered two lexical matching appli-cations to detect anatomical entities from two OBOresources in user-provided terms.
However, theseefforts have not involved the annotation or detectionof mentions in context, which we view as critical forreal-world entity mention detection method devel-opment and evaluation.6 ConclusionsWe have introduced a manually annotated corpus foropen-domain anatomical entity mention detection,consisting of 500 documents (over 90,000 words)drawn from publication abstracts and full texts.
Theprimary corpus annotation consists of the identifi-cation of over 3,000 references to both healthy andpathological anatomical entities, marked using a de-tailed 11-class categorization based on establishedbiomedical domain ontologies.
We demonstratedthe use of the new corpus through a comparativeevaluation of MetaMap, a general semantic classtagger; NERsuite, a CRF-based machine learningsystem; and a stacked combination of the two, find-ing that under a relaxed matching criterion, the com-bination approaches 80% F-score at mention detec-tion and 70% F-score at mention detection and clas-sification.
This level of performance is encourag-ing for a first application and suggests that reliableopen-domain anatomical entity mention detection isnot an unrealistic target.We hope that the introduced corpus can serve as areference standard for the further development andevaluation of methods for anatomical entity men-tion detection.
This corpus, the introduced evalua-tion tools, and other resources created in this studyare made available under open licences from http://www.nactem.ac.uk/anatomy/.AcknowledgmentsThis work was funded by UK Biotechnology and Bi-ological Sciences Research Council (BBSRC) underproject Automated Biological Event Extraction fromthe Literature for Drug Discovery (reference num-ber: BB/G013160/1).34ReferencesS.
Ananiadou, S. Pyysalo, J. Tsujii, and D.B.
Kell.
2010.Event extraction for systems biology by text miningthe literature.
Trends in Biotechnology, 28(7):381?390.A.R.
Aronson and F.M.
Lang.
2010.
An overview ofMetaMap: historical perspective and recent advances.Journal of the American Medical Informatics Associa-tion, 17(3):229?236.A.R.
Aronson.
2001.
Effective mapping of biomedicaltext to the UMLS Metathesaurus: the MetaMap pro-gram.
In Proceedings of AMIA, pages 17?21.M Ashburner, CA Ball, JA Blake, D Botstein, H Butler,JM Cherry, AP Davis, K Dolinski, SS Dwight, JT Ep-pig, MA Harris, DP Hill, L Issel-Tarver, A Kasarskis,S Lewis, JC Matese, JE Richardson, M Ringwald,GM Rubin, and G Sherlock.
2000.
Gene ontology:tool for the unification of biology.
Nature genetics,25:25?29.M.
Bada and L. Hunter.
2011.
Desiderata for ontologiesto be used in semantic annotation of biomedical docu-ments.
Journal of Biomedical Informatics, 44(1):94?101.O.
Bodenreider.
2004.
The unified medical languagesystem (UMLS): integrating biomedical terminology.Nucleic acids research, 32(suppl 1):D267?D270.K.B.
Cohen, H. Johnson, K. Verspoor, C. Roeder, andL.
Hunter.
2010.
The structural and content aspects ofabstracts versus bodies of full text journal articles aredifferent.
BMC bioinformatics, 11(1):492.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1992.
Onesense per discourse.
In Proceedings of the workshopon Speech and Natural Language, pages 233?237.M.
Gerner, G. Nenadic, and C.M.
Bergman.
2010a.
Anexploration of mining gene expression mentions andtheir anatomical locations from biomedical text.
InBioNLP?10, pages 72?80.M.
Gerner, G. Nenadic, and C.M.
Bergman.
2010b.LINNAEUS: a species name identification systemfor biomedical literature.
BMC bioinformatics,11(1):85+.M.A.
Haendel, F. Neuhaus, D. Osumi-Sutherland, P.M.Mabee, J.L.V.
Mejino, C.J.
Mungall, and B. Smith.2008.
CARO?the common anatomy reference ontol-ogy.
Anatomy Ontologies for Bioinformatics, pages327?349.M.A.
Haendel, G.G.
Gkoutos, S.E.
Lewis, andC.
Mungall.
2009.
Uberon: towards a comprehensivemulti-species anatomy ontology.
Nature precedings.J-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.2004.
Introduction to the bio-entity recognition task atJNLPBA.
In Proceedings JNLPBA?04.M.
Krallinger, A. Morgan, L. Smith, F. Leitner, L. Tan-abe, J. Wilbur, L. Hirschman, and A. Valencia.2008.
Evaluation of text-mining systems for biology:overview of the Second BioCreative community chal-lenge.
Genome biology, 9(Suppl 2):S1.A.
Kumar, B. Smith, and D.D.
Novotny.
2004.
Biomed-ical informatics and granularity.
Comparative andfunctional genomics, 5(6-7):501?508.J.D.
Lafferty, A. McCallum, and F.C.N.
Pereira.
2001.Conditional random fields: Probabilistic models forsegmenting and labeling sequence data.
In Proceed-ings of ICML, pages 282?289.N.
Naderi, T. Kappler, C.J.O.
Baker, and R. Witte.2011.
OrganismTagger: Detection, normalization, andgrounding of organism entities in biomedical docu-ments.
Bioinformatics.M.
Neves, A. Damaschun, A. Kurtz, and U. Leser.
2012.Annotating and evaluating text for stem cell research.In Third Workshop on Building and Evaluation Re-sources for Biomedical Text Mining (BioTxtM 2012).
(to appear).T Ohta, Y Tateisi, H Mima, and J Tsujii.
2002.
GE-NIA corpus: an annotated research abstract corpus inmolecular biology domain.
Proceedings of the HumanLanguage Technology Conference (HLT 2002), pages73?77.N.
Okazaki.
2007.
CRFsuite: a fast imple-mentation of conditional random fields (CRFs).http://www.chokkan.org/software/crfsuite/.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
CoNLL-2011shared task: Modeling unrestricted coreference inontonotes.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 1?27.S.
Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,J.
Ja?rvinen, and T. Salakoski.
2007.
BioInfer: A cor-pus for information extraction in the biomedical do-main.
BMC Bioinformatics, 8(50).S.
Pyysalo, T. Ohta, M. Miwa, H-C. Cho, J. Tsujii, andS.
Ananiadou.
2012a.
Event extraction across mul-tiple levels of biological organization.
(manuscript inreview).S.
Pyysalo, T. Ohta, J. Tsujii, and S. Ananiadou.
2012b.Learning to classify anatomical entities using openbiomedical ontologies.
Journal of Biomedical Seman-tics.
(to appear).D.
Rebholz-Schuhmann, A. Yepes, C. Li, S. Kafkas,I.
Lewin, N. Kang, P. Corbett, D. Milward, E. Buyko,E.
Beisswanger, K. Hornbostel, A. Kouznetsov,R.
Witte, J. Laurila, C. Baker, C. Kuo, S. Clematide,F.
Rinaldi, R. Farkas, G. Mora, K. Hara, L.I.
Fur-long, M. Rautschka, M. Neves, A. Pascual-Montano,35Q.
Wei, N. Collier, M. Chowdhury, A. Lavelli,R.
Berlanga, R. Morante, V. Van Asch, W. Daelemans,J.
Marina, E. van Mulligen, J. Kors, and U. Hahn.2011.
Assessment of NER solutions against the firstand second calbc silver standard corpus.
Journal ofBiomedical Semantics, 2(Suppl 5):S11.C.
Rosse and J.L.V.
Mejino.
2003.
A reference on-tology for biomedical informatics: the foundationalmodel of anatomy.
Journal of Biomedical Informat-ics, 36(6):478?500.C.
Rosse and J.L.V.
Mejino.
2008.
The foundationalmodel of anatomy ontology.
Anatomy Ontologies forBioinformatics, pages 59?117.B.
Smith, A. Kumar, W. Ceusters, and C. Rosse.
2005.On carcinomas and other pathological entities.
Com-parative and functional genomics, 6(7-8):379?387.B.
Smith, M. Ashburner, C. Rosse, J. Bard, W. Bug,W.
Ceusters, L. J Goldberg, K. Eilbeck, A. Ireland,C.J.
Mungall, N. Leontis, P. Rocca-Serra, A. Rut-tenberg, S-A Sansone, R.H. Scheuermann, N. Shah,P.L.
Whetzel, and S. Lewis.
2007.
The OBOFoundry: coordinated evolution of ontologies to sup-port biomedical data integration.
Nature biotechnol-ogy, 25(11):1251?1255.P.
Stenetorp, S. Pyysalo, G.
Topic?, T. Ohta, S. Ananiadou,and J. Tsujii.
2012. brat: a web-based tool for NLP-assisted text annotation.
In Proceedings of the EACL2012 Demonstrations, pages 102?107.L.
Tanabe, N. Xie, L. Thom, W. Matten, and W.J.Wilbur.
2005.
GENETAG: a tagged corpus forgene/protein named entity recognition.
BMC bioinfor-matics, 6(Suppl 1):S3.E.F.
Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003, pages 142?147.R.
Travillian, T. Adamusiak, T. Burdett, M. Gruenberger,J.
Hancock, A-M. Mallon, J. Malone, P. Schofield, andH.
Parkinson.
2011.
Anatomy ontologies and poten-tial users: bridging the gap.
Journal of BiomedicalSemantics, 2(Suppl 4):S3.J.
Wilbur, L. Smith, and L. Tanabe.
2007.
BioCre-ative 2 Gene Mention Task.
In Proceedings of SecondBioCreative Challenge Evaluation Workshop, pages7?16.36
