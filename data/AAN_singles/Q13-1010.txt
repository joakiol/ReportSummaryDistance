Transactions of the Association for Computational Linguistics, 1 (2013) 111?124.
Action Editor: David Chiang.Submitted 10/2012; Revised 2/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Incremental Tree Substitution Grammarfor Parsing and Sentence PredictionFederico Sangati and Frank KellerInstitute for Language, Cognition, and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKfederico.sangati@gmail.com keller@inf.ed.ac.ukAbstractIn this paper, we present the first incrementalparser for Tree Substitution Grammar (TSG).A TSG allows arbitrarily large syntactic frag-ments to be combined into complete trees;we show how constraints (including lexical-ization) can be imposed on the shape of theTSG fragments to enable incremental process-ing.
We propose an efficient Earley-based al-gorithm for incremental TSG parsing and re-port an F-score competitive with other incre-mental parsers.
In addition to whole-sentenceF-score, we also evaluate the partial trees thatthe parser constructs for sentence prefixes;partial trees play an important role in incre-mental interpretation, language modeling, andpsycholinguistics.
Unlike existing parsers, ourincremental TSG parser can generate partialtrees that include predictions about the up-coming words in a sentence.
We show thatit outperforms an n-gram model in predictingmore than one upcoming word.1 IntroductionWhen humans listen to speech, the input becomesavailable gradually as the speech signal unfolds.Reading happens in a similarly gradual mannerwhen the eyes scan a text.
There is good evidencethat the human language processor is adapted to thisand works incrementally, i.e., computes an interpre-tation for an incoming sentence on a word-by-wordbasis (Tanenhaus et al 1995; Altmann and Kamide,1999).
Also language processing systems often dealwith speech as it is spoken, or text as it is beingtyped.
A dialogue system should start interpretinga sentence while it is being spoken, and a questionanswering system should start retrieving answers be-fore the user has finished typing the question.Incremental processing is therefore essential bothfor realistic models of human language processingand for NLP applications that react to user inputin real time.
In response to this, a number of in-cremental parsers have been developed, which usecontext-free grammar (Roark, 2001; Schuler et al2010), dependency grammar (Chelba and Jelinek,2000; Nivre, 2007; Huang and Sagae, 2010), or tree-adjoining grammar (Demberg et al 2014).
Typicalapplications of incremental parsers include speechrecognition (Chelba and Jelinek, 2000; Roark, 2001;Xu et al 2002), machine translation (Schwartzet al 2011; Tan et al 2011), reading time modeling(Demberg and Keller, 2008), or dialogue systems(Stoness et al 2004).
Another potential use of incre-mental parsers is sentence prediction, i.e., the taskof predicting upcoming words in a sentence given aprefix.
However, so far only n-gram models and clas-sifiers have been used for this task (Fazly and Hirst,2003; Eng and Eisner, 2004; Grabski and Scheffer,2004; Bickel et al 2005; Li and Hirst, 2005).In this paper, we present an incremental parser forTree Substitution Grammar (TSG).
A TSG containsa set of arbitrarily large tree fragments, which can becombined into new syntax trees by means of a sub-stitution operation.
An extensive tradition of parsingwith TSG (also referred to as data-oriented parsing)exists (Bod, 1995; Bod et al 2003), but none of theexisting TSG parsers are incremental.
We show howconstraints can be imposed on the shape of the TSGfragments to enable incremental processing.
We pro-pose an efficient Earley-based algorithm for incre-mental TSG parsing and report an F-score competi-tive with other incremental parsers.111TSG fragments can be arbitrarily large and cancontain multiple lexical items.
This property enablesour incremental TSG parser to generate partial parsetrees that include predictions about the upcomingwords in a sentence.
It can therefore be applied di-rectly to the task of sentence prediction, simply byreading off the predicted items in a partial tree.
Weshow that our parser outperforms an n-gram modelin predicting more than one upcoming word.The rest of the paper is structured as follows.
InSection 2, we introduce the ITSG framework and re-late it to the original TSG formalism.
Section 3 de-scribes the chart-parser algorithm, while Section 4details the experimental setup and results.
Sections 5and 6 present related work and conclusions.2 Incremental Tree Substitution GrammarThe current work is based on Tree Substitu-tion Grammar (TSG, Schabes 1990; for a recentoverview see Bod et al2003).
A TSG is composedof (i) a set of arbitrarily large fragments, usually ex-tracted from an annotated phrase-structure treebank,and (ii) the substitution operation by means of whichfragments can be combined into complete syntacticanalyses (derivations) of novel sentences.Every fragment?s node is either a lexical node(word), a substitution site (a non-lexical node in theyield of the structure),1 or an internal node.
An inter-nal node must always keep the same daughter nodesas in the original tree.
For an example of a binarized2tree and a fragment extracted from it see Figure 1.A TSG derivation is constructed in a top-downgenerative process starting from a fragment in thegrammar rooted in S (the unique non-lexical nodeall syntactic analysis are rooted in).
A partial deriva-tion is extended by subsequently introducing morefragments: if X is the left-most substitution site inthe yield of the current partial derivation, a fragment1For example nodes NP, VP, S@ are the substitution sites ofthe right fragment in Figure 1.2The tree is right-binarized via artificial nodes with @ sym-bols, as explained in Section 4.1.
The original tree isS.?.?VPVPVBN?disclosed?VBD?were?NPNNS?Terms?SS@S@.?.
?VPVPVBN?disclosed?VBD?were?NPNNS?Terms?SS@S@VPVPVBD?were?NPFigure 1: An example of a binarized2 parse tree and alexicalized fragment extracted from it.rooted in X is chosen from the grammar and sub-stituted into it.
When there are no more substitutionsites (all nodes in the yield are lexical items) the gen-erative process terminates.2.1 IncrementalityIn this work we are interested in defining an incre-mental TSG (in short ITSG).
The new generativeprocess, while retaining the original mechanism forcombining fragments (by means of the substitutionoperation), must ensure a way for deriving syntacticanalyses of novel sentences in an incremental man-ner, i.e., one word at the time from left to right.
Moreprecisely, at each stage of the generative process, thepartially derived structure must be connected (as instandard TSG) and have a prefix of the sentence atthe beginning of its yield.
A partial derivation is con-nected if it has tree shape, i.e., all the nodes are dom-inated by a common root node (which does not nec-essarily have to be the root node of the sentence).For instance, the right fragment in Figure 1 shows apossible way of starting a standard TSG derivationwhich does not satisfy the incrementality constraint:the partial derivation has a substitution site as thefirst element in its yield.In order to achieve incrementality while maintain-ing connectedness, we impose one further constrainton the type of fragments which are allowed in anITSG: each fragment should be lexicalized, i.e., con-tain at least one word (lexical anchor) at the first orthe second position in its yield.
Allowing more thanone substitution site at the beginning of a fragment?syield would lead to a violation of the incrementalityrequirement (as will become clear in Section 2.2).112The generative process starts with a fragment an-chored in the first word of the sentence being gener-ated.
At each subsequent step, a lexicalized fragmentis introduced (by means of the substitution opera-tion) to extend the current partial derivation in sucha way that the prefix of the yield of the partial struc-ture is lengthened by one word (the lexical anchor ofthe fragment being introduced).
The lexicalizationconstraint allows a fragment to have multiple lexicalitems, not necessarily adjacent to one another.
Thisis useful to capture the general ability of TSG to pro-duce in one single step an arbitrarily big syntacticconstruction ranging from phrasal verbs (e.g., asksomeone out), to parallel constructions (e.g., eitherX or Y), and idiomatic expressions (e.g., took meto the cleaners).
For an example of a fragment withmultiple lexical anchors see the fragment in the mid-dle of Figure 2.2.2 Symbolic GrammarAn ITSG is a tuple ?N ,L ,F ,4,5,?, where Nand L are the set of non-lexical and lexical nodesrespectively, F is a collection of lexicalized frag-ments, 4 and 5 are two variants of the substitutionoperation (backward and forward) used to combinefragments into derivations, and  is the stop opera-tion which terminates the generative process.Fragments A fragment f ?F belongs to one ofthe three sets Finit ,F Xlex,FYsub:?
An initial fragment ( finit) has the lexical anchorin the first position of the yield, being the initialword of a sentence (the left-most lexical nodeof the parse tree from which it was extracted).?
A lex-first fragment ( f Xlex) has the lexical anchor(non sentence-initial) in the first position of theyield, and is rooted in X .3?
A sub-first fragment ( f Ysub) has the lexical an-chor in the second position of its yield, and asubstitution site Y in the first.Fringes We will use fringes (Demberg et al2014) as a compressed representation of fragments,3A fragment can be both an initial and a lex-first fragment(e.g., if the lexical anchor is a proper noun).
We will make useof two separate instances of the same fragment in the two sets.NPNNS?Terms?5 SS@S@.?.
?VPVPVBD?were?NP4 VPVBN?disclosed?SFigure 2: An example of an ITSG derivation yielding thetree on the left side of Figure 1.
The second and third frag-ment are introduced by means of forward and backwardsubstitution, respectively.in which the internal structure is replaced by a trian-gle (a or ) and only the root and the yield are vis-ible.
It is possible in a grammar that multiple frag-ments map to the same fringe; we will refer to thoseas ambiguous fringes.
We use both vertical (a, e.g.,in Figure 3 and 4) and horizontal () fringe nota-tion.
The latter is used for describing the states in ourchart-parsing algorithm in Section 3.
For instance,the horizontal fringe representation of the right frag-ment in Figure 1 is S  NP ?were?
VP S@.Incremental Derivation An incremental deriva-tion is a sequence of lexicalized fragments?
f1, f2, .
.
.
, fn?
which, combined together in thespecified order, give rise to a complete parse tree(see Figure 2 for an example).
The first fragment f1being introduced in the derivation must be an initialfragment, and its lexical anchor constitutes the one-word prefix of the sentence being generated.
Sub-sequent fragments are introduced by means of thesubstitution operation, which has two variants: back-ward substitution (4), which is used to substitutelex-first fragments into the partial derivation gener-ated so far, and forward substitution (5), which isused to substitute sub-first fragments into the partialderivation.
After a number of fragments are intro-duced, a stop operation () may terminate the gen-erative process.Operations The three ITSG operations take placeunder specific conditions within an incrementalderivation, as illustrated in Figure 3 and explainedhereafter.
At a given stage of the generative process(after an initial fragment has been inserted), the con-nected partial structure may or may not have sub-113Partial Structure Operation Accepted Fragment Resulting Structure TerminatedY`1 lex.
.
.
`i X ?.
.
.4(backward) X`i+1 ?.
.
.Y`1 lex.
.
.
`i+1 ?.
.
.
?.
.
.NOY`1 lex.
.
.
`i5(forward) XY `i+1 ?.
.
.X`1 lex.
.
.
`i `i+1 ?.
.
.NOY`1 lex.
.
.
`n(stop) ?Y #?`1 lex.
.
.
`n #YESFigure 3: Schemata of the three ITSG operations.
All tree structures (partial structure and fragments) are representedin a compact notation, which displays only the root nodes and the yields.
The i-th words in the structure?s yield isrepresented as `i, while ?
and ?
stands for any (possibly empty) sequence of words and substitution sites.stitution sites present in its yield.
In the first case,a backward substitution (4) must take place in thefollowing generative step: if X is the left-most sub-stitution site, a new fragment of type f Xlex is chosenfrom the grammar and substituted into X .
If the par-tially derived structure has no substitution site (allthe nodes in its yield are lexical nodes) and it isrooted in Y , two possible choices exist: either thegenerative process terminates by means of the stopoperation (Y ), or the generative process contin-ues.
In the latter case a forward substitution (5) isperformed: a new f Ysub fragment is chosen from thegrammar, and the partial structure is substituted intothe left-most substitution site Y of the fragment.4Multiple Derivations As in TSG, an ITSG maybe able to generate the same parse tree in multipleways: multiple incremental derivations yielding thesame tree.
Figure 4 shows one such example.Generative Capacity It is useful to clarify the dif-ference between ITSG and the more general TSGformalism in terms of generative capacity.
Althoughboth types of grammar make use of the substitu-tion operation to combine fragments, an ITSG im-poses more constraints on (i) the type of fragmentswhich are allowed in the grammar (initial, lex-first,4A stop operation can be viewed as a forward substitutionwhen using an artificial sub-first fragment ?Y # (stop frag-ment), where # is an artificial lexical node indicating the termi-nation of the sentence.
For simplicity, stop fragments are omit-ted in Figure 2 and 4 and Y is attached to the stop symbol (Y ).SS@S@.?.
?VPVPVBN?disclosed?VBD?were?NPNNS?Terms?NP?Terms?5 SNP ?were?
VP ?.
?4 VP?disclosed? SS?Terms?
S@4 S@?were?
VP ?.
?4 VP?disclosed? SFigure 4: Above: an example of a set of fragments ex-tracted from the tree in Figure 1.
Below: two incrementalderivations that generate it.
Colors (and lines strokes) in-dicate which derivation fragments belong to.and sub-first fragments), and (ii) the generative pro-cess with which fragments are combined (incremen-tally left to right instead of top-down).
If we com-pare a TSG and an ITSG on the same set of (ITSG-compatible) fragments, then there are cases in whichthe TSG can generate more tree structures than theITSG.In the following, we provide a more formal char-acterization of the strong and weak generative power114SX?a?X?c?XX?b?SX?c?X?c?X?c?X?b?
?a?Figure 5: Left: an example of a CFG with left recursion.Right: one of the structures the CFG can generate.of ITSG with respects to context-free grammar(CFG) and TSG.
(However, a full investigation ofthis issue is beyond the scope of this paper.)
We canlimit our analysis to CFG, as TSG is strongly equiv-alent to CFG.
The weak equivalence between ITSGand CFG is straightforward: for any CFG there isa way to produce a weakly equivalent grammar inGreibach Normal Form in which any production hasa right side beginning with a lexical item (Aho andUllman, 1972).
The grammar that results from thistransformation is an ITSG which uses only back-ward substitutions.Left-recursion seems to be the main obstacle forstrong equivalence between ITSG and CFG.
As anexample, the left side of Figure 5 shows a CFG thatcontains a left-recursive rule.
The types of structuresthis grammar can generate (such as the one given onthe right side of the same figure) are characterized byan arbitrarily long chain of rules that can intervenebefore the second word of the string, ?b?, is gener-ated.
Given the incrementality constraints, there isno ITSG that can generate the same set of struc-tures that this CFG can generate.
However, it maybe possible to circumvent this problem by applyingthe left-corner transform (Rosenkrantz and Lewis,1970; Aho and Ullman, 1972) to generate an equiv-alent CFG without left-recursive rules.2.3 Probabilistic GrammarIn the generative process presented above there area number of choices which are left open, i.e., whichfragment is being introduced at a specific stage ofa derivation, and when the generative process ter-minates.
A symbolic ITSG can be equipped witha probabilistic component which deals with thesechoices.
A proper probability model for ITSG needsto define three probability distributions over thethree types of fragments in the grammar, such that:?finit?FinitP( finit) = 1 (1)?f Xlex?FXlexP( f Xlex) = 1 (?X ?N ) (2)P(Y )+ ?fYsub?FYsubP( f Ysub) = 1 (?Y ?N ) (3)The probability that an ITSG generates a specificderivation d is obtained by multiplying the probabil-ities of the fragments taking part in the derivation:P(d) =?f?dP( f ) (4)Since the grammar may generate a tree t via multiplederivations D(t) = d1,d2, .
.
.
,dm, the probability ofthe parse tree is the sum of the probabilities of theITSG derivations in D(t):P(t) = ?d?D(t)P(d) = ?d?D(t)?f?dP( f ) (5)3 Probabilistic ITSG ParserWe introduce a probabilistic chart-parsing algorithmto efficiently compute all possible incremental de-rivations that an ITSG can generate given an inputsentence (presented one word at the time).
The pars-ing algorithm is an adaptation of the Earley algo-rithm (Earley, 1970) and its probabilistic instantia-tion (Stolcke, 1995).3.1 Parsing ChartA TSG incremental derivation is represented in thechart as a sequence of chart states, i.e., a path.For a given fringe in an incremental derivation,there will be one or more states in the chart, depend-ing on the length of the fringe?s yield.
This is be-cause we need to keep track of the extent to whichthe yield of each fringe has been consumed withina derivation as the sentence is processed incremen-tally.5 At the given stage of the derivation, the statesoffer a compact representation over the partial struc-tures generated so far.5A fringe (state) may occur in multiple derivations (paths):for instance in Figure 4 the two derivations will correspond totwo separate paths that will converge to the same fringe (state).115Start(`0) X  `0?0 : 0X ?`0?
[?,?,?
]?= ?
= P(X  `0?)
?
= ?
(1 : 0X  `0 ??
)Backward Substitution(`i)i : kX ?
?Y ?
[?,?,?]
Y  `i?i : iY ?`i?
[??,??,??]??
+= ?
?P(Y  `i?)
??
= P(Y  `i?
)Forward Substitution(`i)i : 0Y ??
[?,?,?]
X Y `i?i : 0X Y ?
`i?
[??,??,??]??
+= ?
?P(X Y `i?)??
+= ?
?P(X Y `i?)
?+= ??
?P(X Y `i?
)Completioni : jY  ` j??
[?,?,?]
j : kX ?
?Y ?
[??,??,??
]i : kX ?Y ??
[???,???,???]???
+= ??
?
????
+= ??
?
??
+= ???
?
????
+= ???
?
??
?Scan(`i) i : kX ??
`i?
[?,?,?
]i+1 : kX ?`i ??
[??,??,??]??
= ???
= ?
?
= ?
?Stop(#) n : 0Y ??
[?= ?,?]
?Y #n : 0?Y ?# [??,??,??]??
= ??
= ?
?P(Y ) ??
= 1?
= P(Y )Figure 6: Chart operations with forward (?
), inner (?
),and outer (?)
probabilities.Each state is composed of a fringe and some ad-ditional information which keeps track of where thefringe is located within a path.
A chart state can begenerally represented asi : kX ???
(6)where X ??
is the state?s fringe, Greek letters are(possibly empty) sequences of words and substitu-tion sites, and ?
is a placeholder indicating to whichextent the fragment?s yield has been consumed: allthe elements in the yield preceding the dot havebeen already accepted.
Finally, i and k are indicesof words in the input sentence:?
i signifies that the current state is introducedafter the first i words in the sentence havebeen scanned.
All states in the chart will begrouped according to this index, and will con-stitute state-set i.?
k indicates that the fringe associated with thecurrent state was first introduced in the chartafter the first k words in the input sentence hadbeen scanned.
The index k is therefore calledthe start index.For instance when generating the first incrementalderivation in Figure 4, the parser will pass throughstate 1 : 1S  NP ?
?were?
VP ?.?
indicating thatthe second fringe is introduced right after the parserhas scanned the first word in the sentence and beforehaving scanned the second word.3.2 Parsing AlgorithmWe will first introduce the symbolic part of theparsing algorithm, and then discuss its probabilisticcomponent in Section 3.3.
In line with the generativeprocess illustrated in Section 2.2, the parser operateson the chart states in order to keep track of all pos-sible ITSG derivations as new words are fed in.
Itstarts by reading the first word `0 and introducingnew states to state-set 0 in the chart, those mappingto initial fragments in the grammar with `0 as lexi-cal anchor.
At a given stage, after i words have beenscanned, the parser reads the next word (`i) and in-troduces new states in state-sets i and i+1 by apply-ing specific operations on states present in the chart,and fringes in the grammar.Parser Operations The parser begins with thestart operation just described, and continues with acycle of four operations for every word in the inputsentence `i (for i ?
0).
The order of the four opera-tions is the following: completion, backward substi-tution (4), forward substitution (5), and scan.
Whenthere are no more words in input, the parser termi-nates with a stop operation.
We will now describethe parser operations (see Figure 6 for their formaldefinition), ignoring the probabilities for now.Start(`0): For every initial fringe in the grammaranchored in `0, the parser inserts a (scan) state forthat fringe in the state-set 0.116Backward Substitution(`i) applies to acceptorstates, i.e., those with a substitution site followingthe dot, say X .
For each acceptor state in state-set i,and any lex-first fringe in the grammar rooted in Xand anchored in `i, the parser inserts a (scan) statefor that fringe in state-set i.Forward Substitution(`i) applies to donorstates, i.e., those that have no elements followingthe dot and with start index 0.
For each donor statein state-set i, rooted in Y , and any sub-first fringe inthe grammar with Y as the left-most element in itsyield, the parser inserts a (scan) state for that fringein state-set i, with the dot placed after Y .Completion applies to complete states, i.e., thosewith no elements following the dot and with startindex j > 0.
For every complete state in state-set i,rooted in Y , with starting index j, and every acceptorstate in set j with Y following the dot, the parserinserts a copy of the acceptor state in state-set i, andadvances the dot.Scan(`i) applies to scan states, i.e., those with aword after the dot.
For every scan state in state-seti having `i after the dot, the parser inserts a copy ofthat state in state-set (i+1), and advances the dot.Stop(#) is a special type of forward substitutionand applies to donor states, but only when the inputword is the terminal symbol #.
For every donor statein state-set n (the length of the sentence), if the rootof the fringe?s state is Y , the parser introduces a stopstate whose fringe is a stop fringe with Y as the leftmost substitution site.Comparison with the Earley Algorithm It isuseful to clarify the differences between the pro-posed ITSG parsing algorithm and the original Ear-ley algorithm.
Primarily, the ITSG parser is basedon a left-right processing order, whereas the Ear-ley algorithm uses a top-down generative process.Moreover, our parser presupposes a restricted in-ventory of fragments in the grammar (the ones al-lowed by an ITSG) as opposed to the general CFGrules allowed by the Earley algorithm.
In particular,the Backward Substitution operation is more limitedthan the corresponding Prediction step of the Earleyalgorithm: only lex-first fragments can be introducedusing Backward Substitution, and therefore left re-cursion (allowed by the Earley algorithm) is not pos-sible here.6 This restriction is compensated for bythe existence of the Forward Substitution operation,which has no analog in the Earley algorithm.7 Theworst case complexity of Earley algorithm is domi-nated by the Completion operation which is identicalto that in our parser, and therefore the original totaltime complexity applies, i.e., O(l3) for an input sen-tence of length l, and O(n3) in terms of the numberof non-lexical nodes n in the grammar.Derivations Incremental (partial) derivations arerepresented in the chart as (partial) paths alongstates.
Each state can lead to one or more succes-sors, and come from one or more antecedents.
Scanis the only operation which introduces, for everyscan state, a new single successor state (which canbe of any of the four types) in the following state-set.
Complete states may lead to several states withinthe current state-set, which may belong to any of thefour types.
An acceptor state may lead to a numberof scan states via backward substitution (dependingon the number of lex-first fringes that can combinewith it).
Similarly, a donor state may lead to a num-ber of scan states via forward substitution.After i words have been scanned, we can retrieve(partial) paths from the chart.
This is done in a back-ward direction starting from scan states in state-set iall the way back to the initial states.
This is possiblesince all the operations are reversible, i.e., given astate it is possible to retrieve its antecedent state(s).As an example, consider the ITSG grammar con-sisting of the fragments in Figure 7 and the two de-rivations of the same parse tree in the same figure;Figure 7 represents the parsing chart of the samegrammar, containing the two corresponding paths.3.3 Probabilistic ParserIn the probabilistic version of the parser, each fringein the grammar has a given probability, such thatEquations (1)?
(3) are satisfied.8 In the probabilisticchart, every state i : kX???
is decorated with three6This further simplifies the probabilistic version of ourparser, as there is no need to resort to the probabilistic reflex-ive, transitive left-corner relation described by Stolcke (1995).7This operation would violate Earley?s top-down constraint;donor states are in fact the terminal states in Earley algorithm.8The probability of an ambiguous fringe is the marginalprobability of the fragments mapping to it.1170 ?
?Terms?S | 0NP?
?Terms?
[1/2, 1/2, 1]|| 0S?
?Terms?
S@ [1/2, 1/2, 1]1 ?
?were?S | 0SNP ?
?were?
V P ?.?
[1/2, 1/2, 1]|| 1S@?
?were?
V P ?.?
[1/2, 1, 1/2]4 || 0S ?Terms?
?
S@ [1/2, 1/2, 1] ?
|| S@ ?were?
V P ?.?
[1]5 | 0NP ?Terms?
?
[1/2, 1/2, 1] | SNP ?were?
V P ?.?
[1]2 ?
?disclosed?S 2V P?
?disclosed?
[1, 1, 1]4 | 0SNP ?were?
?
V P ?.?
[1/2, 1/2, 1] ?
?|| 1S@ ?were?
?
V P ?.?
[1/2, 1, 1/2] ???
V P ?disclosed?
[1]3 ?
?.
?S | 0SNP ?were?
V P ?
?.?
[1/2, 1/2, 1]|| 1S@ ?were?
V P ?
?.?
[1/2, 1, 1/2]C 2V P ?disclosed?
?
[1, 1, 1] | **|| ***4 ?
#S 0?S ?
# [1, 1, 1] || 0S ?Terms?
S@ ?
[1/2, 1/2, 1]| 0SNP ?were?
V P ?.?
?
[1/2, 1/2, 1] ?S # [1]C || 1S@ ?were?
V P ?.?
?
[1/2, 1, 1/2] || *Figure 7: The parsing chart of the two derivations in Figure 4.
Blue states or fringes (also marked with |) are the ones inthe first derivation, red (||) in the second, and yellow (no marks) are the ones in common.
Each state-set is representedas a separate block in the chart, headed by the state-set index and the next word.
Each row maps to a chart operation(specified in the first column, with S and C standing for ?scan?
and ?complete?
respectively) and follows the samenotation of figure 6.
Symbols ?
are used as state placeholders.probabilities [?,?,?]
as shown in the chart examplein Figure 7.?
The forward probability ?
is the marginal prob-ability of all the paths starting with an initialstate, scanning all initial words in the sentenceuntil `i?1 included, and passing through thecurrent state.?
The inner probability ?
is the marginal proba-bility of all the paths passing through the statek : kX  ??
?, scanning words `k, .
.
.
, `i?1 andpassing through the current state.?
The outer probability ?
is the marginal prob-ability of all the paths starting with an initialstate, scanning all initial words in the sentenceuntil `k?1 included, passing through the currentstate, and reaching a stop state.Forward (?)
and inner (?)
probabilities are propa-gated while filling the chart incrementally, whereasouter probabilities (?)
are back-propagated from thestop states, for which ?
= 1 (see Figure 6).
Theseprobabilities are used for computing prefix and sen-tence probabilities, and for obtaining the most prob-able partial derivation (MPD) of a prefix, the MPDof a sentence, its minimum risk parse (MRP), and toapproximate its most probable parse (MPP).Prefix probabilities are obtained by summing overthe forward probabilities of all scan states in state-seti having `i after the dot:9P(`0, .
.
.
, `i) = ?state si:kX??`i??
(s) (7)3.4 Most Probable Derivation (MPD)The Most Probable (partial) Derivation (MPD) canbe obtained from the chart by backtracking theViterbi path.
Viterbi forward and inner probabilities9Sentence probability is obtained by marginalizing the for-ward probabilities of the stop states in the last state-set n.118(??,??)
are propagated as standard forward and in-ner probabilities except that summation is replacedby maximization, and the probability of an ambigu-ous fringe is the maximum probability among all thefragments mapping into it (instead of the marginalone).
The Viterbi partial path for the prefix `0, .
.
.
, `ican then be retrieved by backtracking from the scanstate in state-set i with max ??
: for each state, themost probable preceding state is retrieved, i.e., thestate among its antecedents with maximum ??.
TheViterbi complete path of a sentence can be obtainedby backtracking the Viterbi path from the stop statewith max ??.
Given a Viterbi path, it is possible toobtain the corresponding MPD.
This is done by re-trieving the associated sequence of fragments10 andconnecting them.3.5 Most Probable Parse (MPP)According to Equation (5), if we want to computethe MPP we need to retrieve all possible derivationsof the current sentence, sum up the probabilities ofthose generating the same tree, and returning thetree with max marginal probability.
Unfortunatelythe number of possible derivations grows exponen-tially with the length of the sentence, and computingthe exact MPP is NP-hard (Sima?an, 1996).
In ourimplementation, we approximate the MPP by per-forming this marginalization over the Viterbi-bestderivations obtained from all stop states in the chart.3.6 Minimum Risk Parse (MRP)MPD and MPP aim at obtaining the structure of asentence which is more likely as a whole under thecurrent probabilistic model.
Alternatively, we maywant to focus on the single components of a treestructures, e.g., CFG rules covering a certain span ofthe sentence, and search for the structure which hasthe highest number of correct constituents, as pro-posed by Goodman (1996).
Such structure is morelikely to obtain higher results according to standardparsing evaluations, as the objective being maxi-mized is closely related to the metric used for eval-uation (recall/precision on the number of correct la-beled constituents).10For each scan state in the path, we obtain the fragment inthe grammar that maps into the state?s fringe.
For ambiguousfringes the most probable fragment that maps into it is selected.In order to obtain the minimum risk parse (MRP)we utilize both inner (?)
and outer (?)
probabilities.The product of these two probabilities equals themarginal probability of all paths generating the en-tire current sentence and passing through the currentstate.
We can therefore compute the probability of afringe f = X ???
covering a specific span [s, t] ofthe sentence:P( f , [s, t]) = ?
(t : s f?)
??
(t : s f?)
(8)We can then compute the probability of each frag-ment spanning [s, t],11 and the probability P(r, [s, t])of a CFG-rule r spanning [s, t].12 Finally the MRP iscomputed asMRP = argmaxT ?r?T P(r, [s, t]) (9)4 ExperimentsFor training and evaluating the ITSG parser, we em-ploy the Penn WSJ Treebank (Marcus et al 1993).We use sections 2?21 for training, section 22 and 24for development and section 23 for testing.4.1 Grammar ExtractionFollowing standard practice, we start with some pre-processing of the treebank.
After removing tracesand functional tags, we apply right binarization onthe training treebank (Klein and Manning, 2003),with no horizontal and vertical conditioning.
Thismeans that when a node X has more than two chil-dren, new artificial constituents labeled X@ are cre-ated in a right recursive fashion (see Figure 1).13 Wethen replace words appearing less than five times inthe training data by one of 50 unknown word cate-gories based on the presence of lexical features asdescribed in Petrov (2009).Fragment Extraction In order to equip the gram-mar with a representative set of lexicalized frag-ments, we use the extraction algorithm of Sangati11For an ambiguous fringe, the spanning probability of eachfragment mapping into it is the fraction of the fringe?s spanningprobability with respect to the marginal fringe probability.12Marginalizing the probabilities of all fragments having rspanning [s, t].13This shallow binarization (H0V1) was used based on goldcoverage of the unsmoothed grammar (extracted from the train-ing set) on trees in section 22: H0V1 binarization results on acoverage of 88.0% of the trees, compared to 79.2% for H1V1.119et al(2010) which finds maximal fragments recur-ring twice or more in the training treebank.
To en-sure better coverage, we additionally extract one-word fragments from each training parse tree: foreach lexical node ` in the parse tree we percolateup till the root node, and for every encountered in-ternal node X0,X1, .
.
.
,Xi we extract the lexicalizedfragment whose spine is Xi ?
Xi?1 ?
.
.
.?
X0 ?
`,and where all the remaining children of the inter-nal nodes are substitution sites (see for instance theright fragment in Figure 1).
Finally, we remove allfragments which do not comply with the restrictionspresented in Section 2.1.14For each extracted fragment we keep track of itsfrequency, i.e., the number of times it occurs in thetraining corpus.
Each fragment?s probability is thenderived according to its relative frequency in thecorresponding set of fragments ( finit , f Xlex, f Ysub), sothat equations(1)?
(3) are satisfied.
The final gram-mar consists of 2.2M fragments mapping to 2.0Mfringes.Smoothing Two types of smoothing are per-formed over the grammar?s fragments: Open classsmoothing adds simple CFG rewriting rules to thegrammar for open-class15 ?PoS,word?
pairs not en-countered in the training corpus, with frequency10?6.
Initial fragments smoothing adds each lex-firstfragment f to the initial fragment set with frequency10?2 ?
freq( f ).16All ITSG experiments we report used exhaustivesearch (no beam was used to prune the search space).4.2 EvaluationIn addition to standard full-sentence parsing results,we propose a novel way of evaluating our ITSG onpartial trees, i.e., those that the parser constructs forsentence prefixes.
More precisely, for each prefix ofthe input sentence (length two words or longer) wecompute the parsing accuracy on the minimal struc-ture spanning that prefix.
The minimal structure isobtained from the subtree rooted in the minimum14The fragment with no lexical items, and those with morethan one substitution site at the beginning of the yield.15A PoS belongs to the open class if it rewrites to at least 50different words in the training corpus.
A word belongs to theopen class if it has been seen only with open-class PoS tags.16The parameters were tuned on section 24 of the WSJ.common ancestor of the prefix nodes, after pruningthose nodes not yielding any word in the prefix.As observed in the example derivations of Fig-ure 4, our ITSG generates partial trees for a givenprefix which may include predictions about unseenparts of the sentence.
We propose three new mea-sures for evaluating sentence prediction:17Word prediction PRD(m): For every prefix ofeach test sentence, if the model predicts m?
?
mwords, the prediction is correct if the first m pre-dicted words are identical to the m words followingthe prefix in the original sentence.Word presence PRS(m): For every prefix of eachtest sentence, if the model predicts m?
?
m words,the prediction is correct if the first m predicted wordsare present, in the same order, in the words followingthe prefix in the original sentence (i.e., the predictedword sequence is a subsequence of the sequence ofwords following the prefix).18Longest common subsequence LCS: For everyprefix of each test sentence, it computes the longestcommon subsequence between the sequence of pre-dicted words (possibly none) and the words follow-ing the prefix in the original sentence.Recall and precision can be computed in the usualway for these three measures.
Recall is the totalnumber (over all prefixes) of correctly predictedwords (as defined by PRD(m), PRS(m), or LCS)over the total number of words expected to be pre-dicted (according to m), while precision is the num-ber of correctly predicted words over the number ofwords predicted by the model.We compare the ITSG parser with the incrementalparsers of Schuler et al(2010) and Demberg et al(2014) for full-sentence parsing, with the Roark(2001) parser19 for full-sentence and partial pars-17We also evaluated our ITSG model using perplexity; theresults obtained were substantially worse than those obtainedusing Roark?s parsers.18Note that neither PRD(m) nor PRS(m) correspond to worderror rate (WER).
PRD requires the predicted word sequence tobe identical to the original sequence, while PRS only requiresthe predicted words to be present in the original.
In contrast,WER measures the minimum number of substitutions, inser-tions, and deletions needed to transform the predicted sequenceinto the original sequence.19Apart from reporting the results in Roark (2001), we alsorun the latest version of Roark?s parser, used in Roark et al(2009), which has higher results compared to the original work.120R P F1Demberg et al(2014) 79.4 79.4 79.4Schuler et al(2010) 83.4 83.7 83.5Roark (2001) 86.6 86.5 86.5Roark et al(2009) 87.7 87.5 87.6ITSG (MPD) 81.5 83.5 82.5ITSG (MPP) 81.6 83.6 82.6ITSG (MRP) 82.6 85.8 84.1ITSG Smoothing (MPD) 83.0 83.5 83.2ITSG Smoothing (MPP) 83.2 83.6 83.4ITSG Smoothing (MRP) 83.9 85.6 84.8Table 1: Full-sentence parsing results for sentences in thetest set of length up to 40 words.ing, and with a language model built using SRILM(Stolcke, 2002) for sentence prediction.
We used astandard 3-gram model trained on the sentences ofthe training set using the default setting and smooth-ing (Kneser-Ney) provided by the SRILM pack-age.
(Higher n-gram model do not seem appropriate,given the small size of the training corpus.)
For ev-ery prefix in the test set we compute the most prob-able continuation predicted by the n-gram model.204.3 ResultsTable 1 reports full-sentence parsing results for ourparser and three comparable incremental parsersfrom the literature.
While Roark (2001) obtains thebest results, the ITSG parser without smoothing per-forms on a par with Schuler et al(2010), and out-performs Demberg et al(2014).21 Adding smooth-ing results in a gain of 1.2 points F-score over theSchuler parser.
When we compare the different pars-ing objectives of the ITSG parser, MRP is the bestone, followed by MPP and MPD.Incremental Parsing The graphs in Figure 8 com-pare the ITSG and Roark?s parser on the incrementalparsing evaluation, when parsing sentences of length10, 20, 30 and 40.
The performance of both modelsdeclines as the length of the prefix increases, withRoark?s parser outperforming the ITSG parser onaverage, although the ITSG parser seems more com-20We used a modified version of a script by Nathaniel Smithavailable at https://github.com/njsmith/pysrilm.21Note that the scores reported by Demberg et al(2014) arefor TAG structures, not for the original Penn Treebank trees.2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length868890929496F-scoreRoark (last)ITSG Smooth.
(MPD)oark et al(2009)2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20Prefix Length868890929496F-scoreRoark (last)ITSG Smooth.
(MPD)SG Smooth.
(MPD)F-score 2 3 4 5 6 7 8 9 109192939495969798992 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20858687888990919293949596972 4 6 8 10 12 14 16 18 20 22 24 26 28 3078808284868890929496982 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 407880828486889092949698Prefix LengthFigure 8: Partial parsing results for sentences of length10, 20, 30, and 40 (from upper left to lower right).petitive when parsing prefixes for longer (and there-fore more difficult) sentences.Sentence Prediction Table 2 compares the sen-tence prediction results of the ITSG and the lan-guage model (SRILM).
The latter is outperformingthe former when predicting the next word of a pre-fix, i.e.
PRD(1), whereas ITSG is better than the lan-guage model at predicting a single future word, i.e.PRS(1).
When more than one (consecutive) wordis considered, the SRILM model exhibits a slightlybetter recall while ITSG achieves a large gain in pre-cision.
This illustrates the complementary nature ofthe two models: while the language model is betterat predicting the next word, the ITSG predicts futurewords (rarely adjacent to the prefix) with high con-fidence (89.4% LCS precision).
However, it makespredictions for only a small number of words (5.9%LCS recall).
Examples of sentence predictions canbe found in Table 3.5 Related WorkTo the best of our knowledge, there are no other in-cremental TSG parsers in the literature.
The parserof Demberg et al(2014) is closely related, but usestree-adjoining grammar, which includes both sub-stitution and adjunction.
That parser makes predic-tions, but only for upcoming structure, not for up-coming words, and thus cannot be used directlyfor sentence prediction.
The incremental parser ofRoark (2001) uses a top-down algorithm and works121ITSG SRILMCorrect R P Correct R PPRD(1) 4,637 8.7 12.5 11,430 21.5 21.6PRD(2) 864 1.7 13.9 2,686 5.3 5.7PRD(3) 414 0.9 20.9 911 1.9 2.1PRD(4) 236 0.5 23.4 387 0.8 1.0PRS(1) 34,831 65.4 93.9 21,954 41.2 41.5PRS(2) 4,062 8.0 65.3 5,726 11.3 12.2PRS(3) 1,066 2.2 53.7 1,636 3.4 3.8PRS(4) 541 1.2 53.7 654 1.4 1.7LCS 44,454 5.9 89.4 92,587 12.2 18.4Table 2: Sentence prediction results.Prefix Shares of UAL , the parent PRD(3) PRS(3)ITSG company of United Airlines , ?
?SRILM company , which is the ?
?Goldstd of United Airlines , were extremely active all dayFriday .Prefix PSE said it expects to report earnings of $ 1.3million to $ 1.7 million , or 14ITSG cents a share , ?
+SRILM % to $ UNK ?
?Goldstd cents to 18 cents a share .Table 3: Examples comparing sentence predictions forITSG and SRILM (UNK: unknown word).on the basis of context-free rules.
These are aug-mented with a large number of non-local fea-tures (e.g., grandparent categories).
Our approachavoids the need for such additional features, asTSG fragments naturally contain non-local informa-tion.
Roark?s parser outperforms ours in both full-sentence and incremental F-score (see Section 4),but cannot be used for sentence prediction straight-forwardly: to obtain a prediction for the next word,we would need to compute an argmax over thewhole vocabulary, then iterate this for each word af-ter that (the same is true for the parsers of Schuleret al 2010 and Demberg et al 2014).
Most in-cremental dependency parsers use a discriminativemodel over parse actions (Nivre, 2007), and there-fore cannot predict upcoming words either (but seeHuang and Sagae 2010).Turning to the literature on sentence prediction,we note that ours is the first attempt to use a parserfor this task.
Existing approaches either use n-grammodels (Eng and Eisner, 2004; Bickel et al 2005) ora retrieval approach in which the best matching sen-tence is identified from a sentence collection given aset of features (Grabski and Scheffer, 2004).
Thereis also work combining n-gram models with lexicalsemantics (Li and Hirst, 2005) or part-of-speech in-formation (Fazly and Hirst, 2003).In the language modeling literature, more sophis-ticated models than simple n-gram models havebeen developed in the past few years, and thesecould potentially improve sentence prediction.
Ex-amples include syntactic language models whichhave applied successfully for speech recognition(Chelba and Jelinek, 2000; Xu et al 2002) and ma-chine translation (Schwartz et al 2011; Tan et al2011), as well as discriminative language models(Mikolov et al 2010; Roark et al 2007).
Futurework should evaluate these approaches against theITSG model proposed here.6 ConclusionsWe have presented the first incremental parser fortree substitution grammar.
Incrementality is moti-vated by psycholinguistic findings, and by the needfor real-time interpretation in NLP.
We have shownthat our parser performs competitively on both fullsentence and sentence prefix F-score.
We also intro-duced sentence prediction as a new way of evaluat-ing incremental parsers, and demonstrated that ourparser outperforms an n-gram model in predictingmore than one upcoming word.The performance of our approach is likely to im-prove by implementing better binarization and moreadvanced smoothing.
Also, our model currently con-tains no conditioning on lexical information, whichis also likely to yield a performance gain.
Finally,future work could involve replacing the relative fre-quency estimator that we use with more sophisti-cated estimation schemes.AcknowledgmentsThis work was funded by EPSRC grantEP/I032916/1 ?An integrated model of syntac-tic and semantic prediction in human languageprocessing?.
We are grateful to Brian Roark forclarifying correspondence and for guidance in usinghis incremental parser.
We would also like to thankKatja Abramova, Vera Demberg, Mirella Lapata,Andreas van Cranenburgh, and three anonymousreviewers for useful comments.122ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
Thetheory of parsing, translation, and compiling.Prentice-Hall, Inc., Upper Saddle River, NJ.Gerry T. M. Altmann and Yuki Kamide.
1999.
Incre-mental interpretation at verbs: Restricting the do-main of subsequent reference.
Cognition, 73:247?264.Steffen Bickel, Peter Haider, and Tobias Scheffer.2005.
Predicting sentences using n-gram lan-guage models.
In Proceedings of the Conferenceon Human Language Technology and EmpiricalMethods in Natural Language Processing, pages193?200.
Vancouver.Rens Bod.
1995.
The problem of computing themost probable tree in data-oriented parsing andstochastic tree grammars.
In Proceedings of the7th Conference of the European Chapter of theAssociation for Computational Linguistics, pages104?111.
Association for Computer Linguistics,Dublin.Rens Bod, Khalil Sima?an, and Remko Scha.
2003.Data-Oriented Parsing.
University of ChicagoPress, Chicago, IL.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured language modeling.
Computer Speech andLanguage, 14:283?332.Vera Demberg and Frank Keller.
2008.
Data fromeye-tracking corpora as evidence for theoriesof syntactic processing complexity.
Cognition,101(2):193?210.Vera Demberg, Frank Keller, and Alexander Koller.2014.
Parsing with psycholinguistically moti-vated tree-adjoining grammar.
ComputationalLinguistics, 40(1).
In press.Jay Earley.
1970.
An efficient context-free pars-ing algorithm.
Communications of the ACM,13(2):94?102.John Eng and Jason M. Eisner.
2004.
Radiologyreport entry with automatic phrase completiondriven by language modeling.
Radiographics,24(5):1493?1501.Afsaneh Fazly and Graeme Hirst.
2003.
Testingthe efficacy of part-of-speech information in wordcompletion.
In Proceedings of the EACL Work-shop on Language Modeling for Text Entry Meth-ods, pages 9?16.
Budapest.Joshua Goodman.
1996.
Parsing algorithms andmetrics.
In Proceedings of the 34th Annual Meet-ing on Association for Computational Linguistics,pages 177?183.
Association for ComputationalLinguistics, Santa Cruz.Korinna Grabski and Tobias Scheffer.
2004.
Sen-tence completion.
In Proceedings of the 27th An-nual International ACM SIR Conference on Re-search and Development in Information Retrieval,pages 433?439.
Sheffield.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages1077?1086.
Association for Computational Lin-guistics, Uppsala.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics, pages 423?430.
Associa-tion for Computational Linguistics, Sapporo.Jianhua Li and Graeme Hirst.
2005.
Semanticknowledge in a word completion task.
In Pro-ceedings of the 7th International ACM SIGAC-CESS Conference on Computers and Accessibil-ity, pages 121?128.
Baltimore.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Com-putational Linguistics, 19(2):313?330.Tomas Mikolov, Martin Karafiat, Jan Cernocky, andSanjeev.
2010.
Recurrent neural network basedlanguage model.
In Proceedings of the 11thAnnual Conference of the International SpeechCommunication Association, pages 2877?2880.Florence.Joakim Nivre.
2007.
Incremental non-projectivedependency parsing.
In Proceedings of HumanLanguage Technologies: The Annual Conferenceof the North American Chapter of the Associa-tion for Computational Linguistics, pages 396?403.
Association for Computational Linguistics,Rochester.123Slav Petrov.
2009.
Coarse-to-Fine Natural Lan-guage Processing.
Ph.D. thesis, University ofCalifornia at Bekeley, Berkeley, CA.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tistics, 27:249?276.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical andsyntactic expectation-based measures for psy-cholinguistic modeling via incremental top-downparsing.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 324?333.
Association for ComputationalLinguistics, Singapore.Brian Roark, Murat Saraclar, and Michael Collins.2007.
Discriminative n-gram language modeling.Computer Speech and Language, 21(2):373?392.D.
J. Rosenkrantz and P. M. Lewis.
1970.
Deter-ministic left corner parsing.
In Proceedings ofthe 11th Annual Symposium on Switching and Au-tomata Theory, pages 139?152.
IEEE ComputerSociety, Washington, DC.Federico Sangati, Willem Zuidema, and Rens Bod.2010.
Efficiently extract recurring tree fragmentsfrom large treebanks.
In Nicoletta Calzolari,Khalid Choukri, Bente Maegaard, Joseph Mar-iani, Jan Odijk, Stelios Piperidis, Mike Rosner,and Daniel Tapias, editors, Proceedings of the 7thInternationalConference on Language Resourcesand Evaluation.
European Language ResourcesAssociation, Valletta, Malta.Yves Schabes.
1990.
Mathematical and Computa-tional Aspects of Lexicalized Grammars.
Ph.D.thesis, University of Pennsylvania, Philadelphia,PA.William Schuler, Samir AbdelRahman, Tim Miller,and Lane Schwartz.
2010.
Broad-coverage pars-ing using human-like memory constraints.
Com-putational Linguististics, 36(1):1?30.Lane Schwartz, Chris Callison-Burch, WilliamSchuler, and Stephen Wu.
2011.
Incremental syn-tactic language models for phrase-based transla-tion.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies, Volume 1, pages620?631.
Association for Computational Linguis-tics, Portland, OR.Khalil Sima?an.
1996.
Computational complexityof probabilistic disambiguation by means of tree-grammars.
In Proceedings of the 16th Confer-ence on Computational Linguistics, pages 1175?1180.
Association for Computational Linguistics,Copenhagen.Andreas Stolcke.
1995.
An efficient probabilis-tic context-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Process-ing, pages 257?286.
Denver, CO.Scott C. Stoness, Joel Tetreault, and James Allen.2004.
Incremental parsing with reference inter-action.
In Frank Keller, Stephen Clark, MatthewCrocker, and Mark Steedman, editors, Proceed-ings of the ACL Workshop Incremental Parsing:Bringing Engineering and Cognition Together,pages 18?25.
Association for Computational Lin-guistics, Barcelona.Ming Tan, Wenli Zhou, Lei Zheng, and ShaojunWang.
2011.
A large scale distributed syntac-tic, semantic and lexical language model for ma-chine translation.
In Proceedings of the 49thAnnual Meeting of the Association for Compu-tational Linguistics: Human Language Technolo-gies, Volume 1, pages 201?210.
Association forComputational Linguistics, Portland, OR.Michael K. Tanenhaus, Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.Sedivy.
1995.
Integration of visual and linguisticinformation in spoken language comprehension.Science, 268:1632?1634.Peng Xu, Ciprian Chelba, and Frederick Jelinek.2002.
A study on richer syntactic dependenciesfor structured language modeling.
In Proceedingsof the 40th Annual Meeting on Association forComputational Linguistics, pages 191?198.
As-sociation for Computational Linguistics, Philadel-phia.124
