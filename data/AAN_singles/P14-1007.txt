Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 69?78,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSimple Negation Scope Resolution through Deep Parsing:A Semantic Solution to a Semantic ProblemWoodley Packard?, Emily M.
Bender?, Jonathon Read?, Stephan Oepen?
?, and Rebecca Dridan?
?University of Washington, Department of Linguistics?Teesside University, School of Computing?University of Oslo, Department of Informatics?Potsdam University, Department of Linguisticsebender@uw.edu, sweaglesw@sweaglesw.org, j.read@tees.ac.uk, {oe |rdridan}@ifi.uio.noAbstractIn this work, we revisit Shared Task 1from the 2012*SEM Conference: the au-tomated analysis of negation.
Unlike thevast majority of participating systems in2012, our approach works over explicitand formal representations of proposi-tional semantics, i.e.
derives the notion ofnegation scope assumed in this task fromthe structure of logical-form meaning rep-resentations.
We relate the task-specificinterpretation of (negation) scope to theconcept of (quantifier and operator) scopein mainstream underspecified semantics.With reference to an explicit encodingof semantic predicate-argument structure,we can operationalize the annotation deci-sions made for the 2012*SEM task, anddemonstrate how a comparatively simplesystem for negation scope resolution canbe built from an off-the-shelf deep parsingsystem.
In a system combination setting,our approach improves over the best pub-lished results on this task to date.1 IntroductionRecently, there has been increased community in-terest in the theoretical and practical analysis ofwhat Morante and Sporleder (2012) call modalityand negation, i.e.
linguistic expressions that mod-ulate the certainty or factuality of propositions.Automated analysis of such aspects of meaningis important for natural language processing taskswhich need to consider the truth value of state-ments, such as for example text mining (Vinczeet al, 2008) or sentiment analysis (Lapponi et al,2012).
Owing to its immediate utility in the cura-tion of scholarly results, the analysis of negationand so-called hedges in bio-medical research liter-ature has been the focus of several workshops, aswell as the Shared Task at the 2011 Conference onComputational Language Learning (CoNLL).Task 1 at the First Joint Conference on Lex-ical and Computational Semantics (*SEM 2012;Morante and Blanco, 2012) provided a fresh, prin-cipled annotation of negation and called for sys-tems to analyze negation?detecting cues (affixes,words, or phrases that express negation), resolv-ing their scopes (which parts of a sentence are ac-tually negated), and identifying the negated eventor property.
The task organizers designed anddocumented an annotation scheme (Morante andDaelemans, 2012) and applied it to a little morethan 100,000 tokens of running text by the nov-elist Sir Arthur Conan Doyle.
While the task andannotations were framed from a semantic perspec-tive, only one participating system actually em-ployed explicit compositional semantics (Basile etal., 2012), with results ranking in the middle ofthe 12 participating systems.
Conversely, the best-performing systems approached the task throughmachine learning or heuristic processing over syn-tactic and linguistically relatively coarse-grainedrepresentations; see ?
2 below.Example (1), where ??
marks the cue and {}the in-scope elements, illustrates the annotations,including how negation inside a noun phrase canscope over discontinuous parts of the sentence.1(1) {The German} was sent for but professed to{know} ?nothing?
{of the matter}.In this work, we return to the 2012*SEMtask from a deliberately semantics-centered pointof view, focusing on the hardest of the threesub-problems: scope resolution.2Where Moranteand Daelemans (2012) characterize negation as an?extra-propositional aspect of meaning?
(p. 1563),1Our running example is a truncated variant of an itemfrom the Shared Task training data.
The remainder of theoriginal sentence does not form part of the scope of this cue.2Resolving negation scope is a more difficult sub-problemat least in part because (unlike cue and event identification) itis concerned with much larger, non-local and often discontin-uous parts of each utterance.
This intuition is confirmed byRead et al (2012), who report results for each sub-problemusing gold-standard inputs; in this setup, scope resolutionshowed by far the lowest performance levels.69we in fact see it as a core piece of composi-tionally constructed logical-form representations.Though the task-specific concept of scope ofnegation is not the same as the notion of quan-tifier and operator scope in mainstream under-specified semantics, we nonetheless find that re-viewing the 2012*SEM Shared Task annotationswith reference to an explicit encoding of seman-tic predicate-argument structure suggests a sim-ple and straightforward operationalization of theirconcept of negation scope.
Our system imple-ments these findings through a notion of functor-argument ?crawling?, using as our starting pointthe underspecified logical-form meaning represen-tations provided by a general-purpose deep parser.Our contributions are three-fold: Theoretically,we correlate the structures at play in the Moranteand Daelemans (2012) view on negation withformal semantic analyses; methodologically, wedemonstrate how to approach the task in terms ofunderspecified, logical-form semantics; and prac-tically, our combined system retroactively ?wins?the 2012*SEM Shared Task.
In the followingsections, we review related work (?
2), detail ourown setup (?
3), and present and discuss our ex-perimental results (?
4 and ?
5, respectively).2 Related WorkRead et al (2012) describe the best-performingsubmission to Task 1 of the 2012*SEM Confer-ence.
They investigated two approaches for scoperesolution, both of which were based on syntac-tic constituents.
Firstly, they created a set of 11heuristics that describe the path from the preter-minal of a cue to the constituent whose projec-tion is predicted to match the scope.
Secondlythey trained an SVM ranker over candidate con-stituents, generated by following the path from acue to the root of the tree and describing eachcandidate in terms of syntactic properties alongthe path and various surface features.
Both ap-proaches attempted to handle discontinuous in-stances by applying two heuristics to the predictedscope: (a) removing preceding conjuncts from thescope when the cue is in a conjoined phrase and(b) removing sentential adverbs from the scope.The ranking approach showed a modest advan-tage over the heuristics (with F1equal to 77.9and 76.7, respectively, when resolving the scopeof gold-standard cues in evaluation data).
Read etal.
(2012) noted however that the annotated scopesdid not align with the Shared Task?provided con-stituents for 14% of the instances in the trainingdata, giving an F1upper-bound of around 86.0 forsystems that depend on those constituents.Basile et al (2012) present the only submissionto Task 1 of the 2012*SEM Conference whichemployed compositional semantics.
Their scoperesolution pipeline consisted primarily of the C&Cparser and Boxer (Curran et al, 2007), which pro-duce Discourse Representation Structures (DRSs).The DRSs represent negation explicitly, includingrepresenting other predications as being within thescope of negation.
Basile et al (2012) describesome amount of tailoring of the Boxer lexicon toinclude more of the Shared Task scope cues amongthose that produce the negation operator in theDRSs, but otherwise the system appears to directlytake the notion of scope of negation from the DRSand project it out to the string, with one caveat: Aswith the logical-forms representations we use, theDRS logical forms do not include function wordsas predicates in the semantics.
Since the SharedTask gold standard annotations included such ar-guably semantically vacuous (see Bender, 2013,p.
107) words in the scope, further heuristics areneeded to repair the string-based annotations com-ing from the DRS-based system.
Basile et al re-sort to counting any words between in-scope to-kens which are not themselves cues as in-scope.This simple heuristic raises their F1for full scopesfrom 20.1 to 53.3 on system-predicted cues.3 System DescriptionThe new system described here is what we callthe MRS Crawler.
This system operates overthe normalized semantic representations providedby the LinGO English Resource Grammar (ERG;Flickinger, 2000).3The ERG maps surface stringsto meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al,2005).
MRS makes explicit predicate-argumentrelations, as well as partial information aboutscope (see below).
We used the grammar togetherwith one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trainedon a combination of encyclopedia articles andtourism brochures.
Thus, the deep parsing front-end system to our MRS Crawler has not been3In our experiments, we use the 1212 release of the ERG,in combination with the ACE parser (http://sweaglesw.org/linguistics/ace/).
The ERG and ACE are DELPH-IN resources; see http://www.delph-in.net.70?
h1,h4:_the_q?0:3?
(ARG0 x6, RSTR h7, BODY h5), h8:_german_n_1?4:10?
(ARG0 x6),h9:_send_v_for?15:19?
(ARG0 e10, ARG1 , ARG2 x6), h2:_but_c?24:27?
(ARG0 e3, L-HNDL h9, R-HNDL h14),h14:_profess_v_to?28:37?
(ARG0 e13, ARG1 x6, ARG2 h15), h16:_know_v_1?41:45?
(ARG0 e17, ARG1 x6, ARG2 x18),h20:_no_q?46:53?
(ARG0 x18, RSTR h21, BODY h22), h19:thing?46:53?
(ARG0 x18),h19:_of_p?54:56?
(ARG0 e23, ARG1 x18, ARG2 x24),h25:_the_q?57:60?
(ARG0 x24, RSTR h27, BODY h26), h28:_matter_n_of?61:68?
(ARG0 x24, ARG1 ){ h27=qh28, h21=qh19, h15=qh16, h7=qh8, h1=qh2} ?Figure 1: MRS analysis of our running example (1).adapted to the task or its text type; it is appliedin an ?off the shelf?
setting.
We combine oursystem with the outputs from the best-performing2012 submission, the system of Read et al (2012),firstly by relying on the latter for system negationcue detection,4and secondly as a fall-back in sys-tem combination as described in ?
3.4 below.Scopal information in MRS analyses deliveredby the ERG fixes the scope of operators?such asnegation, modals, scopal adverbs (including sub-ordinating conjunctions like while), and clause-embedding verbs (e.g.
believe)?based on theirposition in the constituent structure, while leavingthe scope of quantifiers (e.g.
a or every, but alsoother determiners) free.
From these underspec-ified representations of possible scopal configu-rations, a scope resolution component can spellout the full range of fully-connected logical forms(Koller and Thater, 2005), but it turns out that suchenumeration is not relevant here: the notion ofscope encoded in the Shared Task annotations isnot concerned with the relative scope of quantifiersand negation, such as the two possible readings of(2) represented informally below:5(2) Everyone didn?t leave.a.
?
(x)?leave(x) ?
Everyone stayed.b.
??
(x)leave(x) ?
At least some stayed.However, as shown below, the information aboutfixed scopal elements in an underspecified MRS issufficient to model the Shared Task annotations.3.1 MRS CrawlingFig.
1 shows the ERG semantic analysis for ourrunning example.
The heart of the MRS is a mul-tiset of elementary predications (EPs).
Each ele-4Read et al (2012) predicted cues using a closed vocabu-lary assumption with a supervised classifier to disambiguateinstances of cues.5In other words, a possible semantic interpretation of the(string-based) Shared Task annotation guidelines and data isin terms of a quantifier-free approach to meaning representa-tion, or in terms of one where quantifier scope need not bemade explicit (as once suggested by, among others, Alshawi,1992).
From this interpretation, it follows that the notion ofscope assumed in the Shared Task does not encompass inter-actions of negation operators and quantifiers.mentary prediction includes a predicate symbol,a label (or ?handle?, prefixed to predicates witha colon in Fig.
1), and one or more argumentpositions, whose values are semantic variables.Eventualities (ei) in MRS denote states or activ-ities, while instance variables (xj) typically corre-spond to (referential or abstract) entities.
All EPshave the argument position ARG0, called the dis-tinguished variable (Oepen and L?nning, 2006),and no variable is the ARG0 of more than one non-quantifier EP.The arguments of one EP are linked to the argu-ments of others either directly (sharing the samevariable as their value), or indirectly (through so-called ?handle constraints?, where =qin Fig.
1 de-notes equality modulo quantifier insertion).
Thusa well-formed MRS forms a connected graph.
Inaddition, the grammar links the EPs to the ele-ments of the surface string that give rise to them,via character offsets recorded in each EP (shownin angle brackets in Fig.
1).
For the purposes ofthe present task, we take a negation cue as our en-try point into the MRS graph (as our initial activeEP), and then move through the graph accordingto the following simple operations to add EPs tothe active set:Argument Crawling Add to the scope all EPswhose distinguished variable or label is an argu-ment of the active EP; for arguments of type hk,treat any =qconstraints as label equality.Label Crawling Add all EPs whose label is iden-tical to that of the active EP.Functor Crawling Add all EPs that take the dis-tinguished variable or label of the active EP as anargument (directly or via =qconstraints).Our MRS crawling algorithm is sketched inFig.
2.
To illustrate how the rules work, we willtrace their operation in the analysis of example (1),i.e.
traverse the EP graph in Fig.
1.The negation cue is nothing, from character po-sition 46 to 53.
This leads us to _no_q as our en-try point into the graph.
Our algorithm states thatfor this type of cue (a quantifier) the first step is711: Activate the cue EP2: if the cue EP is a quantifier then3: Activate EPs reached by functor crawling from the distinguished variable (ARG0) of the cue EP4: end if5: repeat6: for each active EP X do7: Activate EPs reached by argument crawling or label crawling unless they are co-modifiers of the negation cue.a8: Activate EPs reached by functor crawling if they are modal verbs, or one of the following subordinating conjunctionsreached by ARG1: whether, when, because, to, with, although, unless, until, or as.9: end for10: until a fixpoint is reached (no additional EPs were activated)11: Deactivate zero-pronoun EPs (from imperative constructions)12: Apply semantically empty word handling rules (iterate until a fixpoint is reached)13: Apply punctuation heuristicsFigure 2: Algorithm for scope detection by MRS crawlingaFormally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is =qequated with thelabel of the negation cue, it cannot be in-scope unless its ARG0 is an argument of the negation cue, or the ARG0 of the negationcue is one of its own arguments.
See ?
3.3 for elaboration.functor crawling (see ?
3.3 below), which brings_know_v_1 into the scope.
We proceed with ar-gument crawling and label crawling, which pickup _the_q?0:3?
and _german_n_1 as the ARG1.Further, as the ARG2 of _know_v_1, we reachthing and through recursive invocation we acti-vate _of_p and, in yet another level of recursion,_the_q?57:60?
and _matter_n_of.
At this point,crawling has no more links to follow.
Thus, theMRS crawling operations ?paint?
a subset of theMRS graph as in-scope for a given negation cue.3.2 Semantically Empty Word HandlingOur crawling rules operate on semantic represen-tations, but the annotations are with reference tothe surface string.
Accordingly, we need projec-tion rules to map from the ?painted?
MRS to thestring.
We can use the character offsets recordedin each EP to project the scope to the string.
How-ever, the string-based annotations also includewords which the ERG treats as semantically vacu-ous.
Thus in order to match the gold annotations,we define a set of heuristics for when to count vac-uous words as in scope.
In (1), there are no se-mantically empty words in-scope, so we illustratethese heuristics with another example:(3) ?I trust that {there is} ?nothing?
{of consequencewhich I have overlooked}?
?The MRS crawling operations discussed abovepaint the EPs corresponding to is, thing, of, conse-quence, I, and overlooked as in-scope (underlinedin (3)).
Conversely, the ERG treats the words that,there, which, and have as semantically empty.
Ofthese, we need to add all except that to the scope.Our vacuous word handling rules use the syntac-tic structure provided by the ERG as scaffolding tohelp link the scope information gleaned from con-tentful words to vacuous words.
Each node in thesyntax tree is initially colored either in-scope orout-of-scope in agreement with the decision madeby the crawler about the lexical head of the corre-sponding subtree.
A semantically empty word isdetermined to be in-scope if there is an in-scopesyntax tree node in the right position relative to it,as governed by a short list of templates organizedby the type of the semantically empty word (par-ticles, complementizers, non-referential pronouns,relative pronouns, and auxiliary verbs).As an example, the rule for auxiliary verbs likehave in our example (3) is that they are in scopewhen their verb phrase complement is in scope.Since overlooked is marked as in-scope by thecrawler, the semantically empty have becomes in-scope as well.
Sometimes the rules need to beiterated.
For example, the main rule for relativepronouns is that they are in-scope when they filla gap in an in-scope constituent; which fills a gapin the constituent have overlooked, but since haveis the (syntactic) lexical head of that constituent,the verb phrase is not considered in-scope the firsttime the rules are tried.Similar rules deal with that (complementizersare in-scope when the complement phrase is an ar-gument of an in-scope verb, which is not the casehere) and there (non-referential pronouns are in-scope when they are the subject of an in-scope VP,which is true here).723.3 Re-Reading the Annotation GuidelinesOur MRS crawling algorithm was defined by look-ing at the annotated data rather than the annota-tion guidelines for the Shared Task (Morante et al,2011).
Nonetheless, our algorithm can be seen asa first pass formalization of the guidelines.
In thissection, we briefly sketch how our algorithm cor-responds to different aspects of the guidelines.For negated verbs, the guidelines state that ?Ifthe negated verb is the main verb in the sen-tence, the entire sentence is in scope.?
(Moranteet al, 2011, 17).
In terms of our operations de-fined over semantic representations, this is ren-dered as follows: all arguments of the negatedverb are selected by argument crawling, all in-tersective modifiers by label crawling, and func-tor crawling (Fig.
2, line 8) captures modal auxil-iaries and non-intersective modifiers.
The guide-lines treat predicative adjectives under a separateheading from verbs, but describe the same desiredannotations (scope over the whole clause; ibid.,p.
20).
Since these structures are analogous in thesemantic representations, the same operations thathandle negated verbs also handle negated predica-tive adjectives correctly.For negated subjects and objects, the guidelinesstate that the negation scopes over ?all the clause?and ?the clause headed by the verb?
(Morante etal., 2011, 19), respectively.
The examples given inthe annotation guidelines suggest that these are infact meant to refer to the same thing.
The negationcue for a negated nominal argument will appearas a quantifier EP in the MRS, triggering line 3 ofour algorithm.
This functor crawling step will getto the verb?s EP, and from there, the process is thesame as the last two cases.In contrast to subjects and objects, negation ofa clausal argument is not treated as negation of theverb (ibid., p. 18).
Since in this case, the negationcue will not be a quantifier in the MRS, there willbe no functor crawling to the verb?s EP.For negated modifiers, the situation is somewhatmore complex, and this is a case where our crawl-ing algorithm, developed on the basis of the anno-tated data, does not align directly with the guide-lines as given.
The guidelines state that negated at-tributive adjectives have scope over the entire NP(including the determiner) (ibid., p. 20) and anal-ogously negated adverbs have scope over the en-tire clause (ibid., p. 21).
However, the annotationsare not consistent, especially with respect to thetreatment of negated adjectives: while the headnoun and determiner (if present) are typically an-notated as in scope, other co-modifiers, especiallylong, post-nominal modifiers (including relativeclauses) are not necessarily included:(4) ?A dabbler in science, Mr. Holmes, a picker upof shells on the shores of {the} great ?un?{knownocean}.
(5) Our client looked down with a rueful face at {his}own ?un?
{conventional appearance}.
(6) Here was {this} ?ir?
{reproachable Englishman}ready to swear in any court of law that the accusedwas in the house all the time.
(7) {There is}, on the face of it, {something}?un?
{natural about this strange and sudden friend-ship between the young Spaniard and Scott Eccles}.Furthermore, the guidelines treat relative clausesas subordinate clauses and thus negation inside arelative clause is treated as bound to that clauseonly, and includes neither the head noun of therelative clause nor any of its other dependents inits scope.
However, from the perspective of MRS,a negated relative clause is indistinguishable fromany other negated modifier of a noun.
This treat-ment of relative clauses (as well as the inconsis-tencies in other forms of co-modification) is thereason for the exception noted at line 7 of Fig.
2.By disallowing the addition of EPs to the scope ifthey share the label of the negation cue but are notone of its arguments, we block the head noun?s EP(and any EPs only reachable from it) in cases ofrelative clauses where the head verb inside the rel-ative clause is negated.
It also blocks co-modifierslike great, own, and the phrases headed by readyand about in (4)?(7).
As illustrated in these exam-ples, this is correct some but not all of the time.Having been unable to find a generalization cap-turing when comodifiers are annotated as in scope,we stuck with this approximation.For negation within clausal modifiers of verbs,the annotation guidelines have further informa-tion, but again, our existing algorithm has the cor-rect behavior: The guidelines state that a negationcue inside of the complement of a subordinatingconjunction (e.g.
if ) has scope only over the sub-ordinate clause (ibid., p. 18 and p. 26).
The ERGtreats all subordinating conjunctions as two-placepredicates taking two scopal arguments.
Thus,as with clausal complements of clause-embeddingverbs, the embedding subordinating conjunctionand any other arguments it might have are inac-cessible, since functor crawling is restricted to ahandful of specific configurations.73As is usually the case with exercises in for-malization, our crawling algorithm generalizes be-yond what is given explicitly in the annotationguidelines.
For example, all arguments that aretreated as semantically nominal (including PP ar-guments where the preposition is semanticallynull) are treated in the same way as subjects andobjects; similarly, all arguments which are seman-tically clausal (including certain PP arguments)are handled the same way as clausal complements.This is possible because we take advantage of thehigh degree of normalization that the ERG accom-plishes in mapping to the MRS representation.There are also cases where we are more spe-cific.
The guidelines do not handle coordination indetail, except to state that in coordinated clausesnegation is restricted to the clause it appears in(ibid., p. 17?18) and to include a few examples ofcoordination under the heading ?ellipsis?.
In thecase of VP coordination, our existing algorithmdoes not need any further elaboration to pick upthe subject of the coordinated VP but not the non-negated conjunct, as shown in discussion of (1) in?
3.1 above.
In the case of coordination of negatedNPs, recall that to reach the main portion of thenegated scope we must first apply functor crawl-ing.
The functor crawling procedure has a generalmechanism to transparently continue crawling upthrough coordinated structures while blocking fu-ture crawling from traversing them again.6On the other hand, there are some cases in theannotation guidelines which our algorithm doesnot yet handle.
We have not yet provided any anal-ysis of the special cases for save and expect dis-cussed in Morante et al, 2011, pp.
22?23, and alsodo not have a means of picking out the overt verbin gapping constructions (p. 24).Finally, we note that even carefully worked outannotation guidelines such as these are never fol-lowed perfectly consistently by the human annota-tors who apply them.
Because our crawling algo-rithm so closely models the guidelines, this putsour system in an interesting position to providefeedback to the Shared Task organizers.3.4 Fall-Back ConfigurationsThe close match between our crawling algorithmand the annotation guidelines supported by themapping to MRS provides for very high precision6This allows ate to be reached in We ate bread but no fish.,while preventing but and bread from being reached, whichthey otherwise would via argument crawling from ate.and recall when the analysis engine produces thedesired MRS.7However, the analysis engine doesnot always provide the desired analysis, largelybecause of idiosyncrasies of the genre (e.g.
voca-tives appearing mid-sentence) that are either nothandled by the grammar or not well modeled in theparse selection component.
In addition, as notedabove, there are a handful of negation cues we donot yet handle.
Thus, we also tested fall-back con-figurations which use scope predictions based onMRS in some cases, and scope predictions fromthe system of Read et al (2012) in others.Our first fall-back configuration (CrawlerNinTable 1) uses MRS-based predictions wheneverthere is a parse available and the cue is one thatour system handles.
Sometimes, the analysispicked by the ERG?s statistical model is not thecorrect analysis for the given context.
To com-bat such suboptimal parse selection performance,we investigated using the probability of the topranked analysis (as determined by the parse selec-tion model and conditioned on the sentence) as aconfidence metric.
Our second fall-back configu-ration (CrawlerPin Table 1) uses MRS-based pre-dictions when there is a parse available whose con-ditional probability is at least 0.5.84 ExperimentsWe evaluated the performance of our system usingthe Shared Task development and evaluation data(respectively CDD and CDE in Table 1).
Since wedo not attempt to perform cue detection, we reportperformance using gold cues and also using thesystem cues predicted by Read et al (2012).
Weused the official Shared Task evaluation script tocompute all scores.4.1 Data SetsThe Shared Task data consists of chapters fromthe Adventures of Sherlock Holmes mystery nov-els and short stories.
As such, the text is carefullyedited turn-of-the-20th-century British English,97And in fact, the task is somewhat noise-tolerant: someparse selection decisions are independent of each other, anda mistake in a part of the analysis far enough away from thenegation cue does not harm performance.8This threshold was determined empirically on the devel-opment data.
We also experimented with other confidencemetrics?the probability ratio of the top-ranked and secondparse or the entropy over the probability distribution of thetop 10 parses?but found no substantive differences.9In contrast, the ERG was engineered for the analysis ofcontemporary American English, and an anecdotal analysisof parse failures and imperfect top-ranked parses suggests74Gold Cues System CuesScopes Tokens Scopes TokensSet Method Prec Rec F1Prec Rec F1Prec Rec F1Prec Rec F1CDDRanker 100.0 68.5 81.3 84.8 86.8 85.8 91.7 66.1 76.8 79.5 84.9 82.1Crawler 100.0 53.0 69.3 89.3 67.0 76.6 90.8 53.0 66.9 84.7 65.9 74.1CrawlerN100.0 64.9 78.7 89.0 83.5 86.1 90.8 64.3 75.3 82.6 82.1 82.3CrawlerP100.0 70.2 82.5 86.4 86.8 86.6 91.2 67.9 77.8 80.0 84.9 82.4Oracle 100.0 76.8 86.9 91.5 89.1 90.3CDERanker 98.8 64.3 77.9 85.3 90.7 87.9 87.4 61.5 72.2 82.0 88.8 85.3Crawler 100.0 44.2 61.3 85.8 68.4 76.1 87.8 43.4 58.1 78.8 66.7 72.2CrawlerN98.6 56.6 71.9 83.8 88.4 86.1 86.0 54.2 66.5 78.4 85.7 81.9CrawlerP98.8 65.5 78.7 86.1 90.4 88.2 87.6 62.7 73.1 82.6 88.5 85.4Oracle 100.0 70.3 82.6 89.5 93.1 91.3Table 1: Scope resolution performance of various configurations over each subset of the Shared Taskdata.
Ranker refers to the system of Read et al (2012); Crawler refers to our current system in isolation,or falling back to the Ranker prediction either when the sentence is not covered by the parser (CrawlerN),or when the parse probability is predicted to be less than 0.5 (CrawlerP); finally, Oracle simulates bestpossible selection among the Ranker and Crawler predictions (and would be ill-defined on system cues).annotated with token-level information about thecues and scopes in every negated sentence.
Thetraining set contains 848 negated sentences, thedevelopment set 144, and the evaluation set 235.As there can be multiple usages of negation in onesentence, this corresponds to 984, 173, and 264instances, respectively.Being rule-based, our system does not requireany training data per se.
However, the majority ofour rule development and error analysis were per-formed against the designated training data.
Weused the designated development data for a singlefinal round of error analysis and corrections.
Thesystem was declared frozen before running withthe formal evaluation data.
All numbers reportedhere reflect this frozen system.104.2 ResultsTable 1 presents the results of our various config-urations in terms of both (a) whole scopes (i.e.
atrue positive is only generated when the predictedscope matches the gold scope exactly) and (b) in-scope tokens (i.e.
a true positive for every tokenthe system correctly predicts to be in scope).
Thetable also details the performance upper-bound forsystem combination, in which an oracle selects thesystem prediction which scores the greater token-wise F1for each gold cue.The low recall levels for Crawler can be mostlythat the archaic style in the 2012*SEM Shared Task textshas a strong adverse effect on the parser.10The code and data are available from http://www.delph-in.net/crawler/, for replicability (Fokkens et al,2013).attributed to imperfect parser coverage.
CrawlerN,which falls back just for parse failure brings therecall back up, and results in F1levels closer tothe system of Read et al (2012), albeit still notquite advancing the state of the art (except overthe development set).
Our best results are fromCrawlerP, which outperforms all other configura-tions on the development and evaluation sets.The Oracle results are interesting because theyshow that there is much more to be gained in com-bining our semantics-based system with the Readet al (2012) syntactically-focused system.
Furtheranalysis of these results to draw out the patterns ofcomplementary errors and strengths is a promisingavenue for future work.4.3 Error AnalysisTo shed more light on specific strengths and weak-nesses of our approach, we performed a manual er-ror analysis of scope predictions by Crawler, start-ing from gold cues so as to focus in-depth analy-sis on properties specific to scope resolution overMRSs.
This analysis was performed on CDD, inorder to not bar future work on this task.
Of the173 negation cue instances in CDD, Crawler by it-self makes 94 scope predictions that exactly matchthe gold standard.
In comparison, the system ofRead et al (2012) accomplishes 119 exact scopematches, of which 80 are shared with Crawler; inother words, there are 14 cue instances (or 8%of all cues) in which our approach can improveover the best-performing syntax-based submissionto the original Shared Task.75We reviewed the 79 negation instances whereCrawler made a wrong prediction in terms of ex-act scope match, categorizing the source of failureinto five broad error types:(1) Annotation Error In 11% of all instances, weconsider the annotations erroneous or inconsistent.These judgments were made by two of the authors,who both were familiar with the annotation guide-lines and conventions observable in the data.
Forexample, Morante et al (2011) unambiguouslystate that subordinating conjunctions shall not bein-scope (8), whereas relative pronouns should be(9), and a negated predicative argument to the cop-ula must scope over the full clause (10):(8) It was after nine this morning {when we} reachedhis house and {found} ?neither?
{you} ?nor?
{anyone else inside it}.
(9) ?We can imagine that in the confusion of flightsomething precious, something which {he could}?not?
{bear to part with}, had been left behind.
(10) He said little about the case, but from that little wegathered that he also was not ?dis?
{satisfied} at thecourse of events.
(2) Parser Failure Close to 30% of Crawler fail-ures reflect lacking coverage in the ERG parser,i.e.
inputs for which the parser does not makeavailable an analysis (within certain bounds ontime and memory usage).11In this work, we havetreated the ERG as an off-the-shelf system, butcoverage could certainly be straightforwardly im-proved by adding analyses for phenomena partic-ular to turn-of-the-20th-century British English.
(3) MRS Inadequacy Another 33% of our falsescope predictions are Crawler-external, viz.
owingto erroneous input MRSs due to imperfect disam-biguation by the parser or other inadequacies inthe parser output.
Again, these judgments (assign-ing blame outside our own work) were double-checked by two authors, and we only countedMRS imperfections that actually involve the cueor in-scope elements.
Here, we could anticipateimprovements by training the parse ranker on in-domain data or otherwise adapting it to this task.
(4) Cue Selection In close to 9% of all cases,there is a valid MRS, but Crawler fails to pick outan initial EP that corresponds to the negation cue.This first type of genuine crawling failure often re-lates to cues expressed as affixation (11), as well11Overall parsing coverage on this data is about 86%, butof course all parser failures on sentences containing negationsurface in our error analysis of Crawler in isolation.Scopes TokensMethod Prec Rec F1Prec Rec F1CDEBoxer 76.1 41.0 53.3 69.2 82.3 75.2Crawler 87.8 43.4 58.1 78.8 66.7 72.2CrawlerP87.6 62.7 73.1 82.6 88.5 85.4Table 2: Comparison to Basile et al (2012).as to rare usages of cue expressions that predomi-nantly occur with different categories, e.g.
neitheras a generalized quantifier (12):(11) Please arrange your thoughts and let me know, intheir due sequence, exactly what those events are{which have sent you out} ?un?
{brushed} and un-kempt, with dress boots and waistcoat buttonedawry, in search of advice and assistance.
(12) You saw yourself {how} ?neither?
{of the inspec-tors dreamed of questioning his statement}, extraor-dinary as it was.
(5) Crawler Deficiency Finally, a little morethan 16% of incorrect predictions we attribute toour crawling rules proper, where we see manyinstances of under-coverage of MRS elements(13, 14) and a few cases of extending the scope toowide (15).
In the examples below, erroneous scopepredictions by Crawler are indicated through un-derlining.
Hardly any of the errors in this category,however, involve semantically vacuous tokens.
(13) He in turn had friends among the indoorservants who unite in {their} fear and?dis?
{like of their master}.
(14) He said little about the case, but from thatlittle we gathered that {he also was} ?not?
{dissatisfied at the course of events}.
(15) I tell you, sir, {I could}n?t move a finger, ?nor?
{get my breath}, till it whisked away and was gone.5 Discussion and ComparisonThe example in (1) nicely illustrates the strengthsof the MRS Crawler and of the abstraction pro-vided by the deep linguistic analysis made pos-sible by the ERG.
The negated verb in that sen-tence is know, and its first semantic argument isThe German.
This semantic dependency is di-rectly and explicitly represented in the MRS, butthe phrase expressing the dependent is not adja-cent to the head in the string.
Furthermore, evena system using syntactic structure to model scopewould be faced with a more complicated task thanour crawling rules: At the level of syntax the de-pendency is mediated by both verb phrase coordi-nation and the control verb profess, as well as bythe semantically empty infinitival marker to.76The system we propose is very similar in spiritto that of Basile et al (2012).
Both systems mapfrom logical forms with explicit representations ofscope of negation out to string-based annotationsin the format provided by the Shared Task goldstandard.
The main points of difference are in therobustness of the system and in the degree of tai-loring of both the rules for determining scope onthe logical form level and the rules for handling se-mantically vacuous elements.
The system descrip-tion in Basile et al (2012) suggests relatively littletailoring at either level: aside from adjustments tothe Boxer lexicon to make more negation cues takethe form of the negation operator in the DRS, thenotion of scope is directly that given in the DRS.Similarly, their heuristic for picking up semanti-cally vacuous words is string-based and straight-forward.
Our system, on the other hand, modelsthe annotation guidelines more closely in the def-inition of the MRS crawling rules, and has moreelaborated rules for handling semantically emptywords.
The Crawler alone is less robust than theBoxer-based system, returning no output for 29%of the cues in CDE.
These factors all point tohigher precision and lower recall for the Crawlercompared to the Boxer-based system.
At the to-ken level, that is what we see.
Since full-scope re-call depends on token-level precision, the Crawlerdoes better across the board at the full-scope level.A comparison of the results is shown in Table 2.A final key difference between our results andthose of Basile et al (2012) is the cascading witha fall-back system.
Presumably a similar systemcombination strategy could be pursued with theBoxer-based system in place of the Crawler.6 Conclusion and OutlookOur motivation in this work was to take the designof the 2012*SEM Shared Task on negation analy-sis at face value?as an overtly semantic problemthat takes a central role in our long-term pursuit oflanguage understanding.
Through both theoreti-cal and practical reflection on the nature of repre-sentations at play in this task, we believe we havedemonstrated that explicit semantic structure willbe a key driver of further progress in the analy-sis of negation.
We were able to closely aligntwo independently developed semantic analyses?the negation-specific annotations of Morante et al(2011), on the one hand, and the broad-coverage,MRS meaning representations of the ERG, on theother hand.
In our view, the conceptual correla-tion between these two semantic views on nega-tion analysis reinforces their credibility.Unlike the rather complex top-performing sys-tems from the original 2012 competition, our MRSCrawler is defined by a small set of general rulesthat operate over general-purpose, explicit mean-ing representations.
Thus, our approach scoreshigh on transparency, adaptability, and replicabil-ity.
In isolation, the Crawler provides premiumprecision but comparatively low recall.
Its limi-tations, we conjecture, reflect primarily on ERGparsing challenges and inconsistencies in the tar-get data.
In a sense, our approach pushes alarger proportion of the task into the parser, mean-ing (a) there should be good opportunities forparser adaptation to this somewhat idiosyncratictext type; (b) our results can serve to offer feed-back on ERG semantic analyses and parse rank-ing; and (c) there is a much smaller proportionof very task-specific engineering.
When embed-ded in a confidence-thresholded cascading archi-tecture, our system advances the state of the arton this task, and oracle combination scores sug-gest there is much remaining room to better ex-ploit the complementarity of approaches in ourstudy.
In future work, we will seek to better un-derstand the division of labor between the systemsinvolved through contrastive error analysis andpossibly another oracle experiment, constructinggold-standard MRSs for part of the data.
It wouldalso be interesting to try a task-specific adaptationof the ERG parse ranking model, for example re-training on the pre-existing treebanks but givingpreference to analyses that lead to correct Crawlerresults downstream.AcknowledgmentsWe are grateful to Dan Flickinger, the main devel-oper of the ERG, for many enlightening discus-sions and continuous assistance in working withthe analyses available from the grammar.
Thiswork grew out of a discussion with colleagues ofthe Language Technology Group at the Universityof Oslo, notably Elisabeth Lien and Jan Tore L?n-ning, to whom we are indebted for stimulating co-operation.
Furthermore, we have benefited fromcomments by participants of the 2013 DELPH-IN Summit, in particular Joshua Crowgey, GuyEmerson, Glenn Slayden, Sanghoun Song, andRui Wang.77ReferencesAlshawi, H.
(Ed.).
1992.
The Core Language Engine.Cambridge, MA, USA: MIT Press.Basile, V., Bos, J., Evang, K., and Venhuizen, N.2012.
UGroningen.
Negation detection with Dis-course Representation Structures.
In Proceedings ofthe 1st Joint Conference on Lexical and Computa-tional Semantics (p. 301 ?
309).
Montr?al, Canada.Bender, E. M. 2013.
Linguistic fundamentals for nat-ural language processing: 100 essentials from mor-phology and syntax.
San Rafael, CA, USA: Morgan& Claypool Publishers.Copestake, A., Flickinger, D., Pollard, C., and Sag,I.
A.
2005.
Minimal Recursion Semantics.
An intro-duction.
Research on Language and Computation,3(4), 281 ?
332.Curran, J., Clark, S., and Bos, J.
2007.
Linguisticallymotivated large-scale NLP with C&C and Boxer.In Proceedings of the 45th Meeting of the Associa-tion for Computational Linguistics Demo and PosterSessions (p. 33 ?
36).
Prague, Czech Republic.Flickinger, D. 2000.
On building a more efficient gram-mar by exploiting types.
Natural Language Engi-neering, 6 (1), 15 ?
28.Fokkens, A., van Erp, M., Postma, M., Pedersen,T., Vossen, P., and Freire, N. 2013.
Offspringfrom reproduction problems.
What replication fail-ure teaches us.
In Proceedings of the 51th Meet-ing of the Association for Computational Linguistics(p. 1691 ?
1701).
Sofia, Bulgaria.Koller, A., and Thater, S. 2005.
Efficient solving andexploration of scope ambiguities.
In Proceedings ofthe 43rd Meeting of the Association for Computa-tional Linguistics: Interactive Poster and Demon-stration Sessions (p. 9 ?
12).
Ann Arbor, MI, USA.Lapponi, E., Read, J., and ?vrelid, L. 2012.
Repre-senting and resolving negation for sentiment analy-sis.
In Proceedings of the 2012 ICDM workshop onsentiment elicitation from natural text for informa-tion retrieval and extraction.
Brussels, Belgium.Morante, R., and Blanco, E. 2012.
*SEM 2012 SharedTask.
Resolving the scope and focus of negation.
InProceedings of the 1st Joint Conference on Lexicaland Computational Semantics (p. 265 ?
274).
Mon-tr?al, Canada.Morante, R., and Daelemans, W. 2012.
ConanDoyle-neg.
Annotation of negation in Conan Doyle stories.In Proceedings of the 8th International Conferenceon Language Resources and Evaluation.
Istanbul,Turkey.Morante, R., Schrauwen, S., and Daelemans, W. 2011.Annotation of negation cues and their scope guide-lines v1.0 (Tech.
Rep. # CTRS-003).
Antwerp, Bel-gium: Computational Linguistics & Psycholinguis-tics Research Center, Universiteit Antwerpen.Morante, R., and Sporleder, C. 2012.
Modality andnegation.
An introduction to the special issue.
Com-putational Linguistics, 38(2), 223 ?
260.Oepen, S., and L?nning, J. T. 2006.
Discriminant-based MRS banking.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (p. 1250 ?
1255).
Genoa, Italy.Read, J., Velldal, E., ?vrelid, L., and Oepen, S. 2012.UiO1.
Constituent-based discriminative ranking fornegation resolution.
In Proceedings of the 1st JointConference on Lexical and Computational Seman-tics (p. 310 ?
318).
Montr?al, Canada.Vincze, V., Szarvas, G., Farkas, R., M?ra, G., andCsirik, J.
2008.
The BioScope corpus.
Biomedicaltexts annotated for uncertainty, negation and theirscopes.
BMC Bioinformatics, 9(Suppl 11).78
