Experiments in Word Domain Disambiguation for ParallelTextsBernardo  Magnin i  and Car lo  S t rapparavaITC- i rs t ,  I s t i tuto  per  la R icerca  Scienti f ica e Tecnologica,  1-38050 Trento ,  ITALYemail :  {magnin i , s t rappa)@irs t .
i t  c. itAbst rac tThis paper describes ome preliminaryresults about Word Domain Disambigua-tion, a variant of Word Sense Disam-bignation where words in a text aretagged with a domain label in place of asense label.
The English WoRDNET andits aligned Italian version, MULTIWORD-NET, both augmented with domain la-bels, are used as the main informationrepositories.
A baseline algorithm forWord Domain Disambiguation is pre-sented and then compared with a mu-tual help disambignation strategy, whichtakes advantages of the shared senses ofparallel bilingual texts.1 In t roduct ionThis work describes some preliminary resultsabout Word Domain Disambignation (WDD), avariant of Word Sense Disambiguation (WSD)where for each word in a text a domain label(among those allowed by the word) has to be cho-sen instead of a sense label.
Domain labels, suchas MEDICINE and ARCHITECTURE, provide a nat-ural way to establish semantic relations amongword senses, grouping them into homogeneousclusters.
A relevant consequence of the appli-cation of domain clustering over the WORDNETsenses is the reduction of the word polysemy (i.e.the number of domains for a word is generallylower than the number of senses for that word).We wanted to investigate the hypothesis thatthe polysemy reduction caused by domain clus-tering can profitably help the word domain disam-bignation process.
A preliminary experiment hasbeen set up with two goals: first, providing exper-imental evidences that a frequency based WDDalgorithm can outperform a WSD baseline algo-rithm; second, exploring WDD in the context ofparallel, not aligned, text disambignation.The English WOR~DNET and the Italian alignedversion MULTIWORDNET, both augmented withdomain labels, are used as the main informationrepositories.
A baseline algorithm for Word Do-main Disambignation is presented and then com-pared with a mutual help disambignation strategy,which makes use of the shared senses of parallelbilingual texts.Several works in the literature have remarkedthat for many practical purposes the fine-grainedsense distinctions provided by WoRDNET are notnecessary (see for example \[Wilks and Stevenson,98\], \[Gonzalo et al, 1998\], \[Kilgarriff and Yallop,2000\] and the SENSEVAL initiative) and makeit hard word sense disambignation.
Two relatedworks are also \[Buitelaar, 1998\] and \[Buitelaar,2000\], where the reduction of the WORDNET pol-?
ysemy is obtained on the basis of regnlar polysemyrelations.
Our approach is based on sense clustersderived by domain proximity, which in some casemay overlap with regular polysemy derived clus-ters (e.g.
both "book" as composition and "book"as physical object belong to PUBLISHIN6), but inmany cases may not (e.g.
"lamb" as animal be-longs to ZOOLOQY, while "lamb" as meat belongsto FooD).
Following this line we propose WordDomain Disambiguation asa practical alternativefor applications that do not require fine grainedsense distinctions.The paper is organized as follows.
Section 2introduces domain labels, their organization andthe extensions to WORDNET.
Section 3 discussesWord Domain Disambiguation and presents thealgorithms used in the experiment.
Section 4 givesthe experimental setting.
Results are discussed inSection 5.2 WordNet  and  Sub ject  F ie ldCodesIn this work we will make use of an augmentedWoRDNET, whose synsets have been annotatedwith one or more subject field codes.
This re-source, discussed in \[Magnini and Cavagli~, 2000\],27currently covers all the noun synsets of WORD-NET 1.6 \[Miller, 1990\], and it is under develop-ment for the remaining lexical categories.Subject Field Codes (SFC) group togetherwords relevant for a specific domain.
The bestapproximation of SFCs are the field labels usedin dictionaries (e.g.
MEDICINE, A~;CHITECTURE),even if their use is restricted to word usages be-longing to specific terminological domains.
InWORDNET, too, SFCs seem to be treed occasion-ally and without a consistent desi~;n.Information brought by SFCs is complemen-tary to what is already in WoRDNET.
First ofall a SFC may include synsets of different syntac-tic categories: for instance MEDICINE 1 groups to-gether senses from Nouns, such as doctora l  andhospital#I, and from Verbs such as operate#7.Second, a SFC may also contain .,~nses from dif-ferent WORDNET sub-hierarchies (i.e.
derivingfrom different "unique beginners, or from dif-ferent "lexicographer files").
For example, theSPORT SFC contains senses such as athlete#I,deriving from li:~e~orm#1, game_equipment#1from physical_object#1, sport#1 from act#2,and playingJield#1 from location#1.We have organized about 250 SFCs in a hier-archy, where each level is made up of codes ofthe same degree of specificity: for example, thesecond level includes SFCs such as BOTANY, LIN-GUISTICS, HISTORY, SPORT and RELIGION, whileat the third level we can find specializations suchas AMERICAN.HISTORY, GRAMMAR,  PHONETICSand TENNIS.A problem arises for synsets that do not belongto a specific SFC, but rather can appear in almostall of them.
For this reason, a FACTOTUM SFC hasbeen created which basically includes two types ofsynsets:Gener/c synsets, which are hard to classify ina particular SFC, are generally placed highin the WoRDNET hierarchy and are relatedsenses of highly polysemous words.
For ex-ample:man#1 an adult male person (as opposed to awoman)man#3 the generic use of the word to refer to anyhuman beingdate#1 day of the monthaThroughout the paper subject field codes are inocUcated with this TYPEFACE while word senses are re-ported with this typeface#l, with their correspondingnumbering in WORDNET 1.6.
Moreover, we use sub.ject field code, domain label and semantic field withthe same meaning.dal;e#3 appointment, engagement?
Stop Senses ynsets which appear frequentlyin different contexts, such as numbers, weekdays, colors, etc.
These synsets usually be-long to non polysemous words and they be-have much as stop words, because they do notsignificantly contribute to the overall mean-ing of a text.A single domain label may group together morethan one word sense, resulting in a reduction ofthe polysemy.
Figure 1 shows an example.
Theword "book" has seven different senses in WORD-NET 1.6: three of them are grouped under thePUBLISHING domain, causing the reduction of thepolysemy from 7 to 5 senses.3 Word  Domain  D isambiguat ionIn this section we present wo baseline algorithmsfor word domain disambiguation and we proposesome variants of them to deal with WDD in thecontext of parallel texts.3.1 Basel ine a lgor i thmsTo decide a proper baseline for Word Domain Dis-ambiguation we wanted to be sure that it was ap-plicable to both the languages (i.e.
English andItalian) used in the experiment.
This caused theexclusion of a selection based on the domain fre-quency computed as a function of the frequencyof the WORDNET senses, because we did nothave a frequency estimation for Italian senses.We adopted two alternative frequency measures,based respectively on the intra text frequency andthe intra word frequency of a domain label.
Bothof them are computed with a two-stage disam-bignation process, structurally similar to the al-gorithm used in \[Voorhees, 1998\].Baseline 1: Intra text domain  frequency.The baseline algorithm follows two steps.
First,all the words in the text are considered and foreach domain label allowed by the word the labelscore is incremented by one.
At the second stepeach word is reconsidered, and the domain label(or labels, depending on how many best solutionsare requested) with the highest score is selectedas the result of the disambiguation.Basel ine 2: In t ra  word  domain  f requency.In this version of the baseline algorithm, step 1 ismodified in that each domain label allowed by theword is incremented by the frequency of the la-bel among the senses of that word.
For instance,28{book#1 - p~blishtd co, npositio.
}PUBLISHING"book"{book#2 volume#3 - book a.?
a physical objea}{daybook#2 book#7 ledger#2 - an accounting book as aphisical object}{book#6 - book of the B/b/e} PtrBHSmNG RELIGIONTHEA~{script#1 book#4 playscript#1-mriuenvemionojraplay}~ ok#1COM~nZCEbook#5 ledger#l - recorda of commercial acc.ount}FACTOTt~record#5 recordbook#1 book#3-compilationoflmowfact~regaMing $ome~ing or someone}Figure I: An  example of polysemy reductionif "book" is the word (see Figure 1), PUBLISH-ING will receive .42 (i.e.
three senses out of sevenbelong to PUBLISHING), while the others domainlabels will receive .14 each.3.1.1 The  " fac to tum"  effectAs we mentioned in Section 2, a FACTOTUM la-bel is used to mark WORDNET senses that do notbelong to a specific domain, but rather are highlywidespread across texts of different domains.
Aconsequence is that very often, at the end of step 1of the disambignation algorithm, FACTOTUM out-performs the other domains, this way affecting theselection carried out at step 2 (i.e.
in case of am-biguity FACTOTUM is often preferred).For the purposes of the experiment describedin the next sections the FACTOTUM problem hasbeen resolved with a slight modification at step 2of the baseline algorithm: when FACTOTUM is thebest selection for a word, also the second availablechoice is considered as a result of the disambigua-tion process.3.2 Extens ions  for paral le l  textsWe started with the following working hypothe-sis.
Using aligned wordnets to disambiguate par-allel texts allows us to calculate the intersectionamong the synsets accessible from an English textthrough the English WoRDNET and the synsetsaccessible from the parallel Italian text throughthe Italian WORDNET.
It would seem reasonablethat the synset intersection maximizes the num-ber of significant synsets for the two texts, andat the same time tends to exclude synsets whosemeaning is not pertinent to the content of the text.Let us try to make the point clearer with anexample.
Suppose we find in an English text theword "bank" and in the Italian parallel text theword "banca',  which we do not know being thetranslation of "bank", because we do not haveword alignments.
For "bank" we get ten sensesfrom WORDNET 1.6 (reported in Figure 2), whilefor "banca" we get two senses from MULTIWORD-NET (reported in Figure 2).
As the two wordnetsare aligned (i.e.
they share synset offsets), theintersection can be straightforwardly determined.In this case it includes 06227059, corresponding tobank#1 and banca#1, and 02247680, correspond-ing to bank#4 and banca#2, which both pertainto the BANKING domain, and excludes, among theothers, bank#2, which happens to be an homonymsense in English but not in Italian.Incidentally, if "istituto di credito" were not inthe synset 06227059 (e.g.
because of the incom-pleteness of the Italian WORDNET) and it werethe only word present in the Italian news to de-notate the bank#1 sense, the synset intersectionwould have been empty.As far as disambiguation is concerned it seemsa reasonable hypothesis that the synset intersec-tion could bring constraints on the sense selectionfor a word (i.e.
it is highly probable that the cor-rect choice belongs to the intersection).
Followingthis line we have elaborated a mutua l  he lp  d i sam-biguation strategy where the synset intersectioncan be accessed to help the disambiguation pro-cess of both English and Italian texts.In addition to the synset intersection, wewanted to consider the intersection of domain la-bels, that is domains that are shared among the29Bank (from WordNet 1.6)1.
J{06227059}\[ depos i tory  f inanc ia l  ins t i tu t ion ,  bank, banking concern, banking companyI I-- (a financial institution that accepts deposits and channels the money into lendingact iv i t ies ;  )2.
{06800223} bank -- (sloping land (especially the slope beside a body of water))3.
{09626760} bank -- (a supply or stock held in reserve especially for future use(especially in emergencies))4.
{02247680}\[ bank, bank building -- (a building in which commercial banking istransacted; )B5.
{06250735} bank -- (an ~rrangement of similar objects in a row or in tiers; )6.
{03277560} savings bank, coin bank, money box, bank -- (a container (usually with aslot in the top) for keeping money at home;)7.
{06739355} bank -- (a long ridge or pile; "a huge bank of earth")8.
{09616845} bank -- (the :funds held by a gambling house or the dealer in some gamblinggames; )9.
{06800468} bank, cant, camber -- (a slope in the turn of a road or track;)I0.
{00109955} bank -- (a flight maneuver; aircraft tips laterally about its longitudinalaxis  (espec ia l l y  in turn ing) )Banca (from MultiWordnet)1.
1{06227059}1 i s t i tu to .d i _c red i to  cassa banco bancaq BI{0  4,680}\[ ban.Figure 2: An example of sysnet intersection i  MULTIWORDNETsenses of the parallel texts.
In the example abovethe domain intersection would include just one la-bel (i.e.
BANKING), in place of the two synsetsof the synset intersection.
The hypothesis i thatdomain intersection could reduce problems due topossible misalignments among the synsets of thetwo wordnets.Two mutual help algorithms have been imple-mented, weak mutual help and strong mutual help,which are described in the following.Weak Mutua l  help.
In this version of the mu-tual help algorithm, step 1 of the baseline is mod-itied in that, if the domain label is found in thesynset or domain intersection, a bonus is assignedto that label, doubling its score.
In case of emptyintersection (i.e.
either no synset or no domain isshared by the two texts) this algorithm guaranteesthe same performances of the baseline.Strong Mutua l  help.
In the strong version ofthe mutual help strategy, step 1 of the baselineis modified in that the domain label is scored ifand only if it is found in the synset or domainintersection.
While this algorithm does not guar-antee the baseline performance (because the inter-section may not contain all the correct synsets ordomains), the precision score will give us indica-tions about the quality of the synset intersection.4 Exper imenta l  Set t ingThe goal of the experiment is to establish somereference figures for Word Domain Disambigua-tion.
Only nouns have been considered, mostlybecause the coverage of both MULTIWORDNETand of the domain mapping for verbs is far frombeing complete.Lemmas I Senses \[Mean PolysemyWN 1.6 94474 116317 1.23Ital WN 19104 25226 1.32DISC 56134 118029 2.10Table 1: Overview of the used resources (Nounpart of speech)4.1 Lexlcal resourcesBesides the English WORDNET 1.6 we used MUL-TIWoRDNET \[Artale et al, 1997; Magnini andStrapparava, 1997\], an Italian version of the En-glish WoRDNET.
It is based on the assump-tion that a large part of the conceptual relationsdefined for the English language can be sharedwith Italian.
From an architectural point of view,MULTIWORDNET implements an extension of theWoRDNET lexical matrix to a "multilingual lexi-30Mean Values for Nouns Italian News English NewsLexical Coverage WN 1.6ItalWNDisc# Synsets EnglishItalianIntersection93%100%111.3835.4898%155.21Table 2: Mean lexical coverage and synset amount for AdnKronos newsMean Values for Nouns \] Italian News English NewsSense Polysemy WN 1.6ItalWNDiscDomain Polysemy EnglishItalian3.226.822.684.373.58Table 3: Mean sense and domain polysemy for AdnKronos newscal matrix" through the addition of a third dimen-sion relative to the language.
MULTIWORDNETcurrently includes about 30,000 lemmas.As a matter of comparison, in particular to es-timate the lack of coverage of MULTIWORDNET,we consider some data from the Italian dictionary"DISC" \[Sabatini and Coletti, 1997\], a large sizemonolingual dictionary, available both as printedversion and as CD-ROM.Table 1 shows some general figures (only fornouns) about the number of lemmas, the numberof senses and the average polysemy for the threelexical resources considered.4.2 Para l le l  TextsExperiments have been carried out on a news cor-pus kindly placed at our disposal by AdnKronos,an important Italian news provider.
The corpusconsists of 168 parallel news (i.e.
each news hasboth an Italian and an English version) concerningvarious topics (e.g.
politics, economy, medicine,food, motors, fashion, culture, holidays).
The av-erage length of the news is about 265 words.Table 2 reports the average lexical coverage (i.e.percent of lemmas found in the news corpus) forWORDNET 1.6, MULT IWORDNET and the Discdictionary.
A practically zero variance among thevarious news is exhibited.
We observe a full cov-erage for the Disc dictionary; in addition, the in-completeness of MULT IWORDNET is limited to5% with respect to WoRDNET 1.6.
The tablealso reports the average amount of unique synsetsfor each news.
In this case the incompleteness ofItalian WoRDNET with respect to WORDNET 1.6raises to 30%, showing that a significant amountof word senses is missing.Table 3 shows the average polysemy of the newscorpus considering both word senses and word do-main labels.
The figures reveal a polysemy reduc-tion of 17-18% when we deal with domain poly-semy.Manua l  Annotation.
A subset of forty newspairs (about half of the initial corpus) have beenmanually annotated with the correct domain la-bel.
Annotators were instructed about the domainhierarchy and then asked to select one domain la-bel for each lemma among those allowed by thatlemma.Uncertain cases have been reviewed by a sec-ond annotator and, in case of persisting conflict, athird annotator was consulted to take a decision.Lemmatization errors as well as cases of incom-plete coverage of domain labels have been detectedand excluded.
The whole manual set consists ofabout 2500 annotated nouns.Although we do not have empirical evidences,our practical experience confirms the intuitionthat annotating texts with domain labels is aneasier task than sense annotation.Forty-two domain labels, representing the moreinformative level of the domain hierarchy men-tioned in Section 1, have been used for the ex-periment.
Table 4 reports the complete list.5 Resu l ts  and  D iscuss ionWSD and WDD on the  Semcor  Brown Cor-pus.
In the first experiment we wanted to verifythat, because of the polysemy reduction inducedby domain clustering, WDD is a simpler task than31administrationartcommercefashionmathematicsplaysociologyagricultureartisanshipcomputer.sciencehistorymedicinepoliticssportalimentationastrologyearthindustrymilitarypsychologytelecommunicationanthropologyastronomyeconomylawpedagogypublishingtourismarchaeologybiologyengineeringlinguisticsphilosophyreligiontransportarchitecturechemistryfactotumliteraturephysicssexualityveterinaryTable 4: Domain labels used in the experiment.Baseline 1 Baseline ~,) Weak Mutual (baseline 2)Synset Inter.
I Domain Inter.Italian .83 .86 .87 .88English .85 .86 .87 .87Stron 0 Mutual (baseline ~)Synset Inter.
I Domain Inter..74 1.68 I .77 / .91.7o/.57 I .8o/.9Table 5: Precision and B,ecall (English and Italian) for different WDD algorithmsWSD.
For the experiment we used a subset of theSemcor corpus.
As for WSD we obtained .66 ofcorrect disambiguation with a sense frequency al-gorithm on polysemous noun words and .80 on allnouns (this last is also reported in the literature,for example in \[Mihalcea nd Moldovan, 1999\]).As for WDD, precision has been computed consid-ering the intersection between the word senses be-longing to the domain label with the higher scoreand the sense tag for that word reported in Sem-cor.
Baseline I and baseline 2, described in section3.1, respectively gave .81 and .82 in precision, witha significant improvement over the WSD baseline,which confirms the initial hypothesis.WDD in paral le l  texts .
In this experimentwe wanted to test WDD in the context of par-allel texts.
Table 5 reports the precision and re-call (just in case it is not I) scores for six dif-ferent WDD algorithms applied to parallel En-glish/Italian texts.
Numbers refer to polysemouswords only.Both the baseline algorithms perfbrm quite well:83% for Italian and 85% for English in case ofbaseline 1, and 86% for both languages in case ofbaseline 2 are similar to the results obtained onthe SemCor corpus.The algorithm which includes word domain fre-quency (i.e.
baseline 2) reaches the highest scorein both languages, indicating that the combina-tion of domain word frequency (considered at step1 of the algorithm) and domain text frequency(considered at step 2) is a good one.
In addition,the fact that results are the same for both lan-guages indicates that the method can smooth thecoverage differences among the wordnets.We expected a better esult for the bilingual ex-tensions.
The weak mutual strategy, either con-sidering the synset intersection or the domain la-bels intersection, brings just minor improvementswith respect o the baselines; the strong mutualstrategy lowers both the precision and the recall.There are several explanations for these results.The difference in sense coverage between the twowordnets, about 30%, may affect the quality ofthe synset intersection: this would also explainthe low degree of recall (68% for Italian and 57%for English).
This is particularly evident for thestrong mutual strategy, where the relative lexi-cal poorness of the Italian synsets can stronglyreduce the number of synsets in the intersection.Note also that the length of the synset intersec-tion is about 30-40% of the mean synset numberfor Italian and English news respectively.
Thismeans less material which the disambiguation al-gorithms can take advantage of: relevant sysnsetscan be left out of the intersection.
For these rea-sons it is crucial having wordnet resources at thesame level of completion to exploit he mutual helphypothesis.Furthermore, there may be a significant amountof senses which are "quasi" aligned.
This mayhappen when two parallel senses map into closesynsets, but not in the same one (e.g.
one is the di-rect hypernym of the other).
This problem couldbe overcome considering the IS-A relations duringthe computation of the intersection.
In this situ-ation it is also probable that the senses maintainthe same domain label.
This would explain whythe domain intersection behaves better than thesynset intersection (from 74%-68% to 77%-91%for the Italian and from 70%-57% to 80%-91% forthe English).326 Conc lus ionsWe have introduced Word Domain Disambigua-tion, a variant of Word Sensse Disambiguationwhere words in a text are tagged with a domainlabel in place of a sense label.
Two baseline algo-rithms has been presented as well as some exten-sions to deal with domain disambiguation i  thecontext of parallel translation texts.Two aligned wordnets, the English WORDNET1.6 and the Italian MULTIWORDNET, both aug-mented with domain labels, have been used as themain information repositories.The experimental results encourage to furtherinvestigate the potentiality of word domain dis-ambiguation.
There are two interesting perspec-tives for the future work: first, we want to ex-ploit the relations among different lexical cate-gories (mainly nouns and verbs) when they sharethe same domain label; second, it seems reason-able that the disambiguation process may take ad-vantage of both WDD and WSD, where the initialword ambiguity is first reduced with WDD andthen resolved with more fine grained information.Finally, an in-depth investigation is necessary forwhat we called factotum effect, which is peculiarof WDD.As for the applicative scenarios, we want to ap-ply WDD to the problem of content based usermodelling.
In particular we are developing a per-sonal agent for a news web site that learns user'sinterests from the requested pages that are an-alyzed to generate or to update a model of theuser \[Strapparava et al, 2000\].
Exploiting thismodel, the system anticipates which documentsin the web site could be interesting for the user.Using MULTIWORDNET and domain disambigua-tion algorithms, a content-based user model canbe built as a semantic network whose nodes, in-dependent from the language, represent the wordsense frequency rather then word frequency.
Fur-therrnore, the resulting user model is indepen-dent from the language of the documents browsed.This is particular valuable with muitilingual websites, that are becoming very common especiallyin news sites or in electronic ommerce domains.Re ferencesA.
Artale, B. Magnini, and C. Strapparava.WoRDNET for italian and its use for lexicaldiscrimination.
In AI*IA97: Advances in Ar-tificial Intelligence.
Springer Verlag, 1997.P.
Buitelaar.
CoPJ~LEX: An ontology of sys-tematic polysemous classes.
In Proceedings ofFOIS98, International Conference on FormalOntology in Information Systems, Trento, Italy,June 6-8 1998.
IOS Press, 1998.P.
Buitelaar.
Reducing lexical semantic omplex-ity with systematic polysemous classes and un-derspecification.
In Proceedings of ANLP2000Workshop on Syntactic and Semantic Complex-ity in Natural Language Processing Systems,Seattle, USA, April 30 2000, 2000.J.
Gonzalo, F. Verdejio, C. Peters, and N. Calzo-lari.
Applying eurowordnet to cross-languagetext retrieval.
Computers and Humanities,32(2-3):185--207, 1998.A.
Kilgarriff and C. Yallop.
What's in a the-sanrus?
In Proceedings of LREC-BO00, Sec-ond International Conference on Language Re-sources and Evaluation, Athens, Greece, June2000.B.
Maguini and G. Cavagli~.
Integrating subjectfield codes into WordNet.
In Proceedings ofLREC-2000, Second International Conferenceon Language Resources and Evaluation, Athens,Greece, June 2000.B.
Maguini and C. Strapparava.
Costruzione diuna base di conoscenza lessicale per l'italianobasata su WordNet.
In M. Carapezza, D. Gam-barara, and F. Lo Piparo, editors, Linguaggio eCognizione.
Bulzoni, Palermo, Italy, 1997.K.
Mihalcea nd D. Moldovan.
A method for wordsense disambiguation of unrestricted text.
InProc.
of A CL-99, College Park Maryland, June1999.
held in conjunction with UM'96.G.
Miller.
An on-line lexical database.
Interna-tional Journal of Lexicography, 13(4):235-312,1990.F.
Sabatini and V. Coletti.
Dizionario ItalianoSabatini Coletti.
Giunti, 1997.C.
Strapparava, B. Magnini, and A. Stefani.Seuse-based user modelling for web sites.
InAdaptive Hyperraedia nd Adaptive Web-BasedSystems - Lecture Notes in Computer Science1892.
Springer Verlag, 2000.E.
Voorhees.
Using wordnet for text retrieval.
InC. Fellbaum, editor, WordNet - an ElectronicLexical Database.
MIT Press, 1998.Y.
Wilks and M. Stevenson.
Word sense dis-ambiguation using optimised combination ofknowledge sources.
In Proc.
of COLING-A CL '98, 98.33
