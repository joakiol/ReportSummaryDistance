Proceedings of the 8th International Natural Language Generation Conference, pages 54?63,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsA Hybrid Approach to Multi-document Summarization ofOpinions in ReviewsGiuseppe Di FabbrizioAmazon.com?Cambridge, MA - USApino@difabbrizio.comAmanda J. StentYahoo!
LabsNew York, NY - USAstent@labs.yahoo.comRobert GaizauskasDepartment of Computer ScienceUniversity of Sheffield, Sheffield - UKR.Gaizauskas@sheffield.ac.ukAbstractWe present a hybrid method to gener-ate summaries of product and services re-views by combining natural language gen-eration and salient sentence selection tech-niques.
Our system, STARLET-H, re-ceives as input textual reviews with asso-ciated rated topics, and produces as out-put a natural language document summa-rizing the opinions expressed in the re-views.
STARLET-H operates as a hybridabstractive/extractive summarizer: usingextractive summarization techniques, it se-lects salient quotes from the input reviewsand embeds them into an automaticallygenerated abstractive summary to provideevidence for, exemplify or justify posi-tive or negative opinions.
We demon-strate that, compared to extractive meth-ods, summaries generated with abstractiveand hybrid summarization approaches aremore readable and compact.1 IntroductionText summarization is a well-established area ofresearch.
Many approaches are extractive, thatis, they select and stitch together pieces of textfrom the input documents (Goldstein et al., 2000;Radev et al., 2004).
Other approaches are abstrac-tive; they use natural language generation (NLG)techniques to paraphrase and condense the con-tent of the input documents (Radev and McKeown,1998).
Most summarization methods focus on dis-tilling factual information by identifying the in-put documents?
main topics, removing redundan-cies, and coherently ordering extracted phrases orsentences.
Summarization of sentiment-laden text(e.g., product or service reviews) is substantiallydifferent from the traditional text summarizationtask: instead of presenting facts, the summarizermust present the range of opinions and the con-sensus opinion (if any), and instead of focusingon one topic, the summarizer must present infor-mation about multiple aspects of the target entity.
?This work was conducted when in AT&T Labs ResearchIn addition, traditional summarization techniquesdiscard redundancies, while for summarization ofsentiment-laden text, similar opinions mentionedmultiple times across documents are crucial indi-cators of the overall strength of the sentiments ex-pressed by the writers (Ku et al., 2006).Extractive summaries are linguistically interest-ing and can be both informative and concise.
Ex-tractive summarizers also require less engineer-ing effort.
On the other hand, abstractive sum-maries tend to have better coverage for a particularlevel of conciseness, and to be less redundant andmore coherent (Carenini et al., 2012).
They alsocan be constructed to target particular discoursegoals, such as summarization, comparison or rec-ommendation.
Although in theory, it is possible toproduce user-targeted extractive summaries, user-specific review summarization has only been ex-plored in the context of abstractive summarization(Carenini et al., 2012).Current systems for summarizing sentiment-laden text use information about the attributes ofthe target entity (or entities); the range, meanand median of the ratings of each attribute; re-lationships between the attributes; and links be-tween ratings/attributes and text elements in theinput documents (Blair-Goldensohn et al., 2008).However, there is other information that no sum-marizer currently takes into account.
This in-cludes temporal features (in particular, dependingon how old the documents are, products and ser-vices evaluated features may change over time)and social features (in particular, social or demo-graphic similarities or relationships between doc-ument authors and the reader of the summary).In addition, there is an essential contradiction atthe heart of current review summarization sys-tems: the system is authoring the review, but theopinions contained therein are really attributableto one or more human authors, and those attribu-tions are not retained in the review summary.
Forexample, consider the extractive summary gener-ated with STARLET-E (Di Fabbrizio et al., 2013):?Delicious.
Can?t wait for my next trip to Buffalo.GREAT WINGS.
I have rearranged business trips54so that I could stop in and have a helping or twoof their wings?.
We were seated promptly and thestaff was courteous.The summary is generated by selecting sen-tences from reviews to reflect topics and rating dis-tributions contained in the input review set.
Do thetwo sentences about wings reflect one (repeated)opinion from a single reviewer, or two opinionsfrom two separate reviewers?
The ability to at-tribute subjective statements to known sources canmake them more trustworthy; conversely, in theabsence of the ability to attribute, a reader maybecome skeptical or confused about the content ofthe review summary.
We term this summarizationissue opinion holder attribution.In this paper we present STARLET-H, a hybridreview summarizer that combines the advantagesof the abstractive and extractive approaches tosummarization and implements a solution to theopinion holder attribution problem.
STARLET-Htakes as input a set of reviews, each review ofwhich is labeled with aspect ratings and author-ship.
It generates hybrid abstractive/extractive re-views that: 1) are informative (achieve broad cov-erage of the input opinions); 2) are concise andavoid redundancy; 3) are readable and coherent (ofhigh linguistic quality); 4) can be targeted to thereader; and 5) address the opinion holder attribu-tion problem by directly referring to reviewers au-thorship when embedding phrases from reviews.We demonstrate through a comparative evalua-tion of STARLET-H and other review summariz-ers that hybrid review summarization is preferredover extractive summarization for readability, cor-rectness, completeness (achieving broad coverageof the input opinions) and compactness.2 Hybrid summarizationMost NLG research has converged around a ?con-sensus architecture?
(Reiter, 1994; Rambow andKorelsky, 1992), a pipeline architecture includingthe following modules: 1) text planning, whichdetermines how the presentation content is se-lected, structured, and ordered; 2) sentence plan-ning, which assigns content to sentences, insertsdiscourse cues to communicate the structure ofthe presentation, and performs sentence aggrega-tion and optionally referring expression genera-tion; and 3) surface realization, which performslexical selection, resolves syntactic issues such assubject-verb and noun-determiner agreement, andassigns morphological inflection to produce the fi-nal grammatical sentence.
An abstractive sum-marizer requires the customization of these threemodules.
Specifically, the text planner has to se-lect and organize the information contained in theinput reviews to reflect the rating distributions overthe aspects discussed by the reviewers.
The sen-tence planner must perform aggregation in such away as to optimize summary length without con-fusing the reader, and insert discourse cues thatreveal the discourse structure underlying the sum-mary.
And, finally, the surface realizer must selectthe proper domain lexemes to express positive andnegative opinions.Figure 1: STARLET-H hybrid review summarizerarchitectureFigure 1 shows the architecture we adopted forour STARLET-H hybrid review summarizer.
Weuse a generate-and-select approach: the decisionsto be made at each stage of the NLG process justoutlined are complex, and because they are nottruly independent of each other, a generate-and-rank approach may be best (allowing each com-ponent to express alternative ?good?
choices andchoosing the best combination of these choicesat the end).
Our text planner is responsible foranalyzing the input text reviews, extracting per-attribute rating distributions and other meta-datafrom each review, and synthesizing this informa-tion to produce one or more discourse plans.
Oursentence planner, JSPARKY ?
a freely-availabletoolkit (Stent and Molina, 2009) ?
can produceseveral candidate sentence plans and their corre-sponding surface realizations through SimpleNLG(Gatt and Reiter, 2009).
The candidate summariesare ranked by calculating their perplexity with alanguage model trained over a large number ofsentences from additional restaurant reviews col-lected over the Web.2.1 DataSTARLET-H uses review data directly, as inputto summarization, and indirectly, as training datafor statistical models and for lexicons for variousstages of the summarization process.For training data, we used two sets of la-beled data: one for the restaurant domain andthe other for the hotel domain.
Both corpora in-clude manually created sentence-level annotations55that identify: 1) opinion targets ?
phrases refer-ring to domain-relevant aspects that are the tar-gets of opinions expressed by the reviewer; 2)opinion phrases ?
phrases expressing an opinionabout an entity, and its polarity (positive or neg-ative); and 3) opinion groups ?
links betweenopinion phrases and their opinion targets.
Ad-ditionally, sentences satisfying the properties ofquotable sentence mentioned in Section 3 were la-beled as ?quotable?.
Table 1 summarizes the over-all statistics of the two corpora.
The annotated cor-pora included the following rated aspects: Atmo-sphere, Food, Service, Value, and Overall for theRestaurant domain, and Location, Rooms, Service,Value, and Overall for the Hotel domain1.Table 1: Quote-annotated dataset statisticsDataset RQ4000 HQ4000Domain Restaurant Hotel TotalReviews 484 404 888Sentences 4,007 4,013 8,020Avg sentences / review 8.28 9.93 9.032.2 Text planningReviews present highly structured information:each contains an (implicit or explicit) rating of oneor more aspects of a target entity, possibly withjustification or evidence in the form of examples.The rich information represented in these ratings?
either directly expressed in reviews or extractedby an automatic rating prediction model ?
can beexploited in several ways.
Our text planner re-ceives as input a set of text reviews with associatedper-aspect ratings, and for each review proceedsthrough the following analysis steps:Entity description Extracts basic informationto describe the reviewed entity, e.g., the name andlocation of the business, number of total and recentreviews, review dates and authors, etc.Aspect distribution categorization Catego-rizes the rating distribution for each aspect of thereviewed entity as one of four types: 1) positive?
most of the ratings are positive; 2) negative ?most of the ratings are negative; 3) bimodal ?most of the ratings are equally distributed intopositive and negative values; 4) uniform ?
ratingsare uniformly distributed across the rating scale.1Some examples from the annotated corpus are avail-able at the following address http://s286209735.onlinehome.us/starlet/examplesQuote selection and attribution Classifies eachsentence from the reviews using a quote selec-tion model (see Section 3), which assigns toeach sentence an aspect, a rating polarity (posi-tive/negative) and a confidence score.
The classi-fied sentences are sorted by confidence score anda candidate quote is selected for each aspect of thetarget entity that is explicitly mentioned in the in-put reviews.
Each quote is stored with the nameof the reviewer for correct authorship attribution.Note that when the quote selection module is ex-cluded, the system is an abstractive summarizer,which we call STARLET-A.Lexical selection Selects a lexicon for each as-pect based on its rating polarity and its assignedrating distribution type.
Lexicons are extractedfrom the corpus of annotated opinion phrases de-scribed in Di Fabbrizio et al.
(2011).Aspect ordering Assigns an order over aspectsusing aspect ordering statistics from our trainingdata (see Section 2.4), and generates a discourseplan, using a small set of rhetorical relations orga-nized into summary templates (see below).2.3 Sentence planningThe STARLET-H sentence planner relies on rhetor-ical structure theory (RST) (Mann and Thomp-son, 1989).
RST is a linguistic framework thatdescribes the structure of natural language textin terms of the rhetorical relationships organizingtextual units.
Through a manual inspection of ourtraining data, we identified a subset of six RST re-lations that are relevant to review summarization:concession, contrast, example, justify, list, andsummary.
We further identified four basic RST-based summary templates, one for each per-aspectrating distribution: mostly positive, mostly nega-tive, uniform across all ratings, and bimodal (e.g.,both positive and negative).
These summary tem-plates are composed by the text planner to buildsummary discourse plans.
The JSPARKY sen-tence planner then converts input discourse plansinto sentence plans, performing sentence order-ing, sentence aggregation, cross-sentence refer-ence resolution, sentence tense and mode (passiveor active), discourse cue insertion, and the selec-tion of some lexical forms from FrameNet (Bakeret al., 1998) relations.Figure 2 illustrates a typical RST template rep-resenting a positive review summary and corre-sponding text output generated by JSPARKY.
Foreach aspect of the considered domain, the sentenceplan strategy covers a variety of opinion distribu-56Figure 2: Example of RST structure generated by the text planner for mostly positive restaurant reviewstion conditions (e.g., positive, negative, bimodal,and uniform), and provides alternative RST struc-tures when the default relation is missing due tolack of data (e.g., missing quotes for a specific as-pect, missing information about review distribu-tion over time, missing type of cuisine, and so on).The sentence template can also manage lexicalvariations by generating multiple options to qual-ify a specific pair of aspect and opinion polarity.For instance, in case of very positive reviews aboutrestaurant atmosphere, it can provide few alterna-tive adjective phrases (e.g., great, wonderful, verywarm, terrific, etc.)
that can be used to producemore summary candidates (over-generate) duringthe final surface realization stage.2.4 Ordering aspects and polaritiesThe discourse structure of a typical review consistsof a summary opinion, followed by a sequenceof per-aspect ratings with supporting information(e.g., evidence, justification, examples, and con-cessions).
The preferred sequence of aspects topresent in a summary depends on the specific re-view domain, the overall polarity of the reviews,and how opinion polarity is distributed across thereviewed aspects.
Looking at our training data, weobserved that when the review is overall positive,positively-rated aspects are typically discussed atthe beginning, while negatively-rated aspects tendto gather toward the end.
The opposite orderseems predominant in the case of negative re-views.
When opinions are mixed, aspect orderingstrategies are unclear.
To most accurately modelaspect ordering, we trained weighted finite statetransducers for the restaurant and hotel domainsusing our training data.
Weighted finite statetransducers (WFSTs) are an elegant approach tosearch large feature spaces and find optimal pathsby using well-defined algebraic operations (Mohriet al., 1996).
To find the optimal ordering of ratedaspects in a domain, the text planner creates aWFST with all the possible permutations of theinput sequence of aspects, and composes it with alarger WFST trained from bigram sequences of as-pects extracted from the relevant domain-specificreview corpus.
The best path sequence is then de-rived from the composed WFST by applying theViterbi decoding algorithm.
For instance, the se-quence of aspects and polarities represented by thestring: value-n service-p overall-n food-natmosphere-n2 is first permuted in all the dif-ferent possible sequences and then converted intoa WFST.
Then the permutation network is fullycomposed with the larger, corpus-trained WFST.The best path is extracted by dynamic program-ming, producing the optimal sequence service-pvalue-n overall-n atmosphere-n food-n.2We postfix the aspect label with a ?-p?
for positive andwith ?-n?
for negative opinion572.5 Lexical choiceIt can be hard to choose the best opinion words,especially when the summary must convey thedifferent nuances between ?good?
and ?great?
or?bad?
and ?terrible?
for a particular aspect in aparticular domain.
For our summarization task,we adopted a simple approach.
From our anno-tated corpora, we mined both positive and negativeopinion phrases with their associated aspects andrating polarities.
We sorted the opinion phrasesby frequency and then manually selected from themost likely phrases adjective phrases that may cor-rectly express per-aspect polarities.
We then splitpositive and negative phrases into two levels ofpolarity (i.e., strongly positive, weakly positive,weakly negative, strongly negative) and use thenumber of star ratings to select the right polarityduring content planning.
For bimodal and uniformpolarity distributions, we manually defined a cus-tomized set of terms.
Sample lexical terms are re-ported in Table 2.3 Quote selection modelingThere are several techniques to extract salientphrases from text, often related to summariza-tion problems, but there is a relatively little workon extracting quotable sentences from text (Sar-mento and Nunes, 2009; De La Clergerie et al.,2009) and none, to our knowledge, on extract-ing quotes from sentiment-latent text.
So, whatdoes make a phrase quotable?
What is a properquote definition that applies to review summa-rization?
We define a sentiment-laden quotablephrase as a text fragment with the following char-acteristics: attributable ?
clearly ascribable to theauthor; compact and simple ?
it is typically arelatively short phrase (between two and twentywords) which contains a statement with a simplesyntactic structure and independent clauses; self-contained its meaning is clear and self-contained,e.g., it does not include pronominal references toentities outside its scope; on-topic ?
it refers toopinion targets (i.e., aspects) in a specific domain;sentiment-laden ?
it has one or two opinion tar-gets and an unambiguous overall polarity.
Exam-ple quotable phrases are presented in Table 3.To automatically detect quotes from reviews,we adopted a supervised machine learning ap-proach based on manually labeled data.
The clas-sification task consists of classifying both aspectsand polarity for the most frequent aspects definedfor each domain.
Quotes for the aspect food, forinstance, are split into positive and negative classi-Table 3: Example of quotes from restaurant andhotel domains?Everyone goes out of their way to make sure youare happy with their service and food.?
?The stuffed mushrooms are the best I?ve ever hadas was the lasagna.?
?Service is friendly and attentive even duringthe morning rush.?
?I?ve never slept so well away from home lovedthe comfortable beds.?
?The price is high for substandard mattresseswhen I pay this much for a room.
?fication labels: food-p and food-n, respectively.We identify quotable phrases and associate themwith aspects and rating polarities all in one step,but multi-step approaches could also be used (e.g.,a configuration with binary classification to detectquotable sentences followed by another classifica-tion model for aspect and polarity detection).3.1 Training quote selection modelsWe used the following features for automaticquote selection: ngrams ?
unigrams, bigrams, andtrigrams from the input phrases with frequencyhigher than three; binned number of words ?we assumed a maximum length of twenty wordsper sentence and created six bins, five of themuniformly distributed from one to twenty, and thesixth including all the sentences of length greaterthan twenty words; POS ?
unigrams, bigrams, andtrigrams for part of speech tags; chunks ?
uni-grams, bigrams, and trigrams for shallow parsedsyntactic chunks; opinion phrases ?
a binary fea-ture to keep track of the presence of positive andnegative opinion phrases as defined in our anno-tated review corpora.
In our annotated data onlythe most popular aspects are well represented.For instance, food-p and overall-p are the mostpopular positive aspects among the quotable sen-tences for the restaurant domain, while quotes onatmosphere-n and value-n are scarce.
The dis-tribution is even further skewed for the hotel do-main; there are plenty of quotes for overall-pand service-p and only 13 samples (0.43%) forlocation-n. To compensate for the broad vari-ation in the sample population, we used stratifiedsampling methods to divide the data into more bal-anced testing and training data We generated 10-fold stratified training/test sets.
We experimentedwith three machine learning algorithms: MaxEnt,SVMs with linear kernels, and SVMs with poly-nomial kernels.
The MaxEnt learning algorithmproduced statistically better classification resultsthan the other algorithms when used with uni-58Table 2: Summarizer lexicon for most frequent adjective phrases by aspect and polarityDomain Restaurant HotelAspect positive very positive negative very negative Aspect positive very positive negative very negativeatmosphere nice, good,friendly, com-fortablegreat, wonder-ful, very warm,terrificordinary,depressingreally bad location good, nice,pleasantamazing,awesome,excellent, greatbad, noisy,gloomyvery bad, verybleak, verygloomyfood good, deli-cious, pleasant,nice, hearty,enjoyablegreat, ex-cellent, verygood, to diefor, incrediblevery basic, un-original, unin-teresting, unac-ceptable, sub-standard, poormediocre, ter-rible, horrible,absolutely hor-riblerooms comfortable,decent, clean,goodamazing,awesome,gorgeousaverage, basic,subparterrible, verylimited, veryaverageoverall good, quite en-joyable, lovelywonderful, ter-rific, very nicebad, unremark-able, not sogoodabsolutely ter-rible, horrible,pretty badoverall great, nice,welcomingexcellent,superb, perfectaverage, noth-ing great, noisyquite bad, aw-ful, horribleservice attentive,friendly, pleas-ant, courteousvery atten-tive, great,excellent, veryfriendlyinattentive,poor, notfriendly, badextremelypoor, horrible,so lousy, awfulservice friendly, great,nice, helpful,goodvery friendly,great, ex-cellent, veryniceaverage, basic,not that greatvery bad,dreadfulvalue reasonable,fair, goodvaluevery reason-able, greatnot that good,not worthyterrible, outra-geousvalue great, nice,good, decentvery good,wonderful,perfectly goodnot good not very goodgram features.
This confirmed a general trend wehave previously observed in other text classifica-tion experiments: with relatively small and noisydatasets, unigram features provide better discrimi-native power than sparse bigrams or trigrams, andMaxEnt methods are more robust when dealingwith noisy data.3.2 Quote selection resultsTable 4 reports precision, recall and F-measuresaveraged across 10-fold cross-validated test setswith relative standard deviation.
The label nqidentifies non-quotable sentences, while the otherlabels refer to the domain-specific aspects andtheir polarities.
For the quote selection task, pre-cision is the most important metric: missing somepotential candidates is less important than incor-rectly identifying the polarity of a quote or sub-stituting one aspect with another.
The text plannerin STARLET-H further prunes the quotable phrasesby considering only the quote candidates with thehighest scores.4 EvaluationEvaluating an abstractive review summarizer in-volves measuring how accurately the opinion con-tent present in the reviews is reflected in the sum-mary and how understandable the generated con-tent is to the reader.
Traditional multi-documentsummarization evaluation techniques utilize bothqualitative and quantitative metrics.
The formerrequire human subjects to rate different evaluativecharacteristics on a Likert-like scale, while the lat-ter relies on automatic metrics such as ROUGE(Lin, 2004), which is based on the common num-ber of n-grams between a peer, and one or severalgold-standard reference summaries.Table 4: Quote, aspect, and polarity classificationperformances for the restaurant domainPrecision Recall F-measureatmosphere-n 0.233 0.080 0.115atmosphere-p 0.589 0.409 0.475food-n 0.634 0.409 0.491food-p 0.592 0.634 0.612nq 0.672 0.822 0.740overall-n 0.545 0.275 0.343overall-p 0.555 0.491 0.518service-n 0.699 0.393 0.498service-p 0.716 0.563 0.626value-n 0.100 0.033 0.050value-p 0.437 0.225 0.286Hotel Precision Recall F-measurelocation-n - - -location-p 0.572 0.410 0.465nq 0.678 0.836 0.748overall-n 0.517 0.233 0.305overall-p 0.590 0.492 0.536rooms-n 0.628 0.330 0.403rooms-p 0.667 0.573 0.612service-n 0.517 0.163 0.240service-p 0.605 0.500 0.543value-n - - -value-p 0.743 0.300 0.4014.1 Evaluation materialsTo evaluate our abstractive summarizer, we useda qualitative metric approach and compared fourreview summarizers: 1) the open source MEADsystem, designed for extractive summarization ofgeneral text (Radev et al., 2004); 2) STARLET-E,an extractive summarizer based on KL-divergenceand language modeling features that is describedin Di Fabbrizio et al.
(2011); 3) STARLET-A, theabstractive summarizer presented in this paper,without the quote selection module; and 4) the hy-brid summarizer STARLET-H.We used the Amazon Mechanical Turk3 crowd-3http://www.mturk.com59sourcing system to post subjective evaluationtasks, or HITs, for 20 restaurant summaries.
EachHIT consists of a set of ten randomly ordered re-views for one restaurant, and four randomly or-dered summaries of reviews for that restaurant,each one accompanied by a set of evaluation wid-gets for the different evaluation metrics describedbelow.
To minimize reading order bias, both re-views and summaries were shuffled each time atask was presented.4.2 Evaluation metricsWe chose to carry out a qualitative evaluationin the first instance as n-gram metrics, such asROUGE, are not necessarily appropriate for as-sessing abstractive summaries.
We asked each par-ticipant to evaluate each summary by rating (usinga Likert scale with the following rating values: 1)Not at all; 2) Not very; 3) Somewhat; 4) Very; 5)Absolutely) the following four summary criteria:readability ?
a summary is readable if it is easy toread and understand; correctness ?
a summary iscorrect if it expresses the opinions in the reviews;completeness ?
a summary is complete if it cap-tures the whole range of opinions in the reviews;compactness ?
a summary is compact if it doesnot repeat information.4.3 Evaluation procedureWe requested five evaluators for each HIT.
To in-crease the chances of getting accurate evaluations,we required evaluators to be located in the USAand have an approval rate of 90% or higher (i.e.,have a history of 90% or more approved HITs).Manual examinations of the evaluation responsesdid not show evidence of tampered data, but statis-tical analysis showed unusually widely spread rat-ing ranges.
We noticed that most evaluators onlyevaluated one or two HITs; this may imply thatthey tried a few HITs and then decided not to con-tinue because they found the task too long or theinstructions unclear.
We then re-opened the evalu-ation and directly contacted three additional eval-uators, explaining in detail the instructions and theevaluation scales.
For consistency, we asked theseevaluators to complete the evaluation for all HITs.In our analysis, we only included the five evalu-ators (two from the first round of evaluation, andthree from the second) who completed all HITs.For each evaluation metric, the five workers eval-uated each of the 20 summaries, for a total of 100ratings.
Table 5 shows an example output of thefour summarization methods for a single set ofrestaurant review documents.Table 5: Example of MEAD-based, extractive, ab-stractive and hybrid summaries from the restaurantdomainMEAD Summarya truly fun resturant everyone who like spicyfood should try the rattoes and for a mixed drinkthe worm burner really good food and a fun placeto meet your friends.
We were attracted by thegreat big frog on the exterior of the buildingand the fun RAZZOO S logo during a trip to themall.
it was great the waitress was excellentvery prompt and courteous and friendly to all areal complement to razzoo ?s way of service hername was Tabitha.
The best spicy food restaurantwith great server and fast service.Extractive summaryEat there every chance i get.
We ve been goinghere for years.
Their crawfish etoufee is theBEST.
And such an awesome value for under 10.Excellent as always.
Some of the best food inthe area.
I use to work at Razzoo s. It washard to leave.
The people are great and so isthe food.
I still go in there and miss it moreeverytime.
I Love Loney.
It was great.
Ourserver was great and very observant.
Try theChicken Tchoupitoulas.Abstractive summaryRazzoo?s Cajun Cafe in Concord, NC is an Americanrestaurant.
It has nine reviews.
It had threevery recent reviews.
It is an awesome, Americanrestaurant.
It has many very positive reviews.It has an excellent atmosphere and and has alwaysexceptional service.Hybrid summaryRazzoo?s Cajun Cafe in Concord, NC is an Americanrestaurant.
It has nine reviews.
It had threevery recent reviews.
It is an awesome, Americanrestaurant.
It has many very positive reviews.First it has a great price.
Angela Haithcocksays ?
?And such an awesome value for under 10?
?.Second it has always exceptional service and forinstance Danny Benson says ?
?it was great thewaitress was excellent very prompt and courteousand friendly to all a real complement to razzoo?sway of service her name was Tabitha??.
Third ithas an excellent atmosphere.
Last it has amazingfood.
Scott Kern says ?
?Some of the best food inthe area?
?.4.4 Evaluation results and discussionThe evaluation results are presented in Table 6.Each evaluation metric is considered separately.Average values for STARLET-E, STARLET-A andSTARLET-H are better than for MEAD across theboard, suggesting a preference for summaries ofsentiment-laden text that take opinion into ac-count.
To validate this hypothesis, we first com-puted the non-parametric Kruskal-Wallis statisticfor each evaluation metric, using a chi-square testto establish significance.
The results were not sig-nificant for any of the metrics.However, when we conducted pairwiseWilcoxon signed-rank tests considering twosummarization methods at a time, we found somesignificant differences (p < 0.05).
As predicted,60Table 6: Qualitative evaluation resultsMEAD Starlet-E Starlet-A Starlet-HReadability 2.95 3.17 3.64 3.74Completeness 2.88 3.29 3.290 3.58Compactness 3.07 3.35 3.80 3.58Correctness 3.26 3.48 3.59 3.72MEAD perform substantially worse than bothSTARLET-A and STARLET-H on readability,correctness, completeness, and compactness.STARLET-A and STARLET-H are also preferredover STARLET-E for readability.
While STARLET-A is preferred over STARLET-E for compactness(the average length of the abstractive reviewswas 45.05 words, and of the extractive,102.30),STARLET-H is preferred over STARLET-E forcorrectness, since the former better captures thereviewers opinions by quoting them in the ap-propriate context.
STARLET-A and STARLET-Hachieve virtually indistinguishable performanceon all evaluation metrics.
Our evaluation resultsaccord with those of Carenini et al.
(2012); theirabstractive summarizer had superior performancein terms of content precision and accuracy whencompared to summaries generated by an extractivesummarizer.
Carenini et al.
(2012) also found thatthe differences between extractive and abstractiveapproaches are even more significant in the caseof controversial content, where the abstractivesystem is able to more effectively convey the fullrange of opinions.5 Related workGanesan et al.
(2010) propose a method to extractsalient sentence fragments that are both highly fre-quent and syntactically well-formed by using agraph-based data structure to eliminate redundan-cies.
However, this approach assumes that the in-put sentences are already selected in terms of as-pect and with highly redundant opinion content.Also, the generated summaries are very short andcannot be compared to a full-length output of atypical multi-document summarizer (e.g., 100-200words).
A similar approach is described in Gane-san et al.
(2012), where very short phrases (fromtwo to five words) are collated together to generatewhat the authors call ultra-concise summaries.The most complete contribution to evaluativetext summarization is described in Carenini et al.
(2012) and it closely relates to this work.
Careniniet al.
(2012) compare an extractive summariza-tion system, MEAD* ?
a modified version ofthe open source summarization system MEAD(Radev et al., 2004) ?
with SEA, an abstractivesummarization system, demonstrating that bothsystems perform equally well.
The SEA approach,although better than traditional MEAD, has a fewdrawbacks.
Firstly, the sentence selection mecha-nism only considers the most frequently discussedaspects, leaving the decision about where to stopthe selection process to the maximum summarylength parameter.
This could leave out interest-ing opinions that do not appear with sufficient fre-quency in the source documents.
Ideally, all opin-ions should be represented in the summary accord-ing to the overall distribution of the input reviews.Secondly, Carenini et al.
(2012) use the absolutevalue of the sum of positive and negative contri-butions to determine the relevance of a sentence interms of opinion content.
This flattens the aspectdistributions since sentences with very negative orvery positive polarity or with numerous opinions,but with moderate polarity strengths, will get thesame score, regardless.
Finally, it does not ad-dress the opinion holder attribution problem leav-ing the source of opinion undefined.
In contrast,STARLET-H follows reviews aspect rating distri-butions both to select quotable sentences and tosummarize relevant aspects.
Moreover, it explic-itly mentions the opinion source in the embeddedquoted sentences.6 ConclusionsIn this paper, we present a hybrid summarizer forsentiment-laden text that combines an overall ab-stractive summarization method with an extrac-tive summarization-based quote selection method.This summarizer can provide the readability andcorrectness of abstractive summarization, whileaddressing the opinion holder attribution problemthat can lead readers to become confused or mis-led about who is making claims that they read inreview summaries.
We plan a more extensive eval-uation of STARLET-H. Another potential area offuture research concerns the ability to personal-ize summaries to the user?s needs.
For instance,the text planner can adapt its communicative goalsbased on polarity orientation ?
a user can be moreinterested in exploring in detail negative reviews?
or it can focus more on specific (user-tailored)aspects and change the order of the presentationaccordingly.
Finally, it could be interesting to cus-tomize the summarizer to provide an overview ofwhat is available in a specific geographic neigh-borhood and compare and contrast the options.61ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
The Berkeley FrameNet Project.
InProceedings of the 17th International Con-ference on Computational Linguistics - Vol-ume 1, COLING ?98, pages 86?90, Strouds-burg, PA, USA, 1998.
Association for Com-putational Linguistics.
doi: 10.3115/980451.980860.
URL http://dx.doi.org/10.3115/980451.980860.Sasha Blair-Goldensohn, Kerry Hannan, RyanMcDonald, Tyler Neylon, George Reis, and JeffReynar.
Building a Sentiment Summarizer forLocal Service Reviews.
In NLP in the Informa-tion Explosion Era, 2008.Giuseppe Carenini, Jackie Chi Kit Cheung, andAdam Pauls.
Multi-Document Summarizationof Evaluative Text.
Computational Intelligence,2012.E?ric De La Clergerie, Beno?
?t Sagot, Rosa Stern,Pascal Denis, Gae?lle Recource?, and VictorMignot.
Extracting and Visualizing Quotationsfrom News Wires.
In Language and Technol-ogy Conference, Poznan, Pologne, 2009.
ProjetScribo (po?le de compe?titivite?
System@tic).Giuseppe Di Fabbrizio, Ahmet Aker, and RobertGaizauskas.
STARLET: Multi-document Sum-marization of Service and Product Reviews withBalanced Rating Distributions.
In Proceedingsof the 2011 IEEE International Conference onData Mining (ICDM) Workshop on SentimentElicitation from Natural Text for InformationRetrieval and Extraction (SENTIRE), Vancou-ver, Canada, december 2011.Giuseppe Di Fabbrizio, Ahmet Aker, and RobertGaizauskas.
Summarizing On-line Product andService Reviews Using Aspect RatingDistribu-tions and Language Modeling.
Intelligent Sys-tems, IEEE, 28(3):28?37, May 2013.
ISSN1541-1672.
doi: 10.1109/MIS.2013.36.Kavita Ganesan, ChengXiang Zhai, and JiaweiHan.
Opinosis: A Graph-Based Approach toAbstractive Summarization of Highly Redun-dant Opinions.
In Proceedings of the 23rd Inter-national Conference on Computational Linguis-tics, COLING ?10, pages 340?348, Strouds-burg, PA, USA, 2010.
Association for Compu-tational Linguistics.Kavita Ganesan, ChengXiang Zhai, and EvelyneViegas.
Micropinion Generation: An Unsu-pervised Approach to Generating Ultra-conciseSummaries of Opinions.
In Proceedings of the21st international conference on World WideWeb, WWW ?12, pages 869?878, New York,NY, USA, 2012.
ACM.Albert Gatt and Ehud Reiter.
SimpleNLG: A Re-alisation Engine for Practical Applications.
InProceedings of the 12th European Workshopon Natural Language Generation, ENLG ?09,pages 90?93, Stroudsburg, PA, USA, 2009.
As-sociation for Computational Linguistics.Jade Goldstein, Vibhu Mittal, Jaime Carbonell,and Mark Kantrowitz.
Multi-document Sum-marization by Sentence Extraction.
In Proceed-ings of the 2000 NAACL-ANLP Workshop onAutomatic summarization - Volume 4, pages 40?48, Stroudsburg, PA, USA, 2000.
Associationfor Computational Linguistics.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.Opinion Extraction, Summarization and Track-ing in News and BlogCorpora.
In Proceedingsof AAAI-2006 Spring Symposium on Computa-tional Approaches to Analyzing Weblogs, 2006.Chin-Yew Lin.
ROUGE: A Package for Auto-matic Evaluation of summaries.
In Proc.
ACLworkshop on Text Summarization Branches Out,page 10, 2004.William C. Mann and Sandra A. Thompson.Rhetorical Structure Theory: A Theory of TextOrganization.
In Livia Polanyi, editor, TheStructure of Discourse.
Ablex, Norwood, NJ,1989.Mehryar Mohri, Fernando Pereira, and MichaelRiley.
Weighted Automata in Text and SpeechProcessing.
In ECAI-96 Workshop, pages 46?50.
John Wiley and Sons, 1996.Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, Jahna Otterbacher, Hong Qi, Ho-racio Saggion, Simone Teufel, Michael Top-per, Adam Winkel, and Zhu Zhang.
MEAD?
A Platform for Multidocument MultilingualText Summarization.
In Conference on Lan-guage Resources and Evaluation (LREC), Lis-bon, Portugal, May 2004.Dragomir R. Radev and Kathleen R. McKe-own.
Generating natural language summariesfrom multiple on-line sources.
ComputationalLinguistiscs, 24(3):470?500, September 1998.ISSN 0891-2017.62Owen Rambow and Tanya Korelsky.
Applied TextGeneration.
In Proceedings of the Third Confer-ence on Applied Natural Language Processing,pages 40?47, Trento, Italy, 1992.
Associationfor Computational Linguistics.
31 March - 3April.Ehud Reiter.
Has a Consensus NL Generation Ar-chitecture Appeared, and is it PsychologicallyPlausible?
In David McDonald and MarieMeteer, editors, Proceedings of the 7th.
Inter-national Workshop on Natural Language gen-eration (INLGW ?94), pages 163?170, Kenneb-unkport, Maine, 1994.Luis Sarmento and Se?rgio Nunes.
Automatic Ex-traction of Quotes and Topics from News Feeds.In 4th Doctoral Symposium on Informatics En-gineering (DSIE09), 2009.Amanda Stent and Martin Molina.
Evaluating Au-tomatic Extraction of Rules for Sentence PlanConstruction.
In Proceedings of the SIGDIAL2009 Conference: The 10th Annual Meeting ofthe Special Interest Group on Discourse and Di-alogue, SIGDIAL ?09, pages 290?297, Strouds-burg, PA, USA, 2009.
Association for Compu-tational Linguistics.63
