Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?43,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsEvaluating Distributional Models of Semantics for SyntacticallyInvariant InferenceJackie CK Cheung and Gerald PennDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canada{jcheung,gpenn}@cs.toronto.eduAbstractA major focus of current work in distri-butional models of semantics is to con-struct phrase representations composition-ally from word representations.
However,the syntactic contexts which are modelledare usually severely limited, a fact whichis reflected in the lexical-level WSD-likeevaluation methods used.
In this paper, webroaden the scope of these models to buildsentence-level representations, and arguethat phrase representations are best eval-uated in terms of the inference decisionsthat they support, invariant to the partic-ular syntactic constructions used to guidecomposition.
We propose two evaluationmethods in relation classification and QAwhich reflect these goals, and apply severalrecent compositional distributional modelsto the tasks.
We find that the models out-perform a simple lemma overlap baselineslightly, demonstrating that distributionalapproaches can already be useful for tasksrequiring deeper inference.1 IntroductionA number of unsupervised semantic models(Mitchell and Lapata, 2008, for example) have re-cently been proposed which are inspired at leastin part by the distributional hypothesis (Harris,1954)?that a word?s meaning can be character-ized by the contexts in which it appears.
Suchmodels represent word meaning as one or morehigh-dimensional vectors which capture the lex-ical and syntactic contexts of the word?s occur-rences in a training corpus.Much of the recent work in this area has, fol-lowing Mitchell and Lapata (2008), focused onthe notion of compositionality as the litmus test ofa truly semantic model.
Compositionality is a nat-ural way to construct representations of linguisticunits larger than a word, and it has a long historyin Montagovian semantics for dealing with argu-ment structure and assembling rich semantical ex-pressions of the kind found in predicate logic.While compositionality may thus provide aconvenient recipe for producing representationsof propositionally typed phrases, it is not a nec-essary condition for a semantic representation.Rather, that distinction still belongs to the crucialability to support inference.
It is not the inten-tion of this paper to argue for or against composi-tionality in semantic representations.
Rather, ourinterest is in evaluating semantic models in orderto determine their suitability for inference tasks.In particular, we contend that it is desirable andarguably necessary for a compositional semanticrepresentation to support inference invariantly, inthe sense that the particular syntactic constructionthat guided the composition should not matter rel-ative to the representations of syntactically differ-ent phrases with the same meanings.
For example,we can assert that John threw the ball and The ballwas thrown by John have the same meaning forthe purposes of inference, even though they differsyntactically.An analogy can be drawn to research in imageprocessing, in which it is widely regarded as im-portant for the representations of images to be in-variant to rotation and scaling.
What we shouldwant is a representation of sentence meaning thatis invariant to diathesis, other regular syntactic al-ternations in the assignment of argument struc-ture, and, ideally, even invariant to other meaning-preserving or near-preserving paraphrases.33Existing evaluations of distributional semanticmodels fall short of measuring this.
One evalua-tion approach consists of lexical-level word sub-stitution tasks which primarily evaluate a sys-tem?s ability to disambiguate word senses within acontrolled syntactic environment (McCarthy andNavigli, 2009, for example).
Another approach isto evaluate parsing accuracy (Socher et al 2010,for example), which is really a formalism-specificapproximation to argument structure analysis.These evaluations may certainly be relevant tospecific components of, for example, machinetranslation or natural language generation sys-tems, but they tell us little about a semanticmodel?s ability to support inference.In this paper, we propose a general frameworkfor evaluating distributional semantic models thatbuild sentence representations, and suggest twoevaluation methods that test the notion of struc-turally invariant inference directly.
Both rely ondetermining whether sentences express the samesemantic relation between entities, a crucial stepin solving a wide variety of inference tasks likerecognizing textual entailment, information re-trieval, question answering, and summarization.The first evaluation is a relation classificationtask, where a semantic model is tested on its abil-ity to recognize whether a pair of sentences bothcontain a particular semantic relation, such asCompany X acquires Company Y.
The second taskis a question answering task, the goal of which isto locate the sentence in a document that containsthe answer.
Here, the semantic model must matchthe question, which expresses a proposition with amissing argument, to the answer-bearing sentencewhich contains the full proposition.We apply these new evaluation protocols toseveral recent distributional models, extendingseveral of them to build sentence representa-tions.
We find that the models outperform a sim-ple lemma overlap model only slightly, but thatcombining these models with the lemma overlapmodel can improve performance.
This result islikely due to weaknesses in current models?
abil-ity to deal with issues such as named entities,coreference, and negation, which are not empha-sized by existing evaluation methods, but it doessuggest that distributional models of semanticscan play a more central role in systems that re-quire deep, precise inference.2 Compositionality and DistributionalSemanticsThe idea of compositionality has been central tounderstanding contemporary natural language se-mantics from an historiographic perspective.
Theidea is often credited to Frege, although in factFrege had very little to say about compositional-ity that had not already been repeated since thetime of Aristotle (Hodges, 2005).
Our modernnotion of compositionality took shape primarilywith the work of Tarski (1956), who was actu-ally arguing that a central difference between for-mal languages and natural languages is that nat-ural language is not compositional.
This in turnwas the ?the contention that an important theo-retical difference exists between formal and nat-ural languages,?
that Richard Montague so fa-mously rejected (Montague, 1974).
Composi-tionality also features prominently in Fodor andPylyshyn?s (1988) rejection of early connection-ist representations of natural language semantics,which seems to have influenced Mitchell and La-pata (2008) as well.Logic-based forms of compositional semanticshave long strived for syntactic invariance in mean-ing representations, which is known as the doc-trine of the canonical form.
The traditional justifi-cation for canonical forms is that they allow easyaccess to a knowledge base to retrieve some de-sired information, which amounts to a form of in-ference.
Our work can be seen as an extension ofthis notion to distributional semantic models witha more general notion of representational similar-ity and inference.There are many regular alternations that seman-tics models have tried to account for such as pas-sive or dative alternations.
There are also manylexical paraphrases which can take drastically dif-ferent syntactic forms.
Take the following exam-ple from Poon and Domingos (2009), in which thesame semantic relation can be expressed by a tran-sitive verb or an attributive prepositional phrase:(1) Utah borders Idaho.Utah is next to Idaho.In distributional semantics, the original sen-tence similarity test proposed by Kintsch (2001)served as the inspiration for the evaluation per-formed by Mitchell and Lapata (2008) and mostlater work in the area.
Intransitive verbs are given34in the context of their syntactic subject, and can-didate synonyms are ranked for their appropri-ateness.
This method targets the fact that a syn-onym is appropriate for only some of the verb?ssenses, and the intended verb sense depends onthe surrounding context.
For example, burn andbeam are both synonyms of glow, but given a par-ticular subject, one of the synonyms (called theHigh similarity landmark) may be a more appro-priate substitution than the other (the Low similar-ity landmark).
So, if the fire is the subject, glowedis the High similarity landmark, and beamed theLow similarity landmark.Fundamentally, this method was designed asa demonstration that compositionality in com-puting phrasal semantic representations does notinterfere with the ability of a representation tosynthesize non-compositional collocation effectsthat contribute to the disambiguation of homo-graphs.
Here, word-sense disambiguation is im-plicitly viewed as a very restricted, highly lexi-calized case of inference for selecting the appro-priate disjunct in the representation of a word?smeaning.Kintsch (2001) was interested in sentence sim-ilarity, but he only conducted his evaluation ona few hand-selected examples.
Mitchell and La-pata (2008) conducted theirs on a much largerscale, but chose to focus only on this single caseof syntactic combination, intransitive verbs andtheir subjects, in order to ?factor out inessentialdegrees of freedom?
to compare their various al-ternative models more equitably.
This was notnecessary?using the same, sufficiently large, un-biased but syntactically heterogeneous sample ofevaluation sentences would have served as an ade-quate control?and this decision furthermore pre-vents the evaluation from testing the desired in-variance of the semantic representation.Other lexical evaluations suffer from the sameproblem.
One uses the WordSim-353 dataset(Finkelstein et al 2002), which contains hu-man word pair similarity judgments that seman-tic models should reproduce.
However, the wordpairs are given without context, and homographyis unaddressed.
Also, it is unclear how reliablethe similarity scores are, as different annotatorsmay interpret the integer scale of similarity scoresdifferently.
Recent work uses this dataset mostlyfor parameter tuning.
Another is the lexical para-phrase task of McCarthy and Navigli (2009), inwhich words are given in the context of the sur-rounding sentence, and the task is to rank a givenlist of proposed substitutions for that word.
Thelist of substitutions as well as the correct rankingsare elicited from annotators.
This task was origi-nally conceived as an applied evaluation of WSDsystems, not an evaluation of phrase representa-tions.Parsing accuracy has been used as a prelimi-nary evaluation of semantic models that producesyntactic structure (Socher et al 2010; Wu andSchuler, 2011).
However, syntax does not alwaysreflect semantic content, and we are specificallyinterested in supporting syntactic invariance whendoing semantic inference.
Also, this type of eval-uation is tied to a particular grammar formalism.The existing evaluations that are most similar inspirit to what we propose are paraphrase detectiontasks that do not assume a restricted syntactic con-text.
Washtell (2011) collected human judgmentson the general meaning similarity of candidatephrase pairs.
Unfortunately, no additional guid-ance on the definition of ?most similar in mean-ing?
was provided, and it appears likely that sub-jects conflated lexical, syntactic, and semantic re-latedness.
Dolan and Brockett (2005) define para-phrase detection as identifying sentences that arein a bidirectional entailment relation.
While suchsentences do support exactly the same inferences,we are also interested in the inferences that canbe made from similar sentences that are not para-phrases according to this strict definition ?
a sit-uation that is more often encountered in end ap-plications.
Thus, we adopt a less restricted notionof paraphrasis.3 An Evaluation FrameworkWe now describe a simple, general frameworkfor evaluating semantic models.
Our frameworkconsists of the following components: a seman-tic model to be evaluated, pairs of sentences thatare considered to have high similarity, and pairsof sentences that are considered to have low simi-larity.In particular, the semantic model is a binaryfunction, s = M(x, x?
), which returns a real-valued similarity score, s, given a pair of arbitrarylinguistic units (that is, words, phrases, sentences,etc.
), x and x?.
Note that this formulation of thesemantic model is agnostic to whether the modelsuse compositionality to build a phrase represen-35tation from constituent representations, and evento the actual representation used.
The model istested by applying it to each element in the fol-lowing two sets:H = {(h, h?
)|h and h?
are linguistic units (2)with high similarity}L = {(l, l?
)|l and l?
are linguistic units (3)with low similarity}The resulting sets of similarity scores are:SH ={M(h, h?
)|(h, h?)
?
H} (4)SL ={M(l, l?
)|(l, l?)
?
L} (5)The semantic model is evaluated according toits ability to separate SH and SL.
We will de-fine specific measures of separation for the tasksthat we propose shortly.
While the particular def-initions of ?high similarity?
and ?low similarity?depend on the task, at the crux of both our evalu-ations is that two sentences are similar if they ex-press the same semantic relation between a givenentity pair, and dissimilar otherwise.
This thresh-old for similarity is closely tied to the argumentstructure of the sentence, and allows considerableflexibility in the other semantic content that maybe contained in the sentence, unlike the bidirec-tional paraphrase detection task.
Yet it ensuresthat a consistent and useful distinction for infer-ence is being detected, unlike unconstrained sim-ilarity judgments.Also, compared to word similarity assessmentsor paraphrase elicitation, determining whether asentence expresses a semantic relation is a mucheasier task cognitively for human judges.
This bi-nary judgment does not involve interpreting a nu-merical scale or coming up with an open-endedset of alternative paraphrases.
It is thus easier toget reliable annotated data.Below, we present two tasks that instantiatethis evaluation framework and choice of similar-ity threshold.
They differ in that the first is tar-geted towards recognizing declarative sentencesor phrases, while the second is targeted towards aquestion answering scenario, where one argumentin the semantic relation is queried.3.1 Task 1: Relation ClassificationThe first task is a relation classification task.
Rela-tion extraction and recognition are central to a va-riety of other tasks, such as information retrieval,ontology construction, recognizing textual entail-ment and question answering.In this task, the high and the low similarity sen-tence pairs are constructed in the following man-ner.
First, a target semantic relation, such as Com-pany X acquires Company Y is chosen, and enti-ties are chosen for each slot in the relation, such asCompany X=Pfizer and Company Y=Rinat Neu-roscience.
Then, sentences containing these enti-ties are extracted and divided into two subsets.
Inone of them, E, the entities are in the target se-mantic relation, while in the other, NE, they arenot.
The evaluation sets H and L are then con-structed as follows:H = E ?
E \ {(e, e)|e ?
E} (6)L = E ?NE (7)In other words, the high similarity sentencepairs are all the pairs where both express the tar-get semantic relation, except the pairs between asentence and itself, while the low similarity pairsare all the pairs where exactly one of the two sen-tences expresses the target relation.Several sentences expressing the relation Pfizeracquires Rinat Neuroscience are shown in Exam-ples 8 to 10.
These sentences illustrate the amountof syntactic and lexical variation that the semanticmodel must recognize as expressing the same se-mantic relation.
In particular, besides recognizingsynonymy or near-synonymy at the lexical level,models must also account for subcategorizationdifferences, extra arguments or adjuncts, and part-of-speech differences due to nominalization.
(8) Pfizer buys Rinat Neuroscience to extendneuroscience research and in doing soacquires a product candidate for OA.
(lexical difference)(9) A month earlier, Pfizer paid an estimatedseveral hundred million dollars for biotechfirm Rinat Neuroscience.
(extra argument,subcategorization)(10) Pfizer to Expand Neuroscience ResearchWith Acquisition of Biotech Company RinatNeuroscience (nominalization)Since our interest is to measure the models?ability to separate SH and SL in an unsuper-vised setting, standard supervised classificationaccuracy is not applicable.
Instead, we employ36the area under a ROC curve (AUC), which doesnot depend on choosing an arbitrary classificationthreshold.
A ROC curve is a plot of the true pos-itive versus false positive rate of a binary classi-fier as the classification threshold is varied.
Thearea under a ROC curve can thus be seen as theperformance of linear classifiers over the scoresproduced by the semantic model.
The AUC canalso be interpreted as the probability that a ran-domly chosen positive instance will have a highersimilarity score than a randomly chosen negativeinstance.
A random classifier is expected to havean AUC of 0.5.3.2 Task 2: Restricted QAThe second task that we propose is a restrictedform of question answering.
In this task, the sys-tem is given a question q and a document D con-sisting of a list of sentences, in which one of thesentences contains the answer to the question.
Wedefine:H = {(q, d)|d ?
D and d answers q} (11)L = {(q, d)|d ?
D and d does not answer q}(12)In other words, the sentences are divided into twosubsets; those that contain the answer to q shouldbe similar to q, while those that do not should bedissimilar.
We also assume that only one sentencein each document contains the answer, so H con-tains only one sentence.Unrestricted question answering is a difficultproblem that forces a semantic representation todeal sensibly with a number of other semantic is-sues such as coreference and information aggre-gation which still seem to be out of reach forcontemporary distributional models of meaning.Since our focus in this work is on argument struc-ture semantics, we restrict the question-answerpairs to those that only require dealing with para-phrases of this type.To do so, we semi-automatically restrict thequestion-answer pairs by using the output of anunsupervised clustering semantic parser (Poonand Domingos, 2009).
The semantic parser clus-ters semantic sub-expressions derived from a de-pendency parse of the sentence, so that those sub-expressions that express the same semantic re-lations are clustered.
The parser is used to an-swer questions, and the output of the parser ismanually checked.
We use only those cases thathave thus been determined to be correct question-answer pairs.
As a result of this restriction, thistask is rather more like Task 1 in how it tests amodel?s ability to recognize lexical and syntac-tic paraphrases.
This task also involves recog-nizing voicing alternations, which were automati-cally extracted by the semantic parser.An example of a question-answer pair involv-ing a voicing alternation that is used in this task ispresented in Example 13.
(13) Q: What does il-2 activate?A: PI3KSentence: Phosphatidyl inositol 3-kinase(PI3K) is activated by IL-2.Since there is only one element in H and henceSH for each question and document, we measurethe separation between SH and SL using the rankof the score of answer-bearing sentence amongthe scores of all the sentences in the document.We normalize the rank so that it is between 0(ranked least similar) and 1 (ranked most simi-lar).
Where ties occur, the sentence is ranked asif it were in the median position among the tiedsentences.
If the question-answer pairs are zero-indexed by i, answer(i) is the index of the sen-tence containing the answer for the ith pair, andlength(i) is the number of sentences in the doc-ument, then the mean normalized rank score of asystem is:norm rank = Ei[1?
answer(i)length(i) ?
1](14)4 ExperimentsWe drew a number of recent distributional seman-tic models to compare in this paper.
We first de-scribe the models and our reimplementation ofthem, before describing the tasks and the datasetsused in detail and the results.4.1 Distributional Semantic ModelsWe tested four recent distributional models and alemma overlap baseline, which we now describe.We extended several of the models to compo-sitionally construct phrase representations usingcomponent-wise vector addition and multiplica-tion, as we note below.
Since the focus of this pa-per is on evaluation methods for such models, wedid not experiment with other compositionality37operators.
We do note, however, that component-wise operators have been popular in recent liter-ature, and have been applied across unrestrictedsyntactic contexts (Mitchell and Lapata, 2009),so there is value in evaluating the performance ofthese operators in itself.
The models were trainedon the Gigaword corpus (2nd ed., ~2.3B words).All models use cosine similarity to measure thesimilarity between representations, except for thebaseline model.Lemma Overlap This baseline simply repre-sents a sentence as the counts of each lemmapresent in the sentence after removing stopwords.
Let a sentence x consist of lemma-tokensm1, .
.
.
,m|x|.
The similarity between two sen-tences is then defined asM(x, x?)
= #In(x, x?)
+ #In(x?, x) (15)#In(x, x?)
=|x|?i=11x?
(mi) (16)where 1x?
(mi) is an indicator function that returns1 if mi ?
x?, and 0 otherwise.
This definitionaccounts for multiple occurrences of a lemma.M&L Mitchell and Lapata (2008) propose aframework for compositional distributional se-mantics using a standard term-context vectorspace word representation.
A phrase is repre-sented as a vector of context-word counts (actu-ally, pmi-scaled values), which is derived compo-sitionally by a function over constituent vectors,such as component-wise addition or multiplica-tion.
This model ignores syntactic relations andis insensitive to word-order.E&P Erk and Pado?
(2008) introduce a struc-tured vector space model which uses syntactic de-pendencies to model the selectional preferencesof words.
The vector representation of a word incontext depends on the inverse selectional prefer-ences of its dependents, and the selectional pref-erences of its head.
For example, suppose catchoccurs with a dependent ball in a direct objectrelation.
The vector for catch would then be in-fluenced by the inverse direct object preferencesof ball (e.g.
throw, organize), and the vector forball would be influenced by the selectional pref-erences of catch (e.g.
cold, drift).
More formally,given words a and b in a dependency relation r,a distributional representation of a, va, the repre-sentation of a in context, a?, is given bya?
= va ?Rb(r?1) (17)Rb(r) =?c:f(c,r,b)>?f(c, r, b) ?
vc, (18)where Rb(r) is the vector describing the selec-tional preference of word b in relation r, f(c, r, b)is the frequency of this dependency triple, ?
is afrequency threshold to weed out uncommon de-pendency triples (10 in our experiments), and ?is a vector combination operator, here component-wise multiplication.
We extend the model to com-pute sentence representations from the contextu-alized word vectors using component-wise addi-tion and multiplication.TFP Thater et al(2010)?s model is also sensi-tive to selectional preferences, but to two degrees.For example, the vector for catch might containa dimension labelled (OBJ,OBJ-1,throw),which indicates the strength of connection be-tween the two verbs through all of the co-occurring direct objects which they share.
UnlikeE&P, TFP?s model encodes the selectional prefer-ences in a single vector using frequency counts.We extend the model to the sentence level withcomponent-wise addition and multiplication, andword vectors are contextualized by the depen-dency neighbours.
We use a frequency thresholdof 10 and a pmi threshold of 2 to prune infrequentword and dependencies.D&L Dinu and Lapata (2010) (D&L) assumea global set of latent senses for all words, andmodels each word as a mixture over these latentsenses.
The vector for a word ti in the context ofa word cj is modelled byv(ti, cj) = P (z1|ti, cj), ...P (zK |ti, cj) (19)where z1...K are the latent senses.
By mak-ing independence assumptions and decomposingprobabilities, training becomes a matter of esti-mating the probability distributions P (zk|ti) andP (cj |zk) from data.
While Dinu and Lapata(2010) describe two methods to do so, basedon non-negative matrix factorization and latentDirichlet alcation, the performances are similar,so we tested only the latent Dirichlet alcationmethod.
Like the two previous models, we ex-tend the model to build sentence representations38Pfizer/Rinat N. Yahoo/Inktomi Besson/Paris Antoinette/Vienna AverageOverlap 0.7393 0.6007 0.7395 0.8914 0.7427Models trained on the entire GigaWordM&L add 0.6196 0.5387 0.5259 0.7275 0.6029M&L mult 0.9036 0.6099 0.6443 0.8467 0.7511D&L add 0.9214 0.8168 0.6989 0.8932 0.8326D&L mult 0.7732 0.6734 0.6527 0.7659 0.7163Models trained on the AFP sectionE&P add 0.7536 0.4933 0.2780 0.6408 0.5414E&P mult 0.5268 0.5328 0.5252 0.8421 0.6067TFP add 0.4357 0.5325 0.8725 0.7183 0.6398TFP mult 0.5554 0.5524 0.7283 0.6917 0.6320M&L add 0.5643 0.5504 0.4594 0.7640 0.5845M&L mult 0.8679 0.6324 0.4356 0.8258 0.6904D&L add 0.8143 0.9062 0.6373 0.8664 0.8061D&L mult 0.8429 0.7461 0.645 0.5948 0.7072Table 1: Task 1 results in AUC scores.
The values in bold indicate the best performing model for a particulartraining corpus.
The expected random baseline performance is 0.5.Entities: {X, Y} + NRelation: acquires{Pfizer, Rinat Neuroscience} 41 50{Yahoo, Inktomi} 115 433Relation: was born in{Luc Besson, Paris} 6 126{Marie Antoinette, Vienna} 39 105Table 2: Task 1 dataset characteristics.
N is the totalnumber of sentences.
+ is the number of sentencesthat express the relation.from the contextualized representations.
We setthe number of latent senses to 1200, and train for600 Gibbs sampling iterations.4.2 Training and Parameter SettingsWe reimplemented these four models, followingthe parameter settings described by previous workwhere possible, though we also aimed for consis-tency in parameter settings between models (forexample, in the number of context words).
For thenon-baseline models, we followed previous workand model only the 30000 most frequent lemmata.Context vectors are constructed using a symmet-ric window of 5 words, and their dimensions rep-resent the 3000 most frequent lemmatized contextwords excluding stop words.
Due to resource lim-itations, we trained the syntactic models over theAFP subset of Gigaword (~338M words).
We alsotrained the other two models on just the AFP por-tion for comparison.
Note that the AFP portionof Gigaword is three times larger than the BNCcorpus (~100M words), on which several previ-ous syntactic models were trained.
Because ourmain goal is to test the general performance of themodels and to demonstrate the feasibility of ourevaluation methods, we did not further tune theparameter settings to each of the tasks, as doingso would likely only yield minor improvements.4.3 Task 1We used the dataset by Bunescu and Mooney(2007), which we selected because it containsmultiple realizations of an entity pair in a targetsemantic relation, unlike similar datasets such asthe one by Roth and Yih (2002).
Controlling forthe target entity pair in this manner makes the taskmore difficult, because the semantic model cannotmake use of distributional information about theentity pair in inference.
The dataset is separatedinto subsets depending on the target binary rela-tion (Company X acquires Company Y or PersonX was born in Place Y) and the entity pair (e.g.,Yahoo and Inktomi) (Table 2).The dataset was constructed semi-automatically using a Google search for thetwo entities in order with up to seven contentwords in between.
Then, the extracted sentenceswere hand-labelled with whether they express thetarget relation.
Because the order of the entitieshas been fixed, passive alternations do not appear39Pure models Mixed modelsAll Subset All SubsetOverlap 0.8770 0.7291 0.8770 0.7291Models trained on the entire GigaWordM&L add 0.7467 0.6106 0.8782 0.7523M&L mult 0.5331 0.5690 0.8841 0.7678D&L add 0.6552 0.5716 0.8791 0.7539D&L mult 0.5488 0.5255 0.8841 0.7466Models trained on the AFP sectionE&P add 0.4589 0.4516 0.8748 0.7375E&P mult 0.5201 0.5584 0.8882 0.7719TFP add 0.6887 0.6443 0.8940 0.7871TFP mult 0.5210 0.5199 0.8785 0.7432M&L add 0.7588 0.6206 0.8710 0.7371M&L mult 0.5710 0.5540 0.8801 0.7540D&L add 0.6358 0.5402 0.8713 0.7305D&L mult 0.5647 0.5461 0.8856 0.7683Table 3: Task 2 results, in normalized rank scores.Subset is the cases where lemma overlap does notachieve a perfect score.
The two columns on the rightindicate performance using the sum of the scores fromthe lemma overlap and the semantic model.
The ex-pected random baseline performance is 0.5.in this dataset.The results for Task 1 indicate that the D&L ad-dition model performs the best (Table 1), thoughthe lemma overlap model presents a surprisinglystrong baseline.
The syntax-modulated E&P andTFP models perform poorly on this task, evenwhen compared to the other models trained on theAFP subset.
The M&L multiplication model out-performs the addition model, a result which cor-roborates previous findings on the lexical substi-tution task.
The same does not hold in the D&Llatent sense space.
Overall, some of the datasets(Yahoo and Antoinette) appear to be easier for themodels than others (Pfizer and Besson), but moreentity pairs and relations would be needed to in-vestigate the models?
variance across datasets.4.4 Task 2We used the question-answer pairs extracted bythe Poon and Domingos (2009) semantic parserfrom the GENIA biomedical corpus that havebeen manually checked to be correct (295 pairs).Because our models were trained on newspapertext, they required adaptation to this specializeddomain.
Thus, we also trained the M&L, E&Pand TFP models on the GENIA corpus, back-ing off to word vectors from the GENIA corpuswhen a word vector could not be found in theGigaword-trained model.
We could not do thisfor the D&L model, since the global latent sensesthat are found by latent Dirichlet alcation train-ing do not have any absolute meaning that holdsacross multiple runs.
Instead, we found the 5words in the Gigaword-trained D&L model thatwere closest to each novel word in the GENIAcorpus according to cosine similarity over the co-occurrence vectors of the words in the GENIAcorpus, and took their average latent sense distri-butions as the vector for that word.Unlike in Task 1, there is no control for thenamed entities in a sentence, because one of theentities in the semantic relation is missing.
Also,distributional models have problems in dealingwith named entities which are common in thiscorpus, such as the names of genes and proteins.To address these issues, we tested hybrid modelswhere the similarity score from a semantic modelis added to the similarity score from the lemmaoverlap model.The results are presented in Table 3.
Lemmaoverlap again presents a strong baseline, but thehybridized models are able to outperform simplelemma overlap.
Unlike in Task 1, the E&P andTFP models are comparable to the D&L model,and the mixed TFP addition model achieves thebest result, likely due to the need to more pre-cisely distinguish syntactic roles in this task.
TheD&L addition model, which achieved the bestperformance in Task 1, does not perform as wellin this task.
This could be due to the domain adap-tation procedure for the D&L model, which couldnot be reasonably trained on such a small, special-ized corpus.5 Related WorkTurney and Pantel (2010) survey various types ofvector space models and applications thereof incomputational linguistics.
We summarize belowa number of other word- or phrase-level distribu-tional models.Several approaches are specialized to deal withhomography.
The top-down multi-prototype ap-proach determines a number of senses for eachword, and then clusters the occurrences of theword (Reisinger and Mooney, 2010) into thesesenses.
A prototype vector is created for eachof these sense clusters.
When a new occurrence40of a word is encountered, it is represented as acombination of the prototype vectors, with the de-gree of influence from each prototype determinedby the similarity of the new context to the exist-ing sense contexts.
In contrast, the bottom-up ex-emplar-based approach assumes that each occur-rence of a word expresses a different sense of theword.
The most similar senses of the word are ac-tivated when a new occurrence of it is encounteredand combined, for example with a kNN algorithm(Erk and Pado?, 2010).The models we compared and the above workassume each dimension in the feature vector cor-responds to a context word.
In contrast, Washtell(2011) uses potential paraphrases directly as di-mensions in his expectation vectors.
Unfortu-nately, this approach does not outperform vari-ous context word-based approaches in two phrasesimilarity tasks.In terms of the vector composition function,component-wise addition and multiplication arethe most popular in recent work, but there ex-ist a number of other operators such as tensorproduct and convolution product, which are re-viewed by Widdows (2008).
Instead of vectorspace representations, one could also use a matrixspace representation with its much more expres-sive matrix operators (Rudolph and Giesbrecht,2010).
So far, however, this has only been ap-plied to specific syntactic contexts (Baroni andZamparelli, 2010; Guevara, 2010; Grefenstetteand Sadrzadeh, 2011), or tasks (Yessenalina andCardie, 2011).Neural networks have been used to learn bothphrase structure and representations.
In Socher etal.
(2010), word representations learned by neu-ral network models such as (Bengio et al 2006;Collobert and Weston, 2008) are fed as input intoa recursive neural network whose nodes representsyntactic constituents.
Each node models both theprobability of the input forming a constituent andthe phrase representation resulting from composi-tion.6 ConclusionsWe have proposed an evaluation framework fordistributional models of semantics which buildphrase- and sentence-level representations, andinstantiated two evaluation tasks which test forthe crucial ability to recognize whether sen-tences express the same semantic relation.
Ourresults demonstrate that compositional distribu-tional models of semantics already have someutility in the context of more empirically complexsemantic tasks than WSD-like lexical substitutiontasks, in which compositional invariance is a req-uisite property.
Simply computing lemma over-lap, however, is a very competitive baseline, dueto issues in these protocols with named entitiesand domain adaptivity.
The better performanceof the mixture models in Task 2 shows that suchweaknesses can be addressed by hybrid seman-tic models.
Future work should investigate morerefined versions of such hybridization, as well asextend this idea to other semantic phenomena likecoreference, negation and modality.We also observe that no single model or com-position operator performs best for all tasks anddatasets.
The latent sense mixture model of Dinuand Lapata (2010) performs well in recognizingsemantic relations in general web text.
Becauseof the difficulty of adapting it to a specializeddomain, however, it does less well in biomedi-cal question answering, where the syntax-basedmodel of Thater et al(2010) performs the best.A more thorough investigation of the factors thatcan predict the performance and/or invariance ofa given composition operator is warranted.In the future, we would like to evaluate othermodels of compositional semantics that have beenrecently proposed.
We would also like to collectmore comprehensive test data, to increase the ex-ternal validity of our evaluations.AcknowledgmentsWe would like to thank Georgiana Dinu and Ste-fan Thater for help with reimplementing theirmodels.
Saif Mohammad, Peter Turney, andthe anonymous reviewers provided valuable com-ments on drafts of this paper.
This project wassupported by the Natural Sciences and Engineer-ing Research Council of Canada.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.Yoshua Bengio, Holger Schwenk, Jean-Se?bastienSene?cal, Fre?deric Morin, and Jean-Luc Gauvain.412006.
Neural probabilistic language models.
In-novations in Machine Learning, pages 137?186.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web usingminimal supervision.
In Proceedings of the 45thAnnual Meeting of the Association for Computa-tional Linguistics, pages 576?583.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, page 160?167.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, pages 1162?1172.William B. Dolan and Chris Brockett.
2005.
Auto-matically constructing a corpus of sentential para-phrases.
In Proceedings of the Third InternationalWorkshop on Paraphrasing, pages 9?16.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906.Katrin Erk and Sebastian Pado?.
2010.
Exemplar-based models for word meaning in context.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 92?97.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Jerry A. Fodor and Zenon W. Pylyshyn.
1988.
Con-nectionism and cognitive architecture: A criticalanalysis.
Cognition, 28:3?71.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011.
Experimental support for a categorical com-positional distributional model of meaning.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages1394?1404.Emiliano Guevara.
2010.
A regression modelof adjective-noun compositionality in distributionalsemantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Seman-tics, pages 33?37.Zeller S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Wilfred Hodges.
2005.
The interplay of fact and the-ory in separating syntax from meaning.
In Work-shop on Empirical Challenges and Analytical Al-ternatives to Strict Compositionality.Walter Kintsch.
2001.
Predication.
Cognitive Sci-ence, 25(2):173?202.Diana McCarthy and Roberto Navigli.
2009.
The en-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08: HLT, pages 236?244.Jeff Mitchell and Mirella Lapata.
2009.
Languagemodels based on semantic composition.
In Pro-ceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages430?439.Richard Montague.
1974.
English as a formal lan-guage.
Formal Philosophy, pages 188?221.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of wordmeaning.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.Dan Roth and Wen-tau Yih.
2002.
Probabilistic rea-soning for entity & relation recognition.
In Pro-ceedings of the 19th International Conference onComputational Linguistics, pages 835?841.Sebastian Rudolph and Eugenie Giesbrecht.
2010.Compositional matrix-space models of language.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, pages907?916.Richard Socher, Christopher D. Manning, and An-drew Y. Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
Proceedings of the Deep Learn-ing and Unsupervised Feature Learning Workshopof NIPS 2010, pages 1?9.Alfred Tarski.
1956.
The concept of truth in formal-ized languages.
Logic, Semantics, Metamathemat-ics, pages 152?278.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 948?957.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Justin Washtell.
2011.
Compositional expectation:A purely distributional model of compositional se-mantics.
In Proceedings of the Ninth InternationalConference on Computational Semantics (IWCS2011), pages 285?294.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Second AAAI Sym-posium on Quantum Interaction.42Stephen Wu and William Schuler.
2011.
Structuredcomposition of semantic vectors.
In Proceedingsof the Ninth International Conference on Computa-tional Semantics (IWCS 2011), pages 295?304.Ainur Yessenalina and Claire Cardie.
2011.
Com-positional matrix-space models for sentiment analy-sis.
In Proceedings of the 2011 Conference on Em-pirical Methods in Natural Language Processing,pages 172?182.43
