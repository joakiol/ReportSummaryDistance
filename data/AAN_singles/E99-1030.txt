Proceedings of EACL '99The Development of Lexical Resourcesfor Information Extraction from TextCombining WordNet and Dewey Decimal Classification*Gabriela Cavagli~tITC-irst Centro per la Ricerca Scientifica e Tecnologicavia Sommarive, 1838050 Povo (TN), ITALYe-mail: cavaglia@irst.itc.itAbstractLexicon definition is one of the main bot-tlenecks in the development of new ap-plications in the field of Information Ex-traction from text.
Generic resources(e.g., lexical databases) are promising forreducing the cost of specific lexica defi-nition, but they introduce lexical ambi-guity.
This paper proposes a methodol-ogy for building application-specific lex-ica by using WordNet.
Lexical ambiguityis kept under control by marking synsetsin WordNet with field labels taken fromthe Dewey Decimal Classification.1 IntroductionOne of the current issues in Information Extrac-tion (IE) is efficient transportability, as the costof new applications is one of the factors limitingthe market.
The lexicon definition process is cur-rently one of the main bottlenecks in producingapplications.
As a matter of fact the necessary lex-icon for an average application is generally large(hundreds to thousands of words) and most lexicalinformation is not transportable across domains.The problem of lexicon transport is worsened bythe growing degree of lexicalization of IE systems:nowadays everal successful systems adopt lexicalrules at many levels.The IE research mainstream focused essentiallyon the definition of lexica starting from a corpussample (Riloff, 1993; Grishman, 1997) with theimplicit assumption that a corpus provided for anapplication is representative of the whole applica-*This work was carried on at ITC-IRST as part ofthe author's dissertation for the degree in Philosophy(University of Turin, supervisor: Carla Bazzanella).The author wants to thank her supervisor at ITC-IRST, Fabio Ciravegna, for his constant help.
AlbertoLavelli provided valuable comments to the paper.tion requirement.
Unfortunately one of the cur-rent trends in IE is the progressive reduction ofthe size of training corpora: e.g., from the 1,000texts of the MUC-5 (MUC-5, 1993) to the 100texts in MUC-6 (MUC-6, 1995).
When the cor-pus size is limited, the assumption of lexical rep-resentativeness of the sample corpus may not holdany longer, and the problem of producing a repre-sentative lexicon starting from the corpus lexiconarises (Grishman, 1995).Generic resources are interesting as they con-tain (among others) most of the terms necessaryfor an IE application.
Nevertheless up to nowthe use of generic resources within IE system hasbeen limited for two main reasons.
First the in-formation associated to each term is often not de-tailed enough for describing the relations neces-sary for a IE lexicon; secondly the presence of alarge amount of lexical polysemy.In this paper we propose a methodology forsemi-automatically developing the relevant part ofa lexicon (foreground lexicon) for IE applicationsby using both a small corpus and WordNet.2 Developing IE Lexical ResourcesLexical information in IE can be divided into threesources of information (Kilgarriff, 1997):?
an ontology, i.e.
the templates to be filled;?
the foreground lexicon (FL), i.e.
the termstightly bound to the ontology;?
the background lexicon (BL), i.e.
the termsnot related or loosely related to the ontology.In this paper we focus on FL only.The FL has generally a limited size with re-spect to the average dictionary of a language; itsdimension depends on each application eeds, butit is generally limited to some hundreds of words.The level of quantitative and qualitative informa-tion for each entry in the FL can be very highand it is not transportable across domains and225Proceedings of EACL '99applications, as it contains the mapping betweenthe entries and the ontology.
Generic dictionariescan contribute in identifying entries for the FL,but generally do not provide useful informationfor the mapping with the ontology.
This map-ping between words and ontology is generally tobe built by hand.
Most of the time in transport-ing the lexicon is spent in identifying and build-ing FLs.
Efficiently building FLs for applicationsmeans building the right FL (or at least a reason-able approximation of it) in a short time.
Theright FL contains those words that are necessaryfor the application and only those.
The presenceof all the relevant terms should guarantee that theinformation in the text is never lost; inserting justthe relevant terms allows to limit the developmenteffort, and should guarantee the system from noisecaused by spurious entries in the lexicon.The BL could be seen as the complementary setof the FL with respect to the generic language,i.e.
it contains all the words of the language thatdo not belong to the FL.
In general the quantityof application specific information is small.
Anymachine readable dictionary can be to some ex-tent seen as a BL.
The transport of BL to newapplications i not a problem, therefore it will notbe considered in this paper.2.1 Using Generic Lexical ResourcesWe propose a development methodology for FLsbased on two steps:?
Bootstrapping: manual or semi-automaticidentification from the corpus of an initial lex-icon (Core Lexicon), i.e.
of the lexicon cover-ing the corpus sample.?
Consolidation: extension of the Core Lexi-con by using a generic dictionary in order tocompletely cover the lexicon needed by theapplication but not exhaustively representedin the corpus sample.We propose to use WordNet (Miller, 1990) as ageneric dictionary during the consolidation phasebecause it can be profitably used for integratingthe Core Lexicon by adding for each term in asemi-automatic way:?
its synonyms;?
hyponyms and (maybe) hypernyms;?
some coordinated terms.As mentioned, there are two problems relatedto the use of generic dictionaries with respect othe IE needs.First there is no clear way of extracting fromthem the mapping between the FL and the ontol-ogy; this is mainly due to a lack of information andcannot in general be solved; generic lexica cannotthen be used during the bootstrapping phase togenerate the Core Lexicon.Secondly experience showed that the lexical am-biguity carried by generic dictionaries does notallow their direct use in computational systems(Basili and Pazienza, 1997; Morgan et al, 1995).Even when they are used off-line, lexical ambigu-ity can introduce so much noise (and then over-head) in the lexical development process that theiruse can be inconvenient from the point of view ofefficiency and effectiveness.The next section explains how it is possibleto cope with lexical ambiguity in WordNet bycombining its information with another source ofinformation: the Dewey Decimal Classification(DDC) (Dewey, 1989).3 Reducing the lexical ambiguityin WordNetThe main problem with the use of WordNet is lex-ical polysemy 1.
Lexical polysemy is present whena word is associated to many senses (synsets).
Ingeneral it is not easy to discriminate between dif-ferent synsets.
It is then necessary to find a wayfor helping the lexicon developer in selecting thecorrect synset for a word.In order to cope with lexical polysemy, we pro-pose to integrate WordNet synsets with an addi-tional information: a set of field labels.
Field la-bels are indicators, generally used in dictionaries,which provide information about the use of theword in a semantic field.
Semantic fields are setsof words tied together by "similarity" covering themost part of the lexical area of a specific domain.Marking synsets with field labels has a clear ad-vantage: in general, given a polysemous word inWordNet and a particular field label, in most ofthe cases the word is disambiguated.
For exampleSecurity is polysemous as it belongs to 9 differentsynsets; only the second one is related to the eco-nomic domain.
If we mark this synset with thefield label Economy, it is possible to disambiguatethe term Security when analyzing texts in an eco-nomic context.
Note that WordNet being a hier-archy, marking a synset with a field label meansalso marking all its sub-hierarchy with such fieldlabel.
In the Security example, if we mark the sec-ond synset with the field label Economy we alsoassociate the same field label to the synonym Cer-tificate, to the 13 direct hyponyms and to the 271 Actually the problem is related to both polysemyand omonymy.
As WordNet does not distinguish be-tween them, we will use the term polysemy for refer-ring to both.226Proceedings of EACL '99Figure l: An extract of the Dewey hierarchy relevant for the financial fieldindirect ones; moreover we can also inspect its co-ordinated terms and assign the same label to 9 ofthe 33 coordinate terms (and then to their directand indirect hyponyms).
Marking is equivalent toassigning WordNet synsets to sets each of themreferring to a particular semantic field.
Markingthe structure allows us to solve the problem ofchoosing which synsets are relevant for the do-main.
Associating a domain (e.g., finance) to oneor more field labels should allow us to determinein principle the synsets relevant for the domain.It is possible to greatly reduce the ambiguity im-plied by the use of WordNet by finding the correctset of field labels that cover all the WordNet hier-archy in an uniform way.
Therefore we can reducethe overhead in building the FL using WordNet.Our assumption is that using semantic fieldstaken from the DDC 2 , all the possible domainscan then be covered.
This is because the first tenclasses of the DDC (an extract is shown in fig-ure 1) exhaust he traditional academic disciplinesand so they also cover the generic knowledge of theworld.
The integration consists in marking partsof WordNet's hierarchy, i.e.
some synsets, withsemantic labels taken from the DDC.4 The  deve lopment  cyc le  us ingWN-PDDCThe consolidation phase mentioned in section 2.1can be integrated with the use of the WN+DDC2The Dewey Decimal Classification is the mostwidely used library classification system in the world;at the broadest level, it classifies concepts into tenmain classes, which cover the entire world of knowl-edge.as generic resource (see figure 2).
Before startingthe development, the set of field labels relevant forthe application must be identified.
Then the CoreLexicon is identified in the usual way.Using WN+DDC it is possible for each term inthe Core Lexicon to:?
identify the synsets the term belongs to; am-biguities are reduced by applying the inter-section of the field labels chosen for the cur-rent application and those associated to thepossible synsets.?
integrate the Core Lexicon by adding, foreach term: synonyms in the synsets, hy-ponyms and (maybe) hypernyms and somecoordinated terms.The proposed methodology is corpus centered(starting from the corpus analysis to build theCore Lexicon) and can always be profitably ap-plied.
It also provides a criterion for building lex-ical resources for specific domains.
It can be ap-plied in a semiautomatic way.
It has the advan-tage of using the information contained in Word-Net for expanding the FL beyond the corpus lim-itations, keeping under control the ambiguity im-plied by the use of a generic resource.5 Conc lus ionUp to now experiments have been carried on inthe financial domain, and in particular in the do-main of bonds issued by banks.
Experiments arecontinuing.
The construction of WN+DDC is along process that has to be done in general.
Upto now we have just started inserting in WordNetthe field labels that are interesting for the domain227Proceedings of EACL '99tunin~ tunin~add J._~ add  hiponyms~ ~ ,,  a~WordNet+DDCL I taresFigure 2: Outline of the final Consolidation phase.under analysis.
If the final experiments will con-firm the usefulness of the approach, we will extendthe integration to the rest of the WordNet hierar-chy.
The final evaluation will include a compari-son of the lexicon produced by using WN+DDCwith a normally developed lexicon in the domainof bond-issue (Ciravegna et el., 1999).
The eval-uation will consider both quality and quantity ofterms and development time of the whole lexicon.One of the issues that we are currently investi-gating is that of choosing the correct set of fieldlabels from DDC: DDC is very detailed and it isnot worth integrating it completely with Word-Net.
It is necessary to individuate the correct setof labels by pruning the DDC hierarchy at somelevel.
We are currently investigating the effective-ness of just selecting the first three levels of thehierarchy.ReferencesRoberto Basili and Maria Teresa Pazienza.
1997.Lexical acquisition for information extraction.In M. T. Pazienza, editor, Information Extrac-tion: A multidisciplinary approach to an emerg-ing information technology.
Springer Verlag.Fabio Ciravegna, Alberto Lavelli, NadiaMann Luca Gilardoni, Silvia Mazza, Mas-simo Ferraro, Johannes Matiasek, WilliamBlack, Fabio Rinaldi, and David Mowatt.1999.
Facile: Classifying texts integratingpattern matching and information extraction.In Proceedings of the Sixteenth InternationalJoint Conference on Artificial Intelligence(IJCAI99).
Stockholm, Sweden.Melvil Dewey.
1989.
Dewey Decimal Classifi-cation and Relative Index.
Edition 20.
ForestPress, Albany.Ralph Grishman.
1995.
The NYU system forMUC-6 or where's syntax?
In Sixth mes-sage understanding conference MUC-6.
MorganKaufmann Publishers.Ralph Grishman.
1997.
Information extraction:Techniques and challenges.
In M. T. Pazienza,editor, Information Extraction: a multidisci-plinary approach to an emerging technology.Springer Verlag.Adam Kilgarriff.
1997.
Foreground and back-ground lexicons and word sense disambiguationfor information extraction.
In InternationalWorkshop on Lexically Driven Information Ex-traction, Frascati, Italy.G.A.
Miller.
1990.
Wordnet: an on-line lexicaldatabase.
International Journal of Lexicogra-phy, 4(3).Richard Morgan, Roberto Garigliano, PaulCallaghan, Sanjay Poria, Mark Smith, Ag-nieszka Urbanowicz, Russel Collingham,Marco Costantino, Chris Cooper, and theLOLITA Group.
1995.
University of Durham:Description of the LOLITA system as usedfor MUC-6.
In Sixth message understand-ing conference MUC-6.
Morgan KaufmannPublishers.MUC-5.
1993.
Fifth Message Understanding Con-ference (MUC5).
Morgan Kaufmann Publish-ers, August.MUC-6.
1995.
Sixth Message UnderstandingConference (MUC-6).
Morgan Kaufmann Pub-lishers.Ellen Riloff.
1993.
Automatically constructinga dictionary for information extraction tasks.In Proceedings of the Eleventh National Confer-ence on Artificial Intelligence, pages 811-816.228
