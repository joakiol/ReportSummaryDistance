Tree Insertion Grammar: A Cubic-Time,Parsable Formalism that LexicalizesContext-Free Grammar without Changingthe Trees ProducedYves Schabes ?MERLRichard C. Waters"MERLTree insertion grammar (TIG) is a tree-based formalism that makes use of tree substitution andtree adjunction.
TIG is related to tree adjoining rammar.
However, the adjunction permitted inTIG is sufficiently restricted that TIGs only derive context-free languages and TIGs have the samecubic-time worst-case complexity bounds for recognition and parsing as context-free grammars.An efficient Earley-style parser for TIGs is presented.Any context-free grammar (CFG) can be converted into a lexicalized tree insertion grammar(LTIG) that generates the same trees.
A constructive procedure ispresented for converting aCFGinto a left anchored (i.e., word initial) LTIG that preserves ambiguity and generates the sametrees.
The L,TIG created can be represented compactly by taking advantage of sharing between theelementary trees in it.
Methods of converting CFGs into left anchored CFGs, e.g., the methodsof Greibach and Rosenkrantz, do not preserve the trees produced and result in very large outputgrammars.For the purpose of experimental evaluation, the LTIG lexicalization procedure was appliedto eight different CFGs for subsets of English.
The LTIGs created were smaller than the originalCFGs.
Using an implementation f the Earley-style TIG parser that was specialized for leftanchored LTIGs, it was possible to parse more quickly with the LTIGs than with the originalCFGs.1.
IntroductionMost current linguistic theories give lexical accounts of several phenomena that used tobe considered purely syntactic.
1 The information put in the lexicon is thereby increasedin both amount  and complexity.In this paper, we study the problem of lexicalizing context-free grammars  andshow that it enables faster processing.
In previous attempts to take advantage of lex-icalization, a variety of lexicalization procedures have been developed that convertcontext-free grammars  (CFGs) into equivalent lexicalized grammars.
However, theseprocedures typically suffer from one or more of the following problems.?
Lexicalization procedures uch as those developed by Greibach (1965)* Mitsubishi Electric Research Laboratories, 201 Broadway, Cambridge, MA 02139, USA.
E-mail:schabes/waters@merl.com.1 Some of the linguistic formalisms illustrating the increased use of lexical information are: lexical rulesin LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al 1985), HPSG (Pollard and Sag 1987),Categorial Grammars (Steedman 1987; Karttunen 1986), some versions of GB theory (Chomsky 1981),and Lexicon-Grammars (Gross 1984).
@ 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 4and Rosenkrantz (1967) often produce very large output grammars--solarge that they can be awkward or even impossible to parse with.Procedures that convert CFGs into lexicalized CFGs provide only a weaklexicalization, because while they preserve the strings derived, they donot preserve the trees derived.
Parsing with the resulting rammar canbe fast, but it does not produce the right trees.Strong lexicalization that preserves the trees derived is possible usingcontext-sensitive formalisms uch as tree adjoining rammar (TAG)(Joshi and Schabes 1992; Schabes 1990).
However, these context-sensitiveformalisms entail arger computation costs than CFGs--O(n6)-time in thecase of TAG (Vijay-Shanker and Joshi 1985), instead of O(n 3) for CFG.Tree Insertion Grammar (TIG) is a compromise between CFG and TAG that com-bines the efficiency of the former with the strong lexicalizing power of the latter.
Asdiscussed in Section 2, TIG is the same as TAG except hat adjunction is restricted sothat it no longer generates context-sensitive languages.
In section 3, we compare TIGwith CFG and TAG, showing how it is related to both.Like CFG, TIG can be parsed in O(IGInB)-time.
Section 4 presents an Earley-styleparser for TIG that maintains the valid prefix property.Section 5 presents a procedure that converts CFGs into lexicalized tree insertiongrammars (LTIGs) generating the same trees.
The procedure produces a left anchoredLTIG---one where for each elementary tree, the first element hat must be matchedagainst he input is a lexical item.Section 6 presents a number of experiments evaluating TIG.
Section 6.1 shows thatthe grammars generated by the LTIG procedure can be represented very compactly.In the experiments performed, the LTIG grammars are smaller than the CFGs they aregenerated from.
Section 6.2 investigates the practical value of the grammars created bythe LTIG procedure as a vehicle for parsing CFGs.
It reports a number of experimentscomparing a standard Earley-style parser for CFGs with the Earley-style TIG parserof Section 4, adapted to take advantage of the left anchored nature of the grammarscreated by the LTIG procedure.
In these experiments, parsing using LTIG is typically5 to 10 times faster.The original motivation behind the development of TIG was the intuition that thenatural-language rammars currently being developed using TAG do not make fulluse of the capabilities provided by TAG.
This suggests a different use for TIG--as a(partial) substitute for TAG.
This idea is explored in Section 7.2.
Tree Insert ion GrammarTree insertion grammar (TIG) is a tree generating system that is a restricted variantof tree-adjoining grammar (TAG) (Joshi and Schabes 1992; Schabes 1990).
As in TAG,a TIG grammar consists of two sets of trees: initial trees, which are combined bysubstitution and auxiliary trees, which are combined with each other and the initialtrees by adjunction.
However, both the auxiliary trees and the adjunction allowed aredifferent than in TAG.Def in i t ion 6\[TIG\] A tree insertion grammar (TIG) is a five-tuple (G, NT, L A, S), where ~.
is a set ofterminal symbols, NT is a set of nonterminal symbols, I is a finite set of finite initial480Schabes and Waters Tree Insertion Grammartrees, A is a finite set of finite auxiliary trees, and S is a distinguished nonterminalsymbol.
The set I t3 A is referred to as the elementary trees.In each initial tree the root and interior--i.e., nonroot, nonleaf--nodes are labeledby nonterminal symbols.
The nodes on the frontier are labeled with terminal symbols,nonterminal symbols, or the empty string (e).
The nonterminal symbols on the frontierare marked for substitution.
By convention, substitutability is indicated in diagramsby using a down arrow (D. The root of at least one elementary initial tree must belabeled S.In each auxiliary tree the root and interior nodes are labeled by nonterminal sym-bols.
The nodes on the frontier are labeled with terminal symbols, nonterminal sym-bols, or the empty string (e).
The nonterminal symbols on the frontier of an auxiliarytree are marked for substitution, except that exactly one nonterminal frontier nodeis marked as the foot.
The foot must be labeled with the same label as the root.
Byconvention, the foot of an auxiliary tree is indicated in diagrams by using an asterisk(,).
The path from the root of an auxiliary tree to the foot is called the spine.Auxiliary trees in which every nonempty frontier node is to the left of the foot arecalled left auxiliary trees.
Similarly, auxiliary trees in which every nonempty frontiernode is to the right of the foot are called right auxiliary trees.
Other auxiliary trees arecalled wrapping auxiliary trees.
2The root of each elementary tree must have at least one child.
Frontier nodeslabeled with ~ are referred to as empty.
If all the frontier nodes of an initial tree areempty, the tree is referred to as empty.
If all the frontier nodes other than the foot ofan auxiliary tree are empty, the tree is referred to as empty.The operations of substitution and adjunction are discussed in detail below.
Substi-tution replaces a node marked for substitution with an initial tree.
Adjunction replacesa node with an auxiliary tree.To this point, the definition of a TIG is essentially identical to the definition of aTAG.
However, the following differs from the definition of TAG.TIG does not allow there to be any elementary wrapping auxiliary trees or ele-mentary empty auxiliary trees.
This ensures that every elementary auxiliary tree willbe uniquely either a left auxiliary tree or a right auxiliary tree.
(Wrapping auxiliarytrees are neither.
Empty auxiliary trees are both and cause infinite ambiguity.
)TIG does not allow a left (right) auxiliary tree to be adjoined on any node that is onthe spine of a right (left) auxiliary tree.
Further, no adjunction whatever is permittedon a node # that is to the right (left) of the spine of an elementary left (right) auxiliarytree T. Note that for T to be a left (right) auxiliary tree, every frontier node dominatedby # must be labeled with ~.TIG allows arbitrarily many simultaneous adjunctions on a single node in a man-ner similar to the alternative TAG derivation defined in Schabes and Shieber (1994).Simultaneous adjunction is specified by two sequences, one of left auxiliary trees andthe other of right auxiliary trees that specify the order of the strings corresponding tothe trees combined.A TIG derivation starts with an initial tree rooted at S. This tree is repeatedlyextended using substitution and adjunction.
A derivation is complete when everyfrontier node in the tree(s) derived is labeled with a terminal symbol.
By means ofadjunction, complete derivations can be extended to bigger complete derivations.2 In Schabes and Waters (1993a) these three kinds of auxiliary trees are referred to differently, as rightrecursive, left recursive, and centrally recursive, respectively.481Computational Linguistics Volume 21, Number 4NP VP ND$ N V VP* A N*boy seems prettyFigure 1Example lementary TIG trees.VPVP* AdvsmoothlySNPo$ VPV NPI,\[,sawFigure 2Substitution.As in TAG, but in contrast to CFG, there is an important difference in TIG betweena derivation and the tree derived.
By means of simultaneous adjunction, there can beseveral trees created by a single derivation.
In addition, there can be several differentderivations for the same tree.To eliminate useless ambiguity in derivations, TIG prohibits adjunction: at nodesmarked for substitution, because the same trees can be created by adjoining on theroots of the trees substituted at these nodes; at foot nodes of auxiliary trees, becausethe same trees can be created by simultaneous adjunction on the nodes the auxiliarytrees are adjoined on; and at the roots of auxiliary trees, because the same trees can becreated by simultaneous adjunction on the nodes the auxiliary trees are adjoined on.Figure 1 shows five elementary trees that might appear in a TIG for English.
Thetrees containing 'boy' and 'saw' are initial trees.
The remainder are auxiliary trees.As illustrated in Figure 2, substitution inserts an initial tree T in place of a frontiernode # that has the same label as the root of T and is marked for substitution.Adjunction inserts an auxiliary tree T into another tree at a node # that has thesame label as the root (and therefore foot) of T. In particular, # is replaced by T andthe foot of T is replaced by the subtree rooted at #.
The adjunction of a left auxiliarytree is referred to as left adjunction.
This is illustrated in Figure 3.
The adjunction ofa right auxiliary tree is referred to as right adjunction (see Figure 4).Simultaneous adjunction is fundamentally ambiguous in nature and typically re-sults in the creation of several different rees.
The order in the sequences of left andright auxiliary trees fixes the order of the strings being combined.
However, unlessone of the sequences i empty, variability is possible in the trees that can be produced.The TIG formalism specifies that every tree is produced that is consistent with thespecified order.482Schabes and Waters Tree Insertion GrammarFigure 3Left adjunction.
"/qFigure 4Right adjunction.a w 2w~A* A *~wl % WsFigure 5Simultaneous left and right adjunction.w 2 A A w 4 ?, and"Z,~ w3Figure 5 illustrates the simultaneous adjunction of one left and one right auxiliarytree on a node.
The string corresponding to the left auxiliary tree must precede thenode, and the string corresponding to the right auxiliary tree must follow it.
However,two different rees can be derived---one where the left auxiliary tree is on top and onewhere the right auxiliary tree is on top.
The simultaneous adjunction of two left andtwo right auxiliary trees leads to six derived trees.The adjunction of a wrapping auxiliary tree is referred to as wrapping adjunction.This is illustrated in Figure 6.
The key force of the restrictions applied to TIG, in483Computational LinguisticsFigure 6Wrapping adjunction.Volume 21, Number 4w2~ w4w~comparison with TAG, is that they prevent wrapping adjunction from occurring, bypreventing the creation of wrapping auxiliary trees.
3Wrapping adjunction yields context-sensitive languages because two strings thatare mutually constrained by being in the same auxiliary tree are wrapped around an-other string.
This observation stems from the equivalence of TAG and head grammars(Vijay-Shanker et al 1986).
In contrast, every operation allowed by a TIG inserts astring into another string.
Simultaneous adjunction merely specifies multiple indepen-dent insertions.
Simultaneous left and right adjunction is not an instance of wrapping,because TIG does not allow there to be any constraints between the adjoinability ofthe trees in question.There are many ways that the TIG formalism could be extended.
First, adjoiningconstraints could be used to prohibit he adjunction of particular auxiliary trees (or allauxiliary trees) at a given node.Second, one can easily imagine variants of TIG where simultaneous adjunction ismore limited.
One could allow only one canonical derived tree.
One could allow atmost one left auxiliary tree and one right auxiliary tree as we did in Schabes and Waters(1993a).
One could forbid multiple adjunction altogether.
We have chosen unlimitedsimultaneous adjunction here primarily because it reduces the number of chart states,since one does not have to record whether adjunction has occurred at a given node.Third, one can introduce stochastic parameters controlling the probabilities withwhich particular substitutions and adjunctions occur (see Schabes and Waters 1993b).Fourth, and of particular importance in the current paper, one can require that aTIG be lexicalized.Definition 7\[LTIG\] A lexicalized tree insertion grammar (LTIG) 4 (G, NT, L A, S) is a TIG where everyelementary tree in I U A is lexicalized.
A tree is lexicalized if at least one frontier nodeis labeled with a terminal symbol.An LTIG is said to be left anchored if every elementary tree is left anchored.
Anelementary TIG tree is left anchored if the first nonempty frontier element other than3 Using a simple case-by-case analysis, one can show that given a TIG, it is not possible to create awrapping auxiliary tree.
A proof of this fact is presented in Appendix A.4 In Schabes and Waters (1993a) a formalism almost identical to LTIG is referred to as lexicalizedcontext-free grammar (LCFG).
A different name is used here to highlight he importance of thenonlexicalized formalism, which was not given a name in Schabes and Waters (1993a).484Schabes and Waters Tree Insertion Grammarthe foot, if any, is a lexical item.
All the trees in Figure 1 are lexicalized; however, onlythe ones containing seems, pretty, and smoothly are left anchored.3.
Relations between CFG, TIG and TAGIn this section, we briefly compare CFG, TIG and TAG, noting that TIG shares anumber of properties with CFG on one hand and TAG on the other.Any CFG can be trivially converted into a TIG that derives the same trees byconverting each rule R into a single-level initial tree.
If the right hand side of R isempty, the initial tree created has a single frontier element labeled with e. Otherwise,the elements of the right hand side of R become the labels on the frontier of the initialtree, with the nonterminals marked for substitution.Similarly, any TIG that does not make use of adjoining constraints can be easilyconverted into a TAG that derives the same trees; however, adjoining constraints mayhave to be used in the TAG.
The trivial nature of the conversion can be seen byconsidering the three differences between TIG and TAG.First, TIG prohibits elementary wrapping auxiliary trees.
From the perspective ofthis difference, a TIG is trivially a TAG without the need for any alterations.Second, TIG prohibits adjunction on the roots of auxiliary trees and allows simul-taneous adjunction while TAG allows adjunction on the roots of auxiliary trees andprohibits imultaneous adjunction.
From the perspective of this difference in approach,a TIG is also trivially a TAG without alteration.
To see this, consider the following:Suppose that there are a set of auxiliary trees T that are allowed to adjoin on a node# in a TIG.
Simultaneous adjunction in TIG allows these auxiliary trees to be chainedtogether in every possible way root-to-foot on #.
The same is true in a TAG where thetrees in T are allowed to adjoin on each other's roots.Third, TIG imposes a number of detailed restrictions on the interaction of leftand right auxiliary trees.
To convert a TIG into a TAG deriving the same trees and nomore, one has to capture these restrictions.
In general, this requires the use of adjoiningconstraints to prohibit the forbidden adjunctions.It should be noted that if a TIG makes use of adjoining constraints, then theconversion of the TIG to a TAG deriving the same trees can become more complexor even impossible, depending on the details of exactly how the adjoining constraintsare allowed to act in the TIG and TAG.TIG generates context-free languages.
Like CFG, TIG generates context-free languages.
Incontrast, TAG generates so called tree adjoining languages (TALs) (,Joshi 1985).The fact that any context-free language can be generated by a TIG follows fromthe fact that any CFG can be converted into a TIG.
The fact that TIGs can only generatecontext-free languages follows from the fact that any TIG can be converted into a CFGgenerating the same language, as shown in the following theorem.Theorem 1If G = (E, NT, LA, S) is a TIG then there is a CFG G' = (E, NT',P,S) that generates thesame string set.
55 As usual, a context-freegrammar (CFG) G is a four-tuple (G, NT, P, S) where ~ is a set of terminalsymbols, NT is a set of nonterminal symbols, P is a finite set of finite production rules that rewritenonterminal symbols to, possibly empty, strings of terminal and nonterminal symbols, and S is adistinguished nonterminal symbol that is the start symbol of any derivation.485Computational Linguistics Volume 21, Number 4P~oo~The key step in converting a TIG into a CFG is eliminating the auxiliary trees.
Givenonly initial trees, the final conversion to a CFG is trivial.?
Step 1: For each nonterminal Ai in NT, add two more nonterminals Wiand Zi.
This yields the new nonterminal set NTL?
Step 2: For each nonterminal Ai, include the following rules in P: Yi ---* cand Zi --* c.?
Step 3: Alter every node # in every elementary tree in I and A asfollows: Let Ai be the label of #.
If left adjunction is possible at #, add anew leftmost child of # labeled Yi and mark it for substitution.
If rightadjunction is possible at #, add a new rightmost child of # labeled Ziand mark it for substitution.?
Step 4: Convert every auxiliary tree t in A into an initial tree as follows:Let a i be the label of the root # of t. If t is a left auxiliary tree, add a newroot labeled Yi with two children: # on the left, and on the right, a nodelabeled Yi and marked for substitution.
Otherwise add a new rootlabeled Zi with two children: # on the left, and on the right, a nodelabeled Zi and marked for substitution.
Relabel the foot of t with e,turning t into an initial tree.?
Step 5: Every elementary tree t is now an initial tree.
Each one isconverted into a rule in P as follows: The label of the root of t becomesthe left hand side of R. The labels on the frontier of t with any instancesof c omitted become the right hand side of R.Every derivation in G maps directly to a derivation in G t that generatesthe same string.
Substitution steps map directly.
Adjunctions areconverted into substitutions via the new non-terminals Yi and Zi.
Thenew roots and their children labeled Yi and Zi created in Step 3 allowarbitrarily many simultaneous adjunctions at a node.
The right linearordering inherent in these structures encodes the ordering informationspecified for a simultaneous adjunction.
\[\]It should be noted that while G / generates the same strings as G, it does notgenerate the same trees: the substitutions in G / that correspond to adjunctions in Gcreate trees that are very different from the trees generated by G. For instance, if a leftauxiliary tree T has structure to the right of its spine, this structure nds up on the leftrather than the right of the node "adjoined on" in G ~.
However, this does not alter thestrings that are generated, because by the definition of TIG, the structure to the rightof the spine of T must be entirely empty.The theorem above does not convert TAGs into CFGs, because the constructioninvolving Yi and Zi does not work for wrapping auxiliary trees.
The reason for this isthat a wrapping auxiliary tree has nonempty structure on both the left and the rightof its spine.TIG generates context-free path sets.
The path set of a grammar is the set of all pathsfrom root to frontier in the trees generated by the grammar.
The path set is a set ofstrings in (~.
U NT)*.
CFGs have path sets that are regular languages (RLs) (Thatcher1971).
In contrast, TAGs have path sets that are context-free languages (CFLs) (Weir1988).486Schabes and Waters Tree Insertion GrammarFigure 7A TIG with a context-free path set.AS aBS*The fact that the path sets generated by a TIG cannot be more complex thancontext-free languages follows from the fact that TIGs can be converted into TAGsgenerating the same trees.
The fact that TIGs can generate path sets more complexthan regular languages i  shown by the following example.Consider the TIG in Figure 7.
The path set L generated by this grammar containsa variety of paths including Sx (from the elementary initial tree), SASBSx & SAa (fromadjoining the elementary auxiliary tree once on the initial tree), and so on.
By relyingon the fact that the intersection of two regular languages must be regular, it is easy toshow that L is not a regular language.
In particular, consider:L N {SA}*S{BS}*x = {SA}nS{BS}nxThis intersection corresponds to all the paths from root to x in the trees that aregenerated by recursively embedding the elementary auxiliary tree in Figure 7 into themiddle of its spine.
Since this intersection is not a regular language, L cannot be aregular language.4.
Parsing TIGSince TIG is a restricted case of tree-adjoining rammar (TAG), standard O(n6)-timeTAG parsers (Lang 1990; Schabes 1991; Vijay-Shanker 1987; Vijay-Shanker and Weir1993; Vijay-Shanker and Joshi 1985) can be used for parsing TIG.
Further, they can beeasily optimized to require at most O(n4)-time when applied to a TIG.
However, thisstill does not take full advantage of the context-freeness of TIG.A simple O(nB)-time bottom-up recognizer for TIG in the style of the CKY parserfor CFG can be straightforwardly constructed following the approach shown in Schabesand Waters (1993a).As shown below, one can obtain a more efficient left-to-right parsing algorithmfor TIG that maintains the valid prefix property and requires O(n 3) time in the worstcase, by combining top-down prediction as in Earley's algorithm for parsing CFGs487Computational Linguistics Volume 21, Number 4SA Ba A S*D,I.
bFigure 8An auxiliary tree and its textual representation.,1__ 4.
2 ,4S ~A5 6 7 "A---~"D "bLeftAux(,  1)Subst(, 6)Foot(,  8)(Earley 1970) with bottom-up recognition.
The algorithm is a general recognizer forTIGs, which requires no condition on the grammar.
64.1 An Earley-Style Cubic-Time Parser For TIGNotation.
Suppose that G = (G, NT, L A, S) is a TIG and that al ...  an is an input string.The Greek letters #, v, and p are used to designate nodes in elementary trees.
Subscriptsare used to indicate the label on a node, e.g., #x. Superscripts are sometimes used todistinguish between odes.A layer of an elementary tree is represented textually in a style similar to a pro-duction rule, e.g., #x--*~'Y pz.
For instance, the tree in Figure 8 is represented in termsof four layer productions as shown on the right of the figure.The predicate Init(#x) is true if and only if #x is the root of an initial tree.
Thepredicate LeftAux(px) is true if and only if px is the root of an elementary left auxiliarytree.
The predicate RightAux(px) is true if and only if Px is the root of an elementaryright auxiliary tree.
The predicate Subst(#x) is true if and only if #x is marked forsubstitution.
The predicate Foot(px) is true if and only if #x is the foot of an auxiliarytree.
The predicate Adjoin(px,,x) is true if and only if the restrictions governingadjunction in TIG permit the auxiliary tree px to be adjoined on the node #x.Chart states.
The Earley-style TIG parser collects states into a set called the chart, C. Astate is a 3-tuple, \[p, i,j\] where: p is a position in an elementary tree as described below;and 0 < i < j _< n are integers indicating a span of the input string.During parsing, elementary trees are traversed in a top-down, left-to-right mannerthat visits the frontier nodes in left-to-right order (see Figure 9).
Positions, which aredepicted as dots in Figure 9, are used to represent the state of this traversal.In a manner analogous to dotted rules for CFG as defined by Earley (1968), beingat a particular position with regard to a particular node divides the subtree rootedat the node into two parts: a left context consisting of children that have already beenmatched and a right context hat still needs to be matched.Positions are represented by placing a dot in the production for the corresponding1 2 4 layer.
For example, the fourth position reached in Figure 9 is represented as #S~#Ae#B .6 This parser is the more remarkable because for TAG the best parser known that maintains the validprefix property requires, in the worst case.
more time than parsers that do not maintain the valid prefixproperty (o(ng)-time versus O(n6)) (Schabes 1991).488Schabes and Waters Tree Insertion GrammarFigure 9Left-to-right tree traversal.IIoA ?
B ~,~?
a ?
?
A I S*~ ?
,,gAx~/  NoIn dotted layer productions, the Greek letters ~, fl, and 31 are used to represent se-quences of zero or more nodes.The indices i,j record the portion of the input string that is spanned by the leftcontext.
The fact that TIG forbids wrapping auxiliary trees guarantees that a pair ofindices is always sufficient for representing a left context.
As traversal proceeds, theleft context grows larger and larger.Correctness condition.
Given an input string al "'an, for every node #x in every ele-mentary tree in G, the Earley-style TIG parsing algorithm guarantees that:h\[#x--*~ofl, i,j\] E C if and only if there is some derivation in G of somestring beginning with al ...aj where ai+l ...aj is spanned by:A sequence of zero or more left auxiliary trees simultaneouslyadjoined on #x plusThe children of #x corresponding to g plusif fl = ~, zero or more right auxiliary trees simultaneouslyadjoined on #x.The algorithm.
Figure 10 depicts the Earley-style TIG parsing algorithm as a set ofinference rules.
Using the deductive parser developed by Shieber, Schabes, and Pereira(1995), we were able to experiment with the TIG parser represented directly in thisform (see Section 6).The first rule (1) initializes the chart by adding all states of the form \[#s--sock, 0, 0\],where #s is the root of an initial tree.
The initial states encode the fact that any validderivation must start from an initial tree whose root is labeled S.The addition of a new state to the chart can trigger the addition of other statesas specified by the inference rules in Figure 10.
Computation proceeds with the intro-duction of more and more states until no more inferences are possible.
The last rule(13) specifies that the input is recognized if and only if the final chart contains a stateof the form \[#s--+go, 0, n\], where #s is the root of an initial tree.The scanning and substitution rules recognize terminal symbols and substitutionsof trees.
They are similar to the steps found in Earley's parser for CFGs (Earley, 1970).The scanning rules match fringe nodes against he input string.
Rule 4 recognizes thepresence of a terminal symbol in the input string.
Rules 5 and 6 encode the fact that onecan skip over nodes labeled with c and foot nodes without having to match anything.489Computational Linguistics Volume 21, Number 4InitializationInit(#s) t- \ [#s~.a,  0, 0\] (1)Left Adjunction\[#A--*.a, i j\] A LeftAux(pa) A Adjoin(pA, #A) F \[p,~--,.,y,j,j\] (2)\[#a-*.a,i,j\] A \[pA--*7*,j,k\] A LeftAux(pA) A Adjoin(pA,#A) b \[#A--**a,i,k\] (3)Scanning\[#A--*a*Ua fl, i,j\] A a = aj+l \[- \[#A--+a Ua*fl, i,j +1\] (4)\[#a--*a'Ua fl, i,j\] A a = ?
F \[#A--+a Ua*fl, i,j\] (5)\[#A~a'UB fl, i,j\] A Foot(us) F \[#A--*a ut3"fl, i,j\] (6)Substitution\[#A--*a.uB fl, i,j\] A Subst(uB) A Init(ps) F \[Ps--*'7, j, j\] (7)\[#,~--*a*us fl, i,j\] A \[Ps--'v*,j, k\] A Subst(vB) A Init(pB) F \ [#a~a UB'fl, i, k\] (8)Subtree Traversal\[#a--*a,UB fl, i,j\] I- \[uB--*.%j,j\] (9)\[#A--*a*VBfl, i,j\] A \[VB--*7*,j,k\] F \[#A--*oePB*fl, i,k\] (10)Right Adjunction\[#A--*o~*, i,j\] A RightAux(pa) A Adjoin(pA, #A) b \[pA-**"/, j, j\] (11)\[#A-*a*,i,j\] A \[pA--*"/*,j,k\] A RightAux(pA) A Adjoin(p,~,#A) F \[#A--*a*, i, k\] (12)Final Recognition\[#s--~a., 0, n\] A Init(#s) b Acceptance (13)Figure 10An Earley-style recognizer for TIG, expressed using inference rules.The substitution rules are triggered by states of the form \[#A--*c~euB fl, i,j\] where UBis a node at which substitution can occur.
Rule 7 predicts a substitution.
It does thistop down only if an appropriate prefix string has been found.
Rule 8 recognizes acompleted substitution.
It is a bottom-up step that concatenates the boundaries of afully recognized initial tree with a partially recognized tree.The subtree traversal rules control the recognition of subtrees.
Rule 9 predicts asubtree if and only if the previous siblings have already been recognized.
Rule 10completes the recognition of a subtree.
Rules 9 and 10 are closely analogous to rules 7and 8.
They can be looked at as recognizing a subtree that is required to be substitutedas opposed to a subtree that may be substituted.The left and right adjunction rules recognize the adjunction of left and right aux-iliary trees.
The left adjunction rules are triggered by states of the form \[#A--*ec~, i,j\].Rule 2 predicts the presence of a left auxiliary tree, if and only if a node that theauxiliary tree can adjoin on has already been predicted.
Rule 3 supports the bottom-up recognition of the adjunction of a left auxiliary tree.
The fact that left adjunctioncan occur any number of times (including zero) is captured by the fact that states ofthe form \[#A--~-~, i,j\] represent both situations where left adjunction can occur andsituations where it has occurred.
The right adjunction rules (11 & 12) are analogous tothe left adjunction rules, but are triggered by states of the form \[#a---~c~o, i,j\].As written in Figure 10, the algorithm is a recognizer.
However, it can be straight-490Schabes and Waters Tree Insertion Grammarforwardly converted to a parser by keeping track of the reasons why states are addedto the chart.
Derivations (and therefore trees) can then be retrieved from the chart(each in linear time).For the sake of simplicity, it was assumed in the discussion above that there areno adjunction constraints.
However, the algorithm can easily be extended to handlesuch constraints by including them in the predicate Adjoin(px,/zx).Computational bounds.
The algorithm in Figure 10 requires pace O(IGIn 2) in the worstcase.
In this equation, n is the length of the input string and IG I is the size of thegrammar G. For the TIG parser, IGI is computed as the sum over all the non-leafnodes # in all the elementary trees in G of: one plus the number of children of #.
Thecorrectness of this space bound can be seen by observing that there are only IG\]n 2possible chart states \[#x--+aofl, i,j\].The algorithm takes O(IGI2n 3) time in the worst case.
This can informally be seenby noting that the worst case complexity is due to the completion rules (3, 8, 10, & 12)because they apply to a pair of states, rather than just one state.
Since each of thecompletion rules requires that the chart states be adjacent in the string, each can applyat most O(IGI2n 3) times, since there are at most n 3 possibilities for 0 < i < j < k < n.4.2 Improving the Efficiency of the TIG ParserAs presented in Figure 10, the TIG parser is optimized for clarity rather than speed.There are several ways that the efficiency of the TIG parser can be improved.Parsingthatislinearinthegrammarsize.
The time complexity of the parser can be reducedfrom O(IGI2n 3) to O(IGIn 3) by using the techniques described in Graham et al 1980).This improvement is very important, because IG\[ typically is much larger than n fornatural language applications.
The speedup can be achieved by altering the parser intwo ways.The prediction rules (2, 7, 9, & 11) can apply O(IGI2n 2) times, because they aretriggered by a chart state and grammar node /9; and for each of O(IGIn 2) possiblevalues of the former there can be O(\]GI) values of the latter.
However, the new chartstate produced by the prediction rules does not depend on the identity of the node inthe triggering chart element, nor on the value of i, but rather only on whether there isany chart element ending at j that makes the relevant prediction.
Therefore, the parsercan be changed so that a prediction rule is triggered at most once for any j and p. Thisreduces the prediction rules to a time complexity of only O(IGIn ).The completion rules (3, 8, 10, & 12) can apply O(IGI2n 3) times, because they aretriggered by pairs of chart states; and there can be O(IGI) possibilities for each elementof the pair for each i < j < k. However, the new chart state produced by the completionrules does not depend on the identity of the node p in the second chart element, butrather only on whether there is any appropriate chart element from j to/~.
Therefore,the parser can be changed so that a completion rule is triggered at most once for anypossible first chart state and k. This reduces the completion rules to a time complexityof O(IGIn3).Eliminating equivalent states.
Rules 5 and 6 merely move from state to state withoutchanging the span i,j.
These rules reflect facts about the grammar and the traversalthat do not depend on the input.
These rules can be largely precompiled out of the491Computational Linguistics Volume 21, Number 4algorithm by noting that the following states are equivalent.\[/AA--+.L, Xc~,i,j\] :-- \[/AA---~,X.C~,i,j\] if (X= c VFoot(~'x)) A -~3pA LeftAux(pA)\[#A--*a.vxfl, i,j\] =-- \[#A--+a~X.fl, i,j\] if (X = ?
VFoot(vx))To take advantage of equivalent states during parsing, one skips directly from thefirst to the last state in a set of equivalent states.
This avoids going through the normalrule application process and has the effect of reducing the grammar size.For a state \[/AA-?OL,X a, i,j\] to be equivalent to \[/AA~L, XOa, i,j\], it is not sufficient hatthe first child of vx be empty or a foot node.
It must also be the case that left adjunctionis not possible on/AA.
If left adjunction is possible on/AA, the state \[/AA--*q, vX a, i,j\] mustbe independently retained in order to trigger left adjunction when appropriate.Sharing nodes in a TIG.
An important feature of the parser in Figure 10 is that the nthchild of a node need not be unique and a subtree need not have only one parent.
(Non-uniqueness indicates that a subtree or a supertree appears at several different placesin the grammar.)
The only requirement when sharing nodes is that every possible wayof constructing a tree that is consistent with the parent-child relationships must be avalid elementary tree in the grammar.For example, consider the trees in Figure 11.SA Ba A S*D$ bSA Ba A S*aFigure 11A pair of TIG trees.They can be represented individually as follows:1....+ 2/A4 , 2 3 4 5 8 #s #A #A---~#a, #B---~#A#S, #5A---~#6 7, CeftAux(#~), Subst(#6), Foot(#~),~1~'A2 4 ,  ~'A2--+Va 3' 4 --+~'5 ~,7, ~,AS~V6, CeftAux(L,1), Foot(~,7)However, taking maximum advantage of sharing within and between the trees, theycan be represented more compactly as:14_+ 2 4 2 3 4 5 flS /AA/AB,/AA----~/Aa,/AB--'+{/AA \[ 2}/AS,8/AA___+/ADB 6 /Ab,7 LeftAux(/A1), Subst(/A6), Foot(/A8)In the above, two kinds of sharing are apparent.
Subtrees are shared by using thesame node (for example/AA) on the right-hand side of more than one layer production.Supertrees are shared by explicitly recording the fact that there are multiple alternativesfor the nth child of a some node.
This is represented textually above using curly braces.In the case of Figure 11, sharing reduces the grammar size IG\[ from 21 to 11.Depending on the amount of sharing present in a grammar, an exponential decreasein the grammar size is possible.492Schabes and Waters Tree Insertion GrammarParsing left anchored LTIGs.
The algorithm above can be extended to take advantage ofthe fact that the elementary trees in an LTIG are lexicalized.
This does not change theworst case complexity, but is a dramatic improvement in typical situations, becauseit has the effect of dramatically reducing the size of the grammar that has to beconsidered when parsing a particular input string.Space does not permit a discussion of all the ways lexical sensitivity can be intro-duced into the TIG parser.
However, one way of doing this is particularly importantin the context of this paper.
The LTIG lexicalization procedure presented in Section 5produces grammars that have no left auxiliary trees and are left anchored---ones wherefor each elementary tree, the first element hat must be matched against he input is alexical item.
By means of two simple changes in the prediction rules, the TIG parsercan benefit greatly from this kind of lexicalization.First, whenever considering a node #B for prediction at position j, it should onlybe predicted if its anchor is equal to the next input item aj+l.
Other predictions cannotlead to successful matches.
However, if sharing is being used, then one chart statecan correspond to a number of different positions in different rees.
As a result, eventhough every tree has a unique left anchor, a given chart state can correspond to a setof such trees and therefore a set of such anchors.
A prediction should be made if anyof these anchors is the next element of the input.Second, when predicting a node ~B whose first child is a terminal symbol, it isknown from the above that this child must match the next input element.
Therefore,there is no need to create the state \[#B--*eua c~,j,j\].
One can instead skip directly to thestate \[#B--*ua.c~,j,j + 11.Both of the changes above depend critically on the fact that there are no leftauxiliary trees.
In particular, if there is a left auxiliary tree PB that can be adjoined on/~B, then the next input item may be matched by p8 rather than/~B; and neither of theshortcuts above can be applied.5.
TIG Strongly Lexicalizes CFGIn the following, we say that a grammar is lexicalized (Schabes 1990; Schabes et al 1988)if every elementary structure contains a terminal symbol called the anchor.
A CFG islexicalized if every production rule contains a terminal.
Similarly, a TIG is lexicalizedif every tree contains a terminal symbol.A formalism F' is said to lexicalize (Joshi and Schabes 1992) another formalism F, iffor every grammar G in F that does not derive the empty string, there is a lexicalizedgrammar G' in F' such that G and G' generate the same string set.F' is said to strongly lexicalize F if for every finitely ambiguous grammar G in Fthat does not derive the empty string, there is a lexicalized grammar G ~ in F ~ such thatG and G ~ generate the same string set and tree set.The restrictions on the form of G in the definitions above are motivated by twokey properties of lexicalized grammars (Joshi and Schabes 1992).
First, lexicalizedgrammars cannot derive the empty string, because very structure introduces at leastone lexical item.
Thus, if a CFG is to be lexicalized, it must not be the case that S~e.Second, lexicalized grammars are finitely ambiguous, because every rule intro-duces at least one lexical item into the resulting string.
Thus, if a grammar is to bestrongly lexicalized, it must be only finitely ambiguous.
In the case of a CFG, thismeans that it must not be the case that X=~X for any non-terminal X.As shown by Greibach (1965) and Rosenkrantz (1967), any CFG grammar that doesnot generate the empty string can be converted into a lexicalized CFG.
Moreover, this493Computational Linguistics Volume 21, Number 4grammar can be left anchored--one where the first element of the right hand side ofeach rule is a terminal symbol.
However, this is only a weak lexicalization, becausethe trees generated by the lexicalized grammar are not the same as those generatedby the original CFG.Another way to lexicalize CFGs is to convert hem into categorial grammars (Bar-Hillel 1964).
However, these are again only weak lexicalizations because the treesproduced are not preserved.
7Strong lexicalization can be obtained using TAG (Joshi and Schabes 1992; Schabes1990), but only at the cost of O(n 6) parsing.
TIG is O(n 3) parsable and strongly lexi-calizes CFG.5.1 A Strong Lexicalization ProcedureIn the following, we give a constructive proof of the fact that TIG strongly lexicalizesCFG.
The proof is based on a lexicalization procedure related to the lexicalizationprocedure used to create Greibach normal form (GNF) as presented in Harrison 1978.5.1.1 Lemmas.
Our procedure relies on the following four lemmas.
The first lemmaconverts CFGs into a very restricted form of TIG.
The next three lemmas describeways that TIGs can be transformed without changing the trees produced.Lemma 1Any finitely ambiguous CFG G = (~,NT, P, S) can be converted into a TIG G' =(G, NT, I, {}, S) such that: (i) there are no auxiliary trees; (ii) no initial tree contains anyinterior nodes; (iii) G ~ generates the same trees and, therefore, the same strings as G;(iv) there is only one way to derive a given tree in G'.ProofWe assume without loss of generality that G does not contain any useless production.The set I of initial trees in G' is constructed by converting each rule R in P intoa one-level tree t whose root is labeled with the left-hand side of R. If R has n > 0elements on its right-hand side, then t is given n children, each labeled with the cor-responding right-hand-side element.
Each child labeled with a nonterminal is markedfor substitution.
If the right-hand side of R is empty, t is given one child labeled with c.By construction, there are no auxiliary trees and no interior nodes in any initial tree.There is an exact one-to-one correspondence b tween derivations in G and derivationsusing the initial trees.
Each rule substitution in G becomes a tree substitution in GLAs a result, exactly the same trees are generated in both cases, and there is only oneway to generate ach tree in G t, because there cannot be two ways to derive the sametree in a CFG.
\[\]Lemma 2Let G = (E, NT, LA, S) be a TIG.
Let t c I U A be an elementary tree whose rootis labeled Y and let # be a frontier element of t that is labeled X and marked forsubstitution.
Further, suppose that if t is an initial tree, X ~ Y.
Let T' be the set of7 This is true even if Bar-Hillel's Categorial Grammars are augmented with composition (.Joshi, personalcommunication).494Schabes and Waters Tree Insertion Grammarevery tree t ~ that can be created by substituting an X-rooted tree u E I for #.
DefineG' -= (~, NT, I', A', S) where I' and A' are created as follows.If t E I then I' = ( I -  {t}) U T' and A' = A.If t E A then I' = I and A' = (A -  {t}) U T'.Then, G' generates exactly the same trees as G. Further, if there is only one wayto generate ach tree generated by G, then there is only one way to generate ach treegenerated by GLProofThe transformation specified by this lemma closes over substitution into # and thendiscards t. Since t cannot be substituted into #, this only generates a finite number  ofadditional trees.Any complete derivation in G can be converted into exactly one derivation in G'as follows: A derivation consists of elementary trees and operations between them.Every use of t in a complete derivation in G has to be associated with a substitution ofsome u E I for #.
Taken as a group, the two trees t and u, along with the substitutionoperation between them, can be replaced by the appropriate new tree t ~ E T t that wasadded in the construction of G ~.Since TIGs do not treat the roots of initial trees in any special way, there is noproblem converting any operation applied to the root of u into an operation on thecorresponding interior node of t'.
Further, since it cannot be the case that t = u, thereis no ambiguity in the mapping defined above.Any derivation in G ~ can be converted into exactly one derivation in G by doingthe reverse of the conversion above.
Each instance t' of one of the new trees introducedis replaced by an instance of t with the appropriate initial tree u E I being combinedwith it by substitution.Again, since TIGs do not treat the roots of initial trees in any special way, there isno problem converting any operation applied to an interior node of t ~ that correspondsto the root of u into an operation on the root of u.Further, if there is only one way to derive a given tree in G, there is no ambiguity inthe mapping from derivations in G' to G, because there is no ambiguity in the mappingof T ~ to trees in G. The tree t ~ must be different from the other trees generated whencreating T ~, because t~ contains complete information about the trees it was createdfrom.
The tree t ~ must not be in I U A.
If it were, there would be multiple derivationsfor some tree in G---one involving t ~ and one involving t and u.
Finally, t' must bedifferent from t, because it must  be larger than t.If there is only one way to derive a given tree in G, the mappings between deriva-tions in G' and G are one-to-one and there is therefore only one way to derive a giventree in G ~.
\[\]Lemma 3Let G = (E, NT, I,A, S) be a TIG.
Let t E I be an elementary initial tree whose root islabeled with X ~ S. Further, suppose that none of the substitution odes, if any, onthe fringe of t are labeled X.
Let U' be the set of every initial tree that can be createdby substituting t for one or more frontier nodes in an initial tree u E I that are labeled495Computational Linguistics Volume 21, Number 4X and marked for substitution.
Let W be the set of every auxiliary tree that can becreated by substituting t for one or more frontier nodes in an auxiliary tree v E Athat are labeled X and marked for substitution.
Define G ~ = (G, NT, I~,A',S) whereI' = (I - {t}) U U' and A' = A U V'.Then, G ~ generates exactly the same trees as G. Further, if there is only one wayto generate ach tree generated by G, then there is only one way to generate ach treegenerated by G ~.ProofThe transformation specified by this lemma closes over substitution of t and thendiscards t. Since t cannot be substituted into itself, this generates only a finite numberof additional trees.
Since the root of t is not labeled S, t is not required for any purposeother than substitution.Any complete derivation in G can be converted into exactly one derivation in G ~as follows: Since the root of t is not labeled S, every use of t in a complete derivationin G has to be substituted into some frontier node # of some u E I U A.
Taken as agroup, the two trees u and t, along with any other copies of t substituted into otherfrontier nodes of u and the substitution operations between them, can be replaced bythe appropriate new tree u ~ E U ~ U V ~ that was added in the construction of GLSince TIGs do not treat the roots of initial trees in any special way, there is noproblem converting any operation applied to the root of t into an operation on thecorresponding interior node of u/.
Further, since it cannot be the case that t = u, thereis no ambiguity in the mapping  defined above.Any derivation in G ~ can be converted into a derivation in G by doing the reverseof the conversion above.
Each instance u~ of one of the new trees introduced is replacedby one or more instances of t substituted into the appropriate tree u E I U A.Again, since TIGs do not treat the roots of initial trees in any special way, there is noproblem converting any operation applied to the interior node of u ~ that correspondsto the root of t into an operation on the root of t.Further, if there is only one way to derive a given tree in G, there is no ambiguityin the mapping from derivations in G r to G, because there is no ambiguity in themapping of u I to trees in G. The tree u ~ must  be different from the trees that aregenerated by substituting t in other trees u, because u~ contains complete informationabout the trees it was created from.
The tree u r must not be in I U A.
If it were, therewould be multiple derivations for some tree in G---one involving u ~ and one involvingu and t. Finally, u ~ must be different from t, because it must  be larger than t.If there is only one way to derive a given tree in G, the mappings between deriva-tions in G ~ and G are one-to-one and there is therefore only one way to derive a giventree in G/.
E3Lemma 4Let G = (G, NT, LA, S) be a TIG and X E NT be a nonterminal.
Let T C I be the set ofevery elementary initial tree t such that the root of t and the leftmost nonempty  frontiernode of t are both labeled X.
Suppose that every node labeled X where adjunction canoccur is the root of an initial tree in I.
Suppose also that there is no tree in A whoseroot is labeled X.
Let T ~ be the set of right auxiliary trees created by marking the firstnonempty  frontier node of each element of T as a foot rather than for substitution.Define G' = (~, NT, I - T, A U T', S).Then, G / generates exactly the same trees as G. Further, if there is only one wayto generate ach tree generated by G, then there is only one way to generate ach treegenerated by GC496Schabes and Waters Tree Insertion GrammarProofNote that when converting the trees in T into trees in T ~, every initial tree is convertedinto a different auxiliary tree.
Therefore, there is a one-to-one mapping  between treesin T and T'.
Further, since there are no X-rooted trees in A, A N T' = {}.Since in G, every node labeled X where adjunction can occur is the root of aninitial tree in / ,  it must be the case that in G', every node labeled X where adjunctioncan occur is the root of an initial tree in I', because the construction of T t did notcreate any new nodes labeled X where adjunction can occur.
Therefore, the only waythat any element of T' can be used in a derivation in G' is by adjoining it on the rootof an initial tree u.
The effect of this adjunction is exactly the same as substitutingthe corresponding t E I in place of u and then substituting u for the first nonemptyfrontier node of t.Any complete derivation in G can be converted into exactly one derivation in G'as follows: Every instance of a tree in T has to occur in a substitution chain.
Thechain consists of some number  of instances h, t2 .
.
.
.
,tm of trees in T, with each treesubstituted for the leftmost nonempty  frontier node of the next.
The top of the chaintm is either not substituted anywhere (i.e., only if X = S) or substituted at a node thatis not the leftmost nonempty node of a tree in T. The bottom tree in the chain tl hassome tree u ~ T substituted for its leftmost nonempty  frontier node.
Since there are noX-rooted trees in A, there cannot be any adjunction on the root of u or on the roots ofany of the trees in the chain.
The chain as a whole can be replaced by the simultaneousadjunction of the corresponding trees ' ' ' in T t tl, t 2 .
.
.
.
.
t m on the root of u, with u usedin the same way that tm was used.Any derivation in G' can be converted into a derivation in G by doing the reverseof the conversion above.
Each use of a tree in T' must occur as part of the simultaneousadjunction of one or more auxiliary trees on the root of some initial tree u, becausethere are no other nodes at which this tree can be adjoined.
Since the trees in T' are theonly X-rooted trees in A ~, all the trees being simultaneously adjoined must  be instancesof trees in T t. The simultaneous adjunction can be replaced with a substitution chaincombining the corresponding trees in T, with u substituted into the tree at the bottomof the chain and the top of the chain used however u was used.Further, if there is only one way to derive a given tree in G, there is no ambiguityin the mapping from derivations in G' to G, because there is no ambiguity in themapping of the t; to trees in G. If there is only one way to derive a given tree in G,the mappings between derivations in G ~ and G are one-to-one and there is thereforeonly one way to derive a given tree in GL \[\]After an application of Lemmas 2-4, a TIG may no longer be in reduced form;however, it can be brought back to reduced form by discarding any unnecessary ele-mentary trees.
For instance, in Lemma 2, if # is the only substitution ode labeled Xand X ~ S, then when t is discarded, every X-rooted initial tree can be discarded aswell.5.1.2 Constructing an LTIG.
Using the above lemmas, an LTIG corresponding to aCFG can be constructed.Theorem 2If G = (G, NT, P,S) is a finitely ambiguous CFG that does not generate the emptystring, then there is an LTIG G' = (G, NT, I',A', S) generating the same language andtree set as G with each tree derivable in only one way.
Furthermore, G' can be chosen497Computational Linguistics Volume 21, Number 4so that all the auxiliary trees are right auxiliary trees and every elementary tree is leftanchored.ProofTo prove the theorem, we first prove a somewhat weaker theorem and then extendthe proof to the full theorem.
We assume for the moment hat the set of rules for Gdoes not contain any empty rules of the form A ~ ?.The proof proceeds in four steps.
At each step, none of the modifications madeto the grammar change the tree set produced nor introduce more than one way toderive any tree.
Therefore, the degree of ambiguity of each string is preserved by theconstructed LTIG.An ordering {A1 .
.
.
.
.
Am} of the nonterminals NT is assumed.?
Step 1: Using Lemma 1, we first convert G into an equivalent TIG(Y~,NT, I {}, S), generating the same trees.
Because G does not containany empty rules, the set of initial trees created oes not contain anyempty trees.?
Step 2: In this step, we modify the grammar of Step 1 so that everyinitial tree t E I satisfies the following property fL Let the label of theroot of t be Ai.
The tree t must either:(i)(ii)be left anchored, i.e., have a terminal as its first nonemptyfrontier node; orhave a first nonempty frontier node labeled Aj where i < j.We modify the grammar to satisfy f~ inductively for increasing valuesof i.
Consider the Al-rooted initial trees that do not satisfy Ft.
Such treesmust have their first nonempty frontier node labeled with A1.
Theseinitial trees are converted into right auxiliary trees as specified byLemma 4.
The applicability of Lemma 4 in this case is guaranteed since,after Step 1, there are no auxiliary trees, no interior nodes, and TIGprohibits adjunction at frontier nodes.We now assume inductively that Ft holds for every Ai rooted initialtree t where i < k.Step 2a: Consider the Ak-rooted initial trees that fail to satisfy Ft.Each one must have a first nonempty frontier node # labeledwith Aj where j _< k. For those where j < k, we generate a newset of initial trees by substituting other initial trees for # inaccordance with Lemma 2.By the inductive hypothesis, the substitutions specified byLemma 2 result in trees that are either left anchored, or havefirst nonempty frontier nodes labeled with A l where I > j. Forthose trees where 1 ~ k, substitution as specified by Lemma 2 isapplied again.After at most k - 1 rounds of substitution, we reach asituation where every Ak-rooted initial tree that fails to satisfy Fthas a first nonempty frontier node labeled with Ak.Step 2b: The Ak-rooted initial trees where the first nonemptyfrontier node is labeled with Ak are then converted into rightauxiliary trees as specified by Lemma 4.
The applicability of498Schabes and Waters Tree Insertion GrammarLemma 4 in this situation is guaranteed by the following facts.First, there cannot have previously been any Ak-rooted auxiliarytrees, because there were none after Step 1, and every auxiliarytree previously introduced in this induction has a root labeled Aifor some i < k. Second, there cannot be any internal nodes inany elementary tree labeled Ak, because there were none afterStep 1, and all subsequent substitutions have been at nodeslabeled Ai where i < k.Steps 2a and 2b are applied iteratively for each i, 1 < i _< m until everyinitial tree satisfies f~.?
Step 3: In this step, we modify the set of initial trees further until everyone is left anchored.
We modify the grammar to satisfy this propertyinductively for decreasing values of i.According to property f~, every Am-rooted initial tree is left anchored,because there are no higher indexed nonterminals.We now assume inductively that every Ai rooted initial tree t wherei > k is left anchored.The Ak rooted initial-trees must be left anchored, or have leftmostnonempty frontier nodes labeled with Aj, where j > k. When the label isAj, we generate new initial trees using Lemma 2.
These new rules are allleft anchored, because by the induction hypothesis, all the trees usubstituted by Lemma 2 are left anchored.The above is repeated for each i until i = 1 is reached.?
Step 4: Finally, consider the auxiliary trees created above.
Each is a rightauxiliary tree.
If an auxiliary tree t is not left anchored, then the firstnonempty frontier element after the foot is labeled with somenonterminal Ai.
There must be some nonempty frontier element after thefoot of t because G is not infinitely ambiguous.
We can use Lemma 2 yetagain to replace t with a set of left anchored right auxiliary trees.
All thetrees produced must be left anchored because all the initial treesresulting from Step 3 are left anchored.?
Empty rules: The auxiliary assumption that G does not contain emptyrules can be dispensed with.If G contains empty rules, then the TIG created in Step 1 will containempty trees.
These trees can be eliminated by repeated application ofLemma 3.
Let t be an empty tree.
Since G does not derive the emptystring, the label of the root of t is not S. The tree t can be eliminated byapplying Lemma 3.
This can lead to the creation of new empty trees.However, these can be eliminated in turn using Lemma 3.
This processmust terminate because G is finitely ambiguous.Mark all the interior nodes in all the initial trees created by Lemma 3as nodes where adjunction cannot occur.
With the inclusion of theseadjoining constraints, the procedure above works just as before.
\[\]In the worst case, the number of elementary trees created by the LTIG procedureabove can be exponentially greater than the number of production rules in G. Thisexplosion in numbers comes from the compounding of repeated substitutions in Steps2&3.499Computational Linguistics Volume 21, Number 4CFGA1 ~ A2A2A2 --* A1A2IA2AIIaStep 1 A1 A2 A2 A2/N  /N  /N  IA25 A25 A15 A25 A25 A15 aStep 2A1 A2 A2 A2/N  /N  /N  IA25 A25 A 1 A25 A2* A15 a/NA2* A25Step 3AI A2 A 2 A2A 2 A25 A 1 A25 A2* AI$ aI /Na A2* A25Step 4 (final LTIG)A1 A2 A2 A2/N  /N  /N  IA 2 A25 A 1 A25 A2* A 1 aI /N  /Na A2* A 2 A 2 A25I Ia aFigure 12Example of the operation of the LTIG procedure.However, as noted at the end of Section 4, counting the number of elementarytrees is not an appropriate measure of the size of an LTIG.
The compounding ofsubstitutions in the LTIG procedure causes there to be a large amount of sharingbetween the elementary trees.
Taking advantage of this sharing can counteract theexponential growth in the number of rules completely.
In particular, if the CFG doesnot have any empty rules or sets of mutually left recursive rules involving more thanone nonterminal, then the size of the LTIG created by the procedure of Theorem 2 willbe smaller than the size of the original CFG.On the other hand, if a grammar has many sets of mutually left recursive rulesinvolving more than one nonterminal, even taking advantage of sharing cannot stopan exponential explosion in the size of the LTIG.
In the worst case, a grammar withm nonterminals can have m!
sets of mutually left recursive rules, and the result LTIGwill be enormous.5.1.3 An Example.
Figure 12 illustrates the operation of the LTIG procedure.
Step 1of the procedure converts the CFG at the top of the figure to the TIG shown on thesecond line.500Schabes and Waters Tree Insertion GrammarIn Step 2, no change is necessary in the Al-initial tree.
However, the first A2-initialtree has the Al-initial tree substituted into it.
After that, the first two A2-initial treesare converted into auxiliary trees as shown on the third line of Figure 12.In step 3, the Al-initial tree is lexicalized by substituting the remaining A2-initialtree into it.
Step 4 creates the final LTIG by lexicalizing the auxiliary trees.
The Al-initialtree is retained under the assumption that A1 is the start symbol of the grammar.5.1.4 LTIG Strongly Lexicalized TIG.
It has been shown (Joshi and Schabes 1992;Schabes 1990) that TAG extended with adjoining constraints not only strongly lexical-izes CFG, but itself as well.
We conjecture that our construction can be extended sothat given any TIG as input, an LTIG generating the same trees could be produced.
Aswith TAGs, adjoining constraints forbidding the adjunction of specific auxiliary treeson specific nodes can be required in the resulting LTIG.5.2 Comparison of the LTIG, GNF, and Rosenkrantz Procedures5.2.1 The GNF Procedure.
The LTIG procedure of Theorem 2 is related to the proce-dure traditionally used to create GNF (see, for example, Harrison, 1978).
This proce-dure is referred to below as the GNF procedure.
This procedure is not the procedureoriginally developed by Greibach (1965).
Rather, it is very similar to the proceduredeveloped shortly thereafter by Abbott and Kuno (1965).
The main part of the GNFprocedure operates in three steps that are similar to Steps 2, 3, & 4.
However, thereare five important differences between the LTIG and GNF procedures.First, in lieu of Step 1, the GNF procedure converts the input into Chomsky normalform.
This eliminates infinite ambiguity and empty rules, and puts the input grammarin a very specific form.
The elimination of infinite ambiguity is essential, because theGNF procedure will not operate if infinite ambiguity is present.
The elimination ofempty rules is also essential, because mpty rules in the input to the rest of the GNFprocedure lead to empty rules in the output.
However, the remaining changes causedby putting the input in Chomsky normal form are irrelevant o the basic goal of cre-ating a left anchored output.
A more compact left anchored grammar can typicallybe produced by eliminating infinite ambiguity and empty rules without making theother changes necessary to put the input in Chomsky normal form.
In the follow-ing discussion, we assume a modified version of the GNF procedure that takes thisapproach.Second, the GNF procedure can reduce the ambiguity of the input grammar.
Thisis due to loss of information when the same rule is derived in more than one wayby the GNF procedure.
Ambiguity can be retained simply by retaining any duplicaterules that are derived (Abbott and Kuno 1965).Third, the GNF procedure changes the trees produced.
This is an essential dif-ference and cannot be avoided.
However, as shown by Abbott and Kuno (1965), itis possible to transform parse trees created using the GNF into the parse trees thatwould have been obtained using the original grammar, based on a record of exactlyhow each GNF rule was derived.
In contrast o LTIG, which derives the correct reesin the first place, this transformation requires a separate post phase after parsing.The fourth important difference between the LTIG and GNF procedures i the waythey handle left recursive rules.
The LTIG procedure converts them into right auxiliarytrees.
In contrast, the GNF procedure converts them into right recursive rules.
That isto say, the GNF procedure converts rules of the form Ak --+ AkO~ I fl into rules of theform Ak --~ fl \[ flZk and Zk ~ ~ \[ C~Zk.
This is the source of the most radical changes inthe trees produced.501Computational Linguistics Volume 21, Number 4A1 ~ A2A2CFGA2 ~ A1A21A2AllaA1 --~ A2A2Step2 A2 ~ aZ2\[aZ 2 ---+ AllA2A2\]A2A2Z2\]A1Z2A1 ~ aA2IaZ2A2Step 3 A2 --* aZ21aZ2 ---+ A2A21AllAaA2Z21AIZ2A1 ~ aA21aZ2A2GNF A2 --~ aZ21aZ 2 ----+ aA2\]aA2Z2\]aZ2A21aZ2A2Z2Figure 13Example of the operation of the GNF procedure.A1 --* aZ2A2A2 ~ ag2Z2 ---+ aZ2A2Z2\]cFigure 14The LTIG of Figure 12 converted into a CFG.Figure 13 illustrates the operation of the GNF procedure when applied to the sameCFG as in Figure 12.
Since the input grammar is finitely ambiguous and has no emptyrules, it can be operated on as is.The step of the GNF procedure corresponding to Step 2 of the LTIG procedureconverts the CFG at the top of Figure 13 into the rules shown in the second part ofthe figure.
No change is necessary in the A1 rule.
However, the first A2 rule has the A1rule substituted into it.
After that, the left recursive A2 rules are converted into rightrecursive rules utilizing a new nonterminal Z2.The step of the GNF procedure corresponding to Step 3 of the LTIG procedurelexicalizes the A1 rule by substituting the A2 rules into it.The final step of the GNF procedure l xicalizes the Z2 rules as shown at the bottomof Figure 13.
Note that there are eight ways of substituting an A1 or A2 rule into the firstposition of a Z2 rule, but they yield only four distinct rules.
For example, substitutingA1 ---* aA2 into Z2 --* A1 yields the same result as substituting A2 ~ a into Z2 --+ A2A2.If the LTIG created in Figure 12 is converted into a CFG as specified in Theorem 1,the rules in Figure 14 are obtained.
Ambiguity is lost in this transformation, becauseboth auxiliary trees turn into the same rule.
If the empty rule in Figure 14 is eliminatedby substitution, a grammar identical to the one at the bottom of Figure 13 results.We conjecture that there is, in general, an exact correspondence between the outputof the LTIG procedure and the GNF procedure.
In particular, if (a) the LTIG procedureis applied to a CFG in Chomsky normal form, (b) the LTIG is converted into a CFGas specified in Theorem 1, and (c) any resulting empty rules are eliminated by substi-tution, the result is always the same CFG as that produced by the GNF procedure.The fifth important difference between the LTIG and GNF procedures i that the502Schabes and Waters Tree Insertion Grammaroutput of the LTIG procedure can be represented compactly.
There are two reasonsfor this.
To start with, the use of auxiliary trees in an LTIG can allow it to be expo-nentially smaller than the equivalent GNF.
To see this, note that the elimination ofempty rules required when converting an LTIG into a GNF can cause an exponentialincrease in the number of rules.
Furthermore, the trees created by the LTIG proce-dure have an extremely repetitive structure.
As a result, node sharing can typically beused to represent the LTIG compactly--it is often smaller than the original CFG (seeSection 6.1).5.2.2 The Rosenkrantz Procedure.
Another point of comparison with the LTIG proce-dure is the CFG lexicalization procedure of Rosenkrantz (1967).
This procedure oper-ates in a completely different way from Greibach's procedure--simultaneously elimi-nating all leftmost derivation paths of length greater than one, rather than shorteningderivation paths one step at a time via substitution and eliminating left recursive rulesone nonterminal t a time.One consequence of the simultaneous nature of the Rosenkrantz procedure is thatone need not select an order of the nonterminals.
This contrasts with the Greibach andLTIG procedures where the order chosen can have a significant impact on the numberof elementary structures in the result.As with the GNF procedure, one typically begins the Rosenkrantz procedure byconverting the input to Chomsky normal form.
This is necessary to remove infiniteambiguity and empty rules.
However, it is also needed to remove chain rules, whichwould otherwise lead to nonlexicalized rules in the output.
The conversion to Chore-sky normal form makes a lot of other changes as well, which are largely counterpro-ductive if one wants to construct a left anchored grammar.Also like the GNF procedure, ambiguity can be reduced and the trees derived arechanged.
However, the ambiguity can be retained if duplicate rules are maintained.It should also be possible to convert he resulting parse trees into parse trees in theoriginal grammar.
This could be a complicated process, however, since the Rosenkrantzalgorithm alters the trees more radically than the GNF procedure.A key advantage of the Rosenkrantz procedure is that, unlike the Greibach andLTIG procedures, the output it produces cannot be exponentially arger than the in-put.
In particular, the growth in the number of rules is at worst O(mS), where m isthe number of nonterminals.
However, the Rosenkrantz procedure typically producesgrammars that are less compact han those created by the LTIG procedure (see Sec-tion 6.1).It may be useful to develop a formalism and procedure that bare the same relation-ship to the Rosenkrantz procedure that TIG and the LTIG procedure bare to the GNFprocedure.
Given the fundamental dvantages of the Rosenkrantz procedure over theGNF procedure, this might lead to a result that is superior to the LTIG procedure.5.3 Variants of the LTIG ProcedureThe LTIG procedure above creates a left anchored LTIG that uses only right auxiliarytrees.
As shown in Section 6.3, this is quite an advantageous form.
However, otherforms might be more advantageous in some situations.
Many variants of the LTIGprocedure are possible.
For example, everywhere in the procedure, the word "right"can be replaced by "left" and vice versa.
This results in the creation of a right anchoredLTIG that uses only left auxiliary trees.
This could be valuable when processing alanguage with a fundamentally eft recursive structure.A variety of steps can be taken to reduce the number of elementary trees producedby the LTIG procedure.
To start with, the choice of an ordering {A1 .
.
.
.
.
Am} for the503Computational Linguistics Volume 21, Number 4Nonterminals Terminals Rules SizeTomita I 5 4 8 22Tomita II 13 9 43 133Tomita III 38 54 224 679Tomita IV 45 32 394 1,478Treebank 200 11 31 200 689Treebank 500 14 36 500 1,833Treebank 1000 16 36 1,000 3,919Mike 25 102 145 470Left Cycles of Length Right Cycles of Length1 2 > 2 1 2-9 > 9Tomita I 2 0 0 0 1 0Tomita II 7 0 0 8 3 0Tomita III 10 0 0 11 2,260 12,595Tomita IV 13 0 0 11 3,453 5,964Treebank 200 5 0 0 5 15 0Treebank 500 9 1 0 9 945 44Treebank 1000 11 2 0 10 14,195 5,624Mike 0 0 0 1 1 0Figure 15Properties of the Grammars used as test cases.nonterminals is significant.
In the presence of sets of mutually left recursive rulesinvolving more than one nonterminal (i.e., sets of rules of the form {A ~ Bfl, BAc~}), choosing the best ordering of the relevant nonterminals can greatly reduce thenumber of trees produced.If one abandons the requirement that the grammar must be left anchored, one cansometimes reduce the number of elementary trees produced ramatically.
The reasonfor this is that instead of being forced to lexicalize ach rule in G at the first position onits right hand side, one is free to choose the position that minimizes the total numberof elementary trees eventually produced.
However, one must be careful to meet therequirements imposed by TIG while doing this.
In particular, one must create onlyleft and right auxiliary trees as opposed to wrapping auxiliary trees.
The search spaceof possible alternatives i  so large that it is not practical to find an optimal LTIG;however, by means of simple heuristics and hill climbing, significant reductions in thenumber of elementary trees can be obtained.Finally, one can abandon the requirement that there be only one way to deriveeach tree in the LTIG.
This approach is discussed in Schabes and Waters 1993c.
Inthe presence of sets of mutually left recursive rules involving more than one nonter-minal, allowing increased ambiguity can yield significant reduction in the number ofelementary trees.It should be noted that while exploring ways to create LTIGs with small numbersof elementary trees is interesting, it may not be of practical significance because thenumber of elementary trees is not a good measure of the size of a TIG.
In particular,if a decreased number of elementary trees is accompanied by decreased sharing, thiscan lead to an increase in the grammar size, rather than a decrease.
As illustrated inSection 6.1, the opportunities for sharing between the elementary trees in the LTIGscreated by the LTIG procedure is so high that the grammars produced are often smallerthan alternatives that have many fewer elementary trees.504Schabes and Waters Tree Insertion Grammar6.
Experimental ResultsThe experiments below use eight grammars for fragments of English as test cases(see Figure 15).
The first four grammars are the test CFGs used by Tomita (1985).The next three grammars are derived from the Treebank corpus (Brill et al 1990)of hand-parsed sentences from the Wall Street Journal.
Each "Treebank n" grammarcorresponds to the n most commonly occurring local rules in the corpus that form aCFG with no useless productions.
8 The eighth grammar is a CFG grammar used in thenatural language processing component ofa simple interactive computer environment.It supports conversation with an animated robot called Mike (Rich et al 1994a nd1994b).The grammars are all finitely ambiguous and none generates the empty string.
TheTomita III grammar contains an empty rule.
The relative size and complexity of thegrammars i indicated at the top of Figure 15.
The size \[G\[ is computed as appropriatefor an Earley-style CFG parser--i.e., as the number of possible dotted rules, which isthe sum, over all the rules, of: one plus the number of elements on the right-hand sideof the rule.The bottom of Figure 15 summarizes the left and right recursive structure of thetest grammars.
The grammars have very few sets of mutually left recursive rulesinvolving more than one nonterminal.
In contrast, all but the smallest grammars havemany sets of mutually right recursive rules involving significant numbers of differentnonterminals.
This reflects the fact that English is primarily right recursive in nature.Due to the unbalanced recursive nature of the test grammars, left anchored lexical-izations are more compact than right anchored ones.
For languages that are primarilyleft recursive in nature, the situation would be reversed.The experiments below are based on parsing a corpus of randomly generatedsentences.
For each test grammar, four sentences were generated of each possiblelength from 1-25.
The top of Figure 16 shows the average number of parses of thesesentences versus sentence length.
The ambiguity varies by five orders of magnitudeacross the test corpus.The bottom of Figure 16 shows the average number of chart states created whenparsing the test sentences using a standard Earley-style CFG parser.
As is to be ex-pected, the number of chart states rises significantly with the complexity of the gram-mars, varying by two orders of magnitude.
The number of chart states also growswith the length of the sentences, but not much faster than linearly.6.1 The Size of LTIG GrammarsThe top of Figure 17 shows the number of elementary initial and auxiliary trees ingrammars created by the LTIG procedure given the various test grammars.
Becausemost of the test grammars do not have sets of mutually left recursive rules involvingmore than one nonterminal, the order chosen for the nonterminals typically has noeffect on the output.
However, for the grammars where there is an effect, the orderingthat lead to the smallest number of elementary trees was automatically chosen.The middle portion of the table summarizes the left anchored LTIGs created bythe procedure of Theorem 2.
The rightmost portion summarizes unconstrained LTIGscreated by a hill-climbing algorithm that attempts to minimize the number of elemen-8 A local rule encodes the relationship between a node in a tree and its immediate children.
For example,the second tree on the last line of Figure 12 corresponds to three local rules A 2 ~ AIA2, A1 --* A2A2,and A 2 ---+ a.505Computational Linguistics Volume 21, Number 41-5 6-10 11-15 16-20 21-25Tomita I 1 4 25 174 3,696Tomita II 1 2 3 50 46Tomita III 1 2 6 66 58Tomita IV 1 11 25 140 624Treebank 200 1 1 3 8 36Treebank 500 1 4 20 218 1,721Treebank 1000 2 36 1,376 23,106 279,656Mike 1 1 1 1 11-5 6-10 11-15 16-20 21-25Tomita I 23 51 88 135 205Tomita II 145 308 461 698 898Tomita III 304 577 1,026 1,370 1,788Tomita IV 827 1,436 2,311 3,192 4,146Treebank 200 526 1,054 1,500 2,171 2,717Treebank 500 1,193 2,762 4,401 6,712 8,566Treebank 1000 3,795 8,301 15,404 23,689 32,633Mike 124 163 264 334 435Figure 16Properties of the sentences used as test cases versus sentence l ngth.
Top: average ambiguity.Bottom: average chart size.CFG Left LTIG LTIGRules Initial Auxiliary Initial AuxiliaryTomita I 8 6 2 5 1Tomita II 43 905 7 87 8Tomita III 224 1,790 45 522 51Tomita IV 394 40,788 469 1,456 201Treebank 200 200 648 77 284 76Treebank 500 500 9,558 4,497 794 698Treebank 1000 1,000 1,050,343 667,972 2,792 3,306Mike 145 626 0 267 0CFG Left LTIG LTIGFigure 17Tomita I 22 16 21Tomita II 133 115 125Tomita III 679 528 665Tomita IV 1,478 1,263 1,438Treebank 200 689 517 677Treebank 500 1,833 1,427 1,801Treebank 1000 3,919 3,146 3,839Mike 470 356 470Properties of LTIGS corresponding to the test grammars.
Top: numbers of elementary trees.Bottom: grammar size IGI.tary trees produced.
It can be seen that the left anchored LTIG corresponding to a CFGcan have many more elementary trees than an unconstrained LTIG.The bottom of Figure 17 shows the sizes of the various LTIGS.
The sizes aresmaller than the numbers of trees, because there is a large amount  of sharing between506Schabes and Waters Tree Insertion GrammarCFG Left LTIG Rosenkrantz GNFTomita I 8 8 16 19Tomita II 43 912 861 10,848Tomita III 224 1,835 3,961 4,931Tomita IV 394 41,257 45,834 243,374Treebank 200 200 725 2,462 1,723Treebank 500 500 14,055 20,896 149,432Treebank 1000 1,000 1,718,315 133,170 > 10 sMike 145 626 656 843CFG Left LTIG Rosenkrantz GNFTomita I 22 16 54 68Tomita II 133 115 3,807 100,306Tomita III 679 528 16,208 29,622Tomita IV 1,478 1,263 257,206 2,461,556Treebank 200 689 517 11,104 9,546Treebank 500 1,833 1,427 106,812 1,591,364Treebank 1000 3,919 3,146 766,728 > 10 9Mike 470 356 2,439 4,384Figure 18Comparison of the LTIG, Rosenkrantz,structures.
Bottom: grammar size.and GNF procedures.
Top: number of elementarythe elementary structures in the LTIGs.
In fact, there is so much sharing that the LTIGsare smaller than the corresponding CFGs.The left anchored LTIGs are also smaller than the unconstrained LTIGs.
This ispossible because of the small number of sets of mutually left recursive rules involvingmore than one nonterminal in the test grammars.
If there were many such sets, theleft anchored LTIGs could be larger than the unconstrained ones; and it might befruitful to consider using a right anchored LTIG.
If there were many sets of mutuallyleft recursive rules and many sets of mutually right recursive rules, then every LTIGmight be large.6.2 The GNF and Rosenkrantz ProceduresAs a basis for comparison with the LTIG procedure, the GNF and Rosenkrantz proce-dures were implemented as well.
To minimize the size of the grammars produced bythese latter procedures, the input grammars were not converted to Chomsky normalform, but rather only modified to the minimal extent required by the procedures ( eeSection 5.2).
This yielded savings that were almost always significant and sometimesdramatic.
In the case of the GNF procedure, the order of nonterminals was chosen soas to minimize the number of rules produced.The top of Figure 18 compares the grammars produced by the three procedures interms of the number of elementary structures.
Except for Treebank 200, the Rosenkrantzprocedure created fewer rules than the GNF procedure and on the larger grammars,dramatically fewer.
The LTIG procedure created somewhat fewer elementary structuresthan the Rosenkrantz procedure, except hat for Treebank 1000, the LTIG has thirteentimes more elementary structures than the Rosenkrantz grammar.
Assumedly, the largesize of the LTIG for Treebank 1000 reflects the fundamentally exponential behavior ofthe LTIG procedure in comparison to the polynomial behavior of the Rosenkrantzprocedure.The bottom of Figure 18 takes sharing into account and compares the sizes of thevarious grammars.
It reveals that the LTIGs are much more compact han the othergrammars, particularly for the larger test grammars.The entries in Figure 18 for the Treebank 1000 GNF grammar are only approxi-507Computational Linguistics Volume 21, Number 4CFG Left LTIG Rosenkrantz GNFTomita I 1.00 0.69 0.94 0.95Tomita II 1.00 0.31 0.39 2.14Tomita III 1.00 0.09 0.08 0.13Tomita 1V 1.00 0.14 0.28Treebank 200 1.00 0.12 0.15 0.53Treebank 500 1.00 0.13 0.27Treebank 1000 1.00 0.19Mike 1.00 0.21 0.17 0.19CFG Left LTIG Rosenkrantz GNFTomita I 1.0 1.0Tomita II 1.0 1.0Tomita III 1.0 1.0Tomita IV 1.0 1.0Treebank 200 1.0 1.0Treebank 500 1.0 1.0Treebank 1000 1.0 1.0Mike 1.0 1.0Figure 19Parsing properties of LTIG, Rosenkrantz, andBottom: relative ambiguity.1.0 1.01.0 1.00.7 0.70.81.0 0.90.81.0 1.0GNF grammars.
Top: relative chart sizes.mate, because this grammar is too large to be practically computed, given the facilitiesavailable to the authors.
We had to estimate the number of rules based on the numberof substitutions called for by the GNF procedure.6.3 Parsing with LTIGTo evaluate parsing with LTIG, three experimental parsers were implemented usingthe deductive ngine developed by Shieber, Schabes, and Pereira (1995).
The test gram-mars were parsed using a standard Earley-style CFG parser.
The grammars createdby the Greibach and Rosenkrantz procedures were parsed using an Earley-style CFGparser adapted to take full advantage of left anchored CFG grammars.
The grammarsproduced by the LTIG procedure were parsed with the parser of Section 4 extendedin all the ways discussed in Section 4.2 so that it takes full advantage of sharing andthe left anchored nature of these LTIGs.
Every effort was extended to make the threeparsers as identical as possible, so that any differences in parsing would be due to thegrammars used, rather than the parsers.The top of Figure 19 compares the number of chart states required when parsingusing the various grammars.
The numbers are averages over all the test sentences ofthe ratio of the number of chart states created using Various grammars to the chartstates created when parsing using the original CFG.Chart states, instead of parsing times, are used as a basis for comparison becausethey can be more reliably and repeatably obtained than parsing times and because theyallow the easy comparison of parsers implemented using different technologies.
Chartstates should be a particularly accurate basis for comparison in this case, becausethe overhead per chart element is essentially identical for the three parsers beingcompared.The second column in the table at the top of Figure 19 shows that in all cases,parsing with LTIG requires fewer chart states than parsing with the original CFG.Except for the Tomita I grammar, which is a toy example, the reduction is by a factorof at least 3 and typically in the range of 5-10.
This benefit is obtained without changingthe trees produced and without increasing the grammar size.
The benefit is as great,508Schabes and Waters Tree Insertion Grammar1-5 6-10 11-15 16-20 21-25Tomita I 0.43 0.60 0.69 0.76 0.86Tomita II 0.28 0.30 0.30 0.34 0.35Tomita III 0.06 0.08 0.10 0.10 0.11Tomita IV 0.11 0.14 0.15 0.15 0.17Treebank 200 0.08 0.11 0.12 0.14 0.14Treebank 500 0.08 0.11 0.13 0.16 0.16Treebank 1000 0.10 0.15 0.21 0.25 0.33Mike 0.14 0.23 0.21 0.22 0.211 2-10 11-100 101-1000 > 1000Tomita I 0.44 0.61 0.73 0.80 0.90Tomita II 0.28 0.32 0.36Tomita III 0.06 0.09 0.13Tomita IV 0.11 0.13 0.16 0.18Treebank 200 0.09 0.13 0.15Treebank 500 0.07 0.12 0.15 0.18 0.20Treebank 1000 0.08 0.13 0.17 0.22 0.30Mike 0.20Figure 20Ratio of Left LTIG to CFG chart states.
Top: versus sentence l ngth.
Bottom: versus sentenceambiguity.or greater, for large grammars like Tomita IV and Treebank 1000 as for small ones likeTomita II and Mike.The grammars generated by the Rosenkrantz and GNF procedures also yield re-ductions in the number of chart states.
However, the reduction is not as great as for theLTIG, and is only obtained at the cost of changing the trees produced and increasingthe grammar size.With the Rosenkrantz and GNF procedures, the size of the grammar can be asignificant problem in two ways.
First, the grammar can be so large that even withleft anchored parsing, an unreasonably large number of chart states is created.
InFigure 19, this happens with the GNF for Tomita II.
Second, the grammar can be toolarge to parse with at all.
Several of the entries in Figure 19 are left blank, becauseusing our experimental deduction-based parser, it was not possible for us to parsewith grammars larger than 100, 000 or so.
It is not clear whether any practical parsercould handle the grammar that the GNF procedure creates for Treebank 1000.The bottom of Figure 19 shows the average relative ambiguity of the grammarsproduced by the three procedures when applied to the test sentences.
Each numberis the average ambiguity of the sentences under the grammar in question divided bytheir ambiguity under the original CFG.
The LTIG always has the same ambiguity asthe CFG.
The other procedures often create grammars with less ambiguity.The tables in Figure 20 provide a more detailed analysis of the reduction in chartstates obtained via the LTIG procedure.
As in the top of Figure 19, the numbers areratios of the number of chart states created by the LTIG parser to the number of chartstates created by the CFG parser, for sentences with the indicated properties.The top of Figure 20 shows that the benefit obtained by using LTIG declineswith longer sentences, but continues to be significant.
The bottom of Figure 20 showsthat the benefit obtained by using LTIG also declines with higher ambiguity, but notdramatically.
The missing entries in the table stem from the fact that some of thegrammars do not generate significant numbers of highly ambiguous entences.509Computational Linguistics Volume 21, Number 47.
A Future Direct ionIn the preceding, TIG is primarily presented as an alternative to CFG.
Anotherperspective on TIG is as an alternative to TAG.
To explore the possibilities in thisregard, we investigated the extent o which the lexicalized tree adjoining grammar(LTAG) for English being developed at the University of Pennsylvania (XTAG ResearchGroup 1995) is consistent with LTIG.The current English LTAG consists of 392,001 elementary trees.
These trees are alllexicalized and contain a total of 54,777 different words.
At first glance, it might seemimpractical to parse using such an enormous grammar expressed in any formalism.However, because the elementary trees are lexicalized and there are so many terminalsymbols, only a small fraction of the elementary trees needs to be considered whenparsing any one sentence.
In particular, there are on average only 7 elementary treesfor each word.
Therefore, only on the order of 100 elementary trees need be consideredwhen parsing any one ten to twenty word sentence.In the context of this paper, the most striking aspect of the current English LTAGis that it is nearly an LTIG (see Figure 21).
In particular, the current English LTAG con-tains almost 100,000 elementary left and right auxiliary trees but only 109 elementarywrapping auxiliary trees.
Further, the vast majority of the ways the auxiliary trees canbe used are also consistent with the restrictions imposed by TIG.
The only exceptionsare the small number of situations where an elementary wrapping auxiliary tree canbe adjoined and the even smaller number of situations where an elementary left aux-iliary tree can be adjoined on the spine of an elementary ight auxiliary tree and viceversa.Figure 21 is suggestive, but it has several shortcomings.
The figure implicitly as-sumes that every elementary tree and every interaction between them is equally im-portant.
It is entirely possible that some of the non-LTIG adjunctions occur frequentlyor are linguistically essential, or both.More importantly, the figure considers only simple, unconstrained adjunction.However, the current English LTAG makes use of adjoining constraints and the propa-gation of attributes during parsing.
To capture this additional information, one wouldhave to use an extension of LTIG supporting adjoining constraints and the propaga-tion of attributes.
Due to the switch from adjoining on the roots of auxiliary trees inLTAG to multiple simultaneous adjunction in LTIG, the constraints and propagationof attributes would have to operate very differently.
Further research is needed todetermine whether equivalent operation can be obtained in all situations.Given the above, there is no reason to believe that it would be easy to convert hecurrent English LTAG entirely into an LTIG.
However, there is every reason to believethat it would be worthwhile to try.
Given that no effort was expended to date, yet thegrammar is close to an LTIG, the grammar could probably be brought much closer toan LTIG.
If complete conversion is not possible, one could consider implementing acombined parser for TIG and TAG that would apply TIG parsing to the TIG subset ofNumber Incompatible With LTIGinitial trees 294,568 0 0%auxiliary trees 97,433 109 .11%possible adjunctions 45,962,478,485 49,840,130 .11%Figure 21Most of the current LTAG for English is consistent with LTIG.510Schabes and Waters Tree Insertion Grammara TAG and full TAG parsing to the rest.
For a grammar that was mostly a TIG, sucha parser should be almost as fast as a TIG parser.8.
ConclusionA variety of lexicalization procedures for CFG have previously been developed.
How-ever, they all have significant disadvantages.
The approaches of Greibach and Rosen-krantz, which produce a CFG in Greibach normal form, are only weak lexicalizationprocedures since they do not guarantee that the same trees are produced.
In addition,these approaches often produce very large output grammars.
TAG allows strong lex-icalization that preserves the trees produced; however, because it uses an operationderiving context-sensitive languages, TAG entails larger computation costs than CFGs.Tree insertion grammar (TIG) is a restricted form of tree adjoining rammar (TAG)that is O(ng)-time parsable, generates context-free languages, and yet alows the stronglexicalization ofCFG.
The main results of this paper are an efficient Earley-style parserfor TIG and a procedure that converts any CFG into a left anchored lexicalized TIG(LTIG) that produces the same trees with the same degree of ambiguity.
By takingadvantage of the sharing between trees, these LTIGs can be represented compactly.Experiments with grammars for subsets of English show that the correspondingLTIGs are often even smaller than the original CFGs.
Most importantly, by takingadvantage of the left anchored nature of the LTIG, it is possible to avoid on the orderof 80-90% of the chart states required when parsing with the original CFG.
Given thatthe per-chart-state cost of TIG and CFG parsers are essentially identical, this shouldtranslate directly into an 80-90% decrease in parsing time.A possible future use of TIG is as an alternative for TAG.
TIG is not as powerful asTAG, but it includes a number of the features of TAG.
Further, at least in the currentEnglish LTAG, the features of TAG that are included in TIG are used more often thanthe features that are not included in TIG.
As a result, it may be possible to use TIGinstead of TAG in some situations, thereby gaining O(n 3) parsability.The uses for TIG discussed in this paper all involve starting with an existinggrammar and converting it into a TIG.
An important area for further investigationis using TIG as the original formalism for constructing grammars.
This is potentiallyvaluable because TIG allows greater derivational freedom than CFG, without anyadditional parsing cost.
For instance, one can require that the grammar be lexicalized,without placing any limits on the parse trees produced.
This could result in grammarsthat are better motivated from a linguistic standpoint, or faster to parse, or both.AcknowledgmentsWe thank John Coleman, Aravind Joshi,Esther K6nig, Fernando Pereira, StuartShieber and B. Srinivas for valuablediscussions.
We thank the anonymousreferees for a number of insightfulcomments and suggestions.
We thankMasaru Tomita for making his testgrammars available to us.ReferencesAbbott, Russell, and Kuno, Susumu.
(1965).
"The predictive analyzer and context-freegrammars."
Harvard University TechnicalReport of the Machine Translation Project.Bar-Hillel, Yoshua.
(1964).
"On categorialand phrase structure grammars."
InLanguage and Information, 99-115.
(Addison-Wesley, First appeared in theBulletin of the Research Council of Israel,vol.
9F, 1-16 (1960).
)Brill, Eric; Magerman, David; Marcus,Mitchell; and Santorini, Beatrice.
(1990).
"Deducing linguistic structure from thestatistics of large corpora."
In DARPASpeech and Natural Language Workshop.Morgan Kaufmann, Hidden Valley, PA,June.Chomsky, Noam.
(1981).
Lectures onGovernment and Binding.
Foris, Dordrecht.Earley, Jay C. (1968).
An Efficient Context-Free511Computational Linguistics Volume 21, Number 4Parsing Algorithm.
Doctoral dissertation,Carnegie-Mellon University, Pittsburgh,PA.Earley, Jay C. (1970).
"An efficientcontext-free parsing algorithm."
Commun.ACM, 13(2), 94-102.Gazdar, Gerald; Klein, Ewan H.; Pullum,Geoffrey.
K.; and Sag, Ivan.
A.
(1985).Generalized Phrase Structure Grammars.Blackwell Publishing, Oxford.
Alsopublished by Harvard University Press,Cambridge, MA.Graham, Susan L.; Harrison, Michael A.;and Ruzzo, Walter L. (1980).
"Animproved context-free r cognizer."
ACMTransactions on Programming Languages andSystems, 2(3), 415--462, July.Greibach, Sheila A.
(1965).
"A newnormal-form theorem for context-freephrase-structure grammars."
J. ACM, 12,42-52.Gross, Maurice' (1984).
"Lexicon-grammarand the syntactic analysis of French."
InProceedings, 10 th International Conference onComputational Linguistics (COLING'84),Stanford.Harrison, Michael A.
(1978).
Introduction toFormal Language Theory.
Addison-Wesley,Reading, MA.Joshi, Aravind K. (1985).
"How muchcontext-sensitivity is necessary forcharacterizing structuraldescriptions--Tree Adjoining Grammars.
"In D. Dowty, L. Karttunen, andA.
Zwicky, editors, Natural LanguageProcessing--Theoretical, Computational andPsychological Perspectives.
CambridgeUniversity Press, New York.Joshi, Aravind K., and Schabes, Yves (1992).
"Tree-adjoining grammars and lexicalizedgrammars."
In Maurice Nivat andAndreas Podelski, editors, Tree Automataand Languages.
Elsevier Science.Kaplan, Ronald, and Bresnan, Joan (1983).
"Lexical-functional grammar: A formalsystem for grammatical representation.
"In J. Bresnan, editor, The MentalRepresentation of Grammatical Relations.
MITPress, Cambridge MA.Karttunen, Lauri (1986).
"Radicallexicalism."
Technical Report CSLI-86-68,CSLI, Stanford University, 1986.
Also inAlternative Conceptions ofPhrase Structure,University of Chicago Press, Baltin, M.and Kroch A., Chicago.Lang, Bernard (1990).
"The systematicconstructions of Earley parsers:Application to the production of O(n 6)Earley parsers for Tree AdjoiningGrammars."
In Proceedings, 1stInternational Workshop on Tree AdjoiningGrammars, Dagstuhl Castle, FRG.Pollard, Carl, and Sag, Ivan A.
(1987).Information-Based Syntax and Semantics.Vol.
1: Fundamentals.
CSLI.Rich, Charles; Waters, Richard C.; Schabes,Yves; Freeman, William T.; Torrance,Mark C.; Golding, Andrew R.; and Roth,Michal (1994a).
"An animated on-linecommunity with artificial agents."
IEEEMultimedia, 1(4), 32-42, Winter 1994.Rich, Charles; Waters, Richard C.;Strohecker, Carol; Schabes, Yves; Freeman,William T.; Torrance, Mark C.; Golding,Andrew R.; and Roth, Michal (1994b).
"Demonstration f an interactivemultimedia environment."
IEEE Computer,27(12), 15-22, December 1994.Rosenkrantz, Daniel J.
(1967).
"Matrixequations and normal forms forcontext-free grammars."
Journal of theAssociation for Computing Machinery, 14(3),501-507.Schabes, Yves (1990).
Mathematical andComputational Aspects of LexicalizedGrammars.
Doctoral dissertation,University of Pennsylvania, Philadelphia,PA.
Available as technical report(MS-CIS-90-48, LINC LAB179) from theDepartment of Computer Science.Schabes, Yves (1991).
"The valid prefixproperty and left to right parsing oftree-adjoining grammar."
In Proceedings,Second International Workshop on ParsingTechnologies, 21-30, Cancun, Mexico.Schabes, Yves; Abeill6, Anne; and Joshi,Aravind K. (1988).
"Parsing strategieswith 'lexicalized' grammars: Applicationto tree adjoining rammars."
InProceedings, 12 th International Conference onComputational Linguistics (COLING'88),Budapest, Hungary.Schabes, Yves, and Waters, Richard C.(1993a).
"Lexicalized context-freegrammars."
In 21 st Meeting of theAssociation for Computational Linguistics(ACL'93), 121-129, Columbus, OH, June.Schabes, Yves, and Waters, Richard C.(1993b).
"Stochastic lexicalizedcontext-free grammars."
In Proceedings,Third International Workshop on ParsingTechnologies, 257-266, Tilburg (theNetherlans) and Durbuy (Belgium),August.Schabes, Yves, and Waters, Richard C.(1993c).
"Lexicalized context-freegrammar: A cubic-time parsableformalism that strongly lexicalizescontext-free grammar."
Technical Report93-04, Mitsubishi Electric ResearchLaboratories, 201 Broadway.
Cambridge,512Schabes and Waters Tree Insertion GrammarMA 02139.Schabes, Yves, and Shieber, Stuart (1994).
"An alternative conception oftree-adjoining derivation."
ComputationalLinguistics, 20(1), 91-124, March.Shieber, Stuart M.; Schabes, Yves; andPereira, Fernando C.N.
(1995).
"Principlesand implementation f deductiveparsing."
Journal of Logic and Computation,24 (1&2), 3-36.Steedman, Mark (1987).
"Combinatorygrammars and parasitic gaps."
NaturalLanguage and Linguistic Theory, 5, 403-439.Thatcher, James W. (1971).
"Characterizingderivations trees of context free grammarsthrough a generalization f finite automatatheory."
Journal of Computer and SystemSciences, 5, 365-396.Tomita, Masaru (1985).
Efficient Parsing forNatural Language, A Fast Algorithm forPractical Systems.
Kluwer AcademicPublishers.Vijay-Shanker, K. (1987).
A Study of TreeAdjoining Grammars.
Doctoral dissertation,Department of Computer and InformationScience, University of Pennsylvania.Vijay-Shanker, K., and Joshi, Aravind K.(1985).
"Some computational properties ofTree Adjoining Grammars."
In 23 rdMeeting of the Association for ComputationalLinguistics, 82-93, Chicago, Illinois.Vijay-Shanker, K., and Weir, David (1993).
"Parsing some constrained grammarformalisms."
Computational Linguistics,19(4), 591-636.Vijay-Shanker, K.; Weir, David J.; and Joshi,Aravind K. (1986).
"Tree adjoining andhead wrapping."
In Proceedings, 11thInternational Conference on ComputationalLinguistics (COLING'86).Weir, David J.
(1988).
Characterizing MildlyContext-Sensitive Grammar Formalisms.Doctoral dissertation, Department ofComputer and Information Science,University of Pennsylvania.XTAG Research Group (1995).
"ALexicalized Tree Adjoining Grammar forEnglish."
IRCS technical report 95-03.Institute for Research in CognitiveScience.
University of Pennsylvania,Philadelphia, PA 19104.Appendix A: No wrapping trees can be built in TIGIn this appendix, we give a proof that given a TIG (G, NT, L A, S), it is not possible tocreate wrapping auxiliary trees.ProofThe only elementary trees allowed are left auxiliary trees, right auxiliary trees andinitial trees.
A case-by-case analysis reveals that every possible combination of thesekinds of trees yields a new tree in one of the three categories.
Therefore, no derivationcan ever create a wrapping auxiliary tree.Substitution of an initial tree in an initial tree yields an initial tree.Adjunction of a left or right auxiliary tree in an initial tree yields an initial tree.Substitution of an initial tree in a left (right) auxiliary tree yields a left (right)auxiliary tree, because by definition the node marked for substitution must be left(right) of the foot and therefore all the new frontier nodes must be added left (right)of the foot.Adjunction of a left (right) auxiliary tree S in a right (left) auxiliary tree T yields aright (left) auxiliary tree, because by definition the node adjoined upon must be to theright (left) of the spine of T and therefore all the new frontier nodes must be addedright (left) of the foot of T.Adjunction of a left (right) auxiliary tree S in a left (right) auxiliary tree T yieldsa left (right) auxiliary tree, for the same basic reason as above except hat the nodeadjoined upon can be onthe  spine of T. However, since all the nonempty structurein S is left (right) of the spine of S, even in this case, all the new nonempty frontiernodes are added to the left (right) of the foot of T. \[\]513
