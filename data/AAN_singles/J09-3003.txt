Recognizing Contextual Polarity:An Exploration of Features for Phrase-LevelSentiment AnalysisTheresa Wilson?University of EdinburghJanyce Wiebe?
?University of PittsburghPaul Hoffmann?
?University of PittsburghMany approaches to automatic sentiment analysis begin with a large lexicon of words markedwith their prior polarity (also called semantic orientation).
However, the contextual polarity ofthe phrase in which a particular instance of a word appears may be quite different from theword?s prior polarity.
Positive words are used in phrases expressing negative sentiments, orvice versa.
Also, quite often words that are positive or negative out of context are neutral incontext, meaning they are not even being used to express a sentiment.
The goal of this work is toautomatically distinguish between prior and contextual polarity, with a focus on understandingwhich features are important for this task.
Because an important aspect of the problem isidentifying when polar terms are being used in neutral contexts, features for distinguishingbetween neutral and polar instances are evaluated, as well as features for distinguishing betweenpositive and negative contextual polarity.
The evaluation includes assessing the performanceof features across multiple machine learning algorithms.
For all learning algorithms exceptone, the combination of all features together gives the best performance.
Another facet of theevaluation considers how the presence of neutral instances affects the performance of features fordistinguishing between positive and negative polarity.
These experiments show that the presenceof neutral instances greatly degrades the performance of these features, and that perhaps thebest way to improve performance across all polarity classes is to improve the system?s ability toidentify when an instance is neutral.1.
IntroductionSentiment analysis is a type of subjectivity analysis (Wiebe 1994) that focuses on iden-tifying positive and negative opinions, emotions, and evaluations expressed in naturallanguage.
It has been a central component in applications ranging from recognizing?
School of Informatics, Edinburgh EH8 9LW, U.K. E-mail: twilson@inf.ed.ac.uk.??
Department of Computer Science, Pittsburgh, PA 15260, USA.
E-mail: {wiebe,hoffmanp}@cs.pitt.edu.Submission received: 14 November 2006; revised submission received: 8March 2008; accepted for publication:16 April 2008.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 3inflammatory messages (Spertus 1997), to tracking sentiments over time in onlinediscussions (Tong 2001), to classifying positive and negative reviews (Pang, Lee, andVaithyanathan 2002; Turney 2002).
Although a great deal of work in sentiment analy-sis has targeted documents, applications such as opinion question answering (Yu andHatzivassiloglou 2003; Maybury 2004; Stoyanov, Cardie, and Wiebe 2005) and re-view mining to extract opinions about companies and products (Morinaga et al 2002;Nasukawa and Yi 2003) require sentence-level or even phrase-level analysis.
For exam-ple, if a question answering system is to successfully answer questions about people?sopinions, it must be able not only to pinpoint expressions of positive and negativesentiments, such as we find in sentence (1), but also to determine when an opinion is notbeing expressed by a word or phrase that typically does evoke one, such as condemnedin sentence (2).
(1) African observers generally approved (positive) of his victory whileWestern governments denounced (negative) it.
(2) Gavin Elementary School was condemned in April 2004.A common approach to sentiment analysis is to use a lexicon with informationabout which words and phrases are positive and which are negative.
This lexicon maybe manually compiled, as is the case with the General Inquirer (Stone et al 1966), aresource often used in sentiment analysis.
Alternatively, the information in the lexiconmay be acquired automatically.
Acquiring the polarity of words and phrases is itselfan active line of research in the sentiment analysis community, pioneered by thework of Hatzivassiloglou and McKeown (1997) on predicting the polarity or semanticorientation of adjectives.
Various techniques have been proposed for learning thepolarity of words.
They include corpus-based techniques, such as using constraintson the co-occurrence in conjunctions of words with similar or opposite polarity(Hatzivassiloglou and McKeown 1997) and statistical measures of word association(Turney and Littman 2003), as well as techniques that exploit information about lexicalrelationships (Kamps and Marx 2002; Kim and Hovy 2004) and glosses (Esuli andSebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.Acquiring the polarity of words and phrases is undeniably important, and thereare still open research challenges, such as addressing the sentiments of different sensesof words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006), and so on.
However,what the polarity of a given word or phrase is when it is used in a particular context isanother problem entirely.
Consider, for example, the underlined positive and negativewords in the following sentence.
(3) Philip Clapp, president of the National Environment Trust, sums up wellthe general thrust of the reaction of environmental movements: ?There isno reason at all to believe that the polluters are suddenly going to becomereasonable.
?The first underlined word is Trust.
Although many senses of the word trust express apositive sentiment, in this case, the word is not being used to express a sentiment atall.
It is simply part of an expression referring to an organization that has taken onthe charge of caring for the environment.
The adjective well is considered positive, andindeed it is positive in this context.
However, the same is not true for the words reason400Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarityand reasonable.
Out of context, we would consider both of these words to be positive.1In context, the word reason is being negated, changing its polarity from positive tonegative.
The phrase no reason at all to believe changes the polarity of the proposition thatfollows; because reasonable falls within this proposition, its polarity becomes negative.The word polluters has a negative connotation, but here in the context of the discussionof the article and its position in the sentence, polluters is being used less to express asentiment and more to objectively refer to companies that pollute.
To clarify how thepolarity of polluters is affected by its subject role, consider the purely negative sentimentthat emerges when it is used as an object: They are polluters.We call the polarity that would be listed for a word in a lexicon the word?s priorpolarity, and we call the polarity of the expression in which a word appears, con-sidering the context of the sentence and document, the word?s contextual polarity.Although words often do have the same prior and contextual polarity, many timesa word?s prior and contextual polarities differ.
Words with a positive prior polaritymay have a negative contextual polarity, or vice versa.
Quite often words that arepositive or negative out of context are neutral in context, meaning that they are noteven being used to express a sentiment.
Similarly, words that are neutral out of context,neither positive or negative, may combine to create a positive or negative expression incontext.The focus of this work is on the recognition of contextual polarity?in particular,disambiguating the contextual polarity of words with positive or negative prior polar-ity.
We begin by presenting an annotation scheme for marking sentiment expressionsand their contextual polarity in the Multi-perspective Question Answering (MPQA)opinion corpus.
We show that, given a set of subjective expressions (identified fromthe existing annotations in the MPQA corpus), contextual polarity can be annotatedreliably.Using the contextual polarity annotations, we conduct experiments in automaticallydistinguishing between prior and contextual polarity.
Beginning with a large lexicon ofclues tagged with prior polarity, we identify the contextual polarity of the instancesof those clues in the corpus.
The process that we use has two steps, first classifyingeach clue as being in a neutral or polar phrase, and then disambiguating the contextualpolarity of the clues marked as polar.
For each step in the process, we experiment with avariety of features and evaluate the performance of the features using several differentmachine learning algorithms.Our experiments reveal a number of interesting findings.
First, being able to accu-rately identify neutral contextual polarity?when a positive or negative clue is not beingused to express a sentiment?is an important aspect of the problem.
The importance ofneutral examples has previously been noted for classifying the sentiment of documents(Koppel and Schler 2006), but ours is the first work to explore how neutral instancesaffect classifying the contextual polarity of words and phrases.
In particular, we foundthat the performance of features for distinguishing between positive and negative po-larity greatly degrades when neutral instances are included in the experiments.We also found that achieving the best performance for recognizing contextual po-larity requires a wide variety of features.
This is particularly true for distinguishing1 It is open to question whether reason should be listed as positive in a sentiment lexicon, because the morefrequent senses of reason involve intention, not sentiment.
However, any existing sentiment lexicon onewould start with will have some noise and errors.
The task in this article is to disambiguate instances ofthe entries in a given sentiment lexicon.401Computational Linguistics Volume 35, Number 3between neutral and polar instances.
Although some features help to increase polar orneutral recall or precision, it is only the combination of features together that achievessignificant improvements in accuracy over the baselines.
Our experiments show that fordistinguishing between positive and negative instances, features capturing negation areclearly the most important.
However, there is more to the story than simple negation.Features that capture relationships between instances of clues also perform well, indi-cating that identifying features that represent more complex interdependencies betweensentiment clues may be an important avenue for future research.The remainder of this article is organized as follows.
Section 2 gives an overviewof some of the things that can influence contextual polarity.
In Section 3, we describeour corpus and present our annotation scheme and inter-annotator agreement studyfor marking contextual polarity.
Sections 4 and 5 describe the lexicon used in ourexperiments and how the contextual polarity annotations are used to determine thegold-standard tags for instances from the lexicon.
In Section 6, we consider what kind ofperformance can be expected from a simple, prior-polarity classifier.
Section 7 describesthe features that we use for recognizing contextual polarity, and our experimentsand results are presented in Section 8.
In Section 9 we discuss related work, and weconclude in Section 10.2.
Polarity InfluencersPhrase-level sentiment analysis is not a simple problem.
Many things besides negationcan influence contextual polarity, and even negation is not always straightforward.Negation may be local (e.g., not good), or involve longer-distance dependencies such asthe negation of the proposition (e.g., does not look very good) or the negation of the subject(e.g., no one thinks that it?s good).
In addition, certain phrases that contain negation wordsintensify rather than change polarity (e.g., not only good but amazing).
Contextual polaritymay also be influenced bymodality: whether the proposition is asserted to be real (realis)or not real (irrealis) (no reason at all to believe is irrealis, for example); word sense (e.g.,Environmental Trust vs.
He has won the people?s trust); the syntactic role of a word in thesentence: whether the word is the subject or object of a copular verb (consider pollutersare versus they are polluters); and diminishers such as little (e.g., little truth, little threat).Polanyi and Zaenen (2004) give a detailed discussion of many of these types of polarityinfluencers.
Many of these contextual polarity influencers are represented as features inour experiments.Contextual polarity may also be influenced by the domain or topic.
For example,the word cool is positive if used to describe a car, but it is negative if it is used todescribe someone?s demeanor.
Similarly, a word such as fever is unlikely to be expressinga sentiment when used in a medical context.
We use one feature in our experiments torepresent the topic of the document.Another important aspect of contextual polarity is the perspective of the personwho is expressing the sentiment.
For example, consider the phrase failed to defeatin the sentence Israel failed to defeat Hezbollah.
From the perspective of Israel, failedto defeat is negative.
From the perspective of Hezbollah, failed to defeat is positive.Therefore, the contextual polarity of this phrase ultimately depends on the perspec-tive of who is expressing the sentiment.
Although automatically detecting this kindof pragmatic influence on polarity is beyond the scope of this work, this as well asthe other types of polarity influencers all are considered when annotating contextualpolarity.402Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity3.
Data and AnnotationsFor the experiments in this work, we need a corpus that is annotated comprehensivelyfor sentiment expressions and their contextual polarity.
Rather than building a corpusfrom scratch, we chose to add contextual polarity annotations to the existing annota-tions in the Multi-perspective Question Answering (MPQA) opinion corpus2 (Wiebe,Wilson, and Cardie 2005).The MPQA corpus is a collection of English-language versions of news documentsfrom the world press.
The documents contain detailed, expression-level annotationsof attributions and private states (Quirk et al 1985).
Private states are mental andemotional states; they include beliefs, speculations, intentions, and sentiments, amongothers.
Although sentiments are not distinguished from other types of private statesin the existing annotations, they are a subset of what already is annotated.
This makesthe annotations in the MPQA corpus a good starting point for annotating sentimentexpressions and their contextual polarity.3.1 Annotation SchemeWhen developing our annotation scheme for sentiment expressions and contextualpolarity, there were three main questions to address.
First, which of the existing annota-tions in the MPQA corpus have the possibility of being sentiment expressions?
Second,which of the possible sentiment expressions actually are expressing sentiments?
Third,what coding scheme should be used for marking contextual polarity?TheMPQA annotation scheme has four types of annotations: objective speech eventframes, two types of private state frames, and agent frames that are used for markingspeakers of speech events and experiencers of private states.
A full description ofthe MPQA annotation scheme and an agreement study evaluating key aspects of thescheme are found in Wiebe, Wilson, and Cardie (2005).The two types of private state frames, direct subjective frames and expressive sub-jective element frames, are where we will find sentiment expressions.
Direct subjectiveframes are used to mark direct references to private states as well as speech events inwhich private states are being expressed.
For example, in the following sentences, fears,praised, and said are all marked as direct subjective annotations.
(4) The U.S. fears a spill-over of the anti-terrorist campaign.
(5) Italian senator Renzo Gubert praised the Chinese government?s efforts.
(6) ?The report is full of absurdities,?
he said.The word fears directly refers to a private state; praised refers to a speech eventin which a private state is being expressed; and said is marked as direct subjectivebecause a private state is being expressed within the speech event referred to bysaid.
Expressive subjective elements indirectly express private states through the waysomething is described or through a particular wording.
In example (6), the phrasefull of absurdities is an expressive subjective element.
Subjectivity (Banfield 1982; Wiebe2 Available at http://nrrc.mitre.org/NRRC/publications.htm.403Computational Linguistics Volume 35, Number 31994) refers to the linguistic expression of private states, hence the names for the twotypes of private state annotations.All expressive subjective elements are included in the set of annotations that havethe possibility of being sentiment expressions, but the direct subjective frames to includein this set can be pared down further.
Direct subjective frames have an attribute, expres-sion intensity, that captures the contribution of the annotated word or phrase to theoverall intensity of the private state being expressed.
Expression intensity ranges fromneutral to high.
In the given sentences, fears and praised have an expression intensity ofmedium, and said has an expression intensity of neutral.
A neutral expression intensityindicates that the direct subjective phrase itself is not contributing to the expressionof the private state.
If this is the case, then the direct subjective phrase cannot bea sentiment expression.
Thus, only direct subjective annotations with a non-neutralexpression intensity are included in the set of annotations that have the possibility ofbeing sentiment expressions.
We call this set of annotations, the union of the expres-sive subjective elements and the direct subjective frames with a non-neutral intensity,the subjective expressions in the corpus; these are the annotations we will mark forcontextual polarity.Table 1 gives a sample of subjective expressions marked in the MPQA corpus.Although many of the words and phrases express what we typically think of assentiments, others do not, for example, believes, very definitely, and unconditionally andwithout delay.Now that we have identified which annotations have the possibility of being sen-timent expressions, the next question is which of these annotated words and phrasesare actually expressing sentiments.
We define a sentiment as a positive or negativeemotion, evaluation, or stance.
On the left of Table 2 are examples of positive sentiments;examples of negative sentiments are on the right.Table 1Sample of subjective expressions from the MPQA corpus.victory of justice and freedom such a disadvantageous situationgrown tremendously mustsuch animosity not true at allthrottling the voice imperative for harmonious societydisdain and wrath gloriousso exciting disastrous consequencescould not have wished for a better situation believesfreak show the embodiment of two-sided justiceif you?re not with us, you?re against us appallingvehemently denied very definitelyeverything good and nice once and for allunder no circumstances shameful mummost fraudulent, terrorist and extremist enthusiastically askednumber one democracy hateseems to think gross misstatementindulging in blood-shed and their lunaticism surprised, to put it mildlytake justice to pre-historic times unconditionally and without delayso conservative that it makes Pat Buchanan look vegetarianthose digging graves for others, get engraved themselveslost the reputation of commitment to principles of human justiceultimately the demon they have reared will eat up their own vitals404Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 2Examples of positive and negative sentiments.Positive sentiments Negative sentimentsEmotion I?m happy I?m sadEvaluation Great idea!
Bad idea!Stance She supports the bill She?s against the billThe final issue to address is the actual annotation scheme for marking contextualpolarity.
The scheme we developed has four tags: positive, negative, both, and neutral.The positive tag is used to mark positive sentiments.
The negative tag is used to marknegative sentiments.
The both tag is applied to expressions in which both a positive andnegative sentiment are being expressed.
Subjective expressions with positive, negative, orboth tags are our sentiment expressions.
The neutral tag is used for all other subjectiveexpressions, including emotions, evaluations, and stances that are neither positive ornegative.
Instructions for the contextual-polarity annotation scheme are available athttp://www.cs.pitt.edu/mpqa/databaserelease/polarityCodingInstructions.txt.Following are examples from the corpus of each of the different contextual-polarityannotations.
Each underlinedword or phrase is a subjective expression that wasmarkedin the original MPQA annotations.3 In bold following each subjective expression is thecontextual polarity with which it was annotated.
(7) Thousands of coup supporters celebrated (positive) overnight, wavingflags, blowing whistles .
.
.
(8) The criteria set by Rice are the following: the three countries in question arerepressive (negative) and grave human rights violators (negative) .
.
.
(9) Besides, politicians refer to good and evil (both) only for purposes ofintimidation and exaggeration.
(10) Jerome says the hospital feels (neutral) no different than a hospital in thestates.As a final note on the annotation scheme, annotators are asked to judge the con-textual polarity of the sentiment that is ultimately being conveyed by the subjectiveexpression, that is, once the sentence has been fully interpreted.
Thus, the subjectiveexpression, they have not succeeded, and will never succeed, is marked as positive in thefollowing sentence:(11) They have not succeeded, and will never succeed (positive), in breakingthe will of this valiant people.The reasoning is that breaking the will of a valiant people is negative, so to not succeedin breaking their will is positive.3 Some sentences contain additional subjective expressions that are not underlined as examples.405Computational Linguistics Volume 35, Number 3Table 3Contingency table for contextual polarity agreement.Neutral Positive Negative Both TotalNeutral 123 14 24 0 161Positive 16 73 5 2 96Negative 14 2 167 1 184Both 0 3 0 3 6Total 153 92 196 6 447Table 4Contingency table for contextual polarity agreement, borderline cases removed.Neutral Positive Negative Both TotalNeutral 113 7 8 0 128Positive 9 59 3 0 71Negative 5 2 156 1 164Both 0 2 0 2 4Total 127 70 167 3 3673.2 Agreement StudyTo measure the reliability of the polarity annotation scheme, we conducted an agree-ment study with two annotators4 using 10 documents from the MPQA corpus.
The 10documents contain 447 subjective expressions.
Table 3 shows the contingency table forthe two annotators?
judgments.
Overall agreement is 82%, with a kappa value of 0.72.As part of the annotation scheme, annotators are asked to judge how certain theyare in their polarity tags.
For 18% of the subjective expressions, at least one annotatorused the uncertain tag whenmarking polarity.
If we consider these cases to be borderlineand exclude them from the study, percent agreement increases to 90% and kappa rises to0.84.
Table 4 shows the revised contingency table with the uncertain cases removed.
Thisshows that annotator agreement is especially highwhen both annotators are certain, andthat annotators are certain for over 80% of their tags.Note that all annotations are included in the experiments.3.3 Contextual Polarity AnnotationsIn total, all 19,962 subjective expressions in the 535 documents (11,112 sentences) of theMPQA corpus were annotated with their contextual polarity as just described.5 Threeannotators carried out the task: the two who participated in the annotation study anda third who was trained later.6 Table 5 gives the distribution of the contextual polaritytags.
Looking at this table, we see that a small majority of subjective expressions (54.6%)4 Both annotators are authors of this article.5 The revised version of the MPQA corpus with the contextual polarity annotations is available athttp://www.cs.pitt.edu/mpqa.6 The third annotator received training until her reliability of performance on the task was comparable tothat of the first two annotators who participated in the study.406Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 5Distribution of contextual polarity tags.Neutral Positive Negative Both Total9,057 3,311 7,294 299 19,96145.4% 16.6% 36.5% 1.5% 100%are expressing a positive, negative, or both (positive and negative) sentiment.
We refer tothese expressions as polar in context.
Many of the subjective expressions are neutraland do not express a sentiment.
This suggests that, although sentiment is a major typeof subjectivity, distinguishing other prominent types of subjectivity will be importantfor future work in subjectivity analysis.As many NLP applications operate at the sentence level, one important issue toconsider is the distribution of sentences with respect to the subjective expressionsthey contain.
In the 11,112 sentences in the MPQA corpus, 28% contain no subjectiveexpressions, 24% contain only one, and 48% contain two or more.
Of the 5,304 sentencescontaining two or more subjective expressions, 17% contain mixtures of positive andnegative expressions, and 61% contain mixtures of polar (positive/negative/both) andneutral subjective expressions.4.
Prior-Polarity Subjectivity LexiconFor the experiments in this article, we use a lexicon of over 8,000 subjectivity clues.Subjectivity clues are words and phrases that may be used to express private states.
Inother words, subjectivity clues have subjective usages, though they may have objectiveusages as well.
For this work, only single-word clues are used.To compile the lexicon, we began with the list of subjectivity clues from Riloff andWiebe (2003), which includes the positive and negative adjectives fromHatzivassiloglouand McKeown (1997).
The words in this list were grouped in previous work accordingto their reliability as subjectivity clues.
Words that are subjective in most contexts areconsidered strong subjective clues, indicated by the strongsubj tag.
Words that mayonly have certain subjective usages are considered weak subjective clues, indicated bythe weaksubj tag.We expanded the list using a dictionary and a thesaurus, and added words from theGeneral Inquirer positive and negative word lists (Stone et al 1966) that we judged to bepotentially subjective.7 We also gave the new words strongsubj and weaksubj reliabilitytags.
The final lexicon has a coverage of 67% of subjective expressions in the MPQAcorpus, where coverage is the percentage of subjective expressions containing one ormore instances of clues from the lexicon.
The coverage of just sentiment expressions iseven higher: 75%.The next step was to tag the clues in the lexicon with their prior polarity: positive,negative, both, or neutral.
A word in the lexicon is tagged as positive if out of contextit seems to evoke something positive, and negative if it seems to evoke somethingnegative.
If a word has both positive and negative meanings, it is tagged with thepolarity that seems the most common.
A word is tagged as both if it is at the same time7 In the end, about 70% of the words from the General Inquirer positive word list and 80% of the wordsfrom the negative word list were included in the subjectivity lexicon.407Computational Linguistics Volume 35, Number 3both positive and negative.
For example, the word bittersweet evokes something bothpositive and negative.
Words like brag are also tagged as both, because the one who isbragging is expressing something positive, yet at the same time describing someone asbragging is expressing a negative evaluation of that person.
A word is tagged as neutralif it does not evoke anything positive or negative.For words that came from positive and negative word lists (Stone et al 1966;Hatzivassiloglou and McKeown 1997), we largely retained their original polarity.However, we did change the polarity of a word if we strongly disagreed with itsoriginal class.8 For example, the word apocalypse is listed as positive in the GeneralInquirer; we changed its prior polarity to negative for our lexicon.By far, the majority of clues in the lexicon (92.8%) are marked as having eitherpositive (33.1%) or negative (59.7%) prior polarity.
Only a small number of clues (0.3%)are marked as having both positive and negative polarity.
We refer to the set of cluesmarked as positive, negative, or both as sentiment clues.
A total of 6.9% of the clues inthe lexicon are marked as neutral.
Examples of neutral clues are verbs such as feel, look,and think, and intensifiers such as deeply, entirely, and practically.
Although the neutralclues make up a small proportion of the total words in the lexicon, we retain them forour later experiments in recognizing contextual polarity because many of them are goodclues that a sentiment is being expressed (e.g., feels slighted, feels satisfied, look kindly on,look forward to).
Including them increases the coverage of the system.At the end of the previous section, we considered the distribution of sentences in theMPQA corpus with respect to the subjective expressions they contain.
It is interestingto compare that distribution with the distribution of sentences with respect to theinstances they contain of clues from the lexicon.
We find that there are more sentenceswith two or more clue instances (62%) than sentences with two or more subjectiveexpressions (48%).
More importantly, many more sentences have mixtures of positiveand negative clue instances than actually have mixtures of positive and negative sub-jective expressions.
Only 880 sentences have a mixture of both positive and negativesubjective expressions, whereas 3,234 sentences have a mixture of positive and negativeclue instances.
Thus, a large number of positive and negative instances are either neutralin context, or they are combining to form more complex polarity expressions.
Eitherway, this provides strong evidence of the need to be able to disambiguate the contextualpolarity of subjectivity and sentiment clues.5.
Definition of the Gold StandardIn the experiments described in the following sections, the goal is to classify the con-textual polarity of the expressions that contain instances of the subjectivity clues in ourlexicon.
However, determining which clue instances are part of the same expression andidentifying expression boundaries are not the focus of this work.
Thus, instead of tryingto identify and label each expression, in the following experiments, each clue instanceis labeled individually as to its contextual polarity.We define the gold-standard contextual polarity of a clue instance in terms of themanual annotations (Section 3) as follows.
If a clue instance is not in a subjectiveexpression (and therefore not in a sentiment expression), its gold class is neutral.
Ifa clue instance appears in just one subjective expression or in multiple subjective8 We decided on a different polarity for about 80 of the words in our lexicon that appeared on otherpositive and negative word lists.408Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarityexpressions with the same contextual polarity, its gold class is the contextual polarityof the subjective expression(s).
If a clue instance appears in a mixture of negative andneutral subjective expressions, its gold class is negative; if it is in amixture of positive andneutral subjective expressions, its gold class is positive.
Finally, if a clue instance appearsin at least one positive and one negative subjective expression (or in a subjective ex-pression marked as both), then its gold class is both.
A clue instance can appear in morethan one subjective expression because in the MPQA annotation scheme, it is possiblefor direct subjective frames and expressive subjective elements frames to overlap.6.
A Prior-Polarity ClassifierBefore delving into the task of recognizing contextual polarity, an important questionto address is how useful prior polarity alone is for identifying contextual polarity.
Toanswer this question, we create a classifier that simply assumes the contextual polarityof a clue instance is the same as the clue?s prior polarity.
We explore this classifier?sperformance on a small amount of development data, which is not part of the data usedin the subsequent experiments.This simple classifier has an accuracy of 48%.
From the confusion matrix given inTable 6, we see that 76% of the errors result from words with non-neutral prior polarityappearing in phrases with neutral contextual polarity.
Only 12% of the errors result fromwords with neutral prior polarity appearing in expressions with non-neutral contextualpolarity, and only 11% of the errors come from words with a positive or negative priorpolarity appearing in expressions with the opposite contextual polarity.
Table 6 alsoshows that positive clues tend to be used in negative expressions far more often thannegative clues tend to be used in positive expressions.Given that by far the largest number of errors come from clues with positive,negative, or both prior polarity appearing in neutral contexts, we were motivated to trya two-step approach to the problem of sentiment classification.
The first step, Neutral?Polar Classification, tries to determine if an instance is neutral or polar in context.
Thesecond step, Polarity Classification, takes all instances that step one classified as polar,and tries to disambiguate their contextual polarity.
This two-step approach is illustratedin Figure 1.7.
FeaturesThe features used in our experiments were motivated both by the literature and byexploration of the contextual-polarity annotations in our development data.
A numberTable 6Confusion matrix for the prior-polarity classifier on the development set.Prior-Polarity ClassifierNeutral Positive Negative Both TotalNeutral 798 784 698 4 2284Gold Positive 81 371 40 0 492Class Negative 149 181 622 0 952Both 4 11 13 5 33Total 1032 1347 1373 9 3761409Computational Linguistics Volume 35, Number 3Figure 1Two-step approach to recognizing contextual polarity.of features were inspired by the paper on contextual-polarity influencers by Polanyiand Zaenan (2004).
Other features are those that have been found useful in the pastfor recognizing subjective sentences (Wiebe, Bruce, and O?Hara 1999; Wiebe and Riloff2005).7.1 Features for Neutral?Polar ClassificationFor distinguishing between neutral and polar instances, we use the features listed inTable 7.
For ease of description, we group the features into six sets: word features, gen-eral modification features, polarity modification features, structure features, sentencefeatures, and one document feature.Word Features In addition to the word token (the token of the clue instance beingclassified), the word features include the parts of speech of the previous word, the worditself, and the next word.
The prior polarity and reliability class features represent thosepieces of information about the clue which are taken from the lexicon.General Modification Features These are binary features that capture differenttypes of relationships involving the clue instance.The first four features involve relationships with the word immediately before or af-ter the clue instance.
The preceded by adjective feature is true if the clue instance is a nounpreceded by an adjective.
The preceded by adverb feature is true if the preceding wordis an adverb other than not.
The preceded by intensifier feature is true if the precedingword is an intensifier, and the self intensifier feature is true if the clue instance itself is anintensifier.
A word is considered to be an intensifier if it appears in a list of intensifiersand if it precedes a word of the appropriate part of speech (e.g., an intensifier adjectivemust come before a noun).
The list of intensifiers is a compilation of those listed in Quirket al (1985), intensifiers identified from existing entries in the subjectivity lexicon, andintensifiers identified during explorations of the development data.The modifies/modifed by features involve the dependency parse tree of the sentence,obtained by first parsing the sentence (Collins 1997) and then converting the tree intoits dependency representation (Xia and Palmer 2001).
In a dependency representation,every node in the tree structure is a surface word (i.e., there are no abstract nodes suchas NP or VP).
The parent word is called the head, and its children are its modifiers.
The410Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 7Features for neutral?polar classification.Word Featuresword tokenword part of speechprevious word part of speechnext word part of speechprior polarity: positive, negative, both, neutralreliability class: strongsubj or weaksubjGeneral Modification Featurespreceded by adjective: binarypreceded by adverb (other than not): binarypreceded by intensifier: binaryself intensifier: binarymodifies strongsubj: binarymodifies weaksubj: binarymodified by strongsubj: binarymodified by weaksubj: binaryPolarity Modification Featuresmodifies polarity: positive, negative, neutral, both, notmodmodified by polarity: positive, negative, neutral, both, notmodconjunction polarity: positive, negative, neutral, both, notmodStructure Featuresin subject: binaryin copular: binaryin passive: binarySentence Featuresstrongsubj clues in current sentence: 0, 1, 2, 3 (or more)strongsubj clues in previous sentence: 0, 1, 2, 3 (or more)strongsubj clues in next sentence: 0, 1, 2, 3 (or more)weaksubj clues in current sentence: 0, 1, 2, 3 (or more)weaksubj clues in previous sentence: 0, 1, 2, 3 (or more)weaksubj clues in next sentence: 0, 1, 2, 3 (or more)adjectives in sentence: 0, 1, 2, 3 (or more)adverbs in sentence (other than not): 0, 1, 2, 3 (or more)cardinal number in sentence: binarypronoun in sentence: binarymodal in sentence (other than will): binaryDocument Featuredocument topic/domainedge between a parent and a child specifies the grammatical relationship between thetwo words.
Figure 2 shows an example of a dependency parse tree.
Instances of clues inthe tree are marked with the clue?s prior polarity and reliability class from the lexicon.For each clue instance, the modifies/modifed by features capture whether there areadj, mod, or vmod relationships between the clue instance and any other instances fromthe lexicon.
Specifically, the modifies strongsubj feature is true if the clue instance andits parent share an adj, mod, or vmod relationship, and if its parent is an instance ofa strongsubj clue from the lexicon.
The modifies weaksubj feature is the same, exceptthat it looks in the parent for an instance of a weaksubj clue.
The modified by strongsubj411Computational Linguistics Volume 35, Number 3Figure 2The dependency tree for the sentence The human rights report poses a substantial challenge to theU.S.
interpretation of good and evil.
Prior polarity and reliability class are marked in parenthesesfor words that match clues from the lexicon.feature is true for a clue instance if one of its children is an instance of a strongsubjclue, and if the clue instance and its child share an adj, mod, or vmod relationship.
Themodified by weaksubj feature is the same, except that it looks for instances of weaksubjclues in the children.
Although the adj and vmod relationships are typically local, themod relationship involves longer-distance as well as local dependencies.
Figure 2 helpsto illustrate these features.
The modifies weaksubj feature is true for substantial, becausesubstantial modifies challenge, which is an instance of a weaksubj clue.
For rights, themodifies weaksubj feature is false, because rightsmodifies report, which is not an instanceof a weaksubj clue.
The modified by weaksubj feature is false for substantial, because it hasno modifiers that are instances of weaksubj clues.
For challenge, the modified by weaksubjfeature is true because it is being modified by substantial, which is an instance of aweaksubj clue.Polarity Modification Features The modifies polarity, modified by polarity, and conjpolarity features capture specific relationships between the clue instance and other senti-ment clues it may be related to.
If the clue instance and its parent in the dependency treeshare an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the priorpolarity of the parent.
If the parent is not in the prior-polarity lexicon, its prior polarityis considered neutral.
If the clue instance is at the root of the tree and has no parent,the value of the feature is notmod.
The modified by polarity feature is similar, lookingfor adj,mod, and vmod relationships and other sentiment clues in the children of the clueinstance.
The conj polarity feature determines if the clue instance is in a conjunction.
If so,the value of this feature is its sibling?s prior polarity.
As before, if the sibling is not in the412Wilson, Wiebe, and Hoffmann Recognizing Contextual Polaritylexicon, its prior polarity is neutral.
If the clue instance is not in a conjunction, the valuefor this feature is notmod.
Figure 2 also helps to illustrate thesemodification features.
Theword substantial with positive prior polarity modifies the word challenge with negativeprior polarity.
Therefore the modifies polarity feature is negative for substantial, and themodified by polarity feature is positive for challenge.
The words good and evil are in a con-junction together; thus the conj polarity feature is negative for good and positive for evil.Structure Features These are binary features that are determined by starting withthe clue instance and climbing up the dependency parse tree toward the root, lookingfor particular relationships, words, or patterns.
The in subject feature is true if we finda subj relationship on the path to the root.
The in copular feature is true if in subject isfalse and if a node along the path is both a main verb and a copular verb.
The in passivefeature is true if a passive verb pattern is found on the climb.The in subject and in copular features were motivated by the intuition that the syn-tactic role of a word may influence whether a word is being used to express a sentiment.For example, consider the word polluters in each of the following two sentences.
(12) Under the application shield, polluters are allowed to operate if they havea permit.
(13) ?The big-city folks are pointing at the farmers and saying you arepolluters .
.
.
?In the first sentence, polluters is simply being used as a referring expression.
In thesecond sentence, polluters is clearly being used to express a negative evaluation of thefarmers.
Themotivation for the in passive feature was previous work by Riloff andWiebe(2003), who found that different words aremore or less likely to be subjective dependingon whether they are in the active or passive.Sentence Features These are features that previously were found useful forsentence-level subjectivity classification (Wiebe, Bruce, and O?Hara 1999; Wiebe andRiloff 2005).
They include counts of strongsubj and weaksubj clue instances in the cur-rent, previous and next sentences, counts of adjectives and adverbs other than not inthe current sentence, and binary features to indicate whether the sentence contains apronoun, a cardinal number, and a modal other than will.Document Feature There is one document feature representing the topic or domainof the document.
The motivation for this feature is that whether or not a word isexpressing a sentiment or even a private state in general may depend on the subjectof the discourse.
For example, the words fever and sufferer may express a negativesentiment in certain contexts, but probably not in a health or medical context, as is thecase in the following sentence.
(14) The disease can be contracted if a person is bitten by a certain tick or if aperson comes into contact with the blood of a congo fever sufferer.In the creation of the MPQA corpus, about two-thirds of the documents wereselected to be on one of the 10 topics listed in Table 8.
The documents for each topic wereidentified by human searches and by an information retrieval system.
The remainingdocuments were semi-randomly selected from a very large pool of documents fromthe world press.
In the corpus, these documents are listed with the topic miscellaneous.Rather than leaving these documents unlabeled, we chose to label them using the413Computational Linguistics Volume 35, Number 3Table 8Topics in the MPQA corpus.Topic Descriptionargentina Economic collapse in Argentinaaxisofevil U.S. President?s State of the Union Addressguantanamo Detention of prisoners in Guantanamo Bayhumanrights U.S. State Department Human Rights Reportkyoto Kyoto Protocol ratificationsettlements Israeli settlements in Gaza and the West Bankspace Space missions of various countriestaiwan Relationship between Taiwan and Chinavenezuela Presidential coup in Venezuelazimbabwe Presidential election in Zimbabwefollowing general domain categories: economics, general politics, health, report events,and war and terrorism.7.2 Features for Polarity ClassificationTable 9 lists the features that we use for step two, polarity classification.
Word token,word prior polarity, and the polarity-modification features are the same as described forneutral?polar classification.We use two features to capture two different types of negation.
The negated featureis a binary feature that is used to capture more local negations: Its value is true if anegation word or phrase is found within the four words preceding the clue instance,and if the negation word is not also in a phrase that acts as an intensifier rather than anegator.
Examples of phrases that intensify rather than negate are not only and nothing ifnot.
The negated subject feature captures a longer-distance type of negation.
This featureTable 9Features for polarity classification.Word Featuresword tokenword prior polarity: positive, negative, both, neutralNegation Featuresnegated: binarynegated subject: binaryPolarity Modification Featuresmodifies polarity: positive, negative, neutral, both, notmodmodified by polarity: positive, negative, neutral, both, notmodconj polarity: positive, negative, neutral, both, notmodPolarity Shiftersgeneral polarity shifter: binarynegative polarity shifter: binarypositive polarity shifter: binary414Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarityis true if the subject of the clause containing the clue instance is negated.
For example,the negated subject feature is true for support in the following sentence.
(15) No politically prudent Israeli could support either of them.The last three polarity features look in a window of four words before the clueinstance, searching for the presence of particular types of polarity influencers.
Gen-eral polarity shifters reverse polarity (e.g., little truth, little threat).
Negative polarityshifters typically make the polarity of an expression negative (e.g., lack of understand-ing).
Positive polarity shifters typically make the polarity of an expression positive(e.g., abate the damage).
The polarity influencers that we used were identified throughexplorations of the development data.8.
Experiments in Recognizing Contextual PolarityWe have two primary goals with our experiments in recognizing contextual polarity.The first is to evaluate the features described in Section 7 as to their usefulness forthis task.
The second is to investigate the importance of recognizing neutral instances?recognizing when a sentiment clue is not being used to express a sentiment?for classi-fying contextual polarity.To evaluate features, we investigate their performance, both together and sep-arately, across several different learning algorithms.
Varying the learning algorithmallows us to verify that the features are robust and that their performance is not theartifact of a particular algorithm.
We experiment with four different types of machinelearning: boosting, memory-based learning, rule learning, and support vector learning.For boosting, we use BoosTexter (Schapire and Singer 2000) AdaBoost.MH.
For rulelearning, we use Ripper (Cohen 1996).
For memory-based learning, we use TiMBL(Daelemans et al 2003b) IB1 (k-nearest neighbor).
For support vector learning, weuse SVM-light and SVM-multiclass (Joachims 1999).
SVM-light is used for the experi-ments involving binary classification (neutral?polar classification), and SVM-multiclassis used for experiments with more than two classes.
These machine learning algorithmswere chosen because they have been used successfully for a number of natural languageprocessing tasks, and they represent several different types of learning.For all of the classification algorithms except for SVM, the features for a clue in-stance are represented as they are presented in Section 7.
For SVM, the representationsfor numeric and discrete-valued features are changed.
Numeric features, such as thecount of strongsubj clue instances in a sentence, are scaled to range between 0 and 1.Discrete-valued features, such as the reliability class feature, are converted into multiplebinary features.
For example, the reliability class feature is represented by two binaryfeatures: one for whether the clue instance is a strongsubj clue and one for whether theclue instance is a weaksubj clue.To investigate the importance of recognizing neutral instances, we perform two setsof polarity classification (step two) experiments.
First, we experiment with classifyingthe polarity of all gold-standard polar instances?the clue instances identified as polarin context by the manual polarity annotations.
Second, we experiment with using thepolar instances identified automatically by the neutral?polar classifiers.
Because thesecond set of experiments includes the neutral instances misclassified in step one, wecan compare results for the two sets of experiments to see how the noise of neutralinstances affects the performance of the polarity features.415Computational Linguistics Volume 35, Number 3All experiments are performed using 10-fold cross validation over a test set of10,287 sentences from 494MPQA corpus documents.
Wemeasure performance in termsof accuracy, recall, precision, and F-measure.
Accuracy is simply the total number ofinstances correctly classified.
Recall, precision, and F-measure for a given class C aredefined as follows.
Recall is the percentage of all instances of class C correctly identified.Rec(C) =| instances of C correctly identified || all instances of C |Precision is the percentage of instances classified as class C that are class C in truth.Prec(C) =| instances of C correctly identified || all instances identified as C |F-measure is the harmonic mean of recall and precision.F(C) =2 ?Rec(C)?
Prec(C)Rec(C)+ Prec(C)All results reported are averages over the 10 folds.8.1 Neutral?Polar ClassificationIn our two-step process for recognizing contextual polarity, the first step is neutral?polarclassification, determining whether each instance of a clue from the lexicon is neutral orpolar in context.
In our test set, there are 26,729 instances of clues from the lexicon.
Thefeatures we use for this step were listed above in Table 7 and described in Section 7.1.In this section, we perform two sets of experiments.
In the first, we comparethe results of neutral?polar classification using all the neutral?polar features againsttwo baselines.
The first baseline uses just the word token feature.
The second baseline(word+priorpol) uses the word token and prior polarity features.
In the second set ofexperiments, we explore the performance of different sets of features for neutral?polarclassification.Research has shown that the performance of learning algorithms for NLP tasks canvary widely depending on their parameter settings, and that the optimal parametersettings can also vary depending on the set of features being evaluated (Daelemanset al 2003a; Hoste 2005).
Although the goal of this work is not to identify the optimalconfiguration for each algorithm and each set of features, we still want to make a rea-sonable attempt to find a good configuration for each algorithm.
To do this, we perform10-fold cross validation of the more challenging baseline classifier (word+priorpol)on the development data, varying select parameter settings.
The results from thoseexperiments are then used to select the parameter settings for each algorithm.
ForBoosTexter, we vary the number of rounds of boosting.
For TiMBL, we vary the valuefor k (the number of neighbors) and the distance metric (overlap or modified valuedifference metric [MVDM]).
For Ripper, we vary whether negative tests are disallowedfor nominal (-!n) and set (-!s) valued attributes and howmuch to simplify the hypothesis(-S).
For SVM, we experiment with linear, polynomial, and radial basis function kernels.Table 10 gives the settings selected for the neutral?polar classification experiments forthe different learning algorithms.416Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 10Algorithm settings for neutral?polar classification.Algorithm SettingsBoosTexter 2,000 rounds of boostingTiMBL k=25, MVDM distance metricRipper -!n, -S 0.5SVM linear kernel8.1.1 Classification Results.
The results for the first set of experiments are given inTable 11.
For each algorithm, we give the results for the two baseline classifiers, followedby the results for the classifier trained using all the neutral?polar features.
The resultsshown in bold are significantly better than both baselines (two-sided t-test, p?
0.05) forthe given algorithm.Working together, how well do the neutral?polar features perform?
For BoosTexter,TiMBL, and Ripper, the classifiers trained using all the features improve significantlyover the two baselines in terms of accuracy, polar recall, polar F-measure, and neutralprecision.
Neutral F-measure is also higher, but not significantly so.
These consistentresults across three of the four algorithms show that the neutral?polar features arehelpful for determining when a sentiment clue is actually being used to express asentiment.Interestingly, Ripper is the only algorithm for which the word-token baseline per-formed better than the word+priorpol baseline.
Nevertheless, the prior polarity featureis an important component in the performance of the Ripper classifier using all thefeatures.
Excluding prior polarity from this classifier results in a significant decrease inTable 11Results for neutral?polar classification (step one).Polar NeutralAcc Rec Prec F Rec Prec FBoosTexterword token baseline 74.0 41.9 77.0 54.3 92.7 73.3 81.8word+priorpol baseline 75.0 55.6 70.2 62.1 86.2 76.9 81.3neutral?polar features 76.5 58.3 72.4 64.6 87.1 78.2 82.4TiMBLword token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8word+priorpol baseline 74.6 48.2 73.7 58.3 90.0 74.9 81.7neutral?polar features 76.5 59.5 71.7 65.0 86.3 78.5 82.3Ripperword token baseline 66.3 11.2 80.6 19.6 98.4 65.6 78.7word+priorpol baseline 65.5 07.7 84.5 14.1 99.1 64.8 78.4neutral?polar features 71.4 49.4 64.6 56.0 84.2 74.1 78.8SVMword token baseline 74.6 47.9 73.9 58.1 90.1 74.8 81.8word+priorpol baseline 75.6 54.5 72.5 62.2 88.0 76.8 82.0neutral?polar features 75.3 52.6 72.7 61.0 88.5 76.2 81.9417Computational Linguistics Volume 35, Number 3performance for every metric.
Decreases range from 2.5% for neutral recall to 9.5% forpolar recall.The best SVM classifier is the word+priorpol baseline.
In terms of accuracy, thisclassifier does not perform much worse than the BoosTexter and TiMBL classifiers thatuse all the neutral?polar features: The SVM word+priorpol baseline classifier has anaccuracy of 75.6%, and both the BoosTexter and TiMBL classifiers have an accuracy of76.5%.
However, the BoosTexter and TiMBL classifiers using all the features performnotably better in terms of polar recall and F-measure.
The BoosTexter and TiMBLclassifiers have polar recalls that are 7% and 9.2% higher than the SVM baseline.
PolarF-measures for BoosTexter and TiMBL are 3.9% and 4.5% higher.
These increases aresignificant for p ?
0.01.8.1.2 Feature Set Evaluation.
To evaluate the contribution of the various features forneutral?polar classification, we perform a series of experiments in which differentsets of neutral?polar features are added to the word+priorpol baseline and new clas-sifiers are trained.
We then compare the performance of these new classifiers to theword+priorpol baseline, with the exception of the Ripper classifiers, which we compareto the higher word baseline.
Table 12 lists the sets of features tested in these experiments.The feature sets generally correspond to how the neutral?polar features are presentedin Table 7, although some of the groups are broken down into more fine-grained setsthat we believe capture meaningful distinctions.Table 13 gives the results for these experiments.
Increases and decreases for agiven metric as compared to the word+priorpol baseline (word baseline for Ripper)are indicated by + or ?, respectively.
Where changes are significant at the p ?
0.1 level,++ or ?
?
are used, and where changes are significant at the p ?
0.05 level, +++ or ?
?
?are used.
An ?nc?
indicates no change (a change of less than ?
0.05) compared to thebaseline.What does Table 13 reveal about the performance of various feature sets for neutral?polar classification?Most noticeable is that no individual feature sets stand out as strongperformers.
The only significant improvements in accuracy come from the PARTS-OF-SPEECH and RELIABILITY-CLASS feature sets for Ripper.
These improvements areperhaps not surprising given that the Ripper baseline was much lower to begin with.Very few feature sets show any improvement for SVM.
Again, this is not unexpectedgiven that all the features together performed worse than the word+priorpol baselineTable 12Neutral?polar feature sets for evaluation.Experiment FeaturesPARTS-OF-SPEECH parts of speech for clue instance, previous word, and next wordRELIABILITY-CLASS reliability class of clue instancePRECEDED-POS preceded by adjective, preceded by adverbINTENSIFY preceded by intensifier, self intensifierRELCLASS-MOD modifies strongsubj/weaksubj, modified by strongsubj/weaksubjPOLARITY-MOD polarity-modification featuresSTRUCTURE structure featuresCURSENT-COUNTS strongsubj/weaksubj clue instances in sentencePNSENT-COUNTS strongsubj/weaksubj clue instances in previous/next sentenceCURSENT-OTHER adjectives/adverbs/cardinal number/pronoun/modal in sentenceTOPIC document topic418Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 13Results for neutral?polar feature ablation experiments.Polar Neut Polar NeutBoosTexter Acc F F Ripper Acc F FPARTS-OF-SPEECH + ?
+ PARTS-OF-SPEECH +++ +++ ?
?
?RELIABILITY-CLASS + ?
+ RELIABILITY-CLASS +++ +++ +PRECEDED-POS nc ?
nc PRECEDED-POS ?
?
?INTENSIFY - nc - INTENSIFY ?
?
?
?
?RELCLASS-MOD + ++ + RELCLASS-MOD + +++ +POLARITY-MOD nc ?
+ POLARITY-MOD ?
+++ ?STRUCTURE ?
?
?
?
+ STRUCTURE ?
+ ?CURSENT-COUNTS + ?
?
?
+ CURSENT-COUNTS ?
?
+++ ?
?
?PNSENT-COUNTS + ?
?
?
+ PNSENT-COUNTS ?
?
?
+++ ?
?
?CURSENT-OTHER nc ?
+ CURSENT-OTHER ?
?
?
+++ ?
?
?TOPIC + + + TOPIC ?
+++ ?
?
?Polar Neut Polar NeutTiMBL Acc F F SVM Acc F FPARTS-OF-SPEECH + +++ + PARTS-OF-SPEECH ?
?
?
?
?
?RELIABILITY-CLASS + + nc RELIABILITY-CLASS + ?
+PRECEDED-POS nc + nc PRECEDED-POS nc nc ncINTENSIFY nc nc nc INTENSIFY nc nc ncRELCLASS-MOD + + + RELCLASS-MOD nc + ncPOLARITY-MOD + + + POLARITY-MOD ?
?
?
?
?
?
?STRUCTURE nc + ?
STRUCTURE ?
+ ?CURSENT-COUNTS ?
+ ?
CURSENT-COUNTS ?
?
?PNSENT-COUNTS + +++ ?
PNSENT-COUNTS ?
?
?CURSENT-OTHER + +++ ?
CURSENT-OTHER ?
?
?TOPIC ?
+ ?
TOPIC ?
?
?Increases and decreases for a given metric as compared to the word+priorpol baseline(word baseline for Ripper) are indicated by + or ?, respectively; ++ or ?
?
indicates thechange is significant at the p < 0.1 level; +++ or ?
?
?
indicates significance at thep < 0.05 level; nc indicates no change.for SVM.
The performance of the feature sets for BoosTexter and TiMBL are perhapsthe most revealing.
In the previous experiments using all the features together, thesealgorithms produced classifiers with the same high performance.
In these experiments,six different feature sets for each algorithm show improvements in accuracy over thebaseline, yet none of those improvements are significant.
This suggests that achievingthe highest performance for neutral?polar classification requires a wide variety of fea-tures working together in combination.We further test this result by evaluating the effect of removing the features thatproduced either no change or a drop in accuracy from the respective all-feature classi-fiers.
For example, we train a TiMBL neutral?polar classifier using all the features exceptfor those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPICfeature sets, and then compare the performance of this new classifier to the TiMBL, all-feature classifier.
Although removing the non-performing features has little effect forBoosTexter, performance does drop for both TiMBL and Ripper.
The primary source ofthis performance drop is a decrease in polar recall: 2% for TiMBL and 3.2% for Ripper.419Computational Linguistics Volume 35, Number 3Although no feature sets stand out in Table 13 as far as giving an overall highperformance, there are some features that consistently improve performance acrossthe different algorithms.
The reliability class of the clue instance (RELIABILITY-CLASS)improves accuracy over the baseline for all four algorithms.
It is the only feature thatdoes so.
The RELCLASS-MOD features give improvements for all metrics for BoosTexter,Ripper, and TiMBL, as well as improving polar F-measure for SVM.
The PARTS-OF-SPEECH features are also fairly consistent, improving performance for all the algorithmsexcept for SVM.
There are also a couple of feature sets that consistently do not improveperformance for any of the algorithms: the INTENSIFY and PRECEDED-POS features.8.2 Polarity ClassificationFor the second step of recognizing contextual polarity, we classify the polarity of all clueinstances identified as polar in step one.
The features for polarity classification werelisted in Table 9 and described in Section 7.2.We investigate the performance of the polarity features under two conditions:(1) perfect neutral?polar recognition and (2) automatic neutral?polar recognition.
Forcondition 1, we identify the polar instances according to the gold-standard, manualcontextual-polarity annotations.
In the test data, 9,835 instances of the clues from thelexicon are polar in context according to the manual annotations.
Experiments undercondition 1 classify these instances as having positive, negative, or both (positive andnegative) polarity.
For condition 2, we take the best performing neutral?polar classifierfor each algorithm and use the output from those algorithms to identify the polarinstances.
Because polar instances now are being identified automatically, there will benoise in the form of misclassified neutral instances.
Therefore, for experiments undercondition 2 we include the neutral class and perform four-way classification instead ofthree-way.
Condition 1 allows us to investigate the performance of the different polarityfeatures without the noise of misclassified neutral instances.
Also, because the set ofpolar instances being classified is the same for all the algorithms, condition 1 allowsus to compare the performance of the polarity features across the different algorithms.However, condition 2 is themore natural one.
It allows us to see how the noise of neutralinstances affects the performance of the polarity features.The following sections describe three sets of experiments.
First, we investigate theperformance of the polarity features used together for polarity classification undercondition 1.
As before, the word and word+priorpol classifiers provide our baselines.
Inthe second set of experiments, we explore the performance of different sets of featuresfor polarity classification, again assuming perfect recognition of the polar instances.Finally, we experiment with polarity classification using all the polarity features undercondition 2, automatic recognition of the polar instances.As before, we use the development data to select the parameter settings for each al-gorithm.
The settings for polarity classification are given in Table 14.
They were selectedbased on the performance of the word+priorpol baseline classifier under condition 2.8.2.1 Classification Results: Condition 1.
The results for polarity classification using all thepolarity features, assuming perfect neutral?polar recognition for step one, are given inTable 15.
For each algorithm, we give the results for the two baseline classifiers, followedby the results for the classifier trained using all the polarity features.
For the metricswhere the polarity features perform statistically better than both baselines (two-sidedt-test, p ?
0.05), the results are given in bold.420Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityTable 14Algorithm settings for polarity classification.Algorithm SettingsBoosTexter 2,000 rounds of boostingTiMBL k=1, MVDM distance metricRipper -!s, -S 0.5SVM linear kernelTable 15Results for polarity classification (step two) using gold-standard polar instances.Positive Negative BothAcc Rec Prec F Rec Prec F Rec Prec FBoosTexterword token baseline 78.7 57.7 72.8 64.4 91.5 80.8 85.8 12.9 53.6 20.8word+priorpol baseline 79.7 70.5 68.8 69.6 87.2 85.1 86.1 13.7 53.7 21.8polarity features 83.2 76.7 74.3 75.5 89.7 87.7 88.7 11.8 54.2 19.4TiMBLword token baseline 78.5 63.3 69.2 66.1 88.6 82.5 85.4 14.1 51.0 22.1word+priorpol baseline 79.4 69.7 68.4 69.1 87.0 84.8 85.9 14.6 53.5 22.9polarity features 82.2 75.4 73.3 74.3 88.5 87.6 88.0 18.3 34.6 23.9Ripperword token baseline 70.0 14.5 74.5 24.3 98.3 69.7 81.6 09.1 74.4 16.2word+priorpol baseline 78.9 75.5 65.2 70.0 83.8 86.4 85.1 09.8 75.4 17.4polarity features 83.2 77.8 73.5 75.6 89.2 87.8 88.5 09.8 74.9 17.4SVMword token baseline 69.9 62.4 69.6 65.8 76.0 84.1 79.9 14.1 31.2 19.4word+priorpol baseline 78.2 76.7 63.7 69.6 82.2 86.7 84.4 09.8 75.4 17.4polarity features 81.6 74.9 71.1 72.9 88.1 86.6 87.3 09.5 77.6 16.9Howwell do the polarity features perform working all together?
For all algorithms,the polarity classifier using all the features significantly outperforms both baselinesin terms of accuracy, positive F-measure, and negative F-measure.
These consistentimprovements in performance across all four algorithms show that these features arequite useful for polarity classification.One interesting thing that Table 15 reveals is that negative polarity words are muchmore straightforward to recognize than positive polarity words, at least in this corpus.For the negative class, precisions and recalls for the word+priorpol baseline range from82.2 to 87.2.
For the positive class, precisions and recalls for the word+priorpol baselinerange from 63.7 to 76.7.
However, it is with the positive class that polarity features seemto help themost.
With the addition of the polarity features, positive F-measure improvesby 5 points on average; improvements in negative F-measures average only 2.75 points.8.2.2 Feature Set Evaluation.
To evaluate the performance of the various features forpolarity classification, we again perform a series of ablation experiments.
As before, westart with the word+priorpol baseline classifier, add different sets of polarity features,train new classifiers, and compare the results of the new classifiers to the baseline.421Computational Linguistics Volume 35, Number 3Table 16Polarity feature sets for evaluation.Experiment FeaturesNEGATION negated, negated subjectPOLARITY-MOD modifies polarity, modified by polarity, conjunction polaritySHIFTERS general, negative, positive polarity shiftersTable 17Results for polarity feature ablation experiments.Positive NegativeAcc Rec Prec F Rec Prec FBoosTexterNEGATION +++ ++ +++ +++ +++ + +++POLARITY-MOD ++ +++ + +++ + ++ +SHIFTERS + + + + + + +TiMBLNEGATION +++ +++ +++ +++ +++ +++ +++POLARITY-MOD + + + + ?
+ +SHIFTERS + + + + ?
+ +RipperNEGATION +++ ?
?
+++ +++ +++ ?
+++POLARITY-MOD + +++ ++ +++ + + +SHIFTERS + ?
+ + + ?
+SVMNEGATION +++ ?
+++ +++ +++ + +++POLARITY-MOD + ?
+++ + + ?
+SHIFTERS + ?
+ + + + +Increases and decreases for a given metric as compared to theword+priorpol baseline are indicated by + or ?, respectively;++ or ?
?
indicates the change is significant at the p < 0.1 level;+++ or ?
?
?
indicates significance at the p < 0.05 level.Table 16 lists the sets of features tested in each experiment, and Table 17 shows theresults of the experiments.
Results are reported as they were previously in Section 8.1.2,with increases and decreases compared to the baseline for a given metric indicated by +or ?, respectively.Looking at Table 17, we see that all three sets of polarity features help to increaseperformance as measured by accuracy and positive and negative F-measures.
This istrue for all the classification algorithms.
As we might expect, including the negationfeatures has the most marked effect on the performance of polarity classification, withstatistically significant improvements for most metrics across all the algorithms.9 The9 Although the negation features give the best performance improvements of the three feature sets, theseclassifiers still do not perform as well as the respective all-feature polarity classifiers for each algorithm.422Wilson, Wiebe, and Hoffmann Recognizing Contextual Polaritypolarity-modification features also seem to be important for polarity classification,in particular for disambiguating the positive instances.
For all the algorithms exceptTiMBL, including the polarity-modification features results in significant improvementsfor at least one of the positive metrics.
The polarity shifters also help classification, butthey seem to be the weakest of the features: Including them does not result in significantimprovements for any algorithm.Another question that is interesting to consider is how much the word token featurecontributes to polarity classification, given all the other polarity features.
Is it enoughto know the prior polarity of a word, whether it is being negated, and how it is relatedto other polarity influencers?
To answer this question, we train classifiers using all thepolarity features except for word token.
Table 18 gives the results for these classifiers;for comparison, the results for the all-feature polarity classifiers are also given.
Inter-estingly, excluding the word token feature produces only small changes in the overallresults.
The results for BoosTexter and Ripper are slightly lower, and the results forSVM are practically unchanged.
TiMBL actually shows a slight improvement, with theexception of the both class.
This provides further evidence of the strength of the polarityfeatures.
Also, a classifier not tied to actual word tokens may potentially be a moredomain-independent classifier.8.2.3 Classification Results: Condition 2.
The experiments in Section 8.2.1 show that thepolarity features perform well under the ideal condition of perfect recognition of polarinstances.
The next question to consider is how well the polarity features performunder the more natural but less-than-perfect condition of automatic recognition ofpolar instances.
To investigate this, the polarity classifiers (including the baselines) foreach algorithm in these experiments start with the polar instances identified by thebest performing neutral?polar classifier for that algorithm (from Section 8.1.1).
Theresults for these experiments are given in Table 19.
As before, statistically significantimprovements over both baselines are given in bold.How well do the polarity features perform in the presence of noise from misclas-sified neutral instances?
Our first observation comes from comparing Table 15 withTable 19: Polarity classification results are much lower for all classifiers with the noiseof neutral instances.
Yet in spite of this, the polarity features still produce classifiers thatTable 18Results for polarity classification without and with the word token feature.Acc Pos F Neg F Both FBoosTexterexcluding word token 82.5 74.9 88.0 17.4all polarity features 83.2 75.5 88.7 19.4TiMBLexcluding word token 83.2 75.9 88.4 17.3all polarity features 82.2 74.3 88.0 23.9Ripperexcluding word token 82.9 75.4 88.3 17.4all polarity features 83.2 75.6 88.5 17.4SVMexcluding word token 81.5 72.9 87.3 16.8all polarity features 81.6 72.9 87.3 16.9423ComputationalLinguisticsVolume35,Number3Table 19Results for polarity classification (step two) using automatically identified polar instances.Positive Negative Both NeutralAcc R P F R P F R P F R P FBoosTexterword token 61.5 62.3 62.7 62.5 86.4 64.6 74.0 11.4 49.3 18.5 20.8 44.5 28.3word+priorpol 63.3 70.0 57.9 63.4 81.3 71.5 76.1 12.5 47.3 19.8 30.9 47.5 37.4polarity feats 65.9 73.6 62.2 67.4 84.9 72.3 78.1 13.4 40.7 20.2 31.0 50.6 38.4TiMBLword token 60.1 68.3 58.9 63.2 81.8 65.0 72.5 11.2 39.6 17.4 21.6 43.1 28.8word+priorpol 61.0 73.2 53.4 61.8 80.6 69.8 74.8 12.7 41.7 19.5 23.0 44.2 30.3polarity feats 64.4 75.3 58.6 65.9 81.1 73.0 76.9 16.9 32.7 22.3 32.1 50.0 39.1Ripperword token 54.4 22.2 69.4 33.6 95.1 50.7 66.1 00.0 00.0 00.0 21.7 76.5 33.8word+priorpol 51.4 24.0 71.7 35.9 97.7 48.9 65.1 00.0 00.0 00.0 09.2 75.8 16.3polarity feats 54.8 38.0 67.2 48.5 95.5 52.7 67.9 00.0 00.0 00.0 14.5 66.8 23.8SVMword token 64.5 70.0 60.9 65.1 70.9 74.9 72.9 16.6 41.5 23.7 53.3 51.0 52.1word+priorpol 62.8 89.0 51.2 65.0 88.4 69.2 77.6 11.1 48.5 18.0 02.4 58.3 04.5polarity feats 64.1 90.8 53.0 66.9 90.4 70.1 79.0 12.7 52.3 20.4 02.2 61.4 04.3424Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarityoutperform the baselines.
For three of the four algorithms, the classifier using all thepolarity features has the highest accuracy.
For BoosTexter and TiMBL, the improvementsin accuracy over both baselines are significant.
Also for all algorithms, using the polarityfeatures gives the highest positive and negative F-measures.Because the set of polarity instances being classified by each algorithm is different,we cannot directly compare the results from one algorithm to the next.8.3 Two-step versus One-step Recognition of Contextual PolarityAlthough the two-step approach to recognizing contextual polarity allows us to focusour investigation on the performance of features for both neutral?polar classificationand polarity classification, the question remains: How does the two-step approachcompare to recognizing contextual polarity in a single classification step?
The resultsshown in Table 20 help to answer this question.
The first row in Table 20 for eachalgorithm shows the combined result for the two stages of classification.
For BoosTexter,TiMBL, and Ripper, this is the combination of results from using all the neutral?polarfeatures for step one, together with the results from using all of the polarity features forstep two.10 For SVM, this is the combination of results from the word+priorpol baselinefrom step one, together with results for using all the polarity features for step two.Recall that the word+priorpol classifier was the best neutral?polar classifier for SVM(see Table 11).
The second rows for BoosTexter, TiMBL, and Ripper show the results ofa single classifier trained to recognize contextual polarity using all the neutral?polarand polarity features together.
For SVM, the second row shows the results of classifyingthe contextual polarity using just the word token feature.
This classifier outperformedall others for SVM.
In the table, the best result for each metric for each algorithm ishighlighted in bold.When comparing the two-step and one-step approaches, contrary to our expecta-tions, we see that the one-step approach performs about as well or better than thetwo-step approach for recognizing contextual polarity.
For SVM, the improvement inaccuracy achieved by the two-step approach is significant, but this is not true forthe other algorithms.
One fairly consistent difference between the two approaches isthat the two-step approach scores slightly higher for neutral F-measure, and the one-step approach achieves higher F-measures for the polarity classes.
The difference innegative F-measure is significant for BoosTexter, TiMBL, and Ripper.
The exception tothis is SVM.
For SVM, the two-step approach achieves significantly higher positive andnegative F-measures.One last question we consider is how much the neutral?polar features contributeto the performance of the one-step classifiers.
The third line in Table 20 for BoosTexter,TiMBL, and Ripper gives the results for a one-step classifier trainedwithout the neutral?polar features.
Although the differences are not always large, excluding the neutral?polar features consistently degrades performance in terms of accuracy and positive,negative, and neutral F-measures.
The drop in negative F-measure is significant for allthree algorithms, the drop in neutral F-measure is significant for BoosTexter and TiMBL,and the drop in accuracy is significant for TiMBL and Ripper (and for BoosTexter at thep ?
0.1 level).10 To clarify, Section 8.2.3 only reported results for instances identified as polar in step one.
Here, we reportresults for all clue instances, including the instances classified as neutral in step one.425Computational Linguistics Volume 35, Number 3Table 20Results for contextual polarity classification for both two-step and one-step approaches.Acc Pos F Neg F Both F Neutral FBoosTextertwo-step 74.5 47.1 57.5 12.9 83.4one-step all feats 74.3 49.1 59.8 14.1 82.9one-step ?
neut-pol feats 73.3 48.4 58.7 16.3 81.9TiMBLtwo-step 74.1 47.6 56.4 13.8 83.2one-step all feats 73.9 49.6 59.3 15.2 82.6one-step ?
neut-pol feats 72.5 49.5 56.9 21.6 81.4Rippertwo-step 68.9 26.6 49.0 00.0 80.1one-step all feats 69.5 30.2 52.8 14.0 79.4one-step ?
neut-pol feats 67.0 28.9 33.0 11.4 78.6SVMtwo-step 73.1 46.6 58.0 13.0 82.1one-step 71.6 43.4 51.7 17.0 81.6The modest drop in performance that we see when excluding the neutral?polarfeatures in the one-step approach seems to suggest that discriminating between neutraland polar instances is helpful but not necessarily crucial.
However, consider Figure 3.In this figure, we show the F-measures for the positive, negative, and both classes forthe BoosTexter polarity classifier that uses the gold-standard neutral/polar instances(from Table 15) and for the BoosTexter one-step polarity classifier that uses all features(from Table 20).
Plotting the same sets of results for the other three algorithms producesvery similar figures.
The difference when the classifiers have to contend with the noisefrom neutral instances is dramatic.
Although Table 20 shows that there is room forimprovement across all the contextual polarity classes, Figure 3 shows us that perhapsthe best way to achieve these improvements is to improve the ability to discriminate theneutral class from the others.Figure 3Chart showing the positive, negative, and both class F-measures for the BoosTexter classifier thatuses the gold-standard neutral/polar classes and the BoosTexter one-step classifier that uses allthe features.426Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarity9.
Related Work9.1 Phrase-Level Sentiment AnalysisOther researchers who have worked on classifying the contextual polarity of sentimentexpressions are Yi et al (2003), Popescu and Etzioni (2005), and Suzuki, Takamura, andOkumura (2006).
Yi et al use a lexicon and manually developed patterns to classifycontextual polarity.
Their patterns are high-quality, yielding quite high precision overthe set of expressions that they evaluate.
Popescu and Etzioni use an unsupervised clas-sification technique called relaxation labeling (Hummel and Zucker 1983) to recognizethe contextual polarity of words that are at the heads of select opinion phrases.
Theytake an iterative approach, using relaxation labeling first to determine the contextualpolarities of the words, then again to label the polarities of the words with respect totheir targets.
A third stage of relaxation labeling then is used to assign final polarities tothe words, taking into consideration the presence of other polarity terms and negation.Aswe do, Popescu and Etzioni use features that represent conjunctions and dependencyrelations between polarity words.
Suzuki et al use a bootstrapping approach to classifythe polarity of tuples of adjectives and their target nouns in Japanese blogs.
Includedin the features that they use are the words that modify the adjectives and the word thatthe adjective modifies.
They consider the effect of a single negation term, the Japaneseequivalent of not.Our work in recognizing contextual polarity differs from this research onexpression-level sentiment analysis in several ways.
First, the set of expressions theyevaluate is limited either to those that target specific items of interest, such as productsand product features, or to tuples of adjectives and nouns.
In contrast, we seek to classifythe contextual polarity of all instances of words from a large lexicon of subjectivity cluesthat appear in the corpus.
Included in the lexicon are not only adjectives, but nouns,verbs, adverbs, and even modals.Our work also differs from other research in the variety of features that we use.
Asother researchers do, we consider negation and the words that directly modify or aremodified by the expression being classified.
However, with negation, we have featuresfor both local and longer-distance types of negation, and we take care to count negationterms only when they are actually being used to negate, excluding, for example, nega-tion terms when they are used in phrases that intensify (e.g., not only).
We also includecontextual features to capture the presence of other clue instances in the surroundingsentences, and features that represent the reliability of clues from the lexicon.Finally, a unique aspect of the work presented in this article is the evaluation ofdifferent features for recognizing contextual polarity.
We first presented the featuresexplored in this research in Wilson, Wiebe, and Hoffman (2005), but this work signif-icantly extends that initial evaluation.
We explore the performance of features acrossdifferent learning algorithms, and we evaluate not only features for discriminatingbetween positive and negative polarity, but features for determining when a word isor is not expressing a sentiment in the first place (neutral in context).
This is also thefirst work to evaluate the effect of neutral instances on the performance of features fordiscriminating between positive and negative contextual polarity.9.2 Other Research in Sentiment AnalysisRecognizing contextual polarity is just one facet of the research in automatic senti-ment analysis.
Research ranges from work on learning the prior polarity (semantic427Computational Linguistics Volume 35, Number 3orientation) of words and phrases (e.g., Hatzivassiloglou and McKeown 1997; Kampsand Marx 2002; Turney and Littman 2003; Hu and Liu 2004; Kim and Hovy 2004; Esuliand Sebastiani 2005; Takamura, Inui, and Okumura 2005; Popescu and Etzioni 2005;Andreevskaia and Bergler 2006; Esuli and Sebastiani 2006a; Kanayama and Nasukawa2006) to characterizing the sentiment of documents, such as recognizing inflammatorymessages (Spertus 1997), tracking sentiment over time in online discussions (Tong2001), and classifying the sentiment of online messages (e.g., Das and Chen 2001;Koppel and Schler 2006), customer feedback data (Gamon 2004), or product and moviereviews (e.g., Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, andPennock 2003; Beineke, Hastie, and Vaithyanathan 2004; Mullen and Collier 2004; Bai,Padman, and Airoldi 2005; Whitelaw, Garg, and Argamon 2005; Kennedy and Inkpen2006; Koppel and Schler 2006).Identifying prior polarity is a different task than recognizing contextual polarity,although the two tasks are complementary.
The goal of identifying prior polarity isto automatically acquire the polarity of words or phrases for listing in a lexicon.
Ourwork on recognizing contextual polarity begins with a lexicon of words with establishedprior polarities and then disambiguates in the corpus the polarity being expressedby the phrases in which instances of those words appear.
To make the relationshipbetween that task and ours clearer, someword lists that are used to evaluatemethods forrecognizing prior polarity (positive and negative word lists from the General Inquirer[Stone et al 1966] and lists of positive and negative adjectives created for evaluation byHatzivassiloglou and McKeown [1997]) are included in the prior-polarity lexicon usedin our experiments.For the most part, the features explored in this work differ from the ones used toidentify prior polarity with just a few exceptions.
Using a feature to capture conjunc-tions between clue instances was motivated in part by the work of Hatzivassiloglou andMcKeown (1997).
They use constraints on the co-occurrence in conjunctions of wordswith similar or opposite polarity to predict the prior polarity of adjectives.
Esuli andSebastiani (2005) consider negation in some of their experiments involving WordNetglosses.
Takamura et al (2005) use negation words and phrases, including phrases suchas lack of that are members in our lists of polarity shifters, and conjunctive expressionsthat they collect from corpora.Esuli and Sebastiani (2006a) is the only work in prior-polarity identification toinclude a neutral (objective) category and to consider a three-way classification betweenpositive, negative, and neutral words.
Although identifying prior polarity is a differenttask, they report a finding similar to ours, namely, that accuracy is lower when neutralwords are included.Some research in sentiment analysis classifies the sentiments of sentences.
Morinagaet al (2002), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004),and Grefenstette et al (2004)11 all begin by first creating prior-polarity lexicons.
Yu andHatzivassiloglou then assign a sentiment to a sentence by averaging the prior semanticorientations of instances of lexicon words in the sentence.
Thus, they do not identify thecontextual polarity of individual phrases containing clue instances, which is the focusof this work.
Morinaga et al only consider the positive or negative clue instance ineach sentence that is closest to some target reference; Kim and Hovy, Hu and Liu, andGrefenstette et al multiply or count the prior polarities of clue instances in the sentence.11 In Grefenstette et al (2004), the units that are classified are fixed windows around named entities ratherthan sentences.428Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityThese researchers also consider local negation to reverse polarity, with Morinaga et alalso taking into account the negating effect of words like insufficient.
However, theydo not use the other types of features that we consider in our experiments.
Kaji andKitsuregawa (2006) take a different approach to recognizing positive and negativesentences.
They bootstrap from information easily obtained in ?Pro?
and ?Con?HTML tables and lists, and from one high-precision linguistic pattern, to automaticallyconstruct a large corpus of positive and negative sentences.
They then use this corpus totrain a naive Bayes sentence classifier.
In contrast to our work, sentiment classificationin all of this research is restricted to identifying only positive and negative sentences(excluding our both and neutral categories).
In addition, only one sentiment is assignedper sentence; our system assigns contextual polarity to individual expressions, whichwould allow for a sentence to be assigned to multiple sentiment categories.
As we sawwhen exploring the contextual polarity annotations, it is not uncommon for sentencesto contain more than one sentiment expression.Classifying the sentiment of documents is a very different task than recognizingthe contextual polarity of words and phrases.
However, some researchers have re-ported findings about document-level classification that are similar to our findingsabout phrase-level classification.
Bai et al (2005) argue that dependencies among keysentiment terms are important for classifying document sentiment.
Similarly, we showthat features for capturing when clue instances modify each other are important forphrase-level classification, in particular, for identifying positive expressions.
Gamon(2004) achieves his best results for document classification using a wide variety offeatures, including rich linguistic features, such as features that capture constituentstructure, features that combine part-of-speech and semantic relations (e.g., sentencesubject or negated context), and features that capture tense information.
We also achieveour best results for phrase-level classification using a wide variety of features, manyof which are linguistically rich.
Kennedy and Inkpen (2006) report consistently higherresults for document sentiment classification when select polarity influencers, includingnegators and intensifiers, are included.12 Koppel and Schler (2006) demonstrate theimportance of neutral examples for document-level classification.
In this work, we showthat being able to correctly identify neutral instances is also very important for phrase-level sentiment analysis.10.
Conclusions and Future WorkBeing able to determine automatically the contextual polarity of words and phrases isan important problem in sentiment analysis.
In the research presented in this article, wetackle this problem and show that it is much more complex than simply determiningwhether a word or phrase is positive or negative.
In our analysis of a corpus withannotations of subjective expressions and their contextual polarity, we find that positiveand negative words from a lexicon are used in neutral contexts much more often thanthey are used in expressions of the opposite polarity.
The importance of identifying12 Das and Chen (2001), Pang, Lee, and Vaithyanathan (2002), and Dave, Lawrence, and Pennock (2003) alsorepresent negation.
In their experiments, words which follow a negation term are tagged with a negationmarker and then treated as new words.
Pang, Lee and Vaithyanathan report that representing negation inthis way slightly helps their results, whereas Dave, Lawrence, and Pennock report a slightly detrimentaleffect.
Whitelaw, Garg, and Argamon (2005) also represent negation terms and intensifiers.
However, intheir experiments, the effect of negation is not separately evaluated, and intensifiers are not found to bebeneficial.429Computational Linguistics Volume 35, Number 3when contextual polarity is neutral is further revealed in our classification experiments:When neutral instances are excluded, the performance of features for distinguishingbetween positive and negative polarity greatly improves.A focus of this research is on understanding which features are important forrecognizing contextual polarity.
We experiment with a wide variety of linguisticallymotivated features, and we evaluate the performance of these features using severaldifferent machine learning algorithms.
Features for distinguishing between neutral andpolar instances are evaluated, as well as features for distinguishing between positiveand negative contextual polarity.
For classifying neutral and polar instances, we findthat, although some features produce significant improvements over the baseline interms of polar or neutral recall or precision, it is the combination of features togetherthat is needed to achieve significant improvements in accuracy.
For classifying positiveand negative contextual polarity, features for capturing negation prove to be the mostimportant.
However, we find that features that also perform well are those that cap-ture when a word is (or is not) modifying or being modified by other polarity terms.This suggests that identifying features that represent more complex interdependenciesbetween polarity clues will be an important avenue for future research.Another direction for future work will be to expand our lexicon using existingtechniques for acquiring the prior polarity of words and phrases.
It follows that a largerlexicon will have a greater coverage of sentiment expressions.
However, expanding thelexicon with automatically acquired prior-polarity tags may result in an even greaterproportion of neutral instances to contend with.
Given the degradation in performancecreated by the neutral instances, whether expanding the lexicon automatically willresult in improved performance for recognizing contextual polarity is an empiricalquestion.Finally, the overall goal of our research is to use phrase-level sentiment analysis inhigher-level NLP tasks, such as opinion question answering and summarization.AcknowledgmentsWe would like to thank the anonymousreviewers for their valuable comments andsuggestions.
This work was supported inpart by an Andrew Mellow PredoctoralFellowship, by the NSF under grantIIS-0208798, by the Advanced Research andDevelopment Activity (ARDA), and by theEuropean IST Programme through theAMIDA Integrated Project FP6-0033812.ReferencesAndreevskaia, Alina and Sabine Bergler.2006.
Mining WordNet for fuzzysentiment: Sentiment tag extraction fromWordNet glosses.
In Proceedings of the 11thMeeting of the European Chapter of theAssociation for Computational Linguistics(EACL-2006), pages 209?216, Trento.Bai, Xue, Rema Padman, and EdoardoAiroldi.
2005.
On learning parsimoniousmodels for extracting consumer opinions.In Proceedings of the 38th Annual HawaiiInternational Conference on SystemSciences (HICSS?05) - Track 3, page 75.2,Waikoloa, HI.Banfield, Ann.
1982.
Unspeakable Sentences.Routledge and Kegan Paul, Boston.Beineke, Philip, Trevor Hastie, andShivakumar Vaithyanathan.
2004.
Thesentimental factor: Improving reviewclassification via human-providedinformation.
In Proceedings of the 42ndAnnual Meeting of the Association forComputational Linguistics (ACL-04),pages 263?270, Barcelona.Cohen, William W. 1996.
Learning treesand rules with set-valued features.
InProceedings of the 13th National Conferenceon Artificial Intelligence, pages 709?717,Portland, OR.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meeting ofthe Association for Computational Linguistics(ACL-97), pages 16?23, Madrid.Daelemans, Walter, Ve?ronique Hoste,Fien De Meulder, and Bart Naudts.2003a.
Combined optimization of featureselection and algorithm parameter430Wilson, Wiebe, and Hoffmann Recognizing Contextual Polarityinteraction in machine learning oflanguage.
In Proceedings of the 14thEuropean Conference on Machine Learning(ECML-2003), pages 84?95,Cavtat-Dubrovnik.Daelemans, Walter, Jakub Zavrel, Ko van derSloot, and Antal van den Bosch.
2003b.TiMBL: Tilburg Memory Based Learner,version 5.0 Reference Guide.
ILK TechnicalReport 03-10, Induction of LinguisticKnowledge Research Group, TilburgUniversity.
Available at http://ilk.uvt.nl/downloads/pub/papers/ilk0310.pdf.Das, Sanjiv Ranjan and Mike Y. Chen.
2001.Yahoo!
for Amazon: Sentiment parsingfrom small talk on the Web.
In Proceedingsof the August 2001 Meeting of the EuropeanFinance Association (EFA), Barcelona,Spain.
Available at http://ssrn.com/abstract=276189.Dave, Kushal, Steve Lawrence, and David M.Pennock.
2003.
Mining the peanutgallery: Opinion extraction andsemantic classification of productreviews.
In Proceedings of the 12thInternational World Wide Web Conference(WWW2003), Budapest.
Available athttp://www2003.org.Esuli, Andrea and Fabrizio Sebastiani.
2005.Determining the semantic orientation ofterms through gloss analysis.
InProceedings of ACM SIGIR Conference onInformation and Knowledge Management(CIKM-05), pages 617?624, Bremen.Esuli, Andrea and Fabrizio Sebastiani.
2006a.Determining term subjectivity and termorientation for opinion mining.
InProceedings the 11th Meeting of the EuropeanChapter of the Association for ComputationalLinguistics (EACL-2006), pages 193?200,Trento.Esuli, Andrea and Fabrizio Sebastiani.
2006b.SentiWordNet: A publicly available lexicalresource for opinion mining.
In Proceedingsof LREC-06, the 5th Conference on LanguageResources and Evaluation, pages 417?422,Genoa.Gamon, Michael.
2004.
Sentimentclassification on customer feedback data:Noisy data, large feature vectors, and therole of linguistic analysis.
In Proceedingsof the 20th International Conference onComputational Linguistics (COLING-2004),pages 611?617, Geneva.Grefenstette, Gregory, Yan Qu, James G.Shanahan, and David A. Evans.
2004.Coupling niche browsers and affectanalysis for an opinion mining application.In Proceedings of the Conference Recherched?Information Assistee par Ordinateur(RIAO-2004), pages 186?194, Avignon.Hatzivassiloglou, Vasileios and KathyMcKeown.
1997.
Predicting the semanticorientation of adjectives.
In Proceedings ofthe 35th Annual Meeting of the Associationfor Computational Linguistics (ACL-97),pages 174?181, Madrid.Hoste, Ve?ronique.
2005.
Optimization Issues inMachine Learning of Coreference Resolution.Ph.D.
thesis, Language Technology Group,University of Antwerp.Hu, Minqing and Bing Liu.
2004.
Miningand summarizing customer reviews.
InProceedings of ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining2004 (KDD-2004), pages 168?177,Seattle, WA.Hummel, Robert A. and Steven W. Zucker.1983.
On the foundations of relaxationlabeling processes.
IEEE Transactions onPattern Analysis and Machine Intelligence(PAMI), 5(3):167?187.Joachims, Thorsten.
1999.
Making large-scaleSVM learning practical.
In B. Scholkopf,C.
Burgess, and A. Smola, editors,Advances in Kernel Methods ?
Support VectorLearning, pages 169?184.
MIT Press,Cambridge, MA.Kaji, Nobuhiro and Masaru Kitsuregawa.2006.
Automatic construction ofpolarity-tagged corpus from HTMLdocuments.
In Proceedings of theCOLING/ACL 2006 Main ConferencePoster Sessions, pages 452?459, Sydney.Kamps, Jaap and Maarten Marx.
2002.Words with attitude.
In Proceedings of the1st International Conference on GlobalWordNet, pages 332?341, Mysore.Kanayama, Hiroshi and Tetsuya Nasukawa.2006.
Fully automatic lexicon expansionfor domain-oriented sentiment analysis.In Proceedings of the Conference onEmpirical Methods in Natural LanguageProcessing (EMNLP-2006), pages 355?363,Sydney.Kennedy, Alistair and Diana Inkpen.
2006.Sentiment classification of movie reviewsusing contextual valence shifters.Computational Intelligence, 22(2):110?125.Kim, Soo-Min and Eduard Hovy.
2004.Determining the sentiment of opinions.In Proceedings of the 20th InternationalConference on Computational Linguistics(COLING-2004), pages 1267?1373, Geneva.Koppel, Moshe and Jonathan Schler.
2006.The importance of neutral examples forlearning sentiment.
ComputationalIntelligence, 22(2):100?109.431Computational Linguistics Volume 35, Number 3Maybury, Mark T., editor.
2004.
NewDirections in Question Answering.
AmericanAssociation for Artificial Intelligence,Menlo Park, CA.Morinaga, Satoshi, Kenji Yamanishi, KenjiTateishi, and Toshikazu Fukushima.
2002.Mining product reputations on the Web.In Proceedings of the 8th ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining (KDD-2002),pages 341?349, Edmonton.Mullen, Tony and Nigel Collier.
2004.Sentiment analysis using supportvector machines with diverseinformation sources.
In Proceedingsof the Conference on Empirical Methodsin Natural Language Processing(EMNLP-2004), pages 412?418,Barcelona.Nasukawa, Tetsuya and Jeonghee Yi.2003.
Sentiment analysis: Capturingfavorability using natural languageprocessing.
In Proceedings of the 2ndInternational Conference on KnowledgeCapture (K-CAP 2003), pages 70?77,Sanibel Island, FL.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of theConference on Empirical Methods in NaturalLanguage Processing (EMNLP-2002),pages 79?86, Philadelphia, PA.Polanyi, Livia and Annie Zaenen.
2004.Contextual valence shifters.
InWorkingNotes of the AAAI Spring Symposium onExploring Attitude and Affect in Text:Theories and Applications, pages 106?111,The AAAI Press, Menlo Park, CA.Popescu, Ana-Maria and Oren Etzioni.2005.
Extracting product features andopinions from reviews.
In Proceedingsof the Human Language TechnologiesConference/Conference on EmpiricalMethods in Natural Language Processing(HLT/EMNLP-2005), pages 339?346,Vancouver.Quirk, Randolph, Sidney Greenbaum,Geoffry Leech, and Jan Svartvik.
1985.A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.Riloff, Ellen and Janyce Wiebe.
2003.Learning extraction patterns for subjectiveexpressions.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2003), pages 105?112,Sapporo.Schapire, Robert E. and Yoram Singer.
2000.BoosTexter: A boosting-based system fortext categorization.Machine Learning,39(2/3):135?168.Spertus, Ellen.
1997.
Smokey: Automaticrecognition of hostile messages.
InProceedings of the 8th Annual Conferenceon Innovative Applications of ArtificialIntelligence (IAAI-97), pages 1058?1065,Providence, RI.Stone, Philip J., Dexter C. Dunphy,Marshall S. Smith, and Daniel M. Ogilvie.1966.
The General Inquirer: A ComputerApproach to Content Analysis.
MIT Press,Cambridge, MA.Stoyanov, Veselin, Claire Cardie, andJanyce Wiebe.
2005.
Multi-perspectivequestion answering using the OpQAcorpus.
In Proceedings of the HumanLanguage Technologies Conference/Conference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP-2005),pages 923?930, Vancouver.Suzuki, Yasuhiro, Hiroya Takamura, andManabu Okumura.
2006.
Application ofsemi-supervised learning to evaluativeexpression classification.
In Proceedings ofthe 7th International Conference on IntelligentText Processing and ComputationalLinguistics (CICLing-2006), pages 502?513,Mexico City.Takamura, Hiroya, Takashi Inui, andManabu Okumura.
2005.
Extractingemotional polarity of words using spinmodel.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics (ACL-05), pages 133?140,Ann Arbor, MI.Tong, Richard.
2001.
An operationalsystem for detecting and trackingopinions in online discussions.
InWorking Notes of the SIGIR Workshop onOperational Text Classification, pages 1?6,New Orleans, LA.Turney, Peter.
2002.
Thumbs up or thumbsdown?
Semantic orientation applied tounsupervised classification of reviews.In Proceedings of the 40th Annual Meetingof the Association for ComputationalLinguistics (ACL-02), pages 417?424,Philadelphia, PA.Turney, Peter and Michael L. Littman.
2003.Measuring praise and criticism: Inferenceof semantic orientation from association.ACM Transactions on Information Systems(TOIS), 21(4):315?346.Whitelaw, Casey, Navendu Garg, andShlomo Argamon.
2005.
Using appraisalgroups for sentiment analysis.
InProceedings of the 14th ACM InternationalConference on Information and Knowledge432Wilson, Wiebe, and Hoffmann Recognizing Contextual PolarityManagement (CIKM-2005), pages 625?631,Bremen.Wiebe, Janyce.
1994.
Tracking point of viewin narrative.
Computational Linguistics,20(2):233?287.Wiebe, Janyce, Rebecca Bruce, andThomas O?Hara.
1999.
Developmentand use of a gold standard data setfor subjectivity classifications.
InProceedings of the 37th Annual Meetingof the Association for ComputationalLinguistics (ACL-99), pages 246?253,College Park, MD.Wiebe, Janyce and Rada Mihalcea.2006.
Word sense and subjectivity.In Proceedings of the 21st InternationalConference on Computational Linguisticsand 44th Annual Meeting of theAssociation for Computational Linguistics,pages 1065?1072, Sydney.Wiebe, Janyce and Ellen Riloff.
2005.Creating subjective and objective sentenceclassifiers from unannotated texts.
InProceedings of the 6th InternationalConference on Intelligent Text Processing andComputational Linguistics (CICLing-2005),pages 486?497, Mexico City.Wiebe, Janyce, Theresa Wilson, and ClaireCardie.
2005.
Annotating expressionsof opinions and emotions in language.Language Resources and Evaluation(formerly Computers and the Humanities),39(2/3):164?210.Wilson, Theresa, Janyce Wiebe, and PaulHoffmann.
2005.
Recognizing contextualpolarity in phrase-level sentimentanalysis.
In Proceedings of the HumanLanguage Technologies Conference/Conference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP-2005),pages 347?354, Vancouver.Xia, Fei and Martha Palmer.
2001.Converting dependency structures tophrase structures.
In Proceedings of theHuman Language Technology Conference(HLT-2001), pages 1?5, San Diego, CA.Yi, Jeonghee, Tetsuya Nasukawa, RazvanBunescu, and Wayne Niblack.
2003.Sentiment analyzer: Extracting sentimentsabout a given topic using natural languageprocessing techniques.
In Proceedings of the3rd IEEE International Conference on DataMining (ICDM?03), pages 427?434,Melbourne, FL.Yu, Hong and Vasileios Hatzivassiloglou.2003.
Towards answering opinionquestions: Separating facts from opinionsand identifying the polarity of opinionsentences.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2003), pages 129?136,Sapporo.433
