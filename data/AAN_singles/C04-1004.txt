Discriminative Hidden Markov Modeling with Long State Dependenceusing a kNN EnsembleZHOU GuoDongInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613Email: zhougd@i2r.a-star.edu.sgAbstractThis paper proposes a discriminative HMM(DHMM) with long state dependence (LSD-DHMM) to segment and label sequential data.The LSD-DHMM overcomes the strong contextindependent assumption in traditional generativeHMMs (GHMMs) and models the sequential datain a discriminative way, by assuming a novelmutual information independence.
As a result, theLSD-DHMM separately models the long statedependence in its state transition model and theobservation dependence in its output model.
Inthis paper, a variable-length mutual information-based modeling approach and an ensemble ofkNN probability estimators are proposed tocapture the long state dependence and theobservation dependence respectively.
Theevaluation on shallow parsing shows that theLSD-DHMM not only significantly outperformsGHMMs but also much outperforms otherDHMMs.
This suggests that the LSD-DHMM caneffectively capture the long context dependence tosegment and label sequential data.1.
IntroductionA Hidden Markov Model (HMM) is a modelwhere a sequence of observations is generated inaddition to the Markov state sequence.
It is a latentvariable model in the sense that only theobservation sequence is known while the statesequence remains ?hidden?.
In recent years,HMMs have enjoyed great success in manytagging applications, most notably part-of-speech(POS) tagging (Church 1988; Weischedel et al1993; Merialdo 1994) and named entityrecognition (Bikel et al1999; Zhou et al2002).Moreover, there have been also efforts to extendthe use of HMMs to word sense disambiguation(Segond et al1997) and shallow/full parsing(Brants et al1997; Skut et al1998; Zhou et al2000).Traditionally, a HMM segments and labelssequential data in a generative way, assigning ajoint probability to paired observation and statesequences.
More formally, a generative (first-order)HMM (GHMM) is given by a finite set of statesincluding an designated initial state and andesignated final state, a set of possible observation, two conditional probability distributions: astate transition model from s  to ,  forand an output model,  forSOs ,'o' s )|( 'ssp)|( sopSs?sO S??
,)| 'ss.
A sequence of observations isgenerated by starting from the designated initialstate, transmiting to a new state according to, emitting an observation selected by thatnew state according to p , transmiting toanother new state and so on until the designatedfinal state is generated.
(p)( s|oThere are several problems with this generativeapproach.
First, many tasks would benefit from aricher representation of observations?in particulara representation that describes observations interms of many overlapping features, such ascapitalization, word endings, part-of-speech inaddition to the traditional word identity.
Note thatthese features always depends on each other.Furthermore, to define a joint probability over theobservation and state sequences, the generativeapproach needs to enumerate all the possibleobservation sequences.
However, in some tasks,the set of all the possible observation sequences isnot reasonably enumerable.
Second, the generativeapproach fails to effectively model the dependencein the observation sequence.
Moreover, it isdifficult for the generative approach to model thelong state dependence since it is not reasonablypractical for ngram modeling(e.g.
bigram for thefirst-order GHMM and trigram for the secnod-order GHMM) to be beyond trigram.
Third, thegenerative approach normally estimates theparameters to maximize the likelihood of theobservation sequence.
However, in many NLPtasks, the goal is to predict the state sequence giventhe observation sequence.
In other words, thegenerative approach inappropriately applies agenerative joint probability model for a conditionalprobability problem.
In summary, the main reasonsbehind these problems of the generative approachare the strong context independent assumption andthe generative nature in modeling sequential data.While the dependence between successive statescan be directly modeled by its state transitionmodel, the generative approach fails to directlycapture the observation dependence in the outputmodel.
From this viewpoint, a GHMM can be alsocalled an observation independent HMM.To resolve above problems in GHMMs, someresearches have been done to move from  thegenerative approach to the discriminative approach.Discriminative HMMs (DHMMs) do not expendmodeling effort on the observation sequnce, whichare fixed at test time.
Instead, DHMMs model thestate sequence depending on arbitrary, non-independent features of the observation sequence,normally without forcing the model to account forthe distribution of those dependencies.
Punyakanokand Roth (2000) proposed a projection-basedDHMM (PDHMM) which represents theprobability of a state transition given not only thecurrent observation but also past and futureobservations and used the SNoW classifier (Roth1998, Carlson et al1999) to estimate it (SNoW-PDHMM thereafter).
McCallum et al(2000)proposed the extact same model and usedmaximum  entropy to estimate it (ME-PDHMMthereafter).
Lafferty et al(2001) extanded ME-PDHMM using conditional random fields byincorporating the factored state representation ofthe same model (that is, representing theprobability of a state given the observationsequence and the previous state)  to alleviate thelabel bias problem in projection-based DHMMs,which can be biased towards states with fewsuccessor states (CRF-DHMM thereafter).
Similarwork can also be found in Bouttou (1991).Punyakanok and Roth (2000) also proposed a non-projection-based DMM which separates thedependence of a state on the previous state and theobservation sequence, by rewriting the GHMM ina discriminative way and heuristically extendingthe notation of an observation to the observationsequence.
Zhou et al(2000) systematically derivedthe exact same model as in Punyakanok and Roth(2000) and used back-off modeling to esimate theprobability of a state given the observationsequence (Backoff-DHMM thereafter) whilePunyakanok and Roth (2000) used the SNoWclassifier to estimate it(SNoW-DHMM thereafter).This paper follows our previous work in Zhouet al(2000) and proposes an alternative non-projection-based DHMM with long statedependence (LSD-DHMM), which separates thedependence of a state on the previous states andthe observation sequence.
Moreover, a variable-length mutual information based modelingapproach (VLMI) is proposed to capture the longstate dependence of a state on the previous states.In addition, an ensemble of kNN probabilityestimators is proposed to capture the observationdependence of a state on the observation sequence.Experimentation shows that VLMI effectivelycaptures the long state dependence.
It also showsthat the kNN ensemble captures the dependencebetween the features of the observation sequencemore effectively than classifier-based approaches,by forcing the model to account for the distributionof those dependencies.The layout of this paper is as follows.
Section 2first proposes the LSD-DHMM and then presentsthe VLMI to capture the long state dependence.Section 3 presents the kNN probability estimator tocapture the observation dependence while Section4 presents the kNN ensemble.
Section 5 introducesshallow parsing, while experimental results aregiven in Section 6.
Finally, some conclusion willbe drawn in Section 7.2.
LSD-DHMM: Discriminative HMM withLong State DependenceIn principle, given an observation sequence, the goal of a conditionalprobability model is to find a stochastic optimalstate sequence s  that maximizesnn oooo L211 =)|(log 11nn ospnn sss L211 =)|(logmaxarg 11*1nnsospSn=                 (1)By applying the Bayes?
rule, we can rewrite theequation (1) as:}),()({logmaxarg)}|({logmaxarg11111*11nnnsnnsosMIspospsnn+==(2)Obviously, the second term MIcaptures the mutual information between the statesequence  and the observation sequence o .
Tocompute  efficiently, we propose anovel mutual information independenceassumption:),( 11nn osn1ns1MI ),( 11nn os?==nininn osMIosMI1111 ),(),( or?==?ninnnnpopsposp11111 log)()(),(log ?
niniopsosp11)()(),((3)That is, we assume a state is only dependent onthe observation sequence o  and independent onother states in the state sequence s .
Thisassumption is reasonable because the dependenceamong the states in the state sequence  has beenn1n1n1sdirectly captured by the first term log inequation (2).
)( 1nsp|(log)(log})|()(1211?=nininiospspossp?is 11 )By applying the assumption (3) into theequation (2) and using the chain rule, we have:})),({maxarg})|(log)|(log{maxarglog)(loglog)|(log{maxarg12111121111211*111??????
?==?==?===?+=+?=+?+=niniiisniniiniiisniniiniiisssMIospssppspsspsnnn(4)The above model consists of two models: thestate transition model ?
whichmeasures the state dependence of a state given theprevious states, and the output modelwhich measures the observationdependence of a state given the observationsequence in a discriminative way.
Therefore, wecall the above model as in equation (4) adiscriminative HMM (DHMM) with long statedependence (LSD-DHMM).
The LSD-DHMMseparates the dependence of a state on the previousstates and the observation sequence.
The maindifference between a GHMM and a LSD-DHMMlies in their output models in that the output modelof a LSD-DHMM directly captures the contextdependence between successive observations indetermining the ?hidden?
states while the outputmodel of the GHMM fails to do so.
That is, theoutput model of a LSD-DHMM overcomes thestrong context independent assumption in theGHMM and becomes observation contextdependent.
Therefore, the LSD-DHMM can alsobe called an observation context dependent HMM.Compared with other DHMMs, the LSD-DHMMexplicitly models the long state dependence andthe non-projection nature of the LSD-DHMMalleviates the label bias problem inherent inprojection-based DHMMs.=niisMI2,(?=nini osp11 )|(logComputation of a LSD-DHMM consists of twoparts.
The first is to compute the state transitionmodel: .
Traditionally, ngrammodeling(e.g.
bigram for the first-order GHMMand trigram for the second-order GHMM) is usedto estimate the state transition model.
However,such approach fails to capture the long statedependence since it is not reasonably practical forngram modeling to be beyond trigram.
In thispaper, a variable-length mutual information-basedmodeling approach (VLMI) is proposed as follow:For each i?=?niii ssMI211 ),()2( ni ??
, we first find a minimal)i0( kk p?
where the frequency of  s  isbigger than a threshold (e.g.
10) and then estimateusing1?ik)1?,( 1ii ssMI ))((),( 1?
?= iikiki pspspss))| 1no)| ii EsNio +()1?iksMIni osp 1|(log( is?
)|( 1ni os (piNi oo ?.In this way, the long state dependence can becaptured maximally in a dynamical way.
Here, thefrequencies of variable-length state sequences areestimated using the simple Good-Turing approach(Gale et al1995).
?=ni 1piE = LL)|( iE?The second is to estimate the outputmodel: .
Ideally, we would havesufficient training data for every event whoseconditional probability we wish to calculate.Unfortunately, there is rarely enough training datato compute accurate probabilities when decodingon new data.
Traditionally, there are two existingapproaches to resolve this problem: linearinterpolation (Jelinek 1989) and back-off (Katz1987).
However, these two approaches only workwell when the number of different informationsources is limited.
When a long context isconsidered, the number of different informationsources is exponential and not reasonablyenumerable.
The current tendency is to recast it asa classification problem and use the output of aclassifier, e.g.
the maximum entropy classifier(Ratnaparkhi 1999) to estimate the state probabilitydistribution given the observation sequence.
In thenext two sections, we will propose a more effectiveensemble of kNN probability estimators to resolvethis problem.3.
kNN Probability EstimatorThe main challenge for the LSD-DHMM is how toreliably estimate p  in its output model.For efficiency, we can alwaysassume , where the patternentry .
That is, we onlyconsider the observation dependence in a windowof 2N+1 observations (e.g.
we only consider thecurrent observation, the previous observation andthe next observation when N=1).
For convenience,we denote P  as the conditional stateprobability distribution of the states given E  and i)|( ii Espis iE)|( iEP ?
)(EkNN i)|( iEP ?FrequentEnFrequentEn|(?
kNNEp kias the conditional state probability ofgiven .==?
iEP )|(The kNN probability estimator estimatesby first finding the K nearest neighborsof frequently occurring pattern entriesand thenaggregating them to make a proper estimation of.
Here, the conditional state probabilitydistribution is estimated instead of theclassification in a traditional kNN classifier.
To doso, all the frequently occurring pattern entries areextracted from the training corpus in an exhaustiveway and stored in a dictionary.
In order to limit thedictionary size and keep efficiency, we constrain avalid set of pattern entry forms ValidEntryto consider only the most informative informationsources.
Generally, ValidEntry  can bedetermined manually or automatically according tothe applications.
In Section 5, we will give anexample.
},...,2,1|{ KkE ki =arytryDictionFormFormGiven a pattern entry E  and a dictionary offrequently occurring pattern entries, a simple algorithm isapplied to find the K nearest neighbors of thepattern entry  from the dictionary as follows:iarytryDictioniE?
compare  with each entry in the dictionaryand find all the compatible entriesiE?
compute the cosine similarity between E  andeach of the compatible entriesi?
sort out the K nearest neighbors according totheir cosine similaritiesFinally, the conditional state probabilitydistribution of the pattern entry is aggregated overthose of its K nearest neighbors weighted by theirfrequencies and cosine similarities:)( kiEf)??==????KkkikiKkkikikiEfkNNEpEPEfkNNEp11)()|(?)|()()|(?
(5) p4.
kNN EnsembleIn the literature, an ensemble has been widely usedin the classification problem to combine severalclassifiers (Breiman 1996; Hamamoto 1997;Dietterich 1998; Zhou Z.H.
et al2002; Kim et al2003).
It is well known that an ensemble oftenoutperforms the individual classifiers that make itup (Hansen et al1990).In this paper, an ensemble of kNN probabilityestimators is proposed to estimate the conditionalstate probability distribution P  instead ofthe classification.
This is done through a baggingtechnique (Breiman 1996) to aggregate severalkNN probability estimators.
In bagging, the MkNN probability estimators in the ensemble)|( iE?
}M,...,2,1|{ mkNNENS m == are trainedindependently via a bootstrap technique and thenthey are aggregated via an appropriate aggregationmethod.
Usually, we have a single training set andneed M training sample sets to construct a kNNensemble with M independent kNN probabilityestimators.
From the statistical viewpoint, we needto make the training sample sets different as muchas possible in order to obtain a higher aggregationperformance.
For doing this, we often use thebootstrap technique which builds M replicate datasets by randomly re-sampling with replacementfrom the given training set repeatedly.
Eachexample in the given training set may appearrepeatedly or not at all in any particular replicatetraining sample set.
Each training sample set isused to train a certain kNN probability estimator.Finally, the conditional state probabilitydistribution of the pattern entry E  is averagedover those of the M kNN probability estimators inthe ensemble:iMkNNEPEPMmmii?=?=?
1),|()|(               (6)5.
Shallow ParsingIn order to evaluate the LSD-DHMM and theproposed variable-length mutual informationmodeling approach for the long state dependencein the state transition model and the kNN ensemblefor the observation dependence in the output model,we have applied it in the application of shallowparsing.For shallow parsing, we have o , whereis the word sequence andis the part-of-speech (POS)sequence, while the ?hidden?
states are representedas structural tags to bracket and differentiatevarious categories of phrases.
The basic idea ofusing the structural tags to represent the ?hidden?states is similar to Skut et al(1998) and Zhou et al(2000).
Here, a structural tag consists of threeparts:ii wp=1nn wwww L211 =nn ppp L211 =?
Boundary Category (BOUNDARY): it is a setof four values: ?O?/?B?/?M?/?E?, where ?O?means that current word is a whOle phrase and?B?/?M?/?E?
means that current word is at theBeginning/in the Middle/at the End of a phrase.?
Phrase Category (PHRASE): it is used todenote the category of the phrase.?
Part-of-Speech (POS): Because of the limitednumber of boundary and phrase categories, thePOS is added into the structural tag to representmore accurate state transition and outputmodels.For example, given the following POS taggedsentence as the observation sequence:He/PRP  reckons/VBZ  the/DT  current/JJaccount/NN  deficit/NN  will/MD  narrow/VBto/TO  only/RB $/$  1.8/CD  billion/CD  in/INSeptember/NNP  ./.We can have a corresponding sequence ofstructural tags as the ?hidden?
state sequence:O_NP_PRP(He/PRP)  O_VP _VBZ(reckons/VBZ)  B_NP _DT (the/DT)  M_NP _JJ(current/JJ)  M_NP _NN (account/NN)  E_NP_NN (deficit/NN)  B_VP _MD (will/MD)  E_VP_VB (narrow/VB)  O_PP _TO (to/TO)  B_QP _RB(only/RB)  M_QP _$ ($/$)  M_QP _CD (1.8/CD)E_QP _CD (billion/CD)  O_PP _IN (in/IN)  O_NP_NNP(September/NNP)  O_O _.
(./.
)and an equivalent phrase chunked sentence as theshallow parsing result:[NP He/PRP] [VP reckons/VBZ] [ NP the/DTcurrent/JJ account/NN deficit/NN] [VP will/MDnarrow/VB] [PP to/TO] [QP only/RB $/$ 1.8/CDbillion/CD] [PP in/IN] [NP September/NNP] [O ./.]6.
ExperimentationThe corpus used in shallow parsing is extractedfrom the PENN TreeBank (Marcus et al 1993) of1 million words (25 sections) by a programprovided by Sabine Buchholz from TilburgUniversity.
All the evaluations are 5-fold cross-validated.
For shallow parsing, we use the F-measure to measure the performance.
Here, the F-measure is the weighted harmonic mean of theprecision (P) and the recall (R):PRRP++= 22 )1(?
?Fwith =1 (Rijsbergen 1979), where the precision(P) is the percentage of predicted phrase chunksthat are actually correct and the recall (R) is thepercentage of correct phrase chunks that areactually found.2?Tables 1, 2 and 3 show the detailedperformance of LSD-DHMMs.
In this paper, thevalid set of pattern entry forms ValidEntryis defined to include those pattern entry formswithin a windows of 7  observations(includingcurrent, left 3 and right 3 observations) where forto be included in a pattern entry, all or one ofthe overlapping features in each ofFormjw,p j )(...,1 ijpp ij ?+  or )(...,, 1 jipp jipi ?+)(..., 11 jipp ji p?+should be included in the same pattern entry whilefor  to be included in a pattern entry, all or oneof the overlapping features in each oforshould be included in the same pattern entry.jp(...,, 21 ijpp ij p+ )p j+ ,piTable 1 shows the effect of different number ofnearest neighbors in the kNN probability estimatorand considered previous states in the variable-length mutual information modeling approach ofthe LSD-DHMM, using only one kNN probabilityestimator in the ensemble to estimate inthe output model.
It shows that finding 3 nearestneighbors in the kNN probability estimatorperforms best.
It also shows that further increasingthe number of nearest neighbors does not increaseor even decrease the performance.
This may be dueto introduction of noisy neighbors when thenumber of nearest neighbors increases.
Moreover,Table 1 shows that the LSD-DHMM performs bestwhen six previous states is considered in thevariable-length mutual information-basedmodeling approach and further considering moreprevious states only slightly increase theperformance.
This suggests that the statedependence exists well beyond traditional ngrammodeling (e.g.
bigram and trigram) to six previousstates and the variable-length mutual information-based modeling approach can capture the longstate dependence.
In the following experimentation,we will only use the LSD-DHMM with 3 nearestneighbors used in the kNN probability estimatorand 6 previous states considered in the variable-length mutual information modeling approach.
)|( 1ni ospTable 2 shows the effect of different number ofkNN probability estimators in the ensemble.
Itshows that 15 bootstrap replicates are enough forthe k-NN ensemble on shallow parsing andincrease the F-measure by 0.71 compared with theensemble of only one kNN probability estimator.Table 3 compares the LSD-DHMM withGHMMs and other DHMMs.
It shows that all theDHMMs significantly outperform GHMMs due tothe modeling of the observation dependence andallowing for non-independent, difficult toenumerate observation features.
It also shows thatour LSD-DHMM much outperforms otherDHMMs due to the modeling of the long statedependence using the variable-length mutualinformation-based modeling approach in the LSD-DHMM.
Moverover, Table 3 shows that no-projection-based DHMMs (i.e.
CRF-DHMM,SNoW-DHMM, Backoff-DHMM and LSD-DHMM) outperform projection-based DHMMs.
Itmay be  due to alleviation of the label bias probleminherent in the projection-based DHMMs.
Finally,Table 2 also compares the kNN ensemble withpopular classifier-based approaches, such asSNoW and Maximum Entropy, in estimating theoutput model of the LSD-DHMM.
It shows thatthe kNN ensemble outperforms these classifier-based approaches.
This suggests that the kNNensemble captures the dependence between thefeatures of the observation sequence moreeffectively by forcing the model to account for thedistribution of those dependencies.Table 1: Effect of different numbers of nearest neighbors in the kNN probability estimator and previousstates considered in the variable-length mutual information modeling approach of the LSD-DHMMs, usingonly a probability estimator in the ensembleNumber of nearest neighbors Shallow Parsing1 2 3 4 51 93.12 93.50 93.76 93.70 93.662 93.65 93.82 94.23 94.19 94.124 93.90 94.15 94.42 94.38 94.356 94.12 94.28 94.53 94.54 94.51 Number ofconsideredpreviousstates8 94.15 94.35 94.55  94.52 94.50Table 2: The Effect of different number of kNNprobability estimators in the ensemble on shallowparsingNumber of kNN probabilityestimators in the ensembleF-measure1 94.532 94.774 94.938 95.0614 95.2115 95.2416 95.2420 95.2525 95.2528 95.36Table 3: Comparison of LSD-DHMMs withGHMMs and other DHMMsModels FFirst order 92.14 GHMMsSecond order 92.41ME-PDMM 93.26CRF-DMM 94.04SNoW-PDMM 93.44SNoW-DMM 94.12Backoff-DMM 93.68LSD-DMM(Ensemble) 95.24LSD-DMM(ME) 94.25DHMMsLSD-DMM(SNoW) 94.417.
ConclusionHidden Markov Models (HMMs) are a powerfulprobabilistic tool for modeling sequential data andhave been applied with success to many text-related tasks, such as shallow paring.
In these cases,the observations are usually modified asmultinomial distributions over a discrete dictionaryand the HMM parameters are set to maximize thelikelihood of the observations.
This paper presentsa discriminative HMM with long state dependencethat allows observations to be represented asarbitrary overlapping features and defines theconditional probability of the state sequence giventhe observation sequence.
It does so by assuming anovel mutual information independence to separatethe dependence of a state given the observationsequence and the previous states.
Finally, the longstate dependence and the observation dependencecan be effectively captured by a variable-lengthmutual information model and a kNN ensemblerespectively.In future work, we will explore our model inother applications, such as full parsing.ReferencesBikel D.M., Schwartz R. & Weischedel R.M.(1999).
An Algorithm that Learns What's in aName.
Machine Learning (Special Issue on NLP).34(3): 211-231.Bottou L. (1991).
Une approche theorique del?apprentissage connexionniste: Applications a lareconnaissance de la parole.
Doctoraldissertation, Universite de Paris XI.Brants T., Skut W., & Krenn B.
(1997).
TaggingGrammatical Functions.
Proceedings of theConference on Empirical Methods on NaturalLanguage Processing (EMNLP?1997).
BrownUniv.
RI.Carlson A, Cumby C. Rosen J. and Roth D. 1999.The SNoW learning architecture.
TechinicalReport UIUCDCS-R-99-2101.
UIUC ComputerScience Department.Church K.W.
(1998).
A Stochastic Pars Programand Noun Phrase Parser for Unrestricted Text.Proceedings of the Second Conference onApplied Natural Language Processing(ANLP?1998).
Austin, Texas.Fausett L. (1994).
Fundamentals of neuralnetworks.
Prentice Hall Press.Gale W.A.
and Sampson G. 1995.
Good-Turingfrequency estimation without tears.
Journal ofQuantitative Linguistics.
2:217-237.Jelinek F. (1989).
Self-Organized LanguageModeling for Speech Recognition.
In AlexWaibel and Kai-Fu Lee(Editors).
Readings inSpeech Recognitiopn.
Morgan Kaufmann.
450-506.Katz S.M.
(1987).
Estimation of Probabilities fromSparse Data for the Language Model Componentof a Speech Recognizer.
IEEE Transactions onAcoustics.
Speech and Signal Processing.
35:400-401.Lafferty J. McCallum A and Pereira F. (2001).Conditional random fields: probabilistic modelsfor segmenting and labeling sequence data.ICML-20.Marcus M., Santorini B.
& Marcinkiewicz M.A.(1993).
Buliding a large annotated corpus ofEnglish: The Penn Treebank.
ComputationalLinguistics.
19(2):313-330.McCallum A. Freitag D. and Pereira F. 2000.Maximum entropy Markov models forinformation extraction and segmentation.
ICML-19.
591-598.
Stanford, California.Merialdo B.
(1994).
Tagging English Text with aProbabilistic Model.
Computational Linguistics.20(2): 155-171.Punyakanok V. and Roth D. (2000).The Use of Classifiers in Sequential InferenceNIPS-13.Rabiner L.R.
(1989).
A Tutorial on HiddenMarkov Models and Selected Applications inSpeech Recognition?.
Proceedings of the IEEE,77(2): 257-286.Ratnaparkhi A.
1999.
Learning to parsing naturallanguage with maximum entropy models.Machine Learning.
34:151-175.Roth D. 1998.
Learning to resolve natural languageambiguities: A unified approach.
In Proceedingsof the National Conference on ArtificialIntelligence.
806-813.Segond F., Schiller A., Grefenstette & Chanod F.P.(1997).
An Experiment in Semantic Taggingusing Hidden Markov Model Tagging.Proceedings of the Joint ACL/EACL workshop onAutomatic Information Extraction and Buildingof Lexical Semantic Resources.
pp.78-81.
Madrid,Spain.Skut W. & Brants T. (1998).
Chunk Tagger ?Statistical Recognition of Noun Phrases.Proceedings of the ESSLLI?98 workshop onAutomatic Acquisition of Syntax and Parsing.Univ.
of Saarbrucken.
Germany.van Rijsbergen C.J.
(1979).
Information Retrieval.Buttersworth, London.Viterbi A.J.
(1967).
Error Bounds forConvolutional Codes and an AsymptoticallyOptimum Decoding Algorithm.
IEEETransactions on Information Theory.
13: 260-269.Weischedel R., Meteer M., Schwartz R., RamshawL.
& Palmucci J.
(1993).
Coping  with Ambiguityand Unknown Words through ProbabilisticMethods.
Computational Linguistics.
19(2): 359-382.Zhou GuoDong & Su Jian, (2000).
Error-drivenHMM-based Chunk Tagger with Context-Dependent Lexicon.
Proceedings of the JointConference on Empirical Methods on NaturalLanguage Processing and Very Large Corpus(EMNLP/ VLC'2000).
Hong Kong.Zhou GuoDong & Su Jian.
(2002).
Named EntityRecognition Using a HMM-based Chunk Tagger,Proceedings of the Conference on AnnualMeeting for Computational Linguistics(ACL?2002).
473-480, Philadelphia.
