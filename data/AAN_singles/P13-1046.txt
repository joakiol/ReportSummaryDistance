Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 466?475,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDiscriminative state tracking for spoken dialog systemsAngeliki Metallinou1?, Dan Bohus2, and Jason D. Williams21University of Southern California, Los Angeles, CA, USA2Microsoft Research, Redmond, WA, USAmetallin@usc.edu dbohus@microsoft.com jason.williams@microsoft.comAbstractIn spoken dialog systems, statistical statetracking aims to improve robustness tospeech recognition errors by tracking aposterior distribution over hidden dialogstates.
Current approaches based on gener-ative or discriminative models have differ-ent but important shortcomings that limittheir accuracy.
In this paper we discussthese limitations and introduce a new ap-proach for discriminative state trackingthat overcomes them by leveraging theproblem structure.
An offline evaluationwith dialog data collected from real usersshows improvements in both state track-ing accuracy and the quality of the pos-terior probabilities.
Features that encodespeech recognition error patterns are par-ticularly helpful, and training requires rel-atively few dialogs.1 IntroductionSpoken dialog systems interact with users via nat-ural language to help them achieve a goal.
As theinteraction progresses, the dialog manager main-tains a representation of the state of the dialogin a process called dialog state tracking.
For ex-ample, in a bus schedule information system, thedialog state might indicate the user?s desired busroute, origin, and destination.
Dialog state track-ing is difficult because automatic speech recog-nition (ASR) and spoken language understand-ing (SLU) errors are common, and can cause thesystem to misunderstand the user?s needs.
Atthe same time, state tracking is crucial becausethe system relies on the estimated dialog state tochoose actions ?
for example, which bus scheduleinformation to present to the user.The dialog state tracking problem can be for-malized as follows (Figure 1).
Each system turnin the dialog is one datapoint.
For each datapoint,the input consists of three items: a set of K fea-tures that describes the current dialog context, Gdialog state hypotheses, and for each dialog statehypothesis, M features that describe that dialogstate hypothesis.
The task is to assign a probabil-ity distribution over the G dialog state hypotheses,plus a meta-hypothesis which indicates that noneof the G hypotheses is correct.Note that G varies across turns (datapoints) ?for example, in the first turn of Figure 1, G = 3,and in the second and third turns G = 5.
Alsonote that the dialog state tracker is not predictingthe contents of the dialog state hypotheses; the di-alog state hypotheses contents are given by someexternal process, and the task is to predict a proba-bility distribution over them, where the probabilityassigned to a hypothesis indicates the probabilitythat it is correct.
It is a requirement that the Ghypotheses are disjoint; with the special ?every-thing else?
meta-hypothesis, exactly one hypoth-esis is correct by construction.
After the dialogstate tracker has output its distribution, this distri-bution is passed to a separate, downstream processthat chooses what action to take next (e.g., how torespond to the user).Dialog state tracking can be seen an analogousto assigning a probability distribution over itemson an ASR N-best list given speech input and therecognition output, including the contents of theN-best list.
In this task, the general features de-scribe the recognition overall (such as length ofutterance), and the hypothesis-specific features de-scribe each N-best entry (such as decoder cost).?
Work done while at Microsoft Research466Another analogous task is assigning a probabil-ity distribution over a set of URLs given a searchquery and the URLs.
Here, general features de-scribe the whole set of results, e.g., number ofwords in the query, and hypothesis-specific fea-tures describe each URL, e.g., the fraction ofquery words contained in page.For dialog state tracking, most commercial sys-tems use hand-crafted heuristics, selecting theSLU result with the highest confidence score,and discarding alternatives.
In contrast, statisti-cal approaches compute a posterior distributionover many hypotheses for the dialog state.
Thekey insight is that dialog is a temporal process inwhich correlations between turns can be harnessedto overcome SLU errors.
Statistical state track-ing has been shown to improve task completionin end-to-end spoken dialog systems (Bohus andRudnicky (2006); Young et al (2010); Thomsonand Young (2010)).Two types of statistical state tracking ap-proaches have been proposed.
Generative ap-proaches (Horvitz and Paek (1999); Williams andYoung (2007); Young et al (2010); Thomson andYoung (2010)) use generative models that capturehow the SLU results are generated from hiddendialog states.
These models can be used to trackan arbitrary number of state hypotheses, but can-not easily incorporate large sets of potentially in-formative features (e.g.
from ASR, SLU, dialoghistory), resulting in poor probability estimates.As an illustration, in Figure 1, a generative modelmight fail to assign the highest score to the correcthypothesis (61C) after the second turn.
In contrast,discriminative approaches use conditional mod-els, trained in a discriminative fashion (Bohus andRudnicky (2006)) to directly estimate the distribu-tion over a set of state hypotheses based on a largeset of informative features.
They generally pro-duce more accurate distributions, but in their cur-rent form they can only track a handful of state hy-potheses.
As a result, the correct hypothesis maybe discarded: for instance, in Figure 1, a discrim-inative model might consider only the top 2 SLUresults, and thus fail to consider the correct 61Chypothesis at all.The main contribution of this paper is to de-velop a new discriminative model for dialog statetracking that can operate over an arbitrary numberof hypotheses and still compute accurate proba-bility estimates.
We also explore the relative im-portance of different feature sets for this task, andmeasure the amount of data required to reliablytrain our model.2 Data and experimental designWe use data from the public deployment of twosystems in the Spoken Dialog Challenge (Blacket al (2010)) which provide bus schedule infor-mation for Pittsburgh, USA.
The systems, DS1and DS2, were fielded by AT&T, and are de-scribed in Williams et al (2010) and Williams(2012).
Both systems followed a highly directedflow, separately collecting 5 slots.
All users wereasked for their bus route, origin, and destination;then, they were optionally prompted for a date andtime.
Each slot was explicitly or implicitly con-firmed before collecting the next.
At the end, bustimes were presented.
The two systems differed inacoustic models, confidence scoring model, statetracking method and parameters, number of sup-ported routes (8 vs 40, for DS1 and DS2 respec-tively), presence of minor bugs, and user popu-lation.
These differences yield distinctions in thedistributions in the two corpora (Williams (2012)).In both systems, a dialog state hypothesis con-sists of a value of the user?s goal for a certainslot: for example, a state hypothesis for the originslot might be ?carnegie mellon university?.
Thenumber G of state hypotheses (e.g.
slot values)observed so far depends on the dialog, and turnwithin that dialog.
For instance, in Fig.
1, G pro-gressively takes values 3, 5 and 5.
Dialog statehypotheses with identical contents (e.g., the samebus route) are merged.
The correctness of the SLUresults was manually labeled by professional an-notators.2.1 Experimental setupTo perform a comparative analysis of various statetracking algorithms, we test them offline, i.e., byre-running state tracking against the SLU resultsfrom deployment.
However, care must be taken:when the improved state-tracker is installed into adialog system and used to drive action selection,the distribution of the resulting dialog data (whichis an input for the state tracker) will change.
Inother words, it is known a priori that the trainand test distributions will be mismatched.
Hence,when conducting offline experiments, if train andtest data were drawn from the same matched dis-tribution, this may overstate performance.467Figure 1: Overview of dialog state tracking.
In this example, the dialog state contains the user?s desiredbus route.
At each turn, the system produces a spoken output.
The user?s spoken response is processedto extract a set of spoken language understanding (SLU) results, each with a local confidence score.
Aset of G dialog state hypotheses is formed by considering all SLU results observed so far, includingthe current turn and all previous turns.
For each state hypothesis, a feature extractor produces a set ofM hypothesis-specific features, plus a single set of K general features that describes the current dialogcontext.
The dialog state tracker uses these features to produce a distribution over theG state hypotheses,plus a meta-hypothesis rest which accounts for the possibility that none of the G hypotheses are correct.dataset train set test setMATCH1 half calls from DS2 remaining calls in DS2MATCH2 half calls from DS1,half from DS2remaining calls fromDS1 and DS2MISMATCH all calls from DS1 all calls from DS2Table 1: Train-test data splitsTo account for this effect, we explicitly studytrain/test mismatch through three partitions of datafrom DS1 and DS2 (see Table 1): MATCH1 con-tains matched train/test data from the DS2 dataset;MATCH2 contains matched train/test data fromboth datasets; finally, MISMATCH contains mis-matched train/test data.
While the MISMATCHcondition may not identically replicate the mis-match observed from deploying a new state trackeronline (since online characteristics depend on userbehavior) training on DS1 and testing on DS2 atleast ensures the presence of some real-world mis-match.We assess performance via two metrics: accu-racy and L2 norm.
Accuracy indicates whether thestate hypothesis with the highest assigned proba-bility is correct, where rest is correct iff none ofthe SLU results prior to the current turn include theuser?s goal.
High accuracy is important as a dialogsystem must ultimately commit to a single inter-pretation of the user?s needs ?
e.g., it must committo a route in order to provide bus timetable infor-mation.
In addition, the L2 norm (or Brier score,Murphy (1973)) also captures how well calibratedthe output probabilities are, which is crucial to de-cision theoretic methods for action selection.
TheL2 norm is computed between the output poste-rior and the ground-truth vector, which has 1 inthe position of the correct item and 0 elsewhere.Both metrics are computed for each slot in eachturn, and reported by averaging across all turnsand slots.4682.2 Hand-crafted baseline state trackerAs a baseline, we construct a hand-crafted statetracking rule that follows a strategy common incommercial systems: it returns the SLU resultwith the maximum confidence score, ignoring allother hypotheses.
Although this is very a simplerule, it is very often effective.
For example, if theuser says ?no?
to an explicit confirmation or ?goback?
to an implicit confirmation, they are askedthe same question again, which gives an opportu-nity for a higher confidence score.
Of the G pos-sible hypotheses for a slot, we denote the numberactually assigned a score by a model as G?, so inthis heuristic baseline G?
= 1.The performance of this baseline (BASELINEin Table 3) is relatively strong because the topSLU result is by far most likely to be correct, andbecause the confidence score was already trainedwith slot-specific speech data (Williams and Bal-akrishnan (2009), Williams (2012)).
However,this simple rule can?t make use of SLU results onthe N-best list, or statistical priors; these limita-tions motivate the use of statistical state trackers,introduced next.3 Generative state trackingGenerative state tracking approaches leveragemodels that describe how SLU results are gener-ated from a hidden dialog state, denoted g. Theuser?s true (unobserved) action u is conditioned ong and the system action a via a user action modelP (u|g, a), and also on the observed SLU resultu?
via a model of how SLU results are generatedP (u?|u).
Given a prior distribution b(g) and a re-sult u?, an updated distribution b?
(g) can be com-puted by summing over all hidden user actions u:b?
(g) = ?
?uP (u?|u) ?
P (u|g, a)b(g) (1)where ?
is a normalizing constant (Williams andYoung (2007)).
Generative approaches model theposterior over all possible dialog state hypotheses,including those not observed in the SLU N-bestlists.
In general this is computationally intractablebecause the number of states is too large.
One ap-proach to scaling up is to group g into a few par-titions, and to track only states suggested by ob-served SLU results (Young et al (2010); Williams(2010); Gas?ic?
and Young (2011)).
Another ap-proach is to factor the components of a dialogstate, make assumptions about conditional inde-pendence between the components, and apply ap-proximate inference techniques such as loopy be-lief propagation (Thomson and Young (2010)).In deployment, DS1 and DS2 used the AT&TStatistical Dialog Toolkit (ASDT) for dialog statetracking (Williams (2010); AT&T Statistical Dia-log Toolkit).
ASDT implements a generative up-date of the form of Eq 1, and uses partitions tomaintain tractability.
Component models werelearned from dialog data from a different dia-log system.
A maximum of G?
= 20 state hy-potheses were tracked for each slot.
The per-formance (GENONLINE in Table 3), was worsethan BASELINE: an in-depth analysis attributedthis to the mismatch between train and test datain the component models, and to the underlyingflawed assumption of eq.
1 that observations atdifferent turns are independent conditioned on thedialog state ?
in practice, confusions made byspeech recognition are highly correlated (Williams(2012)).For all datasets, we re-estimated the models onthe train set and re-ran generative tracking withan unlimited number of partitions (i.e., G?
= G);see GENOFFLINE in Table 3.
The re-estimatedtracker improved accuracy in MATCH conditions,but degraded accuracy in the MISMATCH condi-tion.
This can be partly attributed to the difficultyin estimating accurate initial priors b(g) for MIS-MATCH, where the bus route, origin, and destina-tion slot values in train and test systems differedsignificantly.4 Discriminative State Tracking:Preliminaries and existing workIn contrast to generative models, discriminativeapproaches to dialog state tracking directly predictthe correct state hypothesis by leveraging discrim-inatively trained conditional models of the formb(g) = P (g|f), where f are features extractedfrom various sources, e.g.
ASR, SLU, dialog his-tory, etc.
In this work we will use maximum en-tropy models.
We begin by briefly introducingthese models in the next subsection.
We then de-scribe the features used, and finally review exist-ing discriminative approaches for state trackingwhich serve as a starting point for the new ap-proach we introduce in Section 5.4694.1 Maximum entropy modelsThe maximum entropy framework (Berger et al(1996)) models the conditional probability distri-bution of the label y given features x, p(y|x) viaan exponential model of the form:P (y|x, ?)
= exp(?i?I ?i?i(x, y))?y?Y exp(?i?I ?i?i(x, y))(2)where ?i(x, y) are feature functions jointly de-fined on features and labels, and ?i are the modelparameters.
The training procedure optimizes theparameters ?i to maximize the likelihood over thedata instances subject to regularization penalties.In this work, we optimize the L1 penalty using across-validation process on the train set, and weuse a fixed L2 penalty based on heuristic based onthe dataset size.
The same optimization is used forall models.4.2 FeaturesDiscriminative approaches for state tracking relyon informative features to predict the correct di-alog state.
In this work we designed a set ofhypothesis-specific features that convey informa-tion about the correctness of a particular state hy-pothesis, and a set of general features that conveyinformation about the correctness of the rest meta-hypothesis.Hypothesis-specific features can be groupedinto 3 categories: base, history and confusion fea-tures.
Base features consider information aboutthe current turn, including rank of the current SLUresult (current hypothesis), the SLU result confi-dence score(s) in the current N-best list, the differ-ence between the current hypothesis score and thebest hypothesis score in the current N-best list, etc.History features contain additional useful informa-tion about past turns.
Those include the number oftimes an SLU result has been observed before, thenumber of times an SLU result has been observedbefore at a specific rank such as rank 1, the sumand average of confidence scores of SLU resultsacross all past recognitions, the number of possi-ble past user negations or confirmations of the cur-rent SLU result etc.Confusion features provide information aboutlikely ASR errors and confusability.
Some recog-nition results are more likely to be incorrect thanothers ?
background noise tends to trigger certainresults, especially short bus routes like ?p?.
More-over, similar sounding phrases are more likely tobe confused.
The confusion features were com-puted on a subset of the training data.
For eachSLU result we computed the fraction of the timethat the result was correct, and the binomial 95%confidence interval for that estimate.
Those twostatistics were pre-computed for all SLU resultsin the training data subset, and were stored in alookup table.
At runtime, when an SLU hypoth-esis is recognized, its statistics from this lookuptable are used as features.
Similar statistics werecomputed for prior probability of an SLU resultappearing on an N-best list, and prior probabilityof SLU result appearance at specific rank positionsof an N-best list, prior probability of confusion be-tween pairs of SLU results, and others.General features provide aggregate informationabout dialog history and SLU results, and areshared across different SLU results of an N-bestlist.
For example, from the current turn, we usethe number of distinct SLU results, the entropyof the confidence scores, the best path score ofthe word confusion network, etc.
We also includefeatures that contain aggregate information aboutthe sequence of all N-best lists up to the currentturn, such as the mean and variance of N-best listlengths, the number of distinct SLU results ob-served so far, the entropy of their correspondingconfidence scores, and others.We denote the number of hypothesis-specificfeatures as M , and the number of general featuresasK.
K andM are each in the range of 100?200,although M varies depending on whether historyand confusion features are included.
For a givendialog turn with G state hypotheses, there are a to-tal of G ?M +K distinct features.4.3 Fixed-length discriminative statetrackingIn past work, Bohus and Rudnicky (2006) intro-duced discriminative state tracking, casting theproblem as standard multiclass classification.
Inthis setup, each turn constitutes one data instance.Since in dialog state tracking the number of statehypotheses varies across turns, Bohus and Rud-nicky (2006) chose a subset of G?
state hypothe-ses to score.
In this work we used a similarsetup, where we considered the top G1 SLU re-sults from the current N-best list at turn t, and thetop G2 and G3 SLU results from the previous N-best lists at turns t ?
1 and t ?
2.
The problemcan then be formulated as multiclass classification470over G?+1 = G1+G2+G3+1 classes, where thecorrect class indicates which of these hypotheses(or rest) is correct.
We experimented with differ-ent values and found that G1 = 3, G2 = 2, andG3 = 1 (G?
= 6) yielded the best performance.Feature functions are defined in the standardway, with one feature function ?
and weight ?
foreach (feature,class) pair.
Formally, ?
of eq.
2 isdefined as ?i,j(x, y) = xi?
(y, j), where ?
(y, j) =1 if y = j and 0 otherwise.
i indexes over theG?M +K features and j over the G?
+ 1 classes.1The two-dimensional subscript i, j if used for clar-ity of notation, but is otherwise identical in role tothe one-dimension subscript i in Eq 2.
Figure 2 il-lustrates the relationship between hypotheses andweights.Results are reported as DISCFIXED in Table 3.In the MATCH conditions, performance is gener-ally higher than the other baselines, particularlywhen confusion features are included.
In the MIS-MATCH condition, performance is worse that theBASELINE.A strength of this approach is that it enablesfeatures from every hypothesis to independentlyaffect every class.
However, the total numberof feature functions (hence weights to learn) is(G?
+ 1) ?
(G?M +K), which increases quadrat-ically with the number of hypotheses consideredG?.
Although regularization can help avoid over-fitting per se, it becomes a more challenging taskwith more features.
Learning weights for each(feature,class) pair has the drawback that the ef-fect of hypothesis-specific features such as confi-dence have to be learned separately for every hy-pothesis.
Also, although we know in advance thatposteriors for a dialog state hypothesis are mostdependent on the features corresponding to thathypothesis, in this approach the features from allhypotheses are pooled together and the model isleft to discover these correspondences via learn-ing.
Furthermore, items lower down on the SLUN-best list are much less likely to be correct: anitem at a very deep position (say 19) might neverbe correct in the training data ?
when this occurs,it is unreasonable to expect posteriors to be esti-mated accurately.As a result of these issues, in practice G?
is lim-ited to being a small number ?
here we found thatincreasing G?
> 6 degraded performance.
Yet with1Although in practice, maximum entropy model con-straints render weights for one class redundant.G?
= 6, we found that in 10% of turns, the correctstate hypothesis was present but was being dis-carded by the model, which substantially reducesthe upper-bound on tracker performance.
In thenext section, we introduce a novel discriminativestate tracking approach that addresses the abovelimitations, and enables jointly considering an ar-bitrary number of state hypotheses, by exploitingthe structure inherent in the dialog state trackingproblem.5 Dynamic discriminative state trackingThe key idea in the proposed approach is to usefeature functions that link hypothesis-specific fea-tures to their corresponding dialog state hypoth-esis.
This approach makes it straightforward tomodel relationships such as ?higher confidence foran SLU result increases the probability of its cor-responding state hypothesis being correct?.
Thisformulation also decouples the number of modelsparameters (i.e.
weights to learn) from the numberof hypotheses considered, allowing an arbitrarynumber of dialog states hypotheses to be scored.Figure 2: The DISCFIXED model is a traditionalmaximum entropy model for classification.
Everyfeature in every hypothesis is linked to every hy-pothesis, requiring (G?+ 1)(G?M +K) weights.We begin by re-stating how features are in-dexed.
Recall each dialog state hypothesis has Mhypothesis-specific features; for each hypothesis,we concatenate these M features with the K gen-eral features, which are identical for all hypothe-ses.
For the meta-hypothesis rest, we again con-catenateM+K features, where theM hypothesis-specific features take special undefined values.
Wewrite xgi to refer to the ith feature of hypothesis g,where i ranges from 1 to M +K and g from 1 toG+ 1.471Figure 3: The DISCDYN model presented in thispaper exploits the structure of the state trackingproblem.
Features are linked to only their ownhypothesis, and weights are shared across all hy-potheses, requiring M +K weights.algorithm descriptionBASELINE simple hand-crafted ruleGENONLINE generative update, in deployed systemGENOFFLINE generative update, re-trained and run offlineDISCFIXED discr.
fixed size multiclass (7 classes)DISCDYN1 discr.
joint dynamic estimationDISCDYN2 discr.
joint dynamic estimation, using indicatorencoding of ordinal featuresDISCDYN3 discr.
joint dynamic estimation, using indicatorencoding and ordinal-ordinal conjunctionsDISCIND discr.
separate estimationTable 2: Description of the various implementedstate tracking algorithmsThe model is based on M + K feature func-tions.
However, unlike in traditional maximumentropy models such as the fixed-position modelabove, these features functions are dynamicallydefined when presented with each turn.
Specif-ically, for a turn with G hypotheses, we define?i(x, y = g) = xgi , where y ranges over theset of possible dialog states G + 1 (and as abovei ?
1 .
.
.M +K).
The feature function ?i is dy-namic in that the domain of y ?
i.e., the number ofdialog state hypotheses to score ?
varies from turnto turn.
With feature functions defined this way,standard maximum entropy optimization is thenapplied to learn the corresponding set of M + Kweights, denoted ?i.
Fig.
3 shows the relationshipof hypotheses and weights.In practice, this formulation ?
in which generalfeatures are duplicated across every dialog statehypothesis ?
may require some additional featureengineering: for every hypothesis g and generalfeature i, the value of that general feature xgi willbe multiplied by the same weight ?i.
The resultis that any setting of ?i affects all scores identi-cally, with no net change to the resulting poste-rior.
Nonetheless, general features do contain use-ful information for state tracking; to make use ofthem, we add conjunctions (combinations) of gen-eral and hypothesis-specific features.We use 3 different feature variants.
In DIS-CDYN1, we use the original feature set, ignor-ing the problem described above (so that the gen-eral features contribute no information), result-ing in M + K weights.
DISCDYN2 adds indi-cator encodings of the ordinal-valued hypothesis-specific features.
For example, rank is encodedas a vector of boolean indicators, where the firstindicator is nonzero if rank = 1, the second isnonzero if rank = 2, and the third if rank ?3.
This provides a more detailed encoding ofthe ordinal-valued hypothesis-specific features, al-though it still ignores information from the gen-eral features.
This encoding increases the numberof weights to learn to about 2(M +K).Finally, DISCDYN3 extends DISCDYN2 by in-cluding conjunctions of the ordinal-valued generalfeatures with ordinal-valued hypothesis-specificfeatures.
For example, if the 3-way hypothesis-specific indicator feature for rank described abovewere conjoined with a 4-way general indicatorfeature for dialog state, the result would be an in-dicator of dimension 3 ?
4 = 12.
This expansionresults in approximately 10(M + K) weights tolearn in DISCDYN3.2For comparison, we also estimated a simpleralternative model, called DISCIND.
This modelconsists of 2 binary classifiers: the first onescores each hypothesis in isolation, using the Mhypothesis-specific features for that hypothesis +the K general features for that turn, and outputs a(single) probability that the hypothesis is correct.For this classifier, each hypothesis (not each turn)defines a data instance.
The second binary clas-sifier takes the K general features, and outputs aprobability that the rest meta-hypothesis is correct.For this second classifier, each turn defines onedata instance.
The output of these two models isthen calibrated with isotonic regression (Zadroznyand Elkan (2002)) and normalized to generate theposterior over all hypotheses.2We explored adding all possible conjunctions, includingreal-valued features, but this increased memory and computa-tional requirements dramatically without performance gains.472Metric Accuracy (larger numbers better) L2 (smaller numbers better)Dataset MATCH1 MATCH2 MISMATCH MATCH1 MATCH2 MISMATCHFeatures b bc bch b bc bch b bc bch b bc bch b bc bch b bc bchBASELINE 61.5 61.5 61.5 63.4 63.4 63.4 62.5 62.5 62.5 27.1 27.1 27.1 25.5 25.5 25.5 27.3 27.3 27.3GENONLINE 54.4 54.4 54.4 55.8 55.8 55.8 54.8 54.8 54.8 34.8 34.8 34.8 32.0 32.0 32.0 34.8 34.8 34.8GENOFFLINE 57.1 57.1 57.1 60.1 60.1 60.1 51.8 51.8 51.8 37.6 37.6 37.6 33.4 33.4 33.4 42.0 42.0 42.0DISCFIXED 61.9 66.7 65.3 63.6 69.7 68.8 59.1 61.9 59.3 27.2 23.6 24.4 25.8 21.9 22.4 28.9 27.8 27.8DISCDYN1 62.0 70.9 71.1 64.4 72.4 72.9 59.4 61.8 62.3 26.3 21.3 20.9 25.0 20.4 20.1 27.7 26.3 25.9DISCDYN2 62.6 71.3 71.5 65.7 72.1 72.2 61.9 63.2 63.1 26.3 21.4 21.2 24.4 20.5 20.4 26.9 25.8 25.4DISCDYN3 63.6 70.1 70.9 65.9 72.1 70.7 60.7 62.1 62.9 26.2 21.5 21.4 24.3 20.6 20.7 27.1 25.9 26.1DISCIND 62.4 69.8 70.5 63.4 71.5 71.8 59.9 63.3 62.2 26.7 23.3 22.5 25.7 21.8 20.7 28.4 27.3 28.8Table 3: Performance of the different algorithms on each dataset using three feature combinations.
Basefeatures are denoted as b, ASR/SLU confusion features as c and history features as h. Performance forthe feature combinations bh is omitted for space; it is between b and bc.6 Results and discussionThe implemented state tracking methods are sum-marized in Table 2, and our results are presented inTable 3.
These results suggest several conclusions.First, discriminative approaches for state track-ing broadly outperform generative methods.
Sincediscriminative methods incorporate many featuresand are trained directly to optimize performance,this is perhaps unsurprising for the MATCH con-ditions.
It is interesting that discriminative meth-ods are also superior in the more realistic MIS-MATCH setting, albeit with smaller gains.
Thisresult suggests that discriminative methods havegood promise when deployed into real systems,where mismatch between training and test distri-butions is expected.Second, the dynamic discriminative DISCDYNmodels also outperformed the fixed-length dis-criminative methods.
This shows the benefit ofa model which can score every dialog state hy-potheses, rather than a fixed subset.
Third, thethree variants of the DISCDYN model, which pro-gressively contain more detailed feature encodingand conjunctions, perform similarly.
This suggeststhat a relatively simple encoding is sufficient toachieve good performance, as the feature indica-tors and conjunctions present in DISCDYN2 andDISCDYN3 give only a small additional increase.Among the discriminative models, the jointly-optimized DISCDYN versions also slightly out-perform the simpler, independently-optimized DI-SCIND version.
This is to be expected, for two rea-sons: first, DISCIND is trained on a per-hypothesisbasis, while the DISCDYN models are trained on aper-turn basis, which is the true performance met-ric.
For example, some turns have 1 hypothesisand others have 100, but DISCIND training countsall hypotheses equally.
Second, model parametersin DISCIND are trained independently of compet-ing hypotheses.
However, they should rather beadjusted specifically so that the correct item re-ceives a larger score than incorrect items ?
notmerely to increase scores for correct items and de-crease scores for incorrect items in isolation ?
andthis is what is done in the DISCDYN models.The analysis of various feature sets indicatesthat the ASR/SLU error correlation (confusion)features yield the largest improvement ?
c.f.
fea-ture set bc compared to b in Table 3.
The im-provement is smallest for MISMATCH, which un-derscores the challenges of mismatched train andtest conditions during a realistic runtime scenario.Note, however, that we have constructed a highlymismatched case where we train on DS1 (that sup-ports just 8 routes) and test on DS2 (that supports40 routes).
Therefore, many route, origin and des-tination slot values in the test data do not appearin the training data.
Hence, it is unsurprising thatthe positive effect of confusion features would de-crease.While Table 3 shows performance measures av-eraged across all turns, Table 4 breaks down per-formance measures by slot, using the full featureset bch and the realistic MISMATCH dataset.
Re-sults here show a large variation in performanceacross the different slots.
For the date and timeslots, there is an order of magnitude less data thanfor the other slots; however performance for datesis quite good, whereas times is rather poor.
Webelieve this is because the SLU confusion featurescan be estimated well for slots with small cardinal-ities (there are 7 possible values for the day), andless well for slots with large cardinalities (there are24 ?
60 = 1440 possible time values).
This sug-473Accuracy (larger numbers better)algorithms rout origin dest.
date timeBASELINE 53.81 66.49 67.78 71.88 52.32GENONLINE 50.02 54.11 59.05 75.78 35.02GENOFFLINE 48.12 58.82 58.98 72.66 20.25DISCFIXED 52.83 67.81 70.67 71.88 33.34DISCDYN1 54.28 68.24 68.53 79.69 40.51DISCDYN2 56.18 68.42 70.10 80.47 40.51DISCDYN3 54.52 66.24 67.96 82.81 43.04DISCIND 54.25 68.84 70.79 78.13 38.82L2 metric (smaller numbers better)algorithms route origin dest.
date timeBASELINE 33.15 24.67 24.68 21.61 32.35GENONLINE 35.50 35.10 31.13 19.86 52.58GENOFFLINE 46.42 35.73 37.76 19.97 70.30DISCFIXED 34.09 23.92 23.35 17.59 40.15DISCDYN1 31.30 23.01 23.07 15.29 37.02DISCDYN2 30.53 22.40 22.74 13.58 37.59DISCDYN3 31.58 23.86 23.68 13.93 37.52DISCIND 36.50 23.45 23.41 15.20 45.43Table 4: Performance per slot on dataset MIS-MATCH using the full feature set bch.
(a) MISMATCH dataset (b) MATCH2 datasetFigure 4: Accuracy vs. amount of training datagests that the amount of data required to estimate agood model may depend on the cardinality of slotvalues.Finally, in Figure 4 we show how performancevaries with different amounts of training data forthe MATCH2 and MISMATCH datasets, where thefull training set size is approximately 5600 and4400 turns, respectively.
In both cases asymptoticperformance is reached after about 2000 turns, orabout 150 dialogs.
This is particularly encour-aging, as it suggests models could be learned oradapted online with relatively little data, or couldeven be individually tailored to particular users.7 Conclusion and Future WorkDialog state tracking is crucial to the successfuloperation of spoken dialog systems.
Recently de-veloped statistical approaches are promising asthey fully utilize the dialog history, and can in-corporate priors from past usage data.
However,existing methodologies are either limited in theiraccuracy or their coverage, both of which hamperperformance.In this paper, we have introduced a new modelfor discriminative state tracking.
The key idea is toexploit the structure of the problem, in which eachdialog state hypothesis has features drawn fromthe same set.
In contrast to past approaches to dis-criminative state tracking which required a num-ber of parameters quadratic in the number of statehypotheses, our approach uses a constant numberof parameters, invariant to the number of state hy-potheses.
This is a crucial property that enablesgeneralization and dealing with an unlimited num-ber of hypotheses, overcoming a key limitation inprevious models.We evaluated the proposed method and com-pared it to existing generative and discrimina-tive approaches on a corpus of real-world human-computer dialogs chosen to include a mismatchbetween training and test, as this will be foundin deployments.
Results show that the proposedmodel exceeds both the accuracy and probabil-ity quality of all baselines when using the rich-est feature set, which includes information aboutcommon ASR confusions and dialog history.
Themodel can be trained efficiently, i.e.
only about150 training dialogs are necessary.The next step is to incorporate this approachinto a deployed dialog system, and use the esti-mated posterior over dialog states as input to theaction selection process.
In future, we also hopeto explore unsupervised online adaptation, wherethe trained model can be updated as test data isprocessed.AcknowledgmentsWe thank Patrick Nguyen for helpful discussionsregarding maximum entropy modeling and fea-ture functions for handling structured and dynamicoutput classification problems.ReferencesAT&T Statistical Dialog Toolkit.
AT&T StatisticalDialog Toolkit.
http://www2.research.att.com/sw/tools/asdt/, 2013.Adam Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
A maximum entropy approachto natural language processing.
ComputationalLinguistics, 22:39?71, 1996.474Alan W. Black, S. Burger, B. Langner, G. Par-ent, and M. Eskenazi.
Spoken dialog challenge2010.
In Proc.
of Workshop on Spoken Lan-guage Technologies (SLT), 2010.Dan Bohus and Alex Rudnicky.
A k hypothe-ses + other belief updating model.
In Proc.of AAAI Workshop on Statistical and EmpiricalApproaches to Spoken Dialog Systems, 2006.Milica Gas?ic?
and Steve Young.
Effective handlingof dialogue state in the hidden information statepomdp dialogue manager.
ACM Transactionson Speech and Language Processing, 7, 2011.Eric Horvitz and Tim Paek.
A computational ar-chitecture for conversation.
In Proc.
of the 7thIntl.
Conf.
on User Modeling, 1999.Allan H Murphy.
A new vector partition of theprobability score.
Journal of Applied Meteorol-ogy, 12:595?600, 1973.Blaise Thomson and Steve Young.
Bayesian up-date of dialogue state: A POMDP frameworkfor spoken dialogue systems.
Computer Speechand Language, 24(4):562?588, 2010.Jason D. Williams.
Incremental partition recombi-nation for efficient tracking of multiple dialoguestates.
In Proc.
of ICASSP, 2010.Jason D. Williams.
Challenges and opportuni-ties for state tracking in statistical spoken dialogsystems: Results from two public deployments.IEEE Journal of Selected Topics in Signal Pro-cessing, Special Issue on Advances in SpokenDialogue Systems and Mobile Interface, 6(8):959?970, 2012.Jason D. Williams and Suhrid Balakrishnan.
Esti-mating probability of correctness for asr n-bestlists.
In Proc.
SigDial Conference, 2009.Jason D. Williams and Steve Young.
Partially ob-servable markov decision processes for spokendialog systems.
Computer Speech and Lan-guage, 21:393?422, 2007.Jason D. Williams, Iker Arizmendi, and AlistairConkie.
Demonstration of AT&T Let?s Go: Aproduction-grade statistical spoken dialog sys-tem.
In Proc of Workshop on Spoken LanguageTechnologies (SLT), 2010.Steve Young, Milica Gas?ic?, Simon Keizer,Franc?ois Mairesse, Jost Schatzmann, BlaiseThomson, and Kai Yu.
The hidden informa-tion state model: a practical framework forPOMDP-based spoken dialogue management.Computer Speech and Language, 24(2):150?174, 2010.Bianca Zadrozny and Charles Elkan.
Transform-ing classifier scores into accurate multiclassprobability estimates.
In Proc.
of the eighthACM SIGKDD Intl.
Conf on Knowledge Dis-covery and Data mining, pages 694?699, 2002.475
